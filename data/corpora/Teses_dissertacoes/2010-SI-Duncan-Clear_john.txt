As empresas de software investem cada vez mais tempo e recursos na melhoria de seus processos.
Em este contexto, a mineração de processos pode ser de grande valor servindo como ferramenta para a análise dos processos executados a partir de dados extraídos do próprio ambiente de execução e sistemas de gestão utilizados.
Em este trabalho, são discutidos aspectos envolvendo a análise de processos correspondentes ao desenvolvimento de software, utilizando mineração.
Foi realizado um estudo de caso exploratório utilizando dados de métricas provenientes de um projeto de manutenção de uma grande operação de software.
Os resultados serviram como entrada para uma análise exploratória assim como para uma reflexão acerca de as questões que envolvem o processo de descoberta de conhecimento nesse tipo de ambiente.
Além disso, o presente trabalho demonstra um cenário prático de aplicação para análise de conformidade aplicando ferramentas de mineração de processos num processo de software.
Palavras-chave: Análise de Conformidade, Processos de Desenvolvimento de Software, Mineração de Processos.
Produtos de software e seus respectivos projetos são tarefas complexas atualmente.
Processos de desenvolvimento de software (PDS) realizados por grandes corporações podem envolver dezenas de colaboradores, executando um grande número de tarefas numa complexa cadeia de interações.
Sendo assim, a definição de modelos de desenvolvimento padronizados e devidamente controlados torna- se fundamental para o sucesso do negócio.
Prova da importância da formalização, controle de desempenho e garantia de qualidade de processos é o fato de tais requisitos serem áreas chave em modelos de qualidade como CMMI e Iso.
Esses modelos de maturidade reúnem requisitos e práticas consideradas essenciais no desenvolvimento e manutenção de produtos de alto nível de qualidade e valor agregado.
O ambiente de processos na engenharia de software costuma ser bem mais dinâmico do que em outras áreas da indústria.
Um aspecto importante está relacionado à forma de interação entre os participantes.
Projetos de software são altamente cooperativos.
São geralmente desenvolvidos por equipes multidisciplinares, interagindo constantemente durante todo o processo.
Seus colaboradores costumam lidar com múltiplos documentos, onde o compartilhamento de informações e revisões são a tônica.
Além disso, PDS têm uma característica intrinsecamente estocástica.
É difícil fazer generalizações acerca de o perfil de execução de projetos possuindo, cada um, metodologias, formas de interação e prazos bem específicos.
O projeto de um sistema bancário, por exemplo, exige uma abordagem de desenvolvimento e testes bastante distinta de um portal de informações web onde não se trabalha com informações críticas.
Fatores como tipo de projeto, tamanho da equipe, prazo e orçamento acabam afetando consideravelmente o processo adotado por o projeto.
Motivação Diante de o contexto descrito acima, empresas de software atualmente têm empregado esforço e reconhecem os benefícios da formalização, maior controle e melhoria de seus processos como diferencial competitivo.
A definição de um conjunto de processos padronizado é um aspecto fundamental nesse contexto.
Por outro lado, há a necessidade de garantir a aderência das pessoas aos processos definidos.
Normalmente há alguma distância entre esses modelos formais e a realidade de execução.
Em médias e grandes empresas, a tarefa de analisar a conformidade entre planejamento e execução dos processos é realizada normalmente por uma equipe independente.
Esta busca identificar e comunicar o desempenho dos projetos e desvios de execução do processo.
Os resultados são obtidos geralmente por a realização de auditorias, entrevistas, rastreamento de documentos e análise de métricas.
No entanto, manter a visibilidade e controle adequados em processos de Ti não é uma tarefa simples.
Diversos autores expressam a necessidade de se obter indicadores apropriados para governança em processos de Ti.
Além disso, o custo para manter uma equipe de qualidade é muitas vezes um fator discutível em projetos de software.
Tipicamente, empresas de Ti visam máxima produtividade com menor número de pessoas possível, de preferência focadas no projeto e desenvolvimento.
Atividades de gestão de qualidade são vistas em certos casos como &quot;improdutivas», principalmente por ser mais difícil mensurar suas contribuições sobre o sucesso dos projetos.
Atualmente, técnicas automáticas de extração de conhecimento têm sido aplicadas sobre registros de execução de processos de negócio, buscando obter conhecimento mais preciso e detalhado sobre a sua execução.
Tal conjunto de técnicas é conhecido como mineração de processos.
O objetivo principal é a extração de conhecimento -- voltado a processos -- sobre conjuntos de eventos.
A mineração de processos tem sido explorada amplamente nos últimos anos.
Em a linha de descoberta de processos, conjuntos de eventos são submetidos a algoritmos de mineração com o objetivo de induzir padrões de execução recorrentes.
Além disso, a metodologia pode ser aplicada no intuito de verificar se os processos definidos são realmente seguidos por seus participantes.
Esse é objetivo da análise de conformidade (compliance/ conformance analysis).
Contribuições O presente trabalho apresenta um estudo exploratório envolvendo a aplicação de mineração de processos em dados de execução de PDS.
O trabalho explora e discute formas, dificuldades e resultados potenciais relacionados à descoberta de conhecimento sobre processos num ambiente típico de desenvolvimento de sofware, quando utilizada para verificar a distância entre o que é documentado por uma empresa de Ti de grande porte e a realidade.
Todo o estudo é baseado num ambiente real de produção de software, motivado e viabilizado por a parceria de pesquisa PUCRS/ HP Brasil.
A experiência dentro de o grupo de garantia de qualidade da empresa foi um fator fundamental para o desenvolvimento do trabalho.
Essa interação permitiu compreender todo o trabalho, benefícios e desafios envolvidos na realização de auditorias de projetos de software.
Também foi possível explorar os processos organizacionais e registros de atividades de um dos projetos da empresa parceira, resultando no estudo de caso descrito neste trabalho.
Organização do Texto O trabalho possui a seguinte estrutura:
O capítulo 2 apresenta a fundamentação teórica abordando os temas, qualidade de software, conformidade de processos e mineração de processos.
O terceiro capítulo descreve o cenário em o qual a pesquisa está inserida, discutindo também o problema abordado no trabalho.
O capítulo 4 apresenta o estudo realizado sobre a aplicação de mineração de processos e análise de conformidade em PDS.
Primeiramente foram realizados experimentos envolvendo descoberta de modelos de processos sobre dados de execução de um dos projetos da empresa parceira.
Além disso, uma solução de verificação de conformidade é explorada num cenário prático, também utilizando dados reais da empresa.
A o final deste capítulo, foi compilada uma série de sugestões relativas à aplicação de mineração de processos visando comparar formalmente, utilizando algoritmos apropriados, modelo e execução de processos num ambiente de desenvolvimento de software.
O capítulo 5 discute alguns trabalhos relacionados à esta pesquisa.
Conclusões e trabalhos futuros são apresentados no capítulo 6.
Este capítulo apresenta os principais conceitos relacionados ao tema de pesquisa.
Os seguintes assuntos são abordados:
Qualidade de software, conformidade de processos, mineração de processos redes de Petri e análise de conformidade utilizando mineração de processos.
Qualidade de software O trabalho na área de qualidade de software se concentra em desenvolver e aplicar métodos que permitam quantificar o estado atual de um produto ou projeto, com relação a o cumprimento de seus objetivos de negócio.
Em projetos de engenharia complexos, como grande parte do desenvolvimento de software atual, é fundamental o estabelecimento de políticas e práticas de garantia de qualidade dos produtos e serviços fornecidos.
Gestão de qualidade de software A gestão de qualidade de software se aplica a todas as perspectivas:
De processos, produtos e recursos.
Entre outros objetivos, estão envolvidos a definição de procedimentos, hierarquias, medidas e canais de feedback que permitam monitorar e melhorar a qualidade.
A quantificação do desempenho de um processo de software bem como a qualidade do produto decorrente desse processo costuma ser avaliada por meio de métricas.
A engenharia de software tem explorado cada vez mais formas de monitorar e controlar processos por meio de a coleta de métricas.
Os chamados programas de métricas estabelecem todo um conjunto de práticas de coleta, análise e produção de indicadores voltados a informar a qualidade e desempenho de um PDS.
A qualidade do produto está diretamente ligada com a qualidade do processo.
É comum ouvirmos essa frase.
Tal expressão sintetiza o fato de que é difícil se obter um produto de alta qualidade derivado de um processo sem a formalização e controle adequados.
A monitoração através de métricas é parte desse controle na engenharia de software.
Seus indicadores permitem comparar numericamente objetivos com o estado atual do produto/ processo durante seu desenvolvimento, de forma a manter um nível de qualidade e/ ou desempenho dentro de padrões estabelecidos (exemplos:
Número máximo de defeitos x defeitos encontrados, custo estimado x custo real).
No entanto, também se faz necessário monitorar a forma com que cada atividade é desenvolvida, buscando efetivamente garantir a qualidade do processo como um todo.
Em PDS geralmente há profissionais dedicados a essa atividade.
Voltado a qualidade de produtos, atividades de agentes ou equipes de SQA normalmente incluem a verificação sobre o cumprimento de requisitos de funcionalidade e qualidade do software construído.
Já com relação a os processos, o papel da área de SQA é garantir que todas as atividades e políticas organizacionais sejam cumpridas adequadamente, ou seja, de acordo com os padrões definidos por a organização.
Atualmente, empresas que buscam melhoria de qualidade tem se utilizado de modelos padronizados como SPICE, CMMI (Capability Maturity Model Integration) e MPS.
Br. Esses modelos servem como guia para a obtenção de processos gerenciados e otimizados para garantia de máxima qualidade e previsibilidade.
A próxima seção trata com mais detalhes os modelos de maturidade adotados por empresas de software atualmente, dando enfoque ao modelo CMMI.
Modelos de qualidade de software A competitividade e complexidade do mercado de software têm levado as empresas a voltarem cada vez mais as atenções a seus processos.
Essas empresas buscam as melhores práticas de gerenciamento e desenvolvimento, geralmente baseadas em modelos de maturidade como aqueles citados na seção anterior.
A adoção de tais padrões é útil tanto para as empresas, atuando na melhoria de seus processos, quanto para os clientes que podem assegurar- se de um nível de qualidade do produto/ serviço que estão adquirindo.
A seguir, é feita uma breve descrição do modelo CMMI.
Trata- se de um conjunto de práticas para a formalização e melhoria de processos de engenharia.
A literatura coloca o CMMI como um modelo de qualidade amplamente adotado por empresas de software atualmente.
Por este motivo, o modelo é adotado neste trabalho como referência na gestão de qualidade e melhoria de processos em PDS.
Além de uma visão geral do modelo, são salientadas as áreas de maior interesse para este trabalho.
O modelo CMMI tem sido amplamente adotado como conjunto de práticas padrão para a garantia de qualidade e melhoria de PDS.
Sua estrutura é baseada em níveis de maturidade, que por sua vez contêm as chamadas Áreas de Processo (AP) como elemento base.
Cada AP engloba um conjunto de objetivos gerais e específicos para o gerenciamento e melhoria de processos e projetos de engenharia.
O CMMI considera as áreas de processos sob duas perspectivas:
De estágios e contínua.
A representação por estágios utiliza conjuntos de AP predefinidas, que representam um caminho de melhoria para a organização.
Cada conjunto de componentes define um nível de maturidade.
A representação contínua permite que se selecione cada AP independentemente, realizando melhorias com um foco específico.
O CMMI considera que toda a organização está, por definição, no nível 1 de maturidade.
Este nível representa a ausência de controle formal dos processos e da qualidade de seus produtos.
Cada nível de maturidade CMMI é focado num aspecto específico da organização.
O nível 2 considera os processos como gerenciados a nível de projetos, onde cada projeto possui atividades de planejamento, monitoração, controle e avaliação de aderência à defi nição de seus próprios processos.
O terceiro nível tem foco na definição e gerenciamento Processes), devem ser estabelecidos todos os procedimentos, medidas e produtos de trabalho considerados padrão para o desenvolvimento de projetos da organização.
O nível 4, gerenciado quantitativamente, sugere o controle dos processos através da análise de métricas, utilizando avaliação estatística ou outra técnica de análise quantitativa.
O nível 5 caracteriza os processos como otimizados.
É baseado no entendimento acerca de as causas comuns de problemas e desvios do processo, buscando a melhoria contínua através da inovação e desenvolvimento organizacional.
Analisando as AP do CMMI separadamente, observa- se que diversos pontos salientam, direta ou indiretamente, a importância em monitorar os processos como forma de garantir eficiência e qualidade.
Em esse aspecto, quatro AP são de particular interesse para este trabalho.
São elas:·
Garantia de Qualidade do Processo e Produto Esta AP trata da avaliação de processos e produtos de trabalho.
Seus objetivos gerais envolvem:·
Foco no Processo da Organização O propósito dessa AP envolve planejar, desenvolver e implementar melhorias de processo baseadas num profundo entendimento sobre os aspectos positivos e negativos dos processos organizacionais.
Processos organizacionais incluem todos os processos adotados por a organização e seus projetos.
Propriedades dos processos e possíveis pontos de melhoria são obtidas de várias fontes como medição, lições aprendidas, resultados de estimativas, benchmarks ou qualquer outra iniciativa de melhoria na organização.·
Gerenciamento Quantitativo de Projeto O gerenciamento quantitativo de projeto envolve várias questões relacionadas a processos, como:
Considerações sobre a seção Com base na análise do modelo CMMI, é possível concluir que iniciativas de medição e análise de aderência aos processos definidos são conceitos bastante presentes, seja em nível de projeto ou organizacional.
Análise de desempenho de processos, detecção de desvios e suas causas devem também ser questões abordadas por empresas que buscam altos níveis de produtividade e qualidade de seus produtos.
De nada adianta definir processos se não for possível garantir de alguma forma que as pessoas os sigam e que sejam eficientes, resultando numa real contribuição para os objetivos da organização.
Sob essa perspectiva, as AP citadas anteriormente constituem em motivadores para este trabalho.
Outro fator de motivação da pesquisa, dentro de a empresa parceira, está relacionada a uma iniciativa de renovação da certificação CMM-3 (para o atual CMMI-3) e possivelmente obtenção de níveis mais altos.
O padrão CMMI foi apresentado aqui por dois motivos:
Fazer parte do contexto da empresa parceira como modelo de qualidade;
Ser um modelo amplamente adotado na indústria de software.
Entretanto, considera- se que os aspectos discutidos acima devem receber igual importância em qualquer modelo de melhoria de processo.
Fica claro que modelos de melhoria de processos de software consideram importante a análise de aderência entre processos definidos e executados.
Porém, uma questão importante a considerar é que tais modelos costumam descrever o que deve ser feito, e não como fazer.
A forma com que os objetivos apontados serão alcançados fica a cargo de cada organização.
Logo, qualquer empresa engajada numa iniciativa de melhoria deve encontrar formas efetivas de monitorar seus processos.
O presente trabalho envolve monitoração de processos com o objetivo de verificar conformidade entre o processo formalizado e sua execução.
Técnicas de extração automática de conhecimento sobre dados de execução (mineração de processos) são exploradas como uma ferramenta interessante visando, não só facilitar a tarefa de verificação, como aumentar a objetividade dos resultados providos por os métodos tradicionais de auditoria.
Conformidade de processos Definição Segundo a Workflow Management Coalition1 um processo de negócio, ou simplesmente um processo, é &quot;um conjunto de tarefas interligadas, desenvolvidas por um grupo de recursos dentro de uma estrutura organizacional, com um determinado objetivo de negócio».
A fim de padronizar procedimentos e obter melhor controle, processos de negócio costumam ser formalizados de acordo com alguma notação.
Atualmente é possível encontrar uma grande variedade de modelos de processos, em todo o tipo de organização e com os mais diversos propósitos.
Esses modelos podem ser simples fluxogramas ou descrições formais complexas de workflow (utilizadas como referência para a execução de sistemas de gerência de workflow como Oracle Workflow, FLOWer, Staffware, etc.).
Eles descrevem como um determinado processo de negócio deve ser executado, a sequência de atividades, papéis envolvidos e regras de negócio necessárias para o cumprimento dos objetivos do processo.
No entanto, modelos de processos não produzem resultados sozinhos.
Normalmente processos de negócio envolvem pessoas, em ambientes dinâmicos, onde nem sempre é possível seguir todas as regras estabelecidas num modelo formal.
Alguma distância entre o formal e a realidade sempre existe.
Análise de conformidade de processos procura determinar o grau de semelhança entre o processo que é realmente executado e aquele que se espera que o seja.
Entidade sem fins lucrativos, criada em 1993 com o objetivo de estabelecer padrões e terminologias internacionais na área de workflow2 e processos de negócio Motivação Existem diversas razões que tornam a análise de conformidade um tópico importante.
A confiança num modelo de processo formal, por exemplo, é elevada quando se pode mostrar que a execução efetiva do processo é consistente com o modelo prescrito.
Por outro lado, as informações obtidas podem levantar necessidades de evolução no processo para acomodar novas regras e atividades.
De forma geral, busca- se a aderência de processos para:·
Garantir uma execução de processo estável e previsível;·
Garantir a validade dos dados, informações, experiências e conhecimento adquiridos durante o processo;·
Garantir que a execução do processo satisfaça determinados requisitos como por exemplo, um modelo de certificação;·
Reduzir tempos e custos, na medida em que se possui um maior conhecimento e controle sobre os processos;
Conformidade de processos na Engenharia de Software Sempre que o modelo de um processo é criado, a questão que surge é se aquele modelo captura com fidelidade o ambiente.
Em a área de sistemas de software, onde o modelo é tipicamente integrado e executado com auxílio de diversas ferramentas de software, essa questão é evitada.
O modelo e o processo são supostos necessariamente aderentes porque o modelo se torna o processo.
Porém, quando aplicada à engenharia de software na prática essa consideração nem sempre é verdadeira.
Em particular, se assume que todo o processo é executado dentro de o ambiente automatizado.
Em a verdade, aspectos críticos como tomadas de decisão e comunicação entre os participantes ocorrem fora de o computador e, dessa forma, não estão sob o controle desses sistemas.
Sendo assim, não há uma forma efetiva de forçar a aderência ao processo, não sendo possível garantir a consistência entre o modelo formal e sua real execução.
Mesmo que exista uma maneira de garantir a tal aderência, ainda existe o problema em gerenciar mudanças nos processos.
Desvios acontecem e devem, dentro de certo limite, serem suportados.
De essa forma, faz- se necessário obter métodos para detectar e caracterizar as diferenças entre o modelo formal e execução efetiva de processos.
Conformidade de processos tem recebido diferentes nomes em diferentes contextos.
Além de a expressão conformance, definições semelhantes também são conhecidas como correctness, fidelity e compliance.
Fagan foi um dos primeiros autores a reconhecer a importância do tema relacionado à engenharia de software, ao desenvolver técnicas de inspeção.
O mesmo autor ressalta a importância de se executar processos de forma completa e correta, sendo derivado daí o termo correctness nesse contexto.
O tema também está diretamente ligado à área de Software Process Improvement (SPI).
Em conjuntos de práticas de melhoria como o CMMI, desvios dos processos definidos são considerados uma ameaça ao desempenho.
A menos que esses desvios sejam rastreados, revisados e registrados, eles irão acarretar degradação e descontrole sobre os processos com consequente perda de qualidade.
Em a literatura são identificadas três principais maneiras de verificar conformidade em processos de software:·
A realização de entrevistas é uma forma.
É um método simples e flexível, não exigindo ferramentas ou modelos formais.
Podem ser aplicadas a qualquer processo.
Porém, é mais difícil obter informações quantitativas dos resultados.
Além disso, a confiança nas respostas pode ser questionável.
Pode ser difícil para a pessoa entrevistada fornecer as informações necessárias sobre a execução do processo.
Outro problema com entrevistas é que elas exigem tempo e trabalho intensos.·
A segunda abordagem é baseada na execução do modelo formal do processo suportada por um sistema computacional.
Em a verdade nesse caso a conformidade pode ser garantida, mais do que medida.
Isto porque o processo é executado sob o comando do sistema computacional.
Mesmo provendo meios de medir a conformidade, nem sempre essa informação é disponível diretamente.
Partes do processo que ocorrem fora de o computador também se tornam difíceis de monitorar.
A diminuição na flexibilidade do processo também é uma questão a ser considerada.
Isto porque a execução do processo se torna restrita ao fluxo de execução determinado por o sistema.·
Uma terceira forma de monitorar processos utiliza algoritmos aplicados a fluxos de eventos do modelo formal e sua execução para comparar as diferenças entre os dois.
Embora originalmente voltado à descoberta de modelos de processo, o termo mineração de processos também tem sido relacionado à verificação de conformidade utilizando essa abordagem.
O principal ponto em comum é a extração de padrões recorrentes a partir de um conjunto de eventos, provendo informações sobre a realidade de execução de um processo.
Mineração de processos Definição Mineração de processos surgiu como uma forma de analisar sistemas e seu uso efetivo, baseado no log de eventos produzidos por esses sistemas.
O assunto tem sido amplamente pesquisado nos últimos anos.
O tema possui estreita relação com o conceito clássico de mineração de dados.
Segundo Han, mineração de dados refere- se à &quot;extração de conhecimento de grandes quantidades de dados, por meio automático ou semi-automático, a fim de descobrir padrões e regras significativos».
De forma semelhante, a mineração de processos concentra- se na extração de padrões.
Porém, ela é voltada para as relações de sequência e concorrência entre eventos.
Enquanto a mineração de dados concentra- se em evidenciar tendências nos dados e relacionamentos entre atributos, com a mineração de processos busca- se entender como um determinado processo é executado, com base na análise de cada uma de suas instâncias.
Perguntas comuns sobre os processos podem ser:·
Qual o caminho mais frequente no processo?·
Quais são as estruturas de roteamento (E/ OU) para tarefas concorrentes?·
As regras de negócio no processo são realmente obedecidas?·
Onde estão os gargalos do processo?·
Quais as pessoas envolvidas em determinada atividade?·
Qual o fluxo de comunicação entre as pessoas/ sistemas?
A mineração de processos sempre inicia com um log de eventos.
Tais registros são produzidos por praticamente qualquer sistema de informação (desde sistemas bancários e gerenciadores de workflow até redes de sensores e web services).
Em a maioria dos casos, um log registra o início e/ ou conclusão de eventos além de o momento de execução (timestamp).
Em alguns sistemas, o responsável por a execução da atividade e outros dados adicionais também são registrados.
São encontradas na bibliografia inúmeras soluções de descoberta de modelos de processos, que têm seguido basicamente duas frentes:
Mineração de interação entre web services e mineração de workflow (software process discovery ou simplesmente process discovery) (4) (30) (38).
Em a mineração de interação de web services o objetivo é inferir um modelo de processos que represente a interação entre diversos serviços.
O esforço da mineração de workflow concentra- se em descobrir um workflow capaz de modelar o fluxo de controle dentro de um determinado processo.
A principal diferença entre essas duas abordagens é que, enquanto a primeira analisa mensagens trocadas entre serviços que modificam o estado do sistema, a segunda utiliza o conceito de eventos/ atividades para a inferência de modelos de workflow.
A maioria das organizações documenta seus processos de alguma forma.
As razões para tal são diversas.
Modelos de processo podem ser utilizados para comunicação, configuração de sistemas, análise, simulação, entre outros.
Adicionalmente, um modelo de processo pode ser de natureza descritiva ou prescritiva.
Modelos descritivos tentam capturar o ambiente de processo existente de forma não restritiva.
Como exemplo, num procedimento hospitalar deve ser possível reagir a situações de urgência.
Entretanto, a flexibilidade para divergir do fluxo normal do processo definido é fundamental.
Modelos prescritivos descrevem de forma mais rígida a maneira que os processos devem ser executados.
Em sistemas de gerenciamento de workflow, modelos formais são utilizados para imprimir um fluxo específico de execução do processo de negócio.
Em qualquer um dos casos descritos acima, a descoberta de conhecimento sobre processos pode ser de grande utilidade.
No caso de já se existir um modelo de processo predefinido num contexto menos restritivo, surge a necessidade de verificar se o que foi planejado é realmente seguido.
Por outro lado, seja de maneira descritiva ou prescritiva, uma fase de modelagem do processo de negócio é sempre necessária.
Diversos autores afirmam que essa etapa normalmente consome muito tempo, trabalho, sendo ainda sujeita a erros.
Em esse contexto, a mineração de processos pode ser aplicada buscando facilitar a obtenção do modelo de processo com base na exploração de eventos registrados durante a execução.
Propósitos e perspectivas O propósito da mineração de processos é descobrir, monitorar e melhorar processos extraindo conhecimento de eventos produzidos.
A forma do conhecimento extraído está tipicamente relacionada aos objetivos que se tem com a aplicação da mineração de processos.
Autores como Rubin e Aalst estabelecem duas classificações básicas:
Perspectiva; Propósito.
Conforme os dados explorados, o processo pode ser analisado sob basicamente três perspectivas.
Em a perspectiva conhecida como de fluxo de controle, o objetivo é analisar as relações de dependência entre os diversos eventos/ atividades do processo.
Notações comumente utilizadas para representação são Redes de Petri ou FSM (Finite State Machines) (12).
Uma segunda visão está relacionada ao entendimento sobre as relações de cooperação entre os envolvidos no processo (pessoas/ sistemas).
Essa perspectiva é conhecida como organizacional.
Há ainda uma terceira visão, chamada de informação.
Aqui, o objetivo é compreender o conteúdo dos dados envolvidos em cada instância (ex.:
Em um processo de venda, uma atividade fatura com valor acima de 10000 exige que a atividade confirmar_ fatura seja executada duas vezes).
De modo geral, essas três perspectivas buscam responder as questões, respectivamente «Como?,
Quem? E O quê?».
Além de a visão sob as perspectivas descritas acima, a mineração de processos pode ser realizada com os seguintes propósitos:·
Descoberta: Não existe um modelo formal do processo definido a priori.
O modelo pode ser extraído dos próprios dados produzidos durante a execução do processo.·
Análise de conformidade:
Em este caso conta- se com algum tipo de modelagem formal do processo.
No entanto, o que se busca é descobrir se a realidade está conforme com o modelo.
A análise de conformidade pode ser utilizada para detectar desvios do processo, auxiliar a entender suas causas e medir a severidade de tais desvios.·
Extensão: O objetivo aqui é enriquecer, melhorar um modelo pré-existente a partir de a análise de dados de execução.
A análise de dados de desempenho de um processo, por exemplo, poderia ser utilizada para eliminar gargalos e otimizar o processo.
A figura 2.2 fornece uma visão geral acerca de as aplicações de mineração de processos num ambiente suportado por algum tipo de sistema de informação.
Redes de Petri Em certos casos é necessário modelar o comportamento dinâmico de um sistema ou processo.
Por comportamento dinâmico entende- se a representação de um fluxo completo de execução, expressando cada estado por o qual o sistema/ processo passa de acordo um conjunto de regras (condições).
Uma notação amplamente utilizada para tal representação é a rede de Petri.
Seu uso se torna interessante porque, além de ser uma notação gráfica de fácil entendimento, permite a aplicação de diversas técnicas formais de verificação e simulação.
Trata- se de uma estrutura dinâmica, modelada graficamente, que consiste num conjunto de transições, lugares e arcos.
As transições, indicadas por retângulos, são relacionados a alguma atividade ou ação que pode ser executada no sistema.
Entre transições existem lugares, que podem conter tokens.
A distribuição de tokens representa um determinado estado na rede.
Por fim, transições e lugares são ligados por arcos direcionados.
Além de conexões simples, esses arcos podem representar situações de concorrência e escolha (And- Split, OR-Split, And-Join, OR-Join).
Os tipos de comportamento representados são conhecidos como padrões de fluxo.
Para uma transição ocorrer ela deve estar habilitada.
Transições estão habilitadas no momento em que todos os seus lugares de entrada contém um token.
Estando habilitada a transição pode ser executada, consumindo um token de cada uma de suas entradas e produzindo um token para cada uma de suas saídas.
Abaixo são listados os cinco padrões de fluxo básicos para redes de Petri.·
Sequential routing é a ligação direta entre duas transições, sendo caracterizado por o fato de uma transição ter início apenas após o término da execução da transição anterior.·
And- Split consiste num ponto na rede onde um único fluxo se divide em múltiplos fluxos do controle que podem ser executadas paralelamente.·
And-Join é um ponto onde diferentes fluxos paralelos convergem num único fluxo de controle.
Para que a execução da transição fim dos fluxos paralelos prossiga, é necessário que todos esses lugares contenham tokens, estando a transição dessa forma habilitada.·
OR-Split caracteriza- se por a divisão do fluxo, baseada em condições de decisão ou controle que determinam um único caminho a seguir.·
OR-Join é um ponto na rede onde dois ou mais fluxos de execução unem- se sem que haja uma sincronização, ou seja, não são transições concorrentes.
Um exemplo Para que um algoritmo de mineração de processos possa inferir um modelo a partir de um log, são assumidas algumas premissas.
São elas:
Cada evento se refere a uma atividade no processo, um evento pertence a uma instância específica e iii) eventos estão totalmente ordenados ou possuem um timestamp indicando o momento de sua execução.
Para o caso de eventos não atômicos (execução não instantânea), o estado da atividade também deve estar presente (iniciada, completa, suspensa, etc.).
Partindo dessas premissas básicas já se torna possível descobrir a relação entre os eventos contidos no log analisado.
Adicionalmente, um log pode conter o responsável por a execução de cada atividade.
Esse dado possibilita extrair informações sob a perspectiva organizacional citada na seção 2.3.2.
Para ilustrar a aplicação da mineração de processos, sob as perspectivas de fluxo de controle e organizacional, considere a figura 2.3.
Ela mostra um exemplo de log envolvendo 19 eventos, 5 atividades e 6 atores.
O log contém 5 instâncias de processos.
É possível observar que em quatro de elas as atividades A, B, C e D foram executadas.
Em a quinta instância apenas três foram executadas:
A, E e D. Toda instância começa com a execução de A e termina executando a atividade D. Se a atividade B é executada, C também é.
Entretanto, em algumas instâncias a atividade C é executada antes de B. Baseado nessa informação, um algoritmo de mineração poderia encontrar o modelo da figura 2.4.
Um ponto importante a ressaltar aqui é que, nesse caso, assume- se que o número de instâncias é suficiente para representar o ambiente observado.
O modelo está em notação de redes de Petri, contendo informações sobre as perspectivas de fluxo de controle e organizacional.
Além de a relação de sequência entre as atividades, o modelo mostra o envolvimento dos atores com uma das atividades.
À esquerda da figura os atores envolvidos numa mesma atividade estão agrupados por papéis (ex.:
Role X para a atividade A).
À direita é apresentado um diagrama conhecido como sociograma, ou seja, as relações de transferência de trabalho entre os colaboradores envolvidos no processo.
Analisar conformidade está entre as várias aplicações possíveis da mineração de processos.
De um lado se tem um modelo predefinido do processo.
De outro, obtendo registros de eventos de execução desse mesmo processo, é possível comparar o grau de similaridade entre essas duas visões.
Em a área de métodos formais, diversas noções de equivalência já foram propostas.
Entretanto, a maioria dessas abordagens fornece respostas do tipo &quot;verdadeiro/ falso».
Tais respostas não são muito úteis na comparação de modelos de processos de negócio, já que raramente existirá uma equivalência perfeita.
Torna- se mais interessante obter um determinado grau de similaridade, expressado de forma quantificável.
Uma das formas conhecidas para analisar conformidade por mineração de processos é a chamada delta analysis.
Em essa abordagem, um modelo descoberto por meio de mineração é comparado com o modelo predefinido.
A verificação pode se dar, tanto de forma visual, quanto a partir da aplicação de técnicas formais de comparação de modelos com a obtenção de métricas de conformidade.
Para ilustrar a aplicação de delta analysis, é apresentado a seguir um exemplo retirado de.
A figura 2.5 ilustra o exemplo de um processo de compras, representado em notação Workflow-Net (Wf-Nets) (1), que é uma subclasse da notação clássica de redes de Petri.
Os quadrados representam as atividades enquanto os círculos correspondem à parte passiva do processo, representando estados.
A figura 2.5 apresenta o modelo prescritivo do processo.
Essa seria a forma como o processo é &quot;percebido «por os gestores.
Agora suponha que por meio de o log3 da figura 2.7 fosse possível descobrir o modelo da figura 2.6.
A o comparar os dois modelos, quatro diferenças básicas podem ser observadas:
1) no processo descoberto, em caso de falta de estoque uma ordem de renovação é sempre emitida (replenishment, 2) as atividades send_ bill e receive_ payment estão em paralelo no modelo descoberto.
Isso sugere que talvez em algumas instâncias, clientes pagam antes de receber a fatura, 3) no modelo descoberto não é possível enviar múltiplas faturas e 4) no modelo descoberto é possível emitir a fatura antes Para fins de ilustração, é mostrado apenas o fragmento de um log.
Em um caso real, um volume muito maior de eventos seria necessário para inferir um modelo de despachar os produtos.
Cada uma dessas diferenças fornece pistas interessantes sobre a discrepância entre o processo real observado e o que se espera que seja realizado.
Se um sistema de informação é configurado conforme a figura 2.5 e as pessoas na realidade trabalham como sugerido por o modelo descoberto (figura 2.6), existe uma inconformidade.
Esse problema poderia ser resolvido atualizando o modelo definido ou incentivando as pessoas a aderirem ao processo original.
Em esse exemplo simples, contudo, é fácil identificar visualmente as diferenças entre os modelos.
Porém, para processos complexos isso pode não ser possível.
Existem maneiras de destacar graficamente as discrepâncias entre modelos desse regions.
A primeira utiliza noções de herança do desenvolvimento orientado a objetos aplicadas a processos.
Resumidamente, são buscadas sub-classes ou super-classes em comum nos dois modelos.
A segunda abordagem provém da área de adaptação dinâmica de workflows.
A estratégia é identificar, por artifícios matemáticos, regiões no modelo original que são diretamente afetadas, quando comparadas ao modelo modificado.
Essas regões são chamadas change regions.
Considerações sobre o capítulo Em esse capítulo foram discutidos aspectos relacionados à práticas de gestão de conformidade de processos como ferramenta para obtenção de alta qualidade e produtividade na engenharia de software.
Procurou- se, com isso, esclarecer como tais práticas se enquadram no contexto de PDS, destacando a importância e os desafios na gestão de conformidade de processos.
O conceito de mineração de processos foi apresentado como método inovador, que desperta grande interesse atualmente tanto no auxílio à descoberta de modelos de processos quanto na verificação dos mesmos.
Fazendo ligação com o ambiente de PDS o trabalho aborda, de forma ampla, como práticas de análise de conformidade com mineração de processos podem ser aplicadas baseadas na documentação e estrutura computacional típicas de uma empresa de software.
Descrição A pesquisa foi desenvolvida em colaboração com a empresa Hp (Hewlett--Packard Company Brasil Ltda), mais precisamente, em sua operação de software instalada no TECNOPUC -- Parque Tecnológico da PUCRS em Porto Alegre.
A operação está em funcionamento a cerca de cinco anos, desenvolvendo grandes projetos de software para instituições públicas e privadas.
Em novembro de 2005, a empresa obteve a certificação de nível 3 CMM, sendo parte deste esforço realizado em parceria com a PUCRS.
O trabalho de certificação de fato trouxe resultados positivos para a melhoria de qualidade e produtividade da empresa.
A formalização e documentação de todo o conjunto de processos adotados é um dos produtos resultantes do trabalho.
Trata- se inclusive de requisito essencial para qualquer organização certificada em nível 3.
Outro aspecto importante foi a implantação de um repositório central de métricas para todos os projetos da organização, chamado de Base Organizacional (BO).
O projeto foi desenvolvido numa parceria entre o Programa de Pós-graduação de Ciência da Computação da PUCRS (PPGCC-PUCRS) e a Hp, para apoiar o programa de métricas da empresa.
O propósito de tal repositório é prover uma visão consistente da realidade dos projetos, visando auxiliar os gestores no processo de tomada de decisão.
Em a área de garantia de qualidade, uma equipe de SQA é encarregada de garantir o controle de qualidade dos produtos desenvolvidos, além de monitorar a aderência dos colaboradores aos processos definidos.
Note- se que este também é um requisito do nível 3 do modelo de maturidade.
Aderente ao padrão CMM, a empresa possui um conjunto de processos organizacionais (OSSP).
Além disso, cada projeto pode realizar adaptações nesses processos de acordo com necessidades específicas.
As adaptações e dispensas, depois de validadas e autorizadas por a gerências sênior e equipe de SQA, são documentadas no PDSP (Project's Defined Standard Processes).
A equipe de SQA realiza periodicamente, auditorias de produto e processo com base nesses documentos e nas evidências coletadas dos projetos.
O objetivo é manter visibilidade sobre a qualidade dos produtos desenvolvidos e dos processos utilizados para tal.
Cabe salientar aqui que, embora a estrutura dos processos da empresa esteja baseada no modelo CMM, tal estrutura é muito semelhante à encontrada no padrão atual CMMI.
O modelo CMMI possui apenas algumas diferenças conceituais e de nomenclatura, já que trata de processos de engenharia de forma geral e não mais de processos de software como o antigo CMM.
Sendo assim, assume- se que as características apresentadas de acordo com o CMM sejam perfeitamente aplicáveis ao atual CMMI.
Suporte Computacional Como suporte computacional na gestão dos projetos a empresa conta com diferentes ferramentas, sendo cada uma voltada a um aspecto de projeto específico.
A função das principais ferramentas são:
Gestão de cronograma, controle de demandas e defeitos para projetos de manutenção, gestão de requisitos, gerência de configuração, gestão de testes e coleta de métricas.
Considera- se que esse tipo de ambiente seja comumente encontrado em empresas de software de médio e grande porte.
Analisadas por a visão de conformidade de processos, duas questões interessantes são observadas aqui.
Por um lado, esse conjunto de ferramentas é capaz de produzir um grande volume de registros históricos de execução, podendo servir de fonte para a monitoração de processos.
Mas, por outro lado, surge a dificuldade em identificar, extrair e estruturar todos esses dados para produzir informações relevantes sobre o processo executado.
Base Organizacional A empresa possui um repositório de métricas para o controle de indicadores de seus projetos.
A concepção da BO, teve como meta principal prover uma visão unificada dos diferentes projetos da operação, de forma a viabilizar a comparação e análise dos mesmos de maneira sistemática e confiável.
O projeto envolveu a parceria PUCRS/ HP.
Mais detalhes sobre o trabalho podem ser encontrados em.
Em um ambiente de Data Warehousing, são integrados e consolidados os dados provenientes de todo o ambiente computacional utilizado nos projetos.
Mensalmente, é realizado um processo de carga de dados nessa base, para tornar- los acessíveis à gerência por meio de uma interface de Bi (Business Intelligence).
Esse processo pode ser dividido em cinco etapas.
A figura 3.1 ilustra o fluxo de carga das métricas.
Primeiro, os colaboradores atuantes nos respectivos projetos obtêm as métricas no PDS, ilustrado como a primeira etapa na figura.
Como segunda etapa ocorre o registro dessas métricas nas diferentes ferramentas que o projeto tem para apoio no gerenciamento.
Em a terceira etapa ocorre a construção de pacotes onde são agrupados os dados mensais, proporcionando uma visão do projeto naquele período.
A quarta etapa é ilustrada por o armazenamento dos pacotes no repositório central para, na quinta e última etapa, serem extraídos e apresentados aos gerentes e gestores na interface de Bi.
Caracterização do problema A parceria proporcionou a interação direta com a equipe de qualidade da empresa.
Essa experiência mostrou, contudo, que analisar conformidade de processos numa grande empresa de software não é uma tarefa simples.
Os resultados dependem de muitos aspectos subjetivos, como:
Habilidade de comunicação do auditor, disponibilidade dos colaboradores para o fornecimento de informações e formato das questões utilizadas nas auditorias.
Além disso, a presença do auditor pode interferir no comportamento natural do colaborador.
Esse fenômeno é inclusive bastante conhecido, chamado de efeito Hawthorne.
Observouse também que, embora bem documentados, os processos não são impostos ao colaboradores.
Eles ficam acessíveis ao time, incluindo gráficos, guias e templates.
Entretanto, não se pode garantir que cada membro da equipe realmente acesse frequentemente a documentação, seguindo assim o processo fielmente.
Sabe- se que a implantação de formas de gestão mais rígidas, baseados em sistemas de automação de workflow, pode restringir tal subjetividade.
Entretanto, esse tipo de controle possui dois problemas:
Definir um modelo a priori para um processo complexo como um PDS é trabalhoso, exige muito tempo e constantes atualizações a cada mudança de comportamento no processo ou situações que não foram identificadas previamente, PDS são bastante dinâmicos, necessitando de certa flexibilidade, o que pode ser fortemente restringido por a gestão de um sistema de workflow.
De essa forma, a maior dificuldade está justamente em estabelecer um ponto de equilíbrio que permita manter o dinamismo característico de um PDS, obtendo ao mesmo tempo a visibilidade e o controle adequados para garantir a qualidade e o desempenho desejado nos processos.
Tendo em vista os fatores discutidos acima, surge a motivação de buscar métodos que possam complementar e facilitar o processo de análise de conformidade no ambiente de PDS.
Conforme apresentado na seção 2.3, técnicas automáticas de descoberta de conhecimento sobre processos tem sido aplicadas nas mais diversas áreas.
Além de a descoberta de modelos, diversas aplicações existem também com o objetivo de analisar conformidade em processos.
Então, o objetivo do trabalho concentra- se em explorar as possibilidades e resultados potenciais da aplicação de uma solução mineração de processos na análise de conformidade num ambiente de PDS.
É importante destacar que, embora baseado no contexto da empresa parceira, o objetivo busca abordar o tema de forma genérica.
Levamos em conta o fato da empresa ter sido certificada CMM-3, tendo o seu processo de SQA avaliado e aprovado.
Com isso, assume- se que o tipo de método de análise de conformidade encontrado na empresa parceira seja também adotado predominantemente em empresas de software baseadas nos mesmos padrões.
Requisitos da solução A visão de uma solução ideal para verificação de conformidade de processos em PDS é proveniente de duas fontes.
Primeiro, devido a a participação direta no processo de auditorias juntamente com o grupo de SQA.
Segundo, por meio de discussões com auditores e gestores, procurando identificar a sua visão com relação a o processo de auditorias.
Como conclusão, idealmente uma solução de verificação de conformidade de processos deveria atender aos seguintes requisitos:·
Ser não intrusiva e objetiva:
Considera- se que esses dois requisitos sejam diretamente relacionados.
Por não intrusivo, se entende um procedimento que não afeta as atividades normais dos colaboradores (um desenvolvedor interrompe sua tarefas para responder a um questionário, por exemplo).
Além disso, a pessoa entrevistada pode acabar omitindo ou distorcendo informações por vários motivos.
Se o entrevistado não compreender claramente o objetivo daquela entrevista ou questão específica, a qualidade das informações fornecidas pode ser prejudicada.
Até mesmo o receio de ter seu desempenho posto em dúvida dependendo da resposta, pode ser um motivo para o entrevistado omitir a visão real que possui sobre o dia-a-dia do projeto em que está envolvido.
Tal situação acaba acarretando em subjetividade dos resultados, o que é fator negativo.·
Apresentar indicadores:
Um fator indispensável em qualquer verificação é a produção de métricas que possam fornecer uma visão quantitativa acerca de o objeto avaliado.·
Reduzir e qualificar o trabalho de auditoria:
A experiência prática mostrou que a realização de auditorias de SQA numa organização com diversos projetos de grande porte exige muito trabalho.
Em projetos de manutenção que trabalham por demandas, por exemplo, o auditor normalmente escolhe apenas algumas demandas do período para analisar, já que muitas vezes não há tempo hábil para analisar todas.
O aumento da eficiência nesse processo é tido também com um aspecto desejável.·
Não interferir significativamente no processo atual:
É também um requisito na concepção da solução, que não sejam exigidas mudanças significativas de comportamento e gestão dentro de a empresa.
Pequenas mudanças normalmente são necessárias para que se obtenha melhores resultados.
No entanto, é desejável que tais modificações não afetem significativamente o ambiente atual da organização.
Com base no conjunto de requisitos levantados na seção 3.5.1, este trabalho explora como métodos de extração automática de conhecimento podem auxiliar na verificação de conformidade em PDS.
Para isto, foi realizado um estudo amplo, abordando três aspectos principais:
Formato típico de processos definidos em empresas de software, fontes de dados e formas em as quais registros de execução podem ser utilizados na monitoração de processos e soluções de verificação de conformidade existentes e tipo de resultados fornecidos a partir de sua aplicação no ambiente de engenharia de software.
Embora sendo levantadas algumas questões relativas às visões de recursos e artefatos em processos de software, o presente trabalho dá maior enfoque à análise sob a perspectiva de fluxo de controle (sequência de atividades).
Considerando essa visão, primeiramente o OSSP da empresa parceira e o Processo Unificado foram considerados no estudo sobre os processos definidos.
O Processo Unificado foi considerado aqui como uma referência com relação a o nível de abstração e formato tipicamente encontrado em modelos de processo em PDS.
Além disso, foi realizado um estudo de exploratório com dados de um projeto de manutenção da empresa.
Foram executados diversos experimentos com algoritmos de mineração de processos utilizando registros de execução do projeto.
Os experimentos permitiram compreender várias questões envolvendo o uso de dados de execução de PDS na aplicação de mineração de processos.
Algoritmos de análise de conformidade também foram estudados visando compreender seus requisitos, aplicações e potenciais resultados no auxílio à verificação de conformidade em PDS.
As próximas seções reportam o trabalho realizado.
Processos definidos Para garantir maior controle, previsibilidade e qualidade nos projetos, empresas de software costumam adotar modelos de desenvolvimento.
Esses modelos estabelecem métodos sistemáticos que servem como guia para todo o ciclo de vida de um projeto de software.
Ao longo de os anos diversos modelos tem sido propostos.
Exemplos são:
Cascata, Espiral, Evolucionário e Processo Unificado.
Embora os modelos possuam algumas diferenças, todos se baseiam numa definição comum.
Um processo de software é um conjunto ordenado de tarefas, normalmente distribuídas entre diversas pessoas envolvendo projeto, desenvolvimento ou manutenção de um sistema de software.
De forma geral um modelo de desenvolvimento é composto por:
Fases ou elementos de processo, atividades, produtos e responsabilidades.
Em o momento em que uma empresa adota um modelo, ele deve ser documentado de alguma forma.
Tipicamente, tal documentação inclui descrições textuais, responsabilidades, artefatos envolvidos e diagramas descrevendo os passos de cada fase do processo.
Muitas empresas costumam utilizar os chamados Eletronic Process Guides (EPG) (36).
EPGs proporcionam uma forma eficiente de representar o processo de software.
Pode- se navegar facilmente por todos os elementos, visualizando cada fluxo de atividades em notação UML ou alguma outra forma de representação de processos.
De uma forma ou de outra, empresas comprometidas com a qualidade e produtividade possuem seus processos formalizados de acordo com o modelo de desenvolvimento adotado.
Soluções de análise de conformidade baseadas em mineração de processos, em sua maioria, assumem a existência de um modelo predefinido, expresso em alguma notação formal.
Rede de Petri é a forma mais utilizada nesse contexto.
Redes de Petri são apropriadas à aplicação de técnicas de análise formal pois sua notação provê, além de a modelagem visual, uma lógica de execução do sistema modelado.
Esse é inclusive um dos principais motivos de sua ampla utilização.
Além disso, possui padrões bem definidos, generalizáveis, amplamente conhecidos e de fácil entendimento.
Então, para viabilizar a comparação entre um modelo de desenvolvimento de software e sua real execução por meio de mineração de processos deve ser possível obter um workflow em rede de Petri.
Ou pelo menos em alguma notação com padrões equivalentes, que possam ser diretamente mapeados.
Russel em identifica mais de quarenta padrões de fluxo de workflow, incluindo notações como UML e BPMN.
Logo, a obtenção de um modelo de workflow para comparação não deve ser um fator de dificuldade nesse contexto.
Como parte do estudo foram analisados, o conjunto de processos da empresa parceira e o metamodelo de desenvolvimento de software Rational Unified Process (RUP) (33).
O objetivo foi identificar e comparar o formalismo de workflow adotado e o nível de abstração encontrado nesses processos, tendo em vista a sua aplicação na comparação com dados de execução de um PDS.
Primeiramente foram explorados os processos documentados da empresa parceira.
Se observou que todo o elemento do OSSP e seus sub-processos possuem diagramas estabelecendo o fluxo de atividades envolvido.
Com uma semântica semelhante a diagramas de atividade UML, os diagramas documentados contêm a ordem das atividades, papéis e artefatos envolvidos.
Portanto seria possível utilizar tais modelos de workflow como padrão para análise de conformidade.
Um fator a ser considerado, contudo, está relacionado ao nível de abstração dos diagramas.
Em a sua maioria esses workflows contém atividades em alto nível, sem nenhuma estrutura mais complexa como decisões, iterações e paralelismo.
Poucos de eles possuem estruturas de decisão modeladas.
Entende- se que a comparação desse tipo de estrutura com registros de execução não estaria explorando o potencial que a aplicação de técnicas formais de análise de processos pode ter.
O aspecto interessante da descoberta automática de conhecimento está justamente em evidenciar situações que dificilmente podem ser identificadas por meio de análise manual dos dados.
Não é considerado relevante por exemplo, ter como resultado da mineração de processos a confirmação sobre a sequência entre as tarefas de &quot;desenvolvimento «e &quot;testes», o que seria uma informação óbvia.
Além disso, os modelos não preveem concorrência entre atividades.
Entretanto, num estudo exploratório com dados de um projeto da empresa (descrito na seção seguinte) se observou que o paralelismo entre atividades é bastante frequente.
Logo, é importante que os processos sejam modelados levando em conta no mínimo as três estruturas básicas citadas (paralelismo, decisões e iterações), possibilitando assim obter equivalência entre modelo e registros de execução para viabilizar a comparação provendo resultados mais detalhados e precisos.
Por se tratar de um modelo de desenvolvimento amplamente adotado na indústria atualmente, o formato de workflows do processo unificado também foi analisado.
Já nesse modelo, observa- se que as atividades são descritas em menor granularidade, inclusive prevendo paralelismo e decisões.
A figura 4.1 mostra o fluxo de implementação do Processo Unificado.
Evidentemente tal modelagem fornece informações mais precisas acerca de o fluxo de atividades esperado no processo.
Porém, quanto maior a complexidade do modelo mais difícil se torna garantir a aderência a esse processo sem o auxílio de métodos automatizados.
É nesse ponto que técnicas de descoberta de conhecimento podem ser mais efetivas, facilitando a monitoração e controle de processos dinâmicos e complexos como Estudo exploratório sobre descoberta de processos Buscando compreender o tipo de informação tipicamente encontrado em registros de execução de uma empresa de software, foi conduzido um estudo exploratório num dos projetos da empresa parceira.
O ambiente computacional foi explorado a fim de identificar fontes de dados potenciais para a descoberta de conhecimento de processos.
Como descrito na seção 3.3, a empresa possui todas as métricas coletadas de sua plataforma computacional integradas num repositório central.
Entre outras informações, a BO armazena atividades desenvolvidas por todos os seus colaboradores.
Analisando esse conjunto de dados se observou que, com algumas modificações, os pré-requisitos de um log para mineração de processos (ver seção 2.3.4) poderiam ser satisfeitos.
Foi realizado um estudo nesse sentido, buscando avaliar a viabilidade em descobrir relações entre atividades a partir de os registros de esforço da BO.
Então, conforme os resultados obtidos, explorar sua aplicação na comparação entre modelo e execução de processos.
Soluções de mineração de processos permitem comparar dois modelos ou ainda um modelo diretamente com um log.
São as abordagens de delta analysis e conformance testing.
Mesmo sabendo que é possível comparar um modelo formal diretamente com o log (conformance testing), nesse estudo inicial se decidiu investir na descoberta de processos.·
Projeto de grande porte, chegando a contar com uma equipe de mais de 50 pessoas.
Naturalmente o número de interações entre os participantes e a quantidade de dados produzida são grandes.
Além disso, a experiência no grupo de qualidade mostrou ser extremamente difícil obter a visibilidade adequada sobre todos os acontecimentos dentro de um projeto desse porte.
Foi inclusive um fator motivador do trabalho.·
Como o projeto foi iniciado em 2005 e continua em andamento, um grande volume de dados foi produzido nesse período.
Apenas a base de registros de atividades anal- isada continha mais de cem mil linhas na época da realização do estudo, registradas durante quatro anos de projeto.
Sabe- se que a abundância de dados é um fator chave na aplicação de qualquer técnica de descoberta de conhecimento.·
Projetos de manutenção e melhoria são bastante comuns na empresa.
Em o período da pesquisa, metade dos projetos em andamento eram desse tipo.·
O fluxo iterativo é uma característica comum em projetos de manutenção.
Conjuntos de demandas solicitadas por o cliente regularmente, são normalmente agrupadas em ordens de serviço (Os).
Por sua vez, um conjunto de Os forma uma versão de software a ser entregue ao cliente.
Cada Os passa por uma etapa de estimativas e, em seguida, passa por as fases de projeto e desenvolvimento (divida entre equipes cliente e servidor).
Após a etapa de testes, um grupo de Os é agrupado num build e disponibilizado como uma versão do produto.
Tal estrutura favorece a representação dos registros de execução sob a forma considerada nos algoritmos de mineração de processos.
Tanto o identificador de Os quanto a versão podem identificar uma instância (iteração) do processo de manutenção.
A base de registros de atividades Dentro de a BO, grande parte da tarefas desenvolvidas nos projetos são armazenadas numa base de dados específica.
A figura 4.2 demonstra a estrutura dessa base.
Em ela, cada colaborador registra suas atividades no projeto, dentro de uma classificação pré-definida (tabela Tipo_ ATIVIDADE).
Essa tabela contém 57 tipos de atividades, abrangendo todo o ambiente de execução do projeto.
Exemplos de atividades são:
Planejamento, teste unitário, liberação, backup e reuniões.
Os registros são feitos efetivamente na tabela Atividade.
Cada linha dessa tabela possui, entre outras informações, o grupo envolvido, o responsável, o tipo de atividade base, projeto e a duração da atividade.
O atributo Grupo relaciona a equipe envolvida na atividade.
Adicionalmente, uma atividade do projeto está relacionada a um Tipo de Atividade Base (Trabalho, Retrabalho, Revisão e Qualidade).
Atividades não são necessariamente atreladas a uma ordem de serviço, podendo pertencer ao escopo de uma versão.
A tabela Audit_ Trail_ Entries contém os registros de execução efetivamente.
Todas as instâncias de processo devem ser listadas sem repetição na tabela Process_ Instances.
Além de essas duas tabelas obrigatórias, dados adicionais sobre o processo podem ser inseridos nas tabelas Data_ Attributes_ Audit_ Entries e Data_ Attributes_ Process_ Instances.
Os dados armazenados nessas duas últimas tabelas servem principalmente para a análise sob a perspectivida de informação, descrita na seção 2.3.2.
Estudo dos algoritmos Para realizar os testes se fazia necessário selecionar algoritmos de descoberta de modelos apropriados para as propriedades dos dados utilizados.
Então se buscou na literatura as abordagens existentes procurando compreender suas características.
De forma geral, cada algoritmo busca resolver algum problema específico envolvendo a mineração de processos.
Entre esses problemas se pode citar o tratamento de ruído, dados pouco estrurados e incompletude no log.
Durante o trabalho de preparação, foram observadas algumas particularidades nos dados que serviram de parâmetro para a escolha.
Um aspecto importante identificado foi a presença de dados incorretos (ruído).
Alguns campos importantes para a realização da mineração não possuiam restrição no preenchimento.
Consequentemente, se encontrou uma quantidade considerável de registros com campos não preenchidos ou preenchidos incorretamente.
Lidar com informações incorretas é um fator considerado fundamental para a escolha do algoritmo nesse ambiente.
Outro aspecto levado em conta é o tipo do processo.
A bibliografia da área divide processos em estrutura- dos e não estruturados.
Existem algoritmos voltados a cada um desses formatos.
Em processos não estruturados, o número de caminhos possíveis geralmente é muito grande.
Isso torna difícil a obtenção de modelos simples e que ao mesmo tempo representem com fidelidade o processo.
Abordagens voltadas a processos estruturados costumam ser aplicadas a log de sistemas de workflow, tratando- se de processos mais bem comportados.
Por não se tratar de registros produzidos por um sistema governado por um modelo de workflow, a base de atividades se enquadra como um processo não estruturado.
Outro aspecto que caracteriza algoritmos de mineração de processos diz respeito à completude dos dados.
Em a prática, um log tipicamente não contém todas as sequências de eventos possíveis no processo.
Modelos tem de ser inferidos com base em um subconjunto de sequências determinado por o número de instâncias presente no log.
Para o caso dos experimentos realizados, outra característica desejável é com relação a a flexibilidade dos resultados produzidos.
Certos algoritmos permitem interação do usuário por meio de o ajuste de parâmetros, obtendo assim diferentes visões sobre o mesmo log.
A idéia era explorar diferentes visões do modelo a fim de identificar as relações mais interessantes entre as atividades do processo.
A tabela 5.1 mostra um comparativo entre quatro algoritmos estudados, baseado nas dimensões de interesse para o trabalho.
Tais algoritmos foram selecionados por serem os mais conhecidos na literatura.
O algoritmo é considerado o pioneiro na descoberta de processos.
Sua abordagem básica para identificação de relações entre eventos é utilizada por a maioria dos algoritmos solução anterior, sendo capaz de detectar dependências implícitas e laços curtos no log.
Entretanto, ambas as soluções possuem a mesma restrição.
Elas consideram um log sem erros e completos (cada relação tem de aparecer pelo menos uma vez para ser detectada) para a descoberta do modelo.
Porém na prática tal situação não acontece.
Logs são geralmente incompletos e contém erros.
De essa forma, atualmente algoritmos tem adicionado tratamentos estatísticos para lidar com esse problema.
As soluções Heuristics Miner e Fuzzy Miner são dois exemplos de aplicação de métodos estatísticos.
As suas abordagens são bastante semelhantes, sendo capazes de lidar eficientemente com processos pouco estruturados, incompletos e contendo ruído.
Eventos e relações entre esses eventos são construídos com base na frequência em que aparecem no log.
O objetivo das duas soluções é induzir o comportamento mais significativo do processo, mesmo a partir de registros incompletos e incorretos.
Essas duas soluções se mostram interessantes para o tipo de dados explorado no contexto de PDS, justamente por envolver dados pouco estruturados, incompletos e imperfeitos.
É construída uma tabela contendo todas as atividades com suas relações de concorrência (And/ OR-split/ join).
A tabela 4.2 mostra o formato dessa tabela para um possível log com cinco atividades diferentes.
A entrada de atividades iniciais e saída de atividades finais são expressadas como zero.
A partir de uma série de operações lógicas utilizando essa tabela e as relações do grafo de dependências, o algoritmo extrai todas as relações de paralelismo entre as atividades.
Alguns indicadores apresentados no próprio modelo também fornecem pistas sobre o comportamento do processo.
São mostradas a frequência de cada atividade, de cada relação e também o índice de correlação entre cada par de atividades.
A semântica de concorrência também é mostrada no modelo final, como na figura 4.5.
Miner se diferência em dois aspectos.
Primeiro, é empregada uma abordagem multi-perspectiva para correlacionar eventos (analisar semelhança dos atributos dos eventos).
Segundo, são empregadas ferramentas de customização dos resultados para simplificação de modelos de processos pouco estruturados.
Segundo os autores, a inteligibilidade dos resultados é um grande problema na descoberta desse tipo de processos.
Como existem muitas conexões, os modelos tornam- se extremamente confusos para o entendimento humano.
O problema é conhecido como geração de modelos spaghetti-like.
Então, são aplicados conceitos retirados da cartografia para simplificação dos modelos.
Tais conceitos são:
Agregação, abstração, ênfase e customização.
Informações em maior nível de detalhe aparecem agrupadas no resultado (ex.:
Mapa- mundi mostra cidades apenas como pequenos pontos).
Informações irrelevantes para o contexto são abstraídas, ao contrário de o comportamento mais relevante que é enfatizado no modelo (ex.:
Mapa rodoviário enfatiza as auto-estradas e não os pontos turísticos de uma região).
Assim como em mapas, os autores consideram que não existe um único modelo universal.
Diferentes visões são possíveis, dependendo do contexto.
O conceito de customização está relacionado à atribuição de valores ajustáveis, proporcionando diferentes visões do modelo de acordo com o propósito de análise.
O algoritmo produz o modelo do processo baseado em alguns critérios.
Tais critérios de decisão são suportados por duas métricas básicas:
Significância e correlação.
A significância pode ser determinada, tanto para eventos quanto para relações de precedência binária entre esses eventos.
Ela mede a importância relativa de um determinado comportamento.
A significância é baseada na frequência, ou seja, quanto mais frequente é um evento ou uma correlação entre dois eventos maior será sua significância.
Por outro lado, a correlação é calculada apenas para relações de precedência entre eventos.
Ela expressa o quão proximamente relacionados são dois eventos.
O Fuzzy Miner possui várias métricas para o cálculo da correlação, todas baseadas na semelhança de atributos contidos no log.
Uma das métricas mede o grau de semelhança entre identificadores de dois eventos distintos.
Por exemplo, as atividades analisar_ pedido_ cliente e aprovar_ pedido_ cliente são consideradas fortemente correlacionadas devido a a semelhança nos nomes de seus identificadores.
A partir de essas duas métricas o modelo de processo é construído de forma que:·
Comportamento altamente significativo é preservado.·
Comportamento menos significativo, mas altamente relacionado é agregado.·
Atividades pouco frequentes e pouco correlacionadas são omitidas (abstração).
Análise e pré-processamento dos dados A etapa de preparação é parte importante em qualquer processo de descoberta de conhecimento sobre conjuntos de dados.
Em esta fase, é feita uma análise criteriosa a fim de eliminar informações desnecessárias e também estruturar os dados de forma a serem melhor interpretados por a ferramenta de mineração.
No caso de o estudo realizado, a preparação consistiu em duas etapas:
Seleção e pré-processamento dos dados conversão do banco de dados para o formato MXML.
Os experimentos foram realizados sob três perspectivas:
Atividades ligadas a Os, versão e a um único processo.
Cada um de eles exigiu um procedimento diferente de análise e pré-processamento dos dados, o que foi de grande valia para o entendimento sobre a estrutura e o comportamento dos dados de métricas, quando analisados sob a visão de mineração de processos.
Análise de Os e versões Como primeira exploração do registros, experimentos foram realizados relacionando as atividades com a iteração relacionada (Os e versão).
Primeiramente, foram selecionadas as atividades consideradas mais relevantes no processo e com comportamento mais previsível.
Foram descartadas atividades do tipo folga, estudo e viagem por não possuirem um momento específico para ocorrer.
Embora essas atividades possam ser consideradas no planejamento, elas não fazem parte dos processos organizacionais formalizados.
Por isso, foram descartadas.
A seleção foi feita com base na descrição de cada atividade além de o auxílio de colaboradores com conhecimento sobre o contexto do projeto.
A representação de um conjunto de eventos para mineração de processos supõe pelo menos três informações:
Identificador da instância do processo, nome da atividade e timestamp.
Alguns algoritmos também utilizam o responsável (originator) e possivelmente um campo de estado de execução da atividade.
Por isso, esses últimos campos foram também adicionados ao log.
Os dados foram selecionados da seguinte forma (ver figura 4.2):·
Instância: Tanto o campo Os quanto a Versão poderiam servir como identificador para instância.
No entanto, identificou- se que existem atividades específicas pertencentes a cada um desses dois contextos.
Sendo assim, foram gerados dois logs distintos sendo, um com Os como instância e outro com o campo Versão (cada um com suas respectivas atividades).·
Atividade: Foi utilizada a descrição da atividade diretamente.·
Timestamp: Como pode ser observado na figura 4.2, o instante de execução das atividades é dividido em ano, mês, dia.
Além disso, não existe um horário de início e fim, somente a duração.
O timestamp no formato MXML (dd­mm­aa hh:
Mm: Ss) foi gerado concatenando os três primeiros campos e adicionando uma hora padrão de início para todas as entradas.·
Originator: Coluna Id_ Usuario da tabela Usuario.
Tendo selecionado os atributos, a etapa de pré-processamento consistiu na implementação de consultas SQL para replicar as colunas necessárias na tabela principal (Audit_ Trail_ Entries) da estrutura MXML.
Adicionalmente foi realizado uma etapa de limpeza dos dados, buscando eliminar registros incompletos ou com preenchimento incorreto.
Sabe- se que quanto menor o volume de ruído nos registros, melhores são os resultados produzidos por os algoritmos de mineração.
Resultados A figura 4.4 mostra o modelo produzido por o algoritmo Fuzzy Miner.
Cada nodo contém o seu valor de significância.
Os arcos possuem respectivamente os valores de significância e correlação.
O modelo realmente sugere alguns comportamentos coerentes, segundo informações retiradas dos processos organizacionais e confirmadas por os colaboradores do projeto.
A atividade Os é inicial.
De acordo com a descrição dessa atividade, ela envolve o trabalho nas solicitações do cliente (análise de documentos técnicos, e-mails, etc.).
É esperado que ela seja realizada normalmente no início da demanda.
O seu valor de significância igual a 1 também mostra que é uma atividade muito frequente e repetitiva, o que é confirmado por os altos valores de significância e correlação do conector iterativo (loop).
Outro fato a ser observado é a conexão de praticamente todas as atividades com a Os.
O motivo é a inserção de várias entradas dessa atividade no decorrer de o projeto, por ser uma realizada sob demanda.
Como esse comportamento é frequente, o algoritmo o identifica.
Em seguida observa- se as atividades Estimativa, Setup-Dev e Teste Unitário conectadas.
Esse comportamento também é consistente com o processo.
Segundo informações dos colaboradores do projeto, o trabalho de estimativa é realizado logo após a definição da documentação de projeto da demanda.
A próxima atividade Setup-Dev recebe esse nome porque, antes do início do desenvolvimento, é utilizada uma ferramenta de geração de pseudo-código a partir de a documentação de projeto.
Nota- se também a atividade Construção ligada a atividade anterior, o que é bastante coerente.
A execução de testes unitários de código é realizada por o desenvolvedor após a conclusão de cada unidade de código.
A relação entre essas duas atividades também aparece no modelo.
Por fim, há uma correlação entre as atividades Caso de Teste e Caso de Teste -- Retrabalho.
Essa sequência sugere a correção de defeitos encontrados durante a execução inicial dos casos de teste.
Cabe salientar que este modelo não foi obtido como primeiro resultado da execução.
A interface do plugin permite ajustar todos os parâmetros do algoritmo, reconstruindo o modelo imediatamente.
Então, esses valores foram modificados empiricamente até chegar- se a um resultado considerado consistente com o processo real.
Em a figura 4.5 é apresentado o resultado do algoritmo Heuristics Miner.
Mesmo com algumas diferenças do modelo anterior, este algoritmo também é capaz de capturar características relevantes.
Isso reforça a tese de que os dois algoritmos realmente conseguem capturar o comportamento de processo pouco estruturados.
O Heuristics Miner também provê duas métricas, que são mostradas no modelo resultante.
A primeira expressa um valor de confiabilidade calculado para cada relação de precedência.
O segundo valor apresentado é uma contagem direta do número de vezes em que o evento/ conexão apareceu no log.
Primeiro, se observou que nos registros uma tarefa não possui explicitamente um início e fim.
A cada vez que um colaborador trabalha na tarefa, ele registra o tempo em que esteve envolvido na sua execução.
De essa forma, a mesma tarefa pode conter um número indefinido de entradas no registro.
De fato os algoritmos de mineração identificam tal situação, representada como um laço para a própria atividade repetida.
Isso pode ser observado nos resultados anteriores.
Porém, esse tipo de relação não é coerente com a representação natural de eventos em processos.
Tarefas normalmente constituem- se de um estado de início e fim.
Tal forma de representação permite modelar explicitamente a concorrência em processos, onde atividades possuem um período duração determinado.
Mesmo analisando superficialmente os dados de execução do projeto P1, se pode evidenciar a presença frequente de atividades executadas em paralelo.
Sendo assim, é importante que esse comportamento possa ser devidamente rastreado.
A alternativa mais óbvia para modelar os diferentes estados das atividades seria inserir- los na coluna EventType da tabela Audit_ Trail_ Entries.
Entretanto, logo se identificou que a maioria dos algoritmos de mineração não utilizam esse campo para a análise de concorrência.
Então, os estados de início e fim das atividades tem de ser inseridos juntamente com o identificador da própria atividade.
Isso foi feito da seguinte forma:
Para uma mesma instância, foram produzidos eventos de início e fim para todas as atividades.
Para aquelas que possuíam entradas repetidas, o estado de início foi inserido na atividade com data mais antiga e o fim naquela com data mais nova.
As entradas intermediárias foram eliminadas.
Atividades com uma única entrada foram duplicadas, apenas concatenando o texto &quot;inicio «e &quot;fim «em cada uma das cópias.
A figura 4.8 mostra um trecho do log produzido.
Feito o procedimento descrito, a próxima etapa envolveu a seleção das atividades relacionadas à respectiva fase.
A seleção foi feita analisando a semelhança entre o nome da atividade no registro e no modelo definido.
Foram selecionadas inicialmente quatro atividades:
Plano de Testes, Caso de Teste, Caso de Testes--Retrabalho e Liberação Versão.
Contudo, o workflow definido contém dez atividades.
Então a estratégia foi procurar informações adicionais que permitissem mapear as atividades faltantes.
Conforme informação de colaboradores do projeto, um campo Descrição era utilizado para o registro de comentários adicionais sobre a realização das tarefas.
Esse campo foi então explorado, sendo identificados certos padrões que foram utilizados para aumentar o detalhamento do processo.
Em a verdade, na atividade Caso de Teste se encontrou informações mais interessantes.
Palavras do tipo criação, relatório, alteração, execução eram bastante frequentes.
Fuzzy Miner, podemos ver que a atividade Plano de Testes na verdade ocorre em paralelo com parte do processo.
Já alguns outros comportamentos são coerentes com o esperado.
Após a execução dos casos de teste (atividade Caso de Teste), aparecem as atividades de retrabalho em construção e integração (Caso de Teste -- Retrabalho -- Constr/ Integr).
Este também é um comportamento previsto no modelo definido.
Conclusões sobre o estudo A extração de conhecimento sobre processos a partir de a base de métricas se mostrou promissora.
No entanto, durante a preparação dos dados e após a análise dos modelos, ficou claro que algumas características desses dados devem ser observadas a fim de obter melhores resultados na mineração.
Os principais pontos são enumerados a seguir:·
Análise de processos x análise de métricas:
Relevância dos dados Uma questão relevante a ser considerada diz respeito à importância de cada informação para um ou outro contexto (análise de métricas x mineração de processos).
Valor este que é armazenado diretamente na coluna Horas, como pode ser observado na figura 4.2.
O registro completo de timestamp é fundamental para a inferência de ordenação entre atividades.
Algoritmos de mineração de processos detectam ordenação e paralelismo, pressupondo registros de atividades de duas formas:
Atômicas (execução instantânea) ou tendo início e fim.
Para atividades atômicas, são consideradas paralelas duas atividades que aparecem em qualquer ordem no log.
Ainda é possível modelar paralelismo explicitamente dividindo atividades em início e fim.
Con-forme a granularidade assumida no processo, a informação de tempo pode ser mais ou menos completa.
No entanto, para PDS onde várias atividades de curta duração são realizadas durante um único dia, um timestamp completo deve ser registrado para que se obtenha resultados mais precisos.·
Processo definido x executado:
Granularidade de atividades Outro fator observado está relacionado à granularidade e escopo das atividades.
Para possibilitar a comparação entre os processos definido e executado, as atividades no modelo e nos registros devem ser equivalentes.
Deve existir um mapeamento preciso entre as duas partes, tanto na identificação quanto no nível de abstração das atividades.
Em o estudo de caso se identificou que grande parte das atividades são registradas num nível de detalhe diferente daquele encontrado nos processos definidos.
Em a verdade, esse problema é esperado por o fato dos dados de métricas servirem a outro propósito que não a correlação de atividades.
Entretanto algumas modificações são necessárias no tipo e na forma de registros de atividades para ser possível utilizar essa fonte de dados, tanto para a análise de métricas quanto para análise de processos.
Uma dificuldade encontrada no estudo foi estabelecer o escopo adequado de cada atividade.
No caso de o projeto P1, as atividades foram atribuídas ao contexto de versão ou Os.
Mas os resultados permitiram concluir que, embora adotando um desses dois contextos como unidade de iteração, um critério de classificação ainda pode ser necessário para as atividades dentro de uma única instância.
No caso de projetos de software, torna- se claro que o critério de seleção ideal seria analisar a instância sob a visão de fases de projeto.
Isto porque todos os modelos de desenvolvimento dividem o ciclo de vida de projetos em fases.
De essa forma cada fase possui seu workflow específico, que pode ser utilizado como parâmetro para comparação.·
Tipo de projeto:
Desenvolvimento x manutenção O tipo de projeto analisado é também uma questão importante a ser levada em conta.
Projetos de desenvolvimento e manutenção possuem perfis distintos quanto a o seus processos.
Sob a visão de análise de processos, cada um possui pontos positivos e negativos.
Por os motivos já citados, o estudo foi realizado num projeto de manutenção.
Em casos de projetos de manutenção que adotam a estrutura de ordens de serviço, a principal vantagem observada é quanto a o número de instâncias.
Como Os abrangem um volume relativamente pequeno de trabalho, muitas dessas iterações são produzidas durante o ciclo de vida do projeto.
Com isso se tem um maior número de amostras e em menor granularidade para se analisar.
Padrões de desvios podem ser mais facilmente identificados em menor tempo.
Porém, também existe uma desvantagem.
Pro-jetos de manutenção são tipicamente voltados a realização de melhorias e correção de defeitos, conforme solicitação do cliente.
A realização de melhorias costuma ter um processo melhor definido, podendo ser comparada a um pequeno projeto de desenvolvimento.
Entretanto, no caso de a correção de defeitos a análise se torna mais complicada.
Isto porque muitos eventos são imprevisíveis.
Pode- se dizer que o processo para solução de um defeito é menos disciplinado do que o de melhoria, onde é possível fazer um planejamento adequado.
Projetos de desenvolvimento são melhor comportados nesse aspecto.
De forma que é mais fácil mapear as atividades executadas para o modelo definido.
A desvantagem aqui é quanto a o número de instâncias.
Modelos de desenvolvimento iterativos como o espiral, evolutivo e processo unificado costumam adotar o conceito de versões de produto.
Estas versões representam ciclos de evolução do software ao longo de seu desenvolvimento.
No entanto, versões costumam ser produzidas em menor número e com maior duração.
Com isso, ao se utilizar versões como instâncias na mineração de processos, o tempo necessário para se obter um volume de dados adequado pode ser muito grande.
Para ter a visão de uma única versão completa, por exemplo, poderiam ser necessários meses até que a versão fosse terminada para se obter então o registro de todas as atividades.·
Preparação de dados Uma questão também constatada no estudo foi a importância da etapa de preparação dos dados.
A etapa exigiu um tempo considerável no trabalho.
Além disso, a forma de preparação acabou se revelando um aspecto fundamental na obtenção dos resultados.
Principalmente nas tarefas de limpeza dos dados e análise dos mesmos, buscando as melhores formas de estruturar- los para obter resultados mais relevantes.
Deve- se ponderar, contudo, a possibilidade desse cenário ser particular para os registros da empresa parceira.
A tarefa pode ser mais ou menos fácil dependendo da estrutura dos registros da empresa.
No entanto, como em qualquer aplicação de mineração de dados, essa é uma etapa fundamental no processo.
A granularidade e qualidade dos resultados dependerá diretamente de uma preparação de dados adequada.
Análise de conformidade:
Soluções O presente trabalho estuda o problema de verificar conformidade em PDS.
Como parte da solução, a mineração de processos é explorada como ferramenta.
A seção atual apresenta duas soluções encontradas na literatura, relacionadas à análise formal de conformidade aplicando mineração de processos.
Em a seção 4.4 é apresentado um cenário de aplicação, baseado num dos algoritmos apresentados.
Este cenário tem o objetivo de explorar de forma prática o procedimento envolvido na análise e os resultados fornecidos por a solução.
Algoritmos Pesquisando a literatura se identificou várias soluções de verificação de processos tomando um modelo de execução e um fluxo de eventos.
Entre as aplicações estão análise de comunicação de web services, detecção de violações de segurança em processos e verificação do comportamento de sistemas.
Contudo, os trabalhos de Cook e Wolf e Rozinat e Aalst se mostraram interessantes para o contexto.
Primeiro porque seus requisitos relacionados ao modelo formal e dados de execução serem bastante simples, sendo facilmente satisfeitos no ambiente de software estudado.
Também por possuirem métricas definidas para a avaliação da conformidade entre um processo e registros de execução.
As duas soluções citadas são detalhadas a seguir.
Software Process Validation O trabalho de Cook e Wolf é pioneiro na análise de conformidade em mineração de processos.
Os autores introduzem o conceito de software process validation, por o fato de sua solução ser aplicada a processos de software.
São levantadas algumas questões importantes sobre as características de processos de software no trabalho.
No entanto, a solução efetiva não é específica para o contexto.
A o final, modelo e execução são considerados como simples fluxos de eventos para fins de aplicação do algoritmo.
São propostas da diferença de distância entre strings.
Ambos, eventos e modelo, são expressos como cadeias de caracteres.
Então as métricas são obtidas com base no número de inserções e deleções de caracteres para obter a equivalência entre as duas strings.
A métrica SSD Ainda são adicionados pesos a cada operação, de forma que é possível atribuir custos a cada uma.
A equação 4.1 mostra o cálculo dessa métrica.
WI e Ni representam respectivamente o peso da operação de inserção e o número de inserções.
De a mesma forma, na segunda parte do numerador são calculados os custos para operações de deleção.
Em o denominador, Wmax é o valor máximo entre WI e Ni;
E LE é o tamanho da cadeia de caracteres.
A divisão na equação normaliza o valor da métrica entre 0 e 1, independente do tamanho da string.
Wmax+ LE A métrica SSD é focada no custo individual de operações de transformação, já que cada operação tem seu peso individual.
Contudo, para o contexto de processos, os autores consideram interessante diferenciar operações de inserção ou deleção realizadas em sequência.
Consequentemente, um bloco da mesma operação em sequência é considerado um desvio mais sério do que se esse mesmo conjunto estiver disperso no log.
A equação e Nd são os números de blocos de inserção e deleção, b é o comprimento de um bloco em específico e f (b) é uma função de custo aplicada a um comprimento de bloco b.
Os demais termos são os mesmos da métrica SSD.
A definição da função de custo f (b) provê um parâmetro de ajuste adicional da métrica j $= 1 WI f (bj)+ k $= 1 WD f (bk) Wmax LE As duas métricas apresentadas acima fornecem valores únicos como medida da correspondência.
No entanto os autores ainda salientam que, derivadas dessas métricas principais, algumas medidas auxiliares são naturalmente obtidas.
São elas:·
o número de eventos que correspondem com o fluxo comparado· o número de inserções e deleções utilizadas para calcular a métrica· número e tamanho médio dos blocos de operações· os locais do modelo onde os desvios ocorrem O trabalho de Cook e Wolf apresentou um método, até então inovador, para comparar dois fluxos de eventos e mostrar as diferenças quantitativamente.
Mais recentemente, Rozinat e Aalst apresentam uma proposta semelhante porém com alguns avanços, principalmente relacionados ao tratamento visual das informações de conformidade.
Diferentemente de, a proposta em utiliza um modelo em rede de Petri como referência.
Isto é uma vantagem já que, além de facilitar o entendimento por meio de uma notação gráfica, não necessita da aplicação de técnicas de seleção de um único fluxo dentro de as possibilidades do modelo.
O tratamento visual que o algoritmo Conformance Checker da aos resultados da análise é realmente um diferencial da solução.
Além de o provimento de métricas, as discrepâncias entre o modelo e a execução são destacadas e podem ser vistas tanto sob a perspectiva dos registros de execução quanto do modelo prescrito.
A interoperabilidade é considerada também um ponto positivo da proposta de.
O formato XML padrão de entrada e a ferramenta de conversão facilitam bastante o trabalho de préprocessamento de dados para a mineração de processos.
Este algoritmo é apresentado a seguir.
Conformance Checker O algoritmo Conformance Checker utiliza para o cálculo de fitness, uma abordagem semelhante à utilizada em.
No entanto, ao invés de comparar sequências de caracteres esta solução executa o log numa rede de Petri, calculando o número de inserções e deleções de tokens necessárias para essa execução.
Além disso, as métricas auxiliares apresentadas nas duas soluções também são semelhantes.
O algoritmo possibilita detectar as discrepâncias entre um modelo e um log comparandoos diretamente, de forma visual e por meio de métricas.
As métricas implementadas por este algoritmo pertencem as duas classes, denominadas fitness e appropriateness.
A análise de fitness busca investigar se um modelo permite reproduzir todas as sequências de execução dos eventos do log.
Em outras palavras, verifica se os traços de execução do log estão conformes com a descrição do modelo.
No entanto, uma rede de Petri pode ser capaz de gerar os caminhos contidos no log e ainda permitir comportamentos adicionais.
Em esse caso, talvez o modelo contenha caminhos desnecessários.
Para medir esse tipo de situação, é introduzida a dimensão appropriateness.
Essa métrica é analisada sob a perspectiva do modelo.
O objetivo é verificar o quão precisamente o modelo captura o comportamento identificado no log.
De essa forma é possível fazer ajustes no modelo.
Entretanto, para o caso da análise de conformidade, a noção de appropriateness é de menor relevância.
Isto porque não se busca melhoria do processo, e sim verificar se ele é seguido adequadamente.
Portanto este trabalho irá explorar somente a visão de fitness.
Uma forma utilizada para medir a conformidade entre um modelo e um log (fitness) é executar o log no modelo e, de alguma forma, medir as discrepâncias.
O algoritmo Conformance Checker utiliza essa abordagem.
Os eventos do log são executados no modelo de maneira não blocante, ou seja, no caso de faltar um token (transição no modelo não contida no log) ele será criado artificialmente e a execução continua.
Os tokens criados são contabilizados na métrica de fitness ao final da execução.
A execução inicia colocando um token no estado inicial da rede de Petri.
Então, os eventos contidos no log disparam transições no modelo.
Durante a execução alguns tokens terão de ser criados artificialmente (para o caso da transição presente no log não estar habilitada no modelo e dessa forma não poder ser executada) e outros não serão consumidos (transições habilitadas no modelo que não possuem um evento correspondente no log).
A métrica de fitness é calculada com base no balanço final entre tokens criados e restantes no modelo ao final da execução.
Abaixo é mostrada a equação para o cálculo da métrica.
Em a equação, k é o número de trilhas1 do log.
Para cada trilha i, ni é o número de instâncias combinadas nessa trilha.
Este é um artifício utilizado para simplificação dos cálculos.
Como várias instâncias a palavra trilha é utilizada aqui como sinônimo de trace.
Significa o conjunto de eventos de uma instância completa no log podem possuir a mesma sequência de eventos, elas são combinadas numa única trilha.
A quantidade de trilhas agrupadas é dada por ni.
A variável mi representa o número de tokens faltantes.
São aqueles que tiveram de ser criados artificialmente para continuar a execução.
Ri é o número de tokens restantes, ci é o número de tokens consumidos e pi, o número de tokens produzidos (naturalmente, de acordo com a lógica das redes de Petri) durante a execução da trilha corrente no modelo.
A métrica f então é dada por:
Note que, para todo i, mi ci e ri pi.
Então, 0 f 1.
Note também que ci e pi não podem ser zero porque durante a execução do log existirá pelo menos um token produzido no início e outro consumido no fim.
O algoritmo Conformance Checker provê várias formas de visualizar discrepâncias entre um modelo e registros de execução.
A solução encontra- se também disponível como um plugin no ProM logo, sua interface pode ser explorada amplamente.
Além de a métrica de fitness, diversas outras funcionalidades permitem um diagnóstico completo sobre execução do log no modelo em rede de Petri.
Um ponto importante é que a solução não exige que as atividades no modelo e no log sejam equivalentes, ou seja, possuam o mesmo nome e mesma granularidade.
É provida uma funcionalidade de mapeamento entre as duas fontes de dados.
De essa forma, uma pessoa com conhecimento sobre os processos pode identificar visualmente as correspondências entre as atividades do modelo prescrito e os registros de execução.
Até mesmo um metamodelo de desenvolvimento padrão pode ser adotado diretamente, sendo feita então a correspondência para os nomes efetivamente registrados para cada atividade.
Além disso, uma atividade no modelo pode ser ligada com vários eventos no registro e vice-versa.
Isso permite gerenciar diferenças de abstração entre modelo e dados de execução.
Por fim, uma atividade no modelo ainda pode ser tornada invisível para o caso de ela nunca ocorrer nos registros.
Ela não será considerada durante a análise de conformidade.
Essa função também é importante pois permite a um conhecedor do domínio fornecer uma heurística ao algoritmo, evitando assim a detecção incorreta de divergências.
Após o mapeamento, os resultados da análise podem ser vistos sob duas perspectivas:
Com a visão do modelo, a métrica de fitness é mostrada.
Ela é calculada com base nas relações entre tokens produzidos, restantes, consumidos e faltantes, conforme a equação 4.3.
Se o log é executado corretamente no modelo sem nenhum token faltante ou restante no final, o fitness é 1.
Há ainda uma série de opções para facilitar a identificação das diferenças.
Todas as funcionalidades são explicadas a seguir e podem ser observadas na figura 4.11.
À esquerda na janela estão as instâncias do log, que podem ser selecionadas individualmente.
A o centro é apresentado o modelo com todas informações de inconformidades, podendo ser cada uma de elas desabilitada.
O valor de fitness pode ser visto à direita na figura.·
Token counter:
Mostra os tokens faltantes e restantes para cada transição do modelo.
Permite localizar precisamente as partes onde ocorreu um problema.·
Failed tasks: Destaca as atividades que não foram habilitadas e consequentemente não sendo executadas.
Aparecem em amarelo no modelo.·
Remaining tasks: Essas são atividades que deveriam ser executadas (possuem um token na entrada) mas não ocorreram no log.
São destacadas em cinza no modelo.·
Path coverage:
É possível visualizar todas as transições executadas, não importando se foram executadas com sucesso ou tiveram de ser forçadas, gerando tokens faltantes.
Essa funcionalidade permite acompanhar o caminho de uma ou várias instâncias no log.
As atividades são coloridas em verde.·
Passed edges: Mostra em cada conexão, o número de vezes em que ela foi executada considerando todas as instâncias do log.·
Successful execution: Métrica dada por a fração instâncias de processo executadas com sucesso, levando em conta o número de ocorrências por trilha.·
Proper completion: Métrica dada por a fração instâncias de processo completadas propriamente, levando em conta o número de ocorrências por trilha.·
Failed Log Events: Destaca na visão do log os eventos que não puderam ser executados corretamente.
Análise de conformidade:
Aplicação Enfim, a partir de o estudo descrito conclui- se que técnicas de mineração de processos podem ser sim aplicáveis no contexto de PDS, levando em conta suas características específicas.
Esta seção apresenta uma série de considerações acerca de a análise de conformidade em PDS, aplicando mineração de processos.
A discussão é feita fundamentalmente com base no ambiente encontrado na empresa parceira.
Uma aplicação prática foi realizada com o algoritmo Conformance Checker, utilizando os mesmos dados do projeto P1 para a fase de testes.
Modelo predefinido O primeiro passo para comparar um processo definido com sua execução é obter um modelo desse processo.
Para a avaliação com mineração de processos, normalmente é necessário um modelo em rede de Petri.
Tal modelo poderia ser obtido de três diferentes maneiras:
Inferir utilizando técnicas de descoberta de processos por mineração, utilizando os próprios registros de execução, adotar um modelo construído por a própria empresa ou utilizar um modelo padrão de desenvolvimento como por exemplo, Processo Unificado.
Cada uma dessa alternativas possui vantagens e desvantagens.
A adoção de uma de elas dependerá de fatores específicos de cada empresa.
A primeira abordagem pode facilitar o mapeamento entre modelo e dados de execução.
Além disso, o modelo produzido por a descoberta a partir de o histórico de execução provavelmente será mais próximo de a realidade do projeto.
No entanto, o estudo descrito na seção 4.2 mostrou que descobrir processos a partir de registros de PDS pode apresentar algumas dificuldades.
Primeiramente devido a os processos de software serem normalmente pouco estruturados.
Em segundo, algoritmos de mineração de processos exigem um volume grande de instâncias para descobrir um modelo preciso, o que não é uma situação comum no ambiente de desenvolvimento de software.
Por fim, mesmo que represente a realidade, o modelo produzido pode não ser considerado satisfatório por a empresa.
Em esse caso, a melhor alternativa é adotar um modelo previamente construído como padrão.
De essa forma a organização pode ter uma visão clara sobre a semelhança entre o fluxo ideal/ esperado para o processo e a realidade representada por os dados de execução.
A transformação adequada do modelo para uma rede de Petri possibilita a avaliação formal utilizando algum dos algoritmos apresentados anteriormente ou mesmo outras soluções.
Por último, um metamodelo de processo pode ser utilizado.
Metamodelos de PDS costumam expressar detalhadamente o workflow de cada fase de projeto.
O procedimento é semelhante ao da alternativa anterior.
A única condição em qualquer um desses dois casos é que seja possível fazer um mapeamento (mesmas atividades/ mesmo nível de abstração) entre o modelo e os dados de execução do processo.
Exame de um caso real Com relação a o modelo de processo utilizado para comparação foram consideradas duas alternativas:
Adotar os modelos documentados no OSSP da empresa parceira ou obter por mineração.
As duas hipóteses foram exploradas com o intuito de identificar as diferenças.
Em a primeira abordagem foi utilizada uma ferramenta de desenho de redes Petri para construir o modelo, baseado no workflow de testes da organização.
Esse workflow foi construído baseado estritamente na documentação disponível.
A única modificação feita foi a separação das atividades em início e fim.
Isto foi feito observando a estrutura dos dados reais de execução.
Note- se que uma mesma atividade na base de métricas pode estar registrada em várias linhas, dependendo do número de vezes em que o colaborador trabalhou na tarefa.
Para tornar os dados coerentes com a representação de processos, todas as atividades foram divididas nas etapas de inicio e fim.
Entretanto, os modelos de workflow documentados não prevêem esse tipo de situação.
Sendo assim, mesmo que o processo seja executado na ordem correta o algoritmo irá detectar inconformidade quando houver repetição de atividades.
Para possibilitar a comparação adequada, levando em conta a duração de cada atividade, o modelo também deve conter explicitamente identificadores para o início e fim dessas atividades.
Embora essa modificação já possibilite a verificação, o modelo ainda continua sequencial.
O processo em rede de Petri foi derivado dos modelos de workflow originais do OSSP da empresa, que por sua vez não expressam paralelismo.
Consequentemente, atividades executadas em paralelo serão interpretadas como inconformes com o processo definido.
Em esse caso é importante que a organização identifique a concorrência entre atividades, seja por algum conhecimento prévio sobre seus projetos ou mesmo com o auxílio da mineração de processos.
Os resultados do estudo exploratório mostram que esta última abordagem pode ser inclusive um bom meio de compreender a realidade dos processos, visualizando detalhes como concorrência, pontos de decisão e loops.
Dados de execução Os dados utilizados nessa aplicação foram essencialmente os mesmos do experimento relatado no estudo exploratório.
A análise foi aplicada sobre o processo de testes, adotando a versão de produto como instância.
21 versões foram selecionadas, sendo que nenhum procedimento adicional de preparação de dados foi feito além de aquele já descrito na seção duas formas.
Primeiro, os registros podem ser submetidos a um algoritmo de descoberta de processos.
Os modelos obtidos podem ser analisados visualmente, o que já fornece algumas pistas sobre o comportamento do projeto.
Indicadores fornecidos por esses algoritmos também podem ser explorados.
Adicionalmente pode se utilizar um algoritmo de análise de conformidade como os apresentados na seção 4.3.1.
Os resultados desse tipo de solução podem complementar a visão do auditor sobre os processos executados com indicações e medidas objetivas sobre a conformidade.
Execução do algoritmo e resultados providos Tendo os dados de execução devidamente preparados e pré-selecionados, é possível então aplicar um algoritmo de análise de conformidade para analisar o processo.
Note- se que o presente cenário é baseado no algoritmo Conformance Checker.
Antes de executar o algoritmo, cada atividade do modelo definido deve ser relacionada com um evento nos registros de execução.
Como o algoritmo utiliza um modelo em rede de Petri para comparação, cada evento do log irá disparar a transição correspondente no modelo (se existir), de acordo com o mapeamento realizado.
Este algoritmo fornece uma funcionalidade para tal mapeamento, como pode ser visualizado na figura 4.13.
Em a coluna esquerda da janela são mostradas as atividades do modelo predefinido.
Em o centro, encontram- se as atividades que aparecem pelo menos uma vez no log.
A coluna da direita mostra a identificação final que aparecerá no modelo, resultante do mapeamento.
Em essa etapa espera- se que alguém com conhecimento, tanto sobre os processos quanto sobre os dados, seja capaz de identificar a correspondência entre as atividades.
Note- se que eventos que não aparecem nos registros não poderão ser mapeados para o modelo.
Em esse caso, o evento do log pode ser tornado visível ou invisível.
Em o primeiro caso a atividade será inserida explicitamente no modelo, como pode ser observado na atividade Design da figura 4.13, por exemplo.
Entretanto, como a atividade não aparece no registro, isso poderá resultar num ponto de inconformidade caso a execução de tal atividade seja obrigatória no processo predefinido.
Alternativamente, uma atividade pode ser tornada invisível.
Eventos invisíveis nas redes de Petri são geralmente utilizados para controle, não fazendo parte do processo em si.
Em esse caso ficará por conta do algoritmo disparar automaticamente essas transições durante a avaliação do log, de acordo com alguma heurística.
A heurística utilizada por o Conformance Checker é a do caminho mais curto.
Isto significa que, na presença de mais de um caminho com atividades invisíveis, será disparada a transição de menor caminho.
Tal funcionalidade é útil quando se sabe previamente que uma atividade do modelo nunca acontece na execução, evitando então a geração indevida de uma inconformidade.
Após o mapeamento das atividades o algoritmo pode então ser executado.
Um último ponto a ser observado antes da análise diz respeito à busca de atividades invisíveis.
É possível restringir o número de atividades buscadas.
Em este cenário o nível de busca foi estabelecido em 0 porque nenhuma atividade invisível está presente no modelo.
Considerando para comparação os modelos definido e minerado, esses dois cenários são discutidos a seguir.
Quanto maior for o número de instâncias com atividades não executadas ou executadas fora de ordem, menor será o valor de fitness.
Para servir como parâmetro, foi criada artificialmente a versão 00.00.00.
É simulada a execução perfeita de uma iteração do processo, passando uma vez por cada atividade até a liberação da versão.
como se pode observar na figura 4.15, a instância é 100% conforme.
Mesmo fornecendo uma noção quantitativa sobre a conformidade, a medida de fitness é uma informação restrita, baseada num único valor.
Mas, outras informações disponibilizadas por o algoritmo podem ser exploradas.
O número de vezes que cada transição pode ser visualizado com o indicador Passed Edges habilitado.
De essa forma é possível identificar precisamente as atividades e caminhos mais frequentes, o que também é uma pista importante sobre os padrões de execução (correta ou incorreta) do processo.
Ainda sob a visão do modelo, as cores utilizadas para enfatizar certos aspectos também auxiliam na análise.
Em a figura 4.14 se observa eventos marcados com três cores.
Em verde são destacados aqueles que ocorreram pelo menos uma vez no log.
Em amarelo, os eventos que foram executados com sucesso por não estarem prontas (sem token na entrada).
SQA poderia interferir pontualmente junto a esse colaborador buscando identificar a causa do desvio.
Comparando com o modelo minerado O cenário anterior é baseado num modelo padrão definido por a organização.
Contudo, pode ser desejável comparar a execução com um modelo inferido por mineração.
Considerando os aspectos discutidos na seção 4.4.1, um modelo de processo obtido por mineração pode ser uma alternativa interessante visto que se tem um fluxo mais próximo de a realidade do projeto.
A figura A. 2 mostra um modelo em rede de Petri produzido por o algoritmo Heuristics Miner, descrito na seção 4.2.
Embora o modelo original esteja numa notação própria, a ferramenta ProM disponibiliza um plugin que permite fazer a transformação diretamente dessa notação para uma rede de Petri.
De fato esse modelo quando comparado com a execução do processo apresenta maior semelhança.
Isto pode ser verificado por o valor de fitness, que foi maior do que o obtido por a comparação com o modelo do OSSP.
Seu valor foi de 0,81.
Se observa, contudo, que o fluxo do processo minerado possui algumas diferenças com relação a o definido na documentação organizacional.
A existência de tal discrepância é também uma questão importante a ser analisada, onde podem ser consideradas duas hipóteses:
Ou o processo apresenta inconformidades bastante frequentes, ou na verdade o modelo definido não representa adequadamente o comportamento normal dos seus processos.
Cabe ao grupo de qualidade responder tal questão e decidir por alterar o processo definido ou interferir juntamente aos projetos, buscando eliminar tais inconformidades.
Considerações sobre o capítulo Se constatou no estudo realizado, que a mineração de processos pode trazer benefícios na tarefa de analisar a execução de processos de software.
As diferenças entre os processos definidos e executados podem ser acessadas, tanto na visualização dos modelos produzidos por algoritmos de descoberta de processos, quanto por a avaliação de métricas fornecidas por algoritmos de análise de conformidade.
O estudo permitiu traçar parte do cenário que pode envolver a verificação de conformidade em PDS, utilizando técnicas de mineração de processos como ferramenta.
Portanto, derivado da experiência obtida foram compiladas algumas sugestões para a aplicação de uma solução desse tipo no ambiente de desenvolvimento de software.
Embora várias dessas questões já tenham sido levantadas anteriormente, elas são sumarizadas a seguir.
Sugestões ao analisar conformidade em PDS aplicando mineração de processos A o verificar conformidade em PDS, possuindo um ambiente semelhante ao descrito no presente trabalho, certas questões devem ser levadas em conta.·
Processos Se existir, deve ser possível mapear cada atividade para correspondente nos dados de execução.
Além disso, o modelo deve ser transformado para uma rede de Petri possibilitando a aplicação direta num algoritmo de análise de conformidade.
Não havendo um modelo documentado, ainda é possível inferir por mineração.
No entanto, questões como dados insuficientes, incompletos e/ ou incorretos podem reduzir a qualidade dos modelos produzidos.·
Dados são requisitos mínimos para análise de fluxo de controle.
No caso de os algoritmos apresentados, que não utilizam o campo EventType, a diferenciação entre início e fim de atividade deve ser feita junto ao identificador da própria atividade, conforme pode ser observado na figura 4.8.
Adicionalmente, a inclusão do colaborador envolvido na tarefa permite estender a análise do processo fornecendo informações sob a perspectiva organizacional.
É possível, por exemplo, verificar se a correspondência entre atividade/ papel prevista no modelo é realmente obedecida.
O tratamento realizado no cenário de aplicação apresentado consistiu em utilizar a primeira e última entradas da mesma atividade, criando um registro da forma início/ fim e eliminando as atividades repetidas.
Porém, essa abordagem pode acarretar em perda de informação.
Isto porque não é feita distinção entre uma única tarefa, que é registrada em várias partes e um laço efetivamente.
Essa distinção não foi feita no estudo exploratório por não ser possível capturar a infor-mação necessária para tal.
Como exemplo, vários registros de um plano de teste podem ser abstraídos numa única atividade com duração determinada.
No entanto, caso de teste pode ser realmente uma atividade iterativa, sendo executado uma vez para cada caso de teste previsto no plano.
Em esse caso, a atividade poderia ser mantida no registro, sendo necessário, contudo, explicitar tal comportamento no modelo predefinido.
Os/ versão relacionada (se encontrava, por exemplo, Os registradas da forma:
Os-XX, OSXX, XX, etc.), definir algumas diretivas para a inserção de comentários sobre a tarefa realizada.
Essas são algumas das restrições que poderiam diminuir o volume de dados descartados, consequentemente melhorando a qualidade dos achados.·
Resultados fora de ordem são destacadas visualmente, de forma que podem ser facilmente identificadas, contabilizadas, servindo como mais um indicador de conformidade.
Além disso, métricas fornecidas por os algoritmos de descoberta de modelos também podem ser exploradas.
Os valores de significância das atividades e correlação entre elas podem fornecer pistas sobre o fluxo mais frequente de execução de um processo.
Contudo, notou- se que em certos casos a análise sob essa perspectiva pode dificultar a visualização correta da conformidade.
Em certas situações, um token disponível permite que uma atividade seja executada corretamente, mesmo que fora de ordem.
Por exemplo, se a primeira atividade do processo aparecer em qualquer ponto do registro, ela será mostrada como conforme no modelo.
Isto ocorre porque o algoritmo sempre inicia com um token no início da rede de Petri.
Outras combinações também podem gerar situações semelhantes, distorcendo os resultados.
Portanto, é interessante que a perspectiva do log também seja acessada, permitindo- se visualizar a ordem efetiva em a qual as atividades foram executadas.
Atividades não executadas também podem ser facilmente identificadas, visualizando o processo sob essa perspectiva.
A busca por formas de monitorar e melhorar processos de software não é nova.
São encontradas na bibliografia diversas contribuições tratando do assunto.
Este capítulo apresenta os trabalhos vistos como mais relevantes para a pesquisa, envolvendo a exploração de dados de execução de PDS para descoberta de conhecimento sobre processos e também análise de conformidade.
Sorumgard Sorumgard estudou verificação de conformidade na engenharia de software, de maneira experimental.
Em esse sentido, as contribuições do trabalho se concentram em quatro aspectos principais:·
Definir um modelo de conformidade para a engenharia de software· Estabelecer um guia para modificação de processos, de forma que ele possa permitir a medição da conformidade em sua execução· Testar a proposta por meio de um experimento controlado Com relação a a definição do modelo, é introduzido o conceito de vetor de desvio.
Esse vetor serve para representar a conformidade de acordo com determinadas dimensões.
Dimensões são interpretadas aqui como propriedades ou elementos de um processo.
O modelo de conformidade proposto é definido com base em parâmetros, que pertencem a três categorias:
Processos, produtos e recursos.
Para cada um desses parâmetros são estabelecidos atributos observáveis.
Por exemplo, um processo possui tempo, esforço, número de eventos e custo.
Um produto possui uma medida de tamanho e qualidade.
Assume- se que, para cada um desses atributos, possam ser definidos valores esperados.
Tais medidas comparadas com as obtidas na execução do processo produzirão um vetor de desvio, conforme ilustrado na figura 5.1.
Esse vetor de desvio pode ter um número qualquer de dimensões, dependendo dos atributos observados.
A figura 5.1 mostra um possível vetor envolvendo as dimensões tempo e qualidade.
As variáveis Ci, p e Qi, p representam os valores esperados (predicted) para as dimensões em questão.
Já os valores medidos na execução estão representados por Ci, e Qi, e.
O vetor de desvio expressa então a diferença vetorial entre o processo pressuposto e o realizado.
Por exemplo, se o processo é executado exatamente como esperado o vetor de desvio terá valor.
O trabalho é bastante consistente, abrangendo vários aspectos de um processo relacionados à discrepância entre planejamento e execução.
No entanto, é focado na análise da diferença entre medições obtidas sobre cada atributo envolvido no processo.
O trabalho assume que, uma vez definidos os atributos necessários para medir a conformidade, existem valores pré-definidos para cada dimensão.
De essa forma o trabalho foca em definir formalismos matemáticos para obter o vetor de desvio que representa a conformidade do processo.
Huo Em este trabalho os autores propõem um método para a descoberta de padrões sobre dados de PDS.
Tais padrões são expressos num modelo em rede de Petri que, segundo os autores, pode ser usado para ser comparado com um modelo padrão de desenvolvimento adotado por a empresa.
Para descoberta de relações entre atividades, o trabalho propõe uma solução, baseada no algoritmo.
Entretanto, o algoritmo proposto em busca superar as limitações existentes naquela solução.
Primeiro, o algoritmo tenta descobrir um modelo completo.
Consequentemente, o conjunto de dados precisa ser completo.
A solução proposta busca inferir qualquer padrão recorrente identificado nos dados, não necessitando de instâncias do log com todas as atividades executadas.
Essa abordagem se aproxima da utilizada por métodos estatísticos.
Porém, os autores argumentam que tais soluções exigem um grande volume de dados, o que não é uma situação comum em projetos de software.
Além disso, os autores salientam a necessidade de garantir a equivalência no nível de abstração entre os padrões obtidos por mineração e o modelo predefinido.
Em o contexto apresentado, as atividades registradas podem estar em menor granularidade do que os elementos do modelo.
Para resolver o problema os autores sugerem o mapeamento manual, feito por pessoas conhecedoras do domínio.
Para garantir um mapeamento correto é utilizado o método Kappa, para medir a concordância entre difer-entes avaliadores.
De essa forma é garantido que o modelo/ padrão minerado poderá ser comparado diretamente com o modelo de desenvolvimento predefinido.
Uma restrição identificada neste trabalho, contudo, é relacionada à forma da comparação pressuposta.
Primeiro, não é considerada a análise de instâncias em separado, já que os padrões inferidos são obtidos de todo o conjunto de dados.
Segundo, a comparação se daria apenas de forma visual, sem a produção de nenhuma medida objetiva sobre a conformidade entre modelo e execução.
No entanto, a contribuição é vista como complementar à pesquisa por tratar um problema inclusive já identificado no presente trabalho, que envolve o mapeamento e seleção correta das atividades na base de métricas para realização da comparação de forma adequada.
Silva e Travassos Assim como em, Silva e Travassos também estudaram a verificação de conformidade na engenharia de software experimental.
Essa área é identificada por os autores como carente desse tipo de verificação.
São citados estudos constatando que normalmente pessoas não seguem os processos propostos durante a realização de experimentos.
O objetivo principal do trabalho trata- se da implementação e avaliação (por meio de experimentos com observação não intrusiva) de uma ferramenta para auxílio na inspeção de artefatos em PDS.
A ferramenta é baseada no conceito Perspective Based Reading (PBR) para inspeção.
Seus objetivos principais são:
I) guiar o executor da inspeção de acordo com a técnica proposta e ii) facilitar a descrição das discrepâncias identificadas.
Além disso a ferramenta é capaz de coletar dados sobre a execução da inspeção (tempo total de inspeção, tempo e ordem das tarefas na identificação de discrepâncias, número de consultas a uma ferramenta de ajuda, etc.).
Essa funcionalidade serve para monitorar o utilizador da ferramenta de forma não intrusiva, para garantir que a técnica definida na ferramenta foi realmente seguida.
Entretanto, o trabalho cita a utilização das métricas num experimento realizado para avaliar o uso da própria ferramenta.
Embora não contendo nenhum dado conclusivo, os autores esperam responder questões como:·
A eficiência dos revisores é maior quando utilizando a ferramenta?·
O uso da ferramenta pode motivar os revisores a seguirem as técnicas propostas?·
A ferramenta pode melhorar o entendimento sobre a técnica?
Porém, os autores não descrevem detalhes sobre a forma em a qual essas métricas deveriam ser interpretadas, buscando avaliar conformidade na execução da técnica PBR.
O foco do trabalho se restringe a discutir algumas abordagens de monitoração de execução para analisar conformidade, além de a implementação da ferramenta.
Embora o problema da conformidade em PDS seja bastante citado, os trabalhos encontrados não discutem detalhes sobre como uma solução para verificação de conformidade poderia ser implementada nesse ambiente.
Os trabalhos analisados não tratam objetivamente da estrutura, fonte e preparação de dados necessárias para viabilizar a exploração de dados de execução em PDS.
Além disso, as contribuições encontradas são pontuais.
Processos de software não são tratados amplamente, sob todas suas perspectivas incluindo:
Considerações Finais O presente trabalho teve como principal objetivo explorar a mineração de processos como ferramenta auxiliar na verificação de aderência aos processos definidos num ambiente de desenvolvimento de software.
Inicialmente, um estudo de caso exploratório confirmou a viabilidade em descobrir relações entre eventos de um processo típico de software, tendo como fonte de dados uma base de métricas de esforço semelhante àquela encontrada na empresa parceira.
As lições aprendidas com esse primeiro estudo permitiram identificar questões importantes a serem consideradas, como:·
Formato típico dos processos definidos em PDS;·
Algoritmos mais adequados para mineração de dados de execução nesse ambiente;·
Escolha adequada dos dados para mineração, formas de preparação;·
Formato dos resultados produzidos, interpretação dos mesmos;
Sobre verificação de conformidade mais especificamente, o trabalho envolveu a pesquisa de algoritmos de análise existentes além de uma aplicação num cenário prático.
Tendo em vista os requisitos levantados, para a melhoria do procedimento de auditoria de processos de software, se conclui que eles podem ser satisfeitos (ao todo ou em parte) por técnicas automáticas de descoberta de conhecimento.
A cobertura de tais requisitos é compreendida da seguinte forma:·
Auditoria não intrusiva:
Esse é um dos aspectos mais relevantes, visto que a análise dos dados de processos de forma automática diminui a dependência de entrevistas para obtenção de informações;·
Apresentar indicadores:
Durante o estudo foi possível identificar que uma das principais características de algoritmos de análise de conformidade é a apresentação de métricas e outras informações que permitam avaliar quantitativamente o grau de semelhança entre processo definido e executado.
Além disso, os próprios modelos obtidos por descoberta de processos fornecem indicadores que também podem ser utilizados, juntamente com a exploração visual, na avaliação de comportamento da execução dos processos;·
Agilizar o trabalho de auditoria:
Considera- se que a mineração de processos também pode auxiliar nesse aspecto, já que o volume de dados analisados pode ser muito maior do que seria viável manualmente;·
Não exigir grande interferência no processo atual:
Pelo menos no caso de a empresa parceira, algumas modificações poderiam ser necessárias, tanto no modelo quanto no registro, para obter informações mais detalhadas sobre a execução dos processos.
Mesmo assim, entende- se que a rotina de trabalho dos colaboradores não seria afetada consideravelmente.
Com alterações simples na forma e classificação dos registros de atividades se pode aumentar sensivelmente a rastreabilidade dos eventos ocorridos no dia-a-dia dos projetos;
Portanto, considerando os objetivos inicialmente definidos para o trabalho, as principais contribuições são relacionadas a seguir:·
discussão acerca de os problemas envolvendo análise de conformidade em PDS e requisitos de uma solução ideal;·
apresentação de métodos de mineração de processos como ferramenta para melhoria de eficiência e qualificação dos resultados de auditorias de processos de software;·
análise sob o aspecto prático da aplicação da mineração de processos, baseada num estudo de caso exploratório com dados de uma grande empresa de software;·
compilação de uma série de sugestões para exploração de mineração de processos em PDS, produzidas a partir de a experiência adquirida durante o estudo realizado;
Trabalhos futuros De fato, a ordem em a qual atividades são desenvolvidas num processo fornece diversas pistas sobre o andamento desse processo e acerca de a aderência de seus colaboradores com relação a o planejamento.
Porém, também é fato que PDS lidam intensamente com a criação e alteração de todo o tipo de artefato (documentos, códigos, modelos, etc.).
Grande parte desses processos envolve atividades desse tipo.
Trata- se muitas vezes de pequenas tarefas que normalmente não são registradas explicitamente, como revisões informais ou pequenas alterações em documentos ou até mesmo em códigos.
Consequentemente, se torna difícil monitorar precisamente tais atividades.
Mesmo assim é importante que tais informações também possam ser rastreadas, já que afetam o projeto diretamente.
Em esse aspecto, ferramentas computacionais tipicamente utilizadas na engenharia de software podem fornecer dados importantes para uma análise ainda mais consistente sobre os processos.
O conjunto de ferramentas de suporte utilizadas atualmente permite que projetos de software sejam monitorados, não só sob a visão de sequência de atividades, mas também com relação a os recursos e artefatos envolvidos.
Portanto, essa é uma direção importante vista como trabalho futuro:
Explorar outras visões do processo.
É parte central dos modelos de desenvolvimento de software definir os papéis envolvidos em cada atividade e os produtos de entrada e saída em cada parte do processo.
A mineração de processos também fornece suporte esse tipo de análise.
Sistemas de gerência de configuração (SCM), por exemplo, são comumente utilizados em PDS como ferramenta para centralização de dados e controle de versões.
Mas, tais ferramentas normalmente produzem logs de todos os eventos como, por exemplo, quem acessou, quais documentos, que tipo de acesso, quando foi feito.
Utilizar- se do log de um sistema de SCM para complementar a análise de conformidade sob as perspectivas organizacional (recursos) e de informação (artefatos) é parte dos trabalhos futuros dessa pesquisa.
Outro ponto apontado como trabalho futuro diz respeito ao provimento de métricas mais inteligíveis sob a visão de um auditor de SQA.
O presente trabalho se limitou a análise das métricas de fitness produzidas por algoritmos de verificação de conformidade.
Contudo, se considera que a apresentação de métricas derivadas, de fácil entendimento, poderia auxiliar ainda mais a visão da equipe de SQA sobre os processos executados.
Dados estatísticos como o número de instâncias inconformes, valor mínimo e máximo de fitness, quantidade de atividades faltantes e/ ou executadas fora de ordem para cada instância seriam alguns exemplos.
Em a verdade todos esses indicadores podem ser obtidos indiretamente dos algoritmos.
O trabalho necessário seria extrair essas informações e apresentar- las adequadamente, juntamente com aquelas já disponibilizadas.
A melhoria no procedimento de pré-processamento de dados também é uma questão a ser tratada na continuação do trabalho.
Se faz necessária a implementação de uma interface com rotinas automáticas de seleção, consolidação e filtragem dos dados para facilitar essa tarefa.
Tendo como objetivo principal facilitar o trabalho de auditoria em PDS, não é ideal que seja exigido um procedimento trabalhoso e complexo de preparação dos dados.
Com poucos cliques, o auditor deveria ser capaz de, por o mínimo, poder selecionar a fase e o número de instâncias a serem analisadas.
