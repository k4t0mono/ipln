Este trabalho tem como propósito estudar estruturas conceituais geradas seguind método Formal Concept Analysis.
Usamos na construção dessas estruturas informações lexicossemânticas extraídas dos textos, de entre as quais se destacam os papéis semânticos.
Em nossa pesquisa, propomos formas de inclusão de tais papéis nos conceitos produzidos por esse método formal.
Analisamos a contribuição dos papéis semânticos e das classes de verbos na composição dos conceitos, por meio de medidas de ordem estrutural.
Em esses estudos, utilizamos os corpora Penn TreeBank Sample e SemLink 1.1, ambos em Língua Inglesa.
Testamos, também para Língua Inglesa, a aplicabilidade de nossa proposta nos domínios de Finanças e Turismo com textos extraídos do corpus Wikicorpus 1.0.
Essa aplicabilidade foi analisada extrinsecamente com base na tarefa de categorização de textos, a qual foi avaliada a partir de medidas de ordem funcional tradicionalmente usadas nessa área.
Realizamos ainda alguns estudos preliminares relacionados à nossa proposta para um corpus em Língua Portuguesa:
PLN-BR CATEG.
Obtivemos, nos estudos realizados, resultados satisfatórios os quais mostram que a abordagem proposta é promissora.
Palavras-chave: Processamento de Linguagem Natural;
Estruturas Conceituais; Relações Não Taxonômicas;
Papéis Semânticos; Formal Concept Analysis.
Estruturas ontológicas como dicionários, tesauros, taxonomias e ontologias têm se tornado um importante recurso para sistemas de informação.
Em sistemas de recuperação de informações, por exemplo, tais estruturas têm ajudado a minimizar problemas de vocabulário e recuperar informações mais relevantes.
Essas estruturas provêem um suporte semântico que permite modicar a consulta do usuário, substituindo termos desconhecidos do sistema por sinônimos, bem como enriquecendo esta consulta com termos relacionados pertencentes ao mesmo domínio.
Em tarefas de organização da informação, como classificação e agrupamento de documentos, essas estruturas conceituais têm trazido ganhos de precisão.
Elas têm sido usadas para me2 lhorar tais tarefas enriquecends textos com novos conceitos do domínio;
Auxiliando na determinação da classe ou grupo através da desambiguação dos termos dos documentos;
Embora seja grande a aplicabilidade das estruturas ontológicas, é alt custo de sua construção e manutenção.
Por esta razão, e devido a o grande volume e riqueza de documentos textuais digitais disponíveis atualmente, pesquisas têm sido realizadas com bjetivo de construir tais estruturas automaticamente a partir de textos.
As abordagens propostas geralmente usam informações linguísticas e estatísticas sobre os termos no texto, para extrair os conceitos e as relações semânticas entre eles.
É comum, também, o uso de algoritmos de aprendizagem de máquina, principalmente de agrupamento, para identicar os conceitos a partir de a coocorrência de termos num contexto, e para construir as estruturas ontológicas propriamente ditas, usando abordagens hierárquicas.
Como a construção de estruturas ontológicas a partir de textos é considerada um problema de difícil solução, outras abordagens têm surgido.
É o caso do método Formal Concept Analysis (FCA), apresentado no Capítulo 3, que foi introduzido na década de 80 para análise de dados e que vem sendo aplicado na construção de tais estruturas.
Um trabalho muito citado que segue essa abordagem é o de Cimiano, Hotho e Staab que utilizam FCA para construir automaticamente ontologias a partir de textos.
Cimiano Utilizam os argumentos sintáticos dos verbos (sujeito, objeto direto e objeto indireto) para caracterizar as entidades do domínio.
E usam os próprios verbos para definir as relações entre essas entidades.
Através do método FCA, que é fundamentado na teoria dos reticulados, as entidades são agrupadas conceitualmente e organizadas conforme os verbos com os quais se relacionam.
O método FCA tem sido usado para construir estruturas conceituais cujo propósito é apoiar diferentes tarefas em sistemas de informação.
Os resultados relatados por os pesquisadores são muito promissores.
Ele tem sido aplicado em tarefas de classicação, clusterização, recuperação de informações, mineração de dados e outras. Como
método para análise de dados ele também é interessante e trabalhos na área de linguística têm explorado esse aspecto.
Em esse sentido, ele pode ser usado para apoiar a construção manual ou integrar uma das etapas em processos semiautomáticos de elaboração Entendemos termo como uma unidade atômica de signicado.
Pode ser representado por uma palavra, o radical de uma palavra, um sintagma ou 2 n-grama.
Em este documento, estruturas conceituais e estruturas ontológicas são tratadas como sinônimos.
De ontologias.
Quanto a as ontologias, mais especicamente, o método FCA tem sido usado para geração de axiomas, mapeamento, similaridade e aprendizagem de ontologias.
Embora sejam muitas as aplicações do método, os trabalhos têm explorado aspectos mais sintáticos ou estatísticos quand objetivo é construir estruturas conceituais a partir de textos.
Até encontramos algumas abordagens que consideram aspectos semânticos, mas poucos sãs trabalhos que exploram, por exemplo, papéis semânticos de forma mais ampla.
O trabalho de Valverde-Albacete é um dos poucos, na atualidade, que utiliza FCA e papéis semânticos, no entanto, é voltado para análise linguística, tendo como propósito representar a FrameNet (Seção 4.4) através de reticulados de conceitos.
Os papéis semânticos (Capítulo 4), ainda que pouco explorados conjuntamente com o método FCA, têm sido utilizados com frequência em trabalhos na área de aprendizagem de ontologias a partir de textos.
Em esse contexto, eles provêem informações semânticas relevantes na medida em que &quot;expressam o signicado dos argumentos dos verbos em situações descritas por esses verbos».
Tais papéis permitem identicar, por exemplo, quem é o agente de uma ação -- a entidade que provoca o evento caracterizado por o verbo -- e também quem é o paciente dessa ação -- a entidade que sofre modicações em consequência do evento.
Eles permitem, portanto, caracterizar melhor as relações entre as entidades do domínio, mesmo que tais entidades ocorram em diferentes posições sintáticas.
Além disso, os papéis semânticos imprimem aos seus argumentos restrições semânticas.
Domínio e sugerir, de forma automática, uma organização mais adequada para esses conceitos.
Soma- se a iss fato de verbos conseguirem representar relações não taxonômicas que são comuns em estruturas ontológicas de domínio.
Assim como Navok e Hearst em, nós acreditamos que os verbos consigam capturar aspectos sutis de signicado, sendo, portanto, importantes fontes de expressividade em tarefas de representação semântica.
Considerando que o método FCA permite construir estruturas conceituais de domínio cujos relacionamentos entre as entidades podem ser baseados nos verbos, e que os papéis semânticos permitem atribuir características aos conceitos extraídos dos textos, propomos uma abordagem de construção de estruturas ontológicas baseadas em FCA e em papéis semânticos.
Acreditamos que nossa abordagem sugira uma organização mais interessante do ponto de vista semântico, por enriquecer com papéis semânticos a intensionalidade dos conceitos formais.
Além disso, por ser baseada em FCA, nossa abordagem pode ser gerada tanto de forma semiautomática como automática.
Tal abordagem foi avaliada por medidas estruturais e funcionais.
A medida estrutural de coesão lexical foi usada para analisar os conceitos gerados a partir de a abordagem proposta.
Também avaliamos nossos resultados na tarefa de categorização de textos, para a qual usamos medidas funcionais tradicionais.
A maioria dos estudos descritos nesse documento são voltados para a Língua Inglesa.
Utilizamos corpora anotados semanticamente como Penn TreeBank Sample e SemLink 1.1 em nossa investigação.
Utilizamos ainda os corpora WikiFinance e WikiTourism, respectivamente, dos domínios Finança e Turismo, para analisar nossa proposta em diferentes corpora e domínio.
Esses corpora foram extraídos do WikiCorpus 1.0 e anotados com papéis semânticos por o processador F-EXT-WS (Seção 5.3.4).
Realizamos ainda alguns estudos preliminares em Língua Portuguesa para a abordagem proposta, incluindo trabalhos em categorização de textos e extração de conceitos para o corpus Cabe mencionar que combinar estruturas FCA com papéis semânticos não é uma ideia recente.
Kamphuis e Sarbo em, na década de 90, propõem a representação de uma frase em linguagem natural, associando tais elementos.
Também na década de 90, o trabalho de Rudolf Wille apresenta exemplos de estruturas FCA relacionadas a papéis semânticos.
bjetivo de Wille, no entanto, é combinar grafos conceituais com estruturas FCA visando a formalização de uma lógica útil à representação e ao processamento de conhecimento.
Apesar de as abordagens daqueles autores parecerem promissoras à época em que foram propostas, até agora haviam sido pouco exploradas.
Provavelmente devido a a diculdade de anotação dos textos, visto que o surgimento de etiquetadores automáticos de papéis semânticos é mais recente.
Mesmo com a profunda revisão bibliográca realizada, não encontramos, até o momento, trabalhos que explorassem os papéis semânticos em conjunto com o método FCA para apoiar a construção de estruturas ontológicas a partir de textos.
Dado que a proposta de combinar- los é pouco pesquisada, consideramos ser de interesse tod esforço gerado ao investigar- la.
Nossbjetivo primário é combinar o método FCA com papéis semânticos para construir, de forma automática e a partir de informações textuais, estruturas ontológicas fortemente baseadas em relações não taxonômicas.
A ideia é explorar as vantagens do FCA como método de agrupamento conceitual.
O método FCA, quando comparado a outros métodos de agrupamento, permite delinear mais facilmente, do ponto de vista semântico, os grupos e subgrupos de uma hierarquia.
Já no que tange aos papéis semânticos, investigamos o uso dessa informação semântica no processo de extração de informação e construção dos conceitos, sendo que nossa meta é utilizar tal informação para qualicar e, portanto, melhorar os grupos (conceitos) gerados por o método FCA.
Para alcançar bjetivo proposto, realizamos uma pesquisa de carácter fundamentalmente exploratório.
Primeiramente, analisamos os corpora em Língua Inglesa, Penn TreeBank Sample e SemLink 1.1.
Em esse estudo preliminar, procuramos identicar os sintagmas nominais, os papéis semânticos, as relações e as classes de verbos que poderiam contribuir para uma pesquisa mais expressiva do ponto de vista quantitativo (Seção 6.4).
Os resultados dessa análise foram determinantes para o direcionamento de nossa pesquisa.
Em seguida, investigamos formas de incluir, nas estruturas FCA e suas extensões, as informações semânticas identicadas nos corpora analisados.
Com esse m, analisamos a adequação da estrutura Relational Concept Analysis (RCA), que é uma extensão do FCA, quanto a a representação dos papéis semânticos em conceitos formais.
Estudamos também as classes VerbNet 37.7 e 45.4, as quais foram indicadas, na pesquisa quantitativa, como as mais signicativas para os corpora analisados (Capítulo 7).
Em este estudo, investigamos a relação existente entre os papéis semânticos e as classes de verbos.
Tal estudo nos permitiu visualizar e estabelecer a importância dessas classes para nossa pesquisa.
Ainda com o propósito de incluir informações semânticas em reticulados conceituais FCA, exploramos 6 casos de estudo.
Em esses casos, propusemos formas de combinar verbos, papéis semânticos e classes de verbos em conceitos formais.
Avaliamos os casos propostos, inicialmente, apenas sob um ponto de vista estrutural, a partir de a medida de coesão lexical.
Com base nos resultados provenientes dessa investigação, escolhemos os casos de estudo mais promissores para avaliar a aplicabilidade de nossa abordagem em outros corpora e domínios.
Para investigar essa aplicabilidade, realizamos, então, uma pesquisa aplicada na área de categorização de documentos, usands corpora WikiFinance e WikiTourism.
A meta era analisar a efetiva contribuição das estruturas conceituais geradas a partir de nossa proposta na tarefa de classificação.
Os resultados dessa investigação foram avaliados a partir de medidas funcionais usualmente aplicadas em tarefas de categorização.
Incluímos ainda em nossa pesquisa, estudos com um corpus para Língua Portuguesa, o corpus PLN-BR CATEG.
Em esses estudos, abordamos as tarefas:
Extração de conceitos a partir de textos, categorização de documentos e, apesar de a falta de recursos, a inclusão de papéis semânticos em estruturas FCA geradas a partir de textos.
Consideramos como uma de nossas principais contribuições o estudo que realizamos, quanto a a inclusão de papéis semânticos nos contextos formais, a partir de os quais as estruturas FCA são geradas.
O resultado desse estudo foi uma metodologia, aplicável em diferentes corpora e domínios, que permite criar estruturas FCA enriquecidas como papéis semânticos.
Denominamos tal metodologia de Semantic FCA (SFCA) e a descrevemos, na forma de um procedimento, na Seção 10.2.
Embora a metodologia SFCA não gere, como resultado, ontologias tais como descritas na visão de Guarino, acreditamos em sua aplicabilidade.
A abordagem é capaz de gerar estruturas conceituais a partir de textos de forma automática, estruturas estas que podem ser usadas para construir ontologias e apoiar tarefas na área de sistema de informação.
As estruturas assim construídas sãntológicas e de domínio, e suas características cabem perfeitamente na definição de Gruber para ontologias, denição esta que é aceita e utilizada no âmbito dessa investigação.
Julgamos igualmente relevante a pesquisa aplicada que realizamos na área de categorização de documentos.
A partir de essa investigação, que gerou resultados muito animadores quanto a a aplicabilidade das estruturas SFCA, pudemos também propor uma metodologia que descreve como usar essas estruturas na tarefa de classificação.
Tal metodologia foi formalizada na Seçãutra contribuição que consideramos importante refere- se à avaliação estrutural (coesão lexical), que propusemos e utilizamos para analisar as estruturas FCA construídas ao longo de nossa pesquisa.
Consideramos tal avaliação relevante principalmente por existirem poucas medidas dessa natureza para analisar conceitos formais.
Este documento está organizado em 10 capítulos, seguidos das referências bibliográcas, assim constituídos:
É neste capítulo que relatamos algumas das pesquisas realizadas com textos em Língua Portuguesa.
E o Capítulo 10 apresenta as considerações sobre o estudo realizado e as metodologias resultantes de nossa investigação exploratória:
A metodologia para construção de estruturas SFCA e a metodologia para categorização de documentos baseada em SFCA.
Abordamos também nesse capítulo, os trabalhos futuros e as publicações realizadas durante o doutoramento.
Este capítulo faz uma introdução a estruturas ontológicas e à aprendizagem dessas estruturas a partir de texto.
São apresentados conceitos e classicações de estruturas ontológicas, em os quais se destaca o uso do termo &quot;ontologia «para nomear estruturas conceituais com diferentes graus de expressividade semântica.
São comentadas também as tarefas relacionadas à aprendizagem de ontologias, as abordagens mais comumente usadas para extrair conceitos e seus relacionamentos, alguns ambientes de apoio à aprendizagem a partir de textos e também formas de avaliação de ontologias.
Em losoa, a palavra de origem grega &quot;Ontologia «é usada para designar um dos ramos da Metafísica que estuda a essência do &quot;ser «ou da existência.
Em esse contexto, Ontologia se preocupa com as características das entidades reais, provendo- lhes conceitos (Definições) e identicando aspectos essenciais quanto a o tipo e à estrutura dos objetos, das propriedades, dos eventos, dos processos e das suas relações exatamente como elas existem no mundo.
As ontologias descrevem os tipos de fenômenos que podem existir, tais como pessoas, lugares, eventos, relacionamentos, etc.
Têm combjetivo, ainda, prover a catalogação das entidades, denindo classes, hierarquias e relações classes-instâncias em conformidade com a realidade.
Ainda no sentido losóco, Guarino em diferência &quot;Ontologia «(com &quot;O «maiúsculo) de &quot;ontologia «(com &quot;o «minúsculo).
Em o primeiro caso, refere- se à disciplina da Metafísica e no segundo, a um sistema de categorias que expressa uma visão de mundo, tal como a &quot;ontologia de Aristóteles».
Já em ciência da computação, a palavra &quot;ontologia «tem sido usada para expressar um modelo conceitual, recebendo, segundo Guizzardi, duas interpretações:
Uma conforme a comunidade de modelagem conceitual e outra de acordo com as comunidades de inteligência articial, engenharia de software e web semântica.
Em modelagem conceitual, ontologia aproxima- se do sentido losóco, visto que procura definir &quot;um sistema de categorias formais independente de domínio», mas fundamentado na realidade e que pode ser usado para modelar diferentes domínios.
Guarino em também dene ontologias como modelos conceituais.
Para Guarino, uma ontologia aproxima, no sentido intencional, o modelo conceitual pretendido para um determinado domínio.
É em que Guarino introduz o termo &quot;ontologia formal», conceituandntologia como &quot;uma teoria lógica que expressa o signicado atribuído a um vocabulário formal».
A interpretação de Guarino para ontologias é ilustrada na Figura 2.1.
Através de uma linguagem L, pode- se gerar diferentes modelos M (L) para representar a conceituação de um domínio.
Uma conceituação é uma abstração, uma &quot;visão de mundo «sobre um domínio.
Uma determinada conceituação C é expressa segundo um &quot;compromissntológico «K. Um &quot;compromissntológico «estabelece uma terminologia (vocábulos em L) cuja semântica é definida (ou restringida) por axiomas.
Já nas demais comunidades mencionadas -- inteligência articial, engenharia de software e web semântica -- ontologia é considerada um &quot;artefato concreto de engenharia».
Ela é uma estrutura formal composta por conceitos e relações entre conceitos.
Inclui também um conjunto de axiomas que restringem a interpretação dessa estrutura, bem como reetem o conhecimento sobre um domínio no contexto de uma aplicação.
O conhecimento representado refere- se a &quot;casos «(situações) relevantes para a aplicação.
Isso signica que o comprometimento da ontologia com a realidade (fundamentação) nem sempre é bjetivo principal, mas quando aparece é definido em função apenas das necessidades da aplicação.
Para essas comunidades, conforme Gómez-Pérez em, a definição mais citada é a de Gruber, em a qual ontologia é &quot;uma especificação explícita de uma conceituação».
Segundo Gruber, conceituação nada mais é do que uma abstração (visão simplicada) do &quot;mundo «relevante para uma determinada aplicação.
Em esse contexto, a ontologia tem combjetivo compartilhar conhecimento, expressando as entidades dessa abstração de &quot;mundo «segundo a interpretação consensual de um grupo de pessoas e não apenas de um indivíduo.
Apesar de a definição de Gruber ser a mais aceita entre os cientistas da computação, ela tem sido questionada devido a sua abrangência.
Conforme Smith e Welty em, essa definição permite interpretações em as quais ontologias podem ser basicamente qualquer estrutura conceitual, desde simples catálogos até teorias axiomatizadas.
Embora existam muitas críticas quanto a o uso do termntologia para denominar estruturas conceituais como taxonomias, por exemplo, os pesquisadores têm procurado distinguir as &quot;ontologias «quanto a o que alguns chamam de expressividade semântica, outros de precisãu, ainda, de complexidade no tange à capacidade de raciocínio.
A Figura 2.2 apresenta a classificação de ontologias segundo Smith e Welty em, que é uma das primeiras iniciativas neste sentido.
Essa classificação, que não é exaustiva, mas acreditamos abranger boa parte das estruturas conceituais, inclui:
Catálogos que denem uma lista nita de termos de um domínio (vocabulário controlado);
Arquivos-texto de um mesmo domínio;
Glossários em os quais o signicado dos termos é descrito em língua natural (como dicionários);
Tesauros contendo Definições de termos e seus relacionamentos;
Hierarquias que organizam os termos em taxonomias usando a relação is- a (é-um) ou em estruturas utilizandutras relações de ordem como em FCA;
Frames que organizam as entidades do domínio a partir de suas classes e propriedades;
E estruturas conceituais que, além de descrever conceitos e seus relacionamentos, incluem axiomas que restringem a semântica desses conceitos, permitindo a realização de inferências (raciocínio).
De acordo com Guarino em, todas as estruturas da Figura 2.2 podem ser ditas ontológicas visto que a definição de conceituação de Gruber, em, permite interpretações de caráter puramente extensional.
Gruber dene conceituação como uma estrutura D, R\&gt;, onde D é o domínio e R é o conjunto de relações relevantes em D, ou seja, Rconsidera relações que reetem casos (estados particulares de mundo).
Guarino atribui essa exibilidade, focada no signicado, que determine, além de as relações extensionais, relações intensionais.
Esse é um fator que tem sido apontado como um dos entraves aos avanços na área de interoperabilidade de sistemas pois, sem uma semântica formalmente descrita e clara (bem definida), o casamento (matching) de ontologias restringe- se ao casamento de vocábulos (strings) e seus relacionamentos.
&quot;Relacionamentos estes que nem sempre descrevem visões equivalentes de mundo», embora pertençam a um mesmo domínio.
Guarino em também propõe uma classicação para as estruturas ontológicas, no entanto considerando aspectos mais gerais (Figura 2.3).
Ele agrupa as estruturas em:
De acordo com Gruber em, uma estrutura ontológica, no contexto da ciência da computação e da informação, estabelece «um conjunto de primitivas de representação com o qual é possível modelar um domínio de conhecimentu de discurso.
Essas primitivas são tipicamente classes (ou conjuntos), atributos (ou propriedades) e relacionamentos (ou relações entre os elementos das classes)».
Segund autor ainda, as primitivas descrevem informações relativas ao signicado das entidades do domínio, incluindo restrições lógicas a partir de as quais mecanismos de raciocínio podem atuar de forma consistente numa aplicação.
Gómez-Pérez em, baseando- se no trabalho de Gruber, detalha tais primitivas de representação, organizando- as em conceitos (que são as classes mencionadas por Gruber), instâncias, relações, funções e axiomas.
Seguimos a organização de Gómez-Pérez, ao detalhar essas primitivas, à exceção de funções, as quais descrevemos, a exemplo de outros autores, como um tipo de relação.
Desta forma, o conhecimento acerca de um domínio pode ser representado numa estrutura ontológica por meio de:
Conceitos (classes) -- são as principais entidades de uma ontologia e representam um conjunto de indivíduos do domínio.
&quot;Podem ser concretos, elementares ou compostos, reais ou ctícios.
Podem também descrever tarefas, funções, ações, estratégias, processos de raciocínio, etc».
Livro Autor e são exemplos de conceitos.
Instâncias (indivíduos ou objetos) -- são elementos particulares associados a conceitos do domínio.
Por exemplo, Os Maias representa um livro especíco e Eça de Queirós, um autor.
Eles são instâncias, respectivamente, das classes Livro Autor.
Relações -- estabelecem ligações árias entre os conceitos do domínio.
São classi-n* cadas geralmente em relações taxonômicas ou não taxonômicas.
As relações taxonômicas são binárias e permitem a organização dos conceitos numa hierarquia.
Desta forma, usando comrdem a inclusão, conceitos mais abrangentes são relacionados a conceitos cada vez mais especícos, formando uma taxonomia.
As relações taxonômicas são consideradas por Jurafsky e Martin como um subtipo de hiperonímia.
As relações 1 é-um (is-a) ou de hiponímia também são usadas em taxonomias.
Já as relações não taxonômicas, também conhecidas como transversais, podem relacionar os conceitos sob diferentes aspectos, e não formam necessariamente hierarquias e nem sempre são binárias.
São exemplos de relações não taxonômicas as relações de sinonímia, de agregação 3 ou composição como é-parte-de (meronímia/ holonímia), associação como 4 5 conectado- a, etc..
Gómez-Pérez observa que algumas relações não taxonômicas são, na verdade, relações funcionais, com a relação é-mãe-de.
Segundo Wong e Priss esse é o caso também dos papéis semânticos (Capítulo 4), os quais expressam as funções atribuídas por os verbos aos conceitos presentes numa sentença.
É importante mencionar que, para alguns autores, a relação de meronímia é considerada taxonômica, pois permite descrever um conceito a partir de suas partes, viabilizando também a construção de uma hierarquia.
Cabe ressaltar ainda que algumas relações definidas por Gómez-Pérez são denominadas atributos ou propriedades por outros autores, pois relacionam de um livro.
A relação lexical de hiperonímia, também chamada de &quot;é-superior-a, «é-superclasse-de (superordinate), denota que um conceito é superclasse de outro.
Portanto, nessa relação binária, existe um item que funciona como um protótipo (uma classe) que representa a generalização de outros itens mais especícos.
Por exemplo, &quot;ave «é hiperônimo de canário e &quot;pato».
A hiponímia é inversa à hiperonímia.
Em esse caso, &quot;canário «é hipônimo de ave.
Axiomas -- são sentenças lógicas que denem a semântica dos conceitos e das suas relações.
Atuam de forma restritiva, excluindo interpretações.
Obviamente que, no contexto da ciência da computação, as primitivas precisam ser &quot;interpretáveis «por máquina e por esta razão são usadas, na representação de ontologias, linguagens descritivas baseadas geralmente em lógica de primeira ordem.
De acordo com Euzenat e Schvaiko em, ontologias normalmente são representadas em OWL (Web Ontology Language), sendo esta linguagem recomendada, inclusive, por a World Wide Web Consortium (W3C).
6 Diferentemente de outras áreas, no que se refere à interpretação de ontologias, a ciência da computação costuma ser mais pragmática quanto a a limitação das estruturas de representação usadas para caracterizar os elementos de estruturas ontológicas.
Como nem tudo pode ser representado por as linguagens usadas para descrever ontologias, &quot;o que existe no mundo da aplicação é o que pode ser representado e manipulado computacionalmente».
Esse costuma ser o ponto de vista dos pesquisadores de inteligência articial.
Mesmo com a limitação imposta por as estruturas de representação computáveis, construir ontologias com qualidade ainda exige muito esforço manual.
Uma das formas de reduzir esse esforço é construir sistemas computacionais capazes de extrair de fontes de dados, como textos, as primitivas descritas por Gómez-Pérez.
A área que se preocupa com desenvolvimento de tais sistemas é chamada de aprendizagem de ontologias, e é esta área o assunto da próxima Seção.
De acordo com Cimiano em, o termo 'aprendizagem de ontologia' (ontology learning) foi originalmente usado por Maedche e Staab em para descrever o processo de &quot;aquisição de um modelo de domínio a partir de dados».
Esse processo, que tem natureza multidisciplinar, auxilia a engenharia semiautomática e cooperativa de ontologias.
Em ele, diferentes disciplinas atuam de forma complementar, trabalhando com diferentes tipos de dados, sendo estes não estruturados, semiestruturados ou totalmente estruturados.
Já Gómez-Pérez e Manzano-Macho em denem o termo 'aprendizagem de ontologia' como a aplicação de um conjunto de métodos e técnicas para construção de ontologias, que A relação lexical de sinonímia entre dois conceitos estabelece que eles são substituíveis um pelutro em qualquer sentença sem modicar o sentido dessa sentença.
A meronímia expressa uma relação de inclusão semântica entre duas unidades lexicais, uma denotando a parte (merônimo) e outra referenciando a um todo (holônimo).
Por exemplo, &quot;dedo «é merônimo de &quot;mão».
Maedche e Staab em classicam o processo conforme a natureza dos dados, pois a estrutura dos dados geralmente acaba por definir os métodos e técnicas adequados para a extração do modelo de domínio.
Eles organizam as abordagens em:
Com foco deste trabalho é o primeiro caso -- aprendizagem de ontologia a partir de textos apenas esta abordagem será detalhada.
Aprendizagem de ontologia a partir de texto é o processo de identicar termos, conceitos, relações e opcionalmente axiomas a partir de informações textuais com bjetivo de usar tais elementos na construção e manutenção de ontologias.
A primeira etapa desse processo de aprendizagem é a escolha do corpus que será usado como fonte de dados para construção da estrutura conceitual.
A qualidade e a riqueza do corpus são fundamentais para o bom desempenho de qualquer abordagem de extração de informações a partir de textos.
O corpus pode ser composto por textos não estruturados (em formato livre), ou semântica.
Como as ontologias modelam domínios, é comum que os corpora sejam definidos especialmente para construir- las.
Por esta razão, a escolha do corpus, tradicionalmente, cabe ao usuário ou ao especialista do domínio.
Em abordagens automáticas ou semiautomáticas, o mais usual é constituir o corpus a partir de documentos web ou a partir de recursos baseados em web como a Wikipédia.
A web é considerada um importante recurso em tarefas de aquisição de conhecimento pois, além de ser um grande repositório de informações, que atende a inúmeros domínios, ela permite, em razão de a redundância, medir a relevância e a conabilidade dessas informações.
Já o uso da Wikipédia se popularizou por prover as informações de forma semiestruturada, possuir um vocabulário rico, ter capacidade de atualização e, principalmente, por ser um recurso cuja natureza é mais próxima à das ontologias do que à dos textos livres.
A construção de corpora a partir de a web é realizada com o auxílio de motores de busca.
Para isso, são submetidos aos motores de busca um glossário de termos previamente definidu apenas alguns termos (ou palavras-chave), ditos &quot;sementes «(seed-- words), que caracterizam os conceitos mais gerais da estrutura conceitual a ser construída.
Em esse último caso, geralmente, o corpus é formado paulatinamente.
Cabe ao especialista disparar o processo, informando as sementes iniciais.
A partir de essas sementes é formado um corpus com documentos web.
De esse corpus, são extraídos conceitos relacionados às sementes iniciais, os quais passam a ser as novas sementes, reiniciand processo.
Já quando a aplicação é o enriquecimento de ontologia, a seleção é feita a partir de conceitos dessa ontologia.
Em este caso, as consultas feitas aos motores de busca procuram capturar a semântica, associando sinônimos, hiperônimos, atributos e outros elementos relacionados a esses conceitos na ontologia.
Cabe ressaltar, ainda, que muitos experimentos de aprendizagem de ontologias a partir de textos são realizados com corpora jornalísticos, no entanto Jia observam que textos cientícos podem ser mais indicados pois descrevem os conceitos em diferentes níveis de abstração, sendo, assim, naturalmente hierárquicos.
Constituíd corpus, a próxima etapa é a construção da estrutura ontológica, iniciando, desta forma, o processo de aprendizagem propriamente dito.
Para extrair dos textos as primitivas de representação e estruturar- las adequadamente, é necessária a realização de várias tarefas.
Essas tarefas e as abordagens usadas para realizar- las sãs assuntos das próximas subseções.
Em a sequência, são comentados também alguns trabalhos que descrevem ambientes de aprendizagem de ontologias a partir de textos, e, ainda, as diculdades inerentes ao processo de aprendizagem.
O processo de aprendizagem de ontologia a partir de textos envolve diferentes tarefas as quais Cimiano em descreve como um &quot;bolo em camadas «(Figura 2.4).
Essas camadas, que detalhamos a seguir, iniciam com a identificação dos termos (camada base) e vão até a geração de axiomas gerais (camada topo).
Termos -- a extração de termos é a primeira tarefa do processo de construção de uma ontologia e consiste em encontrar os termos ou representações simbólicas relevantes para conceitos e relações.
Sinônimos -- a tarefa de identificação de sinônimos consiste em encontrar palavras que denotem os mesmos conceitos, ou seja, que compartilhem um signicado.
Conceitos -- a formação de conceitos deve incluir preferencialmente uma denição intensional, prover extensões dos conceitos e, ainda, representações simbólicas usadas para referenciar- los.
Hierarquia de conceitos -- a construção da hierarquia inclui indução, extensão e renamento da estrutura conceitual.
A tarefa de indução corresponde à geração da hierarquia propriamente dita, em a qual os conceitos sãrganizados segundo uma relação de ordem, formando um semireticulado superior.
A tarefa de renamento consiste em estender a hierarquia incluindo subconceitos.
E a tarefa de extensão se destina a encontrar novas representações lexicais para um mesmo conceito.
Relações e Hierarquia de Relações -- a identificação de relações, muitas vezes, restringese a relações binárias e inclui tarefas como:
Encontrar os conceitos que possuem algum tipo de relação não taxonômica;
Especicar essas relações, propondo rótulos e Definições adequadas;
E aprender uma ordem hierárquica para essas relações.
Esquemas Axiomáticos -- essa tarefa faz uso de um sistema de axiomas já existente que dene, por exemplo, axiomas de equivalência e disjunção para conceitos, e também axiomas que descrevem as propriedades das relações, como transitividade, simetria, etc..
A tarefa consiste em instanciar axiomas, ou seja, identicar quais conceitos e relações estabelecem tais axiomas.
Axiomas gerais -- essa tarefa consiste em aprender axiomas que estabelecem relacionamentos mais complexos entre conceitos e relações.
Para suportar essas tarefas, uma variedade de recursos e técnicas tem sido usada.
As abordagens usadas para esse m sã assunto da próxima subSeção.
A aplicabilidade das ontologias na solução de diferentes problemas, o volume de textos disponíveis, principalmente na web, e o grande esforço manual ainda necessário para construção de tais estruturas, têm aumentad interesse na área de aprendizado de ontologias a partir de textos.
Tanto que, na última década, diferentes abordagens com esse m foram propostas.
Em essas abordagens, é comum a combinação de técnicas linguísticas, estatísticas e de aprendizagem de máquina com recursos lexicais.
Em as próximas subseções são apresentadas as abordagens para aprendizagem de ontologia conforme a organização proposta por Gómez-Pérez e Manzano-Macho em, que as classicam em abordagens baseadas em técnicas linguísticas, estatísticas e de aprendizagem de máquina.
Cabe ressaltar que descrevemos apenas as abordagens mais usuais na bibliograa pesquisada.
As técnicas linguísticas são dependentes do idioma das sentenças do corpus, pois se baseiam em características morfológicas, sintáticas ou semânticas identicadas nos textos.
Em as abordagens estudadas, é muito comum o uso dessas técnicas no pré-processamento dos documentos.
Elas são utilizadas para:
Segmentação do texto (tokenization) -- os sinais de pontuação ajudam a identicar os limites de uma sentença.
É nessa fase que as sentenças são segmentadas em termos.
Quands conjuntos ordenados não possuem supremu infímo, são chamados de semirreticulados.
Se a estrutura possuir apenas o supremo denomina- se semirreticulado superior;
Se possuir apenas o ínmo, semirreticulado inferior.
Mais informações sobre conjuntos ordenados e reticulados podem ser encontradas no Apêndice A. As entidades nomeadas tipicamente correspondem às instâncias numa ontologia.
Análise sintática -- consiste em identicar, nas sentenças, elementos sintáticos como sujeito, objeto direto, etc..
Com esse propósito podem ser aplicados parsers, que geram a árvore sintática completa das sentenças, ou chunk parsers, que realizam análises parciais abrangendo apenas alguns elementos sintáticos considerados mais importantes conforme a tarefa em questão.
É usada para apoiar diferentes tarefas de aprendizagem de ontologias, principalmente no que se refere à identificação de relações entre os termos, tanto de ordem taxonômica quanto não taxonômica.
Topic signatures, associadas com medidas para calcular a distância semântica como Dice, Jaccard, cosseno, informação mútua e outras, também são usadas.
Stemming elimina as terminações das palavras, reduzindo- as a uma cadeia que, em alguns casos, corresponde ao seu radical.
Palavras como belo, bela e beleza, por exemplo, seriam representadas por &quot;bel».
Lematização converte os termos para sua forma canônica, ou seja, os verbos vão para o innitivo e os substantivos vão para masculino-singular, se existir.
No caso de a lematização, as palavras belo, bela e beleza seriam representadas por o lema &quot;belo».
A relação lexical de antonímia é inversa à sinonímia.
Dois conceitos são antônimos se possuem sentidos opostos.
As técnicas estatísticas são aplicadas geralmente em conjunto com as técnicas linguísticas e com algoritmos de aprendizagem de máquina.
Têm o papel comumente associado a critérios de relevância, sendo usadas frequentemente para identicar os conceitos e as relações mais expressivas entre conceitos no corpus.
Uma das medidas mais citadas na literatura é a tf-idf.
Ela é usada para identicar conceitos em vários trabalhos.
O método C- Value/ NC- Value também tem sido muito referenciado.
Segundo Spasic, ele é adequado para extrair multitermos, pois combina conhecimento linguístico e estatístico ao definir a importância dos termos.
Há ainda trabalhos com essa nalidade que utilizam qui-quadrado e informação mútua, de entre outras medidas.
Por outro lado, há pesquisadores como Navigli que propõem suas próprias medidas de relevância.
Os autores em combinam duas medidas para identicar os termos mais signicativos para um domínio:
Escore de relevância e consenso.
O escore de relevância mede a quantidade de informação capturada a partir de um corpus de domínio em relação a os corpora usados no processo de aprendizagem.
A medida de consenso ajuda a escolher, de entre os termos identicados, aqueles que aparecem com mais frequência nos documentos do domínio.
Em outros trabalhos, com de Yang e Callan em, a máquina de busca do Google tem sido usada para identicar os conceitos multitermos mais relevantes.
São considerados importantes aqueles termos que ocorrem com maior frequência (acima de um determinado limiar).
Uma abordagem que vem ganhando espaço é a híbrida.
Zhang em e Butters e Ciravegna em apresentam resultados animadores ao combinarem mais de uma medida de ponderação dos termos.
Butters e Ciravegna em propõem também o uso de limiares dinâmicos, baseados na média e no desvio padrão das medidas.
De acordo com os autores, em geral, limiaresos não levam em consideração a real distribuição dos termos.
Vale mencionar que, numa de nossas publicações, usamos tal abordagem, combinando as medidas tf-idf e C- Value para extrair conceitos de textos em Língua Portuguesa.
Cimiano em extrai relações não taxonômicas entre verbos e seus argumentos explorando medidas como probabilidade condicional e informação mútua.
Lame e Desprès também utilizam informação mútua, mas para identicar relações entre conceitos a partir de seus contextos.
Já no caso de os algoritmos de aprendizagem de máquina, as medidas são mais comuns em agrupamentos sendo usadas para estabelecer a similaridade entre conceitos a partir de seus contextos.
As técnicas de aprendizagem de máquina associadas com medidas de distância semântica, como Dice, Jaccard, cosseno, informação mútua e outras, são usadas em diferentes tarefas do processo de aprendizagem de estruturas ontológicas.
Elas são utilizadas principalmente para:
Aprendizagem de conceitos:
Vários algoritmos com diferentes paradigmas de aprendizagem são usados para esse propósito.
An e Chen em, por exemplo, propõem melhoramentos no algoritmo Naïve Bayes com bjetivo de descobrir regras no corpus que possam ser usadas para identicar conceitos.
Chin também usam o Naïve Bayes para encontrar conceitos, no entanto seu método consiste em agrupar as palavras em categorias pré-definidas conforme as suas probabilidades de distribuição.
Fortuna Em usam o algoritmo k--Means para descobrir tópicos que possam caracterizar conceitos numa estrutura ontológica.
Já em, esses autores utilizam aprendizagem ativa com base em classicadores SVM (Support Vector Machine) para identicar novos conceitos.
Enriquecimento de ontologias:
A classificação de conceitos é frequentemente relacionada à tarefa de popular ontologias.
Faz uso em geral de algoritmos com abordagem supervisionada, como k-NN, árvores de decisão, SVM e outros.
Esses algoritmos geralmente são treinados para reconhecer as classes dos conceitos a partir de padrões linguísticos, topic signatures ou relações semânticas da WordNet.
Fortuna, por exemplo, propõem classicadores SVM treinados com palavras-chave para categorizar os novos conceitos numa hierarquia ontológica.
Existem muitos trabalhos na literatura sobre a construção e enriquecimento de estruturas ontológicas a partir de textos.
No entanto, mesmo com o desenvolvimento e evolução de ferramentas de apoio, a construção de tais estruturas é ainda um processo que exige um grande esforço humano.
Em geral, as ontologias são construídas através de processos manuais ou semiautomáticos.
Esses processos, além de tediosos e trabalhosos, exigem tempo e requerem manutenção.
Embora a aquisição totalmente automática de conhecimento ainda seja um desão, vários ambientes têm surgido com propósito de minimizar o esforço humano no processo.
A seguir são comentados alguns trabalhos relacionados a esse tema, tais como:
ASIUM, Text2Onto, OntoLearn e OntoLT.
Learning Methods) é auxiliar na construção de ontologias para a língua francesa.
Utiliza agrupamento conceitual e hierárquico para gerar a taxonomia.
Os conceitos são formados por os termos que aparecem combjetos indiretos de verbos.
O agrupamento conceitual é baseado em verbos e em preposições.
A validação da ontologia é feita por o próprintologista.
A ferramenta OntoLearn é adequada para enriquecer ontologias de domínio com conceitos e relações.
Faz uso de técnicas de processamento linguístico e estatístico para ltrar os termos da língua inglesa.
A base lexical WordNet é utilizada para a interpretação semântica dos termos, especialmente multitermos.
A ferramenta usa também a WordNet e métodos de aprendizagem indutiva baseados em regras para extrair as relações entre os conceitos.
A validação é feita igualmente por o especialista do domínio.
O ambiente Text2Onto foi construído a partir de o Text-to-Onto.
Uma das principais diferenças do Text2Onto em relação a o seu antecessor é que, quando identicada uma mudança no corpus, não é necessário reprocessar- lo, o que permite ao usuário rastrear as mudanças de evolução da ontologia.
O ambiente utiliza tanto abordagens linguísticas quanto técnicas de aprendizagem de máquina, e é baseado no framework GATE 12.
O Text2Onto suporta aprendizado de ontologias a partir de textos em inglês, espanhol e alemão.
ntoLT é um plug-in para a ferramenta de desenvolvimento de ontologias Protégé, que suporta a extração e extensão interativa de ontologias a partir de textos em alemão e inglês.
A abordagem dntoLT provê um conjunto de regras de mapeamento que associam funcionalmente entidades linguísticas de coleções de textos anotados com conceitos e atributos candidatos, sob a supervisão do usuário.
Já para a língua portuguesa, tem aumentads esforços quanto a a disponibilização de ambientes para a construção de ontologias.
São exemplos desses esforços NTOLP desenvolvido por Ribeiro e o ambiente de Baségio.
Ambos são abordagens semiautomáticas baseadas em padrões linguísticos e medidas estatísticas.
Ao longo de o processo de aprendizagem de ontologia, existem diculdades e também desaos quanto a a extração de informações relevantes a partir de textos e à organização dessas informações de forma coerente a m de construir uma estrutura conceitual de qualidade.
A preferência dos pesquisadores por abordagens de natureza híbrida, combinando diferentes técnicas e recursos, se justica também por ser uma forma de minimizar os problemas inerentes ao processo de aprendizagem.
Cimiano em, por exemplo, destacam três problemas que independem do método de agrupamento conceitual a ser usado e que podem impactar na estrutura ontológica resultante da aprendizagem a partir de textos:
As relações de dependências escolhidas para formar os conceitos podem não ser corretamente identicadas nos textos, pois as ferramentas usadas para este m podem cometer erros;
Nem todas as dependências identicadas são, de fato, interessantes para distinguir os objetos que formarãs conceitos;
E as informações obtidas a partir de textos não serão completas pois, por maior que seja o corpus usado, nunca será grande o suciente para conter todas as possíveis dependências.
Os dois primeiros problemas geralmente são enfrentados combinando técnicas linguísticas, que são responsáveis por a identicação das dependências escolhidas, com técnicas estatísticas, que servem para valorizar as dependências e assim viabializar o descarte daquelas cuja importância é considerada menor.
Com peso dessas relações é comumente estimado com base em suas frequências no corpus, supõe- se normalmente que os erros ou dependências menos signicativas estão embutidos nas relações menos frequentes.
Já o terceiro problema tem sido contornado a partir de o uso de diferentes corpora, extraídos principalmente da web, e recursos linguísticos como tesauros e bases lexicais.
A WordNet (Seção 4.6) é frequentemente usada para definir os conceitos ou mesmo enriquecer- los, provendo, por exemplo, sinônimos.
É importante mencionar que, em razão de a interpretação não consensual por parte de os pesquisadores quanto a a definição de ontologia e, principalmente, aos problemas inerentes à aprendizagem de ontologias a partir de textos, coms três descritos anteriormente, a maioria dos trabalhos encontrados não atinge a totalidade das tarefas descritas por Cimiano.
Por exemplo, em muitos trabalhos, a aprendizagem de relações não taxonômicas, incluindo a definição automática de seus rótulos, é pouco abordada.
Maedche e Staab em comentam que, de entre as três subtarefas essenciais à aprendizagem de ontologias a partir de texto, que são:
Extração de conceitos, extração da taxonomias e extração de relações não taxonômicas, a última é considerada a mais difícil.
Kavalec e Svatek em mencionam que mesmo para os ontologistas essa tarefa não é trivial, visto que são possíveis várias relações entre instâncias de conceitos mais gerais.
Segundo Weichselbraun essa diculdade manual de definir e associar rótulos a relações não taxonômicas tem impactad processo de construção de ontologias e restringindo a aplicabilidade das mesmas em ambientes mais dinâmicos.
Outro aspecto pouco explorado sãs axiomas.
Aqueles que tentam realizar essa tarefa automaticamente, geram axiomas muito simples.
Trabalhos mais recentes com de Blanco e Dan têm utilizado, para este m, papéis semânticos.
Blanco e Dan exploram as primitivas semânticas nas relações estabelecidas por os papéis para caracterizar e restringir propriedades.
Observamos que, em geral, os trabalhos limitam- se às tarefas que vão até a hierarquia de conceitos.
Isso provavelmente está relacionado ao fato dessas tarefas já estarem mais consolidadas e, portanto, produzirem de forma automática, ou semiautomática com menor esforço manual, estruturas conceituais de melhor qualidade.
Percebemos também que os trabalhos que mais avançaram nessa área são geralmente voltados à Língua Inglesa.
Há resultados signicativos, também, para o alemão e o francês.
Isso é consequência, possivelmente, da riqueza em recursos linguísticos existentes para tais línguas, diferente do que acontece com a Língua Portuguesa, para a qual apenas estão iniciands esforços nesse sentido.
Apesar de os avanços da área, notamos que ainda há muito a ser feito.
Independentemente da língua, percebemos também que a avaliação das estruturas conceituais resultantes desse processo de aprendizagem é um problema em aberto.
As ontologias de referência existentes dicilmente contemplam a totalidade dos elementos desejáveis numa estrutura conceitual.
É comum a ausência, por exemplo, em tais ontologias, de relações não taxonômicas e de axiomas.
Além disso, a ausência de métodos formais e comuns de avaliação, diculta a comparação entre as diferentes abordagens propostas.
Mesmo em avaliações realizadas por especialistas humanos, as métricas usadas para avaliação por os pesquisadores nem sempre coincidem.
Dada a importância do processo de avaliação de ontologias, esse é o assunto da próxima Seção.
Apesar de a diversidade de trabalhos publicados sobre a esse tema, métodos formais e padrões de avaliação para sistemas de aprendizagem de ontologias ainda são um problema em aberto.
De acordo com Obsrt em, há muitos critérios que podem ser usados para se avaliar uma ontologia, tais como:
Critérios quanto a a cobertura do domínio (riqueza, complexidade e granularidade), critérios quanto a o seu desenvolvimento (casos tratados, cenários, requisitos, aplicações e dados de origem) e quanto a propriedades formais relativas à consistência e completude.
Essa avaliação, segunds autores, inclui aspectos de verificação e validação que podem ser estimados a partir de medidas estruturais, funcionais e de usabilidade:
Estruturais: Essas medidas analisam as ontologias enquanto grafos, geralmente de forma mais quantitativa, sem necessariamente medir aspectos semânticos ou de conteúdo.
No entanto, em conjunto com outras medidas, podem ser usadas também para qualicar ou classicar estruturas ontológicas.
Provêem informações, que podem ser calculadas automaticamente, quanto a a profundidade, amplitude, densidade, modularidade, etc..
Cimiano em, por exemplo, descreve as ontologias usadas em seus experimentos através de medidas estruturais como:
Número de nodos (conceitos), número de nodos-folha, altura média dos nodos, altura máxima dos nodos, número máximo de nodos- lhe&amp;lhes o e média de nodos- lhe&amp;lhes o presentes na ontologia.
Funcionais: Essas medidas têm foco em aspectos semânticos, procuram avaliar o modelo conceitual caracterizado por a ontologia.
Objetivam estimar a distância entre o modelo da estrutura ontológica construída e o modelo pretendido.
Essa forma de avaliação é complexa e tem diferentes abordagens.
Envolve desde avaliações manuais, realizadas por especialistas humanos no domínio da ontologia, até avaliações automatizadas e baseadas em tarefas, que medem a qualidade do modelo a partir de o desempenho na resolução de algum problema.
Há também abordagens baseadas em casamento (matching) de ontologias, em o qual o modelo construído é comparado com algum modelo de referência já existente.
Para esse m, as medidas mais usuais são precisão e recall.
De usabilidade:
Essas medidas avaliam os metadados sobre a ontologia e seus elementos.
Medem aspectos como a facilidade de acesso às instruções referentes à forma de utilizar a ontologia, controle de versões, compatibilidade, interface com o usuário, etc..
Medidas de usabilidade podem ser encontradas em.
Em as subseções seguintes, abordamos medidas de caráter estrutural e funcional.
Nos deteremos apenas nesses dois tipos, pois acreditamos que medidas referentes à usabilidade vão além de o escopo deste trabalho.
Em a Seção 2.6.1, apresentamos as métricas estruturais de avaliação utilizadas por o sistema AKTiveRank.
As métricas desse sistema são calculadas de forma relativa.
Elas utilizam, como parâmetros, termos informados por o usuário numa pesquisa.
Desta forma, o sistema é capaz de medir o quanto uma ontologia é abrangente, densa, similar semanticamente e relevante para um conjunto de termos.
Escolhemos essas métricas justamente por serem relativas, e, portanto, permitem análises estruturais mais pontuais sobre determinados elementos de uma ontologia.
Para que pudéssemos medir a aplicabilidade de nossa proposta, incluímos em nossos estudos, também, abordagens funcionais de avaliação.
Em a Seção 2.6.2, descrevemos abordagens funcionais comumente utilizadas e os diferentes aspectos (níveis) ontológicos que tais abordagens são capazes de analisar.
O propósito das métricas estruturais é prover informações quantitativas sobre uma ontologia.
As métricas propostas na literatura, geralmente, são extensões ou adaptações de medidas conhecidas e buscam atender às peculiaridades das ontologias e de suas aplicações.
Como tais medidas sãbjetivas e não requerem esforço manual, elas se tornaram um recurso interessante para a organização das ontologias em ranks.
O sistema AKTiveRank, descrito por Alani e Brewster em, é um exemplo.
Ele possui um conjunto de métricas que permite escolher, de entre várias ontologias, a mais relevante para uma determinada pesquisa num sistema de recuperação de informações.
O sistema possui quatro medidas estruturais:
Class match, densidade, similaridade semântica ebetweenness.
Como já mencionado, elas avaliam a ontologia de forma relativa, considerands termos informados na pesquisa.
De acordo com Alani e Brewster, elas podem se definidas como descrito em:
A medida class match (CMM) avalia a cobertura da ontologia para um conjunto de termos previamente informado.
São procurados na ontologia aqueles rótulos de conceitos que casam exatamente ou parcialmente com tais termos.
Considerando, um conjunto de termos T informados por o usuário e os conceitos C de uma ontologia, a medida CMMo pode ser definida como a soma dos casamentos exatos E e parciaisP existentes entre os rótulos dos conceitos no e os termos em T. A medida CMM, apresentada na Equação em seus experimentos, $= 0.6 e $= 0.4 como pesos dos casamentos exatos e parciais, respectivamente.
CMM (o, T) $= E (o, T)+ P (o, T), onde:
A medida densidade (DEM) tem como meta avaliar o nível de detalhamento dos conceitos numa ontologia.
Para isso, ela calcula a quantidade de relações não taxonômicas, subclasses, superclasses e classes-irmãs desses conceitos.
A medida considera apenas os conceitos, denotados por C, m que casam exatamente ou parcialmente com os termos em T. A Equação 2.2 descreve a medida DEM, em a qual o conjunto S 1 corresponde às relações não taxonômicas;
S 2, às superclasses;
S 3, às subclasses e S4, às classes-irmãs dos conceitos c C, C C. A medida de similaridade semântica (SSM) calcula quão próximos estão, na ontologia o, os conceitos que casam exatamente ou parcialmente com os termos em T. Segundo Alani e Brewster, num sistema de recuperação, quanto mais longe estão, uns dos outros, os conceitos relacionados aos termos pesquisados, maiores são as chances da ontologia não representar o conhecimento procurado de uma forma coerente e compacta.
Usualmente, a similaridade entre dois conceitos é calculada considerands caminhos existentes entre esses conceitos numa estrutura conceitual.
Existem várias medidas na literatura que calculam a distância semântica entre conceitos, tais como as medidas de Wu e Palmer e de Leacock e Chodorow, que podem ser usadas no cálculo da medida SSM.
A Equação 2.3 descreve a medida SSM, em a qual P corresponde ao conjunto de todos os caminhos existentes entre os conceitos c ie j, ec cj i representa um caminho p P, sendo que c, c C.
A medida betweenness (Bem) é baseada no trabalho de Freeman, que estuda métricas para determinar o nodo central num grafo.
Um nodo é considerado central quando a quantidade de caminhos mais curtos que passam por este nodo é alta.
A medida Bem tem como propósito determinar essa quantidade de caminhos mais curtos para os conceitos de uma ontologia.
Alani e Brewster acreditam que, quando a medida Bem de um conceito é a mais alta na ontologia, esse conceito corresponde ao elemento central de tal estrutura.
A medida Bem é apresentada na Equação 2.4, em a qual c, c é o comprimento do menor i j caminho entre os conceitos c c i e j, e c, c é quantidade de caminhos curtos entre i e j.
Tais medidas, uma vez definidas, são reunidas num único escore, o qual pode ser usado posteriormente para estabelecer a classificação da ontologia.
Considerand M com valor da medida CMM;
M, M, o valor de DEM;
O valor de SSM e 4 o valor de Bem para a ontologia;
E aindão w coms pesos dessas medidas e O, o conjunto das ontologias sob avaliação, o Escore da ontologia pode ser definido como apresentado na Equação 2.5.
Cabe ressaltar que, para viabilizar a organização das ontologias num rank, as medidas são normalizadas para o intervalo.
Outros estudos e propostas referentes a medidas estruturais podem ser encontrados em.
A seguir apresentamos algumas abordagens funcionais, e os diferentes aspectos (níveis) de uma ontologia que tais abordagens podem avaliar.
De acordo com Brank as abordagens de avaliação mais comuns entre os pesquisadores possuem um caráter mais funcional e se enquadram numa das quatro categorias mencionadas a seguir:
As abordagens que avaliam as estruturas ontológicas a partir de aplicações ou de juízes humanos têm vantagens sobre as demais.
Para o uso numa aplicação, não é necessário avaliar a estrutura ontológica em si.
Logo, a avaliação se torna mais objetiva, considerando- se apenas os resultados gerados por a aplicação.
Por outro lado, a avaliação também é mais especíca;
As conclusões acerca de os resultados não podem ser generalizadas para outras aplicações.
No caso de o uso de especialistas de domínio no processo avaliativo, a vantagem está nlhar human qual permite uma análise mais subjetiva, levando em conta aspectos semânticos que abordagens automáticas são incapazes de considerar.
Apesar disso, as abordagens automáticas baseadas em ontologias de referência e em corpus são as únicas factíveis quando se trata de avaliações em grande escala e de comparações entre múltiplas abordagens de aprendizagem de ontologia.
Em abordagens automáticas de avaliação é mais usual que o processo avaliativo seja realizado em níveis.
Como uma ontologia é uma estrutura de conhecimento complexa, é mais prático analisar- la sob diferentes aspectos, observando separadamente seus elementos e organização, do que examinar- la como um todo.
Cada nível de avaliação considera um determinado aspecto da estrutura conceitual.
De acordo com Brank, uma ontologia pode ser avaliada em níveis:
Avalia de forma prática a adequação da estrutura ontológia para uma aplicação em particular.
A análise é realizada a partir de os resultados gerados por a aplicação.
Sintático: Este nível de avaliação é especialmente interessante para ontologias desenvolvidas manualmente.
Analisa aspectos referentes à linguagem, tais como uso correto de palavras-chave referentes ao domínio e a presença de loops nas Definições dos conceitos da ontologia.
A Tabela 2.1 relaciona as abordagens mais usuais, anteriormente citadas, aos níveis de avaliação mencionados.
Como se pode observar, os níveis mais amplamente abordados sã léxico, o taxonômico e o não taxonômico.
Em as subseções seguintes, comentamos os quatro primeiros níveis de avaliação:
Léxico, taxonômico, não taxonômico e contextual.
Os níveis sintático e de projeto não são detalhados neste documento por serem mais indicados para abordagens manuais.
Como já mencionado, a avaliação em nível léxico consiste em analisar os rótulos usados para identicar os conceitos (vocabulário) de uma ontologia.
O vocabulário da ontologia sob análise, geralmente, é comparado a um outro vocabulário, sendo que este último é composto de rótulos de conceitos que podem ser provenientes de uma ontologia de referência, gerados estatisticamente a partir de um corpus ou preparados por especialistas do domínio.
Em abordagens para aprendizagem de ontologias, é muito comum que essa comparação seja realizada em relação a uma ontologia de referência.
É usual também a aplicação de medidas como precisão e abrangência nesse processo comparativo.
Dellschaft e Staab em chamam essas medidas de precisão léxica (PL) e abrangência léxica (AL).
Essas medidas permitem comparar o vocabulário de uma ontologia gerada automaticamente O A ao de uma ontologia de referência O R. As Equações 2.6 2.7e apresentam, respectivamente, as medidas PL e AL, onde C O C O A corresponde ao conjunto de rótulos de conceitos de A e R, aos de R. Um dos problemas dessa forma de avaliação, no entanto, é a não correspondência perfeita entre os vocabulários das ontologias que estão sendo comparadas.
O problema de &quot;casamento de cadeias de caracteres «tem sido minimizado através do uso de funções strings, que permitem vericar se um rótulo está contido em outro (é seu substring), ou por meio de medidas de distância entre cadeias de caracteres, como a de Levenshtein.
Esse é um problema comum, em avaliação léxica, quando, por exemplo, a ontologia de referência é a WordNet e a ontologia sob análise contém muitos conceitos cujos rótulos são nomes compostos.
Outro problema ocorre quands conceitos não possuem rótulos ou estes são considerados inadequados.
Este problema é mais frequente em abordagens automáticas, que utilizam algoritmos de agrupamento para identicar os conceitos, com método FCA.
Em esses casos, a abordagem mais comum é a aplicação de medidas de similaridade entre os elementos que compõem os conceitos.
Um exemplo desse tipo de similaridade é apresentado na Seção 3.7.2, que descreve a medida Sim, usada para comparar os conceitos gerados por o FCA.
A medida Sim tem como base a medida ics (information content similarity) que utiliza a WordNet e um corpus para estimar a similaridade semântica entre termos.
Cabe destacar ainda que as medidas PL e AL não levam em consideraçã aspecto polissêmico 13 dos rótulos que identicam os conceitos numa ontologia, o que obviamente pode Uma palavra é dita polissêmica quando pode assumir signicados diferentes, conforme o contexto em que prejudicar a validade e interpretação de seus resultados.
Também no caso de as relações hierárquicas é comum que a avaliação seja realizada com base em ontologias de referência.
bjetivo, neste caso, é analisar o quanto a ontologia sob avaliação está estruturalmente alinhada a uma ontologia de referência.
Dellschaft e Staab em propõem medidas para este nível de avaliação, que são aplicáveis em aprendizagem de ontologias.
As medidas propostas por os autores são extensões do trabalho de Maedche e Staab e permitem a análise das relações de forma local (analisando conceitos) e global (analisando a taxonomia por completo) quanto a a precisão e abrangência.
Segundo Dellschaft e Staab, a precisão taxonômica local analisa a similaridade entre dois con-pt ceitos de ontologias distintas.
Essa medida calcula a proporção de características extraídas de suas respectivas estruturas conceituais que são compartilhadas por os conceitos.
A Equação 2.8 dene a medida pt, onde A corresponde à ontologia construída automaticamente, R indica a ontologia de referência, carac é a operação que extrai as características de um conceito e, c e c R são conceitos das ontologias A e R, respectivamente.
Em nossa pesquisa, observamos que as operações mais usuais para extração de características taxonômicas em processos avaliativos são semantic cotopy sc e sua variação, common semantic cotopy (csc).
A operação scestabelece, para um conceito c de uma determinada ontologia, o conjunto de conceitos que se relacionam hierarquicamente com c, ou seja, seus super ou subconceitos.
Já csc, ao construir esse conjunto, considera apenas os conceitos comuns às duas ontologias sob comparação.
Por esta razão, a cscé menos dependente lexicamente do que sc, visto que desconsidera conceitos que não existem em ambas ontologias.
As Equações 2.9 e 2.10 denem, respectivamente, as operações sc e csc, ond C A é o conjunto de conceitos de O A, Cr, o de O C, O R e o de qualquer ontologia.
Decidida a operação de extração de características, que será usada para calcular a precisão local, pode- se então definir a precisão global, que permitirá obter a precisão da taxonomia de A em relação a R. A precisão global, chamada de precisão taxonômica (PT), é apresentada na Equação 2.11.
Cabe ressaltar que, quando um conceito cnão é comum às duas ontologias, a precisão local é estimada.
Em esse caso, ela é definida como a maior precisão local encontrada entre os valores precisão local calculados para todos os conceitos c em O c R em relação a.
A partir de a medida de precisão PT pode- se calcular também a abrangência At e a medida F T--score taxonômicas, as quais são apresentadas, respectivamente, nas Equações é aplicada.
A relação de polissemia é aquela em que os itens possuem a mesma forma (graa) e diferentes signicados.
A granularidade da diferença semântica pode variar.
Pode ser bem expressiva tal como &quot;verde «para indicar uma cor e &quot;verde «para indicar que algo não está maduro.
Mas também pode ter signicados relacionados, como a palavra banco quando aplicada no sentido de repositório, pode ser um banco de sangue, um banco de dados, um banco de células, etc.
O mesmo se aplica a verbos, como por exemplo:
&quot;pintar um quadro «e &quot;pintar uma parede».
Dellschaft e Staab descrevem ainda a medida F T 2.
14) (Equação que permite avaliar a qualidade da ontologia tanto em nível léxico quanto taxônomico, incluindo na equação a medida de precisão léxica (Equação 2.6).
Em, os autores usaram $= 1 para definir as Outra medida comentada é a Taxonomic Overlap (Te o) que mede a sobreposição média de relações hierárquicas de A em relação a R. A equação para o cálculo de Te o, apresentada em por tosc.
A medida to 2.16) sc..
Alguns autores usam a WordNet comntologia de referência.
No entanto, tal abordagem para avaliação de ontologias de domínio é criticada, pois as relações de hiponímia e hiperonímia da WordNet podem não reetir por completo as relações hierárquicas de conceitos em domínios especícos.
Uma alternativa tem sido validar as relações hierárquicas a partir de as categorias da Wikipédia.
O trabalho de Yu é um exemplo.
Em esse trabalho, os autores comparam a estrutura de navegação proporcionada por as relações entre os conceitos da ontologia sob análise com a navegação permitida por as categorias da Wikipédia.
É importante destacar que essas foram algumas das poucas medidas que encontramos, em nível taxonômico, aplicadas também a estruturas conceituais geradas a partir de o FCA, enquanto método de agrupamento conceitual (como em).
Esse aspecto foi determinante para a inclusão dessas medidas em nosso estudo.
Brank[ 25] argumentam que as medidas como precisão e abrangência usadas em nível taxonômico podem ser adaptadas para analisar relações não taxonômicas.
No entanto, quando a abordagem de avaliação escolhida é por meio de uma ontologia de referência, a diculdade principal não reside nas métricas, mas na ontologia que será usada para esse m..
Segundo Sánchez e Moreno são raras as ontologias de referência que contêm relações não taxonômicas.
A ausência dessas relações é decorrente, principalmente, da diculdade dntologista em denir- las dad volume e as variações de relacionamentos possíveis entre conceitos.
Isso prejudica a avaliação, pois a ausência de uma relação não signica, necessariamente, que ela é incorreta, e ainda penaliza índices como a precisão.
Por esta razão, têm surgido novas propostas incluindo medidas intrínsecas de similaridade comumente baseadas na WordNet e em conteúdo de informação para avaliar tais relações.
No entanto, os próprios autores consideram ainda suas medidas incipientes.
A diculdade relativa ao uso da WordNet, nessa tarefa de avaliação, está no fato desse recurso conter relações mais gerais e não de domínio.
Desta forma, o mais usual é que os resultados seja analisados por especialistas humanos.
Em esse nível de avaliação, o mais comum é que a ontologia seja analisada a partir de o seu desempenho no apoio à realização de uma tarefa.
Em esse caso, a avaliação é extrínseca, são utilizadas métricas de avaliação relativas à tarefa e não se analisa diretamente a ontologia enquanto estrutura de representação de conhecimento.
Segundo Brank, a avaliação nesse nível é mais objetiva quando comparada às dos níveis já comentados, mas tem algumas desvantagens.
A avaliação em nível contextual permite se observar o quanto uma estrutura ontológica é boa ou ruim para uma tarefa especíca, no entanto torna difícil a generalização dessa informação no que se refere a outras tarefas.
Além disso, se a ontologia for dependente de um componente da aplicação, a análise dos resultados poderá ser prejudicada.
Pode ser difícil medir separadamente o desempenho do componente e a contribuição, propriamente dita, da ontologia.
E ainda, no caso de várias ontologias, pode ser difícil também decidir qual a melhor para uma aplicação, se esta aplicação for exível o suciente para desempenhar de forma satisfatória com qualquer uma das ontologias sob avaliação.
Obviamente que, por o fato das ontologias serem estruturas de representação, suas principais aplicações estão na área de sistemas de informação.
Elas têm sido usadas, principalmente, para apoiar tarefas de classificação, agrupamento e recuperação de informações.
E é, portanto, para esse tipo de tarefa que as avaliações de contexto têm se direcionado.
Em nossa pesquisa, encontramos poucos trabalhos cujo tema central seja o desenvolvimento de metodologias para avaliação de ontologias a partir de tarefas.
O trabalho de Netzer em é um dos poucos que têm essa proposta.
Os autores apresentam um método para avaliar estruturas ontológicas por meio de a tarefa de classificação de textos em aplicações de recuperação de informações.
Os autores analisam, por exemplo, a cobertura léxica da ontologia com relação a os termos utilizados em consultas e a qualidade de classificação de textos proporcionada por o uso da ontologia no que tange à recuperação desses textos.
A partir de o estudo realizado sobre estruturas ontológicas e os métodos usados para extrair- las de textos, percebemos a falta de consenso entre os pesquisadores sob diferentes aspectos, que vão desde a definição do que é uma estrutura ontológica aos métodos e técnicas usados para construir- las.
Em o que tange à definição, adotaremos a de Gruber em que é amplamente aceita, sendo referenciada por muitos trabalhos da área.
No entanto, ao invés de o termo &quot;ontologia «usaremos tant termo &quot;estrutura ontológica «como &quot;estrutura conceitual «para nos referirmos a esse tipo de entidade, respeitando, portanto, a abrangência da definição de Gruber e, também, indo ao encontro de a visão de autores como Breitman quanto a a classificação de ontologias (Seção 2.2).
Quanto a a metodologia, percebe- se algumas linhas gerais de procedimento, como as tarefas enumeradas por Cimiano e apresentadas na Seção 2.5.1 para aprendizagem de estruturas ontológicas a partir de textos.
Mas, de uma forma geral, os trabalhos estudados utilizam métodos diversos para realizar essas tarefas, sendo que nem todas as etapas que estudamos são de fato implementadas.
Muitas ontologias são apenas taxonomias e não incluem axiomas e nem relações transversais.
Por outro lado, há um caminho de pesquisa recorrente e, por isso, talvez consensual, que se refere ao uso de abordagens híbridas e ao uso de recursos web.
Muitos trabalhos adotam técnicas línguísticas, estatísticas e de aprendizagem de máquina para realizar as tarefas.
Assim como se apóiam fortemente na WordNet quanto a a identicação de conceitos e relações semânticas.
Em trabalhos mais recentes, observa- se ainda o uso papéis semânticos tanto para identicar e rotular relações não taxonômicas quanto para definir axiomas.
Além disso, já é uma prática o uso de corpora construídos a partir de documentos web e de textos da Wikipédia.
As abordagens são, em sua ampla maioria, semiautomáticas, o que é justicável dada a complexidade envolvida na construção de estruturas ontológicas a partir de textos.
Há uma certa preferência no se refere às avaliações intrínsecas dessas estruturas.
As avaliações dessa natureza, que são realizadas automaticamente, em geral têm um caráter mais estrutural.
Assim, quand objetivo é analisar a qualidade semântica da ontologia enquanto estrutura de representação de conhecimento, a avaliação acaba sendo manual.
Isso se deve à lacuna que existe ainda quanto a medidas de avaliação de cunho semântico que possam ser aplicadas automaticamente e forneçam informações relevantes e qualitativas sobre a estrutura.
Alternativamente, alguns pesquisadores têm adotado avaliações extrínsecas, de caráter funcional, comparando as estruturas geradas com ontologias de referência ou comparando- as com base no desempenho de aplicações.
No entanto, quando a avaliação tem combjetivo qualicar as relações não taxonômicas, a ausência de tais relações em ontologias de referência e de métodos e métricas formais para esse tipo de avaliação têm dicultado tanto a aplicação de abordagens funcionais quanto a comparação dos resultados de pesquisas nessa área.
Já quanto a o aspecto tecnológico, não há ferramentas que suportem tod processo de aprendizagem e de avaliação de tais estruturas.
Faltam também benchmarks para a área de aprendizagem de ontologias a partir de textos, para que seja possível, por exemplo, comparar metodologias e assim estabelecer quais métodos e técnicas são mais indicados para cada tarefa, conforme a natureza do domínio do corpus.
É evidente, também, que os maiores avanços nessa área estão voltados para abordagens relativas à Língua Inglesa.
Acreditamos que isso esteja diretamente ligado à riqueza de recursos disponíveis para processar- la.
No caso de a Língua Portuguesa, o avanço não é tão intenso, possivelmente porque muitos recursos ainda estão em desenvolvimento, como é o caso da WordNet.
Br Observamos, por m, que há ainda muito espaço de investigação na área de aprendizagem de ontologias a partir de texto, cabendo, sem dúvida, novos estudos e propostas, incluindo métodos e abordagens diversas.
Sendo este o caso, inclusive, do método Formal Concept Analysis que, embora não seja novo e nem seja de aplicação recente na área, tem despertad interesse dos pesquisadores e, consequentemente, vem se destacando como uma alternativa de método de agrupamento conceitual na construção de estruturas ontológicas.
Como esse método é parte do nosso estudo, ele é abordado no próximo capítulo.
Este capítulo faz uma introdução ao método Formal Concept Analysis (FCA), apresentando sua definição e embasamento matemático.
Descrevemos também uma extensão do FCA:
O método Relational Concept Analysis (RCA) Segundo Priss, o RCA é indicado para a representação de relações não taxonômicas.
Posto que nossa pesquisa tem interesse em papéis semânticos os quais estabelecem relações dessa mesma natureza, decidimos investigar tal extensão.
Em este capítulo são comentados ainda, algoritmos para gerar estruturas FCA;
1 métodos que procuram reduzir a complexidade computacional desses algoritmos;
E medidas de similaridade apropriadas para tais estruturas.
Além disso, apresentamos as vantagens e desvantagens do FCA enquanto método de agrupamento conceitual, bem como algumas de suas aplicações na área de aprendizagem de ontologias a partir de textos.
Embora muitas formas de estruturas conceituais sejam tratadas, hoje, de ontologias como foi comentado na Seção 2.2, cabe ressaltar que, em nossos exemplos iniciais, o foco restringese mais ao método do que a sua aplicação.
É importante esclarecer que nem sempre o que chamamos de &quot;conceito formal «corresponde a um &quot;conceitntológico «no sentido mais usual em ciência da computação, ou seja, a uma classe.
Em alguns exemplos, o &quot;conceito formal «é composto tanto de classes quanto de instâncias.
Apesar de os métodos geralmente organizarem as classes nos nodos superiores e as instâncias nos nodos mais inferiores, entendemos que a distinção entre o que é classe e o que é uma instância seja um passo anterior ao uso desses métodos formais.
Portanto, essa distinção entre classe e instância é feita apenas nos exemplos de aplicações dos referidos métodos para construção de estruturas ontológicas a partir de textos, visto que para esse m tal distinção é necessária.
Formal Concept Analysis (FCA) foi introduzido por o matemático alemão Rudolf Wille na década de 80 como um método para análise de dados.
A estrutura organizacional proposta por o método, que é baseada na teoria dos reticulados, permite a visualização, a investigação e a interpretação dos dados e de suas estruturas, implicações e dependências inerentes.
O método tem sido aplicado em diferentes áreas, tais como psicologia, sociologia, antropologia, medicina, biologia, linguística, ciência da computação, matemática e engenharia industrial.
Em a área de ciência da informação é usado para análise de dados, representação de conhecimento e gestão de informação.
Em a ciência da computação, o método tem sido utilizado para a construção automática de estruturas conceituais, já que a organização que ele provê para os dados pode ser vista como uma técnica de agrupamento conceitual.
O FCA estrutura os dados em unidades, chamadas conceitos formais, e as organiza na forma de um reticulado de conceitos.
Os conceitos formais e os reticulados de conceitos são abstrações matemáticas do que, em losoa, chamamos de conceitos e de hierarquia de conceitos, respectivamente.
De o ponto de vista losóco, segundo Wille em, os conceitos pode ser entendidos como unidades básicas do pensamento humano, cuja formação decorre de processos dinâmicos em ambientes sociais e culturais.
Os conceitos, ainda nesse sentido, são caracterizados por suas extensões e intensões.
As extensões compreendem todos os objetos que pertencem a esses conceitos.
Já as intensões expressam os atributos que descrevem as propriedades e os signicados de todos os objetos contidos nas extensões dos conceitos.
Wille comenta também que os Ao longo de o texto, usamos o termo &quot;estrutura FCA «para nos referir ao reticulado de conceitos resultante da aplicação do método FCA.
Formalmente, essa definição losóca de conceito é mapeada na dualidade conhecida como conexão Galois, que estabelece relações implícitas entre dois conjuntos parcialmente ordenados, no casbjetos e atributos, de forma que os objetos possam ser descritos através de seus atributos e os atributos, por os objetos que caracterizam.
As Definições matemáticas do método são apresentadas na próxima Seção.
O modelo matemático que permite descrever as extensões e intensões dos conceitos formais, inicialmente introduz a noção de contexto formal.
Contextos formais são caracterizados por a tripla (G, M, I), onde:
G é o conjunto formado por as entidades do domínio, ditas objetos formais;
M é constituído por as características dessas entidades, seus atributos formais;
E I é uma relação binária sobre G × M, chamada relação de incidência, que associa um objeto formal ao seu atributo.
Desta forma, a relação gIm pode ser lida como:
&quot;bjeto g tem o atributo m».
Em um texto, por exemplo, a partir de dependências sintáticas entre os verbos e seus argumentos, podemos definir contextos formais.
A Tabela 3.1 apresenta um subconjunto de relações, entre verbos e seus objetos (diretos e indiretos), extraídas da Seção Esportes do corpus PLN-BR cruzamento dos elementos tabulados (as relações entre G M), e tal como a dbjetoponto com os atributosperder marcar.
E Em esse exemplo, os verbos são interpretados como propriedades dos objetos.
Desta forma, bjeto ponto é algo que pode ser perdidoou marcado.
Para definir os conceitos formais a partir de um contexto formal, são necessários operadores de derivação.
Assim, para dois conjuntos arbitrários de objetos e atributos, denotados, respectivamente, por O eA, os operadores O e A podem ser definidos como:
Considerando que O determina todos os atributos em M compartilhados por os objetos em e determina todos os objetos em G que compartilham os atributos em A, um conceito formal em (G, M, I) é definido por o par (O, A) se e somente se O G, A M, tal que O $= A e A $= O.
Os operadores O eA, portanto, expressam a conexão Galois, formando conceitos como pares do tipo (extensão, intensão).
De a Tabela 3.1 pode- se, então, extrair os conceitos formais:
Para que os conceitos formais possam ser organizados hierarquicamente, é necessário estabelecer a relação subconceito-superconceito.
Matematicamente, esta relação de ordem pode ser definida como:
O conjunto de todos os conceitos formais de um contexto (G, M, I) juntamente com a relação de ordem formam um reticulado completo, chamado de reticulado de conceitos de (G, M, I) e denotado por B (G, M, I).
Isso signica que, para todo conjunto de conceitos, há um único maior subconceito (o ínmo) e um único menor superconceito (o supremo).
A Figura 3.2 ilustra duas representações, por diagrama de linhas, para o reticulado de conceitos formado a partir de a Tabela 3.1.
O diagrama da Figura 3.2b é a representação mais usual e é resultante da técnica de &quot;etiquetagem reduzida «(reduced labeling).
Essa técnica é muito útil para reticulados com um grande número de conceitos, pois facilita a visualização da estrutura, omitindo rótulos de objetos e atributos.
Por meio de esta técnica, em caminhos ascendentes de nodos (conceitos), se um rótulo de objeto pertencer a todos esses nodos, apenas o nodo mais inferior desse caminho exibirá tal rótulo, cando implícita a sua presença nos seus ascendentes.
No caso de atributos, é o inverso.
O rótulo aparecerá apenas num nodo superior, tornando- se sua presença implícita nos seus descendentes.
O contexto formal que originou o reticulado de conceitos apresentado na Figura 3.2 é univalorado.
No entanto, em aplicações reais, os contextos costumam ser multivalorados.
Tais contextos exigem um pré-processamento antes da geração do reticulado e, por isso, sã assunto da próxima Seção.
Este diagrama foi gerado com a ferramenta Concept Explorer 1.3.
Mais detalhes sobre a ferramenta podem ser encontrados na Seção 5.3.5.
&quot;etiquetagem reduzida «Em aplicações reais, o mais comum é descrever os dados por meio de tabelas, em as quais os dados são relacionados por atributos de diferentes tipos (numéricos, ordinais, categóricos, etc).
Essas tabelas são definidas formalmente por contextos multivalorados (G, M, W, I), onde:
G é o conjunto de objetos formais;
M é o conjunto de atributos formais;
W é o conjunto de valores de atributos formais e (que deve satisfazer a condição:
&quot;se (g, m, w) I (g, m, v) I, e então w $= v».
A relação (g, m, w) é lida como:
&quot;bjeto g tem um atributo m com o valor w».
Essa relação pode ser interpretada como uma função parcial de G para W, podendo, portanto, ser reescrita como m (g) $= w.
Para que esses dados possam ser organizados num reticulado de conceitos é necessário transformar esse contexto multivalorado num contexto univalorado (ou binário) equivalente.
Essa transformação é feita mediante o uso de uma escala conceitual e, por esta, razão Ganter e Wille denominam esse processo de conceptual scaling.
Em esse process mais usual é a aplicação de plain scales, principalmente por serem mais simples.
O processo consiste em definir uma plain scale Sm $= (Gm, Mm, Im) para cadam M de um contexto multivalorado (G, M, W, I), onde m (G) G m..
Cada escala S m organiza os objetos de uma forma particular e é determinada de acordo com o tipo e as características do atributo.
As escalas, em geral, requerem subdivisões, as quais devem ser signicativas e selecionadas de forma criteriosa.
Por esta razão geralmente são determinadas manualmente.
Para representar o atributrdinal &quot;data de fundação», por exemplo, pode- se optar por diferentes escalas.
Utilizamos, em nosso exemplo, a escala S, G é definida por a Tabela 3.3.
Já para o atributo categórico &quot;estado «usamos a escala S $ ({ &quot;RS», &quot;SP», RJ, estado ter usado uma escala mais compacta, considerando, por exemplo, apenas as regiões dos estados.
A partir de o contexto formal derivado (Tabela 3.4) pode- se, então, construir o reticulado de conceitos para os clubes de nosso exemplo (Figura 3.3).
Mesmo exigindo um pouco mais de processamento para construir o reticulado de conceitos, quands contextos são multivalorados, o método FCA mostra- se um recurso interessante para a construção de estruturas conceituais.
No entanto, Hacene, em, armam que o método FCA não é suciente para expressar relações não taxonômicas, que são comuns em estruturas ontológicas.
Segundo Priss em, relações semânticas, como papéis semânticos (também foco de nosso estudo) são interpretadas como relações funcionais que não formam ordenações hierárquicas.
Portanto, não podem ser representadas por o método FCA.
Para expressar esses tipos de relações nos reticulados de conceitos, seria necessário usar uma extensão do método, conhecida como Relational Concept Analysis.
Cabe ressaltar que, o fato do método FCA não ser capaz de gerar estruturas em as quais a representação de relações não taxonômicas é explícita, não signica que tais relações não possam fazer parte dos atributos de contextos formais.
Rudolf Wille em, por exemplo, gera estruturas FCA a partir de contextos formais em que as relações de incidência (objeto, atributo) são do tipo (instância, papel_ semântico) (instância, classe).
Relational Concept Analysis (RCA), proposto por Uta Priss em sua tese de doutorado, é uma extensão do método FCA.
Por meio de o RCA pode- se incluir, na estrutura conceitual, outras relações entre objetos e atributos que não são hierárquicas.
Formalmente, os dados de um RCA sãrganizados numa estrutura chamada família de contextos relacionais (FCR).
Uma FCR, denotada por o par (K, R), compreende um conjunto K de contextos formais da forma K $= (G, M, I) R i i i e um conjunto de relações binárias i e j, as quais estabelecem ligações entre os objetos ou atributos dos contextos.
As relações entre objetos ou entre atributos são transformadas em relações entre conceitos.
Segundo Priss em, ao estender as relações aos conceitos é importante analisar a questão da quantificação, ou seja, vericar se as relações valem para todos os objetos e atributos presentes nas extensões e intensões dos conceitos, ou para apenas um subconjunto de eles.
Por esta razão, Priss dene formalmente a relação entre conceitos incluindo quanticadores.
Desta forma, para um contexto formal (G, M, I), uma relação entre objetos r G × G é transformada numa relação R entre conceitos c, c B (G, M, I), c $= (O, A), 1 2 onde 1 1 1 c $= (O, A) Q i 1 i 4, 2 2 2 e representa os quanticadores, para conforme as seguintes Definições:
Dependendo, portanto, dos quanticadores usados em cada relação, diferentes relacionamentos R são estabelecidos entre os conceitos.
Priss chama r de &quot;componente relacional «e ou de &quot;etiqueta quanticacional «de uma relação.
Para compor as etiquetas quanticacionais, a autora usa, em, quanticadores em linguagem natural, tais como &quot;todos», &quot;aomenos1 «&quot;exatamente1».
E Dene, também, abreviações para essas etiquetas, suprimindo, principalmente, quanticadores universais.
Por exemplo, as rer r 2 4 lações R e R podem ser reprer r sentadas, respectivamente, por R R 0 e.
Para simplicar ainda mais a notação, alguns quanticadores são reescritos em notação matemática.
O quanticador &quot;aomenos1 «pode ser representado como &quot;1».
A autora ressalta ainda que, se não houver ambiguidades, tant componente relacional quanto a etiqueta quanticacional podem ser omitidos na notação da relação.
Cabe destacar também que as relações entre atributos r M × M i j são tratadas de forma análoga Para exemplicar as Definições apresentadas, vamos representar a relação transversal «érival-de, que relaciona um clube ao seu principal concorrente.
Em esse caso, a FCR é composta:
Quanto a as etiquetas quanticacionais, para a relação é-rival -- de, Q e Q poderiam corresponder, respectivamente, aos quanticadores &quot;todos «&quot;1».
E Desta forma, a relação entre é-rival-de conceitos c R c M, J), (1), 1 2, para c, c B (G, m 1 2 M m definida em estabelece que, para todo clube que pertence à extensão de um conceito c 1,h á pelo menos um clube na extensão de c 2 que é seu rival.
Como a relação é-rival-de é simétrica, no caso de a definição em, os quanticadores Q e Q poderiam também corresponder, respectivamente, aos quanticadores &quot;todos «&quot;1», ou seja, para todo clube que pertence à extensão de c2, há pelo menos um clube rival em c1.
Já a equivalência em dene apenas a conjunção das duas anteriores.
é-rival-de A Figura 3.4b ilustra a relação 3 R cuja anotação é-rival-de abreviada é R. Após a formalização da relação, Bendaoud, em, dão continuidade ao processo de construção da estrutura conceitual, denindo uma &quot;escala relacional».
Essa escala tem por 3 é-rival-de Cabe ressaltar que nesta Figura a relação transversal R foi editada.
A ferramenta Concept Explorer não é apropriada para representar tal relacionamento.
Desta forma, para o contexto formal K $= (G, M, J) é-rival-de mM m e para a relação definimos a escala relacional S (G, M, J), é-rival-de comnde:
A partir de essa escala relacional podemos reunir as propriedades locais (Tabela 3.4) e relacionais dos clubes, gerand contexto formal derivado que é apresentado na Tabela 3.5.
No entanto, o uso dessa escala é apenas um passo dentro de o processo de análise e construção do reticulado de conceitos.
Esse processo é iterativo, visto que o uso de uma escala relacional modica o contexto formal e o reticulado correspondente, exigindo a aplicação de uma nova escala relacional.
Isso se torna necessário, pois novos conceitos são identicados e, portanto, novas relações r:
Cdevem ser inseridas no contexto formal, provocando, assim, atualizações nas intensões dos conceitos já existentes.
O processo iterativo cessa quanto um pontoo é encontrado, ou seja, a escala relacional aplicada gera um contexto formal cujo reticulado correspondente não se modica.
Esses renamentos podem ser percebidos analisando- se o reticulado de conceitos da Figura 4 inicialmente 8 conceitos, no entanto com a inclusão dos atributos relacionais, surgiu um nono Em nosso exemplo, usamos uma FCR simples com apenas um contexto formal e uma relação.
No entanto, as FCR podem conter vários contextos e relações.
Em esses casos, o resultad reticulado de conceitos apresentado nesta gura foi gerado por a ferramenta ERCA.
Mais detalhes sobre essa ferramenta podem ser encontrados na Seção 5.3.5.
Cabe ressaltar que foi necessário mudar a representação dos atributos de data de fundação, substituinds operadores relacionais por texto, para que o pacote gráco gerados automaticamente por a ferramenta ERCA e, por uma questão de uniformidade, foram usados também nas demais guras do texto.
Mesmo com FCR mais simples, como a apresentada no exemplo, pode- se perceber que o método produz muitos conceitos.
Em as FRR esse problema tende a aumentar.
Isso pode se tornar um inconveniente para a estrutura conceitual pretendida, pois tal estrutura pode conter conceitos redundantes ou mesmo ser ilegível (não interpretável) para o usuário, devido a o seu tamanho.
Além disso, tende a aumentar também o custo computacional para a geração do reticulado de conceitos.
Para minimizar esses problemas, novos algoritmos para geração das estruturas conceituais têm sido propostos.
Além disso, é comum o uso de técnicas que permitam diminuir a complexidade de representação dos reticulados, selecionandu agrupando de alguma forma os seus conceitos.
Em a próxima Seção comentamos alguns algoritmos para geração de reticulados de conceitos e na seguinte, algumas técnicas usadas para reduzir a complexidade desses reticulados.
Os algoritmos para FCA convivem com dois problemas:
A formação do conjunto de conceitos formais e a geração do reticulado de conceitos.
Como ambos os problemas demandam processamento com crescimento exponencial, pesquisadores têm estudado a complexidade desses algoritmos, bem como proposto diferentes abordagens para a construção de reticulados de conceitos.
De acordo com Fu e Nguifo em, os algoritmos para construção de reticulados podem ser divididos em algoritmos incrementais e não incrementais.
A ideia dos algoritmos incrementais é, a cada novo conceito identicado, atualizar a estrutura.
Para isso, os algoritmos fazem, geralmente, uma interSeção entre o novo conceito e os conceitos já existentes na estrutura, a m de determinar onde e como tal conceito deve ser inserido.
O algoritmo de Norris foi um dos primeiros algoritmos incrementais para construção de reticulados.
Esse algoritmo constrói o reticulado nível a nível.
O primeiro nível da estrutura L 1 contém somente o primeiro conceito 1 1 identicado.
Adicionando- se um objet ao nível k, constrói- se o nível k+ 1 da seguinte forma:
Em caso contrário, L é um maximal, esses conceitos devem ser atualizados com as características do conceito k+ 1.
Depois de examinar todos os conceitos gerados, se O k+ 1 é um maximal, deve- se adicionar também a L (O, O).
Godin é outro exemplo de algoritmo incremental.
Ele usa uma função hash, cuja cardinalidade é definida por as intensões.
Essa função mapeia os conceitos numa espécie de repositórios, reduzindo assim a pesquisa de conceitos já inseridos na estrutura que tenham algum tipo de similaridade com o novo conceito identicado.
Em a literatura têm surgido nos últimos anos novos algoritmos incrementais, tais como AddItem em e também versões paralelas que combinam algoritmos incrementais e não incrementais como PSTCL em.
Os algoritmos não incrementais (ou batch) geram novamente a estrutura a cada novo conceito inserido e podem gerar conceitos basicamente de três formas:
O algoritmo de Chein é um dos primeiros algoritmos para construção de reticulados.
Ele usa a estratégia ascendente bottom-up.
Os pares objeto-atributo inicialmente identicados i são considerados como a primeira camada 1 do reticulado.
Para cada par de elementos i e j da camada k, o algoritmo procede iterativamente da seguinte forma com bjetivo de construir a camada L k+ 1:
Se a interSeção das intensões não existe ainda na próxima camada A A L/ i j k+ 1, entã algoritmo gera um conceito(, A A) i j cuja extensão é a união dos objetos e a intensão, a interSeção dos atributos.
A definição de maximal é apresentada no Apêndice A. Em caso contrário A A L i j k+ 1, o algoritmo faz uma junção (merge) com os conceitos em L A A k+ 1 que possuem a mesma intensão i j.
A o nal de cada iteração, são removidos os conceitos dek L kcujos atributos são idênticos aos de elementos da camada L k+ 1.
Bordat e Ganter também são algoritmos não incrementais que surgiram na década de 80.
Bordat é um algoritmo top-down que consiste basicamente em encontrar os objetos máximos do conjunto de conceitos, ou seja, os seus limites superiores ou o supremo, se ele existir.
A partir desses objetos máximos, o algoritmo procura todos os seus subconjuntos.
Os subconjuntos se tornam, então, os novos objetos máximos e o processo se repete, construindo desta forma a hierarquia de conceitos.
Já o algoritmo de Ganter, chamado também de NextClosure, possui uma abordagem enumerativa.
É o mais conhecido, principalmente por ser considerad mais eciente de entre os algoritmos mencionados.
O algoritmrganiza os conceitos conforme seus atributos, estabelecendo uma ordem lexicográca que acelera a construção da estrutura conceitual.
Em os últimos anos, têm surgindo extensões desses algoritmos, como ScalingNextClosure e novos algoritmos, como IETreeCS.
Têm sido pesquisadas também técnicas de ltragem de conceitos para reduzir a complexidade dos reticulados de conceitos.
Essas técnicas sã assunto da próxima Seção.
Conforme Priss e Old, em, a geração de reticulados de conceitos grandes e complexos é um desão para o método FCA.
A quantidade de conceitos e de relacionamentos acabam dicultando tanto a exibição quanto a navegação por os usuários nessas estruturas conceituais.
Para contornar esse problema, é comum o uso de técnicas de ltragem, denominadas data weeding.
Essas técnicas reduzem a complexidade das estruturas conceituais, considerando necessidades especícas e o propósito da aplicação.
Priss e Old organizam essas técnicas em 4 grupos:
Técnicas de redução visual;
Técnicas utilizam faceting plaine scales;
As técnicas de redução visual não modicam o reticulado de conceitos gerado por o método FCA, apenas alteram a sua forma de exibição.
Logo, não há perda de informação.
A maioria das técnicas desse grupo permitem a omissão de objetos e de atributos.
O critério que determina a omissão desses elementos varia conforme a técnica em uso.
Em a técnica de clarication, descrita por Ganter em, por exemplo, os objetos que têm exatamente os mesmos atributos podem ser substituídos por um únicbjeto.
O mesmo pode ser feito com os atributos que possuem a mesma extensão.
Já na técnica de redução, também descrita por Ganter em, sãmitidos os atributos que são equivalentes a combinações de outros atributos.
Um atributo m M de um contexto em que a técnica de clarication foi aplicada é redutível, se há outros atributos m, m M, diferentes dem, tais que a interSeção de suas extensões resulta na extensão de m m $= m Caso contrário, ele é dito irredutível.
A técnica também pode ser usada para reduzir os objetos do contexto.
Estão incluídas também neste grupo de redução visual as técnicas de zoom e aquelas que permitem deslocar gracamente, de alguma forma, tanto a estrutura quanto seus nodos.
Faceting e plaine scale permitem subdividir o reticulado de conceitos em reticulados menores.
Segundo Priss, em, o termo faceting é aplicado na área de ciência da informação para designar o processo de estruturação de conceitos de forma hierárquica.
Por exemplo, um livro pode ser classicado como &quot;Ficção/ ReinoUnido/ Século19», usando as características (facets):
Tópico, local e tempo.
A autora trata os termos faceting plaine scale e como equivalentes quando usadas para FCA, visto que as estruturas conceituais resultantes da aplicação de tais técnicas são muito similares.
Como Priss considera o termos equivalentes, nos deteremos apenas em plaine scale.
As plaine scales foram apresentados na Seção 3.3 quando descrevemos contextos formais multivalorados.
Como elas não promovem a perda de informação, desde que a divisão dos dados seja criteriosa, também podem ser usadas para reduzir a complexidade das estruturas.
Assim, para cada subdivisão pretendida, dene- se uma escala correspondente.
Cada escala permitirá construir apenas uma parte do reticuladriginal.
Para exemplicar, subdividimos o reticulado para clubes de futebol apresentado na Figura dataDeFundação (Figura 3.7 a) e estado (Figura 3.7 b), ambas definidas na Seção 3.3.
A Figura 3.8 ilustra a representação do reticulado de conceitos para clubes após a aplicação dessas escalas.
A seleção de conceitos promovida por a poda é efetuada a partir de algum índice conhecido.
Todos os conceitos cujos índices não atingirem um valor limite pré-definido por o usuário são descartados.
Um dos índices de poda mais citados baseia- se na noção de estabilidade[ 57, 112, A estabilidade de um conceito pode ser medida tanto de forma intensional quanto extensional.
O índice de estabilidade intensional i, definido na Equação 3.1, estabelece o quanto a intensão de um conceito depende de determinados objetos da extensão.
A idéia é determinar quantos objetos são necessários e sucientes para criar um conceito.
O índice de estabilidade intensional e, definido na Equação 3.2, dualmente estabelece o quanto a extensão de um conceito depende de seus atributos.
Para exemplicar, calculamos para os conceitos do reticulado da Figura 3.9b os índices de estabilidade i e.
Para isso, tivemos que reverter a etiquetagem reduzida aplicada ao reticulado.
A Figura 3.9a apresenta a tabela com valores calculados para os índices de estabilidade, conforme o algoritmo apresentado no Anexo A. Se o limite definido por o usuário fosse 0, 7, por exemplo, apenas os conceitos c 3 e 4s eriam selecionados como estáveis.
Há autores, como Falk em, que eliminam também os conjuntos que possuem menos de dois objetos como extensãu menos de dois atributos como intensão.
Em esse caso, apenas o conceito c 3 seria selecionado.
Diferente das técnicas de poda, as técnicas de restrição não exigem a construção completa do reticulado de conceitos para serem aplicadas.
Elas são usadas durante a construção dos chamados &quot;reticulados de vizinhança «(neighbourhood lattices).
Um reticulado de vizinhança é um subconjunto do reticuladriginal.
Seu processo de construção consiste basicamente em escolher um objeto (ou atributo) e buscar itens relacionados a ele.
Um operador, chamado &quot;operador mais», é usado para repetir esse processo um número determinado de vezes.
Assim, a cada aplicação do &quot;operador mais», novos itens relacionados aos já existentes são recuperados A técnica de restrição é usada para modicar o &quot;operador mais», estabelecendo a quantidade (dois, três, etc) de atributos (ou objetos) que os novos objetos (ou atributos) recuperados devem ter em comum com os já existentes no reticulado, para serem selecionados.
As técnicas de decomposição particionam o reticulado de conceitos em reticulados menores.
A técnica de decomposição horizontal, por exemplo, consiste em excluir os elementos\&gt; e do reticulado de conceitos, gerando, assim, subreticulados disjuntos Pn 1 2 n, onde e para todo i $= 1 i.
Para que essa técnica possa ser aplicada, a soma horizontal desses subreticulados, definida em 3.3, deve resultar no reticuladriginal.
Já as general scales transformam os dados do reticulado em &quot;dados mais genéricos».
Utilizam, para isso, algum &quot;operador de composição «que mapeie o conjuntriginal de termos em termos mais abrangentes.
Priss e Old comentam em que o uso de tais escalas não é uma prática muito comum.
Por isso não lhes será dado maior destaque aqui.
Embora não tenham sido citadas por os autores, as medidas de similaridade aplicadas a termos ou conceitos também podem colaborar para reduzir a complexidade do reticulado.
Em aplicações de construção de estruturas conceituais a partir de textos, tais medidas são mais utilizadas.
No entanto, não propriamente para reduzir a complexidade do reticulado, mas para minimizar o problema da esparsidade das informações obtidas a partir de textos (esse problema foi comentado na Seção 2.5.2).
Como essas informações não são completas, visto que os textos não possuem todas as dependências possíveis entre os termos, as medidas permitem a união de termos considerados semelhantes, constituindo assim conceitos mais densos e reticulados mais compactos.
A próxima Seção apresenta algumas medidas de similaridade usadas em aplicações de construção de estruturas conceituais a partir de textos.
A esparsidade da informação para aplicações que utilizam textos como fonte de extração dessas informações não é uma novidade.
Manning e Schütze em comentam esse problema e propõem uma forma de atenuar- lo por meio de um método ao qual chamaram smoothing.
O método smoothing consiste, essencialmente, em atribuir probabilidades diferentes de zero para eventos nãbservados nos textos.
Cimiano em baseiam- se nesse método para definir a &quot;similaridade mútua».
Dois termos t 1 e 2 são considerados mutuamente similares se, conforme uma medida de similaridade previamente estabelecida, t t 1 for o termo mais similar a 2 e, reciprocamente, 2 for o termo mais similar a t1.
Os autores usaram medidas bem conhecidas, como cosseno e Jaccard, para definir a similaridade entre os objetos (nomes) a partir de seus atributos (verbos).
Os objetos considerados mutuamente similares foram agrupados e tiveram seus atributos compartilhados.
Desta forma, relações entre objetos e atributos que não estavam presentes nos textos passaram a existir no contexto formal da estrutura FCA.
Os autores comentam ainda que, embora o cosseno tenha produzido índices de similaridade mais adequados, a técnica &quot;similaridade mútua «de maneira geral não produziu bons resultados.
De acordo com Formica em, o coeciente Dice tem sido usado na literatura para comparar conjuntos de atributos e, desta forma, estabelecer a similaridade entre os objetos.
Esse é o caso do trabalho de Otero em que usam o coeciente Dice para comparar atributos formados por contextos lexicossintáticos.
Embora a similaridade entre conceitos ontológicos seja amplamente estudada, Formica também arma que há poucos trabalhos que pesquisem a similaridade entre os conceitos formais de um FCA.
Alqadash e Bhatnagar em ressaltam que, enquants conceitos em ontologias são expressos como &quot;rótulos «de dados, os conceitos formais não têm rótulos.
Eles contêm simplesmente conjuntos de objetos (as extensões dos conceitos) e atributos (os descritores desses conceitos).
Esta diferença inuência nos resultados e, por isso, justica a necessidade de medidas de similaridade especícas para FCA.
Em as duas próximas subseções apresentamos duas medidas de similaridade recentemente desenvolvidas para conceitos formais.
Alqadash e Bhatnagar em propõem um índice de similaridade para conceitos formais denominado zeros-induced.
Diferente de medidas mais usuais, este índice considera a informação compartilhada por os conceitos.
Para os conceitos formais obtidos a partir de o contexto formal B (G, M, I), o índice zeros-induced é calculado como definido na Equação 3.4.
A função z, 1 2 que aparece na definição, corresponde ao número de zeros inclusos (atributos ausentes) na submatriz de B (G, M, I) induzida a partir de as linhas &quot;G G «e colunas &quot;M M».
Em a submatriz gerada para calcular z, apresentada na Figura 3.12 a, há 4 atributos ausentes, logo S $= 16/20 $= 0,8.
Z, Já na submatriz 1 3 mostrada na Figura Portanto, o índice zerosinduced determinou que os conceitos C C são mais similares.
Se estivéssemos usando uma medida que utilizasse apenas a cardinalidade dos atributos em comum, C seria considerado semelhante a C C 2 e a 3 na mesma proporção.
Alqadash e Bhatnagar em compararam o índice a três outras medidas, de entre elas a medida Jaccard, para diferentes bases de dados.
Em todos os experimentos em que os dados eram esparsos, o índice zeros-induced obteve melhores resultados.
A desvantagem do índice, no entanto, é que seu custo computacional é quadrátic (n) enquanto que o das demais medidas é linearO (n).
Formica em propõe uma medida para comparar conceitos formais cuja base é a medida de similaridade ics (information content similarity).
A medida ics foi introduzida por Resnik em e renada por Lin em.
De acordo com essa medida, quanto mais informações os conceitos compartilharem, mais similares eles serão.
A medida ics, cuja definição é apresentada em 3.5, combina abordagens tradicionais baseadas na distância entre os conceitos numa taxonomia tal como a WordNet (contribuição de Resnik) com a probabilidade de tais conceitos aparecerem num corpus (contribuição de Lin).
A função lso c 1 2 determina na taxonomia o &quot;menor «ancestral que os conceitos 1 e 2 têm em comum.
Já p (n) tem como propósito determinar a probabilidade de um conceito n aparecer num corpus.
Essa probabilidade corresponde ao quociente da frequência absoluta de n por o número total M de instâncias de conceitos que existem no corpus.
A partir de a medida ics Formica, em, propõe a medida Sim, definida em 3.6.
Ela calcula a similaridade entre os conceitos formais de um mesmo contextu de diferentes contextos formais.
Em, g e m correspondem às maiores cardinalidades dos conjuntos &quot;G G», M M», respectivamente.
O valor do peso w é definido por o usuário e, segundo Formica, foi introduzido para exibilizar o método quanto a a importância de objetos e atributos no cálculo da similaridade.
A função s utiliza a medida ics para calcular a similaridade máxima dos atributos.
Para determinar o valor de s, é necessário inicialmente gerar um conjunto P, que é um subconjunto de M × M P 1 2.
O conjunto deve conter os pares de atributos mais similares conforme a medida ics, sendo que não devem ser incluídos nesse conjunto pares que compartilhem atributos.
Assim, para cada atributo em M M 1deve- se encontrar o mais similar em 2.
Para exemplicar, considere que bjetivo é calcular a medida Sim para os mesmos pares de conceitos apresentados na Seção anterior.
Vamos considerar, neste exemplo, o peso w como 0, 5, valorizando assim, tants objetos quants atributos.
Para calcular Sim, C $= 1 2 onde 1 2 começamos denind conjunto P de pares, tal que a soma da medida ics dos seus elementos a similaridade máxima de 1, 79, que é o valor de s.
Assim, a similaridade entre os conceitos C C é definida como Como podemos observar, a medida Sim também considerou os conceitos C C como mais similares e apresenta um custo computacional igualmente quadrático.
No entanto, segundo Formica em, a medida Sim apresenta vantagens.
Ela pode produzir melhores resultados que o coeciente Dice principalmente por usar a medida ics que, de acordo com o mesmo autor, provê avaliações de similaridade próximas ao julgamento humano.
Dado que a complexidade do reticulado de conceitos é um problema, e que medidas para reduzir sua complexidade e calcular a similaridade dos conceitos são necessárias, comentamos na Seção seguinte vantagens e desvantages de se utilizar o FCA como método de agrupamento conceitual.
O cálculo da medida icsfoi realizado com o pacote NLTK (Seção 5.3.2), usando- se a WordNet 3.0 como taxonomia e o Brown como corpus de referência.
Para mais informações, consulte o Apêndice B. De acordo com Cimiano em, a escolha de um método de agrupamento para construção de taxonomias a partir de textos deve- se basear nos seguintes aspectos:
Ecácia (qualidade do resultado), eciência (comportamento em tempo de execução) e rastreabilidade da construção da estrutura conceitual por o engenheiro.
Em a visão de Cimiano, o método FCA atende a todos esses aspectos.
A principal vantagem do método FCA em comparação a métodos de agrupamento baseados em similaridade é que, além de gerar os grupos de conceitos, ele também provê uma descrição intensional para esses grupos.
Conforme Cimiano, essa descrição intensional facilita a rastreabilidade do processo de construção da estrutura ontológica e, de acordo com Zhang, tal descrição torna os grupos gerados melhor interpretáveis.
Já a sua principal desvantagem é a complexidade do reticulado de conceitos gerado por o método.
Em o pior caso, o desempenho do método FCA é exponencial O (2), enquanto que os métodos baseados em similaridade são geralmente quadráticos O (n).
Em a prática, no entanto, tal complexidade não é atingida para aplicações que buscam informações a partir de textos, em razão de o problema da esparsidade das informações.
Cimiano em, por exemplo, avaliaram o método FCA, em seus experimentos para construção de estruturas conceituais a partir de textos, com desempenho próximo a o linear.
Zhang em contornaram o problema da complexidade usando heurísticas para selecionar os conceitos mais relevantes.
Valtchev e Missaoui em, por outro lado, acreditam que o FCA e os métodos de agrupamentos baseados em similaridade são complementares.
Eles compararam os métodos a partir de 6 critérios:
O tipo de similaridade usada para definir os conceitos, a singularidade da saída gerada, a forma de agrupamento (particionamentu de sobreposição), a especialização entre conceitos, o uso de pesos nos atributos e a capacidade de manipular diferentes tipos de dados.
Diferente Cimiano, Valtchev e Missaoui consideram a noção de similaridade melhor definida em métodos de agrupamentos baseados em similaridade, por usarem medidas mais precisas.
Cimiano usam justamente esse argumento em favor de o método FCA.
Segundo Cimiano, os atributos são capazes de justicar de forma mais compreensível a formação dos grupos (conceitos) do que mera medidas numéricas.
De acordo com Valtchev e Missaoui, o método FCA tem uma uniformidade no seu processo, ou seja:
A sua forma de agrupamento, que dene a generalização/ especialização dos conceitos, é baseada na inclusão dos objetos (relação subconceito-superconceito definida na Seção 3.2).
Desta forma, o método produz, para os mesmos dados, sempre uma saída singular.
Já nos demais métodos de agrupamento, esses aspectos, especialmente a saída, dependerão do algoritmo e da configuração usada para esse algoritmo (parâmetros).
Valtchev e Missaoui criticam o método FCA ainda por não permitir a atribuição de pesos aos atributos que formam os conceitos.
Outra crítica refere- se à diculdade de se trabalhar com dados numéricos.
Para representar- los na estrutura conceitual é necessária primeiramente a definição de escalas conceituais (Seção 3.3).
Apesar de as desvantagens do método, muitos autores consideram que suas vantagens ainda são maiores e justicam o uso do FCA e de suas extensões em aplicações voltadas para a construção de estruturas conceituais a partir de textos.
Algumas dessas aplicações são apresentadas na próxima seção.
Cimiano em propõem uma abordagem baseada no método FCA para construir ontologias a partir de textos dos domínios Turismo e Finanças.
Para cada domínio, os autores extraem de textos, em inglês e em alemão, relações de dependência entre os verbos e seus argumentos.
Para encontrar essas relações de dependência, os autores marcam os textos com o lematizador e etiquetador de POS TreeTagger e o parser LoPar.
As relações identicadas são lematizadas e ltradas.
Diferentes medidas de informação, em, são testadas por os autores para selecionar as dependências mais relevantes, sendo que a probabilidade condicional é mencionada como a medida que obteve os melhores resultados.
Após esse processo de seleção, os autores aplicam a técnica de &quot;similaridade mútua «(apresentada na Seção 3.7) para determinar os termos mais similares e agrupar- los.
As relações resultantes são então transformadas num contexto formal a partir de o qual o reticulado de conceitos é gerado.
O reticulado ainda passa por uma etapa de poda, em a qual são removidos os nodos internos cuja extensão é a mesma de um nó lhe&amp;lhes o.
bjetivo da poda é remover conceitos mais abstratos, deixando assim a estrutura mais compacta.
O resultado da poda é uma ordem parcial que se aproxima muito de uma hieraquia de conceitos.
As hierarquias geradas para os domínios Turismo e Finanças são comparadas a ontologias construídas manualmente por meio de as medida Taxonomic Ovelap e semantic cotopy comens, tadas na Seção 2.6.2.2.
Os índices produzidos por a avaliação foram baixos.
Para o domínio Turismo, a precisão foi 29,33% e o recall, 65,49%.
E para Finanças, a precisão e orecallforam, respectivamente, 29,93% e 37,05%.
Otero em também usam o FCA para gerar estruturas conceituais a partir de textos.
Os autores realizam seus experimentos com corpora em português e em inglês, os quais, logo de início, são marcados, respectivamente, por os etiquetadores TreeTagger e Freeling com 8 informações de POS.
De os textos etiquetados, são extraídos nomes (substantivos) e os contextos lexicossintáticos desses nomes.
Para aplicar o método FCA, os autores denem duas operações:
Especificação e abstração.
As operações são aplicadas a pares de objetos e atributos, representados por (O, A), onde os objetos sãs nomes e os atributos, os contextos lexicossintáticos.
Em a operação de especificação, são gerados pares mais restritos.
Em essa operação, os pares (O, A) e (O, A), cujos contextos lexicossintáticos são similares, produzem o par(, A A).
Já na operação de abstração, são gerados pares mais gerais, da forma(, A A).
A estrutura conceitual resultante é analisada por avaliadores humanos, segundo critérios estabelecidos por os próprios autores.
A acurácia com que os nomes foram associados aos contextos lexicossintáticos cou em torno de 92%.
Jia em utilizam o FCA para construir ontologias usando 900 artigos cientícos sobre armazenamento e recuperação de informações da biblioteca digital da ACM.
Diferente de Cimiano, eles extraem dados de categorização que aparecem em artigos ACM, formando pares do tipo (palavras-chave, classificação) para definir os contextos formais e, conseqüentemente, caracterizar os conceitos.
Os autores descrevem também como mapear o reticulado para ontologias em RDF.
E ainda apresentam uma aplicação em que a estrutura é usada para a expansão de consultas.
Segunds autores, o método permitiu enriquecer as consultas com palavras-chave mais relevantes.
Bendaoud em utilizam um tesauro (taxonomia NCBI -- National Center for Biotechnology Information), uma base de dados (sobre bactérias), um 9 corpus (1.244 artigos da PubMed) para gerar ontologias na área de microbiologia a partir de os métodos FCA e RCA.
Inicialmente, os autores constróem reticulados conceituais FCA de domínio.
Para construir o reticulado sobre bactérias, por exemplo, os autores extraem do tesauro as classes dessas bactérias, e da base de dados e do corpus, pares do tipo (objeto, atributo), onde os objetos correspondem às bactérias e os atributos, as suas propriedades.
Em seguida, eles geram dois reticulados conceituais:
Um relacionando as bactérias às suas classes e outro relacionando as bactérias às suas propriedades.
Por m, os reticulados são unidos, formando um só.
O mesmo processo é feito para gerar o reticulado conceitual sobre antibióticos.
O papel do RCA é relacionar os reticulados de domínio, identicando, por exemplo, relações de resistência de certas bactérias a alguns antibióticos.
A avaliação deste esforço, descrita num trabalho posterior, segue a metodologia de Cimiano e coautores em e, portanto, faz uso de uma ontologia de referência.
Embora os resultados sejam ainda preliminares, os autores perceberam que a precisão do método FCA foi maior que a do RCA.
Em o trabalho de Hacene em, o método RCA é usado no processo de construção de uma ontologia para o domínio de Astronomia.
Inicialmente, os autores usam o Stanford parser11 para marcar 830 abstracts de artigos do jornal A&amp;A obtidos na base de dados SIMBAD consideradas apenas as relações em que os nomes que aparecem em tais argumentos estão definidos num tesauro de Astronomia.
Os autores utilizam ferramenta GATE para renar as relações extraídas e escolher as mais relevantes considerando as suas frequências.
Para determinar a similaridade entre os nomes compostos encontrados no texto, os autores usam uma adaptação da medida de similaridade definida por Wu e Palmer em.
Os valores de similaridade dos termos são então avaliados por um especialista no domínio.
A partir de as relações resultantes desse processo, Hacene constróem reticulados de conceitos com o método FCA.
Eles formam dois contextos formais.
Um para os corpos celestiais identicados, relacionands nomes desses corpos celestiais (objetos formais) aos verbos que representam as suas propriedades (atributos formais).
E outro contexto para os telescópios, relacionands nomes desses telescópios às suas características (perigeu, períodrbital e massa).
A seguir, os autores criam uma família de contextos relacionais, que estabelecem relacionamentos transversais entre os corpos celestiais e os telescópios que podem observar- los, segundo a tecnologia utilizada por esses telescópios.
A partir de a família de contextos relacionais é gerado entã RCA.
Os autores denem ainda um conjunto de regras de tradução para representar os elementos do RCA em lógica de descrição e, portanto, gerar uma ontologia.
A ontologia resultante foi avaliada de forma empírica por avaliadores humanos peritos em Astronomia.
Segundos os avaliadores, a maioria das classes encontradas correspondem de fato a categorias de corpos celestiais conhecidas, no entants relacionamentos transversais entre os telescópios e corpos celestiais nem sempre foram julgados signicativos.
Hacene consideram os resultados ainda preliminares e atribuem a má qualidade de certos relacionamentos à ausência de elementos, visto que, em seus experimentos, foram usados apenas 10 diferentes tipos de telescópios e 60 diferentes corpos celestiais.
Analisando- se os trabalhos pesquisados, podemos perceber que os autores costumam utilizar informações lexicossintáticas para gerar os contextos formais.
Geralmente, são extraídas dos textos as relações entre os verbos e seus argumentos, e nem sempre a ambiguidade dos termos é tratada.
Cimiano em, por exemplo, calcula a frequência dos pares (nome, verbo) sem considerar que nomes e verbos podem ser polissêmicos.
O trabalho de Otero é um dos poucos que explicitam a preocupação com a polissemia usando contextos lexicossintáticos ao invés de palavras.
O processo de seleção dos padrões mais relevantes é baseado na frequência desses padrões no texto.
No entanto, não há uniformidade quanto a o uso dessa frequência para medir a importância de cada padrão.
Não há consenso também quanto a os valores mínimos que esses padrões devem atingir, para serem selecionados.
Embora os autores citem o coeciente Dice como a medida mais usual para determinar a similaridade entre os conceitos, entre os trabalhos estudados percebemos que medidas de similaridade que utilizam a WordNet também são utilizadas.
O uso de ontologias de referência no processo de avaliação é recorrente, provavelmente por permitir a comparação entre diferentes métodos de construção de estruturas conceituais de forma automática.
No entanto, é comum o uso de juízes humanos na análise qualitativa dos conceitos.
Isso se deve, principalmente, à diculdade de estabelecer comparações entre os elementos extraídos de um corpus e os conceitos existentes numa ontologia de referência.
Até porque, como já foi mencionado, os conceitos formais são constituídos de extensões e intensões, e os conceitos ontológicos, como vistos na ciência da computação, são &quot;rótulos «que generalizam um conjunto de dados.
Embora os resultados de alguns trabalhos com RCA ainda sejam preliminares, aparentemente o método FCA como método de agrupamento para gerar estruturas conceituais a partir de textos tem provido resultados mais satisfatórios que o RCA.
O uso do método FCA na construção de estruturas ontológicas e em aplicações na área de sistemas de informação tem crescido nos últimos anos.
Encontramos trabalhos recentes em Mas é importante destacar que conferências internacionais dedicadas ao FCA existem desde a década de 90.
As principais conferências relacionadas o método são International Conference on Conceptual Structures (ICCS), Concept Lattices and Their Applications (CLA) e International Em ciência da computação, o interesse por FCA e suas extensões se justica visto que é um método adequado para a análise de dados e tem se mostrado promissor para a representação de conhecimento.
Sua inerente característica de relacionar e organizar os dados de forma hierárquica o torna um método de agrupamento conceitual muito interessante para construção de estruturas ontológicas.
Isscorre especialmente por prover descrições intensionais dos dados que facilitam a interpretação dos agrupamentos gerados e por facilitar a identificação de relações não taxonômicas.
Sua potencialidade está em permitir diferentes representações conceituais que reetem as diversas formas coms dados aparecem relacionados nos textos.
Sendo indicado, portanto, para análises linguísticas, pois gera estruturas que possibilitam o estudo de relacionamentos sintáticos e semânticos, inclusive para desambiguação de sentido.
Por outro lado, é importante o uso de boas heurísticas para seleção dos pares (objeto, atributo), pois, como a estrutura gerada é na verdade um grafo, as possibilidades de combinação dos elementos cresce exponencialmente, à medida que o volume de pares aumenta.
Embora grafos representem melhor o complexo relacionamento das entidades no mundo, visto que não trabalhamos apenas com relações de ordem puramente taxonômica, a complexidade dos algoritmos é um fator a ser considerado.
Um outro aspecto pouco mencionado, mas importante quanto a a representação, é que o método não trata a negação.
Podendo ser, assim, inadequado para aplicações em que essa situação deve ser representada.
Embora os resultados obtidos por os pesquisadores com o método sejam interessantes e motivadores, ainda não é de nosso conhecimento a existência de trabalhos, na atualidade, que usem o método FCA para geração de estruturas conceituais a partir de textos e que incluam aspectos semânticos, mais especicamente classes de verbos e papéis semânticos, nos seus contextos formais.
Para que pudéssemos construir a nossa proposta combinand método FCA com tais aspectos, zemos um estudo sobre papéis semânticos o qual é apresentado no próximo capítulo.
Este capítulo introduz papéis semânticos, apresentando seu conceito e os tipos de etiquetas semânticas geralmente usadas por as ferramentas de anotação.
Comenta trabalhos relacionados à organização dos verbos em classes semânticas e também faz uma breve apresentação dos principais recursos lexicais envolvidos em tarefas de anotação, assim como de alguns etiquetadores automáticos de papéis semânticos.
São apresentadas ainda, algumas aplicações de papéis semânticos na construção de estruturas conceituais a partir de textos.
Papéis semânticos, também chamados papéis temáticos ou ainda de papéis- 1 roles), (tipicamente expressam a relação semântica entre um predicado e seus argumentos.
Tais papéis foram introduzidos na década de 60 através de trabalhos com de Fillmore, sob a justicativa que relações sintáticas eram insucientes para representar as relações de dependência existentes entre os participantes de um evento descrito por um verbo.
Os papéis semânticos permitem caracterizar esses participantes quanto a as suas ações e estados nos eventos.
Essas relações de dependência nas estruturas predicado-argumento podem ser facilmente percebidas analisando- se as sentenças abaixo:
Semanticamente, tanto na sentença quanto na o sintagma the window (a janela) é quem sofre a ação definida por o verbo to break (quebrar), desempenhando, desta forma, o papel de Patient (paciente) do verbo to break.
O efeito do evento to break provocará uma mudança de estado na janela, que presumidamente ao nal desse evento cará danicada.
Além disso, diferente do que acontece na sentença, na entidade que efetuou a ação está claramente definida, ou seja, John é o Agent (agente) de to break.
Cabe ressaltar que, embora e estejam em voz ativa, the window possui funções sintáticas distintas nessas sentenças:
Em ele é bjeto direto de to break e em, o seu sujeito.
O verbo to break apresenta também um comportamento sintático diferenciado nas sentenças:
Em está funcionando como transitivo e em como intransitivo.
Já no caso de as sentenças e sintaticamente não há diferença, John e key (chave) são sujeitos, no entanto em John é o agente do verbo tpen (abrir) e em key é o seu instrumento (Instrument foi o papel associado abjeto utilizado na ação descrita por o verbo).
Como podemos observar, a análise sintática não é o bastante para explicitar o signicado das sentenças.
Embora alguns autores usem o termo papel-para indicar relações de caráter puramente sintático, neste documento seguiremos autores como Jackendo que trata os termos papel-, papéis temáticos e papéis semânticos como sinônimos.
Papéis semânticos podem ser atribuídos a argumentos de predicados nominais, no entanto nos deteremos apenas em predicados verbais por serem os mais mencionados por os pesquisadores e, principalmente, por o fato dos etiquetadores automáticos, em geral, anotarem apenas os argumentos de verbos.
Os papéis Agent, Patient e Instrument apresentados nas sentenças fazem parte de um conjunto de papéis para os quais existe uma certa concordância entre os linguístas quanto a as suas características.
No entanto não há uma lista consensual de papéis semânticos.
Há linguístas, inclusive, que questionam se de fato essa lista poderia existir.
As listas propostas geralmente se referem a papéis para situações especícas ou a um conjunto pequeno de papéis mais gerais.
Normalmente, quands papéis possuem rótulos mais especícos, portanto mais incomuns e menos consensuais, os autores costumam chamar- los simplesmente de papéis semânticos.
Já no caso de os papéis mais gerais e mais conhecidos, o termo mais usado é mesmo papéis temáticos.
Alguns dos papéis temáticos mais comuns na literatura, são abordados a seguir (as sentenças apresentadas como exemplos foram extraídas da página web do pesquisador Sowa).
3 Agent (agente):
É associado frequentemente ao sujeito da sentença;
Corresponde a uma entidade, tipicamente humana ou pelo menos animada, que provoca uma açãu evento de forma voluntária.
Alguns autores também atribuem o papel de agente a objetos (máquinas, por exemplo) e a forças da natureza (chuva, Patient (paciente):
Entidade diretamente afetada por uma ação, mudand seu estado.
Pode ser animadu inanimado.
Sintaticamente costuma ocorrer combjeto dos verbos.
Exemplo: Swallowed.
Instrument (instrumento):
Objeto geralmente inanimado que participa de forma secundária da ação, sendo também uma causa do evento descrito por o verbo.
Pode ser usado por o agente.
Tipicamente não muda o seu estado mas pode provocar mudança de estado de outras entidades após o evento.
É frequentemente introduzido Theme (tema):
Refere- se à entidade que muda de posição (local).
Em geral, aparece associado abjeto do verbo to give (dar) e aos sujeitos dos verbos to walk (caminhar) e to die (morrer).
Alguns autores, no entanto, usam esse papel para indicar uma entidade que funciona como uma espécie de gatilho a partir de o qual uma outra entidade passa a experimentar um estado.
Em este caso, pode aparecer relacionado a verbos que expressam sensações e sentimentos.
Jurafsky e Martin descrevem esse papel com participante mais afetado por um evento.
Exemplo: Billy likes.
Source (fonte):
É o elemento a partir de o qual a ação se inicia.
Jurafsky e Martin atribuem a mesma semântica a esse papel, o qual denem como a origem de um movimento.
Exemplo: The chapter begins.
Destination (meta)/ Recipient (recipiente):
Destination é posto de Source, referese ao elemento para o qual o movimento se dirige.
Aparece geralmente como sujeito dos verbos to receive (receber) e to buy (comprar).
É usado também para indicar uma mudança de posse (propriedade).
Alguns autores, entretanto, costumam utilizar o papel de Recipient para este m, ou seja, o papel é usado em entidades concretas ou abstratas que são alvos de uma transferência.
Exemplos: Bob went[ to Danbury Destinationto Bob Recipient].
Experiencer (experienciador):
Pessoa afetada por um estadu evento (sensorial, cognitivu emocional);
É entendido como um tipo de paciente.
Vários verbos de caráter a esse papel.
Exemplo: Sees the sh.
Beneciary (beneciário):
Usado para indicar a entidade que se benecia com uma ação, geralmente introduzido por a preposição for (para).
Alguns autores não fazem distinção entre os papéis beneciário e recipiente (ou meta).
Exemplo: Diamonds were.
Location (local):
Corresponde à entidade (lugar) em que o evento acontece;
É geralmente introduzido por sintagmas preposicionais.
Exemplo: Vehicles arrive[ at a station Location].
Mesmo no caso de papéis mais gerais, coms temáticos, para os quais se percebe um certo consenso, há algumas divergências quanto a a quantidade e à caracterização.
Dowty tenta minimizar essas divergências propondo, então, apenas dois papéis semânticos, aos quais chamou de proto-agente proto-paciente.
Proto-agente e O tem propriedades que abrangem vários dos papéis citados e se caracteriza por:
Participar de forma voluntária no evento, causar um eventu mudança de estado em outros elementos participantes do evento, mover- se, existir independentemente do evento e outras.
Já o proto-paciente tem a propriedade de:
Suportar mudanças de estado, ser afetado por outro elemento participante do evento, não existir independentemente do evento, etc..
Em abordagens mais atuais esse impasse quanto a a definição dos papéis tem sido contornada, de uma certa forma, por a atribuição de rótulos numéricos aos argumentos dos verbos.
Esse é o caso do PropBank, um corpus anotado manualmente que tem sido muito utilizado principalmente para treinar etiquetadores automáticos de papéis semânticos.
O problema é que a anotação provida por o corpus e, consequentemente, por muitos etiquetadores semânticos não é uniforme quands verbos são de classes diferentes.
Apesar de existir uma determinada regularidade para os rótulos Arg0 Arg1, e que costumam corresponder, respectivamente, aos papéis agente e paciente, o mesmo não se pode armar para os demais rótulos.
Em a anotação PropBank, o rótulo Arg2, por exemplo, para os verbos to kick (com sentido de chutar) e to slice (com sentido de fatiar) corresponde, respectivamente, aos papéis instrumento e fonte.
Como a classe dos verbos é importante para esse tipo de anotação, esta é o assunto da próxima Seção.
O comportamento homogêneo dos verbos quanto a a exão e função sintática permite estabelecer, sob o ponto de vista morfossintático, classicações bem definidas para os verbos.
No entanto, o caráter polissêmico e a variedade de construções em que os verbos podem aparecer nas sentenças dicultam generalizações semânticas e, consequentemente, classicações desta ordem.
Um dos trabalhos mais mencionados na literatura quanto a a classificação semântica dos verbos é o de Beth Levin.
A autora correlaciona os verbos semanticamente a partir de seus comportamentos sintáticos.
Apesar de a relação &quot;comportamento sintático «e &quot;signicado «não ser perfeita, existem regularidades sucientes para a formação de classes.
Verbos que permitem as mesmas alternativas de construção de sentenças em nível sintático (transitivo, ser agrupados.
As relações entre os verbos de uma classe, no entanto, não são necessariamente de sinonímia.
Algumas classes, como Break to break (-- quebrar, to chip- lascar, to crack-rachar, tutras, como Braid to braid (-- trançar, to brush -- escovar, to clip- prender, to comb -- pentear, to difusa, já que não estabelece claramente o tipo de relação semântica entre os verbos.
Por outro lado, os verbos de uma mesma classe possuem estruturas de argumentos similares com uma certa coerência semântica.
Por exemplo, verbos que denotam eventos, como to cut (cortar), to kill (matar) e to destroy (destruir), são geralmente transitivos e possuem uma entidade agente, funcionando como sujeito, que atua e causa a mudança numa entidade paciente, que é um objeto direto.
Essas e outras regularidades encontradas por Levin têm servido como base para o desenvolvimento de ferramentas automáticas e semiautomáticas para classificação de verbos, etiquetagem de papéis semânticos, desambiguação de sentido dos verbos, etc..
E também como ponto de partida para criação de recursos lexicais, como PropBank e VerbNet Os recursos frequentemente mencionados na literatura para anotação de papéis semânticos incluem o corpus Proposition Bank (PropBank) e os léxicos FrameNet e WordNet.
O léxico VerbNet também é usado com esse m em alguns trabalhos.
Por esta razão, esses recursos léxicos são brevemente comentados nas seções a seguir.
O Proposition Bank, ou simplesmente PropBank, é um corpus anotado com papéis semânticos muito mencionado, especialmente, em trabalhos referentes a etiquetadores semânticos.
De acordo com Palmer, Kingsbury e Gildea em, o PropBank foi criado a partir de o Treebank2.
O TreeBank-2 corresponde a aproxidamente um terço do Penn TreeBank.
Ele é formado essencialmente por textos em língua inglesa do Wall Street Journal.
Sendo, portanto, do domínio de Finanças.
O PropBank, que foi etiquetado manualmente, provê anotações semânticas às estruturas predicado-argumento.
O verbo de uma sentença geralmente indica um evento particular e os participantes desse evento estão associados aos argumentos sintáticos desse verbo.
As anotações semânticas propostas por os autores são inspiradas em trabalhos com de Levin (Seção 4.2) e o de Dowty.
Em razão de a diculdade de definir um conjunto universal de papéis semânticos, o PropBank dene um conjunto de papéis, chamado roleset, para cada uso distinto de um verbo.
Os papéis atribuídos aos argumentos dos verbos são rotulados por números de 0 (zero) a 5 (cinco):
Arg0, Os autores usam também um rótulo especial chamado ArgA para agentes cuja ação é induzida, ou seja, para os casos em que a ação executada por o agente é motivada por uma força externa.
É o caso, por exemplo, de verbos como to march (marchar).
Além de os rolesets, há rótulos para indicar as funções dos argumentos nas sentenças.
Tipicamente esses argumentos são atribuídos a elementos adjuntos ArgMs como por exemplo, o modo de um verbo (MOD).
A Tabela 4.1 descreve os rótulos ArgMs definidos por os autores.
A os rolesets são associadas as estruturas sintáticas compostas por os argumentos permitidos para cada verbo, bem como alguns exemplos (sentenças anotadas), formando um frameset.
A Figura 4.1 mostra o frameset 01 do verbo to accept (aceitar) no sentido to take willingly (receber espontaneamente).
A estrutura sintática definida para esse uso do verbo é caracterizada por os argumentos:
Arg0 indicando quem aceitou (Acceptor), Arg1 determinand que foi aceito (Thing accepted), Arg2 denindo a origem do que foi aceito (Accepted-from) e Arg3 descrevends atributos do que foi aceito (Attribute).
Já no caso de verbos polissêmicos em que o uso pode estabelecer diferenças de signicado, há geralmente mais de um frameset.
Um dos fatores que dene um novo frameset, por exemplo, é a necessidade de associar mais argumentos ao verbo num determinado uso.
A o conjunto de framesets de um verbo, os autores chamam de frames le (arquivos de frames).
Os frames les formam o léxico de verbos do PropBank.
A Figura 4.2 mostra os framesets 01 e 02 do verbo to decline, respectivamente no sentido to go down incrementally (em queda) e no sentido to demure, to reject (rejeitar).
Embora os autores tenham procurado atribuir o mesmo conjunto de papéis semânticos a verbos que pertençam à mesma classe, ou seja, que compartilham as mesmas estruturas sintáticas e sentido, não há um padrão definido.
No entanto, os rótulos Arg0 e Arg1, na maioria dos casos, correspondem, respectivamente, aos papéis agente e paciente (ou tema) Inicialmente, dos cerca de 3.300 verbos diferentes existentes no Treebank-2, o PropBank possuía apenas 1.826 de eles.
Atualmente, o PropBank provê anotação para todos os verbos do Treebank-2 sendo considerado, por esta razão, uma amostra representativa quanto a a anotação de papéis semânticos.
Sua limitação, no entanto, está no fato de se referir apenas a um gênero de texto, impossibilitand desenvolvimento de bons anotadores semânticos para outros domínios.
O corpus PropBank é distribuído comercialmente por a LDC (Linguistic Data Consortium).
Além de o léxico de verbos, a LDC fornece a anotação propriamente dita, que corresponde às sentenças TreeBank-2 etiquetadas com os papéis semânticos.
O léxico de verbos, embora esteja no pacote da LDC, também pode ser encontrado na página do projeto PropBank.
Em a Seção 4.5.1.1.2 há mais detalhes sobre a anotação das sentenças.
O FrameNet é uma base de dados composta por frames semânticos para Língua Inglesa, onde cada frame é uma estrutura conceitual que captura a semântica de uma determinada situação.
Os frames, além de descreverem os itens lexicais (em geral, nomes, adjetivos e verbos) e os papéis semânticos desses itens numa dada situação, provêem também sentenças anotadas que exemplicam tais descrições.
Por o fato de conter sentenças, o FrameNet é considerado tanto um léxico computacional quanto um corpus anotado com papéis semânticos.
Ele possui cerca de 141.000 sentenças anotadas manualmente, que foram obtidas principalmente do BNC (British National Corpus).
Os papéis semânticos, no FrameNet, são chamados frame elements (FEs) e são locais aos frames.
Eles descrevem os agentes e os objetos envolvidos na situação semântica caracterizada por o frame.
As situações descritas por os frames pertencem a domínios semânticos como corpo (partes e funções), cognição, comunicação, percepção, transações, tempo, espaço e outros.
A Figura 4.3 apresenta o frame Building (construção).
Este frame foi extraído diretamente da página web do projeto FrameNet.
Ele exemplica de forma simplicada a estrutura de um frame.
Um frame contém basicamente um identicador (Building);
Uma definição textual (Definition);
FEs principais (core) e secundários (non-core) juntamente com seus tipos semânticos (semantic type) e unidades lexicais (lexical units).
Building. Definition:
This frame describes assembly or construction actions, where an Agent joins Components together FEs:
Core: Agent:
Builds a Created_ entity.
Semantic Type: Sentient Semantic Type:
Artifact Non-Core: Semantic Type:
Physical_ entity Lexical Units.
Embora seja considerado um importante recurso linguístico, um dos problemas do FrameNet que justica inclusive o fato de não ser este amplamente usado principalmente por etiquetadores automáticos, é que o seu corpus não é uma amostra representativa da linguagem, é mais um conjunto de sentenças escolhidas manualmente para ilustrar as possibilidades de atribuição de papéis semânticos a itens.
Outro problema é o uso de papéis mais especícos pode dicultar a implementação dos anotadores automáticos.
VerbNet é um léxico de verbos em inglês.
Ele organiza os verbos em classes hierárquicas.
Essa organização é definida a partir de as propriedades sintáticas e semânticas dos verbos, sendo considerada uma extensão das classes de verbos de Levin (Seção 4.2).
Em o primeiro nível da estrutura hierárquica estão as classes definidas por Levin que, de acordo com Kingsbury e Kipper em, sofreram algumas modicações para garantir a sua uniformidade quanto a o compartilhamento de papéis semânticos.
Nos demais níveis da estrutura, as classes vão sendo especializadas.
Elas herdam todas as características de suas classes-pai, mas adicionam informações que denem restrições de papéis semânticos e que expandem as estruturas sintáticas e predicados semânticos herdados.
A Figura 4.4 apresenta um exemplo de classe VerbNet a partir de a classe Hit-18.
1. Cada classe descreve os papéis semânticos e suas restrições.
De os 25 papéis semânticos tratados por o léxico (ator, agente, beneciário, causa, local, etc), os argumentos dos verbos da classe Hit-18.
1 podem assumir apenas 3: Agente, paciente e instrumento.
Os papéis possuem restrições que permitem caracterizar os argumentos quanto a o tipo (concretu abstrato), tempo, estado, localização, escala, etc..
No caso de a classe Hit-18.
1, os argumentos definidos como instrumento (instrument) devem ser concretos.
A classe ainda indica o conjunto de verbos que compartilham características.
Em o exemplo, pertencem à classe os verbos to bang (bater, atirar), to bash (derrubar), to hit (acertar, atingir, golpear), to kick (chutar) e outros.
Os frames dessa classe descrevem as características sintáticas e semânticas desses verbos, indicando regência verbal (transitivo, intransitivo, aspectos temporais, etc.
Os argumentos desses predicados semânticos podem ser eventos, constantes, papéis temáticos e verbos especícos.
Em a classe Hit-18.
1, o predicado cause indica que o agente do verbo é a causa de um evento identicado como E. O VerbNet contém 5.200 verbos organizados em 237 classes.
Esse léxico tem sido considerado um recurso importante na área da linguística computacional.
No entanto, para a tarefa de etiquetagem de papéis semânticos, seu uso tem sido mais restrito, principalmente Alguns desses papéis estão descritos na Tabela C. 4 do Apêndice C. A WordNet é uma base lexical que provê o signicado de mais de 120.000 palavras, incluindo substantivos, verbos, adjetivos e advérbios em inglês.
É uma rede semântica cujos nodos são conjuntos de sinônimos, chamados de synsets.
Os synsets contêm todas as formas (palavras) em que se pode expressar um determinado conceito.
Eles contêm também uma glosa de definição e, em alguns casos, exemplos de sentenças sobre esses conceitos.
As palavras são ligadas dentro de os synsets através de relações sinonímia e antonímia.
No entanto, os synsets por meio de as relações de hiponímia e hiperonímia formam uma estrutura hierárquica de conceitos.
Os synsets também possuem relações de meronímia e holonímia.
No caso de os verbos, mais especicamente, há relações de hierarquia como troponímia, que conecta os verbos quanto a a maneira como eles realizam uma ação.
Por exemplo, to swipe (bater ou golpear com violência) e to smack (dar uma palmada ou tapa) são tropônimos de to hit (bater).
A relação nesse caso refere- se aos graus de força aplicados na ação.
Já em verbos que denotam eventos a relação é mais parecida com hiponímia, conectando conceitos mais gerais a especícos, como no caso de to plumment (cair verticalmente) que é tropônimo de to drop (cair).
A WordNet ainda contém relações de implicação (entailment) e também de causa para verbos.
Basicamente, ela é composta de verbos que denotam ações e eventos em 14 domínios semânticos especícos, que denotam movimento, percepção, contato, comunicação e outros.
Descreve também verbos que indicam estados, incluindo auxiliares e de controle.
Ela provê diferentes características aos verbos.
Nos que indicam movimento, por exemplo, ela especica velocidade, como to walk to run -- (caminhar- correr);
Direção, como to rise -- to fall (levantar- abaixar);
E meios de deslocamento, como to walk -- to drive (caminhar- dirigir).
É um recurso muito utilizado na construção de estruturas ontológicas como mencionado na Seção 2.5.2, e vem sendo usada também na tarefa de anotação de papéis semânticos, classificação semântica de verbos e desambiguação semântica de verbos.
A tarefa de atribuir papéis semânticos automaticamente aos argumentos de um verbo tem sido alvo de pesquisa intensa nos últimos anos.
Sendo considerada por os pesquisadores como uma tarefa importante no processo de compreensão da linguagem.
Para esse tipo de anotação, que não é trivial, várias abordagens têm sido propostas.
Abordagens baseadas em modelos estatísticos e em algoritmos de aprendizagem de máquina parecem ser as mais comuns.
A maioria dos trabalhos utiliza o PropBank ou mesmo as sentenças do FrameNet como corpus de treino.
Essas estratégias envolvem heurísticas para atribuir os papéis apoiadas, muitas vezes, em algum recurso léxico (WordNet, FrameNet e VerbNet) ou na combinação de eles.
Um dos primeiros trabalhos na área, muito citado, é o de Gildea e Jurasfsky.
Os autores extraem de 50.000 sentenças do corpus FrameNet diferentes características léxicas, sintáticas e posicionais dos constituintes dessas sentenças.
Tais características, combinadas com informações probabilísticas quanto a a atribuição de papéis, formam a base dos classicadores estatísticos propostos por os autores para realizar a tarefa de anotação semântica.
Já Swier e Stevenson em apresentam um método não supervisionado para atribuir papéis semânticos.
A abordagem dos autores baseia- se num algoritmo de bootstrapping cujo processo de atribuição de papéis inicialmente não considera qualquer ambiguidade na tarefa.
Para isso, usa padrões preliminares de papéis definidos a partir de as informações semânticas de verbos da VerbNet.
Ao longo de sua execução, no entanto, um modelo probabilístico de atribuição de papéis é criado e iterativamente aperfeiçoado à medida que novas anotações são realizadas.
O modelo leva em consideração informações sobre a classe dos verbos, bem como sobre as funções sintáticas e a classe dos nomes dos argumentos.
A classe dos nomes é definida a partir de a WordNet.
Em trabalhos com de Toutanova em, que utiliza o PropBank como corpus de treino, outras características têm sido incorporadas aos etiquetadores, principalmente no que se refere à relação de dependência entre as etiquetas semânticas de certos argumentos e as funções sintáticas de outros argumentos na sentença.
Sendo uma tendência, ao que parece, o uso de modelos mais globais quanto a a atribuição de papéis.
Para o corpus Wall Street Journal, os autores relatam que o modelo propostbteve na tarefa de anotação 81.9% de precisão, Pradman e Ward em descrevem o etiquetador ASSERT que também usa o PropBank como corpus de treino.
O trabalho dos autores tem uma relação estreita com o de Gildea e Jurasfky quanto a as etapas envolvidas no processo de atribuição dos papéis e às características linguísticas usadas para anotação.
Inicialmente, o sistema identica os argumentos dos verbos das sentenças e dene um conjunto das possíveis etiquetas semânticas para cada argumento.
Por meio de classicadores SVM, são atribuídas aos argumentos as etiquetas apropriadas.
Como a etiquetagem é feita de forma local para cada argumento, ao nal o sistema combina as etapas anteriores provendo assim os rótulos nais aos argumentos.
Os autores conseguem uma pequena melhora no F1 do corpus Brown ao incluírem de forma incremental pequenas partes desse corpus no conjunto de treino.
Para a Língua Portuguesa, o único trabalho que encontramos sobre anotação automática de papéis é o de Bick em.
O autor utiliza 500 regras definidas manualmente para anotar sentenças pré-processadas por o seu parser Palavras.
A abordagem do autor trabalha com 35 papéis semânticos e foi aplicada sobre parte docorpus Floresta Sintática Treebank.
Para este corpus, o autor relata um F1 de 88.6%.
Os etiquetadores de papéis semânticos que utilizamos em nossos estudos são comentados na Seção 5.3.4.
Em a Seção seguinte, apresentamos algumas aplicações de verbos e papéis semânticos na área de aprendizagem de ontologias a partir de textos.
Encontramos com frequência trabalhos que não usam FCA mas exploram verbos e papéis semânticos em diferentes tarefas relacionadas à aprendizagem e ao enriquecimento de estruturas conceituais a partir de textos, especialmente no que se refere à extração e identificação de relações transversais entre conceitos.
Esse é o caso do trabalho de Maedche e Staab em.
Os autores identicaram os conceitos e seus relacionamentos a partir de dependências sintáticas entre os verbos e seus argumentos em textos web do domínio Turismo.
Usaram também regras de associação para definir as relações mais relevantes, bem como estabelecer o nível de abstração mais adequado para tais relações.
Os autores deniram uma nova medida, a Relation Learning Accuracy (RLA), para avaliar a similaridade entre as relações não taxonômicas identicadas automaticamente e aquelas elaboradas manualmente.
Usaram também medidas de precisão e abrangência para avaliar a abordagem adotada.
O melhor RLA obtido foi de 0,67, para um suporte de 0,04 e uma conança de 0,01 (parâmetros das regras de associação), o que resultou em 13% e 11% de abrangência e precisão, respectivamente.
No entanto, a medida RLA de 0,53, com suporte de 0,06 e conança de 0,4 levou a abrangência e a precisão para 0%.
Por esta razão, os autores consideraram o método de avaliação proposto fraco, para abordagens automáticas.
Kavalec e Svátek em utilizam uma abordagem semelhante, no entanto propõem a medida Above Expectation (AE), baseada em probabilidade condicional, para estimar a associação entre verbos e pares de conceitos (identicados sintaticamente como argumentos desses verbos).
Os autores usaram em seus experimentos os corpora em língua inglesa Lonely Planet, do domíno de Turismo, e SemCor, que é uma parte do 9 10 corpus Brown 11 semanticamente anotado com sentidos da WordNet.
Utilizaram ontologias de referência para avaliar seus experimentos.
No entanto, para minimizar o problema referente a a falta de relações não taxonômicas nessas ontologias, o que acaba penalizando a precisão, os autores usaram duas medidas para este m:
Precisão anterior e precisão posterior.
A precisão anterior é a tradicionalmente usada.
Ela compara as relações descobertas com as existentes na ontologia de referência.
Já a posterior é calculada sobre as relações obtidas a partir de a abordagem, que não existiam na ontologia de referência, mas que foram consideradas corretas por os especialistas humanos.
Foram criadas também medidas &quot;anterior «e &quot;posterior «para abrangência e F-measure.
Os melhores resultados para o corpus Lonely Planet, em termos de precisão, foi para AE $= 3.
Em esse caso, as precisões anterior e posterior atingiram 100%, embora a abrangência anterior tenha cado por volta de os 35%.
Já para o SemCor, a precisão posterior atingiu 100% quando foi usado AE $= 4.
Sanchez e Moreno em também centralizam nos verbos o processo de descoberta de relações transversais a partir de textos web.
A metodologia prevê tanto a aprendizagem de relações taxonômicas quanto não taxonômicas.
A aprendizagem de relações taxonômicas inicia com uma semente fornecida por o usuário, que é uma palavra-chave do domínio para o qual a estrutura ontológica deve ser construída.
Essa semente é usada para pesquisar documentos web relacionados ao domínio.
A partir desses documentos são extraídas relações de hiponímia as quais são usadas para construir uma taxonomia com conceitos gerais, de apenas um nível.
De esses documentos também são extraídos os verbos que apareceram no contexto da semente, ou seja, na mesma sentença.
Os autores usam medidas estatísticas web para selecionar os verbos cujas relações com a semente são mais relevantes.
Os verbos escolhidos são usados em padrões de pesquisa para buscar novos textos web.
A partir desses textos, os conceitos relacionados à semente sãbtidos.
Os verbos selecionados são ainda usados para nomear as relações não taxonômicas identicadas.
O processo de aprendizagem se repete, usando como nova palavrachave um dos subconceitos da semente, enriquecendo cada vez mais a estrutura conceitual com novos conceitos e relacionamentos.
Assim como Kavalec e Svátek em, os autores também comentam a diculdade quanto a a avaliação das relações transversais dada a ausência dessas relações em ontologias de referência.
Por isso, propõem uma medida de similaridade baseada na WordNet.
Eles medem, usando a medida cosseno, a relação entre dois conceitos comparando as suas glosas, como vetores de termos.
Em os testes realizados, apenas 70% das relações puderam ser avaliadas, visto que seus conceitos existiam na WordNet.
No entanto, a baixa cobertura da WordNet para alguns dos domínios pesquisados prejudicou fortemente os resultados e os autores optaram então por utilizar também avaliadores humanos.
Estes relataram que os conceitos recuperados eram geralmente muito especícos e que da lista de verbos selecionada poucos verbos eram realmente produtivos.
Já Weichselbraum em têm combjetivo, além de identicar as relações não taxonômicas, atribuir às mesmas rótulos adequados.
A proposta inclui o uso de uma metaontologia que descreve as relações comuns em ontologias de domínio;
De recursos externos como a DBpedia 12 e a OpenCyc;
E de 13 corpora extraídos da web.
Esses recursos são usados para construir semiautomaticamente uma base de conhecimento com informações sobre relações conhecidas (rotuladas) entre conceitos de um determinado domínio.
Para cada relação entre conceitos C C m e n, a base contém uma lista dos verbos que costumam aparecer em textos associand conceito C C m a n;
O nome (rótulo) dessa relação;
Uma conceituação (metaontologia) descrevend domínio, a imagem e as propriedades que caracterizam a relação;
E ainda fragmentos de ontologias como a OpenCyc que formalizam o conhecimento sobre o domínio, descrevendo as classes dos conceitos que seguem a conceituação estabelecida para a relação.
O método pode tanto estender uma ontologia incluindo novas relações quanto gerar uma estrutura conceitual a partir de palavras-chave de um determinado domínio.
Para definir o nome de uma relação desconhecida entre dois conceitos ou instâncias, pertencentes à ontologia a ser estendida ou às palavras-chave informadas, inicialmente são extraídos de textos web, o conjunto de verbos que aparecem nos contextos dos conceitos dessa relação.
Por meio de a medida cosseno, é estabelecida a similaridade entre a lista de verbos da relação desconhecida e a lista de verbos das relações presentes na base de conhecimento.
A relação da base cuja similaridade for maior, terá seu rótulo atribuído à relação desconhecida.
Para avaliar o método proposto, especialistas estenderam manualmente uma ontologia cujo domínio referese a mudanças climáticas, incluindo relações transversais.
Uma parte das relações existentes nessa ontologia foi usada para construir a base de conhecimento e a parte restante, para avaliar a abordagem.
Embora os resultados sejam dependentes do tipo de relação não taxonômica que está sendo avaliada, a F-measure cou em torno de 84%.
Já Balakrishma em utilizam papéis semânticos para identicar e também para nomear as relações não taxonômicas.
De entre as 26 relações apresentadas por os autores, a maioria é baseada em papéis semânticos, tais como:
Agent (X, Y), ondeX é uma pessoa e é o agente de Y Instrument (X, Y),;
Onde é o instrumento de Y Topic (X, Y),;
Onde é o tópico de uma comunicação feita por Y;
Etc.. O método dos autores consiste em usar classicadores conhecidos, como Árvores de Decisão, Navie Bayes e Support Vector Machine e novas propostas como Semantic Scattering, para reconhecer as relações nos textos.
Os classicadores são treinados com informações morfossintáticas e semânticas extraídas de corpora anotados.
Para avaliar a abordagem quanto a a descoberta e etiquetagem correta das relações, os autores usaram, como conjunto de treino para reconhecer padrões de sintagmas nominais, os corpora TreeBank 2, L. A Times eWN 2.0.
Utilizaram também a FrameNet como conjunto de treino para identicar os padrões quanto a os argumentos dos verbos.
Como conjunto de teste, anotaram manualmente 500 sentenças, aleatoriamente escolhidas, do TreeBank 3 com relações semânticas.
Para esse conjunto de teste, os autores obtiveram 49,67 de F-measure.
O trabalho de Basili em também faz uso de papéis semânticos no processo de aprendizagem de ontologias a partir de texto.
Os autores utilizam a WordNet para desambiguação de sentido e a FrameNet para atribuir papéis semânticos aos substantivos encontrados nos textos.
Há trabalhos ainda que utilizam os papéis para caracterizar conceitos em aplicações de categorização e agrupamento de documentos e também para expandir consultas em aplicações de recuperação de informações.
A anotação de papéis semânticos é um recurso interessante para extração e representação de estruturas ontológicas visto que permite identicar e relacionar as entidades que participam de um evento.
No entanto, tratar com anotações semânticas desse tipo envolve pelo menos duas diculdades:
A falta de uma lista consensual de papéis semânticos e a imprecisão dos anotadores atuais para domínios que não sejam o do PropBank.
Embora o PropBank tenha, de uma certa forma, contornad problema da falta de consenso ao numerar os papéis, para aplicações que envolvem representação de conhecimento e venham a utilizar etiquetadores treinados com este corpus, o problema persiste, pois é importante determinar o signicado de cada anotação.
Caso a aplicação faça uso de um etiquetador cujo corpus de treino seja o FrameNet haverá problemas também, pois mesmo com etiquetas mais signicativas, elas são especícas, ou seja, foram definidas para as situações retratadas naquele corpus, que talvez não sejam as mesmas de outros corpora.
Além disso, no caso de os etiquetadores cuja anotação segue a do PropBank o signicado das etiquetas só é padronizado para verbos que pertençam às mesmas classes semânticas.
Apenas duas etiquetas Arg0 e Arg1 são mais regulares, correspondendo, respectivamente, aos papéis agente e paciente/ tema na maioria das vezes.
Somado a isso ainda há o fato que a pesquisa referente a o desenvolvimento de etiquetadores automáticos de papéis semânticos é relativamente recente.
Há ainda poucos corpora de treino mesmo para a Língua Inglesa.
E os melhores F1 obtidos na tarefa de anotação para o próprio Wall Street Journal giram em torno de 80%, caindo signicativamente para corpora de outros domínios.
Ao mesmo tempo que esses aspectos reetem os riscos de nossa pesquisa, também a tornam interessante dado que não encontramos trabalhos atuais com a nossa abordagem.
Este capítulo apresenta uma descrição sucinta dos corpora, bases lexicais, ontologias e ferramentas utilizados em nossa pesquisa.
Em esta Seção são descritos brevemente os corpora usados nos estudos realizados durante o doutorado.
O Penn TreeBank é um grande 1 corpus em língua inglesa, anotado com informações léxicosintáticas que foram revisadas manualmente.
É comercializado por o Linguistic Data Consortium (LDC) e possui 3 versões:
TreeBank-1, TreeBank-2 e TreeBank-3.
O TreeBank-1 é a versãriginal deste corpus e possui cerca de 4,8 milhões de tokens provenientes de textos de diferentes fontes.
Como mostra a Tabela 5.1, o TreeBank-1 contém transcrições de sentenças do Atis, transcrições de notícias de rádio do WBUR, artigos do Wall Street Journal e textos do corpus Brown 7.
Em a versão TreeBank-2 foi introduzido um outro estilo de anotação sintática, baseado em parênteses e ilustrado na Figura 5.1.
Esse estilo é muito usado, atualmente.
Parsers Mais informações pode ser encontradas no site do projeto Penn TreeBank:
Air Travel Information Services (Atis), mais informações em 7 Boston University Radio Speech Corpus, disponível no catálogo da LDC Cabe ressaltar que no pacote do TreeBank-2 distribuído por o LDC está incluído também o TreeBank-1 com sua anotaçãriginal.
Já o TreeBank-3 contém o TreeBank-2 e informações adicionais, tais como problemas de anotação decorrentes de interrupções nas sentenças, provocadas, por exemplo, por interjeições e repetições de palavras.
Os textos são fornecidos em 4 formatos:
Raw (estado bruto), tagged (anotado apenas como POS), parsed (anotado apenas com informações sintáticas) e combined (marcado com etiquetas POS e sintáticas).
O tipo de anotação atribuída aos textos pode ser identicado por as extensões dos arquivos.
Textos em formato bruto não possuem extensão.
Os textos nos formatos tagged, parsed combined e possuem, respectivamente, as extensões pos, prd mrg.
A Figura 5.1 ilustra o formato combined para a sentença em raw:
&quot;Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.»
de o texto wsj_ 0001.
O Penn TreeBank Sample corresponde a 9% do corpus Treebank-2.
Ele está entre os corpora disponibilizados livremente por a ferramenta NTLK.
Possui 100.673 9 tokens distribuídos em 199 textos, com uma média de 19,21 sentenças por texto.
Esta foi a versão do Penn TreeBank usada em nossos estudos iniciais.
Embora seja um corpus relativamente pequeno, ele pode ser baixado livremente e contém um volume de relações verbo-argumento adequado para a realização de nossa pesquisa.
Como já mencionado, o corpus PropBank é particionado em anotação e em léxico de verbos frames le).
Em a versão distribuída por o LDC, há em torno de 113.000 instâncias de verbos anotados.
São instâncias de cerca de 3.300 verbos diferentes.
Sendo que não há anotações para verbos de ligação como to be e auxilares como to do to have.
A Figura 5.2 exibe um trecho do arquivo de anotação prop.
Txt. A primeira coluna identica o texto e a segunda, a sentença desse texto.
As sentenças são enumeradas a partir de 0 (zero).
A primeira linha desse arquivo corresponde à anotação PropBank da sentença 0 do texto wsj_ 0001.
Mrg que foi apresentada na Figura 5.1.
Os corpora disponibilizados por a ferramenta NLTK podem ser encontrados em:
Cada terminal da sentença também é enumerado a partir de zero (Figura 5.3).
A terceira coluna da Figura 5.2 corresponde à posição do verbo na sentença.
O verbo to join está na posição 8.
A quarta coluna da Figura 5.2 identica o anotador.
Quando aparece a palavra gold, que é o caso dessa sentença, signica que mais de uma pessoa foi responsável por a anotação.
A quinta coluna identica o frameset do verbo.
Para esta sentença, oframeset é o join.
01. Já a sexta coluna fornece 5 informações sobre a conjugação do verbo, nesta ordem:
Forma (i $= innitive, g $= gerund, p $= participle e v $= nite), tempo (f $= future, p $= past e n $= present), modo (p $= perfect, o $= progressive e b $= both perfect and progressive), pessoa (3 $= 3 rd person) e voz (a $= active e p $= passive).
A anotação vfa indica que o verbo está na forma nite, no tempo future e na voz active.
Não há informação sobre modo e pessoa.
As colunas seguintes correspondem à anotação dos papéis semânticos.
A anotação 0:2-ARG0 da sétima coluna, indica que 2 níveis acima de o terminal 0 (zero), além de a etiqueta sintática, há também a etiqueta semântica ARG0.
Em a sentença 0 do texto wsj_ 0001.
Mrg esta etiqueta corresponde ao trecho anotado como Np:
Pierre Vinken (Figura 5.4).
Existem 4 formas de anotação para papéis semânticos.
Descrevemos apenas a primeira forma.
Mais informações podem ser encontradas no arquivo readme fornecido juntamente com o corpus.
Cabe ressaltar ainda que a parte referente a o léxico de verbos não foi detalhada nesta Seção, pois já foi apresentada na Seção 4.3.
O corpus SemLink 1.1 10 foi o que acabamos usando de fato em nossos estudos iniciais.
Ele é uma extensão do corpus PropBank e contém informações semânticas da VerbNet.
As informações VerbNet adicionadas às anotações PropBank nos permitiram estudar as classes dos verbos e trabalhar com identicadores mais tradicionais para papéis semânticos.
Desta forma, ao invés de apresentar em nosso estudo apenas os rótulos numéricos do PropBank, zemos uso do mapeamento VerbNet desses rótulos a nomes de papéis semânticos mais usuais (Agent, Theme, etc).
O projeto SemLink tem combjetivo combinar diferentes recursos lexicais:
PropBank, FrameNet, VerbNet e WordNet.
Dentro de o escopo daquele projeto, nós utilizamos o mapeamento vnpbprop.
Txt, que combina o PropBank com a VerbNet.
Em esse arquivo, como mostra a Figura 5.5, foram introduzidas no PropBank anotações referentes à classe do verbo e aos papéis semânticos.
No caso de a sentença 0 do texto wsj_ 0001.
Mrg, a classe VerbNet do verbo to join é 22.1-2 e os papéis semânticos PropBank ARG0 e ARG1 foram mapeados, respectivamente, em Agent e Patient1.
Como pode- se observar ainda na Figura 5.5, o mapeamento não é completo.
Nem todos os papéis PropBank foram mapeados.
Os papéis ARGM da sentença 0 do texto wsj_ 0001.
Mrg, por exemplo, não possuem papel semântico VerbNet correspondente.
Há ainda verbos para os quais não há classe associada, com verbo to usena sentença 0 do texto wsj_ 0003.
Mrg. Como usamos apenas os 199 textos do SemLink que correspondem ao Penn TreeBank 2, chamaremos esse subconjunto de SemLink 1.1 Sample.
O Wikicorpus 11 1.0 é um corpus trilíngue, com textos Wikipédia em Catalão, Espanhol e Inglês.
Esses textos foram selecionados e ltrados por Reese, a partir de um dump de 2006 da Wikipédia.
Possui cerca de 750 milhões de palavras, sendo que 600 milhões correspondem à Lingua Inglesa.
As palavras foram anotadas automaticamente com informações linguísticas Este corpus pode ser baixado livremente e fornece os artigos em 2 formatos:
Raw (estado bruto) e tagged (com anotações linguísticas).
A Figura 5.6 ilustra o formato tagged.
Em o cabeçalho dos textos, há informações sobre o artigo (identicadores e título).
As linhas posteriores estãrganizadas em 4 colunas:
A primeira é o token;
A segunda, seu lema;
A terceira, seu POS e a quarta, seu sentido WordNet.
Quand sentido WordNet não pode ser identicado, este código aparece como zero.
Em nossos estudos, usamos um subconjunto de textos desse corpus.
Como precisavámos de anotações referentes a papéis semânticos, as quais não estavam presentes na versão tagged, utilizamos a versão em formato raw.
Os textos, em formato raw, presentes nesse subconjunto, foram anotados semanticamente por a ferramenta F-EXT-WS descrita na Seção 5.3.4.
Para realizarmos nossa pesquisa, precisávamos de corpora de domínios especícos.
Julgamos que o domínio Finanças fosse o mais adequado, visto que poderia maximizar o desempenho dos etiquetadores de papéis semânticos.
A maioria dessas ferramentas, como a F-EXT-WS, têm sido treinadas usualmente com o PropBank, cujos textos são de Finanças.
Para complementar nossa investigação, precisávamos ainda de um segundo domínio.
Selecionamos, então, o domínio Turismo, por ser uma escolha frequente em trabalhos relacionados ao nosso.
Como no cabeçalho dos documentos do Wikicorpus 1.0 não há informação de categoria, para que pudéssemos selecionar artigos dos domínios mencionados, foi necessário buscar essa informação na própria Wikipédia.
Para isso, processamos um dump de 20/01/2011 e construímos uma lista com todos os títulos de artigos das categorias Finance Financial e para identicar textos do domínio Finanças.
Para o domínio Turismo zemos o mesmo, geramos uma outra lista contendo todos os títulos de artigos das categorias Tourism Travel.
É importante ressaltar que dos 8.958 artigos extraídos do Wikicorpus 1.0, utilizamos em nossa pesquisa apenas 934.
Esses 8.958 textos, no entanto, não correspondem ao total de artigos do Wikicorpus 1.0, mas sim à quantidade de textos que conseguimos processar.
O arquivo compactado do Wikicorpus 1.0 que está disponível na web é formado por 164 arquivostexto.
Cada um desses arquivos-texto contém vários artigos Wikipédia.
De esses 164, 9 foram baixados vazios e 1, processado parcialmente.
Isso corresponde a uma perda de aproxidamente 6%.
Investimos algum tempo na resolução desses problemas, no entanto acabamos decidindo por aceitar a perda, e trabalhando apenas com um subconjunto dado que já era suciente para os propósitos de nossa pesquisa.
Para viabilizar o uso de etiquetadores sintáticos e semânticos nos textos selecionados, foi para outras páginas web, os quais são muito comuns em textos Wikipédia.
O processo de identificação de categorias, limpeza e formação desses corpora foi realizado a partir de programas em linguagem C que implementamos para esse m..
Cabe mencionar, por m, que escolhemos a Wikipédia por conter artigos mais conceituais, o que julgamos mais adequado quando a nalidade é a extração de informações para construção de estruturas ontológicas.
O WikiFinance é um subconjunto do corpus Wikicorpus 1.0 para o Inglês e é constituído por 482 textos do domínio Finanças.
Ele possui 193.836 tokensdistribuídos em 945 sentenças.
A quantidade de sentenças presentes nos textos não é uniforme.
Em a média, há ao menos 2 sentenças por texto.
Inicialmente esses textos foram anotados com o Stanford parser 1.6.5 (Seção 5.3.3).
Este parser nos proveu anotações lexicossintáticas e de dependências.
No entanto, por questões de simplicidade acabamos não usando tais anotações.
Evitamos desta forma a necessidade de alinhamento das anotações realizadas por o Stanford parsercom as da ferramenta F-EXT-WS (Seção 5.3.4), usada para marcar os papéis semânticos.
Acabamos utilizando as anotações lexicossintáticas providas por a própria F-EXT-WS.
A partir de essas últimas anotações analisamos a distribuição dos tokens do corpus WikiFinance em categorias lexicais, contabilizando suas etiquetas POS.
A Tabela 5.2 apresenta a frequência das categorias relativas a substantivos, adjetivos e verbos, que são mais relevantes para o nosso estudo.
O WikiTourism também é um subconjunto do corpus Wikicorpus 1.0 para o Inglês e contém 442 textos do domínio Turismo.
Ele possui 179.399 tokensdistribuídos em 824 sentenças.
De a mesma forma que o WikiFinance, a quantidade de sentenças presentes nos textos não é uniforme.
Contém, em média, cerca de 2 sentenças por texto.
A partir de as anotações providas por a ferramenta F-EXT-WS (Seção 5.3.4), também analisamos a distribuição dos tokens do corpus WikiTourism nas categorias lexicais referentes a substantivos, adjetivos e verbos.
A Tabela 5.3 apresenta tal distribuição.
O corpus PLN-BR CATEG 12 foi utilizado em estudos para Língua Portuguesa.
Ele contém cerca de 30 mil textos do jornal Folha de São Paulo entre os anos de 1994 e O corpus PLN-BR CATEG foi constituído a partir de o projeto Recursos e Ferramentas para recuperação de Informações em Bases Textuais em Português do Brasil (PLN-BR) nanciamento CNPq -- CT INFO&amp; 550388/2005-2.
Mais informações sobre o projeto podem ser encontradas em Os textos desse corpus estãrganizados em 29 seções:
Agrofolha, Brasil, Caderno Especial 2, Ciência, construção (7 textos), Cotidiano (6.458 textos), Dinheiro (4.153 textos), Empregos, Entrevistada 2 (4 textos), Equilíbrio (28 textos), Esporte (4.632 texts), Folha Invest, Folha Negócios (36 textos), Folha Sinapse (11 textos), Folha Teen (260 textos), Folhinha (78 textos), Folha Vest (82 textos), Ilustrada (2.935 textos), Imóveis (120 textos), Informática (408 textos), Mais! (
252 textos), Mundo (2.410 texts), Primeira Página, Revista da Folha (3 textos), Tudo (95 textos), Turismo (464 textos), TVFolha (236 textos) e Veículos (215 textos).
Para que pudessemos utilizar- lo, submetemos esse corpus ao lematizador Forma, mencionado na Seção 5.3.6.
Usamos em nosso estudo as bases lexicais VerbNet e WordNet, já comentadas, respectivamente, nas Seções 4.5 e 4.6 do capítulo anterior.
Ambas foram acessadas a partir de o pacote NLTK (Seção 5.3.2).
Por meio desse pacote, conseguimos buscar classes VerbNet e também utilizar as medidas de similaridade semântica baseadas na WordNet.
Mais especicamente, usamos a implementação, disponível no pacote, da medida de Wu e Palmer no cálculo da coesão lexical dos conceitos formais.
Para esse m foi usada a versão WordNet 3.0.
Também com o propósito de calcular a coesão lexical dos conceitos e ainda para validar nossos estudos em categorização de textos, usamos ontologias do domínio de Finanças e Turismo.
Essas ontologias são para Língua Inglesa e estão representadas em OWL.
Para o domínio de Finanças, encontramos 3 ontologias:
Sumo_ Finance, LSDIS_ Fi-13 nance 14 e Finance.
Tais ontologias possuem conceitos sobre instrumentos nanceiros, partes envolvidas, processos e procedimentos relacionados a títulos.
Tanto a ontologia Sumo_ Finance quanto LSDIS_ Finance foram criadas por o grupo de pesquisa Large Scale Distributed Information System (LSDIS) do departamento de Ciência da Computação da Universidade da Georgia.
Ambas possuem relações transversais, sendo que a LSDIS_ Finance é uma extensão da Sumo_ Finance.
E foi por esta razão que, dessas duas, apenas a LSDIS_ Finance foi escolhida para o uso em nossos estudos iniciais.
Já a ontologia Finance, desenvolvida por Eddy Vanderlinden, foi encontrada no repositório de ontologias do Protégé.
16 Ainda no repositório de ontologias do Protégé, encontramos a ontologia Travel 17 do domínio de Turismo, criada por Holger Knublauch.
Também com o nome de Travel, encontramos 18 outra ontologia desse domínio.
Esta última foi criada por Danica Damljanovic e faz parte de um sistema web na área de Turismo que este autor desenvolveu.
A ontologia de Damljanovic é uma extensão da ontologia PROTON Upper Module, também chamada de WORLD, a qual dene conceitos relativos a locais, datas, línguas, etc..
Ambas as ontologias Travel possuem conceitos relacionados a pacotes de viagens, tipos de viajantes, destinos turísticos, etc..
Para evitar confusões entre as ontologias homônimas, chamaremos a última de TGPROTON, que é o nome do seu arquivwl.
Em a Tabela 5.4 apresentamos dados gerais das ontologias utilizadas em nossa investigação.
Esses dados são provenientes do processamento que realizamos em tais estruturas.
Cabe mencionar que apesar de existirem muitas ontologias na web, nem sempre há informações sucientes que nos permitam avaliar concretamente a relevância de tais ontologias para os domínios aos quais elas se propõem.
Obviamente, as ontologias com mais classes, como as de Finanças, aparentam maior relevância dad maior detalhamento do domínio.
No entanto, quantidade em classes não corresponde necessariamente à qualidade em conceitos.
Dada a falta de informação de ordem semântica dessas ontologias, baseamos nossas escolhas na procedência dessas estruturas.
Logo, consideramos relevantes as ontologias existentes em repositórios conhecidos, com do Protégé, e aquelas desenvolvidas e disponibilizadas por pesquisadores.
Em esta Seção descrevemos brevemente as ferramentas utilizadas no pré-processamento dos textos e em tarefas de agrupamento.
O TreeTagger 19 é um lematizador e etiquetador de POS.
Ele tem sido usado em várias pesquisas, principalmente porque atende a várias línguas, tais como inglês, alemão e português.
Decidimos usar- lo, pois em alguns testes empíricos de lematização dos corporaPenn TreeBank Sample e WikiFinance, observamos que o TreeTagger teve mais sucesso que o lematizador do pacote NLTK (Seção 5.3.2).
Os erros mais signicativos ocorreram com verbos, como mostra a Tabela 5.5.
O Natural Language Toolkit 20 (NLTK) é um pacote de ferramentas de código aberto, escrito em Python, usado para o processamento da linguagem natural.
O pacote inclui tokenizadores, stemmers, lematizadores, chunkers, parsers, clusterizadores e classicadores.
Junto com as ferramentas são disponibilizadas também amostras de corpora, tais como Brown, Reuters, e Penn TreeBank.
O pacote ainda contém ferramentas que permitem o acesso às bases lexicais como a WordNet.
Escolhemos este pacote justamente por reunir diferentes ferramentas essenciais para processamento de textos, de as quais usamos efetivamente o stemmer e as interfaces para WordNet e VerbNet.
Inicialmente, estavamos usando também o lematizador disponível no pacote, mas em razão de erros na lematização de verbos, optamos por o uso da ferramenta Tree Tagger para esse m, como já exposto.
Cabe mencionar que para usar cada uma das ferramentas citadas, tivemos que implementar pequenos programas em Python.
O parser estatístico da universidade de Stanford anota informações lexicossintáticas ao estilo TreeBank-2 e também relações de dependência lexical.
O parser foi criado para marcar textos em Língua Inglesa, mas pode ser adaptado para realizar anotações em outras línguas.
Escolhemos este parser por poder ser obtido e usado livremente e, principalmente, por anotar relações de dependência, que facilitam a identificação dos elementos essenciais à nossa pesquisa, que sãs verbos e seus argumentos.
Além disso, provê anotação ao estilo TreeBank-2 o que também nos favorece, pois evita que tenhamos que trabalhar com anotações em formatos diversos.
Sem contar ainda que é um parser bem conceituado cuja precisão é em torno de 86% para anotação sintática e de 91% para anotação de dependências.
Usamos a versão mais recente do parser21 para Língua Inglesa, 1.6.5, escrita em Java, para anotar os corpora Penn TreeBank Sample e WikiFinance.
A anotação sintática e a anotação de dependência são geradas separadamente, ou seja, o parser devolve duas saídas para o mesmo texto.
Em a Figura 5.7, é apresentado um exemplo de anotação para a sentença &quot;Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.»
de o texto wsj_ 0001 do corpus Penn TreeBank.
Como já mencionado, por razões de simplicidade, acabamos não usamos os textos anotados por este parser.
Utilizamos as anotações linguísticas providas por o processador F-EXT-WS, que é apresentado na próxima Seção.
Para anotar os textos com papéis semânticos, utilizamos duas ferramentas:
Illinois Semantic Role Labeler 22 (Illinois SRL) e F-EXT-WS.
A ferramenta Illinois SRL foi utilizada de forma mais supercial, em estudos preliminares para Língua Portuguesa.
Já a ferramenta F-EXT-WS foi responsável por a anotação semântica dos corporaWikiFinance e WikiTourism, utilizados em nossos estudos para a tarefa categorização de textos.
Cabe ressaltar que ambas as ferramentas utilizam rótulos PropBank para etiquetar os papéis semânticos e disponibilizam esse tipo de anotação apenas para textos em Língua Inglesa.
A ferramenta Illinois SRL foi desenvolvida por o Cognitive Computation Group (CGC) da Universidade de Illinois.
Possui uma versão demo 24 na web e uma versão que pode ser usada remotamente a partir de a instalação de um programa cliente.
Esse programa, no entanto, depende de outros recursos que incluem bibliotecas e parsers.
Encontramos diculdades quanto a a configuração correta dos recursos necessários ao seu funcionamento.
Como não conseguimos resolver tais problemas no tempo destinado para a realização dessa tarefa, dentro de o cronograma de nossa pesquisa, desistimos de utilizar- lo.
Para os estudos preliminares em Língua Portuguesa, usamos a versão demo.
Em os estudos com corpora para Língua Inglesa, utilizamos a ferramenta F-EXT-WS.
Ela é um processador web para linguagem natural.
Foi desenvolvida por o grupo de pesquisa LEARN do departamento de Ciência da Computação da Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio).
O processador F-EXT-WS oferece serviços de anotação tanto para textos em Língua Portuguesa quanto Inglesa, sendo que a anotação semântica é disponibilizada apenas para esta última.
Para Língua Inglesa, ele provê anotações para POS, sintagmas, cláusulas e papéis semânticos.
Para utilizar- lo, basta submeter o texto em formato raw, escolher o tipo de anotação desejada, que a ferramenta gera um outro arquivo com as anotações solicitadas.
Em nossos estudos usamos todas as anotações providas por a ferramenta.
A Figura 5.8 mostra um exemplo da anotação realizada por o F-EXT-WS.
Em o exemplo, o anotador processou a sentença:
&quot;Lockbox is generally divided into Wholesale and Retail.»,
de o corpus WikiFinance.
Observando- se a gura, pode- se perceber que a primeira coluna refere- se ao termo (word);
A segunda, a anotação de POS;
A terceira, à anotação de sintagma (ck);
A quarta, à identificação de sentenças (clause);
A quinta, ao verbo e as demais, aos papéis semânticos (srl0, srl1, srl2 srl3).
Para a sentença do exemplo, o F-EXT-WS associou aos argumentos &quot;Lockbox», &quot;generally «e &quot;into Whosale and Retail «do verbo to divide, respectivamente, os papéis semânticos A1, AMADV e A2.
Com bjetivo de termos uma referência quanto a o desempenho de ambos os etiquetadores Learning do ano de 2005).
A tarefa, nesse ano da conferência, consistia em marcar com papéis semânticos textos do Wall Street Journal.
Para o referido corpus, o anotador Illinois SRL obteve 82,28% na medida precisão;
76,78% na recall e 79,44% na F1.
Já o processador F-EXT-WS atingiu o índice de 80,54% na precisão;
65,47% na recall e 72,23% na F1.
É importante ressaltar que as fontes de as quais extraímos esses dados não são recentes, e, por isso, é bem possível que ambos os anotadores já tenham melhorads seus desempenhos.
Por m, cabe mencionar que, para extrair as anotações providas por o F-EXT-WS, desenvolvemos um parser em linguagem Java.
Usamos duas ferramentas para gerar as representações grácas das estruturas conceituais do tipo FCA apresentadas neste documento.
São elas:
Concept Expert 1.3 e Eclipse's Relational Concept Analysis (ERCA).
A ferramenta Concept Expert 1.3, desenvolvida por a equipe de Serhiy Yevtushenko, foi utilizada, ao longo de o texto, na maioria das guras que ilustram estruturas FCA.
Optamos por utilizar- la por ser uma ferramenta de fácil instalação e uso, além de ser frequentemente citada em trabalhos cientícos que fazem uso do método FCA.
Já a ferramenta ERCA 26 foi usada para gerar as estruturas do tipo RCA.
Essa ferramenta foi desenvolvida por o Laboratório de Informática, Robótica e Microeletrônica de Montpellier (LIRMM).
Esse laboratório reúne pesquisadores de duas instituições:
Universidade de Montpellier 2 e Centro Nacional de Pesquisas Cientícas.
Ela foi a única ferramenta que encontramos capaz de gerar estruturas RCA.
Cabe mencionar que os grafos correspondentes às estruturas FCA, analisadas em nossos estudos, foram gerados a partir de o algoritmo Bordat (Seção 3.5.2), o qual implementamos em linguagem Java.
Utilizamos no pré-processamento do corpusPLN-BR CATEG, as ferramentas Forma e WordSmith Tools.
Usamos, ainda, em estudos com este mesmo corpus a ferramenta Clustering ToolKit (CLUTO), no entanto, para construir conceitos a partir de termos extraídos dos textos.
A ferramenta Forma foi desenvolvida por Marco Gonzalez durante seu doutoramento.
Esta ferramenta segmenta o texto, lematiza e atribui etiquetas morfológicas para palavras e sinais de pontuação, com precisão em torno de 95%.
A precisão inclusive foi uma das razões para a escolha dessa ferramenta.
A outra razão foi o fato da ferramenta Forma ter sido desenvolvida por um integrante do grupo de pesquisa de o qual fazemos parte, o que facilitava o suporte quanto a o uso do lematizador.
Já as outras duas ferramentas, escolhemos por serem amplamente conhecidas.
O WordSmith, desenvolvido por Mike Scott, é uma ferramenta que auxilia na identificação de padrões textuais.
Usamos uma versão demo dessa ferramenta para encontrar palavras-chave para as categorias (seções) do corpus PLN-BR CATEG.
Para formar conceitos a partir de termos extraídos da Seção Esportes docorpusPLN-BR CATEG, utilizamos alguns dos algoritmos de agrupamento disponíveis na ferramenta CLUTO.
28 Esta ferramenta foi desenvolvida por o professor George Karypis do departamento de Ciência da Computação e Engenharia da Universidade de Minnesota.
O CLUTO é uma ferramenta de livre utilização, com muitos recursos e ampla documentação.
Informações De Corpora em Língua Inglesa Este capítulo faz uma introdução à investigação realizada neste trabalho, destacands objetivos traçados e os métodos utilizados para alcançar- los.
São apresentados trabalhos que serviram de inspiração e de base para proposta, a questão de pesquisa e seus desdobramentos, os métodos de pesquisa e avaliação empregados, bem coms estudos realizados para extração de informações de corpora em Língua Inglesa.
Combinar estruturas FCA com papéis semânticos não é uma ideia nova.
Kamphuis e Sarbo em, na década de 90, propõem a representação de uma frase em linguagem natural, associando esses elementos.
A ideia dos autores era viabilizar a interpretação de textos organizando as frases em estruturas conceituais.
O método FCA foi usado para representar hierarquicamente as relações linguísticas presentes nas frases.
Kamphuis e Sarbo, neste artigo, trabalharam com dois tipos de relações linguísticas:
Minor major.
Minor, e tipicamente, relacionava substantivos a adjetivos e advérbios;
E major, verbos a substantivos.
Em esse estudo foram associados manualmente papéis semânticos aos substantivos contidos nas relações minor major.
Apesar de a abordagem parecer promissora na época em que foi proposta, até agora havia sido pouco explorada.
Provavelmente devido a a diculdade de anotação dos textos, visto que o surgimento de etiquetadores automáticos de papéis semânticos é mais recente.
Nossa abordagem não se assemelha à de Kamphuis e Sarbo nem no método e nem no propósito.
Nosso estudo, diferente do trabalho dos autores, extrai automaticamente as relações a partir de textos com anotações linguísticas.
Além disso, restringe- se às relações que os autores chamam de major.
Igualmente não visa a interpretação de textos, apenas a tangência, na medida em que objetiva a construção automática de estruturas conceituais.
Em a década de 90, Rudolf Wille em apresenta exemplos de estruturas FCA combinadas com papéis semânticos.
bjetivo do autor, no entanto, é diferente do nosso.
Ele combinou grafos conceituais com estruturas FCA visando a formalização de uma lógica útil à representação e ao processamento de conhecimento.
Em os exemplos apresentados por o autor, os grafos conceituais, que são mapeados em estruturas FCA, contêm papéis semânticos associados.
Esses papéis aparecem como atributos nos contextos formais dessas estruturas.
Os contextos são formados basicamente por relações objeto-atributo do tipo instância-papel_ semântico e instância-classe.
Algumas das relações usadas em nosso estudo foram inspiradas nas definidas por Wille.
Como não há comentários, naquele trabalho, quanto a o processamento das informações presentes nos grafos conceituais, imaginamos que nem a construção desses grafos e nem o seu mapeamento em estruturas FCA tenham sido realizados de forma automática.
Esta, portanto, é mais uma diferença em relação a o nosso estudo.
O trabalho Rudolf Wille não convive com as diculdades de automaticamente extrair informações de texto para gerar estruturas de representação de conhecimento e nem tampouco analisa, nesse sentido, os limites de sua abordagem.
Em trabalhos mais recentes também encontramos o método FCA combinado com papéis semânticos.
Um exemplo é o trabalho de Valverde-Albacete, publicado em 2008.
De forma distinta de nosso trabalho, o autor não usa o FCA como método de apoio à construção de estruturas ontológicas a partir de textos.
Seu esforço é voltado à análise linguística tendo como propósito representar a FrameNet através de reticulados conceituais.
Sendo assim, não faz uso, como nós, de informações textuais e nem faz uso de anotações PropBank para identicar os papéis.
Há ainda as aplicações descritas na Seção 3.9 que, embora não utilizem papéis semânticos, estão relacionadas ao nosso estudo.
Elas utilizam abordagens centradas em verbos para extrair relações não taxonômicas e pesquisam o uso do FCA ou de suas extensões como métodos de agrupamento para construção de estruturas conceituais a partir de textos.
De entre esses trabalhos, o de Cimiano é o que mais se aproxima de nossa proposta.
No entanto, aqueles autores não consideram, em sua abordagem, a ambiguidade existente nas relações entre verbos e seus argumentos, nem tampouco consideram os diferentes papéis semânticos que cada argumento pode assumir ao longo de um texto.
As aplicações descritas na Seção 4.8, ainda que não façam uso do FCA como método de agrupamento, têm igualmente relação com a nossa proposta.
O aspecto comum refere- se ao fato de usarem verbos e papéis semânticos na definição de relações transversais entre conceitos.
Mesmo com a profunda revisão bibliográca realizada, não encontramos, até o momento, trabalhos que explorem os papéis semânticos em conjunto com o método FCA para apoiar a construção de estruturas ontológicas a partir de textos.
Dado que a proposta de combinálos é pouco pesquisada, consideramos ser de interesse todo esforço gerado ao explorar- la.
No entanto, acreditamos que uma de nossas principais contribuições esteja no estudo de como usar a informação semântica provida por os papéis semânticos em estruturas conceituais geradas a partir de o método FCA.
A seguir, apresentamos nossa questão de pesquisa e seus desdobramentos.
Como já mencionado na Seção 2.5.2, a aprendizagem de relações não taxonômicas a partir de textos é ainda um desão.
A captura de relações dessa natureza é difícil até mesmo para os ontologistas, dada a quantidade e variedade de relacionamentos possíveis entre os conceitos.
A falta de &quot;padronização «de rótulos para tais relações é um dos fatores que tem restringindo, inclusive, a aplicação das ontologias em ambientes mais dinâmicos.
Com bjetivo de auxiliar os ontologistas, diferentes abordagens computacionais têm sido propostas, tanto para identicar as relações quanto para nomear- las.
A maioria das abordagens pesquisadas baseia- se fundamentalmente em verbos para realizar tais tarefas.
Nossa abordagem também segue nessa linha pois, assim como Navok e Hearst em, acreditamos que os verbos consigam capturar aspectos sutis de signicado sendo, portanto, importantes fontes de expressividade em tarefas de representação semântica.
Entendemos que essa expressividade se estenda também às informações semânticas providas por verbos, como classe de verbos e papéis semânticos.
Acreditamos que tais informações possam aumentar a relevância semântica de estruturas conceituais geradas a partir de textos, principalmente no que se refere à extração e à atribuição de nomes a relações não taxonômicas.
Desta forma, nossbjetivo primário é combinar o método FCA com papéis semânticos para construir, de forma automática e a partir de informações textuais, estruturas ontológicas fortemente baseadas em relações não taxonômicas.
A idéia é explorar as vantagens do FCA como um método de agrupamento conceitual.
O método FCA, quando comparado a outros métodos de agrupamento, permite delinear mais facilmente, do ponto de vista semântico, os grupos e subgrupos de uma hierarquia.
Já no que tange aos papéis semânticos, investigamos o uso dessa informação semântica no processo de extração de informação.
Sendo que nossbjetivo principal é utilizar tal informação na geração de conceitos (agrupamento), bem como na definição de relacionamentos entre esses conceitos.
Considerando todos esses aspectos, direcionamos nossa pesquisa por um caminho que nos permitisse analisar e entender a real contribuição dos papéis semânticos na construção de estruturas conceituais FCA geradas a partir de texto.
Esse caminho de pesquisa se desdobrou em muitas indagações as quais envolvem aspectos mais especícos referentes a:
Essas indagações são retomadas na Seção 6.3, que descreve os métodos de pesquisa usados neste trabalho e apresenta os estudos realizados durante nossa investigação.
É importante destacar que nosso estudo não gera, como resultado, ontologias tais como descritas na visão de Guarino, mas sim estruturas conceituais que podem ser usadas para construir ontologias.
Por outro lado, nossas estruturas sãntológicas e de domínio, e suas características cabem perfeitamente na definição de Gruber para ontologias, que é aceita e utilizada no âmbito dessa pesquisa.
A seguir, descrevemos os métodos de pesquisa usados para responder às questões colocadas.
Com bjetivo de responder às questões colocadas na Seção 6.2, usamos os quatro aspectos apresentados naquela Seção para orientar nosso estudo.
Desta forma, para analisar o aspecto referente a a &quot;identificação das relações não taxonômicas», realizamos uma pesquisa exploratória com um caráter mais quantitativo.
Nossbjetivo, nessa fase, era identicar, nos corpora usados para estudo inicial, os sintagmas nominais, os papéis semânticos, as relações e as classes de verbos que nos poderiam prover uma pesquisa mais expressiva do ponto de vista quantitativo.
Chamamos a esta fase de &quot;extração e análise quantitativa preliminar das informações existentes nos corpora Penn TreeBank Sample e SemLink 1.1 «e a descrevemos na Seção 6.4.
Para o aspecto referente a a &quot;organização dessas relações numa estrutura conceitual «realizamos primeiramente uma pesquisa de natureza qualitativa.
Estudamos a adequação de estruturas RCA no que se refere à representação dos papéis semânticos em conceitos formais.
Analisamos, também de forma qualitativa, duas classes VerbNet.
Essas classes foram apontadas por a pesquisa quantitativa, descrita na Seção 6.4, como as mais signicativas para os corpora analisados.
Esse estudo é o assunto do Capítulo 7, ao qual chamamos de &quot;Estudo I -- Análise de estruturas conceituais RCA e de classes de verbos».
Ainda para o aspecto relativo à organização das relações, realizamos uma pesquisa exploratória.
Nossbjetivo foi propor formas de incluir os papéis semânticos em reticulados conceituais, baseados em FCA, e analisar a contribuição dessas informações sob um olhar mais estrutural.
Esse estudo é apresentado no Capítulo 8 sob o título &quot;Estudo II -- representação de informações semânticas em conceitos formais».
Já para o aspecto &quot;aplicabilidade da abordagem em outros corporae domínios «realizamos estudos de natureza exploratória e experimental na área de categorização de documentos.
A meta era analisar a efetiva contribuição das estruturas conceituais geradas a partir de nossa proposta na tarefa de classificação.
Chamamos a esta fase de &quot;Estudo III -- Aplicabilidade da proposta e estudos em Língua Portuguesa «e a detalhamos no Capítulo 9.
Com título do capítulo já diz, incluímos nesse capítulo também estudos com corpuspara Língua Portuguesa.
Cabe ressaltar que até este capítulo, os estudos mencionados foram todos realizados em corpora para Língua Inglesa.
A principal razão para isso reside no fato de não encontramos ferramentas de anotação de papéis semânticos para Língua Portuguesa, o que era fundamental para nossa pesquisa.
No entanto, realizamos alguns estudos usando corpus em Língua Portuguesa, focando a extração de conceitos a partir de textos e a categorização de documentos.
Apesar de a falta de recursos, desenvolvemos um estudo preliminar quanto a o uso de papéis semânticos em estruturas FCA a partir de informações extraídas de textos em português.
Esse estudo é apresentado na Seção 9.2.4.
Cabe ressaltar também que o aspecto &quot;avaliação da abordagem proposta «permeia tant Estudo II, para o qual aplicamos uma avaliação de ordem estrutural -- a coesão lexical -- quant Estudo III, em o qual usamos uma avaliação de ordem funcional e aplicamos métricas usuais em categorização de documentos, tais como precisão e abrangência (recall).
A Seção seguinte descreve o pré-processamento dos textos e o estudo quantitativo dos corpora em Língua Inglesa Penn TreeBank Sample e SemLink 1.1, utilizados em nossa pesquisa.
Para realizar este estudo escolhemos, o corpus PropBank.
A escolha se baseou essencialmente no fato desse corpus conter as anotações semânticas necessárias para tal estudo.
Além disso, como a maioria dos etiquetadores de papéis semânticos atuais segue o formato de anotação PropBank, a aplicação de nossa abordagem a outros corpora se tornaria mais viável.
Apesar de facilitar a aplicabilidade de nossa proposta, o alinhamento dos rótulos numéricos providos por esta anotação ainda era um problema para o processo de agrupamento dos termos que objetiva a construção dos conceitos.
Esses rótulos, ainda que idênticos, só poderiam ser unicados quando fossem associados a verbos de uma mesma classe.
Em essa etapa encontramos o corpus SemLink 1.1 que mapeia a VerbNet ao PropBank.
Em esse corpus, que é uma extensão do corpusPropBank foram incluídos, na anotação, a classe VerbNet do verbo e o papel semântico correspondente ao rótulo numérico PropBank.
Embora esse mapeamento não seja completo, serviu para contornar o problema e viabilizou nosso estudo inicial.
Tant corpus PropBank quant SemLink 1.1 contêm apenas as anotações semânticas, não incluem as sentenças.
Era necessário, portanto, alinhar o TreeBank-2 a um desses corpora para extrair os termos juntamente com suas anotações.
Usamos, nesse alinhamento, o corpus Penn TreeBank Sample que corresponde a 9% do TreeBank-2.
Optamos por este pequeno corpus por ser gratuito e adequado para este estudo exploratório preliminar.
A Figura 6.1 mostra a sentença 0 do texto wsj_ 0001 e as anotações linguísticas providas por os corpora PennTreeBank Sample e SemLink 1.1 antes do processo de alinhamento.
Cabe lembrar que, para cada verbo que foi anotado semanticamente numa sentença, há uma linha no SemLink descrevendo tal anotação.
Portanto, ao alinharmos os corpora, especicamos a que verbo se referem as anotações.
Desta forma, conseguimos relacionar o verbo a seus argumentos e a suas informações semânticas.
Em a Figura com a etiqueta numérica PropBank ARG0 a qual corresponde ao papel VerbNet Agent.
Após o alinhamento, processamos os textos a m de extrair de eles os sintagmas nominais, papéis semânticos e classes de verbos relevantes para nossa pesquisa.
A partir desse préprocessamento, que é detalhado nas Seções 6.4.1 e 6.4.2, realizamos uma análise quantitativa dos corpora quanto a os elementos extraídos.
Os resultados dessa análise são comentados na Seção de classes de verbos usados no Estudo II.
Cabe comentar ainda que implementamos em linguagem Java os programas que realizaram tant alinhamento dos corpora quant pré-processamento das sentenças e a análise quantitativa.
Após o alinhamento dos corpora, realizamos o pré-processamento dos textos cujo principal objetivo, a exemplo de outros trabalhos como, era extrair, das sentenças desses textos, os verbos e seus argumentos, bem como suas anotações semânticas.
Durante esse processo, consideramos apenas verbos que possuíam classe VerbNet definida.
Optamos por este ltro, primeiramente, porque tais classes foram associadas apenas a verbos que receberam anotação PropBank e, consequentemente, também possuíam argumentos etiquetados com papéis semânticos VerbNet.
Outra razão é que poderíamos estudar a contribuição das classes dos verbos na construção dos conceitos formais e, ainda, comparar essa abordagem com outras, tais como as que consideram apenas os verbos, por exemplo.
Para os verbos assim selecionados extraímos, dos argumentos anotados, seus sintagmas nominais e papéis semânticos associados.
Decidimos trabalhar com sintagmas nominais, pois eles são considerados bons candidatos a conceitos em aprendizagem de estruturas ontológicas a partir de textos.
Por a mesma razão, consideramos apenas os sintagmas nominais cujos núcleos são substantivos.
Outro motivo para esta escolha é que, em tarefas de classificação de texto, como a que descrevemos no Capítulo 9, o uso de n-gramas tem contribuído para a melhora dos resultados.
Como sintagmas nominais também são uma espécie de gramas, acreditamos que seu uso seja indicado para nossa pesquisa.
Com o propósito de extrair os verbos e seus argumentos, começamos o pré-processamento (Figura 6.3) identicando e normalizando morfologicamente os terminais (folhas) das árvores sintáticas das sentenças dos textos.
Aplicamos o stemmer do pacote NLTK (Seção 5.3.2) e o lematizador Treetagger aos terminais cujas etiquetas POS se referiam a verbos, substantivos comuns e adjetivos.
Posteriormente, usamos os lemas desses termos para construir os conceitos formais;
E usamos suas bases (stems) para definir um conjunto de sementes.
Os termos, etiquetados como substantivos, cujas bases eram mais frequentes, foram usados como sementes no processo de construção das estruturas conceituais (o processo de definição das sementes é detalhado na Seção 6.4.3).
Terminad processo de normalização, iniciamos, então, o processo de extração propriamente dito.
A extração dos verbos e suas anotações semânticas foi simples, as etiquetas facilitaram a identificação das informações.
Em a sentença 4 do texto wsj_ 0003, exemplicada na Figura 6.6, o verbo (identicado por a etiqueta VBD) é said, sua classe VerbNet (marcada por a etiqueta VNinformação posicionada antes da etiqueta VN).
Cabe mencionar que usamos os lemas dos verbos ao definir os conceitos formais.
Já no caso de os argumentos desses verbos, tivemos que definir algumas heurísticas para a extração dos sintagmas nominais, embora eles estivessem marcados com etiquetas Np.
Isso aconteceu porque, ao analisarmos as anotações semânticas atribuídas às sentenças, percebemos que os argumentos marcados com papéis temáticos não correspondiam apenas a sintagmas nominais, mas também a segmentos de sentenças.
Em esses segmentos encontramos inúmeras construções gramaticais, as quais aumentaram consideravelmente a complexidade de implementação do parser responsável por esse processo de extração.
Encontramos segmentos curtos, de tratamento computacional mais simples, delimitados por sintagmas constituídos de apenas um determinante e um substantivo comum, mas também localizamos segmentos mais longos e complexos, contendrações e sintagmas aninhados.
Visto que a implementação de um parser &quot;completo «(no sentido linguístico) com tal nalidade, ainda que relevante, não era o foco principal de nossa pesquisa, optamos por simplicar tal implementação, tratando um subconjunto dessas construções.
O conjunto de heurísticas definido para tratar tais construções é o assunto da próxima Seção.
Como mencionado, tivemos que simplicar o processo de extração dos sintagmas nominais.
Procuramos primar, então, por a qualidade e não por a quantidade, utilizando, na implementação desseparser, as heurísticas especicadas a seguir:
Argumentos, no entanto, que sãrações, ou seja, que também contêm verbos, poderão ser analisados posteriormente por o parser desde que estejam anotados.
Esse é o caso do argumento 4 que inclui o verbo to make, cujo processamento é mostrado na Figura 6.5.
Os sintagmas nominais resultantes desse processo são, ainda, decompostos em termos aninhados.
No caso de sintagmas contendo a preposição &quot;of», somente é decompost n-grama anterior à preposição.
Os substantivos próprios são extraídos somente quand sintagma não tem substantivos comuns.
Sentença completa e sem anotação:
The fact that New England proposed rate increases 4.8% over seven years against around 5.5% boosts proposed by the asserted.
Trecho referente a o verbo to propose:
The fact that New England proposed 4.8% over seven years against around 5.5% boosts proposed by the excluímos, dos sintagmas nominais extraídos, substantivos relacionados a tempo, como week, day, month, year, today etc..
Também excluímos outros elementos menos signicativos para a formação de conceitos, como caracteres especiais, numerais, artigos e pronomes.
Se, após a eliminação desses elementos, o sintagma nominal restante é inválido, principalmente devido a a ausência de substantivos, o argumento também é descartado.
Cabe mencionar que termos referentes a tempo são muito frequentes em textos jornalísticos, no entanto sua relevância tende a ser baixa para nossa pesquisa, visto que não tratamos a ordem cronológica em que ocorrem os fatos expressos nos textos.
Como também não tratamos numerais, quando esses termos se referem a quantidades, eles igualmente se tornam irrelevantes.
O processo de extração de informações, conforme descrito nesta seção, resultou em 11.076 relações do tipo verbo-argumento.
Visto que estudamos também as relações entre os papéis semânticos, formamos tuplas a partir de essas relações.
Cada tupla é constituída de um verbo, sua classe VerbNet e dois de seus argumentos (sintagmas nominais constituídos de substantivos comuns) existentes numa mesma sentença.
Para cada argumento, associamos os papéis semânticos VerbNet e PropBank correspondentes.
Anexamos ainda, aos argumentos, suas instâncias (sintagmas constituídos de substantivos próprios), quando elas existem.
Também documentamos, para ns de análise, informações quanto a as sentenças e textos a partir de os quais tais tuplas foram extraídas.
A Tabela C. 6 do Apêndice C apresenta exemplos dessas tuplas.
Após esse processo de extração, analisamos as relações expressas por as tuplas e decidimos quais das informações obtidas poderiam ser interessantes para nossa pesquisa.
Essa análise é o assunto da próxima Seção.
O conjunto de sentenças anotadas que tínhamos para este estudo inicial era relativamente pequeno.
Logo, zemos um levantamento a m de determinar quais das informações extraídas poderiam prover resultados quantitativamente mais signicativos.
Durante o processamento das anotações morfossintáticas das 3.914 sentenças pertencentes aos 199 textos do Penn TreeBank Sample, encontramos 100.673 tokens.
De esse total, apenas desses tokens em categorias lexicais, contabilizando suas etiquetas POS.
Nosso interesse, no entanto, estava nas quantidades relativas a substantivos, adjetivos e verbos, os quais tinham maior relevância para nosso estudo.
A Tabela 6.1 apresenta a frequência dessas categorias no Penn TreeBank Sample.
Em seguida, precisavamos determinar quais dos substantivos comuns poderiam ser usados como sementes na construção das estruturas conceituais.
Imaginamos que os mais frequentes poderiam conter uma gama maior de relacionamentos tanto com verbos quanto com outros substantivos e, portanto, poderiam gerar estruturas FCA semanticamente mais relevantes.
Para definir as sementes, usamos os stems dos termos etiquetados como substantivos comuns.
Analisamos aqueles termos cujos stems eram mais frequentes, ou seja, que possuíam mais de 50 ocorrências.
Escolhemos, entre os 57 termos assim definidos, os mais relevantes para o domínio de Finanças.
Para determinar tal relevância, usamos o corpus WikiFinance como referência.
Com esse m, construímos uma lista com os termos desse corpus ordenados por a frequência de seus stems.
Comparamos, então, os termos do WikiFinance aos 57 do Penn TreeBank Sample e selecionamos os 10 mais frequentes em ambos os 2 corpora.
Os termos escolhidos e suas frequências são apresentados na Tabela 6.2.
Cabe mencionar que os 10 termos escolhidos possuem frequência superior a 400 no WikiFinance.
Cabe destacar também que desconsideramos, nessa seleção, termos referentes a tempo, como year month.
Precisávamos ainda determinar quais informações semânticas relativas aos verbos poderiam ser mais signicativas para nosso estudo.
Analisamos, então, as anotações providas por o corpus SemLink 1.1.
De as 112.917 linhas de anotação semântica contidas nesse corpus, 9.353 correspondiam às sentenças do Penn TreeBank Sample.
Iniciamos analisando as classes dos verbos.
De as 12.637 instâncias de verbos, 8.492(~ 67%) possuíam anotação de classe VerbNet.
Encontramos 245 classes diferentes:
178 classes principais e 67 subclasses.
Os verbos pertencentes a subclasses, no entanto, eram menos frequentes.
Vericamos que cerca de 80% das 11.076 relações extraídas continham verbos anotados com classes principais da VerbNet.
Para visualizar a expressividade de cada classe, contabilizamos as instâncias de seus verbos (Tabela C. 2 do Apêndice C).
As 10 classes VerbNet mais frequentes são apresentadas no gráco da Figura 6.9.
A classe mais frequente é a 37.7 Encontramos nos textos 18 verbos dessa classe, tais como:
Te o say, to propose, to announce, to suggest, to claim, to disclose to report.
A segunda classe mais frequente é a 45.4.
Para ela encontramos mais tipos de verbos:
O valor relativo à frequência dos termos analisados foi arbitrária.
C. 3 apresenta a lista completa dos verbos para as 5 classes mais frequentes no corpus Penn TreeBank Sample.
Em seguida, analisamos os papéis semânticos associados aos argumentos dos verbos.
A o processar o corpus SemLink 1.1 Sample, identicamos 22 papéis semânticos.
O signicado de cada uma dessas etiquetas semânticas foi descrito na Tabela C. 4 do Apêndice C. Observamos ainda a ocorrência desses papéis nos corpora.
O gráco da Figura 6.10 mostra os papéis semânticos identicados e suas respectivas frequências.
4 De os 9 papéis mais frequentes, escolhemos 8 para analisar:
Agent, Theme, Patient, Topic, Experiencer, Recipient, Cause e Product.
Excluímos Predicate por julgar- lo pouco informativo.
Observamos, então, a distribuição desses papéis quanto a as classes dos verbos e às instâncias dos argumentos (Tabela 6.3).
Com base nessa distribuição, concentramos nosso estudo nos papéis Agent, Theme, Patient e Topic.
No entanto, há momentos em nossa investigação, em que todos os papéis semânticos são considerados, tal como nos estudos relativos às classes VerbNet dos verbos.
Os papéis Agent e Theme são muito frequentes, aparecem associados a quase um terço dos argumentos.
Além disso, os verbos que denem tais papéis pertencem a mais de 40% das classes VerbNet identicadas.
Essas distribuições poderiam permitir um estudo mais abrangente por o fato dos papéis permearem várias classes, e também um estudo provavelmente mais conclusivo devido a a frequência desses papéis em argumentos.
Já os papéis Patient e Topic são menos frequentes em argumentos, mas estão concentrados em classes que escolhemos estudar.
Patient, por exemplo, aparece associado a argumentos de vários verbos da classe 45.4 e Topic aparece como papel recorrente dos argumentos de verbos da classe 37.7.
Cabe lembrar, SemLink 1.1 Sample foi o nome que atribuímos ao subconjunto do corpus SemLink 1.1 cujas anotações correspondem às sentenças do Penn TreeBank Sample.
Figura 6.10 Frequência das instâncias de papéis semânticos encontrada nos corpora Penn TreeBank Sample e SemLink 1.1 Sample.
Cabe mencionar que não consideramos em nosso estuds papéis ARGM-MOD (modicador de modo) e ARGM-TMP (modicador de tempo), pois entendemos que o modo é pouco relevante para a construção de conceitos e, como já mencionamos, não tratamos termos relacionados a tempo.
Os argumentos associados a esses papéis também foram descartados.
Analisands dados quantitativos apresentados nesse capítulo, acreditamos que a natureza do corpus tenha uma grande inuência na frequência de determinados papéis.
Com corpus Penn TreeBank Sample é de cunho jornalístico, os papéis Agent, Theme e Topic são naturalmente os mais recorrentes.
Observamos que 44% das instâncias de argumentos presentes em relações verbo-argumento estão associadas aos papéis Theme e Topic.
Coms papéis Theme e Topic, de acordo com suas Definições na Tabela C. 4 do Apêndice C, são mais genéricos, a natureza do corpus, de uma certa forma, aumenta quantitativamente a importância desses papéis e, portanto, &quot;impede «que relações semânticas de domínio mais interessantes se destaquem.
Embora as relações do tipo Agent- Theme e Agent-Topic nos tragam informações interessantes na medida que estabelecem ligações entre pessoas ou entidades a termos do domínio, pouco se pode dizer sobre os argumentos anotados como Theme e Topic nessas relações.
Sob esse ponto de vista, torna- se difícil determinar conclusivamente que relações semânticas são mais importantes, pelo menos quantitativamente, para o domínio de Finanças.
De maneira geral, essa análise quantitativa não nos permite responder, de forma contundente, à maioria das questões referentes à identificação das relações não taxonômicas que foram apresentadas na Seção 6.2.
Acreditamos que uma análise quantitativa seria mais expressiva no caso de um corpus de natureza conceitual.
Acreditamos também que no caso especíco dos corpora utilizados, a seleção das relações transversais a serem usadas na construção de estruturas conceituais teria que ser baseada essencialmente no tipo dos papéis envolvidos.
Em o contexto desses corpora, os papéis Theme e Topic, por exemplo, são pouco expressivos, pois são atribuídos indiscriminadamente a diferentes elementos do domínio.
Outra alternativa seria solicitar a ajuda de um especialista no domínio para estabelecer as relações entre papéis mais relevantes.
Apesar de não serem conclusivos, os resultados apresentados nesse capítulo foram úteis para delinear o Estudo I. O Estudo I é abordado no capítulo a seguir.
Este capítulo descreve a investigação realizada para determinar a viabilidade de uso da extensão RCA no que se refere à inclusão de papéis semânticos em conceitos formais.
Este estudo é apresentado na Seção 7.1.
Apresentamos ainda, nesse capítulo, o estudo das classes de verbos signicativas do ponto de vista quantitativo.
Estudamos os verbos e os papéis semânticos associados a essas classes, respectivamente, nas Seções 7.2 e 7.3.
Para analisarmos a viabilidade de uso da extensão RCA, criamos um pequeno exemplo focado no papel semântico Agent.
Com o propósito de delimitar o contexto semântico do exemplo, usamos o termo mais frequente do corpus Penn TreeBank Sample -- company -- como semente inicial.
Aplicamos a essa semente, então, o &quot;operador mais «(comentado na Seção 3.6.3).
Para reduzir a complexidade da estrutura FCA resultante e centralizar a análise no papel Agent, estabelecemos restrições ao &quot;operador mais».
perador só recuperaria tuplas do tipo verboargumento1-argumento2, em que:
Um dos argumentos fosse a semente, esse argumento estivesse anotado com o papel Agent, e sua relação com utro argumentcorresse ao menos 3 vezes no corpus.
Cabe mencionar que, para simplicar nossa análise, consideramos, no exemplo, apenas unigramas como argumentos.
A partir de as tuplas selecionadas por as aplicações sucessivas do &quot;operador mais», o qual utilizava os argumentos recuperados como novas sementes, criamos dois contextos formais.
Um Construímos também o contexto relacional &quot;is_ agent_ of «que conecta os objetos em G conforme tal relação.
Cabe destacar que a relação &quot;is_ agent_ of «não é simétrica e nem reflexiva.
Ela foi definida formalmente como c R is_ agent_ ofc 1 2, onde os conceitos c, c B (G, M, I).
Os contextos assim definidos e a estrutura RCA gerada a partir de eles são apresentados, respectivamente, nas Figuras 7.1 e 7.3 a..
A estrutura RCA foi gerada com a ferramenta ERCA (Seção 5.3.5).
Em seguida, para que pudéssemos comparar a estrutura FCA com sua extensão RCA, criamos um novo contexto formal.
Este novo contexto (Figura 7.2) foi definido a partir de o &quot;contexto formal company».
Ele possui um atributo a mais:
A relação &quot;is_ agent_ of_ share».
A estrutura FCA correspondente a esse contexto é apresentada na Figura 7.3b e foi construída com a ferramenta Concept Expert (Seção 5.3.5).
Analisando as estruturas, percebemos que foi gerado coincidentemente o mesmo número de conceitos.
Como já esperavámos, exceto por o Concept_ 9, todos os demais conceitos da estrutura RCA têm um conceito correspondente na estrutura FCA.
A grande diferença, que inclusive já foi comentada na Seção 3.4, é que, no RCA, is_ agent_ of é uma relação entre conceitos e, no FCA, is_ agent_ of_ share expressa uma relação entre share e os demais objetos.
Como, nesse exemplo, existe Agent somente para share, apenas um atributo com esse m foi criado.
No entanto, se mais agentes existissem para os demais objetos, cada uma essas relações teria que ser especicada como um novo atributo no contexto formal da estrutura FCA.
Cabe mencionar que, com papel semântico Agent foi especicado com um atributo, formalmente, não há uma relação dos demais objetos com bjeto share.
Share, nesse caso, é simplesmente parte do rótulo do atributo Agent.
Já no caso RCA, para incluir mais agentes, bastaria modicar o contexto relacional is_ agent_ of assinalando tais relações.
Apesar de essa facilidade e da aparente conveniência de estabelecer relações entre conceitos, a estrutura estabeleceu generalizações da relação is_ agent_ of que nos pareceram propícias a erros de interpretação.
Considerando que o Concept_ 9 é formado por os objetos company shareholder, e o Concept_ 1, por os os objetos share shareholder, ao determinar que o Concept_ 9 é agente de Concept_ 1, não é explícito que essa relaçãriginalmente valeria apenas para share.
Apesar de o quanticador especicar que a relação não se aplica a todos os objetos, a estrutura RCA resultante não deixa claro, por exemplo, que company não desempenha o papel de agente no contexto semântico de shareholder.
Se os objetos presentes num mesmo conceito fossem sinônimos, a generalização de uma relação entre objetos para uma relação entre conceitos seria oportuna.
No entanto, coms atributos usados são verbos, os grupos de objetos de um conceito, na maioria das vezes, não são formados por sinônimos, apenas por elementos que pertencem a um mesmo contexto semântico.
Para decidir por a aplicação do método RCA, nos parece mais adequada a utilização de uma variedade maior atributos que possam especializar ainda mais os conceitos.
Isso implicaria o uso de um número maior de características e não apenas de verbos, como é o caso de nosso estudo.
Mesmo conscientes de que examinamos apenas um pequeno exemplo, acreditamos que os possíveis erros de interpretação decorrentes dessa generalização são especialmente preocupantes em caminhamentos automáticos sobre a estrutura conceitual.
Visto que esse é um dos nossos objetivos, resolvemos não usar a extensão RCA em nossa pesquisa.
Em as próximas seções analisamos as classes de verbos 37.7 e 45.4.
A classe 37.7 da VerbNet, como já mencionado, é a mais frequente no corpus Penn TreeBank Sample.
Ela é composta essencialmente por verbos de comunicação, o que justica a sua alta frequência, tendo em vista que se trata de um corpus jornalístico.
Durante o pré-processamento dos textos, encontramos 18 verbos associados a esta classe.
A Tabela C. 3 do Apêndice C contém esses verbos e suas respectivas frequências.
No entanto, após a extração dos sintagmas nominais e descarte de argumentos, este número caiu para 12 verbos e a quantidade de instâncias foi reduzida a quase um terço, ou seja, 544.
Apesar de a redução, a proporção de distribuição dos verbos em instâncias foi mantida, e foi sobre esses dados que realizamos nosso estudo.
Para simplicar a análise, consideramos apenas unigramas como argumentos dessas instâncias.
Iniciamos o estudo, vericando a distribuição dos argumentos em papéis semânticos.
Segundo dados da VerbNet, os verbos dessa classe costumam associar 3 tipos de papéis semânticos aos seus argumentos:
Agent, Topic e Recipient.
Os papéis Agent e Recipient são usados para argumentos que representam algo animadu uma organização.
Coms verbos dessa classe são de comunicação, o papel Agent geralmente indica o elemento emissor, o papel Recipient, o receptor e o papel Topic, o assunto dessa comunicação.
A Tabela 7.1 exibe os verbos e a respectiva distribuição das 544 instâncias em papéis semânticos para esta classe.
Observando essas instâncias, percebe- se que o papel Agent é o mais frequente.
Ele aparece associado a 464 argumentos.
O segundo mais frequente é o Topic, com 77 anotações.
E, por m, o papel Recipient aparece associado a apenas 3 argumentos.
Essa distribuição era esperada, pois é natural que, num corpus jornalístico, existam mais declarações, ou seja, mais elementos emissores do que receptores.
Os sintagmas assim anotados correspondem, em sua maioria, a funções, cargos e prossões que expressam &quot;denominações», mais usuais em jornais, com caso de source, que é usado 20 do texto wsj_ 0093).
Os agentes ainda podem ser entidades, como company, por exemplo.
Há casos, porém, em que os agentes são entidades, mas a extração isolada da relação verboargumento não permite tal interpretação.
Esse é o caso de statement.
Em geral, refere- se ao sentença 32 do texto wsj_ 0109).
De acordo com as amostras que analisamos, o papel Topic parece indicar elementos do domínio Finanças que são tópico de alguma discussão, como rate, por exemplo.
No entanto, percebemos que nossas heurísticas para extração dos sintagmas nominais para esse papel semântico não foram muito efetivas.
Vericamos que o papel Topic era constantemente atribuído a segmentos longos de sentenças.
Logo, a simplificação que zemos não foi adequada para extrair informações relevantes para esse papel.
Sendo, inclusive, uma das principais razões para a presença de termos pouco signicativos, como term concern, e sob a etiqueta Topic.
Para o papel Recipient, no entanto, não realizamos análise dado ao pequeno número de termos associados a essa etiqueta semântica.
Como já mencionado, esta classe também é muito frequente no corpus Penn TreeBank Sample.
Seus verbos descrevem ações comuns ao domínio de Finanças, tais como:
Tpen, to close, to improve, to increase, etc..
Encontramos inicialmente 332 instâncias de 73 verbos dessa classe (Tabela C. 3 do Apêndice C).
Após o pré-processamento, foram descartadas 20% dessas instâncias, restando 265.
Mesmo assim, a variedade de verbos ainda se manteve, embora a quantidade tenha caído para 61.
De acordo com a VerbNet, os verbos dessa classe costumam associar aos seus argumentos os papéis semânticos:
Agent, Patient e Instrument.
No entanto, não encontramos nas 265 instâncias analisadas, argumentos anotados como Instrument.
Por outro lado, tivemos 2 argumentos anotados com o papel Theme.
O papel mais frequente nesta classe foi Patient.
Este foi associado a 215 argumentos.
Os 48 argumentos restantes foram etiquetados como Agent.
A Figura seus argumentos.
Em seguida, analisamos os sintagmas nominais com base nas estruturas FCA (Figura 7.6) geradas para aqueles cuja frequência em relações verbo-argumento era superior a 1.
Considerando informações da VerbNet e observando a estrutura FCA da Figura 7.6 b, o papel Agent é associado geralmente a elementos que exerçam algum tipo de controle ou inuência sobre outros design automation that will improve mechanical engineering productivity. «(
sentença 0 do texto wsj_ 0055).
Já os sintagmas anotados com o papel Patient são aqueles que sofrem alguma modificação decorrente da ação dos agentes, tal como market em «South Korea has opened its market to analisar apenas unigramas, alguns desses sintagmas, no entanto, caram pouco informativos como use condition.
De a mesma forma que use condition, e o unigrama demand, quando analisado de forma isolada, é igualmente pouco signicativo.
Além de o papel semântico Patient, ele também foi associado ao papel Theme.
No entanto, no caso deste último papel, demand aparece apenas como argumento do verbo to taper.
Dada a reduzida amostra do papel Theme para classe 45.4, sua análise não teve prosseguimento.
Embora tenhamos descartad uso da estrutura RCA em nossa pesquisa, acreditamos que tal extensão deva ser estudada em maior profundidade.
Possivelmente, a inclusão de mais relações tais como do tipo substantivo-adjetivu mesmo substantivo-advérbio possam contribuir na qualificação dos conceitos formais.
Essa suposta qualificação pode tornar a generalização de relações entre objetos para relações entre conceitos, proposta por o método RCA, mais interessante para a construção de estruturas conceituais.
Um problema que observamos, no entanto, no uso dessa extensão é a escassez de ferramentas.
Como mencionado na Seção 5.3.5, encontramos apenas uma única ferramenta que gera estruturas conceituais do tipo RCA, a ERCA.
No que se refere às classes VerbNet estudadas, observamos que elas delimitam os papéis semânticos.
Conforme a classe, apenas determinados papéis são associados aos argumentos dos verbos.
Por esta razão, acreditamos que as classes VerbNet podem ser um caminho de pesquisa interessante para responder às questões elencadas no aspecto &quot;identificação das relações não taxonômicas», apresentado na Seção 6.2.
Para isso, seria necessário estudar métodos e heurísticas capazes de classicar os verbos uma vez que VerbNet não é completa, ou seja, ela não possui classes para uma grande parte dos verbos.
De posse de um classicador com esse m e com o auxílio dos etiquetadores de papéis semânticos seria possível estudarmos as relações entre papéis e sua relevância para diferentes domínios.
Quanto a a questão de usar as classes de verbos para rotular as relações transversais (questão também mencionada no aspecto &quot;identificação das relações não taxonômicas&quot;), precisaríamos associar, às denominações numéricas atuais, rótulos textuais.
Embora essa ideia seja interessante, acreditamos ser difícil sua realização de forma automática.
Para que os rótulos façam sentido teriam que ser definidos conforme o domínio e, nesse caso, a sugestãu mesmo crítica humana são mais necessárias.
Por outro lado, como estudamos apenas duas classes, não conseguimos enxergar por completo a complexidade envolvendo a definição desses rótulos.
É possível que a relação estabelecida entre os papéis de uma mesma classe não seja tão clara.
De qualquer forma, acreditamos que caiba investigar tal possibilidade.
Continuamos nosso estudo analisando, no capítulo seguinte, a contribuição dos papéis semânticos e das classes de verbos às estruturas FCA.
Este capítulo apresenta o estudo realizado para determinar a contribuição dos papéis semânticos e das classes de verbos às estruturas do tipo FCA.
Com esse m investigamos de forma exploratória o modo como a informação sobre os papéis semânticos e classes de verbos pode ser incluída em estruturas do tipo FCA.
Para isso, definimos algumas congurações de contextos formais com e sem essas informações semânticas.
Essas congurações, as quais chamamos de casos de estudo, são descritas na Seção 8.1.
A forma de seleção de características para construção dessas congurações, bem com método de avaliação utilizado na análise comparativa entre essas congurações, são apresentados na Seção 8.2.
Já a análise em si é comentada nas Seções 8.3, 8.4 e 8.5.
Em esta Seção detalhamos as congurações iniciais dos contextos formais que decidimos investigar.
Essas congurações foram organizadas em 6 casos de estudo.
A diferença entre os casos está basicamente na escolha das informações semânticas que são usadas e, ainda, na forma como essas informações são representadas.
bjetivo é, por meio desses casos, analisar a contribuição dos papéis semânticos e das classes dos verbos para a construção dos conceitos formais e também estabelecer, entre as estudadas, a representação que provê uma estrutura conceitual com conceitos mais coesos.
Para viabilizar a comparação das estruturas FCA geradas a partir de os contextos definidos por esses casos, usamos, em todos eles, o mesmo conjunto G de objetos.
Com bjetivo de facilitar o entendimento das congurações desses contextos formais, utilizamos, nos exemplos os sintagmas nominais constituídos por substantivos comuns.
Descrevemos, a seguir, os 6 casos estudados:
Caso 1 (sn, v):
Em este caso, a estrutura FCA é construída da forma mais &quot;tradicional», seguindo trabalhos com de Cimiano em.
Logo, essa estrutura não inclui qualquer informação semântica.
Para definir seu contexto formal, usamos os verbos, correspondentes aos sintagmas nominais em G, como atributos formais, construindo, assim, o conjunto comparar a estrutura conceitual gerada a partir de ele com as geradas por os demais casos, que incluem, em suas representações, informações semânticas.
Desta forma, poderíamos medir a contribuição da classe dos verbos e dos papéis semânticos na definição dos conceitos formais.
A Figura 8.1 apresenta um exemplo de contexto formal e da respectiva estrutura FCA para este caso de estudo.
Junto a cada caso, especicamos uma legenda em subscrito que identica a relação de incidência Ida estrutura FCA.
Em essas legendas, utilizamos abreviações:
Sncorresponde a sintagma nominal;,
a verbo;
Que utiliza os papéis semânticos como se fossem classes.
Apesar de melhor entendermos papéis semânticos como relações transversais entre conceitos, experimentamos a proposta de Wille e definimos os papéis semânticos VerbNet, correspondentes aos sintagmas de G, como atributos.
Desta forma, no contexto formal deste caso, o conjunto de e da estrutura FCA correspondente são apresentados na Figura 8.2.
Em. Os contextos foram definidos a partir de relações de incidência do tipo (noun, VerbNetSemanticRole_ of_ noun).
Para isso, consideramos apenas os verbos e argumentos que aparecem numa mesma sentença de um texto.
Por exemplo, para a tupla 2 analyst (Agent) say-- dividend (Topic), formada por o verbo to say e seus argumentos e Logo, as relações G × M caso3 (analyst, agent_ of_ dividend) e (dividend, topic_ of_ analyst) são elementos da relação de incidência Icaso3.
A Figura 8.3 apresenta um exemplo desse contexto formal e de sua respectiva estrutura FCA.
Esta tupla é um dos exemplos apresentados na Tabela C. 6 do Apêndice C. Em o caso 3.
Logo, o conjunto de atributos para este caso corresponde a M caso6 $= caso3 caso4 A Figura 8.5b mostra um exemplo de estrutura FCA para este caso.
Em a Seção seguinte, apresentamos a forma de seleção das relações incluídas em nosso estudo, bem como as medidas de avaliação estrutural utilizadas.
Antes de iniciarmos a avaliação dos casos de estudo mencionados, analisamos algumas formas de seleção a serem aplicadas às relações entre os verbos e seus argumentos.
Nossbjetivo era escolher, de entre essas relações, as quais estruturamos na forma de tuplas, aquelas cujas informações fossem mais representativas para o domínio de Finanças.
Estabelecemos, então, 8 formas de seleção distintas.
Em algumas formas, não usamos sementes;
E, em outras, as usamos para iniciar o processo de recuperação efetuado por o &quot;operador mais».
Variamos, também, os pontos de corte, usando valores de 2 a 5.
Em as congurações que não envolviam sementes, os pontos de corte foram usados para determinar a frequência mínima dos argumentos (sintagmas nominais) existentes nas relações.
Em as que eram direcionadas por sementes, os pontos de corte restringiram o processo de recuperação do &quot;operador mais».
Ele se limitava a buscar tuplas em que os argumentos relacionados às sementes possuíam a frequência mínima indicada por o corte.
Usamos como sementes iniciais aquelas definidas na Seção 6.4.3.
Cabe mencionar ainda que, durante a análise dessas congurações de seleção, não consideramos as informações semânticas existentes nas tuplas.
Uma vez definidas as listas de termos resultantes de cada processo de seleção, comparamos seus termos aos nomes das classes existentes na ontologia LSDIS Finance (descrita na Seção uma ontologia para um conjunto de termos previamente informado.
Ela contabiliza e pondera casamentos exatos e parciais entre os rótulos das classes e os termos informados.
Em o cálculo dessa métrica, usamos os pesos sugeridos por os autores Alani e Brewster em, que aplicam o peso 0, 6 para casamentos exatos e 0, 4 para parciais.
Cabe ressaltar que não aplicamos a medida CMM também para a ontologia Finance (descrita na Seção 5.2), pois na época em que esse estudo foi realizado ainda não tinhamos encontrado tal ontologia.
Além disso, julgamos a quantidade de classes disponíveis na LSDIS Finance adequada para essa investigação.
Em a Tabela 8.1 apresentamos as formas de seleção que testamos e os valores calculados para a métrica CMM LSDISem relação à ontologia LSDIS Finance.
Decidimos aplicar as congurações de seleção que obtiveram os 4 melhores valores para CMM LSDIS:
1, 2, 3 e 5.
A configuração 1, que utiliza como ponto de corte o valor 2 e não faz uso de sementes, foi a que obteve maior quantidade de casamentos com os termos da ontologia testada.
Considerando a totalidade de casamentos de termos (exatos+ parciais), a ontologia LSDIS Finance para tal configuraçãbteve cerca de 53% elementos coincidentes.
Como comentamos na Seção 2.6, a avaliação de estruturas conceituais, embora bastante pesquisada, ainda não é um tema consolidado.
Quando avaliamos estruturas baseadas em FCA, aumentam as diculdades pois esse tipo de investigação é mais recente.
Encontramos apenas duas medidas para esse tipo de avaliação, as quais foram estudadas na Seção 3.7.
Visto que nossa meta era analisar os conceitos formais sob um olhar semântico, das duas medidas estudadas, apenas a medida Sim (Seção 3.7.2) atende a nosso propósito.
Entretanto, As tuplas seguem o formato apresentado na Tabela C. 6 do Apêndice C. A relação lexical de hiperonímia, também chamada de &quot;é-superior-a, «é-superclasse-de (superordinate), denota que um conceito é superclasse de outro.
Portanto, nessa relação binária, existe um item que funciona como um protótipo (uma classe) que representa a generalização de outros itens mais especícos.
Por exemplo, &quot;ave «é hiperônimo de canário e &quot;pato».
A hiponímia é inversa à hiperonímia.
Em esse caso, &quot;canário «é hipônimo de ave.
Ela o atende de forma parcial, pois os atributos dos contextos formais de nossos casos de estudo são de tipos diferentes, difíceis de serem comparados.
A diculdade de comparar, por exemplo, a classe de um verbo com um papel semântico, inviabilizou o uso dessa medida.
Focamos nossa análise, então, nos objetos formais.
Precisávamos de uma medida de ordem estrutural que avaliasse semanticamente os grupos de objetos de um conceito formal.
Assim, poderíamos vericar quais congurações geravam agrupamentos cuja relação semântica entre os objetos era mais representativa.
Usamos, para isso, a medida de similaridade semântica, SSM, que calcula quão próximos estão, numa determinada ontologia, os conceitos que casam exatamente ou parcialmente com termos informados.
Como a métrica foi aplicada aos objetos formais de cada conceito das estruturas FCA analisadas, a SSM acabou funcionando como uma espécie de medida de coesão lexical.
Halliday e Hasan usam o termo coesão para se referir &quot;a as relações de signicado que existem dentro de um texto».
Segundo esses autores, a coesãcorre quando a interpretação de um elemento é dependente de outro elemento do discurso.
É expressa tanto por meio de a gramática quanto do vocabulário.
Sendo neste último caso chamada de coesão lexical, a qual analisa a relação semântica entre as palavras do texto.
A coesão lexical toma como base relações como sinomínia, hiponímia, meronímia e antonímia para determinar as relações de sentido entre as palavras do texto.
Trabalhos com de Teike e Fankhauser, ainda que com m diferente, usam a WordNet para medir a coesão lexical.
Teike e Fankhauser têm combjetivo auxiliar a anotação de textos identicando automaticamente n-gramas cujos elementos estão mais fortemente relacionados.
A coesão lexical é determinada com base no comprimento do menor caminho, existente na hierarquia WordNet, entre os synsets dos termos sob análise.
A exemplo de trabalhos desse tipo, empregamos uma medida, comumente aplicada à WordNet, para determinar a coesão lexical dos objetos de um conceito formal.
A medida escolhida foi a definida por Wu e Palmer.
A decisão por sua utilização se baseia em duas razões.
A primeira é que ela foi mencionada, por os autores de SSM em, como uma das medidas que poderiam ser usadas no cálculo da métrica.
Outra razão é que o pacote NLTK (Seção sua interpretação.
Optamos também por aplicar a métrica SMM em relação a a ontologia LSDIS Finance e essa decisão igualmente teve duas razões.
A primeira é que, apesar de a extensão e da riqueza em relações da base WordNet, tais relações não se referem a um domínio especíco.
Como esse é o nosso caso, imaginamos que a medida Wu e Palmer, aplicada à estrutura WordNet, poderia não capturar a relação semântica esperada e gerar valores menos expressivos.
A segunda é que, embora na ontologia LSDIS Finance, o conjunto de conceitos seja menor, é mais usual conceitos rotulados com n-gramas n\&gt; 1) (e as relações entre esses conceitos são de domínio.
Esses fatores podem gerar resultados semanticamente mais signicativos quanto a a qualidade dos agrupamentos (conceitos).
No caso de a ontologia LSDIS Finance, além de implementar a métrica SSM, tivemos que codicar também o cálculo da medida de Wu e Palmer para tal estrutura conceitual.
Essa implementação foi realizada em Java e segue as fórmulas apresentadas a seguir.
A medida SSM E, expressa na Equação 6.1, indica a coesão lexical média dos N conceitos de uma estrutura FCA em relação a uma estrutura conceitual E. Já a medida ssm i (Equação 6.2), calcula a similaridade do conjunto de objetos G de um conceito i de uma estrutura FCA, com base na medida de Wu e Palmer (wup).
Caso esse conjunto G possua cardinalidade igual a 1, a medida ssm wup i resulta zero.
Por m, a medida E, apresentada na Equação 6.3, estima a similaridade entre os conceitos c a 1 e 2 numa estrutura E. Em essa equação, corresponde ao ancestral comum e mais especíco dos conceitos c p, 1 e 2;
a a profundidade de um nodo qualquer, ou seja, o comprimento do caminho (em nós) desse nodo ao nodo raiz;
E, à menor distância (em nós) d de c 1 a 2.
Analisamos também a relação entre a cardinalidade do conjunto de atributos de cada estrutura com a quantidade de conceitos formais produzidos e, ainda, a altura e a largura dessas estruturas.
Outro elemento avaliado é a quantidade de arestas dessas estruturas:
Quanto maior for esse valor, maior será a complexidade na construção do reticulado correspondente.
Em as seções seguintes são apresentadas as análises realizadas para os casos de estudos descritos na Seção 8.1.
Cabe comentar, que para a realização desse estudo, geramos várias estruturas FCA.
Os grafos referentes a essas estruturas foram implementados em linguagem Java.
Usamos para iss algoritmo Bordat (comentado na Seção 3.5.2), o qual foi escolhido por a sua simplicidade de implementação.
Como já mencionado, também implementamos em Java a leitura da ontologia citada e a maioria das medidas estruturais utilizadas na avaliação das estruturas FCA.
A ferramenta Concept Expert (Seção 5.3.5) foi utilizada apenas para visualização dos reticulados de conceitos.
Em esta Seção apresentamos o primeiro estudo comparativo referente a os 6 casos apresentados na Seção 8.1.
Em essa análise, usamos as medidas estruturais e as 4 formas de seleção escolhidas na Seção anterior.
A o gerar os contextos formais dos casos, variamos ainda os papéis semânticos VerbNet considerados nesses contextos.
Nossbjetivo era investigar a inuência dos papéis semânticos na formação dos conceitos formais.
Analisamos duas congurações com esse m..
Uma incluindo apenas os 4 papéis semânticos mais frequentes (Agent, Theme, Patient e Topic) Dados complementares a essa análise são apresentados no Apêndice D. A Tabela 8.2 descreve os resultados referentes à medida estrutural SSM para os 6 casos estudados.
Esses resultados foram produzidos a partir de a forma de seleção 1, em a qual não usamos sementes e realizamos o ponto de corte com o valor 2.
Além disso, os contextos formais desses casos foram gerados considerando apenas os 4 papéis semânticos mais frequentes.
Em essa tabela são apresentados dados quanto a o número de objetos e atributos de cada contexto formal analisado.
Incluímos também na tabela, a quantidade de conceitos formais gerados e as medidas SSM calculadas.
A medida SSM W corresponde à coesão lexical relativa à base WordNet;
E a SSM, à coesão lexical relativa à ontologia LSDIS Finance.
A última coluna da tabela apresenta a média aritmética dessas 2 medidas.
Observando- se os dados da Tabela 8.2, podemos perceber que apenas os casos 3 e 6, que contêm a relação (sn, psV_ sn), obtiveram, na média, coesão lexical inferior à do caso 1 (sn, v).
O caso 3 (sn, psVsn) foi, inclusive, o que gerou conceitos com mais baixa coesão.
Isso aconteceu em decorrência da especicidade dos atributos da forma VerbNetSemanticRole_ of_ noun (psV_ sn).
Poucos objetos compartilhavam tais atributos, o que acabou produzindo muitos conceitos cuja cardinalidade do conjunto de objetos era 1.
A Tabela 8.3, que apresenta os demais dados estruturais (número de arestas, altura e largura média da estrutura FCA) para mesma forma de seleção e quantidade de papéis, mostra que 58,3% dos conceitos formais gerados para o caso 3 possuíam conjuntos unitários de objetos.
Em a tabela chamamos a esses conceitos simplesmente de &quot;unitários».
O caso 6, além de apresentar uma coesão mais baixa que o 1, também apresentou outro inconveniente que foi a quantidade de arestas.
Ele possuía aproximadamente o dobro das arestas do caso 1.
A quantidade bem maior de atributos do caso 6 em relação a o 1, aumentou a complexidade do reticulado e, consequentemente, o processamento computacional.
O caso 5 apresentou o mesmo problema.
Apesar de conter cerca de 30% de atributos a menos que o 1, também produziu muitas arestas.
Por outro lado, percebe- se que a combinação de classes de verbos e papéis semânticos resultou numa estrutura conceitual mais especializada.
De entre as estruturas, a do caso 5 é a de maior altura:
7. Analisando- se os casos 4, 5 e 6, podemos notar que a presença das classes de verbos (cV) como atributos melhora a coesão lexical.
Comparando- se o caso 4 com o 1, percebe- se que, além de a melhora na coesão, houve uma redução:
Em o número total de conceitos (maior agrupamento), no número de conceitos com conjunto unitário de objetos (melhor agrupamento) e na quantidade de arestas (menor processamento).
De todos os casos analisados, o caso 2 foi o que obteve maior índice de coesão lexical.
No entanto, concentrou os objetos em poucos conceitos.
A generalidade de seus atributos, que são papéis semânticos (psV), deve ser a razão da alta coesão.
A presença de mais objetos no mesmo conceito formal aumenta a quantidade de combinações de pares de objetos que são submetidos à medida de similaridade de Wu e Palmer.
Visto que esses objetos possuem alguma relação semântica (no mínimo a definida por o próprio papel semântico), a similaridade resultante acaba sendo maior.
No entanto, ao incluirmos todos os papéis semânticos nos contextos formais dos casos de estudo, mesmo mantendo a forma de seleção, percebemos uma redução na coesão lexical do caso 2.
Essa queda na coesão deve estar relacionada ao acréscimo de atributos e à baixa frequência da maioria de eles.
Em geral, mais atributos provocam uma maior distribuição dos objetos em conceitos.
Em esse caso especicamente, aumentou não apenas a quantidade de conceitos mas também a de conceitos com conjunto unitário de objetos, que foi para 25% (Tabela D. 7 do Apêndice D).
Já para o caso 5 tal inclusão resultou numa melhor coesão lexical, ainda que com os mesmos problemas relatados anteriormente.
Tais variações na coesão lexical não são decorrentes apenas da inclusão de mais papéis semânticos mas também do tipo de seleção utilizada.
Para realizarmos, então, uma análise mais abrangente construímos alguns grácos para avaliar determinados aspectos das estruturas conceituais geradas a partir de os 6 casos.
Iniciamos estudando a medida SSM.
Para isso, criamos dois grácos usando as médias SSM apresentadas nas tabelas desta Seção e nas tabelas do Apêndice D. O gráco da Figura 8.6 mostra o comportamento da média SSM para os 6 casos.
Em esse gráco, os contextos formais foram gerados para as 4 formas de seleção e incluem apenas os 4 papéis semânticos mais frequentes.
Já no gráco da Figura 8.7, os contextos formais incluem todos os papéis semânticos encontrados nas tuplas selecionadas.
Analisando- se esses grácos, observamos que o caso 3 (sn, psVsn) independentemente da forma de seleção e número de papéis, gerou a coesão lexical mais baixa.
De a mesma forma, o caso 6 (sn, psVsn)+ (sn, cV) também não apresenta médias que superem as do caso 1 (sn, v), embora inclua mais informações semânticas em seus contextos formais.
Essa constância se repete igualmente para o caso 2 (sn, pV) que, mesmo com tais variações, mantém seus índices de coesão lexical superiores aos do caso 1 (sn, v).
O caso 4 (sn, cV) e o caso 5 (sn, psV)+ (sn, cV), apesar de gerarem conceitos mais coesos que o caso 1 (sn, v), ao terem seus contextos alterados por a inclusão de mais papéis semânticos, para maioria das formas de seleção, não apresentam melhora signicativa da medida SSM.
No entanto, para forma de seleção 5, esses casos produzem resultados acima de os demais na presença de uma variedade maior de papéis em seus contextos formais.
Visto que ambos os casos incluem classes de verbo e esta forma de seleção utiliza o &quot;operador mais», é possível que, ao incluirmos mais papéis, esse operador tenha recuperado um número maior de tuplas cujos verbos possuem mais classes em comum.
Ainda com o m de analisar os agrupamentos, observamos também os percentuais de conceitos com conjuntos unitários de objetos.
Para isso, geramos os grácos das Figuras 8.8 e 8.9 que correspondem, respectivamente, a congurações com os 4 papéis mais frequentes e com todos os encontrados nas tuplas selecionadas.
Percebemos que o comportamento dos casos em ambos os grácos é muito semelhante.
A redução do número de papéis afeta signicativamente apenas o caso 2 (sn, psV).
Esse caso na presença de um número menor de papéis, que são seus atributos, gera um percentual menor ou nulo de conceitos com conjuntos unitários de objetos.
Entretanto, de maneira geral, o que inuência diretamente a quantidade de unitários é a forma de seleção utilizada.
Em ambas as situações, a forma de seleção 5, que utiliza sementes, produziu um menor índice de unitários.
Com base nas tabelas do Apêndice D, complementamos a observação de que independentemente do número de papéis e da forma de seleção utilizados, os casos 5 e 6 continuam, indesejavelmente, a gerar estruturas FCA com uma maior quantidade de arestas.
Cabe mencionar que analisamos também as relações não taxonômicas existentes na ontologia LSDIS Finance.
Conrmando a literatura, relações desse tipo constavam em menor número nessa ontologia.
Em a LSDIS Finance, encontramos 50 relações funcionais e 68 relações descrevendo propriedades.
Vericamos, então, se os conceitos formais provenientes dos casos estudados coincidiam com tais relações.
Não encontramos nenhum casamento &quot;perfeito «entre os conceitos formais e essas relações.
Com bjetivo de tentar melhorar os resultados, principalmente do caso 3 (sn, psV_ sn), incluímos algumas heurísticas no pré-processamento dos contextos formais dos casos estudados.
A exemplo do trabalho de Otero em, que inspirou a configuração do caso 3 (sn, psV_ sn), aplicamos uma heurística para agrupar atributos similares, baseada no coeciente Dice.
Essa medida é apresentada na Equação 6.4, onde f corresponde à frequência absoluta, min determina o menor de 2 valores e n é a quantidade de objetos compartilhados por os atributos 1 e 2.
Para cada atributo analisado, são geradas novas relações a partir de os seus k vizinhos (atributos mais semelhantes de acordo com a medida Dice).
Otero utilizaram k $= 5.
Nós testamos os valores 4, 5 e 6 para k.
Estabelecemos como similares aqueles atributos cuja medida gerou valores do intervalo.
As novas relações foram definidas a partir de os objetos que não eram compartilhados originalmente por os atributos considerados semelhantes.
No caso de os atributos 1 e 2 serem similares, ao existir a relação (objeto, atributo) objeto j 1 em que o j não é compartilhado por o atributo 2, a relação (objeto, atributo) j 2 é criada.
Aplicamos essas heurísticas apenas aos 4 primeiros casos de estudo.
Desconsideramos os casos 5 e 6 por terem produzido muitas arestas em suas estruturas FCA.
Grandes quantidades de arestas são indesejáveis computacionalmente.
Em todos os testes realizados usamos a forma de seleção 5, que usa utiliza sementes e vários papéis semânticos, pois, de acordo com os resultados apresentados na Seção anterior, foi a que gerou menor quantidade de unitários.
Excluímos ainda atributos menos frequentes.
Testamos 3, 4 e 5 como valores de corte para m, o qual corresponde à frequência mínima exigida para os atributos.
Apesar de as heurísticas terem melhorado a medida SSM de todos os 4 casos, a heurística de agrupamento baseada no coeciente Dice foi mais efetiva para o caso 3 (sn, psV_ sn) (Tabela E. 3 do Apêndice E).
Para os demais casos, os valores de k não foram tão decisivos para os resultados de SSM.
Para esses casos o que prevaleceu foram os pontos de corte.
A Tabela 8.5 mostra os dados das melhores médias SSM obtidas para os 4 casos após a aplicação das heurísticas.
Todos os dados apresentados foram extraídos das tabelas do Apêndice E e correspondem à configuração em que k $= 4 m $= 5.
Após a aplicação das heurísticas, todos os 4 casos melhoraram as médias SSM.
A melhora mais expressiva foi a do caso 3 (sn, psV_ sn).
É interessante observar que, mesmo ainda com a menor média SSM, o caso 3 (sn, psV_ sn) gerou conceitos tão densos quant caso 2 (sn, psV) em número de objetos (cerca de 5 por conceito).
Outro aspecto a se considerar é que a coesão lexical dos conceitos desse caso, principalmente em relação a as ontologias de domínio, melhorou signicativamente.
De entre todos os casos, ele obteve o valor SSM mais alto para a ontologia LSDIS Finance.
O caso 4 (sn, cV) foi o que gerou conceitos mais coesos.
Observamos que tanto em relação a a WordNet quanto em relação a a ontologia LSDIS Finance, a medida SSM, para esse caso, obteve resultados expressivos.
O caso 1 (sn, v), na coesão média, foi tão bom quant caso 2 (sn, psV_ sn).
As heurísticas aplicadas aumentaram de forma signicativa a medida SSM em relação a a WordNet.
No entanto, para a ontologia de Finança, tal medida foi a mais baixa de entre os casos.
Isso pode ser um indicativo de que as informações semânticas incluídas nos conceitos formais têm uma contribuição relevante na captura de relações de domínio.
Analisando as estruturas, percebemos uma certa similaridade entre os conceitos.
No entanto, os atributos do caso 3 (sn, psV_ sn) (Figura 8.10 b) são mais informativos por serem baseados em relações.
Aparentemente, tais atributos conseguem delinear melhor a semântica do domínio por expressarem o contexto em que os papéis semânticos são aplicados.
Já os atributos do caso 2 (sn, psV) (Figura 8.10 a) formam grupos de objetos aparentemente mais abrangentes.
Talvez isso seja uma das explicações para o fato da medida SSM, quando aplicada à WordNet, gerar valores mais altos para este caso.
Por outro lado, os unigramas share, price, trade company e que aparecem nos atributos do caso 3 (sn, psV_ sn), não fazem parte do seu conjunto de objetos formais.
Eles devem ter sido descartados em razão de estarem associados a atributos (contextos lexicossemânticos) menos frequentes.
Em a prática, ainda mais após o uso das heurísticas, os atributos do caso 3 (sn, psV_ sn) não representam de fato relações entre objetos formais.
Mesmo assim, ainda podemos utilizar- los para qualicar os objetos.
Percebemos também que os papéis semânticos Cause e Predicate não aparecem na estrutura FCA do caso 2 (sn, psV), somente na do caso 3 (sn, psV_ sn).
Portanto, os casos 2 e 3 não se baseiam necessariamente nos mesmos papéis semânticos.
Em a próxima Seção, analisamos alguns papéis semânticos por meio de exemplos.
Em esta Seção estudamos de forma qualitativa o comportamente de alguns papéis semânticos.
As características definidas por os papéis semânticos aos argumentos as quais estão associados são mais evidentes quando analisamos o contexto em que eles estão inseridos.
Para estudar os papéis semânticos em relações, construímos a estrutura FCA com base no caso 3 (sn, psV_ sn) (nosso segundo exemplo).
A Figura 8.12 apresenta tal estrutura.
Cabe ressaltar que, para facilitar nossa análise, restringimos o conjunto de atributos, usando apenas aqueles que estabelecem relações com as sementes.
Os atributos, portanto, seguem o formato SemanticRole_ of_ company SemanticRole_ of share.
Em essa estrutura, o substantivo investment obviamente também está associado aos papéis Product e Agent, no entanto tais papéis possuem um contexto que é company.
Investment é um elemento que aciona eventos (Agent) e que está relacionado a transformações ou é o resultado de elas (Product) no contexto de company.
A relação entre as sementes nessa representação é mais explícita.
Company é um agente de share.
Share E é um elemento do contexto de company (Theme).
Há outras relações igualmente interessantes, como unitholder que aparece com recipiente de share (Recipient).
Há ainda os substantivos speculator, stockholder shareholder e que foram anotados como agentes, logo são interpretados como elementos que atuam sobre share.
Shareholder também desempenha o papel de Source de share.
Enquanto shareholder é a origem, market aparece com destino de share.
Os termos cent pence e também aparecem no contexto de share, indicando possivelmente seus valores de grandeza.
Cabe mencionar que, ao usarmos unigramas nos exemplos apresentados, alguns termos tornaram- se menos signicativos, como interest group.
Embora nosso estudo tenha sido realizado de forma pontual, visto que até agora nossa investigação tenha se restringido a um domínio (Finanças) e a um corpus desse domínio, acreditamos que os papéis semânticos possam, de fato, enriquecer as relações em estruturas conceituais.
Claro que nem todos os papéis provêem relacionamentos signicativos.
Theme e Topic, por exemplo, são muito genéricos.
A incidência do papel semântico Topic, no entanto, nos parece estar mais relacionada à natureza do corpus, que é de cunho jornalístico, do que ao domínio em si.
Em esse tipo decorpus, quando argumentos de um verbo numa sentença são anotados com os papéis semânticos Agent e Topic, as chances do argumento anotado com Agent ser uma pessoa ou uma instituiço aumentam.
Logo, o papel Agent pode ser usado para identicar as classes de entidades nomeadas.
A frequente presença do papel Agent nas relações identicadas entre argumentos anotados também nos parece estar mais ligada à natureza do corpus.
Conforme o gráco da Figura 8.13, que apresenta as 12 relações mais frequentes, Agent aparece em cerca de 50% de elas.
Sob o ponto de vista estrutural e lexical, observamos que a inclusão de informações semânticas nos atributos dos contextos formais, de maneira geral, teve como resultado conceitos formais mais coesos.
Em esse sentido, as classes de verbos, para configuração de contexto formal proposta no caso 4 (sn, cV), mostraram- se mais efetivas do que verbos.
As classes, além de aumentar a coesão lexical, ajudaram a reduzir a complexidade de construção do reticulado FCA, na medida em que geraram menos conceitos e arestas.
Já os papéis semânticos mostraram- se mais efetivos, ainda no aspecto coesão, principalmente quando a configuração de contexto formal proposta no caso 2 (sn, psV) foi utilizada.
Apesar desses resultados, a interpretação das estruturas assim geradas não é tãbjetiva quanto aquelas estruturas em que verbos são usados como atributos.
Sob o aspecto intensional, o uso de rótulos numéricos para as classes de verbos, bem com uso de papéis semânticos como classes e não como relações, tornam tais elementos, enquanto atributos, menos informativos do que os verbos.
Já a configuração de contexto formal descrita por o caso 3 (sn, psV_ sn), em a qual os papéis semânticos são utilizados como relações, apresenta atributos que nos pareceram mais descritivos intensionalmente, ainda que inicialmente (antes da aplicação das heurísticas descrita na Seção Quanto às formas de seleção estudadas na Seção 8.3, o uso de sementes em conjunto com o &quot;operador mais «mostrou- se um caminho interessante para gerar estruturas FCA.
Essa aborda126 gem, além de melhorar a coesão dos conceitos, também reduziu a complexidade de construção do reticulado FCA.
Cabe mencionar que não utilizamos, em nosso estudo, contextos formais constituídos por instâncias (substantivos próprios), pois o parser que construímos para extrair os sintagmas nominais (apresentado na Seção 6.4.2) as gerou num número muito reduzido.
Cabe destacar, também, que os estudos realizados nesse capítulo foram descritos num artigo cientíco ainda não publicado mas com a notificação de aceite para uma conferência internacional:
Language Resources and Evaluation.
Com bjetivo de analisar a aplicabilidade de nossa proposta, ainda para os casos de estudo estabelecidos, estudamos e apresentamos, no capítulo seguinte, a contribuição das informações semânticas na construção de conceitos formais por meio de medidas de avaliação de ordem funcional.
Este capítulo descreve a análise realizada para determinar a aplicabilidade de nossa abordagem em relação a outros corporaem Língua Inglesa.
Utilizamos para esse m os corpora no domínio Finanças (WikiFinance) e no domínio Turismo (WikiTourism).
Ambos foram extraídos do Wikicorpus 1.0 que contém textos da Wikipédia.
Por meio desses corpora pudemos analisar nossa proposta em relação a textos de natureza mais conceitual e em domínios diferentes.
Usamos esses textos para estudar a contribuição de informações semânticas na construção de conceitos formais a partir de a tarefa de categorização de textos.
Para avaliar os resultados nesta tarefa, usamos medidas funcionais usuais, como precisão e recall.
Tais medidas nos permitiram comparar os resultados gerados a partir de estruturas FCA aos produzidos por ontologias de domínio e por o algoritmo k--Nearest Neighbor (k-NN).
A o nal desse capítulo, apresentamos os estudos que realizamos para a Língua Portuguesa quanto a a extração de conceitos, à categorização de textos e à construção de estruturas conceituais do tipo FCA baseadas em papéis semânticos.
De acordo com Sebastiani em, &quot;categorização de textos é a tarefa de atribuir um valor j i onde é um domínio de documentos e 1 «C &quot;é um conjunto pré-definido de categorias».
Essa tarefa pode ser definida formalmente por meio A tarefa de categorização foi utilizada para avaliar a viabilidade de uso da nossa abordagem em outros corpora e domínios.
Convivemos com o fato dos etiquetadores de papéis semânticos serem menos precisos para domínios que não sejam Finanças e de anotarem os argumentos dos verbos com as etiquetas numéricas PropBank.
A anotação dos papéis semânticos através de rótulos numéricos nos obrigou a adaptar os contextos formais dos casos de estudo propostos no Cabe mencionar que escolhemos a tarefa de categorização de textos considerands trabalhos pesquisados que utilizam o método FCA nessa área.
Baseamos nosso estudo especialmente no trabalho de Meddouri e Meddouri que dene regras a partir de conceitos extraídos de estruturas FCA com bjetivo de classicar instâncias em classes.
Optamos por utilizar a abordagem de Meddouri e Meddouri principalmente porque poderíamos aplicar- la facilmente tanto para estruturas FCA quanto para ontologia de domínio no tange à extração das regras.
E, também, porque aqueles autores mencionam que tal abordagem é mais rápida que um classicador k-NN.
Por último, embora nossa meta seja avaliar as estruturas FCA de forma mais qualitativa, no que se refere à tarefa de classificação, desempenho computacional também nos interessa.
Por isso, comparamos os resultados obtidos a partir de nossa proposta aos de um classicador k-NN.
Visto que o propósito de Meddouri e Meddouri é diferente do nosso, tivemos que modicar a abordagem desses autores quanto a a extração de regras para que estas pudessem ser utilizadas na categorização de documentos.
Essas modicações são abordadas na Seção 9.1.3.
Os corpora utilizados em nossa investigação e as suas divisões em conjunto de treino e de teste sã assunto da Seção 9.1.2.
Já os resultados da categorização de textos realizada a partir de estruturas FCA, ontologias de domínio e de um classicador k-NN são apresentados, respectivamente, nas Seções 9.1.3, 9.1.4 e 9.1.5.
Já a comparação dos resultados obtidos nessas diferentes abordagens é apresentada na Seção 9.1.6.
É importante ressaltar, também, que zemos uso de classicadores single-label, os quais atribuem cada documento d c j a apenas uma categoria i.
Utilizamos essa forma de categorização visto que os textos dos corpora sobre os quais os classicadores atuam foram associados a apenas uma única categoria.
Para que pudéssemos utilizar as informações de corpora anotados com os rótulos numéricos ao estilo PropBank, tivemos que adaptar os contextos formais dos casos de estudo propostos.
Combservamos que os primeiros 4 casos, analisados no capítulo anterior, produziram reticulados de conceitos de menor complexidade computacional e com coesão lexical relevante, os escolhemos para este estudo.
Desta forma, procuramos adaptar apenas os contextos formais para:
Caso 1 (sn, v), caso 2 (sn, psV), caso 3 (sn, psV_ sn) e caso 4 (sn, cV).
O caso 1 (sn, v) não requereu qualquer adaptação, visto que não faz uso de informações semânticas.
Ele foi mantido, como já mencionado, para ns de comparação.
Já o caso 2 (sn, psV) não pode ser adaptado.
Não podemos simplesmente substituir etiquetas de papéis semânticos VerbNet por etiquetas PropBank, pois um mesmo identicador PropBank, quando relacionado a diferentes verbos, pode ter semântica distinta.
Só há garantia de uniformidade no signicado de etiquetas PropBank idênticas, se os verbos que as usam pertencerem à mesma classe.
Com bjetivo, então, de viabilizar o uso dos papéis PropBank, usamos o pacote NTLK (Seção 5.3.2) para acessar a VerbNet e buscar as possíveis classes dos verbos existentes nos corpora utilizados nesse estudo.
No entanto, tal procedimento não foi efetivo.
Conseguimos associar classes somente a aproximadamente 7% das instâncias dos verbos pertencentes a esses corpora.
Esse insucesso, decorrente possivelmente da incompletude da própria VerbNet, inviabilizou também a adaptação do caso 4 (sn, cV), em o qual os atributos são as classes VerbNet.
Diante de a ausência de classes, para a adaptação do caso 3 (sn, psV_ sn), nossa alternativa acabou sendo a substituição dos papéis semânticos VerbNet por papéis PropBank.
Diferentemente 1 do caso 2 (sn, psV), os atributos do caso 3 (sn, psV_ sn) são definidos por contextos lexicossemânticos.
Imaginamos que os sintagmas nominais usados em sua composição poderiam ajudar a distinguir o signicado dos papéis ainda que seus rótulos fossem idênticos.
Chamamos o contexto formal, resultante dessa adaptação, de caso 7 (sn, psP_ sn).
A Figura 9.1 apresenta um exemplo desse contexto formal e de sua respectiva estrutura FCA.
Os dados usados nesse exemplo foram retirados da Tabela C. 6 do Apêndice C. Em substituição às classes VerbNet, ainda tentamos associar aos papéis PropBank os seus respectivos verbos, constituindo contextos formais cujos atributos seguiam a forma PropBank SemanticRole_ Verb.
Coms testes preliminares com tais contextos formais não foram satisfatórios, desistimos de usar- los.
Usamos a abreviação psP para indicar a presença de papéis semânticos PropBank nas etiquetas em subscrito Desta forma, nos estudos sobre categorização de textos com estruturas FCA abordamos apenas:
Caso 1 (sn, v) e caso 7 (sn, psP_ sn).
Em o estudo de categorização apresentado nesse capítulo, utilizamos essencialmente textos Wikipédia extraídos do corpus Wikicorpus 1.0.
Como já comentado, utilizamos o corpus WikiFinance que possui 482 textos do domínio Finanças e o corpus WikiTourism que contém 442 textos do domínio Turismo.
Ambos foram anotados com papéis semânticos ao estilo PropBank por o processador F-EXT-WS (Seção 5.3.4).
Os termos anotados por o processador, por meio de as etiquetas POS, como adjetivos, substantivos e verbos foram, ainda, normalizados por o lematizador TreeTagger.
Para viabilizar o uso dos diferentes classicadores, usamos a abordagem train- and- test, conforme Sebastiani em, e separamos os textos em conjuntos de treino e de teste.
Como nossbjetivo principal era analisar a aplicabilidade de nossa proposta, não tivemos a preocupação de encontrar o melhor par de conjuntos treino e teste.
Nos preocupamos apenas em garantir que os mesmos conjuntos fossem utilizados por todos os classicadores.
Para que, desta forma, pudéssemos avaliar nossa abordagem e não exclusivamente a categorização em si.
Escolhemos, então, arbitrariamente, em torno de 65% dos textos para formar o conjunto de treino e 35%, para o conjunto de teste.
Essa separação dos textos foi realizada de forma sequencial e é apresentada na Tabela 9.1.
Como pode- se observar nessa tabela, adicionalmente, ainda criamos um segundo conjunto de teste (teste Wiki+ PTBS), unindo as amostras de treino do WikiTourism aos textos Penn TreeBank Sample (PTB Sample).
Criamos esse segundo conjunto de teste para analisar o comportamento dos classicadores, especialmente o que utiliza a nossa proposta, em corpora de natureza distinta.
Optamos por uma frequência baixa na seleção das relações para tentar obter maior quantidade de tuplas.
Nossbjetivo foi minimizar a perda de termos relevantes para a tarefa de classificação, ao construir os conceitos formais.
As Tabelas 9.2 e 9.3 apresentam dados estruturais dos reticulados de conceitos gerados.
Usamos nessa avaliação as medidas apresentadas na Seção 8.2.
Analisando- se os dados da Tabela 9.2, pode- se perceber que os textos do domínio Finanças produziram conjuntos de objetos e atributos com quase o dobro de elementos em relação a o domínio de Turismo.
Uma das razões pode ser o fato do conjunto de treino de Finanças possuir cerca de 12% de textos a mais.
Outra razão pode estar relacionada à variedade dos textos desses domínios.
Em uma análise mais rasa e subjetiva que zemos, percebemos que os textos do domínio de Turismo são menos abrangentes que os de Finanças.
Enquants de Turismo abordam temas mais constantes como pontos turísticos, os de Finanças abrangem desde descrições sobre os termos-chave do domínio a aspectos mais políticos, como a biograa de ministros da área de economia.
Notamos, a partir de os dados da Tabela 9.3, que as estruturas FinanceFCA 3 caso7, apesar de superarem em aproximadamente 70% a quantidade de atributos do caso1 (sn, v), têm apenas 7% de conceitos a mais e cerca de 7% de arestas a menos.
Outro aspecto interessante é que ao analisarmos as medidas de coesão lexical da estrutura FinanceFCA caso7, mesmo ainda menores que as da estrutura FinanceFCA caso1, seus valores são representativos visto que possuem mais unitários.
Esses aspectos podem indicar que os papéis semânticos da estrutura FinanceFCA caso7 contribuíram tanto para reduzir o número de grupos (proporcionalmente menos conceitos) quanto para melhorar a qualidade desses grupos (coesão lexical mais expressiva).
Após a análise das estruturas FCA, começamos, então, o processo de extração das regras a serem usadas na classificação.
Para isso, precisávamos:
Em a tabela 9.3, as legendas em subscrito referem- se às bases e ontologias para as quais as medidas CMM e SMM foram calculadas.
Desta forma, a abreviação:
W signica WordNet;
TG, ontologia de Turismo TGPROTON;
T, ontologia de Turismo Travel;
F, ontologia Finance;
E L, ontologia de Finanças LSDIS.
Essas ontologias foram comentadas na Seção 5.2.
Cabe lembrar que, &quot;unitários «corresponde aos conceitos cuja cardinalidade do conjunto de objetos formais é igual a 1.
De acordo com os trabalhos pesquisados, há vários critérios que podem ser usados para selecionar regras.
Optamos por um critério simples, mas usual, que é a generalidade de os conceitos.
No caso de as estruturas FCA, os conceitos mais &quot;gerais «do domínio localizam- se mais próximos às bases dos seus reticulados.
São, portanto, aqueles conceitos em que a quantidade de atributos é maior.
Desta forma, a exemplo do trabalho de Meddouri e Meddouri, primeiramente, organizamos, de forma decrescente, os conceitos de acordo com a cardinalidade dos seus conjuntos de atributos.
Meddouri e Meddouri utilizam apenas os atributos dos conceitos formais para denir as premissas das regras de classificação.
Para bjetivo daqueles autores é adequada esta escolha, visto que eles usam as regras para associar instâncias aos conceitos.
Para nossa abordagem isso não seria conveniente.
Os FCA construídos a partir de o caso 1 (sn, v), por exemplo, gerariam regras cujas premissas seriam formadas apenas por verbos.
Levando em conta que queremos categorizar textos e que verbos podem ser aplicados a diferentes domínios, trabalhar apenas verbos deixaria as regras pouco seletivas.
Tentamos usar apenas objetos formais nas premissas, mas as regras assim geradas também não resultaram em boas classicações em testes preliminares que realizamos.
Optamos, então, por usar tantbjetos quanto atributos formais nas premissas das regras.
Para as regras de estruturas FCA construídas a partir de o caso 7 (sn, psP_ sn), entretanto, consideramos apenas os sintagmas nominais existentes nos atributos.
Fizemos isso para evitar que os textos usados nos conjuntos de teste precisassem estar anotados com papéis semânticos para ser classicados.
Assim sendo, geramos regras no formato:
Cabe mencionar que a decisão por usar tantbjetos quanto atributos formais nas premissas das regras teve duas consequências.
Uma de elas foi que as premissas das regras caram muito densas.
Por esta razão, usamos apenas um conceito formal na composição de cada regra.
A outra consequência foi que essa alta densidade deixou as regras muito especícas.
Como a classificação baseada em regras é realizada, geralmente, a partir de o casamento (matching) dos termos dos textos com as premissas das regras, regras muito especícas não são interessantes.
Uma solução para isso é o particionamento dessas regras em subregras.
No entanto, tal solução, além de produzir mais regras, ainda geraria mais um problema que é como agrupar as premissas dessas regras para formar novas regras.
Para evitar esse particionamento, criamos um fator de ativação (fa) para a regra.
Esse i em relação a o texto ja ser categorizado, onde e corresponde ao número total de regras.
O fator estabelece uma proporção entre a quantidade de premissas que casam com os termos desse texto com o total de premissas da regra.
A categoria c fa k com o maior valor de fator de ativação acumulado (acum) é definida como a classe do texto dj.
A forma de cálculo desses fatores são apresentadas em 9.1, onde p (r) icorresponde ao conjunto de premissas da regra r t (d), d ie j ao conjunto de termos em j.
É importante destacar ainda que variamos a quantidade q de regras extraídas de cada estrutura FCA.
Analisamos os resultados de categorização para valores q de 23 a 27.
Para avaliar os resultados, usamos medidas usuais em categorização de textos, como precisão (Pr), recall Re) F1 (e.
Calculamos tais medidas por categoria e também de forma geral (macro-médias).
A Tabela 9.4 mostra os melhores resultados de categorizaçãbtidos para os conjuntos de teste.
Os índices mais altos foram obtidos a partir de a extração de 50 regras, 25 regras de cada estrutura FCA.
Os dados dessa tabela foram extraídos das tabelas do Apêndice Analisando-se os dados da Tabela 9.4, podemos notar que as regras extraídas a partir de estruturas FCA baseadas no caso7 (sn, psP_ sn) resultam em medidas de avaliação F1 bem superiores às geradas por o caso1 (sn, v).
Cabe mencionar, ainda para essas mesmas estruturas, que os resultados foram melhores para o conjunto de teste Wikipossivelmente porque esse conjunto tem a mesma natureza dos textos usados para construir tais estruturas.
Em esse estudo, usamos as ontologias de turismo TGPROTON (O TG) e Travel (O), e as ontoT logias de Finanças LSDIS Finance (O) e Finance (O).
A Seção 5.2 descreve brevemente tais ontologias.
Para a escolha dos conceitos, aplicamos o mesmo critério utilizado nas estruturas FCA:
Generalidade. Buscamos, portanto, conceitos próximos ao topo das hierarquias para compor as premissas das regras.
Testamos inicialmente conceitos de nível 2 e, posteriormente, os de nível 3.
As premissas de cada regra foram definidas por o conceito do nível escolhido juntamente com seus superconceitos.
Acabamos usando apenas os conceitos de nível 2 em razão de a melhor qualidade nos resultados.
Para este nível, geramos 28 regras a partir de a ontologia TGPROTON, 14 regras a partir de a ontologia Travel, 22 regras a partir de a ontologia LSDIS Finance e 41 regras a partir de a ontologia Finance.
Como tínhamos duas ontologias de cada categoria, testamos primeiramente conjuntos de regras formados a partir de as seguintes combinações O TG+ OL, TG+ OF, T+ O e O+ O.
Os resultados de categorização dos conjuntos de teste para essas combinações são apresentados nas Tabelas F. 5 e F. 6 do Apêndice F. Com bjetivo de melhorar os resultados, escolhemos a combinação de melhor medida F1 para investigar a quantidade de regras mais adequada.
Selecionamos, portanto, a combinaçã TG+ OF a qual obteve 0,63 e 0,68 em F1 para os conjuntos testeWiki e testeWiki+ PTBS, respectivamente.
A exemplo dos testes realizados na Seção anterior para estruturas FCA, variamos a quantidade q de regras extraídas de cada ontologia.
Testamos para, a princípio, os mesmos valoresq de 23 a 27.
Coms resultados não foram satisfatórios, mudamos os valores de qpara o intervalo de 6 a 13.
Usando este intervalo para a combinaçã TG+ OF, conseguimos uma pequena melhora.
Utilizando 18 regras -- 9 regras da ontologia TGPROTON e 9 regras da ontologia Finance -- a medida F1 para os teste Wikie testeWiki+ PTBSaumentou, respectivamente, para 0,68 e 0,77.
Cabe mencionar que, para selecionar as regras, as organizamos em ordem decrescente conforme a quantidade de premissas.
Nossbjetivo era escolher as regras mais abrangentes de cada ontologia.
Usamos o mesmo fator de ativação acumulado, detalhado na Seção anterior, para determinar a categoria dos textos dos conjuntos de teste.
No entanto, tivemos que tornar o fator de ativação fa mais exível.
Dada a presença maior de n-gramas (para n 2) como rótulos de classes nas ontologias, tivemos que permitir a ativação das regras também a partir de termos aninhados (substrings) desses gramas.
N Visto que os resultados ainda eram bem inferiores aos obtidos a partir de as estruturas FCA, realizamos mais um teste, extraindo regras a partir de as ontologias de mesmo domínio conjuntamente (O TG q T, O F L).
Utilizamos nesses testes o mesmo intervalo para.
Testamos o uso de 6 a 13 regras para cada ontologia.
Esses testes são detalhados nas Tabelas F. 9 e F. 10 do Apêndice F. A Tabela 9.5 apresenta um resumo dos melhores resultados de categorizaçãbtidos para os conjuntos teste Wiki e testeWiki+ PTBS.
Como podemos observar nessa tabela, os melhores valores para F1 foram obtidos a partir de os últimos testes realizados.
Foram necessárias 36 regras para atingir os valores 0,71 e 0,77 em F1 para, respectivamente, os conjuntos teste Wikie testeWiki+ PTBS.
Apesar de os esforços realizados, as regras extraídas a partir de essas ontologias tiveram resultados inferiores aos apresentados por as regras geradas a partir de as estruturas FCA propostas para o conjunto teste Wiki.
Para o conjunto testeWiki+ PTBS, no entanto, atingimos medidas F1 equivalentes.
Acreditamos que os baixos resultados obtidos nessa abordagem estejam relacionados à relevância semântica das ontologias utilizadas.
As ontologias de Turismo, especialmente, possuem um menor detalhamento em conceitos se comparadas às de Finanças.
A pouca profundidade das ontologias de Turismu mesmo a desproporcional riqueza em conceitos entre as ontologias de domínios distintos podem ter prejudicads resultados.
Além disso, o método simples de extração de conceitos que utilizamos para compor as regras de classificação pode não ter sid mais adequado.
Em a Seção seguinte, estudamos a categorização dos textos a partir de o algoritmo k-NN.
O processo de classificação implementado por o algoritmo k-NN consiste em definir a categoria de um documento dj, não rotulado, que pertence ao conjunto de teste, a partir de os k documentos vizinhos a dj, que pertencem ao conjunto de treino usado.
Os vizinhos são determinados com base em uma métrica que avalia a similaridade entre os termos dos documentos.
A categoria mais recorrente entre os documentos vizinhos é associada ao documento d j.
Antes de realizarmos a categorização de textos por meio de o algoritmo k-NN, tivemos que préprocessar os textos do conjunto treinowiki.
Com processador F-EXT-WS já segmenta o texto em tokens e os marca com etiquetas POS, esse processo foi relativamente simples.
Inicialmente, eliminamos os tokens considerados irrelevantes para o processo de categorização de textos, como caracteres especiais estopwords.
Sendo que, para descartar estas últimas, usamos as etiquetas POS referentes, por exemplo, a preposições, pronomes, artigos e conjunções.
Em seguida, aplicamos o lematizador TreeTagger para normalizar os termos restantes.
Cabe ressaltar que, não foram usadas as informações sintáticas e semânticas providas por o anotador F-EXT-WS nesse pré-processamento.
Com bjetivo de definir o conjunto num modelo de bag-of-- words, usamos, a exemplo de um trabalho que realizamos anteriormente em categorização de textos com o algoritmo k-NN, seleção por rank.
Nossa escolha quanto a o tipo de seleção de características a ser utilizado, foi baseada nos bons resultados que encontramos naquele trabalho.
Para aplicar a seleção por rank, então, contabilizamos a frequência dos termos, resultantes do pré-processamento, em cada categoria.
Para cada categoria, escolhemos os n termos de maior ocorrência.
Testamos para n os valores 50, 100 e 150.
Após a união dos n termos mais relevantes para cada categoria, obtivemos 3 congurações de bag-of-- words.
A seleção por rank, para n $= 50, resultou num conjunto de 92 termos.
Para n $= 100, encontramos 184 termos signicativos.
E, por m, para n $= 150, testamos uma bag-of-- words de 276 termos.
A m de garantir uma uniformidade maior em relação a as categorizações realizadas nas seções anteriores, que são baseadas em matching, representamos os conjuntos de treino e teste como vetores binários.
Usamos, também nos baseando no trabalho descrito em, o cosseno como medida de similaridade e três valores de teste para:
7, 13 e 17.
Os resultados das congurações propostas para o algoritmo k-NN estão no Apêndice F. A partir desses dados, construímos a Tabela 9.6, a qual contém as congurações com os melhores resultados para os arquivos teste Wikie testeWiki+ PTB.
Apesar de o conjunto teste Wiki+ PTB ter exigid uso de um número maior de características, o algoritmo k-NN obteve um bom desempenho para ambos os conjuntos.
Categorizou os textos com medidas de avaliação F1 de no mínimo 0,92.
Em a Seção seguinte, comparamos os resultados de categorizaçãbtidos para as 3 abordagens estudadas.
Para compararmos as abordagens, reunimos os melhores resultados de categorização na Tabela partir de nossa abordagem, que é totalmente automática, obteve resultados iguais ou melhores que aquelas extraídas das ontologias usadas.
Claro que devemos considerar que a relevância semântica das ontologias e o fato de não explorarmos profundamente diferentes formas extração de regras a partir de elas podem ter inuenciads resultados.
Percebemos também, comparando as abordagens baseadas em regras, que o conjunto de teste Wikifoi melhor categorizado por as regras geradas a partir de as estruturas FCA, possivelmente, porque tais estruturas foram construídas a partir de os mesmos corpora (WikiFinance e WikiTourism).
Para o algoritmo k-NN, a diferença na natureza dos corpora usados nos conjuntos de teste aparentemente não inuenciou os resultados obtidos.
Comparands resultados do algoritmo k-NN com os obtidos a partir de as estruturas FCA e das ontologias, notamos que para ambos os conjuntos de teste, esse algoritmo foi melhor.
As regras extraídas de estruturas FCA, no entanto, para o conjunto testeWikigeraram classicações cujas medidas caram muito próximas às do algoritmo k-NN.
As regras baseadas em conceitos formais produziram 0,92 em F1 e o algoritmo k-NN, 0,95.
Já no caso de o conjunto de teste Wiki+ PTBS tanto as regras baseadas em conceitos formais quanto aquelas baseadas em conceitntológicos obtiveram F1 de 0,78.
Índice bem abaixo de o gerado por o algoritmo k-NN, que foi de 0,92.
Embora a construção de estruturas FCA baseadas em papéis semânticos, como as testadas, exija textos com maior riqueza em anotações linguísticas e a geração da própria estrutura demande maior processamento computacional, uma vez definidas e suas regras extraídas, o processo de categorização é mais rápido que o de um algoritmo k-NN.
Apesar disso, acreditamos que o mais importante é que conseguimos, por meio de os estudos apresentados, mostrar a aplicabilidade da nossa abordagem.
Cabe mencionarmos que, implementamos em Java o processo de categorização por regras e por o algoritmo k-NN apresentados nesse estudo.
Em a Seção seguinte, são comentados alguns estudos que realizamos com textos em português.
Os estudos apresentados nas seções a seguir foram realizados com o corpus PLN-BR CATEG.
A Seção 9.2.1 mostra os estudos iniciais que zemos na área de categorização de textos.
Levamos em consideração esses estudos ao escolher a tarefa de categorização na avaliação funcional da abordagem proposta na tese.
Já a Seção 9.2.2 apresenta os estudos preliminares que realizamos quanto a a extração de conceitos e à construção de estruturas conceituais do tipo FCA combinadas com papéis semânticos.
Em esta Seção descrevemos brevemente os estudos iniciais que zemos na área de categorização de textos com o corpus PLN-BR CATEG.
Esses estudos geraram publicações conforme citado.
Portuguesa Em este estudo sobre categorização hierárquica de documentos utilizamos 26.606 textos jornalísticos do corpus PLN-BR CATEG.
Os textos foram lematizados por a ferramenta Forma e o processo de categorização foi realizado com o algoritmo k-NN.
Implementamos esse algoritmo em linguagem C usando como métrica de similaridade o cosseno.
Seguimos a hierarquia de categorias definida por Langie em e testamos duas estratégias de classificação:
Limiar baseado em rank e limiar baseado em relevância.
Para representar os documentos, usamos a abordagem bag-of-- words e definimos os pesos dos termos a partir de a medida tf-idf.
A seleção dos atributos foi realizada a partir de as frequências dos termos nos documentos.
Analisamos neste estudo, de forma experimental, a inuência de determinados parâmetros no processo de classificação.
Entre os parâmetros analisados estã número de vizinhos considerados por o algoritmo e sua relação com a quantidade de documentos usados no treinamento dos classicadores, o número de características escolhidas durante a seleção de atributos, bem como a própria estratégia de classificação.
A a época, foi um dos primeiros trabalhos em categorização de textos utilizando um corpus de tamanho expressivo em Língua Portuguesa.
Mais detalhes podem ser encontrados em.
Em este estudo também realizamos categorização hierárquica de documentos sobre os mesmos por utilizar, desta vez, palavras-chave na etapa de seleção de atributos.
Essa etapa foi realizada com o auxílio da ferramenta Wordsmith Tools (Seção 5.3.6), em a qual aplicamos a medida log-- likelihood para escolher as palavras conforme as categorias dos documentos.
Em o total, a ferramenta escolheu 500 palavras-chave, as quais foram usadas para representar os documentos do corpus (abordagem bag-of-- words).
Assim como no estudo anterior, seguimos a hierarquia de Langie.
No entanto, usamos dois tipos de classicadores:
Um baseado no algoritmo k-NN e outro em redes neurais Multi--Layer Perceptron.
Implementamos em linguagem C uma rede neural para cada categoria da hierarquia.
De as 28 redes neurais implementadas, 10 foram construídas para o nível 1 da hierarquia e 18, para o nível 2.
Observamos que a categorização dos documentos foi mais precisa com os classicadores neurais.
Mais detalhes sobre este estudo podem ser encontrados em.
Esta Seção apresenta estudos relacionados à extração de conceitos e a estruturas FCA combinadas com papéis semânticos.
Realizamos um estudo sobre extração de conceitos a partir de textos usando algoritmos de agrupamento.
Utilizamos as dependências sintáticas entre os verbos e seus argumentos para identicar os termos e multitermos (gramas) relevantes nos textos.
A seleção dos termos é realizada através de uma abordagem híbrida que combina as medidas tf-idf e C- Value.
Os conceitos sãbtidos e organizados através de algoritmos de agrupamento disponíveis na ferramenta CLUTO (Seção 5.3.6).
Nossos experimentos foram realizados em 4.407 documentos da Seção Esportes do corpus PLN-BR CATEG, e a qualidade semântica dos clusters foi avaliada manualmente.
A abordagem híbrida, bem com método de seleção de termos propostos mostraramse adequados na escolha de termos relevantes para a identificação de conceitos.
Apesar de relatarmos vários problemas na geração dos grupos de conceitos, os resultados mostram a viabilidade da metodologia.
Mais detalhes sobre esse trabalho podem ser encontrados em Esta Seção descreve brevemente um estudo que realizamos para a Língua Portuguesa, voltado à construção de estruturas conceituais baseadas em FCA e em papéis semânticos.
Usamos nesse estudo preliminar cerca de 58% dos textos da Seção Esportes do corpusPLNBR-CATEG.
Para extrair as relações verbo-argumento dos textos, já lematizados por a ferramenta Forma, desenvolvemos, em linguagem C, um parser baseado em expressões regulares.
Verbos de ligação e auxiliares como &quot;ser «e &quot;ter «não foram considerados.
Foram extraídos, como argumentos dos verbos, os sintagmas nominais que desempenhavam a função sintática de objeto diretu indireto.
Usamos uma heurística simples para eliminar as entidades nomeadas:
Excluímos todos os sintagmas que iniciavam com letra maiúscula e não estavam no início das sentenças.
Tivemos também que criar uma pequena stoplistpara eliminar termos referentes a datas, períodos e dias da semana pois, com corpus era jornalístico, esses termos eram muito frequentes.
Para permitir uma análise qualitativa, utilizamos um exemplo de estrutura FCA.
Essa estrutura foi gerada a partir de as sementes &quot;jogo «e &quot;campeonato».
Seu contexto formal se baseou em relações verbo-argumento cuja frequência em documentos fosse superior a 9.
Para estreitar ainda mais a relação semântica entre as selecionadas, foram descartadas aquelas cujos verbos não estivessem relacionados a pelo menos 3 diferentes sintagmas nominais (argumentos).
Em seguida, traduzimos para o inglês as sentenças cujas relações foram selecionadas.
Para realizar a anotação semântica usamos um dicionário de verbos para Língua Portuguesa, um etiquetador de papéis semânticos para a Língua Inglesa e a VerbNet.
Cabe ressaltar que tivemos que usar um etiquetador de papéis semânticos para a Língua Inglesa devido a a inexistência desse recurso para Língua Portuguesa.
O etiquetador usado Illinois SRL disponibilizado por o Cognitive Computation Group e comentado na Seção 5.3.4.
Esse etiquetador segue a anotação do PropBank para identicar os papéis semânticos.
Como essa anotação não é padrão para todos os verbos e nem corresponde aos nomes geralmente usados na literatura assim definidas são vericadas com o auxílio do dicionário de verbos e, então, usadas como atributos durante a geração dos conceitos que formam o FCA.
Após a anotação analisamos para cada contexto formal, pelo menos 6 sentenças (escolhidas aleatoriamente) em as quais as relações apareciam.
Isso permitiu a identificação de alguns erros do parser e do etiquetador Illinois SRL.
Para formar o contexto formal da estrutura FCA de nosso exemplo, usamos os sintagmas nominais (argumentos dos verbos) combjetos e elementos da forma verb (semanticRole) como atributos formais.
Essa estrutura-exemplo é apresentada na Figura 9.2.
Analisando a estrutura FCA gerada a partir de as sementes &quot;jogo «(game) e &quot;campeonato «(championship), conseguimos distinções interessantes de signicado, principalmente para os papéis Agent, Manner e Location.
Os argumentos do verbo &quot;dizer «(to say), por exemplo, foram separados corretamente em Agent e Topic (assunto de uma comunicação).
Todos os objetos anotados como Agent, nesse caso, eram denominações dadas a pessoas conforme a função que executam (atacante, jogador, indicados, numa abordagem semiautomática, como subclasses de Pessoa.
Poderiam, também, ser usados para encontrar as instâncias de Pessoa no texto.
Já bjeto casa «(home) foi etiquetado como Location no contexto de &quot;jogar «(to play).
&quot;contexto «em que o eventcorreu e isso nem sempre corresponde a um lugar concreto.
Outra diferenciação interessante foi a atribuída abjeto &quot;gol «(goal).
Ele foi anotado como uma forma (Manner) de vencer.
Isso aconteceu em sentenças como:
&quot;O técnico Paulo César Um aspecto também interessante dos papéis semânticos diz respeito aos sinônimos.
Sintagmas nominais (objetos) que compartilham verbos e são etiquetados para esses verbos com os mesmos papéis semânticos, podem ser candidatos a sinônimos, como é o caso de &quot;campeonato «e &quot;torneio».
O papel mais frequente foi Theme.
Aparentemente, todas as palavras etiquetadas com esse papel eram de fato relevantes para o domínio.
Como acreditamos que a construção de estruturas conceituais seja um processo interativo e iterativo, podemos usar os objetos Theme como as sementes da próxima iteração, o que permitiria renar passo a passs conceitos da estrutura.
Para nalizar, é importante destacar que, mesmo com a falta de recursos, zemos estudos iniciais de nossa proposta em Língua Portuguesa.
Como já mencionado, os estudos em categorização de texto para Língua Inglesa mostram que nossa proposta de utilizar estruturas FCA baseadas em papéis semânticos é aplicável.
É importante ressaltar que mesmo diante de diferentes limitações, as estruturas FCA construídas segundo nossa abordagem conseguiram gerar boas regras de classificação.
Observamos que durante a realização dos testes com os corpora WikiFinance e WikiTourism, o parser que implementamos para eliminar tags e elementos indesejáveis em tais textos não foi tão eciente.
Ainda restaram marcadores de itens, por exemplo, em alguns textos.
Essa limpeza deciente, sob alguns aspectos, provavelmente prejudicou o desempenho do processador F-EXT-WS quanto a a anotação semântica.
Notamos também que a simplicidade do parser que desenvolvemos para extrair sintagmas nominais dos textos, produziu, em alguns casos, unigramas menos signicativos.
Se associarmos essas limitações à precisão possivelmente mais baixa de anotadores semânticos, com utilizado, para domínios diferentes do Finanças, podemos armar que os resultados obtidos são satisfatórios.
Em esse capítulo ainda apresentamos estudos relacionados à Língua Portuguesa.
Realizamos estudos em extração de conceitos e categorização de textos.
Desenvolvemos também um estudo preliminar usando estruturas FCA com papéis semânticos, convivendo com a ausência de anotadores dessa natureza especícos para o português.
Embora a tradução das sentenças para a Língua Inglesa tenha dicultado esse estudo, consideramos os resultados promissores.
Em este trabalho, estudamos abordagens tradicionais para aprendizagem de estruturas ontológicas a partir de textos e propomos a construção de tais estruturas a partir de o método FCA e de papéis semânticos.
Apesar de termos encontrado, ao longo de nossa pesquisa, trabalhos que resultaram em estruturas conceituais relevantes, percebemos que não há consenso, entre os pesquisadores, no que se refere a diferentes aspectos referentes à aprendizagem de tais estruturas a partir de textos.
Há divergência quanto a o conceito de ontologia e divergência quanto a quais estruturas conceituais podem, de fato, ser assim chamadas.
Muitas estruturas ditas ontologias são apenas taxonomias e não incluem axiomas e nem relações transversais.
Essa discordância entre os pesquisadores foi uma das razões, inclusive, por as quais optamos por o uso de termos como &quot;estruturas ontológicas «e &quot;estruturas conceituais», para nos referirmos aos reticulados de conceitos gerados por o método FCA.
Quanto a a metodologia, percebe- se algumas linhas gerais de procedimento, como as tarefas enumeradas por Cimiano para aprendizagem dessas estruturas a partir de textos.
Notamos uma certa preferência, por parte de os pesquisadores, por abordagens híbridas e por o uso de recursos web na implementação da maioria dessas tarefas.
Muitos trabalhos combinam técnicas linguísticas, estatísticas e de aprendizagem de máquina para realizar- las, sendo uma prática comum o uso de corpora construídos a partir de documentos web e de textos da Wikipédia.
Observamos também o uso recorrente da WordNet como um recurso de apoio à identificação de conceitos e relações semânticas.
No entanto, percebemos que tarefas referentes à identificação e à etiquetagem de relações transversais, bem como tarefas responsáveis por a definição de axiomas, nem sempre estavam presentes, nos trabalhos pesquisados.
E é justamente em tais tarefas que os papéis semânticos têm sido usados com maior intensidade.
Em trabalhos mais recentes, eles são utilizados para identicar e rotular relações transversais, bem como para prover restrições semânticas úteis na definição de axiomas.
Embora papéis semânticos sejam um recurso interessante para extração e representação de estruturas ontológicas, há ao menos duas diculdades importantes no que tange a sua utilização:
PropBank tenham, de certa forma, contornad problema referente a a identificação dos papéis, a diculdade no seu uso ainda persiste pois, em aplicações que envolvem representação de conhecimento, é necessário determinar o signicado de cada etiqueta.
Além disso, mesmo no caso de a Língua Inglesa, para a qual os avanços na área são maiores, há poucos corpora anotados semanticamente que possam ser usados no treinamento de etiquetadores semânticos.
Essa escassez tem impactado no aperfeiçoamento dessas ferramentas.
Agregadas a essas diculdades de anotação semântica, estão ainda aquelas inerentes a qualquer abordagem que busque informações a partir de textos.
Essas diculdades, somadas, acabam tornando essencial lhar humano.
Desta forma, é muito comum o uso de abordagens semiautomáticas na construção de estruturas ontológicas a partir de textos.
Isso, inclusive, se estende ao processo avaliativo de tais estruturas, comumente conduzido sob a forma de avaliação humana.
Avaliações intrínsecas de cunho puramente semântico, com frequência são realizadas manualmente.
Quand objetivo é analisar a estrutura ontológica enquanto estrutura de representação de conhecimento, a avaliação geralmente é realizada por especialistas humanos.
A ausência de métodos e métricas formais de uso amplo, além de prejudicar as avaliações em si, também tem inviabilizado a comparação dos resultados obtidos em diferentes pesquisas.
Mesmo em avaliações extrínsecas, de caráter funcional, como as que utilizam ontologias de referência ou comparam os resultados no contexto de uma aplicação, se ressentem dessa ausência.
De o ponto de vista tecnológico, não há ainda ferramentas que viabilizem a avaliação automática com componente semântico das estruturas conceituais geradas a partir de textos.
A falta de benchmarks para essa área, por exemplo, inviabiliza a comparação de metodologias.
Desta forma, torna- se difícil decidir que método e técnica são mais adequados para a realização de uma tarefa em corpora de domínio e de natureza distintas.
Todas essas lacunas mostram que a área de aprendizagem de estruturas ontológicas a partir de textos ainda tem muito espaço de investigação.
E é dentro desse espaço que o método FCA tem despertad interesse dos pesquisadores e, consequentemente, vem se destacando como uma alternativa enquanto método de agrupamento conceitual na construção de estruturas ontológicas.
Em ciência da computação, especialmente, o interesse por FCA e suas extensões se justica por o fato do método ser adequado para a análise de dados e ser promissor como forma de representação de conhecimento.
Suas inerentes características de agrupar, relacionar e organizar os dados de forma hierárquica e de, ainda, prover descrições intensionais que facilitam a interpretação dos grupos gerados, o tornam um método de agrupamento conceitual muito interessante.
Em PLN, ele se destaca por permitir diferentes representações conceituais que reetem as diversas formas coms dados aparecem relacionados nos textos.
Sendo indicado, portanto, para análises linguísticas, pois gera estruturas que possibilitam o estudo de relacionamentos sintáticos e semânticos, inclusive para desambiguação de sentido.
A principal desvantagem do método, no entanto, é de ordem computacional.
À medida que a quantidade de dados e de relacionamentos entre esses dados aumentam, cresce de forma exponencial a complexidade de geração dos reticulados de conceitos provenientes da aplicação do método.
Esse aspecto, entretanto, é atenuado por o fato de utilizarmos informações provenientes de textos.
O problema da esparsidade das relações encontradas no textos, tal como as do tipo verbo-argumento, ao mesmo tempo que reduz a riqueza da informação extraída, diminui também a complexidade de construção dos reticulados de conceitos.
Embora essa esparsidade não seja desejável no contexto da extração de informações, pesquisadores que utilizam o método para relacionar informações textuais têm relatado complexidade próxima à linear.
Apesar de o problema referente a a complexidade computacional, os resultados obtidos por os pesquisadores com o FCA, enquanto método de agrupamento conceitual, têm sido motivadores.
Este fato, aliado à aplicabilidade dos papéis semânticos na identificação de relações não taxonômicas e o recente surgimento de etiquetadores automáticos para tais papéis nos levaram a indagar se relações semânticas dessa natureza não poderiam melhorar a qualidade dos conceitos formais gerados por o método FCA.
Como, ao longo de nossa pesquisa, não encontramos trabalhos atuais que, juntamente com o método FCA, explorassem aspectos semânticos, mais especicamente classes de verbos e papéis semânticos, direcionamos nosso estudo nesse sentido.
Dado que os etiquetadores automáticos de papéis semânticos estão disponíveis para a Língua Inglesa, concentramos nossa investigação em corpora nessa língua.
Mesmo com a carência desse recurso para a Língua Portuguesa, realizamos estudos preliminares num corpusde textos em português:
PLN-BR CATEG.
Usamos em Língua Inglesa os corpora Penn TreeBank Sample, SemLink 1.1 e Wikicorpus Geramos, a partir de o Wikicorpus 1.0, pequenos corpora dos domínios de Finanças e Turismo, aos quais chamamos de WikiFinance e WikiTourism.
Enquanto que o SemLink 1.1 já continha as anotações semânticas referentes ao Penn TreeBank Sample e eram adequadas ao nosso estudo, os corpora WikiFinance e WikiTourism não dispunham de tal informação.
Tivemos então que anotar- los e para isso usamos o processador F-EXT-WS.
Cabe lembrar que o corpus SemLink 1.1 é uma extensão do PropBank.
Em essa extensão, foram incluídas informações quanto a as classes VerbNet dos verbos e foram mapeados os papéis semânticos VerbNet às etiquetas numéricas PropBank correspondentes.
Em a investigação que zemos com o uso dos corpora Penn TreeBank Sample e SemLink 1.1 conseguimos estudar os papéis semânticos a partir de rótulos mais tradicionais, tais como Agent e Patient.
No caso de os corpora WikiFinance e WikiTourism, anotados semanticamente por o processador F-EXT-WS, tivemos que conviver com rótulos numéricos ao estilo PropBank para tais papéis.
Essa diferença nos rótulos nos obrigou a tratar tais anotações de forma diferenciada.
Independente disso enfrentamos, no pré-processamento de todos esses corpora, diculdades quanto a a extração dos sintagmas nominais anotados com papéis semânticos.
Como a tarefa de extração de sintagmas nominais é relativamente complexa, tivemos que criar um conjunto de heurísticas para realizar- la.
Embora esse tratamento simplicado tenha provocado consequências, visto que foram gerados menos sintagmas do que de fato existiam nos textos, ou alguns de menor relevância, conseguimos mesmo assim realizar nossa investigação de forma satisfatória.
Um dos aspectos fundamentais de nossa pesquisa foi a inclusão das informações semânticas em estruturas do tipo FCA.
Inicialmente, estudamos o método RCA, que é uma extensão do FCA.
Como esse método é indicado, na literatura, com mais adequado para representar explicitamente relações não taxonômicas, tais como as relações semânticas definidas por os papéis, analisamos a viabilidade de seu uso em nossa abordagem.
A generalização das relações entre objetos formais para relações entre conceitos formais, na análise preliminar que realizamos, produziu relações inicialmente inexistentes entre os objetos.
A estrutura RCA assim gerada não explicitava tais ausências claramente e imaginamos que isso poderia levar a erros de interpretação quanto a os conceitos gerados.
Além disso, observamos a falta de ferramentas para geração de estruturas RCA.
Esses fatores nos levaram a descartar tal método.
O uso do corpus SemLink 1.1 também nos permitiu analisar algumas classes VerbNet quanto a os papéis semânticos frequentemente associados aos verbos dessas classes.
Observamos que tais classes, de fato, delimitavam os papéis semânticos.
Por esta razão, acreditamos que elas pudessem ser um caminho de pesquisa interessante para a investigação de relações não taxonômicas mediadas por papéis semânticos.
Contudo, como a VerbNet não é completa, seria necessário estudar métodos e heurísticas capazes de classicar os verbos ausentes nessa base.
Com esses mecanismos de classificação e com o auxílio dos etiquetadores de papéis semânticos seria possível determinarmos, de forma mais objetiva, as relações entre papéis e sua relevância para diferentes domínios.
Acreditamos que, para esse tipo de análise, o mais indicado seria o uso de corpora de natureza conceitual.
Percebemos, como já mencionado, que a natureza do corpus inuência na frequência de determinados papéis semânticos, podendo &quot;reduzir a importância «daqueles cuja relevância pode ser igual ou maior para o domínio.
Ainda no que se refere às relações não taxonômicas, acreditamos que as classes de verbos possam ajudar a definir rótulos de caráter mais geral para tais relações.
Isso exigiria, entretanto, associar às denominações numéricas atuais, rótulos textuais.
Embora essa ideia pareça atraente, acreditamos ser difícil sua realização de forma automática.
Para que os rótulos façam sentido, têm que ser definidos de acordo com o domínio e, nesse caso, a sugestãu mesmo crítica humana seriam essenciais.
Por outro lado, como analisamos apenas duas classes VerbNet, talvez a complexidade quanto a a definição desses rótulos textuais seja maior do que tenhamos conhecimento.
É possível que a relação estabelecida entre os papéis de uma mesma classe não seja tão clara.
De qualquer forma, acreditamos que caiba investigar tal possibilidade.
Voltando à questão referente a a inclusão de papéis semânticos em estruturas FCA, para determinar a forma como isso seria realizado, tivemos que analisar diferentes congurações de contextos formais.
Um dos problemas que enfrentamos foi a definição do procedimento avaliativo que seria utilizado nessa fase.
Precisávamos analisar os conceitos formais do ponto de vista estrutural e também semântico.
Só assim poderíamos medir a contribuição de classes de verbos e papéis semânticos na construção desses conceitos.
Dada a escassez de medidas estruturais adequadas a nosso propósito, principalmente, no que se refere a conceitos formais, propusemos a adaptação da medida SSM.
Escolhemos tal medida estrutural por ela levar em conta aspectos semânticos.
Para que ela atendesse ao nosso m, operacionalizamos sua aplicação a conceitos.
Desta forma, ela passou a funcionar como uma medida de coesão lexical.
Embora tal medida, se aplicada de forma isolada, não seja conclusiva, aliada a outras medidas estruturais a mesma nos forneceu dados que ajudaram a analisar as congurações propostas e a escolher as mais promissoras, no contexto de nossa pesquisa.
Com auxílio dessa medida, notamos que as classes de verbos, além de aumentarem a coesão lexical, ajudaram a reduzir a complexidade de construção do reticulado FCA, na medida em que geraram menos conceitos e arestas.
Já os papéis semânticos mostraram- se mais efetivos, no aspecto coesão, principalmente quando eram usados como atributos nos contextos formais estudados.
Apesar desses resultados, do ponto de vista intensional e simbólico, o uso, como atributos formais, de rótulos numéricos coms que identicam as classes de verbos é menos informativo do que o uso de verbos.
Entendemos, igualmente, como menos expressivos sob o aspecto intensional, os atributos contendo papéis semânticos dissociados de seus contextos.
Em um contexto, o papel semântico do sintagma nominal assim anotado é mais evidente e, consequentemente, mais relevante, pois as relações com outros elementos do domínio delimitam o seu signicado.
Por esta razão acabamos convergindo para uma representação em que os pares objetoatributo expressavam relações entre papéis semânticos.
Em essa representação, os atributos eram formados por contextos lexicossemânticos.
Os atributos assim definidos nos pareceram intensionalmente mais descritivos, ainda que inicialmente tal configuração tenha produzido conceitos menos coesos.
Com base nos estudos que realizamos para definir tal representação, descrevemos, na forma de um procedimento, na Seção 10.2, a metodologia que usamos para construir estruturas FCA a partir de tais contextos lexicossemânticos.
Chamamos esta metodologia de Semantic FCA (SFCA) e a consideramos um dos principais resultados de nossa investigação.
Com bjetivo de complementar ainda mais nossos estudos e também analisar a viabilidade quanto a a aplicação da proposta, avaliamos a contribuição das informações semânticas na construção de conceitos formais com base nos corpora WikiFinance e WikiTourism.
Em esse caso, contudo, optamos por medidas de avaliação de ordem funcional no contexto da tarefa de categorização de textos.
Escolhemos trabalhar, nessa etapa da investigação, com a classificação de textos baseada em regras, embasados em trabalhos relacionados.
Tal forma de trabalho nos permitiu comparar resultados provenientes das regras extraídas das estruturas FCA enriquecidas semanticamente (construídas conforme nossa abordagem) daquelas extraídas de ontologias.
Em todos os testes realizados, as regras extraídas a partir de as estruturas FCA geraram resultados de classificação iguais ou superiores às produzidas por as obtidas a partir de as ontologias.
Cabe ressaltar que há ainda espaço de investigação quanto a a extração de regras a partir de essa estruturas, visto que o método usado e a relevância semântica das ontologias usadas no processo comparativo podem ter inuenciado de forma signicativa os resultados.
Comparamos, também, os resultados obtidos a partir de nossa abordagem aos de um classicador k-NN.
Em alguns casos conseguimos resultados muito próximos aos desse classicador.
Obviamente que o esforço de anotação linguística para o uso de um classicador desse tipo é muito menor do que o necessário para gerar uma estrutura FCA.
Além disso, a complexidade envolvida na construção do reticulado de conceitos também aumenta a demanda computacional.
No entanto, uma vez construída essa estrutura conceitual e extraídas as suas regras, o processo de categorização é mais rápido do que o de um classicador k-NN.
Embora nem sempre nossa abordagem tenha gerado resultados superiores aos do classicador k-NN, consideramos os resultados obtidos como satisfatórios, visto que, mesmo diante de diferentes limitações, as estruturas FCA enriquecidas semanticamente conseguiram gerar boas regras de classificação.
Embora o pré-processamento dos corpora WikiFinance e WikiTourism, em alguns casos, tenha comprometid desempenho do anotador F-EXT-WS, e a simplicidade das heurísticas usadas na extração dos sintagmas nominais dos textos tenha sido menos efetiva, nossa abordagem mostrou- se aplicável.
Assim, também consideramos esse classicador outra contribuição de nossa pesquisa.
Em a Seção 10.3, descrevemos a metodologia que permite construir e utilizar um classicador de textos baseado em estruturas SFCA.
Quanto a os estudos realizados para a Língua Portuguesa, a ausência de recursos linguísticos é um fator limitante.
Com base nos estudos preliminares que realizamos, acreditamos que nossa abordagem também seja viável para o português.
No entanto sua aplicação exigirá um esforço manual, dada a falta de anotadores de papéis semânticos para essa língua.
Cabe mencionar ainda que, quanto a as formas de seleção estudadas na Seção 8.3, o uso de sementes em conjunto com o &quot;operador mais «mostrou- se um caminho apropriado para gerar estruturas FCA.
Essa abordagem, além de melhorar a coesão lexical dos conceitos formais, também reduziu a complexidade de construção do reticulado FCA.
Em esta Seção descrevemos a metodologia Semantic FCA (SFCA) que permite construir uma estrutura FCA enriquecida com papéis semânticos.
Para aplicar tal metodologia, o corpus a partir de o qual a estrutura será gerada deve ser anotado com informações lexicossemânticas.
Para isso, ocorpus deve ser submetido a um etiquetador de papéis semânticos e a um anotador de POS.
Desta forma, será possível identicar os verbos, os seus argumentos e os papéis semânticos desses argumentos.
Com o corpus assim anotado, a construção de uma estrutura SFCA consiste em:
Normalizar morfologicamente os termos das sentenças do corpuspor meio de um lematizador.
Analisar as sentenças, identicando e extraindo seus verbos, os argumentos desses verbos e os papéis semânticos associados a esses argumentos.
Desconsiderar sentenças ou trechos de sentenças cujos argumentos de verbos não tenham sido anotados semanticamente.
Identicar os sintagmas nominais existentes nos argumentos anotados com papéis semânticos, e considerar apenas aqueles formados por substantivos comuns.
Para efetuar esse passo, será necessário criar heurísticas para tratar os sintagmas nominais, tais como as descritas na Seção 6.4.2.
Formar tuplas, usando as informações extraídas das sentenças nos passos 2 e 3, no seguinte formato: (sn, ps, sn, ps), sn ps 1 1 2 2 onde ie i correspondem, respectivamente, ao sintagma nominal e ao seu papel semântico.
Em as tuplas, os sintagmas nominais devem ser constituídos por os lemas de seus substantivos.
Cada tupla deve conter apenas dois argumentos (sintagmas nominais) cujos papéis semânticos foram atribuídos por um verbo, numa mesma sentença.
Se, na sentença, o verbo tiver mais de dois argumentos anotados semanticamente, devem ser formadas tantas tuplas quantas forem as combinações desses argumentos dois a dois.
Por outro lado, se, na sentença, o verbo tiver apenas um argumento anotado com papel semântico, tal informação será insuciente para formar uma tupla e, portanto, deve ser descartada.
Construir, a partir de as tuplas formadas no passo 4, os pares (objeto, atributo) da seguinte forma: (sn, ps sn) (sn, ps sn).
Selecionar os pares (objeto, atributo) mais signicativos, no caso de uma lista de pares (objeto, atributo) muito longa.
A frequência absoluta pode ser usada para escolher os mais representativos.
Aplicar alguma técnica de agrupamento para os atributos quando esses forem muito especícos, a m de evitar um contexto formal muito esparso.
Para esse m, pode ser usad coeciente Dice tal como zemos em nossa investigação.
Construir o contexto formal usands pares (objeto, atributo) resultantes dos passos 5, 6 e 7.
Gerar a estrutura SFCA a partir de o contexto formal construído no passo anterior.
O reticulado de conceitos pode ser construído a partir de algoritmos coms comentados na Seção 3.5.2 ou a partir de uma ferramenta com esse propósito, tal como as mencionadas na Seção 5.3.5.
Em esta Seção descrevemos a metodologia utilizada para construir e utilizar um classicador baseado em regras, em que tais regras são formadas por conceitos formais extraídos de estruturas SFCA.
Como em todo processo de categorização, é necessário, inicialmente, separar os textos por categoria e particionar- los proporcionalmente em conjuntos de treino e de teste.
Para particionar os textos em tais conjuntos pode ser usada a abordagem train- and- test, que usamos em nossa pesquisa.
Uma vez definidos os conjuntos de treino e teste por categoria, pode- se aplicar a metodologia de categorização de textos baseada em estruturas SFCA, a qual se divide em duas etapas detalhadas a seguir.
Etapa 1: Corresponde à geração das regras do classicador.
Para denir- las, é necessário:
Construir uma estrutura SFCA para cada categoria a partir de o respectivo conjunto de treino.
Cada estrutura SFCA deve ser gerada conforme descrito na Seção anterior.
Extrair de cada estrutura SFCA construída, os q conceitos formais mais gerais.
Em esses conceitos a quantidade de atributos, usualmente, é maior e eles encontram- se próximos à base dos seus reticulados SFCA.
Uma forma de identicar os conceitos mais gerais de uma estrutura SFCA é ordenar- los de forma decrescente, conforme a cardinalidade dos seus atributos.
Após a ordenação dos conceitos dos SFCA, basta escolher os q primeiros de cada estrutura para obtermos os conceitos mais gerais de cada categoria.
Definir as regras do classicador, a partir de os conceitos gerais de cada categoria extraídos no passo anterior.
Cada conceito formal gera uma regra do classicador.
As regras devem possuir o formato descrito a seguir, em o qual os objetos e atributos dos conceitos formais constituem as premissas e as categorias, as conclusões.
Cabe ressaltar que os atributos que compõem as premissas devem conter apenas sintagmas nominais.
A exclusão dos papéis semânticos teve como propósito evitar que, durante a classificação, o conjunto de teste tenha que ser anotado com tais informações semânticas.
Etapa 2: Refere- se à categorização dos documentos propriamente dita.
Tal categorização será realizada com base nas regras do classicador construído na Etapa 1.
Desta forma, para classicar um documento d jdo conjunto de teste é preciso:
Extrair os sintagmas nominais do documento dj, formand conjunto t (d) j.
Calcular o fator de ativação (fa) de d r j para cada regra i do classicador.
Este fator é calculado conforme apresentado na Equação 10.1, onde p (r) i corresponde ao conjunto de premissas de ri.
Tal fator estabelece uma proporção entre a quantidade de premissas que casam com t (d) j com o total de premissas da referida regra.
Calcular, para cada categoria ck, o fator de ativação acumulado (faacum) para o documento dj.
Esse fator é calculado conforme apresentado na Equação 10.2, onde q corresponde ao número de regras do classicador em que a conclusão é ck.
A categoria c fa d k com o maior valor acum é definida como a classe do texto j.
Empregamos medidas usuais em categorização de textos, como precisão (Pr), recall Re) (e F1 para avaliar os resultados do classicador baseado em SFCA.
A qualidade desse classicador está relacionada ao seu conjunto de regras.
Para obter um bom conjunto de regras, é interessante realizar testes variando a quantidade q de conceitos formais extraídos das estruturas SFCA.
É aconselhável utilizar o valor q que gerar a melhor medida F1 para o conjunto de teste usado na etapa de categorização.
Em essa Seção, indicamos estudos que podem melhorar a abordagem apresentada, quanto a o enriquecimento de estruturas FCA com informações semânticas.
São eles:
Estudo mais aprofundado de estruturas RCA:
Embora tenhamos descartad uso da estrutura RCA em nossa pesquisa, acreditamos que tal extensão deva ser estudada em maior profundidade.
Possivelmente, a inclusão de mais relações tais como do tipo substantivoadjetivu mesmo substantivo-advérbio contribua na qualificação dos conceitos formais.
Essa qualificação pode tornar mais viável a generalização de relações entre objetos para relações entre conceitos, proposta por o método RCA, para construção de estruturas conceituais.
Estudo de heurísticas e métodos para identificação de classes de verbos:
De acordo com nosso estudo, as classes ajudam a construir conceitos formais mais coesos.
No entanto, não conseguimos analisar essa contribuição para estruturas FCA a partir de corpora não anotados com tal informação, devido a a incompletude da VerbNet.
A implementação dessas heurísticas e métodos viabilizaria também um estudo mais aprofundado das relações transversais mediadas por papéis semânticos, e sua relevância em diferentes domínios.
Assim como é realizado por algoritmos usados em regras de associação, poderíamos denir fatores de suporte e conança para selecionar os conceitos e as regras durante o processo de classicação.
OWL: Isso potencializaria a utilidade da proposta, visto que tornaria mais fácil o seu uso para a criação de ontologias.
Cabe mencionar que, nesse caso, seria necessário primeiramente converter a estrutura FCA para uma hierarquia de conceitos, antes de exportar- la para OWL.
O procedimento de transformação de uma estrutura FCA numa hierarquia é descrito por Cimiano em.
Organização de nossa abordagem na forma de um framework, incluindo as métricas estruturais usadas e, se possível, um módulo de anotação semântica.
Isso facilitaria a geração e teste de estruturas FCA baseadas em nossa abordagem para outros corpora e domínios.
Estudo quanto a a inclusão das restrições semânticas associadas aos papéis, nos conceitos formais.
Tal inclusão poderia qualicar, ainda mais, os conceitos formais.
Estudo de outras heurísticas, para reduzir ainda mais a complexidade de construção das estruturas FCA, sem perda de informação semântica relevante.
Estudo da aplicação de nossa abordagem na realização de outras tarefas, tais como agrupamento de textos, enriquecimento de textos para categorização, suporte a sistemas de pergunta-e-resposta, entre outros.
Estudo da identificação de instâncias a partir de papéis semânticos e seu uso em estruturas do tipo FCA.
Durante o doutoramento, além de as publicações em preparação, tivemos a seguinte produção:
