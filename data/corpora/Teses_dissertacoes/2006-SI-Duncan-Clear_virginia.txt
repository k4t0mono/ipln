As organizações de software trabalham com diversos projetos de software que se diferenciam tanto por as ferramentas de gestão utilizadas quanto por a forma que armazenam e controlam suas métricas de acompanhamento.
Sendo assim, a inexistência de um repositório centralizado de dados dificulta o acompanhamento dos Processos de Desenvolvimento de Software (PDSs) dessas organizações.
Uma das etapas mais cruciais do Processo de Descoberta de Conhecimento em Banco de Dados é o processo de Extração, Transformação e Carga (Etc), pois este tem como finalidade a transformação dos dados brutos, extraídos de diversas fontes, em informações consistentes e de qualidade.
Considerando que os PDSs possuem suas especificidades, realizou- se um estudo num ambiente real e verificou- se que, em termos de ferramentas, são utilizadas desde planilhas eletrônicas (e.
g MS Excel) até ferramentas para controle da execução de atividades de projetos (e.
g MS Project Server, IBM Rational Clear Quest, Bugzilla).
Detectou- se ainda o uso de diferentes modelos de PDS, com ciclos de vida variados para projetos distintos, que se traduzem em formas totalmente diversas de registrar estes projetos, ainda que na mesma ferramenta.
Outro problema é que cada uma dessas ferramentas possui um modelo de dados próprio, que não segue padronizações estabelecidas de representação de dados, dificultando assim a extração desses dados.
Por conseqüência, o grau de complexidade do processo de Etc, para esta organização, é muito alto.
O modelo proposto neste trabalho tem por mérito tratar, de forma integrada, dois aspectos:
1) a coleta de dados dos projetos de forma não intrusiva, levando em consideração vários tipos de heterogeneidade, 2) a transformação e integração desses dados, proporcionando uma visão organizacional unificada e quantitativa dos projetos.
Esses aspectos são tratados utilizando uma arquitetura orientada a serviços.
A abordagem orientada a serviços busca lidar com vários tipos de heterogeneidade, tanto do ponto de vista organizacional (e.
g especializações do Processo de Software Padrão da Organização (OSSP ­ Organization's Standard Software Process) que resultam em formas distintas de desenvolvimento e registro de fatos sobre projetos), quanto do ponto de vista técnico (e.
g diferentes ferramentas).
Essa heterogeneidade é convenientemente tratada através de serviços que atuam como wrappers dos diferentes tipos de extratores, que suporta um ambiente distribuído de desenvolvimento.
Para avaliação da abordagem proposta, foram desenvolvidos três exemplos, que consideram todas essas questões de heterogeneidade:
Diferentes tipos de projetos, diferentes ciclos de vida, diferentes modelos de gerenciamento e diversas ferramentas de apoio ao acompanhamento.
Palavras Chave: Data Warehousing, Arquiteturas Orientadas a Serviços e Processos de Desenvolvimento de Software.
Qualidade de software é um processo sistemático que focaliza todas as etapas e artefatos produzidos com o objetivo de garantir a conformidade de processos e produtos, prevenindo e eliminando defeitos.
A qualidade dos Processos de Desenvolvimento de Software (PDSs) pode ser quantificada através da utilização de um programa de métricas que auxilie as organizações a organizar, monitorar, detectar e prevenir falhas, tanto do processo quanto do produto final.
As organizações de desenvolvimento de software demonstram grande preocupação com a definição de métricas que representem suas áreas de qualidade.
Porém, a simples definição dessas métricas de qualidade não é a garantia de sua utilização, devido a a dificuldade de se coletar e apresentar os dados relativos aos diferentes projetos, de forma unificada.
As organizações de software buscam formas de gerenciar e acompanhar seus PDSs, seguindo modelos de capacidade1 de software (e.
g Capability Maturity Model (CMM), Capability Maturity Model Integrated (CMMI), Software Process Improvement and Capability Determination (SPICE),).
Esses modelos possuem os elementos necessários para tornar um PDS mais eficiente e controlado, pois focam na proposta de sua melhoria contínua, trazendo disciplina e controle no desenvolvimento e manutenção de software.
No entanto, seguir estes modelos nem sempre é uma tarefa simples.
As organizações buscam gradativamente adquirir novas competências e incrementar seu nível de eficiência e maturidade em relação a os diversos processos críticos envolvidos num desenvolvimento de software.
Para o CMM e CMMI, cada organização define o seu quais devem ser especializados a fim de considerar as particularidades dos diferentes projetos de desenvolvimento de software.
Apesar de a tradução mais apropriada para Capability ser competência, será usado, neste trabalho a forma capacidade, mais em voga em Ciência da Computação.
Os modelos CMM e CMMI definem que a construção de uma Base Organizacional (BO) é um dos requisitos necessários ao amadurecimento das organizações de desenvolvimento de software.
Esta base deve prover o armazenamento e a manipulação dos dados relativos aos diferentes projetos, segundo uma visão unificada da organização.
A construção de tal base organizacional não é uma tarefa trivial.
A grande maioria das organizações de software trabalham simultaneamente com projetos que possuem particularidades quanto a o processo de desenvolvimento, ferramentas de gestão adotadas, bem como a forma como geram, armazenam e controlam suas métricas de acompanhamento.
Ainda, distintas organizações diferem entre as suas práticas e OSSP, considerando em particular o nível de maturidade em que se encontram.
Atualmente, não existe uma infraestrutura genérica de apoio aos programas de mensuração de organizações de software que considere todos estes pontos de variabilidade.
Propostas específicas, discutidas no Capítulo 8, podem ser encontradas em.
Este trabalho propõe um modelo não intrusivo de Extração, Transformação e Carga (Etc) que suporte diversos fatores de heterogeneidade em PDS utilizando padrões orientados a serviços, tais como Extensible Markup Language (XML), Simple Object Access Protocol (SOAP), Web Services Description Language (WSDL) e Universal Description, Discovery and Integration (UDDI), e utiliza como cenário uma organização de software certificada CMM nível 2 que busca certificação CMM nível 3.
Este modelo trata a heterogeneidade dos projetos, e das ferramentas de captura e armazenamento dos dados referentes a cada projeto, através de serviços que atuam como wrappers dos diversos aplicativos presentes nos PDSs.
O armazenamento de dados dos PDSs, por o ponto de vista organizacional, é realizado através de um modelo analítico multidimensional voltado ao acompanhamento de processos e baseado em métricas da organização.
Diversos aspectos apresentados neste trabalho são objetos do artigo aprovado no 1s t International Workshop on Business Process Monitoring&amp; Performance Management intitulado como A A organização de software alvo do estudo de caso (Capítulo 6), apresenta heterogeneidade em termos de as ferramentas de gestão utilizadas no controle das atividades dos projetos, onde são utilizadas desde planilhas eletrônicas (MS Excel) até ferramentas dedicadas a este fim (e.
g MS Project Server, IBM Rational Clear Quest, Bugzilla).&amp;&amp;&amp;
Verificouse também que essa organização possui distintos modelos de PDS, os quais apresentam tipos de projetos distintos e ciclos de vida variados, que resultam em diversas maneiras de registrar seus projetos, mesmo que na mesma ferramenta.
Ainda, as ferramentas utilizadas muitas vezes não seguem paradigmas convencionais de representação de dados.
Todos esses fatores descritos conduzem a um Processo de Etc com um alto grau de complexidade.
O estudo de um ambiente real, juntamente com o estudo sobre o estado da arte em relação a os fatores que afetam a implementação do processo de Etc para PDS, conduziram a seguinte questão de pesquisa para estudo:
Como capturar dados de execução de processos de desenvolvimento de software tal que seja possível acompanhar tais processos?
Capturar, nesse contexto, significa prover mecanismos de extração não intrusivos e que considerem a heterogeneidade das fontes de dados.
Essa captura deve prover uma menor latência dos dados e os processos de limpeza e transformação devem buscar garantir dados sem inconsistências.
Estes dados, quando atualizados, consistentes e de qualidade, podem ser utilizados por os gestores no acompanhamento de seus PDS.
Para tratar a heterogeneidade das fontes, observa- se que a utilização de conceitos de computação orientada a serviços apresentase para essa proposta como uma solução atual e viável.
O foco desse trabalho está no tratamento adequado da heterogeneidade dos dados pois, após analisarmos a organização de software em questão, verifica- se que ela não trabalha com um volume muito grande de dados, mas sim com dados heterogêneos que são constantemente atualizados.
Essa heterogeneidade nas fontes de dados influência na sua captura, pois quanto maior a freqüência de atualização destes, mais extrações, transformações e cargas são necessárias para manter os dados do Data Warehouse (DW) atualizados.
Uma discussão mais exaustiva desse aspecto é apresentada no Capítulo 6.
O objetivo principal deste trabalho é propor uma abordagem para captura de métricas de PDS fazendo uso da computação orientada a serviços.
Esse modelo visa a solucionar alguns problemas na obtenção de métricas organizacionais:
A heterogeneidade em relação a as ferramentas de gestão utilizadas e a forma como as organizações de software controlam e armazenam suas métricas de acompanhamento.
Os objetivos específicos estão descritos como segue.
Definir uma estrutura genérica para extração, transformação e carga de dados através de uma arquitetura orientada a serviços.
Integrar as fontes de dados e ferramentas utilizadas durante o processo de desenvolvimento de software de forma não intrusiva, através da utilização de wrappers.
Possibilitar que as informações contidas no DW sejam mantidas atualizadas, coesas e consistentes, para que os gestores as utilizem no controle e acompanhamento de seus PDS.
Segundo &quot;todo trabalho científico deve ser apreciado em procedimentos metodológicos».
Quanto a os objetivos classifica- se esta pesquisa como exploratória, pois se tem como finalidade a descoberta de teorias e práticas que modificarão as práticas existentes e principalmente fornecerão as inovações tecnológicas necessárias.
Quanto a o procedimento, inicialmente foi realizado um estudo de um ambiente real numa organização de software certificada CMM Nível 2, onde foram identificadas limitações no controle e acompanhamento de seus processos devido a a dificuldade de obter métricas em ambientes heterogêneos.
Ainda, foi utilizado o procedimento experimental para avaliação da solução proposta.
A Figura 1 representa as principais atividades envolvidas no desenvolvimento deste trabalho.
As atividades em negrito no diagrama de atividade representam as atividades macro:
Estudar Fundamentação Teórica, Realizar Estudo de Caso, Desenvolver Protótipo e Avaliar Solução Proposta.
Para cada uma dessas atividades macro são apresentadas as atividades que foram desenvolvidas.
Inicialmente foram estudadas as principais fontes sobre o PDS, qualidade no PDS, processo de Etc e A os (Arquiteturas Orientadas a Serviços), buscando um entendimento geral.
Posteriormente foi realizado o estudo de um ambiente real, onde foi caracterizado o problema a ser tratado nesta pesquisa.
Identificado o problema, foi possível definir a questão de pesquisa e objetivos deste trabalho, e direcionar a pesquisa às abordagens relacionadas ao problema.
Primeiramente o estudo enfocou- se em propostas específicas relacionadas ao Processo de Data Warehousing para PDS.
Posteriormente à definição dos mecanismos que seriam utilizados, o passo seguinte consistiu no desenvolvimento de uma estrutura para capturar métricas de PDS utilizando uma abordagem orientada a serviços.
Buscando avaliar este protótipo utilizou- se a experimentação e para tal, foram definidos os objetivos, questões e os critérios de experimentação.
Este trabalho está dividido em 10 capítulos organizados como segue.
O Capítulo 2 discute o cenário atual do PDS, apresentando conceitos de processos padrão, bem como este pode ser especializado.
O Capítulo 3 discorre sobre a Qualidade no PDS, focando questões sobre mensuração e maturidade dos processos.
O Capítulo 4 descreve os componentes principais de uma A os e como esta pode auxiliar no tratamento adequado da heterogeneidade encontrada em PDS, e ao final deste, uma comparação de A os com outras tecnologias existentes.
O Capítulo 5 discorre sobre o Processo de Descoberta de Conhecimento em Banco de Dados, focando principalmente no Processo de Etc e as implicações que ambientes heterogêneos acarretam ao processo de Etc, bem como possíveis soluções para este problema.
O Capítulo 6 relata o estudo realizado em organização de software certificada CMM Nível 2, que busca a certificação CMM Nível 3.
Já o Capítulo 7 descreve a solução proposta, uma abordagem orientada a serviços para captura de métricas de PDS.
O Capítulo 8 discorre sobre a experimentação da solução utilizando três exemplos que possuem características heterogêneas entre si.
O Capítulo 9 apresenta os trabalhos relacionados comparando- os com nossa abordagem.
O Capítulo 10 discorre sobre as conclusões, limitações e trabalhos futuros.
Posteriormente, encontram- se as referências bibliográficas pesquisadas e os demais anexos.
Este capítulo tem por objetivo relatar o cenário atual de desenvolvimento de software, e para tal apresenta o conceito de processo de desenvolvimento de software padrão, bem como seus elementos principais e relacionamento entre estes e ainda, como um processo padrão pode ser especializado, proporcionando sua usabilidade.
A IEEE define um processo como:
&quot;uma seqüência de passos realizados com um dado propósito».
Define um PDS como um conjunto completo de atividades necessárias para transformar os requisitos de usuários em produtos de software.
Para, este processo pode ser definido como uma estrutura para as tarefas que são necessárias à construção de um software de alta qualidade.
Ainda, segundo, o PDS é definido como um conjunto coerente de políticas, estruturas organizacionais, tecnologias, procedimentos e artefatos que são necessários para compreender, desenvolver e manter um produto de software.
Atualmente, as variáveis envolvidas no PDS têm um nível crescente de complexidade:
Reduzir custos e tempo, aumentar a produtividade e, ainda assim, garantir a qualidade do produto final.
Sendo assim, para garantir a qualidade de um software deve- se estabelecer uma cultura de não-tolerância a erros, ou seja, buscar estruturar processos de forma que eles possuam mecanismos para inibir e impedir falhas, possibilitando que os diversos artefatos gerados durante o ciclo de desenvolvimento tenham procedimentos que avaliem sua qualidade e que possibilitem a identificação prematura de defeitos nesses artefatos.
Segundo definir um processo de software padrão a ser utilizado por a organização é fundamental na obtenção de altos níveis de maturidade.
Atualmente, além de a definição de um PDS, a implantação de um processo padrão de desenvolvimento de software nas organizações tem sido fortemente demandada por as organizações de software e, neste sentido, vários processos têm sido sugeridos por organizações e companhias internacionais Um fundamental conceito na definição de processo, no CMM, é o Processo de Software Padrão da Organização (OSSP ­ Organization's Standard Software Process).
Um OSSP é a definição operacional de procedimentos básicos que guiam o estabelecimento de um processo de software comum entre os projetos de software da organização.
Ele descreve os elementos principais do processo de software que cada projeto de software deve incorporar, bem como os relacionamentos entre esses elementos de software da organização, e é essencial para a estabilidade e melhoramento do PDS.
Em o nível gerencial, o OSSP necessita ser descrito, gerenciado, controlado e melhorado de uma maneira formal.
Isso se torna possível através da utilização de práticas chave de definição de processos organizacionais (ver Figura 2).
Em o nível de projeto, a ênfase está na usabilidade do OSSP e, dessa forma, é necessário definir um Processo de Software Definido do Projeto (PDSP ­ Project's Defined Software Process).
O PDSP é uma especialização do OSSP ajustado às características do projeto.
Este deve ser entendido e bem caracterizado e, para tal, deve ser descrito em termos de padrões, procedimentos, ferramentas e métodos de software.
A parte superior da Figura 2, extraída de, apresenta como as organizações estabelecem e mantêm um conjunto de processos de software.
Esse conjunto de processos inclui:
A base de dados do processo de software da organização, ou seja, um repositório que armazene as métricas dos projetos da organização (Organization's Software Process Database) o A base de dados de processos da organização2 é uma base de dados estabelecida para coletar e disponibilizar dados sobre o processo de software, bem como dos produtos de trabalho gerados.
Exemplos de dados de processos e produtos de trabalho incluem esforço, custo e tamanho de software estimado, produtividade, efiCiência de Peer Review e o número de defeitos encontrados e a severidade destes.
A biblioteca com os documentos relacionados ao processo de software Em este trabalho, denominada Base Organizacional (BO) o Uma biblioteca de documentos relacionados ao processo de software é estabelecida para armazenar documentos do processo que são potencialmente úteis para projetos correntes e futuros e tornar- los disponíveis para que sejam compartilhados entre os projetos.
A descrição dos ciclos de vida aprovados para utilização (Description of the Software Life Cycles) o Um ciclo de vida de software é o período de tempo que se inicia quando um produto de software é concebido e termina quando um produto de software não está distante de ser disponibilizado para uso.
O ciclo de vida de software tipicamente inclui conceitos de estágios:
Estágio de requisitos, design, implementação, teste, instalação, operacional e manutenção.
A Seção 2.1.1 descreve dois entre os ciclos de vida mais utilizados.
As diretrizes e critérios para especialização do OSSP (Guidelines and Criteria for Tailoring the Organization's Standard Software Process) o As diretrizes são estabelecidas para auxiliar os projetos a selecionar o ciclo de vida de software adequado entre aqueles aprovados para o uso e especializar o OSSP e o ciclo de vida de software selecionado para ajustarem- se às características específicas do projeto.
Esses guidelines e critérios ajudam a garantir que exista uma base comum entre todos os projetos de software para planejamento, implementação, Process), incluindo a arquitetura e a descrição dos elementos do processo de software:
A arquitetura do processo de software é uma descrição de alto nível da OSSP.
Ela descreve a ordem, interfaces, interdependências e outros relacionamentos entre os elementos do processo de software do OSSP.
Cada elemento do processo abriga um bem definido e limitado conjunto de tarefas (e.
g elementos relacionados a estimativas de software, projeto de software, codificação e Peer Review).
A parte inferior da Figura 2, ilustra a especialização do OSSP para adequar- se às características específicas de um projeto.
Esta especialização inclui:
Analisar os requisitos do projeto, selecionar um ciclo de vida de entre os aprovados por a organização e desenvolver seu próprio PDSP.
O PDSP provê a base para o planejamento, execução e aprimoramento das atividades, tanto de gestão quanto operacionais.
O Plano de Desenvolvimento de Projeto de Software (Project's Software Development Plan) define como um projeto deve ser executado.
Por exemplo, define quais recursos irão executar quais atividades, baseado num cronograma.
As tarefas do projeto estão divididas em estágios.
Esses estágios representam um significante e mensurável conjunto de atividades que são executadas por o projeto.
O resultado dessas atividades e tarefas consiste em produtos de trabalho (Project's Results and Software Work Products) Ciclos de vida A variedade de ciclos de vida que podem ser utilizados por as organizações de desenvolvimento de software deve- se ao fato dessas produzirem software para uma variedade de clientes e usuários com realidades distintas.
Entre os ciclos de vida mais utilizados podemos citar:
Ciclo de vida em Cascata:
Este ciclo de vida utiliza uma abordagem seqüencial para o desenvolvimento, iniciando por a Definição dos Requisitos, Projeto, Implementação e Teste Unitário, Integração e Teste de Sistema, e Operação e Manutenção (ver Figura 3).
Durante a fase final, Operação e Manutenção, o software é colocado em uso, e erros e omissões nos requisitos originais do software podem ser descobertos, necessitando assim a correção dos erros e a implementação de novas funcionalidades.
Essas mudanças podem ocasionar a repetição de alguns ou todos os estágios anteriores.
Este ciclo de vida é recomendado para projetos grandes e complexos em os quais os requisitos são bem definidos.
Ciclo de vida Evolutionary Development ou Iterativo:
Iterações são pequenas etapas do ciclo de vida definidas por seqüências distintas de atividades com planejamento detalhado e critérios de validação, resultando sempre em algum produto, interno ou externo.
Como o planejamento das iterações nunca é todo feito no início do projeto os riscos de mau planejamento reduzem- se, pois os elementos são integrados progressivamente. Segundo,
ver Figura 4, o ciclo de vida iterativo possui atividades de especificação, desenvolvimento e validação separadamente.
Essas atividades são executadas de forma concorrente e entre elas existe um feedback.
Esse capítulo apresentou conceitos relacionados ao PDS, bem como a fundamental importância de definir um processo padrão, o qual deve ser especializado conforme necessidade dos projetos.
Ainda, a Seção 2.1 ressaltou que mesmo possuindo PDS padrão, existe heterogeneidade, ou seja, que um processo de software definido não é garantia de homogeneização.
Sendo assim, os projetos partem de uma estrutura homogênea organizacional, OSSP, para uma estrutura heterogênea de projetos, PDSP, resultando em, diferentes tipos de projetos, diferentes ciclos de vida, diferentes modelos de gerenciamento, bem como diferentes ferramentas para o acompanhamento e gestão desses projetos.
Este capítulo tem por objetivo discorrer sobre a qualidade no PDS, e para tal descreve mensuração de PDS e maturidade organizacional através do modelo de capacidade de software CMM.
Ainda, apresenta o conceito de métricas, bem como suas classificações.
Essas métricas são utilizadas para mensurar a qualidade dos PDS auxiliando- os a organizar, monitorar e prevenir falhas.
Segundo, as atividades do processo de QA (Quality Assurance) envolvem definir ou selecionar padrões que devam ser aplicados ao PDS e/ ou ao produto de software.
Esses padrões podem ser integrados a procedimentos ou processos que devam ser aplicados durante o desenvolvimento.
Os processos podem ser apoiados por o uso de ferramentas que integrem o conhecimento dos padrões de qualidade.
Existem dois tipos de padrões que podem ser estabelecidos como parte do processo de garantia da qualidade:
O controle de qualidade envolve supervisionar o PDS, a fim de assegurar que os procedimentos e os padrões de garantia da qualidade sejam seguidos.
Com o passar dos anos, muitas organizações começaram a perceber que seu problema principal residia na inabilidade para gerenciar o processo de desenvolvimento de software.
À medida que uma organização vai se tornando madura e capaz, o processo de software vai ficando mais definido, possibilitando que o mesmo seja implementado de modo mais consistente em toda a organização.
A capacidade do processo de software descreve a gama de resultados esperados que podem ser alcançados com a aplicação do processo de software, bem como, formas de se prever os resultados mais prováveis a serem esperados no próximo projeto a ser empreendido por a organização.
Já a maturidade do processo de software é a extensão para a qual um processo específico é explicitamente definido, gerenciado, medido, controlado e efetivado.
Em uma organização madura, o processo de software é compreendido ­ o que geralmente é feito por meio de documentação e treinamento ­ e está sendo continuamente monitorado e melhorado por os seus usuários.
A capacidade de um processo de software maduro é conhecida.
A maturidade de um processo de software implica que a produtividade e a qualidade resultantes do processo possam ser continuamente melhoradas através de ganhos consistentes na disciplina alcançada com a sua utilização.
Essa melhoria se deve à estrutura estabelecida por o CMM, onde o projeto percorre um caminho evolutivo que incrementa, em níveis, a maturidade do processo de software.
A estrutura da maturidade ordena esses níveis pré-estabelecidos (Initial, Repeatable, Defined, Managed e Optimizing), onde os resultados positivos alcançados em cada nível são utilizados como embasamento para o próximo, objetivando melhorias no processo em sua totalidade.
De essa forma, uma estratégia de melhoria projetada a partir de uma estrutura de maturidade de processo de software orienta quanto a o caminho a ser seguido para a contínua melhoria do processo de software.
O resultado desse trabalho foi o modelo CMM, que tem como foco o processo de software na proposta de melhoria contínua, trazendo disciplina e controle no desenvolvimento e manutenção do software.
Essas são as chaves para aperfeiçoar a qualidade do desenvolvimento de produtos de software.
Capability Maturity Model O CMM definido por o Sei (Software Engineering Institute) descreve uma estrutura de trabalho que possui todos os elementos necessários para tornar um PDS mais eficiente e controlado.
O CMM baseia- se num modelo evolutivo de maturidade, em o qual as organizações partem de uma total falta de controle e gerenciamento dos processos (maturidade organizacional) para gradativamente adquirir novas competências, incrementando seu nível de eficiência e maturidade em relação a os diversos processos críticos envolvidos num desenvolvimento de software.
Essa estrutura em níveis do CMM é baseada em princípios de qualidade do produto e esses princípios foram adaptados por o Sei dentro de a estrutura de maturidade que estabelece a gestão de projeto e os fundamentos de engenharia para o controle quantitativo do processo de software, que é a base para a contínua melhoria do processo.
A Figura 53, ilustra o modelo CMM que possui 5 níveis de maturidade:
Initial, Repeatable, Defined, Managed e Optimizing.
Cada nível de maturidade (Maturity Levels) indica a capacidade de um processo e é composto de várias áreas-chave de processo (Key Process Areas), por exemplo, uma área chave de processo do nível 2 é a Gerência de Configuração de Software (Software Configuration Management).
Cada área-chave de processo é organizada em cinco seções denominadas características comuns (Common Features), são elas:
comprometimento para executar, habilidade para executar, mensuração, análise, e verificação da implementação.
Essas características comuns especificam as práticas-chave (Key Practices) que, quando tratadas coletivamente, atingem as metas previstas para a área-chave de processo.
Cada nível da maturidade fornece uma fundação para a melhoria contínua do processo.
Cada área chave de processo compreende um conjunto de objetivos que, quando satisfeitos, estabilizam um componente importante do processo de software.
Obtendo um nível de maturidade, institucionaliza- se um componente diferente no processo de software, que tem por finalidade um aumento total na potencialidade do processo da organização.
Por exemplo, no nível 2 a capacidade de processo de uma organização foi elevada de uma situação &quot;adhoc «para uma situação repetível, por meio de o estabelecimento de sólidos controles de gestão de projeto.
Com exceção do nível 1, cada nível de maturidade é decomposto em várias áreaschave de processo, que indicam as áreas em as quais uma organização deveria fixar seus esforços para a melhoria de seu processo de software.
As áreas-chave de processo identificam os assuntos que devem ser tratados para se obter um determinado nível de maturidade.
Nenhuma empresa consegue sair do nível 1 e chegar ao nível 3 sem antes passar por o nível 2.
Cada nível é um pré-requisito para o outro e cada organização consegue enxergar somente o que a sua maturidade permite.
Apesar disso, as organizações podem usar de maneira vantajosa processos descritos em níveis de maturidade superiores aos que se encontram.
Segundo apud, a mensuração é muito importante.
Se não é possível medir, não é possível controlar.
Se não é possível controlar, não é possível gerenciar.
E, se não é possível gerenciar, consequentemente, não é possível melhorar.
Segundo, um software deve ser medido por diversas razões:
Indicar a qualidade do produto;
Avaliar a produtividade das pessoas que produzem o produto;
Avaliar os benefícios (em termos de produtividade e qualidade) derivados de novos métodos e ferramentas de software;
Formar uma linha básica para estimativas;
Ajudar a justificar os pedidos de novas ferramentas ou treinamento adicional.
Em contradição a, argumenta que o uso da medição e de métricas sistemáticas de software ainda é relativamente incomum.
Ele afirma que existe uma relutância em introduzir a medição, porque os benefícios não são bem definidos.
E afirma que uma razão para isso é que, em muitas empresas, os processos de software utilizados ainda são organizados de maneira precária e não estão suficientemente aperfeiçoados para fazer uso de medições.
Outra razão é que não há padrões para as métricas, e daí decorre o suporte limitado para a coleta e análise de dados.
O mesmo autor afirma, ainda, que a maioria das empresas não estará preparada para introduzir medições até que esses padrões e essas ferramentas estejam disponíveis.
Afirma que as métricas de software podem ser particionadas em três domínios diferentes:
Métricas de produto, métricas de processo e métricas de projeto.
Métricas de produto descrevem as características dos produtos tais como tamanho e complexidade.
Métricas de processo podem ser usadas para melhorar a manutenção e desenvolvimento do software, por exemplo, a métricas eficiência de remoção de defeitos.
Métricas de projeto descrevem as características e a execução do projeto.
Exemplos incluem custo, cronograma e produtividade.
Ainda, segundo, métricas de qualidade de software são um subconjunto das métricas de software que focam aspectos de qualidade dos processos, produtos e projetos.
Definição de métricas A IEEE define um atributo como &quot;uma propriedade física ou abstrata mensurável de uma entidade».
Um fator de qualidade é um tipo de atributo, ou seja, um atributo orientado a gestão de software que contribui para sua qualidade.
Uma métrica é uma função mensurável e uma métrica de qualidade de software é &quot;uma função cujas entradas sejam dados do software e cuja saída seja um único valor numérico que possa ser interpretado como o grau que o software possui, dado um atributo que afete sua qualidade».
Em este trabalho, métrica e medida são consideradas sinônimos.
Classificação de métricas A IEEE classifica as métricas de software em medidas diretas e medidas indiretas, e define que uma métrica direta é uma métrica que não depende da medida de nenhum outro atributo, nem necessita de validação.
Já uma métrica indireta depende de outros atributos e necessita ser validada.
De acordo com a Iso, as métricas podem ser divididas em métricas base e derivadas.
Métricas base fornecem uma indicação quantitativa de um atributo.
Métricas derivadas relacionam- se a métricas base com a finalidade de proporcionar uma informação quantitativa sobre um processo ou um produto.
Elas são importantes para as atividades de gerenciamento de software e para o controle de desempenho dos processos, tanto no âmbito da organização como em cada projeto.
Em oposição à que diz que uma métrica direta é pressupostamente válida, afirma que todas as métricas devem ser validadas.
Ainda, segundo a diferença entre métrica direta e indireta, ou métrica derivada, está em dizer que a métrica direta é uma função métrica cujo domínio é somente de um atributo e o domínio de uma métrica indireta é formado por uma n-tupla.
Exemplos de métricas derivadas em engenharia de software são:
Produtividade: Número de linhas de código/ tempo.
Densidade de defeitos:
Número de defeitos/ tamanho.
Estabilidade dos requisitos:
Número de requisitos inicial/ número total de requisitos.
Número de defeitos, tamanho e número de linhas de código são exemplos de métricas diretas.
Framework de métricas de qualidade de software O uso de métricas de software reduz a subjetividade na estimativa e no controle da qualidade de software por prover uma base quantitativa para a tomada de decisão.
Entretanto, a utilização de métricas de software não elimina a necessidade do julgamento humano em estimativas de software.
O uso de métricas de software, nas organizações e em seus projetos, através da mensuração dos projetos, busca tornar a qualidade do software mais visível.
A Figura 6 ilustra o framework de métricas de qualidade de software proposto por a IEEE.
Ele permite adicionar, excluir e modificar fatores e subfatores de qualidade, bem como métricas.
Cada nível pode ser expandido para diversos subníveis.
O framework pode assim ser aplicado para todos os sistemas e pode ser adaptado como apropriado sem mudar o seu conceito básico.
O primeiro nível do framework de métricas de qualidade de software começa com o estabelecimento dos requisitos de qualidade, que descrevem a qualidade do sistema (na Figura 6 representado por Software quality system X).
Associada a cada fator de qualidade está uma métrica direta que serve como uma representação quantitativa desse fator.
Em o segundo nível, estão os subfatores de qualidade que representam atributos orientados ao software que indicam a qualidade.
Esses subfatores podem ser obtidos por a decomposição de cada fator de qualidade em atributos de software mensuráveis e podem corresponder a mais de um fator de qualidade, pois são atributos independentes do software.
Em o terceiro nível, estão as métricas decompostas dos subfatores utilizadas para medir produtos e processos durante todo o ciclo de vida de desenvolvimento do software.
Realizando uma análise top-down percebe- se que o framework facilita:
O estabelecimento dos requisitos de qualidade em termos de fatores de qualidade e por a possibilidade de gerenciar o ciclo de vida do sistema.
A identificação das métricas que estão relacionadas aos fatores e subfatores de qualidade.
Através de uma análise bottom-up percebe- se que o framework habilita a obtenção de uma:
Avaliação dos produtos e processos de software através das métricas.
Análise dos valores das métricas para estimar e avaliar os fatores de qualidade.
Critérios de Validação de Métricas de Qualidade de Software Para, o propósito da validação de métricas é identificar métricas de processos e de produtos que podem prever valores de fatores de qualidade.
Esses fatores são representações quantitativas dos requisitos de qualidade e as métricas devem indicar se os requisitos de qualidade foram alcançados ou serão alcançados no futuro.
Ainda, afirma que validação não significa uma validação universal de todas as métricas para todas as aplicações.
Preferencialmente, refere- se à validação do relacionamento entre um conjunto de métricas e um fator de qualidade para uma dada aplicação.
A IEEE apresenta seis critérios de validação, os quais são expressos em termos de relacionamentos quantitativos entre o atributo sendo mensurado (o fator de qualidade) e a métrica.
São eles:
Correlação: A métrica deve ser linearmente relacionada ao fator de qualidade como medido por a correlação estatística entre a métrica e o fator de qualidade correspondente.
Consistência: Seja F, a variável fator de qualidade e Y a saída da função métrica, M:
FY. M deve ser uma função monotônica, isto é, se f1\&gt; f2\&gt; f3, então devemos obter y1\&gt; y2\&gt; y3.
Rastreamento: Para funções métricas, M:
FY. Como F muda de f1 para f2 em tempo real, M (f) deve mudar imediatamente de y1 para y2.
Previsibilidade: Para funções métricas, M:
FY. Se o valor de Y é conhecido em algum ponto, pode- se predizer o valor de F. Poder discriminativo:
Uma métrica será discriminativa quando estiver entre componentes de software de alta qualidade e componentes de software de baixa qualidade.
O conjunto de valores de métricas associados com o anterior deve ser significativamente maior (ou menor) do que aqueles associados com o último.
Esse critério avalia se uma métrica é capaz de separar um conjunto de componentes de software de alta qualidade de um conjunto de componentes de baixa qualidade.
Confiança: Uma métrica deve demonstrar a propriedade de correlação, rastreamento, consistência, previsibilidade e poder discriminativo em pelo menos P% das aplicações das métricas, onde P é o percentual de sucesso que garante que uma métrica passou por testes de validação.
Esse capítulo apresentou conceitos relacionados à qualidade de software e como os processos de uma organização devem estar estruturados para garantir a qualidade durante o desenvolvimento e do produto final.
Ainda na Seção 3.1, discorreu- se sobre que, à medida que uma organização vai se tornando madura e capaz através da adoção de modelos de maturidade, o processo de software vai se tornando mais definido, possibilitando que o mesmo seja implementado de modo mais consistente em toda a organização.
A construção de uma BO é um dos requisitos para a obtenção da certificação CMM3.
Essa base histórica de dados deve armazenar métricas da execução dos processos, possibilitando que essas métricas sejam utilizadas no planejamento de novos projetos, bem como na análise e melhoria dos projetos e processos já existentes.
Além disso, esse capítulo serviu de fundamentação teórica para o Capítulo 6, onde se discorre sobre o estudo de caso num ambiente organizacional, apresentando para tal o nível de maturidade, programa de métricas, bem como a estrutura da BO dessa organização.
Este capítulo apresenta os componentes principais de arquiteturas orientadas a serviços, focando- se principalmente em relatar como esses componentes interligados podem ser utilizados no tratamento adequado da heterogeneidade, bem como a comparação dessa arquitetura com outras tecnologias.
Segundo a World Wide Web Consortium (W3C), um «Web Service (Ws) é um sistema de software projetado para suportar interações de máquina- à máquina sobre uma rede.
Este sistema tem uma interface descrita num formato processável (especificamente WSDL).
Outros sistemas interagem com o Ws de uma maneira prescrita por sua descrição através de mensagens SOAP, tipicamente transportadas utilizando o Http num formato XML, em conjunto com outros padrões web relacionados».
O uso de tecnologias baseadas em XML para a construção de Ws permite a utilização de serviços sem que haja a necessidade de se saber qual a plataforma ou linguagem de programação foi utilizada na sua construção.
Esta definição detalha como os Ws devem trabalhar, enfatizando que os Ws devem ser capazes de serem definidos, descritos e descobertos.
Esta definição, ainda, evidência o significado de acessível e torna mais concreta a noção de orientada a internet e interfaces baseada em padrões.
Segundo apud, a mais precisa definição de Ws é provida por o Consórcio UDDI que caracteriza o como &quot;self-contained, modular business application that have open, Internet-oriented, standards-based interfaces».
Essa definição é mais detalhada que a apresentada por a W3C, pois enfatiza a necessidade de ser complacente com os padrões de internet.
Além disso, requer que o serviço seja aberto, o que essencialmente significa que ela tenha uma interface publicada que pode ser invocada através da internet.
Uma outra definição apresentada por, é que um Ws é definido como uma forma padronizada de integrar aplicações baseadas na web utilizando XML, SOAP, WSDL e UDDI.
O XML é usado para tag de dados, o SOAP é usado para transferir os dados, o WSDL é usado para descrever os serviços disponíveis e o UDDI é usado para listar quais serviços estão disponíveis.
Uma arquitetura orientada a serviços é essencialmente uma coleção de serviços.
Esses serviços comunicam- se uns com os outros.
A comunicação pode envolver uma simples troca de dados ou dois ou mais serviços que estejam coordenando alguma atividade.
Essa arquitetura fornece o modelo teórico para todos os Ws e contém três entidades e três operações de serviços (ver Figura 7).
Quando o Service Provider deseja oferecer seus serviços, ele publica seus serviços colocando- os nas entradas apropriadas do Service Directory que contém todas as informações sobre todos os serviços que estão disponíveis.
Um Service Requestor usa o Service Directory para encontrar um serviço apropriado, isto é, um serviço que combina com suas exigências.
Em outras palavras, o Service Requestor é uma aplicação que deseja receber um serviço de outra aplicação.
Ela não sabe onde está essa outra aplicação nem como localizar- la.
Então, ela recorre ao Service Directory e requisita uma operação Find ().
O Service Directory é uma aplicação conhecida que retorna informações sobre Ws em resposta a um critério de pesquisa que tenha sido submetido por um Service Requestor na operação Find ().
Essas operações retornam as informações de contato para Ws com base em suas classificações que correspondem ao critério de pesquisa.
Ele também inclui informações sobre como descobrir detalhes de conexão.
O Service Provider publica (Publish()) esses detalhes de conexão, no Service Directory.
Eles fazem isso por a mesma razão que uma empresa compraria um espaço nas páginas amarelas do catálogo telefônico local.
O Service Requestor usa os detalhes de conexão para se vincular (Bind()) ao Service Provider.
Os componentes principais utilizados para a comunicação entre aplicações de Ws, que inter-relacionados empacotam a requisição e a resposta entre um servidor e um cliente objetivando a integração de interações entre aplicações, são apresentados como segue.
Simple Object Access Protocol (SOAP) SOAP, conforme é um protocolo projetado para invocar aplicações remotas através de RPC ou trocas de mensagens, num ambiente independente de plataforma e linguagem de programação.
O protocolo SOAP define o formato em o qual as mensagens transportadas na rede devem ter para encaminhar requisições a serviços web e também define o formato das mensagens de resposta às requisições.
SOAP é, portanto, um padrão normalmente aceito para utilizar- se com Ws.
Desta forma, pretende-se garantir a interoperabilidade e intercomunicação entre diferentes sistemas, através da utilização de uma linguagem XML.
O mecanismo de transporte de mensagens é feito utilizando o Http, o que o torna um protocolo leve.
Esse mecanismo para interconexão de aplicações é formado por três diferentes partes:
Especificação do Envelope SOAP:
Em a especificação são definidas as regras específicas para encapsular os dados que devem ser transferidos, incluindo dados da aplicação, tais como, métodos a invocar, parâmetros ou valores de retorno.
Pode também incluir informações sobre quem deve processar os conteúdos transferidos e, no caso de falha, como codificar mensagens de erro.
Convenções RPC:
As convenções RPC definem como modelar interações no estilo RPC.
Uma mensagem SOAP, ver Figura 8, consiste basicamente nos seguintes elementos:·
Envelope: O envelope define o documento XML como uma mensagem SOAP.
Este pode conter declarações de namespaces e também atributos adicionais como o que define o estilo de codificação.
Um enconding style define como os dados são representados no documento XML.·
Header: O header é um cabeçalho opcional, que carrega informações adicionais como, por exemplo, se a mensagem deve ser processada por um determinado nó intermediário antes de chegar no destino final.
Quando utilizado, o Header deve ser o primeiro elemento do Envelope.·
Body: O body é um elemento obrigatório, e contém o documento com a informação a ser transportada para o seu destino final.
O elemento Body pode conter um elemento opcional Fault, usado para carregar mensagens de status e erros retornadas por os &quot;nós «ao processarem a mensagem.
Web Services Description Language (WSDL) Especificações WSDL são documentos XML que descrevem os Ws, bem como suas interfaces de serviços.
Ela é o núcleo da estrutura dos Ws e estabelece uma forma padronizada de representação dos tipos de dados passados nas mensagens, das operações realizadas sobre estas mensagens e do mapeamento das mensagens sobre os protocolos de transporte.
Essas especificações são frequentemente caracterizadas por uma parte abstrata e uma parte concreta.
Em a parte abstrata são definidos os portTypes, operations, messages e types.
Já na parte concreta são definidos os protocolos de ligação (bindings) e outras informações (service and ports).
Parte Abstrata o Type:
O WSDL necessita que seja especificado o tipo, de forma que os dados a serem trocados possam ser corretamente interpretados durante a comunicação.
Cada parte é caracterizada por um nome e por um tipo, o qual é definido no XML Schema.
Parte Concreta o Interface Bindings:
Essas interfaces servem como elemento de ligação entre os elementos abstratos e os concretos.
Universal Discovery, Description and Integration (UDDI) Após a definição dos dados das mensagens (XML), da descrição dos serviços que receberão e processarão as mensagens (WSDL) e também dos meios de envio e recebimento das mesmas (SOAP), é necessário ter- se uma forma de publicar e divulgar o serviço que se oferece e de encontrar os serviços oferecidos por outros.
Esta é a função do UDDI, que possui uma estrutura que define um modelo de dados em XML e também interfaces de programação SOAP para registrar, descobrir e integrar informações.
Este componente habilita os usuários dos serviços a publicarem e descobrirem estes serviços na web.
Portanto, pode- se fazer uma analogia do UDDI com as páginas amarelas de uma lista telefônica, onde a especificação UDDI é basicamente um diretório de Ws que oferece uma maneira simples de registrar e localizar os serviços.
Em seu núcleo, UDDI consiste de duas partes:
API UDDI: É uma especificação técnica para construir um diretório de Ws.
A informação UDDI é armazenada dentro de um formato específico XML, definido por WSDL e XML Schema.
A especificação inclui detalhes de uma API própria para buscar dados existentes ou publicar novos dados;
UDDI Business Registry:
Também conhecido como &quot;UDDI cloud «é uma implementação operacional completa da especificação UDDI.
Tal parte habilita qualquer um a buscar dados UDDI existentes, e também, a qualquer empresa registrar- se a si própria e seus respectivos serviços.
O processo de descoberta de um serviço (discovery) é feito através de registries.
Estes, são repositórios contendo documentos que descrevem os negócios.
Um registry pode localizar os serviços disponíveis publicamente que sejam adequados às necessidades de uma organização.
Os registries do UDDI podem ser:
Públicos: Os registros públicos permitem que as informações publicadas no registro possam ser examinadas por todos;
Privados: Os registros privados são encobertos por um firewall de uma organização, sendo acessíveis apenas por os seus membros;
Semiprivados: Os registros semiprivados, são aqueles que possuem um acesso restrito fora de a organização.
Os Ws são, na verdade, uma evolução das tecnologias CORBA (Common Object Request Broker Architecture), DCOM (Distributed Common Object Model) e RMI (Remote Method Invocation).
A Tabela 1 compara as A os a outras tecnologias, em relação a suas características principais, interlinguagem, interplataforma, tradução dos dados, vantagens e desvantagens.
Percebe- se com essa comparação que A os são superiores as demais tecnologias, tendo como principal característica a interoperabilidade.
O primeiro aspecto apresentado é a característica principal dessas tecnologias.
Posteriormente discute- se o suporte a diversas linguagens:
O RMI não satisfaz esse quesito por limitar- se a utilização da Linguagem Java.
Em o suporte a interplataforma, a tecnologia DCOM não provê suporte, pois somente funciona em plataformas Microsoft.
Em relação a tradução dos dados, o CORBA considera esse aspecto através da IDL (Interface Definition Language), embora bem mais complexo que o XML, solução adotada por as A os.
A o final são apresentadas as vantagens e desvantagens dessas tecnologias.
Característica Principal Interlinguagem Interplataforma Tradução de Dados O uso de tecnologias baseadas em XML para a construção de Ws permite a utilização de serviços sem que haja a necessidade de se saber qual a plataforma ou linguagem de programação foi utilizada na sua construção.
Mecanismo para construir aplicações cliente-servidor em ambientes heterogêneos Mecanismo específico da Java para realizar chamadas de cliente-servidor, construída sobre a arquitetura de stub/ estrutura Mecanismo da Microsoft para realizar chamadas remotas IDL (Interface Definition Language) Podem manipular cargas de transação mais altas porque mantêm uma conexão persistente entre clientes e servidores à custa de servir menos clientes por servidor.
Podem manipular maiores cargas de transação porque a RMI mantém uma conexão persistente entre clientes e servidores à custa de servir menos clientes por servidor.
Limita- se a uma solução unicamente Java no cliente e no servidor.
Possibilidade de ser construído utilizando diversas linguagens (Visual Basic, C+, etc).
Vantagens Suporte a interoperabilidade.
Utilização do XML Desvantagens Desempenho:
O Http não mantém conexões de longa duração e a necessidade de escolher linguagens eficientes de programação do lado servidor para construir seu serviço.
Complexidade do Funciona somente em plataformas Microsoft.
Este capítulo apresenta conceitos relacionados a arquiteturas orientadas a serviços, discorrendo sobre seus principais componentes, e como esses componentes, quando interligados, podem auxiliar no tratamento adequado da heterogeneidade.
Ainda, ao final deste capítulo compararam- se os Ws com outras tecnologias.
O interesse deste trabalho em tratar a heterogeneidade, não se resume ao fato de diferentes plataformas, linguagens e ferramentas.
Além desses aspectos, existe ainda a heterogeneidade resultante da especialização do OSSP em PDSP, o que resulta em diferentes tipos de projetos, com ciclos de vida diferenciados e formas de gerenciamento variadas O capítulo seguinte, discorre sobre como a partir de fontes heterogêneas é possível extrair dados brutos e transformar- los em informações consistentes e de qualidade, e posteriormente armazenar- las numa fonte integrada de dados.
Já no Capítulo 7, apresenta- se a solução proposta neste trabalho, onde conceitos de arquiteturas orientadas a serviços apresentados aqui são utilizados como uma solução viável e adequada.
Este capítulo discorre sobre o Processo de Descoberta de Conhecimento em Banco de Dados, destacando o Processo de Extração, Transformação e Carga.
As etapas que compõem o processo de Etc são detalhadas, bem como a criticidade deste quando aplicado em ambientes heterogêneos.
Segundo, o processo de Descoberta de Conhecimento em Banco de Dados, iterativa dos seguintes passos:
Limpeza dos Dados:
Etapa em que devem ser removidas as perturbações e as inconsistências dos dados.
Integração dos Dados:
Etapa em a qual múltiplas fontes de dados devem ser combinadas numa única.
Seleção dos Dados: Etapa onde os dados relevantes para as tarefas de análise devem ser recuperados das fontes de dados.
Transformação dos Dados:
Etapa em a qual os dados brutos devem ser transformados e consolidados em informações relevantes para o negócio.
Mineração dos Dados:
Etapa em a qual métodos inteligentes são aplicados a fim de extrair padrões.
Vários algoritmos podem ser utilizados nessa etapa, tais como, associação, classificação, segmentação, seqüência e sumarização.
Avaliação de Padrões:
Etapa em a qual são identificados padrões interessantes, ou seja, que possam representar conhecimento baseado em alguma métrica.
Apresentação do Conhecimento:
Etapa em a qual são utilizadas técnicas de representação do conhecimento e visualização para apresentar o conhecimento minerado para o usuário final.
Este trabalho considera somente as etapas referentes ao processo de Etc, do processo de KDD.
O processo de Etc consiste na integração de dados de múltiplas fontes e sua limpeza e transformação em informações consistentes e de qualidade, e na inserção destes num DW.
Utiliza a denominação de ferramentas de Back End, as quais são responsáveis por o processo de Etc, bem como a restauração dos dados utilizados num sistema de DW.
O processo de Etc é detalhado na seqüência.
A Figura 11 ilustra um framework genérico para processos de Etc..
Em a parte superior descreve- se os dados armazenados que estão envolvidos em todo processo.
Em a parte esquerda, observa- se os dados brutos provenientes de diversas fontes de dados (e.
g banco de dados relacional e arquivos diversos).
Os dados provenientes dessas fontes são extraídos através de rotinas de extração, e podem ser obtidos através de extrações completas, snapshot completo, ou extrações parciais, quando somente os dados que sofreram alguma alteração são extraídos.
De essa forma, esses dados devem ser propagados para o DSA (Data Staging Area) onde são limpos e transformados antes de serem carregados no DW.
O DW engloba as tabelas fato e dimensão.
Já a parte inferior da Figura 11 ilustra todas as bases de dados utilizadas para extração, limpeza e carga dos dados, respectivamente.
Em um PDS, a Etc significa poder capturar informações da execução de processos completados e ainda em execução, e dos artefatos tratados e produzidos neste processo.
A idéia é efetuar as transformações e tratamentos necessários para obter informações consistentes, úteis e de qualidade, a serem armazenadas num DW.
Estas informações devem possibilitar o monitoramento de processo, indicar problemas e também servir de fontes de dados para prover a descoberta de conhecimento em processos de desenvolvimento de software.
Extração Os sistemas transacionais, também chamados de sistemas fontes ou sistemas legados, são fontes interessantes de informação para o negócio.
Essas informações porém, podem ser encontradas em diversos locais e em diversos formatos.
Apesar de estas fontes serem de grande interesse para o negócio, nem todas as informações contidas em elas podem fornecer algum valor para objetivo do negócio, e nem todas podem ser analisadas.
Isso muitas vezes deve- se ao fato de que esses sistemas fontes mantêm pouca ou nenhuma informação histórica, e ainda possuem um enorme volume de dados.
Em a etapa de extração, os dados a serem extraídos devem estar relacionados com o modelo de dados do DW alvo.
Esses dados armazenados nos sistemas fontes, muitas vezes são incompletos, duplicados, inconsistentes e ruidosos, necessitando assim, de diversas limpezas e transformações, apresentadas a seguir.
Transformação e Limpeza Rotinas de limpeza e transformação de dados buscam eliminar problemas dos dados, provenientes dos sistemas fonte que serão carregados no DW, através do preenchimento de valores nulos, eliminação de dados ruidosos e inconsistentes.
Além disso, essas devem buscar unificar dados provenientes de múltiplas fontes de dados em dados integrados, consistentes e que posteriormente serão armazenados no DW.
Essa etapa tem por objetivo fornecer ao usuário do sistema analítico, dados concisos e com uma qualidade que permita uma tomada de decisão baseada em valores mais próximos dos reais.
Data Staging Area (DSA) Segundo, o processo de data staging4 é a parte mais crítica do projeto de DW.
Sendo assim, busca- se minimizar essa dificuldade através da construção de uma área onde os dados limpos e transformados devem ser mantidos antes de serem carregados no DW.
Propõe um plano de 10 passos para a criação de um DSA, os quais foram seguidos por este trabalho.
Criar um plano de alto nível, através de um esquema de uma página ilustrando o fluxo da fonte para o destino.
Testar, escolher e implementar uma ferramenta de data staging.
Diminuir a granularidade da tabela alvo, esboçando graficamente qualquer transformação e reestruturação de dados complexa, e ilustrando, também, o processo de geração de chaves surrogates.
Construir e testar a carga de uma dimensão estática.
O objetivo principal deste passo é desenvolver uma infra-estrutura que suporte conectividade, transferência de arquivos e problemas de segurança.
Construir e testar minunciosamente processos de mudanças para uma dimensão.
Devido a o uso comum na área, este termo será mantido em inglês.
Construir e testar cargas nas dimensões restantes.
Construir e testar cargas nas tabelas fato históricas, incluindo substituição de chaves surrogates.
Construir e testar o processo de carga incremental.
Construir e testar a carga em tabelas agregadas e/ ou carga em MOLAP.
Projetar, construir e testar a automação da aplicação de staging.
Afirma que investir num sólido DSA permite trabalhar com uma larga escala de requisitos de transformações evitando que cada processo de negócio necessite investir em decifrar sistemas fonte e executar os mesmos cálculos inúmeras vezes.
Rotinas de Limpeza e Transformação Uma vez os dados tenham sido extraídos dos sistemas fonte, inúmeras ações devem ser executadas até transformar esses dados em informações que sejam apresentáveis para o usuário e, além disso, possuam algum valor para o negócio.
Apresenta 20 passos de transformações, citados a seguir:
Integração Manutenção de mudanças nas dimensões Desnormalização Renormalização Limpeza Deduping Merge Purge Conversão de tipo de dados Cálculo Derivação Alocação 19/20.
Pré e pós passos de saída Carga Após os dados extraídos serem limpos e transformados, eles devem ser carregados no DW.
A carga dos dados é feita a partir de as informações armazenadas no DSA que já passaram por o processo de limpeza e transformação.
As tabelas que serão atualizadas no sistema de DW devem ser montadas utilizando- se agregações, sumarizações e ordenações dos dados.
A carga dos dados no DW dá- se inicialmente nas tabelas dimensão e posteriormente nas tabelas fato.
Outro fator a ser considerado é o tipo de carga a ser realizada.
Esta pode ser do tipo incremental ou por cima de os dados.
A carga incremental normalmente é feita para tabelas fatos e a carga por cima de os dados é feita em tabelas dimensões onde o analista terá que excluir os dados existentes e incluir- los novamente.
Em alguns casos poderá acontecer que as tabelas de dimensões tenham que manter o histórico.
Então, o mesmo deverá ser mantido.
O processo de carga, envolve grande complexidade e considera diversos aspectos, tais como integridade referencial e atualização de informações no DW.
Em DW, integridade referencial significa que para cada chave estrangeira da tabela fato existe uma entrada correspondente na tabela dimensão.
De essa forma, no momento da carga é necessário verificar os campos das tabelas fato que são chaves estrangeiras para certificar- se de que os dados existentes nas tabelas fato estão de acordo com a chave primária da tabela dimensão corresponde.
A o final do processo de carga, os dados armazenados no DW estarão prontos para serem analisados por os gestores.
As seções seguintes descrevem o DW, propriamente dito, e os processos de atualizações, bem como o impacto dessas atualizações.
Data Warehouse (DW) Um DW é uma base de dados semanticamente consistente que serve como uma implementação física do modelo de dados de suporte a decisão organizacional e armazena as informações que uma organização necessita para a tomada de decisão estratégica.
Um DW é também visto como uma arquitetura construída através da integração de dados provenientes de múltiplas fontes para suportar consultas estruturadas e/ ou ad-hoc, relatórios analíticos e tomada de decisões.
Segundo, um &quot;DW é uma coleção de dados orientada a assuntos, integrada, variante no tempo e não volátil no suporte ao processo de tomada de decisão».
Esta definição apresenta as maiores características de um DW, onde através dessas quatro palavras chaves consegue- se distinguir DW de qualquer outro sistema de repositório de dados, tais como sistemas de banco de dados relacional, sistemas de processamento transacional e sistemas de arquivos.
Essas características são detalhadas como segue.
Orientado a assunto:
Um DW é organizado em torno de os assuntos principais, tais como projeto, iteração, fase e atividade.
Melhor do que se concentrar no processamento de operações e transações diárias de uma organização, um DW focaliza na análise dos dados para tomadores de decisões.
Portando, o DW fornece tipicamente uma visão concisa através dos assuntos e por a exclusão dos dados que não são úteis no processo de suporte a decisão.
Integrado: Um DW é construído através da integração de múltiplas fontes heterogêneas, tais como sistemas de banco de dados relacional, sistemas de processamento transacional e sistemas de arquivos.
As técnicas de integração e limpeza de dados são aplicadas para assegurar a consistência dos dados.
Variante no tempo:
Os dados são armazenados para prover informações por a perspectiva histórica.
Qualquer chave no DW contém, implícita ou explicitamente, um elemento de tempo.
Não volátil:
Um DW é sempre um repositório fisicamente separado com dados transformados provenientes do ambiente transacional.
Devido a esta separação, um DW não requer processamento de transação, recuperação e mecanismos do controle de concorrência.
Isto geralmente requer somente duas operações no acesso aos dados:
O carregamento inicial dos dados e o acesso aos dados.
Data warehousing é o processo de construção e utilização de DW.
A construção de DW requer a integração, limpeza e consolidação de dados.
A utilização de DW frequentemente necessita de diversas ferramentas de suporte a decisão.
O processo de data warehousing é também útil por o ponto de vista da integração de bases de dados heterogêneas.
Muitas organizações tipicamente armazenam e mantêm diversos tipos de dados provenientes de grandes fontes de informações heterogêneas, autônomas e distribuídas.
O processo de Data Warehousing provê arquiteturas e ferramentas para executivos de negócio organizarem, entenderem e utilizarem seus dados na tomada de decisões estratégicas.
Diversas organizações encontraram nos sistemas de DW valiosas ferramentas no mundo atual competitivo.
Atualização de valores nas tabelas fato e dimensão O processo de carga deve conter regras para determinar como lidar com valores de atributos que sofreram alguma alteração em relação a o valor já armazenado no DW.
A atualização de valores no DW é um dos processos mais complexos, pois as informações carregadas no DW passaram por a etapa de limpeza e transformação desde sua extração das fontes de dados originais.
De essa forma, é necessário, por exemplo, manter um registro no DSA quando chaves substitutas forem utilizadas.
A aplicação de data staging deve conter as regras do negócio para determinar como lidar com valores de atributos que sofreram alguma modificação em relação a o valor armazenado no DW.
Apresenta técnicas que podem ser utilizadas quando mudanças dos valores armazenados nas tabelas dimensão e fato ocorrem:
Tipo 1: Sobrescrita:
Quando os atributos das tabelas dimensão são atualizados, deve- se sobrescrever- los.
Tipo 2: Criação de um novo registro na dimensão:
Quando os atributos das tabelas dimensão são atualizados, deve- se criar um novo registro na tabela.
Segundo, esta técnica é a mais utilizada e mais difícil de ser gerenciada.
Isto porque se deve criar uma nova linha na tabela e uma nova chave atualizada toda vez que se encontrar uma mudança num registro da tabela.
Este capítulo relatou que a etapa de Etc do Processo de Descoberta de Conhecimento em BD é uma das mais importantes, pois essa etapa busca garantir que os dados armazenados no DW possuam um nível de qualidade adequado e possam ser utilizadas por os gestores para a tomada de decisão.
Esse capítulo serviu como embasamento teórico para a solução proposta, Capítulo 7.
A Seção 5.1.2.1 apresentou os passos para a criação de uma DSA, já a Seção 5.1.2.2 descreveu as rotinas de extração utilizadas e a Seção 5.1.3 apresentou o processo de carga, bem como a atualização de informações.
A criticidade do processo de ETL torna- se mais elevada ainda quando esta é aplicada em ambientes heterogêneos, devido a a necessidade de um maior tratamento dos dados, pois estes dados podem encontrar- se espalhados em diversas fontes, em diversos formatos, com significados diferentes, inconsistentes entre outros.
O Capítulo 6 discorre sobre um ambiente real, onde esses diversos fatores críticos para a heterogeneidade estão presentes.
Este capítulo apresenta o estudo de um ambiente real realizado numa organização de software.
Para tal, são apresentadas a descrição do cenário e a caracterização do problema.
Em este, foram identificadas limitações no acompanhamento dos seus PDS devido a a dificuldade de se coletar e armazenar dados provenientes dos diversos projetos num ambiente centralizado, que forneça uma visão organizacional unificada.
Esta seção tem por objetivo apresentar a estrutura da organização, discorrendo sobre o nível de maturidade, programa de métricas e modelo analítico adotado por a organização, bem como a caracterização do problema, descrevendo os três pontos principais a ser observados:
Estrutura da Organização O estudo de um ambiente real foi realizado numa organização de software, certificada CMM Nível 2.
Esta tem como um de seus fundamentos a adoção de um modelo de gestão em qualidade a fim de utilizar um conjunto de processos e práticas definidas e específicas, capazes de satisfazer as necessidades do mercado, de modo a atingir os seguintes objetivos:
Habilitar um desenvolvimento rápido e sustentável para projetos de desenvolvimento, manutenção e migração de software.
Verificar e definir os processos de desenvolvimento de software, priorizando as ações de melhoria da qualidade ao longo de todo o ciclo de vida dos produtos.
Fornecer, à Operação de Software vantagens competitivas no mercado de desenvolvimento de software.
Esta organização segue a analogia de desenvolvimento de produto, conforme ilustrado na Figura 2 (Capítulo 2), onde, a partir de um conjunto de processos de software padrão (OSSP), cada projeto pode- se especializar (PDSP), possuindo assim, características próprias, tais como, distintos tipos, distintos ciclos de vida, distintos modelos de gerenciamento e o uso de distintas ferramentas.
Nível de Maturidade Partindo da premissa que a organização busca certificação CMM Nível 3 e que um dos requisitos necessários para obtenção desse nível de maturidade é a coleta e o armazenamento de métricas do ambiente transacional, foi necessária a construção de uma Base Organizacional (BO) que armazene dados históricos dos projetos da organização, ou seja, que nessa base sejam armazenadas as métricas referentes à execução desses projetos.
Entre os objetivos da construção de uma BO para a organização alvo, destacam- se:
Planejamento de projeto:
Os dados de projetos finalizados devem poder ser utilizados como base para o planejamento de novos projetos e no estabelecimento de metas realistas para os mesmos.
Através da análise do desempenho de projetos similares já finalizados, as atividades de planejamento de recursos, estimativas de prazo, tamanho e custo tornam- se mais precisas.
Controle do desempenho dos processos dos projetos:
As métricas fornecem informações acuradas sobre o estado do projeto e, assim, podem ser utilizadas para tomada de ações corretivas em tempo hábil.
Métricas são utilizadas para monitorar o desempenho do projeto e para verificar sua conformidade em relação a os planos de desenvolvimento do projeto como, por exemplo, os critérios de gatilho e replanejamento5.
Análise e melhoria dos processos da organização:
Somente através de métricas uma organização pode quantificar a qualidade e a produtividade de seus processos.
Também, através de métricas, a identificação de potenciais áreas de melhoria torna- se mais visível.
Os critérios de gatilho e replanejamento sinalizam situações em as quais são necessárias o replanejamento do projeto, como por exemplo, quando a variação de esforço for superior a 10% o projeto deve ser reestimado.
Programa de Métricas A organização em questão definiu seu programa de métricas, apresentado na Tabela 2.
Para cada métrica indireta, apresentam- se os seus objetivos, as métricas diretas que são utilizadas no seu cálculo, bem como sua equação de cálculo e unidade final.
Ainda, para cada métrica direta apresentam- se as possíveis origens, já que essas podem ser provenientes de diversas fontes.
Métricas Indiretas Variação de Cronograma no Baseline Original (VcrBO) Variação de Cronograma no Baseline Revisado (VCrBR) Variação de Esforço no Baseline Original Variação de Esforço Baseline Revisado Variação de Tamanho no Baseline Original Variação de Tamanho no Baseline Revisado Objetivos Dar visibilidade da aderência do projeto em relação a os compromissos do projeto, possibilitando que o mesmo seja entregue no prazo.
Dar visibilidade da diferença entre o esforço estimado para o projeto e o esforço realizado.
Fornecer visibilidade da diferença entre o tamanho previsto para o projeto e do tamanho realizado, buscando dimensionar a magnitude do produto ou serviço a ser desenvolvido.
Métricas Diretas Origem Data Final Baseline Original (DFBO) Data Inicial Baseline Original Data Final Real do Projeto (DFR) Project Data Inicial Baseline Revisado Project Data Final Real do Projeto (DFR) Esforço Estimado Baseline Original Esforço Real (Er) Unidade Project Data Final Baseline Revisado (DFBR) Equação Project, Excel Project Esforço Estimado Baseline Revisado Esforço Real (Er) Tamanho Estimado Baseline Original Tamanho Real (TR) Project VCrBO $= VCrBR $= Project, Excel Project, Excel Project Project, Excel Project, Clear Quest Project, Tamanho Estimado Baseline Revisado Project, Tamanho Real (TR) Project, Métricas Indiretas Variação de Custo no Baseline Original Variação de Custo no Baseline Revisado Objetivos Custo da Qualidade Volatilidade de Requisitos Fornecer a relação entre o n° de requisitos modificados por o cliente (adicionados, removidos, alterados) e o n° de requisitos aprovados inicialmente por o cliente.&amp;&amp;&amp;
Densidade de Defeitos Origem Prover visibilidade da relação entre o número de defeitos encontrados por o cliente e o tamanho atual do produto.
Prover visibilidade da relação entre o n° de defeitos encontrados durante todo o ciclo de vida de desenvolvimento do produto e o tamanho atual do produto.
Custo Baseline Original (CEBO) Custo Real (Cr) Custo Baseline Revisado (CEBR) Custo Real (Cr) Custo total de revisão (CTR) Project, Excel Project, Excel Custo total de teste Project, Excel Custo de prevenção Custo de não conformidades Nº de requisitos adicionados (Ra) Nº de requisitos removidos (RR) Project, Excel Project, Excel Nº de requisitos modificados (RM) Excel Nº de requisitos originais (RO) Excel Nº defeitos encontrados no cliente (De) Tamanho Real em Project , Nº defeitos encontrados internamente (Di) Tamanho Real em Project, Unidade Project Project Equação Project Custo Real (Cr) Fornecer visibilidade da diferença entre o custo realizado de um projeto frente a o custo previsto.&amp;&amp;&amp;
Determinar os custos incorridos para atingir qualidade no projeto, relacionados a garantia de qualidade, controle da qualidade, planejamento da qualidade e retrabalho.
Densidade de Defeitos Entregues Métricas Diretas Project Excel Excel Defeitos Defeitos Métricas Indiretas Objetivos Métricas Diretas Origem Nº defeitos encontrados internamente (Di) Nº defeitos encontrados no cliente (De) Prover visibilidade dos defeitos encontrados em revisões do tipo Peer Review.
Nº defeitos encontrados em revisões (DTR) EfiCiência de Revisão Produtividade Fornecer visibilidade da taxa de trabalho despendido para desenvolvimento de um determinado projeto.
Esforço total em revisão (ETR) Esforço Real (Er) Project, Excel Project, Unidade EfiCiência de Remoção de Defeitos Fornecer visibilidade da efetividade das atividades de verificação e validação executadas no projeto.
Equação Tamanho Real em Horas Modelo Analítico Proposta Inicial O DW, como apresentado no Capítulo 5, é uma base de dados unificada e centralizada.
As tabelas do modelo analítico representam um esquema do tipo constelação de fatos, ilustrado na Figura 13.
As tabelas fato, descritas sucintamente na Tabela 4, representam as medidas de projetos em diferentes níveis de granularidade, sendo que a informação de maior granularidade, prevista por o modelo, é representada por a Fato_ Atividade e a de menor granularidade por a Fato_ Produto.
Considerando as medidas diretas definidas na Tabela 2, tem- se que:
Tamanhos (TEBO, TEBR, TR, Ra, RR, RM e RO) estão presentes em Fato_ Produto;
Esforços (EEBR, EEBO e Er) e custos (CEBO, CEBR e Cr) estão em Fato_ Atividade e Fato_ Fase;
E defeitos (Di e De) estão em Fato_ Produto/ Fase.
Cada métrica direta é caracterizada por as associações que tem com as dimensões.
Por exemplo, num mesmo projeto, diferencia- se o tamanho estimado (TEBO e TEBR) do tamanho real (TR) por a ligação destes últimos para diferentes entradas em Dim_ Tipo_ Fato:
Baseline original, baseline revisado ou realizado.
Nome da Dimensão Dim_ Projeto Dim_ Produto Dim_ Fase Dim_ Atividade Dim_ Defeito Dim_ Status Dim_ Tempo Dim_ Tipo_ Fato Descrição da Dimensão Armazena informações que a organização julga importante referente a algum projeto. (
Exemplo: Porte, tecnologia empregada, etc) Armazena informações referentes a produtos de projetos.
Armazena informações referentes a fases de projetos.
Define atividades de uma fase de projeto, juntamente com seu tipo (trabalho, retrabalho e revisão).
Caracteriza os defeitos encontrados por categoria (interno ou externo) e severidade (baixa, média ou alta).
Define o produto ou fase de um projeto como:
Em desenvolvimento ou concluído.
Armazena datas (data, ano, mês, dia e semestre).
Define se o fato é uma estimativa (baseline original, baseline revisado) ou registro de uma realização.
Ainda, analisando as métricas utilizadas no cálculo das métricas indiretas percebe- se que essas equações nem sempre são somente formadas por métricas diretas.
Algumas vezes, métricas indiretas são combinadas com métricas diretas na produção de outras métricas indiretas.
Por exemplo, a métrica &quot;Custo da Qualidade «é formada por a razão do CTR, CTT, CP e CNC por o Cr, onde CTR é o custo total das atividades do tipo revisão de uma release, CTT é o custo total das atividades do tipo trabalho da fase de teste de uma release, CP é o custo total das atividades do tipo qualidade e CNC é o custo das atividades do tipo retrabalho.
Essas métricas, CTR, CTT, CP e CNC, são exemplos de métricas indiretas utilizadas no cálculo de outra métrica indireta.
Nome do Fato Fato_ Atividade Descrição do Fato Armazena as medidas de esforço, custo e duração de uma atividade específica.
Armazena as medidas estimadas e reais de uma fase.
Estas são:
Esforço, custo e duração.
Armazena os defeitos (internos e externo) de um produto com sua respectiva fase.
Armazena as medidas de tamanho de um produto.
Fato_ Fase Fato_ Produto/ Fase Fato_ Produto Modelo Final Devido a as necessidades organizacionais, o modelo apresentado evoluiu.
Esta nova estruturação deve- se ao fato da organização ter adquirido uma ferramenta de Bi (Business Intelligence), a qual necessita de uma determinada estruturação das tabelas do DW possibilitando o detalhamento da informação, através de operações drill-up e drill-down.
Em este modelo analítico, as tabelas dimensão descrevem os atributos que caracterizam os projetos:
Dim_ Projeto, Dim_ Tipo_ Projeto, Dim_ Indústria, Dim_ Tecnologia, Dim_ Cliente, Dim_ Iteração, Dim_ Fase, Dim_ Porte_ Projeto, Dim_ Atividade, Dim_ Defeito, Dim_ Status, Dim_ Unidade, Dim_ Tempo_ Ini e Dim_ Tempo_ Fim.
A Tabela 5 nomina e descreve sucintamente cada dimensão.
Dimensão Dim_ Projeto Dim_ Industria Descrição Armazena informações referentes aos projetos.
Armazena informações referentes aos tipos de indústria (e.
g Governo, Finanças) Dim_ Cliente Armazena informações referentes ao nome dos clientes de cada projeto.
Dim_ Porte_ Projeto Armazena informações referentes ao porte dos projetos (e.
g grande, médio e pequeno).
Dim_ Tipo_ Projeto Armazena informações referentes ao tipo do projeto (e.
g manutenção, desenvolvimento).
Dim_ Tecnologia Armazena informações referentes à tecnologia empregada nos projetos (e.
g Java, C+, VB).
Dim_ Iteração Armazena informações referentes a iterações de projetos (e.
g Iteração 01, Iteração 02).
Dim_ Fase Armazena informações referentes a fases de projetos (e.
g Planejamento e Análise de Requisitos, Design).
Dim_ Atividade Armazena as atividades de uma fase de projeto, juntamente com seu tipo (e.
g trabalho, retrabalho e revisão).
Dim_ Defeito Armazena os defeitos encontrados por categoria (e.
g interno ou externo), tipo (classifica se defeitos internos são de peer review ou não) e severidade (e.
g baixa, média ou alta).
Dim_ Status Armazena informações sobre o status referente a um projeto uma release ou fase de um projeto (Concluído ou Em desenvolvimento).
Dim_ Unidade Armazena informações sobre a unidade utilizada por cada projeto (e.
g PF (Ponto de Função), PMHP (Ponto Médio Hewlett--Packard) ou UCP (Use Case Points)) Dim_ Tempo_ Ini Armazena as datas inicias em relação a o BO, Br e Real.
Dim_ Projeto Nome_ Projeto Dim_ Cliente Cliente Dim_ Tecnologia Tecnologia Dim_ Tipo_ Projeto Tipo_ Atributo na Tabela 2, apresenta- se as seguintes métricas:
DIBO, DFBO, DIBR, DFBR, Dir e DFR.
Já nas Figura 14 a Figura 18, essas métricas não aparecem nas tabelas Dim_ Tempo_ Ini e Dim_ Tempo_ Fim.
Isso acontece porque para cada métrica existe uma data inicial e uma data final.
Por exemplo, para métrica EEBO, têm- se uma DIBO e uma DFBO;
Para a métrica EEBR, têm- se uma DIBR e uma DFBR e para a métrica Er, têm- se uma Dir e uma DFR.
Dimensão Fato_ Atividade Fato_ Fase Fato_ Iteração Fato_ Defeito Fato_ Release Descrição Armazena as métricas base de esforço, estimadas e reais, de uma atividade.
Armazena as métricas base estimadas e reais de uma fase, assim como as métricas derivadas de variação referentes a estas métricas.
Estas são:
Esforço e duração Armazena as métricas base estimadas e reais de uma iteração, assim como as métricas derivadas de variação referentes a estas métricas base.
Estas são:
Esforço e duração Armazena os defeitos (internos e externos) de um produto, bem como o cálculo das métricas respectivas a eles.
Armazena as métricas bases respectivas a tamanho, custo, esforço e duração, bem como as métricas derivadas referentes a essas métricas numa release.
As organizações de software trabalham com diversos projetos de software que se diferenciam tanto por as ferramentas de gestão utilizadas quanto por a forma que armazenam e controlam suas métricas de análise e acompanhamento.
Esse estudo de um ambiente real serviu para identificar limitações numa organização de desenvolvimento de software no controle e acompanhamento de seus PDS por não possuírem informações integradas de seus processos.
Ainda, permitiu constatar a criticidade na obtenção dessas informações dada a heterogeneidade das fontes do ambiente em questão.
Diversos aspectos são descritos como segue:
A extração das informações é efetuada de forma manual, um ator do PDS deve ser deslocado de suas atividades normais para efetuar a extração das informações a serem carregadas na BO.
Em termos de ferramentas para controle da execução de atividades de projetos, são utilizadas desde planilhas eletrônicas (MS Excel) até ferramentas dedicadas a este fim (e.
g MS Project Server, IBM Rational ClearQuest, Bugzilla).
Detectou- se ainda o uso de diferentes modelos de PDS, com ciclos de vida variados para projetos distintos, que se traduzem em formas bastante diversas de registrar os projetos, ainda que na mesma ferramenta.
A seguir são discutidos três pontos principais observados.
São eles:
Heterogeneidade, intrusão e extração não incremental.
Heterogeneidade Os tipos de heterogeneidade encontrados na organização alvo foram:
Tipos de projetos, ciclos de vida, modelos de gerenciamento e tipos de ferramentas.
Esta heterogeneidade é ocasionada devido a a especialização dos projetos em relação a o OSSP.
Esses tipos de heterogeneidade encontrados na organização são descritos como segue.
Tipos de Projetos A organização apresenta dois tipos de projeto:
Desenvolvimento e manutenção.
Os projetos de desenvolvimento possuem seus ciclos de vida completos, onde todas as fases devem ser executadas.
Já os projetos de manutenção têm seu ciclo de vida iniciado na fase de análise ou projeto.
Ciclos de Vida Como a organização produz diversos projetos para uma variedade de clientes e usuários, um determinado ciclo de vida de software pode não ser apropriado para todas as situações.
De entre os ciclos de vida aprovados para utilização no OSSP estão:
Cascata, V, staged delivery e iterativo.
Atualmente somente são utilizados os ciclos cascata e iterativo.
Modelos de Gerenciamento O modelo de gerenciamento é a forma como os projetos gerenciam seus processos.
A ferramenta utilizada para o controle da execução das atividades é MS Project Server.
Atualmente são utilizadas duas formas de cronogramas para o controle das atividades de um projeto:
Cronogramas orientados a fase e cronogramas orientados a entregáveis.
Em os cronogramas orientados a fase tem- se que um release possui várias fases e estas várias atividades.
Já nos cronogramas orientados a entregáveis, tem- se que um release possui vários entregáveis e estes várias fases, que por sua vez, possuem várias atividades.
Tipos de Ferramentas Assim como os projetos possuem autonomia para escolherem seus tipos, ciclos e modelos de gerenciamento, eles também podem escolher suas ferramentas.
A organização disponibiliza um conjunto de ferramentas com a finalidade de acelerar o processo de desenvolvimento e entrega de produtos, e diminuir o tempo de resolução de problemas e tomada de decisão.
As ferramentas disponibilizadas dividem- se em gerenciadoras de cronogramas, acompanhamento de defeitos ou melhorias, métricas de dados de projetos e gerência de configuração.
Gerenciadoras de cronograma:
MS Project Server, Gantt Project.
Acompanhamento de Defeitos ou Melhorias: IBM Rational Clear Quest, Bugzilla.
Métricas de Projeto e Repositório de Dados do Projeto: Organization Database.
Gerência de Configuração:
IBM Rational Clear Case, MS Visual Source Safe.
Intrusão Atualmente a organização possui um processo intrusivo de obtenção das métricas organizacionais, ou seja algum ator do PDS deve ser deslocado das suas atividades normais para executar todo o processo de obtenção das métricas e transformação destas métricas para o padrão da organização.
O processo de obtenção das métricas organizacionais consiste em realizar diversas consultas nas ferramentas de acompanhamento dos projetos para obter informações sobre esforço (nome do projeto;
Release; Tipo da atividade:
Trabalho, retrabalho, revisão ou qualidade;
Fase da atividade e total de horas), defeitos (nome do projeto;
Release; Tipo do defeito:
Defeito externo ou interno;
Fase de origem, peer review:
Se o defeito foi encontrado em PR ou em teste e severidade do defeito) e requisitos (nome do projeto;
Release; Número de requisitos originais;
Número de requisitos adicionados;
Número de requisitos modificados e;
Extração não incremental A cada carga das métricas na BO, os dados são todos recarregados, mesmo após o projeto estar finalizado.
Atualmente, não existe uma forma incremental de carga dos dados extraídos, ou seja, a cada nova carga os dados são todos novamente carregados, não havendo uma carga apenas daqueles dados que sofreram alguma alteração.
Este capítulo buscou apresentar o estudo de um ambiente real da organização alvo e a caracterização do problema a serem tratados nessa pesquisa, no Capítulo 7.
Não faz parte do escopo desse trabalho discutir a qualidade do modelo analítico utilizado por a organização alvo e sim, como este modelo deve ser alimentado.
Este trabalho define como, a partir de fontes distintas, os dados necessários devem ser extraídos, transformados e carregados no modelo analítico, considerando ainda, diversos aspectos levantados neste capítulo como heterogeneidade, intrusão e extração incremental.
Constatou- se nesse estudo, que grande parte da heterogeneidade é proveniente da especialização dos projetos em relação a o processo organização padrão, conhecido como OSSP.
É importante ressaltar também, que apesar de o modelo analítico sofrer alteração, o programa de métricas organizacionais não foi modificado.
Este capítulo apresenta a solução proposta, para o problema apresentado no capítulo anterior, que está inserida numa arquitetura para o ambiente de Data Warehousing.
Esta proposta abrange as camadas de Integração de Aplicações e Integração de Dados, onde o Processo de Extração, Transformação e Carga segue uma abordagem orientada a serviços.
A arquitetura desenvolvida para o ambiente de Data Warehousing é composta de três camadas:
Integração de aplicações, integração de dados e apresentação.
A camada de integração de aplicações é responsável por a extração dos dados brutos dos projetos considerando as especificidades das ferramentas.
A camada de integração de dados compreende a Data Staging Area (DSA) e o DW propriamente dito.
O processo de Etc, que atua sobre as camadas de integração de aplicações e de integração de dados, segue uma abordagem orientada a serviços.
A camada de apresentação e a modelagem do DW, apresentados em e que não fazem parte do escopo deste trabalho, facilitam o acompanhamento segundo o programa de métricas definido por a organização.
Tendo por base a arquitetura apresentada na Figura 20, o processo de Etc consiste na captura de dados provenientes de múltiplas fontes de informação, na sua transformação em informações consistentes e de qualidade, e na inserção destes num DW.
Os dados devem ser extraídos segundo o modelo analítico alvo.
A limpeza e a transformação dos dados extraídos têm por objetivo corrigir imperfeições contidas nas fontes de dados originais (e.
g O processo de Etc, em PDS, visa capturar dados sobre a execução de processos concluídos ou em execução, e sobre artefatos manipulados e produzidos por cada processo.
O repositório de dados analítico (DW) tem por objetivo estabelecer uma visão organizacional homogênea dos projetos, abstraindo suas especificidades.
Portanto, é responsabilidade do Etc lidar com as especificidades dos projetos, que se traduzem, por o ponto de vista de captura de dados, na necessidade de tratar as seguintes heterogeneidades:
1) diferentes ferramentas utilizadas para registrar dados dos projetos;
2) práticas diversas de registros de dados ainda que numa mesma ferramenta e 3) modelos de PDSs especializados em relação a OSSP.
A abordagem proposta neste trabalho tem as seguintes vantagens.
Primeiro, permite reduzir a complexidade de implementação e manutenção dos procedimentos de extração.
Segundo, os projetos podem evoluir com o tempo (adotar novas ferramentas ou mudar o modelo de gerenciamento).
Terceiro, habilita lidar com a heterogeneidade utilizando os seguintes protocolos padrões:
XML, SOAP, WSDL e UDDI.
Esses protocolos são usados para definir o formato das mensagens, especificar a interface para onde estas são enviadas, descrever as convenções para o mapeamento do conteúdo das mensagens de entrada e saída responsáveis por o serviço, e definir mecanismos para publicar e encontrar interfaces de serviços.
Em a implementação, foi utilizado o framework Apache Axis, que permite a construção de Web Services utilizando o protocolo SOAP.
Considerando que os metadados estão descritos em XML Schema foi necessário utilizar alguma forma que fosse possível manipulálos, o que se resume em interpretar, extrair dados e tratar eventos.
Sendo assim, utiliza- se a Simple API for XML (Sax), onde através da linguagem de programação Java pode- se instanciar os parsers utilizando a Java API for XML Parsing implementada por o Axis.
O modelo Sax permite que esses parsers leiam documentos e respondam a eventos produzidos durante a leitura.
Para implementação da DSA foi utilizado o MS SQL Server.
A seguir, são apresentadas, respectivamente, as camadas de integração de aplicações e integração de dados, descrevendo para tal, seus componentes principais e o relacionamento entre estes.
A camada de integração de aplicações é responsável por a extração de dados brutos dos projetos provenientes das diversas ferramentas e carregar- los na DSA.
Para esta solução, adota- se uma abordagem de baixa intrusão seguindo um padrão arquitetural orientado a serviços, onde os serviços atuam como wrappers.
Cada wrapper endereça a extração de dados considerando uma ferramenta em particular, e é focado no modelo de dados proprietário da mesma.
Além disso, cada projeto é descrito por metadados, expressos em XML Schema, que parametrizam a implementação dos wrappers.
Os metadados de projeto definem as ferramentas adotadas e como os dados requeridos (métricas e atributos das dimensões) são armazenados nessas ferramentas, de acordo com:
O ciclo de vida do projeto (iterativo, cascata), tipo (desenvolvimento, manutenção) e modelo de gerenciamento (cronogramas orientados a fases, cronogramas orientados a entregáveis).
As rotinas de extração exploram o metadado de projeto para localizar o wrapper correto e guiar a extração baseada no mapeamento estabelecido entre o dado bruto e o dado requerido.
A seguir são descritos os três principais elementos da camada de integração de aplicações:
Os metadados de projeto, os wrappers e as rotinas de extração.
Metadados Os metadados são definidos como dados sobre os dados.
Para, os metadados representam um conjunto de informações que descrevem o DW e atuam com um papel ativo na sua criação, uso e manutenção.
Ainda, segundo o mesmo autor, uma das mais importantes características dos sistemas que suportam o processo de Etc é que eles possuem metadados dirigidos.
Esses metadados podem assumir papéis ativos ou passivos, servindo apenas como uma documentação do processo de DW, quando passivos, e podem servir diretamente como um conjunto de instruções para esses processos quando é uma parte ativa do processo.
Segundo os metadados podem ser caracterizados &quot;como um diretório que auxilia os analistas a localizarem os componentes do DW».
Ele ainda define quais informações os metadados mantêm, tais como:
A estrutura dos dados segundo a visão do programador.
A estrutura dos dados segundo a visão dos analistas.
A fonte de dados que alimenta o DW.
A transformação sofrida por os dados no momento de sua migração para o DW.
O modelo de dados do DW.
O histórico das extrações de dados.
Os dados referentes aos relatórios que são gerados por as ferramentas OLAP, assim como os que são gerados nas camadas semânticas.
Considerando a necessidade dessas informações, dois tipos de metadados foram criados:
Os metadados de projeto e os metadados organizacionais.
Os metadados de projeto referem- se à etapa de extração e, os organizacionais, às etapas de limpeza e transformação.
Como o processo de Etc segue uma abordagem orientada a serviços, optou- se neste trabalho por desenvolver os metadados em XML Schema6.
A seguir, são apresentados os metadados de projeto.
Já os metadados organizacionais são apresentados na Seção 7.2.2.
Metadados de projetos Os metadados de projeto definem para cada projeto, o relacionamento entre o dado bruto e o dado requerido, considerando seu tipo, ciclo de vida, modelo de gerenciamento e ferramenta utilizada.
A Figura 21 ilustra, através de um Modelo de Features, a heterogeneidade das fontes de dados.
Analisando de forma bottom-up o modelo apresentado na Figura 21, verifica- se que em termos de as ferramentas de gestão utilizadas no controle das atividades dos projetos são utilizadas desde planilhas eletrônicas (MS Excel) até ferramentas dedicadas a este fim (e.
g MS Project Server, IBM Rational Clear Quest).
Ainda, essa organização possui distintos modelos de PDS (orientado a fases e orientado a entregáveis), os quais apresentam ciclos de vida variados (iterativo e cascata) e tipos de projetos distintos (manutenção e desenvolvimento), que resultam em diversas maneiras de registrar seus projetos, mesmo que na mesma ferramenta.
Em hachurado na Figura 21 estão os projetos (Projetos 3, 5 e 6) utilizados na experimentação (ver Capítulo 8).
Um documento em XML Schema é um documento XML utilizado para determinar que tipos de dados um documento XML está transportando.
A Figura 22, ilustra um metadado de projeto, onde o elemento inicial do XML Schema indica o nome do projeto e os posteriores indicam as ferramentas utilizadas por o projeto.
Para cada ferramenta tem- se a descrição de como os dados estão estruturados.
Por exemplo, o projeto 3 do tipo desenvolvimento, com ciclo de vida do tipo iteração e com cronogramas orientados a fases possui a seguinte estruturação no Project Server:
Nome iteração, nome da fase, tipo das atividades e nomes das atividades.
A diferenciação está na hierarquia apresentada no cronograma que é dependente das características do projeto.
O relacionamento entre o dado bruto e o dado requerido, para cada informação a ser extraída, é especificado através dos elementos tabela, campo, tipo e seu tamanho na fonte de dado original.
O Anexo I ilustra os metadados de projetos construídos.
Wrappers Wrappers são empacotadores que encapsulam uma ou mais aplicações provendo assim uma única interface.
Como esta solução segue uma abordagem orientada a serviços, para cada fonte de dados existe um WSDL referente, onde os serviços atuam como wrappers que consideram a extração de uma ferramenta em particular, considerando suas especificidades.
A Figura 23 ilustra o WSDL gerado a partir de a classe de extração de dados do MS Project Server (ProjectServer.
Java). O arquivo XML (Figura 23) descreve o acesso ao documento que realiza o serviço solicitado.
Ele possui uma série de parâmetros e definições para o acesso do solicitante e para a resposta esperada.
Em as primeiras linhas são definidas a versão do XML, qual o arquivo que será usado como base para gerar o WSDL e o local onde ele se encontra.
Os xmlns informam de onde serão retirados os padrões de formatação para criar o WSDL e de qual implementação`.
Jws' serão retirados os dados.
A tag define o tipo de resultado que será gerado no final da execução do serviço, ou seja, o que é retornado para quem realizou a requisição.
Logo após, são criadas as tags de request e de response, que informam os métodos de chamada e retorno que compõe o serviço.
Em cada função são integradas` tags' name\&gt;, em as quais constam os parâmetros de entrada que serão informados na chamada do serviço.
Após a definição das funções, parâmetros e formatos, então são descritas as operações do serviço.
A tag é composta inicialmente por o nome do arquivo de execução do serviço, seguido por as tags que definem o input e o output de cada método que o arquivo de serviço utiliza.
Após as definições deve- se criar o processo de comunicação SOAP.
Esse processo é iniciado com a tag com a criação do esquema XML SOAP, definindo uma ação de comunicação de input e de output para cada operação que o serviço pode realizar.
Para finalizar o processo ele gera, através da tag a definição do nome do serviço descrito no que será executado, e ainda, qual o local que ele se encontra.
Rotinas de extração Quando as rotinas de extração necessitam buscar as informações nas fontes de dados, elas percorrem os metadados de projeto (apresentados na Figura 22), os quais fornecem informações sobre o mapeamento entre o dado requerido para extração e o dado bruto sendo extraído.
Esses metadados parametrizam as rotinas de extração objetivando a localização do wrapper correto (ver Figura 24).
ProjectServer. Jws, ClearQuest.
Jws ou Excel.
Jws. Onde o wrapper de um Ws é responsável por a extração de uma ferramenta em particular, considerando suas especificidades.
Em a Figura 25 visualiza- se mais facilmente a utilização dos métodos e chamadas descritos acima.
Em este trabalho, o UDDI não foi utilizado, pois se sabe qual o endereço a ser acessado, ou seja, o local onde se podem encontrar os dados a serem extraídos (ferramentas).
Apesar de as rotinas de limpeza e transformação serem responsáveis por a execução das técnicas de transformação, duas técnicas apresentadas por, são executadas nas rotinas de extração, são elas:
Integração: Dados provenientes de diversas fontes devem integrados para que possam ser enviados para a DSA.
Por exemplo, quando uma atividade tem seus valores referentes ao baseline original e revisado proveniente do MS Project Server e seus valores referentes ao realizado proveniente do MS Excel, devem ser geradas novas chaves de registro, evitando o uso das chaves de registro originais dos sistemas legados, proporcionando uma integridade referencial entre as tabelas do modelo dimensional.
Combinação: Através dessa técnica é possível combinar atributos ou registros.
Por exemplo, os campos customizados em nível de projeto no MS Project Server somente são obtidos por a combinação de outros dois atributos.
CASTver Figura 26), os dados extraídos das ferramentas do ambiente transacional são limpos e transformados no DSA, através das rotinas de limpeza e transformação, auxiliadas por os metadados organizacionais.
Após todo esse processo, os dados consolidados são carregados para o ambiente de DW.
A seguir, são descritos os principais componentes da camada de integração de dados:
As rotinas de limpeza e transformação, os metadados organizacionais, a DSA e a carga no DW.
Rotinas de Limpeza e Transformação A execução das rotinas de limpeza e transformação sobre os dados brutos provenientes da camada de integração de aplicações tem por objetivo transformar- los em informações úteis, consistentes e de qualidade.
Apresenta 20 técnicas, de as quais nessa etapa foram implementadas as seguintes7:·
Limpeza: Como os dados brutos provenientes do sistema fonte muitas vezes são informados manualmente por seus usuários, estes podem apresentar diversos problemas.
Esta técnica busca eliminar a utilização incorreta de caracteres, códigos e valores duplicados que, se não eliminados, causariam inconsistências no ambiente de DW.·
Conversão de tipos de dados:
Os dados brutos provenientes de um sistema fonte muitas vezes não possuem atributos com tipos semelhantes aos do De entre as técnicas apresentadas por, duas de elas são executadas na camada de integração de aplicações:
A integração dos dados provenientes das diversas fontes e a combinação de atributos ou registros.
VARCHAR) As Nome_ Fase, CAST (dbo.
MSP_ VIEW_ PROJ_ TASKS_ STD.
TaskName AS onde o atributo Nome_ Fase, armazenada no DW, deve ser do tipo &quot;varchar», enquanto que nas fontes de origem pode ser tanto ntext, quanto text, dependendo da fonte de origem.·
Valores nulos:
Quando os sistemas fonte possuem dados muito antigos, podem existir dados com valores nulos.
Por exemplo, a severidade dos defeitos em determinados projetos não possui classificação.
Buscando não interferir nos valores produzidos por os demais projetos, classificam- se valores nulos como· Verificação da integridade referencial:
Esta última etapa consiste em verificar se um determinado atributo de uma tabela fato corresponde ao mesmo atributo armazenado na dimensão.
O processo de integração de dados pode ser obtido através do pré-processamento dos dados para padronização de nomes e valores, buscando resolver discrepâncias na representação de dados, fusão de valores em comum e valores equivalentes de dados provenientes de diversas fontes.
Após a execução das rotinas de limpeza e transformação, os dados já podem ser carregados para o DW.
Apesar de afirmar que, para o DW ser auto-suficiente e a fonte única de informações, a DSA deva ser limpa.
Em este trabalho, nós apenas excluímos da DSA as atividades completadas, isto se deve à necessidade de manter o registro do mapeamento entre os dados inseridos no DW e os dados sendo extraído das fontes de dados, para que em posteriores cargas, as informações armazenadas no DW sejam atualizadas corretamente.
O escopo dessa pesquisa busca encontrar padrões de transformação possibilitando afirmar que dados de um determinado tipo nas fontes de dados originais, e que no DW sejam de outro tipo, devam sofrer determinadas transformações.
Essa definição tem por objetivo facilitar o processo de Etc no caso de novas métricas de análise serem inseridas.
Por exemplo, dados do tipo ntext ou text nas fontes de dados originais e que no DW são do tipo varchar, devem sofrer a seguinte transformação:
CAST (METRICA_ x As VARCHAR).
Outro exemplo de transformação, são dados do tipo datetime no sistema fonte e no sejam do tipo date, para isso necessário seguinte transformação CONVERT, Metrica_ y, 103), a qual converte a data atual para caracter no formato dd /mm/aaaa.
Metadados Organizacionais Os metadados organizacionais (ver Figura 27) estabelecem um conjunto de regras de transformação da origem para o destino.
Esses metadados são interpretados através de parsers que lêem cada metadado e produzem eventos durante a leitura.
Por exemplo, o projeto 1 de conversão dessas escalas específicas de projeto em escalas organizacionais, por exemplo, projeto tem a liberdade de escolher sua unidade de tamanho.
Esta deve ser convertida para Por exemplo, se o Projeto 1 mede seu tamanho por PF (Ponto de Função) e utiliza a linguagem C+, têm- se 0,053 KLOC por PF.
O Anexo II ilustra o Metadado Organizacional por completo.
Data Staging Area (DSA) Buscando minimizar a intrusão do processo de Etc, utiliza- se uma área temporária8 de dados, denominada DSA, que contém um conjunto de tabelas onde os dados brutos extraídos do sistema fonte devem ser armazenados para serem pré-processados através das rotinas de limpeza e transformação, considerando o modelo analítico alvo (descrito na Seção transformação através da linguagem de programação Java.
Essas rotinas baseiam- se nos metadados organizacionais que provêm uma visão unificada de todos os projetos por a perspectiva organizacional.
Estrutura do DSA sugere uma série de passos para a construção de uma DSA.
São eles:
1) Inicialmente, deve- se criar um plano de alto nível com o fluxo das informações da fonte para o destino.
2) Posteriormente, deve- se construir um plano detalhado para cada fluxo descrito no passo 1.
A Figura 28 ilustra o plano de alto nível, que consiste num esquema simples que ilustra as fontes e o destino.
Segundo, esse esquema de alto nível possibilita visualizar os três grandes passos de um DW:
A obtenção dos dados das fontes, as trasnformações e a carga nas tabelas alvo.
Com esse esquema deve ser possível visualizar:
Volume de dados de cada fonte:
Cada projeto possui várias releases e cada uma dessas possui um cronograma no MS Project Server associado.
O volume de dados de um cronograma fica em torno de as 1000 atividades, o que não pode ser considerado um grande volume de dados.
Freqüência de atualização das fontes:
A freqüência de atualização varia de fonte para fonte;
Em as planilhas Excel verifica- se que a freqüência de atualização é diária, pois esta armazena as informações sobre as atividades realizadas.
Sendo assim, a volatilidade dos dados disponíveis por esta ferramenta é alta.
Estimado x Realizado:
Indica se as informações referem- se aos valores estimados ou realizados.
O MS Project Server fornece tanto valores estimados O termo temporário está diretamente ligado ao tempo em que uma atividade de um projeto demora para ser completada, ou seja, enquanto ela não estiver completa ela será mantida na DSA.
Nível de transformação:
O nível de transformação pode ser alto, médio ou baixo.
As informações provenientes das Planilhas Excel possuem um alto nível de transformação, já as provenientes do IBM Rational Clear Quest possuem um baixo nível.
Segundo um plano detalhado deve, para cada fonte ilustrada no plano de alto nível, detalhar cada um dos seus fluxos.
A Figura 29 apresenta o plano detalhado do MS Project Server, ilustrando para essa fonte de dados, quais as tabelas são trabalhadas, quais os dados são necessários, a ordem de coleta e quais as transformações são necessárias para cada conjunto de dados.
Por exemplo, para as informações requeridas deve- se combinar dois atributos de duas tabelas diferentes para obter seu valor, e ainda deve- se efetuar uma conversão de tipo de dados (ntext para varchar).
Essas transformações são auxiliadas por os metadados de projeto.
A o final desses passos e conhecendo o modelo de dados alvo, consegue- se verificar quais tabelas são necessárias para a criação de uma DSA.
A Figura 30 ilustra a DSA, a qual possui cinco tabelas que correspondem às cinco tabelas fato do DW:
DSA_ Atividade, DSA_ Fase, DSA_ Iteracão, DSA_ Release, DSA_ Defeito.
Por exemplo, tabela DSA_ Atividade contém as informações correspondentes a tabela fato Fato_ Atividade e a tabela dimensão Dim_ Atividade do DW.
Já tabela DSA_ Fase possui as informações referentes as tabelas Fato_ Fase e Dim_ Fase.
A tabela DSA_ Iteração armazena as informações correspondentes as tabelas Fato_ Iteracão e Dim_ Iteracao.
A tabela DSA_ Defeito contém as informações referentes as tabelas Fato_ Defeito e Dim_ Defeito.
Já a tabela DSA_ Release armazena as informações referente a tabela fato Fato_ Release, bem como as informações correspondentes às tabelas dimensão Dim_ Cliente, Dim_ Industria, Dim_ Porte_ Projeto, Dim_ Tecnologia, Dim_ Tipo_ Projeto e Dim_ Unidade.
O Anexo III apresenta os scripts para criação do DSA.
Carga no Data Warehouse A última etapa do processo de Etc consiste em carregar os dados brutos extraídos do ambiente transacional, limpos e transformados no DSA através das rotinas de limpeza e transformação, no DW.
O objetivo do modelo analítico proposto para o DW é prover uma base de dados unificada e centralizada, que permita a análise dos dados resultantes, de todos os projetos de desenvolvimento da organização, através de métricas organizacionais.
O modelo analítico comporta a execução de projetos organizados em fases e atividades, segundo os diferentes ciclos de vida, de os quais resultam um ou mais produtos de software.
A carga dos dados no DW dá- se inicialmente nas tabelas dimensão e posteriormente nas tabelas fato.
Essa carga, envolve grande complexidade e considera diversos aspectos, tais como integridade referencial e atualização de informações no DW.
Em DW, integridade referencial significa que para cada chave estrangeira da tabela fato existe uma entrada correspondente na tabela dimensão.
De essa forma, no momento da carga é necessário verificar os campos das tabelas fato que são chaves estrangeiras para certificar- se de que os dados existentes nas tabelas fato estão de acordo com a chave primária da tabela dimensão corresponde.
O processo de carga deve conter regras para determinar como lidar com valores de atributos que sofreram alguma alteração em relação a o valor já armazenado no DW.
Ainda, deve- se manter o registro no DSA do mapeamento entre as informações de atividades não completadas inseridas no DW e os dados brutos das fontes de origem.
Esse mapeamento permite que os dados do DW sejam atualizados corretamente, evitando assim, que as métricas armazenadas nas tabelas fato sejam incrementadas de forma errônea.
Seguindo o padrão arquitetural orientado a serviços proposto neste trabalho, os dados armazenados no DSA devem ser encapsulados numa mensagem SOAP e enviados para o DW.
Para que esses dados possam ser enviados do DSA para o DW através de mensagens SOAP, foi necessário especificar os serviços que receberão e processarão as mensagens, através do WSDL.
A o final desta etapa, os dados armazenados no DW estarão prontos para serem analisados por os gestores.
Um dos pontos que ainda necessitam ser discutidos são o impacto das atualizações das informações no DW já que a solução proposta propõe ser evolutiva.
Os fatores envolvidos na atualização de informações e os impactos dessas, são discutidos como segue.
Impacto das Atualizações das Informações Diversos autores convergem em suas discussões sobre a complexidade de atualização das informações, ainda mais se esta for efetuada de forma evolutiva.
Segundo, quando os valores são atualizados, deve- se criar um novo registro na tabela fato ou dimensão.
Ainda, o mesmo autor afirma que esta técnica é a mais utilizada e mais difícil de ser gerenciada.
Apesar disso, esta é utilizada neste trabalho e consiste na inserção de uma nova linha na tabela e a criação de uma nova chave atualizada toda vez que ocorrer alguma mudança num registro da tabela.
Estudos foram realizados sobre o impacto das atualizações, tanto nos atributos armazenados nas tabelas dimensões, quanto nas métricas armazenadas nas tabelas fato.
Para os atributos das tabelas cria- se um novo registro na dimensão com o valor atualizado.
Para as métricas armazenadas nas tabelas fato também cria- se um novo registro na fato com o valor atualizado.
A diferença está no tipo da métrica (direta ou indireta):
Se a métrica for direta, além de a criação de um novo registro deve- se verificar o impacto dessa atualização nas métricas indiretas, ou seja, toda vez que uma métrica direta, que compõem uma métrica indireta for atualizada, esta deve ser recalculada.
Além disso, como apresentado na Seção indiretas, o que resulta não somente do cálculo de métrica indireta final, mas também nas métricas indiretas que a compõem, o que ocasiona numa atualização de maior complexidade.
Outro ponto a ser discutido é como manter um relacionamento entre o dado armazenado no DW que deverá ser atualizado e o dado bruto que deverá ser extraído.
Primeiramente, deve- se discutir a diferença entre atividades completadas e em andamento.
Uma atividade considerada completada quando campo MSP_ VIEW_ PROJ_ TASKS_ STD.
WorkComplete for igual 100, ou seja, o percentual de esforço completo dessa atividade no MS Project Server é 100%.
Caso contrário, essa atividade é considerada em andamento.
Enquanto a atividade não for completada ela deverá ser atualizada no DW.
Para evitar que atividades sejam atualizadas erroneamente deve- se manter um relacionamento entre esta informação e o dado bruto sendo extraído.
A solução para isso foi manter as atividades que não possuem campo «DSA_ ATIVIDADE.
WorkComplete $= 100 «na DSA.
Para cada atividade armazenada na DSA deve- se carregar a sua identificação única (DSA_ ATIVIDADE.
UniqueID), a qual mantêm o Id inicial de cada atividade.
Essa informação permite que se mantenha o relacionamento entre o dado carregado no DW, após a integração das informações e mudanças dos registros, e o dado bruto sendo extraído.
Este capítulo apresentou a solução proposta para o problema apresentado no capítulo 6 utilizando os conceitos apresentados nos capítulos 2, 3, 4 e 5.
Essa solução está inserida numa arquitetura de Data Warehousing e abrange as camadas de integração de aplicações e de integração de dados.
Em a camada de integração de aplicações apresentou- se os componentes principais de esta:
Os metadados de projeto que descrevem o mapeamento entre o dado requerido para extração e o dado bruto sendo extraído.
Os wrappers endereçam a extração de dados considerando as especificidades das ferramentas.
Esta solução segue uma abordagem orientada a serviços, onde os serviços atuam como wrappers.
Já as rotinas de extração exploram o metadado de projeto para localizar o wrapper correto e conduzir a extração baseada no mapeamento entre o dado bruto e o dado requerido.
Em a camada de integração de dados, apresentou- se como os dados brutos dos projetos provenientes da camada de integração de aplicações são limpos e transformados e carregados na De as.
Ainda descreveu- se como os metadados organizacionais auxiliam essas rotinas e, ao final, discorreu- se sobre o processo de carga e atualização dados, bem como o impacto dessas atualizações.
O capítulo seguinte propõe através de experimentações demonstrar que a solução proposta neste trabalho é evolutiva, de baixa intrusão e trata a heterogeneidade adequadamente, considerando assim, as especificidades dos projetos.
Este capítulo descreve a experimentação da solução proposta e, para tal, utilizam- se três exemplos.
Esses exemplos buscam mostrar que a solução proposta é evolutiva, de baixa intrusão e que trata a heterogeneidade das fontes.
Para isso, são descritos os elementos do experimento:
O estabelecimento dos objetivos, a formulação das questões, a elaboração das métricas, e o relato do experimento.
A o final, apresenta- se uma análise e interpretação dos resultados.
Segundo, a experimentação é o &quot;centro do processo científico «e tem seus objetivos principais relacionados à execução de experimentos em Engenharia de Software:
Os elementos principais de um experimento são as variáveis, os objetos, os participantes, o contexto do experimento, hipóteses e o tipo de projeto do experimento.
Esses conceitos são descritos como segue.
Variáveis: As variáveis podem ser de dois tipos, independentes e dependentes.
As variáveis independentes, também denominadas fatores, apresentam a causa que afeta o resultado do processo de experimentação (tratamento).
Já as dependentes, referem- se à saída do experimento, ou seja, ao efeito causado por os fatores do experimento (resultado).
Objetos: Os objetos são utilizados para verificar o relacionamento causa-efeito numa teoria.
Durante a execução do experimento os tratamentos são aplicados ao conjunto dos objetos e assim o resultado é avaliado.
Participantes: Os participantes são os indivíduos selecionados para conduzir o experimento.
Contexto do experimento:
O contexto do experimento contém as condições em que o experimento está sendo executado, por exemplo, in-vitro vs.
In-vivo, alunos vs.
Profissionais, problema de sala de aula vs.
Problema real e específico vs.
Geral. Hipóteses:
Normalmente um experimento é formulado de hipóteses.,
o qual é formado por uma hipótese nula e tem por objetivo rejeitar a hipótese nula a favor de uma ou algumas hipóteses alternativas.
Tipo de projeto do experimento:
O projeto do experimento determina a maneira como um experimento será conduzido, ou seja, é o momento em o qual é realizada a alocação dos objetos e dos participantes, bem como a quantidade e a seqüência dos testes experimentais.
A Figura 31 ilustra os relacionamentos entre os elementos principais de um experimento, descritos acima.
Discorre sobre duas metodologias de experimentação:
QIP (Quality Improvement Paradigm) e a (2) GQM (Goal/ Qestions/ Metric).
A primeira metodologia, QIP, define seis passos que ao final resultam num ciclo de melhoria do processo por completo.
Já a segunda metodologia, GQM, utiliza para definição uma abordagem top-down para o estabelecimento dos objetivos, a formulação das questões e a elaboração das métricas.
Já para a interpretação usa uma abordagem bottop-down:
A medição para receber os dados experimentais, a formulação das respostas para as questões baseadas nos dados experimentais, e o agrupamento das respostas para demonstrar o grau de sucesso dos objetivos estabelecidos.
Todos esses passos, que fazem parte do processo principal da abordagem GQM, são ilustrados na Figura 32.
A abordagem GQM é composta de quatro fases:
A fase de Planejamento, quando o projeto da medição está selecionado, definido, caracterizado e planejado, resultando no plano de projeto;
A fase de Definição, quando o programa de medição é conceitualmente preparado, ou seja, os objetivos, as questões, as métricas e as hipóteses são estabelecidos;
A fase de Coleta de dados empíricos, quando a coleta de dados experimentais é efetivamente feita resultando num conjunto de dados prontos para a interpretação;
E a fase de Interpretação, quando os dados são processados a respeito de as métricas, questões e objetivos definidos.
A metodologia de experimentação utilizada neste trabalho foi a GQM, isso deve- se ao fato de que esta permite que os resultados obtidos durante o experimento sejam utilizados na melhoria da solução proposta.
A seguir, são descritos os passos seguidos na experimentação:·
Objetivo do Estudo:
Analisar a solução proposta para verificar se ela atende aos objetivos específicos propostos incialmente.·
Objetivo do Experimento: A realização do experimento tem o propósito de avaliar a solução proposta de uma abordagem orientada a serviços para captura de métricas de PDS, buscando caracterizar- la como evolutiva, de baixa intrusão e adequado tratamento da heterogeneidade.
Questão 1:
A solução proposta para uma abordagem orientada a serviços para captura de métricas de PDS é evolutiva?
Métrica: Somente são carregados no DW os dados que sofreram alguma modificação, sendo o fator de atualização inferior a 100%.
Quando não for a primeira carga.
Questão 2 (Q2):
A solução proposta para uma abordagem orientada a serviços para captura de métricas de PDS é de baixa intrusão?
Métrica: Não é preciso deslocar nenhum ator do PDS para realizar as atividades de extração, transformação e carga, ou seja, quando o indicador de número de atores deslocados for igual a zero.
Questão 3 (Q3):
A solução proposta para uma abordagem orientada a serviços para captura de métricas de PDS trata adequadamente a heterogeneidade?
Métrica: Tratamento das especificidades dos PDS, considerando diferentes tipos de projetos, diferentes ciclos de vida, diferentes modelos de gerenciamento e diferentes ferramentas utilizadas, ou seja, quando o indicador de tratamento adequando da heterogeneidade for igual a 4/9, 5/9 ou 6/99.
Quando esse fator for 1/9, 2/9 ou 3/9 o tratamento é considerado parcial.
Através da Figura 33 verifica- se que esse fator seja igual a 4/9 a solução proposta deve considerar para cada projeto seu tipo, ciclo, Nove corresponde ao conjunto total de elementos:
Ferramentas (MS Project Server, IBM Rational Clear Quest, MS Excel), Modelo de Gerenciamento (Orientado a Fases e Orientado a Entregáveis), Ciclos de Vida (Cascata e Iterativo) e Tipos de Projeto (Desenvolvimento e Manutenção).
Bancada de Teste Com o propósito de validar a implementação que está sendo desenvolvida, foram construídos três exemplos de teste conforme a heterogeneidade apresentada na Figura 33.
Esses três projetos cobrem todo o espectro de aspectos a serem testados.
O modelo apresentado na Figura 33 mostra que quando um determinado projeto é do tipo desenvolvimento, ele não pode ser também do tipo manutenção, ou seja, a partir de o topo do modelo deve- se percorrer os caminhos permitidos por o modelo.
Por exemplo, um projeto pode ser do tipo desenvolvimento, com ciclo iterativo, modelo orientado a fases e utiliza as ferramentas MS Project Server, MS Excel e IBM Rational Clear Quest.
A seguir, apresenta- se uma descrição dos três projetos utilizados como exemplo, salientando suas diferenças e semelhanças.
Projeto 1 O projeto 1 é do tipo desenvolvimento, com ciclo iterativo e modelo de gerenciamento orientado a fases, e utiliza três ferramentas:
MS Project Server, MS Excel e IBM Rational Clear Quest.
Para o gerenciamento dos cronogramas desse projeto é utilizado o MS Project Server que fornece os valores estimados nas baselines para as atividades, bem como o tamanho (estimado e realizado) de cada release.
Para o registro dos valores das atividades realizadas é utilizado o MS Excel.
As informações referentes a defeitos são armazenadas no IBM Rational Clear Quest.
A diferenciação desse projeto para os demais está na hierarquia apresentada no cronograma (MS Project Server) que é dependente das características do projeto e obedece a hierarquia apresentada na Figura 34.
Observa- se nessa figura, que existem cinco níveis de informação a serem coletadas:
O nome do projeto, o nome da iteração, o nome da fase, o tipo e o nome da atividade.
O nome da iteração deve- se ao fato do projeto 1 possuir um ciclo iterativo.
Já o nome da atividade ser pertencente ao seu tipo deve- se ao fato do cronograma ser orientado a fases.
Projeto 2 O projeto 2 é do tipo manutenção, com ciclo cascata e modelo de gerenciamento orientado a fases e utiliza duas ferramentas:
MS Project Server e IBM Rational Clear Quest.
Em esse projeto para gerenciamento de seus cronogramas utiliza- se o MS Project Server, tanto para registro dos valores estimados nas baselines para as atividades, como para os valores das atividades realizadas.
Ainda são armazenados os tamanhos de cada release, tanto valores estimados nas baselines quanto realizados.
Já no IBM Rational Clear Quest são armazenadas as informações referentes a defeitos.
A diferenciação desse projeto para os demais está na hierarquia apresentada no cronograma (MS Project Server) que é dependente das características do projeto e obedece a hierarquia mostrada na Figura 35.
Analisando essa figura, verifica- se que existem quatro níveis de informação a serem coletadas:
O nome do projeto, o nome da fase, o tipo e o nome da atividade.
O motivo por o qual o nome da atividade pertence ao seu tipo é devido a o fato do cronograma ser orientado a fases.
Projeto 3 O projeto 3 é do tipo manutenção, com ciclo cascata e modelo de gerenciamento orientado a entregáveis e utiliza três ferramentas:
MS Project Server, MS Excel e IBM Rational Clear Quest.
Para o gerenciamento dos cronogramas desse projeto é utilizado o MS Project Server que armazena os valores estimados nas baselines para as atividades.
Em o registro dos valores referente a as atividades realizadas é utilizado o MS Excel.
Para armazenar as informações referentes a defeitos e tamanhos (valores estimados nas baselines e realizados) é utilizado o IBM Rational Clear Quest.
A diferenciação desse projeto para os demais está na hierarquia apresentada no cronograma (MS Project Server) que é dependente das características do projeto e obedece hierarquia ilustrada na Figura 36.
Observando essa figura, verifica- se que existem quatro níveis de informação a serem coletadas:
O nome do projeto, o nome da fase, o nome e o tipo da atividade.
O motivo por o qual o tipo da atividade pertence ao seu nome é devido a o fato do cronograma ser orientado a entregáveis.
Descrição da Instrumentalização A Tabela 7 ilustra os aspectos considerados na experimentação, determinando quando a solução proposta pode ser considerada, ou não, evolutiva, de baixa intrusão e que efetua o tratamento adequado da heterogeneidade.
Evolutiva (E) Baixa Intrusão Tratamento da heterogeneidade (H) Não é evolutiva, quando recarrega todas as informações cada atualização do DW.
Ou seja, caso já tenha uma população de fatos no DW, a renovação desses fatos deve ser igual a 100%.
Não é de baixa intrusão, quando necessário deslocar um ou mais atores do PDS para executar o processo de Etc, ou seja, quando o indicador de número de atores deslocados for maior que zero.
Não é heterogênea quando não considera nenhuma das especificidades dos PDS, ou seja, quando o indicador de tratamento adequado da heterogeneidade for igual a zero.
É evolutiva, quando mantêm os valores armazenados no DW e somente carrega os valores que sofreram alguma alteração.
Ou seja, caso já tenha uma população de fatos no DW, a renovação desses fatos deve ser inferior a 100%.
É de baixa intrusão, quando o processo de executado automaticamente, não necessitando que se desloque um ator do PDS, ou seja, quando o indicador de número de atores deslocados for igual a zero.
É heterogênea quando considera as especificidades dos PDS.
Ou seja, quando o indicador de tratamento adequando da heterogeneidade for igual ou superior 4/9.
Quando esse fator estiver entre 1/9 e 3/9 o tratamento da heterogeneidade é considerado parcial.
Ainda, a Tabela 8 relaciona as questões à combinação de aspectos considerados na experimentação.
O resultado desse relacionamento foram oito combinações de aspectos.
De entre esses, o oitavo aspecto (em negrito) que envolve as métricas Q1, Q2 e Q3 é o que demonstra que a solução proposta atinge os objetivos esperados, sendo evolutiva, de baixa intrusão e que trata adequadamente a heterogeneidade.
Para cada aspecto aplicam- se testes para definir:
Se a solução pode ser considerada evolutiva.
Se a solução pode ser considerada de baixa intrusão.
Se a solução trata adequadamente a heterogeneidade.
Descrição dos Aspectos Não é evolutiva, não é de baixa intrusiva, não é Questões heterogênea Não é evolutiva, não é de baixa intrusiva, é heterogênea Não é evolutiva, é de baixa intrusiva, não é heterogênea Não é evolutiva, é de baixa intrusiva, é heterogênea É evolutiva, não é de baixa intrusiva, não é heterogênea É evolutiva, não é de baixa intrusiva, é heterogênea É evolutiva, é de baixa intrusiva, não é heterogênea É evolutiva, é de baixa intrusiva, é heterogênea Os resultados a serem analisados são baseados em sete cargas executadas na seqüência.&amp;&amp;&amp;
Para cada uma dessas cargas foram analisadas as questões a serem respondidas, bem como suas métricas.
A seguir, discorre- se sobre cada uma dessas cargas buscando analisar e interpretar os resultados encontrados.
Carga total, onde as informações referentes aos três projetos foram extraídas, transformadas e carregadas pela primeira vez no DW.
Projeto 1 Projeto 2 Projeto 3 Não pode ser avaliada, pois é a primeira carga a ser efetuada, logo Utiliza procedimentos automatizados para a extração, transformação e carga, logo, Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Carga parcial, somente incremento de valores das atividades sem a finalização das atividades.
Projeto 1 Projeto 2 Projeto 3 Possui 147 atividades carregadas no DW, de as quais 15 sofreram atualizações, logo M1 $= Possui 101 atividades carregadas no DW, de as quais 23 sofreram atualizações, logo M1 Possui 183 atividades carregadas no DW, de as quais 42 sofreram atualizações, logo M1 $= Utiliza procedimentos automatizados para a extração, transformação e carga, logo, M2 $= Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Considerou o tipo, ciclo, modelo de gerenciamento e duas ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Carga parcial, com a inclusão de novas iterações e fases nos cronogramas, sem alterar os indicadores das atividades já carregadas.
Projeto 1 Projeto 2 Projeto 3 Possui 187 atividades carregadas no DW, de as quais 30 foram incluídas, logo M1 $= Possui 126 atividades carregadas no DW, de as quais 25 foram incluídas, logo M1 Possui 222 atividades carregadas no DW, de as quais 39 foram incluídas, logo M1 $= Utiliza procedimentos automatizados para a extração, transformação e carga, logo, M2 $= Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Considerou o tipo, ciclo, modelo de gerenciamento e duas ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Carga parcial, somente incremento de valores das atividades com a finalização de algumas atividades.
Projeto 1 Projeto 2 Projeto 3 Possui 187 atividades carregadas no DW, de as quais 49 sofreram atualizações, logo M1 $= Possui 126 atividades carregadas no DW, de as quais 72 sofreram atualizações, logo M1 $= Possui 183 atividades carregadas no DW, de as quais 124 sofreram atualizações, logo M1 $= Utiliza procedimentos automatizados para a extração, transformação e carga, logo, Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Considerou o tipo, ciclo, modelo de gerenciamento e duas ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Carga parcial, com a inclusão de atividades nos cronogramas ocasionando na alteração da identificação das atividades.
Possui 205 atividades carregadas no DW, de as quais 18 foram incluídas, logo M1 $= 8,78% Possui 144 atividades carregadas no DW, de as quais 18 foram incluídas, logo M1 $= 12,5% Possui 201 atividades carregadas no DW, de as quais 18 foram incluídas, logo M1 $= 8,95% Projeto 1 Projeto 2 Projeto 3 Utiliza procedimentos automatizados para a extração, transformação e carga, porém devido a erros na execução ajustes manuais foram necessários.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Considerou o tipo, ciclo, modelo de gerenciamento e duas ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Carga parcial, somente incremento de valores das atividades com a finalização de todas as atividades de um projeto 1.
Projeto 1 Projeto 2 Projeto 3 Possui 205 atividades carregadas no DW, de as quais 37 sofreram atualizações, logo M1 Possui 144 atividades carregadas no DW, de as quais 56 sofreram atualizações, logo M1 Possui 201 atividades carregadas no DW, de as quais 93 sofreram atualizações, logo M1 Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Utiliza procedimentos automatizados para a extração, transformação e carga, logo, M2 $= 0.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 5/9.
Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Em a primeira execução da quinta carga não houve renovação dos fatos devido a problemas na carga.
O problema estava na falta de um relacionamento entre a informação carregada no DW e o dado bruto sendo extraído.
A solução para tal, foi armazenar a identificação única de cada informação extraída, evitando assim, que cargas posteriores sejam efetuadas erroneamente.
Em a execução da sexta carga, o projeto 2 foi considerado finalizado, pois todas as suas atividades foram concluídas.
Isso siginifica que na próxima carga não devem ser extraídas informações referentes a este projeto.
Carga parcial, somente incremento de valores das atividades com a finalização de algumas atividades.
Projeto 1 Possui 205 atividades carregadas no DW, de as quais 93 sofreram atualizações, logo M1 Projeto 2 Projeto Finalizado.
Projeto 3 Possui 201 atividades carregadas no DW, de as quais 115 sofreram atualizações, logo M1 Utiliza procedimentos automatizados para a extração, transformação e carga, logo, M2 $= 0 Projeto Finalizado.
Utiliza procedimentos automatizados para a extração, transformação e carga, logo, M2 $= 0 Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Projeto Finalizado. Considerou o tipo, ciclo, modelo de gerenciamento e três ferramentas distintas, logo, M3 $= 6/9.
Esse capítulo serviu para verificar se a solução proposta abrange seus aspectos principais:
Evolutiva, de baixa intrusão e adequado tratamento da heterogeneidade.
Inicialmente estabeleceram- se os objetivos, posteriormente para cada aspecto definiram- se as questões a serem respondidas e elaboraram- se as respectivas métricas que foram utilizadas no experimento.
Posteriormente, descreveram- se as características dos três projetos utilizados para teste, salientando suas principais semelhanças e diferenças.
Em a seqüência foram executas sete cargas buscando analisar e interpretar os resultados dessas execuções.
Em essa interpretação observaram- se primeiramente as métricas obtidas, depois as respostas obtidas para as questões levantadas para o experimento e ao final agruparam- se esses os valores obtidos buscando demonstrar que os resultados obtidos durante o experimento foram considerados satisfatórios, tanto por o ponto de vista do atingimento dos objetivos propostos neste trabalho, quanto por a possibilidade de melhoria da solução, pois com este experimento foram encontrados problemas que puderam ser solicionados.&amp;&amp;&amp;
O capítulo seguinte apresenta os trabalhos relacionados à solução proposta, apresentada neste trabalho e, ao final os compara a esta solução considerando os três aspectos discutidos neste capítulo.
Este capítulo discorre sobre propostas específicas relacionadas ao Processo de Data Warehousing para PDS.
Primeiramente, são apresentadas essas propostas, posteriormente é feita uma comparação desses trabalhos com a solução proposta, considerando os seguintes fatores:
Baixa intrusão, extração não incremental e heterogeneidade.
Embora durante a pesquisa não tenham sido encontradas propostas específicas que abordem o processo de Etc para PDS, foram encontrados dois trabalhos direcionados a processos de negócios, os quais são examinados como segue.
A abordagem proposta por apresenta um framework que combina conceitos da arquitetura orientada a serviços, princípios de sistemas de suporte a decisão e uma abordagem multi-agentes.
O mesmo tem por objetivo o monitoramento e análise de processos de negócios, fornecendo informações focadas no status e na performance desses processos, independentemente da aplicação utilizada.
A Figura 40 ilustra esse framework denominado Solution Manager Service que possui quatro elementos principais:
Web Services Interfaces (WSI), (2) Agent Server (As), (3) Event Processing Container (EPC) e (4) Data Warehouse (DW).
O WSI é responsável por a extração de logs de dados.
O EPC é responsável por a integração dos logs de dados provenientes dos processos de negócios que serão posteriormente carregados nas tabelas Audit Log Data e Audit Summary Data no DW.
A tabela Definition Data do DW armazena os dados produzidos por o As resultantes do Ws.
Apresentam um conjunto de conceitos e uma arquitetura de implementação denominada Business Process Intelligence (BPI), que provê diversas características para a gestão de processos.
São elas:
Análise, predição, monitoramento, controle e otimização.
Um dos principais componentes do BPI é o Process Data Warehouse Loader (PDWLoader) cuja funcionalidade é extrair dados de logs de execução de processos, verificar a consistência dos dados, limpar, calcular métricas e carregar no Process Data Warehouse (PDW).
O PDW possui uma estrutura que habilita diversas funcionalidades analíticas, pois os dados são organizados de acordo com um esquema constelação de fatos (ver Figura 42).
As mudanças nos estados dos processos, serviços e nodos são os fatos a serem analisados, enquanto que as definições de processos, as definições de nodos, definições de serviços, definições de dados, recursos, tempo e comportamento são as dimensões sobre as quais os dados fato podem ser analisados.
Através de técnicas de DW e de mineração de dados, o BPI oferece mecanismos para análise e predição sobre execuções de quaisquer tipos de processos de negócio.
Essas análises são realizadas através de um conjunto de interfaces gráficas, denominado Cockpit, focado no acompanhamento dos processos através de métricas e indicadores.
Similarmente ao nosso trabalho, a abordagem proposta por é baseada numa arquitetura orientada a serviços e suporta a integração de dados provenientes de diversas fontes, mas restringe- se a tratar apenas a questão de extração, do processo de Etc, considerando alguns aspectos de heterogeneidade, tais como, independência da plataforma e linguagem de programação utilizada por o sistema de origem.
Já nossa abordagem, endereça além de a extração, a transformação e carga utilizando uma arquitetura orientada a serviços.
O BPI é semelhante a nossa abordagem, pois propõe uma arquitetura abrangente.
Porém não considera especificidades de PDSs, tais como heterogeneidade e métricas específicas dos processos e produtos de software, essenciais para um controle e acompanhamento efetivo, bem como para processos de certificação da maturidade.
A Tabela 9 tem por objetivo comparar os trabalhos relacionados a solução proposta considerando os seguintes aspectos:
Objetivos, heterogeneidade, baixa intrusão, extração evolutiva, especificidades dos PDS, métricas e desvantagens.
Objetivos Solução Proposta Monitorar e analisar Prover processos de negócios características para a uma através uma gestão dos processos orientada a serviços, arquitetura orientada a de negócios:
Análise, informações serviços.
Prover, Heterogeneidade Sim Não Sim Sim Sim Sim Não Não Sim Não para Sim Desvantagens de Sim Métricas Sim Especificidades dos abordagem Sim Extração Evolutiva de Sim Baixa Intrusão através considera especificidades as dos Não considera especificidades as dos Conjunto de métricas especifíco Esse capítulo buscou comparar trabalhos concorrentes a nossa proposta.
O primeiro trabalho apresentado utiliza uma arquitetura orientada a serviços para extração, considerando assim, a heterogeneidade das fontes de dados.
O segundo, apresenta- se como o trabalho mais semelhante a solução proposta, apesar de não tratar as especificidades dos PDS.
A o final é apresentada uma tabela comparativa desses trabalhos à solução proposta, considerando os seguintes pontos:
Objetivos, heterogeneidade, baixa intrusão, extração evolutiva, especificidades dos PDS, métricas, bem como as desvantagens de cada abordagem.
Através desta tabela foi possível mostrar que os trabalhos relacionados não consideram os todos os aspectos considerados na solução proposta, tais como heterogeneidade, baixa intrusão e evolutiva.
Em especial, nenhum dos trabalhos correlatos considera as especificidades de PDS.
Este trabalho propôs um ambiente de Etc como infra-estrutura de apoio à adoção de um programa de métricas numa operação de software de nível CMM2.
O ambiente proposto tem por mérito tratar de forma integrada dois aspectos cruciais do processo de data warehousing:
1) a coleta de dados dos projetos, sujeita a vários tipos de heterogeneidade e 2) sua transformação e integração, para proporcionar uma visão organizacional unificada e quantitativa dos projetos.
Visando caracterizar o problema, os objetivos e a questão de pesquisa a ser tratada, foi realizado um estudo de um ambiente real numa operação de software CMM212.
A solução proposta neste trabalho está inserida numa arquitetura para o ambiente de Data Warehousing, composta de três camadas:
Integração de aplicações, integração de dados e apresentação.
A camada de integração de aplicações é responsável por a extração dos dados brutos dos projetos considerando as especificidades das ferramentas.
A camada de integração de dados compreende a DSA e o DW propriamente dito.
O processo de Etc atua sobre as camadas de integração de aplicações e de integração de dados, e segue uma abordagem orientada a serviços.
No tocante a a Etc, são tratados vários tipos de heterogeneidade, tanto do ponto de vista organizacional (e.
g especializações da OSSP que resultam em formas distintas de desenvolvimento e registro de fatos sobre projetos), quanto do ponto de vista técnico (e.
g Cabe ressaltar que durante a execução dessa pesquisa, a organização de software em questão obteve certificação CMM Nível 3.
Além disso, o presente trabalho contribuiu para essa certificação, a medida que foram realizados estudos de como obter dados brutos de fontes heterogêneas, bem como as transformações necessárias para tornar esses dados brutos em informações consistentes e de qualidade.
Para avaliar a solução proposta, foi definido um ambiente de experimentação considerando os seus três aspectos principais.
Essa experimentação visou demonstrar que essa solução, para uma abordagem orientada a serviços para captura de métricas de PDS, caracteriza- se como evolutiva, de baixa intrusão e adequado tratamento da heterogeneidade.
Quanto a a extensibilidade da abordagem proposta, cabe ressaltar que a mesma foi desenvolvida visando que novas especificidades de PDS possam ser consideradas, bem como novas ferramentas possam ser adicionadas.
Para esta proposta discorre- se sobre os trabalhos futuros, os quais visam tornar a solução proposta a mais genérica possível.
Estes trabalhos, estão descritos como segue.
Analisar como esta solução se comportaria em organizações com níveis de maturidade 4 e 5, já que a solução é focada para organizações de software certificadas nível 2 e 3.
A cada nível de maturidade alcançado são agregadas novas características de medição e análise, onde essas análises devem prover melhorias e a customização dos PDS provendo assim, as inovações tecnológicas necessárias.
Acredita- se que a abordagem proposta seja adequada para a evolução do programa de métricas para níveis superiores, pois uma das vantagens desta solução proposta é a possibilidade de evoluir com o tempo, tanto do ponto de vista da adoção de novas ferramentas, quanto a as mudanças em relação a o ciclo de vida, modelo de gerenciamento, e etc..
O que provavelmente deverá ser trabalhado é a forma como essas informações serão apresentadas para o usuário final.
Examinar a viabilidade de extrair informações de uma maior diversidade de PDS, ampliando assim o espectro de informações que serão agregadas ao DW.
Pesquisar a possibilidade de efetuar a carga em diferentes modelos analíticos através da definição de metadados de carga.
Estudar o suporte a evolução dos programas de métricas, possibilitando assim aplicar essa abordagem para organizações que não possuam um programa fixo de métricas.
Buscar encontrar uma padronização do processo de Etc para organizações de software, buscando determinar que métricas armazenadas no modelo analítico podem possuir um comportamento padrão de extração, transfomação e carga.
