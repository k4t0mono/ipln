Existem diferentes situações em que a detecção de componentes faciais desempenha um papel importante, como por exemplo:
Em o reconhecimento facial, interação humano computador, teleconferências, análise de expressões faciais e animação.
Esta dissertação apresenta, primeiramente, uma revisão bibliográfica dos trabalhos referentes à detecção dos componentes faciais.
Após, é exposto o modelo desenvolvido para a detecção de componentes faciais, sendo este baseado em modelos de cor e antropometria.
A o final, são mostrados resultados referentes a detecção dos componentes em bases de dados públicas e experimentos realizados na animação de avatares 3D.
Palavras-chave: Detecção Componentes Faciais;
Base de Dados IMM;
Animação Avatar 3D.
A detecção de componentes faciais baseada em vídeo representa, atualmente, uma importante área de pesquisa, podendo ser aplicada em inúmeras áreas da ciência e tecnologia, tais como as citadas por:·
Reconhecimento de faces;·
Interação Humano Computador (IHC);·
Teleconferências;· Animação;·
Análise de expressões faciais;·
Aplicações para deficientes físicos (leitura de lábios).
Também podem ser utilizadas em aplicações que necessitam de alto grau de precisão na identificação das características faciais, que é o caso da análise de expressões faciais para fins de animação de personagens (avatares 3 D), como o que ocorre no filme Avatar1, que são personagens animados a partir de expressões realizadas por pessoas reais.
Em as próximas seções serão discutidos o problema, motivação, objetivos e a estrutura desta dissertação.
Problema O problema sob investigação desta pesquisa é a seguir caracterizado:
Dada uma imagem extraída de vídeo, encontrar a posição de uma seqüência de landmarks (pontos de referência) referentes aos componentes faciais que representam os olhos e sobrancelhas de uma pessoa.
Estes pontos localizados podem ser usados como características faciais, para a extração de expressões faciais.
Aplicações diretas podem ser em Interação humano computador (IHC) e na animação de avatares 3D (27;
23; 4).
Existem vários métodos para a extração dos componentes faciais, mas a maioria possui limitações, tais como:·
Não funcionar em tempo real;·
Ser sensível a variações de iluminação;·
Ser sensível a ruídos gerados por o dispositivo de câmera;·
Possuir dependência de orientação;·
Possuir dependência do espaço de cor utilizado;·
Não permitir que gestos sobreponham características (ex..
Mão sobrepondo um dos olhos).
Algumas destas limitações deverão ser tratadas nesta pesquisa.
A questão de pesquisa deste trabalho é endereçar alguns destes aspectos, provendo processamento em tempo real de imagens capturadas por webcam.
Motivação Existem diversas situações em que a detecção de componentes faciais desempenha um papel importante, como as citadas anteriormente:
IHC, reconhecimento facial, análise de expressões faciais, animação, de entre outras.
A principal motivação na escolha deste tema para a dissertação foi o desafio de detectar de uma forma precisa e rápida os componentes da região dos olhos, tendo em vista as limitações existentes na área.
Os olhos e sobrancelhas são fontes representativas dos sentimentos humanos.
Assim sendo, o algoritmo proposto pode ser útil em diferentes aplicações.
Uma das possíveis aplicações seria o monitoramento da fadiga de motoristas ou operadores de máquinas perigosas.
Se o algoritmo captar que o usuário permanece por muito tempo com os olhos fechados poderá ser disparado um sinal sonoro para despertar- lo.
Em a área de cinema e jogos, surgiu a motivação de fazer o mapeamento dos componentes faciais para avatares 3 D, para assim, a partir de estas animações, gerar avatares mais convincentes e interativos.
Em a Figura 1.1, pode- se observar um exemplo de aplicação de animação dirigida por performance (Performance Driven Animation -- PDA), que consiste na animação de personagens 3D a partir de pontos pré-definidos em pessoas.
Em esta figura as expressões faciais do usuário são capturadas por algoritmos de visão computacional e mapeados para um avatar 3D.
Em este exemplo, a precisão dos dados capturados tem um grande impacto na qualidade da animação gerada.
Este é um desafio da área.
Objetivos De forma mais específica, são objetivos desse trabalho:·
Realizar um estudo sobre as técnicas utilizadas na literatura para a detecção de componentes faciais;·
Definir uma arquitetura para o modelo de detecção de componentes faciais;·
Desenvolver o modelo de detecção de componentes faciais;·
Gerar o mapeamento dos componentes faciais para avatares 3 D;·
Comparar os resultados obtidos com outras técnicas.
Estrutura da Dissertação no Capítulo 2, são apresentados diversos trabalhos científicos relacionados ao tema desta dissertação, os quais serviram como referencial teórico para o desenvolvimento do modelo proposto no Capítulo 3.
O Capítulo 3 apresenta o modelo proposto para a detecção de componentes faciais, sendo dividido em diversos módulos para uma melhor elucidação do seu funcionamento.
Os resultados obtidos nos testes do modelo são expostos no Capítulo 4.
Já no Capítulo 5 são expostas algumas considerações sobre o trabalho realizado, destacadas as principais contribuições e sugeridos alguns trabalhos futuros.
Em este capítulo será apresentada uma revisão bibliográfica dos trabalhos referentes à detecção de componentes faciais, como boca, narinas, olhos e sobrancelhas.
Serão descritas as diferentes técnicas utilizadas por os autores a fim de identificar tais características.
Estas técnicas serão expostas nas Seções subseqüentes.
Máscaras Deformáveis Yuille e colegas propuseram as Máscaras Deformáveis (DTs -- Deformable Templates).
Tais máscaras, ao invés de serem definidas por as intensidades dos níveis de cinza dos pixeis, são definidas por funções parametrizáveis que descrevem características do objeto tais como contornos e picos e vales nas intensidade dos pixeis da imagem.
A função de comparação passa a ser uma função de energia que deve ser minimizada.
Tais funções são descritas por potenciais, cujas forças são construídas de tal forma a representar certas propriedades dos objetos de busca.
Como exemplo, considere a máscara representando os olhos mostrada na Figura 2.1, apresentada por os autores em.
A máscara deformável que representa os olhos é descrita por duas parábolas representando as pálpebras, uma circunferência representando a íris e dois pontos correspondendo ao centro das duas partes da esclera localizadas entre a íris e as pálpebras.
A transformada Wavelet é utilizada para a obtenção de uma análise multi-resolução, tanto no domínio espacial quanto no domínio de freqüências de sinais (no caso de imagens, sinais bi-dimensionais).
Wei e colegas propuseram o uso de wavelets de Gabor para a identificação inicial de componentes faciais em.
Em esse sistema a aquisição da imagem é feita através de captação de infra-vermelho, o que facilita a localização exata das pupilas.
Foram utilizadas 18 wavelets, com 3 coeficientes de escala (m $= 3) e 6 coeficientes de rotação (n $= 6).
Após a identificação dos pontos dos componentes faciais, o algoritmo proposto faz o rastreamento da posição da face e dos componentes faciais baseando- se no movimento das pupilas e utilizando filtros de Kalman.
Esta abordagem utiliza Infra-vermelho (Infra-Red) para realizar o rastreamento das características faciais.
Tong e colaboradores utilizaram wavelets de Gabor e gradientes de níveis de cinza para detecção automática dos pontos principais da face no quadro inicial, baseando- se na distância entre a matriz de coeficientes de Gabor obtida durante um treinamento com banco de dados de face anotados e a matriz de coeficientes de Gabor da imagem do quadro inicial.
Assim como em, foram utilizadas 18 wavelets de Gabor.
Em o trabalho de Vukadinovic e Pantic, é utilizada uma versão adaptada do algoritmo de Viola e Jones para detectar a face, e posteriormente são demarcadas regiões de interesse para busca das componentes faciais.
Em estas regiões são aplicados filtros de Gabor para identificar as características faciais, sendo identificadas 19 características faciais, como pode ser observado na Figura 2.2.
Modelos de Forma Ativos -- Active Shape Models (ASM).
O principal objetivo dos autores ao propor os Modelos de Forma Ativos, (ASM) foi construir modelos que pudessem ser deformados apenas de maneiras permitidas, específicas do objeto que estão representando.
Tong e colaboradores, utilizam Active Shape Model (ASM).
Em esse trabalho, os autores utilizaram 26 pontos para construção do modelo de distribuição de pontos (Point Distribution Models -- PDM), distribuídos conforme a Figura 2.3.
Para detecção dos pontos, os autores utilizaram wavelets de Gabor (modelo descrito na Seção 2.2).
Em esse trabalho, Tong Desenvolveram um modelo hierárquico que, a partir de os resultados encontrados por ASM, determina estados para componentes faciais como olhos fechados, olhos abertos, boca aberta, etc..
Zuo e colaboradores propõem a combinação de ASM com a aplicação de uma técnica de casamento descrita como Haar--wavelet.
Esta abordagem destaca- se por a velocidade para identificação, que segundo o autor é de 30 -- 70ms em imagens de 300x300 pixeis.
O trabalho de Mahoor e Abdel-Mottaleb, propõem um melhoramento na técnica de ASM, que consiste em identificar o centro dos olhos e boca para a inicialização da ASM e através destas coordenadas, aplica novos filtros na imagem para salientar as características da boca, para com isso conseguir fazer uma identificação de pontos mais correta nesta região.
São demonstrados em resultados referentes ao reconhecimento de faces utilizando essa abordagem e o ASM padrão, e pode ser observada uma melhora no casamento do modelo com as características faciais.
Em a Figura 2.4, pode ser observado um destes resultados citados.
Haj e colaboradores utilizaram ASM para detecção de componentes faciais, uma vez tendo sido encontrada a face num quadro de vídeo.
Em esse trabalho, os autores utilizaram dois PDMs ao invés de um, associando dois ASMs a esses pontos.
Esse passo é utilizado na inicialização das posições de uma malha 3D), a qual é usada no rastreamento do movimento das sobrancelhas, lábios e pálpebras.
Modelos de Aparência Ativos -- Active Appearence Models (AAM) Os mesmos autores que propuseram o ASM, desenvolveram um outro modelo para detecção de formas, o qual chamaram Active Appearence Models, ou Modelos de Aparência Ativos.
Dornaika e Davoine propuseram a obtenção simultânea tanto da posição da cabeça quanto da deformação da face.
Para o rastreamento da posição e ângulo da cabeça, os autores utilizaram filtros de partículas e para obter as deformações dos componentes faciais, utilizaram AAM.
Zalewski e Gong utilizaram AAM para classificação de estados como alegria, neutralidade, raiva, tristeza, surpresa, medo e repulsa baseados na expressão facial.
Para tanto, na fase de treinamento, os autores preferiram utilizar diferentes imagens de uma mesma pessoa com diferentes expressões, a fim de aumentar a robustez.
Além disso, os autores estenderam o AAM para incorporar implicitamente variação de pose (rotação e deslocamento da cabeça) na distribuição estatística.
Fluxo Óptico O principal objetivo da técnica de Fluxo Óptico é estimar movimento.
A idéia principal é calcular uma aproximação do campo de velocidades a partir de o movimento da intensidade dos pixeis da imagem numa seqüência de vídeo.
Variações na intensidade de iluminação afetam grandemente a estimativa do fluxo óptico.
Existem técnicas para aumento da robustez, como a proposta por Kanade-- Lucas-Tomasi (KLT) (26).
DeCarlo e Metaxas propuseram o uso de fluxo óptico para rastreamento de faces, levando em consideração as restrições impostas por um modelo deformável de face.
A proposta dos autores consiste em projetar os pontos rastreados na imagem no modelo 3D para melhor estimativa da pose da cabeça e da deformação dos componentes faciais.
He e colegas propuseram um modelo para rastreamento de características faciais baseado no algoritmo KLT.
Para aumentar ainda mais a robustes do algoritmo, eles utilizaram restrições impostas por as próprias características da face como distância entre as pupilas, comprimento e altura da boca, comprimento do nariz e comprimento dos olhos.
Com essas restrições, os autores dizem ter alcançado um melhor desempenho do que simplesmente utilizando KLT.
Hsieh e pesquisadores, utilizaram fluxo óptico com restrições para reconhecimento de faces de indivíduos com diferentes expressões em.
Outras Abordagens Algumas abordagens tinham como foco inicial a detecção de pele e posteriormente o Chang utiliza variação de limiares na escala de crominância de imagem para identificar o ponto central dos olhos, nariz e boca, e Wu Utilizam active contour model para estimar a região dos olhos, nariz e boca, como pode ser observado na Figura 2.5.
Em Xue, é proposta uma abordagem nova para a época, a qual propõe a utilização de Bayesian Shape Model para extrair os componentes faciais e juntamente com dados obtidos através de PCA (Principal Component Analysis).
O modelo visa ajustar as formas ao modelo e extrair a face para gerar uma animação da mesma num modelo.
Já Yen e Nithianandan propõem um método baseado em Algoritmos genéticos para localizar a melhor região elíptica que venha a fazer parte dos olhos, nariz ou boca.
Este método foi testado em imagens com iluminação natural e alguma adição de ruído.
Em o trabalho de Cheddad, é proposta uma abordagem diferente das demais:
São utilizados diagramas de Voronoi e triangularização de Delaunay para identificação de face e extração das características faciais.
Panning Utilizam Haar Classifiers para detectar as componentes faciais, sendo desenvolvido um treinamento para classificadores dos componentes desejados.
Utilizando estes classificadores treinados são identificados 13 componentes faciais, e a partir de estas componentes é feita uma análise através de redes neurais para classificar 5 tipos de expressões faciais (raiva, medo, alegria, surpresa e desgosto).
Uma outra abordagem interessante é proposta por Xu, exclusivamente para a detecção dos cantos dos olhos.
Em esse trabalho é construído um modelo para identificação dos cantos com base na comparação dos pontos achados com os pontos vizinhos, sendo um dos parâmetros para validação do ponto o ângulo que este forma com a pálpebra.
Chen e colegas utilizaram o método de agrupamento de dados k--means para encontrar os contornos das sobrancelhas e da parte inferior da face (queixo e bochechas).
Os autores utilizaram o algoritmo proposto em trabalhos anteriores para encontrar os cantos dos olhos, a parte superior da pálpebra e região da sobrancelha.
Como a sobrancelha pode não ser homogênea, os autores sugeriram que a região extraída fosse dividida em três sub-regiões horizontais.
Em cada uma dessas sub-regiões, foi aplicado o método de agrupamento de dados k--means sobre o valor do nível de cinza dos pixeis.
Cristinacce e Cootes propuseram um rastreador por seleção de templates de imagens de face rotuladas manualmente, que usa distância euclidiana.
Os autores afirmam terem chegado a resultados semelhantes aos obtidos em sua implementação de AAM em bases de imagens públicas e até mesmo superiores em imagens de faces em pessoas dentro de carros, embora a implementação do modelo proposto seja mais simples.
Sohail e Bhattacharya desenvolveram um modelo de face baseado em estatísticas antropométricas para detecção dos 18 pontos-chave mais importantes.
Em esse modelo, a distância entre as duas pupilas obtida através de técnicas de detecção e classificação de objetos proposta por Fasel e colegas serve como principal parâmetro de medida para a localização do centro dos outros componentes faciais.
Os autores utilizaram operações morfológicas, binarização e detecção de bordas para obtenção de características que foram submetidas a um conjunto de regras geométricas e baseadas na intensidade dos pixeis para localização de cada componente facial que os autores consideraram relevante.
Por exemplo, para detecção dos olhos, binariza- se a região próxima à localização da pupila, determina- se componentes conexos utilizando um elemento estruturante de 4 pixeis e, para o olho direito, determina- se que o canto direito do componente conexo é o canto direito do olho.
Os sete pontos de referência utilizados por o método e as distâncias calculadas podem ser observadas na Figura 2.6, e as proporções obtidas podem ser observadas na Tabela (a) Pontos de referência utilizados no modelo.
Existem inúmeros outros trabalhos que se propõe a extrair as características faciais, sendo a maioria voltados para a área de reconhecimento facial.
Em esta seção foram citados apenas os trabalhos que tiveram maior afinidade com a proposta do modelo a ser desenvolvido nesta dissertação.
Proporção da distância entre o centro do olho direito e o centro da sobrancelha direita à distância entre os centros dos olhos Proporção da distância entre o centro do olho esquerdo e o centro da sobrancelha esquerda à distância entre os centros dos olhos Proporção da distância entre o ponto médio entre dos centros dos olhos e a ponta do nariz à distância entre os centros dos olhos Proporção da distância entre o ponto médio entre os centros dos olhos e o centro da boca à distância entre os centros dos olhos voltada à performance.&amp;&amp;&amp;
Assim, a hipótese definida leva em consideração a detecção de componentes faciais da região dos olhos baseada em modelos de cor e antropometria, a fim de reduzir o custo computacional comparado com outros métodos.
Foram utilizados Haar Classifiers para a identificação da face na imagem, algoritmo de Viola e Jones, similar aos utilizados por Panning e colaboradores, e as medidas antropométricas calculadas por Sohail e Bhattacharya, bem como algumas outras abordagens descritas no decorrer deste trabalho.
Este modelo destaca- se dos demais por a sua simplicidade, e funcionar em webcams.
Em este capítulo é apresentado o modelo proposto para a detecção de características faciais da região dos olhos.
Este modelo consiste em duas partes distintas:
Um módulo desenvolvido no âmbito do laboratório de pesquisa1 para detecção da íris e o modelo desenvolvido para a detecção de componentes faciais.
Inicialmente será detalhada a abordagem desenvolvida para detecção da face e identificação das regiões de interesse.
Em a Seção 3.2 será detalhado o modelo para a identificação da íris, na Seção 3.3 é explicado o modelo desenvolvido para detecção da região dos olhos e sobrancelhas.
Já na Seção 3.4, é apresentado o protótipo desenvolvido para a detecção das características faciais.
Detecção da Face Este módulo tem o objetivo de detectar uma face na imagem de entrada.
Para este fim, utilizou- se o método proposto por Viola e Jones, que caracteriza- se por a codificação de informações previamente treinadas, para efetuar a detecção.
Em este módulo foi utilizado um treinamento específico para a identificação de poses frontais.
Este método utiliza características Haar-like para codificar a existência de contrastes entre regiões retangulares adjacentes na imagem, a partir de essas características é possível identificar diferentes características faciais humanas.
Utilizou- se a implementação proveniente da biblioteca OpenCV, especificamente o algoritmo HaarDetectObjects, que provê a implementação do método proposto por Viola e Jones, Este algoritmo retorna um retângulo, onde o rosto está inscrito.
Exemplos de imagens resultantes deste algoritmo podem ser observados na Figura 3.1, onde são demonstradas as imagens originais e a saída obtida após a execução deste módulo.
A identificação das regiões de interesse para os olhos e sobrancelhas foi feita utilizando medidas antropométricas apresentadas por, a fim de estimar onde provavelmente estariam situadas estas características faciais.
A Figura 3.2 mostra um exemplo destas regiões de interesse.
Detecção da Íris Uma contribuição deste trabalho foi a colaboração no desenvolvimento de um detector de íris, baseado em template matching, que é uma técnica bastante conhecida em processamento de sinais, para identificação da íris.
As fases deste modelo podem ser (a) Imagens obtidas a partir de o módulo de Entrada de Vídeo (b) Imagens com o retângulo da face identificada evidente.·
Inicialização do Template: É necessário indicar qual template deve ser carregado e para que proporção de face ele foi criado, indicada por o tamanho do raio da face detectado por o algoritmo de detecção de faces.
Alguns templates foram testados, sendo um de eles escolhido, a partir de os valores obtidos nos resultados demonstrados na Seção 4.1.·
Mudança de tamanho do template:
Conforme o tamanho do raio da face detectada, o template sofre aumento ou diminuição de tamanho, isto ocorre devido a a necessidade do objeto a ser procurado ter o mesmo tamanho do objeto alvo.·
Retorno do posicionamento da íris:
Tendo todos os erros calculados anteriormente, nesta etapa é assumido que o erro mínimo computado refere- se à posição da íris na imagem de entrada, sendo esta posição retornada por o módulo.
Este módulo é optativo na execução do modelo apresentado na Seção 3.3.
Conforme sua utilização, ocorrem alterações na fase subseqüente descrita na Seção 3.3.5, que é responsável por a seleção e validação dos olhos detectados.
A sua utilização implica numa perda de velocidade na execução do modelo, devido a este método utilizar a técnica de template matching.
Detecção das Características Faciais Em esta seção será exposto o modelo desenvolvido para identificação das características faciais da região dos olhos.
Um dos objetivos deste trabalho é prover informação para a animação de avatares, tomou- se a decisão de utilizar o padrão MPEG-4 de animação facial.
Este modelo tem o intuito de identificar 14 componentes faciais, que são representados como localizações neste padrão, para os olhos e sobrancelhas.
A Figura 3.4 ilustra os componentes faciais que este trabalho visa identificar.
Como primeira aplicação deste modelo, foi animado um avatar, com o intuito de demonstrar o movimento dos olhos.
Esta aplicação será apresentada no Capítulo 4.
A Figura 3.5 mostra a arquitetura interna do modelo.
Para um melhor entendimento do modelo, os módulos responsáveis por a detecção de características faciais serão explicados separadamente, seguindo a ordem do fluxo de execução, nas próximas seções.
Definição do Canal de Cor Com o intuito de definir o melhor espaço de cor para segmentar as características faciais que o modelo objetiva, adotou- se uma metodologia de variação de contraste da imagem.
A hipótese do modelo é de que deve- se destacar os pixeis que não correspondem à pele na imagem.
Pode- se destacar que os pixeis da pele tem um elevado nível de intensidade no canal de cor vermelho.
Partindo da hipótese que os contornos dos olhos e sobrancelhas têm baixa intensidade no canal vermelho em comparação com a pele, utilizou- se o canal vermelho invertido (para cada pixel calculamos 255 menos o valor do canal vermelho do pixel).
Outros espaços de cor foram testados, como poder ser visto no Capítulo 4.
Deve ser salientado que esta inversão no canal de cor vermelho foi realizada para proporcionar uma melhor visualização dos contrastes na imagem por os seres humanos, mas não há mudança na intensidade dos pixeis.
Em a Figura 3.6, é possível observar esta operação de uma forma mais intuitiva.
Transformação Exponencial Este módulo tem o objetivo de realçar a diferença entre a intensidade dos pixeis no espaço de cor vermelho invertido (Ir).
Para este fim, utilizou- se o operador exponencial para aumentar a diferença entre a intensidade dos pixeis no canal vermelho invertido.
O operador exponencial consiste em calcular a seguinte transformação para cada pixel (i, j) da imagem de entrada I:
Ii, j $= exp Ii, j.
Ln 255 A constante expressa na fórmula foi escolhida empiricamente, a partir de a análise dos resultados gerados por a transformação.
Bwi, j $= senão¯ e representam a média e o desvio padrão da intensidade dos pixeis de Ir, resonde, I pectivamente, e Z é um valor de controle, o qual seleciona quais pixeis vão fazer parte da imagem binária Bw.
Quanto maior o valor de Z, maior o número de pixeis que serão selecionados.
Este parâmetro deve ser calibrado para atingir os melhores resultados.
Em nossos experimentos, o valor ideal para Z é 0,9.
Este módulo é chamado separadamente para cada região de interesse.
A Figura 3.8, mostra os resultados obtidos após a utilização do módulo de binarização para cada região de interesse que o modelo se propõe a identificar.
Busca por Blobs Em este módulo são utilizadas as bibliotecas do projeto cvBlob2, para identificar os componentes conexos na imagem, conhecidos como blobs, utilizando vizinhança de 8 pixels Sempre que uma imagem for binarizada este módulo será chamado para localizar os blobs, para uma posterior identificação dos mesmos nos módulos subseqüentes.
Pode ser observado na Figura 3.9, um exemplo do resultado que a busca por Blobs gera, para a região da sobrancelha direita 3.9-a e do olho direito 3.9-b.
Seleção e Validação do Blob Após a localização dos blobs é necessário identificar- los e validar- los.
Para identificar os blobs são utilizadas duas abordagens distintas, uma para a identificação do blob da sobrancelha e outra para a identificação do blob do olho.
E para o estágio de validação do blob da sobrancelha e do blob do olho é utilizada apenas uma relação antropométrica de proporção.
Em a Figura 3.10, pode ser observado um exemplo do resultado do módulo de Seleção de Blobs.
A identificação do blob da sobrancelha, Figura 3.10-a, é feita da seguinte maneira:
Calcula- se a distância que cada blob está do ponto central da região de interesse da sobrancelha, o blob que tiver a menor distância relativa a este ponto é considerado como sendo o blob da sobrancelha.
Já para a identificação do blob do olho, Figura 3.10-b, foram utilizadas duas abordagens distintas:·
Utilização do ponto central do olho (íris), obtido através do algoritmo de identificação de íris mostrado na Seção 3.2, como ponto de referência para o cálculo das distâncias (a) Em a região da sobrancelha direita.·
Utilização do blob situado mais abaixo na imagem, com área maior que 10% da Roi, devido a o tamanho dos blobs válidos terem mostrado esta proporção nos testes realizados.
Estas duas abordagens apresentaram resultados semelhantes, nos testes realizados com vídeos foi utilizada a segunda abordagem, porque ao utilizar a detecção da íris foi constatado que ao ocorrer o movimento de piscar os olhos, esta abordagem retornava valores errôneos, fazendo com que a identificação do blob retornasse o blob errado.
No caso de a segunda abordagem, é feita uma validação dos blobs, levando em consideração a área do mesmo.
Se a área do blob tiver um tamanho superior a 10% da área Roi, ele será considerado como válido.
Localização das Características Em este módulo é feita a localização dos cantos, situados nos limites horizontais e verticais dos blobs, identificados como olhos e sobrancelhas.
Os pontos mais a esquerda e mais a direita representam o segmento de linha l1 e, são considerados como pontos válidos, sendo eles denominados de p1 e p2, respectivamente.
Após é traçada uma linha normal ao ponto central de l1, o último pixel pertencente a esta linha no sentido superior à normal será considerado como o ponto p3 e o inferior como ponto p4, representando assim os cantos superior e inferior do blob.
A Figura 3.11 ilustra este processo e exemplos de componentes localizados podem ser vistos na Figura 3.12.
Filtragem dos Dados Este módulo é utilizado apenas em seqüencias de imagens com coerência temporal (vídeos).
Tem o intuito de filtrar pequenas variações que possam ocorrer entre os quadros, numa seqüência de imagens, decorrentes de ruídos na captura, compactação, etc..
Com o objetivo de filtrar estas instabilidades, a posição do ponto característico é filtrada, utilizando um filtro linear no ponto atual X $= (x, y) levando em consideração a posição de n quadros anteriores, foram utilizados no experimento 3 quadros.
Este filtro pode ser observado na equação:
Protótipo Para a execução do modelo foi desenvolvido um protótipo, utilizando a linguagem de programação C+, bem como a biblioteca OpenCV para a aquisição e manipulação das imagens de entrada para o modelo.
O fluxo de execução do modelo para a detecção das características faciais pode ser descrito, de uma forma sucinta, por as seguintes etapas de execução:
A arquitetura do protótipo pode ser observada na Figura 3.13.
Os demais módulos do protótipo foram explicados nas Seções 3.2 e 3.3, respectivamente.
Entrada de Vídeo Este módulo utiliza a biblioteca OpenCV para a aquisição de imagens.
Essas podem ser oriundas de uma webcam ou de arquivos de vídeo.
A estrutura do módulo de Entrada de Vídeo pode ser observada na Figura 3.14, e uma descrição do funcionamento deste módulo pode ser vista nos tópicos abaixo:·
Inicializar a captura de imagens:
Em este módulo ocorre a seleção do tipo de entrada de dados, que será tratado por o modelo.
Estas imagens são provenientes de um dispositivo de vídeo, como uma webcam, ou podem ser adquiridas de vídeos previamente gravados.·
Tratamento das Imagens Capturadas:
Em este módulo são adquiridas as imagens do dispositivo de vídeo especificado no estágio anterior.
Estas imagens são capturadas quadro a quadro, sendo gravadas numa estrutura própria da biblioteca OpenCV para este fim.
Esta função deve ser chamada ininterruptamente até o término do vídeo de entrada ou quando ocorrer uma interrupção da comunicação com a webcam.·
Término do Programa:
Este módulo tem o objetivo de sinalizar aos demais módulos do modelo que a obtenção de imagens foi finalizada.
Ele é executado sempre que ocorrem erros na captura de imagens, fim do vídeo de entrada ou por algum tipo de interrupção no fluxo do modelo.
Em este Capítulo são apresentados os resultados obtidos com o modelo proposto.
São mostrados dois experimentos que utilizaram comparação dos dados obtidos com dados marcados por sujeitos e uma aplicação interativa que visa a animação de avatares 3D.
Inicialmente na Seção 4.1 serão mostrados os resultados obtidos a partir de o modelo de detecção da Íris.
Em a Seção 4.2 são mostrados os resultados dos testes realizados em diferentes espaços de cor enquanto na Seção 4.3 são mostradas avaliações do modelo em comparação com outros modelos da literatura.
Já na Seção 4.4 são mostrados alguns resultados obtidos na animação de olhos em avatares 3D.
Detecção da Íris Para o modelo de Detecção da Íris foram realizados testes com as bases de dados BioID, IMM, Japanese Female e Caltech, utilizando quatro templates diferentes.
Estes templates são mostrados na Tabela 4.1, bem como os resultados obtidos a partir de a execução de cada um de eles.
Nota- se que o melhor resultado é obtido a partir de o template de número 3, o qual é formado por um círculo com uma borda branca ao seu redor, notando- se que a parte vermelha do template não é considerada no matching.
Para avaliar os resultados obtidos, foi medida a distância euclidiana d dos pontos detectados (x, y), para os pontos tidos como reais na imagem (xr, yr), de acordo com a Equação marca as localizações na imagem e armazena como valores corretos.
Esta especificação é chamada de Ground Truth.
A localização por sujeitos destes pontos na imagem é muito imprecisa, devido a a possibilidade destes componentes estarem situados em mais de um pixel da imagem, de acordo com a percepção da pessoa que está efetuando a localização dos pontos.
Como pode ser visto na Figura 4.1, a localização exata do canto direito do olho direito pode estar situada em mais de um pixel, dependendo da resolução da imagem de entrada.
Devido a este fato, foi estabelecido como erro aceitável derr a distância de 5 pixel numa face com raio de 240 pixeis.
Os resultados para a avaliação do canal de cor serão expressos também em termos de decis de d para que seja possível verificar o percentual de acertos para outros limiares de distância que não 5 pixeis, visto que esse valor é arbitrário.
A Figura 4.1-a foi selecionada a partir de a base de dados IMM, que consiste numa base de dados pública de 40 pessoas distintas, em 6 posições diferentes, contendo informações de Ground Truth.
Como a proposta do modelo é trabalhar com imagens coloridas, foram excluídas 3 pessoas desta base de dados, devido a estas estarem representadas em imagens em escala de cinza.
Foram eliminadas também as imagens em que as pessoas não estavam na posição frontal, uma vez que o detector de face não foi treinado para localizar faces nestas posições.
Após estas exclusões, resultaram 142 imagens de 37 pessoas diferentes.
A Tabela 4.2, apresenta os decis de erros d, obtidos a partir de a aplicação do modelo proposto no Capítulo 3, em vários canais de diferentes espaços de cor.
Eles foram testados a fim de se verificar qual de eles apresentaria o melhor resultado em termos de detecção das localizações de interesse.
Como pode ser visto na Tabela 4.2, 90% dos valores de d são iguais ou menores que 11 pixeis para o canal vermelho do espaço de cor RGB, 80% destas distâncias são iguais ou menores que 9 pixeis neste mesmo canal, e assim por diante.
Apesar de a pequena diferença entre os resultados alcançados utilizando o canal vermelho do espaço de cor RGB e do canal V (Value) do espaço de cor HSV, foi escolhido o primeiro canal, porque não é necessário fazer transformações adicionais para obter- lo (a) Imagem original, mostrando faixa de pixeis de erro aceita por o modelo.
Avaliação do Modelo Para avaliar a precisão do modelo proposto foi escolhida a base de dados pública IMM, com as mesmas imagens citadas na Seção 4.2.
Esta base de dados contem 57 informações de Ground Truth.
Estes pontos podem ser visto na Figura 4.2.
Foram encontrados na literatura duas implementações de modelos que identificam alguns componentes faciais que o modelo desenvolvido identifica.
A primeira foi desenvolvida por Cristinacce e Coote que utiliza basicamente seleção de templates e outra aplicação desenvolvida por Milborrow e Nicolls, que é uma implementação do modelo ASM.
Para a obtenção das informações dos pontos localizados por a aplicação de Cristinacce foi necessário identificar manualmente os pontos localizados, marcando- os, um a um, a fim de gerar um arquivo de saída para comparação.
Já para a aplicação disponibilizada por Milborrow, os dados de saída do programa com as informações dos pontos, foram gravados diretamente em arquivo por a aplicação.
Com o objetivo de avaliar o desempenho do modelo proposto foram escolhidos os pontos em comum entre as três aplicações:
Modelo proposto, aplicação de Cristinacce e aplicação de Milborrow.
Estes pontos podem ser observados na Figura 4.3.
A partir destes resultados constatou- se que o modelo apresenta uma taxa de erro média entre 3 e 8 pixeis para a detecção dos componentes relacionados ao olhos.
Alguns resultados dos testes realizados na base de dados IMM são mostrados na Figura proposto, as da coluna central são resultados da aplicação de Cristinacce e as imagens da coluna a direita são referentes a saída da aplicação de Milborrow:
Milborrow Foi medido o desempenho dos três modelos, sendo calculado o tempo médio gasto para o processamento de cada quadro com 640x480 pixeis nas imagens da base de dados IMM.
O modelo proposto levou 37ms para calcular um quadro, Milborrow teve um desempenho um pouco inferior, levando 66ms e Cristinacce obteve o pior desempenho, levando 500ms para calcular um quadro.
Animação de Avatares 3D Este experimento consiste em, a partir de os componentes faciais detectados referentes aos olhos direito e esquerdo, fazer o mapeamento, em relação a o tempo, dos pontos que representam cada olho para um avatar 3D.
Foi definido um frame na imagem real para servir de referência para o modelo 3D do avatar.
Como não há informações sobre a forma tridimensional dos pontos detectados nas imagens, neste teste foram feitas apenas variações bidimensionais no modelo, sendo que o usuário real deve estar a uma distância constante da câmera e deve ficar com a cabeça em posição frontal.
Para variação de escala do modelo 3 D, foram considerados os FPU (Face Parameter Units) (32) propostos no modelo MPEG4 padrão, mostrados na Figura 4.8.
Em os experimentos foi solicitado ao usuário que ficasse com o rosto parado por 1 segundo no início da aplicação.
Os primeiros 20 foram utilizados como quadros de treinamento, a fim de estabelecer um frame de referência para o modelo de animação.
Em esta fase do treinamento, foi calculado o ESO e IRISDO, representados na Figura 4.8.
Também foi estabelecida a posição média de cada ponto de referência da face, podendo ser chamados desta forma¯ 1, y¯ 2, y¯ 8, y 1), (x Os FAP's (Facial Animation Parameters) enviados para o modelo de animação facial, consistem em deslocamentos dos pontos de referência de µ e são calculados da seguinte forma:
F api $= µi -- Xi.
F AP Uavatar F AP Ureal onde F AP Ureal são as Facial Animation Units medidas na fase de treinamento, e F AP Uavatar são as Facial Animation Units do modelo do avatar 3D.
As setas na Figura 4.8 indicam o sinal de cada F api.
Os avatares utilizados neste experimento foram gerados com o pacote FaceGen1, que preserva as proporções antropométricas de um rosto humano padrão.
Assim sendo, o mapeamento dos pontos do usuário real para o avatar não necessitaram de nenhum ajuste adicional em funções de deformação.
A Figura 4.9, mostra alguns resultados da animação do avatar 3D.
É importante salientar que não foram utilizadas informações das sobrancelhas para a animação do avatar 3 D, foram utilizadas apenas as informações dos olhos.
Em esta dissertação foi apresentado um modelo para detecção da íris, desenvolvido no âmbito de pesquisa do laboratório de pesquisa VHLAB, e foi apresentada uma abordagem para detecção de componentes faciais referentes a região dos olhos, baseada no modelo de cor vermelho invertido e relações antropométricas.
A partir de os resultados obtidos nos testes realizados no Capítulo 4, foi possível concluir que o modelo proposto identifica de uma forma correta os componentes faciais referentes aos olhos com um erro médio menor que o encontrado utilizando as aplicações de Cristinacce e Coote e Milborrow e Nicolls, buscando as componentes referentes a olhos.
Notou- se também a necessidade de trocar a abordagem para a busca por sobrancelhas, pois somente a execução do operador exponencial não foi suficiente para salientar as diferenças entre os pixeis que representam as sobrancelhas.
Os erros na detecção da sobrancelha são agravados com a grande variação de iluminação que ocorre na base de dados IMM.
O modelo se distinguiu dos demais por seu desempenho computacional, as aplicações de Cristinacce e Milborrow tiveram um tempo de localização de componentes superior ao do modelo proposto, como pode ser observado na Seção 4.3 onde são expostos os tempos médios gastos por cada modelo para processar um quadro.
Pode- se também observar que o mapeamento descrito na Seção 4.4 expressou de uma forma correta a movimentação dos olhos no avatar 3 D, apresentando animações que não condiziam com as faces reais, apenas quando algum componente não foi localizado na imagem.
O modelo proposto apresenta como principal contribuição o funcionamento em tempo real, bem como a sua precisão em comparação com os outros modelos e o fato de funcionar com uma simples webcam.
Trabalhos Futuros Há algumas possibilidades de melhorias para o modelo desenvolvido.
Uma de elas poderia ser a utilização de máscaras deformáveis na Seção 2.1, para a localização dos componentes faciais, ao invés de fazer a localização dos componentes nos sentidos horizontal e vertical do blob.
Também poderia ser utilizada alguma técnica para minimizar a dependência do modelo quanto a variação de iluminação.
Uma destas técnicas poderia ser a utilização do algoritmo Retinex (foi testado no modelo de detecção da íris) para reduzir as variações de intensidade da imagem.
Uma proposta futura seria testar o modelo com um maior número de bases de dados de imagens públicas, e a comparação dos resultados nestas bases de dados ser feita com um maior número de modelos, inclusive comerciais como a FaceAPI1.
