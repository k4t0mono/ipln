A busca de soluções informatizadas, com o objetivo de se obter agilidade e confiabilidade nas informações, faz com que profissionais de diferentes áreas utilizem tecnologias com propósitos semelhantes.
A utilização de sistemas de gerenciamento de workflow é um exemplo desse tipo de solução, a qual empresas e cientistas utilizam para documentar as etapas executadas e otimizar o tempo de execução.
Esta Tese apresenta um padrão capaz de manipular grandes volumes de dados e otimizar seu processamento, identificando grupos de dados promissores, como um componente de workflows científicos.
A área de aplicação é a Bioinformática, uma área multidisciplinar, que se utiliza de várias ferramentas computacionais para a realização de seus experimentos, os quais podem demorar anos para serem finalizados.
A solução proposta beneficia, dentro de a Bioinformática, o desenho racional de fármacos.
Assim, a contextualização da área de estudo é realizada, e é proposta uma solução para o problema por meio de a definição de um padrão de dados que permite a autoadaptação de instâncias de workflow em execução.
O P-MIA:
Padrão Múltiplas Instâncias Autoadaptáveis, assim denominado por manipular um grande conjunto de dados e por, em tempo de execução, definir as ações a serem executadas sobre os dados, é formalizado com base nas definições de redes de Petri e sua representação gráfica feita por meio de redes de Petri coloridas.
Sobre o padrão, são realizados testes experimentais, os quais comprovam que, com a utilização do P-MIA, é possível reduzir a quantidade de experimentos, mantendo um critério de qualidade aceitável.
Palavras-Chave: Workflows Científicos, Padrão de Dados, Autoadaptação, Bioinformática P-SaMI:
SELF-ADAPTIVE MULTIPLE INSTANCES ­ A Data PATTERN Te o SCIENTIFIC Com o passar dos anos e com a evolução da computação, empresas e cientistas convergem na busca de soluções automatizadas que forneçam agilidade e confiabilidade às informações.
A utilização de tecnologias de controle é amplamente difundida no meio empresarial e a implementação de sistemas de gerenciamento de workflow propicia esse controle, automatizando etapas de um processo antes manual, diferenciando empresas e conferindo competitividade.
Com a grande aplicabilidade desses sistemas no meio empresarial, a área científica busca sua utilização, também com o objetivo de controle, mas principalmente com o objetivo de documentar as etapas executadas e de otimizar o tempo de execução.
Assim, a Bioinformática, uma área multidisciplinar, em a qual várias ferramentas computacionais são aplicadas para a realização de experimentos e esses experimentos repetidos por diferentes cientistas, busca, com a utilização de sistemas de gerenciamento de workflow, o registro das etapas executadas (proveniência) e a realização desses experimentos de forma mais rápida, uma vez que experimentos in-silico podem demorar anos para serem finalizados, conforme apresentado no Autores como Coutinho et al.&amp;&amp;&amp;
E Mattoso et al.
Afirmam que uma das maiores características da área de Bioinformática é a manipulação de um grande volume de dados e a realização de experimentos por meio de simulação computacional, ou seja, experimentos in silico, demandando grande capacidade de processamento por parte de os computadores.
Assim, medicamentos passou a ser realizado de maneira mais lógica, sendo chamado de Desenho Racional de Fármacos.
Uma das principais etapas do desenho racional de fármacos é a docagem molecular, em a qual se investiga e avalia o melhor encaixe de um ligante na compostos (ligantes) com uma determinada proteína-alvo (receptor) e sua respectiva conformação (também chamada de snapshot), se torna inviável de ser executada, conforme detalhado também no Capítulo 3, pois se estima que seriam necessários aproximadamente 62 trilhões de minutos até o término da execução de todos os experimentos, considerando- se um tempo mínimo por execução de 1 minuto.&amp;&amp;&amp;
Portanto, além de uma contextualização breve da área, este documento também apresenta a definição de uma solução que possibilita a execução de experimentos em Bioinformática selecionados dinamicamente.
Sabe- se que um dos grandes desafios da docagem molecular, além de manipular grandes volumes de dados, é a otimização de recursos computacionais.
Para essa otimização, neste trabalho, utilizará- se- o conceito de adaptação, que neste caso é aplicado aos dados em execução.
Portanto, a definição de um padrão de dados que possibilite a execução de experimentos, selecionados de maneira inteligente, a partir de critérios de qualidade previamente estabelecidos, possibilita a redução do tempo total de execução, com resultados satisfatórios.
Para isso, utiliza- se da definição de quais dados em execução devem ser finalizados, quais devem ser descartados e quais devem ter suas prioridades alteradas, em tempo de execução.
A principal contribuição desta pesquisa está na definição de um novo padrão de dados que aplica técnicas de adaptação dinâmica sobre um grande volume de dados, sua formalização e aplicação com dados reais, verificando o ganho obtido.
Considera- se, para isso, que o ganho é a redução do número de experimentos realizados, bem como o tempo total para finalização de todo o experimento.
Isso é possível ao se fazer uso dos snapshots que provavelmente apresentarão os melhores resultados e ao considerar a possibilidade de execução em paralelo desses experimentos.
A partir de a necessidade de processamento de grandes volumes de dados da área de Bioinformática, remete- se à seguinte questão de pesquisa:
&quot;Como reduzir a quantidade de snapshots a serem processados, reduzindo o tempo total de processamento e procurando manter o mesmo nível de acerto na identificação de compostos promissores?»
Objetivo Geral O objetivo geral desta Tese de Doutorado é melhorar o tempo final de processamento, reduzindo a quantidade de experimentos de docagem molecular, com base nos resultados obtidos em tempo de execução.
Objetivos Específicos Como objetivos específicos buscam- se:
Para o desenvolvimento desta Tese foram realizados:
Um levantamento, com o intuito de identificar as necessidades de processamento do LABIO -- PUCRS (Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas da Pontifícia Universidade Católica do Rio Grande do Sul);
Uma revisão da literatura para identificar aspectos vistos como importantes nos trabalhos seminais da área de automação de workflows e seus padrões;
A definição de uma proposta de modelo de dados alinhado com a literatura identificada e com o estado da arte apresentado;
E, por fim, uma pesquisa experimental, a partir de a determinação de um objeto de estudo, selecionando as variáveis que seriam capazes de influenciar- lo e de um experimento com dados reais para a verificação e análise do modelo proposto.
A Tese está organizada da seguinte forma:
Em o Capítulo 2 são apresentados conceitos sobre workflows científicos e workflows de negócios, bem como modelos utilizados para a formalização de processos:
Rede de Petri e redes de Petri Coloridas.
Esses conceitos são fundamentais para o entendimento do padrão definido nesta Tese.
Em o Capítulo 3 a Bioinformática, uma área de conhecimento convergente e multidisciplinar, área com o qual o padrão apresentado nesta Tese é validado, é apresentada.
Além de a apresentação de forma genérica da área, um detalhamento sobre algumas pesquisas realizadas por o LABIO, que são convergentes ao trabalho desenvolvido nesta Tese, também é encontrado.
Em o Capítulo 4 é apresentada uma série de padrões de dados, os quais objetivam capturar as diferentes formas de representação e utilização dos dados sobre workflows, além de alguns padrões de fluxo, utilizados como base para o desenvolvimento do trabalho aqui desenvolvido.
O estudo destes padrões objetiva a identificação da existência de algum que seja capaz de atender à necessidade de áreas como a Bioinformática:
Manipular grandes volumes de dados, com características semelhantes, na menor quantidade de tempo possível.
Petri coloridas.
O funcionamento do padrão definido nesta Tese é apresentado no Capítulo 6, identificando suas características e regras para implementação.
Além disso, a integração com as pesquisas realizadas no LABIO, por meio de a substituição de etapas do workflow científico, desenvolvido por Karina Machado, também é detalhada.
Para a validação do funcionamento do padrão proposto, o Capítulo 7 contém resultados de testes que foram realizados com grupos definidos a partir de uma função de similaridade.
Os testes apresentados foram realizados com dois Em o Capítulo 8 os trabalhos relacionados com o trabalho desenvolvido são apresentados e, no final deste capítulo, uma tabela de comparação, contendo as principais características dos diferentes trabalhos sintetiza os estudos realizados.
As considerações finais do trabalho desenvolvido nesta Tese de Doutorado, as principais contribuições e os trabalhos futuros, são apresentados na sequência.
Este capítulo apresenta conceitos sobre workflows de negócio e workflows científicos, suas características, semelhanças e diferenças, subsidiando o desenvolvimento do padrão definido nesta Tese, cuja comparação entre esses dois tipos é realizada nas considerações finais deste capítulo.
Além de os dois tipos de workflows, este capítulo também apresenta redes de Petri e redes de Petri coloridas, utilizadas nesta Tese para a formalização de processos e para a representação do padrão definido, podendo este último ser encontrado no capítulo 5.
O bom funcionamento de uma empresa depende, essencialmente, das possibilidades de acesso à informação e do processo decisório baseado sobre elas.
O trabalho administrativo é um trabalho em equipe, onde cada membro, um ator, tem um conjunto próprio de atribuições, responsabilidades e autonomia para a realização de suas atividades.
Essas atividades, contudo, nem sempre são independentes.
Geralmente, há uma interdependência na realização das atividades, causada por a manipulação e transformação de elementos de informação compartilhados em instantes diferentes.
Há, em consequência, uma estreita cooperação no trabalho realizado.
Para organizar e assegurar a qualidade dessa cooperação é importante que existam modelos com capacidade de descrição e ambientes de automação computacional, possibilitando o emprego desses tipos de descrição para o suporte computacional do fluxo de trabalho.
Uma das soluções para atender a esses requisitos é a utilização da tecnologia de workflow.
Essa tecnologia permite representar, manipular e monitorar informações relativas ao fluxo de trabalho e as utiliza para gerenciar, coordenar e controlar o trabalho administrativo de maneira mais eficiente.
Esse tipo de suporte computacional é chamado automação de workflow.
Para a WfMC (Workflow Management Coalition ­) workflow é a automação do processo de negócio, na sua totalidade ou em partes, onde documentos, informações ou tarefas são passadas de um participante para o outro para execução de uma ação, de acordo com um conjunto de regras de procedimentos.
Plesums afirma que workflow é tradicionalmente definido em relação a termos utilizados em escritórios, movimentando documentos, processando ordens e executando chamadas.
Entretanto, o autor também afirma que esse princípio é o mesmo utilizado para outros sistemas.
Segundo Plesums, sistemas de automação de workflow tornaram- se populares inicialmente com a utilização de sistemas de gerenciamento de imagens (Document Imaging ­ GED:
Gerenciamento Eletrônico de Documentos).
O autor ainda afirma que o gerenciamento automatizado de workflow tem sido continuamente discutido nos últimos 20 anos e que muitas pessoas o visualizam como parte de um sistema de manipulação de imagens.
Em, Prior relata que, na década de 90, os workflows eram utilizados como solução para a reengenharia de processos de negócio, buscando, simplesmente, automatizar- los.
O foco estava, apenas, na utilização da tecnologia, ou seja, nas aplicações e em sistemas com baixa interação humana e isso não significava garantia de sucesso.
A automatização de sistemas de workflow é realizada por meio de um Sistema de Gerenciamento de Workflow (SGWf) que, conforme a WfMC e Aalst, é um sistema que define, cria e gerência a execução de workflows por meio de o uso de software, executando num ou mais servidores.
Sistema esse, apto a interpretar a definição dos processos, a interagir com seus participantes e, quando requerido, ativar ferramentas de software e aplicativos.
Com a habilidade de se modelar processos de negócios, monitorar- los em tempo real e, por esses processos serem mais simples de serem controlados, cresceu o interesse por o gerenciamento dos processos de negócio (Business Process Management ­ BPM).
Para Aalst, BPM é definido como o &quot;apoio aos processos de negócio, usando métodos, técnicas e software para projetar, desempenhar um papel, controlar e analisar processos operacionais, envolvendo pessoas, organizações, aplicações, documentos e outras fontes de informação».
As principais funcionalidades oferecidas por um SGWf são definidas por Georgakopoulos, Hollingsworth e Leymann:
A Figura 1 apresenta a relação entre os principais termos envolvidos na automatização de processos (workflows).
O vocabulário utilizado nesse trabalho é o definido por a WfMC.
Conforme apresenta a Figura 1, um processo de negócio é elaborado por meio de a sua definição, ou seja, o que deve acontecer quando de a execução desse processo, e é gerenciado por um sistema específico.
Esse sistema deve controlar os aspectos automatizados.
A definição dos processos é composta por atividades que, num sistema de workflow, correspondem a uma etapa a ser executada dentro de um processo, podendo ser executadas de forma manual ou automatizada.
As atividades automatizadas são, quando gerenciadas por o SGWf, armazenadas como instâncias das atividades.
A cada implementação de um novo processo informatizado, suas atividades devem ser analisadas e definidas.
Tipos de Workflow Em a literatura são encontradas diferentes classificações para workflow.
A mais popular é a de Georgakopoulos, que os classifica como:
Ad-hoc, administrativo e de produção.
A classificação feita por Leymann acrescenta workflows colaborativos:
Workflow Ad-Hoc: Workflows ad-hoc executam processos de negócios, tais como documentação de produtos ou venda de produtos, onde não há um padrão prédeterminado de movimentação de informação entre pessoas.
Tarefas do tipo ad hoc envolvem a coordenação humana ou a co-decisão, conforme Schael et al.
Workflow Administrativo: Sistemas de workflow administrativos destinam- se a processos simples e estruturados.
São processos burocráticos, repetitivos, com regras bem definidas e do conhecimento de todos os participantes do fluxo.
De a mesma forma que os ad-hoc, também apresentam baixo valor para o negócio.
Workflow de Produção: Um workflow de produção envolve processos de negócios repetitivos e previsíveis.
Englobam processamentos complexos, com acesso a múltiplos sistemas de informação.
São caracterizados por fornecerem muito valor ao negócio.
Workflow Colaborativo: Os sistemas de workflow colaborativo são adequados para processos que envolvam trabalho cooperativo realizado por equipes de pessoas com objetivos comuns, como os sistemas de groupware que, conforme Aalst, são sistemas que suportam esse estilo de trabalho.
Podem ser adotados para automatizar processos empresariais críticos que não são orientados à transação.
Esses processos podem ser executados poucas vezes.
Entretanto, sua execução e seu sucesso são essenciais para a Organização.
Deelman e Gil afirmam que nas últimas duas décadas a revolução está sendo conduzida por a ciência e por a engenharia e que supercomputadores têm sido utilizados para simularem sistemas que envolvem dados complexos e visualização das etapas desses processamentos.
Apresentam como um cenário típico a necessidade cíclica de envio de dados para supercomputadores para análise ou simulação, com armazenamento dos resultados obtidos.
Para os autores, sistemas que implementam workflows científicos objetivam automatizar esse ciclo, possibilitando aos cientistas foco em suas pesquisas e não no gerenciamento computacional.
Assim, definem um workflow como se referindo à sequência de atividades necessárias para se gerenciar um processo de negócio, um processo científico ou um processo de engenharia.
Uma instância desse workflow é a instanciação de um problema particular e inclui a definição de dados de entrada.
O objetivo de sistemas de Workflows Científicos é, portanto, possibilitar um ambiente de programação especializada para simplificar os esforços investidos por cientistas na finalização de um experimento científico com suporte computacional.
Conforme Ludaescher et al.[
LUD06, LUD09+ o termo &quot;workflow «tem sido tradicionalmente utilizado no contexto de aplicações de negócios.
Para os autores um workflow científico facilita ou automatiza um &quot;processo científico», por exemplo, a execução de um experimento numa ferramenta, seguido por o pós-processamento dos dados e a interpretação desses resultados.
Definem um workflow científico como uma descrição dos processos que um cientista necessita executar, e na ordem correta de execução, para criar um produto para a ciência.
Perspectivas de Usuários Ludaescher et al.
Apresentam as diferentes perspectivas de usuários em relação a workflows científicos:
Visão de Cientista do Domínio:
De o ponto de vista de um usuário final, a automação é o maior benefício da abordagem de workflows científicos.
Por exemplo, um cientista executa diferentes conjuntos de dados com a mesma sequência de etapas, usando ou não diferentes parâmetros de configuração.
O sistema está apto a registrar todas as interações dos usuários durante a execução do workflow, além de registrar informações que facilitem a interpretação dos dados registrados e um acompanhamento em caso de necessidade.
Outros requisitos da perspectiva dos cientistas incluem:
O projeto de workflow deve ser intuitivo e direcionado à reutilização e reconfiguração.
Além disso, deve possibilitar a execução por longos períodos de tempo monitorando- a e, se necessário, poder suspender ou abortar essa execução de forma remota ou dinamicamente alterar seus parâmetros de configuração.
A especificação de um workflow científico deve ser compartilhada e discutida com a comunidade científica.
Visão de Engenheiro do Workflow: Os cientistas do domínio são os usuários finais e participam cada vez mais dos projetos dos workflows científicos.
O papel de engenheiro do workflow é similar ao de engenheiro de software:
Conhecer as diferentes ferramentas disponíveis que podem ser utilizadas por sistemas de workflows científicos.
Muitas ferramentas, por exemplo, podem ser executadas por linhas de comando, outras por web services.
Enquanto cientistas da computação desenvolvem e aprimoram sofisticados métodos, alguns detalhes podem estar omissos aos engenheiros de workflow, podendo utilizar uma linguagem de modelagem de alto nível.
Outro objetivo dos engenheiros de workflow é o desempenho:
A partir de uma descrição de workflow, analisar como o workflow poderia ser executado eficientemente num ambiente de cluster ou grade, ou quais as vantagens das tarefas serem executadas em pipeline ou com a utilização de paralelismo.
Visão do Cientista da Computação: A linha que separa os papéis e visões de engenheiro de workflow e cientista da computação não é muito clara.
Por exemplo, workflows científicos modelam e projetam métodos que podem ser baseados em combinações de técnicas de engenharia de software, teorias de bancos de dados, otimização de consultas, processamento de fluxo, programação funcional.
Em particular, quando se cria um novo paradigma de modelagem, um cientista da computação deve estar interessado nos formalismos que possibilitam a análise estática dos workflows para garantir algumas propriedades como definição de tipos, liberdade de deadlocks, entre outros.
Um modelo formal de computação auxilia cientistas da computação a definirem estratégias de planejamento e otimização de workflows.
Alguns problemas relacionados a workflows não são simples e requerem heurísticas que garantam um planejamento eficiente.
Outro desafio da ciência da computação é projetar e implementar sistemas de gerenciamento de proveniência, eficientemente, para workflows científicos.
Ludaescher et al.
E afirmam que muitas pesquisas sobre workflows científicos estão focadas na otimização de algoritmos, nos desempenhos dos sistemas e nos requisitos de memória.
Entretanto, definem que o recurso mais precioso em e-Science é o tempo humano.
Níveis e Abordagens para Workflows Científicos Para Rodriguez et al.
Assim como workflows de negócios, especificações de workflows científicos podem ser executadas por softwares apropriados para execuções científicas, como os Sistemas de Gerenciamento de Workflows.
Para os autores, o significado dos links (conexões entre as tarefas) em workflows científicos pode ser analisado sob diferentes aspectos:
Em a abordagem direcionada a Controle, os links entre as tarefas representam restrições de controle para o desempenho de cada tarefa.
Muitas estruturas de controle podem ser encontradas, desde as mais básicas como sequências que representam a execução ordenada de tarefas até as mais complexas estruturas de controle como splits, joins, loops etc..
Os padrões de workflows foram propostos dentro de o domínio de negócios, para controlar e descrever os possíveis comportamentos do controle.
Em a abordagem direcionada a dados, os links entre as tarefas representam dependências de dados.
Uma tarefa consome dados e produz dados.
Cada tarefa pode ser iniciada quando uma determinada entrada está disponível.
Essa abordagem tem a vantagem de que a execução paralela de tarefas independentes é modelada de forma livre.
Os autores também afirmam que existem diferentes argumentos para se utilizar uma ou outra abordagem.
Por um lado, para Pautasso e Alonso, a utilização direcionada a controle fornece maior domínio sobre a ordem atual de execução das tarefas.
Por outro lado, as representações direcionadas a dados são mais simples para usuários finais.
A abordagem direcionada simplesmente a dados, entretanto, não é suficientemente expressiva para modelar o comportamento eventualmente iterativo dos processos.
Muitas propostas seguem uma abordagem híbrida, baseada na combinação de controle e dados, utilizando as principais características de cada uma de elas.
Rodriguez et al.
Também apresentam que a execução de workflows científicos envolve o processamento de uma especificação de workflow, coordenando o uso apropriado de recursos ou serviços.
Apresentam a definição de três níveis de workflows:
Workflows Abstratos: Alto nível de abstração do workflow que contém a informação sobre o que é feito em cada tarefa e como as tarefas são interconectadas.
Não existe menção de como é a entrada dos dados ou a implementação das tarefas.
Como exemplo, um workflow abstrato pode ser descrito como uma equação linear que recebe entradas de um dispositivo e envia os resultados a um monitor;
Workflows Concretos: O mapeamento para workflows concretos envolve a informação sobre a implementação de cada tarefa e os recursos a serem utilizados, informações sobre quais métodos são chamados, bem como o formato de dados necessário para troca de informações;
Workflows Instanciados: Um workflow instanciado representa o mapeamento atual do workflow concreto dentro de recursos computacionais, envolvendo as respectivas entradas de dados e suas respectivas saídas.
Esse nível pode ser decomposto em mais dois, conforme Deelman e Gil:
Estratégias de Processamento de Workflows em Glatard et al.
Encontra- se a distinção entre duas estratégias de processamento de workflows:
Baseada em tarefas e baseada em serviços.
As soluções baseadas em tarefas, também denominadas de computação global, apresentam cada tarefa formalmente descrita antes de ser submetida à execução.
Os usuários definem qual é a tarefa a ser executada.
Os arquivos de códigos executáveis, dados de entrada e parâmetros são utilizados para habilitar a execução.
O sistema de gerenciamento de workflow encontra o recurso computacional apropriado para a execução.
As soluções baseadas em serviço, onde a execução é manipulada por um serviço externo, são invocadas por uma interface.
Os serviços são vistos como caixas pretas de um sistema de gerenciamento de workflow, onde apenas a chamada por intermédio das interfaces é conhecida.
O sistema de gerenciamento do workflow deve encontrar os serviços apropriados para executar as funcionalidades requeridas.
Para os autores, workflows baseados em tarefas e baseados em serviços diferem na manipulação dos dados.
A natureza não estática da descrição dos dados na abordagem baseada em serviços habilita extensões dinâmicas de conjuntos de dados a serem processados:
Um workflow pode ser definido e executado utilizando um conjunto completo de dados de entrada que não são conhecidos em detalhes, pois novos dados podem ser construídos por novas fontes.
Os autores ainda afirmam que é comum em aplicações científicas que a aquisição dos dados seja realizada por processos &quot;pesados «e que os segmentos de dados sejam produzidos progressivamente.
Alguns workflows, inclusive, podem produzir seus próprios dados, parando a produção desses dados quando as entradas suficientes tiverem sido criadas para produzir os resultados necessários.
Finalmente, essa dinamicidade é requerida quando o dado de entrada é o resultado de uma consulta à base de dados, em a qual o tamanho da resposta não é conhecido antecipadamente.
Para os autores, uma diferença significante entre as abordagens de tarefa e serviço está na habilidade dos serviços trabalharem com conjuntos de dados dinâmicos, em a qual existam laços, coletando dados de diferentes fontes.
Conforme Glatard et al.,
de o ponto de vista de usuário, a principal diferença entre as abordagens surge quando se considera a re-execução de uma mesma aplicação de workflow sobre diferentes conjuntos de entradas de dados, comumente feita por a instanciação de aplicações em paralelo.
Em o workflow baseado em tarefa, uma tarefa computacional é definida por um simples conjunto de dados de entrada e um processamento simples, executando o mesmo processamento sobre dois diferentes conjuntos de dados, resultando na descrição de duas tarefas independentes.
Essa abordagem enfatiza a replicação de grafos de execução para todos os dados de entrada a serem processados.
Para a abordagem baseada em serviço, uma simples extensão da abordagem baseada em tarefas é proposta, em a qual uma tarefa genérica pode ser descrita por um conjunto de dados de entrada, resultando na execução de múltiplos jobs:
Um por dado de entrada.
Tarefas parametrizadas não podem ser utilizadas num workflow, em o qual cada tarefa necessita ser replicada para todos os conjuntos de dados.
Por outro lado, a abordagem baseada em serviços facilmente acomoda conjuntos de entradas de dados por a flexibilidade da abordagem.
Apesar disso, essas são tecnologias de momento, devendo ser definidas em momento de implementação.
Glatard et al.
Assumem que o primeiro nível de paralelismo que pode ser explorado é o paralelismo intrínseco de workflows, representado por o esquema modelado.
Essa implementação é comum e é realizada por os sistemas de gerenciamento de workflow.
Quando se considera aplicações com grande volume de dados, muitos conjuntos de dados de entrada são processados sobre um determinado workflow, beneficiando- se de um grande número de recursos disponíveis num ambiente eventualmente paralelo, onde serviços de workflow podem ser instanciados como muitas tarefas computacionais, executando em diferentes recursos de hardware e processando diferentes dados de entrada em paralelo.
Os autores também definem paralelismo de outros dois níveis:
O paralelismo de dados denota que um serviço está habilitado a processar muitos fragmentos de dados simultaneamente com perda mínima de desempenho.
Essa capacidade envolve o processamento de dados independentes em diferentes recursos computacionais.
Para os autores, então, o paralelismo de dados ocorre quando diferentes conjuntos de dados aparecem numa simples tarefa do workflow, enquanto paralelismo intrínseco ocorre quando o mesmo conjunto de dados aparece várias vezes em diferentes tarefas de um mesmo nível.
Conjuntos de dados de entrada são independentes entre si, para serem instanciados em paralelo.
O paralelismo de serviço denota que o processamento de dois diferentes conjuntos de dados de entrada por dois serviços são totalmente independentes.
Esse modelo em pipeline pode ser adaptado por execuções sequenciais em workflows baseados em serviços.
O paralelismo de serviço ocorre quando diferentes conjuntos de dados aparecem em diferentes células de um mesmo nível.
Os autores supõem que cada serviço processe um único conjunto de dados ao mesmo tempo, fazendo, assim, paralelismo de serviços.
Esta seção apresenta Redes de Petri e Redes de Petri Coloridas, identificando suas formas de representação, analisando suas características e particularidades.
Redes de Petri (rdP) Historicamente, as Redes de Petri surgiram de um estudo realizado por Carl Adam Petri, em sua Tese de Doutorado em 1962 Apud.
Desde então, a utilização e o estudo dessas redes têm crescido consideravelmente, o que faz com que seja um dos exemplos mais conhecidos de teoria de ordem parcial para modelagem e análise de sistemas concorrentes.
De acordo com Braghetto, a popularidade das redes de Petri (rdP) como ferramenta para estudo de sistemas se deve à sua representação gráfica de fácil compreensão e ao seu potencial matemático para a análise de processos.
Uma das grandes vantagens dessas redes é a sua flexibilidade e o alto poder de abstração, possibilitando introduzir e adaptar elementos gráficos e/ ou matemáticos com a finalidade de aproximar usuários de sistemas mais específicos.
Heuser afirma que as Redes de Petri por si só não têm significado algum e que para utilizar- las é necessário que elas tenham uma interpretação.
Além disso, muitas vezes, as redes ainda são complementadas com textos, chamados de anotações.
Os conceitos associados a cada um dos itens de uma rdP são:
Lugar (representado graficamente por um círculo):
Modela uma condição que deve ser satisfeita para que o disparo da ação seja realizado.
Conforme Braghetto* &quot;BRA06+, «um lugar de entrada de uma transição geralmente expressa uma précondição, um dado/ sinal de entrada, um recurso de hardware/ software requerido ou ainda um buffer.
Um lugar de saída pode ser compreendido como uma póscondição, um dado/ sinal de saída, um recurso liberado, uma conclusão ou um buffer.»
Conexão/ Transição (representada graficamente por um retângulo ou barra):
Pode ser compreendida como uma ação/ evento, ou um processamento de um sinal, ou ainda uma tarefa;
Arco (direcionado) orientado:
Liga um lugar a uma transição ou vice-versa, encadeando condições e eventos;
Marca ou ficha (tokens):
Representa um recurso disponível.
O posicionamento dessas fichas nos lugares do grafo constitui a marcação da rdP.
Cada lugar pode possuir 0 (zero) ou mais fichas.
A evolução da marcação permite modelar o comportamento dinâmico do sistema.
Essas marcas são representadas na forma de pontos dentro de os lugares;
Peso: Cada arco possui um peso associado a ele.
O peso indica quantas marcas uma transição consome de um lugar de entrada ou quantas marcas uma transição acrescenta num lugar de saída.
Quando um arco não possui um peso explicitamente indicado no grafo, considera- se que o seu peso é 1.
O grafo de uma rdP é orientado e seus arcos possuem pesos.
Uma rede de Petri em que todos os arcos possuem peso 1 é classificada como ordinária.
O disparo das transições (execução das ações) é controlado tanto por o número de marcas quanto por a sua distribuição nos lugares.
Uma transição t está habilitada se, e somente se, todos os lugares de entrada (pi P) de t são marcados com pelo menos (p, t) tokens, onde w (p, t) é o peso do arco de p para t..
Formalmente, a rede de Petri é dada por uma quíntupla, RP $= T, F, W, M0\&gt;, em que:
F (P x T) (T x P) conjunto de arcos;
P T $= e P T.
As Redes de Petri são uma tripla T, F\&gt; onde:
P é um conjunto finito de lugares;
F (P × T) (T × P) é um conjunto de arcos (fluxos).
A definição de Aalst é mais simples que a de Murata, pois não possui uma marcação inicial da rede.
Considera que um lugar p é chamado de lugar de entrada de uma transição se existir um arco direcionado de p para t..
Um lugar p é chamado de lugar de saída de uma transição se existir um arco direcionado de t para p..
Além disso, Aalst restringe os arcos com peso 1 e afirma que, no contexto de workflows, não faz sentido ter outros pesos, pois os lugares correspondem a condições.
Existe uma série de métodos que permitem analisar um grande número de propriedades de sistemas descritos por rdP.
As propriedades das redes de Petri podem ser divididas em dois grandes grupos:
As comportamentais, que dependem da marcação inicial e as estruturais, que não dependem.
A Figura 3 apresenta uma modelagem de processos numa rede de Petri ordinária, apresentado em Aalst, representando um sistema para processar reclamações.
A modelagem inicia com o registro de uma reclamação;
Após o registro são executadas paralelamente:
O envio de um questionário para o reclamante e a análise da reclamação.
O questionário é processado se o reclamante retornar- lo dentro de um prazo de 2 semanas;
Caso contrário o questionário é descartado.
Com base no resultado da avaliação, a reclamação pode ou não ser processada.
Esse processamento somente acontece depois da ocorrência do processamento do questionário ou da expiração do prazo de 2 semanas (time_ out).
Após, é realizada uma verificação e, caso o processamento esteja correto, a reclamação é arquivada.
A existência de problemas faz com que o processamento seja reiniciado.
Uma transição pode estar associada a um ou mais lugares de entrada e a um ou mais lugares de saída.
Por exemplo, na rede da Figura 3, o conjunto dos lugares de entrada da transição &quot;send_ questionnaire «é, c 1- e o conjunto dos lugares de saída da transição &quot;register «é, c1, c 2-. A quantidade de marcas e sua distribuição nos lugares controlam o disparo das transições.
Uma transição está habilitada se o lugar possui um número de marcas superior ou igual ao peso do arco que liga o lugar à transição.
Quando uma transição é executada cada marca é removida do seu lugar de entrada (de acordo com o peso dos arcos que ligam os lugares de entrada à transição) e cada uma de elas é adicionada em cada um dos seus lugares de saída (de acordo com o peso dos arcos que ligam a transição aos seus lugares de saída).
A marcação de uma rede de Petri muda a cada disparo de transição, permitindo simular o comportamento dinâmico do sistema e definir o seu estado num dado momento.
Um aspecto importante na execução de uma transição é que não há dependência quantitativa entre as marcas de antes e depois da sua execução.
Redes de Petri Coloridas Jensen, em, apresenta uma breve introdução sobre redes de Petri coloridas (CP-net), uma das extensões de redes de Petri, a qual é utilizada nesta Tese para representar o padrão proposto.
O exemplo da Figura 4, modelado por os autores com CPN Tools, descreve um simples protocolo de transporte, transferindo pacotes numa rede confiável (Network) de um remetente (Sender) para um receptor (Receiver).
As elipses e círculos são chamadas de lugares.
Eles descrevem os estados do sistema.
Os retângulos são chamados de transições.
Eles descrevem as ações.
As setas são chamadas de arcos.
As expressões de arco descrevem como o estado de uma CP-net é alterado quando uma transição ocorre.
Cada lugar contém um conjunto de marcações chamadas tokens.
Em contraste com redes de Petri de baixo nível, cada um desses tokens carrega um valor de dado, que é de um tokens no seu estado inicial.
Todos os valores dos tokens são do tipo INTxDATA e representam sete pacotes que serão lidos para serem enviados.
O primeiro elemento é o número do pacote, enquanto o segundo é o conteúdo do pacote.
Todos os 1's indicam que existe exatamente um de cada um dos pacotes para envio, pois em geral um lugar pode ter muitos tokens com o mesmo INT.
Esses lugares representam dois contadores, guardando o número do próximo pacote a ser Data.
Representa o conteúdo das mensagens que tenham sido recebidas com sucesso.
Os quatro lugares restantes:
A, B, C e D não têm tokens no estado inicial.
Eles representam buffers de entradas e saída da rede de transmissão.
Redes de Petri Coloridas tem esse nome porque permitem o uso de tokens que carregam valores de dados e podem se distinguir um dos outros, diferente dos tokens de Redes de Petri de baixo nível, que são desenhados como um ponto preto.
Em o início, somente conjuntos de cores pequenos e não estruturados eram utilizados, enumerando um conjunto fixo de processos, por exemplo.
Mais tarde, percebeu- se que era possível generalizar a teoria e as ferramentas, de tal forma que os tipos de dados complexos pudessem ser utilizados como conjuntos de cores.
Para estar habilitada para ocorrer, uma transição deve ter tokens suficientes nos seus lugares de entrada e esses tokens devem ter valores que correspondam às expressões dos arcos.
Como exemplo, considera- se a transição SendPacket na Figura 4.
Ela possui três arcos, dois dos de tipo Data.
Para que a transição ocorra, deve- se vincular essas duas variáveis a valores dos mesmos tipos, de tal maneira que a expressão de arco, de cada arco de entrada, é avaliada como um valor de token que está presente no lugar de entrada correspondente.
Desde que NextSend somente contém um token com valor 1, é óbvio que n deve possuir o valor 1.
Seguindo, nota- se primeiro elemento do par é 1.
Com a ocorrência da transição SendPacket é Quando a transição ocorre, ela remove os dois tokens especificados dos lugares de entrada, mas logo os coloca de volta, devido a os arcos bidirecionais.
Simultaneamente ela produz uma cópia do token no lugar A. Intuitivamente isso significa que o primeiro pacote foi enviado contador do NextSend, isso porque o pacote pode ser perdido na rede, necessitando de retransmissão.
O protocolo é pessimista, já que mantém o mesmo pacote para retransmissão até que receba um aviso, solicitando um novo pacote.
Quando o token é colocado no lugar A, a transição TransmitPacket é habilitada com duas ligações diferentes:
Se a primeira ligação for a escolhida, o pacote é transferido do lugar A para o B. Se a segunda ligação for a escolhida, o pacote é perdido na rede, representado por a expressão empty.
A escolha entre as duas ligações é não-determinística (na simulação de interação a escolha pode ser feita por o usuário e na simulação automatizada é feita por um gerador randômico).
Em o exemplo há, portanto, 50% de chance de sucesso/ falha.
De qualquer forma é fácil modificar a CP-net para utilizar uma probabilidade diferente.
A transição SendPacket permanece habilitada e de ela uma retransmissão pode ocorrer a qualquer momento.
Os tokens de um determinado lugar podem ser utilizados em qualquer ordem, independente da ordem de sua chegada.
Isso significa que os pacotes podem ultrapassar uns aos outros, tanto para o lugar A quanto para o lugar B. Quando um token chega ao lugar B, a transição ReceivePacket é habilitada.
Ela compara o número n no pacote de entrada ao número k no contador NextRec.
Se os dois valores forem iguais, o pacote é o esperado.
Assim, o conteúdo do dado p do pacote é adicionado ao conteúdo do token str no lugar Received, utilizando o operador de concatenação de strings^, acrescenta- se uma unidade ao contador NextRec e um reconhecimento é enviado por o lugar C, contendo o número do próximo pacote a ser recebido.
Se os valores forem diferentes o pacote é ignorado.
O valor do token nos lugares Received e NextRec são inalterados.
A transição TransmitAcknow trabalha de forma similar ao TransmitPacket.
Ela transmite os reconhecimentos sobre a rede, movendo- os do lugar C para o D, a menos que ok $= false, caso em o qual reconhecimento seria perdido.
ReceiveAcknow manipula esses reconhecimentos que estão ao alcance do remetente.
Atualiza o contador NextSend, assim que o remetente iniciar a transmissão de um novo pacote.
Jensen, também apresenta a definição formal e o comportamento de redes de Petri Coloridas.
Uma rede desse tipo é definida como uma tupla múltipla, sendo entendida com o propósito de oferecer uma notação matemática, sem ambiguidades, sobre a definição de redes de Petri e sua semântica.
Qualquer rede concreta será sempre especificada por meio de um diagrama.
Pode- se ter como princípio, mas não como prática, a facilidade de se traduzir o diagrama de uma rede de Petri Colorida dentro de uma tupla de Rede de Petri e vice-versa.
A especificação dessa tupla é adequada quando se quer formular definições gerais e prover teoremas aplicados a todas as classes de redes de Petri Coloridas ou a parte de elas.
Já a especificação em forma de grafo é mais adequada quando se quer construir a modelagem de um sistema específico.
Para uma definição abstrata de Redes de Petri Coloridas não é necessário se fixar apenas na sintaxe de modelagem da rede.
Assume- se que a sintaxe, com semânticas bem definidas, torna possível a definição de caminhos não ambíguos.
Define- se, portanto, com base em Jensen, elementos de um tipo T:
O conjunto de todos os elementos em T é representado por o próprio nome de T;
Uma expressão sem variáveis é chamada de expressão fechada.
Ela pode ser avaliada em todas as ligações e, em todas elas, fornece o mesmo valor, o que significa a própria expressão.
Portanto, algumas vezes escrever expr é o mesmo que escrever expr\ b\&gt;.
Definição de Rede de Petri Colorida. Uma CP-net é uma tupla CPN $= P, T, A, N, C, G, E, I\&gt; onde:
O conjunto de tipos determina os valores de dados e as operações e funções que podem ser usadas nas próximas expressões, ou seja, nas expressões de arcos, guardas e inicializações.
Assume- se que cada tipo tem, pelo menos, um elemento.
P é um conjunto finito de lugares não vazio.
T é um conjunto finito de transições não vazio.
A é um conjunto finito de arcos em os quais:
P T $= P A $= T A $= Ø.
Os lugares, transições e arcos são descritos por os três conjuntos P, T e A os quais devem ser finitos e pares disjuntos.
Por a requisição dos conjuntos de lugares, transições e arcos de serem finitos, evita- se a possibilidade, por exemplo, de se ter um número infinito de arcos entre dois nodos.
N é uma função de nodo.
Ele é definido por A dentro de P x T T x P. A função de nodo mapeia cada arco dentro de um par onde o primeiro elemento é o nodo origem e o segundo elemento é o nodo destino.
Os dois nodos devem ser de tipos diferentes, isto é, um deve ser um lugar, enquanto o outro é uma transição.
Assume- se que uma rede de Petri Colorida pode ter vários arcos entre um mesmo par de nodos ordenados.
C é uma função colorida.
Ela é definida por P dentro de.
A função colorida C mapeia cada lugar p para um tipo C (p).
Intuitivamente, isto significa que cada token em p deve ter um valor de dado que pertença a C (p).
G é uma função de guarda.
Ela é definida por um T dentro de expressões tais que:
Onde todas as variáveis têm tipos que pertencem a.
Quando se cria esquemas em redes de Petri coloridas omitem- se as expressões de guarda, assumindo- se que sejam verdadeiras.
E é uma função de expressão de arco.
Ela é definida por um A dentro de expressões tais que:
A A:
Onde p é um lugar de N (a) e MS são todos os múltiplos conjuntos de C. A função de expressão de arco E mapeia cada arco a dentro de uma expressão de tipo C (p) MS.
Isso significa que cada expressão de arco deve avaliar os múltiplos conjuntos sobre o tipo de lugar adjacente, p..
Permite- se que um diagrama de rede de Petri colorida tenha uma expressão de arco expr de tipo C (p), e considera- se que esse seja um atalho para 1' (expr).
I é uma função de inicialização.
Ela é definida por P dentro de expressões fechadas tais que:
P: A função de inicialização I mapeia cada lugar p dentro de uma expressão a qual deve ser de tipo C (p) MS.
Quando se desenha uma rede de Petri colorida omitem- se as expressões de inicialização que seriam avaliadas como Ø.
Nardi afirma que as redes de Petri são intuitivas e de fácil compreensão para sistemas pequenos.
Identifica, como exemplo, um processo de fabricação de dois modelos de veículos utilizando três tipos de recursos, conforme demonstrado na Figura 5.
O autor afirma que «extrapolando este exemplo para a produção de trinta modelos usando quinze tipos de recursos, chega- se a uma rede muito menos legível, dado o número de repetições de atividades e estados.
Em esse caso, enquanto há duas atividades T1 na rede da Figura 5, para o novo cenário haveria até trinta atividades T1, sendo uma para cada modelo, tornando- se praticamente ilegível para sistemas grandes ou complexos».
As redes de Petri coloridas surgiram com o objetivo de suprir essa dificuldade.
Conforme Girault et al.
E Penha et al.
O principal objetivo das Redes de Petri Coloridas é &quot;a redução do tamanho do modelo, permitindo que tokens individualizados (coloridos) representem diferentes processos ou recursos numa mesma sub-rede».
Em a definição inicial do padrão, os tokens eram representados por cores ou mesmo por padrões que possibilitassem a distinção dos tokens.
Visando aumentar a abrangência, os tokens são representados por estruturas de dados complexas, contendo informações diferenciadas por tipos, possibilitando que os arcos realizem operações sobre eles.
A Figura 6 ilustra uma rede de Petri para um sistema de manufatura.
Esse mesmo sistema é apresentado em rede de Petri Colorida na Figura 7.
A rede apresentada na Figura 6 modela um sistema de manufatura com dois processos, compartilhando duas máquinas.
A adição de um novo processo, por exemplo, mesmo compartilhando as mesmas máquinas, exige que sejam modelados todos os estágios desse novo processo, por meio de estados, ações e relações entre este processo e os dois já existentes, e entre este processo e os recursos disponíveis.
Isso faz com que a rede de Petri que modela o sistema cresça significativamente, mostrando a conveniência em usar redes de Petri Coloridas, pois independente do número de processos, a estrutura continua a mesma, conforme visualizado na Figura 7.
Em a modelagem de workflow, utilizando redes de Petri, cada tarefa é representada por uma transição correspondente.
Lugares representam as pré e pós-condições ou, ainda, os recursos requeridos para executar determinada tarefa.
Os arcos representam relações lógicas entre as tarefas e o próprio fluxo de trabalho.
A representação gráfica das redes de Petri tem se mostrado muito útil, pois permite a visualização dos processos e a comunicação entre eles.
Em Aalst et al.
São encontradas três razões principais para aplicar redes de Petri na modelagem de workflow:
Para Gil et al.,
um tema importante de investigação está na possibilidade de workflows científicos serem construídos sobre tecnologias já existentes para a criação de workflows de negócios ou da exigência de existência de novas abordagens para essa criação.
Os autores afirmam que workflows científicos e de negócios não são facilmente distinguíveis, pois existem exemplos que apresentam características em comum:
Grande volume de dados, grande quantidade de paralelismo etc..
Por outro lado, os workflows científicos requerem modelagem flexível e capacidade exploratória que os distinguem dos workflows de negócios.
Outra característica de workflows científicos é a variedade e heterogeneidade de dados executados sobre um simples workflow.
Os workflows científicos, conforme os autores, apóiam discursos científicos detalhados tão bem quanto possibilitam a execução de processos repetitivos.
Além disso, deve- se prover uma visão estável do sistema, mesmo com as constantes mudanças de tecnologia e plataforma.
Isso quer dizer que uma vez obtidos resultados, esses devem ser os mesmos obtidos com uma nova execução anos mais tarde, se necessário (proveniência).
O desafio de se utilizar workflows em aplicações científicas é ainda maior por a necessidade de se validar os resultados gerados por os dados e compartilhar essas informações com a comunidade científica.
Assim, a rastreabilidade e o compartilhamento são requisitos fundamentais de workflows científicos.
Para Davidson e Freire sistemas de workflow auxiliam cientistas a conceituarem e gerenciarem processos, permitindo a criação e reuso de tarefas de análise, permitindo também o processo de descoberta por o gerenciamento de dados utilizados e gerados em cada etapa e sistematizando a informação para uso posterior.
Sistemas de workflows científicos utilizam- se, tipicamente, de modelos computacionais simples, modelos de fluxos de dados, onde a ordem de execução é determinada por os fluxos de dados do workflow.
Esse é um contraste em relação a workflows de negócios, os quais necessitam de expressivas linguagens de definição para especificar complexos fluxos de controle.
Os sistemas de workflow apresentam um grande número de vantagens para execução de aplicações científicas:
Conferem um modelo de programação simples, baseado na sequenciação de tarefas, compostas por a conexão de entradas e saídas e interfaces gráficas para criação visual dos fluxos.
Machado apresenta algumas diferenças entre workflows de negócios e workflows científicos discutidas por Meyer e Weske.
Esses autores confirmam algumas das diferenças já descritas e apresentam outras:
A Figura 8 apresenta uma tabela comparativa, distinguindo workflows de negócios dos científicos.
Com base em Mattoso et al.,
a tabela foi adaptada com foco em características da área de aplicação, apresentada no Capítulo 3, enfatizando características direcionadas a workflows científicos.
Este capítulo, além de apresentar a diferenciação entre workflows científicos dos de negócios, possibilitando o entendimento e a forma de aplicação para a realidade estudada, apresentada no capítulo 3, também apresentou conceitos de um dos formalismos utilizados para representação e modelagem de workflows, independente do tipo.
A utilização de formalismos para a representação de situações da realidade é frequente.
Aalst afirma que o uso de um conceito formal possui inúmeras vantagens, principalmente tratando- se de representação de processos:
Este capítulo também apresentou redes de Petri e uma de suas extensões, as redes de Petri Coloridas.
Sobre esses formalismos, Kotb e Baoumgart, em, afirmam que redes de Petri podem ser usadas para especificar o roteamento de instâncias de workflow e que uma rede de Petri pode ser usada, também, para modelar um problema, representando- o por meio de a estrutura de rede.
Gubala e Bubak, em, utilizam as redes de Petri coloridas para modelar workflows científicos e afirmam que existem algumas vantagens em utilizar- las.
Entre elas estão as seguintes:
Em esse contexto, o estudo de workflows e de um formalismo utilizado para sua representação é fundamental para o entendimento do padrão apresentado nesta Tese no Capítulo seu funcionamento.
Em o Capítulo 3, algumas características da área de aplicação, a Bioinformática, são apresentadas, bem como estudos realizados por o LABIO (Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas), norteadores deste trabalho.
Este capítulo apresenta a área de Bioinformática, algumas de suas características e desafios.
Também são apresentadas, neste capítulo, atividades desenvolvidas no LABIO (Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas) da Faculdade de Informática da Pontifícia Universidade Católica do Rio Grande do Sul (PUCRS), com ênfase em algumas linhas de estudo:
Flexibilidade de macromoléculas, definição de workflow científico para a automatização de processos e classificação dos dados.
A área com o qual o padrão apresentado nesta Tese é validado é a área de Bioinformática.
Para Mattoso et al.,
a Bioinformática é uma área de conhecimento convergente e multidisciplinar que envolve o intenso uso de ferramentas computacionais.
É uma área em a qual se busca resolver problemas do cotidiano, principalmente os que envolvam manipulação de grandes bancos de dados de genomas, sequências protéicas, entre outras.
Os autores também acreditam que essa área possui como objetivo final a coleta, a organização, o armazenamento, a recuperação e as análises de dados biológicos, propiciando a inferência ou descoberta de informações sobre a biologia e sobre a evolução dos organismos.
Weske et al.,
em, definem que os experimentos científicos, independentemente do domínio de aplicação, devem obedecer a um conjunto rígido de requisitos.
Esses requisitos, válidos até os dias atuais são:
Deve ser possível reproduzir e disseminar experimentos:
Os resultados científicos devem ser reproduzidos e seus resultados amplamente disseminados, de forma que diferentes cientistas possam comprovar- los.
Para que isso aconteça, os experimentos devem ser bem documentados.
Experimentos devem ser projetados por meio de a utilização de um protocolo, ou metodologia, permitindo que possam ser conduzidos a partir de um ponto inicial ou por meio de a combinação e reutilização de experimentos já realizados anteriormente.
Experimentos devem ter sua condução controlada, uma vez que cada experimento consiste num número de etapas com restrições complexas, podendo ser executadas em paralelo, necessitando de maior controle caso exista a necessidade de se refazer uma determinada etapa do processo.
Coutinho et al.,
em, afirmam que, por muitos anos, cientistas da área de Bioinformática têm manipulado um grande volume de dados e que seus estudos baseiam- se, principalmente, em experimentos por meio de simulação computacional, ou seja, experimentos in silico, e que demandam grande capacidade de processamento por parte de os computadores várias ferramentas computacionais, podendo ser utilizadas de forma encadeada, contemplando todas as etapas necessárias para a finalização dos experimentos.
Esse uso intenso de várias ferramentas computacionais, quando utilizadas de forma encadeada, dá origem aos workflows científicos, já apresentados no capítulo anterior.
Para Machado o avanço da biologia molecular e das ferramentas de simulação in silico, fez com que o planejamento de medicamentos fosse realizado de maneira mais lógica, sendo chamado de Desenho Racional de Fármacos.
A interação entre moléculas é o princípio fundamental do planejamento racional de fármacos.
Para tanto, tem- se receptores e ligantes.
Machado trata proteínas, macromoléculas e receptores como sinônimos e afirma que, assim como enzimas e DNA não são rígidas em seu ambiente celular.
Essa flexibilidade deve ser explicitamente considerada durante o processo de desenvolvimento de novos fármacos (drug design).
Além disso, afirma que uma das principais etapas desse processo de desenvolvimento de novos fármacos é a docagem molecular, em a qual se investiga e avalia o melhor encaixe do ligante na estrutura alvo ou receptor.
As ligações ocorrem em locais específicos:
Sítios de ligação, cavidades ou regiões na superfície das moléculas onde existe um ambiente favorável à interação ligante-receptor.
A partir de ligantes identificados como maior possibilidade de êxito.
Um dos grandes desafios está, justamente, em manipular essa grande quantidade de dados.
Bancos de dados de pequenas moléculas (ligantes), como o ZINC, têm de todos esses possíveis compostos com uma determinada proteína-alvo (receptor) e sua respectiva conformação, se torna inviável de ser executada, pois se estima que seriam necessários aproximadamente 62 trilhões de minutos até o término da execução de todos os experimentos, onde para cada experimento receptor-ligante, chamado de docagem molecular (docking), considera- se um tempo mínimo por execução de 1 minuto, o que nem sempre é verificado, pois o tempo tende a ser maior, dependendo do equipamento a ser utilizado.&amp;&amp;&amp;
Buscando melhorar esse tempo e otimizar o processo de docagem molecular, o LABIO, Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas, reúne pesquisadores que investigam e pesquisam novas soluções.
O Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas ­ LABIO ­ da sobre a enzima InhA do Mycobacterium tuberculosis (MTB).
Esse estudo deve- se por o crescimento mundial da tuberculose e, também, por o aumento dos casos de tuberculose resistentes à isoniazida, um dos principais fármacos utilizados no seu tratamento.
O objetivo do LABIO é contribuir para o desenvolvimento de novos fármacos para o tratamento da tuberculose, por meio de o aumento de qualidade da seleção de compostos candidatos.
Esforços da indústria farmacêutica, combinados com avanços na área de genômica estrutural, permitiram que um grande número de estruturas de proteínas e pequenas moléculas estejam disponíveis em repositórios.
Essas estruturas são de fundamental importância para o entendimento dos sistemas biológicos, assim como para o desenvolvimento mais eficiente de novos fármacos.
Flexibilidade de Macromoléculas Como as macromoléculas não são rígidas em seu ambiente celular, tona- se muito interessante que sua flexibilidade seja levada em consideração num processo de docagem molecular.
Um dos trabalhos desenvolvidos dentro de o LABIO, por Machado em, apresenta que experimentos de docagem molecular podem ser executados por diferentes softwares, como por exemplo:
O AutoDock3.
05, o FLEXX e o DOCK4.
0. A autora também afirma que a maioria desses softwares trata a flexibilidade do ligante, mas apresenta dificuldades em considerar a flexibilidade do receptor.
Os softwares que consideram a flexibilidade do receptor fazem isso somente de uma maneira muito limitada.
A autora, com o objetivo de contornar esse problema e incluir uma representação mais realista da flexibilidade natural dos receptores durante os experimentos de docagem, considerou um conjunto de snapshots do receptor gerados por uma simulação da dinâmica molecular onde, para cada possível conformação do receptor (snapshot), um experimento de docagem deve ser executado e analisado.
Uma conformação de um receptor é um conjunto de coordenadas X, Y, Z para cada átomo do mesmo, em Angstrons (Å), além de informações topológicas referentes a cada um de eles.
Para a enzima InhA que tem 4008 átomos, em cada snapshot tem- se 12.024 valores reais.
Assim, nos experimentos de docagem molecular, são testadas as diversas conformações do ligante com as diversas conformações do receptor.
De esta maneira, podem ser executados vários experimentos utilizando, em cada um, uma das conformações (snapshot) de determinado receptor e um mesmo ligante a se ligar a esse receptor.
Os testes iniciais apresentavam tempo aproximado de 5 minutos, considerando apenas uma conformação do receptor para um ligante.
Por exemplo, se uma simulação de 3 ns capturar a conformação do receptor a cada 0,5 ps, teria- se- 6.000 conformações para testar com um único par receptor-ligante.
A Figura 9 ilustra o processo de docagem molecular.
Flexibilidade do sistema InhA-NADH em diferentes momentos ao longo de uma simulação por dinâmica molecular.
Sobreposição de diferentes conformações da InhA (ciano, amarelo, magenta e verde) gerado por dinâmica molecular em.
Figura extraída de Até o momento, a identificação de ligantes promissores se fez somente por a energia livre liberada, maior a chance do ligante ser promissor.
Cabe ressaltar que experimentos com diferentes snapshots para o receptor podem resultar em FEBs bastante distintos e que o volume de dados resultante de cada experimento de docagem molecular torna tal tarefa humanamente impossível sem um ferramental computacional que assuma grande parte das análises.
Um Workflow Científico para o Processo de Desenvolvimento de Fármacos Buscando melhorar o suporte computacional para a realização desse tipo de experimento, Machado desenvolveu um workflow científico para automatizar o processo de execução de experimentos de docagem molecular, considerando a flexibilidade do receptor.
A Figura 10 e a Figura 11 ilustram a correspondente modelagem.
Com isso, os experimentos puderam ser executados de forma mais automática.
Antes, execuções desse tipo no LABIO eram manuais ou com o auxílio de scripts básicos, que necessitavam serem modificados a cada execução de diferentes experimentos receptor-ligante.
Extraído de Machado et al.
Reportam na execução de 3 estudos de caso utilizando as conformações do receptor InhA gerados no trabalho de Schroeder et al.
E os ligantes conformações do receptor InhA.
Como, em média, cada experimento de docagem molecular despendia de 5 a 10 minutos de execução, e como em cada estudo de caso foram executados em média, 350 horas (o tempo varia de acordo com a máquina onde a execução está sendo feita e o tamanho do receptor e do ligante).
A execução automática de todo esse processo já fez com que os experimentos pudessem ser executados sem a necessidade de intervenção humana.
Contudo, o LABIO demanda por simulações que cubram entre 10 ns e 100 ns, bem mais que os 3 ns da dinâmica molecular simulada e experimentada.
Também demanda por experimentos com mais ligantes, que apresentem alguma similaridade com os fármacos conhecidos.
Mantido o atual protocolo experimental de se ter uma conformação da InhA para cada picossegundo, 10.000 a 100.000 experimentos de docagem molecular seriam necessários para cada ligante a ser testado, o que demandaria muito tempo.
Busca- se, assim, diminuir o número de experimentos de docagem, procurando manter a qualidade dos resultados obtidos.
Classificação dos Dados A representação das conformações das moléculas tem sido feita por arquivos produzidos por simulações de dinâmica molecular, como o programa AMBER 6.
0. Tipicamente, esses arquivos podem ser dos formatos.
PDB (Protein Data Bank) ou uma combinação dos arquivos.
CRD (de coordenadas XYZ dos átomos) e.
Top (de topologia da molécula).
A Figura 12 mostra trechos desses arquivos.
A Figura 12 (a) mostra um trecho de um arquivo.
CRD, composto por um conjunto de coordenadas em sequência.
A Figura 12 (b) mostra o trecho inicial do arquivo.
Top correspondente ao.
CRD da Figura 12 (a), contendo a listagem com os nomes de cada átomo, de cada resíduo, o total de átomos da proteína e outras características.
A Figura 12 (c) mostra um arquivo.
PDB correspondente aos arquivos.
CRD e.
Top, onde os átomos encontram- se associados às suas coordenadas e informações adicionais.
O formato.
PDB é largamente empregado para armazenamento de conformações de moléculas em bancos de dados e na visualização de moléculas por computador.
Trecho do arquivo de topologia.
Top utilizado. (
c) Exemplo de arquivo.
PDB. Extraído de Winck et al.,
modelaram um banco de dados, chamado FReDD (Flexible Receptor Docking Database), para dar suporte completo aos experimentos de docagem molecular e, também, para mineração de dados.
Seu modelo final é composto por 16 tabelas, conforme mostrado na Figura 13.
Inicialmente, FReDD amazenou dados da InhA e de 4 ligantes:
NADH átomos do receptor, que correspondem aos 4,008 átomos de cada conformação (snapshot), multiplicado por 3.100 conformações.
Além disso, tem 3.248.330 registros de coordenadas dos átomos dos ligantes, resultantes dos experimentos de docagem.
A Tabela 1 sumariza essas populações.
É importante destacar que a coluna &quot;Total de Conformações», da Tabela 1, representa que são realizadas 10 execuções de experimentos para cada par ligante.
Em a sequência, Winck et al.
Realizaram um experimento de mineração de dados, usando como atributos preditivos as distâncias mínimas entre átomos do ligante e átomos dos resíduos de aminoácidos do receptor, em cada experimento de docagem, e o valor de FEB (free ser utilizado por Karina Machado para a determinação de uma função de similaridade, assunto de sua Tese de Doutorado, a qual pode ser aplicada ao padrão definido nesta Tese.[
DES95], TCL[ KUO03], PIF[ OLI04] e ETH[ BAN94], armazenados inicialmente no FreDD[ WIN09].
Receptores e Ligantes Número de átomos Total Número de docagens válidas Total de conformações Total de Coordenadas Mattoso et al.
Em afirmam que a ciência tem feito cada vez mais uso de procedimentos computacionais, buscando lidar com o aumento constante dos volumes de dados e manipulações necessárias aos experimentos científicos.
Também afirmam que, apesar de o são estudadas novas modalidades de experimentos científicos:
In virtuo e in silico Apud, fazendo com que os objetos de análise dos experimentos sejam usualmente processados por simulações computacionais, permitindo observar o mundo real por meio de simulações em ambientes virtuais.
Apesar disso, Mattoso et al.
Também afirmam que o cenário atual remete aos primórdios da computação, pois as pesquisas ainda dependem da capacidade individual dos cientistas para o encadeamento das etapas e programas necessários para a execução de experimentos, sendo sujeito a falhas e, muitas vezes, improdutivos, especialmente em se tratando de experimentos complexos, envolvendo muitos programas e grandes quantidades de dados.
Em função de isso, os sistemas de gerenciamento de workflows científicos passaram a ser utilizados.
A implementação de processos antes manuais minimizou o problema, mas não o resolveu por completo.
Os tempos envolvidos ainda são consideráveis, justificando novos estudos na busca de otimização.
Diferentemente do workflow científico desenvolvido por Karina Machado, em, que apresenta características puramente sequenciais, o trabalho desenvolvido nesta Tese tem o propósito de possibilitar a execução de experimentos em paralelo e reduzir a quantidade total de execuções de experimentos, utilizando- se dos conceitos de padrão de dados e de adaptação de instâncias em execução por meio de a otimização dos recursos computacionais.
O desenvolvido no Capítulo 5.
Russel et al.
Afirmam que existem vários conceitos utilizados na representação e manipulação de dados em sistemas de workflow.
Esses conceitos não apenas definem como os dados devem ser armazenados, mas sua aplicação em processos de negócio e a interação com outros sistemas e ambientes.
Os autores apresentam uma série de padrões de dados, os quais objetivam capturar as diferentes formas de representação e utilização dos dados sobre workflows.
Para os autores, uma vantagem significativa na abordagem baseada em padrões está em servir como base de comparação entre ferramentas diferentes.
Buscando aprimorar essa base, Russel et al.
Estenderam um estudo realizado por Jablonski e Bussler, os quais identificaram 40 padrões de dados e os analisaram sobre seis produtos de workflow de mercado da época.
Esses padrões são apresentados nas próximas seções.
Além de os padrões de dados, este capítulo também apresenta os padrões de controle de fluxo utilizados por Nardi em para a definição do padrão Junção Combinada.
A notação diagramática utilizada para representar os padrões de dados foi definida por Russel et al.,
em, enquanto a notação utilizada para a representação dos padrões de controle de fluxo é a da Rede de Petri Colorida, já detalhada no existência de algum padrão que pudesse ser capaz de atender à necessidade de áreas como a Bioinformática:
Manipular grandes volumes de dados, com características semelhantes, na menor quantidade de tempo possível.
Russel et al.
Classificam soluções conforme a perspectiva dos dados e suas características.
De entre essas perspectivas estão:
Visibilidade dos dados:
Dados podem ser vistos por os diversos componentes de um processo de workflow.
Interação dos dados:
Dados se comunicam entre atividades de um workflow.
Transferência de dados:
Considera a transferência entre componentes do workflow e descreve os diversos mecanismos por os quais os dados podem ser enviados de uma interface para um componente do workflow.
Roteamento baseado em dados:
De como os tipos de dados podem influenciar as operações e outros aspectos do workflow, principalmente quando se analisa fluxos de controle.
Visibilidade dos dados Para Russel et al.,
dentro de o contexto de workflow, existem formas distintas para a definição e utilização dos dados.
Tipicamente, dados individuais são utilizados numa estrutura específica de workflow, por exemplo, uma tarefa ou um bloco, e essas estruturas definem o escopo de acesso aos dados, especificando, dessa forma, como os dados podem ser utilizados:
Para buscar informações de produção, para gerenciar o controle aos dados ou a comunicação com o ambiente externo.
Assim, os autores identificam oito padrões que manipulam a visibilidade dos dados.
Os dados podem ser definidos por as tarefas que são acessíveis somente dentro de o contexto da execução individual das instâncias dessas tarefas.
O objetivo é tornar possível operações locais em nível de tarefa.
Geralmente, esses dados são utilizados para prover armazenamento de informações durante a execução da tarefa ou resultados intermediários para manipulação de dados produzidos.
A Figura 14 ilustra a declaração de um dado numa tarefa (variável X na tarefa B) e o escopo em o qual esse dado pode ser utilizado.
Há uma distinção na execução da tarefa B para cada instância em execução, podendo a variável X assumir diferentes valores.
Tarefas de blocos são tarefas que podem ser descritas como subprocessos.
São capazes de definir dados acessíveis por os componentes do subprocesso correspondente.
Dados definidos na tarefa do processo principal são utilizados no subprocesso, conforme ilustra a Figura 15.
Os dados podem ser definidos para serem acessíveis por um subconjunto de tarefas, conforme a aplicação.
Esse padrão é aplicado em casos onde muitas tarefas direcionam suas ações para dados ou conjuntos de dados comuns.
A Figura 16 exemplifica esse padrão.
Tarefas habilitadas a executarem múltiplas vezes sobre um processo podem possuir dados que sejam definidos sobre uma execução individual de uma instância sobre esse processo.
Existem três cenários sobre os quais uma tarefa poderia ser executada mais de uma vez, ilustrados na Figura 17: (a) onde uma tarefa é designada como sendo tarefa de múltiplas instâncias e, uma vez habilitada, múltiplas instâncias poderiam ser iniciadas simultaneamente; (
b) onde diferentes tarefas num workflow compartilham a mesma implementação; (
c) onde uma tarefa pode receber várias entradas durante a execução de um workflow.
Dados são definidos para uma instância específica ou para um caso de um workflow.
De essa forma, podem ser acessados por todos os componentes do workflow durante a execução desse caso e os dados são gerenciados como variáveis globais, durante a execução.
A Figura 18 representa a utilização de um dado sobre um caso.
Em esse padrão os dados podem der definidos como acessíveis por múltiplos casos numa base seletiva.
Por exemplo: Todas as instâncias que passam por uma determinada tarefa podem acessar o valor de um dado específico, possibilitando, dessa forma, o compartilhamento de dados entre instâncias de tarefas de diferentes casos de workflow.
A Figura 19 ilustra o funcionamento desse padrão.
Em esse padrão os dados são acessados por todos os componentes do workflow, em cada um dos seus casos, e são controlados por o sistema de workflow.
A Figura 20 ilustra esse padrão.
Em esse padrão os dados estão num ambiente externo, acessados por os componentes do workflow durante a execução.
A Figura 21 representa esse padrão.
Interação dos dados Para Russel et al.
Os padrões de interação capturam as diferentes possibilidades sobre as quais os dados podem ser executados entre os componentes de um workflow e como as características desses componentes podem influenciar na forma como esses dados são executados.
Um dos principais objetivos é distinguir a interação entre componentes do próprio workflow e de alguns recursos utilizados do ambiente externo.
Os autores identificaram 18 padrões de interação onde seis desses padrões envolvem apenas componentes internos ao workflow e os 12 demais envolvem interação entre os componentes internos e o ambiente externo.
É a habilidade de comunicação dos dados entre uma instância de tarefa e outra num mesmo caso de workflow.
É a possibilidade de se transmitir dados utilizados por diferentes tarefas quando requeridos.
Três possibilidades subsidiam esse padrão, ilustradas na Figura 22: (a) canais de dados e controle integrados:
Quando dados e fluxos são encaminhados simultaneamente entre tarefas utilizando o mesmo canal; (
b) canais de dados distintos:
Quando o dado é passado entre tarefas do workflow por um canal explícito, distinto do controle do processo; (
c) armazenamento global de dados:
Quando as tarefas compartilham os mesmos dados, por compartilhamento global de informações.
É a habilidade de passar dados de uma instância de um bloco de tarefas para seu subworkflow correspondente.
Esse padrão possui abordagens possíveis para implementação, ilustradas na Figura 23: (a) passagem de dados implícitos:
Os dados passados por o bloco são imediatamente acessíveis por todas as tarefas do subworkflow correspondente e não há a necessidade de passagem explícita de parâmetros; (
b) passagem de dados explícita via parâmetros:
Os dados devem ser especificados como parâmetros, sendo encaminhados para o subworkflow; (
c) passagem de dados explícita via canal de dados:
Os dados são passados especificamente via canal de dados para todas as tarefas do subworkflow que requisitam a informação.
É a habilidade de passar dados de um subworkflow para seu bloco de tarefas correspondente.
Esse padrão identifica o processo inverso abordado no padrão 10.
É a habilidade de passar dados de uma tarefa que suporte múltiplas instâncias para uma tarefa seguinte.
Cada execução de uma tarefa que suporte múltiplas instâncias efetivamente é executada de forma independente de outras instâncias e deve passar dados visando à conclusão das tarefas seguintes.
Assim, os dados gerados, como saída da execução de uma instância de tarefa, devem ser agregados às tarefas seguintes.
É a passagem de dados durante a execução de um caso para outro caso também em execução.
Alternativamente é possível acessar a mesma saída indiretamente por meio de o armazenamento dessas informações numa estrutura de armazenamento compartilhada.
Essa alternativa é representada na Figura 25.
A habilidade de uma tarefa iniciar a passagem de dados para um recurso ou serviço no ambiente operacional.
A Figura 26 ilustra os vários cenários de passagem de dados entre tarefas de workflow e o ambiente externo.
Existem duas categorias principais que subsidiam a implementação desse tipo de interação:
Mecanismo de integração explícito:
Onde o sistema de workflow provê construtores específicos para passagem de dados ao ambiente externo;
Mecanismo de integração implícito:
Onde a passagem dos dados ocorre implicitamente dentro de implementações que fazem chamadas nos processos do workflow e não são suportadas diretamente por o ambiente.
A habilidade de uma tarefa de workflow requisitar dados de recursos ou serviços de um ambiente operacional.
Pode envolver o acesso aos dados de um repositório, por exemplo.
A Figura 26 também ilustra esse padrão.
A habilidade de uma tarefa de workflow receber e utilizar dados passados de serviços e recursos do sistema operacional, sem um planejamento prévio.
A possibilidade das tarefas receberem novos itens de dados assim que estiverem disponíveis, sem a necessidade de requisição.
A Figura 26 também ilustra esse padrão.
A habilidade de uma tarefa de workflow receber e responder a requisições de dados para serviços e recursos no sistema operacional.
Essa habilidade pode ser manipulada de três formas:
O workflow provê formas de acessar dados de instâncias de tarefas do ambiente externo;
Durante a execução, as instâncias de tarefas publicam valores dos dados em locais conhecidos, por exemplo, uma base de dados;
As instâncias de tarefas incorporam facilidades aos serviços de requisição de dados aos processos externos.
A Figura 26 também ilustra esse padrão.
A habilidade de um caso de workflow iniciar a passagem de dados para um recurso ou serviço no sistema operacional.
Esse padrão é análogo ao padrão 15, exceto por a utilização de casos e não de uma única tarefa.
A Figura 27 ilustra esse padrão.
A habilidade de um caso de workflow requisitar dados de recursos ou serviços do sistema operacional.
A Figura 27 também ilustra esse padrão.
A habilidade de um caso de workflow aceitar dados passados de recursos ou serviços do sistema operacional.
Existem duas formas de implementação desse padrão:
Valores dos dados podem ser especificados na inicialização de um caso específico;
O workflow pode providenciar formas de habilitar atualização de dados durante a execução de um caso.
A Figura 27 também ilustra esse padrão.
A habilidade de um caso de workflow responder a requisições de dados passados para recursos ou serviços do sistema operacional.
A Figura 27 também ilustra esse padrão.
A habilidade de um workflow enviar dados para recursos ou serviços do sistema operacional.
Interessante quando as aplicações externas necessitam de informações como:
Casos de sucesso, recursos utilizados etc..
A Figura 28 ilustra esse padrão.
A habilidade de um workflow requisitar dados no nível de workflow de aplicações externas.
A Figura 28 também ilustra esse padrão.
A habilidade de serviços ou recursos, no ambiente operacional externo, enviar dados para um processo de workflow.
O objetivo desse padrão é suportar aplicações independentes de ferramenta de workflow, com a possibilidade de criar ou atualizar dados nessas ferramentas.
Para isso pode- se proceder das seguintes formas:
Os dados são passados para a ferramenta de workflow por linha de comando, por exemplo, no momento que a ferramenta é inicializada (assume- se assim, que a aplicação externa inicializou a ferramenta de workflow);
a aplicação externa inicializa a importação de dados por a ferramenta de workflow;
A aplicação externa utiliza Apis para acessar conjuntos de dados na ferramenta de workflow.
A Figura 28 também ilustra esse padrão.
A habilidade de uma ferramenta de workflow manipular requisições de dados para aplicações externas.
É importante destacar que nesse padrão a requisição é iniciada por a aplicação externa.
A implementação desse padrão pode ser realizada das seguintes formas:
Aplicações externas podem utilizar recursos disponibilizados por as ferramentas de workflow que exportem dados para arquivos, podendo ser acessados por as aplicações;
Aplicações externas podem utilizar Apis disponibilizadas por as próprias ferramentas de workflow que acessem os dados requisitados.
A Figura 28 também ilustra esse padrão.
Transferência dos dados Para Russel et al.
Os padrões de transferência de dados baseiam- se em como a troca de informações ocorre entre componentes do workflow.
Esses padrões apresentamse como uma extensão dos já apresentados na seção anterior e objetivam capturar as diversas formas por as quais os dados são manipulados por meio de as interfaces de componentes do workflow, envolvendo, por exemplo, fatores como a possibilidade de acesso exclusivo ou compartilhado a uma determinada informação.
Esses são os padrões 27 a 33.
A habilidade de um componente do workflow receber dados de entrada por valor de outro componente.
A Figura 29 ilustra esse padrão.
A habilidade de um componente do workflow passar dados por valor para outro componente.
A Figura 29 também ilustra esse padrão.
A habilidade de um componente do workflow copiar os valores de um conjunto de dados para dentro de um repositório de dados no início da execução e copiar o valor final desses dados após a execução ser completada.
A Figura 30 ilustra esse padrão.
A habilidade de comunicação de dados entre componentes de workflow por a utilização de um local de acesso compartilhado aos dados.
Não existem restrições de concorrência aplicadas aos dados.
A Figura 31 ilustra esse padrão.
A habilidade de comunicação de dados entre componentes de workflow por a utilização de um local de acesso compartilhado aos dados.
Restrições de concorrência são aplicadas por meio de privilégios de somente leitura ou acesso dedicado.
Essa abordagem estende o padrão 30, bloqueando os dados para leitura ou escrita.
A habilidade de aplicar uma função de transformação sobre um dado antes de ser encaminhado a um componente do workflow.
A Figura 32 ilustra esse padrão.
A habilidade de aplicar uma função de transformação sobre um dado imediatamente antes de sair de um componente do workflow.
A Figura 32 também ilustra esse padrão.
Roteamento baseado em dados Para Russel et al.
Os padrões de roteamento baseado em dados estudam as diversas formas por meio de as quais os dados podem interagir com outras perspectivas e influenciar a operação de um workflow como um todo.
Esses são os padrões 34 a 40.
Pré-condições com base em dados podem ser especificadas por tarefas que definem sua execução por a presença de dados em tempo de execução.
A Figura 33 ilustra esse padrão.
Pré-condições com base em dados podem ser especificadas por tarefas que definem sua execução por a presença de um determinado valor para o dado, especificado em tempo de execução.
Pós-condições com base em dados podem ser especificadas por tarefas que definem sua execução por a existência de parâmetros específicos em tempo de execução.
A Figura 34 ilustra esse padrão, onde a pós-condição de uma tarefa efetivamente estabelece um loop de controle implícito até que ela seja satisfeita.
Pós-condições com base em dados podem ser especificadas por tarefas que definem sua execução por valores de parâmetros específicos em tempo de execução.
A habilidade de disparar uma tarefa específica quando a avaliação de uma expressão de dados do workflow é verdadeira.
A Figura 36 ilustra esse padrão.
A habilidade de alterar o fluxo dentro de um caso de workflow após a análise de expressões baseadas em dados.
A Figura 37 ilustra esse padrão.
Esse padrão serve como uma agregação dos dois maiores padrões de controle de fluxo que dependem de dados:
Escolha exclusiva, onde o fluxo de controle é passado para uma das muitas tarefas seguintes dependendo da saída de uma decisão ou do valor de uma expressão;
Escolha múltipla, onde dependendo da saída de uma decisão ou do valor de uma expressão, o fluxo de controle é passado para várias tarefas seguintes.
Os padrões para Controle de Fluxo (Workflow Control-Flow Patterns ­ WCP) compõem um conjunto proposto em Aalst et al.
De vinte padrões encontrados na execução das etapas de um workflow.
Russell et al.
Identificaram diversas especializações desses padrões, completando um total de quarenta e três.
Algumas das categorias:
Padrões de Controle de Fluxo Básicos:
Essa classe de padrões captura os aspectos elementares de controle de fluxo.
Padrões Avançados para Ramificações e Sincronizações:
Para Nardi esses padrões detalham diversas possibilidades para execução de ramificações como discriminadores e junções parciais, incluindo tratamento de atividades pendentes.
Padrões para Múltiplas Instâncias:
Descrevem situações onde são executadas, sobre uma mesma atividade, diversas instâncias.
Conforme Nardi, esse é um caso específico de padrões para ramificação e sincronização.
Aqui, todas as atividades são instâncias de uma mesma atividade.
Padrões Baseados em Estado:
De acordo com Nardi &quot;quando a relação entre atividades a serem executadas dependerem de estado, o controle do fluxo deve prever situações de concorrência e ordem de execução».
Padrões para Cancelamento e Conclusão Forçada:
Muitos dos padrões citados anteriormente utilizam o conceito de cancelamento de atividade ou tratamento de exceções por meio de o conceito de cancelamento.
Esse conjunto de padrões trata o cancelamento isoladamente.
Conforme Nardi o Padrão de Conclusão Forçada permite que, a partir de informações externas, as atividades pendentes sejam concluídas.
Em o contexto de padrões de controle de fluxo, esta Tese tem como uma de suas bases o trabalho desenvolvido por Nardi.
Em função de isso, são apresentados os padrões de controle de fluxo estudados por o autor.
Nardi descreveu um padrão de divisão paralela e dez padrões de sincronização apresentados por Russel et al.
E definiu o Padrão Junção Combinada, como resultado da integração dos padrões estudados.
Conforme Nardi, dos quarenta e três padrões descritos por Russell et al.,
vinte e sete se relacionam a ramificação, sincronização ou múltiplas instâncias, podendo se aproveitar da execução em grades, utilizado no trabalho do autor.
Nardi se propôs a tratar o padrão «(WCP-2) Divisão Paralela», e os seguintes padrões de sincronização:
WCP-3 Sincronização;
WCP-9 Discriminador Estruturado;
WCP-28 Discriminador com Bloqueio;
WCP-29 Discriminador com Cancelamento;
WCP-30 Junção Parcial Estruturada;
WCP-31 Junção Parcial com Bloqueio;
WCP-32 Junção Parcial com Cancelamento;
WCP-34 Junção Parcial Estática para Múltiplas Instâncias;
WCP-35 Cancelamento de Junção Parcial de Múltiplas Instâncias;
WCP-36 Junção Parcial Dinâmica para Múltiplas Instâncias.
Os Discriminadores, conforme Nardi caracterizam- se por a produção da saída quando uma das atividades em paralelo tiver sido concluída.
O autor também apresenta o conceito de junções que, semelhantes aos discriminadores, são uma forma mais genérica.
Em as junções a saída é produzida quando uma determinada quantidade n tiver sido concluída, sendo que n é menor ou igual que a quantidade total de entradas possíveis, enquanto os discriminadores são junções parciais onde n $= 1.
O autor também apresenta que padrões de Múltiplas Instâncias são aqueles onde as atividades a serem executadas em paralelo são instâncias de uma mesma atividade, quando as instâncias puderem ser executadas independentemente umas das outras.
A notação diagramática utilizada para a representação dos padrões de controle de fluxo é a da Rede de Petri Colorida.
WCP-2 Divisão Paralela Aalst e Russel definem esse padrão como sendo a existência de uma divisão em dois ou mais ramos paralelos, cada um de eles executando concorrentemente.
Esses ramos podem ou não ser sincronizados, novamente, em algum ponto do futuro.
Para Nardi esse é o padrão que explicita a possibilidade de execução em grade, pois as saídas desse padrão são as entradas para os padrões de sincronização, descritos nas próximas seções.
A Figura 38 representa esse padrão.
WCP-3 Sincronização Aalst e Russel definem esse padrão como sendo a convergência de dois ou mais ramos (caminhos de execução paralela) num ramo único por onde passa o controle para as próximas ramificações e caminhos quando todos os demais ramos tiverem sido habilitados (E lógico).
Provê um mecanismo de convergência de duas ou mais ramificações paralelas.
Em geral, esses ramos foram criados a partir de divisões paralelas.
A Figura 39 representa o padrão Sincronização.
WCP-9 Discriminador Estruturado Conforme Nardi, assim como os demais padrões de sincronização, o Discriminador Estruturado trabalha juntamente com o padrão Divisão Paralela WCP-2.
Russel et al.
Afirmam que a convergência de duas ou mais entradas numa única saída é um exemplo desse padrão.
A Figura 40 apresenta esse discriminador.
Nardi* NAR09+ afirma que a aplicação que utiliza o padrão &quot;Discriminador Estruturado «é responsável por garantir que uma nova execução no mesmo processo não seja possível até que todas as tarefas tenham sido concluídas.
Caso essa característica não seja atendida, uma nova execução da tarefa que tenha sido concluída será possível, com consequências que o padrão não prevê.
WCP-28 Discriminador com Bloqueio O padrão Discriminador com Bloqueio contém elementos para não permitir que uma execução seja possível antes do término da execução anterior, independente da aplicação que o utiliza (diferente do padrão Discriminador Estruturado que depende da aplicação).
Trata- se de uma especialização do Padrão Discriminador Estruturado.
Aqui, o padrão independe da aplicação no controle de execuções sucessivas.
A Figura 41 representa esse padrão.
WCP-29 Discriminador com Cancelamento Para Russel et al.
Esse padrão provê o cancelamento de entradas que não sejam necessárias.
Por exemplo, em situações onde uma entrada seja requisitada, as demais são canceladas para que o processamento seja continuado.
Nardi afirma que ao produzir uma saída o Padrão Discriminador Estruturado aguarda que todas as demais atividades sejam concluídas para então poder ser novamente utilizado, mas quando essa espera for desnecessária, o padrão pode ser colocado em prontidão novamente ao sinalizar às demais atividades que seu resultado será descartado, ação realizada por o Discriminador com Cancelamento.
A Figura 42 apresenta esse padrão.
WCP-30 Junção Parcial Estruturada Nardi afirma que esse padrão é semelhante aos Discriminadores Estruturados, a única diferença é a transição responsável por a junção ser habilitada quando n atividades tiverem sido concluídas.
A transição que indica que a atividade foi finalizada permanece sendo habilitada quando as demais atividades forem concluídas.
Enquanto nos discriminadores eram atividades, nas junções parciais esse número é (m-n*).
O padrão é representado na Figura 43.
WCP-32 Junção Parcial com Cancelamento Russel et al.
Afirmam que este padrão prevê que um conjunto de entradas necessitam de sincronização numa junção, mas somente um subconjunto de elas necessita ser finalizado.
Para Nardi esse padrão é semelhante aos Discriminadores com Cancelamento, permite a reutilização do padrão após n tarefas terem sido concluídas, cancelando as demais, podendo ser analisado na Figura 45.
WCP-34 Junção Parcial Estática para Múltiplas Instâncias em Russel et al.
Encontra- se que com uma determinada instância do processo, múltiplas instâncias concorrentes de uma atividade podem ser criadas e uma vez que N instâncias dessa atividade forem finalizadas, a próxima atividade é inicializada.
Nardi afirma que, embora Russell et al.
Apresentem este como um novo padrão, trata- se de uma especialização do Padrão Junção Parcial Estruturada, em que as atividades a serem executadas possuem mesmo código, ou seja, são instâncias de uma mesma atividade, possivelmente com parâmetros distintos.
A Figura 46 ilustra esse padrão.
WCP-35 Junção Parcial de Múltiplas Instâncias com Cancelamento em Russel et al.
Encontra- se que com uma determinada instância do processo, múltiplas instâncias concorrentes de uma atividade podem ser criadas e uma vez que N instâncias dessa atividade forem finalizadas, a próxima atividade é inicializada e as instâncias restantes são canceladas.
Nardi afirma que, embora Russell et al.
Apresentem este como um novo padrão, trata- se de uma especialização do Padrão Junção Parcial com Cancelamento, em que as atividades a serem executadas possuem mesmo código, ou seja, são instâncias de uma mesma atividade, possivelmente com parâmetros distintos.
A Figura 47 ilustra este padrão.
WCP-36 Junção Parcial Dinâmica de Múltiplas Instâncias Com um processo instanciado, múltiplas instâncias concorrentes de uma atividade podem ser criadas.
Assim, o número de instâncias pode depender de diversos fatores de entre eles:
Dados de estado, disponibilidade de recursos, comunicação entre os processos e até mesmo não ser conhecida até que a última instância seja finalizada, uma vez que a execução de uma instância pode gerar a criação de uma nova.
Nardi afirma que esse padrão é uma extensão do padrão Junção Parcial Estática de Múltiplas Instâncias, sendo possível executar novas instâncias da atividade em questão, antes que o padrão tenha produzido uma saída.
A Figura 48 ilustra esse padrão.
Padrão Junção Combinada Nardi apresenta um novo padrão, denominado PJC ­ Padrão Junção Combinada.
A Figura 49 ilustra esse padrão.
Esse padrão é uma proposta para a representação e implementação conjunta dos padrões apresentados anteriormente.
Nardi apresenta como resultado do seu trabalho uma arquitetura de baixo acoplamento para execução de padrões de controle de fluxo, potencialmente com a utilização de grades.
O padrão desenvolvido por Nardi permite a execução de padrões de paralelização em clusters, grades ou equipamentos multi-processados, sendo uma base útil para a aplicação do padrão desenvolvido nesta Tese.
Além disso, o trabalho desenvolvido por o autor também oferece infraestrutura extensível para ser complementada com a implementação de outros padrões.
O padrão Junção Combinada, também desenvolvido para o contexto de workflows científicos, apresenta características de paralelismo, mas não tem como reduzir a quantidade de experimentos a serem processados, por não possuir um tipo de retorno nas execuções que permita a reconfiguração do experimento como um todo, fazendo com que a manipulação de grandes volumes de dados, da área de Bioinformática, ainda seja exaustiva.
A definição de padrões, que representem a manipulação e o fluxo de dados, possibilita a criação de sistemas que gerenciem workflows com base em estruturas semelhantes, facilitando a modelagem e aumentando a possibilidade de migração de uma plataforma para outra.
Além disso, conforme Nardi, essa definição faz com que cientistas de áreas diferentes da computação tenham mais tempo para suas pesquisas, ao invés de ocupar- los na construção da infraestrutura necessária para testar suas teorias ou conceitos.
Este capítulo apresentou os padrões de dados e alguns padrões de controle de fluxo.
Apesar disso, nenhum dos padrões estudados atende completamente à necessidade de áreas como a Bioinformática, em a qual um grande volume de informações é manipulado e encaminhado para processamento.
São informações, muitas vezes, semelhantes, podendo- se utilizar dessa semelhança para otimizar o processamento e que resultados preliminares podem auxiliar na definição e recalibragem dos próximos passos.
Definem- se, portanto, como características para um padrão aplicado a áreas como a Bioinformática:
Em esse contexto, a partir de a análise dos padrões de dados e padrões de fluxos apresentados neste capítulo, o P-MIA (Padrão Múltiplas Instâncias Autoadaptáveis), desenvolvido nesta Tese, atende às necessidades de áreas com as características já definidas.
A Tabela 2 apresenta os padrões de dados, identificados como P e seu respectivo número, por exemplo, P4 (refere- se ao padrão de dados de número 4), comparados ao P-MIA.
Assim, esse estudo serviu como subsídio para a definição do P-MIA a exemplo do padrão definido por Nardi em.
Diferencia- se o padrão apresentado nesta Tese do definido por Nardi por a utilização de padrões de dados e não padrões de controle de fluxo, pois o problema (de Bioinformática) estudado manipula grandes volumes de dados e, além disso, exige a adaptação do workflow em tempo de execução, a qual é realizada com base na análise de valores obtidos a partir de o processamento desses dados, na medida em que forem sendo produzidos.
É importante destacar que o padrão definido por Nardi pode ser utilizado como base para o processamento em paralelo do padrão desenvolvido nesta Tese.
O Capítulo 5 apresenta a formalização do P-MIA.
O funcionamento do padrão e os testes realizados são detalhados nos capítulos 6 e 7 respectivamente.
Este capítulo contém a formalização do Padrão Múltiplas Instâncias Autoadaptáveis, sua modelagem gráfica por meio de redes de Petri coloridas, os padrões de dados utilizados como base para sua definição e como é realizada a manipulação dos dados por o padrão.
Definição 1: (Snapshot e Conjunto de Snapshots) Seja S um conjunto não-vazio e finito de snapshots, produzidos por um simulador de dinâmica molecular para uma molécula de interesse (no caso, InhA).
Seja s S um snapshot e&amp; S o número de snapshots de S. Seja F:
S n, uma função de mapeamento que mapeia cada snapshot s num conjunto Ci.
Seja Ci S, 1 i n, n&amp; S, um subconjunto de snapshots que satisfaz um critério de agrupamento que mapeia cada snapshot num dos n conjuntos de snapshots.
Define- se, portanto:
S S, Ci, 1 i n| s Ci.
C1 Cn $= S. Ci.
Comentário: A Figura 50 esquematiza a aplicação da função de similaridade sobre os snapshots e a definição dos subconjuntos, a partir de aqui denominados de grupos.
Em a Figura 50 verifica- se a existência de um conjunto de snapshots, definido por S, a quantidade total desses snapshots é dada por&amp; S. A função de similaridade, responsável por a separação dos grupos gerou C1, C2, C3 e Cn.
Nota- se que a quantidade de snapshots de cada grupo não é fixa, e que cada snapshot é mapeado para apenas 1 agrupamento.
Definição 2: (Execução de Programa de Workflow Científico) Um programa de Workflow Científico é um programa de computador P que tem como entrada um conjunto de dados P. E e, como saída um conjunto de dados P. S. Dadas 2 execuções distintas de P, P1 e P2:
Comentário: Execuções de Programas de Workflows Científicos precisam ser repetíveis.
P. S também é chamado de resultado da execução.
Definição 3: (Lote de Execução de Programa de Workflow Científico) Um lote L de execuções de P, programa de workflow científico, é um conjunto de m execuções independentes de P, podendo ser disparadas em conjunto, onde:
L. M é o número de execuções de L em P Lix é um dos x lotes de execução de um grupo, onde:
Comentário: As execuções de P, para um lote L, usam dados diferentes e podem ser executadas em paralelo, caso haja recursos computacionais adequados.
A quantidade total de lotes é determinada em tempo de execução, seguindo critérios pré-definidos como a quantidade mínima de snapshots a serem processados e o percentual de snapshots (amostragem) a serem processados de um grupo.
Definição 4: (Lote Residual) Seja&amp; Li o número de snapshots de todos os lotes já definidos para Ci, 1 i n, n&amp; S. Seja&amp; Ci o número de snapshot de Ci.
Um lote Lri de execuções de P, programa de workflow científico, é um lote residual de um grupo Ci se:
Comentário: Lotes residuais são formados toda vez que for criado um lote a partir de um conjunto de snapshots, desde que restem snapshots que não façam parte dos lotes já criados.
Sobre esse lote residual aplicam- se regras de criação de novos lotes para o mesmo grupo, até que os critérios estabelecidos de quantidade mínima de snapshots e amostragem sejam atingidos.
Definição 5: (Quantidade Mínima de Snapshots e Amostragem) Seja Lij, 1 i n, n&amp; S, 1 j m, m&amp; Ci, um lote de execuções de P, correspondendo ao i-ésimo subconjunto de snapshots Ci.
Seja sk, 1 k n, n&amp; S, um snapshot individual.
Seja o número mínimo de execuções de P, para um lote Lij qualquer.
Seja a quantidade, em percentual, de snapshots de um Ci ou de um Lri, a serem envolvidos em execuções de P num lote Lij qualquer.
Portanto: Para cada Ci, criar um conjunto de lotes de execução Lij, tal que:
O Em cada execução de P, P. E $= sk Ci o&amp; Lij o&amp; Lij Comentário:
Os valores para quantidade mínima e amostragem são determinados antes do início da subdivisão dos grupos em lotes.
Esses valores são utilizados para definir quantos snapshots compõem cada um dos lotes de um grupo.
Definição 6: (Status de Snapshot e Transição Válida de Estados de Snapshots) Portanto:
Se (s.
St $= A) s.
St $= F Se (s.
St $= A) s.
St $= D Se (s.
St $= A) s.
St $= P Se (s.
St $= P) s.
St $= A Comentário:
Atributos de status dos snapshots, cujos valores sejam iguais a D ou F, significam que os snapshots não serão mais executados por o Programa de Workflow Científico.
A transição entre possíveis valores do atributo status é encontrada na Figura 51, contendo o diagrama de transição de estados.
Definição 7: (Arquivo ou Tabela de Acompanhamento do Processamento dos Snapshots) Seja At um arquivo ou tabela que contenha todas as informações dos snapshots.
Seja&amp; s o número que corresponda à identificação de um s.
Portanto: Comentário:
A definição de utilização de arquivo ou tabela para o armazenamento das informações depende da forma de implementação do modelo.
Entretanto, as informações são fundamentais para o funcionamento e análise dos resultados.
Definição 8: (Melhor Valor e Pior Valor) Seja P. S. I, 1 i n, n m, onde m é uma amostra de C, o resultado do processamento de cada snapshot num sistema de workflow científico.
Seja MP.
S o maior valor entre todos os P. S gerados por a execução dos snapshots da amostra.
Seja Pp..
S o pior valor entre todos os P. S gerados por a execução dos snapshots da amostra.
Portanto: MP.
S P. S. I| P. S. P. S;
MP. S P. S. Pp..
S P. S. I| P. S. P. S;
Pp.. S P. S. Definição 9: (Análise Horizontal e Vertical, e Análise dos Resultados) Seja Ah, análise horizontal, a comparação dos diferentes resultados (P. S) dos lotes de um grupo.
Seja AV, análise vertical, a comparação dos diferentes resultados (P. S) dos diferentes grupos de S (Ci S, 1 i n, n m).
Seja N o número de snapshots(&amp; s) já processados de um lote (Lij).
Seja P. S o resultado do processamento de um snapshot, após a submissão num programa de workflow científico.
Seja P. S. Lij, 1 i n, n&amp; S, 1 j m, m&amp; Ci, o somatório dos resultados dos snapshots de Lij após serem submetidos à execução num programa de workflow científico.
Seja&amp; Lij a quantidade de snapshots de um lote (Lij).
Seja MP.
S o melhor valor utilizado como parâmetro para análise dos resultados, gerado por amostragem.
Seja Pp..
S o pior valor utilizado como parâmetro para análise dos resultados, gerado por amostragem.
Seja x a média aritmética dos resultados (P. S) dos snapshots já processados.
Portanto: X $= P. S. Li/ N Seja xMP a média aritmética dos resultados (P. S), entre MP.
S e Pp..
S. Portanto: Seja xE a média aritmética estimada dos resultados já obtidos com o processamento dos snapshots do lote, no programa de workflow científico (P. S), e com os prováveis resultados dos demais snapshots do lote, cuja quantidade é chamada de f (N --&amp; Lij $= f).
Para obter xE utiliza- se o superestimando os valores de forma que se obtenha um resultado próximo de o satisfatório.
Portanto: Xe, xE68| xE68 $ ) xE, xE95| xE95 $ ) xE, xE99, 7| xE99, 7 $ ) xE $ /&amp; Lij A definição de status dos snapshots, a partir de a análise dos resultados é feita da seguinte forma:
P. s| (P. S\&gt; x) (P. S\&gt; xE) St $= A P. S| (P. S\&gt; x) (P. s xE)) St $= P P. S|( (P. S x) (P. S xE)) St $= D Comentário:
A interseção de snapshots que pertencem a xE68 e xE95 e a xE95 e xE99, 7, deve ser eliminada para o cálculo da expressão;
A análise dos resultados fará com que se tenha alteração no status (St) dos snapshots (s);
a regra empírica adaptada é apresentada no Definição 10: (Arquivo ou Tabela com Resultados do Processamento dos Lotes) Seja Atp um arquivo ou tabela que contenha todas as informações dos processamentos dos lotes.
Seja Ci, o número que corresponda à identificação de um C. Seja xi a média aritmética de cada grupo, considerando os snapshots processados até o momento da análise.
Seja xEi a média aritmética estimada de cada grupo, considerando os snapshots processados até o momento da análise.
Seja Sti o status de cada grupo, considerando os snapshots processados até o momento da análise.
Portanto: Comentário:
A definição de utilização de arquivo ou tabela para o armazenamento das informações depende da forma de implementação do modelo.
Entretanto, as informações são fundamentais para o funcionamento e análise dos resultados.
Definição 11: (P-MIA:
Padrão Múltiplas Instâncias Autoadaptáveis) Formalmente, o P-MIA é uma tupla P_ MIA $= L, s, P. S, MP.
S, Pp..
S\&gt;, onde:
S é o snapshot contido num lote de um grupo sL| LC um dos snapshots;
MP. S é o melhor valor de entre todos os valores processados de uma amostra de snapshots:
Pp.. S é o pior valor de entre todos os valores processados de uma amostra de snapshots:
Comentário: A tupla é composta por os principais componentes do padrão.
Características importantes como as funções utilizadas para definição de prioridade não são representadas na tupla explicitamente, pois fazem parte do funcionamento.
Com o objetivo de simular o funcionamento do modelo e de se ter uma representação gráfica, o P-MIA foi modelado por meio de redes de Petri Coloridas, utilizando para isso a ferramenta CPN Tools, com a qual é possível editar, analisar e simular Redes de Petri coloridas.
Com essa ferramenta pode- se fazer várias implementações de sistemas, desde uma rede até um sistema completo.
Um modelo inicial, representando o P-MIA em alto nível pode ser visualizado na Figura 52.
Em essa figura, são representados, apenas, os grupos, a separação desses grupos em lotes e a execução dos snapshots, sem a preocupação com as análises intermediárias do modelo.
Em a Figura 52, existe apenas um conjunto de dados 1`.
Em esse conjunto de dados o primeiro valor está representando a quantidade de grupos, ou seja, 1;
o segundo valor está representando a quantidade de lotes, ou seja, 2;
o terceiro e quarto valores são os lotes em si, cujos valores 3 e 4 referem- se à quantidade de snapshots de cada um dos dois lotes.
CPN Tools Com o objetivo de melhor representar o funcionamento do P-MIA, com as análises intermediárias, a Figura 53 apresenta a modelagem em redes de Petri com maior nível de detalhamento, sem considerar dados externos como arquivos ou tabelas em bancos de dados.
Em essa representação, são feitas as análises sobre os resultados obtidos, identificando as diferentes possibilidades:
Altera Prioridade, Finaliza Lote, Descarta Lote ou Mantém Prioridade.
A transição denominada &quot;Armazena resultado ­ gera média», da Figura 53 é a responsável por fazer uma das comunicações com o ambiente externo, buscando informações de resultados já armazenados.
A partir de o resultado (saída) obtido com esse processamento, as demais transições serão executadas.
Em a Figura 53 se tem o conjunto de dados 1`, correspondendo a 1 grupo, 1 lote e 5 snapshots dentro desse lote.
A modelagem apresentada nessa figura também foi feita com a ferramenta CPN Tools.
O P-MIA utiliza, como base para sua elaboração, 10 padrões de dados definidos por Russel et al.
Em. Cada padrão, bem como sua utilização, é detalhado nas próximas seções.
Padrão 4.
Dados de Múltiplas Instâncias Considerando que uma das características desse padrão, já apresentada anteriormente, é que uma tarefa seria designada como sendo tarefa de múltiplas instâncias quando, uma vez habilitada, múltiplas instâncias poderiam ser iniciadas simultaneamente, o P-MIA utiliza esse padrão, uma vez que considera a possibilidade de execuções de suas tarefas em paralelo.
Assim, várias tarefas estariam em execução, com diferentes instâncias, ao mesmo tempo.
Padrão 8.
Dados de Ambiente Esse padrão considera que dados estejam num ambiente externo, acessados por os componentes do workflow durante a execução.
O P-MIA manipula a característica desse padrão quando considera o acesso a arquivos armazenados em estruturas de diretórios no sistema operacional ou a tabelas organizadas em bancos de dados.
A principal utilização desse padrão está no armazenamento dos resultados obtidos por os snapshots para posterior análise das médias geradas, com o objetivo de definir as próximas etapas de execução dos snapshots e a alteração ou não de seus status.
Padrão 14.
Interação de Dados ­ Casos para Casos Esse padrão é utilizado por o P-MIA ao considerar a possibilidade de acesso, por diferentes casos de workflow (instâncias em execução), de uma mesma informação por meio de uma estrutura de armazenamento compartilhada.
De essa forma, diferentes casos em execução do PMIA podem estar utilizando dados armazenados externamente ao programa de execução de workflow científico para realizarem suas análises.
Padrão 15.
Interação de Dados ­ Tarefas para Ambiente Externo ­ Push-Oriented Esse padrão prevê a habilidade de uma tarefa iniciar a passagem de dados para um recurso ou serviço no ambiente operacional.
Conforme definido por o padrão existem duas categorias principais que subsidiam a implementação desse tipo de interação:
Mecanismo de integração explícito:
Onde o sistema de workflow provê construtores específicos para passagem de dados ao ambiente externo;
Mecanismo de integração implícito:
Onde a passagem dos dados ocorre implicitamente dentro de implementações que fazem chamadas nos processos do workflow e não são suportadas diretamente por o ambiente.
O P-MIA utiliza- se do mecanismo de integração explícito, pois se sabe exatamente quais são os momentos em que é necessária a passagem de dados para o ambiente externo.
Esses momentos são:
Obtenção do resultado do processamento de cada snapshot;
Alteração dos status dos snapshots e dos lotes de processamento.
Padrão 16.
Interação de Dados ­ Ambiente Externo para Tarefas ­ Pull-Oriented Em esse padrão, uma tarefa de workflow requisita dados de recursos ou serviços de um ambiente operacional.
Pode envolver o acesso aos dados de um repositório, por exemplo, e é principalmente essa a característica utilizada por o P-MIA, quando dados armazenados externamente (resultados dos processamentos dos snapshots) necessitam ser acessados para que se realizem as médias e se prossiga com as análises.
Padrão 27.
Transferência de Dados por Valor ­ Entrada A habilidade de um componente do workflow receber dados de entrada por valor de outro componente é outra característica do P-MIA.
Quando uma tarefa obtém um resultado de saída e esse resultado deve ser encaminhado como entrada para outra tarefa, esse padrão é utilizado.
Padrão 28.
Transferência de Dados por Valor ­ Saída A habilidade de um componente do workflow passar dados por valor para outro componente é outra característica do P-MIA, pois segue o mesmo princípio da transferência de dados por valor ­ entrada.
Quando uma tarefa obtém um resultado de saída e esse resultado deve ser encaminhado como entrada para outra tarefa, esse padrão é utilizado.
Padrão 35.
Pré-condição para Tarefa ­ Valor de Dados Pré-condições com base em dados podem ser especificadas por tarefas que definem sua execução por a presença de um determinado valor para o dado, especificado em tempo de execução.
Esse padrão também é muito utilizado por o P-MIA, pois a alteração dos status dos snapshots e a execução das diferentes ações:
Alteração de prioridades, descarte dos snapshots etc..
Padrão 37.
Pós-condição para Tarefa ­ Valor de Dados Pós-condições com base em dados podem ser especificadas por tarefas que definem sua execução por valores de parâmetros específicos em tempo de execução.
De a mesma forma que o padrão 35, esse padrão também é utilizado por o P-MIA.
Padrão 40.
Roteamento Baseado em Dados A habilidade de alterar o fluxo dentro de um caso de workflow após a análise de expressões baseadas em dados.
Esse padrão serve como uma agregação dos dois maiores padrões de controle de fluxo que dependem de dados:
Escolha exclusiva, onde o fluxo de controle é passado para uma das muitas tarefas seguintes, dependendo da saída de uma decisão ou do valor de uma expressão;
Escolha múltipla, onde dependendo da saída de uma decisão ou do valor de uma expressão, o fluxo de controle é passado para várias tarefas seguintes.
O P-MIA utiliza- se principalmente da escolha exclusiva, pois a análise dos resultados obtidos pode direcionar o fluxo a diferentes caminhos.
Características Específicas do P-MIA Além de os padrões de dados definidos por Russel et al.,
o P-MIA também possui outras características.
São elas:
Essa característica é utilizada toda vez que for necessário o processamento de um grande volume de dados, agrupados conforme um critério de similaridade qualquer.
Possui duas possibilidades de funcionamento:
Todo o grupo é submetido ao processamento;
É realizada a divisão do grupo, em partes menores, antes da submissão ao processamento.
A Figura 54 apresenta a manipulação de grupos completos de dados e a Figura 55 apresenta a manipulação de lotes menores criados a partir de os grupos.
Tanto a representação da Figura 54, quanto a a da Figura 55 foram modeladas com redes de Petri coloridas, utilizando a ferramenta CPN Tools.
Obtidos Essa característica, muito semelhante em sua essência ao padrão 40, definido por Russel et al.,
diferencia- se, principalmente, em sua aplicação.
Os testes, para a definição por a instanciação de um processo ou não, são realizados antes de qualquer execução de tarefa do workflow.
Portanto, pode- se definir que as tarefas do workflow estariam encapsuladas e que sua execução seria determinada por uma ação externa.
A Figura 56 esquematiza o funcionamento dessa característica, modelado por meio de redes de Petri coloridas.
Essa característica, também é muito semelhante ao padrão 40, definido por Russel et al.
A diferença está na aplicação direcionada, pois a determinação da continuidade de um processo está na determinação de continuidade de execução dos dados de um determinado lote.
A análise dos resultados e dos status dos dados de um lote é fundamental para se inferir sobre a continuidade de um processo.
Sua representação é a mesma de escolha exclusiva.
Essa característica utiliza- se da manipulação de informações que estão no meio externo.
Seu ponto principal está na alteração ou não de atributos dos dados envolvidos no processo.
Não existe, nesse caso, exclusão ou descarte de dados a serem processados.
O que se pode ter é um atraso na execução.
Os dados a serem alterados são definidos em tempo de execução, com base no processamento de resultados já obtidos.
Este capítulo apresentou a formalização do padrão desenvolvido nesta Tese:
P-MIA: Padrão Múltiplas Instâncias Autoadaptáveis.
Para a definição do padrão, utilizou- se como base termos da formalização tradicional de redes de Petri, bem como a representação gráfica do P-MIA em redes de Petri coloridas, desenvolvida por meio de a ferramenta CPN-Tools.
A vantagem em se formalizar o padrão está na validação do modelo, identificando seus principais componentes e na utilização de uma linguagem com grande poder de expressão.
Em a busca por facilitar a implementação do modelo, o Capítulo 6 oferece um detalhamento do funcionamento do padrão, exemplificando e apresentando as funções definidas, em especial, para a análise dos resultados.
Este capítulo também apresentou os padrões de dados, dos diferentes apresentados no específicas do P-MIA que não são encontradas em outros padrões já definidos na literatura.
Em o Capítulo 3 foi apresentada a área de aplicação, em a qual foram realizados os experimentos, apresentados no Capítulo 7, sobre o padrão desenvolvido nesta Tese.
Em o Capítulo 3 também foi apresentada a atuação do LABIO e destaca- se, neste momento, o trabalho desenvolvido por Karina Machado em, de um workflow científico para a modelagem do processo de desenvolvimento de fármacos assistido por computador, utilizando receptor flexível.
O funcionamento completo do workflow científico, desenvolvido por Karina Machado pode ser encontrado em.
O fluxo pode ser visualizado na Figura 57.
O workflow científico desenvolvido por Machado é apresentado em maiores detalhes nesta Tese por ser motivador de seu desenvolvimento.
Considera- se o desenvolvimento do workflow científico como um ganho considerável de processamento, pois automatizou etapas antes manuais, envolvendo a execução de diferentes sistemas.
Apesar disso, existem algumas etapas, no workflow desenvolvido por Machado, que demandam maior atenção.
Conforme a própria autora:
O trabalho desenvolvido nesta Tese busca contribuir com a redução da quantidade de experimentos a serem executados, via a definição de um padrão capaz de substituir as etapas que envolvem o processamento de dados seletivos no workflow desenvolvido por Karina Machado, de forma que não exista a necessidade de execuções exaustivas, tornando a seleção de snapshots uma etapa dinâmica.
A substituição de algumas etapas do workflow desenvolvido por Karina por o P-MIA é apresentada na Figura 58.
A Figura 58 contém o mapeamento de um workflow científico, com base no workflow de, modelado utilizando- se a notação denominada BPMN (Business Process Modeling Notation).
BPMN é uma notação visual para representação de fluxos de processos que pode ser mapeada para diversos formatos de execução.
Proporciona às ferramentas o uso de uma representação gráfica padronizada, permitindo a divulgação dos processos de maneira uniforme, facilitando o entendimento entre profissionais e cientistas.
Em este momento, por apresentar uma notação mais próxima da utilizada por Karina Machado e por representar claramente os pontos de decisão, foi a escolhida.
A ferramenta utilizada para o mapeamento é denominada Business Process Visual Architect Modeler Edition1.
Em a Figura 58 a etapa Executa P-MIA substituiu as seguintes etapas da Figura 57: Dados Seletivo, Selec Snaps, Seta Contador e Atualiza Contador (quando utilizada para dados seletivos).
O funcionamento do P-MIA é apresentado em maiores detalhes nas próximas seções deste capítulo.
O padrão apresentado nesta Tese, apesar de substituir etapas do workflow científico desenvolvido para a área de Bioinformática, pesquisada por o LABIO, busca atender a outras áreas, desde que possuam as seguintes características:
Para que o padrão obtenha os resultados esperados é fundamental a utilização de uma função de similaridade para a definição dos grupos de snapshots, que consiga agrupar- los considerando suas características, de forma que snapshots de um mesmo grupo tenham as mesmas chances de obterem resultados bons ou ruins.
Este capítulo, portanto, apresenta o funcionamento do Padrão Múltiplas Instâncias Autoadaptáveis (P-MIA), podendo ser implementado em aplicações de diferentes áreas, desde que possuam as características já definidas.
O funcionamento do padrão é apresentado considerando sua utilização na área de Bioinformática, com o objetivo de direcionar o entendimento.
Para que se entenda o comportamento do P-MIA, é importante o detalhamento de algumas características utilizadas por o padrão.
A etapa preliminar de submissão dos dados ao Padrão Múltiplas Instâncias Autoadaptáveis prevê a separação de todos os snapshots envolvidos no processo, em grupos, por uma função de similaridade.
A Figura 59 esquematiza a aplicação da função de similaridade sobre os snapshots e a definição dos grupos.
É importante destacar que essa função de similaridade não faz parte do escopo deste trabalho, sendo apenas utilizada para a separação dos grupos, definidos como dados de entrada para o funcionamento do padrão proposto.
Uma função de similaridade, direcionada à realidade da área de Bioinformática, está sendo desenvolvida por Karina Machado em sua Tese de Doutorado.
Em a Figura 59 verifica- se a existência de um conjunto de snapshots, à esquerda da figura.
A função de similaridade, responsável por a separação dos grupos é representada por a seta e, à direita da Figura 59, está representada a geração de quatro grupos, com quantidades diferentes de snapshots.
Com a definição desses grupos, essas informações devem ser organizadas, podendo ser armazenadas sob a forma de um arquivo ou tabela, desde que contenha as seguintes informações:
Snapshot: Identifica o número do snapshot ao qual as demais informações se referem;
Grupo: Identifica o grupo ao qual o snapshot pertence;
Lote: Identifica o lote ao qual o snapshot pertence (melhor detalhado nas próximas seções);
Status: Identifica a situação sobre o processamento do snapshot, podendo conter um dos seguintes valores:
A ­ Ativo e aguardando por o processamento F ­ Processamento Finalizado D ­ Descartado por resultado insatisfatório dos demais snapshots do grupo;
P ­ Prioridade do grupo ao qual o snapshot pertence foi alterada.
A informação referente a o status de cada snapshot é fundamental para a determinação de sua execução ou não.
Considera- se, portanto, que apenas snapshots ativos são processados.
As informações contidas no arquivo criado são utilizadas por o padrão quando de a análise dos resultados e da definição de prioridades e continuidade de processamento.
A Tabela 3 exemplifica a estrutura do arquivo/ tabela criado com as informações definidas anteriormente, após alguma execução.
Em essa tabela se tem três snapshots, cada um de um grupo e de lotes diferentes.
O snapshot de número 320 foi finalizado;
O snapshot de número 1.457 foi descartado;
E o snapshot de número 522 está ativo, aguardando a execução.
O padrão também trabalha com os valores de variáveis, podendo ser definidos por o usuário, antes do início de sua execução, e são representadas na Figura 64 por o identificador &quot;Referência».
Essas variáveis são:
Quantidade mínima de snapshots a serem processados:
O usuário fornece um valor numérico, correspondendo à quantidade mínima de snapshots que devem ser processados para a formação de cada lote de um grupo.
Caso esse valor não seja fornecido, considera- se 50 como quantidade mínima para criação dos lotes, por ser possível criar grupos cujos resultados possam ser interpretados por o padrão.
Amostragem, em percentual, da quantidade de snapshots que formarão cada lote:
O usuário fornece um valor numérico, correspondendo ao percentual do total (ou residual) de snapshots do grupo que formará cada lote.
Caso esse valor não seja fornecido, considera- se 30% como percentual a ser utilizado para a criação dos lotes por ser possível criar grupos cujos resultados possam ser interpretados por o padrão.
Os valores definidos previamente para as variáveis quantidade mínima e amostragem são utilizadas nos testes apresentados no Capítulo 7, comprovando sua viabilidade.
O P-MIA prevê, ainda, a separação dos grupos definidos, por meio de a função de similaridade, em lotes menores e a análise dos resultados obtidos, definindo a alteração dos status dos snapshots.
Essas características também são justificadas no Capítulo 7.
Os snapshots que estão separados em diferentes grupos, cuja separação foi realizada por a aplicação de uma função de similaridade apropriada, devem ser subdivididos em lotes.
Essa é uma característica importante do P-MIA.
Esses lotes são utilizados como escopo para análises preliminares e intermediárias dos resultados obtidos.
É importante destacar que se pode ter uma quantidade previamente desconhecida de lotes, pois essa quantidade é definida em tempo de execução, tendo como base os valores de referência definidos inicialmente e que correspondem à quantidade mínima e à amostragem.
A adoção por lotes menores tem por base os testes encontrados no Capítulo 7, que concluiu que a análise de quantidades menores de dados fornece melhores resultados que a análise de um grupo como um todo.
Além disso, leva- se em conta a amostragem e a quantidade mínima para que os lotes não sejam todos de um mesmo tamanho, conferindo maior flexibilidade ao modelo.
Quando de a criação dos lotes deve- se ter atenção em relação a o número de snapshots que compõe cada um, considerando as seguintes etapas:
Em a Figura 60, por exemplo, pode- se observar a separação dos grupos de snapshots, C1, C2 e C3 apresentados na Figura 59, em lotes.
Considera- se, para a interpretação da figura, que o percentual de amostragem definido por o usuário seja 50% e que a quantidade mínima de snapshots a serem processadas por lote seja igual a 2.
Em a Figura 60, o grupo 1 possui três snapshots.
O primeiro lote, nesse caso, será composto por a quantidade total de snapshots do grupo, pois 50% sobre 3 é igual a 1,5, que é menor que a quantidade mínima definida.
Se for considerado um lote com 2 snapshots, o lote residual ficaria com apenas 1 snapshot, também menor que a quantidade mínima.
Para a criação dos lotes, em momento de execução, segue- se o algoritmo recursivo da Figura 61.
O algoritmo da Figura 61 deve ser executado para cada um dos grupos, gerados a partir de uma função de similaridade, e é importante destacar que a amostragem e a quantidade mínima de snapshots a serem executados permanecem constantes, não sendo alterados após a definição preliminar.
Teste: Representa cada teste de mesa realizado;
Execução: Representa cada lote criado;
Amostragem: Percentual utilizado para a criação dos diferentes lotes de um grupo;
Quantidade_ mínima:
Quantidade mínima de snapshots utilizados para a criação dos diferentes lotes de um grupo;
Total_ snapshots_ grupo:
Quantidade total de snapshots que compõem o grupo;
Os testes de mesa, da Figura 62, realizados a partir de o algoritmo da Figura 61, podem ser detalhados da seguinte forma:
Em o primeiro teste de mesa a quantidade total de snapshots do grupo é dividida em três lotes para execução.
O primeiro lote possui a quantidade de snapshots definido por o percentual de amostragem, pois 40% de 100 é igual a 40, maior que a quantidade mínima definida.
Em o segundo lote, a quantidade de snapshots é igual à quantidade mínima, pois 40% de 70 snapshots (lote residual) é igual a 28, menor que o mínimo definido, e esse é o mesmo princípio para a definição da quantidade do terceiro lote.
Em o segundo teste a quantidade total de snapshots do grupo é dividida em dois lotes.
A quantidade de snapshots do primeiro lote é igual ao percentual da amostragem, pois é maior que a quantidade mínima, restando um lote residual com a mesma quantidade exigida como mínima, sendo o formador do segundo lote.
Em o terceiro teste a quantidade total de snapshots do grupo é dividida, também, em dois lotes.
O primeiro lote possui a quantidade mínima exigida para processamento, que é a mesma definida por o percentual.
O segundo lote, entretanto, possui valor maior que o definido por o percentual e maior que a quantidade mínima.
Isso acontece, pois o cálculo de 40% dos 60 snapshots resulta em 24 snapshots, menor que a quantidade mínima.
Assim, esse lote deveria ser formado por a quantidade mínima, ou seja, 40 snapshots, restando apenas 20 para o lote residual.
Em esse caso, como a quantidade restante é menor que a mínima definida, todos os demais snapshots fazem parte do mesmo lote.
As etapas executadas por o algoritmo para a obtenção dos resultados dos testes de mesa da Figura 62 são detalhados na Figura 63, com os indicadores sendo formados por a sequência de números obtidas por meio de a concatenação dos valores de Teste e Execução, da Figura 61, formando Teste.
Execução. Em a Figura 64, (a), (b) e (c) são snapshots de um mesmo lote.
O snapshot (a) é submetido à execução no programa de workflow científico.
Como resultado dessa execução obtém- se um valor de saída, chamado de &quot;Resultado Execução».
Esse valor, numérico, é armazenado num espaço comum de memória, assim chamado por ser externo ao programa de workflow.
Esse espaço comum de memória pode ser um arquivo, armazenado em estrutura de diretórios do Sistema Operacional, ou uma tabela num Banco de Dados.
Para a análise desse valor, duas outras informações são necessárias e devem ser definidas por o usuário, ou geradas por o sistema por meio de a execução de snapshots por amostragem, também representados na Figura 64 por o identificador &quot;Referência», são elas:
Melhor valor e Pior valor.
Essas informações formam um intervalo que é utilizado para a definição de prioridades de cada um dos snapshots.
É importante destacar que, para a aplicação estudada, considera- se como resultado do processamento de cada um dos snapshots o FEB, cujo resultado quanto mais negativo melhor.
Devido a isso, o melhor valor é o início do intervalo e o pior valor o final do intervalo.
Como exemplo, pode- se considerar o intervalo, considerando 24 como melhor valor e 5 como pior valor.
Os resultados que se aproximarem ou forem menores que o melhor valor serão, portanto, os snapshots com maior probabilidade de sucesso.
Busca- se processar a maior quantidade possível de snapshots, cujos resultados sejam próximos do melhor valor fornecido por o usuário.
Em a Figura 60 encontram- se informações referentes a resultados após a análise dos snapshots:
R1 minute, R1 second, R1 second', R2 minute, R2 second, R3 minute, R4 minute, R4 second que correspondem ao resultado final individual do processamento de cada snapshot;
R11, R21, R31, R41 que correspondem ao resultado médio, obtido por meio de uma função específica, de cada um dos primeiros lotes de cada grupo.
As letras A e B da Figura 60 correspondem à possibilidade da análise dos resultados ser realizada nos dois sentidos:
Horizontalmente (B), analisando o resultado de cada um dos snapshots de cada lote e a possibilidade de continuidade de processamento dos demais lotes de um mesmo grupo, comparando- o com o resultado médio obtido por os demais snapshots do lote ao qual ele pertence;
Verticalmente (A), comparando o resultado médio do processamento de um lote de um grupo com os demais lotes de outros grupos.
Total_ resultado $= 100 numero $= 25 Calcula- se a média amostral, considerando a Equação apresentada em.
A o se aplicar a função de média amostral, apresentada na Equação, com os valores já definidos tem- se a Equação:
Portanto a Equação:
Media (melhor_ valor, pior_ valor):
Retorna o ponto médio entre os dois valores fornecidos, ou obtidos por meio de processamento amostral, servindo como base para a análise do resultado do lote.
Consideram- se os seguintes valores exemplo para o cálculo da média aritmética, definida na Equação, cujos resultados são apresentados nas Equações e:
Melhor valor:
10,8 pior valor:
9,2 media (total_ resultado, numero, (total_ lote ­ numero), grupo, lote):
Retorna a média amostral estimada, considerando os snapshots que ainda não foram processados.
A expressão (total_ lote ­ numero) fornece a quantidade de snapshots que ainda restam para serem processados e as informações referentes a grupo e lote são utilizadas para que se calcule o desvio padrão, resgatando os valores individuais de cada um dos snapshots já processados.
Se for considerado o valor exemplo obtido com o cálculo da média na Equação, 4 está muito distante do melhor valor gerado na Equação, 10.
Além de os valores exemplo já definidos, define- se também que total_ lote é igual a 35.
Portanto, apesar disso, como ainda se tem 10 snapshots a serem processados, considerando que 25 já foram executados, não se pode, simplesmente, inferir que o grupo não atingirá um resultado razoável se continuar seu processamento.
O que se faz é verificar se o grupo tem a probabilidade de atingir e, se possível, obter um resultado médio melhor que o valor médio obtido entre o melhor e o pior valores.
Além de a média aritmética simples, é interessante que se utilize o desvio padrão para análise dos dados.
Assim, o desvio padrão é utilizado para a definição da média amostral estimada, assim chamada nesse trabalho.
Conforme Larson e Farber em, a Regra Empírica, utilizada para análise dos dados por meio de a utilização do desvio padrão, apresenta as características a seguir:
De essa forma, a Figura 66 esquematiza as características já definidas por a regra empírica, com base em.
Considerando, então, a regra empírica e a análise dos desvios padrão para estimar a média dos snapshots faltantes, chega- se à aplicação da própria regra, obtendo o valor médio, conforme Equação:
Os valores utilizados na Equação compreendem:
S $= desvio padrão amostral, por o processamento da população do grupo não estar concluída;
A o se aplicar a Equação, obtém- se um valor muito próximo de a própria média já gerada, uma vez que são considerados os valores de desvio padrão e média aritmética da própria amostra.
A Tabela 4 exemplifica, com dados reais, a aplicação da Equação.
A o se selecionar a primeira linha da Tabela 4 para melhor detalhar a aplicação da Equação se tem a geração dos resultados apresentados na Equação:
Portanto: A diferença entre a média aritmética e a média estimada é mínima.
Isso se deve ao fato de se estar utilizando a regra empírica, que apresenta a probabilidade de distribuição exata entre os valores formadores da média.
Para a realidade em questão, quando se deseja analisar a probabilidade de se encontrar snapshots que obtenham um bom resultado, a regra empírica não deve ser aplicada na sua forma original.
Uma adaptação foi feita para que sejam superestimados os melhores valores.
Assim, a regra empírica, aplicada neste trabalho para a geração da média estimada, apresenta a estrutura conforme definida por a Equação.
A principal alteração da regra utilizada neste trabalho, para a regra empírica original, está em superestimar a probabilidade de bons resultados.
Assim: A Figura 67 esquematiza as características da regra empírica adaptada, a qual é utilizada para o cálculo da média estimada, com base na regra empírica original.
A Tabela 5 exemplifica a aplicação da Equação, que utiliza a regra empírica adaptada para a obtenção da média estimada.
A média estimada é utilizada em conjunto com a média aritmética da amostra para definir a mudança de status dos snapshots do grupo.
Portanto, busca- se que o valor gerado seja melhor em relação a a média definida por o intervalo, pois seu resultado será utilizado para a definição do descarte ou não de snapshots.
O valor obtido em, 10,02, é melhor que o valor médio obtido na Equação.
Portanto, considera- se, para esse caso, que o grupo tem chances de atingir um valor aceitável.
Entretanto, a média amostral do grupo é pior que o valor médio obtido na Equação.
Assim os snapshots restantes desse grupo terão sua prioridade diminuída.
Essa análise deve ser feita para todos os grupos, verificando a possibilidade de que o processamento não seja realizado com todos os snapshots, o que reduz, consideravelmente, o tempo de execução total.
É importante destacar que algumas condições devem ser satisfeitas para a aplicação do algoritmo:
Quando o processamento estiver sendo realizado em paralelo, ou seja, mais de um grupo em processamento ao mesmo tempo, a prioridade de processamento dos lotes e snapshots de um grupo também pode ser alterada.
Para que isso seja possível, a cada média calculada deve ser armazenada a informação numa estrutura de arquivos/ tabelas com a estrutura definida conforme Tabela 6.
A Tabela 6 também apresenta alguns valores que serão utilizados para exemplificar a alteração de status de um grupo.
Em a Tabela 6, o valor correspondente à Média é o valor médio obtido entre todos os valores médios dos lotes já processados.
Assim, a cada nova média aritmética obtida com o processamento de um lote, esse valor deve ser atualizado.
O valor correspondente à Média Estimada segue o mesmo princípio:
A cada nova média estimada obtida com o processamento de um lote, esse valor deve ser atualizado.
O Status refere- se ao status do grupo como um todo.
Um grupo com status igual a P é aquele grupo que possui a menor prioridade para processamento, ou seja, o grupo que apresenta os piores valores em média.
Um grupo com Status igual a D é aquele grupo, cujos valores médios para média e média estimada são piores que o valor utilizado como parâmetro.
Esse grupo foi descartado e seus snapshots não serão processados.
Um grupo com Status igual a A é um grupo que está aguardando o processamento e será processado, considerando os valores obtidos por os demais, seguindo uma ordem de prioridade, que vai do melhor valor ao pior valor.
Cabe ressaltar que os status desses grupos estão sujeitos a alterações, uma vez que esses valores são atualizados e analisados a cada nova média gerada.
A diferença quando se considera status dos snapshots e status do grupo está na continuidade ou não de processamento de outros lotes.
Se for considerado status de snapshots dentro de um lote após a análise da média e da média estimada para o lote, define- se:
Se for considerado status de grupo após a análise da média e da média estimada, definese:
Que os demais lotes do grupo serão descartados:
Isso significa que se passa para o processamento de outro grupo e que todos os snapshots dos lotes restantes não serão processados.
O Padrão Múltiplas Instâncias Autoadaptáveis (P-MIA), cujo funcionamento foi apresentado neste capítulo, busca substituir as etapas envolvidas no processamento denominado como Dados Seletivo no workflow desenvolvido por Karina Machado, de forma que não exista a necessidade de execuções exaustivas, tornando a seleção de snapshots uma etapa dinâmica.
De essa forma, com o P-MIA, é possível que se analise os resultados obtidos por os snapshots em tempo de execução e que se defina se snapshots do mesmo lote, ou do mesmo grupo, continuarão sendo executados e com qual prioridade, fornecendo ao padrão características de autoadaptação de instâncias, sem a interferência do usuário durante a execução.
Estudos sobre adaptação e autoadaptação que serviram de referência para esta Tese podem ser encontrados em e em.
O capítulo apresentou os algoritmos utilizados para a separação dos grupos em lotes menores, bem como as funções utilizadas para o cálculo da média aritmética e da média estimada, cujos resultados são utilizados por o padrão para a definição das próximas etapas de execução.
Quanto a a função utilizada para a média estimada, foi adaptada a Regra Empírica, definida sobre valores do desvio padrão, valor fundamental para a análise dos resultados.
Em o Capítulo 7, testes são apresentados, validando os algoritmos e as funções definidas.
Para a validação do funcionamento do P-MIA, apresentado no Capítulo 6, testes foram realizados com grupos, aqui também chamados de clusters, criados por Karina Machado, fazendo parte de pesquisas para sua Tese de Doutorado, a qual compreende a definição de uma função de similaridade específica para a realidade de estudo do LABIO.
A ferramenta utilizada por Karina foi o PTRAJ que, conforme:
&quot;é um programa utilizado para processar e analisar conjuntos de coordenadas 3D lidas de uma série de arquivos de coordenadas de entrada.
Para cada conjunto de coordenadas lido, uma sequência de ações pode ser executada (numa ordem que deve ser especificada) de acordo com configurações pré-estabelecidas».
São utilizados, nos testes desta Tese, grupos formados por dois dos algoritmos utilizados por Karina por meio de o PTRAJ:
Hierarchical e means (k--means).
A aplicação de cada um dos algoritmos gerou 6 clusters, considerando os mesmos dados de entrada.
Como para este trabalho a função de similaridade não é o foco parte- se, portanto, dos clusters já definidos para a aplicação Os testes experimentais foram realizados seguindo o processo de experimentação sugerido por Höst et al.
E apresentado na Figura 68,.
As próximas seções apresentam o protocolo de experimento utilizado neste trabalho, bem como o detalhamento dos testes realizados.
A motivação do experimento é avaliar os resultados obtidos com o P-MIA a partir de os valores gerados após a execução com snapshots reais.
Para tanto se define que:
O objeto de estudo é o resultado obtido por snapshots individuais após serem submetidos ao processamento num sistema de workflow.
O propósito do experimento é avaliar se o P-MIA pode ser utilizado por a área de Bioinformática, buscando a redução da quantidade total de snapshots a serem processados e garantindo que os snapshots que apresentariam os melhores resultados de processamento, continuariam sendo executados.
O foco principal dos experimentos realizados é o desempenho do P-MIA quanto a o ganho obtido após a finalização de todas as execuções.
Considera- se ganho como a quantidade de snapshots que não foram processados, identificados dinamicamente, sem a interferência do usuário, a possibilidade desses experimentos serem realizados em paralelo e a manutenção de resultados promissores.
O experimento é executado no contexto de Bioinformática, mais especificamente quando executados sobre a enzima InhA.
Portanto, os testes experimentais são realizados com o objetivo de se submeter dados reais ao P-MIA, identificando o ganho obtido.
O experimento é focado em analisar o funcionamento e o desempenho do P-MIA quanto de dados reais, no contexto experimental, conforme Junior em, permite que outros pesquisadores possam reproduzir o experimento em questão, avaliar e sugerir melhorias ao padrão.
Formulação das Hipóteses Para que os experimentos sejam avaliados coerentemente, deve- se saber como e o quê se quer formalmente avaliar.
Assim, é necessário que se formule hipóteses, buscando analisar e validar o padrão proposto:
Hipótese Nula:
A utilização do P-MIA não resulta em ganhos.
Hipótese Alternativa: A utilização do P-MIA resulta em ganhos.
Considera- se ganho como a execução de uma quantidade menor de experimentos, a possibilidade desses experimentos serem realizados em paralelo e a manutenção de resultados promissores.
Seleção das Variáveis De entre as variáveis utilizadas está o resultado do processamento de cada snapshot, o FEB.
O FEB é utilizado, em conjunto com resultados de outros snapshots de um mesmo lote e de um mesmo grupo, para que se obtenha as variáveis média e média estimada, as quais subsidiam a definição de continuidade ou não do processamento.
Além de essas, também são utilizados valores para amostragem e quantidade mínima de snapshots para serem processados, bem como, valores correspondentes ao melhor e pior valores.
Amostra Os snapshots utilizados para execução dos experimentos apresentados nesta Tese são amostra, em si, não define o funcionamento do padrão, o qual pode ser aplicado a qualquer conjunto de dados.
De a mesma forma, foram utilizados grupos formados por os algoritmos Hierarchical e K--Means, gerados por meio de o programa PTRAJ.
A diferença de utilização de algoritmos seria representativa se fosse utilizada uma função de similaridade específica para a realidade de estudo, como a sendo proposto por Karina Machado em sua Tese de Doutorado.
Esboço do Experimento Foram definidos valores para amostragem e quantidade mínima de snapshots para a geração dos lotes a partir de os grupos, previamente criados por uma função de similaridade qualquer.
Após, com os resultados de FEB obtidos com o processamento de cada um dos snapshots que compõem os grupos, foram geradas médias e médias estimadas, subsidiando as análises realizadas.
Instrumentação Os dados foram obtidos por meio de processamentos exaustivos realizados no LABIO.
Os snapshots separados em grupos foram obtidos por meio de diferentes testes realizados por Karina Machado para sua Tese de Doutorado.
A preparação dos dados, para os experimentos, foi realizada associando- se os valores de FEB obtidos a seus respectivos snapshots, dentro de diferentes lotes, de diferentes grupos.
Execução Após a separação dos snapshots em lotes e a associação dos resultados de FEB obtidos, os experimentos com o P-MIA validaram as funções definidas:
Média e média estimada para a determinação de continuidade ou não do processamento.
Os testes com os diferentes algoritmos buscam subsidiar a definição das regras já apresentadas e validar as seguintes questões:
Grupos menores produzem resultados melhores que grupos maiores?
Quanto antes for iniciada a análise, maior é o ganho de processamento?
A função de similaridade aplicada está diretamente relacionada ao ganho obtido após a execução do padrão?
As próximas seções apresentam o detalhamento dos testes realizados.
A etapa de análise e interpretação, da Figura 68, é apresentada em cada teste realizado.
A etapa de apresentação e empacotamento, da Figura 68, não é realizada, por a característica da aplicação e as conclusões obtidas estão no final deste capítulo.
Testes Experimentais com o Ligante PIF Antes da realização dos testes, algumas informações foram definidas.
São elas:
Informações necessárias para a geração dos lotes para processamento que são criados a partir de os clusters:
Quantidade mínima de snapshots a serem processados e amostragem (valor em percentual);
Informações necessárias para a análise dos resultados:
Melhor valor e pior valor (intervalo que será considerado para a análise dos resultados).
Tabela 7 ­ Resultado do Processamento de dois snapshots de cada cluster para obter o melhor e pior valores snapshot Cluster A Tabela 8 apresenta a quantidade de snapshots que compõem cada um dos grupos, gerados a partir de o algoritmo k--Means.
A Tabela 9 apresenta a definição dos lotes de processamento de um dos clusters, seguindo o algoritmo da Figura 63.
Os demais lotes são encontrados no Apêndice A. Para a definição dos lotes considerou- se, conforme já definido, amostragem de 30% e quantidade mínima para processamento de 50 snapshots.
Nota- se, na Tabela 9, que o lote 5 ficou maior que os lotes 2, 3 e 4.
Isso se deve ao fato da amostragem definida resultar em quantidade menor que a quantidade mínina de snapshots exigida para processamento e ao lote residual final ficar com menos snapshots que o mínimo definido.
Após a separação dos snapshots do cluster 1 em lotes inicia- se a execução individual, como instância do processo, de cada um dos snapshots.
A Tabela 10 apresenta 3 snapshots já processados, os resultados individuais de processamento obtidos e os próximos snapshots a serem processados.
Conforme especificado no Capítulo 6, os valores possíveis para o status dos snapshots são:
A (Ativo);
F (Finalizado);
D (Descartado);
P (Prioridade Alterada ­ diminuída).
Para um lote com 166 snapshots, o processamento de 3 snapshots não permite a realização de muitas análises, dessa forma, processam- se mais snapshots antes que seja possível se inferir algumas conclusões.
Testes foram realizados, com quantidades diferentes de snapshots, buscando identificar qual seria o percentual a ser utilizado para definição de continuidade ou descarte do lote.
É importante destacar que a definição de alteração de prioridade e descarte deve ser analisada, conforme já mencionado, com base na média aritmética dos valores obtidos e na média estimada.
As Tabelas 11, 12, 13, 14 e 15 apresentam as análises realizadas.
Para o correto entendimento das informações contidas nas tabelas as colunas referem- se a:
Lote de cada cluster:
Identificado por C_ L;
Quantidade total de snapshots do lote:
Identificado por Quant;
Média aritmética dos snapshots do lote até o momento da análise:
Identificado por M20%, M30%, M50%, M70% e M80%;
Média estimada dos snapshots restantes até o momento da análise:
Identificado por E20%, E30%, E50%, E70% e E80%;
Quantidade de snapshots processados até o momento da análise:
Identificado por Proc;
Quantidade total de snapshots processados:
Identificado por ProcFinal;
E quantidade de snapshots que não precisaram ser processados:
Identificados por Ganho.
Uma característica em comum nas Tabelas 11, 12, 13, 14 e 15 é a presença de sombreamentos.
Os sombreamentos à direita das colunas que estão sendo analisadas significam que os snapshots em questão não foram processados.
Em a Tabela 11, a análise dos resultados da média aritmética e da média estimada começou quando 20% dos snapshots foram processados.
Essa análise deve ser refeita para os lotes que, quando analisados em 20% de processamento, continuaram com snapshots com status de Ativo, ou Prioridade Reduzida.
Os lotes, cujos snapshots passaram a ter status de Descartado não serão mais analisados.
Esse status é definido quando os valores de média e média estimada, ambos, são piores que o valor médio utilizado como parâmetro.
Um exemplo pode ser visualizado na Tabela 11, no cluster 1, Lote 1, quando a média possui valor igual a 9,19 e a média estimada igual a 9,64.
A os snapshots que apresentam essa característica, após esse ponto de análise, foi conferido o sombreamento nas tabelas, sem resultado numérico na célula.
Iniciando a análise, conforme apresentado na Tabela 11, com 20% de processamento, obtém- se um ganho para os dados analisados de 47%, ou seja, 1426 snapshots de um total de 3042 não foram processados, considerando- se a média aritmética e a média estimada.
Uma característica interessante do modelo, que pode ser comprovada por meio de a Tabela 11 nas linhas correspondentes ao Cluster 2, Lotes 3 e 4 (C2_ L3, C2_ L4) é que a média dos lotes (9,63 e 9,92) é pior que a média que está sendo utilizada como parâmetro.
Apesar disso, os snapshots continuaram sendo processados, com prioridade reduzida, pois as médias estimadas dos lotes são maiores que a média utilizada como parâmetro.
Snapshots desses lotes somente deixam de ser processados quando já tiver sido executado 70% de cada um dos lotes.
Em esse ponto, as médias estimadas tornam- se piores que a média utilizada como parâmetro.
Em a Tabela 12, as análises começaram a ser feitas quando 30% do total de snapshots de cada lote estavam processados.
Assim, os cálculos de média aritmética e média estimada foram realizados, considerando os snapshots executados.
A o se confrontar a Tabela 11 com a Tabela 12, verifica- se que o Lote 5 do Cluster 1 apresenta diferenças consideráveis de ação.
Em a Tabela 11, os demais snapshots desse lote, após o processamento de 20%, foram descartados, pois a média aritmética e a média estimada são piores que o valor médio utilizado como parâmetro.
Já na Tabela 12, quando a análise foi iniciada após 30% do processamento do lote concluído, os demais snapshots são processados, com baixa prioridade, mas processados, pois o valor da média aritmética é pior que o valor utilizado como parâmetro, mas a média estimada possui valor melhor.
Os demais lotes seguiram a mesma tendência identificada na Tabela 11.
Com a análise iniciando com 30% do processamento concluído, obtém- se um ganho de 40% dos snapshots, ou seja, 1215 snapshots, dos 3042, não foram processados.
Em a Tabela 13 a análise dos resultados foi iniciada após ter sido concluído o processamento de 50% do total de snapshots do lote.
Não se evidência mudanças da estratégia de execução dos snapshots quando analisados em 50% ou analisados em 30%.
Como a análise foi postergada, o ganho de processamento, nesse caso, é menor.
Quando se inicia a análise para inferir sobre os status dos snapshots com 50% de processamento concluído, obtémse um ganho de 29%, ou seja, 887 snapshots não foram processados.
Em a Tabela 14, da mesma forma que o analisado para a Tabela 13, não existem mudanças a serem consideradas.
Há, apenas, a redução no ganho de processamento por se iniciar a análise após 70% dos snapshots do lote terem sido processados.
O ganho, nesse caso, foi de 19%, ou seja, 565 snapshots não foram processados.
Em a Tabela 15, quando se inicia a análise após 80% dos snapshots terem sido processados, a redução é menor, apenas de 13%, ou seja, 404 snapshots não precisaram ser processados por a característica do lote, gerada por meio de os resultados obtidos por os snapshots já processados.
Com uma análise preliminar, observando apenas a quantidade de snapshots que não foram processados, aqui denominado de ganho, chega- se à conclusão que a melhor alternativa seja iniciar a análise o quanto antes, ou seja, com 20% dos snapshots processados.
Ainda, ao se considerar que a função de similaridade é adequada à realidade em questão e que os dados estão corretamente agrupados e que o resultado de um seja, provavelmente, o resultado dos demais, essa conclusão está correta.
Apesar disso, outros estudos foram realizados, verificando se os snapshots que obtiveram os melhores resultados foram contemplados, ou seja, se foram processados quando as análises das diferentes situações foram feitas.
A Tabela 16 apresenta os números dessa análise.
A linha &quot;Melhores_ 10% «refere- se aos 304 snapshots que, num processamento exaustivo, apresentaram os melhores resultados de FEB.
A linha &quot;Melhores_ 30% «refere- se aos 912 snapshots que, num processamento exaustivo, apresentaram os melhores resultados de FEB.
As colunas indicam a cobertura desses snapshots quando as análises iniciaram com 20%, 30%, 50%, 70% e 80%, respectivamente, do processamento concluído.
A o se analisar a Tabela 16, verifica- se que os dados não estão agrupados de forma tão simétrica como se esperava, pois mesmo com a análise sendo realizada após 80% do total de snapshots terem sido processados, não se chega ao processamento de um número perto de 100% dos snapshots de melhor resultado.
A variação entre os pontos de análise não é grande.
Por mais que possa ser considerado que aproximadamente 24% dos snapshots com melhores resultados não estão sendo processados quando se inicia a análise em 20% e que aproximadamente 20% não são contemplados quando se inicia a análise com 30% dos snapshots processados, esse é um risco a ser assumido quando se deseja reduzir a quantidade de snapshots a serem processados.
A Figura 69 contém o gráfico com a análise dos resultados obtidos após a execução dos experimentos e a visualização da manutenção dos resultados promissores, que correspondam aos 10% melhores.
Em o gráfico da Figura 69 verifica- se que o P-MIA supera a manutenção dos resultados promissores, quando comparado aos 10% dos snapshots processados.
De a mesma forma, a Figura 70 contém o gráfico com a análise dos resultados obtidos após a execução dos experimentos e a visualização da manutenção dos resultados promissores, que correspondam aos 30% melhores.
Testes Experimentais com o Ligante NADH Para os testes realizados com o ligante NADH, os valores foram especificados da mesma forma que para o ligante PIF, são eles:
O intervalo gerado por os valores fornecidos para[ melhor valor, pior valor] é[ 20,01;
6,54]. Esses valores foram obtidos por amostragem, executando- se 2 snapshots de cada grupo, de forma aleatória.
Os resultados desse processamento são apresentados na Tabela 17.
A Tabela 18 apresenta a quantidade de snapshots que compõem cada um dos clusters, gerados a partir de o algoritmo k--Means.
Os testes foram desenvolvidos da mesma forma que para o ligante PIF, já detalhados na seção 7.3.2.
As Tabelas 19, 20, 21, 22 e 23 contêm os resultados dos testes realizados com o ligante NADH e, da mesma forma que para o PIF, o ganho de processamento foi considerável.
O valor médio, utilizado como parâmetro para a realização destes testes é igual a 13,26, gerado a partir de a média aritmética entre o melhor e pior valores:
Em as Tabelas 19, 20, 21, 22 e 23, da mesma forma que para o ligante PIF, as células sombreadas e sem valor correspondem a snapshots que não foram processados após a análise;
E as células sombreadas, com valor, correspondem a snapshots que foram processados antes do ponto de análise.
Em a Em a Tabela 20, com o ponto de análise iniciado aos 30% da totalidade de processamento, o ganho foi de 41%, ou seja, 1261 snapshots não foram processados.
Em a Tabela 21, com o ponto de análise iniciado aos 50% da totalidade de processamento, o ganho foi de 33%, ou seja, 1021 snapshots não foram processados.
Os percentuais de ganho, ao se testar o modelo com o ligante NADH, foram diferentes dos obtidos com os testes realizados com o ligante PIF.
Apesar disso, a diferença não se apresentou muito grande, sendo de aproximadamente 3% para mais ou para menos nos diferentes pontos de análise.
Em a Tabela 22, o ponto de análise iniciou após 70% do total de snapshots de cada lote estar concluído.
Mesmo assim, obtém- se um ganho satisfatório, ao se considerar que a partir de a média aritmética e da média estimada, aproximadamente 21% dos snapshots, ou seja, 655, não foram processados.
Para o ligante PIF, apresentado na seção anterior, esse ganho ficou em 19%.
Em a Tabela 23, quando o ponto de análise foi fixado para iniciar após 80% do processamento estar concluído, o ganho foi de 14%:
435 snapshots não foram processados.
As Tabelas 24, 25, 26, 27 e 28 contêm os testes realizados para o ligante NADH, considerando o grupo como um todo, sem a separação em lotes menores.
Verifica- se, a partir destes testes, que o ganho em processamento é bem menor e não apresenta grande variação caso as análises sejam realizadas mais cedo ou mais tarde, ou seja, não importa se a análise é iniciada com 20% (Tabela 24), 30% (Tabela 25) ou 50% (Tabela 26), pois em todos esses casos o ganho de processamento foi o mesmo, apenas de 22%.
Esse caso é diferente da análise realizada por meio de lotes menores que, com 20%, obteve- se um ganho de 43%, ou seja, 43% dos snapshots não foram processados;
Com 30% obteve- se um ganho de 41%;
E com 50% dos snapshots processados, obteve- se um ganho de 33%.
Com 70% (Tabela 27) e com 80% (Tabela 28) do processamento concluído para se dar início às análises, os valores são ainda piores:
16% e 10% de ganho respectivamente.
Tabela 29 ­ Snapshots com melhores resultados relacionados à quantidade processada para análise -- NADH Proc_ 20% Proc_ 30% Proc_ 50% Proc_ 70% Proc_ 80% Melhores_ 10% Melhores_ 30% A o se analisar a Tabela 29, da mesma forma que para o ligante PIF, verifica- se que os dados não estão agrupados de forma tão simétrica como se esperava, pois mesmo com a análise sendo realizada após 80% do total de snapshots terem sido processados, não se chega ao processamento de um número perto de 100% dos snapshots de melhor resultado.&amp;&amp;&amp;
O desvio padrão dos resultados de FEB, obtidos após o processamento dos diferentes grupos, é muito inconstante, chegando, em alguns lotes, a valor igual a 114, por exemplo, enquanto outros lotes apresentam desvio padrão igual a 0,91 o que demonstra que o critério de similaridade utilizado não está fornecendo uma constância aos dados, já que lotes de um mesmo grupo apresentam valores muito distintos.
Em a Tabela 29 também se verifica que a análise sendo iniciada com 20% do processamento concluído, ou com 30%, obteve a mesma quantidade de snapshots com melhor resultado.
De essa forma, como o ganho de processamento é muito maior quanto mais cedo se iniciam as análises, sugere- se que o início das análises do P-MIA seja realizado, de forma prédefinida, quando 20% do processamento dos snapshots estiver concluído, sendo que esse valor pode ser reduzido ou incrementado antes do início do funcionamento do padrão.
A Tabela 30 contém a quantidade de snapshots que apresentaram os melhores resultados de FEB, que efetivamente foram processados por meio de a aplicação do P-MIA, sem a separação dos grupos em lotes menores.
Nota- se que a quantidade de snapshots processados foi um pouco maior que a separação em lotes, 3%, considerando- se 20%, 30% e 50% do processamento.
Apesar disso, como o ganho em processamento, ao se separar em lotes menores, é muito superior, justifica- se a criação de lotes menores do P-MIA.
A Figura 71 contém os gráficos que apresentam a manutenção dos resultados mais promissores, considerando os 10% melhores, quando o experimento foi realizado separando- se os grupos em lotes menores e sem a separação.
Nota- se o ganho obtido no processamento de lotes menores.
Os testes realizados neste capítulo buscam a validação do Padrão Múltiplas Instâncias Autoadaptáveis (P-MIA), formalizado no Capítulo 5 e cujo funcionamento foi apresentado no experimentais a hipótese nula definida como &quot;A utilização do P-MIA não resulta em ganhos «foi negada, pois, os resultados obtidos com o experimento realizado com o ligante NADH, sintetizados na Tabela 31, ao se manipular lotes de snapshots quando, por exemplo, 20% do processamento está concluído, comprova essa afirmação.
Em essa tabela, considerando- se uma quantidade total de snapshots a serem processados igual a 3100 e que desses, 1332 não foram processados quando a análise foi iniciada aos 20% do processamento, conferindo um ganho de 43% ao experimento, a expectativa poderia ser de que 43% dos snapshots de melhor resultado também não seriam processados.
Esse número ficou em apenas 11%, pois 89% dos snapshots de melhor resultado foram contemplados quando a análise foi iniciada aos 20%.
Assim, os testes realizados demonstraram os ganhos obtidos com a aplicação do padrão, considerados como a redução da quantidade total de execuções e a continuidade de processamento dos snapshots que, num processo exaustivo, apresentariam os melhores resultados.
De essa forma, a hipótese alternativa definida como &quot;A utilização do P-MIA resulta em ganhos «foi comprovada.
Além de a validação das hipóteses, os testes realizados validaram as características do padrão:
Criação de lotes, aplicação da média aritmética e da média estimada, descarte de snapshots e alteração de prioridade.
Também foi possível, a partir de a realização dos testes, que as seguintes questões obtivessem respostas:
Grupos menores produzem resultados melhores que grupos maiores?
SIM. Conforme apresentado por os testes realizados neste capítulo, processamento de grupos menores de snapshots gera maior ganho, pois as análises são realizadas com uma população menor.
Quanto antes for iniciada a análise, maior é o ganho de processamento?
SIM, pois se inicia o descarte de snapshots, de lotes que não apresentam bons resultados, antes, fazendo com que uma quantidade muito maior de snapshots não necessite de processamento.
A função de similaridade aplicada está diretamente relacionada ao ganho obtido após a execução do padrão?
SIM. Acredita- se que o trabalho que está em desenvolvimento por Karina Machado para a definição de uma função de similaridade específica para a área de estudo do LABIO, sobre os mesmos dados manipulados nesta Tese, melhore ainda mais os resultados obtidos, reduzindo a variação dos desvios padrão num mesmo grupo, pois tende a criar grupos cujos resultados de FEB sejam mais próximos e, quando esses snapshots forem submetidos aos P-MIA, que o padrão possa obter uma maior cobertura dos melhores resultados.&amp;&amp;&amp;
Conclui- se, ainda, que os testes apresentados neste capítulo foram fundamentais para a validação do padrão e para a justificativa dos critérios definidos, podendo ser facilmente reproduzidos por meio de a definição dos lotes e da implementação das funções de média e média estimada, apresentadas no Capítulo 6.
Este capítulo apresenta trabalhos relacionados ao desenvolvido nesta Tese, identificando semelhanças e diferenças.
Cabe destacar que são apresentados onze trabalhos que apresentam características que, de alguma forma, relacionam- se com o trabalho desenvolvido, pois o foco da análise está na identificação de iniciativas, envolvendo:
Utilização de workflows científicos, principalmente na área de Bioinformática;
Formalismos de representação de processos, em especial redes de Petri;
E utilização de Padrões de Dados, com o intuito de estabelecer pontos de interseção entre a Tese desenvolvida e os demais trabalhos da área.
Sadiq et al.,
definem que a especificação completa de um workflow requer a integração de diferentes características de processos, como decisões, definições de atividades individuais, lógica do processo e regras de execução.
Afirmam que uma das importantes características de modelagem e especificação de workflows envolve o fluxo dos dados, sua modelagem, especificação e validação e que muitos pesquisadores têm negligenciado esta dimensão da análise de processos.
Os autores identificam e justificam a importância da modelagem de dados na especificação e verificação de workflows.
Identificam possibilidades de problemas com fluxos de dados que, caso não sejam identificados, podem prejudicar o funcionamento dos workflows.
Sadiq et al.,
também afirmam que a tecnologia de workflows é utilizada como uma tecnologia de integração de sistemas existentes, uma prática muito utilizada em Bioinformática, aplicação foco desta Tese.
Os autores apresentam a distinção clara entre aplicações específicas de dados em atividades individuais e de dados relacionados a controle de processos.
Além disso, distinguem dados de entrada dos de saída de uma atividade.
Essa distinção também é aplicada na Tese desenvolvida, sendo utilizada na definição do P-MIA:
Padrão Múltiplas Instâncias Autoadaptáveis.
Os autores em apresentam a diferenciação entre fontes de dados internas e fontes externas.
Essa característica também é utilizada por o P-MIA, uma vez que se trabalha com dados manipulados internamente no processo, onde tarefas geram valores e esses são utilizados por outras tarefas no processo (fonte de dados interna) e estruturas externas como bancos de dados, sendo acessíveis por tarefas do processo (fonte de dados externa).
Apesar de apresentar vários conceitos que se relacionam com o trabalho desenvolvido nesta Tese, o trabalho de Sadiq et al.
Possui foco principal na validação da modelagem dos fluxos de dados, enquanto o desenvolvido apresenta um novo padrão de dados, cujas características envolvem a manipulação de grandes conjuntos de dados e a análise da continuidade do seu processamento.
Qin e Fahringer afirmam que a definição de estruturas de fluxo de dados direcionada à execução de conjuntos flexíveis de dados é um dos requisitos de execução para aplicações científicas em grade.
Apresentam uma alternativa para a introdução do conceito de coleção de dados e sua correspondente distribuição em aplicações de workflows em grade.
Afirmam, também, que a tecnologia em grade fez com que cientistas e engenheiros criassem aplicações cada vez mais complexas, para gerenciar grandes volumes de conjuntos de dados, e executar experimentos científicos sobre essa tecnologia.
Os autores definem que uma aplicação de workflow em grade pode ser vista como uma coleção de tarefas computacionais que são processadas numa ordem bem definida, para atingir a um objetivo específico.
Afirmam que diferentes construtores de controle de fluxo têm sido identificados e desenvolvidos para sistemas de workflows em grade, podendo ser divididos em quatro categorias:
Sequencial, paralela, condicional e interativa.
Com cada um desses construtores, diferentes fluxos de dados podem ser especificados.
Ainda destacam que fluxos de dados manipulados sobre workflows científicos em grade são geralmente complexos por o conjunto de dados envolvidos e essa é uma das motivações do desenvolvimento desta Tese:
A manipulação de grandes conjuntos de dados na área de Bioinformática.
Qin e Fahringer também afirmam que aplicações científicas consomem uma porção de um conjunto de dados produzidos por qualquer aplicação e que um construtor para execuções em paralelo consome múltiplos conjuntos de dados em cada interação.
Os autores afirmam, entretanto, que o problema que trata a especificação de como os conjuntos de dados e seus respectivos elementos podem ser identificados e como esses dados podem ser distribuídos dentro de interações em paralelo, ainda não foi totalmente explorado.
Muitos sistemas de workflow em grade resolvem o problema de replicação das entradas de conjuntos de dados para atividades, mas afirmam que existe a necessidade de mais estudos que busquem flexibilizar mecanismos de fluxo de dados de conjuntos de dados, evitando redundância.
A abordagem definida por Qin e Fahringer reduz a duplicação de dados e otimiza a transferência entre atividades do workflow.
O trabalho desenvolvido nesta Tese não trabalha especificamente com processamento em grade, mas os conceitos aplicados no trabalho de Qin e Fahringer são utilizados na possibilidade do padrão ser executado em paralelo.
Além disso, tanto o trabalho desenvolvido por os autores, quanto o desenvolvido nesta Tese, manipulam grandes volumes de dados e o fazem a partir de a definição de conjuntos.
Chaouiya, em, afirma que o uso de modelos matemáticos é crescente na representação de redes biológicas complexas.
Destaca, principalmente, o uso das redes de Petri e suas extensões.
Seu trabalho envolve a apresentação de como uma rede de Petri poderia representar um sistema biológico.
O autor apresenta um levantamento com diferentes trabalhos que utilizam redes de Petri para modelar aplicações na área científica.
Seu trabalho enfatiza aspectos como efetividade na modelagem com redes de Petri e a análise e simulação das redes.
O autor acredita que o aumento do uso de modelos baseados em redes de Petri para a representação de redes biológicas pode ser justificado por a representação gráfica do modelo, a possibilidade de representar sistemas concorrentes, sua base matemática e a existência de ferramentas de modelagem.
Ainda afirma que, por a característica da aplicação da área Biológica envolver grandes interações de dados, existe a necessidade de se prover modelos qualitativos, os quais permitam uma análise formal.
O autor destaca como pontos principais na utilização de redes de Petri para a formalização de processos científicos:
Redes de Petri possuem representação gráfica, com base teórica matemática e ferramentas de diagramação disponíveis;
Redes de Petri permitem a análise de estruturas qualitativas para propriedades comportamentais quantitativas;
Redes de Petri são efetivas para a modelagem de redes moleculares.
Apesar de as redes de Petri terem sido definidas, inicialmente em 1962, sua utilização ainda é ampla, principalmente, quando se necessita de um formalismo com características matemáticas que represente o comportamento das tarefas (no caso de redes de Petri coloridas).
A Tese aqui apresentada trabalha com redes de Petri coloridas, e o trabalho desenvolvido por Chaouiya embasa essa escolha.
Apesar disso, diferente dos trabalhos analisados por Chaouiya, a modelagem em redes de Petri realizada nesta Tese é de um padrão, podendo ser utilizado por diferentes aplicações da área científica, desde que com características semelhantes.
Hidders et al.,
propõem o que denominam de DFL:
Uma linguagem de workflow formal e gráfica para fluxos de dados.
Justificam seu trabalho por a existência de workflows em o qual grandes volumes de dados complexos são manipulados, e a estrutura desses dados reflete- se no workflow.
Posicionam o trabalho desenvolvido como uma extensão das redes de Petri, sendo responsável por a organização do processamento das tarefas;
E do cálculo relacional aninhado, com uma linguagem de consulta sobre objetos complexos, que também é responsável por manipular coleções de itens de dados.
Os autores afirmam que fluxos de dados são encontrados na prática, por exemplo, em Física, Astronomia e em outras ciências.
A característica dessas áreas é possuir grandes volumes de estruturas de dados que são analisadas por um sistema e organizadas em rede, de forma que os fluxos de dados sejam processados.
Para os autores, existem formalismos bem desenvolvidos para workflows que são baseados em redes de Petri.
Entretanto, afirmam que esses formalismos não manipulam estruturas complexas de dados, de forma que reflitam na estrutura do workflow em questão.
Em função de isso, também utilizam o cálculo relacional.
Em este trabalho, os autores apresentam a formalização da extensão realizada e, com o objetivo de validarem seu trabalho, mapearam um fluxo de dados real de Bioinformática, mesma área de aplicação da Tese aqui desenvolvida.
Este trabalho é relacionado ao desenvolvido nesta Tese por a área de aplicação e por trabalhar com redes de Petri.
Além disso, por formalizar a extensão desenvolvida, da mesma forma que o feito nesta Tese para o P-MIA.
Workflows Científicos Pereira e Travassos, afirmam que a ciência se apóia em infraestrutura computacional complexa para realizar pesquisas, interessando- se, principalmente, em estudos in autores propõem uma abordagem para auxiliar a concepção de workflows científicos para esses tipos de estudo.
Com o objetivo de capturar o conhecimento tácito envolvido na concepção de workflows, a abordagem direciona- se à identificação de requisitos e à modelagem do workflow nos níveis mais altos de abstração, independentemente de sistema gerenciador de workflow científico a ser utilizado.
A abordagem apresentada por os autores explora os conceitos do diagrama de atividades da UML.
O trabalho de Pereira e Travassos, apresenta relação com o trabalho apresentado nessa Tese, por a representação de workflows científicos e por a necessidade de apoiar o processamento computacional.
Apesar disso, preocupa- se, apenas, com a modelagem de workflows científicos, automatizando o processo, sem se preocupar com a redução de dados a serem processados, foco principal do trabalho desenvolvido nesta Tese.
Pautasso e Alonso, em, identificam um conjunto de padrões de workflow relacionados a execuções paralelas.
Apresentam como os padrões podem ser representados em diferentes linguagens de workflow em grade e suas implicações para o projeto de execução de workflows.
Em o trabalho desenvolvido por os autores, o objetivo também é classificar padrões que manipulem paralelismo, direcionando- os à utilização em grade e não, necessariamente, identificar novos padrões.
Para os autores, a execução em paralelo é uma técnica capaz de reduzir o tempo de execução de workflows científicos, pois conjuntos de tarefas que não tenham dependência podem ser executados em paralelo.
Um conceito interessante, explorado por os autores, é o do paralelismo adaptativo de dados, onde a quantidade de partições a serem executadas pode ser definida manualmente, ou em tempo de execução, e que a estrutura de uma instância de workflow não é somente influenciada por os seus dados de entrada, mas por as propriedades do ambiente de execução.
A Tese desenvolvida está relacionada com o trabalho de Pautasso e Alonso por trabalhar com padrões de dados.
Apesar disso, diferencia- se na característica do padrão, de ser específico para manipulação de dados a serem submetidos ao processamento, e por a análise desses dados em tempo de execução determinar as próximas etapas.
Nardi, em, justifica que o uso de padrões de workflow para controle de fluxo em aplicações de e-Science resulta em maior produtividade por parte de o cientista, permitindo que se concentre em sua área de especialização.
O autor também afirma que, apesar de todos os avanços, o uso de padrões de workflow para paralelização em grades permanece uma questão em aberto.
Seu trabalho apresenta uma arquitetura de baixo acoplamento e extensível, que permite a execução de padrões com ou sem a presença de grade de modo transparente ao cientista, e a implementação de padrões de execução de workflow.
O autor define o Padrão Junção Combinada, já apresentado no documento desta Tese no aplicações de e-Science.
Além disso, o autor definiu uma arquitetura, orientada a serviços, oferecendo flexibilidade e extensibilidade à solução.
A relação entre os dois trabalhos está na utilização de padrões amplamente difundidos, para a definição de outro padrão, e na busca por soluções que otimizem o processamento de dados científicos.
A principal diferença está na utilização por parte de Nardi de padrões de fluxo e desta Tese de padrões de dados.
Grando et al.
Discutem a possibilidade de redes de Petri coloridas, como um formalismo, apoiam a análise de expressividade e verificação estrutural, comportamento e propriedades temporais em workflows clínicos (área médica).
Os autores apresentam uma linguagem que pode ser formalmente mapeada para a representação de redes de Petri coloridas.
Durante seu trabalho, os autores buscaram a formalização de algo informal, de orientações médicas baseadas em texto, por exemplo.
O relacionamento do trabalho desenvolvido por os autores com o desta Tese está, apenas, na utilização de um formalismo matemático (redes de Petri coloridas), bem conhecido, com semânticas formais padronizadas, tendo uma representação gráfica padronizada e independente de fornecedor.
Machado et al.
Em e em desenvolveram trabalhos também para o LABIO.
Com o objetivo de reduzir demandas computacionais e descobrir informações sobre a interação entre receptores e ligantes, aplicaram diferentes algoritmos de mineração onde os arquivos de entrada são baseados na energia livre de ligação (FEB).
Uma vez que o FEB apresenta um valor contínuo e os algoritmos de classificação necessitam de atributos categóricos, os autores também compararam três métodos de discretização para o FEB:
Por igualdade de frequência, por igualdade de largura e por a análise do desvio padrão.
Além disso, avaliaram o impacto na geração de árvores de decisão.
Os autores identificaram que o método que apresentou melhor resultado após a aplicação dos algoritmos de mineração foi o que envolveu a análise do desvio padrão dos dados.
De essa forma, por mais que a técnica seja diferente e que a função utilizada não seja a mesma, os trabalhos relacionam- se por a utilização do desvio padrão, relacionado ao resultado médio obtido após o processamento dos snapshots e por a utilização do FEB como parâmetro de entrada.
Coutinho et al.
Em definem que é muito comum, em experimentos de bioinformática, o processamento de grandes conjuntos de dados.
Em função de isso, afirmam que o paralelismo de dados é uma abordagem comum para incrementar o desempenho e reduzir o tempo de execução.
Entretanto, afirmam que muitos sistemas gerenciadores de workflows científicos (SWfMS) suportam execuções paralelas apenas em ambientes computacionais de alto desempenho.
Colocam o Hydra como um middleware com o propósito de ser uma ponte entre os SWfMS e os ambientes de alto desempenho, fornecendo um caminho transparente aos cientistas na paralelização de execuções de workflows.
Seu trabalho analisa diferentes cenários de paralelismo de dados no domínio de Bioinformática e apresenta uma extensão específica para a manipulação de dados em paralelo em workflows na área.
O Hydra, conforme os autores, é um middleware que provê um conjunto de componentes capazes de serem incluídos em especificações de workflow de qualquer SWfMS para controlar o paralelismo de atividades.
De essa forma, esse trabalho relaciona- se com o definido nesta Tese, por a possibilidade de, após se implementar o P-MIA, criando seus componentes, poder ser configurada sua execução em paralelo com a utilização do Hydra, fornecendo um ganho ainda maior de processamento.
Assistido por Computador Utilizando Receptor Flexível Karina Machado em e Machado et al.
Em, apresentaram um workflow científico para subsidiar o processo de desenvolvimento de fármacos assistido por computador.
Para a criação do workflow científico foram implementados shell scripts e programas que executassem efetivamente e de forma automática as sequências das atividades executadas por o workflow.
Esse modelo permitiu que docagens moleculares, que considerassem a flexibilidade explícita tanto do ligante como do receptor, fossem facilmente executadas, sendo então a flexibilidade natural das moléculas biológicas consideradas nos experimentos de docagem.
Além disso, com a automatização do processo, foi possível a inclusão de uma etapa de seleção de snapshots para que não fosse necessária a execução de todas as conformações do receptor, reduzindo o tempo necessário para se analisar a interação receptor-ligante.
Essa etapa, entretanto, somente pode ser utilizada após a realização de um experimento exaustivo, selecionando aqueles snapshots da macromolécula que apresentaram os melhores resultados de docagem com determinado ligante.
Esses snapshots são utilizados para a docagem de outros ligantes pertencentes à mesma classe que o primeiro.
Outros detalhes do trabalho desenvolvido por Karina Machado também foram apresentados dos Capítulos 3 e 6.
O trabalho desenvolvido nesta Tese relaciona- se com o desenvolvido por Machado et al.
Por trabalhar com workflows científicos, na área de Bioinformática, e por os dados da aplicação teste serem provenientes de experimentos do LABIO.
Além disso, conforme já apresentado no Karina, buscando a redução da quantidade total de experimentos a serem executados e, se possível, reduzindo o tempo total de processamento.
Apesar disso, com o padrão definido nesta Tese, busca- se a aplicação em diferentes áreas científicas, desde que com características semelhantes à Bioinformática.
Este capítulo apresentou trabalhos relacionados ao desenvolvido nesta Tese e, buscando sintetizar esses trabalhos, comparando- os com o desenvolvido na Tese, a Tabela 32 foi elaborada.
Analisando- se a Tabela 32, verifica- se o relacionamento entre os trabalhos pesquisados e o P-MIA, identificando suas principais características.
Os critérios de comparação apresentados nessa tabela, além de alguns serem conceitos amplamente difundidos na literatura, também envolvem características de áreas científicas.
São eles:
Padrões de Dados:
São identificados os trabalhos relacionados que utilizam conceitos de padrões de dados, que definem novos padrões ou que reconhecem sua importância, uma vez que o trabalho desenvolvido nesta Tese define um novo padrão de dados.
Padrões de Fluxo:
São identificados os trabalhos relacionados que utilizam conceitos de padrões de fluxos, que definem novos padrões ou que reconhecem sua importância, uma vez que o trabalho desenvolvido nesta Tese, apesar de não manipular especificamente padrões de fluxo, os utiliza como base para sua definição, como é o caso do padrão de fluxo desenvolvido por Nardi.
Paralelismo: São identificados os trabalhos que estudam conceitos de paralelismo aplicados a dados, independente da forma de aplicação, bem como os que reconhecem sua utilização.
Por mais que o padrão de dados desenvolvido nesta Tese não utilize explicitamente conceitos de paralelismo, o padrão pode ser implementado sobre esse conceito.
Execução em Grade:
São identificados os trabalhos que estudam conceitos específicos de execução em grade, sendo essa uma das possibilidades de manipulação de dados em paralelo.
O trabalho de Nardi é definido sobre esse conceito e, uma vez que se afirma que o padrão definido nesta Tese possui como base o trabalho por ele desenvolvido, esses conceitos relacionam- se.
Formalização em redes de Petri coloridas:
Todos os trabalham que mencionam a utilização de redes de Petri ou de redes de Petri coloridas aplicados à área científica são destacados, uma vez que seus conceitos são utilizados para a formalização do padrão definido nesta Tese.
Workflows científicos:
Tese desenvolvido especificamente para utilização em workflows científicos, por a característica do processo.
Grandes volumes de dados:
Uma das características dos workflows científicos é a manipulação de grandes volumes de dados e essa é, também, a principal característica das atividades desenvolvidas por o LABIO, quando se busca redução da quantidade de processamento e otimização dos processos.
Adaptação: Os conceitos de adaptação são importantes por uma das características do padrão:
Definir, dinamicamente, quais os próximos snapshots a serem executados.
De essa forma, esse critério relaciona- se com o padrão definido.
Nota- se, ao analisar a Tabela 32, que os diversos trabalhos contribuem para que o P-MIA agregue as características dos critérios definidos, uma vez que serviram como base para seu desenvolvimento.
De essa forma, atende- se à necessidade de uma área que trabalhe com padrões de dados, manipule grandes volumes de informações, crie e gerencie workflows científicos, possibilite processamento em paralelo, ou em grade, e trabalhe com conceitos de adaptação em tempo de execução.
A área que estuda workflows não é uma área recente.
Estudos têm sido realizados desde a década de 80 e, na década de 90, surgiu a necessidade de uma maior padronização e definição de tipos desses processos.
Este documento apresentou estudos sobre workflows de negócios, workflows científicos, padrões que direcionam a implementação de processos nesses sistemas, formalismos utilizados para a descrição dos processos, bem como a área de aplicação, que propiciou a realização dos experimentos, executados após a definição e formalização do padrão de dados P-MIA:
Padrão Múltiplas Instâncias Autoadaptáveis.
Além disso, facilita a representação de sistemas complexos, bem como do comportamento desses sistemas.
Em o Capítulo 3 a área de Bioinformática foi apresentada e as atividades realizadas por o LABIO.
Alguns padrões de dados definidos na literatura foram apresentados no Capítulo 4 e foram utilizados como base para a definição do padrão Múltiplas Instâncias Autoadaptáveis ­ P-MIA, definido nesta Tese de Doutorado, visando atender à necessidade de autoadaptação de instâncias em execução, com a manipulação de um grande volume de dados.
Conceitos de adaptação e autoadaptação foram discutidos por Hübler e Ruiz em e, por serem apenas aplicados na instanciação dos dados no padrão, não foram detalhados nesta Tese.
Em o Capítulo 5 a formalização do P-MIA foi apresentada, contendo as principais definições, bem como sua representação gráfica, realizada com a ferramenta CPN Tools.
Seu funcionamento foi detalhado no Capítulo 6, enfatizando as regras e características do padrão.
Os testes com dados reais foram detalhados no Capítulo 7 e os trabalhos relacionados foram confrontados com o trabalho desenvolvido nesta Tese no Capítulo 8.
A o se considerar que uma das principais características de áreas como a Bioinformática é a manipulação de grandes volumes de dados e o processamento desses dados no menor espaço de tempo possível, o P-MIA, apresentado neste documento, atende à questão de pesquisa previamente definida:
&quot;Como reduzir a quantidade de snapshots a serem processados, reduzindo o tempo total de processamento e procurando manter o mesmo nível de acerto na identificação de compostos promissores?».
O padrão definido atende à questão de pesquisa, ao propiciar que se alcance o objetivo geral definido em esta «Tese:
&quot;melhorar o tempo final de processamento, reduzindo a quantidade de experimentos de docagem molecular, com base nos resultados obtidos em tempo de execução».
O padrão manipula conjuntos de dados, previamente agrupados por similaridade e, com base em resultados obtidos por elementos desses conjuntos, define a execução ou não dos demais elementos.
De essa forma, nem todos os experimentos serão realizados, reduzindo o tempo final de processamento e mantendo um patamar de qualidade.
Quanto a os objetivos definidos como específicos:
Definir um padrão de dados capaz de ser utilizado por a área de Bioinformática e por outras áreas que apresentem características semelhantes:
O padrão definido P-MIA, formalizado no Capítulo 5 e detalhado no Capítulo 6, apesar de utilizar dados específicos de Bioinformática para a realização dos experimentos, apresentou características que podem ser facilmente aplicadas em outras áreas científicas.
Para essa comprovação, entretanto, seria interessante que experimentos com dados de outras áreas fossem realizados.
Definir uma função que não descarte dados que apresentem a probabilidade de serem promissores:
Esse objetivo foi alcançado ao se adaptar a função da Regra Empírica, denominada neste trabalho de Regra Empírica Adaptada, apresentada no Capítulo 6, servindo como subsídio para o cálculo da média estimada.
Com o valor gerado a partir de a média estimada, os snapshots não são descartados sem que apresentem a probabilidade de obterem bons resultados, pois a média estimada leva em consideração o desvio padrão dos resultados.
Reduzir a quantidade total de dados a serem processados:
O princípio fundamental do P-MIA é a redução da quantidade de snapshots a serem processados.
Os testes realizados no Capítulo 7 demonstram que se obtém um ganho, com a utilização do padrão, podendo chegar, quando a análise inicia aos 20% do processamento dos 43% de ganho.
Buscar manter a qualidade dos dados processados:
Com os testes realizados no melhores resultados de processamento foi processada, também, por meio de o PMIA.
Entretanto, acredita- se que esse objetivo seria alcançado com um índice ainda maior se o critério de similaridade, para o agrupamento dos dados, fosse específico para a realidade implementada.
Em este contexto, com a definição de um padrão de dados capaz de inferir maior velocidade de processamento aos experimentos, com a formalização do padrão e a descrição em detalhes do seu funcionamento e dos testes realizados, acredita- se que o P-MIA é uma contribuição interessante para a comunidade científica.
Como sugestões de trabalhos futuros:
Implementação do P-MIA:
Padrão Múltiplas Instâncias Autoadaptáveis, na forma de um componente, possibilitando sua utilização por diferentes aplicações de workflow científico.
Essa implementação é assunto em desenvolvimento na Dissertação de Mestrado de Fábio Frantz, em parceria com o LABIO.
Estudos de viabilidade de execução do padrão em diferentes ambientes de alto desempenho, junto ao Laboratório de Alto Desempenho da PUCRS:
Já em desenvolvimento por a Mestranda Renata de Paris, também em parceria com o Definição de uma função de similaridade que seja específica para a realidade estudada também tende a fornecer melhores resultados.
Essa função de similaridade já está em definição, sendo foco do trabalho de Doutorado de Karina Machado.
Assim, após a definição dessa função, novos testes devem ser realizados, buscando evidenciar o ganho ou não de processamento.
Aplicação do padrão com dados de outras áreas científicas, diferentes da Bioinformática, mas com características semelhantes.
