Com o avanço nos experimentos biológicos, a manipulação e análise do grande volume de dados sendo gerados por esses experimentos têm sido um dos desafios em bioinformática, onde uma importante área de pesquisa é o desenho racional de fármacos (RDD -- Rational Drug Desing).
A interação entre macromoléculas biológicas, chamadas de receptores, e pequenas moléculas, chamadas ligantes, é o princípio fundamental do RDD.
É em experimentos in silico de docagem molecular que se investiga o melhor encaixe e conformação de um ligante numa cavidade do receptor.
O resultado de um experimento de docagem pode ser avaliado a partir de um valor contínuo de energia livre de ligação (FEB -- Free Energy of Binding).
Tem- se empregado esforços em minerar dados de resultados de docagem molecular, com o objetivo de selecionar conformações relevantes para reduzir o tempo de futuros experimentos de docagem.
Em esse sentido, foi desenvolvido um repositório para armazenar todos os dados a respeito desses experimentos, em nível de detalhe.
Com esse repositório, os dados foram devidamente pré-processados e submetidos a diferentes tarefas de mineração de dados.
De entre as técnicas aplicadas, a que apresentou- se mais promissora para o tipo de dados sendo utilizado foi árvore de decisão para regressão.
Apesar de os resultados alcançados por esses experimentos serem promissores, existem algumas propriedades nos experimentos que dificultam a efetiva seleção de conformações.
De essa forma, propõe- se uma estratégia que considera as propriedades tridimensionais (3 D) do receptor para predizer o valor de FEB.
Assim, nesta Tese é apresentado o 3 D-Tri, um algoritmo de indução de árvore de regressão que considera essas propriedades 3 D, onde essas propriedades são definidas como atributos no formato x, y, z.
O algoritmo proposto faz uso dessas coordenadas para dividir um nodo em duas partes, onde o átomo sendo testado para o nodo é avaliado em termos de sua posição num bloco que melhor represente sua posição no espaço, onde i indica a posição inicial de uma coordenada, e f indica a posição final.
O modelo induzido pode ser útil para um especialista de domínio para selecionar conformações promissoras do receptor, tendo como base as regiões dos átomos que aparecem no modelo e que indicam melhores valores de FEB.
Palavras-chave: Mineração de Dados, Propriedades Tridimensionais, Docagem Molecular, Receptor Flexível, Dinâmica Molecular.
3 D-Tri:
Mineração de dados, de acordo com Tan É um processo de descoberta de padrões úteis em grandes repositórios de dados.
Essa é uma das etapas do processo de descoberta de padrões.
Em problemas preditivos de aprendizado de máquina existem basicamente um conjunto de dados de entrada e uma saída onde a tarefa é aprender como mapear o conjunto de entrada para a saída.
Ainda que existam diversos algoritmos desenvolvidos para atender problemas de predição, muitos desses apenas constroem uma função preditiva que indica a qual valor alvo os objetos minerados pertencem.
Entretanto, em alguns problemas de mineração de dados se faz necessário entender o modelo induzido.
Conforme Freitas, apesar de a falta de consenso na literatura em mineração de dados a respeito de as tarefas que produzem resultados mais compreensíveis, existe um acordo que representações na forma de árvore de decisão e conjuntos de regras podem ser melhor compreendidos por usuários finais do que representações do tipo caixa-preta, como SVM (Support Vector Machine) ou Redes Neurais.
Árvores de decisão têm a vantagem de representar o conhecimento descoberto na forma de um grafo, sendo que sua estrutura hierárquica é capaz de apontar a importância dos atributos utilizados para predição.
Existem várias áreas de aplicação onde a construção de um modelo compreensível se faz necessário.
Em bioinformática, apenas um conjunto de dados e um conjunto de resultados provenientes da execução de algoritmos de mineração de dados podem não ser suficientes.
É preciso que tanto os dados quanto os resultados obtidos representem de forma satisfatória o contexto ao qual eles fazem parte, de modo com que um especialista de domínio possa utilizar e criticar o modelo.
Este trabalho está inserido no contexto de Desenho Racional de Fármacos (RDD -- Rational Drug Design), onde o princípio fundamental diz respeito à interação entre macromoléculas -- chamadas receptores -- e pequenas moléculas -- chamadas de ligantes.
É nos experimentos in-silico de docagem molecular que se investiga e avalia a melhor ligação de um determinado ligante nas diferentes conformações que um dado receptor pode ter.
Tal ligação é avaliada através de uma medida chamada pode ser gerado.
O processo de docar o ligante na estrutura alvo não é uma tarefa trivial.
Um dos fatores que influência nos resultados é a flexibilidade do receptor.
Apesar disso, a maioria dos algoritmos que executam docagem molecular somente considera a flexibilidade do ligante, considerando o receptor como uma estrutura rígida.
De entre vários trabalhos que incorporam a flexibilidade do receptor, nesta Tese utiliza- se uma série de experimentos de docagem molecular, considerando em cada experimento uma conformação do receptor gerada por uma simulação de dinâmica molecular (DM).
Motivação O principal problema na utilização de uma trajetória por DM refere- se ao tempo necessário para a execução de todos os experimentos, bem como à grande quantidade de dados gerados.
O nosso interesse no desenvolvimento deste trabalho está em minerar dados de resultados de experimentos de docagem molecular, a fim de obter modelos que auxiliem na seleção de conformações promissoras do receptor para futuros experimentos de docagem molecular e, assim, contribuir para a redução no tempo desses experimentos.
Para tanto, foram empregados esforços em construir um processo completo de KDD para tratar o grande volume de dados envolvido em seus diferentes tipos.
Desta forma, foi desenvolvido um repositório de dados suficientemente abrangente para armazenar os diferentes dados envolvidos em experimentos de docagem molecular.
De posse desse repositório, foi possível aplicar estratégias de pré-processamento sobre os dados armazenados e submeter- los a diferentes tarefas de mineração, como regras de associação, árvores de decisão para classificação e árvores de decisão para regressão.
De entre as técnicas aplicadas, os resultados que se mostraram mais promissores foram obtidos a partir de árvores de decisão para regressão.
No entanto, apesar desses trabalhos mostrarem resultados significativos, acredita- se que eles servem como base e motivação para evoluir a abordagem sendo aplicada e, assim, produzir modelos melhores e mais bem acurados.
A abordagem desenvolvida inicialmente utiliza como dados de entrada as diferentes conformações do receptor e a distância dos átomos do resíduo desse receptor em relação a os átomos de um dado ligante.
Como modelo de saída, são induzidas árvores que mapeiam resíduos do receptor, indicando qual o melhor intervalo de distância em relação a o ligante que podem produzir melhores valores de FEB.
Em direção a a uma nova estratégia de mineração neste contexto, busca- se considerar como dados de entrada as propriedades tridimensionais de cada átomo do receptor para predizer um dado valor de FEB, para um dado ligante.
Em esse sentido, este trabalho apresenta um novo algoritmo de indução de árvore de regressão capaz de identificar as propriedades tridimensionais inerentes ao problema e induzir uma árvore que indique as melhores posições no espaço Euclidiano de determinados átomos que possam resultar em importantes resultados de FEB e, assim, contribuir para a efetiva seleção de conformações do receptor.
Objetivos O objetivo desta Tese é definir o algoritmo 3 D-Tri (Three-Dimensional Regression Tree Induction Algorithm), um novo algoritmo de indução de árvore de regressão para propriedades tridimensionais, onde o algoritmo seja capaz de ler um conjunto de dados provenientes de resultados de simulação por DM, tendo como instâncias as diferentes conformações de um receptor, como atributos preditivos as coordenadas espaciais dos átomos deste receptor, e como atributo alvo o valor de FEB.
Além de apresentar uma maneira de interpretar os atributos do arquivo de entrada como parte de coordenadas num espaço euclidiano, este algoritmo também se destaca na forma de indução desses atributos, introduzindo uma abordagem de definição de um intervalo ideal para cada coordenada dos átomo envolvidos.·
Criar uma rotina de pré-processamento dos dados de simulação por dinâmica molecular e experimentos de docagem molecular para gerar um conjunto de dados contendo as coordenadas espaciais dos átomos para cada conformação do receptor e os respectivos valores de FEB para um dado ligante;·
Desenvolver uma estratégia de identificação de um intervalo com pontos iniciais e finais das coordenadas de cada átomo, cuja estrutura pode ser graficamente representada na forma de um bloco;·
Estabelecer uma abordagem de indução de árvores binárias para o intervalo, ou bloco, identificado para cada átomo;·
Definir um algoritmo de indução de árvore de regressão que utilize as propriedades citadas nos objetivos descritos anteriormente, e mostrar a qualidade dos mesmos.
Organização da tese Esta Tese está organizada conforme segue:·
O Capítulo 2 apresenta conceitos de mineração de dados, com foco nos algoritmos tradicionais de indução de árvores de decisão, tanto para classificação quanto para regressão.
São apresentadas as principais estratégias empregadas por cada algoritmo para indução de uma árvore e a avaliação dos modelos induzidos;·
Em o capítulo 3 são apresentados os conceitos de RDD, enfatizando a docagem molecular e a dinâmica molecular.
Ainda neste capítulo são apresentadas a proteína alvo, bem como os ligantes utilizados em todos os experimentos desta Tese;·
O capítulo 4 apresenta um repositório de dados chamado FReDD, desenvolvido para o armazenamento de todos os dados relacionados aos experimentos de docagem molecular, bem como os testes deste repositório com os dados do receptor e dos ligantes utilizados nesta Tese.
Além disso, esse capítulo mostra como esse repositório facilita o pré-processamento dos dados, apresentando uma rotina de pré-processamento dos dados em ele armazenado;·
Em o capítulo 5 é apresentado os experimentos com mineração de dados sobre os dados préprocessados a partir de o FReDD.
São realizados experimentos com regras de associação, árvores de decisão para classificação e para regressão.
Para cada uma dessas técnicas utilizadas é mostrada uma nova rotina de pré-processamento, a partir de os dados já pré-processados a partir de o FReDD, atendendo aos objetivos de cada tarefa de mineração;·
O capítulo 6 introduz o algoritmo 3 D-Tri, apresentando as estrategias de pré-processamento dos dados, de definição do bloco para cada átomo e de indução de uma árvore de regressão para propriedades tridimensionais;·
Em o capítulo 7 é apresentado o teste do algoritmo proposto;·
O capítulo 8 descreve os trabalhos relacionados à esta Tese;·
Em o capítulo 9 são apresentadas as conclusões desta Tese, com sugestões para trabalhos futuros;·
Por fim são apresentadas as referências bibliográficas.
O processo de KDD apresentado por Fayyad É uma sequência de passos interativos e iterativos de apoio à tomada de decisão, principalmente quando de a existência de grandes volumes de dados.
Em a visão de Han e Kamber, o processo de KDD envolve os seguintes principais itens:
Os modelos induzidos por a mineração apresentam padrões que, após analisados, podem permitir atingir o conhecimento esperado.
Embora cada etapa do processo de KDD seja suficientemente abrangente para ser tratada de forma isolada, nota- se uma forte relação de dependência entre elas.
Este capítulo discorre sobre as etapas do processo de KDD, dividindo as mesmas em dois grandes grupos:·
Construção de um repositório alvo, abrangendo as etapas (a), (b), (c) e (d);·
Técnicas de mineração de dados, composta por as etapas (e), (f) e (g).
Repositório alvo A construção de um processo de KDD se dá, geralmente, quando há interesse em analisar distintas fontes de dados, as quais apresentam seus dados, na maioria das vezes, de forma heterogênea.
A ideia de construção de um repositório alvo é para que esses dados sejam armazenados num ambiente semanticamente consistente.
Em esse sentido, a heterogeneidade dos dados de origem é tratada de forma a garantir a integração dos dados num único formato.
Planejar, modelar e construir esse repositório é uma tarefa que requer conhecimento do domínio dos dados em questão.
Além disso, a forma como esse repositório é modelado também depende do contexto e das necessidades de aplicação desse repositório.
Han e Kamber sugerem que este repositório alvo seja construído na forma de um data warehouse (DW), buscando manter um histórico organizado dos registros, de modo a auxiliar a tomada de decisão.
Um DW é construído de forma a satisfazer sua estrutura multidimensional.
Um modelo analítico, que comporta essa estrutura multidimensional, possui dois tipos de tabela:
Fato e dimensão.
Entende- se por dimensão as perspectivas de uma base de dados que possam gerar registros referentes às características do problema modelado.
Essas são tipicamente organizadas em torno de um termo central, ou tabela fato, a qual contém os atributos chave das dimensões e atributos que representam valores relevantes ao problema.
Alternativamente, o repositório pode ser modelado na forma de Entidade-Relacionamento (Er) e, mesmo assim, conter todos os dados relevantes organizados e de fácil acesso a consulta.
Han e Kamber dizem que os dados armazenados num modelo relacional são muito utilizados para mineração por ser o tipo de repositório mais rico em dados e com maior disponibilidade.
Tendo o repositório alvo modelado, implementado e com os dados devidamente armazenados, já é possível realizar consultas analíticas.
Um exemplo, para o caso de um DW, é a manipulação de seus dados por ferramentas OLAP (On-Line Analytical Processing).
Operações como pivoteamento de dados, roll-up, drill-down e slice&amp; Dice são operações típicas de ambientes de processamento analítico de dados.
Para o caso de um repositório Er, consultas podem ser realizadas sobre esses dados a fim de extrair algum tipo de informação.
Além de essas abordagens, e seguindo as demais etapas do processo de KDD, os dados uma vez armazenados estão aptos a serem utilizados por algoritmos de mineração de dados, podendo ser pré-processados para atender às necessidades da tarefa de mineração a ser empregada.
Embora os dados armazenados no repositório já possam ser minerados, é interessante que os mesmos passem por uma etapa de pré-processamento.
Para Tan Técnicas de préprocessamento devem ser aplicadas para que os dados se tornem mais adequados para a mineração de dados.
Mesmo que essa seja uma etapa trabalhosa e que demanda boa parte do tempo consumido durante todo o processo de KDD, este esforço merece especial atenção por contribuir fortemente para o grau de confiança e qualidade dos resultados obtidos por os diferentes algoritmos de mineração empregados.
De entre diferentes técnicas de pré-processamento, Tan Destacam:·
Agregação: Essa técnica tem por objetivo sumarizar os dados a partir de diferentes perspectivas, através da combinação de um ou mais valores num único objeto, reduzindo o espaço a ser minerado;·
Amostragem: Em essa técnica, um conjunto de atributos são selecionados de maneira aleatória para serem analisados;·
Redução de dimensionalidade:
Com a redução de dimensionalidade busca- se diminuir o número de atributos a serem analisados, seguindo alguma estratégia que pode estar contida nas demais aqui listadas;·
Seleção de atributos:
Essa técnica visa eliminar atributos que possam ser redundantes ou irrelevantes ao objetivo da mineração, de modo com que o subconjunto de dados selecionado seja tão representativo ou até mesmo melhor do que seria o conjunto de dados original;·
Criação de atributos:
Técnica que consiste em criar atributos, tendo como base os dados dos atributos já existentes, de modo que, ao mesmo tempo em que o escopo é reduzido, percebe- se uma melhora na qualidade dos dados;·
Discretização: Em esta técnica os atributos contínuos são transformados em intervalos que representam classes categóricas;·
Transformação de variáveis:
Utilizada para modificar os objetos de um determinado atributo, tendo como base uma mesma regra como, por exemplo, a normalização dos valores.
Essas técnicas podem ser combinadas entre si para que um arquivo de entrada seja adequadamente produzido e, assim, permitir obter melhores resultados com os experimentos de mineração de dados.
Mineração de dados Mineração de dados é a etapa do processo de KDD que converte dados brutos em informação.
A mineração de dados pode ser dividida em dois tipos de tarefas:
Descritivas e preditivas.
Tarefas descritivas sumarizam relações entre dados, tendo como objetivo melhorar o conhecimento a seu respeito.
Tarefas preditivas buscam apontar conclusões a respeito de os dados analisados, predizendo um dado atributo de interesse.
De entre as tarefas descritivas, pode- se citar regras de associação e agrupamento.
Já algoritmos de classificação e regressão são exemplos de tarefas preditivas.
Para o melhor entendimento do trabalho realizado nesta tese, este capítulo mostra os algoritmos para essas diferentes tarefas.
Para o caso de tarefas preditivas, esse capítulo focaliza em algoritmos de indução de árvores de decisão, seja para classificação ou para regressão.
Algoritmos de regra de associação ocupam- se de identificar relações entre itens frequentes, onde uma regra de associação é uma implicação na forma X æ Y onde X e Y são conjuntos não vazios.
A o minerar itens frequentes, busca- se por relações recorrentes num mesmo conjunto dá- se por:
X µ I, Y µ I e X fl Y $ ,.
Há duas medidas tipicamente utilizadas para medir o quão interessante é uma regra encontrada:
Suporte e confiança.
Essas regras dizem respeito à utilização e à certeza da regra, respectivamente, onde:
Suporte (X æ Y) $= P (X fi Y) Conf iança (X æ Y) $= P (Y| X) Assim, tendo um conjunto de transações T, o algoritmo busca encontrar todas as regras onde Suporte Ø SuporteM inimo e Conf iança Ø Conf iançaM inima, sendo que SuporteM inimo e Conf iançaM inima são seus respectivos limites parametrizados.
A construção de regras de associação pode ser vista como dois principais passos:·
Geração de itens frequentes:
Esse passo busca encontrar todos os itens que satisfazem o limite de SuporteM inimo.
A esse conjunto de itens dá- se o nome de itens frequentes;·
Geração de Regras: Em essa etapa o objetivo é extrair as regras que satisfaçam o limite da Conf iançaM inima a partir de os itens frequentes encontrados no passo anterior.
O algoritmo de regra de associação mais utilizado é o Apriori, proposto por Agrawal.
Este algoritmo segue o princípio de que se um conjunto de itens é frequente, todos os subconjuntos desse item também o são.
Algoritmos de agrupamento, ou segmentação, tem por objetivo agrupar objetos em classes com objetos similares.
Em esse sentido, um agrupamento é uma coleção de objetos similares entre si, e diferentes de objetos pertencentes a um outro agrupamento.
Trata- se de um método não supervisionado de aprendizagem, onde os exemplos não estão associados a uma categoria, ou atributo de interesse.
Uma das maneiras mais simples de medir a similaridade entre objetos é através da distância entre padrões de um objeto.
Para objetos cujas características são numéricas, a distância pode ser medida por a distância Euclidiana entre dois pontos num espaço de multidimensional.
Ainda, existem métodos que fazem uso de probabilidades para medir essa similiaridade.
Assim, um conjunto inicial de objetos é selecionado para integrar cada agrupamento, e os demais objetos são agregados com base em um cálculo de probabilididade das características desse objeto serem similares com o grupo ao qual está- se agregando.
De entre diferentes algoritmos de agrupamento, este capítulo concentra- se no K--means.
Esse é um algoritmo baseado em protótipo, que tenta encontrar um número específico de grupos (k), os quais são representados por seus protótipos.
Em conjuntos de dados numéricos o protótipo se dá na forma de um centróide, o qual é a média de todos os pontos do grupo.
Quando o centróide não é significativo, ou seja, quando há atributos categóricos no conjunto de dados, o protótipo é definido por o medóide.
Um medóide é o ponto mais representativo no grupo.
O K--medóides, como é chamado o algoritmo para o caso do protótipo ser um medóide, é comumente aplicado para conjuntos de dados científicos.
O pseudo-código do K--means está representado no Algoritmo 2.1.
Algoritmo 2.1: Pseudo-código do K--means.
Adaptado de e selecione k pontos como sendo os centróides iniciais repita Compute k agrupamentos atribuindo objetos que sejam o mais próximo de o seu centróide Recompute o centróide de cada cluster até que os centróides não mudem Tarefas preditivas de mineração de dados buscam construir modelos que apresentem a melhor combinação de relacionamentos entre um conjunto de atributos, denominados atributos preditivos, em função de um dado atributo de interesse, ou atributo alvo.
Em predição existem basicamente um conjunto de dados de entrada x e uma saída y, onde a tarefa é aprender como mapear o conjunto de entrada para a saída.
Esse mapeamento pode ser definido como uma função y $= g (x|) onde g(.)
é o modelo e são os seus parâmetros.
Algoritmos de predição podem ser aplicados tanto para classificação quanto para regressão.
A classificação se dá quando o conjunto de dados a ser minerado possui como atributo alvo valores categóricos, ou seja, valores que podem ser separados em classes que descrevem os atributos preditivos.
Já algoritmos de regressão são aplicados onde o atributo alvo de um conjunto de dados é numérico.
Ainda que existam diferentes tipos de algoritmos utilizados para predição, muitos de eles apenas constroem uma função preditiva que rotula o atributo alvo dos objetos minerados.
Esse é o caso de algoritmos desenvolvidos na forma de uma caixa-preta, como redes neurais e SVM.
Freitas Argumenta que apesar de a falta de consenso na literatura de mineração de dados a respeito de algoritmos que produzem resultados mais compreensíveis, existe um acordo de que representações na forma de árvores de decisão e conjuntos de regras podem ser melhor compreendidos por usuários finais do que representações do tipo caixa-preta.
Árvores de decisão podem ser vistas como um dos métodos mais utilizados para inferência indutiva, onde a indução é feita a partir de um conjunto de dados rotulados, ou seja, que contenham um valor como atributo alvo.
Uma árvore de decisão é uma estrutura de dados hierárquica implementada a partir de uma estratégia de dividir para conquistar.
A vantagem de uma árvore de decisão é que ela representa o conhecimento descoberto na forma de um grafo, sendo que sua estrutura hierárquica é capaz de apontar a importância dos atributos utilizados para predição.
Alpaydim explica que uma árvore de decisão é um modelo hierárquico de aprendizagem supervisionada, onde regiões locais são identificadas em sequências recursivas de divisões do conjunto de dados.
Uma árvore é composta por nodos de decisão internos e por nodos terminais, os quais são chamados de folha.
Cada nodo m implementa uma função de teste fm com resultados discretos que servem para rotular as arestas.
Assim, a partir de um conjunto de dados de entrada, um teste é aplicado para cada nodo e uma das arestas é percorrida, dependendo do resultado de rótulo.
Esse processo inicia num nodo denominado raiz e é recursivamente repetido até atingir um nodo folha, o qual descreve a saída, ou atributo alvo.
Esse é um método não-paramétrico eficiente tanto para classificação quanto para regressão.
A maioria dos algoritmos de árvores de decisão faz uso de uma estratégia gulosa para a indução da árvore, onde a partição de um nodo é feita com base em um parâmetro que identifica o ótimo local para este nodo.
O algoritmo básico de indução de árvore é o algoritmo de classificação de Hunt, o qual serviu de base para a construção de algoritmos clássicos de indução de árvore de decisão, como o ID3, C4.
5 e CART.
A estratégia do algortimo de Hunt é dada como segue:
A Figura 2.1 ilustra um conjunto de dados (à esquerda) e sua correspondente árvore de decisão (à direita).
Esse conjunto de dados é composto por uma série de dados rotulados em C1, para o caso dos círculos, e C2, para o caso dos retângulos.
Para a indução de modelos bem acurados, algoritmos de indução de árvore devem abordar algumas questões importantes, Tan Indica as duas mais importantes como:·
Qual a melhor maneira de particionar os atributos?
Cada passo recursivo do crescimento da árvore demanda a seleção de um atributo para que, a partir de um teste de condição desse atributo, o conjunto de dados seja dividido em subconjuntos.
Para tanto, o algoritmo precisa implementar um método que avalie qual o melhor atributo a ser utilizado no momento, em busca de um ótimo local;·
Como o procedimento de partição dos atributos deve parar?
É preciso estabalecer uma condição de parada de crescimento da árvore.
Uma estratégia possível é seguir expandindo até todos os exemplos pertencerem ao mesmo atributo alvo ou todos os exemplos terem atributos de igual valores.
Apesar disso, outras estratégias devem ser analisadas para que o procedimento de crescimento termine antes, sempre visando alguma vantagem em relação a o modelo.
Questões de particionamento de atributos e critério de parada devem ser implementadas de acordo com o objetivo do algoritmo e tipos de dados envolvidos.
Classificação é o processo de encontrar um modelo ou função que descreva e diferencie classes de um determinado conjunto de dados, com o propósito de utilizar esse modelo para predizer classes de objetos cuja categoria é desconhecida.
Tan et al complementa que esta é a tarefa de aprender uma função f que mapeia cada conjunto de atributos x à uma das y classes pré-definidas.
Em este caso, os dados de entrada podem ser descritos por um par de atributos (X, y) sendo que X é um vetor que representa o conjunto de atributos preditivos, onde X $ , e y é o rótulo da classe a qual esse exemplo pertence.
Algoritmos de indução de árvore de decisão para a classificação que usam uma estratégia gulosa têm o mesmo princípio do algoritmo de Hunt, apresentando na Seção 2.2.3.1.
O algoritmo, entretanto, implementa funções que buscam responder às questões de particionamento e critério de parada.
Existem diferentes medidas para selecionar o melhor particionamento, onde tais medidas são definidas em termos de a distribuição das classes em relação a os exemplos, antes do particionamento.
Essa medida calcula o grau de impureza do particionamento.
Isto é, um particionamento é puro se, para todas as arestas, todas as instâncias de uma mesma aresta pertecencem à mesma classe.
Em esse sentido, assume- se que c seja o número de classes e p (i| t) seja a fração de exemplos que pertencem à classe i para um dado nodo t, busca- se encontrar o menor grau de impureza para esse particionamento, onde exemplos de medidas de impureza do nodo são Entropia e Gini.
Entropia (t) $= c1 p (i| t) log2 p (i| t) i $= 0 Gini (t) $= 1 c1 i $= 0 Dadas essas medidas, é possível calcular quão bem um dado teste ocorreu.
Para tanto, é calculado o total de impureza após a partição, ou seja, o ganho de informação da partição.
GanhoInf o $= I (nodo) j $= 1 N (vj) I (vj), onde I(.)
é o cálculo da medida de impureza utilizada, N é o número total de exemplos, k é o número de atributos sendo utilizados e vj é um nodo filho do nodo sendo avaliado.
O critério de parada do algoritmo é geralmente implementado na forma de um limiar do grau de pureza I(.)
calculado, onde esse limiar é definido como I.
Por esse limiar entende- se que o algoritmo não pára de induzir a árvore apenas quando a impureza seja exatamente 0 ou 1, mas quando ela está perto o suficiente, conforme limiar parametrizado.
Dadas essas medidas, um algoritmo de indução de árvore de decisão para classificação pode ser implementado conforme ilustra o Algoritmo 2.2.
A qualidade dos modelos induzidos podem ser avaliadas a partir de diferentes métricas.
De entre elas pode- se citar:·
Acurácia;· Medida-F;·
Tamanho da árvore.
A acurácia de um modelo representa quão satisfatória foi a classificação.
Uma das maneiras de avaliar a qualidade é fazer a indução do modelo a partir de um método denominado validação cruzada.
Por esse método, conjunto de dados é dividido em n partições, onde utiliza- se n 1 para treino e 1 para teste, n vezes.
Logo, é possível determinar as instâncias que foram preditas corretamente, onde tem- se:·
Número de instâncias classificadas como verdadeiro positivo (Vp);·
Número de instâncias classificadas como verdadeiro negagtivo (VN);·
Número de instâncias classificadas como falso positivo (FP);·
Número de instâncias classificadas como falso negativo (FN).
Assim, a acurácia do modelo é calculada conforme Equação 2.6.
O resultado da acurácia diz respeito uma taxa de acerto, a qual compreende uma faixa de 0 a 100%, onde quanto mais próximo de 100%, melhor.
Algoritmo 2.2: Arvore de decisão para classificação, adaptado de.
Procedure GeraArvore 1: Se Impureza I então Cria folha rotulada com a maioria das classes em X retorna 4: Fim se 5: I P articionaAtributo 6: Para cada ramo de xi faça Encontre Xi seguindo no ramo GeraArvore (Xi) 9: Fim para Procedure ParticionaAtributo 10: M inImpureza M AX se xi é discreto com n valores então e GanhoInf o se e M inImpureza então M inImpureza e senão M elhorP articão i fim se senão para todos partições possíveis faça Particiona X em X1, X2 sobre xi e GanhoInf o se e M inImpureza então M inImpureza e senão M elhorP articão i fim se fim para fim se 31: Fim para 32: Retorna M elhorP articão Acurácia $= P rediçõesCorretas T otaldeP redições Outra maneira de avaliar modelos de classificação é com uma métrica denominada medida-F. Essa faz uso de duas outras medidas denominadas precisão e revocação.
De acordo com Han &amp; Kamber, essas medidas são assim calculadas:
P recisão $ | Relevantes fl Recuperadas| Recuperadas| Revocação $= Onde entende- se por:·|
Relevantes fl Recuperadas|:
As instâncias que foram recuperadas corretamente (Vp);·|
Recuperadas|: Total de instâncias recuperadas, sejam elas corretas ou não (Vp+ FP);·|
Relevantes|: Todas as instâncias que foram recuperadas corretamente, mais as instâncias que não foram recuperadas mas que deveriam ter sido (Vp+ FN).
A medida-F é uma composição dessas duas métricas, conforme ilustra Equação 2.9.
O resultado apresenta uma valores entre 0 e 1, onde os valores melhores são aqueles mais próximos de 1.
M edida F $= P recisão Revocação (P recisão+ Revocação)/ 2 Por fim, uma outra métrica para avaliar não a qualidade do modelo, mas a compreensibilidade do mesmo, é o tamanho da árvore.
Essa diz respeito à profundidade da árvore, ou seja, quantos níveis existem até o nodo folha mais profundo.
Regressão é um modelo de predição em que o atributo alvo é contínuo.
Essa é a tarefa de aprender uma função alvo f que mapeia cada conjunto de atributos X para uma saída de valores contínuos y.
Árvores de decisão para predição numérica são chamadas de árvores de decisão para regressão, quando a função f mapeada para cada nodo folha contém a média dos valores de y para os exemplos que compõe uma folha.
Além de árvores de regressão também existem as árvores que rotulam cada nodo com um modelo de regressão linear, sendo essa técnica chamada de árvores modelo.
Segundo Witten et al e Alpaydim, tanto árvores de regressão quanto árvores modelo seguem o mesmo princípio de indução de árvores para classificação, mudando o foco do método de particionamento e critério de parada.
Um dos algoritmos que implementa árvores de regressão e árvores modelo, e cujos critérios são utilizados para esta Tese, é o M5P.
O maior objetivo deste algoritmo é maximizar a redução do desvio padrão (RDP), considerando o desvio padrão dos exemplos no conjunto de dados dp.
RDP $= dp ÿ| Xi| dp (Xi) É por o valor de RDP que o algoritmo particiona os atributos:
Para cada ciclo recursivo da indução, o atributo escolhido é aquele que apresenta o maior valor de RDP.
Com relação a o critério de parada, este considera limiares do número de exemplos em X, bem como um limiar em relação a uma taxa do dp.
O pseudo-código adaptado do M5P está descrito no Algoritmo 2.3.
Algoritmo 2.3: Arvore de decisão para regressão, adaptado de e.
Procedure GeraArvore 1: Se DP não foi calculado então DP dp 3: Fim se 4: Se Número de exemplos em X 4 ou dp 0.05 DP então Cria folha rotulada com a média dos valores de y em X retorna 7: Fim se 8: I P articionaAtributo 9: Para cada ramo de xi faça Encontre Xi seguindo no ramo GeraArvore (Xi) 12: Fim para Procedure ParticionaAtributo Calcula RDP se xi é discreto com n valores então M elhorP articão i com maior valor de RDP senão para todos partições possíveis faça Particiona X em X1, X2 sobre xi M elhorP articão i com maior valor de RDP fim para fim se 24: Fim para 25: Retorna M elhorP articão O objetivo da regressão é induzir um modelo tal que a sua função f minimize um erro.
Típicas funções de erro para tarefas de regressão são Erro Médio Absoluto (MAE -- Mean Absolute Error) e Erro Médio Quadrático (RMSE -- Root Mean Squared Error).
Essas duas medidas são calculadas conforme segue, onde p é o valor predito e a é o valor real.
Essas duas medidas retornam valores numa faixa de 0 a 1, sendo que quanto mais próximo de 0, melhor o resultado.
Ainda, é possível medir a correlação estatística entre a e p, cujo cálculo está apresentado na Equação 2.13.
Esses valores variam entre 0 e 1, onde 1 apresenta uma correlação perfeita e 0 a ausência de correlação.
Além disso, o valor 1 também é válido, o qual apresenta uma correlação inversa perfeita.
Correlação $= Ô i (pi p) (ai a) n1 i (pi p) n1, e SA $= i (ai a) n1 Considerações do capítulo Este capítulo discorreu sobre o processo de KDD, enfatizando as etapas de construção de um repositório alvo, de pré-processamento e de mineração de dados.
Com relação a a construção de um repositório foi relatado que esse pode ser construído tanto na forma de um DW como na forma de um modelo relacional.
Esse repositório serve de base para que diferentes técnicas de préprocessamento sejam aplicadas para atender a algum objetivo de mineração de dados.
Com relação a a etapa de mineração de dados, apresentou- se os conceitos de algoritmos descritivos e preditivos, onde o primeiro sumariza dados e apresenta relações entre eles, e o segundo constrói um modelo que apresente a melhor combinação de relacionamentos entre atributos preditivos em relação a um atributo alvo.
Para tarefas descritivas apresentou- se os algoritmos de regras de associação e algoritmos de agrupamento.
Já para tarefas preditivas enfatizou- se árvore de decisão, explicando como as mesmas são geradas tanto para classificação como para regressão.
Inicialmente a bioinformática era definida como uma área interdisciplinar envolvendo biologia, ciência da computação, matemática e estatística para analisar dados biológicos.
Com o advento da era genômica, bioinformática passou a ser definida em termos de moléculas e a aplicação da computação para entender e organizar informação associada a esses dados biológicos em larga escala.
Para Lesk, uma das principais características dos dados de bioinformática é o seu grande volume.
Bancos de dados biológicos não são apenas extensos, mas crescem a uma taxa bastante elevada.
Esse grande volume de dados e seu crescimento definem os objetivos da bioinformática:
De entre as diversas áreas de atuação em bioinformática, Lesk aponta algumas frentes como:
Genômica, proteômica, alinhamentos de árvores filogenéticas, biologia de sistemas e descoberta de fármacos.
Esta Tese está inserida no contexto de Desenho Racional de Fármacos (RDD -- Rational Drug Design).
O princípio fundamental do RDD é a interação entre receptores e ligantes.
Ligantes são definidos como moléculas que se ligam a outras moléculas biológicas, chamadas receptores, para realizar ou inibir funções específicas.
É na docagem molecular que se investiga e avalia o melhor encaixe do ligante no receptor.
Um dos maiores desafios dessa área de pesquisa é lidar com o grande volume de dados envolvidos, como catalogação de ligantes, conformações do receptor obtidas por simulações por a dinâmica molecular (DM) e resultados de experimentos de docagem molecular.
Para a execução de docagem molecular é necessário um receptor, um ligante e um software para executar as simulações.
Em esse trabalho considera- se como receptor a enzima InhA do Mycobacterium tuberculosis (Mtb);
Como ligantes, NADH, TCL, Este capítulo discorre sobre RDD, em especial sobre os experimentos de docagem molecular e aquisição de dados com as moléculas sendo utilizadas.
Desenho racional de fármacos A indústria farmacêutica encontra- se constantemente sob pressão para aumentar a taxa com que novos medicamentos são inseridos no mercado.
Hoje em dia, o tempo para que um novo fármaco seja disponibilizado para comercialização é de 10 a 15 anos, e os custos associados são estimados em 1,2 bilhões de dólares.
Por essas razões, existem diversos esforços sendo aplicados para tentar reduzir tanto o tempo como o custo e, ao mesmo tempo, aumentar a qualidade dos compostos candidatos a fármacos.
Avanços em biologia molecular, modelagem computacional e ferramentas de simulações tem apresentado um impacto positivo no processo de planejamento de fármacos, tornando viável a aplicação de RDD.
A abordagem in-silico para o RDD é um processo que combina informação estrutural e esforços computacionais baseados no entendimento da interação entre uma proteína alvo, ou receptor, e diferentes ligantes, ou pequenas moléculas.
O RDD é um ciclo que combina quatro etapas:
A interação entre moléculas é o princípio do desenho de fármacos.
É na docagem molecular que se investiga e avalia o melhor encaixe do ligante na estrutura alvo ou receptor.
De acordo com Lybrand, um ligante deve interagir com um receptor para exercer uma função fisiológica vinculada à ligação dessa com outras moléculas, e essas ligações determinam se as funções do receptor serão estimuladas ou inibidas.
Essas ligações ocorrem em locais específicos, chamados sítios ativos ou de ligação.
A associação entre duas moléculas no sítio de ligação não depende somente do encaixe.
Existe a necessidade de haver uma energia favorável para que essa interação ocorra.
Essa energia é determinada por a carga e tamanho dos átomos em ele contidos.
Essas ligações que ocorrem entre os átomos são medidas por a quantidade de energia despendida, sendo que quanto mais negativa, melhor a interação entre as moléculas.
Conforme Jeffrey, a maior distância que permite um contato significante entre átomos do receptor e do ligante, para que haja energia liberada, é de 4.0 Å.
Uma das maneiras de avaliar um resultado de docagem é através do valor estimado da energia ligação de uma conformação de um ligante num receptor, aplicando uma técnica que combina um algoritmo de busca de conformações com um método baseado em grade para avaliação da energia.
Esse método baseado em grade é executado por o módulo AutoGrid do Autodock3.
0.5. Esse módulo pré-calcula uma grade tridimensional de energias de interações, tendo como base as coordenadas do receptor e do ligante.
Um exemplo de grade utilizada neste trabalho está descrito na Figura 3.1, considerando o receptor InhA e o ligante PIF.
A maioria dos algoritmos que executam docagem molecular somente considera a flexibilidade do ligante, considerando o receptor rígido.
Entretanto, sabe- se que as proteínas não permanecem rígidas em seu ambiente celular, sendo de fundamental importância a consideração dessa flexibilidade do receptor na execução dos experimentos de docagem molecular.
Existem diferentes trabalhos que visam a incorporação da flexibilidade de receptores na docagem abordagens, nesta Tese utiliza- se a execução de uma série de experimentos de docagem molecular, considerando em cada experimento uma conformação do receptor gerada por uma simulação de dinâmica molecular (DM).
A simulação por DM é uma das técnicas computacionais mais versáteis e amplamente utilizadas para o estudo de macromoléculas biológicas.
Com simulações por a DM é possível estudar o efeito explícito de ligantes na estrutura e estabilidade das proteínas, os diferentes parâmetros termodinâmicos envolvidos, incluindo energias de interação e entropias.
Aquisição de dados O receptor sendo utilizado nesta Tese é a enzima InhA do Mycobacterium tuberculosis (MTB).
Essa enzima, a qual pode ser vista como um importante alvo para o controle da tuberculose, contém um total de 268 resíduos de amino-ácido, totalizando 4.008 átomos.
Em esse trabalho o receptor é totalmente flexível e sua representação foi obtida a partir de uma trajetória de simulação por dinâmica molecular coletada por 3.100 ps, conforme descrito em.
Cada 1.0 ps da simulação por dinâmica molecular corresponde a uma conformação num formato PDB.
Parte deste arquivo é apresentado na Figura coluna correspondem ao número e nome do átomo, respectivamente.
O nome e o número do resíduo ao qual este átomo pertence, aparecem na quarta e quinta coluna.
As colunas seis, sete e oito correspondem às coordenadas x, y, z do átomo.
Essa figura mostra os 12 primeiros átomos que fazem parte do primeiro resíduo da InhA.
Esse conjunto de conformações é utilizado para representar a flexibilidade explícita do receptor InhA durante o procedimento de experimentos de docagem molecular considerando o receptor flexível.
A esse modelo totalmente flexível dá- se o nome de modelo FFR (Fully--Flexible Receptor).
Para ilustrar a flexibilidade desse receptor, parte da estrutura 3D da enzima InhA pode ser visualizada na Figura 3.3, onde cada cor representa uma conformação distinta.
A estrutura cristalográfica obtida do Protein Data Bank (PDB) está representada em laranja;
As outras quatro estruturas são conformações médias que variam de 0.0 a 500 ps (em ciano), de 500 a 1.000 ps (em azul), de 1.050 a 1.500 ps (em magenta) e de 1.550 a 2.000 ps (em verde).
Os arquivos dos ligantes são representados num formato MOL2, conforme Figura 3.5.
Esse formato é composto por as seções MOLECULE, ATOM, BOND, SUBSTRUCTURE e Set.
Para fins de ilustração, a Figura 3.5 mostra a seção ATOM, a qual contém infomações a respeito de o nome dos átomos, tipo, coordenadas x, y, z e cargas parciais.
Os experimentos de docagem molecular foram executados utilizando- se o workflow científico experimentos de docagem considerando- se, em cada experimento, uma diferente conformação do receptor.
O workflow utiliza o AutoDock3.
0.5 como software de docagem, o qual faz uso do protocolo de Simulated Annealing, com 10 execuções para cada conformação.
O resultado do AutoDock é um arquivo texto, conforme mostra a Figura 3.6.
O resultado de cada execução é composto essencialmente por os valores destacados nas caixas (a), (b) e (c) da Figura 3.6.
A Figura ligante está em relação a sua posição inicial, tipicamente estipulada por um especialista de domínio.
O principal problema com a utilização da trajetória da DM em experimentos de docagem molecular é o tempo necessário para a execução de experimentos e a grande quantidade de dados gerados.
Visando reduzir esse tempo de execução e melhor entender como ocorre a interação receptor-ligante considerando a flexibilidade do receptor, é necessário investir em técnicas que contribuam para esses objetivos.
Para tanto, durante o desenvolvimento desta Tese aplicou- se diferentes etapas do processo de KDD, as quais são descritas nos capítulos 4 e 5 subsequentes.
Considerações do capítulo Em este capítulo foram apresentados os conceitos de desenho racional de fármacos e de experimentos de docagem molecular.
Em esta Tese os experimentos de docagem molecular fizeram uso da enzima InhA do Mycobacterium Tuberculosis, um importante alvo para a Isoniazida, principal fármaco para a tuberculose.
Essa proteína possui uma grande flexibilidade, de modo com que sua flexibilidade explícita foi obtida através de simulações por dinâmica molecular.
A esse modelo totalmente flexível para a enzima InhA dá- se o nome de modelo FFR.
Os experimentos de docagem molecular para o modelo FFR utilizaram 4 ligantes distinos:
NADH, PIF, TCL e ETH.
Experimentos de docagem molecular que consideram a flexibilidade explícita do receptor costumam estar relacionados com uma grande quantidade de dados, principalmente no que diz respeito às conformações tanto dos ligantes quanto do receptor.
Tendo esses resultados de docagem, o deasafio está em contribuir para acelerar o processo de execução de novos experimentos de docagem, desenvolvendo estratégias para identificar características de conformações que possam contribuir para essa seleção e, assim, reduzir o número de conformações a serem testadas.
Uma análise detalhada dos resultados de experimentos do modelo FFR em docagem molecular, em especial aqueles relacionados aos detalhes das interações entre ligante e receptor, é essencial para o melhor entendimento e aprimoramento do processo de experimentos de docagem como um todo.
Esse tipo de análise pode ser melhor explorada se houver uma integração entre dados de simulação por DM e dos dados de docagem sobre o modelo FFR.
Considerando o grande volume de dados envolvidos, aspectos de persistência, fácil acesso e recuperação dos dados merecem ser explorados.
Por essa razão foi desenvolvido um repositório abrangente para armazenar todas as características envolvidas quanto a as conformações obtidas por simulações por DM bem como às relacionadas ao modelo FFR e todos seus níveis de detalhe como, por exemplo, as coordenadas espaciais dos átomos envolvidas em cada resultado de experimentos de docagem molecular.
Esse repositório, denominado FReDD (Flexible-Receptor Docking Database), pode ser visto como uma infraestrutura eficaz para preparação de dados.
Este capítulo apresenta o repositório FReDD, o qual foi desenvolvido para integrar esses dados em todos os seus níveis de detalhe, permitindo com que análises a respeito de a flexibilidade do receptor fossem realizadas.
Além disso, este capítulo também mostra como o modelo do FReDD facilita a etapa de pré-processamento dos dados para a mineração, apresentando as técnicas utilizadas.
Como resultados dos tópicos descritos neste capítulo, os seguintes trabalhos foram publicados durante o desenvolvimento desta Tese:·
O modelo de dados inicial do FReDD está na forma de um resumo expandido no Brazilian· O modelo final deste repositório, bem como análises a respeito de a flexibilidade do receptor· O pré-processamento dos dados contidos no repositório FReDD, o qual foi utilizado para os experimentos de mineração de dados realizados durante o desenvolvimento desta Tese, foi publicado como um resumo no ISCB Latin America.
A versão completa deste Bioinformatics, atualmente em revisão;·
De forma resumida, tanto o modelo do repositório quanto a estratégia de pré-processamento realizada a partir de o mesmo estão descritas como um capítulo do livro Tópicos em sistemas colaborativos, multimídia, web e banco de dados de 2010, o qual foi apresentado como O repositório FReDD foi desenvolvido para integrar dados de simulações por DM e resultados de experimentos de docagem molecular considerando o modelo FFR, e tendo como objetivo proporcionar um ambiente adequado para o pré-processamento desses dados.&amp;&amp;&amp;
Em este sentido, este repositório é suficientemente abrangente para armazenar dados de proveniência de simulações por DM e suas respectivas conformações, integrados com as características dos resultados de docagem.
Em outras palavras, o repositório FReDD permite recuperar características espaciais e temporais tanto de receptores quanto de ligantes armazenados, desde que tais conformações estejam descritas em termos de tempo de execução (para simulações de DM) e em termos de coordenadas espaciais para cada um de seus átomos.
Relações entre os resíduos de um ligante e seus respectivos átomos Relações entre os resíduos de um receptor e seus respectivos átomos Relações entre um ligante e seus respectivos resíduos Relação entre resíduos de um receptor e os 20 aminoacidos naturais Cada uma das execuções de um experimento de docagem molecular Coordenadas espaciais dos átomos de um ligante Coordenadas espaciais de uma conformação do receptor Características da execução do experimento de docagem molecular Detalhes a respeito de o nome e número de átomos de um ligante Dados de proveniência da simulação por DM Detalhes do receptor, como o cabeçalho do arquivo PDB Todos os resíduos de um ligante Todos os resíduos de um receptor Detalhes de uma conformação, como ordem de uma trajetória de DM Para testar o modelo de dados da Figura 4.1, está sendo considerado um receptor e quatro ligantes (os mesmos descritos no capítulo 3).&amp;&amp;&amp;
Entretanto, é importante salientar que, conforme modelo da Figura 4.1, este repositório está apto a armazenar tantos receptores e ligantes quantos necessários.
A atual população de algumas das tabelas do FReDD está sumarizada na Tabela 4.2.
Esta tabela trás na primeira linha informações sobre o receptor e nas demais sobre os ligantes.
Em a segunda coluna é mostrado, para cada ligante, o número de docagens válidas, ou seja, os experimentos de docagem que convergiram, de um total de 3.100.
A terceora coluna mostra o número total de átomos para estas estruturas.
Em a quarta coluna são apresentadas o número total de conformações para cada estrutura, sendo que para o caso dos ligantes considera- se cada uma das 10 execuções para cada experimento de docagem.
A quinta coluna indica o total de coordenadas, a qual corresponde ao produto do número de átomos por o número de conformações.
Nota- se que para o receptor, o qual possui um total de 4.008 átomos distribuídos entre seus 268 resíduos, há um total de mais de 12 milhões registros de coordenadas para a trajetória de DM sendo considerada.
Com relação a os ligantes, somando o total de coordenadas para cada uma dessas estruturas, há um total de mais de 3 milhões de registros de coordenadas.
FReDD como uma infraestrutura para pré-processamento A partir de o FReDD é possível extrair diferentes tipos de informações a respeito de os dados em ele armazenados.
O objetivo em analisar dados de simulações de DM e resultados de experimentos de docagem é reduzir o número de conformações a serem consideradas em experimentos de docagem molecular para o modelo FFR e um dado ligante.
Para tanto busca- se extrair padrões relacionados à interação ligante-receptor, os quais apresentem informações sobre quais características são importantes para serem observadas para selecionar um subconjunto de conformações.
Em o FReDD estão disponíveis várias características que podem ser consideradas.
Escolher qual de elas pode ser a mais importante reflete diretamente nos experimentos de mineração a serem realizados.
Aqui é apresentada uma abordagem de pré-processamento, baseado no contexto de docagem molecular, para produzir um conjunto de dados para análise.
Uma das maneiras de avaliar a qualidade de um experimento de docagem molecular é por o valor estimado de FEB resultante, sendo que quanto mais negativo esse valor, melhor.
Em esse sentido, o valor estimado de FEB é utilizado como o atributo alvo dos experimentos de mineração sobre os dados de docagem.
Não existe um consenso a respeito de um valor ideal para o FEB.
É necessário que ele seja considerado e avaliado individualmente para cada tipo de ligante sendo utilizado.
A Tabela 4.3 mostra a variação de valores de FEB, de acordo com os dados armazenados na tabela Conformation_ Lig, para cada um dos ligantes.
As colunas 2, 3 e 4 mostram os valores mínimos, máximos e a média de FEB, respectivamente e para cada ligante, para todas as 10 execuções de docagem molecular para cada conformação.
As colunas 5, 6 e 7 apresentam os valores mínimos, máximos, e média de FEB, respectivamente e para cada ligante, apenas da execução que apresentou melhor valor de FEB, de entre as 10 execuções.
Nota- se, por a Tabela 4.3, que os valores de FEB para os quatro ligantes sendo utilizados são bastante distintos.
Como exemplo, para o ligante NADH, nas 10, execuções o valor mínimo de FEB é 20.61 enquanto para o ligante ETH o valor mínimo é 8.22.
Isso ratifica que o valor de FEB deve ser analisado por tipos de ligante pois, para o exemplo dos ligantes NADH e ETH, o melhor valor de FEB encontrado para o ligante ETH corresponde à média dos valores de FEB para o NADH.
Uma característica que contribui para a determinação do valor de FEB é a distância entre átomos dos resíduos do receptor e do ligante, onde essa distância é medida em Angstroms (Å).
Isso é, para cada resíduo (Residue_ Recep) de um receptor R é calculada a distância Euclidiana entre os seus átomos (Atom_ Residue_ Recep) e os átomos de um ligante L (Atom_ Residue_ Lig), conforme equação 4.1.
XR, yR e zR correspondem às coordenadas espaciais dos átomos dos resíduos do receptor (Coord_ Atom_ Recep), e xL, yL e zL correspondem às coordenadas espaciais dos átomos do ligante (Coord_ Atom_ Lig).
DistR, L $= (xR xL) 2+ (yR yL) 2+ (zR zL) 2 Para recuperar essas distâncias é necessário combinar todos os registros de coordenadas do receptor com todos os registros de coordenadas de cada ligante.
Tendo todas as distâncias recuperadas, para formar o arquivo de entrada optou- se por fazer uso da menor distância entre os átomos de um dado resíduo do receptor e dos átomos do ligante min (DistR, L), para cada resultado de experimento de docagem molecular, sendo esses os atributos preditivos.
A Figura 4.2 ilustra esse do receptor (em cinza).
Para todas as distâncias calculadas, considera- se apenas a distância mínima que, neste exemplo, é 2,72 Å.
Cada resíduo do receptor é um atributo no arquivo de entrada.
Como o receptor InhA contém 268 resíduos de aminoácidos, cada arquivo de entrada preparado para a mineração conta com um total de 268 atributos preditivos, mais o atributo alvo com o valor de FEB.
Para cada ligante é produzido um arquivo de entrada distinto.
Cada linha do arquivo de entrada é uma conformação do receptor, sendo que para cada atributo preditivo (ou resíduo) o valor no arquivo será o da menor distância entre o referido resíduo e o ligante para o qual o arquivo está sendo composto.
O Algoritmo 4.1 ilustra esse processo, onde o mesmo é aplicado para cada um dos ligantes, produzindo um arquivo de entrada para cada um de eles:
A matriz.
Algoritmo 4.1: Geração do conjunto de dados inicial.
Seja R um Receptor Seja L um Ligante Seja t um snapshot de R Seja r um resíduo de R Seja a um átomo em t snapshot Seja l um átomo de L Seja M atrizDist uma matriz onde cada linha corresponde a um resíduo r e cada célula corresponde à distância entre um a e l Seja Result uma matriz que armazena, para cada t snapshot, todas as distâncias mínimas entre os a e l Seja Input uma matriz contendo Result e, para cada t, o respectivo valor de FEB.
T otalAtomosLigL faça DistRa, Ll (xR xL) 2+ (yR yL) 2+ (zR zL) 2 DistRa, Ll fim para fim para fim para Para ilustrar, a Equação 4.2 apresenta a matriz para o resíduo GLY95 do receptor InhA, considerando a preparação do Algoritmo 4.1 para o ligante PIF.
Como o resíduo GLY95 possui átomos, essa mesma matriz é composta por um total de 24 colunas (na ilustração são mostradas apenas as 4 primeiras e 4 últimas colunas).
É importante mencionar que esta matriz com 168 elementos (7 24) é o resultado da matriz de distância apenas para um resíduo do receptor e um ligante.
Em esse sentido, para cada recupera- se apenas um único valor para a matriz (para o exemplo da Equação 4.2, o valor 2,72 destacado).
A Tabela 4.4 ilustra a matriz 10 execuções de docagem para cada conformação, conforme mencionado no capítulo 3) instâncias, correspondentes ao total de conformações utilizadas para o PIF.
Os arquivos de entrada gerados para cada ligante podem ser vistos como um pré-processamento inicial, mas abrangente, o qual pode ser aprimorado para obedecer às necessidades de cada análise ou experimento de mineração de dados realizado sobre eles.
Análises sobre os dados armazenados no repositório Antes de submeter o arquivo de entrada produzido para um algoritmo de mineração, a primeira análise realizada tem por objetivo identificar quais resíduos do receptor que mais interagem para um dado complexo receptor-ligante.
O objetivo dessa análise é investigar a importância da flexibilidade explícita do receptor e suas interações intermoleculares com pequenas moléculas.
Essa análise concentra- se em identificar resíduos da InhA, considerando o modelo FFR, que mais interagem com os quatro ligantes investigados.
Para tanto, a matriz binária é gerada a partir de a transformação de (Tabela 4.4), num formato binário, indicando se há ou não interação receptor-ligante para um dado resíduo e a respectiva conformação do receptor.
Essa transformação é obtida conforme Equação 4.3, a qual gera um arquivo semelhante ao ilustrado por a tabela 4.5.
Com a matriz[ M atrizBinaria] foi possível somar quantas interações houve para cada resíduo em cada complexo InhA-ligante.
De posse desse resultado ordenou- se em ordem descrescente os resíduos que mais interagiram e os 10 primeiros foram selecionados para cada ligante, conforme ilustrado na Figura 4.3.
O objetivo foi de verificar se diferentes ligantes fazem contato numa mesma região do receptor.
A união dos top10 resíduos para cada ligante está exposto na Tabela 4.6, totalizando 25 resíduos diferentes, sendo que os top10 resíduos de cada ligante estão destacados.
Em essa tabela, cada célula está preenchida com o número de vezes que o resíduo interagiu para cada ligante.
PIF, TCL e ETH.
O receptor é a estrutura cinza na forma de Ribbons.
Os 10 resíduos que mais interagem com cada ligante estão na forma de esfera de van der Walls e os ligantes na forma de palitos.
25 resíiduos.
Os top10 resíduos para cada ligante estão destacados.
Resíduo dos experimentos de docagem (coluna 2);
O número de resíduos da estrutura cristalográfica que estão até 4,0 Å de distância do ligante, ou seja, que interagem (coluna 3);
E a intersecção dos resíduos que interagem com a estrutura cristalográfica e dos top10 do modelo FFR.
Por os dados da Tabela 4.7 é possível identificar a importância em considerar a flexibilidade do receptor em experimentos de docagem molecular.
A o observar, por exemplo, os dados para o ligante NADH, nota- se que o mesmo interage com apenas 22 resíduos da estrutura cristalográfica, enquanto para o modelo FFR o mesmo ligante interage com 185 resíduos.
De forma análoga, para o TCL, 139 resíduos interagem com o modelo FFR e apenas 12 com a estrutura cristalográfica e desses apenas 5 também estão presentes no top10 resíduos.
Isso significa que há outros 5 resíduos que podem interagir várias vezes mas que não são identificados se considerar apenas a estrutura rígida do receptor.
Considerações do capítulo Considerar a flexibilidade do receptor em experimentos de docagem molecular é um processo que produz uma vasta quantidade de dados que necessitam ser explorados.
Para um melhor entendimento desta flexibilidade em experimentos de docagem, neste capítulo foi proposto um repositório suficientemente abrangente que integra conformações de simulação por DM e todos os dados relacionados a respeito de as interações receptor-ligante nos seus respectivos resultados dos experimentos de docagem.
Em este capítulo foi mostrado o desenvolvimento do repositório FReDD, o qual foi inicialmente publicado em.
O FReDD é capaz de armazenar, indexar e recuperar resultados de docagem molecular.
Em este repositório está armazenados dados do receptor InhA e quatro ligantes (NADH, dos dados, como apresentado em.
Esse pré-processamento possibilitou um análise dos dados, com a qual foi possível extrair informações a respeito de a interação ligante receptor, como reportado em.
Por essa análise foi possível identificar relações de interações de resíduos do modelo FFR com os ligantes, sendo que essa análise seria difícil de ser realizada sem uma infraestrutura como o FReDD.
O conjunto das características descritas neste capítulo demonstram o quão efetivo é centralizar esse tipo de dados num repositório apropriado, de modo com o que o acesso e recuperação dos dados se dá de maneira fácil e clara.
A partir de as facilidades em termos de pré-processamento de dados que o repositório FReDD oferece, busca- se aplicar técnicas de mineração de dados sobre esses dados para aumentar o entendimento a respeito de o comportamento da flexibilidade do receptor e, assim, contribuir para diminuir a quantidade de execuções de experimentos de docagem molecular.
Para tanto, os experimentos de mineração de dados executados durante todo o desenvolvimento desta Tese teve por objetivo responder a seguinte pergunta:·
Como selecionar um subconjunto de conformações que sejam as mais relevantes para indicar se um dado ligante é um composto promissor?
Buscando por diferentes tipos de padrões que pudessem apontar uma direção para responder a essa pergunta, foram aplicadas diferentes técnicas de mineração de dados sobre os dados préprocessados a partir de o FReDD, como regras de associação e árvores de decisão para classificação e para regressão.
Os experimentos foram realizados considerando o conjunto de dados incial, pré-processado por o Algoritmo 4.1.
Entretanto, para cada técnica empregada, esse mesmo conjunto de dados passou por novas etapas de pré-processamento, para que se tornasse apropriado para a tarefa de mineração sendo empregada e seus respectivos objetivos.
Este capítulo apresenta as diferentes técnicas de mineração de dados aplicada sobre os dados armazenados no FReDD, bem como seus respectivos procedimentos de pré-processamento.
Além disso, são apresentadas as diferentes avaliações realizadas para esses modelos, e qual conhecimento foi possível extrair a partir de os mesmos.
Como resultados dos experimentos realizados e apresentados neste capítulo, obteve- se os seguintes trabalhos científicos:·
Os experimentos realizados com regras de associação estão publicados no Brazilian Symposium on Bioinformatics em 2008;·
Os resultados objtidos com árvores de decisão para classificação estão publicados no Brazilian Applied Computing de 2010;·
Os diferentes resultados obtidos por a relização de experimentos com árvore de decisão para revisão;·
Por fim, uma compilação desses trabalhos encontra- se na forma de um capítulo do livro Tópicos em sistemas colaborativos, multimídia, web e banco de dados de 2010, o qual foi apresentado como minicurso durante o Simpósio Brasileiro de Banco de Dados em 2010, bem como na forma de um artigo no periódico WIREs Data Mining and Knowledge Discovery em Experimentos com regras de associação A utilização de regras de associação sobre os dados aqui apresentados tem por objetivo identificar relações de interação entre diferentes resíduos do receptor.&amp;&amp;&amp;
Para tanto, utilizou- se o conjunto de dados gerado por o Algoritmo 4.1, e binarizado conforme Equação 4.3, eliminando- se o atributo alvo (F EB).
Ou seja, cada célula do conjunto de dados contém o valor 0 quando não há interação com o resíduo, e 1 quando há interação.
Para cada ligante utilizado nesta Tese foi preparado um arquivo distinto.
Os arquivos preparados foram submetidos ao algoritmo Apriori, ajustando o valor de suporte para 0,005 e confiança para 0,9, com um número máximo de 1.000 regras.
O baixo valor de suporte se justifica por o alto número de registros com conteúdo distinto no conjunto de dados.
Em as três primeiras regras para o ligante NADH é possível identificar que para as vezes em que o resíduo THR100 não interage com o NADH, os resíduos ILE94, SER19 e THR195 interagem.
Isso significa que, apesar de o resíduo THR100 aparentemente não interagir com o receptor, ele se torna representativo para indicar os resíduos que possam interagir.
Muitas outras regras podem ser extraídas.
Embora o modelo obtido com regras de associação não estabeleça relação entre os resíduos e valores de FEB, elas podem contribuir para indicar quais os resíduos que mais interagem com o ligante sendo estudado.
Isso pode ser útil na busca de novos Experimentos com árvores de decisão para classificação Como técnica preditiva, um dos métodos utilizados foi árvore de decisão para classificação.
Uma vez em que algoritmos de classificação requerem atributos alvo categóricos, o desafio em aplicar essa técnica para os dados sendo utilizados está na transformação do atributo alvo FEB, o qual é numérico, para valores discretos, distribuídos de maneira adequada ao problema.
Para tanto, foram empregadas três técnicas de discretização, as quais foram avaliadas a partir de os modelos de árvore de decisão para classificação induzidos.
Discretização é o processo de transformar valores contínuos em intervalos de classes que representam esses valores.
O procedimento de discretização envolve, basicamente, duas etapas:
E então divididos em n intervalos, especificados por n 1 pontos de partição;
De entre diferentes métodos de discretização existentes na literatura, utilizaram- se os métodos por igual frequência de intervalos e por por igual tamanho de intervalo.
Além desses dois métodos, propôs- se a discretização por moda e desvio padrão:·
Método 1: Discretização por igual frequência de intervalos.
Esse é um método simples que considera que n é o número de intervalos parametrizado e m o número total de instâncias.
Assim, os valores contínuos do atributo a ser discretizado são divididos em n intervalos, de modo com que cada intervalo contenha m/ n valores, aproximadamente;·
Método 2: Discretização por igual tamanho de intervalo.
Em essa abordagem, os valores contínuos são divididos em n intervalos parametrizados, onde cada intervalo deve possuir o mesmo tamanho.
Para, esse é considerado um dos métodos mais simples de discretização, porém vulnerável a pontos discrepantes;·
Método 3.
Discretização por moda e desvio padrão.
Esse método de discretização propõe- se a fazer uma separação dos melhores e piores valores de FEB em classes bem definidas.
Para tanto considera- se a moda e o desvio padrão da frequência de distribuição dos valores de FEB sendo discretizados.
Para esse método definiu- se um total de 5 intervalos, ou classes, conforme apresentado na equação 5.1 onde x é o atributo a ser discretizado, e Me o e representam a Moda e o Desvio Padrão para a distribuição de x.
De essa maneira, ocorrências de melhores e piores casos, as quais são menos frequêntes no conjunto de dados, são agrupadas nos intervalos das extremidades da distribuição normal, sendo que ocorrências regulares são distribuidas nos demais intervalos.
Classe $= Excelente Bom Regular Ruim M Ruim se se se Me o 2 ú Me o Me o+ Me o+ 2 ú Para os três tipos de discretização utilizados foram parametrizadas 5 classes, sendo elas Excelente, Bom, Regular, Ruim e MRuim (Muito Ruim).
A Tabela 5.2 mostra para cada ligante, o número de exemplos (resultados de docagem), o valor médio de FEB e seu respectivo desvio padrão, o valor da Moda e a distribuição dos exemplos em cada classe, para cada um dos 3 métodos.
Por a Tabela 5.2 é possível observar que no Método 1, os exemplos estão dispostos nas 5 classes de maneira balanceada.
Como o Método 2 distribui os exemplos num intervalo igual de tamanho, os mesmos podem aparecer desbalanceados.
Isso acontece especialmente para os casos dos ligantes de sua Moda é 9,0 kcal/ mol, mais próximo de o valor mínimo do que do valor máximo de FEB.
Além disso, esse mesmo ligante apresenta um desvio padrão de 0,3 kcal/ mol, o que significa que o valor de FEB não varia muito, apresentando valores próximos à Moda.
Como, para o caso do Método modelo de árvore de decisão induzido sobre esse conjunto de dados pode ser distorcido.
Por outro lado, ao observar a distribuição do Método 3, apesar de apresentar um grande desbalanceamento nas classes, nota- se que os valores que de fato representam os melhores e piores valores de FEB estão distribuídas nas classes Excelente e MRuim.
Os conjuntos de dados foram submetidos ao algoritmo J48 (implementação do C4.
5), parametrizando o número mínimo de instâncias em cada nodo folha para 50, objetivando gerar árvores mais legíveis, requisito importante para o problema e tipo de dados sendo explorados.
Os modelos induzidos foram avaliados em termos de as métricas típicas utilizadas para árvore de decisão para classificação, como acurácia, tamanho da árvore e Medida-F. Além disso, introduziu- se uma quarta métrica, a qual indica a taxa de instâncias que pertencem às classes Excelente e Bom.
Para essa métrica, aqui denominada TEB, busca- se os menores valores, ou seja, quanto menor a taxa, melhor o resultado.
Os resultados dos modelos estão dispostos na Tabela 5.3, onde cada execução corresponde a uma linha da tabela, e cada coluna mostra o resultado obtido para cada uma das métricas sendo avaliadas.
Os melhores valores para cada métrica e cada ligante estão destacados.
Por o Método 1 é possível observar que as métricas foram as piores para todos os ligantes, com exceção da métrica TEB para o ligante NADH.
Seus resultados mostram que esse tipo de discretização não é eficiente para dados de docagem molecular.
O Método 2 apresentou os melhores resultados para o ligante PIF.
Entretanto, para esse ligante em particular, o método apresenta um total de 99,31% de instâncias nas classes Excelente e Bom (ver Tabela 5.2).
Isso significa que o modelo induzido não é útil para extrair informações a respeito de os resíduos envolvidos em bons resultados de docagem, uma vez que quase todas as instâncias do conjunto de dados está classificada como sendo das classes Excelente e Bom.
Para o TCL esse mesmo método mostrou um melhor resultado em relação a a Medida-F mas, assim como para o PIF, o mesmo método classificou quase todas as instâncias como sendo parte das mesmas classes (Excelente e Bom).
Assim, olhando para os resultados de TEB, nota- se distorção nos modelos induzidos.
Por fim, os arquivos discretizados por o Método 3 obtiveram os melhores resultados dos modelos de árvore de decisão para os ligantes ETH e NADH.
Para os ligante TCL este mesmo modelo se mostrou mais efetivo em 3 das 4 métricas utilizadas.
Já para o PIF, a métrica que que se destacou com esse método foi a TEB.
Por as árvores de decisão é possível extrair informações a respeito de a relação entre os resíduos do Modelo FFR da InhA e as classes de FEB.
A Figura 5.1 ilustra o modelo induzido para o complexo InhA-NADH, gerada a partir de o conjunto de dados discretizado por o Método 3.
InhA-NADH. Nota- se por a Figura 5.1 que essa árvore está dividindo bem os exemplos, de modo com que todos os exemplos cujas classes sejam Ruim (R) ou Muito Ruim (R) estão à esquerda do nodo raiz e todas as instâncias cujas classes estejam associadas a Excelente (E) e Bom (B) estão à direita do nodo raiz.
Apenas por essa separação já é possível inferir que a posição do resíduo THR100 pode ser fundamental para identificar conformações promissoras para o ligante NADH, isso é, quando o mesmo está a uma distância maior do que 11.0 Å os resultados são promissores.
Além disso, percorrendo a árvore identifica- se que além desse resíduo, as distâncias dos resíduos GLY101, SER18, SER19 e GLY39 podem levar às conformações cuja classe de FEB é Excelente.
Experimentos com árvores de decisão para regressão A o aplicar árvores de decisão para regressão sobre os dados de docagem molecular, busca- se avaliar como diferentes estratégias de pré-processamento podem melhorar a qualidade dos modelos induzidos, bem como melhorar a compreensão dos mesmos.
Para tanto, são utilizadas quatro estratégias de processamento:·
Estratégia 1: Primeiramente são utilizados o conjunto de dados inicial produzidos conforme o Algoritmo 4.1;·
Estratégia 2: Em seguida, busca- se aprimorar esse conjunto de dados, onde é aplicada uma técnica convencional de seleção de atributos;·
Estratégia 3: Em a busca de um conjunto de dados melhor, é proposta uma outra técnica de seleção desses atributos, a qual é realizada com base no contexto dos dados envolvidos;·
Estratégia 4: Por fim, busca- se uma combinação dos atributos selecionados por as estratégias 2 e 3.
A Estratégia 1 conta com o conjunto de dados incial, o qual contém um total de 268 atributos preditivos para cada um dos ligantes sendo testados.
Em a busca por melhorar a qualidade do conjunto de dados, aplicou- se algumas técnicas de seleção de atributos sobre esses dados.
Primeiramente optou- se, na Estratégia 2, aplicar um algoritmo convencional de seleção de atributos, denominado Correlation-based Feature Selection (CFS).
O algoritmo CFS é construído a partir de um método de filtro, o qual ordena as características de acordo com uma função de avaliação baseada em correlação.
O objetivo é encontrar um subconjunto de atributos que contenha características fortemente correlacionadas com o atributo-alvo e fracamente correlacionada em relação a as demais.
O CFS é baseado na seguinte equação:
Krcf k+ krf f onde MX é uma função heurística de um subconjunto de dados X que contém k atributos;
Rf c é a média de correlação dos atributos (f oe X) e rf f é a média entre dois atributos inter-correlacionados.
A Equação 5.2 forma o núcleo do algoritmo CFS.
Esse algoritmo foi aplicado para cada um dos conjuntos de dados gerados por o Algoritmo 4.1, para cada ligante.
Como resultado dessa seleção de atributos, obteve- se conjuntos de dados disjuntos, onde o número de atributos preditivos selecionados para cada ligante está listado na Tabela 5.4.
Além de uma técnica tradicional de seleção de atributos, buscou- se também aplicar uma técnica com base no contexto dos dados de docagem.
A Estratégia 3 baseia- se na definição de Jeffrey, o qual diz que a maior distância que permite um contato significante entre átomos do receptor e do ligante é 4,0 Å.
Em esse sentido, partiu- se do princípio de que se um dado resíduo não faz contato em nenhuma das conformações, é pouco provável que este resíduo seja representativo para predizer bons valores de FEB.
Assim, propôs- se uma estratégia de seleção de atributos considerando tal distância, onde todos os atributos (ou resíduos) cuja distância mínima do conjunto de dados é maior do que 4,0 Å são removidos.
O Algoritmo 5.1 ilustra como esse novo conjunto de dados é gerado, a partir de o conjunto de dados inicial.
Algoritmo 5.1: Seleção de atributos baseada no contexto de dados de docagem molecular.
1: Seja R um Receptor 2: Seja uma matriz que representa o conjunto de dados produzido por o Algoritmo 4.1 3: Seja uma matriz contendo o conjunto de dados gerado após a seleção de atributos 4: Para cada 1 r em T otalResisduos R faça se min 4 então fim se fim para Aplicando o Algoritmo 5.1 sobre cada conjunto de dados inicial, em vez de os 268 atributos preditivos para cada ligante, obteve- se o número de atributos selecionados ilustrados na Tabela 5.5.
Por fim, fez- se a união dos atributos selecionados por as Estratégias 2 e 3.,
de modo com que os conjuntos de dados gerados por a Estratégia 4 contêm o número de atributos descritos na Tabela 5.6.
Os conjuntos de dados foram submetidos ao algoritmo M5P.
De entre os parâmetros disponíveis para este algoritmo, concentrou- se na calibragem dos parâmetros relacionados à legibilidade e precisão das árvores induzidas.
Portanto, definiu- se o número mínimo de instâncias para 1.000, onde este tamanho está relacionado com o tamanho da árvore modelo resultante e o número de modelos lineares produzidos.
A Figura 5.2 ilustra a árvore induzida para o complexo InhA-NADH, a qual é composta por 5 nodos e 6 Modelos Lineares (LM).
A equação 5.3 ilustra como um modelo linear é composto, onde o valor de FEB é calculado aplicando pesos diferentes para alguns resíduos selecionados do conjunto de dados e calibrando os mesmos com um valor constante.
No caso de a equação 5.3, é ilustrado o LM6 da árvore da Figura 5.2, por ser o melhor modelo encontrado (mais detalhes nas seção seguinte).
Os modelos induzidos foram avaliados considerando- se métricas preditivas e métricas de contexto.
Com relação a as métricas preditivas, utilizou- se aquelas comuns à avaliação de árvores de regressão, como número de nodos, correlação, MAE e RMSE.
No que diz respeito às métricas de contexto, avaliou- se os modelos considerando os resíduos presentes tanto nos nodos internos quanto os presentes nos modelos lineares de cada modelo, já que o valor estimado de FEB é calculado baseado nos resíduos que fazem parte da grade (Capítulo 3).
Uma vez que a mineração de dados sendo aplicada sobre o conjunto de dados de docagem molecular tem como objetivo predizer o valor de FEB, é adequado avaliar os modelos que consideram os resíduos que fazem parte desse grid.
Um especialista de domínio mapeou todos os resíduos que fazem parte do grid, ou seja, todos os resíduos que pertencem ao sítio ativo de ligação da enzima InhA.
Foram selecionados 52 resíduos, aqui denominados ESR.
Em seguida, mapeou- se, para cada modelo induzido, quais os resíduos que fazem parte dos nodos ou dos modelos lineares (Figura 5.2, Equação 5.3), onde tais resíduos são chamados de M R. Para avaliar esses dois conjuntos de resíduos, optou- se por comparar- los considerando as métricas de Precisão, Revocação e Medida-F.
Em o contexto de ESR e M R, precisão e revocação são assim definidas:
P recisão $= ESR fl M R Revocação $= ESR fl M R A Estratégia 1 foi, talvez, a mais importante em termos de contexto, uma vez que, sem o conhecimento prévio a respeito de a semântica dos dados envolvidos, seria difícil gerar um conjunto de dados que pudesse produzir modelos interpretáveis.
A o aplicar as diferentes técnicas de pré-processamento, a idéia é que o pré-processamento baseado em contexto, incluindo a seleção de atributos por a Estratégia 3, pudesse gerar resultados melhores do que aqueles cuja seleção de atributos se desse a partir de técnicas convencionais, como a Estratégia 2.
As Tabelas 5.7 e 5.8 mostram a avaliação das métricas preditivas e de contexto, respectivamente.
Essas medidas são individualmente aplicadas para cada ligante.
Em as duas tabelas, os melhores valores obtidos estão destacados.
Para as métricas preditivas, foi avaliado se a significância da Estratégia 2 é pior do que as demais.
Obteve- se os níveis de significância p $= 0, 04 para MAE e p $= 0, 54 para RMSE, indicando que a Estratégia 2, a qual não utilizou nenhum conhecimento do domínio, é pior do que as demais.
Entretanto, esforços ainda precisam ser aplicados sobre esses dados para melhorar sua qualidade.
Por outro lado, no que diz respeito às métricas de contexto, avaliou- se se a Estratégia 3 é significativamente melhor do que as demais.
Em esse caso é possível afirmar que sim, pois obteve- se um nível de significância p $= 0, 014, de modo com que é possível inferir que a seleção de atributos baseada no contexto melhora a qualidade dos modelos em relação a o pré-processamento inicial.
Essas medidas corroboram com o pressuposto que modelos compreensíveis são essenciais neste contexto.
Como o objetivo de minerar os dados de docagem é selecionar conformações promissoras, apenas avaliar a qualidade dos modelos induzidos não é suficiente.
De esse modo, estabeleceu- se uma abordagem de pós-processamento das árvores induzidas para seleção de modelos lineares que representem bons valores de FEB.
A o selecionar esses modelos lineares, é possível percorrer a árvore para indentificar as conformações dos conjuntos de dados que pertencem a cada modelo linear.
Essa avaliação é realizada em três passos:·
As árvores são percorridas e um teste é aplicado para identificar quais instâncias, ou conformações do conjunto de dados, pertencem à cada nodo folha, ou LM;·
Um critério de seleção de melhores modelos lineares é estabelecido;·
É feita uma avaliação para verificar se as conformações selecionadas são, de fato, promissoras.
Como conjunto de teste, utilizou- se os resultados de docagem com melhores valores de FEB para cada conformação, em vez de fazer uso das 10 execuções (Tabela 4.3, Capítulo 4).
Após mapear as conformações que pertencem a cada nodo folha, foi possível estabelecer o critério de seleção de modelos lineares representativos e, assim, utilizar- los para a seleção de conformações:·
Como ponto de partida considerou- se a média dos valores de FEB para cada ligante, para o conjunto de teste (F EB T este);·
Em seguida, para cada LM calculou- se a média dos valores de FEB das instâncias que compõe o modelo linear (F EB LM);·
Tendo esses valores médios, definiu- se que um LM é considerado representativo se a média dos valores de FEB que o compõe é menor ou igual à média dos valores de FEB do conjunto de teste (F EB LM= F EB T este) Aplicando- se esse critério para a árvore ilustrada na Figura 5.2, foi possível selecionar apenas um modelo linear representativo:
LM6 (Equação 5.3).
Assim, a partir de o modelo gerado para o NADH, pode- se afirmar que o resíduo THR100 é essencial para determinar o valor de FEB para este ligante.
Isso é, se o resíduo THR100 estiver a uma distância maior do que 11,49 Å do NADH, então a conformação provavelmente apresentará um bom valor estimado de FEB, e essa pode ser considerada como uma conformação promissora.
Para os modelos, buscou- se avaliar quais conformações foram selecionadas, bem como verificar se essas selecionadas correspondem, de fato, às melhores.
Assim, todas as conformações do conjunto de dados inicial foram ordenadas de acordo com o seu valor de FEB, em ordem crescente, selecionando as primeiras 10, primeiras 100 e primeiras 1.000.
O mesmo foi feito para o conjunto de teste, onde verificou- se quais dessas conformações listadas fazem parte das listadas para o conjunto de dados inicial.
Como resultado, obteve- se os dados informados na Tabela no conjunto de treino, e a coluna 5 indica o total de conformações realmente selecionadas em relação a o total de conformações disponíves.
Top 10 Top 100 Top 1000 Conformações Selecionadas/ Conformações Com base nos resultados da Tabela 5.9 é possível notar que a seleção de conformações foi satisfatória para todos os ligantes.
Para os ligantes NADH e PIF, dos 10, 100 e 1.000 melhores valores de FEB, o método selecionou quase que 100% das conformações.
Apesar de a seleção dos demais ligantes apresentar um valor menor, ela ainda representa aproximadamente 60% do total.
Considerações sobre os modelos induzidos A o analisar os modelos induzidos tanto por regras de associação, quanto por árvore de decisão para classificação e para regressão, considerando- se os resultados para o ligante NADH, nota- se que o resíduo THR100 sempre aparece.
Esse é um resíduo que se encontra na alça superior direita da proteína InhA (Figura 3.3, Capítuo 3) e, sendo assim, distante da região do sítio ativo de ligação.
Aparentemente esse é um resíduo que não deveria ser representativo para o entendimento da flexibilidade do receptor e sua relação com os melhores experimentos de docagem.
E, de fato, ao observar os modelos induzidos por árvore de decisão, nota- se que o teste das arestas desse resíduo é de aproximadamente 11,00 Å.
Essa distância é, de fato, uma distância longa em relação a o sítio ativo e não apresenta nenhum contato.
Entretanto, os modelos de árvore de decisão indicam que os melhores resultados de docagem molecular são, justamente, quando esse resíduo está a uma distância superior a 11,00 Å.
A partir de a análise de um especialista de domínio sobre esses modelos, concluiu- se que esse resíduo é realmente importante para definir conformações que possam resultar em bons resultados de docagem, para o ligante NADH, pois quando o resíduo THR100 está distante do sítio ativo, o mesmo faz com que outros resíduos que formam contato estejam próximos.
Por essa análise, observa- se que os modelos induzidos foram importantes para o entendimento da flexibilidade do receptor e para a identificação das características das conformações, no que diz respeito à distância entre os resíduos do receptor em relação a o ligante, que direcionam à resultados de FEB mais promissores.
Contudo, a partir de o conjunto de dados sendo utilizado, torna- se difícil selecionar as conformações do receptor para futuros experimentos de docagem.
Isso porque as distâncias entre os resíduos do receptor em relação a o ligante só podem ser obtidas a partir de resultados de docagem.
De essa forma, não é possível fazer uso de conformações que não tenham passado por esses experimentos e inferir quais de elas teriam mais chance de apresentar bons resultados de FEB após a docagem molecular.
Considerações do capítulo Em este capítulo foram apresentadas três técnicas de mineração de dados empregadas nos dados de docagem molecular, onde o principal objetivo foi o de contribuir para a seleção de conformações.
Foram utilizadas regras de associação, árvore de decisão para classificação e árvore de decisão para regressão (árvore modelo).
Para cada uma dessas técnicas evoluiu- se o pré-processamento inicial apresentado no Capítulo 4.
Regras de associação foram aplicadas para identificar quais resíduos interagem mais com o receptor.
Essa técnica foi primeiramente utilizada com um conjunto reduzido de dados e foi posteriormente evoluida para utilizar todo o conjunto de dados apresentado.
A o utilizar árvore de decisão para classificação, propôs- se um método de discretização do FEB e comparou- se os resultados dos modelos induzidos.
O mesmo foi feito para árvores de decisão para regressão, onde aplicou- se estratégias de pré-processamento sobre esses dados, buscando efetuar uma seleção de atributos baseada no contexto dos dados envolvidos,.
Os modelos de árvore de decisão induzidos sobre esses dados foram pós-processados para identificar a sua qualidade quando de a seleção de conformações.
Observou- se que o pré-processamento é uma importante etapa a ser considerada em mineração de dados, onde diferentes técnicas podem ser aplicadas para melhorar a qualidade dos dados minerados.
Em o contexto de dados de docagem molecular observou- se que uma preparação de dados baseada no contexto apresenta- se melhor do que estratégias convencionais de preparação de dados.
Os resultados obtidos com as diferentes técnicas de mineração aplicadas mostram alguns exemplos de informações que podem ser obtidas sobre os experimentos de docagem molecular, que não seria possível de serem extraídas sem a aplicação das técnicas de pré-processamento e rotinas de mineração de dados.
Um exemplo são os resíduos que aparecem tanto na árvore de regressão quanto na árvore de decisão do NADH, que são resíduos que numa inspeção visual com uma conformação desse receptor e o NADH não parecem estar em contato com o mesmo.
Apesar de os bons resultados encontrados, os mesmos não são suficientes para a efetiva seleção das conformações, isso porque não é possível obter as distâncias dos resíduos do receptor em relação a o ligante (requisito do conjunto de dados sendo utilizado) sem ter- se efetuado experimentos de docagem.
Em esse sentido, é importante fazer uso de uma estratégia de mineração de dados que permita efetivamente selecionar conformações da proteína de modo que, no futuro, seja possível acelerar os experimentos de docagem molecular, utilizando novos e diferentes ligantes as conformações indicadas como mais promissoras nos experimentos já executados.
O processo desenvolvido, incluindo a construção de um repositório alvo, as estratégias de préprocessamento desenvolvidas e os experimentos de mineração de dados, apresentaram resultados interessantes.
Esses resultados, entretanto, apesar de promissores ainda podem ser considerados modestos.
Em esse sentido, acredita- se ser possível aprimorar os modelos induzidos, mantendo o objetivo de que estes modelos contribuam para a seleção de conformações de receptores para futuros experimentos de docagem molecular.
As estratégias de pré-processamento apresentadas nos capítulos 4 e 5 concentram- se nas distâncias entre átomos do ligante e dos resíduos do receptor.
Ainda que essa estratégia tenha sido fundamental para entender e aferir a importância da flexibilidade do receptor, bem como permitir diversos experimentos de mineração de dados sobre esse tipo de dados, seus atributos preditivos demandam uma prévia execução de experimentos de docagem molecular.
Em outras palavras, os modelos induzidos indicam quão distante um dado resíduo do receptor precisa estar do ligante sendo testado para que seja atingido um bom valor de F EB.
Mas, para obter esse valor de distância, é necessário que os experimentos de docagem molecular tenham sido executados.
Uma vez em que objetiva- se reduzir o número de conformações do receptor a serem considerados em experimentos de docagem molecular, é interessante que apenas dados de simulação por DM sejam utilizados como atributos preditivos, onde os resultados de docagem molecular sejam considerados apenas no atributo alvo como, por exemplo, fazendo uso dos valores de F EB.
Esta Tese apresenta um algoritmo de indução de árvore de decisão para regressão denominado 3 D-Tri, o qual é capaz de interpretar propriedades tridimensionais, no formato x, y, z, e induzir uma árvore que representa essas propriedades, predizendo um valor de F EB.
Para tanto, a estratégia é minerar dados de simulações por DM, considerando as propriedades tridimensionais (3 D) de cada conformação do receptor.
Isto é, em vez de fazer uso da distância entre os átomos dos resíduos do receptor e os átomos do ligante sendo considerado, assume- se como atributos preditivos as coordenadas espaciais, no espaço euclidiano, de cada átomo dos resíduos do receptor, em cada uma de suas conformações.
Em tal estratégia, os valores de F EB para cada conformação ainda são considerados como atributo alvo.
A proposta desse algoritmo foi submetida e aceita para Mining) em 2011.
Pré-processamento dos dados A primeira etapa para atender aos objetivos de minerar dados provenientes dos resultados de simulações por DM diz respeito ao pré-processamento desses dados e a geração do conjunto de dados apropriado.
Esse conjunto de dados deve conter as conformações tridimensionais dos átomos dos resíduos do receptor para cada conformação.
Isto é, para cada receptor identifica- se cada um de seus átomos e, para cada átomo, obtem- se sua posição espacial x, y, z.
O Algoritmo 6.1 apresenta um pseudo código para geração deste conjunto de dados.
Algoritmo 6.1: Geração de um conjunto de dados 3D.
Seja R um Receptor Seja L um Ligante Seja t uma conformação de R Seja F EBtL o valor de F EB estimado na conformação t para o ligante L Seja T otalSS o número de conformações de R Seja T otalSSR o conjunto de conformações de R Seja a um átomo de R Seja T otalAtomos o número de átomos de R Seja T otalAtomost o conjunto de átomos de R Seja (x, y, z) uma coordenada espacial da Seja xRa, yRa, zRa os valores de x, y e z da coordenada espacial do átomo a de R Seja Datasett, a3+ 1 uma matriz bidimensional de t linhas e a 3+ 1 colunas, contendo cada coordenada espacial da na conformação t, e seu respectivo valor de F EBtL para cada t em T otalSSR faça para cada a em T otalAtomost faça Datasett, ú xRa Datasett, ú yRa Datasett, ú zRa fim para Datasett, ú F EBtL fim para Cada três colunas de indica uma coordenada espacial de um átomo do receptor.&amp;&amp;&amp;
Uma vez que cada linha diz respeito a uma conformação do receptor, o último atributo em é o valor estimado de F EB para a conformação corrente, considerando um dado ligante.
Em este sentido, considera- se a execução de um conjunto de dados distinto para cada ligante.
Para exemplificar, ao gerar um conjunto de dados para o receptor InhA, considerando todos os seus resíduos, este algoritmo produz um total de 12.024 colunas em.
A Tabela 6.1 ilustra, para o primeiro e último átomo da InhA, como esse dataset seria estruturado para o ligante PIF.
O conjunto de dados gerado por o Algoritmo 6.1 pode não ser corretamente interpretado por um algoritmo convencional de indução de árvore de decisão.
Isso é, um algoritmo convencional, ao ler esse conjunto de dados, consideraria cada atributo individualmente, em vez de considerar a relação entre cada três atributos, os quais representam, em conjunto, uma propriedade tridimensional.
Em outras palavras, as propriedades tridimensionais do conjunto de dados seriam ignoradas e o modelo induzido poderia não ser preciso, bem como sua interpretação poderia apresentar distorções.
Em esse sentido, apresenta- se o Algoritmo 3 D-Tri, um algoritmo de indução de árvore capaz de ler o conjunto de dados gerado por o Algoritmo 6.1, interpretar suas propriedades tridimensionais e induzir uma árvore com base nessas propriedades.
Algoritmos de indução de árvore são basicamente implementados obedecendo a uma estratégia de dividir para conquistar, onde um conjunto de dados X é dividido em regiões locais a fim de predizer o atributo alvo.
Em o algoritmo proposto por esta Tese, o principal objetivo é fazer com que essa divisão represente regiões num espaço euclidiano para um dado atributo preditivo, em função de um valor de F EB.
Em o contexto do conjunto de dados gerado por o Algoritmo 6.1, um átomo é um atributo preditivo.
Basicamente, para cada nodo da árvore executa- se duas divisões, onde um nodo é um atomo e as arestas testam se os objetos sendo avaliados fazem parte de um dado intervalo, onde i indica a posição inicial de uma dada coordenada e f representa sua posição final.
A Figura se as instâncias estão dentro ou f ora do intervalo.
O algoritmo proposto apresenta dois módulos principais:·
O primeiro diz respeito à definição do melhor intervalo, ou bloco, para cada átomo, sendo possível induzir uma árvore binária a partir deste intervalo;·
O segundo refere- se à indução recursiva da árvore a partir de as árvores binárias induzidas no módulo anterior.
Para gerar as árvores binárias, o primeiro passo é identificar qual o intervalo que um dado átomo deve estar, considerando as instâncias (ou conformações) envolvidas, para que haja um bom valor de F EB.
Para tanto, cada átomo de Dataset é submetido a uma estratégia de agrupamento, como K--means.
O algoritmo K--means assume um valor de k como parâmetro e particiona um dado conjunto de dados X em k grupos, apresentando uma alta similaridade entre objetos num mesmo grupo, e baixa similaridade entre objetos de grupos distintos.
Para tanto, são selecionados k objetos, de maneira aleatória, onde cada um desses objetos representa, inicialmente, uma média ou centro de um dos k grupos.
Os objetos restantes são atribuidos ao grupo de maior similaridade, e uma nova média do grupo é calculada, até que haja uma convergência entre os objetos.
O critério de convergência é geralmente definido por o erro quadrático:
Para o conjunto de dados Dataset, o algoritmo K--means é executado para cada átomo, de modo com que cada um desses átomos seja dividido em k grupos.
Para as instâncias de cada k grupo será retornado:·
O valor médio de cada coordenada espacial, obtendo x, y, z;·
O valor médio de F EB, definido por F EB.
Após a execução do K--means é necessário, para cada átomo, eleger um dos k grupos a ser considerado (Ge).
Para cada k agrupamento seleciona- se aquele cujas instâncias representam o melhor valor estimado de F EB, ou seja, o menor valor médio de F EB.
De essa maneira define- se o centróide de F EB como:
F EBc $= F EB Ge De a mesma forma, o centróide das coordenadas é definido por o valor médio das coordenadas do agrupamento escolhido:
Em seguida, para cada instância t de Dataset, obtem- se a distância Euclidiana (d) entre as coordenadas de t e o centróide das coordenads (xc, yc, zc):
Para fins de ilustração, assume- se que a Tabela 6.2 apresenta um conjunto de dados fictício, onde Dataset contém 19 t instâncias, composto por um único átomo e seus respectivos valores de F EB.
Como t é um valor dinâmico, cada uma dessas instâncias é representada por um número de conformação Ss, o qual é invariável.
Executando K--means sobre esses dados, onde k $= 2, os registros de 1 a 9 são agrupados no grupo 1 e os registros de 10 a 19 no grupo 2 (coluna 7).
A coluna 8 ilustra o centróide das coordenadas e a coluna 9 mostra o F EB para as instâncias de cada grupo.
Uma vez que F EB do grupo 2 é menor do que F EB do grupo 1, o grupo escolhido é o de número 2, de modo com que, para este conjunto de dados, F EBc $= 7, 8.
A construção do melhor intervalo para cada átomo é feita a partir de os centróides e da distancia Euclidiana calculados.
Em um primeiro momento considera- se, para cada coordenada tridimensional, todo o intervalo presente em Dataset.
Esses intervalos são armazenados numa estrutura de dados denominada Bloco, a qual é composta por, e definida por Coordi $= min Coordf $= max, onde T otalSS é o total de coordenadas, ou instâncias t, em Dataset.
Para esta equação, Coord deve ser substituido por cada coordenada x, y e z individualmente.
Essa estrutura de dados será posteriormente atualizada por a expansão dos valores de suas coordenadas.
Para a expansão, ordena- se Dataset por a distância Euclidiana, em ordem crescente.
Caso exista mais de uma instância t com o mesmo valor de d, a ordenação então se dá por o menor valor de F EB.
Após o ranqueamento é calculado um valor de erro denominado ErroBloco para cada instância t, onde esse erro se dá de forma cumulativa por (F EBt F EBc):
ErroBlocot $= i $= 1| (F EBi F EBc)| A Tabela 6.3 mostra os 12 primeiros registros ordenados por d, (segunda coluna) para as conformações (terceira coluna) da Tabela 6.2.
As coordenadas tridimensionais estão dispostas nas colunas 4, 5 e 6 e seu respectivo valor de F EB na coluna 7.
Os valores de ErroBloco estão na última coluna.
Para expandir as coordenadas do centróide do grupo escolhido, tendo como base os valores ordenados de d, é necessário estabelecer um critério de parada (CritP aradaBloco), o qual é baseado em duas métricas.
A primeira diz respeito à taxa de crescimento do erro para uma dada instância t, em relação a uma instância anterior (T axaErroExpansao), sendo que esse erro é cumulativo.
A segunda refere- se à quantidade de instâncias t, ou população (T axaP opExpansao), que fazem parte da expansão.
As taxas de expansão são assim calculadas:
T axaErroExpansaot $= ErroBlocot ErroBlocot1 T otalSS T axaP opExpansaot $= O critério de parada baseia- se nessas duas taxas de expansão, comparando- nas com taxas limites de erro (LimiteErroBloco) e de população (LimiteP opBloco) definidas como parâmetro.
Assim, CritP aradaBloco $ _ se (T axaErroExpansaot LimiteErroBloco) e (T axaP opExpansaot LimiteP opBloco) caso contrário Aplicando CritP aradaBloco no conjunto de dados já ordenados por d, na Tabela 6.3, assumindo que LimiteErroBloco $= 1, 5 e LimiteP opBloco $= 0, 25, ignora- se as instâncias t Ø 6.
Em esse sentido, considera- se as 5 primeiros t instâncias.
A Tabela 6.4 ilustra os valores de T axaErroBloco e T axaP opBloco para o conjunto de dados da Tabela 6.3.
Após aplicar o critério de parada, é possível definir o intervalo[ (xi, xf) (yi, yf) (zi, zf)] para um átomo.
Assim, para cada coordenada, i é substituido por o menor valor desta coordenada nas instâncias t após aplicar o critério de parada.
De a mesma forma, f é substituido por o maior valor desta coordenada.
Observa- se que, caso o menor ou maior valor de uma determinada coordenada seja igual ao menor ou maior valor desta mesma coordenada no Dataset inicial, então assume- se que o limite desta coordenada tende ao infinito.
Assim, Bloco é atualizado com:
Coordi $ [ Coordf $= Onde Coord deve ser substituido por x, y, e z individualmente.
Todo esse processo é repetido para cada átomo de Dataset, de modo a existir uma árvore binária, no formato da apresentada na Figura 6.2, para cada um desses átomos.
Para a posterior indução da árvore, é necessário escolher um desses átomos.
Para tanto, calcula- se o RDP (Equação O Algoritmo 6.2 apresenta o pseudo-código para todo o processo de definição de um bloco para cada átomo de Dataset.
Tendo- se definido o critério de construção do bloco, é preciso estabelecer o critério para indução da árvore.
Para isso faz- se uso da técnica clássica de indução de árvore, a qual é construida recursivamente a partir de uma abordagem top-down.
O conjunto de dados Dataset contém como atributo alvo o valor estimado de F EB.
Por se tratar de um atributo numérico, trata- se de uma indução de árvore de regressão.
O procedimento principal da indução da árvore recebe como parâmetro o conjunto de dados Dataset.
Em a primeira execução desse procedimento é calculado o valor do desvio padrão do valor de F EB de todas as instâncias de Dataset Algoritmo 6.2: Definição do Bloco.
Procedure DefineBloco (Dataset, DP) 11: Para cada a em T otalAtomos faça Cria Dataset_ a de Dataset Divide Dataset_ a em k grupos Computa F EB para as instâncias de cada grupo Computa x, y, z para cada grupo Ge grupo com o menor F EB F EBc F EB Ge xc, yc, zc (x, y, z) GE para cada t em Dataset_ a faça Computa d (xt, yt, zt), (xc, yc, zc) fim para todos T otalSS faça Inicializa Bloco fim para repita Computa ErroBloco Computa T axaErroExpansão Computa T axaP opExpansão até CritP aradaBloco Atualiza Bloco Computa RDP (Bloco) 33:&amp;&amp;&amp; Fim para 34: Retorna (a, Bloco) com o maior RDP DP $= dp (F EBDataset) O valor de DP é utilizado para verificação do critério de parada (CritP arada) de indução.
Este critério avalia o tamanho de Dataset (T amDataset) em relação a uma população mínima (P opM inima) de instâncias que devem fazer parte de uma ramificação, bem como o desvio padrão das instâncias de Dataset sendo avaliado em relação a uma taxa do valor de DP (T axaDP), sendo:
CritP arada $= se (T amDataset P opM) ou (dp (Dataset) T axaDP) caso contrário Obedecendo- se CritP arada, o processo de indução é então parado para este ramo e é calculado o valor médio de F EB de Dataset (F EB Dataset), criando- se um nodo folha para esta aresta da árvore, rotulando tal nodo com F EB Dataset:
N odo F olha (F EB Dataset) Se o critério de parada não for obedecido, então o procedimento de definição do bloco é chamado (Def ineBloco, Algoritmo 6.2), passando como parâmetro Dataset e DP, retornando para a estrutura de dados BlocoN odo o átomo e o seu correspondente intervalo de coordenadas.
Assim, um novo nodo é criado na árvore:
N odo BlocoN odo Para cada aresta ar deste nodo, cujo rótulo é Bloco, testa- se as instâncias que estão dentro ou f ora deste intervalo.
Atualiza- se Dataset com essas instâncias para cada aresta (Datasetar), e fazse uma chamada recursiva do procedimento InduzArvore, passando como parâmetro Datasetar.
O Algoritmo 6.3 apresenta o pseudo-código para o processo recursivo de indução da árvore para o conjunto de dados Dataset.
A avaliação da árvore induzida por o Algoritmo 6.3 pode ser realizada seguindo os mesmos critérios tradicionais de indução de árvore de regressão, utilizando as métricas de erro M AE e RM SE.
Considerações do capítulo Em este capítulo foi apresentado o algoritmo 3 D-Tri, um novo algoritmo de indução de árvore de regressão para propriedades tridimensionais.
Este algoritmo foi desenvolvido para ler um conjunto de dados provenientes de resultados de simulação por DM, o qual contém como atributos as coordenadas tridimensionais de átomos de uma determinada proteína, tendo como atributo alvo um valor de F EB.
Cada instância desse conjunto de dados é uma conformação do modelo flexível dessa proteína.
A produção desse conjunto de dados, o qual foi denominado neste capítulo como Dataset, está descrita na Seção 6.1 e representada por o Algoritmo 6.1.
O algoritmo 3 D-Tri diferencia- se de abordagens clássicas de indução de árvore, seja para classificação ou para regressão, por dois principais motivos.
O primeiro se dá por a mais acurada interpretação do arquivo Dataset, o qual possui propriedades dependentes a cada três atributos.
Ou seja, cada coordenada no espaço euclidiano (x, y, z) é representada por atributos distintos, e este algoritmo as trata como um único objeto.
O segundo ponto por o qual o algoritmo 3 D-Tri se destaca é por a maneira como estes atributos são tratados para indução da árvore.
Isso é, estabeleceu- se uma Algoritmo 6.3: Indução da Árvore.
Procedure InduzArvore (Dataset) 8: Se DP não foi Computado então Computa DP 10:
Fim se 11: Se CritP arada então Computa F EB Dataset N odo F olha (F EB Dataset) 14: Senão BlocoN odo Def ineBloco (Dataset, SD) N odo BlocoN odo para cada aresta ar de N odo faça Datasetar instâncias que fazem parte do teste da aresta InduzArvore (Datasetar) fim para 21: Fim se estratégia de definição de um intervalo ideal para cada átomo, de modo com que para cada átomo é induzida uma árvore binária, no formato da apresentada na Figura 6.1, onde o teste das arestas avalia se os atributos pertencem ou não ao bloco definido para este átomo.
A leitura de Dataset, a definição do bloco e a indução da árvore estão descritas nas Seções 6.2.1 e no capítulo 7.
Em este capítulo é apresentado o teste realizado para o algoritmo 3 D-Tri, proposto no Capítulo 6.
São detalhados:·
o conjunto de dados utilizado;·
o plano de teste para esse conjunto de dados;·
os resultados do teste realizado.
Dados utilizados O teste do algoritmo 3 D-Tri foi realizado sobre dados do ligante ETH.
O conjunto de dados, denominado DatasetET H, foi gerado com base no Algoritmo 6.1.
Esse conjunto de dados foi produzido com um total de 12.024 colunas, que correspondem às coordenadas tridimensionais dos moleculares convergiram para esse ligante (ver Tabela 4.2).
Para fins de redução de dimensionalidade, DatasetET H foi pré-processado selecionando- se como atributos apenas os átomos dos top 10 resíduos identificados para este ligante, os quais fazem parte dos resíduos ILE20 e ILE193, respectivamente.
A sigla dos átomos são acompanhadas do número sequencial com que os mesmos aparecem no arquivo PDB da proteína InhA.
Por fim, DatasetET H foi dividido em duas partes, sendo uma para Treino (DatasetET H_ T reino) e outra para teste (DatasetET H_ T este).
Para o conjunto de teste foram extraídos aproximadamente 3% dos registros de DatasetET H, de modo com que DatasetET H_ T reino contém 2.943 conformações e DatasetET H_ T este contém 100 conformações.
Plano de teste O plano de teste do algoritmo divide- se em três etapas.
A primeira diz respeito à indução do modelo a partir de o algoritmo 3 D-Tri e os parâmetros utilizados para a indução.
A segunda se refere à indução de um modelo de árvore de regressão a partir de o algoritmo M5P para comparação dos resultados.
A terceira parte corresponde à avaliação dos modelos induzidos e às métricas utilizadas para tal avaliação.
A indução do modelo é feita a partir de o conjunto de dados DatasetET H_ T treino.
Os parâmetros configurados para execução do teste são:·
Número de grupos.
Foram definidos dois grupos (k $= 2) para a identificação do centróide na geração do bloco (Algoritmo 6.2);·
Taxa de erro na expansão do bloco.
O limite da taxa de erro para o critério de parada da expansão do bloco foi definido como 0,5;·
Taxa de população na expansão do bloco.
O bloco é expandido até atingir o limite de erro do item anterior, ou enquanto o número de exemplos que fazem parte do bloco for inferior a taxa de população mínima.
Essa foi definida como uma taxa de 0,05 em relação a o número de exemplos sendo computados;·
População mínima para a indução.
Definiu- se um mínimo de 10 exemplos para o critério de parada da indução;·
Taxa do desvio padrão para a indução.
Foi definido uma taxa de 0,05 para o desvio padrão dos exemplos sendo computados, para o critério de parada da indução;·
Profundidade da árvore.
Para que fosse induzido um modelo enxuto e de fácil interpretação, defindiu- se uma profundidade máxima de 5 níveis para a indução da árvore, incluindo os nodos folha.
Para a indução do modelo a partir de o algoritmo M5P, utilizou- se DatasetET H, em vez de DatasetET H_ T treino.
Para que os modelos pudessem ser comparados e serem o mais equivalentes possíveis, os seguintes parâmetros foram configurados:·
Árvore de regressão.
Optou- se por induzir árvore de regressão em vez de árvores modelo, uma vez que essa última é opção padrão do algoritmo;·
Número mínimo de instâncias.
Esse parâmetro se refere ao número mínimo de instâncias que devem estar presentes no nodo folha.
Esse parâmetro foi calibrado com 600 instâncias para que a árvore apresentasse uma profundidade equivalente à profundidade definida para a indução do modelo por o algoritmo proposto nesta Tese;·
Percentual de partição.
Para o teste do modelo habilitou- se a opção de percentual de partição, definindo- se 97%.
Com esse valor tem- se o percentual de instâncias para treino e de teste equivalentes aos conjuntos de dados DatasetET H_ T reino e DatasetET H_ T este.
Para a avaliação dos modelos induzidos são observadas as seguintes métricas:·
Erros. São calculados os erro médio absoluto e erro médio quadrático para as instâncias de teste, aplicando- se o modelo induzido por as instâncias de treino;·
Número de nodos.
São observados quantos nodos internos e nodos folha compõe o modelo induzido;·
Profundidade. É avaliada qual a profundidade máxima da árvore, considerando- se os nodos folha;·
Exemplos nos nodos folha.
São verificados o número de exemplos, considerando- se o conjunto de dados DatasetET H, que pertencem a cada nodo folha dos modelos induzidos;·
Distribuição das melhores conformações nos nodos folha.
São ordenadas as 100 melhores conformações (aquelas com valor de FEB mais negativo) para o conjunto de dados DatasetET H e avaliado como ocorre a distribuição dessas instâncias nos nodos folha;·
Semântica. Além de métricas preditivas, também é avaliada a semântica do modelo induzido, e como o modelo pode ser útil para um especialista de domínio.
Resultados As árvores resultantes dos modelos induzidos estão ilustradas nas Figuras 7.1 e 7.2 para o algoritmo 3 D-Tri e para o algoritmo M5P, respectivamente.
Para a árvore induzida por o algoritmo 3 D-Tri, os nodos indicam o átomo sendo testado, no mesmo formato do cabeçalho do conjunto de dados DatasetET H, conforme exemplo da Tabela 7.1.
As arestas apontam o teste do intervalo das coordenadas x, y, z identificado para o átomo, e esse intervalo está diposto no centro das duas arestas que dividem o nodo.
As arestas à esquerda dos nodos correspondem ao teste das instâncias que pertencem ao intervalo, e as arestas à direita dos nodos correspondem às instâncias que não fazem parte do intervalo.
Os nodos folha contém um número de indicação da folha, entre parênteses, e o valor médio de FEB de suas instâncias.
Em a árvore induzida por o algoritmo M5P (Figura 7.2), os nodos representam uma dada coordenada de um átomo do receptor.
A descrição do átomo está no mesmo formato do cabeçalho da Tabela a coordenada do átomo sendo testado por o nodo, onde esse valor de referência está disposto no centro das duas arestas que dividem o nodo.
O teste das arestas indicam, à esquerda, se a posição da coordenada do átomo é menor ou igual à posição de referência, e à direita se é maior do que o valor de referência.
Os nodos folha contém um número de indicação da folha, entre parênteses, e o valor médio de FEB de suas instâncias.
As métricas de erro, números de nodo e profundidade das árvores induzidas estão detalhadas na Tabela 7.2, para cada um dos dois algoritmos.
A essas métricas dá- se o nome de métricas preditivas.
Nota- se, por a Tabela 7.2, que os valores de erro (MAE e RSME) são muito próximos para os dois algoritmos, mas ainda sendo menor para o algoritmo M5P.
Além disso, por a calibragem dos parâmetros do M5P, foi possível obter uma árvore equivalente à do algoritmo 3 D-Tri em número de nodos e semelhante em relação a a profundidade.
Apenas a partir de as métricas preditivas não é possível inferir qual dos dois modelos apresenta melhor qualidade.
Em esse sentido, os modelos são também avaliados em termos de o contexto da base de dados DatasetET H. Para tanto, avalia- se o número de exemplos em cada nodo folha, e quantos desses exemplos pertencem aos 100 melhores exemplos de DatasetET H. Essa avaliação pode ser visualizada na Tabela 7.3.
Por a Tabela 7.3 nota- se que para o algoritmo 3 D-Tri, 96 das 100 melhores conformações estão concentradas no nodo folha 8 que é, justamente, o nodo com menor valor médio de FEB.
Em o modelo induzido pleo M5P essas mesmas conformações estão distribuídas entre as folhas da árvore, com maior concentração nas folhas 1, 3 e 8, que também representam os três menores valores médio de FEB para este modelo.
Avaliando- se essas métricas, nota- se que o modelo induzido por o algoritmo 3 D-Tri é promissor, por agrupar as melhores instâncias na mesma folha.
Entretanto, o modelo ainda precisa ser expandido para diminuir a concentração de exemplos no nodo folha e verificar a distribuição desses exemplos com essa expansão.
Por fim, é avaliada a semântica dos modelos induzidos.
Isso é, é verificado se a árvore pode ser confortavelmente interpretada por um especialista de domínio, e se a mesma pode ser utilizada para a efetiva seleção de conformações para redução do tempo de experimentos em futuros experimentos de docagem molecular.
O modelo induzido por o algoritmo 3 D-Tri tem em seus nodos um átomo e um valor de referência para a posição de suas coordenadas espaciais, até atingir o nodo folha.
Por selecionar o nodo folha de menor FEB médio, e sabendo- se que ele concentra as melhores conformações (Tabela 7.3), é possível selecionar conformações a partir de as posições dos átomos que fazem parte dos nodos que levam até aos nodos folha escolhidos.
Por outro lado, o modelo induzido por o M5P não trata as coordenadas espaciais dos átomos, indicando um valor de referência apenas para uma das três coordenadas.
Apesar de o modelo induzido apresentar bons resultados com relação a as métricas preditivas (Tabela 7.2), buscar uma única coordenada de um dado átomo faz pouco sentido para um especialista de domínio, em especial ao tentar identificar esse átomo dentro de a estrutura da proteína.
Isso se dá, principalmente, porque uma coordenada representa um vetor no espaço, não sendo possível que o especialista de domínio analise o modelo em termos de o posicionamento dos átomos e de sua afinidade química.
O diferencial do algoritmo 3 D-Tri está em permitir ao especialista essa análise tridimensional do átomo no espaço.
Considerações do capítulo Este capítulo apresentou um teste para o algoritmo 3 D-Tri, comparando seus resultados com os resultados do algoritmo M5P.
Foram definidos parâmetros de execução para os dois algoritmos, bem como um plano de avaliação dos modelos.
Essa avaliação se deu em termos de métricas preditivas, métricas de contexto e semântica dos modelos induzidos.
Por as métricas preditivas os dois modelos apresentam qualidade semelhante.
Por as métricas de contexto nota- se que o algoritmo 3 D-Tri é promissor para seleção de conformações, uma vez que as melhores conformações do conjunto de dados estão agrupadas num único nodo folha do modelo induzido.
No que diz respeito à semântica, entende- se que a árvore induzida por o algoritmo 3 D-Tri pode ser melhor interpretada por um especialista de domínio, de modo com que essa melhor interpretação facilite na seleção de conformações para futuros experimentos de docagem.
Em a pesquisa na literatura por trabalhos relacionados, não encontrou- se nenhum que se proponha a minerar dados de docagem molecular para seleção de conformações do receptor, nem trabalhos que utilizam mineração de dados sobre dados tridimensionais como apresentado nesta Tese.
Em este capítulo são apresentados os três trabalhos encontrados que apresentam uma maior proximidade em relação a o trabalho desenvolvido nesta Tese.
Os trabalhos relacionados são avaliados em termos de:·
Contexto de RDD.
Verifica- se se o trabalho está inserido num contexto de RDD;·
Propriedades Tridimensionais. É avaliado se o trabalho trata propriedades tridimensionais e como isso é realizado;·
Tarefa de mineração.
Confere- se se o trabalho utiliza alguma técnica de mineração de dados e, em caso positivo, qual o objetivo de mineração;·
Utilidade para o problema desta Tese.
Os trabalhos são analisados em termos de sua utilidade e aplicação para o problema desta Tese.
Banco de dados integrado para RDD de fármacos.
A ideia desse repositório é relacionar dados para a descoberta de novos compostos candidatos.
Em o repositório são integrados dados de diferentes bases disponíveis, como DrugBank, UniProt e BLAST.
A relação entre os dados é realizada por a construção de uma rede que contém nodos e arestas, onde os nodos são definidos como conceitos e as arestas são definidas como relações.
Os conceitos e relações são capturados das bases de dados integradas, onde essas informações são tratadas de forma textual.
Como resultados, é mostrado como a rede de integração pode ser útil para a busca de novos fármacos.
Esse trabalho está inserido no contexto de RDD.
Entretanto, não trata as propriedades estruturais das proteínas e ligantes, apenas identifica relações textuais entre elas.
Para a construção da rede o trabalho combina diferentes estratégias, como ontologias e técnicas de mineração de textos.
No entanto, não é detalhado qual tarefa de mineração foi utilizada.
A plataforma ONDEX pode ser utilizada no contexto desta tese para identificar na literatura novos ligantes que tenham chance de serem promissores para experimentos de docagem com a enzima InhA.
Banco de dados para informações tridimensionais de moléculas Em Groom e Allen é apresentada uma base de dados para armazenamento de informações tridimensionais de pequenas moléculas, juntamente com informações textuais a respeito de suas propriedades físico-químicas.
O repositório denominado CSD (Cambridge Structural Database) foi desenvolvido para facilitar a busca por conhecimento a respeito de a interação entre receptores e ligantes, sendo possível identificar a geometria das estruturas e de suas interações intermoleculares.
O ambiente do CSD tem por objetivo armazenar as estruturas tridimensionais das proteínas, buscando contribuir para o entendimento de interações receptor-ligante, mas sem fazer uso de uma abordagem completa de docagem molecular.
Ou seja, essa base de dados não relaciona estruturas de proteínas com resultados de experimentos de docagem molecular.
Os dados são analisados por os recursos que a plataforma oferece, sem fazer uso de mineração de dados.
Essa plataforma poderia contribuir para o entendimento dos resíduos próximos ao sítio de ligação e, assim, melhorar o pré-processamento dos dados para a execução do algoritmo 3 D-Tri.
Detecção de contatos atômicos em estruturas tridimensionais A proposta de Toofanny É de identificar contatos entre átomos de uma proteína, através da análise tridimensional das suas conformações, as quais são obtidas por simulações de dinâmica molecular.
Em esse sentido, é implementado um índice para acelerar o processo de identificação dessas estruturas na base de dados, onde o objetivo está em reduzir o tempo para descoberta desses contatos.
Como resultados é apresentado como esse índice contribuiu para a redução no tempo da identificação dos contatos.
Este trabalho está inserido no contexto de simulações por dinâmica molecular, mas não faz referência a experimentos de docagem sobre as conformações do modelo flexível do receptor.
As propriedades tridimensionais das estruturas são essenciais para a construção do índice proposto.
Os autores sugerem que pode ser aplicado mineração de dados sobre esses dados futuramente, mas não detalham como isso pode ser feito.
O índice proposto pode contribuir para a identificação de novas conformações promissoras, num modelo de dinâmica molecular mais extenso do que o de 3.100 ps utilizado nessa tese, e tendo por base os dados das distâncias calculadas no pré-processamento dos dados armazenados no FReDD.
Considerações do Capítulo Em este capítulo foram apresentados os três trabalhos encontrados na literatura que apresentam uma maior proximidade com o trabalho desta Tese.
Por esses trabalhos foi possível identificar que há espaço para pesquisas que consideram as estruturas tridimensionais de proteínas num contexto de simulação por dinâmica molecular, bem como interesse em realizar pesquisas que fazem uso dessas estruturas para pesquisas em bases e a identificação das relações entre elas.
Apesar de os objetivos desses trabalhos relacionados serem diferentes dos objetivos desta tese, a abordagem dos mesmos podem contribuir em algumas das etapas do trabalho desenvolvido nesta Tese.
Esta tese está inserida no contexto de desenho racional de fármacos, onde o principal objetivo é minerar dados de docagem molecular sobre um modelo flexível do receptor, gerado a partir de simulação por dinâmica molecular.
Com isso busca- se contribuir para a seleção de conformações promissoras do receptor para um dado tipo de ligante e, assim, reduzir o tempo de execução em novos experimentos de docagem.
Os dados utilizados nesta Tese são de um modelo flexível da proteína InhA, do M. Tuberculosis, considerando quatro ligantes distintos nos experimentos de docagem molecular:
NADH, PIF, TCL e ETH.
Durante o desenvolvimento desta Tese foram empregados esforços em fazer uso de diferentes etapas do processo de KDD para tratar os dados envolvidos, onde as principais contribuições estão no desenvolvimento de um repositório alvo para o armazenamento dos dados relacionados aos experimentos de docagem molecular, no pré-processamento desses dados e na aplicação de diferentes técnicas de mineração sobre os dados pré-processados, como regras de associação, árvores de decisão para classificação e árvores de decisão para regressão.&amp;&amp;&amp;
Em o capítulo 4 foi apresentado o repositório FReDD.
Este repositório foi desenvolvido de maneira com que pudesse ser suficientemente abrangente para armazenar, indexar e recuperar resultados de docagem molecular, bem como servir como uma infraestrutura de apoio ao pré-processamento dos dados.
Em esse repositório estão armazenados dados a respeito de a proteína e dos ligantes sendo considerados nesta Tese.
O pré-processamento foi realizado considerando as distâncias mínimas (em Angstroms) entre o ligante e os resíduos do receptor como atributos preditivos, e assumindo o valor de FEB para cada conformação como atributo alvo.
Os testes com esse repositório mostram que sua implementação não apenas contribuiu para o pré-processamento dos dados, mas também serviu de apoio para a identificação de padrões a respeito de a interação ligante-receptor sobre os dados armazenados.
Por essas análises foi possível encontrar relações ente os ligantes utilizados e o modelo flexível do receptor.
O capítulo 5 mostrou como os dados pré-processados a partir de o FReDD puderam ser utilizados por diferentes técnicas de mineração de dados.
Por regras de associação foi possível extrair regras que estabelecem relações de interações entre os diferentes resíduos do receptor, contribuindo para a identificação, por um especialista de domínio, de quais resíduos do receptor mais interagem com o ligante sendo testado.
Por árvores de decisão, seja para classificação ou para regressão, buscou- se extrair modelos que indicassem quais resíduos e sua distância em relação a o ligante contribuem para que o resultado de docagem produza um bom valor de FEB.
A o utilizar árvore de decisão para classificação, propôs- se um método de discretização do FEB e comparou- se os resultados dos modelos induzidos.
O mesmo foi feito para árvores de decisão para regressão, onde aplicou- se estratégias de pré-processamento sobre esses dados, buscando efetuar uma seleção de atributos baseada no contexto dos dados envolvidos.
Os modelos de árvore de decisão induzidos sobre esses dados foram pós-processados para identificar a sua qualidade quando de a seleção de conformações.
Os resultados obtidos com as diferentes técnicas de mineração aplicadas mostram alguns exemplos de informações que podem ser obtidas sobre os experimentos de docagem molecular, as quais seriam de difícil identificação sem a aplicação das técnicas de pré-processamento e rotinas de mineração de dados.
Apesar de os bons resultados encontrados, os mesmos não se mostraram suficientes para a efetiva seleção das conformações.
Isso porque não é possível obter as distâncias dos resíduos do receptor em relação a o ligante sem ter- se efetuado experimentos de docagem molecular.
Em direção a a uma nova estratégia de mineração para esse contexto, buscou- se considerar como dados de entrada as propriedades tridimensionais de cada átomo do receptor para predizer um dado valor de FEB para cada ligante.
O capítulo 6 apresentou o algoritmo 3 D-Tri, um novo algoritmo de indução de árvore de regressão capaz de identificar as propriedades tridimensionais inerentes ao problema e induzir uma árvore que indique as melhores posições no espaço Euclidiano de determinados átomos que possam resultar em importantes resultados de FEB e, assim, contribuir para a efetiva seleção de conformações do receptor.
O algoritmo 3 D-Tri diferencia- se de abordagens clássicas de indução de árvore por fazer uso da relação entre as coordenadas x, y, z e considerar- las como um único objeto, bem como por a maneira como esses atributos são tratados para a indução da árvore.
Isto é, por a estratégia de definição de um intervalo ideal para cada átomo, representado por, onde o particionamento do atributo testa se o átomo sendo considerado faz parte ou não deste bloco.
Em o capítulo 7 foi apresentado o teste do algoritmo 3 D-Tri para o ligante ETH, onde utilizouse as coordenadas dos átomos da proteína para cada conformação obtida por simulação de DM.
O modelo induzido por o algoritmo 3 D-Tri foi comparado com um modelo induzido por o algoritmo M5P.
A avaliação considerou métricas preditivas, métricas de contexto e a semântica dos modelos induzidos.
As métricas preditivas indicam que os dois modelos têm qualidade semelhantes.
As métricas de contexto sugerem que o algoritmo 3 D-Tri é promissor para a seleção de conformações por agrupar as melhores conformações do conjunto de dados num único nodo folha.
Por fim, com relação a a semântica dos modelos, aquele produzido por o algoritmo 3 D-Tri pode ser melhor interpretado por um especialista de domínio e, assim, facilitar a seleção de conformações para futuros experimentos de docagem molecular.
É importante ressaltar que o algoritmo foi parcialmente implementado e, por isso, os testes não foram realizados de maneira exaustiva.
Publicações O trabalho desenvolvido durante essa tese atingiu importantes resultados, com os quais foi possível obter as seguintes publicações científicas:·
Dois artigos publicados em periódicos e um sob revisão;·
Nove artigos em conferência, incluindo artigos completos e resumos· Um capítulo de livro;·
Além disso, a proposta do algoritmo 3 D-Tri foi aceita para apresentação no forum de doutorado do SIAM-SDM 2011.
Trabalhos futuros A o término de uma tese de doutarado, espera- se que o trabalho desenvolvido e resultados obtidos não representem o fim da pesquisa, mas sim um importante avanço para a identificação de novas oportunidades.
Com o trabalho apresentado nesta Tese, foi possível identificar diferentes oportunidades relevantes para a continuidade da pesquisa, de entre as quais pode- se citar:·
Implementação completa do algoritmo 3 D-Tri;·
Testes mais exaustivos para todos os quatro ligantes, calibrando diferentes valores para os parâmetros definidos no plano de testes;·
Explorar diferentes maneiras de identificação da posição do átomo em relação a o intervalo do bloco;·
Expandir os testes para outros domínios de problema como, por exemplo, para proteínas e ligantes relacionadas a outras doenças de impacto social;·
Examinar diferentes tipos de conformações da região espacial de um átomo, que não apenas um bloco.
De entre essas conformações pode- se investigar como representar esse espaço por uma esfera ou diferentes tipos de superfície.
