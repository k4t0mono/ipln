Em a Geofísica, a subdivisão apropriada de uma região em segmentos é extremamente importante.
O ICTM (Interval Categorizer Tesselation Model) é uma aplicação capaz de categorizar regiões geográficas utilizando informações extraídas de imagens de satélite.
O processo de categorização de grandes regiões é considerado um problema computacionalmente intensivo, o que justifica a proposta e desenvolvimento de soluções paralelas com intuito de aumentar sua aplicabilidade.
Recentes avanços em arquiteturas multiprocessadas caminham em direção a arquiteturas do tipo Em uma (Non-uniform Memory Access), as quais combinam a eficiência e escalabilidade das máquinas MPP (Massively Parallel Processing) com a facilidade de programação das máquinas SMP (Symmetric Multiprocessors).
Em este trabalho, é apresentada a Numa-ICTM:
Uma solução paralela do ICTM para máquinas Em uma explorando estratégias de alocação de memória.
Primeiramente, o ICTM é paralelizado utilizando- se somente OpenMP.
Posteriormente, esta solução é otimizada utilizando- se a interfade MAI (Memory Affinity Interface), a qual proporciona um melhor controle sobre a alocação de dados em memória em máquinas Em uma.
Os resultados mostram que esta otimização permite importantes ganhos de desempenho sobre a solução paralela que utiliza somente OpenMP.
Palavras-chave: ICTM, Computação de Alto Desempenho, Em uma, Libnuma, OpenMP, MAI.
Contexto e aplicabilidade do ICTM.
Máquinas Em uma utilizadas neste trabalho Afinidade de threads e memória Memory Interface Library (MAI) Distribuição do trabalho entre threads Trabalhos relacionados O modelo ICTM Integração com a MAI.
Avaliação de desempenho Conclusão Numa-ICTM x OpenMP-ICTM.
1 Introdução A Geofísica é uma ciência voltada à compreensão da estrutura, composição e dinâmica do planeta Terra, sob a ótica da Física.
Consiste basicamente na aplicação de conhecimentos da Física ao estudo da Terra.
Em este contexto, sob o ponto de vista dos pesquisadores desta área, a subdivisão apropriada de uma área geográfica em segmentos é extremamente importante, visto que é possível extrapolar os resultados obtidos em determinadas partes de um segmento específico já estudadas anteriormente para outras partes deste mesmo segmento ainda não analisadas.
Isto permite um bom entendimento de um segmento como um todo, sem a necessidade de analisar- lo completamente.
O ICTM (Interval Categorizer Tessellation Model) é um modelo multi-camada desenvolvido para categorização de regiões geográficas utilizando informações extraídas de imagens de satélite.
Porém, a categorização de grandes regiões requer um alto poder computacional.
Além disso, o processo de categorização como um todo resulta num uso intensivo de memória.
Conseqüentemente, estas duas características principais motivam o desenvolvimento de uma solução paralela para esta aplicação com intuito de categorizar grandes regiões de forma mais rápida.
Em arquiteturas Uma (Uniform Memory Access) tradicionais, o computador possui somente um controlador de memória, o qual é compartilhado por todos os processadores.
Esta única conexão com a memória muitas vezes se torna o gargalo do sistema quando muitos processadores acessam a memória ao mesmo tempo.
Este problema se mostra ainda pior em arquiteturas com um grande número de processadores, em as quais um único controlador de memória não é satisfatoriamente escalável.
Portanto, estes tipos de arquiteturas podem não suprir os requisitos básicos para determinados tipos de aplicações.
O ICTM é um exemplo deste tipo de aplicação, onde resultados melhores poderão ser obtidos em arquiteturas que ao mesmo tempo ofereçam uma memória com alta capacidade de armazenamento e reduzam o problema da disputa da mesma entre os processadores.
Em arquiteturas Em uma (Non-Uniform Memory Access) o sistema é dividido em múltiplos nodos.
Esta divisão, tem como principal objetivo aumentar a escalabilidade de arquiteturas Uma.
Este tipo de arquitetura se caracteriza por o uso de hierarquias de memória as quais são vistas por o desenvolvedor como sendo uma única memória global.
Arquiteturas do tipo Em uma combinam a eficiência e escalabilidade das arquiteturas MPP (Massively Parallel Processing) com a facilidade de programação das arquiteturas SMP (Symmetric Multiprocessors).
Porém, devido a o fato da memória ser dividida em blocos, o tempo necessário para realizar um acesso a ela é condicionado por a &quot;distância «entre o processador (o qual acessa a memória) e o bloco de memória (em o qual o dado a ser acessado está fisicamente alocado).
Para que seja possível utilizar ao máximo os recursos oferecidos por as arquiteturas Em uma é importante que sejam considerados alguns fatores.
O primeiro de eles é a questão referente a os diferentes tempos de acesso à memória.
Uma utilização incorreta da memória fará com que o desenvolvedor não obtenha os benefícios oferecidos por ela.
Outro fator importante é arquitetura alvo, para a qual será paralelizada a aplicação.
É necessário que o desenvolvedor tenha conhecimento da forma com que ela está organizada para que então seja possível realizar um bom uso da mesma.
Por fim, além de o conhecimento geral da aplicação a ser paralelizada é importante também conhecer os mecanismos e estratégias utilizadas por a aplicação ao acessar os dados armazenados na memória.
Em o trabalho realizado por Silva et al.,
os autores apresentaram uma proposta de paralelização do ICTM para clusters utilizando- se o padrão MPI (Message Passing Interface).
Em este trabalho, os autores exploraram três possibilidades de decomposição do problema, são elas:·
Camadas: Cada processo paralelo calcula uma determinada camada do modelo;·
Funções: Cada processo paralelo calcula uma determinada fase do modelo;·
Domínios: Cada processo paralelo calcula uma parte da região que será analisada.
Estas três possibilidades de decomposição do problema são bastantes distintas e algumas de elas exigem grande comunicação entre os processos.
Tendo em vista que esta solução foi proposta para clusters, os autores optaram por a decomposição em camadas, pois esta mostrouse uma forma simples e direta de paralelizar o problema.
Além disso, como cada camada pode ser processada de forma individual, não haverá grandes necessidades de trocas de mensagens entre os processos.
Em a prática, a decomposição em camadas foi implementada utilizando- se o modelo mestreescravo.
O processo mestre é responsável por ler o arquivo de dados das camadas, criar nc tarefas (o número total de tarefas é igual ao número de camadas -- nc), realizar a distribuição inicial das tarefas e aguardar os resultados.
Os processos escravos somente são responsáveis por processar as tarefas (categorizar camadas) e informar o processo mestre quando o processamento for finalizado.
A a medida em que os resultados forem sendo enviados por os escravos, caso hajam mais tarefas a serem processadas, o mestre enviará novamente mais trabalho para os escravos ociosos.
Os resultados obtidos com a aplicação deste modelo apresentam interessantes ganhos de desempenho.
Esta conclusão foi obtida através da análise dos resultados obtidos em dois dife-rentes clusters.
Porém, considerando o fato de que cada processo escravo irá calcular uma dada camada do modelo, o tamanho máximo desta camada (em termos de espaço físico a ser armazenado em memória) está limitado por a quantidade de memória disponível no nodo em que este processo estará sendo executado.
Como conseqüência disto, grandes regiões não poderão ser categorizadas utilizando este método de decomposição do problema, visto que é pouco comum dispor- se de um cluster onde cada nodo possua uma memória principal da ordem de dezenas de Por outro lado, num outro trabalho realizado por Silva et al.,
os autores propuseram uma extensão do trabalho anteriormente citado para o ambiente de grades computacionais.
No caso de grades computacionais, existem diversos ambientes que oferecem serviços e controle de tarefas a serem distribuídas na grade.
Um destes ambientes é o OurGrid, que foi utilizado no referido trabalho.
Os resultados deste trabalho mostraram o desempenho da solução para grades computacionais ao utilizar as três formas de composição do problema:
Em camadas, domínios e funções.
A solução para grades permitiu com que regiões maiores pudessem ser processadas, mostrando também ganhos interessantes de desempenho.
Além de as três formas de decomposição do problema, duas abordagens relacionadas a localização dos dados foram propostas.
A primeira utilizou dados geográficos centralizados, enquanto a segunda utilizou dados geográficos distribuídos.
A segunda solução é mais apropriada para grades computacionais, visto que através da distribuição dos dados há uma redução drástica da comunicação entre nodos da grade.
Todavia, para que esta solução mostre um ganho de desempenho considerável, os dados precisam estar previamente armazenados nos nodos da grade.
As motivações para realização deste trabalho surgiram da análise das características do ICTM e suas propostas de paralelização anteriormente citadas, da identificação da importância do ICTM no contexto da Geofísica e da possibilidade de avaliar os benefícios ao utilizar- se a MAI (Memory Affinity Interface):
Uma biblioteca específica para programação para máquinas Em uma.
Os trabalhos descritos na Seção 1.1 popuseram diferentes formas de paralelizar o ICTM e mostraram interessantes ganhos de desempenho.
Entretanto, estas propostas possuem limitações, especialmente quanto a o fato de utilizar regiões extremamente grandes a serem categorizadas.
Tendo em vista o exposto e considerando as características do ICTM, é bastante provável que esta aplicação possa ser melhor adaptada para arquiteturas com memória compartilhada.
Através do uso de máquinas Em uma é possível utilizar não somente os benefícios oferecidos por uma arquitetura com memória compartilhada, mas também a possibilidade de desenvolver uma solução paralela que apresente bom desempenho com a utilização de muitos processadores.
O uso de uma memória compartilhada permite o desenvolvimento de outras formas de paralelização do problema, pois o custo de comunicação neste tipo de arquitetura é bastante inferior em comparação com os obtidos em clusters.
O ICTM é uma aplicação que auxilia no estudo de regiões geográficas e pode ser aplicado em diversas áreas da Geofísica.
Porém, a solução seqüencial limita- se a somente análises de regiões relativamente pequenas, o que muitas vezes não é interessante no ponto de vista de pesquisadores da área.
Arquiteturas de alto desempenho aparecem como uma forma de reduzir esta limitação, possibilitando a categorização de grandes áreas geográficas.
Uma das arquiteturas de alto desempenho que está se tornando difundida nos últimos anos é representada por as máquinas do tipo Em uma.
Porém, por introduzirem o conceito de &quot;distância «através dos diferentes tempos de acesso à memória, são arquiteturas mais complexas em comparação com arquiteturas do tipo Uma.
Para tratar com esta maior complexidade, o INRIA (Institut National de Recherche en Informatique et en Automatique) em Grenoble (França), está desenvolvendo uma interface inovadora denominada MAI.
A utilização desta interface na solução paralela do ICTM proposta neste trabalho servirá também para avaliar o ganho de desempenho e os benefícios ao utilizar- la, servindo como um caso de estudo onde a interface foi aplicada à paralelização de uma aplicação real.
Isto somente foi possível devido a uma cooperação entre o GMAP (Grupo de Modelagem de Aplicações Paralelas), o qual o autor desta dissertação pertence, e o grupo Mescal (Middleware Efficiently SCALable), localizado na Universidade de Grenoble (França).
O objetivo principal deste trabalho é apresentar uma solução paralela do ICTM para máquinas Em uma explorando estratégias de alocação de memória.
Primeiramente, será apresentada a paralelização do ICTM através do uso da API (Application Program Interface) OpenMP (Open Multiprocessing), denominada OpenMP-ICTM).
OpenMP provê um modelo portável e escalável para desenvolvedores de aplicações paralelas para arquiteturas com memória compartilhada.
Porém, esta API não foi originalmente desenvolvida para arquiteturas com acesso nãouniforme à memória.
Tendo isto em vista, através da utilização da MAI (Memory Affinity Interface), diferentes políticas de memória serão aplicadas na solução paralela proposta neste trabalho com o intuito de otimizar a utilização das arquiteturas Em uma (solução denominada Numa-ICTM).
A avaliação dos benefícios ao utilizar- se a interface MAI servirá como um objetivo secundário deste trabalho.
Para a avaliação de desempenho da solução proposta serão utilizadas duas arquiteturas Em uma com características bastante distintas.
Com isto, será possível não somente analisar- se o impacto das arquiteturas no desempenho da solução paralela, mas também como será seu comportamento sob o uso de diferentes políticas de memória.
Este trabalho possui a seguinte estrutura:
O processo de categorização e detalhes de implementação;
Em esta solução, somente a API OpenMP é utilizada e nenhuma atenção é dada à alocação de dados em memória.
Posteriormente, uma análise de desempenho desta solução é feita considerando- se as duas arquiteturas alvo.
Com isto, mostra- se a importância de utilizar afinidade de threads e memória com intuito de utilizar ao máximo as arquiteturas alvo.
A proposta inicial de um modelo para categorização de áreas geográficas foi apresentado em.
Esta proposta deu origem a diversos outros trabalhos.
Estes trabalhos foram idealizados com o intuito de generalizar cada vez mais o modelo inicial.
Os projetos criados por o GMFC (Grupo de Pesquisa de Matemática e Fundamentos da Computação) denominados ACI (Autômatos Celulares Intervalares) com Aplicações em Topografia, no contexto do fundo setorial CT-PETRO (Fundo Setorial do Petróleo e Gás Natural), e FMC2 (Fundamentos Matemáticos da Computação:
Modelos e Aplicações de Computações Intervalares), no contexto do fundo setorial CT-INFO (Fundo Setorial para Tecnologia da Informação), possibilitaram, entre outros resultados, a criação de uma versão seqüencial para a categorização de áreas geográficas denominada ICTM (Interval Categorizer Tesselation Model).
Em este capítulo é apresentado o funcionamento geral do ICTM.
O processo de categorização utilizado por o ICTM é composto por diversas etapas e envolve a manipulação e o processamentos de diversas matrizes de dados.
O entendimento do processo de categorização servirá como base para, posteriormente, definir a estratégia de paralelização para máquinas Em uma.
Além disso, são apresentadas as etapas mais custosas do processo de categorização e a influência de alguns parâmetros no desempenho da versão seqüencial.
O emprego da computação no processamento de dados geográficos vem crescendo ao longo de os últimos anos.
Em o início da década de 80, os primeiros SIGs (Sistemas de Informações Geográficas) começaram a ser oferecidos comercialmente.
Em poucas palavras, um SIG é um conjunto de aplicativos, hardwares, procedimentos de entrada e saída de dados, entre outros.
O objetivo principal é fornecer funções de coleta, tratamento e apresentação de informações.
Os SIGs podem ser aplicados em diferentes áreas e por isso são muito importantes.
Em a logística, podem ser aplicados para escolha da melhor rota a ser seguida por caminhões para a distribuição de produtos de uma empresa.
Em a agricultura, através do suporte para plantação e colheita extraído através de previsões climáticas.
Estes são alguns dos inúmeros exemplos de aplicação dos SIGs.
A relação entre os SIGs e o ICTM está na aplicação destes na análise de terrenos para as ciências ambientais.
Pode- se dizer que o ICTM implementa uma funcionalidade de um SIG, pois sua função é extrair informações geográficas (como por exemplo vindas de imagens de satélite) e gerar outras informações geográficas.
Uma outra questão importante é o aspecto inovador do modelo ICTM.
Os resultados da categorização gerados por o ICTM ainda não são produzidos por os SIGs existentes.
O ICTM é um modelo multi-camada baseado no conceito de tesselações para categorização de áreas geográficas.
O conceito de &quot;multi-camada «está baseado no fato de que a categorização de uma mesma região poderá considerar diferentes características tais como:
Sua topografia, vegetação, clima, uso do terreno, entre outras.
Cada uma destas características é representada no modelo como sendo uma camada.
Através de um procedimento apropriado de projeção numa camada base, é possível construir uma categorização final, a qual permite uma análise de como estas características são combinadas.
Esta análise possibilita, para os pesquisadores da área, uma compreensão geral de dependências mútuas entre elas.
O conceito geral do ICTM é mostrado na Figura 1.
Os dados que servem como entrada para o ICTM são extraídos de imagens de satélites, em as quais as informações são referenciadas por pontos correspondentes as coordenadas de latitude e longitude.
A região geográfica é então representada por uma tesselação regular.
Esta tesselação é determinada por a subdivisão da área total em sub-áreas retangulares suficientemente pequenas.
Cada uma destas sub-áreas representa uma célula da tesselação (Figura 2).
Esta subdivisão é feita de acordo com o tamanho da célula, o qual é estabelecido por um analista geofísico ou ecologista e está diretamente associado ao grau de refinamento dos dados de entrada.
Com intuito de minimizar e controlar os erros oriundos da discretização da região em células da tesselação, o ICTM utiliza Matemática Intervalar.
O uso de intervalos traz diversos benefícios ao processo de categorização, porém sua utilização faz com que ocorra um aumento da necessidade de poder computacional para o processamento da categorização.
A seguir será dada uma explicação mais aprofundada do processo de categorização e os impactos no desempenho da solução seqüencial quando são alterados parâmetros importantes do modelo, tais como a dimensão das matrizes e o raio.
O processo de categorização é realizado em cada camada do modelo de entrada de forma seqüencial.
Portanto, todas as camadas passam por o mesmo processo de categorização para que, posteriormente, estes resultados possam ser projetados numa camada base.
A categorização de cada camada é composta por diversas etapas seqüenciais, onde cada uma utiliza os resultados obtidos na etapa anterior.
A tesselação mostrada na Figura 2 é representada na prática como um conjunto de matrizes com nr linhas e nc colunas.
É possível dividir o processo de categorização em duas fases:
A fase de preparação e a fase de categorização.
A Figura 3 apresenta o processo de categorização de uma camada c qualquer do ICTM.
A fase de preparação compreende três etapas seqüenciais.
A primeira de elas involve a leitura dos dados de entrada (extraídos de imagens de satélite) e estes são armazenados numa matriz denominada Matrix Absoluta.
Normalmente, imagens fotografadas por satélites armazenam muitas informações as quais muitas vezes são irrelevantes.
Desta forma, para cada subdivisão da tesselação de entrada são extraídos os valores médios e armazenados na Matrix Absoluta.
A Matriz Absoluta é normalizada através da divisão dos valores de cada célula por a célula que possui o maior valor, criando- se assim a Matriz Relativa.
Como os dados extraídos de imagens de satélite são muito exatos, os erros contidos na Matriz Relativa são resultado da discretização da região em células da tesselação.
Por este motivo, técnicas da Matemática Intervalar são utilizadas para controlar os erros associados aos valores das células (vantagens do uso de intervalos para resolver problemas semelhantes podem ser vistas em e).
Portanto, duas Matrizes Intervalares são criadas contendo os valores intervalares para as coordenadas x e y, representando assim, a terceira etapa da fase de preparação.
Terminada a fase de preparação, as Matrizes Intervalares são utilizadas como dados de entrada para a fase de categorização.
Esta é a principal fase do modelo, capaz de categorizar as células de acordo com a suas características em comum.
O processo de categorização compreende duas etapas.
De a mesma forma que a fase de preparação, cada etapa da fase de categorização depende dos resultados obtidos na etapa anterior.
A primeira etapa desta fase de categorização será responsável por construir a Matriz de Estados.
Esta matriz representará a relação do comportamento de cada célula com seus vizinhos em cada uma das quatro direções:
Norte, sul, leste e oeste.
O processamento é feito por direção, ou seja, primeiro cada célula é comparada com seus vizinhos na direção norte, posteriormente cada célula é comparada com seus vizinhos na direção sul e assim sucessivamente.
O número de células vizinhas a serem utilizadas para determinar a relação de cada célula com as demais é parametrizável.
A este parâmetro é dado o nome de raio.
O processo de comparação de cada célula com as suas vizinhas leva em consideração a propriedade que está sendo representada por a camada.
Para tanto, esta propriedade é representada por uma função e esta é utilizada para comparar cada célula com a sua vizinhança.
Desta forma, esta etapa se caracteriza por analisar a monotonicidade da função que mapeia a propriedade representada na camada nas quatro direções.
Em o modelo teórico, estas informações são representadas através de quatro registradores de monotonicidade ­ reg..
N (norte), reg..
S (sul), reg..
L (leste) and reg..
O (oeste) ­ indicando o comportamento da célula em relação a as quatro direções.
Para as células que não fazem parte da borda:
Reg.. X $= 0, se existe uma função de aproximação não-crescente entre a célula e seus vizinhos na direção X;
Caso contrário, reg..
X $= 1.
No caso de células que fazem parte da borda nas direções norte, sul, leste e oeste, reg..
N $= 0, reg..
S $= 0, reg..
L $= 0 and reg..
O $= 0, respectivamente.
Sejam wreg.
N $= 1, wreg.
S $= 2, wreg.
L $= 4 and wreg.
O $= 8 pesos associados aos registradores de declividade.
A Matriz de Estados é então definida como uma matriz de dimensão nr × nc, onde cada valor representa o estado da célula correspondente, calculado por a Equação 2.1.
Desta forma, cada célula poderá assumir somente um estado do intervalo de valores estadoscelula $ .
Em a prática (implementação do ICTM), a Matriz de Estados do modelo teórico foi representada por 4 matrizes separadas, onde cada uma representa o comportamento de cada célula com as suas vizinhas numa única direção (Figura 4).
Portanto, durante o processo de comparação de cada célula numa direção, somente uma matriz estará sendo escrita em memória, melhorando o desempenho da aplicação.
Finalmente, a última etapa compreende a criação da Matriz de Limites.
Em esta matriz é armazenada a informação de quais células são consideradas limítrofes entre regiões de características distintas.
Com isto é possível identificar quais grupos de células possuem características em comum.
É importante salientar que a categorização de somente uma camada que representa uma grande região possui um custo computacional muito alto.
Este custo está relacionado basicamente a dois parâmetros:
A dimensão da tesselação e o número de vizinhos a serem analisados.
Quanto maior a região e maior o raio, maior será a necessidade de poder computacional.
A versão seqüencial do modelo ICTM foi implementada utilizando- se a liguagem C+ e utiliza a biblioteca OpenGL para visualização gráfica dos resultados.
Porém, neste trabalho será utilizado somente o módulo de categorização, sendo este o alvo da paralelização deste trabalho.
Os últimos anos foram marcados por o grande investimento em estudos sobre microprocessadores.
Cada vez mais nos deparamos com novas arquiteturas de computadores e, principalmente, com a diminuição considerável do tamanho dos chips.
Porém, a velocidade de processamento de uma máquina não está somente relacionada ao poder de processamento dos microprocessadores.
Um microprocessador somente conseguirá processar dados de forma rápida se estes dados puderem ser acessados também de forma rápida na memória.
A tecnologia atual não permite a construção de memórias com grande capacidade de armazenamento que acompanhem a velocidade dos microprocessadores.
Porém, deseja- se que cada vez mais seja possível construir memórias que, ao mesmo tempo, possuam maior capacidade de armazenamento e menor tempo de acesso (latência).
Estudos sobre novas tecnologias de microprocessadores continuam avançando.
Atualmente, ao invés de continuar investindo em formas de aumentar a freqüência dos processadores, as empresas mudaram sua estratégia e estão investindo em arquiteturas com mais de um núcleo de processamento.
Arquiteturas mais avançadas com mais de um processador, que antigamente eram somente utilizadas em áreas específicas da Física, Geografia e Biologia, estão presentes nos computadores pessoais através da tecnologia multi-core.
Um computador convencional consiste, em poucas palavras, num processador, também chamado de CPU (Central Processing Unit), executando um programa que está armazenado numa memória principal centralizada.
Normalmente, o tempo de acesso à memória por o processador é uniforme, ou seja, depende da latência (custo de comunicação) com a memória.
O processador é conectado à memória primária e ao sistema de I/ O através de um barramento.
A memória cache ajuda a manter o processador ocupado através da redução do tempo de acesso aos dados (usufruindo das características de localidade temporal e espacial).
Um multiprocessador com memória centralizada é uma extensão simples da arquitetura com um único processador.
Em este caso, processadores são adicionados e conectados ao mesmo barramento compartilhando os dados localizados na memória primária.
Este tipo de sistema é denominado, por o ponto de vista do tempo de acesso à memória, uma arquitetura do tipo Uma (Figura 5).
O problema dos multiprocessadores com memória centralizada, também conhecidos por máquinas SMP, é que a largura de banda do barramento tipicamente limita o número de processadores conectados a ele.
A a medida em que são adicionados mais processadores ao mesmo barramento ocorre uma grande disputa para a utilização do mesmo, fazendo com que este tipo de arquitetura não tenha bom desempenho com muitos processadores.
Em multiprocessadores, a comunicação entre os processadores é feita através de dados compartilhados.
Dizemos que um dado é compartilhado quando este é utilizado por mais de um processador.
Por outro lado, um dado é dito privado quando este é acessado somente por um único processador.
A alternativa para aumentar o limite do número de processadores num sistema com memória central compartilhada é a distribuição desta memória, dividindo- a em blocos, entre os processadores.
Desta forma, cria- se um sistema onde o acesso a uma memória local é muito mais rápido do que ao acesso a uma memória remota.
Considera- se acesso local quando um processador busca um dado ocasionado a leitura no bloco de memória que está diretamente conectado a ele.
Por outro lado, o acesso remoto ocorre quando o dado está localizado fisicamente em outro bloco de memória (o qual está mais próximo de outro processador).
Em este caso, o tempo de acesso é maior e é realizado através de uma rede de interconexão.
Um multiprocessador com memória distribuída emprega um único espaço de endereçamento, permitindo com que cada processador possa endereçar toda a memória.
Além disso, o mesmo endereço em diferentes processadores faz referência à mesma posição em memória.
Este tipo de sistema é também denominado multiprocessador Em uma, pois o tempo de acesso à memória varia consideravelmente, dependendo se o endereço que está sendo referenciado está localizado na memória &quot;próxima «ao processador ou não.
A Figura 6 mostra a idéia genérica de um sistema com memória distribuída (Em uma).
Arquiteturas Em uma introduzem a noção de distância entre componentes do sistema, como por exemplo, CPUs, memória e I/ O. A métrica utilizada para determinar a distância varia, porém, a quantidade de &quot;saltos «(hops) é uma métrica popular e bastante utilizada.
Estes termos significam essencialmente o mesmo daqueles utilizados em redes de interconexão.
Em o exemplo da Figura 6, se um determinado dado não está localizado no módulo de memória mais &quot;próximo «do processador ocorrerá o uso da rede de interconexão para que o dado possa ser buscado em outro bloco.
Uma métrica muito importante que permite avaliar a relação entre o tempo de acesso local e remoto de uma arquitetura Em uma é o fator Em uma.
O fator Em uma (F N) é a razão entre o tempo médio necessário para buscar um determinado dado armazenado no bloco de memória distate de um processador (tremoto) e tempo médio necessário para buscar este mesmo dado no bloco de memória próximo a este mesmo processador (tlocal).
A Equação 3.1 demonstra como é feito o cálculo do fator Em uma.
Basicamente, a forma com que os processadores e blocos de memória são organizados numa arquitetura Em uma e o fator Em uma permitem uma boa compreensão geral da arquitetura.
Além disso, fornecem subsídios ao desenvolvedor para que este possa definir suas estratégias de paralelização de aplicações para este tipo de arquitetura.
Localidade de dados O mecanismo principal utilizado em caches agrupa posições de memória contíguas em blocos.
Quando um processo referência pela primeira vez um ou mais bytes em memória, o bloco completo é transferido da memória principal para a cache.
Desta forma, se outro dado pertencente a este bloco for referenciado posteriormente, este já estará presente na memória cache, não sendo necessário buscar- lo na memória principal.
Portanto, sabendo- se o tamanho dos blocos utilizados por a cache e a forma com que os dados são armazenados por o compilador é possível desenvolver uma aplicação paralela que possa melhor utilizar estas características.
Blocos são utilizados em caches devido a características básicas em programas seqüenci-ais:
Localidade espacial e temporal.
Se um determinado endereço foi referenciado, há uma grande chance do endereço seguinte também ser referenciado num curto espaço de tempo (localidade espacial).
Por isto, ao invés de somente trazer um único dado da memória principal para a cache, um bloco de dados é copiado, pois há uma grande probabilidade de que os dados contíguos a ele também sejam utilizados em curto espaço de tempo.
Por outro lado, um programa é também normalmente composto por um conjunto de laços.
Cada laço poderá acessar um grupo de dados de forma repetitiva.
Por causa de isto, se um endereço de memória foi referenciado, há também a chance deste ser referenciado novamente em pouco tempo (localidade temporal).
A maior desvantagem em utilizar- se blocos de dados contíguos em multiprocessadores é que diversos processadores podem necessitar de partes diferentes de um bloco, como por exemplo bytes diferentes.
Se um determinado processador escreve somente numa parte de um bloco em sua cache (somente alguns bytes), cópias deste bloco inteiro nas caches dos demais processadores devem ser atualizadas ou invalidadas, dependendo da política de coerência de cache utilizada.
Esta questão é conhecida por false sharing (Figura 7), podendo reduzir o desempenho de uma aplicação paralela.
Em esta figura, três processadores acessam dados que estão presentes num mesmo bloco.
Por isto, cada processador possui, em sua cache, uma cópia deste bloco.
Ocorrerá false sharing, pois cada processador acessa bytes diferentes neste bloco, fazendo com que todo o bloco seja invalidado ou atualizado nas caches dos demais processadores.
Por questões de simplificação, os módulos de memória foram omitidos na figura.
Em máquinas Em uma, estas questões referentes à localidade dos dados se agravam.
Em máquinas Uma, se um endereço em memória referenciado por um processador não está disponível em sua cache, este deverá ser trazido da memória principal.
O tempo desperdiçado para esta operação será uniforme, visto que a memória é centralizada neste tipo de arquitetura.
Porém, em arquiteturas Em uma, este endereço poderá pertencer a um módulo de memória que não está próximo de o processador, havendo assim um desperdício de tempo ainda maior.
Existem mecanismos que permitem o programador definir em que módulo de memória um determinado dado deverá ser armazenado.
Em grande parte das arquiteturas Em uma, um dado é armazenado no nodo em que o acessou primeiro:
Política denominada first-touch (ver Seção que o próprio sistema operacional mantenha este controle sobre a localidade dos dados.
Em este caso, o sistema operacional poderá alterar a localidade dos dados à medida que o programa é executado.
Isto pode ser feito de duas formas:
Através da migração ou replicação de páginas.
Em a primeira, existe somente uma cópia de cada página de memória, podendo esta ser movida entre os nodos.
Em a segunda, uma página poderá ter diversas cópias, cada uma residindo num nodo diferente do sistema.
Tendo em vista as questões expostas, é importante que o desenvolvedor possua conhecimento sobre os mecanismos utilizados no gerenciamento de memória em máquinas Em uma.
Tais fatores influenciam nas estratégias utilizadas na paralelização de aplicações neste tipo de arquitetura influenciando consideravelmente o desempenho das mesmas.
Escalonamento de processos O mecanismo responsável por determinar qual processo executará num determinado processador e por quanto tempo deverá permanecer é denominado escalonador.
O objetivo principal deste mecanismo é permitir que a carga do sistema possa ser balanceada, distribuindo as tarefas a serem realizadas entre os processadores.
Um sistema possuirá um bom balanceamento de carga se, na maior parte do tempo, todos os processadores estiverem trabalhando.
Em se tratando de arquiteturas paralelas, o escalonamento pode ser analisado em dois níveis.
Em o primeiro, é possível tratar o mecanismo de escalonamento em nível de aplicação.
Em este caso, a solução paralela para um problema é desenvolvida levando- se em consideração as características da arquitetura alvo, como por exemplo, número de processadores e suas freqüências, rede de interconexão, memória disponível, entre outras.
O mecanismo de escalonamento estará presente dentro de a aplicação, distribuindo as tarefas a serem executadas de forma a melhor utilizar a arquitetura alvo.
Como o mecanismo é implementado dentro de a aplicação, se esta for executada numa arquitetura diferente, poderá resultar numa perda de desempenho, necessitando muitas vezes modificar- la.
Por outro lado, o mecanismo de escalonamento poderá ser tratado como uma entidade externa à aplicação.
Em este caso, o esforço de programação é reduzido, porém, muitas vezes os resultados não serão tão bons quando se espera.
Isto se deve ao fato de que o mecanismo de escalonamento tem conhecimento da arquitetura mas não sobre o comportamento da aplicação que está sendo executada.
Por isto, muitas vezes as decisões tomadas por o escalonador não serão adequadas.
Em multiprocessadores, a entidade externa às aplicações que implementa os mecanismos de escalonamento é o sistema operacional.
Quando tratamos de máquinas multiprocessadas Em uma, há uma questão importante relacionada ao balanceamento de carga.
Se um processo p, num determinado processador localizado num nodo n, aloca dados em memória e os utiliza com freqüência, é bem provável que este dado esteja fisicamente localizado no módulo de memória do nodo n (próximo a a este processador).
Porém, se por algum motivo o sistema acaba por ficar desbalanceado, ou seja, existem processadores inativos enquanto outros estão com muito trabalho, o algoritmo de balanceamento de carga pode optar por migrar este processo p para outro nodo do sistema.
Em este caso, o tempo de acesso ao módulo de memória será comprometido, visto que a memória alocada estará ainda no nodo n..
Existem diversos estudos sobre a migração de processos em máquinas Em uma com o objetivo de avaliar em quais casos é importante a migração de páginas em memória.
Mas, esta migração de páginas poderá ser muito custosa dependendo do sistema e da freqüência com que é realizada.
Portanto, uma solução paralela somente terá ótimos resultados se estas questões referentes ao escalonamento e balanceamento de carga forem levadas em consideração.
Entender como o sistema operacional escalona os processos ajudará o desenvolvedor a definir a melhor maneira de atribuir threads a processadores e quando será necessário utilizar mecanismos de migração.
Basicamente, existem dois mecanismos mais utilizados para implementar paralelismo em arquiteturas com memória compartilhada:
Threads e a API (Application Programming Interface) OpenMP.
Em esta seção serão apresentados estes dois mecanismos mais comuns, além de outros também utilizados, porém, com menor freqüência.
Threads Threads é um mecanismo muito utilizado para implementar programação concorrente em sistemas com memória compartilhada.
São conhecidas como processos leves, sendo diferentes de processos usuais, pois compartilham a mesma área de memória.
Normalmente, ao desenvolver uma aplicação utilizando threads, regiões de memória compartilhada são utilizadas para comunicação entre processos leves.
A idéia principal é criar processos concorrentes de forma a melhor utilizar os recursos da arquitetura.
Existem diversas implementações de threads disponíveis para diferentes sistemas operacionais.
Uma das implementações mais utilizadas em sistemas Linux é a Pthreads (POSIX Threads).
Trata- se de uma implementação do padrão IEEE POSIX 1003.
1c de 19995 para manipulação de threads.
Pthreads especifica uma API para lidar com a maior parte das ações requeridas por threads.
Estas ações incluem a criação e destruição de threads, espera por término do processamento de threads e interação entre elas.
Além disso, estão também disponíveis diversos mecanismos para o tratamento de regiões críticas como mutexes, variáveis condicionais e semáforos.
Para fazer o uso de paralelismo utilizando Pthreads, os desenvolvedores precisam necessariamente escrever seu código especificamente para esta API.
Isto significa que é necessário incluir bibliotecas, declarar estruturas de dados da biblioteca Pthreads e invocar funções específicas.
Basicamente, este processo não é muito diferente em outras Apis que implementam o mecanismo de threads.
Mesmo a biblioteca Pthreads sendo consideravelmente simples e portável, esta sofre de uma séria limitação assim como as demais Apis que implementam este mecanismo:
A necessidade de utilização de código bastante específico.
Em outras palavras, para que o paralelismo possa ser incluído em aplicações as quais originalmente não foram feitas para tanto, muitas vezes são necessárias modificações drásticas no código fonte original.
Tarefas que deveriam ser relativamente simples como por exemplo a paralelização de uma estrutura de laço necessitam de modificações bastante importantes como a criação de threads e controle da divisão de trabalho entre elas.
Nada disto será automatizado, ficando assim a cargo de o desenvolvedor.
Devido a o fato de ser necessário inserir uma quantidade considerável de código específico para realizar operações relativamente pouco complexas, os desenvolvedores vêm cada vez mais procurando por alternativas mais simples.
OpenMP (Open Multiprocessing) é uma API para programação paralela em multiprocessadores.
O padrão OpenMP consiste num conjunto de diretivas de compilação e uma biblioteca de funções suporte que auxiliam o compilador a gerar códigos multi-thread, fazendo o uso de múltiplos processadores num sistema multiprocessado com memória compartilhada.
OpenMP funciona em conjunto com as linguagens Fortran, C e C+.
A idéia do padrão OpenMP é modificar o menos possível o código fonte original de uma aplicação que realizava um processamento seqüencial.
Desta forma, diretivas permitem a criação automática de threads em partes do código que possam ser paralelizadas.
Através do uso de diretivas, o desenvolvedor informa para o compilador que determinado bloco de código deve ser paralelizado.
Esta biblioteca utiliza o modelo fork-join, criando threads para executar determinado processamento em paralelo e, posteriormente, destruindo- as ao final do processamento.
Imaginando que a função computa fará um grande processamento considerando o valor armazenado na posição i do vetor, a paralelização deste laço poderá ser feita utilizando uma diretiva omp parallel for da seguinte forma:
Durante a execução deste laço, a thread inicial criará threads adicionais e todas trabalharão em conjunto para cobrir todas as iterações do laço.
O número de threads a serem criadas é do laço tiverem sido executadas somente a thread inicial continuará executando o restante do código.
É importante notar que nem todo laço poderá ser paralelizado.
Para que o compilador possa transformar este laço seqüencial num laço paralelo é preciso ser possível determinar o número de iterações do laço em tempo de compilação.
Devido a este fato, o laço não poderá conter testes condicionais que façam com que a sua execução seja interrompida prematuramente (como por Além de a primitiva mostrada existem diversas outras que permitem paralelizar porções do código, como por exemplo criar regiões onde todas as threads executam o mesmo código em incluir funções de redução.
Estas funções são bastante úteis quando o laço possui algum tipo de variável incremental.
Através do uso destas funções a biblioteca OpenMP realiza uma determi- nada operação definida por o desenvolvedor para reduzir os resultados parciais computados por cada thread num resultado global ao final do laço.
Como pode ser visto, a biblioteca OpenMP oferece grande facilidade ao programador no desenvolvimento de uma aplicação paralela para uma máquina multiprocessada com memória centralizada.
Grandes benefícios podem ser obtidos ao utilizar esta biblioteca, como por exemplo, a simplicidade de programação.
Trata- se de uma biblioteca que torna questões de mais baixo nível transparentes para o desenvolvedor.
Demais bibliotecas Esta seção apresenta outras bibliotecas que podem ser utilizadas para paralelização de aplicações para máquinas Em uma.
Porém, estes são casos mais específicos que dependem de características da rede de interconexão e da forma com que a arquitetura alvo é construída.
A biblioteca SMI (Shared Memory Interface) baseia- se num modelo de programação que utiliza regiões compartilhadas de memória.
O foco principal desta biblioteca é permitir o programador lidar com todos os efeitos resultantes de uma máquina Em uma.
Levando- se em consideração a tendência de construir máquinas Em uma através do agrupamento de diversas máquinas multiprocessadas (e.
g, SMPs), há a necessidade de existir uma biblioteca capaz de lidar com estas diferenças de tempos de acesso e alocação de regiões compartilhadas de uma forma mais alto nível.
Um exemplo de rede de interconexão que permite o agrupamento de máquinas multiprocessadas formando uma máquina Em uma é a SCI (Scalable Coherent Interface).
Este é um padrão de interconexão que especifica hardware e protocolos para conectar nodos numa rede de alto desempenho.
A grande diferença entre SCI e redes de interconexão como, por exemplo, Myrinet reside na forma como a comunicação é realizada.
Em SCI, a comunicação não é baseada em troca de mensagens.
Toda a comunicação dá- se por comunicação implícita através de acessos remotos à memória.
Desta forma, cada nodo pode mapear para seu próprio espaço de endereçamento segmentos remotos de memória pertencentes a qualquer outro nodo, atuando como se tais segmentos fossem locais.
Toda a comunicação real é feita de forma transparente por o hardware e protocolos de comunicação, que se responsabilizam por leituras e escritas remotas.
Diferentemente das bibliotecas anteriormente descritas, as quais subentendiam a existência de uma memória compartilhada, MPI é um padrão que define um modelo para troca de men-sagens em sistemas onde não existe memória compartilhada.
A idéia básica é abstrair detalhes de baixo nível, permitindo ao desenvolvedor focar em detalhes da aplicação.
Uma execução de uma aplicação em MPI consiste num conjunto de processos que se comunicam utilizando métodos que permitem enviar e receber mensagens, agrupar e sincronizar processos.
Uma biblioteca que implementa o padrão MPI muito conhecida e utilizada é a MPICH.
Mesmo sendo originalmente utilizada para comunicação entre processos em arquiteturas onde não há memória compartilhada (e.
g, clusters), esta biblioteca também pode ser utilizada em máquinas Em uma.
Em este caso, os vários processos serão criados e a comunicação é feita localmente, porém, por troca de mensagens.
A biblioteca MPICH possui otimizações para troca de mensagens entre processos que estão sendo executados num mesmo nodo, não havendo a necessidade do dado ser transmitido por a rede de interconexão.
Diversos estudos foram realizados com o objetivo de propor modificações no padrão MPI de forma a obter melhor desempenho em máquinas multiprocessadas.
O uso desta biblioteca torna- se interessante em clusters de máquinas multiprocessadas.
Em este caso, cada nodo do cluster é uma máquina Em uma com vários processadores.
É possível utilizar o padrão MPI para realizar comunicações dos processos entre os nodos do cluster e outra biblioteca (originalmente desenvolvida para programação paralela em multiprocessadores) para paralelizar o processamento dentro de o nodo.
Os experimentos deste trabalho foram realizados em duas máquinas Em uma bastante distintas.
A primeira arquitetura é composta por 8 processadores dual core AMD R Opteron de 2.2 GHz e 2 MB de memória cache em cada processador.
A máquina é organizada em 8 nodos e possui no total 32 GB de memória principal.
A Figura 9 mostra um esquema desta arquitetura.
A memória principal é dividida em 8 blocos (4 GB de memória em cada bloco) e o tamanho de uma página é de 4 KB.
Cada nodo possui 3 conexões as quais são utilizadas para comunicar com os demais nodos e com os controladores de entrada e saída.
Estas conexões resultam em diferentes latências para acessos remotos.
O sistema operacional utilizado nesta arquitetura é uma distribuição Debian do Linux versão disponível é o GCC (Gnu Compiler Collection).
De agora em diante esta arquitetura será referenciada por o nome de Opteron.
A segunda máquina Em uma utilizada neste trabalho é composta por 16 processadores Itanium 2 de 1.6 GHz e 9 MB de cache L3 em cada processador.
A máquina é organizada em 4 nodos de 4 processadores e possui no total 64 GB de memória principal.
Esta memória é dividida em 4 blocos e o tamanho de uma página é 64 KB.
E é caracterizada por o alto fator Em uma.
Bility Switch), o qual é um backplane desenvolvido por a empresa Bull.
Esta conexão resulta em diferentes tempos de acesso à memória (fator Em uma de 2 à 2.5).
A Figura 10 demonstra a idéia geral da organização desta arquitetura.
O sistema operacional utilizado é uma distribuição Red Hat do Linux com suporte a arquiteturas Em uma (chamadas de sistema e a API numactl).
O compilador disponível nesta arquitetura é o ICC (Intel C Compiler).
De agora em diante será utilizado o nome Itanium 2 para fazer referência a esta arquitetura.
Este capítulo apresentou alguns conceitos importantes que devem ser levados em consideração ao desenvolver aplicações paralelas para máquinas Em uma.
Além disso, foram apresentadas algumas bibliotecas que possibilitam a implementação de paralelismo neste tipo de arquitetura.
Por fim, foram apresentadas duas máquinas Em uma com características bastante distintas.
Estas máquinas serão utilizadas para avaliar o desempenho da solução proposta neste trabalho.
O grande problema das bibliotecas apresentadas anteriormente, com exceção da SMI, está relacionado ao fato de que nenhuma de elas oferece diretivas ou mecanismos que permitem ao programador obter um maior controle sobre a localidade dos dados em memória.
Em a teoria, máquinas Em uma podem ser tratadas como sendo máquinas SMP (onde a memória é centralizada), ignorando- se as diferenças entre tempo de acesso local e remoto à memória.
Em alguns casos, dependendo das características da aplicação e da arquitetura alvo, é possível que bons resultados sejam obtidos sem maiores controles sobre localidade dos dados.
Porém, na maioria das vezes há um impacto relativamente grande e o desempenho ideal não é obtido.
A biblioteca SMI oferece serviços para trabalhar com o conceito de localidade (dados remotos ou locais).
Porém, esta é uma biblioteca que foi implementada para ser utilizada juntamente com a rede de interconexão SCI.
Isto não torna a biblioteca portável para diversos tipos de arquiteturas Em uma.
A afinidade de memória é muito importante e normalmente traz benefícios consideravelmente grandes.
O Capítulo 4 tratará exatamente sobre este tema:
De que forma é possível atingir a afinidade de memória utilizando- se outros mecanismos em conjunto com bibliotecas que não implementam tais funcionalidades, como por exemplo, Pthreads e OpenMP.
Em este trabalho optou- se por a utilização da biblioteca OpenMP devido a os seguintes fatores:·
Controle de threads:
O controle sobre a criação e destruição de threads é feito automaticamente por a API;·
Modelo fork-join:
É um modelo que se adapta facilmente às características do ICTM.
O ICTM é composto por uma seqüência de etapas dependentes, onde cada etapa poderá ser paralelizada internamente;·
Facilidade de paralelização de laços:
Todas as etapas do ICTM seqüencial possuem uma estrutura básica de laços aninhados para realizar o processamento.
Este tipo de estrutura pode ser facilmente paralelizado utilizando- se diretivas específicas da API.
A utilização de duas arquiteturas Em uma bastante distintas mostra- se muito importante.
Em este contexto, o fator Em uma apresenta- se como uma propriedade importante para diferenciar arquiteturas deste tipo.
A escolha de duas arquiteturas com processadores diferentes e fatores Em uma contrastantes permitirá uma avaliação mais completa do comportamento da solução paralela proposta neste trabalho.
Um sistema operacional que oferece suporte a máquinas Em uma é desenvolvido com o intuito de automaticamente alocar memória no bloco mais próximo de cada processo que está sendo executado num determinado nodo de uma máquina Em uma (política denominada firsttouch).
Porém, o que acontecerá com processos que disparam diversas threads através de diversos nodos da máquina?
Como os desenvolvedores podem assegurar- se de que a memória está sendo utilizada de forma otimizada por estas threads?
Um dos conceitos mais importantes para desenvolvedores de aplicações paralelas para máquinas Em uma é o conceito de afinidade.
A afinidade deve ser entendida como uma forma de &quot;associação».
Existem dois tipos de afinidade:
A afinidade de memória e a afinidade de thread.
A afinidade de memória significa dizer que um certo conjunto de endereços de memória é fisicamente mapeado para um bloco de memória local num nodo específico da máquina Em uma.
Ou seja, há uma espécie de associação destes endereços a um certo bloco da memória principal.
Isto significa dizer que, quando algum destes endereços é requisitado por quaisquer processadores, o dado deverá ser buscado no bloco de memória o qual ele está fisicamente alocado.
De a mesma forma, afinidade de thread significa dizer que uma determinada thread será executada somente num conjunto particular de processadores/ núcleos.
Tipicamente, este conjunto pertence a um mesmo nodo de máquina Em uma.
Através do uso explícito de afinidade, os desenvolvedores podem assegurar- se de que cada thread estará acessando dados locais os quais estão armazenados no bloco de memória mais próximo a elas.
Portanto, o desempenho geral da aplicação pode ser melhorado.
A partir de a versão 2.6 do kernel, o sistema operacional Linux introduziu chamadas de sistema que permitem atribuir threads para processadores específicos.
A Em uma API (libnuma) extende esta funcionalidade, permitindo especificar em qual nodo a memória deverá ser alocada.
A fim de permitir que programas possam tirar um maior proveito de arquiteturas Em uma, esta API captura informações sobre a arquitetura, oferecendo especificações sobre a topologia.
Atualmente esta API está disponível na distribuição SUSE R Linux Enterprise Server 9 para processadores AMD R 64 e para a família de processadores Intel R Itanium.
Todavia, é possível instalar- la em qualquer distribuição do Linux através do pacote numactl.
A libnuma é a API recomendada para controlar afinidade de threads e afinidade de regiões de memória em máquinas Em uma e para isto, oferece uma interface aos desenvolvedores.
A afinidade é obtida através da utilização de políticas que modificam a maneira com que as threads são escalonadas e forma com que a memória pode ser alocada.
As políticas sobre threads e regiões de memória são aplicadas utilizando- se um conjunto distinto de funções oferecidas por a API.
A seguir serão explicadas como estas políticas poderão ser aplicadas.
Afinidade de threads Para que uma thread possa ser atribuída a um processador/ núcleo específico ou migrar entre processadores/ núcleos pertencentes a um conjunto determinado, a Em uma API oferece a função máscara.
A especificação dos processadores/ núcleos os quais esta thread poderá ser executada é passado como parâmetro através de uma máscara de bits, onde cada bit representa o identificador único do processador o qual poderá executar- la.
A máscara é configurada utilizando- se a função A Figura 11 exemplifica o funcionamento da máscara de bits.
Caso o desenvolvedor deseje fixar uma thread em apenas um processador ou núcleo, somente o bit correspondente deverá ser configurado na máscara.
Isto indica que o escalonador do sistema operacional não poderá migrar- la em hipótese alguma.
Afinidade de memória As políticas de memória podem ser definidas por processo ou por região de memória.
A política por processo (process policy) é aplicada a todas as alocações de memória realizadas no contexto de um processo.
Por outro lado, políticas definidas por região de memória, também chamadas de virtual memory area policy, permitem que processos possam determinar uma política para um bloco em memória em seu espaço de endereçamento.
Seção 3.3).
Não foram acessadas pela primeira vez são chamadas de untouched.
A alocação física destas páginas só ocorrerá no momento em que algum dado pertencente a elas for escrito.
A política básica implementada por o sistema operacional é denominada first-touch, onde o escalonador realizará a alocação física no bloco de memória mais próximo de o processador o qual realizou o primeiro acesso.
A Figura 12 demostra o funcionamento desta política.
Em o exemplo da Figura 12, uma arquitetura Em uma hipotética com 4 processadores divididos em dois nodos é utilizada para mostrar o funcionamento da política first-touch.
Uma O processador P 0 acessa algum dado armazenado numa página pela primeira vez.
Como este processador está localizado no nodo 0, a página é então fisicamente alocada no bloco de memória pertencente a este nodo.
De a mesma forma, o processador P 2 realiza um acesso a outra página, sendo esta alocada no bloco de memória localizado no nodo 1, o qual pertence o processador P 2.
A princípio esta política parece ser suficiente para controlar as alocações de memória em máquinas Em uma.
Porém, dependendo de como uma aplicação paralela realiza o acesso à memória, muitas vezes esta política não mostra bons resultados.
Além disso, muitas vezes os algoritmos implementados por o escalonador do sistema operacional fazem com que threads sejam migradas entre processadores.
Em estes casos, o acesso que anteriormente era local pode deixar do ser, pois a thread que o acessava pode não estar mais sendo executada no mesmo nodo.
Portanto, não somente é necessário ter- se conhecimento sobre a política de memória mais adequada para uma aplicação paralela que é executa numa arquitetura Em uma, mas também é importante manter- se um controle sobre a localização de threads em processadores.
A Em uma API oferece quatro tipos de políticas de memória, são elas:·
default: É a política padrão (first-touch), onde o dado será alocado no nodo o qual fez o primeiro acesso;·
bind: Aloca um conjunto de páginas num conjunto específico de nodos (utiliza uma máscara de bits para especificar os nodos);·
interleave: Entrelaça alocações de páginas num conjunto de nodos (utiliza uma máscara de bits para especificar os nodos);·
preferred: Aloca preferencialmente num nodo específico.
A diferença entre a política bind e preferred está no fato de que a primeira falhará ao tentar alocar no nodo específico caso não haja espaço necessário para isto.
Por outro lado, a segunda alocará em qualquer outro nodo caso não haja espaço no nodo requisitado.
Diversas propostas anteriores utilizaram- se de algoritmos específicos, mecanismos e ferramentas para alocação de dados em memória, migração e replicação de páginas para garantir a alocadas estejam untouched.
A interface MAI surgiu com o intuito de permitir que diferentes políticas de memória possam ser aplicadas a diferentes dados num mesmo processo pesado.
Através de sua utilização, é possível obter- se um controle mais refinado da afinidade de memória e threads em máquinas Em uma.
Esta biblioteca utiliza como base a Em uma API (libnuma), ou seja, funciona como uma interface de mais alto nível que esconde as chamadas de sistema e a utilização de máscaras de bits, o que muitas vezes necessitam de maiores conhecimentos dos desenvolvedores.
Além de ser mais amigável, também implementa novas políticas de memória não oferecidas por a Numa Além de as funções para controlar políticas de memória, a MAI também oferece funções para migração de páginas.
Estas funções podem ser muito úteis para aplicações com padrões de acesso à memória bastante irregulares.
Há também funções que retornam informações estatísticas sobre a execução da aplicação, como por exemplo o número de migrações de threads entre processadores ou de páginas de memória.
Esta interface está sendo desenvolvida por a estudante de doutorado Christiane Pousa Ribeiro sob a orientação do Prof. Dr. Jean-François Méhaut no LIG (Laboratoire d'Informatique), ligado ao INRIA (Institut National de Recherche en Informatique et en Automatique) em Grenoble, estando sob os direitos da Gnu Public License v. 2.
É implementada em C e foi desenvolvida para utilização em máquinas Em uma com sistema operacional Linux.
O modelo de programação com memória compartilhada é um pré-requisito para o uso da interface.
Além de isto, como esta interface baseia- se na utilização da Em uma API, o pacote numactl precisa necessariamente estar instalado.
Visão geral Como dito anteriormente, a MAI foi desenvolvida para ser utilizada em conjunto com um modelo de programação com memória compartilha.
Portanto, assume- se que o desenvolvedor utilizará threads para processar tarefas em paralelo.
Atualmente existem duas bibliotecas para programação multi-thread suportadas por a interface:
Pthreads e OpenMP.
Isto significa dizer que a interface é capaz de utilizar funções específicas destas bibliotecas para controlar a localização de threads em máquinas multiprocessadas.
A estrutura de uma aplicação paralela que utiliza threads em conjunto com a interface MAI é mostrada na Figura 13.
O lado direito da Figura 13 descreve a estrutura geral de uma aplicação paralela multithread.
Em poucas palavras, o desenvolvedor escreve código fonte de sua aplicação e inclui uma biblioteca que oferece serviços de criação, controle e destruição de threads (as bibliotecas Pthreads e OpenMP foram utilizadas como exemplo).
Estas threads irão então executar processamentos em paralelo, utilizando os processadores disponíveis na arquitetura alvo.
Para que a afinidade de threads e memória possa ser adicionada, basta que o desenvolvedor inclua a interface MAI em seu código multi-thread.
A interface, por sua vez, necessitará de um arquivo de configuração o qual irá informar- la a respeito de quais blocos de memória serão utilizados para alocar dados por as políticas de memória e quais processadores irão executar as threads.
Esta figura mostra claramente que a interface MAI é tratada como um serviço adicional, oferecendo funcionalidades para que o desempenho desejado possa ser obtido em máquinas Em uma.
O arquivo de configuração servirá para informar a MAI sobre quais os blocos de memória utilizar para alocar dados de acordo com a política especificada e também quais processadores ou núcleos utilizar para executar threads.
A Figura 14 mostra a idéia geral da estrutura do arquivo de configuração.
Primeiramente deverão ser especificados a quantidade de nodos da arquitetura Em uma a ser utilizada, seguida por seus identificadores únicos.
Os nodos são tratados por a interface como sendo compostos por um bloco de memória e um conjunto de processadores ou núcleos.
De a mesma forma, deverá ser especificado a quantidade de processadores ou núcleos (em caso de arquiteturas multi-core) a ser utilizada, seguida por os identificadores únicos dos processadores ou núcleos.
A seguir serão descritas as funções oferecidas por a MAI para controlar as políticas de memória, threads, migração de páginas, entre outras.
Principais funções As funções implementadas por a MAI podem ser divididas em cinco grupos:
Funções de sistema, alocação, políticas de memória, políticas de threads e estatísticas.
Funções de Sistema:
São divididas em funções para configurar a interface e funções para permitem inicializar a interface com o arquivo de configuração especificado e finalizar a aplicação desalocando dados em memória, respectivamente.
As demais funções possuem finalidades diversas de coleta de informações.
Listagem das principais funções de sistema:
V o i d i n i t (char f i l e n a m e);
Funções para Alocação de Memória:
Realizam o mapeamento de uma determinada área da memória RAM física numa memória virtual, permitindo a aplicação de políticas de memória nesta área.
A estrutura a ser mapeada pode ser um vetor, matrizes bi-dimensionais ou tridimensionais com tipos primitivos do C ou estruturas de dados.
Estas funções possuem como Listagem das principais funções para alocação de memória:
Funções de Controle de Políticas de Memória:
São as funções que permitem aplicar as políticas de memória.
As políticas cyclic e cyclic_ block distribuem as páginas de uma região de memória virtual entre os nodos (especificados no arquivo de configuração) de forma circular.
A política cyclic distribui página à página, enquanto a política cyclic_ block permite distribuir grupos de páginas de tamanho blocksize.
A política bind_ all realiza a alocação física dos dados em quaisquer um dos nodos especificados no arquivo de configuração.
Porém, quem determina qual página deverá ser alocada em cada um destes nodos é o sistema operacional.
Por outro lado, a política bind_ block divide uma região virtual em n partes, onde n é o número de nodos especificados no arquivo de configuração.
É possível também realizar a migração de páginas através da função migrate_ pages.
Listagem das principais funções para o controle de políticas de memória:
V o i d c y c l i c (v o i d p);
Funções de Controle de Políticas de Threads: Permitem associar threads à um processador configuração feita por uma das duas funções anteriores.
Listagem das principais funções para o controle de políticas de threads:
Funções para Exibição de Estatísticas:
Exibem informações a respeito de a execução da Listagem das principais funções para a exibição de estatísticas:
Este capítulo apresentou um dos conceitos mais importantes para o desenvolvimento de aplicações paralelas para máquinas Em uma:
O conceito de afinidade.
Além disso, foi apresentado como a afinidade pode ser aplicada utilizando- se duas bibliotecas:
Em uma API e MAI.
As funções de gerenciamento de memória juntamente com as funções de gerenciamento de localização de threads permitem que o controle sobre dados e processos possa ser realizado por o desenvolvedor.
Com a utilização destas funcionalidades é possível construir aplicações paralelas que utilizem melhor os recursos de máquinas Em uma.
Porém, este controle se dá através do uso de funções e chamadas de sistema de baixo nível por a Em uma API.
A utilização de máscaras de bits para especificar processadores (no caso de as de memória que armazenam uma determinada região de memória virtual, visto que a aplicação das políticas se dará sempre neste nível de abstração.
Portanto, a aplicação de diferentes políticas de memória a uma mesma região virtual exige do desenvolvedor cálculos específicos para que isto seja feito em termos de páginas de memória.
Caso contrário, as políticas não serão aplicadas corretamente.
Tendo em vista o exposto, mostra- se necessário o uso de uma interface capaz de abstrair estas questões de mais baixo nível, facilitando assim aplicação e o controle de políticas de memória e threads.
A MAI fornece este alto nível de abstração, além de incluir novas políticas e a possibilidade de realizar a migração de páginas de memória.
Além de isto, esta interface possui suporte às bibliotecas OpenMP e Pthreads, tornando- se simples o processo de integração.
Devido a isto, a interface MAI será utilizada neste trabalho para a aplicação das políticas de memória e threads.
O primeiro passo para paralelização do ICTM para máquinas Em uma foi baseado na utilização da biblioteca OpenMP.
Como dito na Seção 3.4, a escolha desta biblioteca se deu por diversas razões.
A razão principal está relacionada a simplicidade de uso:
O código seqüencial pode ser paralelizado com poucas modificações, pois o controle de criação e destruição de threads é feito por a API.
Além disso, OpenMP utiliza o modelo fork-join, o qual consiste no uso de diversas regiões paralelas em meio a porções de código seqüencial.
Portanto, este modelo pode ser facilmente aplicado ao ICTM seqüencial, onde cada etapa do processo de categorização pode ser paralelizada.
O principal objetivo deste capítulo é descrever uma proposta de uma solução paralela para o ICTM sem levar em consideração a localidade de threads e de dados em memória, denominada OpenMP-ICTM.
Desta forma, será possível avaliar a necessidade de utilização de mecanismos de afinidade para aumentar o desempenho da solução em máquinas Em uma.
Para isto, primeiramente será descrito como foi feita a paralelização das etapas do processo de categorização do ICTM.
Após, uma descrição dos casos de estudo utilizados para avaliar o desempenho da solução paralela serão apresentados.
Finalmente, uma análise de desempenho desta solução será mostrada.
O processo de categorização do ICTM inicia com a leitura dos dados de entrada.
Estes dados são então armazenados na Matriz Absoluta para então serem computados ao longo de o processo de categorização.
Em este trabalho, a paralelização será focada nas demais etapas do processo, pois estas necessitam de um alto poder computacional ao categorizar grandes áreas geográficas.
De um modo geral, cada etapa do processo de categorização realiza um determinado tipo de computação sobre a matriz relacionada a etapa.
Para isto, são consultados valores referentes a matrizes anteriormente criadas.
A Figura 15 mostra como as etapas utilizam as matrizes durante o processamento.
A computação realizada em cada uma das etapas mostradas na Figura 15 pode ser entendida, de um modo estrutural, como sendo composta por dois laços de repetição encadeados.
Estes laços serão responsáveis por a computação de todas as células das matrizes em cada etapa.
Porém, para o cálculo das matrizes de monotonicidade o parâmetro raio é levado em consideração.
Desta forma, a cada uma destas etapas é adicionada mais uma estrutura de laço, pois cada célula da matriz é comparada com r vizinhos, onde r varia de 1 até o valor do raio passado como parâmetro.
Logo, a utilização de valores maiores para o raio aumenta ainda mais a quantidade de processamento nestas etapas de monotonicidade.
O encadeamento de laços em cada etapa pode ser paralelizado utilizando- se uma diretiva OpenMP denominada omp parallel for (descrita anteriormente na Seção 3.2.2).
A o aplicar esta diretiva no laço mais externo, o desenvolvedor informa a biblioteca OpenMP que as iterações deste laço deverão ser quebradas em t partes, onde t corresponde ao número de threads criadas por a biblioteca OpenMP:
A diretiva omp parallel for é responsável por a criação das threads, divisão do trabalho entre elas e destruição (após o término do laço mais externo).
Em o caso específico do ICTM, a divisão do trabalho entre as threads é realizada da seguinte forma:
Cada thread será responsável por processar um conjunto de linhas da matriz.
A Figura 16 ilustra esta forma de divisão do trabalho entre as threads através do uso da diretiva omp parallel for no laço mais externo de cada etapa do processo de categorização.
Em o exemplo da Figura 16, as matrizes possuem 9 linhas e 10 colunas.
A o executar a solução paralela com 3 threads, a biblioteca OpenMP dividirá as iterações dos laços em 3 partes.
Esta será a forma de divisão das matrizes durante todo o processo de categorização.
Esta solução não apresenta problemas para as etapas de monotonicidade, onde a vizinhança é consultada para o processamento de cada célula, pois como dito anteriormente, estas etapas somente consultam dados das Matrizes Intervalares.
Logo, não há dependência mútua entre etapas de cálculo das Matrizes de Monotonicidade.
De um modo geral, estas primitivas foram adicionadas nos laços de cada etapa do processo de categorização, com exceção da etapa de construção da Matriz Relativa.
Em a solução seqüencial, dois procedimentos são feitos nesta etapa:
Busca por o célula com maior valor e divisão de todas as células por a célula de maior valor.
Para a solução paralela, utilizou- se a mesma estratégia apresentada anteriormente, onde a matriz é dividida em conjuntos de linhas e cada thread será responsável por processar somente as linhas do respectivo conjunto.
Porém, não é possível utilizar somente a primitiva omp parallel for devido a necessidade de encontrar- se o maior valor.
Logo, a seguinte estratégia foi adotada:
Cada thread encontrará a célula com maior valor local (para o seu conjunto de linhas), armazenando este valor num vetor indexado por o identificador único de cada thread, após, (ii) cada thread consultará este vetor afim de descobrir o maior valor global (maior dos maiores locais) e finalmente, (iii) cada thread ficará responsável por normalizar as células do seu conjunto de linhas.
A API OpenMP oferece a diretiva omp parallel que permite criar uma região paralela onde todas as threads executarão o mesmo código.
Dentro de esta região paralela, é possível utilizar a diretiva omp for para que, neste ponto, todas as threads trabalhem em conjunto para dividir o processamento de uma estrutura de laço.
Estas primitivas serão utilizadas para paralelizar a etapa de construção da Matriz Relativa, de acordo com as estratégia anteriormente descrita.
O primeiro conjunto de linhas da listagem de código abaixo é referente a o procedimento, onde cada thread encontrará a célula com maior valor local.
O segundo conjunto de linhas diz respeito ao procedimento, onde cada thread encontrará o maior valor global (que será o mesmo para todas as threads).
Finalmente, o último conjunto de linhas é responsável por dividir o processo de normalização entre as threads, dividindo os valores das células por o maior valor encontrado em.
A o final, a Matriz Relativa terá todas as suas células normalizadas:
A instrução opcional private utilizada em conjunto com a diretiva omp parallel permite informar ao compilador que as variáveis i, j, contMaior, threadId e maior serão privadas, ou seja, cada uma destas variáveis poderá assumir valores diferentes em cada thread.
A variável threadId é responsável por armazenar o identificador único da thread, assumindo OpenMP.
Acredita- se que esta solução para a etapa de construção da Matriz Relativa e a utilização da diretiva omp parallel for para as demais etapas do processo de categorização seja uma forma elegante e pouco evasiva de paralelizar o ICTM.
Todavia, a alocação de memória nos nodos Em uma e a migração de threads é feita de acordo com as regras do kernel do Linux, não sendo possível controlar- las utilizando somente diretivas da biblioteca OpenMP.
Em esta seção será apresentado a avaliação de desempenho da paralelização do ICTM através do uso da biblioteca OpenMP (OpenMP-ICTM).
Primeiramente, serão descritos os casos de estudo escolhidos para avaliar a solução.
Após, será feita uma análise dos resultados obtidos nas duas arquiteturas alvo (Seção 3.3).
Em o final, será feita uma discussão sobre os resultados e algumas conclusões serão apontadas.
Uma listagem completa dos resultados, onde mostra- se a média dos speed-ups com raio 20, 40 e 80 e seu desvio padrão, é feita nos Apêndices A e B. Casos de estudo De acordo com as características do ICTM, basicamente dois parâmetros possuem influência imediata no desempenho da solução seqüencial:
A dimensão das matrizes e o raio utilizado nas etapas de cálculo da monotonicidade.
Portanto, os casos de estudo apresentados aqui foram baseados na modificação destes parâmetros com o intuito de avaliar a solução paralela proposta neste trabalho.
É importante salientar que os dados de entrada utilizados neste trabalho não são dados reais oriundos de imagens de satélite.
Devido a dificuldade destes dados serem obtidos, optou- se por criar casos de estudo com valores aleatórios.
Isto é possível devido a o fato de que o poder de processamento necessário para categorizar uma determinada camada do ICTM está vinculado aos parâmetros citados anteriormente (dimensão das matrizes e raio).
Logo, a utilização de dados aleatórios, ao invés de reais, para a geração da Matriz Absoluta não altera o tempo de processamento durante o processo de categorização de regiões de mesma dimensão e raio.
A solução proposta neste trabalho divide o processamento entre as threads dentro de cada etapa do processo de categorização de uma camada.
Devido a este fato, os casos de teste apresentados aqui sempre utilizarão somente uma única camada no modelo.
A Tabela 1 mostra os parâmetros escolhidos para a realização dos testes.
As dimensões das matrizes foram calculadas considerando- se a quantidade de memória necessária para armazenar- las.
Além disso, com o intuito de compreender a influência do raio na solução paralela, optou- se por realizar os testes com três valores para o raio:
20 (pequeno), 40 (médio) e 80 (grande).
Duas importantes medidas de qualidade de programas paralelos serão utilizadas para avaliar o desempenho das soluções apresentadas neste trabalho:
Speed-up e eficiência.
Todos os resultados foram baseados na utilização de médias, onde cada caso de estudo, com a mesma configuração de parâmetros, foi executado 10 vezes, excluindo- se o pior e melhor tempo de execução.
Estas médias apresentaram um desvio padrão bastante pequeno, pois todos os experimentos foram realizados com acesso exclusivo às máquinas Em uma.
Resultados Esta seção apresenta os resultados obtidos com a solução paralela do ICTM utilizando- se a biblioteca OpenMP.
Diferentes experimentos foram realizados variando- se os 4 casos de estudo descritos anteriormente.
Além disso, foram utilizadas as duas arquiteturas Em uma descritas na Seção 3.3.
A medição dos tempos de execução do ICTM paralelo utilizando- se a biblioteca OpenMP foi feita individualmente em cada etapa, excluindo- se o tempo necessário de leitura dos dados de entrada e escrita na Matriz Absoluta (parte não paralelizada).
O tempo de execução final compreende a soma dos tempos individuais de todas as demais etapas do processo de categorização de uma camada.
A o comparar os resultados com os diferentes casos de estudo pode- se perceber um comportamento interessante das curvas de speed-up:
A medida que se aumenta a dimensão das matrizes, as curvas tendem a se aproximar umas das outras.
Em o Caso 1, a utilização de matrizes pequenas aliada a raios pequenos resultam em pouco ganho de desempenho.
Porém, a medida que a dimensão das matrizes aumenta (demais casos) o ganho de desempenho utilizando- se raios menores aumenta.
Isto faz com que as curvas de speed-up utilizando- se raios 20 e 40 se aproximem da curva com raio 80.
O baixo fator Em uma desta arquitetura permite que, mesmo aumentandose significantemente a probabilidade de ocorrerem acessos remotos à memória (raio maior e matrizes de dimensão maior), o desempenho da solução continue aumentando.
Arquitetura Itanium 2 Os resultados dos experimentos na arquitetura Itanium 2 são mostrados na Figura 18.
Em oposição aos resultados obtidos com a arquitetura Opteron, à medida em que o casos de estudo maiores são utilizados, percebe- se uma redução considerável de desempenho.
O fator de speedup máximo obtido no Caso 1 foi de aproximadamente 12,5 enquanto o máximo no Caso 4 foi de 11.
Comparando os resultados das duas arquiteturas é possível concluir principalmente duas diferenças.
A primeira está relacionada à proximidade das curvas de speed-up à curva ideal com poucos processadores.
Em este caso, mesmo com poucos processadores existe uma distância entre elas e a curva ideal (em oposição a curvas muito próximas da ideal na arquitetura Opteron).
A segunda está relacionada à influência do raio no desempenho:
Mesmo com poucos processadores é possível observar diferenças de speed-up ao utilizar raios de tamanho diferente (em oposição às curvas sobrepostas à ideal na arquitetura Opteron).
A utilização de valores maiores para o raio faz com que mais acessos à memória sejam feitos.
Como nenhum controle sobre a localidade dos dados é feito, há uma maior chance de ocorrerem acessos remotos.
Como o fator Em uma é alto, há um maior impacto no desempenho geral da solução paralela.
Este capítulo apresentou a proposta inicial de uma solução paralela do ICTM para máquinas Em uma (OpenMP-ICTM) utilizando- se somente a biblioteca OpenMP.
Em este caso, as máquinas Em uma foram consideradas como sendo Uma, pois nenhum controle específico sobre a localidade de dados em memória foi feito.
Após a análise dos resultados do OpenMP-ICTM é possível concluir que este apresentou um interessante ganho de desempenho.
Porém, as medidas de speed-up e eficiência apontam que esta solução possui limitações.
O maior speed-up obtido foi de aproximadamente 12,8 na arquitetura Opteron utilizando- se 16 processadores enquanto que na arquitetura Itanium 2 o fator foi de aproximadamente 12,5.
O impacto do fator Em uma pode ser observado principalmente ao submeter os testes na arquitetura Itanium 2.
O aumento da dimensão das matrizes juntamente com a utilização de raios maiores resultaram em desempenhos piores.
Uma análise geral da eficiência da solução OpenMP-ICTM em ambas arquiteturas é mostrada nas Figuras 19 e 20.
Em estas, são mostradas as eficiências médias para cada caso de estudo.
Isto foi feito através do cálculo de uma curva de speed-up média para cada caso de estudo, que conterá a média dos speed-ups relativos aos três valores de raios.
É importante ressaltar que estes speed-ups médios apresentaram um desvio padrão pequeno (estes valores estão descritos nos Apêndices A e B).
A Figura 19 mostra as eficiências médias obtidas com a solução OpenMP-ICTM na arquitetura Opteron.
Os resultados mostram que com poucos processadores a solução apresenta uma eficiência parecida, independentemente do caso de estudo (dimensão das matrizes).
Porém, a medida em que são inseridos mais processadores, as diferenças entre as eficiências dos quatro casos de estudo comparados aumentam.
Estas diferenças se dão por o aumento da eficiência dos casos de estudo maiores quando comparados aos casos de estudo menores.
A utilização de um número maior de processadores faz com que casos de estudo que utilizam matrizes maiores possuam também eficiências melhores.
Novamente, salienta- se que isto ocorre devido a o baixo fator Em uma desta arquitetura.
De forma análoga, a Figura 20 mostra as eficiências médias obtidas com a solução OpenMPICTM na arquitetura Itanium 2.
Em oposição ao comportamento observado na arquitetura anterior (Opteron), a utilização de mais processadores implica numa redução significativa da eficiência dos casos de estudo maiores.
A má localização dos dados em memória aliada ao fato desta arquitetura possuir um alto fator Em uma impacta drasticamente nesta redução da eficiência.
As diretivas oferecidas por a biblioteca OpenMP não oferecem mecanismos de controle sobre a localidade dos dados e posicionamento de threads.
É evidente que, com um melhor controle sobre a afinidade de memória e threads, é possível reduzir a interferência dos acessos não uniformes à memória, tornando- se possível extrair significativos ganhos de desempenho em máquinas Em uma.
Em arquiteturas Em uma torna- se importante reduzir o impacto do fator Em uma sobre a aplicação que está sendo paralelizada.
A redução deste fator pode ser feita considerando dois tipos de otimizações:
Latência e largura de banda.
Manter os dados mais próximos do processador que os utilizam reduz a latência, reduzindo assim o número de acessos remotos.
Por outro lado, é possível reduzir a disputa de acesso entre os processadores ao mesmo bloco de memória, distribuindo- se os dados entre estes blocos de memória, otimizando- se assim a largura de banda.
Estas duas estratégias serão exploradas através da utilização de diferentes políticas de memória.
A alocação de dados em memória será feita com o uso da interface MAI, através da aplicação de políticas de memória implementadas por ela.
Isto possibilitará o desenvolvimento de uma nova solução paralela para o problema, denominada Numa-ICTM.
Primeiramente será descrito como a interface MAI foi integrada a OpenMP-ICTM (apresentada na Seção 5).
Após, uma análise de desempenho desta nova solução será discutida, mostrando- se os resultados obtidos.
Finalmente, uma discussão geral sobre os resultados será feita onde serão apontadas algumas informações adicionais que puderam ser extraídas dos resultados.
Após a proposta de uma solução utilizando- se somente diretivas OpenMP para paralelizar o modelo ICTM para máquinas Em uma, foram adicionadas funções específicas da biblioteca MAI para controlar políticas de memória e threads.
Basicamente, estas modificações foram realizadas no código correspondente ao processo de inicialização, onde as matrizes são alocadas.
Quatro grupos de funções foram utilizadas:·
Funções para o controle de políticas de threads:
Duas funções foram utilizadas para· Funções para o controle de políticas de memória:
As quatro funções que permitem modificar as políticas aplicadas durante a alocação física de dados nos blocos de memória Os arquivos de configuração da interface MAI foram criados de acordo com a plataforma Em uma e o número de threads a serem utilizadas em cada execução.
No caso de a máquina Opteron, cada thread é atribuída a um núcleo (pois trata- se de uma arquitetura com processadores dual core), enquanto na máquina Itanium 2, cada uma é atribuída a um processador.
Como dito anteriormente, o controle de afinidade de threads foi feito através do uso de duas funções da interface MAI.
Estas funções foram utilizadas da seguinte forma:
A diretiva omp parallel cria uma região paralela onde todas as threads executam o da MAI.
Com isto, é possível se assegurar que não ocorrerão migrações entre processadores ou núcleos, visto que cada thread será associada a somente um processador ou núcleo.
O controle sobre threads permite a utilização de afinidade de memória para melhor controlar a localização dos dados.
A alocação das matrizes na solução OpenMP-ICTM foi feita utilizando- se a função padrão um mapeamento da memória RAM física numa memória virtual.
Devido a similaridade entre estas duas funções, poucas modificações foram necessárias para adaptar a antiga forma de para que seja possível, posteriormente, especificar a política de memória a ser utilizada na alocação das matrizes.
A Figura 21 mostra visualmente como a política de memória bind_ block é aplicada às matrizes do ICTM.
Em esta figura, as células das matrizes foram agrupadas em termos de páginas de memória, visto que todas as políticas são aplicadas a este tipo de estrutura.
Em o caso específico desta política, os dados armazenados nas matrizes são fisicamente alocados nos blocos de memória de acordo com a distribuição do trabalho feita por a primitiva omp parallel for.
Devido a isto, cada thread irá acessar, principalmente, páginas de memória que estão fisicamente armazenadas no mesmo nodo, reduzindose assim o número de acessos remotos.
Considerando- se a matriz com 9 linhas mostrada na figura e supondo- se que a máquina Em uma possui três processadores em cada nodo, cada thread que estará sendo executada em cada processador será responsável por computar somente uma linha da matriz.
Esta linha estará fisicamente alocada no bloco de memória pertencente ao nodo onde a thread está sendo executada.
Isto permite a redução de ocorrências de false-sharing e, mesmo quando ocorram, o tempo desperdiçado para atualização das caches dos demais processadores será menor, visto que os dados estarão armazenados no bloco de memória próximo a eles.
De forma similar, a política bind_ all poderá ser aplicada especificando- se somente os nodos os quais possuem threads processando.
Porém, a localidade física de cada página será determinada por o kernel do Linux e não por o desenvolvedor, como na política bind_ block.
A o aplicar a política cyclic, as páginas de memória serão fisicamente alocadas da forma mostrada na Figura 22.
Estas páginas de memória são distribuídas através dos nodos da máquina Em uma através de um processo cíclico:
A primeira página de cada matriz será fisicamente armazenada no bloco de memória pertencente ao Nodo 0, a segunda página no bloco de memória pertencente ao Nodo 1, a terceira página no bloco de memória pertencente ao Nodo 2, repetindo- se este processo para as demais páginas.
Um comportamento similar ocorre quando a política cyclic_ block é aplicada.
Porém, ao invés de a distribuição ser feita página a página, o processo distribuirá conjuntos de páginas.
A esta nova solução paralela do ICTM otimizada para a utilização de afinidade de memória deu- se o nome de Numa-ICTM.
Em esta solução foram adicionadas as quatro políticas implementadas por a interface MAI.
A política de memória a ser aplicada é especificada através de um parâmetro de entrada, permitindo- se que esta solução possa ser facilmente configurada para diferentes tipos de máquinas Em uma.
A inclusão de afinidade de memória permite com que esta solução utilize melhor os recursos das máquinas Em uma, como será visto na seção a seguir.
Esta seção apresenta uma análise de desempenho da solução paralela do ICTM que explora estratégias de alocação de memória (Numa-ICTM).
De a mesma forma apresentada na Seção Novamente, as duas arquiteturas Em uma descritas na Seção 3.3 foram utilizadas para a realização de todos os experimentos.
A forma de medição dos tempos de execução da Numa-ICTM não foi alterada, ou seja, são tomados os tempos de execução de cada etapa do processo de categorização, excluindo- se o tempo necessário para a leitura dos dados de entrada e escrita na Matriz Absoluta.
Portanto, o tempo de execução final compreende a soma dos tempos individuais de todas as demais etapas do processo de categorização.
A seguir, são apresentados os resultados obtidos por arquitetura Em uma comparando- se as diferentes políticas de memória e seus impactos no desempenho final da solução proposta.
Uma listagem completa dos resultados, onde mostra- se a média dos speed-ups com raio 20, 40 e 80 e seu desvio padrão, é feita nos Apêndices A e B. Arquitetura Opteron Os resultados apresentados nesta seção são oriundos da realização de experimentos na arquitetura Opteron.
Foram realizados experimentos com as quatro políticas implementadas por a interface MAI, sendo os resultados divididos em duas partes:
Comparação entre as duas polí-ticas do tipo bind (bind_ block e bind_ all) e comparação entre as duas políticas do tipo cyclic (cyclic e cyclic_ block).
Políticas bind_ all e bind_ block A Figura 23 apresenta um quadro comparativo dos speed-ups obtidos ao utilizar as políticas bind_ block e bind_ all na arquitetura Opteron.
Cada curva representa o speed-up de uma política utilizando- se um determinado valor para o raio.
Logo, seis curvas serão apresentadas em cada caso de estudo (duas políticas e três valores de raio para cada uma).
A o analisar os resultados da Figura 23 conclui- se que a aplicação destas políticas resultou num ganho de desempenho considerável comparado ao resultado da solução OpenMP-ICTM (5.2.2).
No geral, a política bind_ block apresentou melhores resultados.
Porém, ao comparar curvas de mesmo raio (entre políticas diferentes) percebe- se que a diferença de desempenho não é tão significativa em diversos casos.
Como explicado na Seção 6.1, a diferença entre as políticas bind_ block e bind_ all está no fato de que na segunda o kernel do Linux será responsável por determinar onde cada página será alocada.
Como esta arquitetura possui um baixo fator Em uma, não ocorrem grandes diferenças de desempenho em relação a estas duas políticas.
Isto pode ser verificado também por a proximidade entre as curvas nos gráficos de speed-up.
Políticas cyclic e cyclic_ block O desempenho das políticas cyclic e cyclic_ block pode ser observado na Figura 24.
Em contraste com os resultados nesta arquitetura, onde as políticas bind_ block e bind_ all foram aplicadas, aqui percebe- se um maior ganho de desempenho.
Além disso, os resultados mostram curvas mais lineares e bastante próximas à ideal.
A a medida em que o caso de estudo é aumentado percebe- se uma pequena perda de desempenho, porém ainda assim os ganhos são superiores aos da solução OpenMP-ICTM.
O baixo fator Em uma desta arquitetura permite a utilização de uma estratégia que otimize a largura de banda.
Isto é feito por a política cyclic, onde as páginas de memória serão distribuídas entre os blocos de memória da arquitetura através de um processo circular.
Isto faz com que seja possível reduzir a disputa de acesso entre os processadores ao mesmo bloco de memória, pois os dados a serem acessados por processadores de um mesmo nodo não necessariamente estarão armazenados neste nodo.
Como dito na Seção 6.1, a política cyclic_ block distribui conjuntos de páginas de forma cíclica (e não página à página como na política cyclic).
Em os resultados mostrados na Figura 24 a política cyclic_ block foi parametrizada para distribuir conjuntos compostos por 10 páginas.
Como pode ser observado, os resultados foram piores quando comparados a política cyclic.
Diversos outros experimentos com 20, 30, 40 e 50 páginas por conjunto foram realizados e apresentaram resultados ainda piores.
Isto pode ser interpretado da seguinte forma:
Uma granulosidade fina é mais apropriada para esta aplicação e esta arquitetura.
Arquitetura Itanium 2 Os resultados apresentados nesta seção foram extraídos de experimentos na arquitetura Itanium 2.
Novamente, estes estão divididos em duas partes:
Comparação entre as duas políticas do tipo bind e comparação entre as duas políticas do tipo cyclic.
Políticas bind_ all e bind_ block A comparação dos speed-ups obtidos com a aplicação das políticas bind_ all e bind_ block é mostrada na Figura 25.
A política bind_ block apresentou um desempenho melhor e curvas bem próximas da ideal.
Devido a o alto fator Em uma desta arquitetura, é possível que um maior ganho de desempenho seja obtido quando os dados são armazenados próximos aos processadores que os utilizam.
Desta forma, diminui- se o custo de comunicação decorrente dos acessos remotos (dados em blocos de memória distantes).
A variação do valor do raio não mostrou interferir de forma significativa no desempenho da política bind_ block, o que pode ser comprovado por a proximidade das curvas com raio 20, 40 e 80.
Diferentemente dos resultados da arquitetura Opteron, aqui é possível observar uma diferença significativa de desempenho entre as políticas bind_ block e bind_ all.
Novamente o fator Em uma é determinante:
Como a política bind_ all deixa a cargo de o kernel do Linux a alocação física das páginas, caso estas não estejam próximas dos processadores que as acessam, um grande tempo será desperdiçado para acessar- las em blocos de memória remotos, impactando assim no desempenho.
A a medida em que o caso de estudo é aumentado a diferença de desempenho entre as políticas bind_ block e bind_ all também aumenta, pois há uma maior chance do kernel alocar os dados em blocos de memória distantes.
Políticas cyclic e cyclic_ block Os speed-ups após a aplicação das políticas cyclic e cyclic_ block na arquitetura Itanium 2 são mostrados na Figura 26.
No geral, as políticas cyclic e cyclic_ block apresentaram resultados melhores que a política bind_ all.
Porém, é visível que esta estratégia de distribuição de páginas de memória não é a mais adequada para esta arquitetura com alto fator Em uma.
Devido a isto, ambas as políticas mostraram desempenho inferior à política bind_ block.
Novamente, o uso de uma granulosidade maior resultou em desempenhos piores.
Além disso, o aumento do tamanho do caso de estudo impactou drasticamente no desempenho destas políticas nesta arquitetura.
Quanto maior a quantidade de dados, maiores serão as chances de ocorrerem acessos remotos devido a a alocação de páginas de memória distantes dos processadores que as acessam.
Este capítulo apresentou a otimização da solução OpenMP-ICTM através da utilização de afinidade (Numa-ICTM).
A afinidade de threads e memória foi aplicada utilizando- se a interface MAI, explorando- se as quatro políticas de memória que ela implementa.
Uma análise de desempenho foi feita comparando- se os resultados nas duas arquiteturas Em uma descritas na Seção 3.3.
Os experimentos realizados demonstraram que determinados tipos de políticas de memória são mais adaptadas para máquinas com alto fator Em uma (como por exemplo a política bind_ block).
Em este tipo de política, é possível alocar fisicamente páginas de memória em blocos específicos de forma a reduzir a distância entre dados e processadores os quais os acessam.
Por outro lado, políticas cíclicas (como por exemplo a cyclic) são mais indicadas para arquiteturas com baixo fator Em uma, onde uma estratégia que prioriza largura de banda pode resultar num melhor desempenho.
Uma avaliação geral em termos de eficiência da mesma forma apresentada na seção que discutiu os resultados da solução OpenMP-ICTM (Seção 5.3) é mostrada nas Figuras 27 e 28.
Porém, como neste capítulo foram realizados experimentos com as quatro políticas de memória em ambas arquiteturas, optou- se por apresentar somente a eficiência média da solução NUMAICTM com a política que apresentou melhores resultados em cada arquitetura.
Novamente foi observado um pequeno desvio padrão no cálculo da média dos speed-ups e estes podem ser vistos nos Apêndices A e B. A Figura 27 apresenta a eficiência média da solução Numa-ICTM na arquitetura opte-ron.
Em este caso, a política de memória que apresentou melhores resultados foi a cyclic.
Como dito anteriormente, esta estratégia permite um aumento do número de acessos concorrentes a diferentes blocos de memória, reduzindo a disputa de acesso ao mesmo bloco.
Os resultados indicam que os maiores ganhos foram obtidos com casos de estudo menores (onde há um grande aumento da eficiência).
Todavia, pode- se perceber também ganhos nos casos de estudo maiores.
Outra consideração importante a ser feita está relacionada ao número de processadores:
A o aumentar o número de processadores, a importância da utilização de uma política de memória que permita um melhor uso da arquitetura também aumenta.
Isto pode ser percebido ao comparar as eficiências médias da solução OpenMP-ICTM e Numa-ICTM a partir de 8 processadores.
Em esta última, há uma atenuação da perda de desempenho de 8 a 16 processadores (a solução OpenMP-ICTM apresenta uma queda considerável de desempenho neste intervalo).
Isto mostra que a solução Numa-ICTM é mais escalável.
A Figura 28 apresenta a eficiência média da solução Numa-ICTM na arquitetura Itanium problema do alto fator Em uma desta arquitetura é atenuado mantendo- se os dados próximos aos processadores que os acessam.
Como esperado, o ganho de desempenho da solução Numa-ICTM na arquitetura Itanium 2 foi mais significativo.
Devido a o alto fator Em uma desta arquitetura, aumenta- se a importância de aplicar uma política que mantenha os dados mais próximos dos processadores que os acessam.
Em comparação com a solução OpenMP-ICTM nesta arquitetura, percebe- se aqui um grande aumento da escalabilidade, principalmente quando são utilizados mais do que 8 processadores.
Este trabalho apresentou a Numa-ICTM:
Uma solução paralela do ICTM para máquinas Em uma explorando estratégias de alocação de memória.
Para isto, uma primeira solução denominada OpenMP-ICTM foi desenvolvida utilizando- se OpenMP (uma API desenvolvida para implementação de aplicações paralelas para multiprocessadores com memória centralizada).
Em esta solução, as arquiteturas alvo (Em uma) foram vistas como sendo Uma, ou seja, ignorouse completamente o conceito de &quot;distância «no acesso à memória, característica marcante deste tipo de arquitetura.
O objetivo principal deste primeiro passo foi de comprovar a necessidade de utilização de estratégias de alocação de memória.
Posteriormente, esta solução foi otimizada incluindo- se a interface MAI, a qual permite aplicar políticas de memória e threads de uma forma mais alto nível que a Em uma API.
A esta nova solução deu- se o nome de Numa-ICTM, a qual permite a utilização de diferentes políticas de memória.
O comportamento de cada política (desempenho) foi analisado em duas arquiteturas Em uma bastante distintas, mostrando- se que através da afinidade de memória é possível obter- se resultados mais próximos do ideal.
A seguir, será feito um paralelo entre a solução apresentada neste trabalho e os trabalhos relacionados (HPC-ICTM).
Posteriormente, uma comparação final entre a solução Numa-ICTM e a solução OpenMP-ICTM será feita, mostrando- se o quanto, na média, a solução NUMAICTM é melhor que a OpenMP-ICTM.
Finalmente, uma breve avaliação da interface MAI será feita, onde serão mostrados os para os e contras de utilizar- la.
A última seção deste capítulo tratará sobre trabalhos futuros.
A solução descrita neste trabalho (Numa-ICTM) e as soluções descritas em e (HPCICTM) apresentam propostas diferenciadas de paralelizar o ICTM.
A principal diferença se dá na escolha das arquiteturas alvo.
O HPC-ICTM foi proposto para multicomputadores, ou seja, são arquiteturas caracterizadas por o não compartilhamento de memória (a comunicação se dá através de trocas de mensagens entre processos).
Por outro lado, a Numa-ICTM foi proposta para multiprocessadores:
Arquiteturas caracterizadas por o compartilhamento de memória.
A escolha da arquitetura alvo é de grande importância e irá definir as estratégias a serem utilizadas no processo de paralelização da aplicação.
Em o contexto de multicomputadores, existe uma grande preocupação em reduzir- se o número de mensagens a serem trocadas entre pro-cessadores, pois este é um fator importante e diretamente relacionado ao desempenho final da solução paralela.
Devido a isto, busca- se estratégias que permitem a realização de processamentos independentes em paralelo, reduzindo- se assim a necessidade por troca de informações.
Uma das grandes vantagens da solução HPC-ICTM é a de necessitar um tipo de arquitetura de mais baixo custo:
Clusters. Desta forma, é possível que para alguns casos um menor investimento seja necessário, trazendo resultados bastante interessantes.
Porém, dependendo das propriedades da região a ser categorizada (nesta solução o número de camadas do modelo, dimensão da região e tamanho do raio) é provável que a decomposição em camadas não seja a melhor alternativa.
A decomposição em camadas exige pouca troca de mensagens mas somente será vantajosa se o modelo de entrada possuir diversas camadas (o número de processos escravos é igual ao número de camadas do modelo).
Já na decomposição por funções, devido a dependência entre etapas do processo de categorização, somente 4 processos poderão executar funções do modelo em paralelo.
Por fim, na decomposição por domínios, caso a dimensão da região seja muito grande, uma maior quantidade de troca de mensagens será necessária.
Por outro lado, a solução Numa-ICTM apresenta, como uma de suas grandes vantagens, a possibilidade de tratar regiões extremamente grandes com um importante ganho de desempenho.
Diferentemente da solução HPC-ICTM, esta define somente um modelo de decomposição do problema que oferece ganhos independentemente das características da região de entrada.
A categorização de regiões extremamente grandes se torna possível devido a a grande disponibilidade de memória oferecida por as máquinas do tipo Em uma.
Além disso, este tipo de arquitetura é uma tendência forte atualmente e seus conceitos estão sendo aplicados em desktops.
Como desvantagem desta solução aponta- se o alto investimento necessário numa máquina deste porte.
Em comparação com clusters, este tipo de arquitetura é muito mais custosa, porém muitas vezes necessária para a computação de determinados tipos de problema.
Acredita- se que o ICTM é uma aplicação que pode ser melhor adaptada para arquiteturas Em uma devido a a quantidade de dados a serem armazenados, à disponibilidade dos mesmos durante o processo de categorização e por exigir grande capacidade de processamento.
O desempenho de uma solução paralela de uma aplicação em máquinas Em uma está basicamente relacionado a dois fatores:
As características da máquina e da aplicação.
As principais características que descrevem uma máquina Em uma são o fator Em uma, o número de processadores em cada nodo e o tamanho da memória.
Máquinas que possuem um baixo fator Em uma normalmente possuem uma maior quantidade de nodos e poucos processadores por nodo.
Isto permite a possibilidade de explorar diversos acessos simultâneos a diferentes blocos de memória, reduzindo- se assim a disputa de diversos processadores a um mesmo bloco.
Por outro lado, máquinas com alto fator Em uma são normalmente organizadas numa estrutura com poucos nodos e muitos processadores por nodo, tendo- se em vista que o tempo de acesso a um bloco de memória distante é muito maior.
As características da aplicação alvo também influenciam no desempenho da solução paralela.
Quanto mais acessos a dados forem necessários, maior a importância a ser dada para uma melhor distribuição dos mesmos nos diversos blocos de memória.
Outro fator importante é o padrão de acesso aos dados, que pode ser regular ou irregular.
Aplicações com um padrão de acesso regular normalmente apresentam bons resultados em máquinas com alto fator Em uma, visto que a regularidade do acesso a dados permite uma melhor distribuição dos dados, deixando- os mais próximos dos processadores que os utilizam.
Um exemplo de uma aplicação com padrão de acesso regular aos dados é o ICTM.
Como sabe- se de antemão a forma com que os dados serão acessados, pode- se definir a melhor localização dos mesmos de forma a extrair ao máximo os benefícios oferecidos por a máquina alvo.
Isto foi comprovado por os resultados obtidos na arquitetura Itanium 2, onde a Numa-ICTM mostrou resultados bem mais expressivos em comparação com a solução OpenMP-ICTM.
Por outro lado, aplicações com um padrão de acesso irregular podem apresentar resultados melhores em arquiteturas com baixo fator Em uma, pois o custo do acesso a dados distantes não é muito maior.
Mostra- se importante salientar também o fato de que não existe uma melhor política de memória.
A escolha da política a ser aplicada está relacionada às características da máquina alvo e da aplicação.
Em este contexto, o fator Em uma e o padrão de acesso aos dados são informações importantes que devem ser levadas em consideração para a escolha da política mais adequada.
Para que possa ser feita uma análise de quanto (na média) a solução Numa-ICTM é melhor que a OpenMP-ICTM, o seguinte procedimento foi feito:
A comparação dos resultados entre as duas soluções foi feita utilizando- se a seguinte fórmula, onde A significa &quot;arquitetura», C significa &quot;caso de estudo «e P significa &quot;política&quot;:
GanhoA, C, P $= (SpM edN U M A_ ICT MA, C, P -- SpM edOpenM P_ ICT MA, C) 100 SpM edOpenM P_ ICT MA, C Esta equação fornecerá a seguinte informação: Quanto (
em a média) a solução Numa-ICTM é melhor que a solução OpenMP-ICTM em termos percentuais.
Um percentual negativo para uma combinação Arquitetura X Caso de Estudo X Política de memória indica que a solução NUMAICTM apresentou resultados piores que a solução OpenMP-ICTM para esta combinação.
Salienta- se que estas médias servirão somente para a comparação entre as duas soluções de forma bastante aproximada, dando assim uma idéia geral de quanto a solução otimizada é melhor.
A Tabela 2 apresenta o resultado da aplicação da Equação 7.1 (A, C, P), onde SpM edN U M A_ ICT MA, C, P e SpM edOpenM P_ ICT MA, C representam a média dos speedups de 2 a 16 processadores das soluções Numa-ICTM e OpenMP-ICTM, respectivamente.
Opteron Itanium 2 Opteron Itanium 2 Opteron Itanium 2 bind_ all bind_ block 20,49% 15,01% 10,56% 15,36% 11,08% 12,15% cyclic cyclic_ block 18,20% Como pode ser visto na Tabela 2, a política bind_ block apresentou melhores resultados na arquitetura Itanium 2, enquanto a política cyclic foi a melhor para a arquitetura Opteron.
A política bind_ all em alguns casos apresentou resultados piores que a solução OpenMP-ICTM (representados na tabela por valores negativos).
Como observado anteriormente, a melhora de desempenho da solução Numa-ICTM é mais significante à medida em que são utilizados mais processadores.
Isto é comprovado por a Tabela 3, onde apresenta- se o resultado da aplicação da Equação 7.1 (A, C, P), porém, as médias dos speed-ups utilizadas compreendem somente o intervalo de 8 a 16 processadores.
Opteron Itanium 2 Opteron Itanium 2 Opteron Itanium 2 bind_ all bind_ block 26,25% 17,40% 14,92% 17,41% 15,86% 13,60% cyclic cyclic_ block 29,39% Como esperado, considerando- se somente os resultados de 8 a 16 processadores o ganho de desempenho em relação a solução OpenMP-ICTM foi maior para as políticas com melhor resultado em cada arquitetura.
Isto comprova a melhor escalabilidade da solução NUMAICTM, a qual apresenta melhores resultados quando um maior número de processadores é utilizado.
Por fim, a Tabela 4 apresenta o quanto a solução Numa-ICTM é melhor que a OpenMPICTM quando considera- se somente os resultados com 16 processadores.
Novamente, os resultados são superiores visto que com muitos processadores a solução Numa-ICTM apresenta um desempenho bem mais interessante.
Opteron Itanium 2 Opteron Itanium 2 Opteron Itanium 2 bind_ all bind_ block 37,67% 29,68% 28,08% 28,67% 31,01% 24,23% 11,61% 30,85% cyclic cyclic_ block 44,07% 12,80% 10,95% 12,05% 29,17% No geral, a utilização de políticas de memória trouxe grandes benefícios.
Os maiores ganhos puderam ser observados quando um maior número de processadores foi utilizado.
A queda drástica de desempenho apresentada por a solução OpenMP-ICTM a partir de 8 processadores não foi observada na solução Numa-ICTM.
A aplicação de políticas adequadas para as arquiteturas alvo e características do ICTM possibilitou uma melhor utilização dos recursos, resultando em expressivos ganhos de desempenho.
Um dos fatores os quais motivaram a realização deste trabalho foi a possibilidade da utilização e avaliação da MAI:
Uma interface inovadora que está sendo desenvolvida no INRIA para controle de políticas de memória em arquiteturas Em uma.
Acredita- se que o trabalho aqui apresentado serviu como um caso de estudo onde a interface pôde ser aplicada a um problema real.
De acordo com os resultados mostrados, a utilização da interface trouxe inúmeros benefícios, aumentando- se o desempenho da aplicação alvo.
Acredita- se que os mesmos resultados obtidos neste trabalho também poderiam ter sido alcançados utilizando- se a Em uma API.
Porém, devido a o baixo nível de abstração (máscaras de bits, chamadas de sistema, entre outras peculiaridades) um maior tempo seria necessário para que resultados de mesma qualidade fossem conquistados.
A Em uma API é bastante eficaz, porém exige um maior conhecimento dos programadores. Como
exemplos dos benefícios da MAI destaca- se:·
Funções de Controle de Políticas de Memória:
Forma prática de aplicar diferentes políticas de memória.
Abstraem máscaras de bits necessárias para especificar em quais blocos de memória as políticas serão aplicadas;·
Funções de Controle de Políticas de Threads: De a mesma forma que as funções de controle de políticas de memória, abstraem máscaras de bits necessárias para especificar em quais processadores cada thread deverá ser executada.
A utilização de arquivos de configuração facilita a especificação da arquitetura alvo, como por exemplo quais processadores e nodos deverão ser utilizados, reduzindo drasticamente o número de linhas a serem codificadas.
Infelizmente, a migração de processos não foi utilizada neste trabalho, pois o ICTM possui um padrão de acesso a dados regular.
Logo, esta funcionalidade da interface não pode ser testada.
Levando- se em conta os benefícios da solução HPC-ICTM (para clusters e grids) e aqueles apresentados neste trabalho (Numa-ICTM), pontua- se como uma possibilidade de trabalho futuro a integração destas duas soluções, oferecendo assim uma proposta para cluster de máquinas Em uma.
Em este contexto, o padrão MPI poderia ser utilizado para a comunicação entre diferentes nodos do cluster enquanto a API OpenMP em conjunto com a interface MAI poderiam ser utilizadas para a paralelização interna em cada nodo.
Uma estratégia possível seria a utilização da decomposição em camadas proposta por os trabalhos relacionados.
Em este caso, cada processo escravo processaria uma determinada camada do modelo, paralelizando- o da forma apresentada neste trabalho.
A Figura 29 mostra uma idéia geral desta proposta de integração do HPC-ICTM com a Numa-ICTM.
Acredita- se que com esta integração seja possível aumentar ainda mais os casos de estudo de entrada, sendo possível assim, a categorização de regiões geográficas ainda maiores.
