O teste de software é uma técnica utilizada para fornecer informações sobre a qualidade de um software operando num contexto específico.
O tipo de teste responsável por avaliar o desempenho e a eficiência de um software num cenário de uso é conhecido como teste de desempenho.
A elaboração de um teste de desempenho é uma tarefa que exige testadores com conhecimentos especializados referentes às ferramentas, atividades e métricas do domínio.
Para representar tais conhecimentos, este trabalho propõe uma ontologia sobre o domínio de teste de desempenho.
Ontologia é uma técnica de representação do conhecimento considerada o estado da arte dentro de a Inteligência Artificial.
Além disso, uma das contribuições desta pesquisa é mapear, com base na análise de trabalhos relacionados, o que se sabe sobre a utilização de ontologias no teste de software.
Por fim, a ontologia proposta é avaliada por especialistas do domínio, comparada com ontologias relacionadas e explorada em aplicações que têm por objetivo auxiliar os testadores quanto a o planejamento e à elaboração dos testes de desempenho.
Palavras Chave: Ontologia, teste de software, teste de desempenho.
Os profissionais da computação perceberam a necessidade de desenvolver técnicas de teste de software em meados da década de 1980.
Contudo, foi apenas na década de 1990 que começaram a aparecer whitepapers, seminários e artigos nesta área.
Consequentemente, o conhecimento em teste de software ainda está em fase de expansão e consolidação.
O teste é uma tarefa não trivial por causa de as características próprias dos softwares, como intangibilidade e complexidade.
A dificuldade em testar software é caracterizada, sobretudo, devido a a falta de profissionais especializados;
A a dificuldade em implantar um processo de teste;
A o desconhecimento de técnicas e procedimentos de teste e ao desconhecimento sobre como planejar os testes.
Alinhado a isto, as atividades de planejamento e elaboração dos testes de software demandam testadores com conhecimento sobre as técnicas de teste, critérios, artefatos e ferramentas.
Esta diversidade de conceitos levanta uma questão:
Como estabelecer um entendimento compartilhado sobre teste de software?
Ontologias são ferramentas capazes de capturar o conhecimento de domínios e, por este motivo, mostram potencial para preencher as lacunas citadas no processo de teste.
O tema ontologia é um tópico de pesquisa popular em diferentes comunidades como engenharia do conhecimento, processamento da linguagem natural e gestão do conhecimento.
Em Inteligência Artificial, ontologias são desenvolvidas para facilitar a aquisição, organização, representação, compartilhamento e reuso do conhecimento de um domínio.
De acordo com as motivações apresentadas, este trabalho tem como objetivo propor uma ontologia para modelar os conhecimentos sobre o domínio de teste de desempenho.
Visando explorar os temas essenciais deste estudo, os tópicos iniciais apresentam uma fundamentação teórica sobre ontologias e sobre teste de software.
Em seguida, são apresentadas sínteses dos trabalhos relacionados de modo a identificar e comparar suas principais contribuições na área.
A revisão da literatura também possui como objetivo tornar claro de que forma ontologias foram ou podem ser aplicadas nos domínios de teste de software e teste de desempenho.
Uma das aplicações se refere ao uso de ontologias no planejamento dos testes de desempenho, sendo um dos incentivos para construir uma ontologia neste domínio.
Devido a a complexidade inerente aos testes de desempenho, o indicado é que eles sejam planejados por um time de testadores experientes.
Em outras palavras, elaborar um teste de desempenho exige conhecimentos de teste encontrados em testadores especializados, e não apenas num único indivíduo.
Portanto, este trabalho propõe uma ontologia para modelar os conhecimentos sobre o domínio de teste de desempenho com o objetivo de apoiar a decisão de testadores, em especial aqueles que ainda não possuem a experiência necessária para planejar os testes de desempenho.
A ontologia foi representada na linguagem OWL (Web Ontology Language), desenvolvida no editor Protégé e seguindo a metodologia de Noy e McGuinness.
As aplicações da ontologia exploradas nesta dissertação foram direcionadas a auxiliar os testadores quanto a o planejamento de seus testes.
Uma das aplicações foi desenvolvida como plugin para o Protégé, usando a API oferecida por o Protégé para manipular a ontologia.
Foi desenvolvida também uma aplicação em Java que usa a OWL API para manipular a ontologia.
Além disso, foi investigada a utilização do reasoner Pellet para inferência de novos axiomas com base na ontologia e foi explorada a utilização da linguagem SQWRL (Semantic Query--Enhanced Web Rule Language) para executar consultas na ontologia.
Por fim, a ontologia proposta é comparada com duas ontologias relacionadas e também passa por um processo de avaliação por especialistas em teste de desempenho.
Esta pesquisa tem como objetivo geral a construção de uma ontologia sobre teste de desempenho de software.
Um dos objetivos da ontologia será fornecer uma base de conhecimento sobre os conceitos mais relevantes do domínio como, por exemplo, métricas de desempenho, ferramentas, atividades, objetivos de teste, etc..
Embora a utilização desta base de conhecimento possa ser variada, a pesquisa se concentrou na aplicação da ontologia nas atividades de planejamento e documentação dos testes de desempenho.
Embora existam ontologias sobre teste de software, como a OntoTest encontrados trabalhos propondo ontologias sobre o domínio de teste de desempenho.
Portanto, este trabalho teve como base os seguintes objetivos:
Investigar aplicações de ontologias no teste de software e no teste de desempenho.
Pesquisar tecnologias utilizadas na construção e utilização de ontologias, em especial o Protégé, a linguagem OWL e recursos relacionados.
Desenvolver e explorar aplicações baseadas na ontologia proposta.
Investigar e aplicar técnicas para avaliar a ontologia proposta.
Comparar a ontologia proposta com ontologias relacionadas.
Comparar as principais contribuições entre este trabalho e os trabalhos relacionados.
Este trabalho está organizado em sete capítulos.
O capítulo 1 introduziu uma visão geral sobre a contextualização, a motivação e os objetivos desta pesquisa.
O capítulo 2 é responsável por a fundamentação teórica sobre teste de software, teste de desempenho e ontologias.
Em seguida, o capítulo 3 apresenta um estudo sobre trabalhos relacionados que definem ou aplicam ontologias sobre o domínio de teste de software.
Os conceitos, propriedades, axiomas e instâncias da ontologia proposta são explicados na seção 4.
A comparação da ontologia proposta com duas relacionadas e a opinião dos especialistas em teste de desempenho sobre a ontologia é detalhada na seção 5.
Os softwares desenvolvidos que utilizam esta ontologia são apresentados na seção 6, juntamente com exemplos de uso.
As considerações finais da pesquisa são apresentadas na seção 7, onde as principais contribuições desta dissertação são destacadas e comparadas com as contribuições dos trabalhos relacionados.
Para facilitar o entendimento e unificar as definições adotadas no desenvolvimento deste trabalho, as subseções seguintes apresentam uma fundamentação teórica sobre teste de software, teste de desempenho e ontologias.
O teste é utilizado para avaliar a qualidade de um produto e possibilitar sua melhoria por meio de a identificação de problemas e defeitos.
O teste de software pode ser definido como o processo de executar um software de uma maneira controlada com o objetivo de avaliar se o mesmo se comporta conforme o especificado.
Segundo, o teste é o processo de analisar um item de software para avaliar suas características e detectar diferenças entre as condições existentes e as condições especificadas.
O teste de software engloba questões relacionadas ao processo, às técnicas, às ferramentas, ao ambiente e aos artefatos.
Além disso, o alvo do teste de software pode variar desde um módulo em singular, um grupo de módulos ou o sistema inteiro.
De este modo, alguns autores dividem as etapas de teste em três:
Unitário, de integração e de sistema.
O desempenho é avaliado por o teste de sistema, que é responsável por a avaliação também de outros requisitos não funcionais (como segurança e confiabilidade).
Segundo o Corpo de Conhecimento em Engenharia de Software (SWEBOK), o teste de software consiste na verificação dinâmica do comportamento de um programa por meio de um conjunto finito de casos de teste.
Estes casos de teste são selecionados de um domínio geralmente infinito de possíveis execuções.
Finalmente, os resultados obtidos nos testes devem ser comparados com um dos seguintes resultados esperados:
A especificação do software (referido como testes para verificação);
As expectativas dos usuários (referido como testes para validação);
O comportamento previsto de requisitos implícitos ou expectativas razoáveis.
Por este motivo, o teste está estreitamente relacionado com os requisitos, uma vez que os requisitos expressam necessidades ou restrições num software.
Além de esta definição, o SWEBOK define requisito de software como uma propriedade que deve ser exibida por o software a fim de resolver algum problema do mundo real.
Os requisitos podem ser classificados em funcionais e não funcionais.
Os requisitos funcionais descrevem funções que o software deve executar, ou seja, o que «deve ser feito, por exemplo:
Cadastrar usuário, efetuar transferência bancária e enviar mensagem.
Os requisitos não funcionais tratam de questões como desempenho, segurança, confiabilidade, usabilidade, acessibilidade e manutenibilidade.
Em outras palavras, os requisitos não funcionais expressam restrições sobre qualidades de comportamento do sistema.
É importante ressaltar que tanto os requisitos funcionais quanto os requisitos não funcionais podem causar a inutilização do sistema se não forem atendidos.
Segundo, os softwares apresentam características como intangibilidade e complexidade, o que torna o teste uma tarefa não trivial.
A dificuldade em testar software é caracterizada principalmente devido a os seguintes pontos:
O teste de software é um processo caro;
Existe uma falta de conhecimento sobre a relação custo/ benefício do teste;
Há falta de profissionais especializados na área de teste;
Existem dificuldades em implantar um processo de teste;
Há o desconhecimento de um procedimento de teste adequado;
Há o desconhecimento de técnicas de teste adequadas;
Há o desconhecimento sobre como planejar a atividade de teste;
E a preocupação com a atividade de teste somente na fase final do projeto.
Os casos de teste podem ser projetados visando diferentes objetivos, como revelar divergências entre os requisitos dos usuários;
Avaliar a conformidade do software com uma especificação padrão;
Analisar sua robustez em condições estressantes de carga, e assim por diante.
Segundo o SWEBOK, o planejamento do teste de software deve começar nos estágios iniciais do processo de requisitos e as atividades de teste devem estar presentes em todo o processo de desenvolvimento e manutenção.
O SWEBOK conclui que a prevenção é a atitude mais correta em direção a a qualidade, ou seja, é melhor evitar problemas do que corrigir- los.
Em este contexto, o teste, por meio de a descoberta de falhas, deve verificar até que ponto a prevenção está sendo efetiva.
Os termos mais importantes relacionados ao teste de software, de acordo com o glossário da Engenharia de Software definido por a IEEE, estão listados a seguir:
Test bed: Ambiente contendo os hardwares, instrumentação, simuladores, ferramentas de software e demais elementos necessários à condução do teste.
Test case:
Um conjunto de entradas do teste, condições de execução e resultados esperados.
Um caso de teste é desenvolvido para um objetivo particular como, por exemplo, exercitar um caminho de um programa ou verificar um requisito específico.
Test case generator:
Uma ferramenta de software que recebe como entrada código fonte, critérios de teste, especificações ou definições de estruturas de dados e gera como saída dados de entradas do teste e resultados esperados.
Test case specification:
Um documento que especifica as entradas do teste, condições de execução e resultados previstos para um item a ser testado.
Test criteria:
Os critérios que um sistema ou componente deve satisfazer para passar num dado teste.
Test design:
Documentação que especifica os detalhes da abordagem de teste para um recurso de software e identifica os testes associados.
Test documentation: Documentação descrevendo planos para, ou resultados de, o teste de um sistema ou componente.
A documentação de teste inclui especificação dos casos de teste, relatório de incidente de teste, registro de teste, plano de teste, procedimento de teste e relatório de teste.
Test incident: Documento que descreve um evento ocorrido durante o teste e que requer uma investigação mais aprofundada.
Test item:
Um item de software que é um objeto do teste.
Test item transmittal report:
Documento que identifica um ou mais itens submetidos ao teste, contendo o status atual e informações de localização.
Test log:
Registro cronológico dos detalhes relevantes sobre a execução de um teste.
Test objective:
Conjunto de características do software a serem medidas em condições especificadas, para que o comportamento real seja comparado com o comportamento requerido descrito na documentação do software.
Test phase:
Período de tempo no ciclo de vida de software em que os componentes de um produto de software são integrados e avaliados para determinar se os requisitos estão sendo satisfeitos.
Test plan: Documento que descreve a abordagem técnica e administrativa a ser seguida para testar um sistema ou componente.
O plano de teste identifica os itens a serem testados, tarefas e responsáveis por a execução de cada tarefa, cronogramas e recursos necessários para a execução do teste.
Test procedure:
Descreve instruções detalhadas para a configuração, execução e avaliação dos resultados de um dado caso de teste.
O procedimento de teste é uma documentação usada para especificar uma sequência de ações essenciais para a execução de um teste (sinônimo de script de teste).
Test report: Documento que descreve a execução e os resultados de um teste realizado num sistema ou componente.
Test summary: Documento que resume as atividades de teste e os resultados, contendo também uma avaliação dos itens de teste relacionados.
Testing: Processo de operar um sistema ou componente dentro de condições específicas, observando ou gravando resultados e fazendo uma avaliação de algum aspecto do sistema ou componente.
Também pode ser definido como o processo responsável por analisar características dos itens de software com o objetivo de detectar diferenças entre as condições existentes e as requeridas.
O desempenho é uma qualidade dos softwares que é afetado desde suas camadas inferiores, como sistema operacional, middleware, hardware e redes de comunicação.
Um sistema possui bom desempenho quando apresenta um tempo de resposta adequado e aceitável, mesmo se submetido a um volume de processamento próximo de situações reais ou de pico.
A área responsável por o estudo do desempenho é conhecida Engenharia de Desempenho de Software.
De acordo com Woodside, Franks e Petriu, a Engenharia de Desempenho de Software é o campo responsável por o estudo de atividades de Engenharia de Software e análises relacionadas usadas ao longo de o ciclo de desenvolvimento de software e direcionadas aos requisitos de desempenho.
Segundo estes autores, existem duas abordagens possíveis dentro de a Engenharia de Desempenho de Software:
Baseada em medição ou baseada em modelos.
Esta dissertação possui como foco a abordagem de medição, incluindo, portanto, atividades de teste, diagnóstico e tuning.
Estas atividades são realizadas apenas no final do ciclo de desenvolvimento, dado que o sistema real precisa ser executado e medido.
A outra abordagem de avaliação de desempenho é baseada em modelos, onde os resultados quantitativos dos modelos são usados para ajustar a arquitetura e o projeto com o objetivo de atingir os requisitos de performance.
Tendo conceituado essa área de estudo, podemos entender melhor a definição de teste de desempenho dada a seguir.
Segundo, o teste de desempenho determina ou valida a capacidade de resposta (responsiveness), vazão (throughput), confiabilidade (reliability) e/ ou escalabilidade (scalability) de um sistema sob uma dada carga (workload).
O teste de desempenho geralmente tem o objetivo de:
Definir as características de desempenho do sistema;
Além disso, o teste de desempenho abrange as seguintes atividades:
Identificação do ambiente de teste.
Além de o ambiente físico de teste e do ambiente de produção, devem ser identificados os recursos e as ferramentas disponíveis à equipe de testes.
O ambiente físico inclui configurações de hardware, software e rede.
Identificação dos critérios de aceitação de desempenho.
Quais os objetivos e as restrições de tempo de resposta, vazão e utilização de recursos.
Em geral, tempo de resposta é uma preocupação (concern) do usuário, vazão é uma preocupação do negócio e utilização de recursos é uma preocupação no nível do sistema.
Planejamento e projeto dos testes.
Identificar cenários principais, determinar a variabilidade entre os usuários representativos e como simular esta variabilidade, definir os dados do teste e estabelecer métricas para coletar.
Configuração do ambiente de teste.
Preparação do ambiente de testes, ferramentas e recursos necessários para executar o teste.
Implementação dos testes.
Desenvolvimento dos testes de desempenho de acordo com o planejamento.
Execução do teste e monitoramento.
Análise dos resultados, relatórios e retestagem.
Quando todos os valores monitorados estiverem dentro de os limites aceitáveis, o teste finalizou para aquele cenário em particular e para aquela configuração específica.
O teste de desempenho pode acabar recebendo diferentes classificações de acordo com seus objetivos, e.
g, teste de resistência, teste de pico ou outros.
Por exemplo, segundo, teste de carga é uma subcategoria do teste de desempenho focada em avaliar o desempenho do sistema quando submetido a uma carga equivalente a de produção.
Por outro lado, um teste de estresse representa outra subcategoria de teste de desempenho focada em avaliar o desempenho do sistema quando submetido a uma carga superior a de produção.
Este último tipo de teste também pode avaliar o sistema sob condições estressantes como insuficiência de espaço em disco, limitação de memória, etc..
O teste de estresse deve ser projetado para avaliar sob quais condições uma aplicação vai falhar, como será essa falha e quais indicadores podem ser monitorados para avisar que uma falha está prestes a ocorrer.
Esta seção introduziu os principais conceitos relacionados ao teste de software e ao teste de desempenho.
Dada a riqueza de conhecimentos da área de testes que os testadores devem possuir, este trabalho se propõe a investigar como ontologias podem auxiliar- los a representar estes conhecimentos.
Ontologia é uma técnica de representação de conhecimento atualmente considerada o estado da arte em Inteligência Artificial.
Assim, a próxima seção apresenta uma fundamentação teórica sobre ontologias, com o objetivo de fornecer uma visão geral sobre definições, aplicações, metodologias e ferramentas envolvidas no desenvolvimento e utilização de ontologias.
&quot;Our everyday is not precise, but it is nevertheless efficient.
Nature, itself, from galaxies to genes, is approximate and inexact.
Philosophical concepts are among the least precise.
Terms such as` mind',` perception',` memory', and` knowledge' do not have either a fixed nor a clear meaning but they make sense just the same. «(
Gian-Carlo Rota) Uma ontologia é geralmente utilizada como uma estrutura que captura o conhecimento sobre uma determinada área, fornecendo conceitos e relacionamentos relevantes.
A ontologia de um domínio abrange sua terminologia (ou vocabulário), os conceitos fundamentais, sua classificação, taxonomia, relações e axiomas.
O autor Thomas Gruber define uma ontologia como uma &quot;especificação explícita de uma conceitualização».
Porém, o autor Willem Borst, em sua tese de doutorado, modificou a definição anterior ao dizer que ontologia é uma &quot;especificação formal de uma conceitualização compartilhada».
O termo conceitualização faz referência ao modelo abstrato de algum fenômeno identificado no mundo por ter conceitos relevantes.
Formal refere- se ao fato de que a ontologia deve ser legível por máquina, enquanto que compartilhada significa que o conhecimento representando na ontologia é consensual (aceito por um grupo).
Segundo Noy e McGuinness, ontologias descrevem um domínio de discurso utilizando classes, propriedades, relacionamentos, atributos e restrições de conceitos.
Ainda, uma ontologia e suas instâncias individuais de classes constituem uma base de conhecimento.
Seguindo essa mesma definição, de acordo com, uma ontologia deve possuir, no mínimo, os seguintes componentes:
Classes, que representam conceitos.
As classes podem ser organizadas em taxonomias com o uso de herança.
A linguagem ontológica usada pode permitir a definição de atributos ou propriedades em classes.
Relacionamentos, que representam um tipo de interação ou associação entre conceitos do domínio.
Um relacionamento é como uma função matemática, onde o domínio e a imagem são conceitos.
Os atributos das classes são relacionamentos unários, pois possuem como domínio um conceito e a imagem é formada por um tipo de dado (número, data, string).
Axiomas, que modelam sentenças que são sempre verdade.
Os axiomas são úteis na inferência de novos conhecimentos e podem ser construídos em lógica de primeira ordem.
Instâncias, que representam elementos de um dado conceito do domínio.
Segundo, podem existir fatos representando relações entre instâncias.
Além de capturar o conhecimento sobre um domínio de interesse, segundo, ontologias podem ser aplicadas para:
Compartilhar um entendimento comum sobre a estrutura da informação entre pessoas e softwares;
Possibilitar análise e reuso do conhecimento do domínio;
Tornar explícitas as suposições sobre um domínio;
E criar uma separação entre o conhecimento operacional e o conhecimento do domínio.
Para se trabalhar com ontologias é necessário antes representar- las numa linguagem ou formalismo.
Diferentes linguagens ontológicas são comparadas em quanto a as suas capacidades de representação do conhecimento e inferência.
O domínio de conhecimento é formado por os componentes ontológicos citados anteriormente.
Por fim, o processo de raciocínio proporcionado por uma ontologia é executado sobre o seu domínio de conhecimento.
São exemplos de linguagens ontológicas:
Ontolingua, OCML, OWL, LOOM, F-Logic e Oil.
Sobre a construção de ontologias, segundo, não existe uma forma única e correta de modelar um domínio, ou seja, sempre existem alternativas viáveis e a melhor solução quase sempre depende da aplicação em mente.
Além disso, o desenvolvimento de ontologias é inevitavelmente um processo iterativo.
Segundo essas premissas, em foi elaborada uma metodologia para desenvolvimento de uma ontologia:
Determinar o domínio e o escopo da ontologia;
Considerar o reuso de ontologias existentes;
Enumerar termos importantes na ontologia;
Definir classes e hierarquia de classes;
Definir propriedades das classes;
Definir as características (facets) das propriedades;
Criar instâncias.
Outra metodologia para desenvolvimento de ontologias foi proposta em e está ilustrada na Figura 1.
As atividades descritas em ambas as metodologias possuem semelhanças, contudo a metodologia apresentada em utiliza uma linguagem de menor abstração, pois usa termos como classes, propriedades e instâncias.
Outra diferença identificada foi a presença, na metodologia definida em, de atividades específicas para a escolha da linguagem e das ferramentas de desenvolvimento.
Com relação a as ferramentas para desenvolvimento de ontologias, um estudo que compara suas características gerais e capacidades de representação do conhecimento pode ser encontrado em.
As ferramentas analisadas neste estudo incluem Ontolingua, WebOnto, Protégé e Ode, sendo o Protégé uma das ferramentas mais difundidas e robustas.
Assim sendo, o foco da subseção seguinte é apresentar a ferramenta Protégé.
Mais informações sobre o estado da arte em relação a as ferramentas, metodologias e linguagens para desenvolvimento de ontologias estão disponíveis nas referências,, e.
O Protégé é um ambiente para apoio ao desenvolvimento de sistemas baseados em conhecimento.
Este software open source, desenvolvido na Universidade de Stanford e aplicado inicialmente no domínio da medicina, pode ser utilizado para criar e editar ontologias e bases de conhecimento.
O Protégé implementa um conjunto de estruturas de modelagem de conhecimento e ações que suportam a criação, a visualização e a manipulação de ontologias representadas em diversos formatos.
Assim, esta ferramenta suporta dois diferentes paradigmas referentes a modelagem de ontologias:
Baseado em frames ou em OWL.
O editor Protégé-Frames possibilita aos usuários a criação e o povoamento de ontologias no formato frame-based.
Em este modelo, uma ontologia consiste num conjunto de classes organizadas numa hierarquia para representar os conceitos do domínio;
Um conjunto de slots associados às classes que representam suas propriedades e relacionamentos;
E as instâncias das classes.
O editor Protégé-OWL permite aos usuários a construção de ontologias para a web semântica, em particular usando OWL (Web Ontology Language).
Uma ontologia em OWL pode incluir descrições de classes, propriedades e suas instâncias.
Para o desenvolvimento da ontologia proposta neste trabalho, o editor Protégé-OWL foi adotado por ser a versão mais atual da ferramenta e com suporte a um número maior de recursos quando comparado ao Protégé-Frames.
A versão 4.2 foi utilizada por ser a versão mais atual até o momento.
Como ilustrado na Figura 2, a interface gráfica da versão 4.2 é dividida em abas, sendo cada aba responsável por determinados aspectos do desenvolvimento da ontologia.
A Figura 2 exibe as informações da aba &quot;Classes «(é importante observar que classes e conceitos são sinônimos no contexto do Protégé).
Tendo apresentado uma visão geral sobre ontologias e o editor Protégé, a seção seguinte apresentará aplicações propostas na literatura referentes ao uso de ontologias no teste ou no desempenho de software.
O objetivo desta seção é fornecer uma visão geral do estado da arte em relação a o uso de ontologias no teste de software e no desempenho de software.
Assim, serão apresentadas comparações e sínteses dos trabalhos relacionados considerados os mais relevantes na literatura analisada.
O critério de inclusão adotado foi que o trabalho deve definir ou utilizar ontologias no domínio de teste de software ou desempenho de software.
A seguir são destacados os principais argumentos dos autores quanto a a aplicação de ontologias no domínio de teste.
Uma vantagem encontrada em para justificar o uso de ontologias sobre o desempenho de software se refere à habilidade de raciocínio, que possibilita a tomada de decisões sobre o desempenho e outras características não funcionais do sistema.
Em outras palavras, ontologias permitem uma interpretação lógica e a aplicação de regras de inferência sobre a representação semântica de informações do domínio de desempenho de software.
Em a visão de e, ontologias são ferramentas capazes de capturar o conhecimento de domínios num formato compreensível por máquinas e, por este motivo, mostram potencial de serem usadas na automação do processo de teste.
Além de fornecer um vocabulário para representar e comunicar o conhecimento sobre o teste de software, tal ontologia deve apresentar um conjunto de relacionamentos existentes entre os conceitos deste domínio.
Ontologias podem contribuir na área de teste de software, pois:
Fornecem uma fonte de definição precisa de termos que podem ser usados na comunicação entre testadores, desenvolvedores, gerentes e usuários.
Oferecem um entendimento consensual compartilhado do teste de software.
Tornam explícitas as suposições ocultas relacionadas aos objetos pertencentes ao conhecimento de teste de software.
Segundo a literatura analisada, a seguir são identificadas as aplicações baseadas em ontologias.
Tais aplicações são bastante diversificadas quanto a o uso de ontologias nos domínios de teste de software ou desempenho de sistemas computacionais.
Algumas destas aplicações já chegaram a ser desenvolvidas e validadas, contudo outras ainda estão na fase conceitual.
As principais áreas de estudo identificadas são as seguintes:
Interoperabilidade entre os softwares para teste de software.
Diferentes softwares podem ser usados para criar, executar e analisar testes de desempenho, porém cada uma dessas ferramentas estrutura as informações de maneiras distintas.
Um nível maior de interoperabilidade entre as ferramentas para teste de desempenho poderia ser alcançado se as ferramentas compartilhassem a mesma ontologia.
Em este caso, seria possível especificar o teste numa ferramenta qualquer e executar- lo usando quaisquer outras ferramentas sem necessidade de alterar o teste projetado.
Além disso, os resultados monitorados durante a execução do teste poderiam ser carregados e analisados usando quaisquer ferramentas que compartilhassem a mesma ontologia.
Esta vantagem de interoperabilidade foi comentada em e.
Além disso, uma ontologia de teste de software pode auxiliar no estabelecimento de uma arquitetura de referência, o que facilitaria o uso e a integração de ferramentas, processos e artefatos.
Sugestão e validação do planejamento dos testes.
De acordo com a configuração de um sistema que se deseja testar o desempenho, como são escolhidas as tecnologias que podem ser usadas?
Por exemplo, se desejamos monitorar o sistema operacional Linux, quais ferramentas podem ser usadas?
Estas perguntas poderiam ser respondidas por uma ontologia que represente o conhecimento sobre tecnologias (técnicas, processos ou ferramentas) utilizadas no teste de desempenho.
Para isso, é necessário representar informações relacionadas as restrições e as capacidades envolvidas na aplicação de cada tecnologia, e.
g, uma capacidade fornecida por a ferramenta LoadRunner é conseguir testar aplicações sobre o protocolo Http.
De essa forma, uma vez que o ambiente de teste fosse definido, seria possível escolher quais tecnologias podem ser usadas no teste de desempenho.
Além disso, de acordo com as escolhas selecionadas, pode ser gerado um plano de teste correspondente.
Uma referência relevante para definir a estrutura do plano de teste é, por exemplo, o Standard 829 da IEEE.
O plano de teste é um documento que fornece a base para a execução dos testes de software de modo organizado, sistemático e bem documentado.
O uso de ontologias para o planejamento e a especificação de testes foi abordado como aplicação teoricamente viável em.
Consulta semântica na documentação de testes.
O uso de ontologias sobre documentos de testes permite que sejam executadas consultas semânticas sobre as informações dos testes.
O exemplo dado por é que seria possível perguntar quais classes e métodos não foram suficientemente testadas após a execução de um teste de cobertura de código.
A referência propõem uma técnica para recuperação de documentos sobre teste de software com base em ontologias.
A documentação de teste pode ser construída com base no padrão IEEE Standard for Software and System Test Documentation, que descreve o conteúdo de cada documento de teste.
Geração de casos de teste.
Alguns dos trabalhos analisados (e.
g, e) comentam sobre a possibilidade de geração de casos de teste para uma dada aplicação a partir de a combinação de duas ontologias:
Uma sobre teste de software e uma sobre o domínio da aplicação a ser testada.
Uma abordagem para geração de testes unitários baseada em ontologias é apresentada em, onde é argumentado como próximos passos a expansão da técnica nas fases de teste de integração e de sistema.
Por outro lado, em foi explicado como gerar dados para testes de robustez aplicando uma ontologia que abrange o domínio dos serviços web a serem testados.
Melhoria do desempenho durante a execução do sistema.
A melhoria do desempenho durante a execução do sistema foi citada, por exemplo, em e.
Estes trabalhos utilizam ontologias para representar informações do contexto atual do sistema e usam medições de desempenho ao longo de o tempo para alterar a configuração do sistema para tentar melhorar o desempenho.
Por exemplo, em, o raciocínio sobre o desempenho é baseado na ontologia e em regras definidas em lógica de primeira ordem.
Aprendizagem e padronização do vocabulário sobre teste de software.
Alguns autores, e.
g e, justificam que uma ontologia sobre teste de software pode ser utilizada para padronizar o vocabulário da área e facilitar a troca de conhecimentos.
Segundo, também é possível usar tal ontologia para aprendizagem sobre o domínio.
Em esta direção, as ontologias também oferecem um entendimento compartilhado sobre os conceitos e relacionamentos existentes no teste de desempenho.
Após apresentar resumidamente quais aplicações de ontologias foram propostas na área de teste, as duas subseções seguintes apresentam uma síntese de cada trabalho.
Estas sínteses possuem como foco executar uma análise individual da abordagem e dos resultados de cada um dos trabalhos relacionados.
Um dos objetivos desta pesquisa é avaliar como técnicas de representação do conhecimento (i.
e, ontologias) podem auxiliar nas atividades de teste.
Para identificar as aplicações comentadas na literatura, o tópico a seguir resume trabalhos que abordam simultaneamente os assuntos ontologias e desempenho de software, destacando os pontos principais de cada trabalho analisado e comparando as aplicações identificadas.
Object Management Group (OMG) para representar dados de desempenho em UML.
Core Scenario Model (CSM):
Abordagem desenvolvida por o grupo de Sistemas Distribuídos e de Tempo Real da Universidade de Carleton para integrar anotações de desempenho em modelos de software.
Software Performance Engineering Meta--Model (SPE-MM):
Abordagem criada por Connie Smith, Lloyd Williams e Catalina Lladó para definir entidades e relacionamentos úteis na construção de Software Execution Models e System Executions Models.
Além disso, na referência são idealizadas duas abordagens para a criação de uma ontologia neste domínio:
Bottom-up, que extrai conhecimento dos metamodelos;
E top-down, que é dirigida por um conjunto de requisitos.
O desempenho de software inclui conceitos como medição de desempenho, monitoramento, gerenciamento e geração de carga.
Porém, o trabalho de Vittorio foca em aspectos de modelagem do desempenho de software e por este motivo não considera estes conceitos citados anteriormente.
As entidades de cada um dos três metamodelos analisados são agrupadas numa das seguintes categorias:
Software Behavior:
Entidades que representam as dinâmicas do software.
Resources: Entidades que representam os recursos computacionais (exemplo:
Espaço de armazenamento e poder de processamento).
Workload: Entidades que representam a carga.
Além de comparar os conceitos comuns nos modelos SPT, CSM e SPE-MM, este trabalho definiu características que deveriam existir numa ontologia de desempenho de software no caso de construir- la usando a abordagem top-down.
Segundo, uma ontologia de desempenho de software deveria:
Ser capaz de representar os resultados da análise de desempenho.
Os resultados obtidos na análise devem ser representados de uma forma única, independente do tipo de modelo e solução adotada.
Este requisito foi abordado parcialmente na UML SPT, onde entidades foram introduzidas para representar explicitamente índices de interesse (tempo de resposta, utilização) e suas representações matemáticas (média, distribuição cumulativa).
A análise deve ser capaz de relacionar resultados de desempenho (tempo de resposta, utilização) para fornecer recomendações aos engenheiros de software.
Lidar com o maior número de formalismos de desempenho quanto possível.
O conjunto de notações de desempenho geralmente adotados (filas de rede de espera, redes de Petri estocásticas) suportam a viabilidade deste requisito.
Ou seja, a conversão de modelos UML anotados em outros formalismos (e.
G., cadeia de Marvok).
Prover formas de integrar facilmente um modelo de software com anotações de desempenho.
Hoje, raciocinamos sobre modelos UML, mas tal ontologia não pode ser adaptada em torno de uma linguagem de modelagem de software, caso contrário ela não seria capaz de representar &quot;a natureza e as relações do ser».
Ser compatível com representações internas das ferramentas de desempenho existentes.
O motivo deste requisito é evitar o desperdício do conhecimento prático já consolidado dos especialistas de desempenho.
Lidar com desenvolvimento de software baseado em componentes.
A composição de software é um aspecto estudado no ponto de vista funcional, enquanto que a composição de aspectos não funcionais, como o desempenho, ainda é uma questão em aberto.
Fornecer ferramentas para representar este aspecto é uma característica sutil, mas crucial de uma ontologia nesta área.
Uma questão em aberto é:
&quot;O desempenho de um sistema pode ser modelado a partir de o desempenho de seus componentes?».
Performance-related ontologies and semantic web applications for on-line apresenta uma ontologia de desempenho e uma técnica de análise de desempenho para aplicações inteligentes baseadas na web semântica.
Além disso, o trabalho comenta a construção de regras de desempenho usando OWL para inferência automática de novas restrições de desempenho e conhecimento da Qualidade de Serviço (QoS) do sistema em execução.
Assim, esta abordagem pode ser aplicada para detecção de gargalos e garantia da QoS.
O metamodelo de desempenho usado neste trabalho é baseado na Algumas das variáveis de desempenho definidas em estão detalhadas a seguir:
Tempo de resposta:
Intervalo entra a requisição e a resposta de uma dada transação.
Vazão (throughput):
Taxa em que requisições são servidas.
Utilização: Probabilidade do serviço ou recurso estar ocupado.
Demanda: Tempo para completar um conjunto composto de requisições.
Outras variáveis como tempo de serviço, latência, prioridade, capacidade.
Em, as variáveis de desempenho são diferenciadas em quatro classes:
Em é utilizada uma descrição em OWL para armazenar, analisar e raciocinar sobre informações relacionadas às métricas de desempenho do sistema em execução.
Após implementar a ontologia OWL sobre avaliação de desempenho, foi construída uma aplicação externa (broker) para interpretar, computar e atualizar os valores de desempenho dos clientes e componentes do sistema.
O raciocínio pode ser baseado em regras definidas por os usuários em lógica de primeira ordem.
A seguir é exemplificado o raciocínio para seleção da tarefa a ser executada de acordo com o workload.
Workload baseado na utilização de recursos:
Foram definidas três classes disjuntas denominadas HardWork, MediumWork e LightWork.
O conceito HardWork foi definido como qualquer tarefa que utiliza uma quantidade de recurso superior ao seu respectivo limiar (threshold), como ilustrado na Figura 4.
Portanto, esta classificação depende do ambiente de execução, ou seja, se o recurso for trocado por outro mais poderoso computacionalmente, a classificação pode mudar.
Workload baseado na prioridade do produtor:
Duas classes disjuntas denominadas HighPriority e LowPriorityWork foram definidas para permitir a classificação do workload com base na prioridade do agente que produz a carga.
A Figura 5 ilustra as premissas para inferir que uma instância pertence ao conceito HighPriorityWork.
Workload baseado na eficiência:
Para permitir a classificação do workload com base na eficiência foram criados os seguintes conceitos disjuntos:
EfficientExecutionWork e NotEfficientExecutionWork.
A Figura 6 exemplifica a lógica que permite deduzir que uma instância é do tipo EfficientExecutionWork.
Em o nível de Qualidade de Serviço (QoS), é possível executar ações ou inferir conhecimento por meio de a aplicação de regras.
Por exemplo, ações podem parar a execução de uma tarefa que não está de acordo com o nível exigido de QoS.
Esta ação é ilustrada na Figura 7, que considera o tempo de resposta como o parâmetro de avaliação da qualidade.
Por outro lado, também é possível detectar recursos que estejam diminuindo a eficiência global da aplicação e reconfigurar o sistema, por exemplo, distribuindo o workload para recursos ociosos.
De acordo com os autores, a motivação deste trabalho é determinada por a inexistência de técnicas no campo da engenharia ontológica que apoiem uma análise inteligente do desempenho do sistema.
Em outras técnicas de desempenho, a maioria das ferramentas usa linguagens próprias para anotar medições ou trocar dados entre ferramentas de modelagem e avaliação.
O reuso de medições é feito por relacionamentos estruturais (XML parsers ou bancos de dados relacionais), contudo a semântica não é considerada.
Além disso, existem novas áreas onde a riqueza semântica das informações pode auxiliar no processo de tomada de decisão.
Sistemas inteligentes requerem um processo de decisão baseado na avaliação de desempenho e na informação da Qualidade de Serviço.
Portanto, o modelo deve levar em consideração características dos serviços como prioridades, limiares, etc..
Seguindo o exemplo do processador, uma maneira natural de expressar sua Qualidade de Serviço poderia ser:
&quot;o processador não sistema automaticamente dispara alguma regra pré-definida para garantir o Service Level Agreement sobre a utilização do processador.
Este artigo mostra que é possível raciocinar sobre o desempenho num ambiente inteligente e até mesmo executar ações para influenciar- lo durante a execução.
Assim, segundo os autores, o conhecimento acumulado na Engenharia de Desempenho de Software deve incluir novas questões relacionadas a ontologias.
Web Performance and Behavior Ontology. Este trabalho define uma arquitetura para melhoramento contínuo do desempenho de sistemas web durante sua execução.
Para isso, são usadas informações sobre o contexto atual do sistema, o que inclui os usuários, os servidores e as medições do desempenho ao longo de o tempo.
É importante lembrar que a referência também apresenta estudos sobre a adaptação dinâmica de sistemas de acordo com o estado atual e o ambiente.
Em é proposta uma abordagem que utiliza ontologias para representar as seguintes informações:
Behavior Knowledge Base: Módulo centralizado que armazena em ontologias informações sobre os elementos significativos de um sistema web (usuários, requisições, proxies, nodos de cache, gateways, servidores, etc).
Web System Elements Knowledge Base:
Base que mede, representa e analisa informações sobre o desempenho de cada elemento contido no sistema de interesse.
Para cada parte do sistema, devido as suas características próprias, deve- se usar uma descrição ontológica.
Além disso, a Base de Conhecimento Comportamental (Behavior Knowledge Base) deve possuir acesso às informações armazenadas nesta base.
Por meio de inferências na Base de Conhecimento Comportamental, é possível modificar o estado global do sistema de modo dinâmico.
O desempenho de um sistema é uma característica resultante da combinação do desempenho de cada uma de suas partes, o que pode incluir camadas de banco de dados, servidor web, etc..
Portanto, para melhorar o desempenho global do sistema é preciso considerar informações de todas as suas camadas.
Este trabalho parte do princípio de que o comportamento de cada elemento do sistema seja modelado numa ontologia e armazenado numa base de conhecimentos.
Estas ontologias possuem conhecimento sobre o funcionamento do sistema, como por exemplo, informações sobre o balanceamento de carga, políticas de cache, priorização de tarefas, etc..
De este modo, os reasoners usam estas informações para aperfeiçoamento (tuning) do sistema, o que é feito por meio de a aplicação de regras, heurísticas e operações.
Quando o estado do sistema é alterado, o reasoner avalia o comportamento e o desempenho dos elementos neste novo estado.
Depois de um período de tempo, se o desempenho melhorou, o reasoner considera este novo estado como estável (stable).
Porém, se o desempenho piorou, o reasoner restaura o estado antigo e tenta alterações diferentes no sistema.
Para criar um novo estado, o reasoner aplica operações selecionadas de acordo com parâmetros da Base de Conhecimentos Comportamental.
Esta abordagem é ilustrada na Figura 8.
Em, é acrescentada uma camada de cache entre os clientes e o servidor para armazenar as requisições geradas, reduzindo o custo computacional e o tempo de resposta.
Ao contrário de armazenar páginas inteiras, são armazenados fragmentos de páginas na tentativa de aumentar o desempenho.
A justificativa para isto é que, a melhor possibilidade de fragmentação, do ponto de vista do desempenho, pode mudar ao longo de o tempo.
Portanto, para inferir a melhor forma de fragmentação, a camada de cache coleta informações sobre:
Usuários: Páginas mais solicitadas, caminhos de navegação, etc..
Servidor: Tempo de resposta e tamanho por requisição, taxa de mudança no conteúdo de cada requisição e taxa de compartilhamento entre conteúdos de páginas web.
Os autores de consideram a ampliação deste trabalho em dois ramos diferentes.
O primeiro é a criação de linguagens ontológicas específicas para modelar os diferentes elementos de um sistema web e a integração dessas ontologias nos elementos que usariam a informação armazenada na base de conhecimento correspondente.
O segundo ramo é a definição de operadores e regras para alterar os modelos de cada elemento de um sistema web usando a informação da base de conhecimento de desempenho.
An Approach to Ontology--aided Performance Engineering through Framework.
Este trabalho teve como objetivo formalizar uma base de conhecimento que incluísse medidas, como tempo de resposta, e técnicas úteis para atingir um bom desempenho, como indexação.
Segundo os autores, este conhecimento deve ser representado de uma forma intuitiva, fácil de ler, apoiado por diagramas e, se possível, numa linguagem formal tanto sintática quanto semanticamente.
A pesquisa descrita em se baseia no Non-Functional Requirement (NFR) Framework Softgoal Interdependency Graph.
Este framework auxilia os desenvolvedores a lidarem com requisitos não funcionais, o que inclui, entre outros, o desempenho.
Um softgoal define características desejáveis do sistema, eventualmente qualitativas e podendo não apresentar critérios de satisfação bem definidos.
A Figura 9 ilustra um exemplo de Grafo de Interdependência de Softgoals, onde as nuvens representam softgoals e as flechas conectando as nuvens representam Interdependências entre softgols.
Se as Interdependências estiverem rotuladas com o símbolo+, então significa que a contribuição é positiva.
Se a nuvem estiver em negrito, então o softgoal é do tipo operacionalização, que representa técnicas para atingir outros softgoals.
Os símbolos dentro de as nuvens foram colocados por um procedimento que avalia os softgoals de baixo para cima, seguindo as interdependências.
A Figura 9 ilustra o modelo proposto e uma explicação mais detalhada pode ser encontrada em.
Em são apresentadas duas ontologias projetadas em OWL:
Uma para descrever o Grafo de Interdependência de Softgoal e outra para representar o tipo dos requisitos não funcionais (NFR).
Assim, é possível usar uma notação mais formal que adiciona semântica e capacidade de inferência por computadores.
Uma das ontologias proposta contém o metamodelo da descrição hierárquica dos tipos de NFR.
Esta primeira ontologia é estruturada em formato de árvore e possui apenas uma classe denominada NFR_ Type com duas propriedades:
HasSubtype e isSubtypeOf.
Estas propriedades indicam a hierarquia (ascendente e descendente) dos requisitos não funcionais.
A segunda ontologia descreve os Grafos de Interdependência de Softgoals (SIG), que são representados como instâncias das classes.
Esta segunda ontologia possui classes para representar os Softgoals e as Interdependências.
Para definir uma Interdependência é preciso relacionar qual o seu tipo e também qual a sua contribuição.
O type da ontologia denominado topic.
De essa forma, é possível representar que atingir um bom throughput (type) para uma impressora (topic) não é o mesmo que para um banco de dados.
Segundo os autores, a aplicação destas ontologias pode auxiliar no processo de tomada de decisão baseado no conhecimento de decisões anteriores que impactaram negativa ou positivamente no desempenho.
E se a experiência acumulada de todos os projetos de software estivesse disponível?
Os softwares designers poderiam escolher soluções que funcionaram bem para outros e evitar escolhas prejudiciais.
Os autores deste artigo denominaram esta base de conhecimento de Software Performance que se sabe sobre a Engenharia de Desempenho, o que inclui qualquer abordagem comprovadamente útil ou prejudicial do ponto de vista do desempenho.
Contudo, o conhecimento armazenado não precisa ser restrito às questões de desempenho, por exemplo, a criptografia de dados aumenta a segurança, mas pode degradar o tempo de resposta.
Este artigo mostra os primeiros blocos de construção do SPEBoK e a intenção dos autores é combinar a Engenharia de Software e a Engenharia de Desempenho para permitir a troca de conhecimentos e a definição de um vocabulário comum.
A seção anterior apresentou trabalhos que abordaram simultaneamente os temas ontologia e desempenho de software.
Esta seção é responsável por apresentar trabalhos que relacionem ontologias no domínio de teste de software.
É esperado que uma ontologia de teste forneça um vocabulário para representar e comunicar o conhecimento sobre o domínio de teste de software.
Além disso, tal ontologia deve apresentar um conjunto de relacionamentos existentes entre os conceitos do domínio.
Com o objetivo de entender melhor como essa questão é tratada na literatura, a seguir são apresentadas sínteses de trabalhos sobre ontologias em teste de software.
Ontology--Based Web Application Testing.
De acordo com a referência, para automatizar a geração e a execução de casos de testes usando ontologias são obrigatórias duas etapas.
A primeira etapa consiste em desenvolver uma ontologia que capture num nível adequado o conhecimento necessário para efetuar o processo de teste.
O conhecimento sobre o processo de teste inclui os diferentes tipos de teste, seus objetivos, limitações, capacidades e atividades envolvidas.
Esta ontologia de teste de software poderia ser criada com base no SWEBOK e reusada em qualquer novo teste a ser executado.
Além disso, é necessário codificar também, em formato processável por máquina, o conhecimento sobre o domínio da aplicação a ser testada.
Este conhecimento inclui os conceitos, possibilidades, limitações, relacionamentos e funcionalidades esperadas da aplicação sob teste.
A ontologia do domínio da aplicação pode ser elaborada simultaneamente com o desenvolvimento da aplicação em si.
A etapa final consiste em desenvolver procedimentos que usem o conhecimento embarcado nestas ontologias para automatizar as tarefas de teste.
Em são citados 5 exemplos teóricos para ilustrar como ontologias podem auxiliar na automação de testes em aplicações web:
Especificação e planejamento do teste:
Usando uma ontologia que contém o conhecimento das atividades de teste (como ordem e relacionamentos), é possível especificar o plano de teste de forma automática.
Por exemplo: Se a especificação diz que &quot;o sistema x deve ser testado usando uma estratégia de caixa preta», pode ser inferido que tipos de testes e em qual ordem devem ser executados.
Ainda é possível definir quais critérios de teste e quais métodos de geração de casos de teste devem ser usados.
Consulta semântica:
O uso de ontologias nas atividades de teste (como planejamento, especificação, execução e análise) permite a geração automática de documentos do processo inteiro.
Segundo, seria possível recuperar informações do processo de teste usando consultas semânticas, como, por exemplo, após executar um teste de cobertura do código na aplicação, seria possível perguntar quais classes ou métodos não foram suficientemente testadas.
Usar ontologias como facilitadoras:
Ontologias podem auxiliar na definição, publicação, registro, anúncio e recuperação de web services.
Os web services (ou também agentes de software) podem ser usados nas diferentes atividades do processo de teste e as ontologias podem ser usadas como meio de comunicação, compartilhando o conhecimento do domínio e aumentando a cooperação entre os agentes ou serviços.
Geração dos casos de teste:
Se a geração for baseada na ontologia de teste de software, o tipo de teste a ser executado define o método de geração.
Por exemplo: A o executar um teste de segurança em aplicações web, pode ser usado SQL injection para gerar os dados que vão preencher os campos do formulário.
Contudo, se a geração for baseada na ontologia de domínio da aplicação, é possível gerar dados semanticamente válidos para preencher as requisições.
Oráculos de teste:
Um oráculo de teste julga o resultado dos testes para decidir se passaram ou falharam.
Este julgamento é baseado em critérios que podem ser definidos formalmente em ontologias.
Por exemplo, quando executando um teste de desempenho em aplicações web, os resultados podem ser julgados com base no atraso da resposta.
Ou, igualmente, os resultados podem ser julgados com base no código de status das respostas Http ou se um registro foi inserido ou alterado de um banco de dados.
Tais arquiteturas são responsáveis por facilitar o uso e a integração de ferramentas, processos e artefatos em ambientes de engenharia de software.
Em este sentido, a OntoTest foi usada para estabelecer uma arquitetura de referência sobre teste de software, denominada RefTEST.
Para estabelecer arquiteturas de referência é preciso conhecimento profundo sobre o domínio, mas uma vez estabelecidas, o conhecimento do domínio é associado em suas atividades e relacionamentos.
Outro cenário possível de aplicação desta ontologia é o estabelecimento de um processo de aprendizagem sobre teste de software.
Assim, a ontologia pode facilitar o acompanhamento de mudanças no conhecimento de teste, assim como o ensinamento deste novo conhecimento por meio de uma abordagem sistemática.
A Figura 10 ilustra a estrutura da OntoTest:
Knowledge--based Software Test Generation.
Este trabalho aponta que a geração de testes deve considerar três aspectos:
A especificação do que deve ser testado, que geralmente é definida em oráculos de teste e critérios de cobertura.
A especificação define os requisitos e o correto comportamento do software.
A abordagem proposta em consiste na utilização de ontologias e regras para descrever este conhecimento.
Esta ontologia é então combinada com uma ontologia do modelo mental do especialista em testes, que pode incluir conhecimentos sobre a implementação, aspectos propensos a erros e outros.
Por fim, os critérios de cobertura são especificados de acordo com um padrão ou por o especialista.
A identificação dos objetivos do teste, os quais, nesta abordagem proposta, são deduzidos sobre a especificação representada no oráculo de teste e nos critérios de cobertura.
Cada objetivo de teste representa um caso de teste, porém podem existir objetivos redundantes, devendo ser aplicadas regras de verificação de redundância.
A geração de casos de teste de acordo com os objetivos identificados.
Técnicas de verificação de modelos, planejamento de Ia ou algoritmos de travessia de grafos são usadas para gerar os casos de teste, que são escritos numa ontologia de suíte de testes independente de linguagem de programação.
Os casos de testes executáveis são gerados com base nesta ontologia e em outra que representa o conhecimento de implementação dependente de uma linguagem de programação.
Segundo os autores, esta última ontologia contém informações sobre o código do programa a ser testado e pode ser gerada por engenharia reversa.
A abordagem proposta baseada em ontologias desacopla estes três aspectos relativos à geração de testes, e, com isso, aumenta o nível de abstração da representação do conhecimento.
Além disso, se especialistas em testes ampliarem o oráculo de teste com critérios de cobertura personalizados e conhecimentos sobre aspectos do sistema propensos a erros, a qualidade dos testes gerados tende a aumentar.
A arquitetura proposta em é abstrata, como ilustrada na Figura 11, e pode ser construída com tecnologias variadas.
Os autores implementaram essa abordagem para a geração de casos de teste unitário baseado em máquinas de estados e utilizaram as tecnologias JUnit, OWL, POSL e OO jDREW.
Os autores citam que um próximo passo seria expandir esta técnica para as fases de teste de integração e de sistema.
&quot;LoadRunner». Um cálculo de distância semântica é usado para a inferência destas similaridades.
O processo de reuso de casos de teste baseado em ontologias inclui:
Construção da ontologia de teste de software usando o Protégé com a ajuda de especialistas do domínio.
Coleta do conjunto de casos de teste e anotação de cada caso de teste de acordo com a ontologia estabelecida.
Os casos de testes e seus metadados originados por esta anotação são armazenados numa base de dados.
Pré-processamento da consulta do usuário (de acordo com a ontologia definida).
Inferência por o módulo reasoner, de acordo com a ontologia, e tradução da consulta em casos de testes a serem recuperados.
Devolução do resultado encontrado na interface de usuário.
Testing Process. Em é apresentado um sistema para gerenciamento do conhecimento de teste de software por meio de a recuperação de documentos e identificação de especialistas.
Para isto, ontologias são utilizadas para identificar os seguintes conceitos:
Documentos, referências, projetos, quadro de funcionários (staff) e nível de conhecimento.
O sistema proposto trabalha com uma base de dados de documentos, onde os usuários podem adicionar, recuperar e avaliar documentos usando diversos critérios.
Analistas de conhecimento podem alterar o nível de conhecimento dos usuários de acordo com uma análise de seus históricos.
A recuperação do conhecimento consiste na identificação de um documento que atenda as necessidades do usuário.
Caso não exista, o sistema pode indicar um especialista capaz de resolver o problema.
A ontologia representa o papel de base de conhecimento no sistema de recuperação de informação, classificando os conceitos, relacionamentos, atributos, instâncias e restrições nos conceitos sobre teste de software.
Assim, é possível identificar conceitos ou atributos correlacionados ao recuperar o conhecimento requisitado por o usuário.
Ontology--based Web Service Robustness Test Generation.
Este trabalho gera testes de robustez utilizando uma ontologia do domínio dos serviços web que serão testados.
A propriedade &quot;robustez «é definida por a IEEE como o grau num sistema ou componente pode funcionar corretamente mesmo na presença de entradas inválidas ou condições ambientais estressantes.
A ontologia especifica semanticamente os serviços e workflows, e com isso é possível derivar restrições sobre as classes, propriedades e dependências dos parâmetros.
De acordo com essas restrições, são aplicados operadores de mutação de dados nos casos de testes funcionais para gerar dados de teste de robustez.
Segundo os autores de, anteriormente a este estudo, somente era possível verificar violações sintáticas e de workflow nos serviços, mas com o uso da ontologia é possível violar restrições semânticas.
A Figura 12 ilustra as três abordagens adotadas na geração de teste de robustez, dando ênfase a parte tracejada em vermelho que corresponde à abordagem proposta em.
An Ontology--based Software Test Generation Framework.
Em é definido um framework para geração automática de suítes de teste baseado em ontologias.
Esta geração foi dividida em quatro etapas, como ilustrado na Figura 13 e explicado a seguir:
Fase 1) Geração dos objetivos do teste.
Para executar esta etapa é preciso definir uma representação ontológica do que deve ser testado, o que inclui a especificação do modelo comportamental, o conhecimento de especialistas e os critérios de cobertura.
A saída desta etapa é um conjunto de objetivos do teste.
Para isso, são executadas inferências nas ontologias que descrevem a especificação.
Fase 2) Verificação de redundância.
Para cada objetivo de teste, uma regra de verificação de redundância é usada para conferir se o objetivo do teste é satisfeito por um caso de teste já incluído na ontologia de testes abstratos.
Esta fase também utiliza inferência nas ontologias.
Esta etapa é executada em conjunto com a Fase 3.
Fase 3) Geração da ontologia de testes abstratos.
Para cada objetivo não redundante de teste, um caso de teste abstrato é gerado e adicionado na parcialmente gerada ontologia de testes abstratos.
Esta fase é executada usando métodos de geração de testes predominantes na literatura (como graph traversal algorithms).
Fase 4) Geração da suíte de teste executável.
Para cada caso de teste abstrato da ontologia de testes abstratos, um caso de teste executável é gerado usando a ontologia de conhecimento da implementação.
Ontology--Based Case Generation for Simulating Complex Production Automation Systems.
Este trabalho apresenta uma abordagem de extração do conhecimento de especialistas de teste.
O conhecimento é representado explicitamente numa ontologia, que é usada para gerar casos de testes para um simulador de linha de montagem automatizada.
Os autores argumentam que a abordagem apoiada por ontologias não exige conhecimento de programação e apresenta maior facilidade de alteração dos casos de teste, porém é inicialmente mais complexa de aplicar.
A ontologia deve modelar tanto o conhecimento sobre o domínio de teste, quanto o conhecimento sobre o sistema a ser testado.
Embora os usuários tenham que modificar a ontologia com ferramentas de edição gráfica (como o Protégé), esta tarefa envolve menos esforço do que modificar o código hardcoded de um script.
O processo proposto de geração de casos de teste é dividido em três fases.
A primeira fase carrega a ontologia que é usada para gerar dinamicamente uma interface gráfica de acordo com os parâmetros de teste.
A segunda fase corresponde à parametrização dos testes na interface gráfica gerada.
Em a terceira fase, os casos de teste são gerados e exportados para arquivos XML.
Ontology--based Test Generation for MultiAgent Systems.
A pesquisa feita em utiliza ontologias de interação entre agentes para definir o conteúdo semântico da comunicação entre os agentes.
A finalidade deste estudo é automatizar a geração de teste por sistemas multiagentes.
De acordo com a ontologia é possível verificar as mensagens trocadas e também gerar entradas válidas ou inválidas para testar o comportamento dos agentes.
Esta abordagem foi integrada num framework de teste denominado eCat, o qual gera e executa casos de teste automaticamente.
Para gerar os testes, é feito um alinhamento de uma ontologia que caracteriza o domínio dos agentes com a ontologia de interação.
Este trabalho avalia também a exploração do espaço de entradas e a possibilidade de usar ontologia como oráculo de teste.
Se as entradas do teste forem inválidas, o objetivo é avaliar a robustez do sistema (o que também foi feito usando ontologias em outro trabalho).
A abordagem adotada no eCat é a seguinte:
O Tester Agent seleciona o template de um caso de teste, invoca o O-based Generator para preencher os valores e executa o teste.
Enquanto isto, os Monitoring Agents observam o comportamento dos agentes e reportam se alguma falha ocorreu.
Além disso, o Tester Agent verifica se as mensagens recebidas estão de acordo com a ontologia ou não.
Este ciclo de geração e execução de casos de testes pode continuar e avançar para a próxima iteração até que seja interrompido.
Os autores definem que os casos de teste, que são codificados em XML, podem ser uma sequência de interações definidas por o usuário ou podem seguir um protocolo padrão de interações como o FIPA (The Foundation for Intelligent Physical Agents).
Usando estes conceitos básicos definidos anteriormente, em, os autores estabelecem conceitos compostos como capacidade e tarefa.
A capacidade de um tester é determinada por a atividade que ele pode executar, de acordo com um determinado contexto, método, ambiente e artefatos de entrada e saída.
Uma tarefa consiste de uma atividade de teste e informações relacionadas sobre como a atividade deve ser executada (contexto, método, ambiente e artefatos de entrada e saída).
Usando estes conceitos, os autores definem as seguintes relações parciais de ordem:
Subsunção (subsumption) entre métodos de teste, compatibilidade entre formatos de artefatos, melhoramento entre ambientes, inclusão entre atividades e sequenciamento temporal de atividades de teste.
Com estas relações, é possível inferir que um tester possui capacidade igual ou superior a outro, pois consegue executar todas as tarefas que o outro pode fazer.
De a mesma forma, pode ser deduzida a relação que uma tarefa contém outra (s) e a relação de combinar uma tarefa com o tester de maior capacidade para executar- la.
A referência, destes mesmos autores, utiliza esta ontologia como meio de comunicação de agentes que possuem o objetivo de testar aplicações web.
A multi-agent software environment for testing web-- based applications.
Este trabalho utiliza a ontologia de teste de software descrita anteriormente (em) para definir o conteúdo das mensagens trocadas entre agentes de software que possuem o objetivo de testar aplicações web.
Em este caso, a ontologia funciona como facilitadora, pois é usada para apoiar a comunicação entre os agentes.
Em esta abordagem, agentes intermediários usam a ontologia como forma de inferência para gerenciar o conhecimento sobre os agentes e atribuir cada tarefa para o agente mais apropriado.
Os conceitos principais da ontologia são:
Method, artifact, testing context, tester, activity e environment.
O uso desta ontologia permite uma integração flexível dos múltiplos agentes e a implementação de diversos métodos diferentes de teste.
Além disso, o agente mais adequado para executar novas tarefas num determinado contexto e momento no tempo é escolhido de forma dinâmica.
Ontology--Based Case Generation for Testing Web Services.
O trabalho descrito em combina ontologias que descrevem web services (OWL-S) e redes de Petri para gerar casos de teste automaticamente.
Para cada serviço, são analisados os parâmetros de entrada necessários, a saída gerada, pré-condições e pós-condições de sua execução.
Com base na rede de Petri, processos de teste são gerados para cobrir diversos caminhos possíveis de execução.
Os dados de teste são gerados por meio de inferências na ontologia.
Uma visão geral da abordagem baseada em ontologias para geração de casos de teste proposta por estes autores pode ser visualizada na Figura 14.
Esta seção analisou trabalhos relacionados que aplicaram ontologias relacionadas ao teste de desempenho de software.
A seção seguinte da dissertação apresentará a ontologia proposta neste trabalho de mestrado.
&quot;Computers are useless.
They can give you answers.».
A ontologia proposta sobre teste de desempenho foi construída com o apoio do software Protégé e está representada na linguagem OWL.
OWL é uma linguagem destinada a representar explicitamente o significado de termos em vocabulários e as relações entre estes termos.
Além de ser caracterizada por uma semântica formal, a linguagem OWL foi projetada com a finalidade de permitir uma forma comum de processar informações.
A metodologia de desenvolvimento de ontologia adotada foi a proposta por Noy e McGuinness.
Visto que a primeira etapa da metodologia seguida consiste em determinar o escopo da ontologia, este é o tema abordado na próxima seção.
A ontologia possui como domínio o teste de desempenho, portanto um dos seus objetivos consiste em formalizar os principais conceitos, relacionamentos e indivíduos deste domínio.
Além disso, a ontologia proposta tem como objetivo auxiliar a gerência dos testes de desempenho de software, possuindo como usuários finais os testadores de desempenho.
Uma forma de determinar o escopo da ontologia é esboçar uma lista de questões que devem ser respondidas por uma base de conhecimento que utilize- a.
Estas perguntas são chamadas questões de competência, sendo responsáveis por facilitar o delineamento do escopo da ontologia.
Portanto, a seguir estão listadas questões de competência que a ontologia proposta deve responder:
Quais características devem ser consideradas durante a elaboração de um teste de desempenho?
Em outras palavras, quais os principais conceitos e propriedades sobre o domínio de teste de desempenho que um testador deve ter conhecimento?
Quais tecnologias podem ser utilizadas numa determinada configuração de um sistema que se deseja testar o desempenho?
Quais atividades o teste de desempenho deve possuir para atingir o objetivo do teste?
Dado uma ferramenta de teste, quais métricas podem ser coletadas e quais protocolos podem ser testados?
Quais ferramentas podem ser utilizadas para gerar carga ou monitorar um determinado ambiente de teste?
Por exemplo, quais ferramentas para teste de desempenho podem ser utilizadas para monitorar o sistema operacional Linux?
Representando o conhecimento sobre o domínio teste de desempenho numa ontologia, as respostas para estas perguntas podem ser deduzidas com base nos conceitos, propriedades, instâncias e axiomas.
Para responder tais perguntas é necessário representar informações como as restrições e as capacidades envolvidas na aplicação de cada tecnologia, e.
g, uma capacidade fornecida por a ferramenta de teste de desempenho LoadRunner é conseguir testar aplicações sobre o protocolo Http.
De essa forma, uma vez definido o ambiente de teste, é possível escolher quais tecnologias podem ser utilizadas numa determinada instância do conceito teste de desempenho.
Além disso, de acordo com as escolhas na elaboração de um teste de desempenho, um plano de teste pode ser gerado por uma aplicação que utilize tal ontologia.
O plano de teste é um documento que fornece a base para a execução dos testes de software de modo organizado, sistemático e bem documentado.
Estas questões de competência foram mapeadas nos casos de uso ilustrados na Figura 15.
Em um nível alto de abstração o objetivo da aplicação baseada na ontologia é auxiliar os testadores a definir o que deve ser levado em consideração para a execução de um teste de desempenho.
Assim, apoiando as decisões dos testadores como, por exemplo, sugerindo uma metodologia para elaboração dos testes ou indicando ferramentas alinhadas aos objetivos de teste.
UC1 ­ Elaborar teste de desempenho.
Quais características um teste de desempenho pode possuir?
Como selecionar tecnologias e ferramentas compatíveis com o ambiente de teste?
O caso de uso &quot;Elaborar teste de desempenho «inclui a criação de uma instância do conceito que representa o teste de desempenho na ontologia.
Esta nova instância deve obedecer as restrições deste conceito, podendo possuir relacionamentos com outros conceitos de acordo com as propriedades existentes na ontologia.
Por exemplo, um teste de desempenho pode se relacionar com conceitos como os objetivos, as atividades, as ferramentas e as métricas de desempenho.
Desta forma, a ontologia deverá representar os objetivos que um teste de desempenho pode possuir, as atividades executadas nos testes de desempenho, as ferramentas que podem resolver tais tarefas e as métricas de desempenho que podem ser monitoradas.
Como ilustrado na Figura 15, o caso de uso &quot;Elaborar teste de desempenho «pode ser extendido por os casos de uso &quot;Selecionar objetivos, atividades, ferramentas e métricas para o teste «e &quot;Gerenciar instâncias dos conceitos do domínio».
UC2 ­ Selecionar objetivos, atividades, ferramentas e métricas para o teste.
Este caso de uso permite que o testador selecione instâncias da ontologia que vão possuir relacionamentos com um determinado indivíduo do conceito teste de desempenho.
Em outras palavras, quais objetivos, atividades, ferramentas e métricas estarão relacionadas com um teste de desempenho.
É preciso definir também qual o ambiente de teste, a aplicação a ser testada e quais protocolos a aplicação utiliza.
Então, é possível deduzir e selecionar, por exemplo, de acordo com os axiomas da ontologia, quais ferramentas podem gerar carga nos protocolos da aplicação, quais atividades devem ser executadas para atingir o objetivo do teste e quais métricas podem ser monitoradas por as ferramentas escolhidas.
De acordo com existem aproximadamente 300 ferramentas de teste atualmente, o que dificulta a identificação da ferramenta mais adequada para um determinado teste.
Portanto, se um testador possuir conhecimento sobre as tecnologias utilizadas em testes de software, este testador pode recomendar as melhores opções para o ambiente a ser testado.
Pensando nisto, um dos objetivos da ontologia proposta é modelar este conhecimento para que até mesmo pessoas sem experiência em teste de software possam selecionar opções coerentes com seus objetivos específicos de teste.
UC3 ­ Gerenciar instâncias dos conceitos do domínio.
O caso de uso &quot;Gerenciar instâncias dos conceitos do domínio «deve permitir a criação, recuperação e atualização de instâncias dos conceitos e propriedades representados na ontologia.
Como comentado anteriormente, é possível imaginar que a ontologia possuirá conceitos para representar os testes de desempenho, as ferramentas, as atividades, o ambiente de teste, as métricas de desempenho, entre outros.
Portanto, uma aplicação baseada nesta ontologia proposta deve permitir criar, pesquisar e editar instâncias destes conceitos.
UC4 ­ Criar plano de teste de desempenho.
O caso de uso &quot;Criar plano de teste de desempenho «se refere à criação de um documento contendo os axiomas relacionados a uma instância do conceito que representa o teste de desempenho.
Portanto, para criar um plano de teste, é preciso possuir uma instância do teste, como indica a relação no diagrama de casos de uso da Figura 15.
Segundo, o plano de teste pode ser utilizado para aumentar o gerenciamento do teste e a visibilidade do processo.
O plano de teste define o escopo, a abordagem, os recursos, as características e itens a serem testados, as atividades de teste a serem executadas, o cronograma com os responsáveis por as atividades e os riscos associados.
Segundo, o plano de teste pode incluir a aplicação ou sistema sob teste, os objetivos do teste (incluindo requisitos e riscos), o escopo e as limitações do teste, o ambiente de teste, a estratégia de teste, os detalhes para cada fase de desenvolvimento e o cronograma.
De acordo com, um esboço inicial do plano de teste deve ser criado ao final da atividade de análise da fase de desenvolvimento e atualizado iterativamente durante as fases de desenvolvimento subsequentes.
Portanto, o plano de teste é desenvolvido de forma iterativa, sendo atualizado conforme novas informações sobre a aplicação e os objetivos de teste se tornam disponíveis.
Tendo uma ideia sobre o escopo da ontologia e o que uma aplicação baseada em ela deve fazer, a etapa seguinte consiste em conceitualizar o domínio, ou seja, definir as classes, propriedades, axiomas e instâncias da ontologia.
Após definir o escopo da ontologia, a metodologia utilizada sugere o reuso de ontologias existentes.
Assim, ontologias de domínios relacionados, como a OntoTest para o desenvolvimento deste trabalho.
A ontologia proposta foi construída também com base em conceitos extraídos, sobretudo, do Corpo de Conhecimento em Engenharia de Software (SWEBOK), do glossário da Engenharia de Software (IEEE Std.
610.12) e do padrão para documentação do teste de software.
Estas referências foram escolhidas para minimizar o viés, proporcionar uma base teórica mais sólida e garantir a coerência entre os conceitos e as propriedades da ontologia.
Além disso, os trabalhos que utilizam ontologias no teste de software, analisados na Seção 3 desta dissertação, também foram considerados no desenvolvimento e aplicação da ontologia proposta.
A ontologia foi representada na língua inglesa para aumentar sua abrangência e seguir o padrão adotado na OntoTest e na SwTO.
De acordo com a metodologia adotada, os principais termos da ontologia podem ser definidos como os termos sobre os quais se deseja fazer declarações ou explicar para um usuário, que no caso é um testador de desempenho.
Em outra palavras, a metodologia sugere pensar em &quot;Quais termos gostaríamos de falar sobre? «e &quot;O que gostaríamos de dizer sobre estes termos?».
Como a ontologia deve especificar um teste de desempenho, espera- se que a ontologia defina, por exemplo, por que fazer um teste, como fazer- lo, o que deve ser feito e quais resultados podem ser obtidos.
Assim, o teste de desempenho, as atividades dos testes, as ferramentas de teste, os objetivos dos testes e as métricas de desempenho compõem os principais termos, estando de acordo com as questões de competência e os casos de uso que representam o escopo da ontologia.
Portanto, os conceitos principais da ontologia foram definidos, como ilustrado na Figura 16 e detalhados a seguir:
PTActivity ­ O conceito PTActivity representa uma atividade de teste, que é parte do processo de execução de um teste de desempenho.
PTArtifact ­ Este conceito representa um artefato ou entregável de teste, ou seja, um produto de trabalho gerado a partir de a execução de uma atividade de teste (PTActivity).
SystemUnderTestConcept ­ O conceito SystemUnderTestConcept possui subclasses que especificam o alvo de um teste de desempenho, ou seja, o sistema sob teste.
PerformanceTest ­ Como esperado de uma ontologia sobre o domínio de teste de desempenho, existe um conceito responsável por representar os testes de desempenho e este conceito foi denominado PerformanceTest.
PerformanceTester ­ O conceito PerformanceTester representa um testador, que é o responsável por a execução das atividades do teste de desempenho (PTActivity).
PerformanceMetric ­ O conceito PerformanceMetric é utilizado para representar as métricas de desempenho, que são monitoradas por as ferramentas de teste (PTTool).
ValuePartition ­ O conceito ValuePartition representa conceitos que não pertencem ao domínio da ontologia, mas são utilizados por conceitos pertencentes ao domínio.
Esta abordagem faz parte de um padrão de desenvolvimento de ontologias adotado, que separa os conceitos em DomainConcept ou ValuePartition.
PTGoal ­ O conceito PTGoal representa uma razão ou objetivo que justifica a execução de um teste de desempenho (PerformanceTest).
PTTool ­ Uma ferramenta de teste (PTTool) oferece recursos para apoiar a execução de atividades (PTActivity), porém cada ferramenta possui restrições de funcionamento.
É importante observar na Figura 16 que o conceito PerformanceTest recebeu uma cor diferente em comparação aos outros conceitos.
Isto se deve ao fato do conceito teste de desempenho ter sido representado como uma classe definida, enquanto que os outros conceitos da ontologia são classes primitivas.
A linguagem OWL permite que os conceitos da ontologia sejam representados como classes definidas ou primitivas.
Classes primitivas possuem restrições que todos os indivíduos pertencentes à classe devem satisfazer, mas não significa que um indivíduo aleatório que satisfaça estas condições necessariamente pertencerá àquela classe.
Já uma classe definida significa tanto que os indivíduos da classe satisfazem suas restrições como que qualquer indivíduo aleatório que satisfaça tais restrições pode ser classificado como pertencente àquela classe.
Estas características definem a semântica da linguaguem OWL e são levadas em consideração, por exemplo, quando um reasoner é aplicado sobre a ontologia.
Um reasoner é um software capaz de inferir consequências lógicas com base em um conjunto de axiomas.
Os axiomas do conceito PerformanceTest estão ilustrado na Figura 17 como &quot;Necessary &amp; Sufficient Asserted Conditions».
Por exemplo, um destes axiomas define que uma instância de PerformanceTest deve possuir pelo menos um relacionamento com um indivíduo do tipo PTActivity por meio de a propriedade hasActivity.
Ainda, um teste de desempenho deve possuir relacionamentos com pelo menos uma instância dos conceitos PTGoal, ApplicationUnderTest, PerformanceMetric e PTTool por meio de as propriedades correspondentes.
Alguns conceitos da ontologia utilizam o acrônimo PT como abreviação de Performance Test (Teste de Desempenho).
Classes definidas, como é o caso de PerformanceTest, apresentam suas restrições como &quot;Necessary &amp; Sufficient».
Em contrapartida, as restrições das classes primitivas são representadas no item &quot;Necessary», como é o caso da Figura 18, que apresenta os objetivos dos teste de desempenho e as restrições sobre o conceito NormalWorkloadPerformanceEvaluation.
Se o objetivo do teste é avaliar o desempenho do sistema com uma carga normal (semelhante a carga real que o sistema receberá dos usuários), então deve ser executada pelo menos uma atividade de análise das medições de desempenho (PerformanceMeasurementAnalysis).
Esta restrição foi representada por a propriedade imposesActivity, cujo domínio é o conceito PTGoal e a imagem é PTActivity.
Cada objetivo pode possuir diferentes restrições e, como ilustrado na Figura 18, a ontologia possui 7 conceitos que representam os objetivos dos testes de desempenho:
DifferentConfigurationsComparison ­ Este conceito representa o objetivo de executar o mesmo caso de teste em diferentes configurações para comparar determinada métrica de desempenho (também pode ser chamado de benchmarking).
LongRunPerformanceEvaluation ­ Este objetivo simboliza a execução de um caso de teste durante intervalos maiores de tempo para avaliar se existe a degradação das métricas de desempenho ao longo de o tempo (também chamado de endurance test).
NormalWorkloadPerformanceEvaluation ­ Representa a execução de um teste utilizando uma carga normal, i.
e, semelhante a carga que o sistema será submetido por usuários reais quando estiver em produção (também chamado de load test).
PTArtifactElaboration ­ Representa um teste de desempenho que possui como objetivo a elaboração de um entregável, que pode ser um caso de teste, um relatório, uma especificação, a medição de uma métrica de desempenho, entre outros.
PerformanceBottleneckIdentification ­ Este objetivo simboliza a execução de um teste visando encontrar os gargalos do sistema, sendo necessário utilizar cargas altas para identificar qual recurso atinge seu limite primeiro (também chamado de stress test).
PerformanceImprovement ­ Este conceito define o objetivo de executar um teste de desempenho com a finalidade de melhorar o desempenho do sistema, o que geralmente se torna possível após a identicação dos gargalos do sistema.
PerformanceRequirementsVerification ­ A verificação dos requisitos de desempenho é também um objetivo que os testes podem possuir, sendo estes requisitos usualmente definidos com base nas métricas de desempenho.
Os objetivos foram modelados como conceitos na ontologia para permitir a especificação de restrições em OWL, como é o caso, por exemplo, da restrição explicada anteriormente que utiliza a propriedade imposesActivity.
Porém, cada um destes conceitos só precisa possuir uma única instância, dado que esta única instância denomina o objetivo do teste e pode ser referenciada por diversas instâncias de PerformanceTest.
A Figura 19 apresenta a hierarquia das atividades dos testes de desempenho, que foi subdividida em 4 categorias:
Atividades de desenvolvimento (PTDevelopmentActivity), de documentação (PTDocumentationActivity), de execução (PTExecutionActivity) ou de planejamento (PTPlanningActivity).
As atividades de desenvolvimento (PTDevelopmentActivity) focam na geração ou modificação de um caso de teste.
São exemplos os conceitos PTCaseDevelopment e PTCaseParameterization, responsáveis por representar, respectivamente, as atividades de criação e parametrização de um caso de teste de desempenho.
A Figura 20 apresenta as restrições do conceito PTCaseDevelopment, que permitem afirmar que qualquer instância do tipo PTCaseDevelopment necessariamente:
UseCasesToTestDefinition e PerformanceToolsDefinition;
E gera como resultado de sua execução uma instância do conceito PTCase.
É importante observar que cada conceito que especializa PTActivity pode ser definido com base em diferentes restrições.
Estas restrições de classes utilizam as propriedades da ontologia que possuem como domínio o conceito PTActivity.
Um exemplo é a propriedade transitiva dependsOn, que possui PTActivity como domínio e como imagem.
Devido a a característica transitividade, se a atividade A depende da atividade B e a atividade B depende da atividade C, então se pode deduzir que A depende de C. O conceito PTActivity é domínio também da propriedade generatesArtifact, responsável por representar que uma atividade pode gerar uma instância do tipo entregável (PTArtifact).
O conceito atividade de documentação (PTDocumentationActivity) representa as atividades que possuem como característica a finalidade de documentar os artefatos relacionados ao teste.
A Figura 21 apresenta as restrições de dois conceitos que especializam PTDocumentationActivity, permitindo comparar suas diferentes restrições.
As atividades de execução possuem como característica o foco na execução de um caso de teste.
São exemplos de PTExecutionActivity, como ilustrado na Figura 22, as atividades PTCaseExecution e PerformanceMetricMonitoring.
As atividades de planejamento são responsáveis por a identificação e definição das características a nível de planejamento do teste de desempenho.
A Figura 23 apresenta as restrições de dois conceitos do tipo PTPlanningActivity:
PerformanceToolsDefinition e WorkloadDefinition.
Estas 4 categorias de atividades de teste não são mutuamente excludentes, como é o exemplo da atividade de elaboração do plano de teste (PTPlanElaboration) que foi classificada simultaneamente como atividade de documentação e de planejamento.
É importante observar também que existem 2 subconceitos de PTActivity (denominados PerformanceRequirementsDefinition e PerformanceMeasurementAnalysis) que não estão classificados em nenhuma destas 4 categorias apresentadas anteriormente.
Um outro conceito principal da ontologia proposta foi denominado PTArtifact e é responsável por representar os artefatos, entregáveis ou produtos de trabalho dos testes de desempenho.
Como ilustrado na Figura 24, de entre os artefatos gerados no teste de software encontra- se as medições de desempenho, os casos de testes e a documentação.
A ontologia proposta pode auxiliar o testador a definir qual conjunto de documentos de teste serão usados e o que cada um destes documentos deve conter.
Os conceitos da ontologia fornecem uma descrição padrão dos documentos de teste que pode servir de referência e facilitar a comunicação sobre o significado dos artefatos de teste, como, por exemplo, o plano de teste.
A taxonomia dos artefatos de teste desempenho na ontologia seguiu o padrão 829 definido por a IEEE, que divide a documentação de teste em planejamento, especificação ou relato dos resultados.
A explicação para os conceitos da hierarquia de PTArtifact é a seguinte:
PTPlan. O plano de teste determina o escopo, a abordagem, os recursos, os itens a serem testados, as atividades de testes a serem realizados, os responsáveis por cada tarefa e os riscos associados.
PTIncidentReport. O relatório de incidentes é utilizado para descrever eventos que ocorreram durante a execução do teste e que necessitam de investigação adicional.
PTItemTransmittalReport. Este relatório identifica itens de teste sendo transmitidos para o teste no evento que marca o início da fase de execução dos testes, separando do desenvolvimento do teste.
PTLog. Um log do teste é utilizado para registrar acontecimentos que ocorreram durante a execução do teste.
PTSummaryReport. Este relatório contém uma avaliação dos itens de teste correspondentes e um resumo das atividades de teste associadas a uma ou mais especificações de projeto de teste.
PTCaseSpecification. A especificação de um caso de teste documenta os valores de entrada, os valores esperados de saída e as restrições sobre a execução de um item de teste.
Segundo, a documentação de um caso de teste pode incluir:
A fase do desenvolvimento em que o caso de teste será executado, os objetivos específicos de teste, os dados de teste sugeridos, as ferramentas de teste, os procedimentos de inicialização, finalização e reinicialização do teste, as etapas de execução do teste (cada etapa possui uma descrição das ações, resultados esperados e resultados obtidos) e a lista de defeitos do software encontrados neste caso de teste.
PTDesignSpecification. A especificação do projeto de teste é responsável por refinar a abordagem de teste, especificar os critérios de aprovação/ falha, identificar as características a serem cobertas por o projeto e destacar os casos e os procedimentos necessários para a realização do teste.
PTProcedureSpecification. A especificação do procedimento de teste identifica os passos necessários para operar o sistema e exercitar os casos de teste definidos, os quais implementam o projeto de teste associado.
As métricas de desempenho também estão entre os principais conceitos modelados na ontologia, agrupadas conceitualmente em duas categorias:
MachinePerformanceMetric (métricas referentes aos recursos das máquinas) e PTMetric (relacionadas a execução do teste).
As métricas de máquina são divididas em conceitos mais especializados, como DiskMetric, MemoryMetric, NetworkMetric, ProcessMetric e ProcessorMetric, utilizados para representar, respectivamente, as métricas de disco, memória, rede, processo e processador.
Como ilustrado na Figura 25, estes conceitos já possuem instâncias, como por exemplo, a quantidade de memória disponível e o número de páginas acessadas por segundo são instâncias do conceito MemoryMetric, que possui ao total 11 instâncias.
Ainda em relação a as métricas, a ontologia possui a propriedade canMonitor para representar que uma instância de ferramenta de teste pode monitorar determinados indicadores de desempenho, ou seja, a propriedade canMonitor possui o conceito PTTool como domínio e PerformanceMetric como imagem.
A conceitualização das métricas de desempenho foi baseada em, onde foi definida uma taxonomia para especificar parâmetros relacionados à Qualidade de Serviço.
Em, as métricas referentes a Qualidade de Serviço foram classificadas de acordo com três perspectivas:
Aplicação, recurso ou sistema.
O tempo de resposta de uma transação ou o número de erros são exemplos de métricas do ponto de vista da aplicação e a utilização de processadores, memória e discos são métricas a nível de recursos.
Tabela 1: Principais instâncias de conceitos que especializam o conceito PerformanceMetric.
Conceito Instâncias PTMetric RequestsPerSecond, ResponseSizeBytes, ResponseTimeSeconds, ThroughputMegabytes, TransactionsPerSecond SystemMetric ContextSwitchesPerSecond, FileReadBytesPerSecond, FileWriteBytesPerSecond, ProcessorQueueLenght, FileWriteOperationsPerSecond, SystemCallsPerSecond, FileReadOperationsPerSecond ProcessorMetric InterruptsPerSecond, PercentageOfInterruptTime, PercentageOfPrivilegedTime, PercentageOfUserTime, PercentageOfProcessorTime MemoryMetric AvailableMegabytes, PageWritesPerSecond, CommittedBytes, CacheFaultsPerSecond, FreeSystemPageTableEntries, PageFaultsPerSecond, PagesPerSecond, PoolPagedBytes, PoolNonPagedBytes, PageReadsPerSecond, CacheBytes ProcessMetric ProcessPageFaultsPerSecond, ProcessPrivateBytes, ProcessPercentageOfProcessorTime, ProcessThreadCount, ProcessPoolNonPagedBytes, ProcessPoolPagedBytes, ProcessPercentageOfUserTime PhysicalDiskMetric PDAverageBytesPerRead, PDAverageBytesPerWrite, PDAverageQueueLenght, PDAverageReadQueueLenght, PDAverageSecondsPerRead, PDAverageSecondsPerWrite, PDAverageWriteQueueLenght, PDBytesReadPerSecond, PDBytesWritePerSecond, PDCurrentQueueLenght, PDPercentageOfDiskReadTime, PDPercentageOfDiskTime, PDPercentageOfDiskWriteTime, PDPercentageOfFreeSpace, PDReadsPerSecond, PDWritesPerSecond NetworkInterfaceMetric BytesReceivedPerSecond, BytesSentPerSecond, CurrentBandwidth, BytesTotalPerSecond, OutputQueueLenght, PacketsSentPerSecond, PacketsReceivedPerSecond, PacketsTotalPerSecond ICMP_ Metric EchoReplyReceivedPerSecond, EchoReplySentPerSecond, EchoRequestReceivedPerSecond, MessagesPerSecond, MessagesSentPerSecond, RedirectSentPerSecond, RedirectReceivedPerSecond, EchoRequestSentPerSecond IPv4_ Metric FragmentedDatagramsPerSecond, FragmentsCreatedPerSecond, FragmentsReassembledPerSecond, FragmentsReceivedPerSecond, IPDatagramsForwardedPerSecond, IPDatagramsPerSecond, IPDatagramsReceivedPerSecond, IPDatagramsSentPerSecond TCP_ Metric ConnectionFailure, ConnectionsActive, ConnectionsEstablished, ConnectionsPassive, ConnectionsReset, SegmentsPerSecond, SegmentsReceivedPerSecond, SegmentsRetransmittedPerSecond, SegmentsSentPerSecond As ferramentas de teste (PTTool) estão também entre os principais conceitos do domínio de teste de desempenho.&amp;&amp;&amp;
A Figura 26 ilustra as 18 instâncias e as 9 subclasses do conceito PTTool.
As instâncias podem ser visualizadas por os losangos de cor roxa na direita da Figura 26 e as subclasses estão detalhadas na Figura 27.
Além disso, o conceito PTTool é domínio das propridades canCollectMetric com imagem em PerformanceMetric, canMonitor e worksOn com imagem no conceito OperatingSystem, generatesLoadOn com imagem em CommunicationProtocol e supports cuja imagem é o conceito PTActivity.
As restrições dos conceitos que especializam PTTool estão ilustradas na Figura 27.
Por exemplo, se uma ferramenta de teste (instância de PTTool) possuir pelo menos um relacionamento com uma instância de CommunicationProtocol por meio de a propriedade generateLoadOn, então é possível classificar esta instância como uma ferramenta capaz de gerar carga (LoadGenerationTool).
De a mesma forma, o conceito HTTPLoadTestTool representa as ferramentas que podem gerar carga no protocol Http;
O conceito MonitoringTool agrupa ferramentas que podem monitorar pelo menos uma instância de MachinePerformanceMetric;
O conceito LinuxMonitoringTool classifica as ferramentas que podem monitorar pelo menos uma instância do sistema operacional Linux;
Entre outros conceitos ilustrados na Figura 27.
Os conceitos definidos apresentados na Figura 27 não possuem instâncias, contudo um reasoner pode ser aplicado para inferir se alguma instância da ontologia pode ser classificada nestes conceitos.
Foi aplicado o reasoner Pellet sobre os conceitos e instâncias da ontologia e o resultado da inferência pode ser visualizado na seção 5.4.
Por exemplo, das 18 instâncias de PTTool, 15 foram classificadas como LoadGenerationTool, 14 instâncias são do tipo HTTPLoadTestTool, 10 foram classificadas como MonitoringTool e assim por diante.
É esperado de um reasoner OWL as funcionalidades de verificação de consistências, satisfatibilidade dos conceitos, classificação e realização.
Como ilustrado na Figura 28, o conceito SystemUnderTestConcept, utilizado para especificar as características do sistema sob teste, possui as seguintes subclasses:
ApplicationUnderTest, ApplicationUseCase, CommunicationProtocol e OperatingSystem.
O conceito ApplicationUnderTest representa a aplicação sob teste e ApplicationUseCase simboliza os casos de uso da aplicação a serem testados.
É importante observar que estes dois conceitos ainda não possuem instâncias, uma vez que devem ser instanciados durante a elaboração de um teste de desempenho.
Já o conceito CommunicationProtocol possui 20 instâncias que se relacionam com instâncias dos conceitos PTTool e ApplicationUseCase.
Em outras palavras, os protocolos são utilizados nas aplicações e as ferramentas de teste conseguem gerar carga em determinados protocolos.
O conceito CommunicationProtocol apresenta instâncias como, por exemplo, ARP, DNS, FTP, Http, ICMPv4, ICMPv6, IMAP, IPv4, JMS, NFS, RPC, SMPT, SNMP, SOAP, SSH, TCP, UDP.
O conceito OperatingSystem possui duas instâncias (Mac e Solaris) e duas subclasses:
Uma denominada LinuxBasedOperatingSystem (possui as instâncias Debian, Fedora, Gentoo, Mandriva, OpenSuse, RedHat, Ubuntu);
E outra subclasse denominada WindowsBasedOperatingSystem.
Ainda, foi utilizado um padrão de desenvolvimento de ontologias denominado Value Partition 1.
De acordo com este padrão, o conceito ValuePartition possui subclasses que são utilizadas para especificar características dos conceitos que pertencem ao domínio da ontologia.
O conceito PerformanceMetricDatatypeValue é uma destas subclasses, sendo utilizado para especificar o tipo de dado de uma instância de PerformanceMetric e possuindo as instâncias IntegerNumber, conceito SoftwareLicenseValue é outro exemplo de conceito do tipo ValuePartition, que é utilizado para especificar a licença de uso e comercialização de uma instância de PTTool, permitindo os valores OpenSourceLicense e ProprietaryLicense.
Embora muitas propriedades da ontologia já tenham sido mencionadas ao longo de as explicações anteriores acerca de os conceitos, uma lista completa das propriedades existentes pode ser visualizada na Figura 29.
Além disso, uma explicação detalhada destas propriedades está disponível a seguir:
Propriedade: ContainsUseCase (inversa de belongsTo) Domínio:
ApplicationUnderTest Imagem: ApplicationUseCase Significado:
Esta propriedade representa a informação de que uma aplicação sobre teste pode possuir múltiplos casos de uso a serem testados.
Propriedade: CanCollectMetric (inversa de canBeCollectedByTool) Domínio:
PTTool Imagem:
PerformanceMetric Significado: Contém a informação de que uma instância de ferramenta de teste pode coletar métricas de múltiplas instâncias do conceito PerformanceMetric.
Propriedade: CanMonitor (inversa de canBeMonitoredBy) Domínio:
PTTool Imagem:
OperatingSystem Significado: A propriedade canMonitor armazena a informação de quais sistemas operacionais é possível monitorar com cada uma das ferramentas de teste.
Propriedade: GeneratesLoadOn (inversa de canBeTestedBy) Domínio:
PTTool Imagem:
CommunicationProtocol Significado: Representa o conhecimento de quais protocolos de comunicação é possível gerar carga utilizando cada uma das ferramentas de teste de desempenho.
Propriedade: CanExecuteTool (inversa de worksOn) Domínio:
OperatingSystem Imagem: PTTool Significado:
Esta propriedade é utilizada para selecionar em quais sistemas operacionais é possível executar uma ferramenta de teste específica.
Propriedade: MonitorsMetric (inversa de correspondsTo) Domínio:
ApplicationUseCase Imagem: PTMetric Significado:
Representa quais métricas a nível de teste devem ser monitoradas para cada caso de uso da aplicação a ser testada.
Propriedade: DependsOn Domínio:
PTActivity Imagem:
PTActivity Significado: Simboliza a relação transitiva de dependência entre as atividades executadas nos testes de desempenho.
Propriedade: ExecutesActivity (inversa de isExecutedBy) Domínio:
PerformanceTester Imagem: PTActivity Significado:
Contém a informação dos responsáveis por a execução de cada atividade.
Propriedade: GeneratesArtifact (inversa de requiresActivity) Domínio:
PTActivity Imagem:
PTArtifact Significado: A propriedade generatesArtifact é utilizada para especificar quais entregáveis cada atividade de teste pode gerar.
Propriedade: HasActivity (inversa de isPartOfTest) Domínio:
PerformanceTest Imagem: PTActivity Significado:
Contém a informação das atividades de teste a serem executadas numa instância específica de teste de desempenho.
Propriedade: HasDatatype (inversa de isDatatypeOf) Domínio:
PerformanceMetric Imagem:\&gt; Esta propriedade mapeia instâncias de métricas de desempenho com um tipo de dado representado por instâncias do conceito PerformanceMetricDatatypeValue.
Propriedade: HasGoal (inversa de isGoalOf) Domínio:
PerformanceTest Imagem: PTGoal Significado:
Contém a informação dos objetivos que um teste de desempenho possui.
Propriedade: HasLicense (inversa de isLicenseOf) Domínio:
PTTool Imagem:
SoftwareLicenseValue Significado: Mapeia as ferramentas de teste numa instância de SoftwareLicenseValue.
Propriedade: HostsApplication (inversa de isHostedOn) Domínio:
OperatingSystem Imagem: ApplicationUnderTest Significado:
Define o sistema operacional onde a aplicação sob teste está hospedada.
Propriedade: ImposesActivity (inversa de isImposedByGoal) Domínio:
PTGoal Imagem:
PTActivity Significado: Especifica quais atividades devem ser executadas para atingir um objetivo.
Propriedade: Measures (inversa de isMeasurementOf) Domínio:
PerformanceMeasurement Imagem: PerformanceMetric Significado:
Mapeia as medições das métricas de desempenho.
Propriedade: Monitors (inversa de isMonitoredBy) Domínio:
PerformanceTest Imagem: PerformanceMetric Significado:
Representa a informação das métricas de desempenho que um determinado teste é responsável por monitorar.
Propriedade: Supports (inversa de isSupportedBy) Domínio:
PTTool Imagem:
PTActivity Significado: Indica em quais atividades cada ferramenta de teste pode ser aplicada.
Propriedade: TestsApplication (inversa de isTestedBy) Domínio:
PerformanceTest Imagem: ApplicationUnderTest Significado:
Contém a informação de qual aplicação está sendo testada por uma instância de teste de desempenho.
Propriedade: TestsUseCase (inversa de isTestedByTestCase) Domínio:
PTCase Imagem:
ApplicationUseCase Significado: Representa qual caso de uso da aplicação é testado por cada caso de teste.
Propriedade: UsesProtocol (inversa de isUsedByApplicationUseCase) Domínio:
ApplicationUseCase Imagem: CommunicationProtocol Significado:
Mapeia quais protocolos de comunicação são utilizados em cada caso de uso da aplicação.
Propriedade: UsesTool (inversa de isUsedByTest) Domínio:
PerformanceTest Imagem: PTTool Significado:
Indica as ferramentas de teste escolhidas para um teste de desempenho.
A Figura 30 conclui esta seção e utiliza um diagrama de classes UML para resumir os principais termos representados na ontologia.
Cada classe do diagrama simboliza um conceito e cada associação simboliza uma propriedade.
Além disso, a origem da associação é o domínio da propriedade em OWL, enquanto que o destino é a imagem.
&quot;Not everything be counted counts and not everything that counts can be counted. «(
Albert Einstein) Avaliar a qualidade de uma ontologia permite a identificação de partes que devem ser melhor especificadas e possibilita a comparação entre duas ou mais ontologias.
A qualidade de uma ontologia pode ser avaliada em diferentes dimensões, e.
g, a nível estrutural ou a nível semântico.
Estas dimensões estão listadas a seguir:
Léxico, vocabulário ou camada de dados:
Foca nos conceitos, instâncias e fatos.
Hierarquia ou taxonomia:
ênfase na avaliação dos relacionamentos de subsunção.
Outras relações semânticas:
Avalia outros relacionamentos da ontologia, o que pode incluir o cálculo de métricas como precisão e revocação (precision and recall).
Nível de contexto:
Avalia como o uso de uma ontologia por uma aplicação afeta seus resultados do ponto de vista dos usuários da aplicação.
Nível sintático:
Avalia se a ontologia (representada numa linguagem formal) está de acordo com os requisitos sintáticos da linguagem.
Estrutura, arquitetura, projeto:
Avalia questões estruturais como a organização da ontologia e sua adequação para desenvolvimentos posteriores.
Para avaliar uma ontologia nos níveis listados anteriormente, existem abordagens que podem ser classificadas de acordo com uma das seguintes categorias:
Comparar a ontologia com um padrão de ouro (gold standard);
Usar a ontologia numa aplicação e analisar os resultados;
Comparar a ontologia com uma fonte de dados do domínio;
Avaliar, por meio de especialistas do domínio, se a ontologia atende determinados critérios, padrões ou requisitos.
A importância de uma ontologia pode ser atribuída a razões como adequação a uma finalidade, capacidade de responder questões de competência, presença de propriedades formais, entre outros fatores.
Para uma ontologia ser uma boa especificação de um domínio, tanto a conceitualização do domínio quanto sua formalização e especificação precisam ser consideradas boas.
Estes dois requisitos estão ilustrados como e na Figura 31.
A conceitualização (equivalente ao modelo mental do domínio) é resultado de processos cognitivos como percepção e reconhecimento.
Se a conceitualização for especificada numa ontologia formal, então será possível avaliar sua qualidade.
De acordo com, nenhuma abordagem completa para a avaliação de ontologias foi proposta até o momento.
Segundo, a avaliação de ontologias permanece um importante problema em aberto.
Apesar deste reconhecimento dos autores de que não existem técnicas perfeitas para a avaliação de ontologias, este trabalho aplicou duas das técnicas de avaliação de ontologias existentes, como será explicado nas subseções seguintes.
As abordagens adotadas para avaliar a ontologia proposta foram:
A comparação com duas ontologias de domínios semelhantes (OntoTest e a SwTO) e a avaliação da ontologia por especialistas no domínio de teste de desempenho.
A comparação da qualidade de ontologias é especialmente vantajosa quando as ontologias comparadas cobrem o mesmo domínio.
Entretanto, como não foram encontradas ontologias sobre o domínio de teste de desempenho ou que possuam questões de competência semelhantes, a ontologia proposta foi comparada com duas ontologias relacionadas:
A OntoTest e a SwTO.
Contudo, é importante destacar que o domínio da ontologia proposta é o teste de desempenho, enquanto que as outras duas ontologias foram desenvolvidas para o domínio de teste de software.
A OntoTest foi construída para apoiar a aquisição, compartilhamento, reuso e organização do conhecimento em teste de software.
A OntoTest explora os aspectos envolvidos na atividade de teste de software, define um vocabulário comum e permite estabelecer arquiteturas de referência, responsáveis por facilitar o uso e a integração de ferramentas, processos e artefatos na engenharia de software.
A OntoTest foi usada para definir a RefTEST, uma arquitetura de referência sobre teste de software.
Estabelecer estas arquiteturas exige conhecimento profundo do domínio, mas uma vez estabelecidas, estes conhecimentos permanecem associados em suas atividades e relacionamentos.
A segunda ontologia utilizada para comparação, denominada SwTO (Software Test Ontology), é uma ontologia sobre o domínio de teste de software projetada para auxiliar o teste no sistema operacional Linux.
A SwTO foi utilizada para gerar sequências de teste para o Linux e, segundo os autores, três vantagens foram identificadas:
A definição de um vocabulário formal sobre o domínio de teste no Linux que representa o consenso do grupo;
O armazenamento semântico do critério de elaboração dos testes;
E a representação semântica do conhecimento dos projetistas de teste.
Mais informações sobre a OntoTest e a SwTO podem ser encontradas nas referências citadas neste trabalho e na seção 3 denominada Trabalhos Relacionados.
Uma vez que as três ontologias estão corretas a nível sintático (de acordo com a como cada uma das ontologias aborda os principais conceitos da área.
Ontologias podem ser avaliadas e comparadas quantitativamente por meio de técnicas baseadas em métricas.
Este tipo de técnica descreve determinados aspectos da ontologia, ao invés de classificar- la como eficaz ou ineficaz.
De o ponto de vista do desenvolvedor de ontologias, estas métricas podem indicar áreas que necessitam de mais trabalho.
Por exemplo, a Tabela 2 ilustra diferenças quanto a o número de subclasses, instâncias e propriedades relacionadas aos principais conceitos destas ontologias.
Embora a Tabela 2 apresente uma forma de comparação entre diferentes ontologias, existem diferenças que não se limitam apenas a estas métricas.
Em outras palavras, mesmo que duas ontologias apresentem um conceito considerado idêntico e que possua o mesmo número de subclasses nas duas ontologias, ainda assim não é possível afirmar que estas subclasses representam as mesmas entidades.
Em a SwTO, o conceito atividade de teste é chamado de TestActivity e é utilizado como domínio apenas da propriedade isTestActivity.
Este mesmo conceito, na OntoTest, é chamado de TestingActivity e não foi definido como domínio de nenhuma propriedade.
Por outro lado, na ontologia proposta, o conceito que representa uma atividade de teste (PTActivity) é usado como domínio de 6 diferentes propriedades:
DependsOn, generatesArtifact, isExecutedBy, isImposedByGoal, isPartOfTest e isSupportedBy.
A Tabela 2 mostra que a ontologia sobre teste de desempenho possui mais conceitos, que estes conceitos são utilizados em mais propriedades e que existem mais instâncias quando comparadas as outras duas ontologias.
Isto é um indicador de que a ontologia proposta representa uma quantidade maior de informação.
A Tabela 3 ilustra métricas calculadas no software Protégé para comparação da ontologia proposta com a OntoTest e a SwTO.
Observando a Tabela 3, é possível constatar que a ontologia proposta possui 99 conceitos, se posicionando entre os 85 conceitos da SwTO e os 126 conceitos da OntoTest.
A princípio, quanto maior o número de conceitos mais elementos do domínio estão representados na ontologia.
Contudo, o número de conceitos em si não leva em consideração se estes conceitos estão corretos ou ainda se eles são úteis.
De forma semelhante, a ontologia sobre teste de desempenho possui 44 propriedades relacionais, ficando entre as 19 propriedades da OntoTest e as 85 da SwTO.
As propriedades são responsáveis por representar relações semânticas entre os conceitos de uma ontologia.
Uma curiosidade que pode ser observada nesta comparação é que a OntoTest foi a ontologia com o maior número de conceitos, mas com o menor número de propriedades.
Quanto a o quesito número de instância, a ontologia proposta possui 171 indivíduos, enquanto que a OntoTest possui 18 e a SwTO, 36.
A maioria das instâncias na ontologia proposta pertence ao conceito métrica de desempenho ou ferramenta de teste, conforme apresentado na Seção 4.
O número de axiomas lógicos da ontologia sobre teste de desempenho também é maior que o das outras duas ontologias:
3170 axiomas lógicos contra 295 (OntoTest) e 578 (SwTO).
A explicação para esta grande diferença é que a ontologia proposta possui um número maior de instâncias, que por sua vez possuem relações entre si.
Por exemplo, uma instância de ferramenta de teste se relaciona com diversas outras instâncias da ontologia, como, por exemplo, métricas de desempenho, protocolos de comunicação, sistemas operacionais, etc..
Em outras palavras, destes 3170 axiomas lógicos, 2506 são do tipo ObjectPropertyAssertion, sendo que as outras duas ontologias não possuem nenhum axioma deste tipo.
Contudo, o número de axiomas SubClassOf na ontologia proposta, 118, é o menor valor quando comparado aos 161 da OntoTest e aos 166 da SwTO.
A ontologia sobre teste de desempenho possui 231 axiomas do tipo DisjointClasses contra 180 da SwTO e nenhum da OntoTest.
A OntoTest também não possui axiomas do tipo AnnotationAssertion, que são utilizados para anotar ou documentar os elementos da ontologia.
A ontologia proposta apresenta 187 AnnotationAssertion e a SwTO possui 153 axiomas deste tipo.
De acordo com, é importante que as definições formais e as declarações representadas na ontologia estejam acompanhadas de documentação em linguagem natural, sendo esta documentação significativa, coerente, atualizada e consistente com as definições formais.
Figura 32: Legenda utilizada para definir a expressividade em Description Logic adotada por o Protégé.
Ontologias podem variar quanto a a qualidade, cobertura e nível de detalhe.
Esta afirmação pode ser observada na prática segundo as comparações entre a ontologia proposta, a OntoTest e a SwTO.
Ainda de acordo com, decidir se uma ontologia é apropriada para um uso em particular ainda é uma tarefa subjetiva.
Isto se deve, principalmente, a escassez de métricas objetivas e computáveis para determinar a qualidade de uma ontologia.
A complexidade de avaliar e comparar ontologias se deve ao fato de que diferentes ontologias podem descrever as mesmas entidades utilizando diferentes conceitos ou, ainda, utilizar o mesmo conceito para representar diferentes entidades.
A justificativa para a ocorrência disto é que o conhecimento pressupõe um grau de interpretação, resultando em diferentes perspectivas conhecidas como paradigmas, contextos, percursos cognitivos, espaços mentais, entre outros.
Portanto, uma ontologia é o resultado de um processo de interpretação (sense-- making), que resulta numa modelagem conceitual e que representa o ponto de vista de quem participou deste processo.
Apesar de tudo, estas comparações apresentadas são importantes para a identificação de aspectos como os levantados anteriormente.
Possuir mais conceitos, propriedades, axiomas e instâncias podem indicar que o conhecimento está representado num nível maior de detalhamento.
Contudo, deve ser lembrado que a ontologia sobre teste de desempenho está sendo comparada com duas ontologias de domínios relacionados (teste de software), uma vez que não foi encontrada outra ontologia sobre teste de desempenho.
Sabendo disto e após comparar a ontologia proposta com duas ontologias relacionadas, a próxima seção apresentará os resultados de entrevistas com especialistas em teste de desempenho visando avaliar a qualidade dos conceitos representados na ontologia desenvolvida.
A relação entre uma ontologia e uma conceituação é sempre dependente da concepção desta conceitualização por um agente racional (semântica cognitiva) e da codificação formal desta conceituação (semântica formal).
Portanto, uma avaliação deve também incluir maneiras de medir o grau em que a ontologia reflete uma dada experiência, também conhecido como &quot;o julgamento de especialistas».
O objetivo desta abordagem é medir o quanto os especialistas estão de acordo sobre os elementos existentes na ontologia.
Assim, com a finalidade de avaliar a qualidade da ontologia, um questionário foi aplicado em especialistas em teste de desempenho (pessoas que possuem conhecimento teórico ou experiência prática na área).
Em uma primeira etapa deste questionário foi solicitado para cada especialista enumerar o maior número de atividades, objetivos, métricas e ferramentas relacionadas ao domínio de teste de desempenho, uma vez que estes são os principais conceitos da ontologia.
Em a segunda etapa desta avaliação, foram apresentados os conceitos da ontologia para os especialistas que receberam a tarefa de avaliar o quanto estavam de acordo com a existência de cada conceito.
Assim, é possível comparar as respostas dos especialistas com a representação da ontologia e identificar quais conceitos os especialistas conhecem, se eles concordam com os conceitos já existentes e também perguntar se existem novos conceitos que deveriam ser adicionados na ontologia.
O questionário foi respondido online2 e pode ser encontrado no Apêndice A desta dissertação.
As instruções gerais para todos que o responderam foram exatamente as seguintes:
&quot;Você pode escolher responder as seguintes perguntas utilizando unicamente seus conhecimentos ou pode consultar qualquer site, livro ou outros materiais.
Se algum material for consultado, é importante citar as fontes consultadas após cada resposta.
Dado que as seguintes perguntas possuem mais de uma resposta, tente responder- las listando a maior quantidade possível de respostas de acordo com o seu conhecimento.
As respostas não possuem tamanho mínimo ou máximo, contudo, espera- se que quanto mais detalhadas as respostas melhores serão os resultados do questionário.
Fica a critério do participante o nível de detalhamento das suas respostas.
É importante que o participante não converse sobre este questionário ou suas respostas com nenhum outro participante que já respondeu ou irá responder este mesmo questionário.»
Em uma etapa de pré-questionário, os participantes foram perguntados sobre seus conhecimentos e experiências.
Os 8 especialistas que responderam o questionário serão identificados por os números de 1 a 8.
A formação acadêmica dos participantes pode ser visualizada na Tabela 4 sendo todos os 8 ligados ao curso de Ciência da Computação:
3 atualmente cursando graduação, 2 cursando mestrado e 3 cursando doutorado.
Além disso, quando perguntados há quanto tempo estudam ou trabalham na área de teste de desempenho, os participantes relataram que sua experiência na área foi adquirida em no mínimo 2 meses (participante 3) e em no máximo 4 anos (participante 5), sendo a média do tempo de experiência dos participantes em torno de 19 meses.
As respostas completas dos 8 especialistas podem ser encontradas no Apêndice B desta dissertação.
Em seguida, os participantes foram perguntados sobre quais cargos profissionais ou acadêmicos relacionados ao teste de desempenho eles já ocuparam ou ainda ocupam.
Resumidamente, 3 participantes adquiriram suas experiências em teste como estagiário, 3 participantes como bolsistas de mestrado e 2 como doutorandos.
É importante destacar que todos os participantes estiveram ou ainda estão envolvidos no mesmo projeto de pesquisa sobre teste de desempenho resultante de um convênio da PUCRS e de uma grande empresa de Ti.
Em seguida, os participantes foram questionados sobre onde e como adquiriram a maior parte de seus conhecimentos em teste de desempenho, e também, se eles classificariam seus conhecimentos mais como práticos ou teóricos.
Apenas 2 participantes classificaram seus conhecimentos mais como teóricos, 1 participante definiu seus conhecimentos mais como práticos e os demais 5 participantes responderam que seus conhecimentos práticos e teóricos estão na mesma proporção.
O questionário foi dividido numa parte inicial de perguntas abertas e uma parte final de perguntas fechadas.
Os participantes responderam as perguntas abertas ainda sem conhecer os conceitos, propriedades e instâncias da ontologia.
A finalidade das perguntas abertas é verificar os conhecimentos dos especialistas e comparar suas respostas com a ontologia.
Após responder as perguntas abertas, os especialistas foram apresentados aos conceitos da ontologia por meio de perguntas fechadas que visavam avaliar o quanto eles estão de acordo com um determinado conceito.
As perguntas fechadas do questionário seguiram a escala Likert, que é uma das escalas mais adotadas para pesquisas de opinião.
De acordo com esta escala, os participantes devem escolher a alternativa mais adequada entre as seguintes opções:
A primeira pergunta sobre teste que os participantes tiveram que responder foi &quot;Quais objetivos um teste de desempenho pode possuir?».
De acordo com a ontologia proposta existem 7 conceitos que representam os objetivos de teste, contudo os participantes ainda não têm conhecimento desta ontologia.
As pessoas questionadas enumeram no mínimo 1 e no máximo 4 objetivos dos testes de desempenho (em média compara estas respostas com a ontologia.
O maior consenso é que um dos objetivos do teste de desempenho é a &quot;avaliação do desempenho sob condições normais de carga».
Além disso, apenas um dos objetivos identificados por os especialistas não está representado na ontologia, que é &quot;estimar uma configuração de hardware adequada para cada aplicação».
De os conceitos relacionado aos objetivos de teste, o que obteve a maior média de concordância (4.62) foi a &quot;avaliação do desempenho durante longos períodos de interação «e a pior média (3.75) se refere a &quot;elaboração de um entregável de teste de desempenho».
Em a escala adotada o valor 4 simboliza que a pessoa concorda com afirmação em questão.
Também faz parte do questionário a seguinte pergunta:
&quot;Quais ferramentas existem para teste de desempenho e o que você sabe sobre elas?».
Em média, os participantes enumeraram 2.87 ferramentas.
A ontologia tem informação sobre 18 instâncias do conceito PTTool, contudo, apenas 4 ferramentas foram citadas por os participantes.
O Visual Studio e o LoadRunner foram as ferramentas mais citadas, por 7 dos 8 participantes.
Em segundo lugar, o JMeter foi citado por 5 especialistas.
O questionário solicitava também a enumeração das atividades que podem fazer parte de um teste de desempenho.
Em média, os participantes enumeraram 6.87 atividades, de as quais 6.25 já estão representadas na ontologia.
Todos os 8 participantes foram unânimes em responder as atividades de &quot;análise das medições de desempenho «e &quot;execução dos casos de teste».
Em segundo lugar, 6 participantes citaram as atividades de &quot;desenvolvimento dos casos de teste «e &quot;elaboração do plano de teste».
Contudo, algumas atividades não foram lembradas por nenhum participante, como, por exemplo, a &quot;parametrização dos casos de teste de desempenho».
Além disso, existem atividades citadas que atualmente não fazem parte da ontologia, como a &quot;modelagem do teste de desempenho», caso seja utilizada uma abordagem baseada em modelos para geração do teste.
Quando apresentado aos conceitos do tipo PTActivity, a maior média de aceitação (4.25) ficou com a atividade &quot;monitoramento das métricas de desempenho».
Ainda, as atividades foram agrupadas em planejamento, desenvolvimento, execução e documentação.
Quando perguntados o quanto os especialistas estavam de acordo com estas categorias as respostas foram:
Faz parte do questionário também a pergunta &quot;Quais artefatos (ou entregáveis) podem resultar de um teste de desempenho?».
Embora os participantes enumeram em média 2.62 entregáveis, a ontologia possui 13 conceitos especializando PTArtifact.
A o total, os participantes citaram 6 diferentes conceitos do tipo PTArtifact, sendo o &quot;relatório do teste de desempenho «o mais citado, por 7 dos 8 questionados.
Foi lembrado também que o caso de teste, além de ser um script para uma ferramenta de teste, pode ser um modelo caso o testador use uma abordagem baseada em modelos.
Quando apresentado aos conceitos da ontologia do tipo PTArtifact, o &quot;relatório do teste de desempenho «recebeu a maior média (4.25).
Este tinha sido o entregável mais citado por os especialistas antes de eles conhecerem os conceitos da ontologia.
Uma outra pergunta do questionário foi a seguinte:
&quot;Quais métricas de desempenho podem ser monitoradas?».
Em média os participantes enumeraram 11.75 diferentes respostas para essa questão, de as quais 10.12 já estão representadas na ontologia como conceitos ou instâncias do conceito PerformanceMetric.
A ontologia possui 107 instâncias e 23 subclasses que especializam este conceito.
Além disso, todos os 8 participantes citaram MachinePerformanceMetric, 7 identificaram a instância que representa a métrica &quot;tempo de resposta «e 6 responderam o conceito PTMetric.
Ferramenta para geração de teste.
Este conceito pode ser criado como qualquer ferramenta de teste (subclasse de PTTool) que suporta (propriedade supports) a atividade de desenvolvimento dos casos de teste (conceito PTCaseDevelopment).
Ferramenta de análise.
Este tipo de ferramenta deve suportar as atividades de análise das medições de desempenho (PerformanceMeasurementAnalysis) e elaboração do relatório de teste (PTReportElaboration).
Ferramenta open source.
As ferramentas open source são instâncias de PTTool que têm como licença de uso (propriedade hasLicense) a instância OpenSourceLicense.
Ferramenta proprietária.
Semelhante ao conceito anterior, são instâncias de PTTool que se relacionam por a propriedade hasLicense com a instância ProprietaryLicense.
A Tabela 6 mostra a avaliação geral dos especialistas com relação a os conceitos da ontologia que lhes foram apresentados.
A o total, cada um dos 36 conceitos foram avaliados por os 8 especialistas, resultando em 288 respostas.
Dentro de a escala Likert adotada, a média de respostas ficou em 4.03, lembrando que o valor 4 significa que o especialista está de acordo com o conceito apresentado.
Os conceitos que obtiveram o maior nível de aceitação foram os do tipo PTGoal, com média de 4.16.
Entretanto, os conceitos do tipo PTArtifact obtiveram a média mais baixa de aceitação, com 3.85.
Com base nisso, podem ser levantadas as hipóteses de que a conceitualização não reflete com precisão o conhecimento do domínio ou que os especialistas não estão habituados com os conceitos apresentados.
Como as subclasses de PTArtifact foram extraídas do padrão IEEE 829, ou seja, fazem parte do conhecimento do domínio, o mais provável é que os especialistas não estivéssem familiarizados com estes termos.
Em este caso, a ontologia pode ajudar- los a conhecer ainda melhor os principais conceitos do domínio de teste de desempenho.
É interessante observar no Apêndice B que muitas das referências citadas por os especialistas para responder as perguntas também haviam sido utilizadas para definir a ontologia, estando presentes na seção de referências desta dissertação.
Além disso, foi comentado por dois especialistas que a ontologia representa o teste de desempenho de maneira convencional, não distinguindo o fato de que o teste de desempenho possa ser baseado em modelos.
Seria possível expandir os conceitos da ontologia neste sentido, de modo a cobrir o domínio de teste de desempenho baseado em modelos.
Esta seção contemplou os resultados da comparação da ontologia proposta com outras duas ontologias relacionadas e, também, as opiniões de especialistas em teste de desempenho sobre a ontologia.
A comparação com a OntoTest e SwTO permitiu identificar semelhanças e diferenças da ontologia proposta com relação a duas ontologias existentes.
A avaliação por especialistas possibilitou semelhanças e diferenças quanto a o conhecimento de eles sobre o domínio e o conhecimento representado na ontologia.
A seção seguinte apresentará aplicações baseadas nesta ontologia.
Aplicações que utilizam a ontologia podem ser desenvolvidas de diferentes formas como, por exemplo, estendendo as funcionalidades do Protégé ou utilizando alguma API para manipulação da ontologia.
Assim sendo, as seções seguintes exemplificam algumas destas maneiras de utilizar a ontologia proposta.
Embora estas aplicações de ontologias utilizem abordagens variadas, o objetivo é sempre auxiliar o testador de software a estabelecer testes de desempenho, como apresentado na Seção &quot;4.1 Escopo da Ontologia».
O objetivo é apoiar as decisões dos testadores com o conhecimento das tecnologias utilizadas em testes de desempenho do software, validando e recomendando opções de acordo com o ambiente de teste, os objetivos e as atividades que o teste pode possuir.
Para isto, a aplicação deve consultar a ontologia que possui conhecimento sobre as atividades necessárias para a realização de um teste de desempenho, as ferramentas que podem resolver tais atividades, quais são os requisitos para a utilização de uma dada ferramenta de teste, entre outros.
Uma das premissas para as aplicações baseadas na ontologia é que a identificação de ferramentas, atividades, artefatos e métricas adequadas para um teste específico requer determinados conhecimentos prévios sobre o domínio.
Em este sentido, as aplicações podem auxiliar os testadores de acordo com os conhecimentos da ontologia.
Para executar inferências sobre o domínio, os axiomas existentes na ontologia são manipulados a fim de responder as perguntas elaboradas de acordo com o escopo da ontologia, que são as seguintes:
De acordo com o ambiente de teste, quais ferramentas podem ser utilizadas?
De acordo com o objetivo do teste, que atividades devem ser executadas?
De acordo com os objetivos do teste e ambiente de teste, quais métricas de desempenho podem ser coletadas?
Em a seção 4, a Figura 15 apresentou os casos de uso que guiaram o desenvolvimento da ontologia.
Portanto, as aplicações baseadas nesta ontologia deverão executar estes casos de uso.
Uma das vantagens encontradas nas ontologias se deve a existência de ferramentas que apoiam seu desenvolvimento e utilização.
O Protégé pode ser configurado para permitir a exibição e manipulação da ontologia de forma intuitiva por os usuários finais.
Ainda, o Protégé possui uma arquitetura que permite o desenvolvimento de plugins que adicionem novas funcionalidades à ferramenta.
Desta forma, é possível aproveitar as funcionalidades já existentes no Protégé, como por exemplo, a utilização de reasoners e a execução de regras escritas em SWRL e SQWRL.
Foram desenvolvidos quatro novos componentes para o Protégé:
Dois tab widget e dois slot widget plugins.
Os plugins do tipo tab widget são novas abas que podem aparecer na interface do Protégé e serem utilizadas para implementar novas funcionalidades.
Em contrapartida, um plugin do tipo slot widget é um componente gráfico que estende a interface gráfica do Protégé com o objetivo de ampliar a visualização de informações e a edição de indivíduos e propriedades de uma ontologia.
Os plugins tab widget foram denominados ActivitiesDependencies e TestPlanGenerator, como pode ser visualizado na Figura 33 (o Protégé pode ser configurado para exibir ou ocultar suas abas).
O plugin ActivitiesDepedencies é uma nova aba para o Protégé que infere quais atividades devem fazer parte de uma dada instância de teste de desempenho.
Para fazer isto, é levado em consideração que o conceito PerformanceTest é domínio da propriedade hasActivity, que tem como imagem PTActivity, indicando que um teste de desempenho pode possuir múltiplas atividades.
Por sua vez, cada atividade pode apresentar a relação de dependência com outras PTActivity, por meio de a propriedade dependsOn.
Com estes conhecimentos, é gerado um grafo, como ilustrado na Figura 34, onde o nodo cinza indica a instância de teste de desempenho, os nodos verdes indicam as atividades que fazem parte deste teste e os nodos vermelhos indicam atividades que deveriam fazer parte do teste, porque são requisitos de alguma atividade existente.
A visualização do grafo foi implementada em Java com apoio da JUNG (Java Universal Network/Graph Framework).
A JUNG é uma biblioteca open source para modelagem, análise e visualização de grafos ou redes.
Os plugins do tipo slot foram denominados AnnotationsWidget e RestrictionsWidget.
Estes dois componentes utilizam herança para especializar a classe AbstractSlotWidget (disponível no pacote edu.
Stanford. Smi.
Protege. Widget da API do Protégé).
Desta forma, é possível personalizar a interface original do Protégé e permitir a exibição de características não disponíveis em sua versão padrão.
Os componentes gráficos que estendem SlotWidget são apresentados para o usuário no momento da instanciação dos conceitos da ontologia.
A Figura 35 demonstra o plugin RestrictionsWidget em uso na edição do formulário de instâncias do conceito PerformanceTest.
O RestrictionsWidget permite a visualização das restrições representadas na ontologia para um determinado conceito.
O componente AnnotationsWidget também está presente na Figura 35 (parte esquerda inferior), sendo responsável por exibir as anotações e comentários de um determinado conceito da ontologia.
Esta parte da interface do Protégé corresponde às informações exibidas por a aba denominada OWLFormsTab.
Em esta abordagem de utilização da ontologia, as funcionalidades que o Protégé oferece podem ser aproveitadas.
Um exemplo é a instanciação dos conceitos e propriedades, que é uma funcionalidade oferecida por uma aba já existente no Protégé, denominada OWLIndividualsTab ou apenas Individuals.
A Figura 36 ilustra como criar um novo indivíduo do conceito PerformanceTest.
Após criar uma nova instância de um conceito da ontologia, o resultado esperado é mostrado na Figura 37: Em o contexto denominado &quot;Individual Editor «deve ser possível visualizar e editar informações relacionadas a instância selecionada.
Observe também na Figura 37 que foram marcados em vermelho que a instância possui inconsistências de acordo com as restrições do conceito ao qual ela pertence.
Por exemplo, um conceito de PerformanceTest deve se relacionar com pelo menos uma instância de cada um dos seguintes conceitos:
PTGoal, PTActivity, PerformanceMetric, ApplicationUnderTest e PTTool.
As informações apresentadas ao usuário diferem de acordo com as propriedades de cada conceito sendo instanciado, de acordo com os axiomas da ontologia.
Em outras palavras, as informações variam de acordo com o conceito sendo instanciado, que pode ser PerformanceTest, PTTool, PTActivity, entre outros.
Além de facilitar a instanciação, outra vantagem de utilizar o Protégé é a execução de consultas em SQWRL sobre a ontologia, como será apresentado na seção seguinte.
Nome da regra:
Disk_ Monitoring_ Tools Resultado esperado:
Retorna instâncias do conceito PTTool que podem monitorar pelo menos uma métrica do disco.
Utiliza a propriedade canBeCollectedByTool.
Expressão lógica:
Retorna instâncias do conceito PTTool que podem monitorar pelo menos um sistema operacional baseado no Windows.
Utiliza a propriedade canMonitor.
É esperado que um reasoner OWL apresente as funcionalidades de verificação de consistência, satisfatibilidade dos conceitos, classificação e realização.
A verificação de consistências garante que a ontologia não contém fatos contraditórios;
A satisfabilidade dos conceitos verifica se é possível para um conceito possuir alguma instância;
A classificação computa relações hierárquicas entre as classes;
E realização encontra conceitos aos quais os indivíduos da ontologia pertencem.
Em outras palavras, reasoners são capazes de inferir consequências lógicas a partir de um conjunto de axiomas.
Para utilizar o reasoner, foram definidos conceitos que possuem as restrições apresentadas na Tabela 7.
Por exemplo, se uma ferramenta de teste (instância de PTTool) se relacionar com pelo menos uma instância de CommunicationProtocol por meio de a propriedade generateLoadOn, então é possível classificar- la como uma ferramenta capaz de gerar carga (LoadGenerationTool).
Ainda, o conceito HTTPLoadTestTool representa as ferramentas que podem gerar carga no protocol Http;
O conceito MonitoringTool agrupa ferramentas que podem monitorar pelo menos uma instância de MachinePerformanceMetric;
O conceito LinuxMonitoringTool classifica as ferramentas que monitoram pelo menos uma instância do sistema operacional Linux;
Entre outros conceitos ilustrados na Tabela 7.
Estes conceitos apresentados na Tabela 7 não possuem instâncias, contudo um reasoner pode ser aplicado para inferir se alguma instância pode ser classificada nestes conceitos.
Foi aplicado o reasoner Pellet sobre a ontologia e o resultado da inferência pode ser visualizado na Figura 39.
Por exemplo, das 18 instâncias de PTTool, 15 foram classificadas como LoadGenerationTool, 14 instâncias são do tipo HTTPLoadTestTool, 10 foram classificadas como MonitoringTool e assim por diante.
O WebProtégé é um projeto open source que utiliza o Protégé para oferecer as funcionalidades de desenvolvimento de ontologias e o Google Web Toolkit na interface de usuário.
Além de oferecer suporte a edição colaborativa, o WebProtégé permite que as informações da ontologia sejam facilmente acessadas por os clientes web.
Uma das vantagens oferecida por o WebProtégé é tornar uma ontologia disponível na web, de forma que os usuários possam navegar em ela sem a necessidade de instalar softwares específicos para se trabalhar com ontologias.
Com o objetivo de testar se a ontologia proposta poderia também ser utilizada no WebProtégé, o Tomcat 3 foi instalado para hospedar uma versão do WebProtégé que incluia a ontologia proposta.
A Figura 40 ilustra a tela inicial da aplicação incluindo a ontologia denominada &quot;Performance Test Ontology».
A configuração padrão do WebProtégé exibe 4 abas:
Classes, Properties, Individuals e Notes and Discussions.
Quanto a navegação na ontologia, a Figura 41 apresenta as informações disponíveis na aba Individuals:
As classes da ontologia podem ser visualizados na parte esquerda da figura;
A parte central exibe os indivíduos do conceitos selecionado (que no caso é PTTool);
E as propriedades do indivíduo selecionado (no caso é LoadRunner) podem ser visualizadas na parte direita da Figura 41.
O WebProtégé possui uma interface web customizável para navegação e edição de ontologias para facilitar a utilização por especialistas do domínio, que podem não possuir experiência com ontologias.
Esta afirmação é confirmado por os menus &quot;Add content «e &quot;Add tab «na Figura 41.
Além disso, é possível configurar os privilégios de cada usuário em relação a visualização e edição das ontologias.
A Figura 42 apresenta as informações da aba Classes, que possui a hierarquia de conceitos na parte esquerda e as propriedades e restrições para o conceito selecionado (na imagem foi selecionado o conceito PerformanceBottleneckIdentification).
Também foi desenvolvida uma aplicação que utiliza a ontologia proposta, por meio de a OWL API.
Como os axiomas estão codificados num arquivo OWL gerado por o Protégé, a aplicação consulta as informações representadas neste arquivo.
Para manipular os axiomas da ontologia, a aplicação Java utiliza a OWL API, que é uma biblioteca Java open source para a criação, manipulação e serialização de ontologias OWL.
A aplicação permite criar novas instâncias do conceito teste de desempenho, vincular uma instância específica do conceito teste de desempenho com instâncias de ferramentas de teste, atividades, objetivos, métricas, entre outros.
Os conceitos e propriedades do domínio representados na ontologia podem ser criados, recuperados e atualizados na aplicação.
Além disso, os axiomas relacionados a uma instância específica de teste de desempenho são utilizados por a funcionalidade de geração do plano de teste.
Para criar o plano de teste no formato PDF foi utilizada a iText API versão 5.2.1, uma biblioteca open source para manipulação de documentos PDF em Java.
A interface gráfica pode ser visualizada na Figura 43, onde uma instância de PerformanceTest está selecionada e suas propriedades estão destacadas na interface.
Utilização da OWL API Esta seção tem como objetivo explicar e exemplificar como os métodos da OWL API foram utilizados para carregar e manipular uma ontologia OWL.
Como exemplo de uso, será demonstrado como carregar uma ontologia de um arquivo, criar um novo indivíduo e adicionar o axioma de que este indivíduo pertence a um conceito da ontologia.
A aplicação apresentada utiliza este código para criar instâncias de conceitos como PerformanceTest, PTActivity e PTTool.
A Figura 44 ilustra o código e apresenta uma linha de comentário antes de cada comando para explicar os métodos utilizados.
Estes trechos de código são explicados em maior detalhe a seguir.
Para acessar e manipular ontologias com a OWL API é preciso criar um objeto da classe OWLOntologyManager:
Em a documentação javadoc da OWL API, a classe OWLOntologyManager é dita como responsável por gerenciar um conjunto de ontologias, sendo o ponto principal para criar- las e acessar- las.
Este exemplo carrega a ontologia de um arquivo, contudo a OWL API também permite que a ontologia seja carregada a partir de a web por meio de um IRI (Internationalized Resource Identifier).
Para carregar a ontologia de um arquivo, é preciso possuir uma referência que aponte para a localização de um arquivo OWL:
Possuindo os dois objetos criados anteriormente, é possível carregar a ontologia.
Em a OWL API, a ontologia é representada por a classe OWLOntology, que consiste de um conjunto de axiomas representados por a classe OWLAxiom.
O seguinte código inicializa um objeto OWLOntology:
Os conceitos, indivíduos, propriedades e axiomas da ontologia podem ser criados ou carregados por métodos disponibilizados por a classe OWLDataFactory.
Um objeto do tipo OWLDataFactory é obtido com o seguinte código:
Cada componente da ontologia é identificado por um IRI, que é formado por a composição do namespace da ontologia com a identificação do componente.
O código a seguir cria um IRI para representar uma referência de um indivíduo:
Possuindo o IRI do indivíduo que se deseja criar, é possível criar uma referência para um objeto do tipo OWLNamedIndividual por meio de o seguinte trecho de código:
Assim como cada instância deve possuir um IRI, os conceitos também são identificados por IRI.
É preciso criar um IRI para o conceito que o indivíduo pertencerá:
Os conceitos de uma ontologia são representados por a classe OWLClass, como demonstra o código abaixo:
O axioma que afirma que um indivíduo pertence a um determinado conceito é representado por um objeto da classe OWLClassAssertionAxiom:
OWLClassAssertionAxiom assertion Para adicionar o novo axioma na ontologia e salvar as modificações, é preciso executar, os métodos addAxiom e saveOntology da classe OWLOntologyManager:
Esta seção apresentou um exemplo de como carregar uma ontologia utilizando a OWL API, como criar um novo indivíduo e adicionar o axioma de que este indivíduo pertence a um determinado conceito da ontologia.
A próxima seção deste trabalho explica um exemplo de uso da API utilizada para geração dos planos de teste em PDF.
Exemplo de Uso da iText API Este trabalho utilizou o iText4 versão 5.2.1 para geração do plano de teste em formato PDF.
O iText é uma API desenvolvida em Java para criação e manipulação de documentos PDF.
A Figura 45 demonstra um trecho de um plano de teste em PDF gerado contendo as informações referentes à instância do conceito teste de desempenho denominada &quot;Skills Performance Test».
Para gerar o plano de teste para um indivíduo do tipo teste selecionado, o algoritmo inicia com um objeto da classe OWLIndividual (da OWL API) que representa o teste desejado e executa os seguintes passos:
Descobrir todas as propriedades do teste por meio de o método getObjectPropertyValues existente na classe OWLIndividual.
Para cada propriedade P, escrever a identificação da propriedade P no PDF e cada indivíduo da ontologia relacionado por a propriedade P com o teste em questão.
Visando demonstrar a utilização da iText API, o código ilustrado na Figura 46 cria o arquivo &quot;Test plan».
Pdf &quot;contendo o parágrafo «Hello World».
Esta seção demonstrou diferentes aplicações baseadas na ontologia proposta.
Para o desenvolvimento destas aplicações, foram estudadas tecnologias como, por exemplo, Protégé, Pellet, Java, OWL API, SQWRL, JUNG e iText.
A seção seguinte apresentará as considerações finais desta pesquisa, incluíndo uma comparação das contribuições deste trabalho com os trabalhos relacionados.
&quot;A ciência se compõe de erros que, por sua vez, são os passos em direção a a verdade. «(
Júlio Verne) Inicialmente, esta seção de considerações finais destaca as principais contribuições resultantes desta pesquisa de modo a comparar- las com as contribuições dos trabalhos relacionados.
Uma visão geral desta comparação é apresentada na Tabela 8.
Os títulos dos 19 trabalhos que foram comparados estão listados a seguir:
T1) Ontologia para teste de desempenho de software (este trabalho) T3) Performance-related ontologies and semantic web applications for on-line T4) Web performance and behavior ontology T6) An approach to ontology-- aided performance engineering through NFR framework T7) Ontology--based web application testing T9) Knowledge--based software test generation testing process T12) Ontology--based web service robustness test generation T13) An ontology-- software test generation framework T14) Ontology--based case generation for simulating complex production automation systems T15) Ontology--based for multiagent systems web-- based applications T17) A multi-agent software environment for testing web-- based applications T18) Ontology--based case generation for testing web services Os trabalhos relacionados anteriormente listados foram comparados de acordo com as seguintes contribuições:&amp;&amp;&amp;
De acordo com a Tabela 8, é possível observar que:
A contribuição mais frequente, em 17 dos 19 trabalhos, é a proposta de uma aplicação baseada em ontologias.
Contudo, apenas 12 trabalhos apresentam uma ontologia;
A avaliação ou validação de uma ontologia não é comum de acontecer, uma vez que apenas 5 dos 19 trabalhos apresentaram uma contribuição deste tipo;
Comparações entre duas ou mais ontologias não foram encontradas em trabalhos relacionados, entretanto nesta dissertação são comparadas 3 ontologias;
A comparação de trabalhos relacionados às áreas de ontologia e teste de software é uma contribuição que foi identificada em apenas 2 dos 19 trabalhos analisados;
De os 19 trabalhos, 12 citam referências que podem ser ou foram utilizadas para criar uma ontologia.
Os trabalhos T2, T7 e T13 não criaram ontologias, mas citaram estas referências.
Por outro lado, os trabalhos T15, T16 e T17 criaram uma ontologia sem citar se ou quais referências foram utilizadas para isto;
Embora 17 trabalhos apresentem aplicações de ontologias, apenas 15 comentaram utilizar tecnologias relacionadas à área de ontologia.
A Tabela 9 e a Tabela 10 aumentam o nível de detalhamento sobre a informação apresentada na Tabela 8.
De acordo com a Tabela 9, podemos perceber que:
A mesma ontologia é apresentada nos trabalhos T4 e T5, assim como os trabalhos T15 e T16 também se referem a mesma ontologia;
As aplicações baseadas em ontologias são bem diversificadas, sendo o tema mais recorrente a geração de teste.
Contudo, mesmo neste tema, o foco varia de trabalho para trabalho (teste funcional, robustez, de web service).
As formas de avaliação ou validação encontradas foram por meio de o cálculo de métricas da ontologia, questionando especialistas do domínio ou simulando o uso da aplicação baseada na ontologia.
A Tabela 10 permite concluir que:
A comparação entre ontologias e entre trabalhos relacionados ainda é incipiente;
A referência mais citada para criação de uma ontologia (5 votos) é o SWEBOK;
A tecnologia mais utilizadas, por 9 dos 19 trabalhos, é OWL.
Além disso, OWL-S, que é uma variação de OWL, é utilizada por mais 2 trabalhos.
A segunda tecnologia relacionada a ontologia mais aplicada é o Protégé, citado por 7 trabalhos.
Tabela 10: Comparação das contribuições dos trabalhos relacionados (parte 2).
Performance Test Ontology, SwTO e OntoTest Compara 19 trabalhos SWEBOK, IEEE Std.
829 e IEEE Std.
610.12 OWL, Protégé, WebProtégé, OWL API, Pellet, SQWRL UML SPT, Core Scenario Model e SPE-MM OWL, Protégé, SWRL, Jess, Jena, Kazuki OWL, Jastor NFR Framework Softgoal Interdependency Graph Compara 11 trabalhos SWEBOK, IEEE Std.
829, Iso/ IEC 15939:2007 e Ontologia em UML e OWL, POSL, OO jDREW Iso/ IEC 9126-1:2001 e OWL, Protégé OMG Ontology Definition Meta Model Protégé OWL, Protégé, Jade Ontologia OWL-S, Protégé, Jena2 SWEBOK, Basic Linux Ontology em OWL, Protégé, Jena, Racer Este trabalho propôs uma ontologia sobre teste de desempenho de software e explorou aplicações que a utilizam.
Para a construção da ontologia, foram estudas técnicas, ferramentas e metodologias da engenharia ontológica, conforme apresentado na fundamentação teórica.
Os conceitos, relações, propriedades, axiomas e instâncias que representam o conhecimento da ontologia foram extraídos da bibliografia de testes de desempenho e de ontologias de domínios relacionados.
A ontologia proposta é utilizada em aplicações projetadas para auxiliar o gerenciamento e o planejamento dos testes de desempenho.
Ontologias são representações formais do conhecimento que permitem o compartilhamento do conhecimento do domínio entre diferentes aplicações.
Assim, outras aplicações podem ser exploradas com base na mesma ontologia, o que resulta em interoperabilidade e um &quot;comprometimento ontológico».
Este comprometimento é um acordo onde fica especificado que será utilizado um vocabulário consistente com os significados definidos numa ontologia.
Portanto, utilizar uma abordagem ontológica permite padronizar o vocabulário da área e pode facilitar a troca de conhecimentos e a comunicação entre testadores, gerentes, desenvolvedores e usuários.
Além disso, ontologias já foram pesquisadas para a geração de testes, melhoramento do desempenho em tempo de execução, para o estabelecimento de um processo de aprendizagem sobre teste de software, entre outras aplicações.
Este estudo apresentou o uso de ontologias no planejamento e especificação dos testes de software.
A motivação da pesquisa foi investigar se ontologia, como técnica de representação do conhecimento, pode melhorar o processo de teste de desempenho, auxiliando a tomada de decisão e aumentando a qualidade dos testes.
A ontologia representa tanto o conhecimento sobre o domínio de teste de desempenho, quanto o conhecimento de testes anteriores.
Portanto, uma abordagem ontológica pode apoiar a aquisição e o compartilhamento de conhecimentos sobre novas tecnologias, como novas ferramentas de teste.
Além disso, ao longo de esta pesquisa foram investigadas e aplicadas as seguintes tecnologias:
Protégé, Pellet, Java, OWL API, SQWRL, JUNG e iText.
O objetivo geral deste trabalho foi a construção de uma ontologia sobre teste de desempenho de software.
Como consequência, também foi explorada a aplicação desta ontologia nas atividades de planejamento dos testes de desempenho.
Como vantagens desta abordagem, é possível perceber que a utilização de ontologias pode proporcionar uma maior organização e gerenciamento do conhecimento nos testes de desempenho.
Sobre a ontologia de teste de desempenho proposta, foram elaboradas inferências como, por exemplo, deduzir quais tecnologias podem ser utilizadas para testar determinada aplicação num dado ambiente de teste.
Em um nível mais alto de abstração, as aplicações podem auxiliar o testador a definir o que deve ser levado em consideração durante a elaboração de um teste de desempenho de software.
Assim, a ontologia pode auxiliar o gerenciamento das instâncias dos conceitos e propriedades no domínio, como ferramentas de teste de desempenho, atividades, métricas, objetivos e artefatos.
As aplicações baseadas nos axiomas da ontologia podem ser utilizadas, por exemplo, para indicar atividades e ferramentas alinhadas aos objetivos do teste.
A ontologia proposta foi comparada com duas ontologias relacionadas e validada por especialistas no domínio de teste de desempenho.
Segundo, uma ontologia não deve conter todas as informações possíveis sobre um domínio uma vez que a conceitualização não precisa exceder as necessidades das aplicações que utilizam a ontologia.
Além disso, uma ontologia é um modelo abstrato da realidade e possui como limitação a capacidade de representar apenas parcialmente as informações existentes num domínio de conhecimento.
Em outras palavras, embora os principais termos do domínio de teste de desempenho tenham sido representados, existem mais conceitos, propriedades e instâncias que atualmente não fazem parte da ontologia proposta.
Contudo, a qualquer momento é possível criar novos conceitos para expandir a ontologia;
Um exemplo disso é o conceito PerformanceTest que pode ser especializado por as classes LoadTest, StressTest, EnduranceTest.
Um LoadTest pode ser definido como um PerformanceTest que deve possuir como objetivo a avaliação do desempenho sob condições normais de carga, uma instância de StressTest apresentará o objetivo de identificar o gargalo do sistema e um EnduranceTest é um teste cujo objetivo é avaliar o desempenho por longos períodos de interação.
Os objetivos citados anteriormente são conceitos que já existem na ontologia, mas, como mostrado, novos conceitos podem ser adicionados.
Outro exemplo é o conceito PerformanceTester, que pode receber especializações segundo critérios de uma empresa que utilize a ontologia, como TestAnalyst, TestDeveloper, TestEngineer, SeniorTester, etc..
Além disso, seria interessante utilizar uma técnica chamada modular design, que visa separar uma ontologia grande em ontologias menores (módulos) visando aumentar o reuso e facilitar a importação apenas das partes desejadas.
Em este caso, a ontologia sobre o domínio de teste de desempenho poderia ser a composição de uma ontologia das atividades de teste, uma ontologia sobre o sistema sob teste, uma ontologia sobre as ferramentas de teste, e assim por diante.
A ontologia foi desenvolvida para atender os casos de uso explicados na Seção 4, contudo é possível ampliar a conceitualização da ontologia para novas funcionalidades de interesse aos testes de desempenho.
A identificação de sequências de interações mais relevantes para um teste e a identificação de gargalos nas métricas de desempenho são exemplos de aplicações que podem ser exploradas como trabalhos futuros.
Ainda, é possível explorar como a ontologia poderia ser expandida para permitir a criação de um caso de teste de desempenho conceitualmente e, após, converter este caso de teste para uma determinada ferramenta.
Além de isto, seria interessante investigar como a ontologia poderia converter um caso de teste já implementado de uma ferramenta para outra.
Também, a ontologia pode ser futuramente utilizada em aplicações visando auxiliar o testador com a geração de outros documentos além de o plano de teste.
As atividades planejadas para o mestrado foram concluídas, a saber:
Estudar trabalhos relacionados e comparar suas contribuições;
Por fim, podemos perceber que o foco dos sistemas de informação está se movendo do &quot;processamento de dados «em direção a o &quot;processamento de conceitos».
