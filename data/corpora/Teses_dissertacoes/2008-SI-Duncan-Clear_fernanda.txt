As organizações de software buscam, cada vez mais, aprimorar seu Processo de Desenvolvimento de Software (PDS), com o intuito de garantir a qualidade dos seus processos e produtos.
Para tanto, elas adotam modelos de maturidade de software.
Esses modelos estabelecem que a mensuração da qualidade seja realizada através de um programa de métricas (PM).
As métricas definidas devem ser coletadas e armazenadas, permitindo manter um histórico organizacional da qualidade.
Contudo, apenas mensurar não é o bastante.
As informações armazenadas devem ser úteis para apoiar na manutenção da qualidade do PDS.
Para tanto, os níveis mais altos dos modelos de maturidade sugerem que técnicas estatísticas e analíticas sejam utilizadas, com a finalidade de estabelecer o entendimento quantitativo sobre as métricas.
As técnicas de mineração de dados entram neste contexto como uma abordagem capaz de aumentar a capacidade analítica e preditiva sobre as estimativas e o desempenho quantitativo do PDS.
Este trabalho propõe um método para a execução do processo de KDD (Knowledge Discovery in Database), denominado de SPDW-Miner, voltado para a predição de métricas de software.
Para tanto, propõe um processo de KDD que incorpora o ambiente de data warehousing, denominado SPDW+.
O método é composto por uma série de etapas que guiam os usuários para o desenvolvimento de todo o processo de KDD.
Em especial, em vez de considerar o DW (data warehouse) como um passo intermediário deste processo, o toma como ponto de referência para a sua execução.
São especificadas todas as etapas que compõem o processo de KDD, desde o estabelecimento do objetivo de mineração;
A extração e preparação dos dados;
A mineração até a otimização dos resultados.
A contribuição está em estabelecer um processo de KDD num nível de detalhamento bastante confortável, permitindo que os usuários organizacionais possam adotar- lo como um manual de referência para a descoberta de conhecimento.
Palavras Chave: Processo de KDD, Técnica de Classificação, Métricas de Software e Data warehouse.
As organizações de software buscam, cada vez mais, aprimorar seu Processo de Desenvolvimento de Software, com o intuito de garantir a qualidade dos seus produtos.
Segundo, a qualidade do produto de software está diretamente relacionada à qualidade do seu processo de desenvolvimento.
Desta forma, na tentativa de assegurar a qualidade do PDS, muitas empresas estão adotando modelos de maturidade de software, como o CMMI (Capability Maturity Model Integration) e o MPS.
Br (Melhoria de Processos do Software Brasileiro).
Esses modelos definem os elementos necessários para tornar o PDS definido, eficiente e controlado, através de etapas evolutivas do processo.
O CMMI estabelece que as organizações devem definir um conjunto de processos especializado para refletir as particularidades dos diferentes projetos de desenvolvimento de software.
Após os processos serem definidos, eles devem ser mensurados para permitir o seu controle e assegurar a sua qualidade.
A mensuração é realizada através do estabelecimento de um Programa de Métricas (PM).
Para que uma organização de software seja certificada com o CMMI nível 3, esta deve apresentar um programa de métricas definido e um repositório para armazenar- las.
No entanto, compreender o complexo relacionamento entre as métricas e os demais atributos do PDS, para controlar a qualidade, não é uma tarefa trivial.
O CMMI nível 4 prevê a utilização de técnicas estatísticas e analíticas sobre as métricas para estabelecer o entendimento quantitativo dos processos.
Esse entendimento é fornecido através do estabelecimento de modelos de desempenho de processo, os quais são usados para representar comportamentos passados e atuais, assim como para predizer futuros resultados dos processos.
As técnicas de mineração de dados entram neste contexto como uma abordagem capaz de aumentar a capacidade analítica e preditiva sobre as estimativas e o desempenho quantitativo do PDS.
Este trabalho aborda, justamente, o uso de técnica de classificação para predição de métricas de software.
Para tanto, propõe um processo de KDD que incorpora o ambiente de data warehousing, denominado SPDW+, para previsão de métricas de software.
Em especial, um método de execução do processo de KDD que, em vez de considerar o DW como um passo intermediário deste processo, o toma como ponto de referência para a sua execução.
Este trabalho está organizado da seguinte forma:
O capítulo 2 apresenta o referencial teórico, o capítulo 3 a descrição do cenário, o capítulo 4 relata o método de execução do processo de KDD, o capítulo 5 apresenta o estudo de caso onde é aplicado o método, o capítulo 6 descreve os trabalhos relacionados e o capítulo 7 discorre sobre as considerações finais e os trabalhos futuros.
Este capítulo apresenta conceitos e assuntos relacionados ao tema de pesquisa.
Para tanto, o mesmo aborda uma breve explanação sobre:
Métricas de software, modelo de maturidade de software, data warehouse, e processo de descoberta de conhecimento em base de dados.
Segundo o padrão IEEE 1061, a qualidade de software é o grau em o qual é apresentada uma combinação desejada de atributos.
Este padrão estabelece requisitos de qualidade, através da identificação, implementação, análise e validação de métricas de qualidade de produtos e processos de software.
A fim de medir os atributos da qualidade do software, um conjunto apropriado de métricas deve ser identificado e definido.
Uma métrica é uma função mensurável, cujas entradas são dados de software e seu resultado corresponde a um único valor numérico, que pode ser interpretado como o grau de qualidade do software, sendo esse afetado por determinado atributo.
O objetivo das métricas é avaliar o produto de software durante todo o seu ciclo de vida, em relação a as exigências de qualidade que foram definidas.
As métricas permitem controlar a qualidade do processo e dos produtos desenvolvidos.
O uso de elas numa organização ou projeto permite uma melhor visibilidade da qualidade.
A definição de um conjunto de métricas deve ser realizada de acordo com as reais necessidades da organização.
Os modelos de maturidade prevêem que métricas de produto e processo sejam definidas.
O padrão IEEE 1061, define métricas de software em duas categorias:
De produto e de processo.
As métricas de produto são usadas para medir as características de uma documentação ou código (e.
g tamanho do produto e complexidade do fluxo de dados).
As de processo são utilizadas para medir características do processo e do ambiente de desenvolvimento (e.
g eficiência de remoção de defeitos e experiência do programador).
Utiliza a mesma classificação para as métricas que o padrão IEEE 1061, porém prevê mais uma categoria de métrica, as de projeto.
As métricas de projeto definem características do projeto e de sua execução (e.
g número de recursos e produtividade).
O padrão IEEE 1061 também prevê a classificação de métricas como diretas e indiretas.
As diretas são aquelas que não dependem de nenhum outro atributo.
E as indiretas são calculadas em função de outros atributos.
A utilização de métricas de software reduz a subjetividade da avaliação da qualidade, pois fornece uma base quantitativa para a tomada de decisão sobre a qualidade do software.
Entretanto, o uso de métricas não elimina a necessidade do julgamento humano na avaliação do software.
Um PM é uma forma de documentar e organizar as métricas, permitindo que o uso de elas seja fomentado e institucionalizado numa organização.
A norma IEEE 1061 e os modelos de maturidade como CMM e CMMI sugerem a definição e utilização de um PM que seja significativo para os projetos e para a organização.
Contudo, não definem quais métricas devem ser utilizadas.
Estas devem ser estabelecidas através das reais necessidades da organização, buscando atingir os diferentes níveis de gerenciamento organizacional.
Para facilitar a análise e focar em todos os pontos de necessidades de informação elas podem ser, convenientemente, agrupadas em áreas de qualidade.
Um PM para ser abrangente e eficiente deve cobrir todas as áreas de qualidade de uma empresa (e.
g defeitos, requisitos, cronograma e etc).
Em um PM além de a definição de cada métrica, deve constar:
O seu nome;
O custo, os benefícios e os impactos associados a sua utilização;
As faixas de valores esperados;
As ferramentas de armazenamento utilizadas;
A sua aplicabilidade;
Os valores de entrada necessários para o seu cálculo;
E um exemplo de sua aplicação.
Segundo, processo de desenvolvimento de software é um conjunto de atividades e resultados associados que levam à produção de um produto de software.
Para, este processo pode ser definido como uma estrutura para as tarefas que são necessárias à construção de um software de alta qualidade.
A busca por qualidade no PDS tem impulsionado a adoção de modelos de maturidade de software, como CMMI e o MPS.
Br.. Em estes modelos a evolução do PDS é adquirida através de etapas evolutivas do processo, auxiliando as organizações de software no processo de seleção de estratégias de melhoria, determinando a maturidade atual dos respectivos PDS, bem como, identificando questões críticas para seu aperfeiçoamento.
O CMMI é um modelo de maturidade que fornece orientações para melhorar os processos de uma organização e a habilidade de gerenciar o desenvolvimento, aquisição, manutenção de produtos e serviços.
Existem dois tipos de representação do CMMI:
A representação em estágios é a mesma utilizada no CMM (Capability Maturity Model) antecessor do CMMI, e a sua estrutura é mostrada na Figura 1.
Em ela são definidos cinco níveis de Maturidade (Maturity Levels), os quais são compostos de áreas de processo (Process Areas), objetivos genéricos (Generic Goals) e específicos (Specific Goals), práticas genéricas (Generic Practices) e específicas (Specific Practices).
A representação em estágio é a abordada neste trabalho por ser a mais usual, esta é mostrada na Figura 1.
Os cinco níveis de maturidade da representação em estágio do CMMI são:
Inicial, Gerenciado, Definido, Gerenciado Quantitativamente, e Otimizado.
O nível 1 é caracterizado por não apresentar nenhum procedimento eficiente de gerência de projeto.
O processo de desenvolvimento é ad hoc e caótico.
Em o nível 2 os processos são planejados, executados, medidos e controlados.
Assim, existe a necessidade de se estabelecer um PM, que contemple as diferentes áreas de qualidade da organização.
Em o nível 3 os processos são bem definidos e entendidos, um conjunto de processos padrões da organização é estabelecido, e assim, cada projeto pode adaptar- los de acordo com a sua realidade.
Além disso, existe a necessidade da implementação de um repositório de dados para armazenar as métricas definidas no nível 2.
Esta base de dados permite uma visão unificada das informações de todos os projetos da organização, possibilitando suporte à decisão organizacional.
Além de compartilhar boas práticas experiências de desenvolvimento de software.
Em muitos casos, esses repositórios de métricas baseiam- se em data warehouse (DW)(, e), estruturados segundo um modelo analítico multidimensional e inseridos num ambiente de data warehousing.
Em estes ambientes além de o repositório deve existir um processo de extração, transformação e carga (Etc) para extrair, preparar e carregar as métricas provenientes de várias fontes no repositório central, sendo esta etapa crucial para garantir a qualidade das métricas coletadas.
Tanto as características desse modelo, como os conceitos que envolvem a definição de um DW encontram- se detalhadas na seção 2.3.
O nível 4 é o gerenciado quantitativamente.
Em ele os subprocessos são selecionados para serem gerenciados quantitativamente, através de técnicas estatísticas e analíticas.
Em este nível os objetivos quantitativos para a qualidade são estabelecidos e utilizados como um critério de gerenciamento.
A qualidade e o desempenho dos processos são entendidos em termos quantitativos, e controlados durante todo o ciclo de vida do projeto.
Em este nível existe a necessidade dos processos serem previsíveis e, para tanto, técnicas voltadas para este fim devem ser usadas.
Em o nível 5 os processos são continuamente otimizados, através do entendimento quantitativo.
Esse nível foca na contínua manutenção do desempenho dos processos através de inovações tecnológicas.
Os objetivos de melhoramento quantitativos da organização são estabelecidos e continuamente revisados para refletir os diferentes objetivos de negócios.
Uma área de processo (AP) é um conjunto de práticas relacionadas que, desempenhadas coletivamente, satisfazem um conjunto de objetivos considerados importantes para alcançar um melhoramento significativo numa determinada área.
Para uma organização alcançar o nível 3 de maturidade, por exemplo, esta deve alcançar todos os objetivos genéricos e específicos das áreas de processo do nível 2 e 3.
A Tabela 1, apresenta todas as áreas de processo distribuídas por níveis.
Entre as AP do modelo CMMI são destacadas, neste trabalho, a de Mensuração e Análise (nível 2), Gerenciamento Quantitativo de Projeto e Desempenho de Processo Organizacional (nível 4).
Em síntese, essas AP visam definir a mensuração dos processos para, assim, estabelecer o gerenciamento quantitativo.
A AP de Mensuração e Análise é uma das mais importantes do modelo CMMI.
Ela tem a finalidade de desenvolver e sustentar a capacidade de mensuração, a qual é usada para dar suporte ao gerenciamento de informações.
Ela provê práticas específicas que conduzem os projetos e a organização no estabelecimento de um PM e dos resultados que estas podem gerar para apoiar à tomada de decisão e ações corretivas do processo em tempo hábil.
Para permitir a utilização das métricas como uma ferramenta de apoio à gestão dos processos, o CMMI aponta a necessidade da implementação de um repositório.
Este último visa armazenar os processos organizacionais e suas métricas coletadas, de forma organizada e concisa, permitindo que os gestores possam acessar- lo para efeitos de análise.
Através das métricas estabelecidas no nível 2 e do repositório no nível 3, é possível estimar e planejar atividades, prazos, custos, e ainda fornecer uma visão unificada e quantitativa sobre a qualidade dos projetos da organização.
As outras duas AP que se destacam são Gerenciamento Quantitativo de Projeto e Desempenho de Processo Organizacional, ambas do nível 4.
Essas duas visam implantar e manter o entendimento quantitativo dos processos organizacionais, provendo suporte à qualidade, aos objetivos de desempenho e às estimativas e, ainda, a disponibilização de modelos para gerenciar quantitativamente os projetos organizacionais.
O entendimento quantitativo é estabelecido através de modelos de desempenho, os quais são usados para representar desempenhos passados e atuais, e também para predizer futuros resultados dos processos.
As organizações usam essas informações quantitativas e técnicas analíticas para caracterizar produtos e processos.
Essa caracterização é útil para:
Determinar se os processos estão se comportando consistentemente ou se têm tendências estáveis, isto é, podem ser preditos;
Identificar os processos onde o desempenho está dentro de os limites estabelecidos;
Estabelecer critérios para identificar se um processo ou elemento de ele está quantitativamente controlado, e determinar as medidas e técnicas analíticas pertinentes a tal gerência;
Identificar os processos que se mostram anômalos;
Identificar todos os aspectos dos processos que podem ser melhorados no conjunto de processos padrão da organização;
Desta forma, uma possível técnica analítica para permitir o estabelecimento dos modelos de desempenho dos processos é a técnica de mineração.
As técnicas de mineração, como a classificação e a regressão, podem ser usadas para gerar os modelos de desempenho de processo, permitindo dar maior previsibilidade aos mesmos, através do estabelecimento de modelos preditivos.
Esses modelos são úteis para justificar determinados comportamentos observados nos processos de software, realizar estimativas de prazo e custo.
Ainda, pode- se prever possíveis falhas e mostrar, antecipadamente, desalinhos nos objetivos do projeto.
Um data warehouse (DW) é um conjunto de dados baseado em assuntos, integrado, não volátil e variável em relação a o tempo, de apoio às decisões gerenciais.
O fato deste ser baseado em assunto e integrado fornece uma visão simples e concisa sobre o negócio.
O DW é usualmente construído a partir de a integração de várias fontes de dados heterogêneas (banco de dados relacionais, arquivos de texto e registros de transações on-line).
A não volatilidade diz respeito à forma como os dados são tratados no DW, neste último eles são carregados e acessados, mas as atualizações geralmente ocorrem no ambiente operacional.
A última característica significativa deste tipo de base de dados é ser variável em relação a o tempo, possibilitando a manutenção de uma perspectiva histórica dos dados.
O processo de consolidação das diferentes fontes de dados é realizado por intermédio de técnicas de integração e limpeza, as quais são aplicadas nos dados originais antes do seu armazenamento efetivo no DW.
Esse processo é denominado de Etc (extração, transformação e carga), e visa adequar os dados originais através de transformações, assegurando que estes sigam padrões desejados, conforme as convenções de nomes, os atributos físicos e as unidades de medidas de atributos.
Desta forma, é possível garantir que os dados sejam integrados de forma consistente e com qualidade, possibilitando à tomada de decisão a partir de valores confiáveis.
Tipicamente, um DW é construído segundo uma estrutura multidimensional, a qual apresenta como componentes principais:
Tabelas fato e dimensão.
A primeira apresenta atributos numéricos que caracterizam fatos do negócio e atributos chaves que permitem um relacionamento com as dimensões.
A segunda representa as diferentes perspectivas ou entidades de análise.
Esse tipo de repositório é o principal componente de ambientes de apoio à tomada de decisão;
A sua utilização é uma tendência na indústria da informação.
A descoberta de conhecimento em base de dados, também conhecida como KDD, é um processo não trivial de identificar padrões válidos, novos e potencialmente úteis em base de dados.
Este se caracteriza por ser um processo iterativo e interativo, em o qual várias etapas são executadas considerando a decisão do usuário. Segundo
e o processo de KDD é composto das seguintes etapas:
Limpeza e integração (Cleaning and Integration);
seleção e transformação (Selection and Transformation);
mineração (Data Mining);
E avaliação e apresentação (Evaluation and Presentation).
Já descreve o mesmo processo como sendo constituído de três grandes etapas:
Pré-processamento, mineração e pós-processamento de dados.
Entre as etapas citadas, a mineração é a mais importante.
É em ela que algoritmos de descoberta de conhecimento são aplicados sobre os dados buscando encontrar informações úteis.
A Tabela 2 apresenta um comparativo entre as três abordagens citadas, onde as linhas correspondem aos diferentes processos de KDD e as colunas representam as suas etapas equivalentes.
O processo de KDD proposto por[ HAN01] é ilustrado na Figura 2.
Um fato que o diferência das propostas de e é a presença de um DW entre as etapas:
Limpeza e integração e seleção e transformação.
Ele supõe o uso de um DW para armazenar os dados relevantes ao problema de forma concisa e organizada, a partir de o qual os dados podem ser selecionados e transformados e, então, submetidos ao algoritmo de mineração.
Os outros dois autores não prevêem este repositório, admitem apenas que os dados sejam extraídos de suas fontes originais, e por fim preparados para a mineração.
O pré-processamento ou preparação é a etapa do processo de KDD onde os dados são tratados, de forma a se adequarem à entrada do algoritmo de mineração.
Esta engloba uma série de outras etapas menores, tais como limpeza, integração, seleção e transformação.
Ela é considerada bastante laboriosa e consome em torno de 85% do tempo necessário para executar o processo de KDD.
No entanto, é através da aplicação de uma adequada preparação que os algoritmos de mineração obtêm resultados satisfatórios.
Muitas técnicas voltadas para preparação de dados são sugeridas por e:
Agregação ­ busca sumarizar os dados em diferentes perspectivas como, por exemplo, combinando dois ou mais objetos num único objeto, reduzindo o escopo a ser minerado;
Amostragem ­ visa selecionar um determinado subconjunto dos dados que tenham certas características, visando também reduzir o escopo da mineração.
Redução de Dimensionalidade ­ busca reduzir o número de atributos (colunas) de um conjunto de dados, através da eliminação de atributos irrelevantes ou redundantes.
Esta técnica é um caso particular de seleção de atributos, a qual será apresentada a seguir;
Seleção de Atributos ­ assim como a redução de dimensionalidade, essa técnica visa eliminar atributos.
Nem sempre os atributos eliminados são irrelevantes, mas o subconjunto selecionado deve ser tão representativo quanto seriam os dados originais.
A necessidade de selecionar atributos está associada ao desempenho dos algoritmos de mineração, uma vez que os mesmos apresentam desempenho melhor, em termos de velocidade de execução e interpretabilidade dos modelos, se a dimensão dos dados é menor.
A seleção de atributos pode ser realizada de três formas:
Experiência sobre os dados:
Se o usuário conhece bem os dados que está preparando, esse pode decidir quais atributos do conjunto podem ser eliminados.
Método Filter: Consiste na seleção dos atributos por alguma abordagem independente do algoritmo de mineração.
Por exemplo, através da construção de uma matriz de correlação de atributos;
Assim, se dois atributos forem altamente correlacionados um de eles pode ser eliminado.
Método Wrapper: Este método usa o próprio algoritmo de mineração para encontrar o melhor subconjunto de atributos.
Criação de Atributos ­ permite que, baseado em valores de outros atributos já existentes, seja possível criar outro atributo num escopo menor;
Categorização ­ permite transformar atributos contínuos (numéricos) em atributos discretos (categorias).
A necessidade de categorizar atributos surge quando se trabalha com a técnica de classificação, pois esta só permite atributo classe categórico.
A transformação de um atributo contínuo em categórico envolve duas etapas:
Decidir o número de categorias que o atributo terá e, mapear os valores contínuos para essas categorias.
Em o primeiro passo, após os valores contínuos terem sido ordenados, estes são divididos em n intervalos, determinando n-1 pontos de divisão.
Em um segundo momento, todos os valores pertencentes a um determinado intervalo são mapeados para a mesma categoria.
A dificuldade em categorizar atributos está em decidir o número de intervalos e os limites de eles.
O resultado da categorização é um conjunto intervalos.
Transformação de Atributos ­ busca aplicar alguma regra que seja inferida sobre os valores de um dado atributo como, por exemplo, normalizar uma escala de valores.
Todas as técnicas descritas são usadas quando os dados são extraídos diretamente a partir de DW.
Em esta situação os dados já estão previamente adequados para serem armazenados no DW, pois estes passam por um processo de Etc, e, assim, precisam apenas ser adequados de acordo com as necessidades do algoritmo de mineração.
Porém, para realizar mineração, muitas vezes apenas os dados disponíveis no DW não são suficientes.
Desta forma, é necessário recorrer a fontes de dados originais, tais como bases de dados das ferramentas de gestão de projetos.
Em essas bases, os dados ainda são brutos;
Em este caso, para que estes sejam usados na mineração é interessante que os mesmos passem por a etapa de Etc, para que adquiram o mesmo nível de qualidade dos dados armazenados no DW.
A mineração de dados é a etapa mais importante do processo de KDD.
Ela consiste na aplicação de algoritmos para extrair informações úteis e desconhecidas, a partir de grandes repositórios de dados. Segundo
e as técnicas de mineração são divididas em duas categorias:
Descritivas e preditivas.
As descritivas objetivam derivar padrões (correlações, tendências, grupos e anomalias) que sumarizam o entendimento sobre os dados, como exemplo desse tipo de técnica tem- se:
Em este trabalho é abordada a utilização de classificação como técnica preditiva.
Para tanto, a seguir são apresentadas as particularidades desta, bem como os algoritmos e critérios de avaliação dessa técnica.
A classificação é a tarefa de atribuir objetos a uma entre várias categorias prédefinidas.
Através da classificação é possível analisar dados e extrair modelos que descrevem classes ou predizem tendências futuras nos dados.
Segundo a classificação é a tarefa de apreender uma função alvo f, a qual mapeia cada atributo de um conjunto X para uma classe pré-definida y.
Os atributos do conjunto X são denominados de explanatórios e o atributo y é chamado de atributo classe.
A função alvo mencionada é conhecida como modelo de classificação.
A tarefa de classificação é executada em dois passos.
Em o primeiro, uma porção dos dados, denominada de conjunto de treino, é usada para construir o modelo preditivo.
Em este conjunto o atributo classe é conhecido para todos os registros.
Em seguida, após o modelo ter sido estabelecido, este é testado com um outro conjunto de dados, denominado conjunto de teste.
Em este último, os registros têm o atributo classe desconhecido.
É através do conjunto de teste que a capacidade de generalização do modelo é avaliada, ou seja, o quanto do total de registros de teste foi previsto corretamente.
Diversas técnicas são sugeridas para estabelecer modelos de classificação, tais como árvore de decisão, redes neurais e redes bayesianas.
Este trabalho aborda o uso da classificação através de um algoritmo de árvore de decisão.
Em a seqüência, são apresentadas as principais características desta técnica de classificação.
A árvore de decisão é uma técnica muito utilizada em problemas de classificação.
Entre as principais características que a tornam bastante difundida está a facilidade de interpretação dos modelos gerados.
A árvore de decisão é um gráfico de fluxo em estrutura de árvore.
Cada nodo da árvore representa um teste a ser realizado e as arestas definem um caminho para cada resposta desse teste.
Os nodos folha representam as classes.
Os algoritmos de aprendizagem desta técnica adotam a abordagem de divisão e conquista.
Assim, partindo de um nodo raiz a árvore é construída recursivamente, dividindo o conjunto de treino em subconjuntos, sucessivamente, de acordo com um critério de divisão, até que cada sub-árvore chegue a um nodo folha.
O critério de divisão do conjunto é muito importante no processo de construção da árvore, pois determina o próximo nodo da árvore, se será um nodo interno ou folha.
Existem vários critérios de divisão em algoritmos de árvore de decisão;
Porém este assunto não é tratado neste trabalho.
Entre os principais algoritmos de árvore de decisão estão:
Hunt, ID3, C4.
5 e o CART (Classification And Regression Trees).
A principal vantagem de utilizar algoritmos de árvore de decisão está relacionada à facilidade de interpretação dos modelos gerados, uma vez que os mesmos podem ser expressos em regras do tipo Se-Então, facilitando o entendimento por parte de os usuários.
A avaliação dos modelos de classificação resultantes é fundamental para garantir a credibilidade da etapa de mineração.
Segundo os modelos preditivos, obtidos a partir de algoritmos de classificação, podem ser avaliados e comparados de acordo com os seguintes critérios:
Acurácia da Predição:
Este quesito avalia a habilidade do modelo em predizer as classes alvos de novos registros, ou seja, daqueles que não foram usados para gerar o modelo. $=
1 ou atributo classe $= 0).
Os verdadeiros positivos (VPo) e os verdadeiros negativos (VN) representam os valores preditos corretamente.
Já os falsos positivos (FP) e os falsos negativos (FN) representam os valores preditos erroneamente.
Velocidade: Avalia o custo computacional envolvido na generalização e uso do modelo.
Robustez: Habilidade do modelo em fazer predições corretamente perante dados faltantes ou com ruídos.
Escalabilidade: Habilidade de construir um modelo eficiente dado um grande conjunto de dados.
Interpretabilidade: Este item é referente a o nível de entendimento e discernimento fornecidos por o modelo em relação a o conhecimento descoberto.
De entre as cinco métricas apresentadas, a acurácia e a interpretabilidade são tomadas como referências para avaliação dos modelos de classificação estabelecidos no capítulo 5, visto que as mesmas revelam o nível de qualidade do modelo resultante e o quanto o mesmo é de fácil interpretação por parte de os usuários.
O pós-processamento compreende as etapas de avaliação e apresentação do conhecimento extraído.
Através da avaliação, o usuário deve reconhecer se os padrões extraídos com a mineração representam conhecimento útil para o negócio.
Ela deve ser realizada por intermédio de uma das métricas definidas na seção 2.4.2.3.
Já a apresentação é a forma como o conhecimento obtido é mostrado para o usuário;
Sugere as seguintes formas de apresentação:
Regras, tabelas, gráficos ou árvores de decisão.
Este capítulo descreve o cenário onde a pesquisa está inserida:
Como predizer métricas de software com o uso de mineração de dados.
Para tanto, é apresentada:
A problemática a ser tratada;
O ambiente SPDW+, o qual é tomado como referência para a proposta;
O cenário real da operação parceira e a caracterização da contribuição.
São relatados os aspectos particulares a cada um desses itens, buscando estabelecer como os mesmos se relacionam neste trabalho.
As empresas de Ti devem oferecer produtos e serviços, conforme os prazos e custos estabelecidos com seus clientes, para se manterem competitivas no mercado.
Por essas razões, estimativas precisas são essenciais para que essas empresas consigam executar um planejamento dos seus projetos o mais próximo possível do efetivamente realizável, além de estabelecer o entendimento quantitativo dos dados de seus processos.
O modelo de maturidade CMMI aborda esta necessidade, e define os requisitos para alcançar essas exigências.
O CMMI nível 2 estabelece a necessidade de um PM para quantificar a qualidade do PDS.
Já o nível 3 requer um repositório para armazenar de forma concisa e organizada as métricas organizacionais, permitindo uma visão unificada e comparável entre os diferentes projetos.
Desta forma, a contínua manutenção da qualidade dos processos é alcançada através da mensuração dos mesmos.
Com base nas métricas definidas e armazenadas num repositório, é possível manter um histórico da qualidade dos projetos, bem como realizar a análise, predição e monitoração do seu PDS.
Contudo, apenas mensurar não é o bastante.
As organizações necessitam obter informações de seus processos de forma rápida, para que assim, possam tomar ações corretivas e impedir falhas nos mesmos.
A análise consiste no entendimento do histórico de métricas através do uso de técnicas estatísticas, gráficos, acompanhamento de indicadores de qualidade e técnicas de mineração.
Por exemplo, analisar os indicadores de retrabalho para corrigir defeitos, tentar encontrar as causas destes, e atuar sobre cada uma de elas, visando melhorar os processos do projeto.
A monitoração, por sua vez, tem por objetivo oferecer informações do andamento dos projetos em relação a o que foi inicialmente planejado, a partir de um acompanhamento regular do planejamento e buscando detectar desvios significativos.
Esse acompanhamento pode ser efetuado por intermédio de técnicas específicas de monitoração como, por exemplo, EVA (Earned Value Analysis).
A monitoração deve oferecer, também, informações momentâneas e atualizadas, que efetivamente auxiliem na tomada de decisão, permitindo que os problemas possam ser detectados tão logo apareçam, e ações corretivas possam ser executadas, no momento certo.
Por exemplo, quando uma determinada tarefa encontra- se em atraso e a sua finalização é prérequisito para o início de outras tarefas, o gestor pode optar por alocar mais recursos nesta tarefa atrasada, ou transferir- la para outro recurso mais qualificado e/ ou disponível, para que a mesma não cause impacto nas demais e não ocasione um atraso do prazo de entrega e aumento do orçamento, ou, pelo menos minimizar esse impacto.
A previsão também está relacionada com o estudo de dados passados.
Porém, ela objetiva a realização de estimativas confiáveis e próximas de valores reais, bem como na detecção de desalinhos de objetivos, além de a descoberta da causa raiz de falhas dos produtos (desatenção do colaborador, falha de lógica, falta de conhecimento, etc).
Os resultados da previsão podem ser oferecidos através de modelos preditivos, produzidos por técnicas de mineração sobre métricas de software.
De essa forma, os gestores podem se beneficiar do histórico de mensurações para melhorar suas estimativas, e conseguir desempenhar um gerenciamento quantitativo de melhor qualidade, o que constitui o principal objetivo do nível 4 do CMMI.
Por exemplo, com base nas características de defeitos como:
Severidade, causa raiz, fase de origem (análise, projeto, implementação, etc) e tamanho do código, é possível estabelecer através de técnicas preditivas o esforço necessário para corrigir- los.
Assim, a predição pode:
Estabelecer modelos capazes de auxiliar nas estimativas iniciais do projeto;
E apoiar na análise quantitativa de processos.
A predição voltada para o estabelecimento de estimativas mais precisas é útil na fase inicial do projeto quando o gerente deve estipular os objetivos iniciais de referência para o trabalho e quando deseja também prever o esforço para corrigir problemas.
Já a análise quantitativa mostra- se útil durante o andamento do projeto, para, dada uma situação presente, permitir identificar as chances de ocorrerem desalinhos dos objetivos por, por exemplo, probabilidades.
As técnicas de Mineração de dados entram neste contexto como uma abordagem capaz de aumentar a capacidade analítica e preditiva sobre as estimativas e o desempenho quantitativo do PDS, através da proposta de modelos e técnicas voltadas a este fim.
Contudo, como empregar a mineração de dados para predizer métricas?
Isoladamente ou integrada com o repositório organizacional de métricas?
É o processo de KDD diretamente aplicável ou é conveniente adaptar- lo ao contexto de métricas de software?
O que precisa ser feito para que resultados satisfatórios possam ser obtidos?
Este trabalho aborda, justamente, o uso de técnica de classificação para predição de métricas de software.
Para tanto, propõe um processo de KDD que incorpora o ambiente de data warehousing, denominado SPDW+, para previsão de métricas de software.
Em especial, um método de execução do processo de KDD que, em vez de considerar o DW como um passo intermediário deste processo, o toma como ponto de referência para a sua execução.
Em a seção seguinte o SPDW+ é apresentado, destacando suas funcionalidades e limitações.
O SPDW+ é um ambiente de data warehousing que oferece suporte para análise e monitoração da mensuração da qualidade de software, a partir de um Repositório de Dados e de um processo automatizado de Etc das métricas, orientado a serviço.
Contudo, não apresenta nenhum recurso de predição.
A arquitetura do SPDW+, apresentada na Figura 3, está organizada em camada de Integração de Aplicações, de Integração dos Dados, de Apresentação e Repositório de Dados.
A camada de Integração de Aplicações é responsável por a extração automática dos dados diretamente das fontes dos projetos, advindos de diferentes ferramentas utilizadas no ambiente de desenvolvimento de software.
Os dados são carregados num componente denominado DSA, que pertence à Camada de Integração de Dados, onde tais dados são devidamente transformados conforme o padrão organizacional.
Após o DSA, os dados são carregados para o repositório (DW) de maneira incremental, implementado na forma de web service.
Por último, a Camada de Apresentação permite a exibição dos dados armazenados no DW, de acordo com os diferentes perfis de usuário e objetivos de análise.
Em a próxima seção é apresentado em detalhes o repositório de dados, o qual é o cerne desta arquitetura.
Para tanto, é descrito o modelo analítico e o PM suportados por ele.
O repositório de métricas é construído na forma de um DW, definido a partir de um modelo analítico conseqüente dos modelos de processo de desenvolvimento de software e do PM.
É uma base de dados que armazena, de forma unificada e centralizada, os dados de todos os projetos da organização, permitindo análises e monitorações, em diferentes perspectivas, níveis de sumarizações e papéis organizacionais.
A estrutura analítica do repositório suporta modelos de processo de desenvolvimento tipicamente utilizados por grandes organizações, como os ciclos de vida cascata e iterativo.
Desta forma, possibilita que cada projeto contenha uma ou mais versões, sendo que as mesmas podem apresentar um conjunto de iterações ou fases.
As fases contêm atividades classificadas segundo seu tipo:
Trabalho, retrabalho, revisão e qualidade.
Os defeitos são mensurados a partir de as fases e devem ser classificados conforme o seu grau de severidade (e.
g alto, médio ou baixo).
O programa de métricas suportado por o modelo analítico é apresentado na Tabela 4.
Este compreende métricas das seguintes áreas de qualidade:
Tempo, Esforço, Tamanho, Custo, Requisitos e Qualidade.
As métricas de Tempo são responsáveis por determinar o intervalo de tempo em que uma determinada tarefa deve ser realizada.
As de Esforço, Custo e Tamanho apresentam os valores estimados e realizados, num determinado intervalo de tempo, bem como as variações dos mesmos.
A área de qualidade Requisitos engloba as métricas que controlam a variação do número de requisitos inicialmente acordados com os clientes, pois estes podem ser alterados no decorrer de o desenvolvimento do produto.
E, na de Qualidade tem- se as métricas relativas à qualidade do produto, como número de defeitos internos e externos, eficiência de remoção de defeitos e satisfação do cliente.
A monitoração é estabelecida através de métricas específicas (as quais estão destacadas na Tabela 4 por sombreamento), definidas por intermédio da técnica de EVA.
A Figura 4 ilustra o modelo analítico estabelecido segundo os modelos de PDS e o PM.
Ele é organizado segundo um esquema multidimensional do tipo constelação de fato, em a qual são definidos dois tipos de tabelas:
Fato e dimensão.
As tabelas fato armazenam métricas nas seguintes granularidades:
Atividade, versão e defeito.
As tabelas dimensão, por sua vez, armazenam os atributos que qualificam os fatos.
Esse modelo analítico oferece suporte à análise e monitoração.
Uma métrica preditiva é aquela que pode ser usada para prever um atributo ou fator de qualidade do PDS.
Desta forma, considerando o PM do SPDW+ várias métricas podem ser usadas com essa finalidade, desde que alguma técnica voltada para este fim seja aplicada sobre elas, como por exemplo, técnicas de classificação.
Entre as métricas potencialmente úteis para serem preditas tem- se as métricas de Tempo (e.
gduração), Esforço (e.
g esforço original), Tamanho (e.
g tamanho original), Custo (e.
g custo original) e Qualidade (e.
g O SPDW+ apresenta um processo automático de Etc, o qual proporciona a coleta de métricas a partir de diferentes ferramentas e, posterior armazenamento no DW.
Através deste processo e de métricas específicas, o ambiente oferece suporte à análise e a monitoração.
Contudo, o SPDW+ não trata os aspectos relacionados à predição O cenário abordado neste trabalho é de uma empresa de Operação de Software, certificada CMM3, que em breve pretende alcançar certificação CMMI 5.
Esta empresa apresenta um ambiente de Data warehousing implementado e operacional, denominado de Base Organizacional (BO), similar ao proposto em que, por sua vez, é uma evolução de.
Este ambiente é composto por a) Repositório de Métricas;
B) Processo de Carga;
O repositório de métricas é um DW, implementado a partir de um modelo analítico conseqüente do PM e do modelo de PDS da organização.
Ele suporta o programa de métricas organizacional, exigência da certificação CMM3.
A sua estrutura analítica está organizada segundo um esquema multidimensional, do tipo constelação de fatos, onde são armazenadas métricas de seis áreas de qualidade (Esforço, Qualidade, Custo, Cronograma, Tamanho e Requisitos) nas seguintes granularidades:
Versão, fase, iteração, tipo atividade e defeito.
Ele foi desenvolvido para análise;
Porém as métricas são bastante abrangentes podendo ser úteis para a predição.
Apesar de sua abrangência, este apresenta limitações em relação a a granularidade das informações e às perspectivas de análise.
A partir de o mesmo não é possível acompanhar regularmente o desempenho das atividades de maneira individual, ou sumarizar- las por intervalo de data, devido a a consolidação estabelecida ser por tipo atividade e não por atividade, pois os projetos classificam suas atividades em:
Trabalho, retrabalho, revisão e qualidade.
Além disso, as informações de custo são apresentadas, apenas no nível de versões impedindo, por exemplo, a verificação do valor consumido por determinada fase ou iteração, bem como a monitoração do mesmo numa granularidade menor.
O processo de Carga do ambiente é responsável por coletar métricas a partir de cada um dos projetos da organização e consolidar- las no repositório central.
Em esse ambiente as métricas de Cronograma (Tempo) encontram- se armazenadas no MS Project.
Por sua vez, as de Qualidade podem ser capturadas das ferramentas de acompanhamento de defeitos:
Bugzilla, ClearQuest ou Mantis, dependendo do projeto.
Já as métricas referentes ao esforço realizado podem estar armazenadas na base de dados do MS Project, localizada no ambiente do projeto, ou no Banco de Horas, desenvolvido por um dos projetos da operação.
E, por fim, os requisitos e os tamanhos são armazenados em documentos distintos não estruturados (planilhas ou arquivos de texto), variando conforme o projeto.
Desta forma, o processo de carga apresenta uma série de etapas manuais e semiautomatizadas, executadas por pessoas específicas.
Em resumo, o processo como um todo é composto por a execução dos seguintes passos:
Preparação dos dados, homologação dos mesmos, efetivação do processo de carga e homologação dos dados carregados.
A preparação tem o objetivo de capturar as métricas de fontes distintas e organizar- las em arquivos estruturados, conforme um padrão determinado por a organização.
Após serem coletadas, estas devem passar por uma vistoria para verificação de coerência geral do preenchimento e da formatação, a qual denomina- se homologação das fontes de informação, realizada segundo o documento checklist de carga.
Depois que os dados são homologados, eles encontram- se no formato adequado para que possa ser realizado o processo de Etc das métricas.
Esse é automatizado por intermédio de pacotes DTS (Data Transformation Services) do SQL Server 2000, tendo como pré-condição que todos os dados dos projetos estejam coletados, transformados e disponibilizados, segundo o padrão organizacional especificado nos documentos de carga.
Concluída essa etapa, os dados estão devidamente armazenados no repositório central.
Por fim, um novo processo de homologação de dados deve ser realizado, onde ocorre a conferência dos dados apresentados por a Interface de Bi.
A interface de Bi (Business Intelligence) utilizada por a organização consiste num portal web, o qual apresenta as seguintes funcionalidades:
Recursos OLAP (On-Line Analytical Process) para acessar os dados (ii) um painel de indicadores;
recursos para visualização das consultas realizadas por os usuários;
E calendário e integração com correio eletrônico.
A interface OLAP permite apresentação dos dados através de tabelas dinâmicas.
A partir de os dados dessas tabelas, gráficos podem ser gerados e visões sobre os cubos podem ser pré-definidas.
O ambiente da BO permite extração de dados de diferentes fontes e o armazenamento no repositório central, após uma etapa de transformação.
Em o repositório são armazenadas métricas em diferentes granularidades e áreas de qualidade, maiores detalhes são apresentados em.
No entanto, a BO não apresenta recursos nem de monitoração nem de predição, como demonstrado em.
As estimativas iniciais do projeto são realizadas empiricamente, baseadas na experiência do gerente de projeto ou através de exaustivas análises sobre os dados do repositório.
Para tentar facilitar esse processo, num dos projetos da organização foi criada uma planilha eletrônica onde são inseridos dados das demandas do cliente.
Esta tem sido usada para agilizar o processo de aprazamento de versões e atividades.
Porém, os critérios presentes em ela para estabelecer a estimativa de esforço estão defasados, causando grandes erros e forçando a correção dos mesmos por a experiência do gerente.
Desta forma, a organização tem grande interesse em dispor de recursos ou técnicas que possam dar suporte à predição de seus dados de maneira rápida e o mais confiável possível e, assim, contribuir para o alcance de níveis mais altos de maturidade.
Diante de a problemática apresentada, este trabalho tem o objetivo de estabelecer um processo de KDD, voltado para o estabelecimento de predição de métricas de software.
Para tanto, define um método de execução deste processo, denominado de SPDW-Miner.
Este método estabelece uma série de etapas que guiam a execução do processo de KDD.
O SPWDMiner incorpora o ambiente de data warehousing SPDW+ e, emprega seu DW como ponto de referência para a execução de todo o processo de KDD.
Através do conceito de DW, apresentado no capítulo 2, é possível identificar a sua representatividade no ambiente organizacional.
O fato de ele armazenar dados de maneira integrada e baseados em assuntos, mostra que a sua concepção é um tarefa onerosa e complexa, conforme apresentado em e constatado no ambiente da organização parceira.
Em a passagem de dados de um ambiente operacional (por exemplo, ambiente de um projeto de software) para o DW, geralmente ocorre uma série de transformações sobre os dados.
Essas transformações visam adequar- los de acordo com os padrões organizacionais, além de corrigir prováveis inconsistências.
Os dados armazenados no DW podem ser considerados de alta qualidade e dotados de inteligência a respeito de o negócio.
O uso de repositórios deste tipo é uma tendência em organizações certificadas CMM e CMMI, como apresentado em, e.
O processo de KDD proposto é apresentado na Figura 5, ele prevê a interação entre as seguintes entidades:
Bases de Dados de Origem e Data warehousing, Preparação de Dados, Mineração de Dados, e Avaliação de Resultados.
A seguir cada uma de elas é detalhada. --
Bases de Dados de Origem e Data warehousing O DW fornece uma visão dos dados disponíveis sobre os projetos, permitindo identificar quais as métricas que podem ser úteis para estabelecer as estimativas.
Em este tipo de repositório, o nível de detalhamento das informações auxilia a tomada de decisões estratégicas.
Mas, em alguns casos, essa granularidade não é a adequada para a mineração.
Com isso, às vezes, torna- se necessário buscar em outras fontes de dados (bases de dados de origem) informações que possam ser agregadas às informações dispostas no DW e, desta forma, permitindo dispor de informações relevantes para solucionar o problema de mineração desejado.
Em a Figura 5, as setas tracejadas entre a entidade Base de Dados de Origem e a Preparação de Dados representam essa possibilidade, caso os dados do DW não forem suficientes para compor o arquivo de dados:
Então são buscadas mais informações nestas bases.
Essa busca pode ser efetuada durante a efetivação da preparação.
Por exemplo, se em algum momento durante a preparação for percebida a necessidade de mais dados, então existe a possibilidade de retornar tanto ao DW quanto a as bases de origem.
Essa situação é representada por as setas tracejadas entre a etapa de preparação e o DW, e entre a preparação e a Base de Dados de Origem. --
Preparação de Dados A preparação de dados consiste na aplicação de técnicas específicas para adequar os dados de acordo com as exigências do algoritmo de mineração, tomando por referência a padronização já empregada nos dados dispostos no DW.
A preparação deve considerar parâmetros já definidos para evitar que um esforço já realizado anteriormente, no processo de Etc, seja repetido sem necessidade.
Desta forma, as informações do DW servem de parâmetros para a tomada de decisão na etapa de preparação.
A o final da preparação os dados estão prontos para serem minerados. --
Mineração de Dados A mineração de dados, nesta pesquisa, é utilizada para estabelecer modelos capazes de predizer métricas de software.
Em esse sentido, é usado um algoritmo de classificação para, dadas as características do PDS, definir estimativas, prever desalinhos de objetivos, estabelecer causas de defeitos, entre outras métricas.
Para o estabelecimento desses modelos é usada a técnica de classificação, e as razões para a sua escolha encontram- se definidas na seção -- Avaliação de Resultados Em a avaliação os resultados do algoritmo são verificados segundo critérios previamente estabelecidos, tais como:
Acurácia, taxa de erro e interpretabilidade.
Após a interpretação, o usuário pode decidir se o modelo foi satisfatório ou não, podendo retornar à etapa de preparação para realizar mais alguns ajustes ou aplicar alguma técnica para otimizar o resultado obtido.
A avaliação é realizada por intermédio da verificação de algum critério, sendo que o mais usual e recomendado na literatura é a acurácia.
No entanto, a interpretabilidade do modelo obtido é um critério bastante relevante para o usuário, sendo que este apenas se beneficiará do conhecimento extraído se for capaz de interpretar- lo.
Se os resultados obtidos não forem satisfatórios, então o usuário pode retornar à etapa de preparação na tentativa de melhorar a qualidade dos dados, ou ainda, aplicar alguma outra técnica de transformação (essa situação é mostrada através da seta tracejada entre a etapa de Avaliação de Resultados e Preparação de Dados).
Este capítulo descreveu a problemática do cenário, apresentando como exemplo dois ambientes onde ela é constatada, o SPDW+ e o ambiente real da organização parceira.
Para tanto, apresentou particularidades de cada um de eles e a contribuição deste trabalho.
Esta última foi apresentada através do processo de KDD proposto para o estabelecimento de predição de métricas de software e, do método que estabelece todas as etapas de execução deste processo.
Em o capítulo 4 é apresentado em detalhes o método SPWD- Miner.
Vários autores de trabalhos relevantes no contexto de mineração de dados, como, e abordam e definem o processo de KDD, apresentando questões como a importância da preparação de dados, as técnicas voltadas para este fim, algoritmos de mineração e critérios de avaliação dos resultados.
Porém, nenhum de eles explicita como as etapas do processo devem ser realizadas no nível de detalhe, conforme o abordado em para Etc de um DW.
O processo de KDD proposto por é representado por três grandes etapas:
Pré-processamento, mineração de dados e pósprocessamento.
Já o proposto por e apresenta quatro etapas:
Limpeza e integração;
Seleção e transformação;
Mineração e avaliação dos resultados.
Considera o DW como um recurso intermediário entre as etapas e, onde os dados limpos e integrados são armazenados, para então serem submetidos à etapa.
Os outros dois autores, e, não mencionam a utilização de repositórios intermediários.
Este trabalho sugere um processo de KDD diferente, onde o DW é ponto de referência para a execução das etapas.
A vantagem dessa abordagem está em se beneficiar da qualidade dos dados armazenados no DW e da inteligência em eles incorporada para guiar a eventual busca por dados adicionais.
Outro aspecto relevante neste contexto, é que organizações CMMI que necessitam de recursos de predição, geralmente, apresentam repositórios construídos na forma de um DW, segundo, e Desta forma, este capítulo apresenta o método para realização do processo de KDD, denominado SPDW-Miner, o qual tem por objetivo auxiliar usuários de métricas de software na obtenção de modelos preditivos.
Para tanto, apresenta- se uma seqüência de etapas que, se realizadas coerentemente, podem contribuir para a obtenção de bons resultados na etapa de mineração.
A seguir, é definido o perfil do usuário que se beneficiará com o método, as etapas que o constituem e como estas devem ser realizadas.
O potencial usuário capaz de se beneficiar do método é um profissional da área de qualidade de uma organização de software, o qual possui conhecimentos básicos em Bi e métricas de software.
E utiliza a mineração de dados para predizer informações, as quais são úteis para auxiliar na tomada de decisão sobre o projeto.
Um exemplo de utilização destas é a realização de estimativas de esforço para correção de defeitos, com maior precisão.
A adoção de um perfil conveniente influência o grau de detalhamento na descrição do método.
De acordo com, um método é uma ferramenta para a aquisição e construção do conhecimento.
Este consiste num conjunto de etapas ordenadamente dispostas a serem executadas, e que tem por finalidade a investigação de fenômenos naturais para a obtenção de conhecimento.
E este deve ser objetivo e sistemático para os resultados serem passíveis de reprodução e confirmação.
A seguir são definidas as etapas do método SPDW- Miner.
No decorrer de a definição das mesmas são discutidas as particularidades inerentes ao contexto dos dados e os objetivos a serem alcançados em cada uma de elas.
Os dados utilizados na execução do SPDW- Miner são métricas de software, definidas num programa de métricas padrão de uma operação de software de médio ou grande porte.
Para propor o método tomou- se como referência o modelo já mencionado em para, então, expandir- lo em etapas.
As etapas que compõem o método são:
A. Estabelecer o objetivo de mineração;
B. Conhecer os dados disponíveis;
C. Extrair os dados;
D. Preparar os dados;
E. Adequar os dados para o formato de entrado do algoritmo de mineração;
F. Aplicar o algoritmo de mineração;
G. Verificar e Interpretar os resultados;
H. Otimizar o Modelo Resultante.
As etapas A, B, C, D e E do SPDW- Miner equivalem à etapa de pré-processamento proposta por.
A etapa F corresponde à mineração.
Já as etapas G e H correspondem à etapa de pós-processamento.
A Figura 6 ilustra as oito etapas de execução do SPWD Miner, as quais compreendem todo o processo de KDD, conforme segue.
Primeiramente é definido o objetivo da mineração (etapa A), o qual deve ser atingido ao término do processo de KDD.
Logo após, na etapa B, o usuário se familiariza com os dados disponíveis no DW, e avalia se há, de entre estes, atributos que sirvam para a execução da mineração anteriormente planejada.
Caso os dados disponíveis não sejam suficientes para tal, buscam- se outras bases de dados, dentro de a organização, que possam suprir a carência de informações.
Conhecidos os dados e suas respectivas fontes, o usuário deve extrair- los e consolidar- los em arquivo apropriado (etapa C).
Então, na etapa D, devem ser realizadas uma série de transformações visando adequar- los e corrigir- los para serem usados por algoritmos de mineração.
A etapa E visa formatar os dados de acordo com as exigências da ferramenta de mineração a ser usada.
Em a etapa F os dados são minerados, e na etapa G os resultados obtidos são avaliados.
É na etapa G que o modelo é verificado para identificar se o mesmo é satisfatório ou não.
Se este não foi satisfatório o usuário pode decidir por retornar a uma das etapas anteriores, visando:
Alterar o objetivos de mineração ou categorização do atributo classe (etapa A);
buscar mais informações relevantes (etapa B ou C);
realizar mais algum ajuste nos dados, por exemplo, testar uma nova categorização (etapa D);
realizar alguma formatação exigida por a ferramenta (etapa E);
testar outra configuração do algoritmo ou até mesmo outro algoritmo (etapa F);
Ou otimizar o modelo resultante (etapa H).
A etapa H é realizada apenas como uma tentativa de otimizar os modelos preditivos estabelecidos, não sendo obrigatória a sua execução.
Contudo, se ela é realizada, é necessário retornar à etapa G, para então avaliar o modelo após a otimização.
Vale ressaltar que, antes de efetivamente realizar a mineração, o usuário pode retornar a etapas anteriores sempre que necessário.
Em a seqüência, cada uma das oito etapas são detalhadas.
A. Estabelecer o objetivo de mineração ­ quando se deseja minerar dados, o primeiro passo é estabelecer os objetivos de mineração.
O objetivo de mineração representa o conhecimento que o usuário pretende extrair a partir de os dados disponíveis.
A seguir é apresentada a seqüência de ações que devem ser realizadas nesta etapa:
Horas]',`\&gt; 2 Horas';
Vários critérios de avaliação de modelos preditivos, já discutidos na seção 2.4.2.3, o usuário deve definir qual métrica usar para avaliar os modelos e quais os limites de valores aceitáveis desta.
Este método sugere o uso da métrica de acurácia, por a sua facilidade de interpretação.
Outra métrica que deve ser considerada na avaliação dos resultados é o nível de facilidade de interpretação dos modelos resultantes, sendo este caracterizado por o número de nodos ou níveis da árvore.
Pode se ter, como exemplo, uma árvore de decisão que possua uma acurácia 75% e tenha no máximo 50 nodos, e esta pode ser facilmente interpretada por o usuário.
Em o final desta etapa o usuário deve ter estabelecido o objetivo de mineração, o atributo classe, o domínio deste e os critérios de aceitação dos modelos.
Após, na etapa B, o usuário deve identificar quais dados são relevantes, os quais podem ser usados como atributos explanatórios.
B. Conhecer os dados disponíveis ­ Diante de a variedade de fontes de armazenamento, muitas vezes é necessário extrair dados de mais de uma fonte para conseguir reunir todos os dados que são úteis para alcançar os objetivos de mineração.
Assim, a etapa de conhecer os dados disponíveis consiste na execução dos seguintes passos:
A o final desta etapa, o usuário deve conhecer o conjunto de métricas disponível e o local onde estas estão armazenadas, além de as características de cada métrica, as quais passam a ser os atributos explanatórios da classificação.
C. Extração e consolidação dos dados ­ se for necessário buscar dados de outras fontes, que não sejam o DW, é interessante executar alguns dos passos de transformação sugeridos por, para adequar- los ao padrão organizacional, sempre tendo em mente as padronizações adotadas no DW.
O processo de extração pode ser realizado através de consultas SQL ou por exportação dos dados a partir de a interface da própria ferramenta de gestão.
Após terem sido extraídos, uma forma conveniente de acomodar os dados é em arquivos do tipo CSV (arquivos separados por vírgula).
Estes arquivos são de fácil manipulação, podendo ser editados como planilha eletrônica ou como arquivo texto.
A série de passos a serem realizados nesta etapa é descrita abaixo:·
Limpar ruído:
I. Identificar os registros considerados ruídos;·
Corrigir dados faltantes i.
Identificar cada atributo que possui algum registro faltante;·
Formatar os valores de acordo com os padrões pré-estabelecidos (DW) i.
Identificar o atributo que necessita ser adequado aos padrões organizacionais;·
Eliminar registros desnecessários i.
Identificar registros desnecessários, tais como duplicados;·
Combinar fontes de dados i.
Verificar a integridade entre as chaves primárias ou realizar a combinação entre atributos não chaves;
Ter especial atenção com as dimensões do modelo OLAP do DW;
Severidade estava numa tabela e o atributo Causa Raiz estava numa segunda tabela, assim os dois tiveram de ser combinados num mesmo arquivo, sem perder a integridade dos registros.
A o final desta etapa, os dados extraídos estão dispostos num arquivo para serem manipulados na etapa de preparação, onde são adaptados de acordo com as exigências do algoritmo de mineração.
D. Preparar os dados ­ essa etapa do método prevê a utilização de várias técnicas sugeridas na literatura, em, e.
Essas são aplicadas de acordo com as necessidades de preparação impostas por os algoritmos e ferramentas de mineração.
Desta forma, as diferentes técnicas são aplicadas interativamente e iterativamente de acordo com as necessidades.
Para tanto, o usuário deve examinar o arquivo de dados para identificar quais técnicas de preparação devem ser aplicadas.
Essas últimas são agrupadas em quatro tipos:·
Agregação: Usada quando for verificada a necessidade de reduzir o escopo a ser minerado;
Assim, com esta técnica, dois ou mais registros são agrupados.
Os atributos numéricos podem ser agregados através da substituição dos valores por a média ou soma dos mesmos, ou até por normalização.
Os atributos categóricos devem ser omitidos ou sumarizados.·
Redução de Dimensionalidade: Realizada quando se tem à disposição muitos atributos para serem usados como atributos explanatórios.·
Criação de atributos:
Usada quando o usuário identifica no contexto dos dados a necessidade de um novo atributo.
A criação do mesmo representa a inserção de mais uma coluna no arquivo CSV.·
Categorização: Usada quando devem ser identificados os atributos contínuos que necessitam ser tratados por o algoritmo de mineração como categóricos.
A definição das categorias pode ser realizada por o usuário ou através do uso de algoritmos de agrupamento;·
Transformação de atributo:
É necessária para alterar algum atributo de acordo com as exigências do algoritmo de mineração.
E após o processo de transformação, passar a ter outra categorização:(` pequeno',` médio 'e` grande').
Função; Ii.
Selecionar- lo e consolidar- lo num novo arquivo de dados.
Em o término da etapa de preparação de dados, estes estarão prontos para serem submetidos ao algoritmo de mineração.
Isso não garante que os mesmos posteriormente não necessitem retornar à etapa de preparação para serem novamente ajustados.
As etapas do método de preparação são iterativas e interativas, como os passos do processo de KDD.
E. Adequar os dados para o formato de entrada do algoritmo de mineração ­ geralmente as ferramentas de mineração necessitam de formatos próprios de entrada de dados para seus algoritmos, fato que garante que os dados estão tratados e perfeitamente adequados para serem minerados.
Desta forma, o usuário deve seguir os seguintes passos:
F. Aplicar o algoritmo de mineração -- após os dados terem sido preparados e estarem dispostos no formato de entrada, o próximo passo então é executar o algoritmo de mineração desejado.
J. 48, ajustar se o algoritmo deve realizar a poda da árvore e o tipo de validação do modelo (e.
g conjunto de treino e conjunto de teste ou validação cruzada);
G. Verificar e Interpretar os resultados ­ a verificação dos modelos deve ser realizada por intermédio do critério de aceitação definido na etapa A. Se o critério for satisfeito, então o modelo deve ser interpretado e entendido por o usuário, para que então este último possa se beneficiar do conhecimento extraído.
Cada algoritmo tem uma forma de representar os seus resultados:
Por exemplo, os algoritmos de árvore de decisão podem ser avaliados segundo a acurácia e a interpretabilidade da árvore obtida.
Se os resultados não forem os esperados, o usuário deve decidir se retorna a alguma etapa anterior para fazer mais algum ajuste nos dados, ou se tenta executar a etapa H (otimização).
Se o critério apresentar um valor aceitável, pode- se passar para a interpretação do modelo obtido.
Para compreender o conhecimento representado por uma árvore, o usuário deve identificar cada um dos seus nodos, e reconhecer o que cada um de eles representa.
Então, deve- se percorrer a estrutura da árvore para entender o conhecimento representado.
H. Otimizar o Modelo Resultante ­ quando o resultado do algoritmo de classificação não é o desejado, uma alternativa que pode garantir a melhora deste é a eliminação dos registros que foram classificados erroneamente.
Por exemplo, se a acurácia do modelo obtido for muito baixa em relação a o critério definido, o usuário pode eliminar os registros errôneos e aplicar o algoritmo de mineração novamente.
Em é sugerido que este processo de reaprendizado do algoritmo sobre o novo subconjunto de dados deva ser realizado até os resultados serem os desejados.
Retirar do arquivo de dados os registros classificados erroneamente;
Retornar à etapa F. O SPDW é um ambiente de Data Warehousing para apoiar o Programa de Métricas da Hp EAS Brasil, desenvolvido por o projeto de parceria entre o Programa de PósGraduação de Ciência da Computação da PUCRS (PPGCC-PUCRS) e a Hp EAS Brasil, durante o seu processo de certificação CMM3.
A Figura 3 ilustra a arquitetura do SPDW+, organizada em camadas distintas:
Camada de Integração das Aplicações (Application Integration Component), Camada de Integração dos Dados (Data Integration Component) e Camada dos Componentes de Apresentação (Presentation Components).
Este capítulo apresenta a aplicabilidade do método, SPDW- Miner, usando como cenário a operação de software parceira.
Para tanto, são apresentados vários experimentos, todos com o mesmo objetivo de mineração, porém, abordando a aplicação de diferentes técnicas de preparação sobre as métricas para mostrar a versatilidade e consistência do método.
Para tanto, a seguir são propostos o objetivo dos experimentos;
A apresentação dos mesmos, seguindo a seqüência de execução das etapas do método;
Os resultados obtidos e análise dos mesmos;
E considerações sobre o estudo de caso.
Os experimentos têm o objetivo de mostrar uma aplicabilidade do método SPDWMiner, e avaliar a sua contribuição com a etapa de mineração do processo de KDD.
Para tanto, é usado o cenário da organização parceira para testar- lo e, então, averiguar os resultados.
O cenário real engloba um programa de métricas amplo e um DW, conforme já apresentado estimativas de esforço para correção de defeito (retrabalho), por ser este de interesse da empresa parceira e, também, por representar uma necessidade real, já que prever corretamente essa métrica representa manter a credibilidade junto aos clientes.
A seguir é a apresentada a seqüência de experimentos executados.
Estes foram realizados seguindo as etapas do SPDW- Miner.
Todos os experimentos seguem o mesmo objetivo de mineração;
Contudo se diferenciam por as técnicas de preparação aplicadas.
Assim, as etapas A, B, C, E e F do método são comuns a todos os experimentos.
As etapas D, G e H são apresentadas na descrição de cada experimento, por apresentarem particularidades para cada um de eles.
Em a seqüência são definidas as etapas A, B C, E e F. Posteriormente, são apresentados experimentos e, para cada um, é mostrado como a etapa D foi realizada.
A etapa G que consiste na verificação e interpretação dos resultados é apresentada na discussão dos resultados, e a etapa H é apresentada nos experimentos em que ela foi utilizada. --
A. Estabelecer os objetivos de mineração ­ o objetivo de mineração é estabelecer estimativas de esforço para correção de defeito (retrabalho).
O atributo classe para atingir esse objetivo é a métrica de esforço de retrabalho.
Inicialmente o domínio do atributo classe foi estabelecido através da categorização em 10 faixas de valores, por intermédio da ferramenta de mineração.
Porém, no decorrer de a apresentação dos experimentos são apresentadas outras formas de categorização testadas.
Para verificar a aceitação dos modelos preditivos foi utilizado o critério da acurácia e interpretabilidade dos mesmos.
A acurácia acima de 70 foi estabelecida como aceitável e a interpretabilidade será considerada através do número de nodos da árvore resultante, se a árvore tiver até 50 nodos é considerada interpretável por o usuário. --
B. Conhecer os dados disponíveis ­ a partir de o objetivo de mineração procurou- se no DW métricas que fossem interessantes para compor o modelo de estimativa de retrabalho.
Porém, métricas que fossem de interessantes para compor o modelo de estimativa de retrabalho.
Porém, percebeu- se que os dados de defeitos estavam muito sumarizados, apresentando as informações de defeitos apenas na granularidade de versão, tais como NDI, NDE, ERD, DDE, DDI e EVR.
Essas métricas revelam muitas informações sobre a qualidade de uma versão.
No entanto, o DW não dispõe de informações sobre o esforço necessário para corrigir um determinado defeito.
Conforme já apresentado na seção 3.3.1, em ele não constam informações no nível de Atividade, apenas na granularidade de Tipo Atividade.
Assim, não é possível através do mesmo estabelecer o esforço para corrigir um defeito, apenas consegue- se extrair o esforço total de retrabalho despendido em versão do software.
Desta forma, para atingir o objetivo de mineração estabelecido foi necessário agregar mais informações.
Para tanto, buscou- se diretamente na base de dados do ClearQuest, ferramenta de acompanhamento de defeitos de um dos projetos da organização.
As informações do DW serviram como parâmetros para a busca, tais como a informação a ser procurada, o tipo desta e a granularidade da mesma.
A base de dados do ClearQuest armazena nas suas tabelas, entre outras informações, os seguintes dados:
Nome do projeto, nome da versão, fase de origem do defeito, severidade, tipo do defeito (interno e externo) e quantidade.
A Figura 7 ilustra o resultado de uma consulta contendo:
O nome do projeto, a versão, o identificador do defeito, o seu tipo, a fase de origem (fases do ciclo de vida do projeto) e a severidade. --
C. Extrair os dados ­ os dados da ferramenta de acompanhamento de defeitos foram extraídos com consulta SQL.
Já os do DW foram exportados através da ferramenta de Bi.
Após a extração os dados foram acomodados num arquivo CSV, para então passarem por a etapa de transformação sugerida por.
Um trecho desse arquivo bem como uma breve descrição de seu layout encontram- se no Apêndice A. De acordo com as necessidades dos dados as seguintes etapas foram realizadas:
Corrigir Dados Faltantes:
Os dados faltantes foram corrigidos através do uso do filtro ReplaceMissingValues, disponíveis na ferramenta Weka;
Eliminar Registros Desnecessários: Os registros duplicados foram identificados e removidos manualmente do arquivo de dados;
Combinar Fontes de Dados: A integridade dos dados foi observada, considerando a granularidade do DW, assim apenas os dados de mesma granularidade foram extraídos e consolidados no arquivo CSV. --
D. Preparar os Dados ­ esta etapa se diferenciou para cada um dos experimentos, e será apresentada na descrição dos mesmos. --
E. Adequar os dados para o formato de entrada do algoritmo de mineração ­ para a mineração foi utilizada a ferramenta open source, Weka 3.5, desenvolvida por a Universidade de Waikato, na Nova Zelândia.
O Weka recomenda a inserção de um cabeçalho, onde devem constar todos os atributos explanatórios e o atributo classe com os seus respectivos tipos.
Outra recomendação é que todos os registros (colunas) estejam separados por vírgula.
O arquivo deve ser salvo com a extensão.
Arff. -- F. Aplicar o algoritmo de mineração -- no Weka é selecionada a técnica de classificação e o algoritmo J. 48.
Os parâmetros iniciais dos algoritmos foram mantidos padrão como especificado na ferramenta, ou seja, ativada a função de poda da árvore e usando a validação cruzada. --
G. Verificar e Interpretar os resultados ­ a verificação dos modelos é apresentada na discussão dos resultados dos experimentos. --
H. Otimizar o Modelo Resultante ­ esta etapa, quando utilizada, é apresentada na descrição do experimento.
Em as próximas seções são apresentadas as diferentes configurações dos experimentos realizados.
Cada um se caracteriza por a aplicação de diferentes técnicas de preparação, conforme sugeridas na etapa D do SPDW- Miner.
Para facilitar o entendimento da seqüência de execuções dos mesmos, eles são agrupados de acordo com a técnica de preparação aplicada no atributo classe.
Para cada um de eles é definido o volume de dados utilizado, os atributos explanatórios, o atributo classe definido, a técnica de preparação aplicada e a acurácia do modelo obtido.
Em o final de cada grupo de experimentos são discutidos os resultados, considerando os critérios definidos na etapa A. Em os experimentos 1 e 2 é realizada a categorização do atributo classe em 10 faixas de valores.
Para tanto, seguiu- se os passos estabelecidos por o SPDW- Miner para categorização.
As faixas foram definidas através de um filtro de preparação de dados, denominado de Discretize, presente na ferramenta Weka.
As faixas definidas por o filtro são:']
inf-9. 322]',`] 9.32218.627]',`.
Esse filtro divide o atributo contínuo em intervalos de mesmo tamanho.
A seguir são definidos esses dois experimentos em maiores detalhes.
Experimento 1 o Volume de dados:
9280 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 2 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Causa Raiz com a antiga nomenclatura foi eliminada.
O especialista definiu, também, que os registros que apresentavam o atributo Causa Raiz $ ` Inc_ corrigida_ por_ DI' representavam registros duplicados, logo haviam de ser removidos, restando 8986 registros no arquivo de dados.
Essas duas transformações foram mantidas em todos os demais experimentos.
O atributo classe foi categorizado em 10 categorias:(`] inf-9.
322]',`] 9.322-18.627]', o Acurácia do Modelo:
93,0825%.
Apesar de a acurácia dos modelos resultantes ser alta, os mesmos não foram satisfatórios.
A categorização realizada concentrou um grande número de registros numa única categoria, conforme ilustra a Figura 8.
Os modelos ficaram tendenciosos, classificando a maioria dos registros nessa faixa de valor.
Verificou- se também que os modelos foram construídos em função de o atributo Número de Colaboradores, como ilustra a Figura 9.
Para tentar melhorar os resultados, os próximos experimentos abordam outras formas de categorização do atributo classe e outras técnicas de preparação de dados.
Em os experimentos 3 a 8 o atributo classe foi categorizado através do uso do algoritmo de Agrupamento K--Means, disponível no Weka.
O algoritmo categorizou os dados em 13 faixas distintas:
A seguir são detalhados os experimentos realizados com essa categorização.
Experimento 3 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Com a nova categorização a distribuição de registro por valor do atributo classe ficou mais uniforme.
Contudo, a acurácia ficou muito baixa.
Através da análise da matriz de confusão, apresentada na Figura 10, percebe- se que os maiores erros estavam em posições próximas da diagonal principal, indicando que o erro do classificador foi em relação a a categoria imediatamente inferior ou superior a categoria correta.
Por exemplo, na primeira linha da matriz os acertos do classificador são 648, porém na posição ao lado é onde se encontra o maior erro.
Este fato indica que a categorização do atributo classe não foi adequada.
Outro fato observado é que as regras são sempre estabelecidas em função de o atributo Número de Colaboradores, como ilustra a Figura 9.
Experimento 4 o Volume de dados:
8986 registros.
Experimento 5 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Número de Colaboradores sofreu uma transformação, passando a ter uma nova categorização.
A Figura 11 apresenta a distribuição de valores para esse atributo.
Pode- se perceber que as categorias acima de seis colaboradores têm uma baixa concentração de valores.
Desta forma, esses valores foram reorganizados, passando a integrar a nova categoria,` Mais de 5 colaboradores'.
Assim, as categorias do atributo Número de Colaboradores são:
Para tanto, foi aplicada a técnica de criação de atributo para comportar a nova categorização deste atributo.
Experimento 6 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Número de Colaboradores sofreu uma categorização, passando a ter 6 categorias:
O atributo Tamanho, do tipo contínuo, também foi categorizado.
Todos os registros que tinham Tamanho $ ` 94.34 PF 'foram enquadrados na categoria` P'.
E os registros de Tamanho\&gt;` 94.34 PF 'em a` G'.
Essa categorização foi adotada levando em consideração a distribuição dos dados, ou seja, tentou- se obter duas categorias uniformes.
Experimento 7 o Volume de dados:
7379 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Número de Colaboradores sofreu uma categorização, passando a ter 6 categorias:
Visando melhorar a acurácia optou- se por aplicar a técnica de amostragem.
Com esta técnica foram selecionados apenas os registros onde atributo Tipo de Defeito $ ` Interno'.
Após a amostragem foi realizada a seleção de atributo para eliminar o Tipo de Defeito, pois os registros resultantes da amostragem possuem o atributo Tipo de Defeito $ ` Interno'.
Experimento 8 o Volume de dados:
1607 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Número de Colaboradores sofreu uma categorização, passando a ter 6 categorias:
Outra tentativa de preparação de dados foi realizada para melhorar a acurácia:
Foi aplicada a técnica de amostragem, e então foram selecionados apenas os registros para os quais o atributo Tipo de Defeito $ ` Externo'.
Após a amostragem foi realizada a seleção de atributos para eliminar o Tipo de Defeito, pois os registros resultantes da amostragem possuem o atributo Tipo de Defeito $ ` Externo'.
Os resultados dos experimentos mostraram que a categorização através da técnica de Agrupamento não melhorou a acurácia dos modelos.
Pode- se perceber que o atributo Número de Colaboradores contribui para a melhora da acurácia.
A categorização deste último num número menor de faixas de valores facilitou a interpretação dos resultados.
A técnica de amostragem não contribuiu significativamente para a melhora da acurácia.
A categorização do atributo Tamanho ajudou a melhorar a interpretabilidade do modelo preditivo resultante, porém a acurácia não teve melhora significativa.
As Figuras 12, 13 e 14 apresentam trechos dos modelos preditivos obtidos.
Percebe- se que o atributo Número de Colaboradores, sempre que presente entre os atributos explanatórios, aparece como o atributo raiz da árvore.
Em os experimentos 9 ao 14 optou- se por reduzir o número de categorias do atributo classe na tentativa de melhorar a qualidade dos modelos.
A partir de a categorização em 13 faixas foram definidas as novas 9 faixas:
Essas foram estabelecidas tentando manter a uniformidade na distribuição dos valores em cada faixa.
A seguir os experimentos são mostrados.
Estes seguem a mesma configuração dos experimentos 3 a 8, para que, desta forma, possam ser comparados.
Experimento 9 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 10 o Volume de dados:
8986 registros.
Colaboradores no modelo resultante do experimento 9, foi realizada uma seleção de atributo, onde este foi retirado do conjunto de atributos explanatórios.
Experimento 11 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 12 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Essa categorização foi adotada levando em consideração a distribuição dos dados, ou seja, tentou- se obter duas categorias uniformes.
Experimento 13 o Volume de dados:
7379 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 14 o Volume de dados:
1607 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Os experimentos 9 a 14 mostraram que a redução do número de categorias do atributo classe contribuiu para a melhoria dos resultados da acurácia, em relação a os experimentos anteriores.
O atributo Número de Usuário mais uma vez se mostrou importante para o resultado.
A amostragem não melhorou significativamente os resultados.
As Figuras 15, 16 e 17 ilustram trechos dos modelos preditivos obtidos nestes experimentos.
Os experimentos anteriores mostraram que a redução do número de categorias do atributo classe melhorou a acurácia dos modelos.
Desta forma, os próximos experimentos são realizados categorizando o atributo classe em apenas quatro faixas de valores.
Estas foram escolhidas por representarem intervalos de tempo de interesse da organização parceira, ou seja, um turno de trabalho é representado por 4 horas.
A seguir são detalhados os experimentos 15 a 20, usando a categorização definida.
Experimento 15 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 16 o Volume de dados:
8986 registros.
Experimento 17 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
Experimento 18 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
O atributo Tamanho, do tipo contínuo, também foi categorizado.
Todos os registros que tinham Tamanho $ ` 94.34 PF 'são enquadrados na categoria` P'.
E os registros de Tamanho\&gt;` 94.34 PF 'em a` G'.
Essa categorização foi adotada levando em consideração a distribuição dos dados, ou seja, tentou- se obter duas categorias uniformes.
Experimento 19 o Volume de dados:
7379 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
Visando melhorar a acurácia optou- se por aplicar a técnica de amostragem.
Com essa foram selecionados apenas os registros onde atributo Tipo de Defeito $ ` Interno'.
Após a amostragem foi realizada a seleção de atributo para eliminar o atributo Tipo de Defeito, pois os registros resultantes da amostragem são todos do Tipo de Defeito $ ` Interno'.
Experimento 20 o Volume de dados:
1607 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
Visando melhorar a acurácia optou- se por aplicar a técnica de amostragem.
Com essa foram selecionados apenas os registros onde atributo Tipo de Defeito $ ` Externo'.
Após a amostragem foi realizada a seleção de atributo para eliminar o atributo Tipo de Defeito, pois os registros resultantes da amostragem são todos do Tipo de Defeito $ ` Externo'.
A redução do número de categorias conseguiu melhorar os resultados tanto em termo de acurácia quanto em interpretabilidade dos modelos preditivos resultantes.
Contudo, os mesmos ainda não foram satisfatórios de acordo com os critérios definidos na etapa A do método.
As Figuras 19, 20 e 21 mostram trechos dos modelos obtidos.
Os experimentos anteriores mostraram um aumento da acurácia dos modelos preditivos, quando era reduzido o número de categorias do atributo classe.
Contudo, a acurácia diminuía quando o atributo Número de Colaboradores era retirado.
Desta forma, os próximos experimentos, de 21 a 24, são realizados considerando o atributo Número de Colaboradores, este categorizado em faixas:
Experimento 21 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 22 o Volume de dados:
8986 registros o Atributos explanatórios:
Tamanho (categórico), Número de Colaboradores (numérico), Causa Raiz (categórico), Fase de Origem (categórico), Tipo de Defeito (categórico) e Severidade (numérico).
Esse passou a ser categórico, apresentando as seguintes categorias:`
P 'e` G'.
Todos os registros que tinham Tamanho $ ` 94.34 PF 'são enquadrados na categoria` P'.
E os registros de Tamanho\&gt;` 94.34 PF 'em a` G'.
Essa categorização foi adotada levando em consideração a distribuição dos dados, ou seja, tentou- se obter duas categorias uniformes.
Experimento 23 o Volume de dados:
7379 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Experimento 24 o Volume de dados:
1607 registros.
Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Tipo de Defeito $ ` Externo'.
Após a amostragem foi realizada a seleção de atributo para eliminar o atributo Tipo de Defeito, pois os registros resultantes da amostragem são todos do Tipo de Defeito $ ` Externo'.
Através da análise da acurácia dos modelos, pode- se verificar que a redução do número de categorias melhorou os resultados.
Contudo, os modelos ainda não se enquadram nos critérios definidos na etapa A do método.
As Figuras 22 e 23 ilustram trechos dos modelos obtidos.
Através dos experimentos anteriores percebeu- se que a redução do número de categorias do atributo classe é uma alternativa para melhorar a acurácia dos modelos.
Desta forma, nos experimentos a seguir, 25 e 26, é testada a categorização em duas faixas de valores.
Utilizou- se a técnica de amostragem para separar o conjunto de registros em duas partes:
Uma com registros que tinham o esforço de retrabalho de até 4 horas e a outra continha os dados com esforço de retrabalho maior que 4 horas.
A estratégia de dividir os registros por o atributo classe se deve à necessidade de tentar encontrar categorias mais precisas, ou seja, reduzir o escopo do problema.
Experimento 25 o Volume de dados:
5897 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Retrabalho` 4 horas'.
Desta forma, o atributo classe foi categorizado em duas faixas:
Essas duas foram escolhidas por representarem intervalos bem distribuídos de valores.
O atributo Número de Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
Experimento 26 o Volume de dados:
3089 registros o Atributos explanatórios:
Tamanho (numérico), Número de Colaboradores (numérico), Causa Raiz (categórico), Fase de Origem (categórico), Tipo de Defeito (categórico) e Severidade (numérico).
Retrabalho\&gt;` 4 horas'.
Desta forma, categorizou- se o atributo classe duas faixas(`] 4-6]',`\&gt; 6 Horas').
Essas duas foram escolhidas por representarem intervalos bem distribuídos de valores.
O atributo Número de Colaboradores passou por uma categorização, onde foram definidas 6 categorias:(` 1',` 2',` 3',` 4',` 5', e` Mais de 5 colaboradores').
Em os experimentos 25 e 26 foi aplicada a técnica de amostragem, onde os registros foram divididos de acordo com o valor do atributo classe.
Os resultados apresentaram modelos preditivos com acurácia de mais de 70%.
A Figura 24 ilustra trechos dos modelos obtidos.
Porém, com esta abordagem é necessário conhecer de antemão a faixa de esforço a que um determinado registro pertence.
No entanto, o objetivo dos experimentos é justamente estabelecer o esforço para correção de defeito.
Desta forma, não faz sentido utilizar- la, pois os usuários desconhecem esta informação antecipadamente.
A solução adotada foi dividir os registros de acordo com o Número de Colaboradores.
Essa abordagem foi utilizada, pois o atributo Número de Colaboradores tem uma alta correlação com o atributo classe, assim ele foi escolhido para tentar dividir os registros e obter melhor acurácia dos modelos.
Os experimentos 25 e 26 contribuíram para a definição das faixas de valores do atributo classe.
Assim, nos próximos experimentos é adotada esta categorização.
Para tentar obter modelos mais acurados são testadas as categorias definidas nos experimentos 25 e 26, já que a partir de estas obteve- se os resultados mais satisfatórios.
Experimento 27 o Volume de dados:
8986 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores passou por uma categorização, onde foram definidas 6 categorias:
SPDW- Miner para otimizar o modelo preditivo.
Para tanto, aplicou- se o filtro RemoveMisclassified da ferramenta Weka, e executou- se o algoritmo novamente.
O modelo teve 99.1058% e 160 nodos.
Experimento 28 o Volume de dados:
3300 registros.
Defeito (categórico) e Severidade (numérico).
O Atributo Classe:
Esforço de Retrabalho (numérico).
Colaboradores igual a` 1 'ou a` 2' foram usados.
O atributo classe foi categorizado, para tanto adotou- se as categorias definidas nos experimentos 25 e 26:
SPDW- Miner, para otimizar o modelo preditivo obtido.
Para tanto, aplicou- se o filtro RemoveMisclassified da ferramenta Weka, e executou- se o algoritmo novamente.
O modelo teve 99.9115% de acurácia e 17 nodos.
Experimento 29 o Volume de dados:
5897 registros o Atributos explanatórios:
Tamanho (numérico), Número de Colaboradores (numérico), Causa Raiz (categórico), Fase de Origem (categórico), Tipo de Defeito (categórico) e Severidade (numérico).
Colaboradores igual a` 3',` 4',` 5 'e` Mais de 5 colaboradores' foram usados.
O atributo classe foi categorizado, para tanto, adotou- se as categorias definidas nos experimentos 25 e 26:
SPDW- Miner, para otimizar o modelo preditivo obtido.
Para tanto, aplicou- se o filtro RemoveMisclassified da ferramenta Weka, e executou- se o algoritmo novamente.
O modelo teve 98.5084% de acurácia e 146 nodos.
Os experimentos mostraram a abrangência do SPDW-Miner, diante de a diversidade de cenários apresentados.
Várias situações de preparação foram testadas, para mostrar a coerência das etapas do método.
O diferencial do processo de KDD apresentado por o SPDWMiner foi comprovado no ambiente real da operação parceira, onde se considerou o DW como referência para o processo.
Desta forma, a busca por fontes de dados adicionais e a preparação foram parametrizadas por as informações dispostas no DW, facilitando essas duas etapas.
Além disso, a execução de um processo de KDD organizado e conciso permitiu que resultados satisfatórios fossem alcançados na etapa de mineração, a qual é o principal objetivo deste processo.
Este capítulo apresenta os trabalhos, encontrados na literatura, que são relacionados com o tema de pesquisa.
A seguir eles são detalhados e, por fim, é mostrado um comparativo entre eles e a proposta dessa pesquisa.
Este trabalho mostra uma aplicação de mineração em métricas de software, mostrando como o uso de técnicas de mineração pode contribuir para o melhoramento do processo de desenvolvimento.
O objetivo específico deste trabalho é encontrar padrões nos dados, que permitam prever o tempo de reparação de problemas de software (esforço de retrabalho) que aparecem durante um PDS, tais como erro de especificação e erro no código.
Os dados utilizados neste estudo são relativos a projetos de software de uma companhia de Telecomunicações.
Estes dados são coletados em todas as etapas de desenvolvimento de software, a partir de relatórios de problemas, por um software de acompanhamento de problemas (bug-- tracking), o qual é mantido na Intranet da organização.
Os dados são coletados de todos os departamentos da empresa, totalizando mais de 40.000 PR registrados no sistema.
Para a realização da mineração foram utilizadas as técnicas de classificação e associação.
Com a classificação esperava- se encontrar padrões de tempo consumido para corrigir problemas de software.
Com a associação, pretendia- se descobrir correlações entre os atributos de RP, ou seja, encontrar quais valores de um atributo implicam em determinados valores em outro.
Os algoritmos empregados foram:
O C5 para a classificação e o CBA para classificação e associação.
Os resultados obtidos não atenderam diretamente o objetivo de mineração proposto por os autores, que era descobrir o tempo necessário para consertar determinadas classes de problemas.
As regras geradas mostraram conhecimento para a classe de problema, e não diretamente para o atributo classe como foi definido no objetivo de negócio.
Contudo através da interpretação das regras consegue- se estimar o tempo para conserto.
As taxas de erros dos modelos ultrapassaram os limites previstos.
Vários artifícios foram utilizados para tentar minimizar- las, tais como validação cruzada, diferentes tamanhos de conjunto de treinamento, contudo nenhuma destas técnicas trouxe sucesso.
Os autores atribuíram esses resultados a alguns ruídos existentes nos dados.
Desta forma, novamente é ressaltada a importância de uma eficiente preparação de dados para garantir sucesso à mineração.
Os padrões encontrados são úteis para auxiliar líderes de projetos a estimar ou predizer o tempo necessário para resolver um determinado tipo de problema.
E ainda caracterizar certas classes de problemas de software.
Diante de isso, os autores confirmam que a mineração pode contribuir com a Engenharia de Software, uma vez que ela permite que conhecimento novo e útil possa ser encontrado para auxiliar no melhoramento da qualidade de um PDS.
Este trabalho discute como a mineração de dados aplicada à base de dados de Processo de Desenvolvimento de Software pode contribuir para a melhoria da qualidade de desenvolvimento de software de alto risco.
Em este contexto, Khoshgoftaar et al.
Tentam descrever modelos de qualidade de software para sistemas de telecomunicações, que sejam capazes de prever se um determinado módulo de software poderá apresentar falha depois de entregue ao cliente.
Estes sistemas de telecomunicações requerem alta qualidade para que não apresentem falhas, já que são sistemas complexos e, conseqüentemente, de difícil manutenção.
Os modelos de qualidade são construídos a partir de a aplicação de técnicas de classificação nos dados de PDS.
Estes modelos devem ser capazes de prever quais módulos poderão apresentar defeitos futuros.
Isso permite que esforços possam ser concentrados no desenvolvimento destes módulos, buscando melhorar- los e evitando uma possível falha futura.
Os dados utilizados para realizar a mineração foram extraídos de uma grande base de dados, que contém relatórios de problemas e dados de gestão de configuração de softwares.
As métricas usadas dizem respeito a atributos do código fonte de um módulo de software, tais como relacionamento entre procedimentos do código fonte, número de procedimentos, e número de estruturas de controle, além de informações sobre o tempo gasto para realizar uma alteração no código e o número de alterações feitas por determinada pessoa durante sua carreira (sua experiência profissional).
O pré-processamento dos dados foi considerado por os autores como indispensável.
Em a etapa de limpeza dos dados alguns atributos com erro ou com dados faltantes foram eliminados.
As transformações foram necessárias, visando melhorar o modelo preditivo resultante.
Alguns atributos altamente correlacionados foram transformados, para diminuir o número de atributos explanatórios.
As métricas que tinham média dos valores também foram transformadas, passando a ter o valor total, pois a medida de qualidade era calculada em relação a o total de falhas encontradas por o cliente;
Então, os demais atributos também tiveram que considerar valores totais.
Foi utilizado o algoritmo CART (Classification And Regression Trees).
Os resultados dos experimentos foram bastante satisfatórios, mostrando a capacidade da aplicação de mineração de dados em dados de PDS.
A taxa de erros dos modelos preditivos foi de apenas de 19,8%.
Através dos resultados, os autores concluíram que modelos preditivos de qualidade podem descrever, competentemente, módulos que possam apresentar defeitos depois de entregues ao cliente.
Em o estudo de são usadas técnicas preditivas, classificação e regressão, para compor modelos capazes de prever se um módulo de software apresentará falhas após a entrega ao cliente.
Através dos modelos estabelecidos, os autores, pretendem descobrir se existe um conjunto de métricas que pode ser usado para prever falhar em projetos de software distintos.
Desta forma, desejam propor uma especificação de como construir, sistematicamente, modelos preditivos de defeitos post release (após a entrega ao cliente).
Este trabalho utiliza métricas de cinco produtos de software da Microsoft para estabelecer os modelos.
Para tanto utiliza características do código fonte dos produtos, tais como número de linhas, complexidade, número de parâmetros de entrada, número de classe, para prever se os mesmos serão propensos a falhas.
Em a etapa de preparação dos dados foi construída uma matriz de correlação para identificar quais atributos são altamente correlacionados com o atributo classe.
Desta forma, apenas os atributos altamente relacionados foram considerados para a aplicação das técnicas preditivas.
Os autores não mencionaram mais nenhuma técnica de preparação que tenha sido aplicada.
Este trabalho relatou que não é possível considerar um conjunto específico de métricas para prever falhas post release em todos os projetos.
Mostrou, também, que um modelo preditivo, construído a partir de dados de um dado projeto, não consegue estabelecer resultados para dados de outro projeto.
Para estabelecer os modelos, os autores descrevem uma especificação de como construir- los.
Contudo, a abordagem não é ampla o suficiente, mostrando apenas a sistemática de como selecionar um módulo de software, e selecionar o conjunto de métricas a serem usadas para estabelecer os modelos.
Este trabalho propõe um processo de KDD para auxílio na reconfiguração de ambientes virtualizados, como o Xen.
Este último é um paravirtualizador que permite que várias máquinas virtuais (Mv) sejam executadas simultaneamente sobre um mesmo hardware, onde cada uma dessas Mv possuem diferentes níveis de recursos.
O processo de KDD construído visa melhorar a performance do Xen, verificando qual a melhor alocação de recursos para o mesmo, sugerindo modificações em seus parâmetros.
Como fonte de dados são executados diferentes benchmarks sobre as Mv, a fim de coletar dados referentes ao desempenho das mesmas.
Para organizar e armazenar esses dados, foi construído um modelo de DW, focalizado em captura de métricas de benchmarks, o qual permite que sejam armazenadas quaisquer execuções de benchmarks, em diferentes ambientes computacionais.
Sobre os dados devidamente organizados no DW, é aplicada mineração de dados, onde são utilizadas tarefas preditivas de classificação, cujo objetivo é que os modelos preditivos gerados sugiram uma configuração vigente, através de novos parâmetros de reconfiguração e, assim, se possa alcançar um ganho de desempenho.
Para que a mineração utilizada alcance os resultados esperados, são aplicadas, sistematicamente, técnicas de preparação de dados para a mineração.
Essas técnicas buscam trabalhar com os dados já inseridos no DW, de maneira que possam ser especialmente úteis para produzir os resultados esperados.
Os testes efetuados mostraram a qualidade e abrangência da solução proposta.
O trabalho propõe um processo de KDD mas não o centra no DW, o que é uma característica do SPDW- Miner.
A seguir, na Tabela 5, é apresentada uma comparação entre os trabalhos relacionados, em relação a os aspectos abordados nesta pesquisa.
Os aspectos mencionados são os seguintes:·
Contexto abordado:
Com este item pretende-se conhecer o domínio de conhecimento em o qual está sendo aplicada mineração.
Por exemplo, neste trabalho será aplicada mineração no contexto de processo de desenvolvimento de software.·
Dados Utilizados: Especificar que dados foram utilizados como atributos preditivos.
Por exemplo, métricas de software.·
Objetivo da mineração:
Estabelecer o conhecimento que se pretende descobrir.·
Pré-processamento: Identificar se foi aplicada alguma técnica de preparação.·
Técnica de mineração:
Qual técnica ou quais técnicas de mineração foram utilizadas.·
Processo de KDD: Identificar se a proposta define algum método para guiar o usuário na execução do processo de KDD.·
Data warehouse:
Identificar se a proposta define a utilização de algum repositório para auxiliar no processo de KDD.
Os três primeiros trabalhos relatados,[ NAY05],[ KHO01] e[ NAG06], mostram a aplicação de técnicas de mineração em métricas de software.
Porém, nesses estudos não fica claro se é adotado um processo de KDD, ou se é utilizado um DW como parte do processo.
Apenas em existe a preocupação de definir uma especificação de como estabelecer modelos preditivos de métricas de software.
Contudo a especificação apresentada não contempla todas as etapas de um processo de KDD, apenas se preocupa em definir as métricas que devem ser consideradas para estabelecer os modelos preditivos.
A proposta mais próxima deste trabalho foi a de que estabelece um processo de KDD completo para auxiliar na reconfiguração de ambientes virtualizados.
Contudo, o mesmo não centra a preparação dos dados no modelo e conteúdo do DW, aspecto que, para PDS, mostra- se bastante conveniente.
Este trabalho propõe um método para a execução do processo de KDD, denominado de SPDW-Miner, voltado para o estabelecimento de predições de métricas de software, por exemplo:
Esforço de retrabalho, custo, esforço de trabalho, tamanho.
O método é composto por uma série de etapas que guiam os usuários para o desenvolvimento de todo o processo de KDD, tomando como referência um repositório de métricas de software estruturado na forma de um DW.
Foram especificadas todas as etapas que compõem o processo de KDD, desde o estabelecimento do objetivo de mineração;
A extração e preparação dos dados;
A mineração até a otimização dos resultados.
Para caracterizar um cenário real de aplicação desta pesquisa foi estudado o ambiente de uma operação de desenvolvimento de software, certificada CMM3, e uma proposta de evolução do mesmo, relatados no capítulo 3.
A partir desse estudo foi possível constatar as limitações e necessidades do cenário, onde foi verificada a necessidade da presença de recursos de predição, que possibilitem estimativas mais precisas, as quais podem ser consideradas essenciais para a obtenção de níveis de maturidade mais altos.
A validação da solução proposta foi realizada através da aplicação das etapas do SPDW- Miner no contexto da operação parceira.
Para tanto, foi definido um objetivo de mineração de interesse da parceira, e então aplicou- se exaustivamente o método.
O objetivo da mineração é estabelecer modelos capazes de predizer o esforço de retrabalho.
Em a experimentação foram testadas várias situações de preparação de dados.
Desta forma, pode- se constatar a abrangência do SPDW- Miner, pois este conseguiu guiar as várias problemáticas constatadas e, por fim, estabelecer resultados satisfatórios na mineração.
O SPDW- Miner representa uma inovação em relação a os trabalhos relacionados, através da sua proposta de adotar toda uma sistemática para a execução coerente do processo de KDD e, também, por se beneficiar das informações do DW para guiar o processo.
A continuidade deste trabalho visa estender os benefícios oferecidos por o método SPDW- Miner.
Desta forma, pretende-se atingir os seguintes objetivos:
Utilizar os modelos preditivos resultantes do processo de KDD, guiado por o SPDW- Miner, no ambiente da operação parceira.
Explorar o SPDW- Miner com outras técnicas preditivas.
Aplicar o SPDW- Miner em outros contextos que necessitem de recursos de predição.
