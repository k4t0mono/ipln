Essa tese descreve um processo para extrair conceitos de textos em língua portuguesa.
O processo proposto inicia com corpora de domínio linguisticamente anotados, e gera listas de conceitos dos domínios de cada corpus.
Utiliza- se uma abordagem linguística, que baseia- se na identificação de sintagmas nominais e um conjunto de heurísticas que melhoram a qualidade da extração de candidatos a conceitos.
Essa melhora é expressa por incrementos aproximadamente de 10% para mais de 60% nos valores de precisão e abrangência das listas de termos extraídas.
Propõe- se um novo índice (tf-dcf) baseado na comparação com corpora contrastantes, para ordenar os termos candidatos a conceito extraídos de acordo com suas relevâncias para o corpus de domínio.
Os resultados obtidos com esse novo índice são superiores aos resultados obtidos com índices propostos em trabalhos similares.
Aplicam- se pontos de corte para identificar, de entre os termos candidatos classificados segundo sua relevância, quais serão considerados conceitos.
O uso de uma abordagem híbrida para escolha de pontos de corte fornece valores adequados de medida F, trazendo qualidade ao processo de identificação de conceitos.
Adicionalmente, propõem- se quatro aplicações para facilitar a compreensão, manipulação e visualização dos termos e conceitos extraídos.
Essas aplicações tornam as contribuições dessa tese acessíveis a um maior número de pesquisadores e usuários da área de Processamento de Linguagem Natural.
Todo o processo proposto é descrito em detalhe, e experimentos avaliam empiricamente cada passo.
Além de as contribuições científicas feitas com a proposta do processo, essa tese também apresenta listas de conceitos extraídos para cinco diferentes corpora de domínio, e o protótipo de uma ferramenta de software (EATOLP) que implementa todos os passos propostos.
Palavras-chave: Processamento de linguagem natural;
Extração automática de termos;
Recuperação de informação;
Ontologias. Processamento de Linguagem Natural (PLN) é a área de pesquisa que estuda o desenvolvimento de programas de computador que analisam, reconhecem ou geram textos em linguagens humanas, ou linguagens naturais.
PLN é uma área com grandes desafios, devido a a rica ambiguidade da linguagem natural, sendo isso um dos fatores que torna PLN diferente do processamento das linguagens formais que são definidas evitando a ambiguidade.
Segundo DuRoss Liddy, um dos objetivos usuais em PLN é a recuperação de informação a partir de textos, pois textos são, segundo Maedche e Staab, a forma mais abundante de informação disponível.
De entre os tipos de recuperação de informação em textos, a busca de termos em corpora (plural de corpus) de domínio é uma das principais aplicações de PLN.
Um corpus de domínio é um conjunto de textos sobre um domínio específico que pode ser utilizado para caracterizar esse domínio.
Portanto, detectar termos relevantes num corpus é uma forma adequada de identificar termos relevantes para o domínio descrito por esse corpus.
Mais que isso, segundo Perini, a identificação de termos através da observação de corpora permite a observação dos padrões de uso da linguagem livre de preconceitos.
A extração de termos de corpora de domínio possui diversas aplicações, como por exemplo, a categorização de textos, a identificação de termos para mecanismos de busca[ 167, 149, 7, 164198, 217, 38, 71].
Cada uma dessas aplicações possui suas especificidades, mas em todas elas existe a necessidade de identificar termos que sejam de alguma forma relevantes ao domínio.
Motivação Uma das iniciativas mais ambiciosas e necessárias da computação é o estabelecimento da Web Semântica.
Essa iniciativa se propõe a organizar, semanticamente, o acesso à extraordinária quantidade de dados disponíveis com o advento e expansão da Internet, em a qual a maior dificuldade não é encontrar o que se procura, mas reconhecer o que foi encontrado.
Um dos caminhos para essa organização é a representação do conhecimento através de ontologias.
Ontologia, segundo Gruber, é uma forma de estruturar informações para representar conhecimento.
No entanto, a representação desse conhecimento através de formalismos tratáveis por máquinas, como é o caso das ontologias, torna- se um grande desafio frente a a quantidade enorme de dados textuais a estruturar.
Portanto, é necessário automatizar o processo de construção de ontologias a partir de textos.
Retoma- se, então, a questão de extração de termos de corpora, visando a identificação dos conceitos que são, conforme será visto em detalhe nessa tese, os componentes fundamentais de ontologias.
Em esse sentido, a motivação central do trabalho proposto é a dificuldade inerente à construção de ontologias, principalmente no que diz respeito à identificação dos elementos básicos, que são os conceitos e a sua expressão em termos linguísticos.
A esse fato soma- se que, em alguns cenários a utilização de ontologias em língua portuguesa se faz necessária.
Alguns exemplos desses cenários são:
A comunicação entre grupos de especialistas de domínio falantes do português, descrição de elementos culturais brasileiros, típicos de museu de cultura, ou em domínios que envolvem comunicação do especialista com o leigo como no caso de medicina e governo eletrônico.
Objetivo e Metodologia O objetivo geral dessa tese é propor um processo para extrair automaticamente conceitos, ou seja, termos relevantes com valor conceitual, para um domínio caracterizado por um corpus em língua portuguesa, composto por textos representativos para este domínio.
Em esse sentido, para realizar o objetivo geral dessa tese é necessário alcançar os seguintes objetivos específicos:·
definir um método de extração de termos candidatos a conceitos a partir de um corpus anotado linguisticamente;·
definir um método de ordenar os termos extraídos segundo sua relevância;·
definir uma forma de identificar, de entre os termos extraídos, quais devem ser considerados conceitos do domínio;·
definir um conjunto de aplicações dos conceitos extraídos, que facilite a sua compreensão, manipulação e visualização.
Detalhamento do Processo Proposto O processo desenvolvido nessa tese recebe como entrada um conjunto de corpora de domínio anotados linguisticamente e, após a aplicação do processo proposto, gera- se uma lista de conceitos e um conjunto de informações contextuais sobre esses conceitos.
Em linhas gerais, esse processo pode ser dividido em quatro grandes etapas:
Extração de termos e contextos, ordenação de termos de acordo com sua relevância, identificação de conceitos e geração de recursos linguísticos (aplicações dos conceitos gerados).
Essas quatro etapas e as informações sobre cada uma de elas são descritas esquematicamente na Figura 1.1.
A primeira etapa, extração de termos e contextos, descrita na Figura 1.1 corresponde a um processo linguístico onde recebe- se um corpus de domínio anotado e detectam- se os termos candidatos a conceitos desse domínio.
Adicionalmente, informações referentes à forma como esses termos foram empregados no corpus (contextos), além de os números de ocorrências em que o termo foi encontrado em cada uma das suas situações de uso no corpus, são extraídos.
A segunda etapa, consiste em ordenar os termos segundo sua relevância através de um processo estatístico que leva em conta além de o corpus de domínio, um conjunto de corpora usados como contraste ao domínio.
Como resultado dessa etapa, cada termo recebe um valor numérico que pode ser usado como índice de ordenação dos termos de acordo com sua relevância para o domínio.
A terceira etapa, identificação de conceitos, recebe a lista de termos ordenada segundo sua relevância e escolhe quantos destes termos devem ser considerados conceitos do domínio.
Em esse sentido, essa etapa consiste em escolher e aplicar um ponto de corte à lista ordenada de termos.
A quarta etapa, consiste em utilizar os conceitos e seus respectivos contextos para gerar recursos linguísticos sofisticados.
De entre essas aplicações, apresenta- se nessa tese:
Listas de conceitos com informações contextuais;
Concordanciador para visualizar as frases onde cada um dos conceitos foi empregado no corpus;
Nuvens de conceitos (tag clouds) com informações visualmente estruturadas dos conceitos;
E hierarquia de conceitos onde estruturam- se os conceitos segundo critérios linguísticos, ou seja, detectam- se relações taxonômicas entre os conceitos.
A metodologia empregada para definir e testar o processo descrito na Figura 1.1 consistiu da definição de cada uma das etapas, experimentação de técnicas pré-existentes, proposta de novas técnicas e, finalmente, a avaliação objetiva do resultado de cada etapa.
Especificamente, a avaliação de cada etapa foi feita com um corpus de domínio pre-existente (corpus de Pediatria) para qual uma lista de termos relevantes (conceitos) foi disponibilizada.
Maiores detalhes sobre esse corpus e lista de referência serão apresentados na Seção 3.1.
Cabe salientar que além de as avaliações feitas e apresentadas individualmente nos capítulos que descrevem, respectivamente, as etapas de extração, ordenação e identificação, outras avaliações externas a essa tese foram realizadas e são citadas na Seção 7.2 da conclusão.
Um ponto prático da metodologia de desenvolvimento dessa tese, é que todas as etapas do processo proposto foram implementadas numa ferramenta de software, chamada EATOLP, Extrator Automático de Termos para Ontologias em Língua Portuguesa.
Essa ferramenta[ 117, 118] se encontra ainda em estágio de protótipo, mas ela foi utilizada para a totalidade dos experimentos descritos nessa tese.
Partes desse Documento Esse volume de tese é composto por cinco capítulos, além de essa introdução, uma conclusão e três anexos.
A conclusão dessa tese resume o trabalho desenvolvido salientando as contribuições científicas e tecnológicas obtidas.
Igualmente, a conclusão cita os recursos linguísticos criados durante esse doutorado e sugere trabalhos futuros a essa tese.
Os anexos dessa tese apresentam listas de termos de referência (anexo A), listas de conceitos extraídos dos corpora utilizados nessa tese (anexo B) e uma lista de etiquetas semânticas utilizadas por o parser (anexo C).
Esse capítulo situa as contribuições científicas dessa tese dentro de a área de Processamento de Linguagem Natural (PLN).
Para tanto, define- se genericamente essa área através de um breve histórico.
Após, apresenta- se uma definição formal de ontologias e hierarquias de conceitos para permitir localizar claramente onde se insere o objetivo central dessa tese, que é a extração automática de conceitos (Seção 2.2).
Por fim, apresenta- se os problemas específicos de criação de ontologias, com ênfase na extração de termos e suas métricas usuais de qualidade (Seção 2.3).
PLN, Processamento de Corpus e Web Semântica Historicamente1, a área de PLN começou com tentativas de tradução automática na segunda metade da década de 1940.
Esses trabalhos iniciais estavam relacionados com esforços prévios de quebra de códigos durante a Segunda Guerra Mundial.
De um ponto de vista teórico, esses trabalhos iniciais em tradução automática estavam baseados na criptografia e teoria da informação.
Em 1957, Chomsky desenvolveu trabalhos relevantes sobre o tema.
Um trabalho particularmente relevante dessa época é o livro Syntactic Structures que introduziu a gramática gerativa.
A partir desse trabalho, ficou mais claro como a área de linguística poderia auxiliar a área de tradução automática.
Em essa época houve também a inclusão de outras aplicações de PLN, especialmente a do reconhecimento da fala (speech recognition).
Com isso, houve a primeira grande divergência, que, de certa forma, permanece até hoje, pois parte da comunidade optou por o uso de linguística teórica e parte optou por métodos estatísticos.
Infelizmente, cada uma dessas partes rechaçava os métodos da outra parte, prejudicando a integração dessas duas abordagens.
Esse período marca também o advento da Teoria Sintática da Linguagem e dos Algoritmos de Parsing.
Esses avanços foram muito importantes para a área, ainda que na época tenham sido recebidos com um entusiasmo excessivo, gerando a expectativa de que, em poucos anos, tradutores automáticos perfeitos estariam disponíveis.
Essa expectativa se mostrou indevida tanto por os conhecimentos linguísticos e computacionais da época, quanto por uma impossibilidade teórica da tarefa de tradução automática perfeita.
Consequência disso ou não, em 1966 o comitê assessor para processamento automático da língua (ALPAC) da Academia Americana de Ciência recomendou que a área de tradução automática não recebesse mais financiamento governamental, pois a tradução automática estava muito aquém de os conhecimentos científicos da época.
Em contraste com essa decisão, vários avanços teóricos e práticos foram feitos nos anos seguintes.
Entre eles, pode ser citado o trabalho teórico de Chomsky que introduziu o modelo computacional de competência linguística, que resultou nas gramáticas gerativas transformacionais.
Diversos trabalhos subsequentes tentaram aproximar esses conceitos de modelos computacionalmente tratáveis.
A descrição histórica da área de PLN apresentada nessa seção é um resumo do capítulo &quot;Processamento de linguagem natural e o tratamento computacional de linguagens científicas «originalmente publicado em 2010 no livro &quot;Linguagens Especializadas em Corpora -- modos de dizer e interface de pesquisa».
A partir desse período, houve uma multiplicação dos estudos sobre PLN com o estabelecimento de diversas subáreas que vêm sendo pesquisadas até hoje.
Essas áreas se dedicam a assuntos tão variados quanto categorização de textos e extração de informações, passando por os tradicionais temas de tradução automática e sistemas de diálogo.
Os trabalhos desenvolvidos nesse período podem, segundo Jurafsky e Martin, ser agrupados em quatro grupos de acordo com os paradigmas utilizados:
Os métodos estocásticos, os métodos baseados em lógica, os métodos de entendimento de linguagem natural, e os métodos de modelagem de discurso.
Os trabalhos do grupo de métodos estocásticos são baseados em abordagens estatísticas e frequentemente utilizam formalismos com os modelos ocultos de Markov (HMM -- Hidden Markov Models).
Esses métodos estão na base de diversos trabalhos de reconhecimento e síntese de fala.
Esses trabalhos estão na origem dos atuais trabalhos em que métodos estatísticos são empregados para diversas aplicações de PLN.
Os trabalhos baseados em lógica começaram com Q--systems e gramáticas metamórficas que foram os precursores da linguagem Prolog e das gramáticas de cláusulas definidas (DCG -- Definite Clause Grammar).
De essa mesma época datam também as iniciativas de gramáticas funcionais na sua versão inicial e na versão léxica.
Os trabalhos baseados em entendimento da linguagem natural seguiram na vertente do entendimento do discurso.
De um ponto de vista teórico são típicos desses trabalhos aqueles sobre Gramáticas de Caso, Redes Semânticas, Teoria de Dependência Conceitual, Redes de Transição Aumentada e Semântica de Preferência.
De um ponto de vista puramente prático, esse período viu o aparecimento de diversos programas que faziam uso intensivo de PLN.
Esse foi o caso dos sistemas de diálogo Eliza e PARRY, mas também os sistemas de reconhecimento de fala SHRDLU, Lunar, LIFER/ LADDER e Em seguida, as iniciativas centradas na modelagem do discurso focaram suas atenções em questões semânticas.
Trabalhos significativos dessa época como o trabalho de Grosz visavam diálogos funcionais (diálogos que especificam uma tarefa a ser executada).
Os trabalhos subsequentes de Grosz e Sidner definem uma teoria de partição do discurso baseado em relações entre a estrutura da tarefa a executar e a estrutura do diálogo que descreve essa tarefa.
Em essa mesma época foi desenvolvida por Mann e Thompson a Teoria de Estrutura Retórica que associa uma estrutura hierárquica para o discurso com o intuito de geração automática de texto.
Outros trabalhos desse período também foram dedicados à geração de linguagem natural, como é o caso dos geradores de resposta TEXT e MUMMBLE, que usam predicados retóricos para produzir descrições declarativas na forma de parágrafos.
Processamento de Corpus Desde o início da década de 1990, o crescimento da internet e a profusão de textos disponíveis direcionaram os esforços do PLN para o tratamento de textos, mais do que para o discurso falado.
Em essa época iniciaram as pesquisas sobre corpora anotados sintaticamente, ou seja, conjuntos de textos sobre um domínio de conhecimento, em que cada uma das suas palavras são identificadas segundo sua função sintática.
Vários desses trabalhos foram desenvolvidos para a língua inglesa, utilizando três corpora bastante populares:
Brown corpus, Lancaster-Olso-Bergen corpus e Penn Treebank.
Por ocasião de o uso de corpora, diversos trabalhos de pesquisa baseados em conceitos linguísticos utilizados em conjunto com abordagens estatísticas, possibilitaram resultados práticos mais robustos.
Essa reconciliação entre métodos linguísticos e estatísticos, que se percebe atualmente, desfaz a divisão de abordagens feita na área desde do final da década de 1950.
A grande quantidade de informação a ser tratada que impulsionou a reconciliação dos métodos estatísticos e linguísticos teve desde a virada do século uma outra consequência in-teressante com a incorporação de técnicas de aprendizado de máquina.
As técnicas de aprendizado de máquina são particularmente aplicáveis no contexto de conjuntos de dados humanamente intratáveis, mas, de os quais se pode inferir padrões e, consequentemente, informação.
Naturalmente, os últimos anos têm testemunhado uma convergência das técnicas de PLN baseadas em corpus com técnicas de aprendizagem de máquina, ou mais especificamente, técnicas de mineração de dados.
Esse aumento significativo das ferramentas à disposição dos pesquisadores de PLN permitiu também um aumento significativo nas ambições da área.
Retomomaram- se seriamente os trabalhos de tradução automática, apesar de a consciência de ser inatingível uma tradução perfeita.
Uma quantidade muito grande de tradutores automáticos está disponível na internet como é o caso dos sites especializados como babelfish, mas também de serviços de tradução embutidos como os disponíveis automaticamente por o gigante de pesquisas Google.
Web Semântica Igualmente, a evolução de outras áreas da computação, como é o caso da computação pervasiva (recursos computacionais presentes em atividades cotidianas) ou aplicações web, abriu espaço para novos sistemas de reconhecimento e geração automática da linguagem.
Alguns sistemas recentes utilizam PLN com o objetivo de responder perguntas de forma clara e direta através de conhecimento semântico.
Exemplos práticos desse tipo de aplicação, disponíveis na internet, são os sistemas Ask, Lexxe e Hakia.
Outro exemplo desenvolvido com o mesmo princípio é o sistema True Knowledge, que, além de direcionar a vários links relacionados com perguntas simples feitas por o usuário, também insere a resposta direta a pergunta do usuário.
Esse sistema permite ainda que os usuários acrescentem informações, tornando- o cada vez mais completo e preciso.
Porém, talvez o objetivo mais ambicioso do processamento de linguagem natural resida atualmente na construção da web semântica que pretende estabelecer uma ponte entre o enorme volume de dados disponível na Internet e as demandas de informação e conhecimento de seus milhões de usuários.
A web semântica é uma iniciativa que busca identificar e representar o significado de páginas na web de forma que tanto pessoas como máquinas possam identificar- los.
Em esse sentido, o grande desafio é a representação do conhecimento num formato adequado, que nesse contexto é feito através de Ontologias.
Ontologias Segundo Gruber, &quot;Ontologia é uma especificação explícita de uma conceitualização».
De essa forma, ontologias podem ser consideradas representações formais (um conjunto concreto de especificações) de um modelo de domínio que exista de forma abstrata.
Geralmente, uma ontologia é entendida como um conjunto de conceitos organizados hierarquicamente, um conjunto de relações e um conjunto de atributos.
Essa seção apresenta uma definição formal de ontologias, mas o leitor interessado pode encontrar um extenso material diversas definições formais de ontologias na literatura, e qualquer uma de elas se prestaria aos propósitos dessa tese.
Apesar disso, no contexto dessa tese, é apresentada a definição proposta por Cimiano, por ser uma referência formal de ampla difusão na área empregada por diversos autores da comunidade nacional.
Definição Formal de Ontologias De um ponto de vista formal uma ontologia é uma estrutura:
O: «pC, C, R, R, A, T q Composta de:·
Quatro conjuntos disjuntos:
­ C -- identificadores de conceitos;·
Um semireticulado superior C definido sobre os elementos de C (conceitos) chamado de hierarquia de conceitos ou taxonomia, que possui:
&quot;é um «(em inglês &quot;is a&quot;) com c2;·
Uma função R:
que estabelece relações entre conceitos, chamada assinatura de relação.
Essas funções definem uma relação do conjunto R e dois conjuntos de conceitos de C, respectivamente:·
Uma ordem parcial R sobre R que estabelece uma ordem de precedência de certas relações sobre outras, chamada hierarquia de relação, que de forma análoga à hierarquia de conceitos define:·
Uma função A:,
similar à função R, mas que relaciona atributos ao invés de conceitos, chamada assinatura de atributos.
Sistema de Axiomas, Base de Conhecimentos e Extensões Usualmente, define- se ao mesmo tempo que uma ontologia O um conjunto de axiomas que permite estabelecer as propriedades necessárias entre os conceitos, as relações e os atributos dessa ontologia.
De um ponto de vista formal, um sistema de axiomas S de uma ontologia O é definido por a tripla:
S: «pAS, Lq Composta de:·
uma linguagem lógica L;·
um conjunto de axiomas As que pode fazer referência a conceitos, relações e atributos;·
um mapeamento:
Uma vez definida uma ontologia O e um sistema de axiomas S, a definição geral de uma ontologia é completada através da definição de instâncias para os conceitos, as relações e os atributos.
De um ponto de vista formal, isto é feito através da definição de uma base de conhecimento:
KB: «pI, C, R, A q Composta de:·
um conjunto I de identificadores de instâncias, ou simplesmente instâncias;·
uma função C:,
chamada instanciação de conceitos, que define para cada conceito c P C qualquer subconjunto de I;·
uma função R:,
chamada instanciação de relações, que define para cada relação r P R qualquer tupla3 contendo elementos de I;·
uma função A:,
chamada instanciação de atributos, que define para cada atributo a P A um par com uma instância de I e um elemento do seu tipo de dados t..
Aplicando- se a uma ontologia O, instanciada por uma base de conhecimentos KB, e levandose em consideração um sistema de axiomas S, é possível popular esse conjunto tO, KB, Su com instanciações adicionais decorrentes do semireticulado C, da ordem parcial R e da aplicação dos axiomas.
Essas extensões são definidas como rcs, para conceitos c P C, rrs, para relações r P R e ras, para atributos a P A. Hierarquia de Conceitos Frente a a complexidade de uma ontologia completa, com sua estrutura básica (O), seu sistema de axiomas (S), sua base de conhecimentos (KB) e suas extensões, muitos trabalhos estão baseados em construir apenas uma hierarquia de conceitos.
Formalmente, uma hierarquia de conceitos é definida por um conjunto de conceitos e um semirreticulado superior, ou seja:
H: «pC, C q A notação PpIq representa o conjunto com todos os subconjuntos possíveis do conjunto I. A notação I representa todos os conjuntos possíveis de tuplas formadas por elementos de I. Obter um conjunto qualificado de conceitos torna possível construir melhores hierarquias, que, por sua vez, é a estrutura base para definir uma ontologia (O).
O propósito dessa tese éa extração automática e qualificada de conceitos, ou seja, a definição qualificada do conjunto C para um corpus de domínio.
Porém, como será visto posteriormente (Seção 6.4), desenvolve- se também uma hierarquia de conceitos, ou seja, infere- se as relações expressas por o semirreticulado C.
Cabe salientar que o processo de construção de hierarquia de conceitos apresentado como exemplo de aplicação na Seção 6.4, é apenas um exercício de estruturação sem maiores ambições científicas, pois o foco científico dessa tese é a extração de conceitos.
Construção Automática de Ontologias Para construção de ontologias é necessário realizar um processo bastante complexo e trabalhoso, que pode ser feito manualmente por um engenheiro de ontologias com o auxílio de um ou vários especialistas de um determinado domínio.
No entanto, essa construção manual demanda muito tempo e trabalho de todos os envolvidos.
Uma alternativa é automatização da construção de ontologias, porém essa tarefa representa grandes desafios computacionais.
Diversas abordagens de aprendizagem, técnicas e ferramentas para a construção de ontologias podem ser encontradas atualmente, pois dada a complexidade do processo, é difícil imaginar a construção de uma ontologia sem o auxílio de ferramentas computacionais.
Curiosamente, por a mesma razão, a alta complexidade, ainda não existem sistemas efetivos capazes de construir uma ontologia completa de forma totalmente automática.
A construção de ontologia feita através de métodos automáticos ou semi-automáticos de extração de conhecimento é denominada &quot;Aprendizagem de Ontologia «(Ontology Learning).
Esse termo foi introduzido originalmente por Madche e Staab em 2001 que inicialmente incorporaram o uso de técnicas oriúndas da área de aprendizagem de máquina.
Apesar de isto, o termo &quot;Apredizagem de Ontologia «não se restringe apenas a técnicas baseadas em aprendizagem de máquina, podendo envolver diversas outras áreas do conhecimento como linguística computacional e recuperação de informações.
Os esforços semiautomáticos são baseados na utilização de ferramentas, e.
g, software de edição, que permitam organizar ontologias que serão projetadas por um usuário que conheça o domínio a ser descrito.
De entre essas ferramentas, provavelmente a mais popular é o Protégé[ 77, 157], que permite ao usuário construir e manipular ontologias.
As funcionalidades básicas desta ferramenta incluem algumas verificações e visualizações automáticas.
Porém, o Protégé oferece a possibilidade de adicionar plugins capazes de realizar operações sobre ontologias, e.
g, OntoLP, um extrator de termos de fontes textuais (textos).
Protégé permite armazenar ontologias modeladas segundo dois protocolos:
OKBC -- Open Knowledge Base Connectivity e OWL -- Ontology Web Language.
Outra ferramenta semi-automática de construção de ontologias é o OntoGen que combina técnicas de mineração de textos com uma interface de utilização, que facilita a escolha dos conceitos e relações.
De essa forma, o OntoGen, parte de um corpus e oferece ao usuário conjuntos de termos candidatos a conceitos, e cabe ao usuário estabelecer manualmente a hierarquia entre os conceitos, bem como as relações entre eles.
Em esse sentido, OntoGen, assim como alguns plugins do Protégé, é uma ferramenta para edição de ontologias que possui um processo de extração de termos a partir de textos.
Em a verdade, muitas ferramentas buscam em fontes textuais (textos) o conhecimento a ser armazenado numa ontologia.
Segundo Maedche e Staab, a busca de informações em textos se justifica, pois a grande maioria do conhecimento disponível encontra- se em fontes textuais.
Em esse sentido, o trabalho desenvolvido nessa tese está baseado na extração de informações contidas em textos.
O primeiro problema para gerar ontologias a partir de textos é identificar quais tarefas são necessárias para a construção efetiva de uma ontologia.
Segundo Buitelaar, esse processo divide- se em cinco4 etapas básicas:
Extração de termos candidatos a conceitos de um domínio;
Determinação de sinônimos entre os termos candidatos e escolha dos conceitos;
Identificação da relação hierárquica entre os conceitos;
Identificação de relações entre os conceitos;
Logicamente, os passos descritos na Figura 2.1 devem ser executados sequencialmente, sendo a extração de termos candidatos a conceitos a primeira e mais importante tarefa, pois da qualidade dos resultados dessa etapa depende a qualidade dos resultados de todas as demais etapas.
Note- se que essa afirmação não significa que as outras etapas sejam mais simples, ou que não seja necessário preocupar- se com a eficiência de cada uma de elas.
A qualidade da ontologia é dependente de todas as etapas, porém, caso a extração de termos candidatos seja deficiente, o resultado de todas as demais etapas não poderá compensar essa deficiência.
Essa opinião é compartilhada por diversos autores da área.
Extração de Termos e Conceitos A importância da extração de termos para a construção automática de ontologias é clara[ 170, 180, 203, 162, 186, 213].
No entanto, em mecanismos de busca e mineração de textos em geral a importância de uma correta extração de termos também vem sendo tema de pesquisas há mais de quatro décadas.
Segundo diversos autores, e certas vezes até em publicações de um mesmo autor, é possível encontrar diversas variações na definição das etapas de construção automática de ontologias.
A versão considerada nessa tese é uma ligeira adaptação realizada a partir de a publicação de Buitelaar Que reflete a organização do processo proposto nessa tese.
Abordagens de Extração de Termos Uma das primeiras observações relevantes, no que diz respeito à extração de termos, é o fato de que existem diferenças entre extração de termos simples, ou seja, termos com uma única palavra, e extração de termos compostos.
Um termo composto é um conjunto de duas ou mais palavras que possui um significado comum, e que por sua natureza são mais difíceis de detectar do que termos simples (uma única palavra).
Historicamente, os trabalhos de extração iniciaram, e ainda têm uma importante vertente, com contabilizações do número de termos simples extraídos.
Em seguida, por volta de a década de 80, um grande número de trabalhos centrou seu interesse na extração de termos compostos.
De qualquer maneira, devido a a importância da qualidade na extração de termos, muitos trabalhos científicos dedicam- se a aperfeiçoar esse processo, e como é comum em PLN, as abordagens para a extração de termos se dividem em abordagens estatísticas e linguísticas.
As abordagens estatísticas de extração de termos têm no extrator NSP sua ferramenta mais popular.
Essa ferramenta alia simplicidade da busca de termos por combinação de palavras adjacentes com um método de descarte de termos através de stop list, ou seja, listas de termos comuns que não possuem grande valor terminológico.
Em a verdade, a eficiência da abordagem utilizada por a ferramenta NSP depende muito da escolha de termos a incluir na stop list.
Outras abordagens estatísticas, como a ferramenta BootCat, oferecem recursos mais sofisticados, principalmente, no que concerne a extração de termos compostos.
Apesar disso, a abordagem utilizada por a ferramenta BootCat também depende da especificação de stop lists, como toda abordagem estatística.
Ainda se inclui de entre as abordagens estatísticas de extração de termos as iniciativas que tentam calcular índices, como os populares tf-idf e loglikelihood, que sejam mais efetivos do que a simples frequência de ocorrência dos termos.
Porém, segundo Wermter e Udo, não é possível, sem o uso de informações linguísticas, obter melhores resultados do que a simples frequência absoluta de termos.
Essa conclusão, de certa forma, explica o sucesso de uma abordagem simplista como a implementada na ferramenta NSP.
Por outro lado, as abordagens baseadas em informações linguísticas tendem a oferecer bons resultados na extração de termos.
Ainda que tenham como desvantagem o fato de precisarem de ferramentas de anotação linguística eficazes, e que sejam, quase sempre, específicas para textos num único idioma.
De entre as abordagens linguísticas, alguns métodos têm apresentado resultados bastante precisos, como é o caso das abordagens baseadas no método C- value e sua versão estendida NC- value.
Esse método baseia- se na observação de padrões sintáticos para detectar, com grande sucesso, termos compostos aninhados, que são particularmente frequentes em inglês 5.
Infelizmente, esse método não parece ter a mesma eficiência quando portado para outras línguas, tipicamente línguas latinas.
Um exemplo recente de abordagem linguística para extração de termos é o trabalho de Bui e Sloot, onde através de padrões sintáticos buscam- se termos específicos de eventos biológicos6.
A abordagem desse artigo não procura termos gerais, mas sim padrões específicos que possuam uma semântica clara e um conjunto de termos conhecidos previamente (por exemplo, nomes de proteínas).
Abordagens como essa são facilitadas por a especificidade, e chegam a taxas de acerto com valores médios de precisão em torno de 50%.
Termos compostos aninhados não são uma exclusividade da língua inglesa.
No entanto, seu uso em inglês apresenta uma dificuldade adicional devido a a possibilidade de composição de diversos substantivos como na expressão &quot;movie actor studio «(estúdio de atores de filme), onde três substantivos são utilizados para descrever, além de o termo geral, dois termos aninhados:
&quot;movie actor «(ator de filme) e &quot;movie «(filme).
Eventos biológicos são termos específicos da área de biologia que descrevem um momento de interesse, por exemplo, a interação entre duas proteínas.
De maneira genérica, é possível afirmar que a extração de termos é uma tarefa que, apesar de ser objeto de estudo há um longo tempo, ainda apresenta desafios consideráveis.
Uma das formas mais eficazes de extração de termos é realizar a anotação linguística de corpora e em seguida extrair termos segundo uma análise estatística.
O processo de extração proposto no decorrer de essa tese se enquadra nesse tipo de abordagem híbrida.
Alguns exemplos similares Identificação de Conceitos Um aspecto importante para a recuperação de informações textuais é o passo posterior à extração de termos, que consiste em escolher de entre os termos extraídos aqueles que são portadores de valor conceitual, e não apenas terminológico.
Uma distinção importante, segundo Petasis, é a definição de conceitos, que se presta a controvérsias.
No entanto, um bom número de autores parece concordar que um conceito é uma generalização associada a uma ideia, podendo ter várias manifestações textuais.
Em o processo proposto nessa tese, alguns dos termos relevantes extraídos e identificados como conceitos, poderiam ser melhor classificados como instâncias A subclassificação de um termo relevante como conceito ou instância é um processo de grande complexidade.
Para atacar esse problema, faz- se uso de técnicas de análise sintática, desambiguação, coreferência, etc..
Dentro de uma área denominada população de ontologias, que foge ao escopo dessa tese.
O leitor interessado pode achar grande material sobre o assunto em publicações específicas[ 42, 128, Apesar de não estabelecer uma distinção teórica entre conceitos e instâncias, a grande vantagem da abordagem proposta, reside no fato de que o processo, baseado na estimativa da relevância dos termos, permite automatizar a identificação dos principais conceitos de um domínio sem maiores intervenções humanas.
De essa forma, o esforço de extração de conceitos de um corpus de domínio feito nessa tese se alinha com outros trabalhos científicos que partem de um processo básico de extração de termos, e, em seguida, se empenham em estimar a relevância dos termos extraídos a fim de identificar os conceitos.
Alguns exemplos desse tipo de trabalho, são os esforços de Pantel e Lin, Chung, Milios, Drouin, Park, Medidas de Avaliação Uma questão importante que se coloca nessa área de extração de informação é que todas as iniciativas de identificação de conceitos são, por a natureza do objetivo, obrigatoriamente empíricas.
Assim sendo, uma das questões fundamentais de pesquisa é definir uma forma de verificar a qualidade do processo proposto.
Em essa tese optou- se por utilizar, quando disponível, uma lista de termos relevantes do domínio previamente estabelecida como referência para o sucesso do processo (gold standard).
De essa forma, é possível comparar listas de termos resultantes da extração segundo diversas abordagens com as listas de referência.
Com o propósito de comparar listas de termos ao longo de essa tese, definem- se três índices oriúndos da área de teoria da informação e de uso frequente na área de recuperação de informação.
Esses índices são as tradicionais medidas de precisão (em inglês_ Precision -- P), abrangência (em inglês_ Recall -- R) e medida F (em inglês_ F-measure -- F).
Essas medidas são utilizadas para comparar dois conjuntos, por exemplo, duas listas de termos.
Um desses conjuntos, denominado LR (lista de referência), contém os termos de referência considerados corretos para o propósito, ou seja, o alvo da identificação de conceitos.
O outro conjunto, denominado LE (lista extraída), contém os termos a comparar com a referência, ou seja, os termos extraídos que por alguma métrica foram escolhidos por a aplicação do ponto de corte.
A precisão (P) é dada por a equação abaixo que expressa a razão entre o número de termos da lista de referência que foram extraídos e considerados (tamanho da intersecção entre os conjuntos LR e LE) e o tamanho da lista de termos extraídos e considerados(| LE|).
De essa forma, a precisão (em inglês_ Precision) expressa o percentual de termos corretamente extraídos, ou seja, o percentual dos termos localizados como corretos, quantos são efetivamente corretos.
A abrangência (R) é semelhante à precisão, porém expressa a razão entre o número de termos da lista de extraídos e considerados (LE) presentes na lista de referência (LR) e o tamanho da lista de referência(| LR|).
De essa forma, a abrangência (em inglês_ Recall) expressa o percentual de termos da lista de referência coberta por a extração de termos feita.
A medida F (F) expressa o equilíbrio entre os valores de precisão e abrangência.
A sua expressão numérica é a média harmônica entre os valores de P e R. Os valores da medida F (em inglês_ F-measure) são valores situados entre P e R, e quanto maior for a diferença entre esses valores, mais próxima a medida F será do menor valor entre eles.
O uso desses índices de qualidade é bastante difundido em diversas áreas, e.
g[ 141, 25, 188, 65].
Em a área de PLN, e em especial nas tarefas de extração de termos, diversos trabalhos justificam a sua validade baseados em seus resultados numéricos, e.
g,. A primeira etapa do trabalho desenvolvido no contexto dessa tese consiste em extrair um conjunto de termos sobre um corpus de domínio específico.
O ponto de entrada nessa tarefa é um corpus linguisticamente anotado, e como saída gera- se uma lista com todos os termos empregados no corpus, bem como uma série de informações sobre o contexto em o qual cada termo foi empregado.
De essa forma, nesse capítulo faz- se uma breve descrição de um conjunto de corpora que serão utilizados como exemplos ao longo de essa tese.
Após, descreve- se informações sobre a anotação realizada, bem como, noções básicas de gramática necessárias à compreensão da tarefa de extração (Seção 3.2).
Em seguida, são propostas heurísticas de ajuste, descarte e inclusão aplicadas aos termos linguisticamente anotados, ou seja, a contribuição central desse capítulo (Seção 3.3).
Após, são avaliadas as heurísticas propostas através de uma série de experimentos práticos que comparam as listas extraídas às listas de referência (Seção 3.4).
Finalmente, sumariza- se na Seção 3.5 o processo de extração exemplificando todas as informações extraídas.
Os experimentos práticos relativos às heurísticas apresentadas nesse capítulo fazem parte de uma publicação recentemente aceita na conferência PROPOR 2012 que será realizada em Abril de 2012 em Coimbra, Portugal.
Corpora de Domínio Utilizados nessa Tese O objetivo central dessa tese é a extração automática de conceitos a partir de um corpus de domínio específico.
Logicamente, para que se possa alcançar esse objetivo é necessário ter disponível um certo número de corpora para que o procedimento possa ser experimentado.
Formalmente, corpora (o plural de corpus) são conjuntos de dados linguísticos pertencentes ao uso oral ou escrito de uma linguagem devidamente sistematizado de acordo com critérios suficientemente abrangentes para ser considerados representativos do uso linguístico.
Segundo Perini, &quot;O uso de corpora no processo científico se torna relevante por causa de sua imparcialidade e indicação confiável de frequências das formas, posto que eles representam a realidade da linguagem sem preconceitos teóricos».
Apesar de longo e laborioso, o processo de construção de corpus é válido, pois, uma vez criado, ele pode ser utilizado para diferentes aplicações como extração automática de termos, análises de estilo de escrita, construção de glossários, etc..
Muitos trabalhos na área de PLN são baseados no uso de corpus de domínio.
Um corpus de domínio é um conjunto de textos que pode ser considerado suficientemente representativo de uma área específica (o domínio).
Exemplos de trabalhos científicos baseados em manipulação de corpus são muito abundantes.
Isto se explica por que o formato textual (bases não estruturadas) é, segundo Maedche e Staab, o formato em o qual se encontra a maior parte do conhecimento disponível.
Frequentemente, os corpora são constituídos sobre um domínio específico com o intuito de servir como descrição/ definição/ caracterização desse domínio.
É possível afirmar que o uso desse tipo de corpora permite economizar os esforços de especialistas do domínio para realizar tarefas de extração de termos e outras formas de descoberta de conhecimento em geral.
Diversos corpora estão disponíveis, sendo a maior parte de eles em língua inglesa.
Alguns corpora de ampla divulgação são:
Brown corpus, Lancaster-Olso-Bergen corpus, Penn Treebank, Lonely Planet corpus e Genia corpus.
Além de o inglês, outros idiomas possuem uma relativa abundância de corpora, como é o caso do corpus utilizado por Kietz Com textos em alemão coletados na intranet de uma companhia de seguros.
Outro exemplo é o corpus utilizado por Bourigault e Lame composto por códigos legais franceses.
Eventualmente, encontram- se corpora bilíngues, e.
g, o corpus desenvolvido por Kilgarriff Que reúne textos em irlandês e inglês.
Infelizmente, para o português o número de corpora disponíveis é consideravelmente menor, principalmente tratando- se de corpora de domínios científicos.
Uma das exceções é o corpus de Pediatria (PED) desenvolvido por Coulthard a partir de 183 textos do Jornal de Pediatria, um periódico bilíngue da Sociedade Brasileira de Pediatria.
Devido a essa escassez de corpora sobre domínios científicos em português, e para suprir as necessidades dessa tese, foi construído um conjunto de corpora sobre domínios específicos.
Especificamente, foram criados quatro corpora sobre os seguintes domínios específicos1:
Modelagem estocástica (Me);
Mineração de dados (Md);
Processamento paralelo (Pp);
E Geologia (Geo).
De um ponto de vista prático, nessa tese utilizam- se, então, cinco corpora, cujas características estão descritas na Tabela 3.1 que apresenta o número de textos, frases e palavras de cada um dos corpora.
A anotação linguística de um corpus é um processo complexo e empregado em abordagens de processamento de linguagem natural que não sejam puramente estatísticas.
Diversas possibilidades de anotação linguística estão disponíveis em várias línguas, mas, em português, poucas opções estão operacionais enquanto parsers completos.
De entre as opções disponíveis para português existe a ferramenta Lx parser, recentemente disponibilizada online por a equipe dirigida por António Branco da Universidade de Lisboa.
Uma outra opção de parser para o português é a ferramenta de software Palavras desenvolvida por Eckhard Bick na Universidade de Arhus (Dinamarca) desde 2000.
Ao contrário de o Lx parser, o Palavras vem sendo utilizado por diversos pesquisadores da área de processamento de linguagem natural há vários anos, e portanto seu uso se configura numa verdadeira referência no tratamento de língua portuguesa.
De essa forma, o parser Palavras foi utilizado como ferramenta de anotação linguística para os trabalhos desenvolvidos nessa tese.
No entanto, cabe salientar que o uso de outros parsers não inviabiliza nenhuma das contribuições científicas aqui apresentadas.
Em a verdade, conforme será visto nas conclusões dessa tese, um trabalho futuro natural será experimentar Os domínios escolhidos para compor os corpora se justificam por a disponibilidade de especialistas disponíveis no Programa de Pós-graduação em Ciência da Computação durante essa etapa desse trabalho de tese.
Palavras para a anotação linguística.
Anotação Linguística O processo de anotação linguística do Palavras é aplicado individualmente a cada frase dos documentos.
A base linguística para esse processo foge ao escopo dessa tese, e por isso, todas as descrições dessa seção irão limitar- se à apresentação do processo de extração empregado, sem se aprofundar em questões linguísticas ou terminológicas.
O leitor interessado em maiores detalhes sobre o Palavras deve consultar a bibliografia original em e também visitar o site Floresta Sintáctica que apresenta alguns detalhes específicos além de a anotação on line de frases.
Cada frase reconhecida é armazenada por o parser como uma estrutura em árvore composta por nós terminais (as folhas da árvore) que representam as palavras e nós não-terminais que representam estruturas gramaticais.
Um exemplo disso é apresentado na Figura 3.1, em que está representada a anotação linguística realizada por o parser para a frase &quot;Essas duas cidades são os maiores e mais importantes centros de pesquisa no Brasil.».
A primeira observação quanto a o exemplo da Figura 3.1 é que utilizam- se nós não-terminais para representar estruturas gramaticais que podem ser tão complexas como orações, mas também estruturas mais simples como uma única palavra.
Importa saber que cada estrutura, seja uma oração ou uma palavra única, receberá do Palavras pelo menos duas etiquetas:
Já os nós terminais serão utilizados para representar palavras (ou tokens) que compõem as frases.
Para cada token, o parser associa um conjunto maior de informações como a forma canônica de cada palavra, sua morfologia, sua função sintática e sua provável semântica.
Processo Básico de Extração de Termos A primeira informação importante relativa ao processo de extração de termos, no contexto dessa tese, é considerar os sintagmas nominais (SNs) como os portadores de informação conceitual.
Em função de isso, todo Sn é, em princípio, um termo candidato a conceito do domínio.
Em esse sentido, somente critérios arbitrários (que serão vistos em detalhe nos próximos capítulos) irão definir quais SNs serão efetivamente considerados conceitos.
Porém antes disso, é necessário definir como os SNs serão detectados a partir de a saída do parser Palavras.
O processo de identificação de SNs passa inicialmente por a detecção dos não-terminais identificados por a etiqueta np, que para o Palavras são todos SNs compostos por mais de um token.
Um exemplo claro dessa detecção pode ser visto na Figura 3.2, em que encontram- se os SNs indicados por as etiquetas np.
Em a frase:
&quot;A gastroesquise é um defeito da parede abdominal anterior.»,
esses SNs são:·
&quot;A gastroesquise», indicado como Sn (etiqueta np), que cumpre a função de sujeito (etiqueta &quot;S&quot;);·
&quot;um defeito da parede abdominal anterior», indicado como Sn (etiqueta np), que cumpre a função de complemento do sujeito (etiqueta &quot;Cs&quot;);·
&quot;a parede abdominal anterior», indicado como Sn (etiqueta np), que cumpre a função de argumento da preposição &quot;de «(etiqueta &quot;DP&quot;).
Porém, SNs que são compostos por um único token não são identificados por o Palavras com a etiqueta np.
Por exemplo, reescrevendo a frase da Figura 3.2, retirando o artigo que inicia a frase, temos a nova frase anotada na Figura 3.3.
Em esse novo exemplo (Figura 3.3), o primeiro Sn é composto por um único token (&quot;Gastroesquise&quot;), que é anotado por o parser como sujeito da oração (etiqueta &quot;S&quot;) e substantivo próprio (etiqueta &quot;prop&quot;).
Mesmo não estando indicado por a anotação do Palavras com a etiqueta np, sem dúvida esse Sn deve ser considerado para a extração.
A diferença entre as duas frases (Figura 3.2 e 3.3) se resume a uma diferença de estilo de escrita, logo é natural que, para esse exemplo, sejam extraídos praticamente os mesmos SNs, ou seja:·
&quot;Gastroesquise», termo com único token indicado como sujeito da oração (etiqueta &quot;S&quot;) e indicado como um substantivo próprio (etiqueta &quot;prop&quot;);·
&quot;um defeito da parede abdominal anterior», indicado como sintagma nominal (etiqueta np), que cumpre a função de complemento do sujeito (etiqueta &quot;Cs&quot;);·
&quot;a parede abdominal anterior», indicado como sintagma nominal (etiqueta np), que cumpre a função de argumento da preposição &quot;de «(etiqueta &quot;DP&quot;).
De essa forma, o método básico de extração proposto analisa o resultado da anotação linguística feita por o parser para extrair todos os termos multi-- token marcados com a etiqueta np e todos os termos com um token único que estejam marcados como sujeito (etiqueta &quot;S&quot;), objeto (etiquetas &quot;Od», &quot;Oi «e &quot;Op&quot;) ou seus complementos (etiquetas &quot;Cs «e «Com o).
Aplicando o processo básico de extração de termos a todos os corpora citados na Seção 3.1 (Pediatria -- PED, Modelagem estocástica -- Me, Mineração de dados -- Md, Processamento paralelo -- Pp, e Geologia -- Geo) são apresentados, na Tabela 3.2, os números de SNs em cada corpus, devidamente divididos segundo o número de palavras que cada termo contém (unigramas, bigramas, etc.).
Em essa tabela a penúltima linha (N-grama) indica o número de SNs extraídos com 10 ou mais palavras e a última linha indica o total de termos extraídos.
Os SNs extraídos por o método básico, no entanto, carecem de um tratamento para que possam ser considerados candidatos a conceitos de um domínio.
Em esse sentido, as duas próximas seções propõem e avaliam um conjunto de regras heurísticas que visa refinar o conjunto de SNs extraídos com o processo básico.
Heurísticas Propostas Todas as heurísticas propostas são baseadas em análise linguísticas, logo, a efetividade das heurísiticas é dependente da qualidade da anotação.
Para aplicar as heurísticas assume- se que sejam recebidos todos sintagmas nominais (SNs) do corpus, a anotação sintática de cada palavra que o compõe e a função gramatical que o Sn desempenha na frase (sujeito, objeto ou complemento).
As heurísticas propostas estão divididas em três grupos:
Heurísticas de ajuste:
Heurísticas de Ajuste As heurísticas de ajuste têm por objetivo remover palavras que não carregam significado para o termo representado por o Sn.
As regras propostas são a remoção de:·
artigos no começo de um Sn;·
artigos em qualquer posição de um Sn;·
pronomes no começo de um Sn;
Além de· pronomes em qualquer posição de um Sn.
Ferramentas de extração que seguem abordagens estatísticas também oferecem técnicas semelhantes através do uso de listas de &quot;stop word».
No entanto, é importante salientar que as regras propostas de remoção de palavras são baseadas numa anotação linguística prévia, logo, elas tendem a ser mais precisas do que abordagens estatísticas.
Outro ponto importante das heurísticas de ajuste é que sua utilização reduz o número de palavras de um Sn.
Por exemplo, sua aplicação num trigrama pode transformar- lo num bigrama.
A1 -- Regra de Ajuste 1 ­ Remoção de Artigos no Início de SNs A primeira heurística é a simples remoção de artigos que aparecem no início do Sn.
Ainda que artigos tenham um papel importante como determinantes, a remoção do primeiro artigo de um Sn é coerente com o objetivo de extrair termos candidatos a conceitos.
O Sn &quot;o leite materno «é, sem dúvida, diferente do Sn &quot;um leite materno», porém ambos SNs fazem referência ao candidato a conceito de domínio &quot;leite materno».
Posto que os artigos são um conjunto finito de palavras, a remoção de artigos no início de um Sn é um processo simples que pode ser feito sem o auxílio de uma anotação linguística.
Apesar disso, a anotação linguística permite uma remoção mais precisa, pois nem sempre palavras usadas como artigo possuem uma única classe gramatical.
Por exemplo, o artigo definido feminino &quot;a», escreve- se igual à preposição &quot;a», ou ainda ao pronome oblíquo feminino &quot;a».
Portanto, ao colocar a palavra &quot;a «numa &quot;stop list», uma extração puramente estatística iria remover esta palavra sendo ela empregada como artigo, preposição ou pronome.
A aplicação dessa heurística sobre os 189.146 SNs encontrados no corpus de Pediatria resulta no ajuste de 81.031 SNs (cerca de 43%).
A2 -- Regra de Ajuste 2 ­ Remoção de Todos Artigos de SNs A segunda heurística de ajuste é a remoção de todos artigos encontrados num Sn, e não apenas artigos que aparecem no início de um Sn.
De essa forma, a regra A2 é uma generalização da anterior, e todas considerações feitas para a regra A1 sobre como a remoção de artigos altera o significado de um termo, continuam verdadeiras para a regra A2.
No entanto, essa segunda heurística dificilmente poderia ser aplicada num método puramente estatístico, pois com ela é possível considerar termos compostos por palavras não contíguas.
Um exemplo de aplicação da regra A2 sobre o Sn &quot;o leite da mãe «resulta no termo &quot;leite de mãe».
Note- se que a preposição &quot;de «e o artigo definido feminino &quot;a «estão contraídos na palavra «da.
A aplicação dessa heurística é a mais impactante, pois dos 189.146 SNs inicialmente anotados no corpus de Pediatria, pouco menos da metade possuíam artigos e foram, portanto, ajustados.
A3 -- Regra de Ajuste 3 ­ Remoção de Pronomes no Início de SNs Semelhante a a primeira heurística, a remoção de pronomes no início de SNs, a regra A3 tem por objetivo manter o Sn genérico o suficiente para ser considerado candidato a conceito.
Portanto, essa heurística sóé aplicada quando o pronome a ser removido não é o núcleo do Sn.
Cabe lembrar que, embora o usual seja que o núcleo de um Sn seja um substantivo (comum ou próprio), é possível ter como núcleo um adjetivo ou um verbo no particípio passado fazendo o papel de um substantivo, ou até um pronome referenciando um substantivo citado em outro lugar (uma anáfora).
A aplicação da regra A3 sobre os 189.146 SNs extraídos do corpus de Pediatria ajusta 12.793 SNs.
A4 -- Regra de Ajuste 4 ­ Remoção de Todos Pronomes de SNs De forma análoga aos artigos, a regra A4 propõe a remoção de pronomes que se encontram em qualquer posição de um Sn.
As mesmas considerações feitas na regra A3 são válidas, principalmente, a restrição que, sóé possível remover pronomes que não sejam núcleo do Sn.
De essa forma, a aplicação da regra A4 ao Sn &quot;o objetivo de seu movimento «transforma- o no termo &quot;o objetivo de movimento».
Note- se que o artigo &quot;o «não é removido, pois exemplifica- se a aplicação da regra A4 sozinha.
A aplicação dessa heurística sobre as frases que compõem o corpus de Pediatria resulta no ajuste de 18.230 termos do total de 189.146 termos extraídos.
Heurísticas de Descarte As heurísticas de descarte são regras que recusam Sn anotados que provavelmente não são termos representativos de um domínio.
Essas heurísticas são regras que descartam SNs que:·
contém numerais;·
contém outros símbolos além de letras, dígitos ou hífen;·
o núcleo é um pronome;
Ou· começam com um advérbio.
Ao contrário de as heurísticas de ajuste, as heurísticas de descarte não alteram o número de palavras dos SNs, mas reduzem significativamente o número total de SNs extraídos.
Considerando a aplicação de todas heurísticas de descarte sobre os 189.146 SNs originalmente extraídos do corpus de Pediatria recusou 55.896 SNs, ou seja, um pouco menos de 30% dos termos originalmente extraídos são descartados.
D1 -- Regra de Descarte 1 ­ Recusa de SNs com Numerais A primeira heurística de descarte recusa SNs que contêm numerais, seja na forma escrita ou utilizando caracteres numéricos (dígitos).
Apesar de ser uma heurística bastante restritiva que ignora termos como &quot;as sete maravilhas «ou &quot;os três mosqueteiros», essa heurística é frequentemente válida para descartar SNs que expressam quantidades que são comuns em textos científicos.
Exemplos de sucesso da aplicação dessa regra no corpus de Pediatria é o descarte dos SNs &quot;três meses «e &quot;ano 2000».
Em a verdade, essa heurística é bastante eficiente por excluir SNs que fazem referências a datas.
A aplicação da regra D1 sobre os 186.146 SNs extraídos do corpus de Pediatria resultou na recusa de 30.969 termos.
D2 -- Regra de Descarte 2 ­ Recusa de SNs com Símbolos Analogamente à recusa de SNs com numerais, a regra D2 descarta SNs que contém símbolos, ou seja, só aceita SNs compostos por letras e dígitos.
Porém, aceita- se também o caracter hífen(&quot;) que é usual em palavras compostas, como por exemplo:
&quot;recém-nascido «e &quot;bem-estar».
Muitos dos SNs recusados por a presença de símbolos também possuem numerais, como por exemplo, valores percentuais (&quot;46%&quot;).
Encontra- se também símbolos em endereços eletrônicos (info@saude.gv.
Br&quot;) ou representações abreviadas de números ordinais (&quot;2&quot;).
A aplicação da regra D2 nos 189.146 SNs extraídos do corpus de Pediatria resultaram na recusa de 40.989 SNs, tornando essa regra a mais restritiva de entre as heurísticas de descarte, ou seja, mais de 21% dos termos extraídos são descartados devido a essa heurística.
D3 -- Regra de Descarte 3 ­ Recusa de SNs com um Pronome como Núcleo Usualmente o núcleo de um Sn é um substantivo comum ou próprio.
No entanto, o núcleo de um Sn também pode ser um adjetivo, um verbo no particípio passado ou um pronome.
A terceira heurística de descarte visa aceitar somente SNs cujo o núcleo possui um significado autocontido, ou seja, o núcleo é um substantivo, adjetivo ou verbo no particípio passado.
Consequentemente, recusa- se SNs quando o núcleo é um pronome, ou seja, quando o Sn indica um termo explicitamente mencionado em outro ponto do texto (anáfora).
Algumas situações de SNs com núcleos de diferentes classes gramaticais são exemplificados nas frases indicadas na Tabela 3.3.
O esforço empregado gerou grandes expectativas, mas frustou as nossas.
Aqueles que sabiam, perguntaram.
Madalena arrependida eles nossas aqueles substantivo comum adjetivo particípio passado substantivo comum substantivo próprio particípio passado pronome pessoal pronome possessivo pronome demonstrativo Como pode ser observado nas frases 1 e 2 da Tabela 3.3, o Sn com o núcleo &quot;espertos «(um adjetivo) pode não ser tão adequado como conceito quanto o Sn &quot;os alunos espertos «que possui como núcleo um substantivo.
Por outro lado, observando a frase 3 da Tabela 3.3, o Sn &quot;os recém-nascidos», que também não possui substantivo, é bastante significativo, sendo talvez mais significativo que o Sn encontrado na frase 4, &quot;crianças recém-nascidas».
Por essa razão, opta- se por aceitar SNs que possuem como núcleo um adjetivo ou verbo no particípio passado, e não só substantivos.
Para a frase 5 observa- se a utilização de um nome próprio com uma função que se assemelha mais à de um substantivo, logo aceitar nomes próprios pode ser adequado.
Isso fica claro se comparado ao exemplo da frase 6, em que o adjetivo &quot;arrependida «traz menos informação que o Sn utilizado na frase 5.
Finalmente, para os exemplos nas frases 7, 8 e 9, fica claro que SNs que possuem pronomes como núcleo não fornecem bons candidatos a conceitos.
Isso verifica- se tanto em utilizações comuns, como na frase 7, quanto em estruturas mais complexas, como nas frases 8 e 9.
Note- se que, de acordo com o propósito da extração de termos, pode ser interessante descartar SNs segundo a classe gramatical do núcleo.
Para os trabalhos desenvolvidos nessa tese, são aceitos SNs que possuem como núcleo substantivos comuns ou próprios, adjetivos ou verbos no particípio passado, ou seja, recusa- se SNs cujo núcleo é um pronome.
A aplicação da regra D3 sobre os 189.146 SNs extraídos do corpus de Pediatria causou a recusa de 6.109 termos.
D4 -- Regra de Descarte 4 ­ Recusa de SNs que Iniciam com Advérbio Aúltima heurística de descarte baseia- se no fato de que alguns SNs não se referem explicitamente a um termo, mas apenas fazem referência a termos previamente mencionados.
Em esses casos, usualmente o Sn começa com um advérbio e possui como núcleo um adjetivo.
Esses SNs não são adequados a serem considerados candidatos a conceitos, pois eles não carregam uma informação completa.
Por exemplo, no corpus de Pediatria o Sn &quot;mais frequente «foi encontrado 11 vezes, mas nessas ocorrências ele foi empregado 5 vezes para referenciar o uso frequente de um medicamento, e 6 vezes para referenciar a adoção frequente de um hábito por um paciente.
No entanto, é inútil considerar o Sn &quot;mais frequente «como um candidato a conceito, pois somente observando os contextos onde o termo é empregado torna- se possível saber se ele está se referindo a um medicamento ou um hábito de pacientes.
A aplicação da heurística D4 sobre os 189.146 SNs do corpus de Pediatria fez com que apenas 650 termos fossem descartados.
Esse número é relativamente baixo, porém é importante perceber que sua remoção representa uma clara melhora no processo de extração, pois descartase SNs que não carregam informação conceitual.
Heurísticas de Inclusão As heurísticas de inclusão têm por objetivo detectar SNs implícitos.
De um ponto de vista linguístico, essas regras são as mais sofisticadas de entre as propostas, pois através de elas considerase SNs que não aparecem no texto, mas podem ser inferidos por a anotação linguística.
As heurísticas de inclusão são:·
remoção sucessiva de adjetivos;·
uso de predicado múltiplo;
E· conjunção de adjetivos.
O efeito prático das heurísticas de inclusão é o aumento no número total de SNs.
Por exemplo, os 133.250 SNs extraídos do corpus de Pediatria, que não foram descartados após a aplicação das heurísticas de descarte, dão origem a 46.617 SNs implícitos.
De essa forma, as heurísticas de inclusão são responsáveis por adicionar um número menor do que os 55.896 SNs que foram removidos por as heurísticas de descarte.
I1 -- Regra de Inclusão 1 ­ Detecção de SNs Implícitos por Remoção Sucessiva de Adjetivos A primeira heurística de inclusão está baseada na detecção de SNs contidos em SNs maiores por a remoção sucessiva de adjetivos.
Por exemplo, a frase &quot;Estudos realizados mostram o perigo de doenças virais hemorrágicas. «(
Figura 3.4), mostra um caso em que o processo básico de extração detectaria apenas os seguintes SNs:
&quot;Estudos realizados&quot;;
&quot;perigo de doenças virais hemorrágicas&quot;;
E &quot;doenças virais hemorrágicas».
A proposta dessa heurística consiste em gerar termos adicionais por a remoção dos adjetivos (ou verbos no particípio passado) ao fim de cada termo.
De essa maneira, a Tabela 3.4 apresenta os termos que seriam extraídos da frase exemplificada na Figura 3.4.
Em o corpus de Pediatria 40.156 SNs terminam com pelo menos um adjetivo.
A aplicação da heurística I1 a esses SNs resultou na inclusão de 44.020 novos SNs, ou seja, essa heurística é responsável por quase todos os 46.617 SNs incluídos por as heurísticas desse terceiro grupo.
I2 -- Regra de Inclusão 2 ­ Detecção de SNs Replicados por o Uso de Predicado Múltiplo Em a língua portuguesa é comum encontrar o uso de predicados com mais de um verbo.
Em esses casos, a sentença representa múltiplas frases com o mesmo sujeito e objeto, cada uma de elas utilizando um dos verbos do predicado.
A segunda heurística de inclusão atua nesse tipo de situação, considerando como se as frases com predicado múltiplo, fossem desmembradas em diversas frases com um único verbo.
De essa forma, a regra I2 não cria SNs diferentes dos originalmente encontrados, pois ela somente replica ocorrências de SNs, que são sujeito ou objeto de uma sentença que possui predicado com mais de um verbo.
Por exemplo, a frase &quot;Pacientes idosos compram e tomam remédios mais caros.»,
ilustrada na Figura 3.5, mostra esse tipo de situação.
Percebe- se, por a atribuição de etiquetas feita por o parser, assim como por o próprio sentido da frase descrita na Figura 3.5, que ela poderia ser reescrita por duas frases iguais em tudo exceto por o predicado:
&quot;Pacientes idosos compram remédios mais caros.»
&quot;Pacientes idosos tomam remédios mais caros.»
Caso essa frase seja desdobrada em duas, os SNs relacionados ao predicado duplo serão computados com duas ocorrências cada.
Em esse sentido, a regra I2 propõe que SNs que estejam relacionados com predicados múltiplos sejam computados tantas vezes quantos forem os verbos do predicado.
A aplicação da regra I2 sobre a frase da Figura 3.5 faz com que os SNs &quot;Pacientes idosos «(sujeito) e &quot;remédios mais caros «(objeto) sejam considerados duas vezes cada um, ou seja, como se a extração fosse feita sobre as frases desmembradas.
Em o corpus de Pediatria foram encontradas 3.413 frases com predicado múltiplo que deram origem a 3.472 novas ocorrências de SNs.
Cabe salientar que os predicados múltiplos podem ocorrer entre dois verbos, ou ainda numa lista de três ou mais verbos separados por vírgulas.
A Tabela 3.5 apresenta alguns exemplos de termos implícitos criados.
Observando a primeira e a segunda frase dessa tabela, percebe- se que a heurística I3 pode ser empregada sem riscos quando a conjunção é alternativa (&quot;ou&quot;), porém, quando a conjunção aditiva &quot;e «é empregada, a semântica da frase se presta a diferentes interpretações.
Enquanto que, da primeira frase, se compreende que basta a uma pessoa ser &quot;esperta «ou ser &quot;sábia «para prever dificuldades, a segunda frase sugere que somente a pessoa que for ao mesmo tempo &quot;esperta e sábia «poderá prever dificuldades.
No entanto, essa duplicidade de interpretação não invalida a existência, enquanto termos portadores de informação, dos SNs &quot;pessoas espertas «e &quot;pessoas sábias».
O aleitamento materno é vital para recém-nascidos normais e prematuros.
O defeito pode aparecer na parede abdominal anterior ou toráxica posterior.
Sn implícito pessoas sábias recém-nascidos prematuros a parede toráxica posterior As frases 3 e 4 mostram que é necessário substituir o mesmo número de adjetivos do Sn explícito, quantos forem os adjetivos encontrados após a conjunção.
Por exemplo, na frase 4 não seria correto gerar o termo implícito &quot;A parede abdominal toráxica posterior», pois os adjetivos que seguem a conjunção &quot;ou «correspondem a duas palavras (&quot;toráxica posterior&quot;) e portanto devem substituir em igual medida os adjetivos do termo explícito (&quot;abdominal anterior&quot;).
A aplicação da regra I3 sobre os termos originalmente extraídos do corpus de Pediatria resultou na inclusão de 861 novos SNs.
Avaliação Numérica das Heurísticas Propostas Em essa seção são relatados os experimentos realizados sobre o corpus de Pediatria com o intuito de avaliar o uso das heurísticas propostas.
Foi escolhido esse corpus por o fato de possuir associado a ele listas de termos de referência construídas por um grupo externo (www.
Ufrgs. Br/ textecc).
Essas listas são compostas por 1.534 bigramas e 2.660 trigramas e estão disponíveis no anexo A. Logo, torna- se possível comparar termos extraídos com a referência, utilizando medidas usuais da área de recuperação de informação.
Especificamente, exemplifica- se o benefício trazido por as 11 heurísticas propostas para a extração de bigramas e trigramas do corpus de Pediatria.
A quantificação desses benefícios é feita por o cálculo da precisão, abrangência e medida F (Seção 2.3.3).
Para avaliar a aplicação de cada heurística, compara- se as listas de bigramas e trigramas extraídos mais frequentes às listas de referência.
A extração básica de termos do corpus de Pediatria detecta 58.504 bigramas e 25.485 trigramas (veja Tabela 3.2), porém observando o número de bigramas e trigramas distintos, contabiliza- se apenas 17.407 e 15.577, respectivamente.
Logo, para as experiências dessa seção escolheu- se considerar listas com 10% desses termos, ou seja, listas com os termos mais frequentes.
Essa escolha de considerar os 10% mais frequentes organizados segundo a frequência absoluta é consistente com resultados preliminares das listas de referência (denominada LR), respectivamente.
Resultados Numéricos para as Heurísticas de Ajuste A Tabela 3.6 ilustra os benefícios trazidos por a aplicação das heurísticas de ajuste sobre os SNs extraídos do corpus de Pediatria.
Além de os valores de precisão (P), abrangência (R) e medida-F (F), a última coluna indica quantos termos da lista de referência foram encontrados na lista dos termos extraídos mais frequentes, ou seja, a intersecção entre LE e LR.
A lista de termos extraídos contém diversas ocorrências de termos repetidos.
Porém ao contabilizar o número de ocorrências de cada termo, reduz- se o tamanho da lista, pois considera- se apenas o número de termos distintos.
A primeira linha (nenhuma) mostra os resultados obtidos sem a aplicação de nenhuma das heurísticas.
A próximas 4 linhas indicam os resultados obtidos aplicando cada uma das heurísticas de ajuste individualmente.
Finalmente, a última linha (todas) indica os resultados obtidos aplicando todas heurísticas de ajuste simultaneamente.
A primeira observação dos dados da Tabela 3.6 é que a extração de SNs sem nenhuma heurística resulta em valores baixos de precisão e abrangência.
Esses valores são similares àqueles encontrados em métodos básicos de extração baseados no uso de anotação linguística feita por o Palavras.
No entanto, após a remoção de artigos percebe- se um grande aumento (de 25% a 43%) nos valores de precisão e abrangência.
As heurísticas de remoção de pronomes (A3 e A4) foram menos efetivas, mas ainda assim essas permitem um aumento razoável de 2% a 3% na precisão.
Note- se que a aplicação combinada de todas heurísticas de ajuste (linha todas) traz benefícios enormes como pode ser visto por o aumento de 38% e 35% nos valores de medida-F para bigramas e trigramas, respectivamente.
Resultados Numéricos para as Heurísticas de Descarte A análise das heurísticas de descarte inicia considerando os resultados já obtidos com a aplicação de todas as heurísticas de ajuste.
De essa forma, os resultados apresentados na primeira linha (todas A) da Tabela 3.7 consideram a aplicação de todas heurísticas de ajuste e nenhuma das heurísticas de descarte.
As 4 linhas seguintes representam os resultados obtidos aplicando todas heurísticas de ajuste e cada uma das heurísticas de descarte individualmente.
Finalmente, a última linha da Tabela 3.7 (todas A D) apresenta os resultados obtidos com todas as heurísticas de ajuste, bem como todas as heurísticas de descarte.
Observando as informações na Tabela 3.7 é possível perceber que a maior parte dos benefícios das heurísticas de descarte ocorre devido a a regra de recusa de SNs com símbolos (D2).
A recusa de Sn com numerais também causou um aumento interessante da medida-F (até 3%).
Além disso, para essas duas heurísticas (D1 e D2) percebeu- se um aumento mais significativo para bigramas, enquanto que para trigramas os benefícios foram menos impactantes.
As outras duas heurísticas (D3 e D4), ainda que afetando um número razoável de SNs, conforme informado nas Seções 3.3.2.3 e 3.3.2.4, tiveram efeitos menores tanto na precisão como abrangência.
Apesar disso, tanto para bigramas como para trigramas, as heurísticas ainda contribuíram com a recusa de termos inadequados, aumentando, portanto, o número de termos encontrados nas listas de referência.
Adicionalmente, o uso combinado de todas as heurísticas de descarte trouxe um inegável benefício na precisão das listas de 9% para bigramas e 5% para trigramas.
Este aumento de precisão é ainda mais notável devido a ser acompanhado por um aumento de 10% e 3% de abrangência, para bigramas e trigramas respectivamente.
Resultados Numéricos para as Heurísticas de Inclusão Analogamente a a análise feita para as heurísticas de descarte, a avaliação quantitativa das heurísticas de inclusão é feita considerando a aplicação de todas heurísticas dos dois grupos anteriormente citados.
A primeira linha (todas A D) da Tabela 3.8 apresenta os resultados obtidos com a aplicação de todas heurísticas de ajuste e descarte, e nenhuma das heurísticas de inclusão.
As 3 linhas seguintes indicam os resultados com a aplicação de todas heurísticas de ajuste e descarte com cada uma das heurísticas de inclusão aplicada individualmente.
Finalmente, a última linha (todas) indica os resultados obtidos com a aplicação de todas as 11 heurísticas propostas.
Observando os resultados da Tabela 3.8 é possível perceber que todas as heurísticas de inclusão apresentam incrementos na precisão e abrangência.
Observando cada heurística de inclusão individualmente percebe- se um aumento de 1% a 2% em precisão e abrangência.
Numericamente, mesmo a aplicação das 3 heurísticas traz um incremento entre 2% e 3% para todos os índices.
No entanto, cabe salientar que após a aplicação das heurísticas de ajuste e descarte os valores de precisão e abrangência já estavam altos em comparação com outras abordagens com o mesmo propósito de extração de termos.
De essa forma, mesmo o incremento de 1% de precisão obtido jáé significativo quando se passa de uma precisão de 57% a 58%.
O benefício trazido por a aplicação das heurísticas é claro.
Os resultados combinados mostram um aumento consistente que trouxe os valores de 7% a 13%, somente com o processo básico de extração, a valores entre 40% e 68%, com a aplicação de todas as heurísticas.
Cabe salienar que a ordem de aplicação das heurísticas não afeta o resultado final das listas extraídas.
Outro fator importante a observar é que os resultados foram testados a partir de uma anotação linguística feita por o parser Palavras, considerando especificamente os sintagmas nominais.
Em um trabalho anterior, uma outra ferramenta chamada OntoLP seguindo os mesmos passos, ou seja, anotação por o Palavras e detecção de SNs, chegou a valores de precisão semelhantes aos valores iniciais sem o uso de heurísticas.
Ainda que seja difícil comparar trabalhos distintos devido a os corpora utilizados, listas de referência e número de termos extraídos, percebe- se que a precisão obtida anteriormente ao uso das heurísticas propostas era sensivelmente inferior aos valores por volta de 60% de precisão conseguidos com o uso de todas as heurísticas.
Por essas razões, acredita- se que as heurísticas propostas são uma contribuição clara para qualificar o processo de extração automática de termos.
Ainda que os testes de precisão, abrangência e medida-F tenham sido realizados somente sobre o corpus de Pediatria, os resultados obtidos para bigramas e trigramas foram consistentes entre si.
Cabe lembrar que a razão por a qual não foram feitos mais testes, foi a inexistência de listas de referência a serem usadas como paradigma de qualidade do processo automático de extração de termos.
O processo de extração com todas heurísticas propostas aplicado aos corpora citados anteriormente resultou no número de termos descritos na Tabela 3.9.
Em essa tabela temos o número de termos gerados para cada corpora (Pediatria -- PED, Modelagem estocástica -- Me, Mineração de dados -- Md, Processamento paralelo -- Pp, e Geologia -- Geo) e dividos segundo o número de palavras dos termos (unigramas, bigramas, etc.).
Essa tabela atualiza o número de termos originalmente extraídos expresso na Tabela 3.2.
Uma observação comparativa do número de termos antes e após a aplicação das heurísticas, respectivamente, Tabelas 3.2 e 3.9, mostra que o número total de termos varia pouco.
No entanto, há um incremento de qualidade, pois descartou- se termos inadequados e incluiu- se termos adequados.
A Figura 3.7 mostra graficamente essa variação para cada corpus.
Observando a Figura 3.7, percebe- se um grande aumento no número de termos com menos palavras, especialmente unigramas, enquanto que o número de termos com muitas palavras diminui bastante.
O mais interessante é que essa alteração na distribuição do número de termos acontece com um incremento de qualidade, pois descartou- se termos inadequados e incluiu- se termos adequados, como indicam os testes de precisão vistos anteriormente.
Produto Final da Extração Como produto final da extração realizada gera- se um recurso linguístico composto por um conjunto de termos (SNs) extraídos ao qual associa- se informações contextuais que podem ser muito úteis em várias aplicações dos conceitos.
Essas informações oferecem dados relevantes de cada Sn extraído por si só, como sua forma original e sua forma lematizada (forma canônica), mas também informações que remetem ao contexto em o qual cada termo foi encontrado, como por exemplo, a função gramatical que o Sn desempenha na frase, ou o verbo ao qual o termo está relacionado.
Especificamente, para cada Sn extraído associam- se as seguintes informações:
O termo na sua forma original;
O termo na sua forma canônica;
O número de palavras que compõem o termo;
A palavra indicada como núcleo do termo na sua forma canônica;
A etiqueta sintática do núcleo (substantivo, adjetivo, etc.);
A (s) etiqueta (s) semântica (s) do núcleo (uma estimativa feita por o parser);
A etiqueta morfológica do núcleo (gênero, número, etc.);
A função gramatical do termo na oração (sujeito, objeto, etc.);
A posição ocupada por o termo na frase (onde situam- se as palavras que compõem termo);
O número total de palavras da frase;
O predicado ao qual o termo exerce sua função gramatical na forma original;
O predicado ao qual o termo exerce sua função gramatical na forma canônica;
A etiqueta sintática do predicado ao qual o termo exerce a sua função gramatical;
A etiqueta morfológica do predicado ao qual o termo exerce a sua função gramatical;
A posição ocupada por o predicado ao qual o termo exerce sua função gramatical na frase (onde situam- se as palavras que compõem o predicado);
Um identificador da frase de onde o termo foi extraído;
Um identificador do documento de onde o termo foi extraído.
Para exemplificar o recurso linguístico disponível após a extração, considere- se um documento d composto por as frases &quot;Os problemas sociais encontrados demonstram a necessidade e propiciam a busca de soluções criativas.»
e &quot;O problema social é o principal vetor de progresso científico e tecnológico.»,
cuja anotação linguística está descrita na Figura 3.8.
O processo de extração proposto, aplicado a esse documento, resulta nos 17 termos apresentados na Tabela 3.10 com suas respectivas informações associadas (o número em negrito identifica o campo, segundo a enumeração definida nessa seção).
Uma vez extraídas essas informações, é possível transformar- las de diversas maneiras que servirão de base para as próximas etapas desenvolvidas nessa tese.
Após a extração dos termos exposta no capítulo anterior, o próximo passo é ordenar os termos extraídos segundo sua relevância para o domínio de interesse.
Conforme discutido na introdução, ainda que técnicas linguísticas sejam utilizadas para detectar os termos, técnicas estatísticas são a base para identificar a importância de cada termo no domínio representado por o corpus.
Admite- se que os termos mais frequentes tendem a ser mais importantes do que os menos frequentes.
Cabe lembrar que, no contexto dessa tese, o que consideramos termos candidatos a conceito são fruto de uma extração linguística refinada, que fornece somente termos portadores de informação, posto que são sintagmas nominais cuidadosamente tratados por as heurísticas descritas no capítulo anterior.
Essa qualidade de termos extraídos é um fato importante no contexto da abordagem de ordenação adotada.
De essa forma, podemos assumir que a frequência é adequada como critério de importância, ao contrário de o que acontece com termos obtidos através de extração puramente estatística, em que palavras muito frequentes podem ser completamente desprovidas de informação conceitual.
As abordagens tradicionais da área de recuperação de informações que buscam índices para estimar a relevância dos termos extraídos baseiam- se na análise de um único corpus.
Abordagens mais recentes, porém, se valem do uso de corpora contrastantes, ou seja, corpora de outros domínios, para melhor estimar a relevância de termos no domínio de interesse.
Em esse sentido, a abordagem proposta nesse capítulo segue a linha dessas abordagens recentes propondo um novo índice que leva em consideração a frequência do termo, mas também a sua disjunção no corpus de domínio em relação a os corpora contrastantes.
Por essa razão, o novo seja, frequência de termo, frequência de disjunção de corpora.
Esse capítulo, inicialmente, descreve alguns índices já existentes para a ordenação de termos segundo a relevância.
Em seguida, na Seção 4.2, apresenta- se o novo índice tf-dcf, proposto para ordenar, segundo a relevância, os termos extraídos.
Em a Seção 4.3 avalia- se esse índice através de sua aplicação prática sobre o corpus de Pediatria descrito anteriormente e a comparação com a aplicação de índices já existentes.
A Seção 4.4 apresenta uma análise dos limites do índice proposto examinando o impacto na sua precisão em função de a escolha de diferentes corpora contrastantes.
Essa seção apresenta índices tradicionais para estimar a relevância de termos extraídos de um corpus.
Especificamente, apresentam- se os seguintes índices:
Frequência absoluta de termo;
Frequência de termo e frequência inversa de documento segundo Manning e Schültz;
Índice de especificidade de domínio segundo Park;
Índice termhood segundo Kit e Liu;
E frequência de termo, Frequência Absoluta de Termo -- tf A maneira mais direta de estimar a relevância de um termo extraído é contar sua frequência absoluta, ou seja, o número de vezes que esse termo aparece nos textos.
Esse índice, chamado frequência absoluta de termo, tem o apelo de ser intuitivo e fácil de calcular.
Apesar disso, é necessário decidir algumas questões práticas frequentemente encontradas no processamento de textos em linguagem natural.
Especificamente, nessa tese discute- se brevemente o tratamento de termos na forma canônica, de sinônimos e de anáforas.
Após essas discussões, apresenta- se formalmente a definição da frequência absoluta de termos adotada nessa tese.
Tratamento de Termos na Forma Canônica A primeira das questões relevantes para o cálculo da frequência absoluta diz respeito às variações morfológicas dos termos extraídos.
Variações de número são particularmente frequentes.
Por exemplo, o termo &quot;recém-nascido «no corpus de Pediatria, aparece 122 vezes na sua forma singular &quot;recém-nascido «e 177 vezes na sua forma plural &quot;recém-nascidos».
Variações de gênero também são encontradas, como é o caso do termo &quot;paciente hospitalizado «que aparece 6 vezes na sua forma plural masculina &quot;pacientes hospitalizados «e 2 vezes na sua forma plural feminina &quot;pacientes hospitalizadas», nesse mesmo corpus de Pediatria.
Parece razoável considerar essas diferentes variações linguísticas de um termo como ocorrências do mesmo termo.
De essa forma, os termos são comparados e computados segundo sua forma canônica, ou seja, sempre considerados no singular, masculino e infinitivo (para verbos).
Por exemplo, as ocorrências dos termos &quot;recém-nascido «e &quot;recém-nascidos «são agrupadas com uma frequência absoluta de 299 ocorrências, devido a todas essas terem a mesma forma canônica:
&quot;recém-nascer». Note- se que essa aglutinação de termos com diversos formatos, mas a mesma forma canônica, sóé possível quando os recursos computacionais empregados na extração automática de alguma forma disponibilizam essa informação.
Em o contexto do processo utilizado nessa tese, é necessário que o parser associe a forma canônica a cada termo extraído e a extração preserve essas informações.
Em a eventualidade de utilizar um parser que não disponibiliza forma canônica, é necessário prover a associação de termos segundo as variações, por exemplo, através de técnicas de redução a formas radicais (stemming).
Tratamento de Sinônimos A segunda questão relevante para o cálculo da frequência absoluta é a ocorrência de sinônimos.
O uso de sinônimos se presta a discussão, pois, recomendações de estilo de escrita sugerem que não se escreva de forma repetitiva, ocasionando um maior uso de sinônimos.
Por exemplo, os termos &quot;rocha magmática «e &quot;rocha ígnea», presentes no corpus de Geologia, representam, na imensa maioria dos contextos de utilização, o mesmo conceito.
Segundo literatura especializada em geologia os termos &quot;rocha magmática «e &quot;rocha ígnea «referem- se ao mesmo tipo de rocha que é a rocha gerada por cristalização de magma.
Infelizmente, esse tipo de sinônimo é bastante difícil de ser detectado somente a partir de os textos que compõem o corpus.
Assim, esse processo poderia ser feito com o auxílio de outros recursos linguísticos, como, por exemplo, um dicionário de sinônimos.
Um caso mais fácil de detectar durante a extração são termos como &quot;areia marinha «e &quot;areia de mar», pois, é possível reconhecer radicais num adjetivo (&quot;marinha&quot;) e inferir que o uso de um sintagma preposicional com a preposição &quot;de «e o substantivo correspondente (&quot;de mar&quot;).
No entanto, algumas vezes, como é o caso com esses termos, o uso de jargão especializado pode invalidar essa tentativa.
Por exemplo, &quot;areias marinhas «e &quot;areia de mar «não são sinônimos no contexto de Geologia.
Um sinônimo mais adequado para &quot;areia marinha», nesse contexto, seria o termo &quot;areia de praia».
Essa situação, onde sinônimos de fato são de difícil identificação, e termos onde a semelhança é mais facilmente detectável, mas não necessariamente confiável, motiva a decisão de desconsiderar a busca por sinônimos nos trabalhos dessa tese.
Note- se que abandona- se a busca de sinônimos para o propósito de identificação de conceitos, mas esse fato não implica que essa busca não possa ser bem mais relevante para outros propósitos.
Tratamento de Anáforas Uma situação semelhante aos sinônimos é o problema da identificação de anáforas1 nas frases do corpus.
O processo de identificação de anáforas também poderia ser extremamente útil na identificação do número total de vezes que um determinado termo está sendo referenciado num texto, pois além de as referências explícitas, as anáforas representam ocorrências implícitas do termo ao qual elas se referem.
Se contabilizarmos somente as ocorrências explícitas de um termo e ignorar diversas referências que podem ter sido feitas a esses termos através de outras expressões que, muitas vezes, são empregadas por questões de estilo de escrita.
Porém, o processo de resolução (identificação) de anáforas é bastante complexo.
Em contraposição a essa dificuldade, é natural assumir que o número de anáforas referenciando a cada termo seja proporcional ao seu número de ocorrências explícitas.
Obviamente, essa suposição não será matematicamente precisa para todos os termos, mas é razoável esperar que, em linhas gerais, ela ocorra de forma relativamente homogênea para os termos mais frequentes.
Devido a a dificuldade do tratamento de anáforas e à proporcionalidade no aumento de ocorrências implícitas, decidiu- se ignorar anáforas no escopo dessa tese.
Definição Formal da Frequência Absoluta de Termo Partindo do número de ocorrências de cada termo em cada um dos documentos de um corpus c, a definição formal da frequência absoluta de um termo t é expressa por:
Frequência de Termo e Inversa de Documento -- tf-idf O uso da frequência absoluta como medida de relevância para listas obtidas com métodos puramente estatísticos é uma abordagem muito simples, que pode produzir resultados precários.
Termos muito frequentes, como expressões usuais numa língua, podem ter frequências absolutas muito altas, apesar de não possuir grande relevância para o corpus de domínio.
Essa éa motivação do uso de &quot;stop lists «que define termos (ou palavras) que devem ser desconsideradas durante o processo de extração.
Em a verdade, sem &quot;stop lists «qualquer método puramente estatístico indica como os mais frequentes, termos sem relevância conceitual, como preposições e expressões usuais.
O uso de frequência de termos como índice de relevância é menos prejudicial para métodos de extração baseados em abordagens linguísticas.
Por exemplo, a anotação sintática de um corpus, Uma anáfora é uma expressão que se refere a, ou substitui, outra expressão no texto.
Por exemplo, sejam as frases «Os sedimentos preenchem os espaços criados por a subida relativa do nível do mar.
Eles são depositados episodicamente e possuem distribuição local.».
A expressão &quot;Eles «que inicia a segunda frase é uma anáfora que refere- se à expressão &quot;Os sedimentos «que inicia a primeira frase.
Uma alternativa para a frequência absoluta de termo, bem conhecida na área de recuperação de informação, é considerar de maneira distinta a frequência dos termos entre os vários documentos do corpus.
O trabalho seminal de Spärck--Jones mostra a importância de considerar termos frequentes e infrequentes para a recuperação de documentos.
Essas ideias levaram ao modelo probabilístico de relevância de termos para documentos de Robertson e Spärck--Jones.
Croft e Harper, e mais tarde Robertson e Walker, propuseram formulações para um índice que leva positivamente em consideração a frequência do termo (tf), i.
e, o número de ocorrências de um termo t num documento d, e negativamente o número de documentos onde esse termo aparece pelo menos uma vez (idf).
Esse índice, denominado tf-idf possui muitas formulações, e.
g, mas nessa tese será considerado a formulação proposta por Bell, por ser uma definição mais robusta que as demais citadas.
O índice tf-idf é formalmente definido para o termo t, para cada documento d que pertence ao corpus c e possui pelo menos uma ocorrência de t, da seguinte forma:
Onde tft, d é o número de ocorrências do termo t no documento d;
Dpcq é o conjunto de todos pcq documentos de um corpus c;
E Dt é o subconjunto desses documentos onde t ocorre pelo menos uma vez.
Observando a equação (4.2), é possível observar as partes tf e idf.
A parte tf considera a variação logarítmica da frequência do termo, pois a variação das ocorrências dos termos se aproxima da distribuição exponencial.
De essa forma, um termo que possui 10 ocorrências não é 10 vezes mais importante que um termo que ocorre uma única vez, mas sua relevância é uma ordem de magnitude maior.
A parte idf corresponde a um valor numérico que varia de logp2q para um termo que aparece em todos documentos do corpus, até logp1`| Dpcq| q para um termo que aparece apenas num documento.
A ideia por trás da fórmula do tf-idf é que um termo t é mais relevante para um documento d, se ele é muito frequente nesse documento, e aparece em poucos documentos, ou idealmente num único documento.
A popularidade desse índice é justificada em parte porque ele evita que termos frequentes presentes em vários documentos sejam considerados mais relevantes do que mereçam.
Em a verdade, tf-idf é um índice eficaz para identificar palavras chave, pois ele atribui relevância a termos adequados para indexação ou categorização de documentos.
O uso de tf-idf para estabelecer a relevância de termos para corpus de domínio, ao invés de pares termo-documento, foi proposta por Manning and Schütze.
Com esse propósito, é necessário obter um índice único por termo, logo, a proposta desses autores é somar os valores de um mesmo termo para todos os documentos, de forma a obter um valor único para cada termo.
De acordo com esses autores, a expressão formal desse índice para estimar a relevância de um termo t num corpus c é dada por:
Além de as iniciativas baseadas em analisar um único corpus, a comunidade científica vem propondo novas abordagens baseadas na observação de um conjunto de corpora que permita uma visão em perspectiva de quais termos são relevantes para um corpus de domínio.
As primeiras iniciativas para considerar a relevância de termos num corpus de domínio com o auxílio de corpora contrastantes inclui os trabalhos de Chung em 2003 e Drouin em 2004.
Entretanto, é somente com o trabalho de Park, em 2008, que aparecem as primeiras definições formais de um índice para expressar a relevância de termos baseadas em corpora contrastantes.
Em esse trabalho, um índice chamado Especificidade de Domínio de Termo (em inglês, term domain specificity) foi expresso como a razão entre a probabilidade de um termo t num corpus de domínio c e a probabilidade desse mesmo termo num corpus genérico contrastante g..
Formalmente, o índice proposto por Park É expresso por:
Onde pt expressa a probabilidade de ocorrência do termo t no corpus c;
E N pcq é o número total de termos nesse corpus c, i.
e, N&quot;@ t1 tft1.
Adaptando a definição original de Park Para considerar, não um único corpus, mas um conjunto de corpora contrastantes G, e adotando uma notação mais simples, a definição do índice de especificidade de domínio de termo t num corpus de domínio c redefine- se por:
Onde V pcq corresponde ao vocabulário do corpus c, ou seja, todos os termos que fazem parte do pGq corpus c;
V corresponde a união dos vocabulários de todos os corpora contrastantes, ou seja, todos corpora que pertencem a G. Seguindo a mesma abordagem de corpora contrastantes, o trabalho de Kit e Liu, em 2008, propõe um índice denominado termhood.
Esse índice, assim como o tds, segue a ideia que um termo relevante para um domínio é mais frequente no corpus desse domínio do que em outros corpora.
A principal diferença da proposta de Kit e Liu é que, ao invés de considerar a probabilidade de ocorrência do termo, considera- se a ordenação (rank) do termo no vocabulário (conjunto de todos os termos) do corpus.
A definição formal de Kit e Liu para o índice termhood do termo t no corpus c, considerando a existência de um corpus contrastante g, é expressa por:
Para o segundo termo mais frequente, o valor de rt é igual a| V pcq|´ 1, e assim por diante pcq até rt igual a 1 para o termo menos frequente.
Observando o índice termhood (Eq.
4.6), percebe- se que ele é a diferença entre o valor de ordenação normalizado para o corpus de domínio c e o valor de ordenação normalizado para o corpus contrastante g..
A normalização é feita com o objetivo de manter o valor do índice termhood dentro de o intervalo r´ 1, 1s, pois assim o tamanho do vocabulário dos corpora c e g não desequilibra o valor do índice thd.
Intuitivamente expandindo a definição de Kit e Liu para uma situação onde exista um conjunto G de corpora contrastantes, a expressão formal do índice termhood é generalizada por:
Onde rt é o valor de ordenação do termo t para o corpus composto por a união de todos os corpora contrastantes em G, ou seja, o termo mais frequente da união de todos corpora contrastantes será igual a cardinalidade da união dos vocabulários de todos os corpora contrastantes.
Cabe salientar que, mesmo nessa situação com vários corpora contrastantes, o valor do índice thd ficará dentro de o intervalo r´ 1, 1s.
Frequência de Termo e Inversa de Domínio -- TF-IDF Kim Propuseram, num artigo publicado em 2009, uma ideia intuitiva de índice de relevância considerando o princípio básico do índice tf-idf, cujo propósito original é identificar quando um termo é adequado para representar um documento específico.
De essa forma, a proposta de Kim Não propõe um índice verdadeiramente novo, mas sim faz uma releitura do índice tf-idf que originalmente considera a frequência de termo e frequência inversa de documento.
A proposta de Kim Aplica a mesma ideia, porém considera, ao invés de as ocorrências de termos em documentos individualmente, as ocorrências de termos em cada corpus individualmente.
A nomenclatura utilizada por Kim Utiliza as mesma letras (TF-IDF), porém utiliza- as para abreviar a expressão frequência de termo e frequência inversa de domínio, em inglês, term frequency, inverse domain frequency.
Para evitar confusão com a definição original do índice tf-idf, o índice proposto por Kim Será escrito com letras maiúsculas.
Conforme proposto por Kim, o índice TF-IDF é formalmente definido por:
Onde tft é a frequência absoluta do termo t no corpus c;
G° é o conjunto de todos os corpora contrastantes e o corpus c;
E Gt° é o subconjunto de G° onde o termo t aparece pelo menos uma vez.
Cabe salientar que a definição formal do índice tf-idf usada como inspiração da proposta feita por Kim Não é tão robusta como a proposta por Bell (Eq.
4.3). Por exemplo, se um termo t aparece em todos corpora, a parte IDF da equação 4.8 será igual a 0, portanto, o valor do índice TF-IDF para o termo t será igual a 0, ou seja, o termo t será considerado menos relevante do que qualquer um dos demais, independente do número de vezes que ele possa ocorrer.
Outra diferença significativa entre as equações 4.3 e 4.8, ocorre na parte tf.
A formulação de Bell (Eq.
4.3) utiliza o logaritmo da frequência absoluta, enquanto Kim (Eq.
4.8) considera diretamente a frequência relativa de termo.
Proposta de um Novo índice de Relevância O objetivo de todos os índices apresentados na seção anterior é obter, para cada termo, um valor numérico diretamente proporcional a sua relevância no domínio.
De essa forma, ordenando os termos segundo os índices é possível descobrir quais de eles são os mais relevantes de entre os extraídos do corpus.
Aplicações da área de engenharia de conhecimento, como a extração de termos candidatos a conceitos de uma ontologia, podem, então, se valer desses índices de relevância.
A frequência absoluta de termo, obviamente indica relevância, pois um termo que é muito frequente será provavelmente relevante para o domínio.
De a mesma forma, o índice tf-idf (equação 4.3) pode ser visto como uma alternativa para indicar a relevância, pois ele permite detectar termos que são típicos de documentos do corpus de domínio, ou seja, palavras chaves para indexar os documentos.
No entanto, os índices tds (Eq.
4.5), thd (Eq.
4.7) e TF-IDF (Eq.
4.8) possuem um diferencial para indicar relevância de termos, pois eles permitem uma observação dos termos do domínio de interesse em perspectiva com a observação de corpora de outros domínios.
Apesar disso, esses índices que utilizam corpora contrastantes possuem particularidades bastante distintas, que revelam iniciativas empíricas de contornar o problema de ordenação de termos segundo a relevância.
A Tabela 4.1 sumariza essas diferenças que são, na sequência, discutidas em detalhe.
A primeira diferença entre eles é a forma como esses índices consideram as ocorrências de termos no corpus de domínio.
Os índices tds (Eq.
4.5) e TF-IDF (Eq.
4.8) calculam uma pcq frequência relativa de termo, pois a probabilidade de termo (pt) do índice tds e a parte tf do índice TF-IDF são calculadas com a frequência absoluta dividida por o número total de termos no corpus de domínio.
O índice thd (Eq.
4.7), porém, calcula um valor de ordenação (rank) normalizado que, ainda que seja função da frequência absoluta, fornece uma relação linear entre os termos.
Cabe salientar que a distribuição dos valores de frequência absoluta tende a seguir uma lei de Zipf, i.
e, o termo mais frequente tende a ter o dobro de ocorrência do segundo mais frequente, o triplo de ocorrências que o terceiro mais frequente, e assim por diante.
Eventualmente, de acordo com a língua escolhida a distribuição dos termos pode não seguir uma distribuição de acordo com a lei de Zipf, mas para os propósitos da análise feita nesse capítulo a distribuição permanece equivalente.
A segunda diferença consiste na forma como ocorrências nos corpora contrastantes afetam o valor numérico do índice.
O índice tds (Eq.
4.5) penaliza termos que ocorrem nos corpora contrastantes dividindo a probabilidade de ocorrência no corpus de domínio por a probabilidade no conjunto de corpora contrastantes.
O índice thd (Eq.
4.7) também penaliza termos que são encontrados nos corpora contrastantes, mas nesse caso, é subtraído o valor de ordenação normalizado no corpus de domínio por o valor equivalente nos corpora contrastantes.
Por outro lado, a abordagem utilizada no índice TF-IDF segue uma outra ideia ao recompensar termos que aparecem apenas no corpus de domínio, através da multiplicação da parte tf por o logaritmo do número total de corpora.
Essa recompensa atribuída por o índice TF-IDF vai decaindo conforme o termo aparece num número maior de corpora contrastantes, até cair para 0 quando o termo aparece em todos corpora.
Cabe salientar que apesar de o valor da recompensa atribuída decair proporcionalmente ao número de corpora onde o termo aparece, a recompensa não depende do número total de ocorrências do termo nos corpora contrastantes.
Tendo essas questões em mente, propõe- se um novo índice para estimar a relevância de termos para um domínio, seguindo a ideia geral de observar corpora contrastantes.
No entanto, esse novo índice se distingue dos demais por a forma como são consideradas as ocorrências de um termo no corpus de domínio, e, principalmente, na forma como as ocorrências do termo em corpora contrastantes afetam numericamente o índice.
Especificamente, propõe- se modelar o efeito de ocorrências de um termo em corpora contrastantes com um mecanismo chamado forma matemática de penalizar um termo proporcionalmente ao número de corpora contrastantes em que ele aparece, e também ao número de ocorrências desse termo em cada um desses corpora.
Frequência de Termo e Disjunção de Corpora -- tf-dcf A proposta feita nessa tese, assim como outras iniciativas com corpora contrastantes, baseia- se numa indicação primária de relevância de termo (devido a ocorrências no corpus de domínio) e de um mecanismo de recompensa/ penalização (devido a ocorrência em corpora contratantes).
A base do índice tf-dcf é considerar a frequência absoluta de termo como indicação primária da relevância de um termo.
Em seguida, escolhe- se penalizar termos que aparecem nos corpora contrastantes dividindo a frequência absoluta do termo no corpus de domínio por a composição geométrica da sua frequência absoluta em cada um dos corpora contrastantes.
A definição formal do índice tf-dcf, para o termo t no corpus c, considerando um conjunto de corpora contrastantes G, é:
A escolha da frequência absoluta como indicação primária da relevância do termo t no corpus c, ao invés de a frequência relativa (como tds e TF-IDF), ou rank (como thd), visa manter a simplicidade do índice por duas razões principais:·
Acredita- se que não existe a necessidade de linearização, como o uso de rank no índice thd, nem existe a necessidade de normalizar o valor por o tamanho do corpus, como nos índices tds e TF-IDF, na verdade, se desejado, qualquer normalização permanece possível após o cálculo do índice tf-dcf;·
Acredita- se que, manter uma relação numérica direta do índice tf-dcf com a frequência absoluta (tf), preserva uma interpretação intuitiva dos valores do índice, ou seja, o valor numérico do tf-dcf será igual ao valor de tf, caso o termo não ocorra nos corpora contrastantes, ou inferior ao valor de tf, caso o termo ocorra nos corpora contrastantes.
A composição geométrica das frequências absolutas de um termo nos corpora contrastantes foi escolhida para expressar a penalização aos termos que não são exclusivos ao corpus de domínio.
Essa penalização se materializa por a divisão expressa na equação 4.9, que tenta abranger as seguintes premissas:·
O número de ocorrências de um termo em cada corpora contrastante se distribui segundo a lei de Zipf ou outra lei com comportamento semelhante 2, logo para estimar corretamente a influência das ocorrências nos outros corpora é necessário linearizar esse número de ocorrências;·
Um termo que aparece somente no corpus de domínio não deve ser penalizado, ou seja, termos que não ocorrem nos corpora contrastantes devem ter o divisor da equação 4.9 igual a 1, ou seja, o valor de tf-dcf será igual ao valor de tf;
E· Um termo que ocorre em vários corpora contrastantes tende a ser menos relevante do que se ele ocorresse em poucos corpora.
Devido a a primeira premissa, decide- se utilizar uma função logarítmica da frequência absoluta pgq do termo em cada corpora contrastante (tft).
Essa decisão segue o mesmo princípio adotado na proposição original do índice tf-idf feita por Robertson and Spärck--Jones.
A segunda premissa motivou uma adaptação na função logarítmica com a adição do valor 1 dentro e fora de a função logarítmica, para retornar um valor 1 quando o número de ocorrências de um termo nos corpora contrastantes é igual a 0.
Essa adaptação segue o mesmo princípio adotado por Bell Em a sua definição formal do índice tf-idf (Eq.
4.2). Finalmente, a terceira premissa leva ao uso do produto do logaritmo das ocorrências em cada corpora contrastante.
O produto representa que a importância das ocorrências deverá crescer geometricamente, conforme o termo ocorra em diversos corpora contrastantes.
A definição formal, proposta na equação 4.9, faz com que um termo seja menos relevante para o corpus de domínio, caso ele ocorra poucas vezes em muitos corpora contrastantes, do que se ele ocorresse muitas vezes em poucos corpora contrastantes.
Adicionalmente, o uso do produto do logarítimo das ocorrências é compatível com a intenção de que o divisor da equação 4.9 seja igual a 1 quando o termo não ocorra nos corpora contrastantes.
Análise Comparativa da Precisão do índice Proposto A exemplo da avaliação das heurísticas propostas feita no capítulo anterior (Seção 3.4), essa seção apresenta a avaliação do índice tf-dcf como indicador da relevância de termos extraídos.
Mostra- se que os resultados obtidos com o índice proposto são superiores aos resultados obtidos com todos os demais índices da literatura apresentados nesse capítulo.
Mais uma vez retoma- se o corpus de Pediatria e suas listas de referência para bigramas e trigramas.
Cabe salientar, porém, que ao contrário de os experimentos realizados para as heurísticas propostas, o processo aqui desenvolvido para avaliar o índice tf-dcf proposto é independente da língua, ou mesmo do processo de extração que disponibilizou os termos extraídos.
Os testes realizados a seguir utilizaram o corpus de Pediatria (PED) como corpus de domínio, e os demais corpora, apresentados anteriormente na Seção 3.1, como corpora contrastantes (Modelagem estocástica -- Me, Mineração de dados -- Md, Processamento paralelo Pp, e Geologia -- Geo).
Todos esses corpora foram submetidos ao processo de extração descrito no capítulo anterior, considerando a aplicação de todas as 11 heurísticas propostas.
Em esse contexto, considera- se como lei com comportamento semelhante aquelas que seguem uma progressão geométrica.
Processo Geral de Experimentação De entre os termos extraídos de todos os corpora, apenas os bigramas e trigramas foram mantidos, pois as listas de referência disponíveis não possuíam unigramas, nem termos com mais do que 4 palavras.
Cabe lembrar que, as listas de referência foram desenvolvidas por um grupo externo, são compostas de 1.534 bigramas e 2.660 trigramas, e que estão disponíveis no anexo A dessa tese.
Conforme dito no capítulo anterior, o processo de extração aplicado ao corpus de Pediatria resultou num total de 33.340 bigramas e 27.587 trigramas, considerando termos repetidos.
Porém, feita a contabilização do número de termos distintos conforme descrito na Seção 4.1.1.1, o total de bigramas e trigramas é de 15.485 e 18.172, respectivamente.
Para essas listas de bigramas e trigramas extraídos calculam- se os seguintes índices:
TF-IDF a frequência de termo e inversa de domínio proposta por Kim;
E tf-dcf a frequência de termo e disjunção de corpora, proposta nesse capítulo.
Os resultados numéricos calculados foram os valores de precisão3 (Seção 2.3.3) das listas compostas por os primeiros termos extraídos, ordenados segundo cada um dos seis índices citados.
Para avaliar incrementalmente o benefício de cada índice, foram consideradas listas com os 50, 100, 150, 200, 250, 300, 350, 400, 450 e 500 primeiros termos, ordenados de acordo com cada um dos índices.
Análise Numérica dos índices Observando em detalhe alguns termos extraídos, é possível entender melhor o efeito de cada índice, e, por consequência, perceber os benefícios do índice tf-dcf como indicador de relevância de termos.
Os dez termos mais frequentes do corpus de Pediatria são apresentados na Tabela 4.2.
Em essa tabela mostra- se o número de ocorrências de cada termo em cada corpus (Pediatria -- PED, Modelagem estocástica -- Me, Mineração de dados Md, Paralelo -- Pp e Geologia -- Geo).
Adicionalmente, a última coluna (lista de ref.) indica se o termo pertence (&quot;In&quot;), ou não (&quot;OUT&quot;), à lista de referência.
Os mesmos dez termos mais frequentes são mostrados na Tabela 4.3, mas nessa tabela são indicados os valores de cada um dos seis índices apresentados, bem como a posição que o termo ocupa na lista ordenada segundo cada índice.
Por exemplo, na terceira linha da Tabela 4.3, o termo &quot;faixa etária», que pertence à lista de referência, tem como frequência absoluta (tf) o valor 234, que o faz ocupar a terceira posição na lista organizada segundo o índice tf.
O índice tf-idf desse mesmo termo é igual a 169.18, que também o coloca na terceira posição da lista Ao contrário de experimentos tradicionais da área de recuperação de informação e mesmo outros experimentos realizados nessa tese, limitou- se a observação da precisão, pois o cálculo de abrangência não acrescentaria informação devido a todas as listas terem tamanhos fixos.
Tabela 4.3: Análise de termos frequentes do corpus de Pediatria.
Termos (lista de ref.) aleitamento materno recém nascido faixa etária presente estudo leite materno idade gestacional ventilação mecânica via aérea pressão arterial sexo masculino tf Eq.
4.1 1a 2 a 3a 4 a 5a 6 a 7a 8 a 9a 10 a tf-idf Eq.
4.3 1a 2 a 3a 4 a 5a 7 a 6a 8 a 19a 9 a tds Eq.
4.5 1a 1 a 1a 1 a 1a 13,318 a thd Eq.
4.7 1a 2 a 4a 42 a 3a 5 a 6a 7 a 8a 14 a Eq.
4.8 1a 2 a 6a 57 a 3a 4 a 5a 7 a 8a 35 a tf-dcf Eq.
4.9 1a 2 a 15a 1,276 a 3a 4 a 5a 6 a 7a 543 a Observando as diferenças de posição dos termos nas listas ordenadas por os índices tf e tf-idf (Eq.
4.3), percebe- se uma semelhança muito grande.
A única diferença significativa ocorre para o termo &quot;pressão arterial «que cai da 9 posição, segundo tf, para a 19a posição, segundo tf-idf.
No entanto, este rebaixamento não justifica- se, pois, intuitivamente, o termo &quot;pressão arterial «não parece menos relevante que o termo &quot;via aérea», por exemplo.
Contrário a essa situação, o termo genérico &quot;presente estudo «não é afetado no que diz respeito a sua posição, devido a o uso do índice tf-idf.
A observação do efeito do uso do índice tds (Eq.
4.5) mostra uma falta de discernimento ao atribuir valores numéricos aos termos.
A forma de cálculo do índice tds atribui valores iguais a 1,00 para todos os termos que ocorrem somente no corpus de Pediatria.
De essa forma os termos que ocorram pelo menos uma vez em algum dos corpora contrastantes serão banidos de qualquer lista de termos relevantes, pois existem mais de 13.000 bigramas que ocorrem somente no corpus de Pediatria.
Apesar desse problema, o índice tds consegue fazer uma certa diferenciação entre os termos que ocorrem nos corpora contrastantes.
Os termos &quot;faixa etária», &quot;sexo masculino «e &quot;presente estudo «parecem ter uma ordem de relevância compatível com a ordem do valor do índice tds.
A lista ordenada segundo o índice thd (Eq.
4.7) mostra um efeito de rebaixamento nos três termos que ocorrem nos corpora contrastantes, mas esse rebaixamento não é muito grande.
Por exemplo, mesmo o termo &quot;presente estudo», que é bastante frequente nos corpora contrastantes, cai da quarta posição (segundo tf) e para a 42a posição usando o índice thd.
A lista organizada de acordo com o índice TF-IDF mostra um efeito mais forte do que o obtido com o índice thd (Eq.
4.7), pois ele é dependente do número de corpora contrastantes onde o termo ocorre.
Como consequência, o termo &quot;faixa etária «cai da terceira posição, segundo tf, para a sexta posição, segundo o índice TF-IDF.
Já o termo &quot;presente estudo «cai da quarta para a 57a posição, pois esse termo ocorre em todos corpora contrastantes, exceto o de Geologia.
O termo &quot;presente estudo «sofre o maior rebaixamento, caindo da quarta posição, segundo tf, para a 1.276 posição, segundo tf-dcf.
Um pouco menos impactante é o rebaixamento do termo &quot;sexo masculino «que cai da vigésima para a 543a posição.
Por outro lado, a queda do termo faixa etária é bem pequena, pois ele cai da terceira para a décima quinta posição.
Análise da Precisão dos índices Seguindo o processo de experimentação definido, a precisão obtida para listas ordenadas segundo os seis índices apresentados é exposta nas Figuras 4.1 e 4.2, para bigramas e trigramas respectivamente.
Em esses resultados, utiliza- se a frequência absoluta, representada por uma curva com quadrados cheios (em azul), como resultado padrão, pois esse índice é a escolha elementar para estimar a relevância de termos.
Os resultados obtidos com o uso do índice tf-idf (Eq.
4.3) exemplificam o esforço de estimar a relevância de termos sem utilizar corpora contrastantes.
As demais 3 curvas (tds -- Eq.
4.5, thd -- Eq.
4.7, e TF-IDF -- Eq.
4.8) representam os trabalhos da literatura que utilizam corpora contrastantes.
Finalmente, a curva tf-dcf, representada com triângulos vazados (em vermelho), mostra os valores de precisão obtidos para as listas organizadas segundo o novo índice proposto nesse capítulo.
A primeira observação para a precisão obtida com bigramas são os baixos valores para listas ordenadas com o índice tf-idf (Eq.
4.3). Para listas ordenadas com a frequência absoluta, a precisão varia entre 78% e 85%, enquanto que para listas ordenadas com tf-idf, a precisão se situa entre 76% e 83%.
De acordo com os tamanhos das listas, a queda de precisão resultante do uso do índice tf-idf frente a o índice tf vai de 1% (listas com 150 termos) a 6% (listas com 350 termos), mas a perda média fica em torno de 4%.
Também é fácil observar os melhores resultados alcançados com índices que usam corpora contrastantes, ou seja, tds (Eq.
4.5), thd (Eq.
4.7), TF-IDF (Eq.
4.8) e tf-dcf (Eq.
4.9). O ganho médio de precisão em listas organizadas por esses índices frente a o índice tf é de 9%, e, exceto por o índice tds (Eq.
4.5) aplicado a listas com 400 ou mais termos, os valores de precisão foram sempre superiores àqueles obtidos com o índice tf.
Esses resultados ilustram a superioridade de índices que usam corpora contrastantes.
No entanto, observando de perto os resultados de cada um desses índices, percebe- se comportamentos distintos.
Os resultados obtidos com o índice thd (Eq.
4.7) apresentaram uma precisão em torno de 90% para listas de todos os tamanhos testados.
O uso desse índice mostra ganhos frente a o uso do índice tf variando de 12% a 5%, com uma média de 7% de ganho para todos os tamanhos de listas.
Esses resultados indicam que o índice termhood oferece ganhos consistentes frente a a frequência absoluta.
O uso do índice TF-IDF mostrou uma melhora significativa dos valores de precisão, variando de 98% a 93%.
Esses resultados representam um ganho médio de 11% frente àqueles obtidos com o índice padrão.
Os resultados obtidos para listas de trigramas (Figura 4.2) mostram um comportamento similar ao encontrado para listas de bigramas.
Os resultados para listas ordenadas com o índice tf-idf (Eq.
4.3) são mais uma vez claramente abaixo de os valores obtidos com o índice padrão.
Os resultados para listas organizadas segundo índices que usam corpora contrastantes continuam sendo, em geral, superiores aos resultados utilizando o índice padrão.
No entanto, percebe- se que o índice tds (Eq.
4.5) aplicado a trigramas mostra uma curva de precisão que cai um pouco mais rápido do que sua similar para bigramas.
A precisão de listas com 200 ou mais trigramas apresenta valores inferiores aos obtidos com a frequência absoluta.
A observação mais importante, porém, é que, também para os resultados da Figura 4.2, a precisão obtida para listas ordenadas com o índice tf-dcf, proposto nesse capítulo, é superior aos valores de precisão de todos os demais índices.
Esses resultados obtidos para listas de trigramas confirmam a impressão causada por o sucesso obtido com as listas de bigramas Impacto da Escolha dos Corpora Contrastantes Todos resultados apresentados até agora consideram a ordenação feita com o uso do corpus de Pediatria (PED) e de todos os demais corpora como contrastantes (Modelagem estocástica Me, Mineração de dados -- Md, Processamento paralelo -- Pp, e Geologia -- Geo).
Naturalmente, a escolha dos corpora contrastantes pode afetar a eficiência do índice tf-dcf.
Logo, nessa seção são feitos três experimentos adicionais, variando o conjunto de corpora contrastantes conforme apresentado na Tabela 4.4.
O primeiro experimento adicional corresponde à remoção do corpus de Geologia, mantendo apenas os corpora relacionados à Ciência da Computação (Me, Md e Pp) como corpora contrastantes.
O segundo experimento adicional (Exp.
2) corresponde ao complemento do Experimento 1, pois em ele remove- se todos corpora relacionados à Ciência da Computação e mantém- se apenas o corpus de Geologia como corpus contrastante.
Finalmente, o último experimento adicional (Exp.
3) também usa um único corpus contrastante, porém nesse caso utiliza- se apenas o pequeno corpus de Processamento paralelo.
A Figura 4.3 mostra os resultados de precisão para listas de 50 a 500 bigramas, de forma análoga aos experimentos feitos na Seção 4.3.3.
Em a verdade, os valores correspondentes à primeira curva (Me Md Pp Geo) correspondem ao experimento original, ou seja, a curva referente a o índice tf-dcf apresentada na Figura 4.1.
Uma observação importante dos resultados apresentados na Figura 4.3 é que a remoção do corpus de Geologia causa uma pequena redução nos valores de precisão.
Esse resultado é esperado, pois os três corpora relacionados à Ciência da Computação ainda fornecem uma boa comparação para o corpus de Pediatria.
O uso do corpus de Geologia como único contrastante (Exp.
2) reduz bem mais os valores de precisão.
Essa redução poderia ser explicada por uma distância conceitual existente entre os corpora de Pediatria e Geologia, mas provavelmente, a redução possa ser consequência do tamanho do corpus de Geologia.
Essa afirmação é consistente com o último experimento realizado (Exp.
3), onde o uso de um corpus ainda menor, produziu os mais baixos valores de precisão.
Os resultados das Figuras 4.3 e 4.4 mostram uma tendência que segue a intuição que a variabilidade dos corpora contrastantes é relevante.
O uso de vários grandes corpora cobrindo diferentes domínios parece trazer uma vantagem considerável para o índice tf-dcf.
Em a verdade, o sucesso da abordagem tf-dcf é bastante dependente do uso de corpora contrastantes grandes e com domínios tão ortogonais quanto possível, como é o caso dos corpora utilizados nessa tese.
Uma vez estabelecido o índice para ordenar termos, o índice tf-dcf, assume- se essa ordenação como uma expressão da relevância dos termos no corpus de domínio.
Cabe salientar, que as listas geradas tendem a ser bastante extensas.
Apesar disso, devido a a ordenação feita, existe a clara expectativa de que os termos mais relevantes estejam mais concentrados nas primeiras posições.
De essa forma, deve buscar- se um ponto da lista que maximize a densidade de termos relevantes acima, e minimize o número de termos relevantes abaixo, ou seja, definir um ponto de corte que equilibre a precisão e a abrangência.
O trabalho a fazer, então, é definir pontos de corte adequados para escolher, automaticamente, quais termos considerar ou não conceitos do domínio.
Em esse sentido, conforme discutido no referencial teórico (Seção 2.3.2), assume- se no contexto dessa tese que os conceitos são os termos mais relevantes do domínio, e a relevância é definida segundo o índice tf-dcf que apresentou a melhor precisão nos experimentos do capítulo anterior.
Oíndice tf-dcf serve para ordenar os termos extraídos, ou seja, a aplicação do ponto de corte vai apenas indicar quais termos serão desprezados.
Portanto, o objetivo desse capítulo é definir uma forma de escolher e aplicar pontos de corte às listas de termos extraídos, identificando aqueles que serão considerados conceitos do domínio.
Em esse sentido, esse capítulo analisa o comportamento de diversas políticas de escolha de pontos de corte aplicados sobre listas devidamente ordenadas segundo o índice tf-dcf proposto no capítulo anterior.
Especificamente, são vistos pontos de corte tradicionalmente encontrados na literatura:
Os pontos de corte absolutos, os pontos de corte por limiares e pontos de corte relativos.
Em seguida, na Seção 5.2 é proposta uma forma de escolher automaticamente pontos de corte para listas de termos extraídos e ordenados.
Finalmente, sumariza- se o resultado da aplicação de pontos de corte a todas as listas de termos extraídos de todos os corpora utilizados nessa tese.
Uma parte das contribuições relativas às políticas de pontos de corte apresentadas JBCS/ Springer em Novembro de 2010.
Pontos de Corte Tradicionais A maneira mais simples de se aplicar pontos de corte é escolher um número arbitrário de termos que serão considerados.
Vários trabalhos da literatura definem arbitrariamente pontos de corte de forma empírica.
Essa escolha pode ser feita de várias formas.
Por exemplo, as diversas curvas apresentadas no capítulo anterior mostram resultados obtidos para listas com tamanhos arbitrários, ou seja, com a aplicação de pontos de corte absolutos.
Em esses resultados do capítulo anterior, e em muitos trabalhos da literatura[ 216, 48, 124, 62, 215, 16, 54], as listas geradas separam os termos segundo o número de palavras que os compõem, ou seja, trata- se separadamente listas de unigramas, bigramas, trigramas, etc..
Essa análise em separado faz sentido, uma vez que os termos tendem a apresentar variações distintas para os índices, segundo o número de palavras que os compõem.
Por exemplo, ordenando os unigramas do corpus de Geologia segundo a frequência absoluta de termo (tf), o termo &quot;topo «ocupa a 60a posição com 433 ocorrências.
Porém, de entre os demais termos (bigramas, trigramas, etc.), o termo mais frequente (&quot;matéria orgânica&quot;) ocorre 430 vezes.
Portanto, uma lista dos 60 termos mais frequentes, que não distingue os termos por o número de palavras, será composta apenas por unigramas.
De essa forma, o estudo de pontos de corte será feito escolhendo um ponto de corte para unigramas, outro para bigramas, e assim por diante.
Pontos de Corte Absolutos O corpus de Pediatria descrito anteriormente possui uma lista de termos de referência desenvolvida por um grupo externo, composta por 1.534 bigramas e 2.660 trigramas considerados conceitos desse domínio (vide anexo A).
Em esse sentido, a primeira experiência feita consiste em extrair os termos do corpus de Pediatria, organizar- los segundo o índice de relevância tf-dcf proposto no capítulo anterior, e aplicar pontos de corte absolutos às listas de bigramas e trigramas.
Para cada uma das listas organizadas e reduzidas por a aplicação dos pontos de corte, calcula- se a precisão, abrangência e medida F para listas obtidas com diversos pontos de corte absolutos.
Aplicam- se pontos de corte considerando os 100 termos mais frequentes, ou seja, os primeiros 100 termos dessas listas.
Sucessivamente, analisam- se pontos de corte considerando os 200, 300, e assim por diante até 3.500 termos mais frequentes.
A Figura 5.1 apresenta os tamanhos de listas conforme os pontos de corte, além de valores de precisão (P), abrangência (R) e medida F (F) obtidos para cada uma das listas de bigramas e trigramas.
A tabela contida nessa figura indica também o número de termos encontrados na intersecção entre a lista de termos extraídos (LE) e lista de referência (LR).
A observação dos resultados apresentados na Figura 5.1 mostra inicialmente que existe um ponto de cruzamento entre as curvas de precisão e abrangência.
Observando as listas obtidas com aplicação de pontos de corte crescentes, esse ponto de cruzamento indica quando uma lista deixa de ser excessivamente restritiva.
Esse ponto acontece para listas de 1.600 bigramas e, pois listas com menos termos do que a referência tem obrigatoriamente abrangência inferior a 100% e listas com mais termos que a referência sempre tem precisão inferior a 100%.
No entanto, os valores máximos da medida F, que representa o melhor equilíbrio entre precisão e abrangência, ocorrem um pouco depois desse cruzamento de curvas, respectivamente, nas listas de 2.000 bigramas e 3.300 trigramas.
Isto se deve ao fato das curvas de precisão e abrangência terem um comportamento diferente, pois a queda dos valores de precisão é mais lenta, em comparação com o aumento rápido dos valores de abrangência.
Para os bigramas, percebe- se que o valor de abrangência se estabiliza por volta de a aplicação do ponto de corte com 2.000 termos, onde atinge- se cerca de 93% de abrangência.
Esse fato é curioso, posto que a lista de referência de bigramas possui 1.534 termos, ou seja, é necessário estender o ponto de corte absoluto para extrair cerca de 500 termos a mais de o que 1.534 (tamanho da lista de referência) para atingir uma alta abrangência.
A precisão, ao contrário, se mantém em valores altos até esse mesmo ponto de corte de 2.000 bigramas, caindo de maneira mais acentuada para pontos de corte menos restritivos.
Para os trigramas, que possuem 2.660 termos na lista de referência, percebe- se um comportamento análogo das curvas de precisão e abrangência somente para listas com 3.300 termos.
Em esse caso, mais de 600 termos adicionais tiveram de ser extraídos para se chegar a uma alta abrangência.
Pontos de Corte por Limiar Uma forma popular de descartar termos é o uso de pontos de corte através da determinação de limiares arbitrários de ocorrências de termos no corpus.
Por exemplo, o trabalho de Bourigault e Lame sugere o uso de um número mínimo de 10 ocorrências para considerar um termo relevante.
Essa forma de identificar termos relevantes, corresponde à escolha de um ponto de corte baseado em limiar, ou seja, organizar a lista de termos extraídos segundo um índice e considerar apenas os termos em os quais o seu índice possui um valor acima de o limiar escolhido.
No caso de Bourigault e Lame, o índice escolhido foi a frequência absoluta de termos, porém qualquer índice poderia ser escolhido.
O uso de pontos de corte por limiar baseados na frequência absoluta é adotado com base em um raciocínio intuitivo, que sugere uma relação direta entre o tamanho do corpus e o ponto de corte a escolher.
Esta intuição, ainda que verdadeira, não é uma relação linear, pois o número de ocorrências de termos num corpus decresce exponencialmente.
O formato da curva de decréscimo exponencial pode variar bastante segundo o método de extração, por exemplo, para palavras extraídas segundo um processo puramente estatístico, o decréscimo segue a lei de Zipf.
No entanto, o processo linguístico de extração de termos utilizado nessa tese, não segue esta mesma lei, como pode ser verificado por o número de ocorrência dos 10 termos mais frequentes do corpus de Pediatria na Tabela 4.3.
Por essa razão, é difícil propor uma fórmula que permita estimar automaticamente um limiar para ponto de corte a partir de o tamanho do corpus.
Logo, nessa seção analisam- se diversos valores de limiar escolhidos de forma arbitrária.
Retomando o corpus de Pediatria e listas de bigramas e trigramas de referência utilizados na seção anterior, podemos analisar diversos pontos de corte segundo a frequência de termo, disjunção de corpora dos termos extraídos.
Cabe lembrar que o índice tf-dcf mantém uma semântica similar à frequência absoluta de termo (tf), pois um termo que possui ocorrências somente no corpus de domínio, ou seja, nenhuma ocorrência nos corpora contrastantes, terá os mesmo valores para os índices tf e tf-dcf.
A Figura 5.2 apresenta os resultados de precisão, abrangência e medida F para os pontos de corte por limiar para o índice tf-dcf de 0 a 15.
Em a parte inferior dessa figura, uma tabela indica ainda nas suas últimas duas colunas o tamanho da lista após a aplicação do ponto de corte(| LE|), bem como o tamanho da sua intersecção com a lista de referência(| LE X LR|).
Por exemplo, a linha central dessa tabela indica o ponto de corte por o limiar 8, ou seja, apenas termos que tenham valor de tf-dcf igual ou superior a 8 são considerados.
Isso resulta numa lista com 573 bigramas, de os quais 530 estão presentes na lista de referência, assim como, uma lista de 209 trigramas, de os quais 196 estão presentes na lista de referência.
A primeira linha da tabela contida na Figura 5.2 indica um ponto de corte com limiar igual a 0 que corresponde a não desprezar nenhum dos 15.487 bigramas e 18.174 trigramas extraídos.
Consequentemente, as listas representadas por essa linha incluem, cada uma de elas, todos os 1.534 bigramas e 2.660 trigramas da lista de referência (LR), resultando numa abrangência de 100% e aproximadamente 10% e 15% de precisão para bigramas e trigramas, Segundo a lei de Zipf, a frequência de uma palavra num corpus é inversamente proporcional a sua posição (rank).
De essa forma, a palavra mais frequente de um corpus possui:
O dobro de ocorrências do que as ocorrências da segunda palavra mais frequente;
O triplo de ocorrências do que as ocorrências da terceira palavra mais frequente;
E assim por diante. Para bigramas, o ponto que maximiza a combinação de precisão e abrangência (medida F) ocorre com um limiar de 3, que aponta para os valores de 72% de precisão e 92% de abrangência.
Para trigramas, o ponto com maior valor de medida F situa- se no limiar de 2, que aponta para os valores de 77% de precisão e 93% de abrangência.
Análogo ao que foi observado com os pontos de corte absolutos, os valores da Figura 5.2 mostram que bigramas e trigramas possuem seus melhores resultados (maior medida F) para distintos valores de limiar (3 e 2, respectivamente).
De essa forma, um limiar único a ser utilizado como ponto de corte que seja adequado a todas listas extraídas não parece ser possível.
Pontos de Corte Relativos Uma alternativa de pontos de corte, encontrada na literatura, é manter apenas um percentual da lista extraída.
Essa alternativa é denominada ponto de corte relativo, pois define- se o tamanho da lista a ser considerada proporcional ao total de termos extraídos.
A primeira observação interessante dos resultados apresentados na Figura 5.3 é que os pontos de corte entre 8% e 15% para bigramas e entre 14% e 22% para trigramas possuem valores de medida F iguais ou superiores a 75%.
Isso ocorre, pois conforme aumenta o ponto de corte relativo, a precisão se mantém acima de 62% para bigramas e trigramas, porém a abrangência já mostra valores superiores a 67% para bigramas e superiores a 74% para trigramas.
Em função de essas faixas, é possível observar que um ponto de corte relativo de 15% oferece um compromisso razoável de medida F não inferior a 75% para bigramas e trigramas.
Ainda assim, os valores ótimos de equilíbrio entre precisão e abrangência ocorrem nos pontos de corte relativos de 13% para bigramas e de 18% para trigramas.
Portanto, também não é possível determinar um único ponto de corte relativo adequado tanto a bigramas, quanto a trigramas.
Proposta de Ponto de Corte para Identificar Conceitos Em a seção anterior foram feitos experimentos sobre bigramas e trigramas extraídos do corpus de Pediatria, pois somente esse corpus possui listas de referência, e mesmo assim, essas listas só contêm termos com duas ou três palavras.
No entanto, o que se busca não é descobrir uma forma adequada de aplicar pontos de corte exclusivamente a essas listas de termos, mas sim, a todas as listas extraídas de todos os corpora.
De essa forma, busca- se critérios que possam ser generalizados para, por exemplo, unigramas do corpus de Geologia, ou N-gramas do corpus de Modelagem estocástica, etc., pois para essas listas de termos não existe referência disponível.
A análise de um ponto de corte único, seja absoluto, por limiar, ou relativo, não parece ser possível, pois mesmo para os bigramas e trigramas do corpus de Pediatria não foi possível estabelecer um ponto de corte único que permitisse valores equilibrados de precisão e abrangência.
Se observarmos em detalhe os resultados apresentados nas tabelas das Figuras 5.1, 5.2 e 5.3, percebe- se que existem três formas distintas de escolher um ponto de corte adequado:·
buscar um ponto de corte que garanta uma alta precisão, mantendo, com menor prioridade, uma boa abrangência;·
buscar um ponto de corte que garanta uma alta abrangência, mantendo, com menor prioridade, uma boa precisão;·
buscar o maior equilíbrio possível entre precisão e abrangência, ou seja, o maior valor da medida F. Evidentemente, se a preocupação individual é somente com precisão, ou somente com abrangência, não se trata de um ponto de interesse científico.
Se quisermos maximizar apenas a precisão, basta colocar um ponto de corte muito restritivo, ou seja, com pouquíssimos termos.
Por outro lado, a maior abrangência possível ocorre quando não se despreza nenhum termo.
Logo, o interesse científico existe quando busca- se o equilíbrio entre precisão e abrangência.
Observando os valores nas Figuras 5.1, 5.2 e 5.3, percebe- se a maximização da medida F no ponto de inflexão da sua curva correspondente, ou seja:·
No caso de pontos de corte absolutos, trata- se de 2.000 bigramas, 3.300 trigramas;·
Para pontos de corte por limiar (Figura 5.2), trata- se dos limiares 3 para bigramas e 2 para trigramas;·
Os pontos de corte relativos (Figura 5.3), apontam para listas com 13% dos bigramas extraídos, e 18% dos trigramas extraídos.
De essa forma, propõe- se um método híbrido para estimar um ponto de corte que seja próximo desse ótimo.
Esse método é dito híbrido, pois segue alternativamente ideias vistas para ponto de corte.
Especificamente, a proposta é aplicar em conjunto:
Um ponto de corte por limiar e um ponto de corte relativo.
As seções a seguir detalham essa proposta.
Cabe salientar que, o uso de ponto de corte absoluto não faz sentido nesse contexto.
Um ponto de corte absoluto é a definição de um número fixo (e arbitrário) de termos, enquanto que os demais (relativo e por limiar) são naturalmente dependentes dos corpora e listas de termos extraídos.
Aplicação de um Ponto de Corte por Limiar Aplicando um ponto de corte por limiar a todas as listas de termos extraídos, parece ser razoável descartar termos que não atingem um valor mínimo segundo o índice proposto.
De essa forma, baseado nas análises feitas na Seção 5.1.2 (Figura 5.2), sugere- se, como primeiro passo do método híbrido proposto, descartar termos que tenham um índice tf-dcf inferior a 2.
Essa escolha de um limiar 2 é conservadora, pois para bigramas do corpus de Pediatria um limiar 3 foi mais adequado (melhor medida F).
Cabe lembrar que, para trigramas desse mesmo corpus um limiar 2 foi mais adequado, logo a escolha de um limiar 3 iria descartar trigramas relevantes para o corpus de Pediatria.
Apesar de essa escolha conservadora, o limiar 2 descarta um grande número de termos, como pode ser observado na Tabela 5.1, que apresenta a redução no número de termos extraídos em todos os corpora (Pediatria -- PED, Modelagem estocástica -- Me, Mineração de dados -- Md, Processamento paralelo -- Pp, e Geologia -- Geo).
Em essa tabela indica- se o número de termos originalmente extraídos (O) e o número restante após a aplicação do ponto de corte com o limiar 2 ao índice tf-dcf (L2).
Observando os resultados da Tabela 5.1, percebe- se grandes reduções, por exemplo, o descarte de 3.839 termos da lista de termos com 7 ou mais palavras extraídos do corpus de Pediatria, ou seja, o descarte de 97% dos termos dessa lista.
Mesmo, nos casos de menor redução, como por exemplo, a redução na lista de unigramas do corpus de Geologia, 5.106 termos foram descartados, ou seja, descartou- se 66% dos termos dessa lista.
Aplicação de um Ponto de Corte Relativo Analogamente ao ponto de corte com limiar 2 para o índice tf-dcf, a segunda etapa do método híbrido proposto é o descarte de termos por um ponto de corte relativo.
Com base nos resultados apresentados na Tabela 5.3, onde um ponto de corte de 13% para bigramas e de 18% para trigramas, mostrou os melhores valores da medida F, escolheu- se utilizar o ponto de corte intermediário para ser aplicado a todas as listas de termos extraídos.
A Tabela 5.2 apresenta a aplicação do ponto de corte relativo de 15%.
Essa tabela indica o número de termos originalmente extraídos (O) e o número de termos restantes após a aplicação do ponto de corte relativo que mantém 15% dos termos extraídos, o que resulta no descarte de 85% dos termos das listas originais.
Observando os dados das Tabela 5.1 e 5.2, verifica- se que listas de unigramas e bigramas são mais reduzidas por o uso de ponto de corte relativo.
Por outro lado, listas de 6-gramas a N-gramas são mais reduzidas por o uso de ponto de corte por limiar.
Esse fenômeno se deve ao fato de que termos mais simples (unigramas e bigramas, por exemplo) tendem a ser mais frequentes, portanto, possuem valores mais altos de tf-dcf do que termos compostos por um número maior de palavras.
Método Híbrido para Escolha de Ponto de Corte Frente a o apresentado nas seções anteriores, o método proposto para determinar automaticamente os pontos de corte das listas extraídas de um corpus de domínio consiste nas seguintes etapas:·
As listas extraídas e ordenadas segundo o índice tf-dcf são submetidas a um ponto de corte por o limiar 2;·
As listas extraídas e ordenadas segundo o índice tf-dcf são submetidas a um ponto de corte relativo de 15%;·
Apenas os termos que não foram descartados por ambos os pontos de corte são mantidos.
Note- se que, como ambos os pontos de corte são aplicados a listas ordenadas por o mesmo índice (a mesma lista), esse processo equivale a determinar o tamanho da lista resultante por cada um dos dois pontos de corte e utilizar o menor de eles.
Por exemplo, aplicando o ponto de corte por o limiar 2 à lista de unigramas do corpus de Geologia indica- se manter 2.573 termos, porém aplicando- se o ponto de corte relativo de 15% indica- se manter 1.152 termos.
Por consequência, considera- se como conceitos de corpus de Geologia os 1.152 unigramas com os maiores valores do índice tf-dcf.
O resultado desse método, na forma de quantos conceitos são identificados para cada um dos corpora utilizados, é apresentado na Tabela 5.3.
Em essa tabela, indica- se o número de termos originalmente extraídos (O) e o número de termos considerados conceitos após a aplicação dos pontos de corte (C).
Indica- se ainda nessa tabela, ao lado de o número de conceitos, se o ponto de corte mais restritivo foi por limiar (L) ou relativo (R).
Observando os resultados da Tabela 5.3, percebe- se que o uso combinado dos pontos de corte por limiar e relativo permite restringir as listas de termos, desde unigramas até N-gramas.
Percebe- se também que, conforme dito, os pontos de corte relativos são mais restritivos para as listas de termos mais simples, enquanto que os pontos de corte por limiar são mais efetivos para termos mais longos.
Resultado Final da Identificação de Conceitos A definição de pontos de corte conclui o processo de extração automática de conceitos de um corpus de domínio.
De essa forma, é possível descrever o processo completo de extração de conceitos através das seguintes etapas:·
Os termos são extraídos e tratados por as heurísticas descritas no Capítulo 3;·
Os termos extraídos são ordenados segundo o processo de comparação com corpora contrastantes e cálculo do índice tf-dcf descrito no Capítulo 4;·
As listas de termos extraídos e ordenados são submetidas a um ponto de corte duplo (por limiar e relativo) conforme descrito nesse capítulo.
Feitas essas três etapas, os termos que não forem descartados são considerados conceitos do domínio.
De um ponto de vista prático, para cada um dos cinco corpora utilizados nessa tese são extraídos os conceitos descritos no anexo B. De um ponto de vista numérico, a Figura 5.4 representa graficamente o número de termos extraídos e o número de conceitos identificados.
Uma vez extraídos os conceitos, várias recursos linguísticos podem ser disponibilizados.
Em esse capítulo exemplificam- se algumas dessas possíveis aplicações que foram implementadas na ferramenta EATOLP.
Essa ferramenta de software realiza todo o processo de extração e ordenação de termos, bem como a identificação de conceitos proposta nessa tese.
Os recursos linguísticos disponibilizados, ou seja, as informações detalhadas de termos extraídos e as listas de conceitos (anexo B), possibilitam a geração de recursos mais sofisticados por a manipulação dessas informações.
Em esse sentido, esse capítulo apresenta as seguintes aplicações:·
Geração de listas de termos e conceitos;·
Concordanciador de termos extraídos (Seção 6.2);·
Geração de nuvens de conceitos (Seção 6.3);·
Geração de hierarquia de conceitos (Seção 6.4).
Cabe salientar que, essas aplicações representam algumas utilizações dos recursos linguísticos produzidos por o processo de extração de conceitos proposto nessa tese, mas muitas outras aplicações podem ser implementadas.
No entanto, as aplicações descritas nesse capítulo representam um conjunto de funcionalidades práticas disponibilizadas com a ferramenta EATOLP, e que já vem sendo utilizadas por diversos grupos de pesquisa.
Listas de Termos e Conceitos A disponibilização de listas de termos e listas de conceitos dos corpora é a principal aplicação do processo desenvolvido nessa tese.
Dados alguns corpora de domínio, é possível disponibilizar listas, não somente de conceitos, mas de quaisquer termos extraídos.
Enquanto os conceitos tem um uso mais específico, como por exemplo, construção de hierarquias de conceitos, ontologias, glossários, etc..
As listas de termos podem ser úteis para aplicações mais ligadas a uma análise humana detalhada, como por exemplo, análise e geração de vocabulários, dicionários de tradução, etc..
Adicionalmente, também é possível enriquecer a lista de termos gerada com outras informações.
Essas informações adicionais, por sua vez, podem ser manipuladas por consultas que permitam ao usuário da ferramenta EATOLP inferir conhecimentos sobre o uso dos termos e conceitos no corpus que está sendo analisado.
A Figura 6.1 mostra um exemplo de consulta aos bigramas do corpus de Geologia que possuem como núcleo a palavra &quot;lago».
Em esse exemplo, inclui- se os termos como foram encontrados no corpus (term), sua forma canônica (lemma), seu núcleo (head), sua etiqueta semântica (sem tag) e seus índices tf e tf-dcf.·
Variações morfológicas em que o termo foi encontrado no corpus;·
Número de vezes em que o termo foi empregado como sujeito, objeto ou complemento;·
Verbos aos quais o termo foi relacionado;·
Valor numérico dos índices tf-idf, tds, thd e TF-IDF relativos ao termo;·
Informações referente a o núcleo do termo.
As variações morfológicas em as quais o conceito foi encontrado permitem observar características de como o termo é empregado.
Esse tipo de informação éútil a pesquisadores que podem, através desse recurso linguístico, observar padrões de uso de diversos termos.
Por exemplo, no corpus de Pediatria, os termos &quot;criança «e &quot;bebê «têm padrões bem distintos de variações morfológicas.
O termo &quot;criança «é empregado 984 vezes no singular e 1.076 no plural.
O termo &quot;bebê «é empregado 138 vezes no singular e 64 vezes no plural.
O número de ocorrências em que o termo foi empregado como sujeito, objeto ou complemento, também pode auxiliar na detecção de padrões de uso em áreas distintas.
Por exemplo, o termo &quot;ordem «aparece em todos os corpora, porém ele é encontrado como sujeito 19% das vezes no corpus de Processamento paralelo, enquanto que no corpus de Geologia ele é encontrado como sujeito somente 8% das vezes.
Os verbos aos quais o termo foi relacionado podem indicar mais um aspecto das características de uso do termo.
Por exemplo, no corpus de Pediatria, os únicos unigramas que estão relacionados com o verbo &quot;desconhecer «são os termos &quot;mãe», &quot;sorologia «e &quot;universo».
Sendo que desses, apenas o termo, &quot;mãe», foi utilizado como sujeito do verbo desconhecer.
O valor numérico dos índices relativos a cada termo extraído também permite analisar as características do termo.
Esse tipo de informação permite que sejam feitas análises, ordenações e até aplicações de pontos de corte experimentais segundo outros critérios, além de os adotados nessa tese (Capítulo 5).
Finalmente, as informações relativas ao núcleo do termo possibilitam observar outros aspectos da utilização dos termos.
Um exemplo do uso desse tipo de informação é a identificação das etiquetas sintáticas (pos-tag) dos núcleos, que permite, por exemplo, identificar quais termos possuem como núcleo substantivos comuns.
Informações como essas podem permitir análises linguísticas avançadas, e até a redefinição de métodos de extração de termos e identificação de conceitos.
Concordanciador de Termos Uma aplicação de grande utilidade para pesquisadores da área é um concordanciador, ou seja, uma ferramenta que localiza ocorrências de um determinado termo no corpus e mostra o seu contexto de utilização e outras informações adicionais.
Alguns exemplos de concordanciadores são os softwares Unitex e WordSmith.
Por contexto de utilização do termo, entende- se as frases onde o termo ocorre no corpus e sua posição na frase.
Por exemplo, a utilização do concordanciador implementado no EATOLP para o termo &quot;parente «presente 7 vezes no corpus de Pediatria resulta nas informações parcialmente apresentadas na Figura 6.2.
Em a Figura 6.2 estão indicadas as frases do corpus de Pediatria onde o termo &quot;parente «foi encontrado.
De essas sete ocorrências, está indicada na última de elas a ocorrência do termo flexionado no plural (&quot;parentes&quot;), que aparece sendo empregado como sujeito do verbo &quot;reclamar», precedido do artigo definido, tendo como núcleo um substantivo (etiqueta &quot;n&quot;) e com uma etiqueta semântica &quot;Hfam «que significa humano com relação familiar, segundo o parser utilizado.
Cabe salientar que, a qualidade das informações adicionais disponibilizadas por o concordanciador é bastante dependente do parser utilizado, pois as informações de etiquetas apresentadas são originadas na anotação linguística feita no início de todo o processo.
Apesar disso, ou seja, independente da qualidade do parser, a função principal do concordanciador é preservada, pois ele facilita a visualização em detalhe da posição onde o termo se encontra, e, por consequência, a possibilidade de análise visual do seu contexto de utilização (frases).
Nuvens de Conceitos A aplicação de nuvens de conceitos (tag clouds) é um recurso de visualização que permite observar graficamente a relevância dos conceitos extraídos.
A saída do processo de identificação de conceitos (lista de conceitos) é associada ao índice tf-dcf de cada conceito, que indica sua relevância.
A nuvem é produzida escrevendo cada um dos conceitos em posições e cores aleatórias, porém, com tamanho de fonte proporcional ao seu índice, logo a sua relevância no corpus de domínio.
Essa forma de visualização é bastante intuitiva e seu uso em vários materias gráficos e na internet vem sendo bastante difundido recentemente.
Exemplos de nuvens de conceitos para bigramas e trigramas do corpus de Pediatria são apresentados nas Figuras 6.3 e 6.4, respectivamente.
Em essas duas figuras escreve- se 2.323 bigramas e 2.726 trigramas em tamanho proporcional ao seus índices tf-dcf.
A observação dessas nuvens de conceitos permite visualizar, de maneira muito clara, que os bigramas &quot;aleitamento materno «e &quot;recém-nascido», assim como os trigramas &quot;uso de chupeta», &quot;fator de risco «e &quot;aleitamento materno exclusivo», são os conceitos mais relevantes, segundo oíndice tf-dcf, de entre bigramas e trigramas do corpus de Pediatria.
Hierarquias de Conceitos A aplicação mais ambiciosa de entre as apresentadas nesse capítulo é a construção de uma hierarquia de conceitos, conforme definição formal feita na Seção 2.2.3.
Segundo essa definição, uma hierarquia é um conjunto de conceitos e um semi reticulado superior, ou seja, um conjunto de relações de subconceitos e superconceitos1.
O conjunto de conceitos é disponibilizado como saída do processo descrito nos capítulos anteriores.
Portanto, para construir a hierarquia é necessário definir/ descobrir esse tipo de relação entre esses conceitos.
De entre as abordagens clássicas de detecção de subconceitos e superconceitos, cita- se a busca em dicionários, a busca por padrões morfossintáticos proposta para o Inglês por Hearst, e adaptada para diversas outras línguas, como o Francês e o Português.
Outras abordagens tradicionais são a análise de coocorrência, a busca de similaridade distribucional, a análise distribucional, e diversas iniciativas regrupadas na denominação genérica de Análise Formal de Conceitos (FCA -- Formal Concept Analysis).
Em essa seção, adota- se uma solução distinta das abordagens clássicas, que é composta por dois níveis detalhados a seguir.
Hierarquia por Etiquetas Semânticas O parser Palavras define 174 diferentes etiquetas semânticas, que são agrupadas em 16 classes.
A Figura 6.5 apresenta essas classes nas folhas da árvore (representados por os retângulos da Figura 6.5).
Os demais nodos não terminais da árvore representam uma divisão por contextos de aplicação dos termos.
Cada uma das 16 classes apresentadas na Figura 6.5 congrega diversas etiquetas semânticas.
Maiores informações sobre as etiquetas semânticas, e suas classes, podem ser encontradas na especificação do parser, porém no anexo C o leitor encontra uma listagem dessas etiquetas, bem como sua classificação.
Inicialmente, todos os conceitos extraídos são associados, por o parser, a uma das 174 etiquetas, e por consequência, a uma das 16 classes.
Essa primeira hierarquização dos conceitos, feita por uma classificação segundo suas etiquetas semânticas, é denominada hierarquia semântica.
Por exemplo, os conceitos extraídos do corpus de Geologia:
&quot;lago», &quot;mar», &quot;lago profundo», &quot;lago costeiro «e &quot;mar aberto «possuem a etiqueta semântica &quot;lugares aquáticos «(Lwater) por Um termo t1 é considerado superconceito de um termo t2 quando t1 é uma generalização de t2.
Em esse caso t2 é dito subconceito de t1.
Por exemplo, o termo &quot;meio de locomoção «é um superconceito do termo &quot;trem».
Hierarquia por Núcleo de Sintagmas Após esse nível de hierarquia semântica, faz- se uma nova estruturação interna a cada subgrupo de conceitos, que possuem a mesma etiqueta semântica.
Este segundo nível de hierarquização é feito em dois passos:·
agrupa- se os conceitos (sintagmas nominais) que possuem o mesmo núcleo, por exemplo, os conceitos &quot;lago», &quot;lago profundo «e &quot;lago costeiro «possuem o mesmo núcleo (&quot;lago&quot;), e portanto são agrupados no mesmo ramo;·
dentro de o grupo de conceitos com mesmo núcleo, considera- se superconceito de um conceito, o conceito que estiver contido em ele, por exemplo, o conceito &quot;lago «será considerado superconceito dos conceitos &quot;lago profundo «e &quot;lago costeiro».
A Figura 6.7 mostra o resultado da associação dos conceitos para esse exemplo de lugares aquáticos.
Essa estruturação por núcleo guarda uma semelhança com outros trabalhos que consideram o núcleo do sintagma nominal em suas análises.
Esse é o caso do método hiperN descrito por Freitas, mas também de trabalhos de Amsler baseados exclusivamente em dicionários, e, ainda, de iniciativas como a de Nová cek.
No entanto, nenhuma dessas abordagens infere relações de subconceito/ superconceito de conceitos extraídos através dos núcleos dos sintagmas nominais.
Exemplo Completo de Hierarquia Para o corpus de Geologia utilizado anteriormente, a geração da hierarquia de conceitos resultou na estrutura apresentada nas Figuras 6.8, 6.9, 6.10 e 6.11.
Em essas figuras apresenta- se representações gráficas utilizando árvores hiperbólicas que permitem a visualização interativa da hierarquia.
A Figura 6.8 apresenta uma visão geral da hierarquia, mostrando a hierarquização das etiquetas semânticas.
A Figura 6.9 apresenta um nível de detalhe intermediário do ramo que corresponde às etiquetas classificadas dentro de o ramo &quot;concreto», subramo &quot;inanimado», subramo &quot;lugares».
A Figura 6.10 apresenta a subárvore dos conceitos com a etiqueta semântica &quot;lugares aquáticos».
Finalmente, a Figura 6.11 apresenta em detalhe os conceitos &quot;mares «e &quot;lagos».
Em essa última figura percebe- se claramente, entre outros, o conceito extraído &quot;mares», que foi encontrado 798 vezes, e que possui como subconceitos os conceitos extraídos &quot;mar devoniano», &quot;mar regressivo», &quot;mar de norte», etc..
Percebe- se também o conceito &quot;lago», que foi encontrado 243 vezes, e que possui como subconceitos &quot;lagos profundos», &quot;lagos altos», &quot;lagos baixos», &quot;lagos atuais», etc. &quot;mares «e &quot;lagos».
O objetivo central dessa tese foi o desenvolvimento de um processo de extração de conceitos a partir de corpora de domínio.
De essa forma, assumiu- se como entrada corpora anotados linguisticamente e como saída do processo uma lista de conceitos dos domínios que cada um dos corpus caracteriza.
Esse objetivo foi alcançado e experimentado sobre cinco corpora de domínio, que juntos totalizam um conjunto de textos com quase 6 milhões de palavras.
A avaliação de cada etapa do processo foi feita de forma empírica através de experimentos com bigramas e trigramas de um dos corpus (Pediatria), para o qual havia listas padrão de referência (gold standard).
Cabe lembrar que, segundo a literatura, a própria natureza da extração de termos e conceitos é subjetiva, e, portanto, somente avaliações empíricas são possíveis.
Completando o objetivo, a utilidade dos conceitos extraídos foi exemplificada por a disponibilização de recursos de grande utilidade para pesquisadores e usuários de ferramentas da área de linguística computacional.
Adicionalmente, todos os métodos propostos, bem como a geração automática dos recursos linguísticos, foram implementados numa ferramenta de software, EATOLP, que, ao mesmo tempo, ilustra e permite avaliar empiricamente todas as propostas dessa tese feitas nos Capítulos 3, 4, 5 e 6.
Contribuições Científicas e Tecnológicas Em a busca do objetivo dessa tese, foram desenvolvidos avanços científicos expressos por:
Uma abordagem linguística de extração de termos, que propôs um conjunto de heurísticas a aplicar a sintagmas nominais extraídos de um texto linguisticamente anotado por um parser, que trouxe um aumento de precisão e abrangência de cerca de 50% frente a a extração tradicional;
Um novo índice de relevância de termos, que permite, por a comparação com corpora contrastantes, estimar a relevância de termos para um domínio específico com precisão superior aos demais índices análogos existentes;
Uma proposta de estimativa genérica de pontos de corte em listas de termos organizados por relevância, que permite a identificação de conceitos, resultando em bons valores de medida F;
Um conjunto de aplicações práticas dos conceitos extraídos e seus contextos, que permite a sua compreensão, manipulação e visualização.
Em relação a o estado da arte, a proposta de extração (Capítulo 3) identificou heurísticas para transformação de sintagmas nominais em termos e conceitos, enquanto que os outros trabalhos nessa linha se limitam a fazer uma extração puramente estatística, como é o caso do NSP.
Mesmo trabalhos mais próximos, como o da ferramenta OntoLP que também faz extração baseada em sintagmas nominais e utiliza a entrada de textos anotados linguisticamente, possuem valores de precisão e abrangência semelhantes aos conseguidos com a extração sem o uso de heurísticas.
De essa forma, nossa contribuição ao estado da arte da extração de termos de corpora na língua portuguesa é de um processo, que por o uso das heurísticas, traz, em relação a os trabalhos correlatos, um aumento de, approximadamente, 10% para mais de 60%, tanto na precisão, como na abrangência de listas de termos extraídos comparados com listas de referência.
No que diz respeito ao estado da arte no estabelecimento de um índice de relevância para termos extraídos, a proposta do índice tf-dcf (Capítulo 4) traz uma contribuição clara por a formalização de um índice com sólida básica matemática.
O ganho de precisão trazido por o índice tf-dcf frente a abordagens tradicionais, como o popular tf-idf, é de cerca de 10%.
Mesmo frente a trabalhos mais recententes, com abordagens similares por o uso de corpora contrastantes, o índice tf-dcf apresentou valores mais altos de precisão em todos os experimentos realizados.
A proposta de ponto de corte para a identificação de conceitos (Capítulo 5) traz contribuições frente a o estado da arte por a sua originalidade.
Outros trabalhos similares utilizam alternativamente pontos de corte absolutos, pontos de corte relativos, ou pontos de corte por limiar, mas nenhuma publicação prévia cita o uso híbrido de pontos de corte.
De essa forma, a abordagem proposta por o uso combinado de um ponto de corte por limiar do índice tf-dcf e de um ponto de corte relativo dos termos extraídos traz uma contribuição objetiva ao fornecer valores adequados de medida F, mas, principalmente, por propor uma forma híbrida de escolha de pontos de corte.
As aplicações desenvolvidas (Capítulo 6) trazem uma contribuição ao estado da arte de disponibilização de termos e conceitos por conjugar conceitos existentes, como listas, concordanciadores, nuvens de etiquetas e árvores hiperbólicas, com a saída qualificada de termos e conceitos extraídos.
Porém, uma contribuição relevante ao estado da arte é a proposta, ainda inicial, de uma forma de construir hierarquias de conceitos com uma parte semântica, e outra parte baseada em núcleo de sintagmas nominais.
Essa forma de construir hierarquias pode em trabalhos futuros ser uma alternativa a outras abordagens similares encontradas na literatura.
Além de as contribuições científicas, essa tese traz três contribuições tecnológicas, que de um ponto de vista prático, se materializam nos os seguintes recursos:
A ferramenta EATOLP, que, além de implementar todo o processo de extração de conceitos descrito, oferece diversos modos de saída de termos e conceitos na forma de listas, concordanciador, nuvens de conceitos e uma hierarquia de conceitos;
Os cinco corpora de domínio que serviram para todas as experiências dessa tese, e, por ser um conjunto homogêneo de corpora, se configura num importante recurso linguístico para o tratamento computacional da língua portuguesa;
Listas de conceitos (termos mais relevantes) dos corpora de domínio, que podem ser utilizados diretamente, ou após revisão manual por especialistas, como listas de referências para os corpora desenvolvidos.
Essas contribuições serão disponibilizadas imediatamente após a publicação dessa tese em o qual esse trabalho de doutoramento se insere.
Difusão das Contribuições dessa Tese na Comunidade Acadêmica Apesar de ser uma ferramenta recente, e ainda em estágio de protótipo, a comunidade acadêmica tem utilizado resultados provenientes da ferramenta EATOLP para suas pesquisas.
Essa rápida disseminação da ferramenta atesta um reconhecimento das contribuições propostas nessa tese, e materializadas na implementação da ferramenta.
Além de trabalhos do grupo de PLN da PUCRS, resultados gerados por o método proposto nessa tese, e implementados no EATOLP, vem sendo utilizados também por pesquisadores do NILC da USP-São Carlos, DIE da UFPI, e do NIED da UNICAMP, como extrator de termos relevantes de corpus de domínio.
Já o grupo do Projeto TEXTCC da UFRGS, tem utilizado, além de a extração de termos, a geração de hierarquias de conceitos, que podem ser visualizadas on-line na forma de árvores hiperbólicas.
Somam- se a esses trabalhos já publicados, trabalhos em desenvolvimento por pesquisadores do PPGIA da PUCPR, do CIn da UFPE, e do Lia da UNESP, que têm utilizado listas de termos geradas por o EATOLP.
É importante ressaltar, ainda, a existê encia de trabalhos científicos, publicados, comparando a ferramenta EATOLP com outras ferramentas com o mesmo propósito.
Em esses trabalhos, verifica- se o melhor desempenho dos métodos propostos nessa tese, frente a o estado da arte da extração de termos e conceitos em textos de língua portuguesa.
De entre esses trabalhos, dois de eles comparam o desempenho dos métodos propostos nessa tese e implementados no EATOLP com a ferramenta NSP.
Outro trabalho compara o EATOLP com duas outras ferramentas similares, KEA e CLUTO, e um último trabalho, compara a ferramenta OntoLP ao EATOLP.
Trabalhos Futuros Os trabalhos futuros dessa tese se manifestam em cinco eixos de pesquisa:
Um eixo experimental;
Dentro de o eixo experimental, imagina- se a extensão dos experimentos feitos no decorrer de essa tese com outros corpora, mas principalmente com outras listas de referência.
Posto que a avaliação do processo desenvolvido é obrigatoriamente empírica, é interessante aumentar o número de experiências para outros corpora.
Porém, a escassez de recursos linguísticos, em especial listas de referências em português, limitou os experimentos feitos nessa tese.
No entanto, experiências com o índice tf-dcf e com os pontos de corte propostos podem ser repetidas para corpora e listas de referências em outras línguas.
Esse tipo de experiência pode aumentar a credibilidade dos métodos propostos nessa tese.
Dentro de o eixo de aplicações linguísticas, a disponibilização da ferramenta EATOLP abre a possibilidade de uma série muito grande de estudos linguísticos sobre padrões de uso da língua em diversos contextos, como foi feito no trabalho de Finatto Que analisou o vocabulário empregado em jornais populares.
Outros trabalhos nessa linha podem ser realizados comparando diferenças de estilo de escrita entre áreas do conhecimento, regiões do país, escolas de pensamento, etc..
Um trabalho particularmente interessante nesse eixo é a aplicação da extração de conceitos proposta em projetos da área de Inteligência Competitiva, que já estão em desenvolvimento no grupo de PLN da PUCRS.
A riqueza dos recursos linguísticos disponíveis na ferramenta EATOLP permite automatizar grande parte do trabalho hoje realizado quase que manualmente por terminólogos e linguistas, ao realizarem análises profundas de vasto material textual.
Dentro de o eixo de desenvolvimento (programação), se sobressai a ideia de adaptar a entrada da ferramenta EATOLP a textos anotados por outros parsers, como é o caso da ferramenta LXCenter.
Esse tipo de experiência irá permitir a verificação prática de que todo o processo proposto nessa tese pode ser empregado com qualquer ferramenta de anotação linguística.
Cabe salientar que essa linha de trabalho futuro implica numa adaptação quase que exclusivamente de programação, pois o processo proposto de extração, ordenação e identificação descritos nos esse trabalho futuro é de grande relevância prática, por aumentar o escopo de aplicação da ferramenta EATOLP.
Dentro de o eixo de extração de termos, existe a ideia de adaptar todas as propostas dessa tese para a aplicação em outras línguas além de o Português.
Algumas das etapas devem ser diretamente aplicáveis com pouca ou nenhuma alteração necessária, como é o caso do índice tf-dcf e, provavelmente, da análise de pontos de corte.
No que diz respeito às heurísticas utilizadas na extração de termos, a adaptação a ser feita será obrigatoriamente grande, pois as construções linguísticas são por definição dependentes do idioma.
Por outro lado, toda a parte de aplicações é completamente independente da língua.
De essa forma, a extensão das propostas feitas nessa tese para tratar corpora de domínio em outros idiomas não deve representar um trabalho futuro muito complexo, mas que pode ampliar enormemente o escopo de aplicação das contribuições dessa tese.
Dentro de o eixo de construção automática de ontologias, o caminho natural a seguir após essa tese é dar sequência ao processo de construção automática de ontologias.
Essa continuação implica observar em detalhe, e com um olhar mais científico, a proposta inicial de construção de hierarquias (Seção 6.4).
Também é importante sugerir novas aplicações dos conceitos extraídos, além de as quatro já disponibilizadas e descritas no Capítulo 6.
Em seguida, parece intuitivo buscar nas informações dos termos extraídos, os contextos verbais em os quais os conceitos apareceram de forma a deduzir relações não taxonômicas.
Também parece ser viável deduzir relações e diferenciações entre conceitos e instâncias (população de ontologias), de forma a obter pelo menos uma ontologia elementar conforme definição formal feita na Seção 2.2.1 Esse eixo de trabalhos futuros, apesar de ambicioso, parece viável por a quantidade de informação disponibilizada por o processo de extração proposto, além de promissor por a qualidade dos conceitos extraídos.
