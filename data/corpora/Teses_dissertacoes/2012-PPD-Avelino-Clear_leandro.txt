Atualmente, o desenvolvimento de sistemas por parte de a indústria de software tem aumentado.
Assim como a necessidade dos clientes em automatizar seus processos, cresce também a exigência dos mesmos na melhoria da qualidade dos sistemas informatizados.
Em esse sentido, o teste de software desempenha um papel fundamental.
Apesar de os benefícios que os testes proporcionam, a maioria de eles é realizada de forma manual e sem embasamento teórico e fundamentado, tornando a atividade de teste lenta e ineficaz.
Uma alternativa para a solução deste problema é a utilização de ferramentas de automatização de teste.
Essas ferramentas, além de agilizar o trabalho de uma equipe de testadores, provêem maior qualidade e eficácia para o processo de teste.
Entretanto, ainda que essas ferramentas possam tornar o processo de teste mais rápido, a criação de casos de teste para elas é realizada manualmente.
O ideal é automatizar também o processo de criação e execução de casos de teste para essas ferramentas.
Com o intuito de superar esta limitação, este trabalho propõe um conjunto de características que contempla as informações necessárias para automatizar a geração e execução de casos de teste concretizados para ferramentas de automatização de teste de desempenho.
A partir de as informações deste conjunto, foi realizada a implementação de plugins para uma linha de produto de ferramentas para teste baseado em modelos (Model-Based Testing MBT) denominada PLeTs (Product Line Testing Tools).
Estes plugins implementam a geração e a execução automática de scripts e cenários de teste utilizando duas ferramentas de automatização de teste de desempenho, Hp LoadRunner e Microsoft Visual Studio.
Com o objetivo de demonstrar a viabilidade da proposta deste trabalho foi definido um exemplo de uso, o qual se baseia na geração e execução automatizada de casos de teste utilizando os produtos gerados por a linha de produto PLeTs.
Palavras-chave: Teste de desempenho;
Ferramentas de teste;
Geração e execução de casos de teste.
Ferramentas para Teste de Desempenho Definição dos Casos de Teste Concretizados.
Com a presente expansão tecnológica, muitas empresas estão buscando cada vez mais agilizar seus negócios através do uso de sistemas computacionais.
Tendo em vista este contexto, além de o interesse em automatizar seus processos de negócio, nota- se uma grande demanda dos usuários em obter confiabilidade e disponibilidade para suas aplicações.
A expectativa dos usuários na qualidade dos serviços está fazendo com que cada vez mais seja necessária a utilização de técnicas que busquem avaliar, verificar e garantir a qualidade do software.
Atualmente, existem diversas técnicas cujo objetivo é avaliar a qualidade do software.
Em este contexto citam- se as técnicas de teste funcional e estrutural.
Entretanto, estas técnicas não podem ser aplicadas apenas ao final do processo de construção do software, mas sim durante toda a etapa de desenvolvimento do sistema.
Além destes, existem outros conceitos para avaliação de qualidade, como por exemplo, tolerância a falhas ou prevenção de falhas.
Esses conceitos compõem um conceito mais amplo conhecido como dependabilidade (dependability).
Para um sistema ser considerado confiável (dependable) ele deve possuir um conjunto de atributos, tais como, confiabilidade, disponibilidade, segurança, integridade, manutenabilidade, entre outros.
Desta forma, para que uma aplicação seja classificada com determinado índice de qualidade, a mesma deve passar por constantes etapas de testes e verificação.
Segundo, a execução de processos de &quot;Validação, Verificação e Teste «(VV&amp; T) nunca deve ser considerada como suficiente.
Logo, o processo para a geração de produtos de alta qualidade deve estar presente durante toda a etapa de construção do software, desde a sua concepção até o final da entrega do produto e não algo para ser pensado posteriormente.
Contudo, a evolução e incremento da complexidade dos sistemas computacionais existentes têm tornado o processo de teste destes sistemas uma atividade tão ou mais complexa que o processo de desenvolvimento em si.
Para contornar os problemas decorrentes do aumento da complexidade dos sistemas, e aumentar a eficiência no processo de geração de testes das aplicações, diversas ferramentas foram criadas para automatizar os processos de avaliação e verificação de software.
Atualmente, existem diversas ferramentas que avaliam a funcionalidade, desempenho, disponibilidade e escalabilidade das aplicações.
De entre elas citam- se Apache JMeter, IBM Rational Functional Tester (RFT), JUnit, Hp Quick Test Professional (QTP), Hp LoadRunner, IBM Rational Performance Tester (RPT), Selenium, Microsoft Visual Studio, e JaBUTi (Java Bytecode Understanding and Testing).
No entanto, apesar de os benefícios advindos do uso destas ferramentas para automatização da execução dos testes, ainda é necessária a execução de algumas atividades de forma manual ou semi-automatizada como, por exemplo, a etapa de elaboração dos scripts e cenários de teste.
Esta geração manual ou semi-automatizada torna o processo de teste ineficiente e suscetível à inserção de falhas nos scripts até mesmo por profissionais experientes.
Uma alternativa para melhorar o uso destas ferramentas é automatizar o processo de geração destes scripts e cenários.
Todavia, automatizar este processo pode não ser trivial, devido a a complexidade de mapear e determinar para diversas ferramentas qual o conjunto de informações que elas necessitam para tornar possível a geração automatizada de scripts e cenários de teste.
Visando superar esta limitação, esta dissertação apresentará um conjunto de características para teste de desempenho de aplicações web.
Este conjunto contém uma série de informações necessárias para a geração e execução de casos de teste concretizados.
Estes casos de teste são instâncias de casos de teste abstratos e são definidos como os scripts e cenários para as ferramentas de automatização de teste de desempenho.
Em outras palavras, este conjunto dispõe de um conjunto de informações, com as quais é possível automatizar a geração e execução de scripts e cenários para estas ferramentas.
Tais informações foram extraídas a partir de a análise de trabalhos que descrevem o processo de criação de scripts e cenários para diversas ferramentas de teste de desempenho.
Esses trabalhos apresentam detalhadamente as informações referentes às diversas configurações utilizadas por as ferramentas durante a definição de um caso de teste, tais como:
Quantidade de usuários virtuais para acessar a aplicação, tempo de execução do teste, contadores utilizados para monitoração como, por exemplo, tempo de resposta, percentual de utilização de CPU e quantidade de memória disponível.
Para a efetivação do trabalho adotou- se uma classificação para as características de teste das ferramentas.
Para essa classificação foi utilizada como base uma abordagem similar a utilizada por a Microsoft, que descreve em seis etapas o processo de teste de desempenho para aplicações web.
Outras abordagens foram estudadas,, porém devido a o fato das informações referentes às características corresponderem e se relacionarem com as informações descritas nas seis etapas do processo de teste da Microsoft, esta abordagem foi a que melhor se adaptou ao trabalho desta dissertação.
Com base nas informações definidas no conjunto de características, é possível implementar uma ferramenta que automatiza a geração e execução de scripts e cenários para as ferramentas de automatização de teste de desempenho existentes, tais como Hp LoadRunner e Visual Studio.
O objetivo é que a configuração das informações necessárias para o teste seja realizada de forma automatizada.
Desta forma, o tempo gasto na remoção de falhas em scripts e cenários pode ser utilizado para outros fins como maior e melhor documentação do projeto de teste do sistema, entre outras atividades.
Além de definir um conjunto de características para ferramentas de teste de desempenho, este trabalho está diretamente relacionado à outra pesquisa de dissertação de mestrado, cujo foco também está na descrição de um conjunto de características e informações necessárias para a geração de casos de teste de desempenho.
Entretanto, as informações contidas no conjunto proposto em foram geradas a partir de a análise de trabalhos que descrevem as características que modelos, como os definidos na Linguagem de Modelagem Unificada (Unified Modeling Language -- UML), devem conter para a geração dos casos de teste.
O conjunto destas duas pesquisas faz parte de um trabalho maior e integra também uma tese de doutorado, onde está sendo desenvolvida uma linha de produto de ferramentas de teste baseado em modelos, denominada PLeTs (Product Line Testing Tools).
A PLeTs utiliza as técnicas de linhas de produto de software para automatizar processos de teste e é capaz de gerar produtos que automatizam a geração de scripts e cenários de teste aplicando a técnica de teste baseado em modelos (Model Based Testing -- MBT).
Para prover esta automatização foram implementados plugins para a PLeTs, os quais provêem a geração automatizada de scripts e cenários, bem como a execução de testes utilizando as ferramentas LoadRunner e Visual Studio.
Estes plugins foram implementados por meio de a análise das informações definidas no conjunto de características proposto para esta dissertação.
Além disso, esta dissertação apresenta o processo utilizado para a implementação de um plugin para a ferramenta PLeTs que provê a geração e execução automatizada de casos de teste estruturais, utilizando a ferramenta JaBUTi.
Esta dissertação possui 7 capítulos e está estruturada da seguinte forma:
O Capítulo 2 apresenta os princípios básicos e os principais níveis do teste de software.
O Capítulo 3 apresenta uma descrição das funcionalidades de algumas das principais ferramentas para automatização de teste de software existentes no mercado.
O Capítulo 4 apresenta o conjunto de características para ferramentas de teste de desempenho, os trabalhos que foram referência para a sua construção e a efetivação do conjunto utilizando como base o processo de teste de desempenho adotado por a Microsoft.
Skills como exemplo de uso.
O Capítulo 6 apresenta as conclusões e trabalhos futuros.
O final do documento apresenta ainda um apêndice com a descrição de características e funcionalidades de algumas ferramentas para automatização de teste de software.
Em este capítulo serão descritos, inicialmente, os conceitos básicos de teste, em seguida serão apresentadas as fases de teste e as principais técnicas e métodos utilizados para a geração de casos de teste.
Para este capítulo o objetivo é introduzir uma base teórica fundamentada nos principais conceitos da área de teste de software.
Para um estudo mais detalhado sugere- se a leitura dos trabalhos,,, e.
Principios Básicos Antes de abordar as questões referentes aos princípios de teste de software é importante se familiarizar com alguns conceitos básicos da área e sua nomenclatura.
Atualmente, existem algumas divergências entre os autores quando o assunto se trata da definição dos conceitos de falha, erro e defeito.
Portanto, com o intuito de facilitar o entendimento desses conceitos será utilizada uma nomenclatura definida por autores com trabalhos focados na área de &quot;Tolerância a Falhas».
A razão desta escolha se baseou no fato desta nomenclatura ser utilizada como referência por grande parte da comunidade científica e por estar relacionada à área de atuação dos pesquisadores do projeto que originou esta dissertação.
Durante o desenvolvimento de um sistema computacional, o mínimo que se espera é que o mesmo esteja de acordo com o que foi definido durante sua especificação.
Quando o software não atinge essa meta diz- se que o sistema está com defeito (failure), ou seja, não está em conformidade com o que foi descrito e especificado para ele nas etapas iniciais do projeto.
Isto pode ocorrer, por exemplo, quando o sistema não atende alguma de suas especificações funcionais.
Um erro (error) é um estado do sistema em que um processamento a partir desse estado resulta num defeito.
Sabe- se que a execução de um serviço possui um conjunto de estados, quando algum desses estados diverge do estado correto que se espera do serviço, diz- se que o sistema está em estado de erro.
Por fim, uma falha (fault) é definida como sendo a causa inicial, física ou algorítmica de um erro, normalmente uma consequência da ação humana.
A Figura 2.1 a seguir exemplifica a definição dos conceitos de falha, erro e defeito descritos anteriormente.
Princípios de Validação, Verificação e Teste De posse do conhecimento referente a os conceitos básicos da área de teste e sua nomenclatura, as características e aplicabilidade dos princípios do teste de software podem ser introduzidos.
Com a evolução dos sistemas computacionais existentes, o processo de desenvolvimento de sistemas computacionais torna- se uma tarefa bastante complicada.
Esta situação pode agravar- se dependendo do tamanho e do grau de complexidade do software a ser projetado.
Por este motivo, a construção de um sistema está sujeita a muitos problemas e, por consequência, pode ser gerado um produto diferente daquele especificado durante as etapas iniciais de projeto.
Existem inúmeros fatores que influenciam na causa desses problemas, mas a principal causa de erros ocorre devido a a influência humana.
Como se sabe, o desenvolvimento de sistemas computacionais depende da capacidade de interpretação dos problemas por parte de pessoas, desta forma, a existência de falhas e erros é quase que inevitável, ainda que se utilizem técnicas fundamentadas nos princípios da engenharia de software como auxílio na prevenção desses erros.
Para que esses erros não persistam e sejam identificados antes da entrega do produto final junto ao cliente, o sistema deve passar por inúmeros processos de &quot;Validação, Verificação e Teste «(VV&amp; T).
Desta forma, é possível que o produto final esteja em conformidade com o que foi descrito durante a etapa de especificação.
Processos de VV&amp; T devem estar presentes desde a concepção do projeto e não algo para ser pensado posteriormente.
Por isso, existem diversas técnicas que podem ser aplicadas ao longo de o desenvolvimento do sistema, de forma a identificar falhas e com isso, garantir a consistência das funcionalidades da aplicação.
Segundo, existem princípios que caracterizam estas técnicas, neste sentido, citam- se os princípios da sensitividade, particionamento e restrição.
Durante o desenvolvimento de um projeto de software a equipe de desenvolvimento inevitavelmente, em determinado momento, será responsável por alguma falha, gerando erros no sistema que podem resultar em defeitos.
Um sistema com falha pode nem sempre gerar defeitos durante sua execução.
Em esse sentido, é melhor que um defeito num sistema sempre ocorra do que apenas às vezes, isso é o que afirma o princípio da sensitividade.
O custo na reparação de falhas de software pode variar de acordo com o momento em que uma falha é encontrada.
O custo de uma falha encontrada durante a etapa inicial de testes através, por exemplo, da utilização de uma ferramenta que utiliza uma verificação sintática em tempo real é muito pequeno.
Em contrapartida, um defeito detectado no sistema ocasionado por uma falha encontrada nas etapas finais de teste pode causar um custo enorme.
Maior ainda é o prejuízo quando ocorre um defeito no sistema detectado durante a utilização do software por parte de o usuário final.
Desta forma, quanto antes se detectar um defeito, menores serão os custos envolvendo tempo para os devidos reparos.
Ainda que o ideal seja encontrar falhas nas etapas iniciais de projeto, nem sempre é trivial encontrar- las.
Falhas encontradas muito tarde ou que são difíceis de encontrar têm como principal característica a não geração de defeitos.
Durante a execução de testes, determinados cenários podem ser difíceis de simular.
O resultado é que nem sempre um defeito será detectado e a consequência é a persistência da falha.
Segundo, o ideal é fazer com que essas falhas sejam mais fáceis de serem detectadas.
Desta forma, os defeitos irão aparecer mais frequentemente.
O princípio da sensitividade pode ser aplicado de três maneiras, em nível de desenvolvimento, nível de teste e nível de ambiente.·
Nível de desenvolvimento: Verificação do tamanho e alocação de vetores em memória em tempo de execução é um exemplo concreto da aplicação da sensitividade no nível de desenvolvimento.
Linguagens como Java, por exemplo, possuem suporte para aplicação do princípio neste nível;·
Nível de teste: Em este nível o ideal é utilização de técnicas de teste que busquem identificar falhas que geram defeitos.
Em sistemas multithreads por exemplo, a identificação de deadlocks pode ou não ocorrer dependendo do tempo em que os processos são executados, ou seja, vai depender do tempo que um processo leva para utilizar determinado recurso e do momento que outro processo solicita esse mesmo recurso.
Desta forma, um defeito no sistema pode ser observado em raras situações, dependendo do estado do ambiente.
A execução de testes que não cobrem todas essas possibilidades pode fazer com que não sejam revelados deadlocks, neste caso diz- se que o teste é pouco ou insuficientemente sensitivo;·
Nível de ambiente: Fatores externos à aplicação podem afetar o resultado esperado, para contornar este problema prioriza- se a utilização de técnicas para diminuir a influência desses fatores no sistema.
O ideal é que equipes e projetistas de teste busquem, de maneira eficiente, simular um ambiente mais próximo de a realidade.
Por exemplo, se determinada aplicação é executada num cenário onde o acesso a rede é privado, testar- la num ambiente onde diversas aplicações concorrem por recursos de rede pode influenciar significativamente no resultado final.
Assim como o princípio da sensitividade, o princípio do particionamento tem por objetivo auxiliar na definição de técnicas para assistir a atividade de teste.
Entretanto, diferentemente do princípio da sensitividade, o particionamento consiste em dividir um problema de maior magnitude, em termos de complexidade e &quot;tamanho», em diversos subproblemas que podem ser resolvidos de maneira independente, por este motivo, é também conhecido como &quot;divisão e conquista».
Com o intuito de facilitar a resolução de problemas, a Engenharia de Software utiliza muito este princípio, inicialmente nas etapas de especificação e levantamento de requisitos e durante a etapa de desenvolvimento.
Como não poderia ser diferente, este princípio é também muito utilizado nas áreas de teste e análise.
Os níveis de processo e técnicas constituem a aplicabilidade do particionamento.
Segundo o particionamento pode ser aplicado em cada um dos níveis descritos a seguir:·
Nível de processo: Em este nível o foco é dividir um problema complexo em diversas partes independentes, tornando mais simples de executar cada uma dessas atividades.
Um projeto de teste bem elaborado é organizado em diversas fases:
Fase de teste de unidade, integração, validação e sistema.
A ideia é focar na atividade de detectar defeitos em cada etapa e aproveitar o resultado das etapas anteriores para as etapas seguintes;·
Nível de técnicas: Em este nível são utilizadas técnicas de teste que tem por objetivo dividir a análise do sistema em etapas.
Como exemplos dessa aplicabilidade citam- se o teste funcional, onde os processos de teste são derivados da especificação e o teste estrutural que deriva casos de teste a partir de a análise da estrutura interna do programa.
Outro princípio que caracteriza abordagens e técnicas para teste e análise é definido como princípio da restrição.
É utilizado, normalmente, quando o custo é muito alto ou inviável de realizar a verificação de uma propriedade.
Muitas vezes, uma solução é realizar a verificação de uma propriedade diferente, mais restritiva ou limitando a verificação para uma classe de programas menor ou mais restritiva.
Um exemplo típico onde o problema poderia ser resolvido utilizando o conceito de restrição é a utilização de variáveis em determinado trecho do código que não foram inicializadas, ou seja, exigir a inicialização de variáveis que serão utilizadas posteriormente.
Muitos compiladores como, por exemplo, o do Java faz uso do conceito de restrição, que verifica e avisa quando um problema desse gênero ocorre.
Portanto, é muito importante, para o desenvolvimento de um sistema, a escolha de ferramentas e linguagens de programação que possuem esse tipo de funcionalidade.
Técnicas e Métodos de Teste Durante o processo de teste algumas fases podem ser identificadas, tais como teste de unidade, teste de integração, teste de validação e teste de sistema.
Quando todas essas fases são executadas, diz- se que o objetivo de validação e verificação do software foi atingido.
Em o processo de teste é definido da seguinte forma:·
Teste de unidade:
Também conhecido como teste unitário, ou teste de módulo, o teste de unidade tem como objetivo testar as menores unidades do software.
Visa testar cada unidade, a fim de garantir uma correta funcionalidade dos componentes mais elementares do programa.
Esses componentes podem ser funções, métodos ou classes.
Normalmente o tipo de defeito que o teste de unidade busca detectar está relacionado a falhas em algoritmos, falhas de lógica e até mesmo pequenos falhas de programação.
O teste de unidade é aplicado durante o processo de desenvolvimento podendo ser aplicado por o próprio programador;·
Teste de integração:
O teste de integração tem por objetivo encontrar erros derivados da integração dos módulos internos do software.
Diferente do teste de unidade que tem por objetivo testar cada módulo do sistema individualmente, o teste de integração busca detectar defeitos quando os módulos começam a ser utilizados em conjunto, ou seja, integrados.
O teste de integração também tem por objetivo verificar a compatibilidade entre os módulos, por exemplo, cada módulo pode ter passado por o teste de unidade e funcionar corretamente, porém, isso não garante que quando esses módulos passarem a se comunicar não haverá problemas relacionados à conectividade entre eles;·
Teste de validação:
O teste de validação visa verificar se o produto está de acordo com que foi descrito durante a etapa de especificação dos requisitos do software.
Em esta fase é executado um conjunto de testes visando avaliar as funcionalidades do software.
O objetivo é garantir que o sistema atende a todos os requisitos funcionais descritos durante a etapa de especificação;·
Teste de sistema:
Em o teste de sistema um conjunto de testes com diferentes características são executados.
Em esta fase o objetivo é testar o sistema do ponto de vista do usuário, onde as condições e ambiente de teste devem ser idênticos ou no mínimo estar próximo de o real.
Segundo, alguns exemplos de teste de sistema são o teste de desempenho, teste de segurança e teste de recuperação.
Também é importante esclarecer que durante cada uma das fases de projeto técnicas e métodos de teste devem ser aplicados sob o sistema com o objetivo de encontrar falhas e com isso, garantir com eficácia o cumprimento dos requisitos especificados.
Entretanto, essas técnicas e métodos possuem características diferentes entre si, devendo ser aplicados em cada fase de acordo com o amadurecimento do projeto em desenvolvimento.
Técnicas e métodos, tais como, teste funcional, teste estrutural, teste de regressão, teste orientado a objetos, teste baseado em defeitos e teste de desempenho servem como exemplo.·
Teste Funcional: O teste funcional é uma técnica que deriva casos de teste a partir de a especificação do programa.
Desta forma, toda a criação de casos de teste é baseada em especificações funcionais.
Esta técnica busca avaliar o comportamento externo do programa e não os detalhes internos de programação, por este motivo, é chamado de teste de especificação ou teste caixa-preta (black-box).
O teste funcional avalia o conjunto de saídas a partir de as entradas, verificando se o resultado obtido corresponde ao resultado esperado.
O teste funcional é capaz de identificar todo e qualquer defeito num programa ou aplicação, desde que para este se aplique todas entradas possíveis, quando isto ocorre o teste é chamado de teste exaustivo.
O problema é que o conjunto de entradas pode ser muito grande ou até mesmo infinito, tornando, devido a o tempo, inviável e impraticável o processo de teste.
Este problema pode fazer com que não se possa afirmar ou ter certeza de que o programa esteja totalmente correto.
Para contornar esse problema/ limitação foram definidos critérios de teste de modo a permitir que este processo se tornasse mais sistemático.
Particionamento por equivalência, análise de valor limite e Grafo Causa--Efeito são alguns dos critérios mais conhecidos para teste funcional;·
Teste Estrutural: O teste estrutural é uma técnica que tem por objetivo a geração de casos de teste a partir de a análise do código fonte.
Busca avaliar os detalhes internos da implementação, tais como teste de condição, caminhos lógicos, etc..
Por este motivo, é também chamado de teste orientado à lógica ou teste caixa-branca (white-box).
Como citado anteriormente, a técnica de teste estrutural se baseia na análise da estrutura do programa para a geração dos casos de teste.
Esta técnica define um conjunto de critérios estruturais, os quais se baseiam em diferentes elementos de programa para definir requisitos de teste.
Estes critérios tem por objetivo a execução de componentes e partes elementares de um programa e, são classificados basicamente em:
O critério de teste de caminho é um dos mais conhecidos critérios de complexidade.·
Teste de Regressão:
O teste de regressão é uma técnica de teste aplicada quando é realizada alguma alteração no sistema, por exemplo, quando se acrescenta ou retira determinada funcionalidade, quando o sistema migra para outra plataforma, ou seja, aplica- se o teste de regressão a cada nova versão do software.
Quando o software não mantém a corretude de suas funcionalidades em sua nova versão, diz- se que o sistema &quot;regrediu».
Em este contexto, o teste de regressão tem por objetivo contribuir para a &quot;não regressão «das novas versões do sistema.
Todo processo de teste que visa evitar problemas relacionados à regressão em sistemas é denominado teste de &quot;não regressão», por convenção omite- se o &quot;não «e geralmente diz- se teste de regressão.
A utilização desta técnica é fundamental, pois cada modificação realizada no sistema pode gerar um mau funcionamento dos módulos que em versões anteriores estavam corretos.
Desta forma, os testes realizados em versões anteriores do software devem ser repetidos em suas novas versões, a fim de garantir que os componentes atuais funcionam corretamente e continuam válidos.
O ideal é que esses testes sejam automatizados, com isso, o tempo para executar- los novamente será menor.
A automatização do processo de teste pode ser em muitos casos essencial, principalmente se o sistema está constantemente em processo de manutenção;·
Teste Orientado a Objetos:
O paradigma de programação orientada a objetos surgiu trazendo um enfoque diferente do paradigma de programação procedural.
Surgiu com o intuito de possibilitar uma melhor gerência e tratamento da complexidade de construção de um sistema.
Possibilita uma maior abstração por focar na utilização de classes para agrupar um conjunto de objetos autônomos e de técnicas como herança e polimorfismo, que contribuem para a reutilização de código trabalhando de forma hierárquica.
Ainda que o paradigma de programação orientada a objetos tenha surgido com o intuito de facilitar a construção de software, as mesmas técnicas que contribuem nesse sentido podem dificultar quando o assunto diz respeito ao teste desses sistemas.
Quando um defeito é detectado devido a uma falha encontrada, por exemplo, numa classe que herda os métodos de uma classe mais abstrata (superclasse), pode ser difícil de afirmar qual das classes possui falhas.
Segundo, as características de softwares com paradigma de orientação a objetos que podem impactar na testabilidade são as seguintes:·
Teste Baseado em Defeitos:
O teste baseado em defeitos tem por objetivo extrair a maior quantidade de informação possível dos defeitos de software detectados mais frequentemente.
A partir de essas informações é possível gerar casos de testes com o intuito de detectar e corrigir mais rapidamente esses defeitos.
Quanto maior o conhecimento sobre os defeitos de software, maiores são as garantias de se obter qualidade na implementação de um sistema.
O teste baseado em defeitos utiliza um modelo, baseado nas informações dos defeitos que ocorrem mais frequentemente num programa, com o intuito de determinar um conjunto de possíveis defeitos que podem ocorrer no sistema testado.
Em posse das informações do modelo podese introduzir propositalmente no software um conjunto de falhas e executar o teste, a fim de verificar se quantidade de defeitos provenientes da inserção dessas falhas está coerente com a quantidade de defeitos que o teste deveria detectar.
Em outras palavras, o objetivo do teste baseado em defeitos é testar a qualidade do teste.
A eficácia do teste baseado em defeitos vai depender da qualidade do modelo de informações sobre os possíveis defeitos do software.
É bem possível que os métodos de teste disponíveis para verificar a qualidade do sistema não revelem os defeitos provenientes de uma injeção de falhas nesse sistema.
Para contornar este problema são utilizados alguns critérios de teste baseado em defeitos, o mais comum e conhecido desses critérios é denominado análise de mutantes.
Um mutante é um programa variante em relação a o original, onde essa variação é proveniente de uma pequena mudança no código (inserção de falha), como por exemplo, a substituição de um operador &quot;por «Diz- se que um programa mutante é válido se o resultado de sua execução é diferente do resultado do programa original;
Teste de Desempenho Com o crescente crescimento tecnológico, os clientes não estão apenas exigindo que suas aplicações atendam os requisitos funcionais previamente acordados, mas que também atendam suficientemente os requisitos relacionados ao desempenho.
Um survey relacionado ao desempenho constatou que metade das empresas de software encontrou problemas relacionados ao desempenho em pelo menos 20% das aplicações por elas implantadas.
Estes problemas foram evidenciados principalmente em aplicações web, as quais são responsáveis por um alto processamento provenientes de requisições de usuários.
Em este contexto, a modelagem de desempenho possui grande importância, pois visa identificar os problemas de desempenho existentes e soluções para estes problemas.
A modelagem de desempenho, a qual é uma parte da engenharia de desempenho de software (Software Performance Engineering -- SPE), tem por objetivo compreender as características de desempenho de aplicações sob diferentes cargas e diferentes configurações de hardware e software.
Técnicas de modelagem de desempenho são classificadas em medição, análise e simulação.
As técnicas baseadas em medição tem por objetivo a realização de inúmeros testes de desempenho sob aplicação a ser testada, entretanto, estes testes somente são feitos após a aplicação estar totalmente pronta e disponível.
Para contornar este problema, existem técnicas analíticas e de simulação que se baseiam na construção de modelos para estudar e prever as características de desempenho das aplicações.
As técnicas analíticas utilizam modelos teóricos, enquanto técnicas de simulação emulam as funcionalidades de uma aplicação por meio de simulações computacionais, onde o desempenho pode ser observado.
O problema é que técnicas analíticas e de simulação exigem um conhecimento aprofundado da aplicação, onde uma documentação detalhada das funcionalidades do sistema se faz necessária.
Entretanto, em muitos casos, a maior parte das informações do sistema está presente somente no próprio código da aplicação e, por este motivo, as técnicas baseadas em medição acabam sendo mais utilizadas.
O teste de desempenho baseado em medições tem por objetivo determinar o limite do sistema submetido a uma determinada carga num ambiente de teste específico.
Ele fornece um indicador utilizado para avaliar o quanto um sistema ou o componente de um sistema é capaz de cumprir os requisitos de desempenho, tais como, tempo de resposta ou throughput.
Além disso, o teste de desempenho busca identificar os possíveis gargalos causadores de uma degradação de desempenho no sistema.
Diversos autores definem conjuntos distintos para representar os diferentes tipos de teste de desempenho existentes.
Por exemplo, define cinco subconjuntos de teste de desempenho, Baseline test, Load test, Stress test, Soak or stability e Smoke test..
Entretanto, define apenas três subconjuntos, Load test, Stress test, Endurance or Durability test..
Em é definido um subconjunto para teste de desempenho muito semelhante ao utilizado em, o qual é formado por três tipos, Load test, Stress test e Strength test..
Outra abordagem utilizada para representar os diferentes tipos de teste de desempenho pode ser encontrada em, o qual define três tipos, sendo eles Load test, Stress test e Capacity test..
Apesar de a divergência entre autores quanto a a definição dos tipos de teste de desempenho existentes, os mais comuns e conhecidos são:·
Teste de carga (Load testing):
Tem por objetivo determinar ou validar o comportamento de um sistema sob condições normais de carga.
Busca verificar se o sistema cumpre os requisitos de desempenho especificados;·
Teste de estresse (Stress testing):
Tem por objetivo determinar o comportamento de um sistema quando ele é submetido além de as condições normais de carga.
Busca revelar bugs e determinar os pontos de defeito do sistema.
Devido a a importância do teste de desempenho, existem diversas ferramentas cujo objetivo é automatizar a execução de testes.
De entre estas ferramentas citam- se Apache JMeter, Hp LoadRunner, IBM Rational Performance Tester, SilkPerformer e Visual Studio, as quais visam mitigar os problemas de desempenho das aplicações.
Para maiores detalhes acerca de as funcionalidades destas ferramentas recomenda- se a leitura do apêndice A. Considerações Os conceitos, métodos e técnicas de teste são fundamentais para a elaboração de um projeto de teste bem estruturado e confiável.
Entretanto, devido a a complexidade dos sistemas computacionais, a atividade de teste está tão complexa quanto o processo de desenvolvimento.
Para mitigar estes problemas e otimizar o processo de teste, diversas ferramentas de teste de software foram desenvolvidas.
Estas ferramentas se baseiam nos conceitos, métodos e técnicas de teste encontradas na literatura e, são utilizadas para a criação e execução de casos de teste de forma mais eficiente.
Em o capítulo seguinte serão descritas as características e funcionalidades de algumas ferramentas de teste, desenvolvidas com o intuito de avaliar a qualidade de sistemas.
O mercado atual tem apresentado empresas cada vez mais procurando automatizar seus processos de negócio através da utilização de sistemas computacionais.
Essa necessidade tem contribuído para um aumento da produção desses sistemas por parte de a indústria de software.
Assim como a necessidade dos clientes em automatizar seus processos, cresce também a exigência dos mesmos na busca por a qualidade dos sistemas informatizados.
Em esse sentido, e conforme mencionado anteriormente, o teste de software torna- se essencial para se atingir com êxito a qualidade de sistemas.
Apesar de os benefícios que o teste traz, a maioria de eles é realizada de forma manual e sem um embasamento teórico e fundamentado, tornando a atividade de teste lenta e ineficaz.
Uma alternativa que contribui na solução deste problema é a utilização de ferramentas de automatização de teste para auxiliar nesses processos.
Essas ferramentas, além de agilizar o trabalho de uma equipe de testadores, contribuem no sentido de obter com eficácia a qualidade dos testes e programas, por serem criadas com base em técnicas e métodos de teste bem conhecidos, com um fundamento teórico e científico.
Em este capítulo será apresentada, inicialmente, uma análise das diferenças entre algumas ferramentas de teste1.
A análise contempla ferramentas para diversos tipos de teste, tais como:
Ferramentas para automatização de teste funcional, estrutural, desempenho, unitário e regressão.
Além disso, entre as diversas ferramentas de automatização de teste de software, optou- se por focar em ferramentas e frameworks, cuja principal característica é prover recursos para automatização de teste de desempenho.
Desta forma, as ferramentas selecionadas para o foco do estudo foram JMeter, Hp LoadRunner, IBM Rational Performance Tester, SilkPerformer e Visual Studio.
Características e Funcionalidades das Ferramentas Tendo apresentado no Capítulo 2 um estudo relacionado aos conceitos de diferentes tipos de teste de software, o objetivo para esta seção é apresentar uma análise de algumas ferramentas que visam automatizar os diferentes processos de teste mencionados no Capítulo 2.
Portanto, pretendese apresentar uma visão geral dos conceitos e funcionalidades destas ferramentas e os diversos tipos de teste que elas executam.
Entre as ferramentas estudadas, o JMeter, uma ferramenta open source escrita em Java, possui como foco a execução de testes de desempenho.
Outras ferramentas que possuem a característica de automatizar e executar teste de desempenho, porém comerciais, são o LoadRunner da Hp e o Rational Performance Tester da IBM.
Ambas possuem algumas características em comum, por exemplo, as duas ferramentas utilizam a técnica Record &amp; Playback para a geração de scripts de teste.
Esta técnica consiste na gravação de todas as interações realizadas por o usuário 1 Uma descrição com maiores detalhes encontra- se no apêndice desta dissertação.
Essas duas empresas (Hp e IBM) também possuem ferramentas para automatização de teste funcional e regressão:
Hp Quick Test Professional e IBM Rational Functional Tester.
Além disso, as duas ferramentas têm como foco a execução de testes para aplicações web e possuem etapas de criação e execução de testes bem semelhantes, por exemplo, ambas geram relatórios para análise de resultados ao final de cada teste executado.
Outra ferramenta que provê automatização de testes funcionais é o Selenium.
Esta ferramenta possui suporte para criação de scripts de teste em diversas linguagens, tais como, Java, C&amp;, Perl, PHP, Python e Ruby.
Além de analisar ferramentas que automatizam testes de desempenho e testes funcionais, foi realizado um estudo sobre uma ferramenta utilizada para auxiliar a geração de testes estruturais.
Denominada JaBUTi, esta ferramenta open source, foi desenvolvida por o grupo de engenharia de software da USP-São Carlos.
Sua principal característica, e que a faz diferenciar- se das demais ferramentas para teste estrutural, diz respeito à possibilidade de executar testes sem que seja necessária uma análise do código fonte.
Toda execução de teste é realizada a partir de a análise do Java Bytecode.
Tendo apresentado na seção 3.1 uma visão geral das características e funcionalidades de algumas ferramentas que automatizam diferentes tipos de teste, o objetivo desta seção é focar a análise em ferramentas que automatizam um tipo de teste específico.
Em este contexto, optou- se por a seleção de ferramentas para o teste de desempenho de aplicações web.
Desta forma, foi realizado um estudo mais aprofundado das características e funcionalidades destas ferramentas.
Para este estudo foram analisadas cinco ferramentas para teste de desempenho web:
JMeter, LoadRunner, Rational Performance Tester, SilkPerformer e Visual Studio.
O objetivo desta análise foi identificar as características de cada uma e apresentar como cada ferramenta necessita ser configurada durante o processo de criação e execução de testes.
Além disso, serão apresentadas as diferenças e semelhanças entre elas e como cada uma necessita ser configurada durante as duas principais etapas que constituem a execução automática de testes de desempenho:
Para a etapa de geração dos scripts de teste, as cinco ferramentas analisadas utilizam uma técnica denominada Record &amp; Playback.
Esta técnica consiste na gravação de todas as interações realizadas por o usuário com determinada aplicação.
Entretanto, antes da gravação ser iniciada, é necessária a configuração do parâmetro referente a o endereço IP da aplicação que se deseja testar, o qual ocorre por meio de a interface que cada ferramenta em particular possui.
Para o JMeter, essa configuração é feita por meio de a especificação do parâmetro Http Request.
Uma desvantagem é que para cada web link da aplicação é necessário configurar um Http Request.
Desta forma, quando se deseja configurar um cenário de teste com usuários requisitando 10 web links, por exemplo, será necessário configurar 10 Http Request.
Não é o caso das demais ferramentas, para o LoadRunner, RPT, SilkPerformer e Visual Studio apenas um parâmetro necessita ser configurado.
Com isso, basta editar este parâmetro somente com o endereço inicial da aplicação.
Os demais web links são gerados automaticamente por meio de a técnica Record &amp; Playback, onde um navegador como o Internet Explorer, por exemplo, é carregado automaticamente com a página da aplicação definida.
Em este momento toda a interação feita por o usuário com a aplicação é gravada para gerar o script de teste.
Como citado anteriormente, as cinco ferramentas dispõem desta funcionalidade e o processo de gravação é muito semelhante para cada uma.
Entretanto, o JMeter é a única ferramenta de entre as demais que necessita de configurações adicionais.
Para habilitar esta funcionalidade o JMeter necessita adicionar um elemento denominado Http Proxy Server e alterar a configuração de proxy do navegador.
O IP da máquina local também deve ser definido e a porta 8080 (padrão do apache) deve ser utilizada.
Existem outras diferenças entre as ferramentas para esta etapa de geração de scripts.
A principal de elas diz respeito ao número de protocolos que cada uma suporta.
Em este contexto, destaca- se o LoadRunner por ter suporte a uma gama muito maior de protocolos (Http/ HTML, Oracle NCA, SAPGUI, etc.) se comparado com as demais ferramentas.
O destaque negativo é o JMeter por suportar apenas os protocolos Http e Https para a gravação dos scripts de teste.
Outra diferença entre as ferramentas está relacionada às linguagens de scripts que cada uma possui.
O LoadRunner, por exemplo, permite a gravação de scripts nas linguagens C, Visual Basic ou Java.
A linguagem Java também é utilizada para a gravação dos scripts de teste com o RPT.
Por outro lado, o JMeter grava todas as informações relativas às interações dos usuários com a aplicação num arquivo no formato XMI (XML Metadata Interchange).
O SilkPerformer implementa uma linguagem específica para a gravação de seus scripts, os quais são gerados na linguagem denominada Benchmark Description Language (BDL).
Outra ferramenta que, assim como o LoadRunner, possui suporte à gravação de scripts em três linguagens distintas é o Visual Studio, suportando a gravação de scripts em Visual Basic, C&amp; e C+ (ver Tabela A segunda etapa que constitui a geração e execução automática de casos de teste de desem-penho refere- se à configuração do cenário de teste.
Para esta etapa são configurados basicamente quatro parâmetros:
Quantidade de usuários, duração do teste, perfil da carga de trabalho e contadores para avaliar o desempenho do ambiente.
Após a definição dos scripts para um cenário de teste específico as cinco ferramentas analisadas necessitam especificar cada um destes quatro parâmetros.
A quantidade de usuários refere- se ao número de usuários virtuais que farão requisições a o (s) endereço (s) Http configurado (s).
Para cada uma das ferramentas analisadas basta editar o parâmetro referente a essa informação.
Em o JMeter essa configuração é feita por meio de a edição do parâmetro VUsers, para o RPT é feita por meio de a edição do parâmetro Quantidade de Usuários, no SilkPerformer é feita através da edição do parâmetro Vusers e no Visual Studio é realizada por meio de a edição do parâmetro User Count.
Quanto a a configuração do parâmetro relativo ao tempo de duração do teste, não há diferenças significativas entre as ferramentas.
A distinção na verdade fica por conta dos nomes ou identificadores que cada uma possui para a configuração deste parâmetro.
Em o JMeter, LoadRunner, RPT, SilkPerformer e Visual Studio esta informação é representada respectivamente por Duration (seconds), Duration (Loops), Stop running the schedule after an elapsed time, Simulation time, Run Duration.
Outro parâmetro necessário configurar diz respeito ao perfil da carga de trabalho.
Entretanto, somente o LoadRunner, o SilkPerformer e o Visual Studio possuem essa funcionalidade.
Refere- se ao perfil de teste que será executado.
Em o LoadRunner essa informação é configurada em Start VUsers, com ela pode- se definir um perfil onde todos os VUsers irão iniciar o teste ao mesmo tempo to run, onde existem cinco tipos de perfis, são eles:
Increasing, Steady State, Dynamic, All Day, Queuing.
Os principais perfis são Increasing e Steady State, no perfil Increasing os usuários iniciam o teste de forma gradativa e no perfil Steady State todos os usuários iniciam o teste ao mesmo tempo.
Para o Visual Studio essa informação é definida por o parâmetro Load Pattern, assim como para o LoadRunner existem dois tipos de perfis de configuração para o Visual Studio.
O primeiro se trata do perfil Constant Load, onde os usuários iniciam o teste ao mesmo tempo.
O segundo é denominado Step Load, onde os usuários iniciam o teste de forma gradativa.
As demais ferramentas (JMeter e RPT) mesmo não permitindo definir um perfil da carga de trabalho possibilitam especificar informações de rampa de subida (rump-up).
Este parâmetro determina a quantidade de usuários virtuais que iniciarão o teste num intervalo de tempo pré-determinado.
Inicialmente, o teste inicia com um determinado número de usuários virtuais e a cada intervalo de tempo previamente configurado a mesma quantidade de usuários definida anteriormente inicia suas interações sob a aplicação a ser testada.
Este processo ocorre até que a quantidade total de usuários virtuais definidas para o teste esteja requisitando a aplicação.
Para o JMeter a configuração é realizada por meio de a especificação do parâmetro Ramp-Up period e através da especificação do parâmetro Delay between starting each user para o RPT.
Para finalizar a configuração do cenário de teste é necessária a definição de alguns contadores, os quais são utilizados para medir informações de desempenho do ambiente de teste.
Para cada uma das cinco ferramentas é possível especificar e definir diversos tipos de contadores, tais como:
Percentual de utilização de CPU, quantidade de memória disponível, percentual de utilização do disco para leitura e escrita, bem como contadores referentes ao tráfego de dados na rede como, por exemplo, throughput.
Esta análise se baseou nas principais funcionalidades das ferramentas, as quais possibilitam a geração dos scripts e configuração do cenário de teste.
Em este contexto, se observou que principalmente a etapa de geração dos scripts de teste é muito semelhante para cada uma das cinco ferramentas analisadas.
Todas possuem métodos para a captura das interações do usuário com a aplicação.
Entretanto, elas suportam diferentes linguagens para gravação dos scripts de teste e algumas permitem utilizar mais linguagens que outras, o que pode ser um diferencial no momento que se deseja optar por a aquisição desta ou daquela ferramenta de teste.
Também é importante, a fim de estabelecer um processo de teste eficiente e eficaz dentro de a empresa, que a equipe de teste tenha domínio das linguagens que a ferramenta de teste definida suporta.
Outro fator a ser levado em consideração diz respeito à quantidade de protocolos de geração de scripts que cada uma possui, pois é no mínimo desejável que a ferramenta de teste tenha suporte à tecnologia utilizada por a aplicação a ser testada.
Também existem algumas diferenças entre as ferramentas do ponto de vista da configuração dos cenários de teste.
Entretanto, essas diferenças são evidenciadas, por exemplo, nos identificadores ou nos nomes dos parâmetros utilizados para a configuração das informações do cenário.
Visto que as cinco ferramentas analisadas possibilitam a configuração de parâmetros como quantidade de usuários, tempo de duração do teste, perfil da carga de trabalho e contadores para avaliar o desempenho do ambiente.
Em este contexto, as maiores distinções são verificadas na configuração dos contadores, pois algumas ferramentas permitem mensurar uma quantidade maior de informações que outras.
Também verificou- se diferenças relacionadas ao perfil da carga de trabalho, pois somente LoadRunner, SilkPerformer e Visual Studio possuem tal funcionalidade.
As demais ferramentas (JMeter e RPT) ainda possibilitam a configuração de informações de rampa de subida, porém possuem características mais simples que as outras ferramentas do ponto de vista da configuração do perfil da carga de trabalho.
Este capítulo apresentou, inicialmente, uma análise referente a as características e funcionalidades de algumas ferramentas que automatizam diferentes tipos de teste.
Tendo realizada esta análise, a próxima etapa focou no estudo de um conjunto de ferramentas que automatizam a execução de casos de teste de desempenho.
Este estudo apresentou uma análise referente a as diferenças e semelhanças entre as ferramentas no que diz respeito às etapas de geração de scripts e configuração do cenário de teste.
Por meio de esta análise pôde- se verificar que algumas informações são fundamentais para a criação e execução de casos de teste e, muitas dessas características estão presentes e são fundamentais para geração e configuração de scripts e cenários de teste em diversas ferramentas.
Em o próximo capítulo será apresentado um conjunto de características para ferramentas de teste de desempenho, o qual possui informações para gerar automaticamente scripts e cenários de teste.
Este capítulo apresenta, detalhadamente, a descrição referente a a implementação de um conjunto, o qual define, com base em diversos trabalhos, as características e informações necessárias para a geração e execução de casos de teste concretizados para aplicações web.
Para a efetivação do trabalho adotou- se uma classificação para as informações contidas no conjunto de características implementado.
Para essa classificação foi utilizada como base uma abordagem similar à utilizada por a Microsoft, que descreve em seis etapas o processo de teste de desempenho para aplicações web.
Este capítulo também apresenta alguns conceitos referentes à Linha de Produto de Software (Software Product Line -- SPL), bem como as etapas para implementação de plugins para uma ferramenta que se baseia em conceitos de SPL.
Esta ferramenta é denominada PLeTs (Product Line Testing Tools).
A o final, este capítulo ainda apresenta as etapas para a implementação de um plugin para a PLeTs exclusivamente para a geração e execução de testes utilizando a ferramenta de teste estrutural denominada JaBUTi.
Levantamento de Informações Inicialmente, foi realizado um estudo e uma análise de diversos trabalhos que descrevem o processo de criação de scripts e cenários de teste utilizando ferramentas para automatização de teste de desempenho.
Esta análise se baseou, principalmente, em trabalhos que fazem referência a cinco ferramentas, sendo elas:
Apache JMeter, Hp LoadRunner, IBM Rational Performance Tester, Borland SilkPerformer e Microsoft Visual Studio.
A seleção dessas ferramentas como base para esta análise foi motivada principalmente por uma pesquisa de mercado realizada por.
Como pode- se observar na Tabela 4.1, as cinco ferramentas citadas neste trabalho foram relacionadas, com destaque para LoadRunner, Rational Performance Tester e SilkPerformer que segundo a pesquisa ocupam o maior percentual da fatia de mercado.
Para esta pesquisa, verificou- se que as ferramentas comerciais são utilizadas por grande parte das empresas de desenvolvimento e teste de software.
Por outro lado, ferramentas open source eram pouco utilizadas e com isso, foram fracamente evidenciadas na pesquisa.
Mesmo assim, juntamente com outras ferramentas elas chegam a ocupar 3% do mercado.
Tendo definido o conjunto de ferramentas, o próximo passo foi realizar a instalação de cada ferramenta e, posteriormente, executar um conjunto de testes com cada uma de elas.
O objetivo foi analisar o que necessita ser configurado em cada ferramenta para gerar e executar scripts e cenários de teste.
Desta forma, após esta etapa de instalação e análise foi possível obter um conhecimento referente a as características de desempenho necessárias para automatizar a geração de casos de teste para as ferramentas.
De posse desse conhecimento, a próxima etapa foi realizar uma análise e uma pesquisa por trabalhos científicos que descrevessem as mesmas características de desempenho evidenciadas após a etapa de instalação e configuração das ferramentas.
O objetivo foi buscar na literatura trabalhos que comprovassem/ justificassem as características de desempenho previamente definidas.
A seguir, são descritos alguns dos trabalhos utilizados para a identificação das características necessárias para geração de casos de teste de desempenho concretizados.
1 Correspondem às ferramentas Open Source e outras ferramentas comerciais.
O trabalho realizado por tem por objetivo determinar os principais fatores responsáveis por a degradação de desempenho em sistemas computacionais.
Para isto, foram utilizados métodos de simulação para construir um ambiente de sistemas com diminuição de recursos, a fim de determinar os fatores referentes à estrutura do ambiente que impactam na queda de desempenho.
Em este trabalho, o JMeter foi utilizado para a criação deste ambiente computacional de simulação e com isso, reproduzir este processo de diminuição de recursos.
Apesar de a pesquisa deste artigo focar na definição de fatores responsáveis por a degradação de desempenho em determinados sistemas computacionais, a maior contribuição deste trabalho para a pesquisa desta dissertação foi apresentar um conjunto de informações utilizadas para a execução de testes de desempenho utilizando o JMeter.
Esse trabalho, o qual é apresentado na Tabela 4.2 (Jing), descreve um conjunto de informações respectivamente, as informações URLs da Aplicação, Quantidade de Usuários e Tempo de Rampa de Subida, descritas na Tabela 4.3.
Em são descritas as informações básicas e necessárias para a criação e execução de scripts e cenários de teste para o RPT.
As características e informações descritas nesse documento servem como base para comprovar que é possível a geração de casos de teste utilizando um conjunto determinado de informações.
O estudo é válido, uma vez que o relatório descreve e ilustra o que é necessário configurar na ferramenta para que um simples caso de teste possa ser criado e executado.
Esse trabalho, ainda que apresente diversas informações relacionadas a diversas funcionalidades do RPT, sua contribuição para esta dissertação foi relevante por apresentar informações e características relativos à geração e a execução de scripts e cenários para o RPT.
Este trabalho, o qual é apresentado na Tabela 4.2 (Chadwick), descreve o seguinte conjunto de informações:
Host/ URL, Number of users e Think Time.
Estas informações representam, respectivamente, as informações URLs da Aplicação, Quantidade de Usuários e Tempo de Pensamento, descritas na Tabela 4.3.
Em a última linha desta tabela são encontrados trabalhos que descrevem as características e informações de outras ferramentas, completando então um total de 30 trabalhos utilizados como referência para a implementação do conjunto de características.
A seleção deste número de trabalhos, sendo a maioria 5 por ferramenta, foi motivada por o fato de que após a pesquisa de 30 trabalhos se verificou que muitas informações referentes às características das ferramentas se repetiam.
E, mesmo ao final da análise destes 30 trabalhos já não haviam características diferentes das que já tinham sido mapeadas.*
Outras: Correspondem às ferramentas WebLoad, The Grinder, Spirent Avalanche, Parasoft WebKing.
Com base nas informações adquiridas dos trabalhos estudados, pôde- se fazer um levantamento das características necessárias para a criação de casos de teste concretizados.
E, por meio de esta análise foi definido um conjunto de características para cada ferramenta.
Analisando este conjunto de características verificou- se que muitas de elas possuem o mesmo tipo de informação, a diferença está relacionada ao nome correspondente a cada característica.
Por esse motivo, as características comuns entre as ferramentas podem ser representadas por um único nome ou identificador.
Entretanto, existe um conjunto de características que não está presente em todas as ferramentas, mas sim em algumas e um outro conjunto que é particular de uma ferramenta específica.
Desta forma, o conjunto de características pode ser classificado em três formas distintas:·
Características Comuns (CC):
Conjunto das características comum a todas as ferramentas;·
Características Parcialmente Comuns (CPC):
Conjunto das características comum a algumas ferramentas;·
Características Individuais (Ci):
Conjunto das características pertencentes a uma única ferramenta.
Tendo definido as características para cada ferramenta, o que se fez foi construir o conjunto a partir de as informações das características pertencentes a CC e CPC, sendo que para o segundo conjunto apenas as características comuns a três ferramentas optou- se por incluir no conjunto.
Esta decisão foi tomada tendo como premissa o fato de que um dos objetivos para o conjunto de características é tornar- lo independente de tecnologia, permitindo que o mesmo disponibilize principalmente as informações essenciais para o teste de desempenho e não informações para teste utilizando alguma ferramenta em questão.
Portanto, pretende-se implementar um conjunto de características mais &quot;limpo «e robusto.
Uma alternativa para aproveitar as informações que não foram incluídas no conjunto é utilizar- las num &quot;template».
Desta forma, sempre que alguém optar por implementar um software para automatizar a geração e execução de casos de teste utilizando alguma ferramenta que possui características que não foram inseridas no conjunto, poderá fazêlo a partir de a inserção dessas informações num arquivo &quot;template», onde o projetista poderá configurar- las da forma que achar mais conveniente.
Com base nas informações adquiridas dos trabalhos analisados e na estratégia utilizada, o conjunto de características pôde começar a ser construído.
As características escolhidas são descritas a seguir e sua estrutura pode ser melhor visualizada na Tabela 4.3.·
URLs da Aplicação:
Refere- se às informações de configuração dos endereços IP da aplicação para onde será gerada a carga;·
Quantidade de Usuários: Refere- se ao número total de usuários que farão requisições a o (s) endereço (s) configurado (s);·
Tempo de Duração do Teste: Refere- se ao tempo total de execução do teste;·
Tempo de Pensamento: Refere- se ao tempo que cada usuário leva para realizar determinada atividade entre duas requisições consecutivas.
Por exemplo, preenchendo algum formulário ou lendo algum tipo de informação referente a a página web da aplicação;·
Contadores: Refere- se ao conjunto de contadores que serão utilizados para medir o desempenho do ambiente.
Cada uma das ferramentas analisadas possui um conjunto de contadores que pode ser configurado para o teste.
Algumas ferramentas possuem ou podem utilizar um número maior de contadores em relação a as outras, mas todas são capazes de mensurar informações básicas, como:
Utilização de CPU/ Memória e throughput.
Durante a execução do teste para cada ferramenta as informações referentes aos contadores são mostradas em tabelas e gráficos;·
Record &amp; Playback: É uma técnica que consiste na gravação de todas as interações realizadas por o usuário com uma aplicação.
Para cada uma das ferramentas analisadas essa técnica é utilizada automaticamente para a geração dos scripts de teste.
A utilização dessa técnica ocorre logo após a definição/ configuração do parâmetro referente a o endereço IP da aplicação que se deseja testar.
Em seguida, um navegador como o Internet Explorer, por exemplo, é carregado automaticamente com a página da aplicação definida e toda a interação feita por o usuário com a aplicação é gravada para gerar o script de teste;·
Tempo de Rampa de Subida: Define o tempo que levará para um determinado conjunto de usuários (definido em Quantidade de Usuários da Rampa de Subida) iniciar o teste, ou seja, realizando requisições ao endereço previamente configurado (ocorre durante o início do teste);·
Quantidade de Usuários da Rampa de Subida: Define a quantidade de usuários que iniciarão o teste num determinado tempo, este tempo é definido através da edição do parâmetro Tempo de Rampa de Subida;·
Tempo de Rampa de Descida: Define o tempo que levará para um conjunto de usuários (definido em Quantidade de Usuários da Rampa de Descida) deixe de realizar requisições à aplicação (ocorre durante o final do teste).
As ferramentas que possuem esta característica são LoadRunner, SilkPerformer e RPT.
Para cada uma de elas é muito semelhante a edição desse parâmetro.
É necessário simplesmente informar o tempo (o formato dependerá da ferramenta) que levará para cada usuário ou conjunto de usuários terminar sua interação na aplicação tão logo o teste seja iniciado;·
Quantidade de Usuários da Rampa de Descida: Define a quantidade de usuários que deixarão de fazer carga no sistema num determinado tempo, este tempo é definido através da edição do parâmetro Tempo de Rampa de Descida;·
Tempo de Aquecimento: Define um período de tempo em que a ferramenta de teste irá coletar informações referentes aos contadores configurados para o teste.
Durante esse período não é realizado nenhum tipo de carga no sistema.
O objetivo é verificar se não há influência de nenhum fator externo à aplicação que possa, por exemplo, estar concorrendo por recursos e com isso, afetar nos resultados do teste.
As ferramentas que possuem essa funcionalidade são o Visual Studio, SilkPerformer e RPT;·
Perfil da Carga de Trabalho: Somente o LoadRunner, o SilkPerformer e o Visual Studio possuem essa funcionalidade.
Refere- se ao perfil de teste que será executado.
Os usuários podem iniciar o teste gradativamente ou de forma simultânea.
Efetivação do Conjunto de Características Para a efetivação do conjunto adotou- se uma classificação das características necessárias para a criação e execução de casos de teste de desempenho concretizados.
Essa classificação se baseou no processo de teste utilizado por a Microsoft, o qual descreve em seis etapas o processo de geração de casos de teste de desempenho para aplicações web.
Entretanto, outras abordagens foram estudadas,, porém devido a o fato das informações referentes às características corresponderem e se relacionarem com as informações descritas nas seis etapas do processo de teste da Microsoft, esta abordagem foi a que melhor se adaptou ao trabalho desta dissertação.
A seguir são descritas cada uma das etapas deste processo:·
Identificar os Principais Cenários:
Em esta etapa são definidas as informações referentes aos cenários mais críticos para o desempenho do sistema.
É onde são definidos os caminhos e atividades mais comuns de serem executados por os usuários.
Um exemplo de caminho crítico poderia ser:
Login na aplicação, navegar a procura de um determinado produto, adicionar itens no carrinho, informar detalhes da forma de pagamento, finalizar compra, Logout na aplicação;·
Identificar Carga de Trabalho: Em esta etapa são definidas as informações de carga como, por exemplo, número máximo de usuários esperado para acessar a aplicação.
Esta informação é normalmente definida durante a etapa de especificação de requisitos de desempenho.
Também são definidas nesta etapa informações referentes aos tipos de perfis de execução dos usuários.
Em aplicações e-commerce, por exemplo, pode- se configurar três tipos diferentes de perfis:
Browse mix, onde a maior parte do tempo os usuários interagem com a aplicação realizando tarefas de simples navegação;
Search mix, onde a maior parte do tempo os usuários interagem com a aplicação realizando tarefas de busca e algumas tarefas de compra e;
Order mix, onde a maior parte do tempo os usuários interagem com a aplicação realizando tarefas de compra;·
Identificar Métricas: Em esta etapa são definidas informações referentes aos contadores e métricas que serão configuradas para o teste, a fim de avaliar o desempenho da aplicação sob teste.
O objetivo é identificar os possíveis gargalos responsáveis por uma degradação de desempenho.
Tempo de resposta, percentual de utilização de CPU e memória são algumas das diversas métricas utilizadas para monitorar o teste;·
Criar Casos de Teste: Em esta etapa são definidas as características referentes à configuração de carga de trabalho realizada sob a aplicação.
Por exemplo, número de usuários para o teste, percentual de usuários que irão executar os diferentes caminhos definidos na Etapa 1 do processo, tempo de duração do teste, etc.;·
Simular Carga: Em esta etapa define- se qual ferramenta será utilizada para gerar e executar o teste;·
Analizar Resultados: Em esta etapa é realizada a análise das informações dos dados obtidos das métricas durante o teste.
O objetivo da adoção deste processo para a construção do conjunto, o qual tem como finalidade complementar as informações referentes às características, foi de classificar as informações adquiridas baseando- se numa abordagem estruturada, sedimentada e conhecida na comunidade científica Como é possível visualizar na Figura 4.1, a efetivação da construção do conjunto de características, baseando- se em alguns aspectos das etapas do processo descrito anteriormente, ocorreu da seguinte forma.
Inicialmente, a Etapa 1 (Definir script) classifica as informações referentes às características URLs da Aplicação e Tempo de Pensamento.
Em esta etapa define- se, na forma de web links, o conjunto de caminhos e atividades (scripts) a serem executados por os usuários durante o teste e o tempo decorrido entre uma interação e outra.
Esta etapa também possui todo o tipo de informação relativa à interação do usuário com a aplicação e banco de dados, como por exemplo, dados para preenchimento de formulário, dados referente a a informação necessária para realizar login na aplicação, dados de transações bancárias como número e data do cartão de crédito.
A Etapa 2 (Definir Perfis de Execução) possui informação referente a os perfis de execução dos usuários.
Esta etapa pode ser utilizada para representar, através de um identificador, cada um dos tipos de caminhos definidos na Etapa 1.
Por exemplo, para um tipo de caminho que possui mais informações de navegação do que qualquer outro tipo de operação, esta etapa pode ser utilizada para representar o nome correspondente a este tipo de caminho.
Em este caso pode ser identificado como browse mix.
A Etapa 3 (Definir Métricas e Contadores) é utilizada para representar as informações referentes à característica Contadores.
Em esta etapa são definidas as diversas métricas e contadores que serão utilizados para monitorar os recursos e comportamento do sistema durante a execução de um teste.
Resposta, Vazão e Utilização de Recursos (CPU, Memória, Disco E/ S, Rede E/ S).
A partir desses indicadores é possível derivar um conjunto de métricas e contadores.
Como é possível visualizar na Figura 4.1, o indicador Utilização de Recursos, por exemplo, possui um conjunto de cinco métricas, são elas Processors, Memory, Disk I/ O, Process e Network.
Para cada uma dessas métricas é definida uma lista de contadores, os quais servirão para monitorar o comportamento do sistema durante o teste.
A mesma classificação se aplica aos indicadores Tempo de Resposta e Throughput.
Uma vantagem na forma de organizar estes dados é que este tipo de estruturação permite uma fácil visualização das informações referentes às métricas e contadores.
Em a Etapa 4 (Definir Cenário) são definidas todas as informações necessárias para a configuração da carga de trabalho para o teste.
Em esta etapa são classificadas as informações referentes às características Quantidade de Usuários, Tempo de Duração do Teste, Tempo de Rampa de Subida, Quantidade de Usuários da Rampa de Subida, Tempo de Rampa de Descida, Quantidade de Usuários da Rampa de Descida, Tempo de Aquecimento e Perfil da Carga de Trabalho.
A característica Tempo de Pensamento, assim como na Etapa 1, é classificada nesta etapa, porém ela é utilizada somente para determinar o valor médio considerando todas as interações do usuário com a aplicação.
Além de essas informações, nesta etapa define- se o percentual de usuários que irão executar os diferentes scripts que serão utilizados no teste.
A Etapa 5 (Definir Ferramenta) contém informação referente a a escolha da ferramenta de automatização de teste que será utilizada para o teste de desempenho.
A Etapa 6, como descrito anteriormente, é uma etapa de análise dos resultados realizada após o término do teste.
Como o conjunto de características implementado descreve as informações necessárias para criação e execução de casos de teste concretizados, esta etapa acaba não sendo utilizada para a classificação de nenhuma característica e por este motivo, não foi incluída no conjunto.
Após a classificação de todas as características definidas, utilizando como referência o processo de teste de desempenho descrito por a Microsoft, o conjunto de características pôde ser implementado.
Além de servir como uma importante base para a classificação e estruturação das características, o processo de teste descrito por a Microsoft também contribuiu para a complementação das informações destas características.
Com a definição do conjunto de características de teste de desempenho é possível desenvolver ferramentas para automatizar a geração e a execução de scripts e cenários de teste por meio de a instanciação das informações contidas no conjunto apresentado.
Em outras palavras, estas ferramentas receberiam como entrada um conjunto de informações (características) referentes aos dados da aplicação a ser testada e, posteriormente, processariam tais informações para gerar casos de teste a serem executados por ferramentas como LoadRunner ou Visual Studio.
Em este contexto, uma alternativa interessante é a utilização de linhas de produto de software para derivar ferramentas que automatizam o processo de geração e execução de casos de teste.
Em a seção 4.3 será apresentada uma linha de produto capaz de derivar diversas ferramentas que automatizam a geração de casos de teste de desempenho por meio de a instanciação das informações contidas no conjunto de características definido.
Ferramenta para Linha de Produto de Software:
PLeTs Atualmente, muitas empresas estão buscando alternativas para reaproveitar as funcionalidades dos softwares por elas produzidos, com o intuito de diminuir tempo e custo no desenvolvimento das novas versões destes softwares.
Em este contexto, uma alternativa é a utilização dos conceitos de Linha de Produto de Software (Software Product Line -- SPL).
SPL possibilita, por meio de a reutilização de componentes de software, criar um conjunto de sistemas similares, reduzindo assim o tempo de comercialização, custo e com isso, obter maior produtividade e melhoria da qualidade.
Conceitualmente, uma SPL é definida como um conjunto de softwares que compartilham carac-terísticas comuns e gerenciáveis com o intuito de satisfazer as necessidades de um domínio específico, podendo este ser um segmento de mercado ou missão.
O objetivo é explorar as semelhanças entre os sistemas visando gerenciar os aspectos relativos à variabilidade entre eles e, dessa forma, determinar uma maior reusabilidade dos componentes de software.
Segundo Software Engineering Institute (Sei), a engenharia de SPL possui três conceitos principais:
O primeiro é denominado desenvolvimento do núcleo de artefatos (core assets development), também conhecido como engenharia de domínio (domain engineering).
O segundo conceito é chamado desenvolvimento do produto (product development), também conhecido como engenharia de aplicação (application engineering) na nomenclatura alternativa.
O terceiro conceito é A parte mais importante de uma SPL diz respeito ao núcleo de artefatos, o qual forma a base de uma SPL e pode ser formado por componentes reusáveis, modelos de domínios, requisitos da SPL, casos de teste e modelo de características (feature models), o qual representa os aspectos relacionados à variabilidade numa linha de produto.
O modelo de características apresenta todas as características de uma linha de produto e a relação entre os componentes.
Segundo, uma característica é uma funcionalidade importante/ relevante do sistema, a qual é visível ao usuário final.
Uma característica pode ser opcional (optional), obrigatória (common/ mandatory) ou alternativa (alternatives).
Uma característica opcional pode ou não estar presente no produto.
Entretanto, uma característica obrigatória, necessariamente fará parte do produto.
Quanto a a característica alternativa, ela se trata de uma característica excludente, ou seja, a seleção de uma característica alternativa determina que as demais características alternativas pertencentes ao mesmo grupo e nível hierárquico da característica selecionada não estarão presentes no produto.
A literatura também define que uma característica pode ser do tipo &quot;ou «(or), onde uma ou mais características pertencentes ao mesmo grupo e nível hierárquico podem estar presentes no produto.
As características também possuem relação entre si e algumas restrições são determinadas, tais como:
Relação de dependência (depends/ requires) e relação de exclusão (excludes).
Em a Figura 4.2, a qual apresenta um modelo de características de telefone móvel, possui quatro características no primeiro nível.
As características Chamadas e Tela são do tipo obrigatória, enquanto as características GPS e Mídia são opcionais.
A característica Tela possui três subcaracterísticas alternativas:
Básico, Colorido e Alta Resolução, implicando que estas três subcaracterísticas não podem estar presentes simultaneamente no mesmo produto.
A subcaracterística Básico possui uma relação de exclusão com a característica GPS, onde a seleção de uma característica exclui a outra, ou seja, se a tela de um produto for do tipo básica, necessariamente este produto não terá a característica GPS e vice-versa.
A característica Mídia possui duas características do tipo &quot;ou&quot;:
Câmera e MP3, onde a característica Câmera possui uma relação de dependência com a característica Alta Resolução.
Em este contexto, caso um produto possua uma mídia do tipo Câmera, necessariamente a tela deverá ser Alta Resolução.
Devido a a capacidade de modelar e representar variabilidades e, com isso, obter maior reuso das características, funcionalidades e componentes, a utilização de SPLs pode trazer inúmeras vantagens e benefícios a clientes e consumidores.
Segundo, depois da chegada das linguagens de programação de alto nível, as SPLs podem representar a mais &quot;empolgante «e significativa mudança no paradigma de desenvolvimento, devido a a facilidade e eficiência em desenvolver sistemas com a utilização de SPLs.
O autor ainda enfatiza que, em nenhuma outra área da engenharia de software, são evidenciadas melhorias como as que a SPL provê.
O fator mais preponderante relativo à afirmação do autor, diz respeito aos benefícios advindos com a utilização de SPL.
Em este sentido, citam- se a qualidade dos produtos, menor tempo para o lançamento no mercado de um novo produto da família e produtividade no desenvolvimento dos produtos.
Além de essas vantagens, muitas empresas, de entre elas citam- se Philips, Nokya, têm descoberto que, quando bem implementada, uma estratégia para utilização de linhas de produto pode trazer diversas outras melhorias, tais como:
Atualmente, está em processo de desenvolvimento no Centro de Pesquisa em Engenharia de Sistemas da PUCRS uma linha de produto denominada PLeTs (Product Line Testing Tools).
Esta linha de produto faz parte do trabalho de uma tese de doutorado e utiliza técnicas de SPL para automatizar processos de teste.
A PLeTs é uma SPL que busca facilitar a derivação de ferramentas de teste baseado em modelos (Model Based Testing -- MBT), com os quais é possível criar e executar casos de teste de forma automatizada.
O objetivo desta SPL não é apenas gerenciar a reutilização de artefatos e componentes de software, mas também tornar mais fácil e rápido o desenvolvimento de uma nova ferramenta e, como citado há pouco, otimizar a criação e execução de casos de teste.
Ela está sendo desenvolvida com o intuito de ser utilizada por engenheiros de software, programadores e engenheiros de teste, auxiliando no processo de planejamento e execução de casos de teste e scripts de teste.
Para melhor entendimento, é apresentado na Figura 4.3 o modelo de características atual da SPL PLeTs.
Atualmente, o modelo de características da PLeTs é constituída de quatro características em seu primeiro nível, Parser, Test Case Generation, Script Generation e Execution.·
Parser: Esta é uma característica ou ponto de variabilidade obrigatório e seu objetivo é extrair as informações contidas, por exemplo, num arquivo UML para então gerar um modelo formal na característica Test Case Generation.
Este modelo, atualmente pode ser representado por uma Máquina de Estados Finitos (Finite State Machine -- FSM) ou uma Rede de Petri (Petri Net -- PN).
Atualmente ela possui duas variantes mutuamente excludentes:
UML -- FSM e UML -- PN, onde apenas uma das duas variantes é selecionada;·
Test Case Generation:
Também é uma característica obrigatória.
É responsável por gerar sequências de teste a partir de as informações contidas no modelo formal (FSM ou PN).
Podem ser geradas sequências para teste funcional, desempenho ou segurança.
Este ponto de variabilidade possui três variantes (Functional Testing, Performance Testing e Security Testing) e obrigatoriamente, assim como a característica Parser, apenas uma de elas pode ser selecionada.
Como pode- se observar na Figura 4.3, as três variantes possuem uma relação de dependência (depends) com as variantes do parser.
Indicando a seleção de uma variante é dependente de outra.
Por exemplo, caso a variante Functional Testing seja escolhida, obrigatoriamente a variante UML -- PN deve ser selecionada;·
Script Generation: Consiste na geração de scripts de teste para ferramentas de automação de teste.
Também possui três variantes, sendo elas:
JMeter Script Generation, LR Script Generation e Vs Script Generation, as quais representam a implementação de scripts e cenários de teste, respectivamente, para as ferramentas LoadRunner, JMeter e Visual Studio;·
Execution: Representa a execução da ferramenta e também a execução do teste sob a aplicação a ser testada, utilizando uma determinada ferramenta com os scripts e cenário de teste gerados na etapa anterior.
Possui três variantes e cada uma possui uma relação de dependência com as variantes pertencentes ao ponto de variabilidade Script Generation.
É importante de salientar que, o modelo de características atual pode evoluir e ser incrementado.
Podem ser adicionados novos pontos de variabilidade ou até mesmo outras variantes aos pontos de variabilidade existentes.
É totalmente factível que uma nova variante representando a geração e execução de teste com outra ferramenta de automatização de teste seja incluída.
Também poderia ser adicionada uma nova variante ao Parser.
Por exemplo, as informações do teste que, atualmente, provêm de diagramas UML poderiam ser expressas utilizando outra estrutura, como arquivos de texto, logs, XML (Extensible Markup Language), entre outras.
Geração de Casos de Teste O conjunto de características para ferramentas de teste de desempenho, o qual é o foco desta dissertação, serviu como referência para a implementação de plugins para a PLeTs.
Estes plugins são responsáveis por a geração de scripts e cenários para os produtos derivados da PLeTs.
Em este contexto, serão detalhadas as funcionalidades de cada plugin implementado.
Entretanto, para realizar a geração destes scripts e cenários, foi necessária a extração de informações provenientes de outra pesquisa de mestrado.
A referida pesquisa possui como foco a análise de características de modelos formais ou semi-formais a serem utilizadas para descrição de requisitos de teste.
Ela se utiliza de modelos como Linguagem de Modelagem Unificada (Unified Modeling Language UML), Redes de Autômatos Estocásticos (Stochastic Automata Networks -- SAN), Redes de Petri (Petri Nets -- PN), Máquina de Estados Finitos (Finite State Machine -- FSM).
A dissertação de mestrado oriunda da pesquisa tem como objetivo a extração de características de teste que os modelos possuem para posterior geração de scripts e/ ou cenários de teste.
Em esse contexto, o objetivo foi verificar se as informações necessárias para a execução de casos de teste para as ferramentas coincidem com as informações presentes nos modelos.
Durante o início da pesquisa três hipóteses foram levantadas, a primeira hipótese e também a menos desejável era que todas as características presentes nos modelos fossem diferentes das características para a geração de casos de teste para as ferramentas (ver Figura 4.4-a).
Outra hipótese levantada era de que o conjunto das características presentes nos modelos fosse igual ao conjunto das características definidas para as ferramentas, entretanto, era pouco provável que esta hipótese fosse verdadeira (ver Figura 4.4-b).
A terceira hipótese e também a mais coerente era que houvesse uma intersecção entre as características presentes nos modelos e as características definidas para as ferramentas, ou seja, poderia ocorrer que algumas características presentes nos modelos não fossem necessárias para a geração de scripts e cenários para as ferramentas e que algumas características definidas para as ferramentas não estivessem presentes nos modelos (ver Figura 4.4-c).
A o final, foi evidenciado que a última hipótese foi de fato verdadeira, pois a maioria das características definidas para as ferramentas coincidem com as informações referentes às características presentes nos modelos.
Como é possível visualizar na Tabela 4.4, existe uma série de características comuns a ambos os conjuntos, ainda que possuam nomes ou identificadores distintos representam na verdade o mesmo tipo de informação.
Isto é o que está descrito na quinta coluna da Tabela 4.4, a qual apresenta a relação dos identificadores das características coincidentes entre os dois conjuntos.
Por exemplo, a característica SUT com I D1 é coincidente com a característica Host da Aplicação que também possui Id igual a 1.
As demais colunas (2 e 4) desta tabela apresentam respectivamente as características presentes nos modelos e as características definidas para as ferramentas.
As pesquisas que definiram o conjunto de características presentes nos modelos e o conjunto das características para as ferramentas foram utilizadas para a implementação de plugins para as quatro etapas do modelo de características da PLeTs.
O conjunto de características presentes nos modelos foi utilizado para a implementação de plugins para as etapas Parser e Test Case Generation, enquanto, o conjunto de características para as ferramentas foi utilizado como referência na implementação de plugins para as duas etapas subsequentes, Script Generation e Execution.
Até o presente momento a PLeTs é constituída de seis plugins:·
FSM Test Case Generation:
Extrai as informações do teste (apenas as informações referentes às ações dos usuários na aplicação), as quais foram armazenadas na estrutura intermediária por o plugin Parser UML, para gerar uma FSM.
Após gerar a FSM é aplicado o método Harmonized State Identification (HSI) para geração de sequências de teste e em seguida, as informações do teste são armazenadas em outra estrutura de dados, a qual chamamos de &quot;estrutura de sequência de teste «(a Seção 4.5 explica com maiores detalhes os componentes desta estrutura).
A estrutura de sequência de teste também é populada com as informações referentes ao cenário de teste, as quais foram armazenadas na estrutura intermediária, entretanto, estas informações são diretamente repassadas de uma estrutura a outra.
As informações contidas na estrutura de sequência de teste são utilizadas para a geração do cenário e scripts de teste para as ferramentas LoadRunner e Visual Studio;·
LoadRunner Script Generation:
Extrai as informações armazenadas na estrutura de sequência de teste para gerar o cenário e os scripts para a ferramenta LoadRunner.
Entretanto, antes da geração dos cenários e scripts de teste, uma interface é apresentada ao usuário que por meio de ela tem a opção de selecionar o cenário de teste a ser executado e os scripts vinculados a este cenário;·
Visual Studio Script Generation: Extrai as informações armazenadas na estrutura de sequência de teste para gerar o cenário e os scripts para a ferramenta de teste do Visual Studio.
Entretanto, antes da geração dos cenários e scripts de teste, uma interface é apresentada ao usuário que por meio de ela tem a opção de selecionar o cenário de teste a ser executado e os scripts vinculados a este cenário;·
LoadRunner Test Execution:
Executa o teste com a ferramenta LoadRunner utilizando o cenário e os scripts gerados por o plugin LoadRunner Script Generation;·
Visual Studio Test Execution: Executa o teste com a ferramenta de teste do Visual Studio utilizando o cenário e os scripts gerados por o plugin Visual Studio Script Generation.
Por o fato de sua arquitetura ser baseada em plugins, a PLeTs permite a seleção de diversos plugins e a integração dos plugins selecionados possibilita gerar até dois produtos para teste de desempenho.
Sendo que um produto é capaz de automatizar a geração e execução de scripts e cenários de teste com o LoadRunner e o outro com o Visual Studio.
Outra característica importante de salientar diz respeito à arquitetura dos novos produtos gerados, a qual é integralmente baseada nas funcionalidades dos plugins selecionados durante a execução da PLeTs.
Quando um novo produto (ferramenta de teste de desempenho) é gerado por a PLeTs por meio de a combinação dos plugins descritos recentemente, a geração e execução de casos de teste pode ser iniciada.
Entretanto, é necessária, num primeiro momento, a geração de um modelo de teste.
Este modelo é composto por diagramas de caso de uso, os quais possuem informações referentes ao cenário de teste como número de usuários, tempo de duração do teste, entre outras.
Cada diagrama de caso de uso é decomposto num diagrama de atividades, o qual possui informações referentes às ações dos usuários sob a aplicação.
As informações de teste descritas nos diagramas são representadas num arquivo no formato XMI e, durante a execução da ferramenta de teste, este arquivo é submetido a um parser com a finalidade de popular a estrutura intermediária.
Em seguida, são geradas sequências de teste quando aplicado o método HSI sob uma FSM gerada a partir de as informações inseridas na estrutura intermediária.
Estas sequências de teste, bem como as informações referentes ao cenário armazenadas na estrutura de sequência, se tratam de casos de teste não instanciados e por este motivo são chamados de &quot;casos de teste abstratos».
A próxima etapa diz respeito à instanciação dos casos de teste abstratos, a qual consiste na geração do cenário e scripts de teste para uma determinada tecnologia (LoadRunner ou Visual Studio).
Estes casos de teste quando instanciados são chamados de casos de teste concretizados.
A etapa final consiste na execução automática do teste utilizando determinada tecnologia.
Implementação do Conjunto de Características Como citado no Capítulo 4.3, a PLeTs é uma SPL que utiliza técnicas de Linhas de Produto para gerar ferramentas que automatizam processos de teste.
Utilizando os produtos por ela gerados é possível, por meio de a extração de informações de modelos como UML, automatizar o processo de geração e execução de casos de teste.
Para tornar possível este processo de automatização, foram implementados plugins para as etapas Script Generation e Execution da PLeTs, os quais utilizam as informações do conjunto de características apresentado anteriormente como referência.
Tendo como base as informações do conjunto, estes plugins implementam a geração automática de scripts e cenários para as ferramentas de automatização de teste de desempenho.
A seguir, é apresentado o processo de criação destes plugins para a PLeTs, utilizando duas ferramentas para execução automática de testes de desempenho, Hp LoadRunner e Microsoft Visual Studio.
A implementação dos plugins para teste de desempenho para as ferramentas LoadRunner e Visual Studio abrangeu duas etapas:
A primeira foi realizar uma busca por arquivos de configuração para as duas tecnologias que representassem as informações definidas no conjunto de características;
A segunda etapa consistiu na implementação de uma estrutura de dados, a qual foi utilizada para representar as informações definidas no conjunto de características para as ferramentas.
Esta estrutura de dados foi chamada de &quot;estrutura de sequência de teste «e as informações contidas nesta estrutura foram utilizadas para a geração do cenário e scripts de teste para as ferramentas LoadRunner e Visual Studio.
Para a ferramenta LoadRunner, inicialmente, foi realizada uma pesquisa por arquivos de configuração que pudessem conter o conjunto de informações definidas para a Etapa 1 do conjunto de características.
Foi identificado que as informações especificadas na primeira etapa do conjunto deveria estar presente em arquivos (scripts) que possuíssem todo o tipo de informação referente a a interação do usuário com a aplicação.
Como é possível visualizar na Figura 4.5, informações definidas na Etapa 1 do conjunto, como URLs da aplicação, Tempo de Pensamento e informações referentes aos dados de autenticação de usuário (username e password) foram necessárias incluir no script LoadRunner.
Validando portanto, a primeira etapa do conjunto de características implementado.
Entretanto, as informações contidas nas duas etapas subsequentes do conjunto de características foram fundamentais e imprescindíveis para o processo de geração automática de casos de teste concretizados para o LoadRunner.
Isto se deve ao fato de que as etapas 3 e 4 do conjunto foram utilizadas para configurar todo o tipo de informação referente a o arquivo que o LoadRunner usa para armazenar os dados do cenário de teste.
Dados de configuração, tais como:
Quantidade de Usuários, Quantidade de Usuários que irão executar as atividades de determinado script, Tempo de Rampa de Subida, Tempo de Rampa de Descida, Perfil da Carga de Trabalho e todas as informações de métricas e contadores de desempenho são definidas neste arquivo.
Em a Figura 4.6, são apresentadas algumas das informações utilizadas para a configuração do cenário de teste para o LoadRunner.
Em esta figura estão destacados sete tipos de informação:
Vusers, que corresponde ao número total de usuários configurados para executar o teste;
RunFor, refere- se ao tempo total de execução do teste em milissegundos;
TotalVusersNumber, que corresponde à quantidade de usuários que irá executar as atividades descritas em determinado script;
Count, que corresponde ao número de usuários que iniciarão o teste (Quantidade de Usuários da Rampa de Subida) ou irão deixar de realizar suas requisições (Quantidade de Usuários da Rampa de Descida) e Interval, que define o tempo que levará para um conjunto de usuários iniciar o teste (Tempo de Rampa de Subida) ou terminar- lo (Tempo de Rampa de Descida).
Para o produto derivado da PLeTs, aquele arquivo foi utilizado como template, pois em ele existem diversas informações, como por exemplo, a versão do LoadRunner, onde a modificação é desnecessária, pois estas informações não são parametrizáveis e, por este motivo, não variam de um teste para outro.
Portanto, somente as informações referentes à configuração do cenário de teste são consideradas parametrizáveis e portanto, é obrigatória a informação destes parâmetros.
As informações referentes à definição dos perfis de execução dos scripts de teste, especificados na Etapa 2 do conjunto de características, são configuradas para o Visual Studio no arquivo de desta tag possui o nome dado ao perfil de execução e referência o arquivo webtest (script de teste) que será utilizado para a configuração do cenário de teste.
As informações referentes às etapas 3 e 4 do conjunto, assim como na Etapa 2, são utilizadas para mapear as informações referentes à configuração do arquivo de cenário do Visual Studio (loadtest).
Assim como no LoadRunner, a Etapa 5 do conjunto de características é utilizada para definir as informações da ferramenta de teste a ser utilizada.
Em esta etapa são definidas, portanto, diversas informações referentes ao Visual Studio, como por exemplo, versão da ferramenta e funcionalidades.
O objetivo de criar esta estrutura de dados, a qual serviu como referência para a geração do cenário e scripts de teste, foi definir uma estrutura genérica por a qual as informações necessárias para a criação dos casos de teste concretizados (cenário e scripts) não fosse condicionada ao método ou formalismo onde tais informações foram, inicialmente, geradas.
Por exemplo, não é desejável que a geração dos scripts e cenários de teste seja totalmente dependente das informações do teste geradas a partir de diagramas UML, como é o caso da abordagem utilizada com as ferramentas ou produtos originados da PLeTs.
Visto que o objetivo do conjunto de características é automatizar a geração e execução de cenários e scripts independentemente de onde as informações necessárias para o teste foram geradas, seja a partir de diagramas UML, seja de logs com informações das interações do usuário com a aplicação ou qualquer outra abordagem.
As informações contidas no conjunto de características, foram utilizadas para a implementação de plugins para a PLeTs, neste caso foram implementados plugins para as ferramentas LoadRunner e Visual Studio.
Estes plugins implementam a geração automática dos scripts e cenários de teste para estas ferramentas de teste de desempenho.
Durante sua execução, a ferramenta de teste gerada por a PLeTs cria e configura estes arquivos e em seguida executa o software de automatização de teste selecionado com os arquivos de script e cenário criados.
Conjunto de Características para Uma Ferramenta de Teste Estrutural Conforme apresentado anteriormente, o foco da pesquisa desta dissertação se baseou num estudo onde diversas ferramentas foram analisadas a fim de determinar um conjunto de características necessário para automatizar a geração e a execução de scripts e cenários para ferramentas de teste de desempenho.
Com o objetivo de expandir o escopo da pesquisa foi realizado outro estudo com a finalidade de verificar se aplicando a mesma abordagem utilizada para teste de desempenho seria possível definir um conjunto de características para automatizar a geração e a execução de casos de teste para uma ferramenta de teste estrutural, a JaBUTi.
O objetivo deste estudo foi verificar quais arquivos de configuração a JaBUTi utiliza para armazenar seus dados de execução e análise de teste e com isso, determinar um conjunto de características (informações) necessário para a geração e execução de casos de teste estrutural.
Para essa pesquisa um conjunto de trabalhos foi analisado como, por exemplo, dissertações, artigos e relatórios técnicos.
Por meio de a análise desses trabalhos e também da própria ferramenta, considerações importantes puderam ser realizadas a respeito de seu funcionamento interno.
Para a execução de casos de teste com a JaBUTi, é necessário criar um arquivo de projeto, cuja extensão é».
Jbt». Todas as informações referentes ao bytecode das classes que serão testadas e o caminho dessas classes são armazenadas neste arquivo.
Em este arquivo também são descritos os caminhos referentes a todas as bibliotecas que pertencem à aplicação a ser testada, bem como o bytecode da classe &quot;TestDriver», esta que contém informações que serão utilizadas para testar as classes da aplicação sob teste.
Com base nas informações do bytecode das classes referenciadas neste arquivo, a JaBUTi constrói o grafo definição-uso (Def-Use Graph -- DUG) para cada uma dessas classes, baseando- se nos critérios de análise de fluxo de dados e análise de fluxo de controle.
Após a criação do arquivo de projeto, a JaBUTi realiza a instrumentação das classes a serem testadas.
Ela executa os casos de teste, descritos no arquivo TestDriver, por meio de a chamada do método probe.
DefaultProber. Probe que armazena as informações do programa que será testado.
Em seguida, outro método é chamado (probe.
DefaultProber. Dump) e todos os dados coletados na chamada do método anterior são armazenados num arquivo de rastro(».
Trc&quot;). Os dados gravados no arquivo de rastro correspondem aos caminhos percorridos por o programa durante a execução do teste.
A JaBUTi extrai as informações deste arquivo de rastro, atualiza os dados do teste e recalcula as informações de cobertura.
De posse desse conhecimento, foi possível criar um conjunto de características contendo as informações necessárias para a automatização de teste estrutural utilizando a ferramenta JaBUTi.
Como é possível visualizar na Figura 4.9, o conjunto de características necessário para a geração e execução automática de casos de teste para a JaBUTi é dividido em três etapas.
Em a primeira etapa são especificadas as informações referente a o conjunto de classes que se deseja testar;
O conjunto de métodos que serão chamados por essas classes;
E o conjunto de parâmetros que será utilizado por esses métodos.
As informações da Etapa 1 são utilizadas para a criação automática da classe TestDriver.
Em a Etapa 2 são descritas todas as informações necessárias para a criação do arquivo de projeto da JaBUTi, como por exemplo, o caminho referente a o bytecode das classes que serão testadas.
Em a Etapa 3 são definidas as informações necessárias para a execução dos casos de teste e consequentemente a geração do arquivo de rastro, utilizado para recalcular as informações de cobertura.
Com base nessas informações foi possível implementar um outro plugin para a PLeTs, com o qual é possível gerar um produto que possibilita a geração e execução automática de casos de teste estrutural utilizando a ferramenta JaBUTi.
Para tornar possível a implementação deste plugin, inicialmente, foram utilizadas as informações descritas na Etapa 1 do conjunto para a geração automática da classe TestDriver.
Após a geração deste arquivo é criado um processo do compilador Java (Javac) passando por parâmetro a classe TestDriver gerada.
Em seguida, as informações descritas na Etapa 2 do conjunto são utilizadas para a geração automática do arquivo de projeto da JaBUTi.
Em a Etapa 3 são utilizadas as informações para automatizar a execução de um caso de teste e gerar o arquivo de rastro.
A o final, um processo Java é criado para executar a JaBUTi contendo as informações de cobertura já atualizadas.
Em as Figuras 4.10 e 4.11 é possível visualizar respectivamente algumas informações do arquivo de projeto da JaBUTi gerado por o produto derivado da PLeTs e a interface da JaBUTi com as informações de cobertura atualizadas.
Considerações Este capítulo apresentou as etapas para a implementação de um conjunto de características para ferramentas de teste de desempenho.
Com base nas informações descritas no conjunto implementado, foram criados quatro plugins para a PLeTs.
Também foi implementado um plugin para automatizar a atividade de teste utilizando a ferramenta JaBUTi.
Em o próximo capítulo será apresentado um exemplo de uso utilizando uma aplicação web desenvolvida no Centro de Pesquisa em Engenharia de Sistemas da PUCRS, onde as ferramentas derivadas da PLeTs serão utilizadas para automatizar a geração e execução de scripts e cenários de teste para essa aplicação.
Em este capítulo será apresentada a definição de um conjunto de casos de teste para serem gerados e executados a partir de os produtos gerados por a ferramenta PLeTs.
Para isto, será utilizada como exemplo de uso uma aplicação denominada Skills, a qual tem por objetivo a gerência de perfis profissionais de funcionários de uma empresa.
O objetivo é verificar e validar os aspectos funcionais do conjunto de características apresentado no capítulo anterior, bem como, os plugins implementados para a PLeTs.
A o final, pretende-se realizar uma análise, apresentando as vantagens e desvantagens em automatizar casos de teste utilizando como referência o conjunto de características implementado.
Ferramenta a Ser Testada:
Skills O projeto Skills (Workforce Planning:
Skill Management Prototype Tool) consiste de uma aplicação que tem por objetivo gerenciar os perfis profissionais de funcionários de uma dada empresa.
Esta aplicação foi desenvolvida por um grupo de pesquisa da PUCRS em colaboração com uma empresa de tecnologia e tem como principal funcionalidade o gerenciamento do cadastro de habilidades, certificações e experiências de funcionários.
Este software utiliza o Sistema Gerenciador de Banco de Dados (SGBD) MySQL para a persistência de dados e o TomCat como servidor de aplicação.
Para melhor compreensão, é apresentada na Figura 5.1 um exemplo referente a o cadastro de uma habilidade utilizando a interface da aplicação.
Em o exemplo descrito na Figura 5.1, o usuário utiliza o campo &quot;filter», localizado no canto superior esquerdo da interface da aplicação, para procurar a habilidade que deseja cadastrar, neste caso optou- se por o cadastro da habilidade XML.
Após encontrar- la, as informações referentes ao nível de proficiência e as datas de aquisição e última oportunidade de uso de tal habilidade são informadas no formulário localizado no canto direito da mesma interface.
Em seguida, a habilidade é cadastrada e as informações são persistidas na base de dados do usuário.
Em este exemplo, o cadastro da habilidade XML foi realizado através da utilização do campo &quot;filter», porém qualquer habilidade ou certificação pode ser cadastrada ou editada navegando- se nos itens da árvore até encontrar a habilidade/ certificação desejada.
O usuário ainda pode visualizar localizado no canto superior esquerdo da interface da aplicação.
O usuário também é capaz de trocar a sua senha de &quot;login «e obter informações de ajuda, quando possuir alguma dificuldade durante a interação com as funcionalidades da aplicação.
Definição dos Casos de Teste Concretizados Com o intuito de verificar os aspectos funcionais do conjunto de características e dos plugins implementados para a PLeTs, foram definidos alguns casos de teste para serem executados por as duas ferramentas por ela geradas.
Para isso, foram criados casos de teste utilizando a aplicação Skills como exemplo de uso.
Em este contexto, foram definidos casos de teste de desempenho idênticos para serem executados por as ferramentas LoadRunner e Visual Studio.
O objetivo é mostrar o funcionamento dos plugins desenvolvidos para a PLeTs e o que muda de uma ferramenta para outra, durante a criação e configuração automática dos scripts e cenários de teste.
Os casos de teste definidos para os produtos descrevem o processo de cadastro e edição das informações de habilidades, certificações e experiências de um determinado funcionário de uma empresa.
As informações referentes aos casos de teste são representados por diagramas UML (caso de uso e atividades).
Em a Figura 5.2 é apresentado um diagrama de casos de uso, o qual descreve as informações referentes a dois cenários de teste.
Este diagrama apresenta dois atores e cada um define informações de um cenário de teste específico.
O cenário representado por o ator Gerente Rh define, no comentário a ele conectado, um conjunto de informações necessárias para o teste, tais como:·
Quantidade de usuários (TDpopulation) $= 50· Host da aplicação (TDhost) $= localhost· Tempo de duração do teste (TDtime) $= 02:00:00· Tempo de rampa de subida (TDrampUpTime) $= 00:01:00· Quantidade de usuários da rampa de subida (TDrampUpUser) $= 10· Tempo de rampa de descida (TDrampDownTime) $= 00:01:00· Quantidade de usuários da rampa de descida (TDrampDownUser) $= 10 De a mesma forma que o ator Gerente Rh descreve um conjunto de informações configuradas para o cenário de teste, o mesmo se aplica ao ator Empregado o qual define as seguintes informações para o teste:&amp;&amp;&amp;
Além de definirem uma série de informações necessárias para o teste por meio de uma janela de comentários, os atores do diagrama da Figura 5.2 estão vinculados a um conjunto de casos de uso.
Em este exemplo, o ator Gerente Rh está conectado aos quatro casos de uso presentes no diagrama:
Gerenciar Habilidades, Gerenciar Experiências, Gerenciar Certificações e Alterar senha.
Por outro lado, o ator Empregado está conectado somente a três casos de uso.
Por questões relativas a permissões de acesso à aplicação, ele não possui acesso à funcionalidade trocar senha e, por este motivo, não está conectado ao caso de uso que descreve tal funcionalidade.
Os casos de uso possuem informações referentes ao perfil de tarefa a ser realizada por os usuários sob a aplicação a ser testada.
Por exemplo, o caso de uso Gerenciar Habilidades determina que o conjunto de usuários definidos para ele irá interagir com a aplicação apenas no que se referir ao cadastro ou edição de habilidades.
O mesmo é válido para os demais casos de uso.
Cada caso de uso também possui uma probabilidade vinculada a um ator, este atributo é utilizado para determinar a quantidade de usuários que irá realizar, sob a aplicação, o perfil de tarefa descrito por um caso de uso específico.
Entre o ator Gerente Rh e o caso de uso Gerenciar Habilidades, por exemplo, foi atribuído um valor de probabilidade igual a 40%.
Informando, portanto, que 20 dos 50 usuários definidos para o cenário representado por o ator Gerente Rh irá interagir com a aplicação e realizar somente tarefas de gerência de habilidades.
Também foram atribuídos valores de probabilidade para os demais casos de uso e seus respectivos atores.
Cada caso de uso é decomposto num diagrama de atividades e o conjunto dos quatro diagramas de atividades deste exemplo, os quais podem ser visualizados nas Figuras 5.3, 5.4, 5.5 e 5.6, determina o possível comportamento dos usuários sob a aplicação.
Em outras palavras, os diagramas de atividades descrevem os aspectos dinâmicos do sistema sob teste.
A seguir, são descritas as etapas que representam o fluxo de atividades da aplicação:·
Logar: Em esta etapa o usuário insere seus dados de autenticação (usuário e senha) para logarse na aplicação.
Em seguida é redirecionado para a página principal da aplicação, onde são mostradas as habilidades, certificações e experiências cadastradas;·
Cadastrar habilidade -- modo 1:
Esta etapa inicia com o usuário clicando no link Skills.
Com isso, uma lista de tipos de habilidades é apresentada ao usuário.
Este navega entre os itens da lista até encontrar a habilidade que deseja cadastrar (Árvore de Habilidades).
A o encontrar a habilidade procurada o usuário irá selecionar- la e com isso, um formulário é apresentado.
O usuário insere dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal habilidade, finalizando assim o cadastro;·
Cadastrar habilidade -- modo 2:
Esta etapa inicia com o usuário clicando no link Skills.
Com isso, uma lista de tipos de habilidades é apresentada ao usuário.
Este preenche o campo &quot;filter «com o nome da habilidade a ser cadastrada (Pesquisar Habilidade).
Em seguida, a habilidade pesquisada é mostrada ao usuário que por sua vez a seleciona e com isso, um formulário é apresentado.
O usuário insere dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal habilidade, finalizando assim o cadastro;·
Editar habilidade -- modo 1:
Esta etapa inicia com o usuário clicando no link Skills.
Com isso, uma lista de tipos de habilidades é apresentada ao usuário.
Este navega entre os itens da lista até encontrar a habilidade que deseja editar.
A o encontrar a habilidade procurada o usuário irá selecionar- la e com isso, um formulário é apresentado.
O usuário edita os dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal habilidade, finalizando assim edição das informações;·
Editar habilidade -- modo 2:
Esta etapa inicia com o usuário clicando no link Skills.
Com isso, uma lista de tipos de habilidades é apresentada ao usuário.
Este preenche o campo &quot;filter «com o nome da habilidade a ser editada.
Em seguida, a habilidade pesquisada é mostrada ao usuário que por sua vez a seleciona e com isso, um formulário é apresentado.
O usuário edita os dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal habilidade, finalizando assim edição das informações;·
Cadastrar certificação -- modo 1:
Esta etapa inicia com o usuário clicando no link Certifications.
Com isso, uma lista com diversas certificações é apresentada ao usuário.
Este navega entre os itens da lista até encontrar a certificação que deseja cadastrar (Árvore de Certificações).
A o encontrar a certificação procurada o usuário irá selecionar- la e com isso, um formulário é apresentado.
O usuário insere dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal certificação, finalizando assim o cadastro;·
Cadastrar certificação -- modo 2:
Esta etapa inicia com o usuário clicando no link Certifications.
Com isso, uma lista com diversas certificações é apresentada ao usuário.
Este preenche o campo &quot;filter «com o nome da certificação a ser cadastrada (Pesquisar Certificação).
Em seguida, a certificação pesquisada é mostrada ao usuário que por sua vez a seleciona e com isso, um formulário é apresentado.
O usuário insere dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal certificação, finalizando assim o cadastro;·
Editar certificação -- modo 1:
Esta etapa inicia com o usuário clicando no link Certifications.
Com isso, uma lista de diversas certificações é apresentada ao usuário.
Este navega entre os itens da lista até encontrar a certificação que deseja editar.
A o encontrar a certificação procurada o usuário irá selecionar- la e com isso, um formulário é apresentado.
O usuário edita os dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal certificação, finalizando assim edição das informações;·
Editar certificação -- modo 2:
Esta etapa inicia com o usuário clicando no link Certifications.
Com isso, uma lista de diversas certificações é apresentada ao usuário.
Este preenche o campo &quot;filter «com o nome da certificação a ser editada.
Em seguida, a certificação pesquisada é mostrada ao usuário que por sua vez a seleciona e com isso, um formulário é apresentado.
O usuário edita os dados referentes ao nível e data de quando adquiriu e última vez que fez uso de tal certificação, finalizando assim edição das informações;·
Cadastrar experiência:
Esta etapa é muito semelhante ao cadastro de uma habilidade ou certificação.
O usuário clica no link Experience e uma lista de experiências já cadastradas é apresentada.
O usuário tem a opção de editar ou cadastrar uma nova experiência.
Em este caso, o usuário opta por o cadastro de uma nova experiência e insere, no formulário apresentado, dados referentes à companhia, área de atuação, função, nível e período em que adquiriu a experiência;·
Editar experiência:
Esta etapa é muito semelhante edição de uma habilidade ou certificação.
O usuário clica no link Experience e uma lista de experiências já cadastradas é apresentada.
O usuário tem a opção de editar ou cadastrar uma nova experiência.
Em este caso, o usuário opta por o edição de uma experiência já cadastrada e atualiza, no formulário apresentado, dados referentes à companhia, área de atuação, função, nível e período em que adquiriu a experiência;·
Alterar senha:
O usuário clica no link Change your password com isso, um formulário para preenchimento de informações relativas à senha atual e nova é apresentado ao usuário.
A o final a senha é modificada e o usuário é redirecionado ao menu principal da aplicação;·
Deslogar: O usuário finaliza seu acesso à aplicação e é redirecionado à página inicial da aplicação.
Assim como os diagramas de casos de uso são fundamentais para a definição dos casos de teste, o mesmo é válido para os diagramas de atividades.
Entretanto, diferentemente dos diagramas de caso de uso, os quais definem informações referentes aos cenários de teste, os diagramas de atividades apresentam informações para posterior geração dos scripts de teste.
Como é possível visualizar na Figura 5.3, a primeira transição do diagrama possui informações para a realização de login na aplicação, tais como:
Tempo de pensamento, página da aplicação (login.
Pl) e parâmetros para inserção de informações relativas a nome de usuário (admin) e senha (admin).
Para as demais transições dos quatro diagramas também foram definidas tais informações, porém com o objetivo de não poluir os diagramas, apenas a primeira transição do diagrama de atividades referente a a gerência de habilidades foi ilustrada e utilizada como exemplo.
Com a definição dos casos de teste por meio de a modelagem de diagramas de caso de uso e atividades, a geração e execução dos testes são então iniciadas.
Em um primeiro momento, optouse por o produto que utiliza o LoadRunner para automatizar a geração dos scripts e cenários de teste e posteriormente o produto que utiliza o Visual Studio.
Durante a execução da ferramenta, as informações referentes aos casos de teste definidos nos diagramas de caso de uso e atividades são exportadas para um arquivo no formato &quot;XMI».
Em seguida, este arquivo é submetido a um parser, o qual armazena as informações deste arquivo na estrutura intermediária.
A próxima etapa consiste na geração dos casos de teste abstratos, onde uma FSM é gerada com as informações referentes às interações do usuário com a aplicação.
Posteriormente, o método HSI para geração de sequências de teste é aplicado sob a FSM.
A próxima etapa consiste em armazenar as informações das sequências de teste geradas e as informações dos cenários de teste presentes na estrutura intermediária na estrutura de sequência de teste.
Em seguida, uma interface é apresentada ao usuário, o qual tem opção de selecionar o cenário e as sequências de teste que serão utilizadas para gerar os scripts de teste.
Em este exemplo, foram selecionados o cenário representado por o ator Gerente Rh e quatro scripts representados por o diagrama de atividades para gerência de habilidades.
Após a definição do cenário e dos scripts cada produto derivado da PLeTs cria uma instância para uma ferramenta específica.
Sendo que um produto instancia a interface do LoadRunner e o outro instancia a interface do Visual Studio, onde ambos executam automaticamente os casos de teste definidos anteriormente.
Para exemplificar, as Figuras 5.7 e 5.8 apresentam, respectivamente, as interfaces do LoadRunner e Visual Studio instanciadas por os produtos derivados da PLeTs.
Ambas as figuras apresentam um cenário de teste programado para executar por um período de duas horas, onde os scripts Cadastrar habilidade modo 1, Cadastrar habilidade -- modo 2, Editar habilidade -- modo 1, Editar habilidade -- modo 2 são utilizados para a interação dos usuários com a aplicação durante o teste.
Com a definição destes casos de teste para os produtos derivados da PLeTs, foi possível verificar os aspectos funcionais do conjunto de características e os plugins desenvolvidos.
E, portanto, demonstrar que é possível automatizar casos de teste baseando- se nas informações contidas no conjunto de características implementado.
Discussão Tendo apresentado as etapas do processo de criação do conjunto de características para ferramentas de teste de desempenho e a implementação dos plugins com base nas informações contidas neste conjunto, o objetivo para esta seção é apresentar uma discussão referente a as vantagens e desvantagens em automatizar casos de teste utilizando como referência o conjunto de características implementado.
Com base nas experiências obtidas com a utilização das ferramentas LoadRunner, Visual Studio foi possível estimar aproximadamente o tempo gasto na criação e execução manuais de casos de teste para cada uma dessas ferramentas.
Também é correto afirmar que, por o fato das ferramentas para teste de desempenho da Hp e Microsoft possuírem interfaces e características relativamente parecidas, não possuem diferença significativa no tempo gasto para a criação e execução de seus casos de teste.
O tempo gasto na implementação dos plugins para as ferramentas LoadRunner e Visual Studio também foi aproximadamente o mesmo.
Isso ocorreu, principalmente, devido a a semelhança entre elas no que diz respeito à complexidade na forma de estruturar os arquivos e dados necessários para a configuração automática de teste.
Apesar de o tempo gasto na criação dos plugins ser consideravelmente superior quando comparado ao tempo gasto na geração e execução de casos de teste utilizando a abordagem manual, o tempo gasto utilizando a abordagem automática é, por outro lado, muito inferior à abordagem manual.
Com base nestas informações as seguintes considerações podem ser feitas:
Quando deseja- se automatizar o processo de geração e execução de casos de teste concretizados, inicialmente, o custo de tempo envolvido na implementação de plugins para uma determinada ferramenta será consideravelmente superior se comparado ao tempo gasto na execução utilizando somente a abordagem manual.
Entretanto, por o fato da abordagem automática ser consideravelmente mais rápida que a abordagem manual, o tempo gasto na implementação dos plugins pode se tornar irrelevante à medida que uma quantidade maior de casos de teste irão sendo definidos para serem executados.
Desta forma, ao longo de o tempo a abordagem automática pode se tornar uma alternativa mais atrativa que a geração e execução de casos de teste utilizando a abordagem manual.
Outra vantagem na utilização da abordagem automática é a possibilidade de criar e executar casos de teste sem a necessidade de um conhecimento técnico aprofundado da ferramenta de automação de teste.
Outra vantagem em utilizar a abordagem automática, diz respeito à documentação das informações dos casos de teste.
Visto que as informações referentes ao teste são definidas nos diagramas UML da aplicação, é mais fácil guardar documentos que descrevem o modelo de uma aplicação do que os arquivos de teste de determinada tecnologia.
Por essas razões, a geração e execução automática de casos de teste concretizados torna- se uma alternativa interessante.
Considerações Este capítulo apresentou a definição de um conjunto de casos de teste para serem executados por os produtos derivados da PLeTs.
Estes produtos foram gerados com base nos plugins implementados, os quais se baseiam nas informações contidas no conjunto de características para ferramentas de teste de desempenho descrito no Capítulo 4.
A o final foi apresentada uma análise referente a as vantagens e desvantagens de gerar e executar casos de teste utilizando a abordagem automática (geração e execução de scripts e cenários de teste com os produtos derivados da PLeTs) e a abordagem manual (método convencional de teste por meio de a geração e configuração manual de scripts e cenários de teste).
Resumo Este trabalho apresentou a implementação de um conjunto de características para geração e execução de casos de teste concretizados para ferramentas de teste de desempenho em aplicações web.
Para isso, destacam- se duas etapas como sendo as principais para efetivação do trabalho.
Inicialmente, foi realizado uma análise de diversos trabalhos que descrevem o processo de criação e execução de scripts e cenários de teste utilizando ferramentas para automatização de teste de desempenho.
Com base nas informações adquiridas dos trabalhos estudados, pôde- se fazer um levantamento das características necessárias para a criação de casos de teste concretizados.
Para a efetivação do conjunto, foi adotada uma classificação para essas características, com base no processo de geração de casos de teste de desempenho para aplicações web utilizado por a Microsoft.
A utilização deste processo serviu para complementar as informações referentes às características e, com isso, definir o conjunto de características apresentado nesta dissertação.
A partir de as informações do conjunto apresentado, foi realizada a implementação de plugins para as etapas Script Generation e Execution da PLeTs.
Estes plugins implementam a geração e execução automática de scripts e cenários de teste utilizando duas ferramentas de automatização de teste de desempenho, sendo elas, Hp LoadRunner e Microsoft Visual Studio.
O objetivo foi utilizar as informações contidas no conjunto para automatizar a geração e execução de casos de teste concretizados.
Também foi realizada a implementação de um plugin para a geração e execução de casos de teste estrutural utilizando a ferramenta JaBUTi e, foi definido um conjunto com informações específicas para a ferramenta de teste estrutural.
Para complementação do trabalho foram criados casos de teste para as ferramentas de teste de desempenho LoadRunner e Visual Studio, onde a aplicação Skills foi utilizada como exemplo de uso.
O objetivo foi verificar as diferenças de configuração existentes durante a criação e execução de casos de teste concretizados para as duas ferramentas.
A o final, foi apresentada uma discussão referente a as vantagens e desvantagens em automatizar a geração e execução de casos de teste concretizados.
Foram mostrados os custos e os ganhos obtidos com esta automatização quando comparada com a abordagem de geração e execução de casos de teste manuais.
Contribuição e Trabalhos Futuros A pesquisa que originou o trabalho apresentado nesta dissertação teve início no estudo dos conceitos relacionados ao teste de software e na análise de diversas ferramentas de automatização de teste.
Após uma visão geral, a pesquisa focou no teste de desempenho e, neste contexto, fez- se necessária a análise de um conjunto de ferramentas que automatizam este tipo de teste.
A etapa seguinte da pesquisa se baseou na implementação de um conjunto de características para ferramentas de teste de desempenho.
O objetivo principal foi implementar um conjunto que auxiliasse a geração e execução automática de scripts e cenários de teste de desempenho para diversas ferramentas, pois ainda que as ferramentas de teste existentes automatizem a execução de testes, a geração e criação destes scripts e cenários continuam sendo realizadas de forma manual e por este motivo propensa a falhas.
Desta forma, sempre que alguém desejar implementar uma ferramenta para automatizar a geração destes scripts e cenários, poderá fazer- lo por meio de a análise das informações contidas no conjunto de características para ferramentas de teste de desempenho definido na pesquisa desta dissertação.
Como descrito anteriormente, o conjunto de características foi utilizado para a implementação de plugins para a PLeTs, onde as informações referentes ao teste são configuradas em modelos UML, tais como, diagramas de casos de uso e atividades.
Entretanto, este foi apenas um exemplo onde as informações do conjunto implementado foram aplicadas.
Poderia, por exemplo, implementar uma ferramenta onde as informações estivessem, inicialmente, descritas em logs de uma aplicação.
Desta forma, as informações do teste poderiam ser diretamente extraídas.
Apesar de as vantagens adquiridas com a utilização das informações contidas no conjunto de características, existem algumas questões em aberto e pontos que poderiam ser aprimorados.
Atualmente, ainda que o conjunto disponibilize um conjunto de informações necessárias para automatizar a geração de scripts e cenários, é necessário um conhecimento aprofundado relacionado à ferramenta de automatização de teste que se deseja utilizar.
É necessário conhecer os arquivos de configuração de teste da ferramenta e como as informações para o teste estão estruturadas nestes arquivos.
Outra questão em aberto diz respeito à falta de informações de análise de resultados do teste no conjunto de características.
Atualmente, o conjunto de características apresenta informações necessárias para a geração e configuração do teste, mas não dispõe de informações que podem contribuir na automatização da análise dos resultados.
Por exemplo, o conjunto poderia descrever quais as informações do teste são mais relevantes de serem consideradas para análise e qual relação entre esta ou aquela informação pode contribuir numa análise mais criteriosa do teste.
Os plugins desenvolvidos para a PLeTs automatizam a geração e execução de testes para ferramentas, no entanto, estes plugins não implementam nenhuma função de análise.
Em este contexto, seria interessante a PLeTs disponibilizar plugins que implementassem funções que agissem como oráculos, apresentando informações do teste relativas ao cumprimento ou não dos requisitos de desempenho especificados.
