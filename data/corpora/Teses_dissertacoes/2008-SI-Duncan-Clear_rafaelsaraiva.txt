Diversos processos de negócio das organizações podem ser automatizados com o auxílio de sistemas de Workflow.
Alguns de eles, estrategicamente importantes, necessitam de ferramentas que permitam análises gerenciais e auxiliem os gestores no processo de tomada de decisão.
Em este contexto, a aplicação das técnicas de descoberta de conhecimento sobre os registros de execução das instâncias dos processos de negócio mostra- se uma prática promissora.
No entanto, o ambiente computacional utilizado por as aplicações de KDD pode ser significativamente complexo, tendo suas etapas executadas de forma independente como, por exemplo, num ambiente orientado a serviços.
Esta arquitetura possui um problema relacionado à troca de informações entre as etapas do processo, visto que cada serviço pode ter sido escrito em linguagens diferentes e necessitar que os dados estejam dispostos num determinado formato.
Em este caso, uma vez que este formato seja único, distintas aplicações podem trabalhar utilizando mesmos dados, agregando ao procedimento com diversidade de opções.
Seguindo esta problemática, este trabalho versa sobre uma abordagem que visa tornar independentes duas etapas do processo de descoberta de conhecimento:
A mineração de dados e a visualização dos resultados.
Para isto, a solução proposta está baseada no uso das tecnologias de XML e XML Schema para a definição de estruturas para as saídas e entradas dos algoritmos de mineração e técnicas de visualização.
Além de isto, o uso de técnicas de XSLT contribui para que a transformação entre estes formatos possa ser realizada de modo automatizado.
Para a validação da solução, criada com base teórica, foram realizadas alguns testes utilizando as implementações de código livre.
A principal contribuição deste trabalho está na criação de formatos únicos e genéricos para a troca de informações entre as etapas citadas, bem como sua transformação.
Palavras-chave: Descoberta de conhecimento, técnicas de mineração de dados, visualização de dados, XML, XML Schema.
Diversos processos presentes no dia-a-dia das grandes empresas podem ser automatizados com o auxílio de ferramentas computacionais.
Tais processos vão desde a solicitação, aprovação e liberação de férias, viagens e compras de materiais até a definição das tarefas que uma determinada pessoa deve desenvolver dentro de um projeto.
Estes fluxos de atividades são denominados processos de negócio e podem ser definidos como &quot;um conjunto de um ou mais procedimentos ou atividades relacionados, os quais coletivamente atingem um objetivo de negócio, dentro de o contexto de uma estrutura organizacional que define papéis funcionais e suas relações».
A necessidade de automação e controle dos processos de negócio nas organizações deu origem a sistemas conhecidos como Workflow Management Systems (WfMS).
Estes sistemas são responsáveis por o armazenamento dos dados dos processos de negócio, tanto os relacionados à definição dos modelos (automação), quanto a a execução de suas atividades (gerência).
A tecnologia de workflow pode ser definida como a automação de processos de negócios, no todo ou em partes, em a qual documentos, informações ou tarefas são passadas de um participante para outro, de acordo com um conjunto de regras procedimentais,.
Com o auxílio da tecnologia da informação diversas soluções e ferramentas foram desenvolvidas para o gerenciamento dos processos de negócio.
Algumas de elas são comercializadas por fabricantes importantes no mercado de sistemas como a Oracle que possui um WfMS denominado Oracle Workflow.
Outras ferramentas, no entanto, estão baseadas em software livre, como o Shark.
O uso sistemático desses sistemas acaba gerando grandes quantidades de dados que representam o estado e o comportamento real dos processos das organizações.
Desta forma, o foco das empresas está se desviando do mero acompanhamento e controle desses processos para a medição, análise e monitoração dos mesmos.
Isto é realizado com o emprego de técnicas de descoberta de conhecimento (KDD) para análises baseadas em resultados obtidos por sumarizações de dados históricos e mineração de dados dos processos.
Assim, é possível identificar anomalias de execução dos processos, perceber as constantes modificações (tanto nos modelos quanto nas formas de execução de suas instâncias), verificar possíveis problemas de modelagem e sumarizar o comportamento dos processos de negócio, de acordo com os objetivos de cada organização,.
O autor deste trabalho apresenta em, e uma ferramenta que tem por objetivo disponibilizar um ambiente computacional completo para a execução do processo de descoberta de conhecimento sobre bases de dados de processos de negócio.
Esta ferramenta foi, inicialmente, implementada utilizando o modelo analítico multidimensional proposto por Casati em e, posteriormente, alterada para utilizar o modelo analítico proposto por Garcia em e.
Além de isto, ainda em, foi proposta, de forma superficial, uma maneira de se relacionar algoritmos de mineração de dados com técnicas de visualização.
A proposta consistia num mapeamento explícito entre classes de algoritmos e tipos de visualizações, realizado por um usuário com perfil de administrador do sistema.
O objetivo deste mapeamento era tornar as etapas de mineração de dados e visualização dos resultados do processo de descoberta de conhecimento independentes.
A necessidade de padronizar mecanismos de mineração para serem utilizados na forma de componentes por outras ferramentas de KDD é bastante atual na comunidade cientifica.
Esforços nesta direção podem ser categorizados nos seguintes aspectos:
Modelos, atributos, interfaces, Apis, configuração, processo e acesso remoto e distribuído.
O presente trabalho propõe uma abordagem para o mapeamento entre resultados de algoritmos de mineração de dados e formas de visualização destas informações.
A partir de estudos sobre as técnicas de mineração é possível identificar um conjunto principal de dados que são essenciais para a representação do resultado da execução de um algoritmo.
De a mesma forma, uma análise sobre métodos de visualização proporciona a identificação do formato necessário para os seus dados de entrada.
Assim, os resultados de execuções de algoritmos de mineração podem ser analisados com o auxílio de diferentes técnicas de visualização, mesmo que historicamente incompatíveis.
Para a validação desta proposta, são utilizados algoritmos e métodos de visualização fornecidos por o software livre Weka.
Esta escolha deve- se ao fato da ferramenta ser uma das mais utilizadas atualmente em contextos acadêmicos.
As contribuições desta pesquisa são:
Identificação de padrões nos resultados de diferentes classes de algoritmos de mineração de dados;
Identificação de padrões necessários para que seja possível a geração de visualizações de informações;
Criação de um processo de transformação automático entre os formatos gerados.
Este documento encontra- se organizado em oito capítulos.
O capítulo 2 discorre sobre alguns conceitos, definições e informações relacionadas à mineração de dados, além de apresentar um estudo sobre duas classes de algoritmos.
O capítulo 3 trata sobre as técnicas de visualização de dados, apresentando uma análise sobre alguns métodos.
Um estudo sobre a linguagem de marcação XML é apresentado no capítulo 4 e será utilizado como base para a criação dos formatos únicos para a troca de informações.
O capítulo 5 trata de trabalhos relacionados à proposta ou utilizados como base.
O capítulo 6, por sua vez, discorre sobre o problema que este trabalho visa resolver, fazendo sua contextualização e análise.
O capítulo 7 apresenta os estudos de caso, os arquivos de padronização resultantes da pesquisa e descreve sobre a forma como foram desenvolvidos.
Por fim, o capítulo 8 apresenta uma visão geral de todo o trabalho, expondo suas conclusões e sugestões para trabalhos futuros.
Este capítulo apresenta uma visão geral sobre o processo de descoberta de conhecimento, e, após, relata um estudo sobre as duas das principais técnicas de mineração de dados:
Associação e Classificação.
A descoberta de conhecimento em base de dados (também identificada como KDD) é um processo não trivial de identificação de padrões que sejam válidos, novos, potencialmente úteis e compreensíveis.
Este processo é composto por várias etapas de manipulação de dados, definidas por Han como: (
a) limpeza, (b) integração, (c) seleção, (d) transformação, (e) mineração de dados, (f) validação dos resultados e (g) representação do conhecimento.
A Figura 1 ilustra estas etapas e a hierarquia entre elas.
Em ela pode ser visualizado o ponto de partida para o processo de KDD, ou seja, uma base de dados contendo informações de execução de diversas aplicações salvas em formatos distintos (databases e flat para uma base multidimensional (data warehouse) onde ainda passam por as etapas (c) e (d).
A execução da etapa (e) gera como resultado uma série de padrões que descrevem o comportamento dos dados.
Estes padrões são validados através da etapa (f) e, após isto, na etapa (g), podem ser dispostos aos usuários finais do sistema de KDD.
A mineração de dados é uma das principais etapas do processo de KDD e está relacionada ao uso de técnicas para extrair ou &quot;minerar «conhecimentos a partir de uma base de dados analítica (ou multidimensional).
Em ela, métodos inteligentes são aplicados visando à busca de padrões.
De entre os diversos tipos de técnicas de mineração, as duas classes de algoritmos que, historicamente, recebem maior destaque são Associação e Classificação.
Em as próximas seções é apresentado um estudo sobre estas duas técnicas que tem por objetivo identificar as suas principais características, focando na identificação da estrutura de como os resultados são gerados por estes algoritmos.
A técnica de mineração de dados denominada Associação possui como objetivo a representação de padrões interessantes entre itens do domínio de uma aplicação, desde que eles possam ser verificados com freqüência na base de dados Um exemplo típico de uso desta classe de algoritmo é a análise de cestas de compras de supermercados, onde o processo analisa os hábitos de compra dos clientes buscando identificar associações entre os diferentes produtos comprados, conforme ilustrado por a Figura 2.
Em ela pode ser visualizado o trabalho de um analista de marketing tentando identificar as possíveis relações entre as compras dos clientes, com base no histórico de quatro cestas de compras.
Os resultados deste processo podem auxiliar estes profissionais a desenvolver novas estratégias de venda, contando com o conhecimento de quais produtos são freqüentemente comprados juntos.
De entre estas estratégias, pode- se citar a alteração da distribuição dos produtos no supermercado.
Desta forma, produtos que, geralmente, são comprados juntos podem estar em prateleiras mais próximas, aumentando a venda de ambos.
Outra opção é manter estes produtos localizados em prateleiras separadas com certa distância, forçando com que os clientes se desloquem mais por o supermercado a sua procura.
Conseqüentemente, isto poderá aumentar as vendas de outros produtos que estejam próximos dos mais procurados.
Segundo Han, uma regra de Associação é representada através da implicação de um ou mais atributos em outros.
Sendo assim, dada uma base de dados D, um conjunto de transações T e dois atributos quaisquer A e B, uma regra de Associação de A implicando em B é representada na forma A B, onde A T, B TeA B $ .
Em este exemplo, o atributo A é definido como sendo a cabeça da regra e o atributo B, o corpo.
Para cada regra de Associação existe um fator de suporte (support ou coverage) que representa o percentual de todas as transações sob análise na base de dados, a qual contém todos os atributos especificados.
Seguindo o exemplo supracitado, o fator de suporte é o percentual de transações T em D que contém A B (os dois atributos juntos).
Este valor é dado por a probabilidade B), ou seja, representa a razão entre transações onde a regra é verificada sobre todas as que estão sob análise na base de dados.
Além de isto, toda regra de Associação também possui um fator de confiança (confidence ou accuracy).
Este fator representa o percentual de transações sob análise na base de dados.
Se estiver presente o atributo que compõe a cabeça da regra, o atributo que constitui o corpo também estará.
Pensando novamente no exemplo anterior, o fator de confiança é o percentual de transações T em D que, se o atributo A estiver presente, também estará o atributo B. Este valor é dado por a probabilidade P (B| A), ou seja, é a razão entre transações onde a regra é verificada sobre as que a cabeça está presente.
Uma regra que satisfaça um valor mínimo de suporte e de confiança é denominada regra forte.
Estes valores são definidos, por convenção, como sendo um número entre o intervalo de 0% e 100% O conjunto de atributos utilizado para a criação das regras recebe o nome de itemset e pode ser composto por n elementos.
A sua freqüência é o número de transações sobre análise na base de dados em o qual pode ser identificado.
Os conjuntos compostos por apenas um atributo são denominados one-item sets, os compostos por dois são denominados two-item sets, e, assim, sucessivamente.
Cada itemset satisfaz um suporte mínimo se a freqüência com que ocorre é maior ou igual ao produto do suporte mínimo com o número total de transações na base de dados, sendo, assim, chamado de frequent itemset.
Os algoritmos de mineração da classe Associação executam, basicamente, duas etapas:
Encontrar os frequent itemsets e gerar regras fortes a partir de eles.
O conjunto de dados de entrada necessário é a base que contém todos os registros desejados e os valores mínimos de suporte e confiança que deverão estar associados às regras resultantes do processo.
Após a sua execução, esta técnica produz como resultado n estruturas que correspondem às regras geradas, onde cada uma é composta por um conjunto de objetos que representam a cabeça da regra e outro que representa o corpo, conforme ilustrado na Figura 3.
Além de isto, conforme pode ser visualizado na figura, para cada regra, estão associados os seus respectivos valores de suporte e de confiança.
Tanto a cabeça quanto o corpo de uma regra de Associação são um conjunto de um ou mais atributos, onde cada um é formado por o seu nome, conforme a base de dados original, e por o valor assumido por ele na regra correspondente.
De entre os algoritmos existentes para a classe Associação, um dos mais simples e conhecidos é o Apriori.
Ele foi proposto por Agrawal com o objetivo de minerar regras associativas em bases de dados grandes e complexas.
Esta técnica tornou- se bastante influente na mineração de dados, originando novos algoritmos, como o Predictive Apriori.
O nome Apriori está baseado no seu principal funcionamento, qual seja, utilizar conhecimentos identificados em execuções anteriores (prior) sobre as propriedades de conjuntos de atributos.
Seu grande diferencial está na sua simplicidade original e versatilidade em bases de dados volumosas.
O Apriori aplica uma abordagem interativa conhecida como busca de nível inteligente (level-wise), onde k conjuntos de itens são utilizados para explorar k+ 1 conjuntos.
Em a sua primeira execução, o algoritmo identifica os conjuntos de itens freqüentes compostos por um único atributo, denominados L1.
Estes conjuntos são utilizados para encontrar novos conjuntos, denominados L2, ou seja, contendo todos os itens freqüentes compostos por dois atributos (2-itemsets).
Este, por sua vez, é utilizado para encontrar L3, que representa os conjuntos de três atributos (3-itemsets) e, desta forma, o algoritmo segue sucessivamente até que não seja possível a identificação de nenhum novo conjunto de n atributos.
O Apriori possui como ponto negativo o fato de que a busca por cada um dos conjuntos exige uma leitura completa de todos os dados presentes no banco de dados.
O pseudocódigo deste algoritmo é ilustrado por a Figura 4.
Em ela, pode ser visualizada a especificação da função principal que é responsável por procurar por todos os itemsets que podem ser gerados a partir de o conjunto de dados disponível, identificado por D. Para cada itemset gerado, denotado por Lk-1, é realizada uma análise para remover conjuntos que não atendem aos critérios estabelecidos, sendo realizada a chamada para a sub-rotina apriori_ gen. O algoritmo denominado Preditive Apriori deriva do Apriori, o qual foi discutido na seção anterior.
Ele foi criado por Scheffer e sua contribuição está fundamentada na importância que os valores de suporte e confiança possuem na geração de regas associativas.
Para que o Apriori possa ser executado, é necessária a definição de parâmetros que determinem os limites de suporte e confiança.
Estes valores são utilizados para se tentar garantir a qualidade das regras geradas.
Porém, ao selecionar somente as que superam este limite, nem sempre é possível obter como resultado o conjunto com as melhores regras.
Assim, a proposta do Predictive Apriori, consiste em buscar uma relação entre os valores de suporte e confiança que possam maximizar a chance de uma correta predição de dados não analisados (dados futuros ou que não foram utilizados no processo de mineração).
Para isto, este algoritmo utiliza uma distribuição binomial onde a ocorrência do atributo analisado é classificada como correta ou incorreta.
Algorithm: Apriori.
Find frequent candidate generation.
Output: L, frequent itemsets in D. Method:
Outro algoritmo existente para a classe Associação é o Tertius.
Ele é descrito através de um sistema de descoberta do tipo top-down que faz uso de métricas de confirmação.
Sua principal implementação, feita na linguagem de programação Gnu C, livre para uso acadêmico, possui aproximadamente 7.500 linhas de código.
Já a sua versão Java, disponibilizada juntamente com o software de mineração de dados Weka, é composta por doze classes e mais de 2.300 linhas de código.
O Tertius utiliza- se de lógica de primeira ordem para a geração da sua representação, o que o permite trabalhar com vários tipos de dados.
Além de isto, possibilita que os usuários possam escolher a forma de representação que lhe seja mais conveniente, ou compreensível, de entre as disponíveis.
O seu funcionamento é parecido com o Apriori, porém as regras são formadas utilizando a forma normal disjuntiva (or) ao invés de a forma normal conjuntiva (and).
O Tertius pode ser configurado para encontrar regras de predição de uma única condição ou de um atributo pré-determinado.
Durante a personalização deste algoritmo, um parâmetro determina quando a negação é permitida no antecedente, conseqüente ou nos dois.
Outros parâmetros determinam o número de regras a serem recuperadas, entre outros.
Este algoritmo gera, como resultado, o padrão da sua categoria, ou seja, um conjunto de n regras de associação, cada uma composta por grupos de atributos que representam a cabeça e o corpo das regras.
Os valores de confiança e suporte, no entando, estão disponíveis com os nomes de confirmação e freqüência observada.
A classe de mineração de dados denominada Classificação constitui uma forma de análise de informações que pode ser utilizada com dois objetivos, segundo Han e Tan:
Modelo descritivo:
Extrair modelos que descrevam as categorias dos dados.
Estes modelos podem ser utilizados para fornecer um melhor entendimento sobre a organização da base de dados.
Modelo Preditivo: Realizar previsões de tendências futuras para registros de dados cujas classes ainda são desconhecidas.
Um típico exemplo de uso deste tipo de algoritmo é a criação de modelos de categorização para aplicações de empréstimo bancário que podem definir clientes com crédito &quot;excelente «ou não, conforme ilustrado por a Figura 5.
Em ela, pode ser visualizada a existência de uma base de treinamento, contendo registros de clientes cuja classificação referente a empréstimos já é conhecida.
A partir deste modelo, as regras de classificação são geradas.
Uma de elas, por exemplo, diz que clientes cuja taxa de crédito é excelente possuem, entre outros atributos, idade entre 31 e 40 anos.
Em a figura também pode ser observada a existência de uma base de testes, onde as regras geradas são validadas.
De acordo com estas regras, é possível definir qual a crédito &quot;excelente «para um novo cliente chamado John Henri que possui, entre outros atributos, idade entre 31 e 40 anos.
Assim, o uso deste modelo permite a categorização de futuros clientes, além de fornecer um melhor entendimento do conteúdo atual do banco de dados.
O processo de Classificação é realizado em duas etapas.
Em a primeira, um modelo é construído descrevendo um conjunto pré-determinado de classes ou conceitos.
Este processo é realizado através da análise das tuplas de dados compostas por n atributos.
Cada tupla é definida como pertencente a uma categoria, indicada por o valor de um de seus atributos, mais conhecido como o atributo identificador de classe (class label attribute).
A o conjunto de tuplas utilizado para a construção do modelo, selecionado aleatoriamente a partir de a amostra, dá- se o nome de conjunto de treinamento (training data set).
Esta etapa também é chamada de aprendizado supervisionado, vez que a classificação de cada item da amostra de treino é conhecida.
Em a segunda etapa do processo, o modelo criado ao longo de a primeira é testado para a validação da sua eficiência.
Além de isto, sua acurácia de predição é estimada utilizando o método de holdout.
Esta é uma técnica simples e utiliza um grupo de tuplas de teste (test set) selecionado de forma aleatória e totalmente independente do conjunto de treinamento, podendo, inclusive, conter os mesmos registros.
A acurácia final é o percentual de tuplas que são corretamente classificadas.
Se ela estiver acima de um limiar considerado aceitável significa que o modelo gerado pode ser utilizado para futuras classificações de registros de categoria desconhecida.
Os algoritmos de Classificação possuem como entrada um conjunto de registros, compostos por n atributos, além de a indicação de qual de eles representa a sua classe.
O resultado da sua execução é, tipicamente, uma estrutura em forma similar a de uma árvore de decisão, composta por os atributos cujos testes sobre os valores são relevantes para a determinação da categoria do registro, conforme diagrama ilustrativo representado por a Figura 6.
Em esta figura, pode ser visualizado o atributo principal, cujo teste separa o maior número de registros, identificado como o atributo raiz da árvore.
A partir de ele, existe uma lista de resultados possíveis e, para cada resultado, o seu relacionamento com um novo nodo, composto por outro atributo.
Este, por sua vez, também poderá ter resultados associados ou não.
Em o final, o valor dos nodos que não possuem resultados, ou seja, os nodos folha, representam a categoria dos registros classificados por a árvore.
De entre os algoritmos que compõem esta classe, merecem destaque os que geram resultados na forma de árvores de decisão.
De estes, o mais conhecido é o ID3 (Iterative Dichotomiser 3), um algoritmo guloso de indução.
O seu funcionamento é baseado na criação de árvores de decisão utilizando a técnica recursiva de dividir e conquistar (divide- and- conquer).
O ID3, cujo algoritmo sumarizado pode ser verificado na Figura 7, funciona da seguinte forma:
Uma árvore é criada composta por um único nodo que representa as amostras de treinamento.
Se as amostras forem todas da mesma classe, então o nodo torna- se uma folha e é identificado com o valor desta classe (passos 2 e 3).
Caso contrário, o algoritmo utiliza uma heurística conhecida como ganho de informação (information gain) para a seleção de um atributo que separa melhor as amostras em classes individuais (passo 6).
Ele torna- se, então, o atributo de teste (ou decisão) para o nodo (passo 7).
O algoritmo necessita que todos os seus valores sejam discretos.
Desta forma, atributos com valores contínuos devem passar por um processo de discretização antes da sua execução.
Um arco é criado para cada um dos valores conhecidos como resultado para atributo de teste.
As amostras são separadas de acordo com estes valores.
O algoritmo utiliza este mesmo processo recursivamente para formar a árvore de decisão das amostras de cada partição.
Uma vez que um atributo é selecionado para teste de um determinado nodo, não poderá mais ser utilizado nos nodos subseqüentes.
As repartições recursivas terminam apenas quando uma das seguintes condições for verdadeira:
Todas as amostras para um determinado nodo pertencerem a uma mesma classe (passos 2 e 3);
Ou Não existirem mais atributos disponíveis para particionar as amostras (passo 4).
Em este caso, a técnica de votação majoritária (majority voting) é empregada (passo 5).
Isto envolve converter um determinado nodo em folha e identificar- lo com a classe principal das amostras;
Ou Não existirem mais amostras para o arco do atributo de teste.
Em este caso, uma folha é criada com a classe principal das amostras (passo Algorithm:
Generate_ decision_ tree.
Generate a decision tree from the giving training data.
Output: A decision tree.
Method: Create a node N;
Gain; Outro algoritmo historicamente importante da categoria Classificação é o C4.
5, também utilizado para a geração de árvores de decisão e, eventualmente, denominado como sendo um classificador estatístico.
Este algoritmo, provavelmente um dos mais utilizados atualmente, é uma extensão do ID3, sendo que ambos foram propostos por Quinlan.
O C4.
5 apresenta uma série de melhorias em relação a o seu antecessor, merecendo destaque as seguintes:
Uso de atributos contínuos e discretos:
Para que seja possível a manipulação de atributos contínuos, o C4.
5 cria um limiar e, então, divide os valores dos atributos entre os que estão acima deste limiar e aqueles que estão abaixo ou são iguais e ele;
Tratamento de dados de treino com valores perdidos:
O C4.
5 permite que valores de atributos que não estejam disponíveis sejam marcados como&quot;?».
Os atributos que não tiverem valores simplesmente não são utilizados.
Otimização das árvores após a criação:
Assim que uma árvore é criada o C4.
5 faz uma revisão na tentativa de remover caminhos que não auxiliam no entendimento dos dados.
Eles são substituídos por nodos folhas.
O presente capítulo apresentou um estudo relacionado ao processo de descoberta de conhecimento, listando as suas principais etapas conforme definidas por Han.
Além de isto, foi apresentada uma breve análise sobre duas das classes de algoritmos mais significativas:
Associação e Classificação.
A classe de mineração de dados denominada Associação, apresentada na seção 2.1, almeja a busca de relacionamentos entre os diversos atributos de registros de uma base de dados.
O resultado da sua execução é uma lista de regras de associação compostas de elementos denominados cabeça e corpo.
Cada um de eles é formado por um conjunto de um ou mais atributos, bem como por os valores assumidos para cada regra.
Além de isto, as regras de associação possuem, ainda, os valores de suporte e confiança.
A seção 2.1.1 discorreu sobre o algoritmo Apriori, que é um dos principais e mais simples desta classe.
Ele influenciou na criação de outro algoritmo, um pouco mais eficiente, denominado Predictive Apriori, apresentação na seção 2.1.2.
Além destes, existe, ainda, o Tertius, que foi analisado na seção 2.1.3 e utiliza lógica de primeira ordem para a geração das regras.
A seção 2.2 apresentou outra importante classe de algoritmos de mineração, a Classificação.
Os algoritmos deste tipo possuem dois objetivos:
Extrair modelos para descrever as categorias dos dados e realizar previsões de tendências futuras utilizando os modelos criados.
De os diversos algoritmos existentes para esta classe, foram discutidos dois:
O ID3 e o C4.
5. O primeiro, apresentado na seção 2.2.1, é um algoritmo guloso de indução, cujo resultado é o mesmo que o da maioria dos demais algoritmos desta classe:
É expresso através de árvores de decisão.
Já o C4.
5, discutido na seção 2.2.2, é, provavelmente, o mais utilizado atualmente, sendo uma evolução do algoritmo ID3.
Ele apresenta uma lista de melhorias como, por exemplo, a possibilidade de se trabalhar com valores contínuos.
Este capítulo foi importante para o contexto deste trabalho por fornecer o referencial teórico sobre as principais classes de mineração de dados, buscando identificar o formato como os dados de saída das execuções dos algoritmos de mineração estão organizados.
Com base nesta análise, foi possível a identificação de padrões nos resultados dos diferentes algoritmos de cada uma das classes estudadas, que serão utilizados na criação dos arquivos XML de definição de estruturas, apresentados no capítulo 0 deste documento.
Em este sentido, pôde- se observar que algoritmos da classe Associação geram como resultado n regras, onde cada uma possui os seguintes elementos:
Cabeça (composta por um ou mais atributos), corpo (também formado por uma lista de um ou mais atributos), fator de confiança e de suporte.
Já as regras do tipo Classificação criam estruturas de decisão para categorização de instâncias.
De entre as opções de saída desta categoria, optou- se por trabalhar com a que produz árvores de decisão.
Para esta opção, o resultado é uma estrutura que inicia a partir de um atributo onde o teste sobre os seus possíveis valores assumidos indica novos testes ou o atributo responsável por a classificação da instância.
Este capítulo apresenta uma visão geral sobre técnicas de visualização de informação, e, após, relata um estudo sobre algumas de elas, como Árvores de Decisão e Redes Bayesianas.
A visualização de dados consiste em disponibilizar informações numa forma textual, gráfica ou tabular, sendo que o seu principal objetivo é possibilitar a interpretação visual de informações por pessoas.
Bons métodos de visualização necessitam que os dados sejam convertidos num determinado formato, sendo que suas características e relacionamentos com itens ou atributos devem possibilitar boas análises e relatórios.
A primeira etapa do processo de visualização é o mapeamento da informação para um formato visual.
Assim, dados de objetos, seus atributos e seus relacionamentos são transformados em elementos gráficos como pontos, linhas, formas e cores.
Os objetos são, geralmente, representados numa das seguintes três maneiras:
Se apenas um atributo objeto está sendo considerado para categorização, então são gerados aglomerados de dados organizados de acordo com os seus possíveis valores.
Estas categorias são visualizadas como uma entrada de uma tabela ou como uma área na tela;
Quando um objeto possui múltiplos atributos, então pode ser visualizado como uma linha (ou coluna) de uma tabela ou como uma linha de um gráfico;
Um objeto que é representado como um ponto num espaço bidimensional ou tridimensional pode ser representado como uma figura geométrica, como círculos e caixas, entre outros.
Existem diversas formas para a representação de padrões gerados por métodos de descoberta de conhecimento.
Em as próximas seções é apresentado um estudo sobre algumas destas técnicas que tem por objetivo identificar as suas principais características, focando na identificação da estrutura de como os dados devem estar dispostos para que possam ser analisados visualmente.
A técnica de visualização de informações denominada Árvore de Decisão é um resultado natural da abordagem do tipo dividir e conquistar (divide- and- conquer) para um conjunto de instâncias independentes.
Uma árvore de decisão é uma estrutura semelhante a um gráfico de fluxo em estrutura de árvore, conforme ilustrado por a Figura 8.
Em ela, pode ser observado o nodo principal, a raiz, que representa o primeiro atributo que é levado em consideração para a classificação de instâncias.
Este atributo representa uma taxa de produção e possui dois possíveis resultados:
Reduzido ou normal.
Para o valor reduzido existe um novo nodo que não possui nenhum teste, denominado nodo folha.
Para o valor normal da raiz, existe um novo nodo, com um novo atributo para testes e novos resultados.
Em uma árvore de decisão, os nodos internos representam testes para determinados atributos comparando- os com constantes pré-estabelecidas, com valores de outros atributos ou com o resultado da execução de funções.
Quanto a os arcos da árvore, estes representam as possíveis saídas para a realização dos testes.
Além de isto, os nodos folhas representam a classificação que é aplicada a todas as instâncias que chegam até eles.
Segundo Tan, as principais características de árvores de decisão são:
Interpretabilidade: Os modelos gerados são de fácil compreensão;
Robustez: A acurácia dos modelos não é afetada por eventuais ruídos ou dados redundantes.
Para que seja possível verificar a classificação uma determinada instância utilizando este técnica, é necessário percorrer a árvore, partindo da raiz e escolhendo os arcos correspondentes aos valores dos atributos representados por os nodos até que se alcance os nodos folhas.
Em outras palavras, partindo- se do nodo raiz, a árvore deve ser percorrida recursivamente, dividindo o conjunto de treinamento em subconjuntos de acordo com o critério de divisão (testes dos nodos) até que cada sub-árvore resulte num nodo folha.
Em este momento, a instância que está sob análise é classificada conforme o valor do nodo folha.
Se um determinado atributo, utilizado como teste num nodo, for nominal, então o número de nodos filhos é, geralmente, o número de possíveis valores que o atributo pode conter.
Em estes casos, como existe um arco para cada valor possível, o mesmo atributo não é utilizado novamente na estrutura árvore.
Por outro lado, se o valor do nodo em teste for do tipo numérico, seu teste determina quando o valor é igual, maior ou menor que uma determinada constante, gerando três possíveis resultados.
Outra possibilidade de uso de atributos numéricos inclui o teste do seu valor com um intervalo (gerando os arcos abaixo, contidos ou acima).
Um dos problemas óbvios, para a técnica de Árvores de Decisão, é a ocorrência de instâncias com atributos utilizados como testes em nodos sem valores, visto que não se sabe qual o arco deve ser selecionado durante a sua classificação.
Uma abordagem para solucionar este problema é criar um arco no nodo que represente atributos sem valor.
Porém, em alguns casos esta não é uma possibilidade e os atributos sem valores devem ser tratados como um caso especial.
Uma alternativa é salvar o número de instâncias do conjunto de transações que são direcionadas para cada arco do nodo e, quando uma instância não possuir valor para o atributo correspondente, selecionar o arco mais utilizado até o momento.
Este técnica de visualização necessita como entrada para a sua correta execução de uma estrutura em forma de árvore.
Em esta estrutura, composta por nodos, cada um deve conter o nome do atributo de teste que representa e, para cada um de eles, deve existir uma lista de possíveis resultados, os quais formam os arcos.
Cada arco é a representação do relacionamento entre dois nodos, ou seja, o resultado do teste para o atributo de origem que indica a próxima validação a ser realizada, no seu componente de destino.
Por fim, nodos que não possuírem arcos, ou seja, nodos folhas, representam o valor resultante da classificação expressa na árvore.
Os classificadores bayesianos utilizam uma abordagem probabilística para modelar o relacionamento entre os atributos preditivos e a classe alvo.
Uma Rede Bayesiana é um gráfico que representa o relacionamento probabilístico de um conjunto de variáveis.
Uma Rede Bayesiana é dividida em duas partes:
Qualitativa: Grafo acíclico dirigido, que representa os relacionamentos de dependência entre um conjunto de variáveis;
Quantitativa: Tabelas de probabilidades condicional, associadas a cada nodo e seu relacionamento com outros.
A Figura 9 ilustra uma Rede Bayesiana.
Em ela, é possível verificar um exemplo de classificação de fraudes, baseado na probabilidade de ocorrência de atributos como idade e sexo.
Cada nodo da rede representa um atributo do domínio e os arcos representam as relações de dependência entre eles.
A probabilidade entre dois nodos na rede representa a força do relacionamento causal entre eles.
Desta forma, o relacionamento entre as variáveis do domínio é explorado através de probabilidades condicionais.
A abordagem probabilística das redes bayesianas é baseada no teorema de Bayes, o qual consiste num princípio estatístico para combinar conhecimento a priori de classe com novas evidências, obtidas a partir de os dados.
Este teorema pode ser traduzido da seguinte por a fórmula P (B| A) $= P (A| B)* P (B)/ P (A).
Em ela, P (A| B) significa a probabilidade do atributo A, tal que tudo que se tem conhecimento é o atributo B. Através do uso de probabilidades é possível conhecer a influência de um atributo sobre o outro.
Dada uma distribuição de probabilidades para cada atributo, uma Rede Bayesiana permite predizer um atributo desconhecido (atributo classe) através do cálculo da distribuição de probabilidades a posteriori.
Entre as principais características das Redes Bayesianas estão:
Representatividade: Representação das dependências causais entre atributos.
Interpretabilidade: Os modelos gerados são de fácil interpretação, sendo bem adaptados para situações contendo atributos sem valor.
Robustez: A acurácia dos modelos não é afetada por eventuais ruídos ou dados redundantes.
Sobre a técnica de representação do conhecimento de Redes Bayesianas pode- se concluir que é necessário um conjunto de nodos e suas dependências (quando houver), além de a probabilidade associada ao nodo e o seu imediatamente relacionado.
As técnicas de visualização são, tipicamente, especializadas para o tipo de dados analisado.
Novos métodos de representação do conhecimento, bem como variações dos já existentes, são constantemente criados.
Além de as representações de dados apresentadas nas seções anteriores, duas outras técnicas também podem ser citadas:
Histogramas e gráficas de pizza.
Os histogramas são gráficos que exibem distribuições de valores para atributos.
Eles funcionam através da divisão de possíveis valores em conjuntos e exibindo o número de objetos presentes em cada um.
Para dados categóricos, cada valor do histograma representa um conjunto.
Após a criação dos grupos e da divisão dos objetos em eles, um gráfico de barras é construído.
Em ele, cada conjunto é representado por uma barra e a área de cada barra é proporcional ao número de valores (objetos) que faz parte do intervalo correspondente.
A Figura 10 exibe quatro histogramas para atributos da flor Íris, onde cada um é composto por dez conjuntos de valores diferentes.
Um gráfico de pizza é uma visualização de dados semelhante a um histograma.
Porém é tipicamente utilizado com atributos categóricos que possuem um número relativamente pequeno de valores.
Ao invés de representar a freqüência relativa dos diferentes valores através de uma área ou altura de uma barra, esta visualização utiliza a área relativa de um circulo para indicar a freqüência.
Mesmo que este tipo de gráfico seja popular no mundo dos negócios, eles são pouco utilizados em publicações técnicas porque o tamanho das áreas relativas pode ser difícil de julgar.
Para esta finalidade, histogramas ainda são preferíveis.
A Figura 11 exibe um exemplo de um gráfico de pizza para as possíveis distribuições de cores da flor Íris para um determinado conjunto de dados.
Este capítulo discorreu sobre um estudo relacionado a algumas técnicas de visualização de dados.
Este estudo apresentou alguns métodos importantes como Árvores de Decisão e Redes Bayesianas.
A técnica de visualização de informações denominada Árvores de Decisão, apresentada na seção 3.1, é o resultado natural da abordagem de dividir e conquistar.
Para que a sua representação seja possível, os dados devem estar organizados em estruturas de nodos e arcos.
A primeira estrutura representa o valor de atributos do domínio de visualização, enquanto que a segunda representa o relacionamento entre eles.
A seção 3.2 discorreu sobre o método de geração de visualização Redes Bayesianas.
Este técnica, baseada no teorema de Bayes, utiliza uma abordagem probabilística para modelar o relacionamento entre os atributos preditivos e a classe alvo.
Para que a sua representação seja criada, é necessária a existência dos atributos, os relacionamentos entre eles, e a probabilidade condicional da sua ocorrência.
Além de estas técnicas, foram apresentadas na seção 3.3, as técnicas de histograma e gráficos de pizzas, que definem visualizações interessantes, dependendo do contexto dos dados que se deseja analisar.
Para o contexto deste trabalho, no entanto, foi selecionada para uso apenas a visualização de Árvores de Decisão, visto que é uma das mais comumente utilizadas.
Em este sentido, pode- se observar que a técnica de representação de dados Árvore de Decisão necessita, como informações de entrada, estruturas que representam nodos.
Cada uma de eles deve conter o atributo do conjunto de dados correspondente e a sua lista de arcos relacionados, quando existirem.
Os nodos que não possuírem relacionamentos são responsáveis por indicar as classificações do modelo.
Estes devem ser apresentados de forma diferenciada, chamando a atenção dos usuários.
Este capítulo apresenta um estudo sobre o XML, que é uma das principais linguagens de marcação, além de discorrer sobre os esquemas para definição de arquivos deste tipo e suas transformações.
Uma linguagem de marcação é um conjunto de códigos aplicados a um texto ou a dados.
Sua principal finalidade é adicionar informações particulares a eles.
As linguagens de marcação são usadas, por exemplo, na indústria editorial para marcar a formatação (exibição gráfica) de páginas.
Se o código de marcação for padronizado, ou puder ser processado por um programa de computador, garante- se o intercâmbio de uma publicação complexa entre autores, editores e impressoras.
A XML (Extensible Markup Language) é uma linguagem de marcação padrão criada por a W3C com o intuito de ser um formato universal para a troca de informações na Web.
Ela define um conjunto de regras para a definição de marcas semânticas que dividem um determinado documento em diversas partes Os documentos escritos utilizando a XML são compostos de uma mistura de dados e marcações (tags).
É importante destacar que as marcas em XML descrevem estrutura e significado de documentos, mas não descrevem a formatação dos seus elementos.
Para isto, podem ser utilizadas folhas de estilo.
Com o uso de XML porque é possível fazer sistemas diferentes trabalharem juntos através da troca de informações, desde simples números até dados estruturados.
Um dos principais aspectos que tornam a XML flexível decorre do fato de que as suas marcações são maleáveis, além de autodescritivas.
Com isto, é possível atribuir nomes mais significativos ao conteúdo referenciado.
Um documento escrito nesta linguagem pode conter, opcionalmente, uma descrição de sua gramática, que é utilizada por diversas aplicações para validar a sua estrutura.
Esta descrição, conhecida como esquema, possui, atualmente, duas propostas:
DTD e XML Schema A DTD (Document Type Definition) especifica um conjunto de regras para a estrutura dos documentos.
Este formato define uma lista de elementos, atributos, notações e entidades que podem existir nos documentos XML, bem como o relacionamento entre eles.
Os arquivos DTD podem estar incluídos no próprio documento XML ou vinculados através de uma URL externa.
A DTD possui uma notação própria para esta definição.
O formato XML Schema, por outro lado, é escrito como sendo um documento XML.
Este padrão foi desenvolvido com o objetivo de ser um vocabulário compartilhado para a validação automática da estrutura de outros arquivos XML.
Um documento deste formato, comumente denominado XSD (XML Schema Definition) é, tipicamente, um arquivo com a extensão &quot;xsd», e pode ser utilizado para definir Elementos que podem aparecer num documento;
Atributos que podem aparecer num documento;
Quais são os elementos filhos;
A ordem dos elementos filhos;
O número máximo de elementos filhos;
Se um elemento é vazio ou poderá ter a inclusão de textos;
Tipos de dados de elementos e atributos;
Valores padrões e/ ou fixos para elementos e atributos.
Além de a definição de esquemas para os documentos XML, a W3C definiu alguns formatos para visualização dos dados contidos nos documentos.
Estes padrões começaram a ser desenvolvidos porque existia uma demanda por linguagens do tipo folha de estilos para documentos XML.
A XSL (XML Style Sheets) descreve como os arquivos XML devem ser exibidos.
Por exemplo, um elemento do tipo pode significar uma estrutura de tabela na linguagem Html, uma peça de mobília ou qualquer outra coisa (e os navegadores não sabem como exibir isto).
O padrão XSL define tanto uma nova linguagem de marcação quanto uma linguagem de formatação.
A transformação de um documento XML pode utilizar as marcações e esquemas originais do documento ou um conjunto de marcas completamente diferentes.
A XSL é composta por três partes:
XSLT: Linguagem para transformação entre documentos XML;
XPath: Linguagem para navegação em documentos XML;
XSL-FO: Linguagem para formatação de documentos XML.
XSLT é considerada a parte mais importante da especificação do XSL.
Ele permite que um arquivo XML possa ser transformado num outro arquivo XML (com estrutura semelhante ou diferente), ou outros formatos, que possam ser reconhecidos por os navegadores, como Html e XHTML, sendo, este último, o seu uso mais freqüente.
O XSLT permite a adição e/ ou exclusão de elementos e atributos no arquivo de saída.
Além de isto, fornece meios para que os elementos possam ser organizados, ordenados, terem seus valores validados com testes para tomadas de decisões sobre quais elementos exibir e quais ocultar, entre outros Em o processo de transformação, a XSLT utiliza a tecnologia de XPath para definir partes do documento de origem que devem equiparar com um ou mais modelos pré-definidos.
Quando uma igualdade é encontrada, o código de origem é transformado no código de destino, conforme as regras definidas no arquivo de transformação.
O presente capítulo apresentou um estudo sobre a linguagem de marcação de dados denominada XML.
Além de isto, foi discutida a forma de definição da estrutura de documentos deste tipo e a linguagem de transformação dos dados.
A XML Schema, discutida na seção 4.2, é uma especificação utilizada para definir a estrutura que deve ser seguida por novos documentos criados com a linguagem XML, apresentada na seção 4.1.
Ela, ao contrário de a DTD, também é escrita utilizando- se o padrão XML, fato que facilita a sua utilização.
Entre outros pontos positivos do seu uso, pode- se citar a flexibilidade para futuras melhorias na especificação, além de a possibilidade de criação de esquemas mais ricos e poderosos, contando, inclusive, com a possibilidade de definição do tipo dos dados.
A seção 4.3 apresentou a XSL.
Este formato, também definido por a W3C, é utilizado para definir como os documentos XML podem ser visualizados, servindo como uma espécie de folha de estilos.
Com o seu uso, é possível transformar um documento XML numa página Html.
A XSL é composta de três partes:
XSL Transformation, XPath e XSL-FO.
Este capítulo foi importante para o contexto deste trabalho por fornecer o referencial teórico sobre a linguagem XML e os demais padrões que estão relacionados com esta tecnologia.
Considerando que ela é uma das mais importantes linguagens de marcação em uso atualmente, tornou- se a opção mais lógica para a proposta deste trabalho, detalhada no capítulo 6.
Este capítulo apresenta um trabalho que está relacionado ao tema de pesquisa exposto por este documento, descrevendo suas principais características além de listar seus pontos positivos e negativos.
A PMML (Predictive Model Markup Language) é uma linguagem baseada em XML que proporciona uma forma de definição de modelos estatísticos e de mineração de dados para compartilhamento entre aplicações distintas.
Além de isto, esta linguagem fornece meios para o armazenamento de informações relacionadas às operações da etapa de limpeza, extração, transformação e carga dos dados, desta linguagem é o DMG (Data Mining Group), que é um grupo independente focado no desenvolvimento de formatos para mineração de dados.
Uma vez que a PMML é descrita como sendo um documento no padrão XML, a sua especificação é fornecida na forma de um arquivo do tipo XML Schema que, atualmente, encontra- se na versão 3.0.
A principal idéia desta especificação é fornecer uma forma independente para o intercâmbio de dados entre aplicações distintas.
Através do seu uso, os usuários podem desenvolver seus modelos utilizando uma determinada aplicação (de licença proprietária ou não) e utilizar outras, de outros fornecedores ou baseadas em software livre, para a visualização, análise e avaliação.
Cada documento PMML pode conter a definição de um ou mais modelos de mineração de dados.
O elemento principal (tag root) é denominado com o nome da linguagem (PMML) e, a partir de ele, todas as demais marcações são definidas.
A estrutura geral de um documento, seguindo esta especificação, pode ser visualizada na Figura 12 De entre os benefícios alcançados por o uso deste formato para o intercâmbio de informações, um fator bastante positivo é que os documentos PMML são extremamente dinâmicos.
A especificação da PMML, além de fornecer marcações para armazenamento de informações da fase de ETL (extração, transformação e carga), também fornece tags especificas para a documentação de modelos de dados, incluindo diversas opções de técnicas.
A definição dos elementos que são aceitos dentro de a tag principal pode ser visualizada na Figura 13.
Em ela pode- se verificar que um documento seguindo a estrutura do PMML pode conter um cabeçalho, tarefas executadas durante a etapa de preparação dos dados, dicionários de dados e de transformação e, por fim, modelos os dados dos tipos suportados, tais como:
Associação, Clustering, Redes Neurais, Seqüências e Árvores.
A especificação da PMML diz que um modelo de Associação representa regras onde alguns itens estão relacionados a outros.
A Figura 14 ilustra a definição da marcação que representa o modelo de dados desta categoria de algortimo de mineração de dados.
Analisando a estrutura representada por a figura, com base nas características de algoritmos de mineração de dados estudadas na seção 2.1, é possível concluir que a especificação da PMML vai muito além de os atributos básicos, fornecendo informações adicionais que podem servir para melhorar a descrição do modelo, como, por exemplo, número de transações do conjunto de dados utilizadas ou o número de itemsets gerados.
O padrão PMML não especifica modelos da categoria Classificação de forma explícita.
Em contrapartida a isto, descreve marcações que podem ser utilizadas para a definição de modelos de Árvores de Decisão.
Estas tags podem ser utilizadas tanto na construção de estruturas de classificação quanto em estruturas de predição.
A Figura 15 exibe a definição de algumas marcações que podem ser utilizadas para esta finalidade.
Novamente, é possível verificar a grande diversidade de informações que podem estar presentes em documentos PMML, o que o torna extremamente versátil.
Comparando estes dados com os estudos realizados por as seções 2.2 e de outros, como o valor de distribuição dos valores dos atributos de teste que representam um nodo.
Uma das principais características da PMML é o fato de que toda a sua estrutura está descrita por um único esquema.
Isto pode ser citado como sendo seu principal ponto forte e, também, o seu ponto fraco.
A diversidade das marcações XML pode ser considerada como um fator positivo no sentido de que com apenas um documento de definição de esquemas, utilizando XML Schema, é possível criar arquivos com diferentes tipos de modelos de dados.
Com isto, tem- se uma facilidade de uso, vez que não é necessário a existência de n diferentes esquemas, um para cada tipo de modelo.
Por outro lado, esta mesma característica também pode ser considerada como um fator negativo para o usuário final deste padrão.
Isto pode ser concluído devido a a dificuldade de manipulação do arquivo de definição do esquemas, composto por mais de 2.000 linhas.
Esta característica, exigida por os objetivos da linguagem, torna difícil a compreensão da forma de criação de algumas marcações, além de agregar um fator a mais de dificuldade no desenvolvimento de aplicações que interpretam este formato.
Outro ponto negativo associado à esta característica é que não se pode, por exemplo, criar aplicações de visualização que utilizam como entrada uma estrutura fixa, visto que cada documento PMML pode conter um modelo de dados completamente diferente.
Em este caso, as implementações devem ser flexíveis a ponto de manipular todos os formatos disponíveis ou gerar mensagens para uso de formatos não suportados.
Outro ponto que pode ser considerado como negativo é o fato de que a especificação não possui nenhum tipo de framework que facilite o seu manuseio.
O uso de classes Java, por exemplo, que recebessem um conjunto de dados e retornassem o código do XML no formato PMML, ou fizesse o caminho contrário, poderia auxiliar a utilização do padrão, além de contribuir para a sua maior difusão e, conseqüentemente, uso.
O presente capítulo apresentou o padrão PMML, que é uma especificação, descrita em XML, que proporciona uma forma de definição de modelos de dados resultantes de execuções de algoritmos de mineração.
Além de isto, permite a documentação das operações realizadas durante etapa de extração, transformação e carga.
A principal idéia deste padrão é fornecer uma forma independente para intercâmbio de informações entre aplicativos distintos.
De entre os diversos modelos de dados aceitos por o formato, descrito num arquivo XML Schema de mais de 2.000 linhas, estão Associação, Seqüências, Árvores e Clustering.
Este capítulo foi importante para o contexto deste trabalho por fornecer uma visão de outro trabalho que está, direta ou indiretamente, buscando os mesmos objetivos propostos neste documento, ou seja, a padronização dos dados entre as etapas do processo de KDD.
Após a análise da solução apresentada, foi possível perceber que o formato PMML está bem encaminhado rumo a a padronização.
Porém, como conseqüência da grande diversidade de modelos propostos, sua estrutura torna- se um tanto quanto difícil de ser utilizada.
Com isto, aplicações que se propõem a trabalhar com este formato, devem se preocupar em tratar todos os tipos de modelos possíveis, ao invés de focar apenas uma funcionalidade.
Este capítulo apresenta a caracterização do problema de padronização entre as etapas do processo de descoberta de conhecimento, discorrendo sobre a sua importância.
O processo de descoberta de conhecimento é uma seqüência de sete etapas bem definidas, conforme descrito por Han e discutido no capítulo 2 deste documento.
Cada uma destas etapas é responsável por tarefas específicas, sendo que todas são importantes para o processo como um todo.
Porém, o relacionamento entre elas não é algo trivial, vez que, para que se possa executar completamente o processo de KDD, pode ser necessário o desenvolvimento de todas estas etapas num mesmo aplicativo, utilizando arquitetura e linguagem de programação idênticas, conforme ferramenta apresentada em, e.
Segundo e, este ambiente computacional pode ser ainda mais complexo.
Assim, as etapas do processo de KDD poderiam estar sendo executadas de forma independente, num ambiente distribuído, orientado a serviços, com o uso da tecnologia Java J2EE ou, ainda, controlado por uma aplicação de orquestração de serviços (BPEL).
Esta abordagem distribuída pode ser bastante benéfica para a melhoria de desempenho nas aplicações de KDD.
Tanto o processo de mineração quanto o de geração da visualização podem possuir algoritmos complexos que, quando executados com uma grande quantidade de dados, demandem grande esforço computacional do servidor em o qual estão instalados.
A Figura 16 ilustra uma possível arquitetura para implementação do processo de KDD de forma distribuída.
Conforme pode ser visualizado na ilustração, a aplicação principal, instalada num ou mais servidores, pode possuir todas as principais funcionalidades para interface com os usuários, além de regras para controle de acessos.
A partir de as operações disponíveis nos seus itens de menus, pode ser possível visualizar um resumo do conteúdo da base de dados e selecionar o conjunto desejado para o processo (data sets).
Depois, pode ser possível optar por uma das técnicas de mineração de dados existentes e iniciar a sua execução.
A o final deste processo, o algoritmo pode gerar um resultado a ser analisado utilizando um dos métodos de visualização acessíveis na ferramenta.
Em esta arquitetura, as etapas de mineração e visualização são apresentadas como sendo duas aplicações distintas, implementadas com base nos conceitos da tecnologia J2EE (Servlets e EJBs).
Para um melhor aproveitamento de recursos, cada um destes módulos pode estar instalado num servidor independente.
Em um mundo ideal, estes servidores encontram- se localizados na sede de uma mesma empresa.
Porém podem estar em data centers separados numa mesma cidade ou, ainda, em cidades, estados ou países diferentes.
Em, e é discutida uma ferramenta cuja proposta é uma arquitetura de suporte ao processo de descoberta de conhecimento sobre bases de dados de processos de negócio (workflow).
Esta ferramenta, criada sob o conceito web, foi implementada seguindo uma arquitetura monolítica, ou seja, todas as etapas do processo de KDD são realizadas numa mesma aplicação que se encontra disponibilizada num servidor único.
A Figura 17 ilustra a arquitetura da ferramenta, divida em três módulos:
Importação, Análise e Visualização.
Para que fosse possível a implementação desta ferramenta, foi utilizado o modelo analítico definido por Casati em.
Este modelo, desenvolvido para o mapeamento de processos de negócio implementados na ferramenta Hp Process Manager 5.0, é composto da seguinte forma:
Tabelas dimensão, responsáveis por as definições dos modelos:
Proc_ Defs_ D (modelos de processos), Service_ Defs_ D (serviços ou funções), Node_ Defs_ D (atividades), Arcs_ Defs (transições entre as atividades), Data_ Defs_ D (dados), Proc_ Bhv_ D (comportamentos dos processos), Time_ D (datas) e Resources_ D (usuários e grupos).
Tabelas fato, responsáveis por as definições das instâncias:
Proc_ Inst_ F (processos), Proc_ Data_ Inst_ F Proc_ Bhv_ Inst_ F (valores (comportamentos das de dados instâncias para processos), de processos), Service_ Inst_ F (atividades e serviços ou funções) e Node_ Inst_ Data_ F (valores de dados para atividades).
O Modelo Casati é bastante abrangente e aborda diversos conceitos de processos de negócio.
Porém, ele possui algumas características que, para a proposta de ferramenta discutida em, não foram totalmente satisfatórias.
Conforme analisado em, alguns problemas deste modelo são:
Falta de opções para o mapeamento de processos de negócio organizados na forma de subprocessos, uma prática de modelagem bastante comum;
Ausência de forma de relacionar participantes com mais de um grupo sem que seja necessário repetir o cadastro do participante para cada grupo que ele faça parte;
Falta de uma distinção clara entre a origem dos dados, vez que a definição do dado está associada ao modelo do processo e suas instâncias podem estar vinculadas tanto a processos como atividades.
Buscando a contínua melhoria da ferramenta e, conseqüentemente, do processo como um todo, em e é descrito um novo modelo multidimensional denominado Athena Model ­ Beta.
Este modelo é composto da seguinte forma:
Tabelas dimensão, responsáveis por as definições dos modelos:
Entities (entidades), Participants_ Groups Entities_ Participants Participants (relacionamento entre entre (usuários grupos), entidades participantes), Process_ Groups (grupos de processos), Process_ Definitions (modelos de processos), Activity_ Definitions (atividades e funções), Process_ Activities (relacionamentos entre atividades e funções com processos e subprocessos), Transitions (transição entre as atividades), Data_ Definitions (dados), Process_ Data_ Definitions (dados de processos), Activity_ Data_ Definitions (dados de atividades) e Times (datas).&amp;&amp;&amp;
Tabelas fato, responsáveis por as definições das instâncias:
Process_ Instances (processos), Activity_ Instances (atividades e funções), Data_ Instances (valores de dados), Process_ Data_ Instances (valores de dados de processos) e Activity_ Data_ Instances (valores de dados de atividades ou funções).
Uma das propostas do modelo Athena era corrigir os problemas identificados com o uso do Modelo Casati.
Para que fosse possível atender- la, buscou- se uma maior adequação aos padrões definidos por a WfMC.
Desta forma, foi realizado um estudo sobre a padronização de processos de negócio e, como resultado, criou- se uma lista de atributos essenciais e outra de opcionais totalizando, nas duas, 40 itens.
Em um comparativo entre os dois modelos, chegou- se a conclusão de que o modelo Casati atende a 26 atributos, enquanto que o modelo proposto, o Model Athena -- Beta, atende a 30 atributos.
A partir de o uso destes modelos analíticos o desenvolvimento da etapa de PL/ SQL.
A escolha por esta linguagem está baseada em questões de desempenho, aliado ao fato de que as bases original e analítica estão no mesmo banco de dados.
Estas rotinas, então, são responsáveis por acessar a base que armazena as informações de execuções dos processos de negócio, selecionar os atributos desejados, executar a limpeza e padronização dos valores e, por fim, importar estes dados no modelo multidimensional.
O sistema de gerência de workflow utilizado é o Oracle Workflow.
Para a base de dados foram utilizados dois processos de negócios de uma pequena empresa de desenvolvimento de software, contendo o registro de execução de mais de 2.500 instâncias de processos.
A ferramenta proposta em possui a opção de importação de arquivos contendo diferentes implementações para as etapas de mineração de dados e visualização das informações.
Buscando a viabilidade deste módulo, foi proposto um framework de desenvolvimento utilizando a linguagem Java.
Assim, para a importação de novos algoritmos de mineração de dados, o desenvolvedor do código deve definir uma classe principal que implementa uma determinada interface.
Esta classe recebe os dados que devem ser utilizados, conforme contrato estabelecido por a especificação dos métodos da interface, e é responsável por executar o algoritmo e salvar seu resultado para posterior recuperação.
Para as visualizações, o princípio é exatamente o mesmo, ou seja, todas elas devem possuir uma classe principal que implementa uma outra interface responsável por definir a forma como os resultados das execuções serão acessados.
O autor da visualização deve então se preocupar em como o seu algoritmo se comporta e transmite suas informações.
Após a importação das implementações, a ferramenta ainda fornece uma forma de definir relacionamento entre elas, ou seja, é possível especificar quais as técnicas de visualização podem ser utilizadas para a análise dos resultados de um determinado algoritmo de mineração.
Este relacionamento, no entanto, é realizado por um usuário com perfil de administração do sistema e é livre, não existindo nenhuma restrição que impeça o uso de visualizações incompatíveis com determinados algoritmos.
Assim, para que as implementações de técnicas de visualização possam funcionar corretamente, os seus autores devem conhecer a estrutura em a qual os dados referentes aos resultados dos métodos de mineração estão dispostos, independente de formato e mídia.
Esta característica faz com que as visualizações adicionadas ao sistema tenham a necessidade de serem projetadas de forma específica para as técnicas de mineração.
Em o contexto apresentado nas seções anteriores, pode- se perceber que é importante a organização da forma como a informação é trocada entre as etapas de mineração e de visualização, do processo de KDD.
Esta comunicação, sendo realizada de forma única, pode facilitar o desenvolvimento de aplicações de mineração de dados, além de a integração entre bases distintas.
Para isto, torna- se necessário que as entradas e saídas dessas etapas do processo de descoberta de conhecimento possuam um determinado nível de padronização, de forma que consigam ser executadas de maneira independente, mas, ainda assim, estarem integradas.
Além de isto, as diferentes etapas podem estar sendo desenvolvidas por pessoas dispostas em várias localidades do mundo.
O problema abordado por este trabalho é, então, como realizar a padronização da troca de informações entre as etapas de mineração de dados e visualização dos resultados do processo de KDD.
Em este sentido, para que este problema possa ser resolvido, os requisitos de uma solução ideal são:
Padronização da forma de geração dos resultados dos algoritmos de mineração de diferentes classes;
Padronização da maneira como técnicas de visualização distintas lêem seus dados de entrada;
Existência de um framework para a transformação automática entre os formatos de mineração para os de visualização.
O cenário descrito anteriormente é caracterizado como sendo um problema interessante de ser tratado, vez que existem diferentes classes de algoritmos de mineração de dados e técnicas de visualização de informação.
Somado a isto, existe o problema de compatibilidade de uso entre determinadas técnicas de análise visual de dados com certas classes de algoritmos de mineração.
Em o capítulo 5 deste trabalho foi apresentada a especificação PMML.
Este formato busca exatamente a citada padronização, sendo constituído por apenas um esquema e contendo definições para diversos tipos de algoritmos de mineração.
Apesar de toda a sua diversidade, o PMML não resolve totalmente o problema descrito anteriormente, visto que não trata as técnicas de visualização.
Esta característica faz com que as implementações de métodos deste tipo, para que possam trabalhar com o formato, tenham que entender a maneira como os resultados dos algoritmos de mineração estão organizados.
Por outro lado, mesmo com um foco maior na integração de diferentes algoritmos e visualizações, a ferramenta proposta em também não resolve o problema da padronização.
Como a instalação e definição dos relacionamentos entre as implementações estão a cargo de um usuário com perfil de administrador, não existe uma forma de garantir uma troca de informações padrão, forçando as implementações a conhecerem as estruturas umas das outras.
Os capítulos 2 e 0 apresentaram estudos sobre as classes de mineração de dados e as técnicas de visualização de informação, respectivamente.
Estes estudos foram importantes por fornecerem os requisitos necessários para a compreensão das estruturas de dados que são utilizadas.
Tanto os resultados dos algoritmos de mineração quanto os dados necessários para as técnicas de visualização podem estar disponíveis em estruturas de dados e formatos diversos.
Entre eles, pode- se destacar o uso de arquivos textuais e do tipo CSV, onde os dados encontram- se separados uns dos outros por o uso de vírgulas.
Algumas ferramentas de mineração, no entanto, possuem estruturas específicas para o armazenamento dos seus dados.
Conforme comentado anteriormente, este trabalho está utilizando o software Weka como base para os testes da solução.
Esta ferramenta, implementada em linguagem de programação Java, armazena os resultados das execuções dos seus algoritmos de mineração em estruturas internas das suas classes.
Para a visualização a ferramenta apenas imprime as informações de forma textual numa determinada área da aplicação.
A Figura 20 ilustra este comportamento, exibindo o resultado da execução do algoritmo J48, uma implementação do C4.
5. Em ela é possível perceber o formato padrão de como este software gera suas visualizações.
Durante esta operação, nenhum arquivo é criado, nem mesmo que de forma temporária.
Algumas categorias de algoritmos, como o próprio J48 ou Redes Bayesianas, possuem áreas de análise de dados próprias, que podem exibir as informações de forma gráfica.
A Figura 21 ilustra esta visualização para o mesmo resultado da execução do algoritmo J48 apresentado na figura anterior.
Para que estas funcionalidades possam ser acionadas, é necessário que as informações sobre as execuções dos algoritmos, armazenadas em memória, sejam transformadas em textos seguindo um determinado formato, especifico de cada implementação.
A partir de isto, os algoritmos são responsáveis por criar suas estruturas internas e gerar o componente de visualização correspondente.
Este capítulo discorreu sobre a caracterização do problema que o trabalho se propõe a resolver.
Conforme apresentado, o processo de descoberta de conhecimento pode envolver algoritmos de mineração e técnicas de visualização complexos que, durante sua execução, necessitam de muitos recursos computacionais, prejudicando o rendimento das aplicações.
Para estes casos, a melhor solução seria uma abordagem distribuída para as ferramentas de KDD.
A ferramenta descrita em propõe um ambiente computacional para a execução de processos de descoberta de conhecimento sobre bases de dados de workflow.
Para o modelo analítico, foram utilizadas duas versões da mesma abordagem, sendo que a primeira foi desenvolvida por Casati, enquanto que a segunda, é o modelo proposto por Garcia,.
Esta ferramenta fornece ainda uma maneira de importar e relacionar algoritmos de mineração e técnicas de visualização.
Para a importação, é necessário que as implementações utilizem, como base, duas interfaces definidas através de um framework da própria aplicação.
Já o relacionamento é realizado de forma livre por um usuário com perfil de administração.
Assim, tem- se que a proposta principal deste trabalho é criar uma forma de relacionamento padrão entre duas importantes etapas do processo de KDD:
Este capítulo visa descrever a proposta para solução do problema apresentado no capítulo anterior, além de relatar todos os passos realizados para a criação da solução.
A partir de o problema exposto no capítulo anterior, que não pode ser resolvido totalmente com o uso da linguagem PMML, pois a mesma limita- se a padronizar a saída de algoritmos de mineração, torna- se necessário a definição de uma solução que atenda as necessidades apresentadas.
Assim, o foco principal da proposta deste trabalho é a definição de uma forma de padronização para troca de informações entre duas etapas do processo de descoberta de conhecimento:
A partir de o estudo sobre a linguagem XML, apresentado no capítulo 4, foi possível concluir que esta tecnologia é a melhor escolha quando o assunto é troca de informações entre aplicações distintas.
Desta forma, com base no problema descrito no capítulo 6 deste documento, a solução proposta é a criação de diferentes esquemas, utilizando a técnica de XML Schema.
Para as classes de mineração de dados, os esquemas devem representar a estrutura como os resultados dos seus algoritmos são gerados, independente de implementação, sendo que deve existir um esquema para cada classe de algoritmo.
Porém, para as técnicas de visualização, os esquemas indicam o formato como os dados de entrada devem estar organizados, existindo, também, um esquema para cada tipo de método.
Para a criação dos esquemas responsáveis por representar a estrutura dos algoritmos de mineração de dados (XML Schemas), foram utilizadas as duas classes estudadas no capítulo 2 deste trabalho:
Associação e Classificação.
O esquema tem um cabeçalho, com informações de identificação da base de dados, como o nome e descrição, além de o algoritmo de mineração utilizado.
A Figura 22 ilustra a definição desta marcação, denominada Info, utilizando a notação XML Schema.
Em ela podem ser visualizadas as estruturas que representam o nome do conjunto de dados (Name), a sua descrição (Description) e o nome algoritmo de mineração responsável por o resultado do arquivo (Algorithm).
A criação do esquema que representa a estrutura de saída de algoritmos de Associação foi realizada com base nas características estudadas na seção 2.1.
Foi verificado que a saída desta classe apresenta uma lista composta por várias regras.
Assim, a estrutura principal do seu esquema é uma marcação que pode conter uma lista de objetos, denominada Rules.
Para representar cada uma das possíveis regras, foi criado um componente específico, chamado Rule.
Seguindo as definições da classe, tem- se que cada regra de associação é uma estrutura que possui quatro dados:
Cabeça, corpo, valor de confiança e valor de suporte.
Desta forma, o esquema do componente Rule foi definido como sendo um tipo de dado complexo.
Em ele estão definidos dois novos elementos, um representando a cabeça da regra, denominado Head, e outro o corpo, definido como Body.
Além de eles, existem duas tags numéricas para os valores de suporte e confiança denominadas, respectivamente, Support e Confidence.
A definição das estruturas que representam a cabeça e o corpo de regras de associação especifica que os objetos devem ser compostos por um ou mais atributos.
Desta forma, o tipo de dado complexo que representa as marcações Head e Body, é definido de forma semelhante ao utilizado para a lista de regras, ou seja, uma estrutura de dados que armazena uma lista de objetos.
Porém, neste caso, esta lista é composta por marcações que representam atributos, denominada Attribute.
Seguindo a definição de que cada atributo presente numa regra é representado por o seu nome e valor assumido, tem- se que o conteúdo da marcação Attribute são duas tags:
Name e Value.
Demonstrando o esquema especificado, a Figura 23 exibe um trecho do código de definição de arquivos XML para resultados de algoritmos de mineração da classe Associação.
O esquema completo está disponível no Anexo I. Objetivando a validação do esquema proposto, foram realizadas alguns testes.
Para isto, foram utilizadas as implementações, fornecidas por a ferramenta Weka, de dois algoritmos da classe Associação:
O Apriori e o Tertius apresentados, respectivamente, nas seções 2.1.1 e 2.1.3.
A comparação entre a saída fornecida originalmente por a aplicação Weka e o arquivo XML gerado com base no esquema proposto, foi realizada a partir de a execução dos respectivos algoritmos para um mesmo conjunto de dados.
Para isto, foi selecionado um dos arquivos de teste fornecido juntamente com a instalação desta ferramenta, denominado weather.
O weather é composto por um total de 14 registros e, para cada um, estão listados quatro atributos que indicam características climáticas:
Condição do tempo (outlook), temperatura (temperature), umidade (humidity) e existência de ventos (windy).
Com base nos seus valores, um quinto atributo (play) informa se um determinado esporte é praticado ou não.
O resultado da execução do algoritmo Apriori no Weka, ilustrado por a Figura 24, mostra que, para o conjunto de dados selecionado, foram criadas 10 regras de associação.
Já a execução do algoritmo Tertius, para os mesmos dados, cujo resultado pode ser visualizado na Figura 25, gerou um total de 25 regras.
A implementação Java do algoritmo Apriori, fornecida juntamente com o Weka, foi criada por a Universidade de Waikato e sua classe principal é a weka.
Associations. Apriori.
Para iniciar a sua execução, de acordo com a interface padrão do Weka para algoritmos, weka.
Core. Instances.
Em ele devem estar necessário armazenados um objeto todos os do tipo registros recuperados do conjunto de dados que está sendo utilizado para o processo de mineração.
Internamente esta classe contém um vetor com todos os atributos presentes no conjunto de dados e, para cada um de eles, existe uma outra lista contendo todos os seus possíveis valores.
Assim, um registro é armazenado como sendo uma estrutura composta por vários objetos, cada um com dois índices numéricos.
O primeiro representa a posição do vetor referente a o atributo constante no registro, enquanto o segundo indica o valor assumido por o atributo, conforme definido na lista de valores correspondente.
As regras resultantes da execução desta implementação são armazenadas num vetor de objetos do tipo weka.
Core. FastVector.
A primeira e a segunda posição deste vetor são compostas por novos componentes, os quais representam a cabeça e o corpo de todas as regras geradas.
Tais componentes são constituídos por listas relacionadas entre si por os valores dos seus índices.
Exemplificando: O objeto presente na posição zero da primeira lista representa a cabeça da regra cujo corpo esta armazenado na posição zero da segunda lista.
Tanto a cabeça quanto o corpo das regras de associação armazenam seus dados dentro de uma instância da classe weka.
Associations. AprioriItemSet.
Em ela existem dois novos vetores contendo índices que referenciam os atributos e os valores salvos nas listas do objeto Instances.
A terceira posição da lista de objetos da classe FastVector armazena um vetor de valores numéricos que representam a confiança de cada uma das regras.
Esta lista segue a mesma lógica das anteriores, ou seja, a posição zero indica o valor de confiança da regra representada por as posições zero das listas armazenadas nas duas primeiras posições do vetor.
Por fim, o valor de suporte de cada regra é armazenado como sendo uma variável interna da classe AprioriItemSet, estando disponível apenas para os objetos que representam a cabeça das regras.
Para o mapeamento do resultado da execução do algoritmo Apriori para o arquivo XML, é necessário a execução dos seguintes passos:
Criar a tag principal do arquivo, denominada Rules;
Iterar sobre as listas das duas primeiras posições do objeto FastVector;
Para cada par de posições destas listas que representam um regra, gerar a estrutura corresponde, ou seja, a marcação do tipo Rule;
Gerar para cada regra, por padrão, as tags Head e Body;
Percorrer os vetores do objeto AprioriItemSet, correspondentes aos indíces que representam a cabeça e corpo de cada regra no objeto FastVector;
Para cada posição destes vetores, criar a estrutura da tag Attribute;
Criar, dentro de a tag Attribute, as marcações Name e Value e preencher- las com o nome e o valor dos atributos recuperados da classe Instances, conforme os índices informado nos vetores;
Após a marcação que define o corpo, criar a tag Confidence, preenchendo seu valor com o número armazenado do índice correspondente à regra na terceira posição do vetor da classe FastVector;
Para cada regra, com base no valor da variável m_ counter do objeto AprioriItemSet da estrutura que representa a cabeça, gerar uma tag do tipo Support.
A Figura 26 apresenta uma parte do arquivo XML resultante deste mapeamento, utilizando, como entrada para a execução do algoritmo, o conjunto de dados descrito na seção 7.2.1.1.1.
Em esta imagem, pode ser visualizada apenas uma das 10 regras geradas, sendo que o arquivo completo está disponível no Anexo III.
A implementação Java do algoritmo Tertius, fornecida juntamente com o software Weka, é de criação de Amelie Deltour e foi desenvolvida com base na versão original, escrita na linguagem C por Flach.
O componente principal desta implementação é a classe weka.
Associations. Tertius.
Para iniciar a execução, também é necessário um objeto do tipo Instances, contendo todos os registros do conjunto de dados, conforme descrito na seção anterior.
A estrutura de armazenamento de resultados do Tertius é bastante diferente da utilizada na implementação do Apriori.
Enquanto que o Apriori armazena diversas listas de índices para os atributos e valores salvos na classe Instances, o Tertius utiliza melhor os conceitos de orientação a objetos e, cada estrutura, possui uma classe distinta com seus devidos atributos.
Assim, no Tertius as regras de associação são todas salvas numa instância da classe weka.
Associations. Tertius.
SimpleLinkedList, que possui uma implementação do conceito de listas encadeadas.
Em ela, cada nodo é composto por um objeto do tipo weka.
Associations. Tertius.
Rule, representando uma regra.
Tanto a cabeça quanto o corpo das regras, por terem estruturas semelhantes, são armazenados como sendo instâncias da classe weka.
Associations. Tertius.
LiteralSet. Em ela, existe uma lista de objetos que representa todos os atributos que fazem parte do componente, sendo que cada um de eles é um objeto do tipo weka.
Associations. Tertius.
AttributeValueLiteral. Esta classe permite que o nome do atributo seja recuperado através da execução do método getPredicate, enquanto que o valor correspondente deve ser extraído de uma variável interna, denominada m_ value.
Os valores de confiança e suporte devem ser recuperados a partir de os valores de confirmação e freqüência observada para cada regra, acessíveis por os métodos getConfirmation e getObservedFrequency da classe Rule.
Como o Tertius trabalha de forma um pouco diferente do padrão de algoritmos de associação, ele não possui valores explícitos para confiança e suporte, sendo necessário um mapeamento das suas informações semelhantes.
Para o mapeamento do resultado da execução do algoritmo Tertius para o arquivo XML, é necessário a execução dos seguintes passos:
Criar a tag principal do arquivo, denominada Rules;
Iterar sobre a lista encadeada de objetos do tipo Rule, armazenados na classe SimpleLinkedList;
Para cada objeto recuperado, criar uma tag do tipo Rule.
Gerar para cada regra, por padrão, as tags Head e Body;
Para cada objeto do tipo LiteralSet, que representa a cabeça ou o corpo da regra, percorrer a lista de atributos correspondente;
Cada instância da classe AttributeValueLiteral recuperada deve resultar na criação da marcação Attribute;
Executar o método getPredicate da classe AttributeValueLiteral e salvar o valor resultante na marcação Name do atributo correspondente;
Extrair o valor da variável m_ value da classe AttributeValueLiteral e salvar na marcação Value do atributo correspondente;
Após a marcação que define o corpo, criar a tag Confidence e preencher seu valor com o resultado da execução do método getConfirmation da instância que representa a regra;
De a mesma forma, o valor para a tag Support é o retorno da execução do método getObservedFrequency do objeto da classe Rule.
A Figura 27 apresenta o arquivo XML resultante deste mapeamento, utilizando, como entrada para a execução do algoritmo, o mesmo conjunto de dados descrito na seção 7.2.1.1.1.
Em esta imagem, pode ser visualizada apenas uma das 25 regras geradas, sendo que o arquivo completo está disponível no Anexo III.
Estas são as mesmas 25 regras de associação resultantes da execução do algoritmo do software Weka, exibidas na Figura 25.
É importante destacar nesta ferramenta as regras estão dispostas ao contrário, ou seja, primeiro é mostrado o corpo da regra e, depois, a cabeça.
Com base nas experimentações apresentadas nas seções anteriores, é possível chegar a uma conclusão quanto a o procedimento padrão para o mapeamento de resultados de execução de algoritmos de mineração da classe Associação para um arquivo XML.
Assim, independente das estruturas de dados utilizadas ou da forma como foi realizada a implementação do algoritmo, para a criação do arquivo para uma nova implementação, é necessário seguir os seguintes passos:
Identificar qual a estrutura que armazena a lista de regras de associação e criar no arquivo XML a marcação Rules;
Percorrer esta lista e, para cada objeto recuperado criar, no arquivo XML, uma nova tag Rule;
Para cada objeto que representa uma regra, identificar os componentes que armazenam as informações referentes à cabeça e ao corpo, criando as marcações Head e Body no arquivo XML;
Percorrer a estrutura que representa a cabeça e o corpo da regra identificando as estruturas correspondentes aos atributos;
Para cada atributo, na marcação correspondente (Head ou Body) do arquivo XML, criar uma nova tag Attribute;
Identificar a forma como o nome e o valor dos atributos são salvos em cada objeto, bem como recuperar- los e salvar seus dados nas marcações Name e Value correspondentes;
Identificar, no objeto que representa a regra, os valores de suporte e confiança criando, dentro de a marcação correspondente, as tags Confidence e Support com seus valores respectivos.
Após a definição do formato para o armazenamento de resultados de algoritmos do tipo Associação, é possível traçar uma comparação entre este formato e o PMML, apresentado no capítulo 5.
O PMML utiliza a marcação AssociationRule para definir a sua lista de regras, enquanto que o formato proposto por este documento utiliza uma tag identificada por Rule.
A primeira possui uma série de outros atributos, utilizando para identificação de parâmetros do modelo, enquanto que a segundo não possui outras definições.
Em os dois formatos, cada regra é composta por os quatro elementos básicos:
Cabeça, corpo, confiança e suporte.
Em o PMML, está estrutura é bastante complexa pois deve ser definida uma marcação do tipo ItemSet que irá conter os dados de todos os atributos que compõem a cabeça e o corpo.
Cada atributo é definido como uma tag do tipo ItemRef.
O relacionamento entre as marcações é realizado a partir de identificadores.
Desta forma, a marca AssocationRule deve possuir os identificadores dos componentes do tipo ItemSet que representam a cabeça e corpo da regra.
Estes, por sua vez, devem possuir o identificador de todos os elementos do tipo ItemRef que definem os pares de atributos e valores.
Em o formato proposto por este documento, esta estrutura é mais simples pois basta a definição de marcações específicas para os quatro elementos.
Dentro de a cabeça e corpo, existem os atributos dispostos sem relacionamentos.
Com base neste breve comparativo, pode- se perceber que o novo formato proposto é mais simples e de fácil manuseio que o PMML.
Isto acontece pois não existe a necessidade de definição de diversas marcações com identificadores diferentes que devem estar relacionados entre si.
A criação do esquema que representa a estrutura de saída de algoritmos de Classificação foi realizada com base nas características estudadas na seção 2.2.
Foi verificado que a principal forma de apresentação dos resultados desta classe utiliza uma estrutura de árvores de decisão, composta por nodos e arcos.
Assim, optou- se por utilizar esta forma de organização dos resultados.
Os algoritmos de Classificação utilizam os valores de um dos atributos do conjunto de dados para representar a informação de categorização dos registros.
De este modo, esta informação deve estar presente nos arquivos XML resultantes do processo.
Assim, a estrutura da marcação Info, descrita na seção 7.2.1, também é definida para esta categoria, recebendo, além de as informações principais, um elemento, denominado ClassLabelAttribute.
A definição desta marcação, em XML Schema, pode ser visualizada na Figura 28.
O componente principal deste esquema possui a denominação de Attribute para manter compatibilidade com as nomenclaturas utilizadas por os algoritmos e representa o nodo raiz.
Seguindo as definições da classe, tem- se que cada nodo de uma árvore de decisão é composto por o nome do atributo de teste e por uma lista de arcos com outros nodos, representando seus possíveis valores.
Desta forma, o esquema do componente Attribute foi definido como sendo um tipo de dado complexo.
Em ele, estão definidos dois novos elementos:
Um correspondente ao nome do atributo, denominado Name, e outro equivalente à lista de resultados, denominada Results.
Por a definição da categoria de Classificação, existem dois tipos de nodos:
Internos: Representam um atributo de testes e possuem uma lista de arcos de saída;
Folha: Representam o atributo de classificação dos registros possuindo apenas o seu valor.
A definição da estrutura Results, então, apresenta uma lista de elementos do tipo Result.
Cada resultado é composto por o valor e por um componente de escolha.
O valor é representado por a marcação Value e corresponde ao arco de saída de um nodo.
Logo, se um determinado atributo possuir 5 valores no conjunto de dados original, por a definição do esquema do arquivo XML, será transformado num elemento do tipo Attribute, contendo uma lista de cinco elementos do tipo Result, cada um mantendo um dos seus possíveis valores.
O componente de escolha da marcação Result indica se o nodo é do tipo interno ou folha.
Os nodos do tipo folha possuem o valor do atributo de classificação mapeado para o elemento Label.
Por outro lado, os nodos internos possuem a definição de qual atributo representa o próximo teste, definido por uma nova tag do tipo Attribute.
A especificação desta marcação segue exatamente a mesma lógica exposta anteriormente.
Demonstrando o esquema especificado, a Figura 29 exibe um trecho do código de definição de arquivos XML para resultados de algoritmos de mineração do tipo Classificação.
O esquema completo está disponível no Anexo I. Objetivando a validação do esquema proposto, foram realizadas alguns testes.
Para isto, foram utilizadas as implementações, fornecidas por a ferramenta Weka, de dois algoritmos de Classificacao:
O ID3 e o C4.
5, apresentados, respectivamente, nas seções 2.2.1 e 2.2.2.
A comparação entre a saída fornecida originalmente por a aplicação Weka e o arquivo XML gerado com base no esquema proposto, foi realizada a partir de a execução dos respectivos algoritmos para o mesmo conjunto de dados apresentado na seção 7.2.1.1.1.
O atributo play foi selecionado como sendo o responsável por a identificação da categoria dos registros.
O resultado da execução do algoritmo ID3 no Weka, ilustrado por a Figura 30, mostra que, para o conjunto de dados selecionado, foi gerada uma árvore de decisão cujo nodo raiz é representado por o atributo outlook.
A partir de ele, dependendo do resultado, os atributos humidity e windy, foram utilizados como nodos internos.
Já a execução do algoritmo C4.
5, cuja implementação é denominada J48, resultou na árvore de decisão ilustrada por a Figura 31.
Casualmente, este algoritmo gerou uma árvore de decisão muito similar a do ID3, onde também foram utilizados o atributo outlook como raiz, e os atributos humidity e windy como nodos internos.
A implementação Java do algoritmo ID3, fornecida juntamente com a instalação do software Weka, foi criada por a Universidade de Waikato e sua classe principal é a weka.
Classifiers. Trees.
Id3. A exemplo do que acontece com as classes principais de outros algoritmos de mineração, discutidas nas seções anteriores, a execução do ID3 também é iniciada a partir de o objeto padrão Instances, que representa o conjunto de dados a ser utilizado.
Cada instância da classe Id3 possui dois objetos:
Um representando o atributo de testes de um determinado nodo e outro representando a lista de relacionamentos, denominada de sucessores.
O objeto que representa o atributo, do tipo weka.
Core. Attribute, armazena o seu nome e uma lista contendo todos os seus possíveis valores, conforme definido no conjunto de dados inicial.
A lista de relacionamentos é composta por um vetor de novos objetos do tipo ID3, sendo que a posição destes elementos possui relação direta com a lista de valores do atributo.
Assim, o objeto da primeira posição deste vetor forma o arco com o atributo, sendo que a identificação do relacionamento é fornecida por a primeira posição da lista de valores.
Com o uso destas estruturas, o ID3 consegue, recursivamente, montar toda a árvore de decisão.
Para o mapeamento do resultado da execução do algoritmo ID3 para o arquivo XML, é necessário a execução dos seguintes passos:
Recuperar o objeto do tipo Id3, que representa o atributo principal da árvore;
Gerar a tag principal do arquivo, denominada Attribute;
Recuperar o nome do atributo armazenado no objeto Id3 e gerar a marcação Name para o atributo principal;
Gerar a marcação responsável por a definição da lista de resultados, denominada Results;
Percorrer a lista de valores da classe Attribute;
Para cada posição desta lista, gerar a marcação Result, salvando na tag Value o valor da iteração, recuperado a partir de a execução do método value;
Verificar se a lista de sucessores do atributo possui elementos ou está vazia;
Se esta lista estiver vazia, gerar a marcação Label para o resultado e preencher com valor recuperado partir da variável interna m_ ClassAttribute da classe Id3;
Se a lista possuir elementos, criar uma nova estrutura da marcação Attribute e, para cada um dos itens da lista, executar todo o processo de forma recursiva até chegar a um atributo de classificação.
A implementação em Java do algoritmo C4.
5, fornecida juntamente com a ferramenta Weka, é denominada J48, e, a exemplo do ID3, também foi desenvolvida por a Universidade de Waikato.
A classe principal desta implementação é a weka.
Classifiers. Trees.
J48 e, cumprindo o contrato estabelecido por a interface padrão do Weka, também necessita de uma instância da classe Instances.
Após a sua execução, o J48 armazena os resultados num objeto do tipo weka.
Classifiers. Trees.
J48. ClassifierTree.
A partir de ele, de forma recursiva, é possível acessar toda a estrutura que compõe a árvore de decisão resultante.
O atributo principal está armazenado como uma variável interna desta classe, identificada por m_ root.
Cada instância do tipo ClassifierTree, possui três objetos que armazenam, entre outros dados, o nome do atributo de teste, a lista de componentes filhos e o conjunto original de dados, representado por o objeto Instances.
Tanto o nome quanto a lista de valores dos atributos são armazenados, na classe ClassifierTree, como instâncias do objeto weka.
Classifiers. Trees.
J48. ClassifierSplitModel.
Para a recuperação do nome do atributo, é necessário a execução do método leftSide.
Ao contrário de a forma como foi desenvolvido o ID3, onde um atributo folha não possui sucessores, na implementação do J48, o componente responsável por a representação dos atributos (classe ClassifierTree), sempre possui uma lista de filhos.
O valor de cada relacionamento recuperado partir do mesmo objeto da classe ClassifierSplitModel que contém o nome do atributo.
Porém, neste caso, é utilizado o método rightSide juntamente com o índice do resultado que está sendo processado.
A classe ClassifierTree possui uma variável interna denominada m_ isLeaf.
Esta variável indica se um determinado nodo é uma folha ou não.
Assim, para cada elemento da lista de filhos do atributo deve- se testar o conteúdo desta variável.
Se ele for verdadeiro, o valor da classificação dos registros é extraído utilizando- se o método dumpModel da instância da classe ClassifierSplitModel.
Esta classe é a mesma utilizada anteriormente para a recuperação do nome e dos valores do atributo.
Porém, sendo falso este teste, significa que o elemento representa um atributo interno.
Para o mapeamento do resultado da execução do algoritmo J48 para o arquivo XML, é necessário a execução dos seguintes passos:
Recuperar o objeto do tipo ClassifierTree armazenado por a variável m_ root da classe J48;
Gerar a tag principal do arquivo, denominada Attribute;
Recuperar objeto ClassifierSplitModel armazenado na classe ClassifierTree;
Executar o método leftSide e salvar o resultado na tag Name da marcação de atributo;
Gerar a marcação responsável por a definição da lista de resultados, denominada Results;
Iterar sobre a lista de filhos do classe ClassifierTree;
Para cada posição desta lista, gerar a marcação Result, salvando na tag Value o valor da iteração, recuperado a partir de a execução do método rightSide;
Testar o valor da variável m_ isLeaf da classe ClassifierTree;
Se o valor for verdadeiro, executar o método dumpModel da instância da classe ClassifierSplitModel e mapear o resultado para a tag Label;
Se o valor for falso, criar uma nova estrutura da marcação Attribute e, executar todo o processo de forma recursiva para a lista de objetos filhos até que se chegue a um atributo de classificação.
A Figura 33 apresenta uma parte do arquivo XML resultante deste mapeamento, utilizando, como entrada para a execução do algoritmo, o mesmo conjunto utilizado anteriormente.
Em esta imagem, pode ser visualizado que o resultado é bastante semelhante ao gerado por o algoritmo Id3.
Este fato, no entanto, não é uma regra e sim apenas uma casualidade devido a o conjunto de dados reduzido.
O arquivo completo está disponível no Anexo III.
Esta árvore é exatamente a mesma gerada por o software Weka, conforme pode ser visualizado na Figura 31.
Com base nos estudos de caso apresentados nas seções anteriores, é possível concluir como deve ser o procedimento padrão para o mapeamento dos resultados da execução de algoritmos de mineração do tipo Classificação para o arquivo XML correspondente.
Assim, independente das estruturas de dados utilizadas ou da forma como foi implementado o algoritmo, para a criação do arquivo, é necessário seguir os seguintes passos:
Identificar qual a estrutura que armazena os dados do atributo principal, ou seja, o nodo raiz da árvore de decisão e criar no arquivo XML a marcação Attribute;
Verificar a forma como o nome do atributo é salvo no objeto correspondente e mapear para a marca Name;
Recuperar todos os atributos filhos do nodo atual, criando a tag Results no arquivo XML;
Iterar sobre a lista de atributos filhos recuperada e, para cada objeto selecionado, criar uma estrutura correspondente à marcação Result no arquivo;
Recuperar o valor do arco formado entre o atributo pai e o filho, salvando esta informação na marca Value que compõe o componente Result.
Verificar o tipo atributo filho, se representa um nodo folha um novo atributo de teste;
Se o atributo representar um nodo folha, deve ser criada a marcação Label para o componente Result correspondente do arquivo XML contendo o valor do atributo;
Se o atributo representar um nodo de teste, então deve ser criada uma nova marcação do tipo Attribute, dentro de o resultado correspondente, e todo o processo deve ser executado novamente de forma recursiva.
Após a definição do formato para o armazenamento de resultados de algoritmos do tipo Classificação, é possível traçar uma comparação entre este formato e o PMML, apresentado no capítulo 5.
O PMML não possui um esquema para modelos de classificação.
Desta forma, uma vez que este trabalho optou por utilizar somente árvores de decisão como saída deta categoria, será utilizado a especificação da estrutura de árvores para a comparação.
O PMML utiliza a marcação TreeModel para definir modelos do tipo árvores.
De entre as diversas marcações de controle e parametrização do modelo, existe a tags Node.
Este elemento define uma lista de novos itens deste tipo, representando o relacionamento entre os nodos.
O nome do atributo fica armazenado através de uma estrutura do tipo Partition.
O formato proposto nesta seção mostra- se, novamente, mais simples, visto que devem ser apenas definidos os nodos, seus nomes e a sua lista de relacionamento.
Estas listas podem estar relacionadas com novos nodos os conter o valor de classificação dos registros, representando nodos folhas.
Para a criação dos esquemas responsáveis por representar a estrutura das técnicas de visualização de informações, foi utilizada apenas um dos métodos estudados no capítulo 0 deste trabalho:
Árvores de Decisão.
Esta técnica foi escolhida por ser a mais comum para a visualização de resultados de algoritmos de Classificação.
A criação do esquema que representa a estrutura de entrada necessária para a técnica de visualização de Árvores de Decisão foi realizada com base nas características identificadas na seção 3.1.
Foi verificado que a estrutura de dados necessária para a geração da visualização deve, obrigatoriamente, iniciar com um elemento que represente o nodo raiz da árvore.
Assim, a estrutura principal do seu esquema é composta por uma marcação definida como um tipo complexo, denominada Node.
Seguindo as definições da classe, tem- se que cada nodo possui o nome do atributo presente no conjunto de dados inicial que está sendo representado.
Desta forma, o esquema do componente Node recebe a definição de um elemento denominado Name, responsável por armazenar o nome atributo de teste representado por ele.
Um nodo pode possuir uma lista de arcos que define seu relacionamento com outros nodos.
Estes, por sua vez, podem representar novos testes sobre valores de atributos ou armazenarem o valor de classificação dos registros, sendo denominados nodo folhas.
Por isso, a marcação Node recebe a definição de um elemento que pode conter uma lista de objetos, denominado Children.
Para representar os relacionamentos de um nodo com seus filhos, foi criada a marcação Child.
Ela possui a definição do elemento que representa o arco, ou seja, um dos valores do atributo de teste, representado por a tag Value.
Além de isto, a marca Child possui a especificação de uma estrutura de escolha responsável por indicar se o próximo nodo do relacionamento é uma folha ou outro nodo intermediário.
No caso de o próximo nodo ser uma folha, a marcação Child passa a possuir um elemento denominado Result, o qual apenas armazena o valor do atributo de classificação.
Porém, se for outro atributo de testes, a marcação que deve estar presente é a Node, sendo que ela é do mesmo tipo da utilizada para definir o nodo raiz.
Desta forma, o arquivo XML que utiliza esta estrutura como base poderá conter diversos nodos internos, seguindo o mesmo formato, até que um de eles seja uma folha.
Demonstrando o esquema especificado, a Figura 35 exibe um trecho do código de definição de arquivos XML para dados de entrada da técnica Árvores de Decisão.
O esquema completo está disponível no Anexo I. A visualização fornecida para a análise dos resultados do algoritmo J48, implementação do padrão C4.
5, é apresentada na Figura 21.
Em ela podem ser verificados todos os tipos nodos existentes, ou seja, raiz, internos e folhas.
Já o algoritmo ID3, não possui uma opção gráfica para apresentação dos resultados, sendo utilizada apenas a notação textual, conforme ilustrada por a Figura 30.
Objetivando a validação do esquema proposto, foram realizadas alguns estudos de caso.
Para isto, foi utilizada a técnica de visualização fornecida por o Weka para o algoritmo de Classificação J48.
A comparação entre a visualização gerada originalmente por a aplicação Weka e por o arquivo XML foi realizada a partir de os resultados da execução dos algoritmos ID3 e C4.
5 apresentados nas seções arquivo XML, correspondente ao esquema de visualização explicado na seção anterior.
O resultado deste mapeamento, para resultados do algoritmo J48, está ilustrado na Figura 36.
Como a estrutura de resultado dos algoritmos de Classificação é bastante similar à visualização de Árvores de Decisão, o mapeamento entre os arquivos foi realizado na proporção de um para um, ou seja, cada marcação de resultado possui a sua correspondente no esquema de visualização.
A visualização para árvores fornecida por o Weka também foi desenvolvida, a exemplo das implementações dos algoritmos, por a Universidade de Waikato.
Ela é constituída por uma aplicação Java Swing, ou seja, é executada no próprio computador, ao invés de ser uma aplicação disponibilizada num servidor acessado via navegador.
Para a sua execução, é necessário que os dados a serem visualizados sejam transformados numa estrutura especifica, conforme ilustrado por a Figura 37.
Em ela pode ser visualizado que existe a definição de todos os nodos que fazem parte da árvore, identificados por Nx, onde x é um numérico seqüencial.
Cada nodo possui a definição do seu nome de exibição através do atributo identificado por label.
No caso de nodos folhas, mais duas informações devem ser definidas, indicando que a forma geométrica a ser gerada deve ser um quadrado (atributos shape e style).
Os arcos entre os atributos, por sua vez, devem estar descritos em estruturas do tipo NxNy, onde x e y representam os identificadores seqüenciais que definem o nodo que faz parte do relacionamento.
Além de isto, os arcos devem possuir o valor do atributo de teste, definido através do atributo label.
Assim, para transformar o arquivo XML, apresentado na Figura 37, na entrada necessária para a visualização, é preciso proceder a recuperação do nodo raiz e a sua transformação no componente N0.
A partir deste componente, faz- se necessária a navegação nos seus filhos para a criação das estruturas que irão representar os demais nodos.
Durante este processo, são utilizados identificadores numéricos seqüenciais para a especificação de cada nodo.
O valor encontrado deve ser armazenado e relacionado com o seu nodo correspondente no arquivo XML para que, posteriormente, seja realizada a referência.
Após a criação das estruturas que definem os nodos, é necessário criar as estruturas que definem os arcos.
Isto não pode ser realizado junto da execução da etapa anterior, pois a ordem das definições é importante.
Logo, todo o processo de recuperar o nodo raiz e percorrer- lo deve ser executado novamente, buscando, desta vez, o valor do relacionamento (marcação Value da tag Result).
Para cada par de nodos que compõe o arco, os identificadores gerados na etapa anterior devem ser utilizados junto com o valor do relacionamento para a criação da estrutura necessária para a visualização.
Seguindo este processo, foi possível utilizar a visualização padrão de árvores, empregada por o J48 para, além de a análise dos resultados do próprio algoritmo, analisar as informações geradas por a implementação ID3.
É importante destacar que este algoritmo não possui uma técnica de visualização correspondente, e a visualização, que foi gerada a partir de o uso do arquivo XML, não é possível, atualmente, no software Weka.
A Figura 38 ilustra a visualização dos resultados do algoritmo J48, enquanto que a Figura 39 exibe as informação referentes a execução do ID3.
Com base nos estudos de caso apresentados na seção anterior, é possível concluir como deve ser o procedimento padrão para o mapeamento dos resultados de execução de algoritmos de mineração para o arquivo XML que pode ser utilizado por a técnica Árvores de Decisão.
Assim, independente das estruturas de dados utilizadas, ou da forma como foi implementada a visualização, para a criação do arquivo é necessário seguir os seguintes passos:
Identificar a estrutura, no arquivo XML de origem, que representa o ponto principal para entendimento dos dados, mapeando como sendo o nodo raiz da árvore utilizando a marcação Node, no arquivo de destino;
Recuperar o valor que representa a identificação deste dado, como o nome do atributo, e mapear para a tag Name dentro de a marcação Node Criar a marca Childen dentro de a marcação Node;
Procurar por elementos que possuam relacionamentos com a estrutura principal, no arquivo XML de origem, criando, para cada um, uma marcação do tipo Child, no arquivo de destino;
Identificar o valor que representa o relacionamento entre os elementos, como o valor do atributo definido na tag Node, e mapear para a marcação Value da marca Child;
Se estes elementos não possuírem novos relacionamentos significa que irão representar os nodos folhas da visualização, sendo que deverá ser identificado o valor que representa a classificação, salvando os dados na marcação Result da tag Child;
Por outro lado, se os elementos possuírem outros relacionamentos, deve ser criada a marcação Node e o processo seguirá como exposto anteriormente.
A partir de o estudo da linguagem XML e das demais estruturas que compõe esta linguagem, apresentado no capítulo 4, foi possível definir o uso desta tecnologia para a criação dos arquivos de padronização entre algoritmos de mineração e técnicas de visualização.
Estes arquivos, cuja estrutura é definida através de documentos do tipo XML Schema, possuem estruturas próprias, sendo bem diferentes uns dos outros.
Afim de que haja a transformação automática dos dados dos arquivos XML, criados com base nos esquemas de mineração, para a estrutura de visualização, pode ser empregado o uso da técnica de XSLT, descrita na seção 4.3 deste documento.
Em este caso, é necessário que exista um arquivo de transformação para cada par de algoritmos de mineração e técnica de análise de informações.
Classificação. O mapeamento entre as marcações do arquivo de origem para o de destino é realizado quase que na proporção de um para um, ou seja, para cada marcação do documento XML do tipo Classificação deve existir uma similar no arquivo de Árvores de Decisão.
Para este mapeamento, é necessário a execução dos seguintes passos:
Recuperar, no arquivo de origem, a marcação Info, onde podem ser encontradas as informações referentes ao nome e descrição do conjunto de dados, algoritmo de mineração que foi utilizado e nome do atributo cujo valor representa a classe;
Criar, no arquivo de destino, a marcação Info copiando os dados referentes ao nome e descrição do conjunto de dados e algoritmo de mineração que foi utilizado;
Criar a marcação que representa o nome da visualização com um valor fixo que representa a técnica de Árvores de Decisão (Decision Tree);
Recuperar, recursivamente, todos as tags do tipo Attribute no arquivo de origem, criado a marcação Node no arquivo de destino;
Copiar o nome do atributo, armazenado na marcação Name dos dois arquivos;
Recuperar, no arquivo de origem, a marcação Results, que contém a lista de arcos e criar, no arquivo de destino, a tag Children;
Para cada resultado, identificado no arquivo de entrada por a marcação Result, criar no documento de saída a estrutura da tag Child;
Copiar o valor que identifica o arco, armazenado na marcação Value dos dois arquivos;
Se o resultado definir outro atributo no arquivo de origem, através da marcação Attribute, criar uma nova estrutura da tag Node no arquivo de destino;
Para a identificação da classificação dos registros, recuperar o valor da marcação Label, presente nos resultados do arquivo de origem, e copiar- la para a tag Result, definida dentro de a marcação Child, no arquivo de destino.
A Figura 40 apresenta uma parte do arquivo XSLT resultante deste mapeamento, sendo que o arquivo completo está disponível no Anexo II.
Tipicamente, os resultados dos algoritmos da classe Associação não podem ser visualizados como estruturas de Árvores de Decisão.
Para este tipo de análise podem ser criados diferentes tipos de visualizações.
Assim, foram utilizados os esquemas apresentados nas seções 7.2.1.1 e 7.2.2.1, onde é possível verificar as estruturas em as quais os dados estão dispostos, conforme ilustrado por a Figura 23 e por a Figura 35.
O tipo de visualização definido possui um atributo principal genérico.
Os atributos que compõem a cabeça das regras de associação são a identificação dos arcos.
Estes, estão ligados a nodos internos identificados por os valores dos atributos.
Os nomes dos atributos que compõem o corpo das regras estão definidos como os arcos que ligam os nodos internos com as folhas.
Estes últimos são representados por os valores dos atributos que definem o corpo.
Para este mapeamento, é necessária a execução dos seguintes passos:
Recuperar, no arquivo de origem, a marcação Info, onde podem ser encontradas as informações referentes ao nome e descrição do conjunto de dados e algoritmo de mineração que foi utilizado;
Criar, no arquivo de destino, a marcação Info copiando os dados referentes ao nome e descrição do conjunto de dados e algoritmo de mineração que foi utilizado;
Criar a marcação que representa o nome da visualização com um valor fixo que representa a técnica de Árvores de Decisão (Decision Tree);
Como as regras de associação não possuem relacionamentos entre si (não sendo possível identificar qual deve representar o nodo raiz da árvore de decisão), deve- se criar a estrutura da marcação Node com um valor genérico (Default Association Root) para a tag Name.
Criar a marcação Children;
Recuperar, no arquivo de entrada, a lista de regras de associação, identificada por a marcação Rules;
Para cada regra recuperada desta lista (tag Rule), criar, no arquivo de destino, a marcação Child;
Recuperar a estrutura que representa a cabeça da regra de associação no arquivo de entrada (tags Head\&gt; Attribute);
Concatenar o nome de todos os atributos que representam a cabeça da regra (tag Name) e salvar na marcação Value;
Criar o componente que representa um nodo interno no documento de saída, através de uma nova estrutura da marcação Node;
Concatenar o valor de todos os atributos que representam a cabeça da rega (tag Value) e salvar na marcação Name;
Recuperar a estrutura que representa o corpo da regra de associação no arquivo de entrada (tags Body\&gt; Attribute);
Criar, no arquivo de saída, a estrutura das marcações Children e Child;
Concatenar o nome de todos os atributos que representam o corpo da regra (tag Name) e salvar na marcação Value;
Concatenar o valor de todos os atributos que representam o corpo da regra (tag Value) e salvar na marcação Result.
A Figura 41 apresenta uma parte de código do arquivo XSLT resultante deste mapeamento, sendo que o arquivo completo está disponível no Anexo II.
Objetivando a validação dos documentos de transformação propostos, foram realizados alguns testes.
Assim, os resultados da execução de dois algoritmos de mineração de dados apresentados na seção 7.2.1, Apriori (Figura 26) e C4.
5 (Figura 33), foram utilizados como arquivos de entrada.
Após a execução dos arquivos de transformação, para cada um dos arquivos de origem, foi gerado um novo arquivo disposto na estrutura de Árvores de Decisão.
A Figura 42 exibe um resultado bastante interessante, visto que a análise de resultados de algoritmos de Associação como Árvores de Decisão não é uma visualização usual.
Por outro lado, a Figura 43, que exibe o resultado da transformação dos dados de um algoritmo de Classificação mapeado para a estrutura de Árvores de Decisão que, por a convergência das estruturas, criou um resultado já conhecido, conforme exposto por a Figura 36.
As duas figuras não ilustram os arquivos completos, que estão disponíveis no Anexo III.
Classificação. Esta visualização é exatamente a mesma gerada por a execução do algoritmo C4.
5 no software Weka.
Conforme apresentado no capítulo 5, o formato PMML também possui como objetivo a padronização dos resultados dos algoritmos de mineração, além de fornecer maneiras de documentar informações referentes à base de dados e à etapa de pré-processamento.
Um dos aspectos negativos apontados para o uso deste padrão é a ausência de um framework de desenvolvimento que facilite algumas operações.
Assim, visando complementar o uso dos esquemas para arquivos XML, propostos neste capítulo, desenvolveu- se um framework para manipulação destes formatos.
A idéia principal deste componente de software é fornecer uma maneira fácil e ágil para que os responsáveis por a programação de algoritmos de mineração e técnicas de visualização de dados possam manipular os documentos XML sem a necessidade de conhecer detalhadamente suas estruturas.
Este framework está dividido em duas partes, uma contendo classes utilitárias para o processo de mineração de dados e outra para o processo de geração da visualização.
A Figura 46 apresenta o conjunto de classes responsáveis por armazenar os dados de execução dos algoritmos.
O conjunto de classes disponíveis no framework está relacionado às categorias de algoritmos apresentadas neste documento, ou seja, Associação e Classificação.
Conforme pode ser visualizado no diagrama, cada categoria de algoritmo possui uma classe principal, sendo que esta extende o objeto abstrato identificado como Mining.
Ele é utilizado para que o processo de execução dos algoritmos possa ser definido de forma genérica e não possui nenhuma propriedade ou método.
Para os resultados provenientes de algoritmos de Associação, a classe principal para o armazenamento dos dados é denominada Association.
Ela possui a mesma estrutura definida para os arquivos XML correspondentes a esta técnica de mineração, ou seja, define uma propriedade que representa as informações do modelo e outra que armazena uma lista de regras.
A primeira é representada por a classe AssociationInfo, enquanto que a segunda é composta por objetos do tipo Rule.
Cada regra, conforme já discutido anteriormente, possui duas propriedades do tipo Attribute que representam a cabeça e o corpo.
De a mesma forma, a classe principal para os algoritmos de Classificação é denominada Classification.
Em a tentativa de recriar a estrutura dos arquivos XML seguindo o conceito de orientação a objetos, ela também define uma propriedade que representa as informações do modelo, denominada ClassificationInfo.
Além de isto, possui uma segunda propriedade responsável por mapear o atributo principal do modelo de classificação, representado por um objeto do tipo Attribute.
Este define uma lista com os possíveis resultados do atributo, composta por instâncias da classe Result.
A Figura 47 ilustra a estrutura de classes responsável por a execução dos algoritmos de mineração de dados.
A classe AbstractMining define todos os métodos principais para o uso de qualquer categoria de algoritmo, como a função de iniciar a execução do algoritmo e recuperar seus resultados.
Por outro lado, as classes abstratas específicas AbstractAssociation e AbstractClassification, definem métodos que são responsáveis por a extração dos dados seguindo a categoria do algoritmo.
Assim, a primeira define um método para recuperar a lista de regras de associação, enquanto que a segunda busca o atributo principal da classificação.
Estas classes são as responsáveis por criar o arquivo XML contendo o resultado obtido para o esquema correspondente.
Para adicionar novos algoritmos utilizando este framework, os autores devem criar novas classes filhas de uma das duas classes abstratas apresentadas anteriormente, de acordo com seu tipo.
Em a Figura 47, são apresentadas quatro exemplos de objetos que implementam as classes abstratas:
TertiusRunner, AprioriRunner, J48Runner e Id 3 Runner.
Elas são responsáveis por a execução, respectivamente, dos algoritmos Tertius, Apriori, J48 e ID3, detalhados na seção Seguindo a mesma lógica das classes de mineração de dados, a Figura 48 apresenta o conjunto de classes responsáveis por armazenar os dados necessários para a geração de visualizações.
O conjunto de classes disponíveis no framework também está relacionado à categoria Árvores de Decisão abordada neste documento.
A classe principal para o armazenamento de dados de visualização é a DecisionTree.
Sua estrutura é bastante similar à classe Classification discutida anteriormente, ou seja, também possui uma propriedade responsável por armazenar as informações de identificação da visualização e outra que define uma lista de nodos.
A primeira é representada por a classe DecisionTreeInfo, enquanto que a segunda é composta por objetos do tipo Node.
Além de isto, a classe Node define uma outra lista, responsável por armazenar todos os arcos de relacionamentos do nodo com outros, utilizando instâncias da classe Child.
Para a geração da visualização, deve ser utilizada como base a classe abstrata AbstractVisualization.
Ela define os métodos principais que devem ser utilizados para a leitura do arquivo XML, o qual contém os dados seguindo a estrutura definida para a técnica correspondente e posterior execução da visualização.
Os autores das técnicas de visualização, então, devem apenas implementar os métodos abstratos desta classe e, a partir de o objeto contendo os dados, gerar a sua visualização.
Conforme também é ilustrado por a Figura 48, é apresentado juntamente com o framework a classe DecisionTreeRunner.
Ela é responsável por a execução da visualização de árvores fornecida juntamente com o software Weka conforme detalhado na seção 7.3.
Além de isto, o diagrama apresenta a classe TransformUtils.
Ela é responsável por realizar a conversão entre os formatos de documentos XML, com base nos arquivos XSLT especificados anteriormente.
Para que novas classes de algoritmos de mineração sejam adicionadas ao framework, é necessário seguir os seguintes passos:
Criar uma nova classe abstrata que implemente a classe AbstractMining;
Implementar o método que é responsável por criar o arquivo XML para a categoria de algoritmo a ser adicionada;
Definir novos métodos abstratos para que as classes que irão executar realmente os algoritmos possam retornar os dados necessários para a criação dos arquivos;
Criar uma nova classe principal para o armazenamento dos dados que extenda a classe Mining.
Para que novos algoritmos de mineração de dados possam ser utilizados, é necessário seguir os seguintes passos:
Criar uma nova classe que implemente a abstrata correspondente à categoria de algoritmos desejada;
Implementar o método abstrato que responsável por o retorno da execução do algoritmo;
Implementar o processo de execução do algoritmo.
A adição de novas técnias de visualização de dados segue a mesma lógica descrita para algoritmos de mineração.
A diferença, naturalmente, é referente a a classe que devem ser implementada.
Em este caso, deve ser utilizada a classe AbstractVisualization.
Este capítulo discorreu sobre a solução proposta para a resolução do problema descrito anteriormente.
O primeiro passo para a criação desta solução foi a definição do formato em o qual está baseada a idéia de padronização entre as etapas do processo de descoberta de conhecimento.
Seguindo a mesma proposta do formato PMML, estudada no capítulo 5.1, foi definido que a melhor estrutura para atender a padronização seria o uso de XML Schema.
Inicialmente, trabalhou- se com a padronização do resultado das técnicas de mineração de dados.
Para isto, foram utilizados os estudos referentes aos algoritmos das classes Associação e Classificação.
Cada uma destas categorias teve a identificação do formato padrão de como o seu resultado é gerado e, a partir de isto, os esquemas dos documentos correspondentes foram criados.
A validação desta proposta foi realizada a partir de testes utilizando os algoritmos fornecidos por o software Weka.
O resultado destes testes foi importante no sentido de fornecer uma visão de um processo padrão para o mapeamento dos resultados para os arquivos Seguindo a mesma idéia, a geração das visualizações também tem a entrada de dados padronizada com o uso de arquivos XML.
Assim, a partir de o estudo da técnica de Árvores de Decisão, foi identificada a forma principal como os dados devem ser passados para o componente de visualização, sendo que o resultado disto foi a criação de um novo esquema, em XML Schema.
Para a validação, novos testes foram realizados a partir de a conversão dos arquivos que representam os resultados dos algoritmos de mineração para a estrutura de entrada da técnica de visualização.
Este novo documento XML serviu de base para a visualização de árvores fornecida com a ferramenta Weka.
Novamente, foi feita a definição de um processo padrão para o mapeamento.
Além de isto, para que a transformação entre os formatos acontecesse de forma mais independente possível, foram criados documentos do tipo XSLT, responsáveis por ler um arquivo XML e gerar um novo documento.
A conversão entre os formatos de Classificação e Árvores de Decisão aconteceu naturalmente, visto que as duas técnicas são complementares.
Porém, o mapeamento entre Associação e Árvores de Decisão necessitou de uma estruturação própria, gerando uma árvore em a qual os arcos representam os atributos que compõem a cabeça e o corpo das regras de associação, enquanto que os nodos representam os valores destes atributos.
Por fim, para facilitar o uso destes formatos, foi criado um framework de desenvolvimento.
A partir de o uso das classes definidas, novos algoritmos de mineração de dados e técnicas de visualização podem passar a utilizar os arquivos XML.
Todavia, para isso, é necessário que sigam o contrato estabelecido através das interfaces padrões fornecidas.
Este framework foi criado contendo as classes principais para manipulação dos algoritmos e visualizações estudados.
Conforme discutido ao longo deste trabalho, com o aumento do uso de ferramentas computacionais tem se observado uma grande geração de dados que representam o estado e comportamento dos processos diários das organizações.
Este fato está contribuindo para que o foco das empresas se desvie do mero acompanhamento e controle para ações de medição, análise e monitoração.
Em este sentido, o uso de técnicas de descoberta de conhecimento sobre estas bases de dados se mostrou uma prática bastante promissora, vez que as empresas começam a aprender sobre a forma de execução dos seus processos e, com isto, atuar diretamente em pontos problemáticos.
Porém, ferramentas deste tipo podem necessitar de muitos recursos computacionais e, dependendo da operação que está sendo executada, impactar em problemas para os usuários.
Desta forma, a opção que traz melhores desempenhos passa por um ambiente distribuído.
Este fato leva, então, ao problema que este trabalho se propôs a resolver:
Como realizar a troca de informações entre as etapas do processo de KDD, focando nas saídas do processo de mineração de dados e entradas para a geração da visualização?
O estudo do principal trabalho relacionado, apresentado no capítulo 5, permitiu uma visão sobre a melhor forma de criar esta padronização, ou seja, através do uso de documentos XML.
Esta tecnologia, apresentada no capítulo 4, é a mais indicada para troca de informações entre aplicações.
A proposta dos documentos de esquema, utilizando o padrão XML Schema, demonstrou, por meio de estudos de caso envolvendo resultados de algoritmos de mineração fornecidos gratuitamente por a ferramenta Weka, e por técnicas de visualização deste mesmo aplicativo, que os documentos XML gerados proporcionam uma forma de integração entre as etapas citadas.
Além de a simplicidade da proposta, o fato dos esquemas terem sido criados com base nos padrões das técnicas de mineração e visualização facilita a compreensão para a sua utilização.
A transformação entre os formatos, automatizada por o uso da técnica de XSLT, propicia a manipulação automatizada entre os arquivos XML, beneficiando desenvolvedores das aplicações.
Outro ponto importante a ser destacado é que, com o uso dos documentos propostos, técnicas que historicamente não eram compatíveis, como Associação e Árvores de Decisão, puderam ser utilizadas para a visualização de mesmos conjuntos de dados.
Esta proposta permite que, futuramente, novas classes de algoritmos de mineração sejam adicionadas para uso.
Para isto, basta que seja criado o esquema de definição da estrutura dos dados e o documento de transformação para as técnicas de visualização existentes.
Além de isto, novos algoritmos das classes utilizadas poderão ser incorporados à solução, agregando em diversidade de opções para a manipulação dos conjuntos de dados.
Outra proposta seria a melhoria da visualização de resultados de algoritmos de Associação utilizando a técnias de Árvores de Decisão.
O arquivo XSLT correspondente poderia identificar regras que possuem cabeças com mesmos atributos e criar um único nodo interno.
Os diferentes resultados, ou valores dos atributos, poderiam direcionar para nodos folhas contendo o corpo da regra ou nodos internos seguindo a mesma idéia.
De a mesma forma, outra possibilidade de trabalho futuro é a adição de novas técnicas de visualização de informações, tanto de novos tipos como Redes Bayesianas, como outras versões da categoria estudada.
Novamente, para que isto possa ser possível, é necessária a criação de um novo esquema referente a a estrutura de dados da visualização.
Em complemento, devem ser criados tantos documentos de transformação quantas opções de algoritmos de mineração disponíveis.
Em complemento a esta tese de dissertação, foi publicado mais um trabalho cientifico:
Um artigo na III Escola Regional de Banco de Dados.
Para dar continuidade ao trabalho, novos artigos estão sendo previstos para futuras submissões.
