Cada vez mais as organizações de software estão preocupadas com melhoria do seu processo e consequentemente do seu produto.
Para isso, as organizações utilizam modelos de maturidade, os quais indicam a coleta de métricas para o controle de seus processos.
No entanto, o esforço com relação a essas métricas está relacionado à sua intensa coleta e utilização e não é dada a devida atenção à qualidade dos dados das mesmas.
O impacto da falta de qualidade dos dados dessas métricas é refletido diretamente nos custos da organiza ção visto que as métricas embasam o processo de tomada de decisão o qual pode ser de baixa confiabilidade devido os seus dados de base também o serem.
Uma avalia ção adequada da qualidade desses dados é o primeiro passo para garantir que as métricas possam ser usadas com a devida confiabilidade.
Uma abordagem que pode auxiliar essa avaliação está relacionada ao uso de data provenance (proveniência de dados) associado a um mecanismo de inferência lógica.
Este trabalho propõe uma arquitetura para avaliação da qualidade de dados de esforço composta por quatro principais componentes:
Uma base de data provenance de métricas, 2- um modelo de inferência baseado em fuzzy logic, 3- uma base de dados para armazenamento de avaliações e 4- um modelo analítico para análise de histórico de qualidade de dados de esforço.
A contribuição deste trabalho é prover uma avaliação da qualidade dos dados de métricas de esforço em PDS, buscando evidenciar as razões da eventual baixa qualidade.
Através do modelo de inferência, é possível atribuir níveis de qualidade aos dados possibilitando assim a identificação daqueles que são efetivamente úteis para um processo de tomada de decisão confiável.
Além disso, de acordo com seus níveis de qualidade, os dados podem ser direcionados para diferentes tipos de acompanhamento do projeto, cujos níveis de exigência de qualidade podem ser distintos.
Segundo a medição é crucial para o progresso da ciência e da engenharia, pois sem a verificação empírica de dados de medições, teorias e proposições continuariam abstratas.
A medição de PDS (Processo de Desenvolvimento de Software) é prática comum nas organizações para, através de comparações entre os valores das métricas e as metas da organização, tirar conclusões sobre a qualidade do processo ou produto de software.
No entanto, muitas vezes não é feito um esforço para garantir a qualidade desses valores e conseqüentemente estes deixam de ser confiáveis.
Problemas com qualidade de dados é uma realidade nas grandes organizações, pois a preocupação é geralmente com a quantidade de dados armazenados e não com a qualidade dos mesmos.
As conseqüências da baixa qualidade são refletidas nos custos da organização, visto que as decisões tomadas são baseadas nesses dados.
Em o contexto de operações de software, pode ser armazenada uma grande quantidade de dados referentes às medições no processo de desenvolvimento.
Modelos de maturidade, como CMMI, indicam a coleta de métricas como meio para controlar e analisar processos, para sua contínua melhoria.
De entre as métricas geralmente coletadas em PDS (por exemplo:
Esforço, defeitos, tamanho, variação de cronograma), a métrica de esforço exerce um dos maiores impactos na gestão dos projetos, pois mede o tempo que a equipe precisou para desenvolver, revisar e/ ou consertar um produto.
Por isso, a avaliação de dados referentes a essa métrica merece especial tratamento.
A qualidade dos dados de métricas de esforço é especialmente complexa de ser avaliada porque os dados não são obtidos por instrumentos automatizados que possibilitem o apontamento direto de falhas na medição.
Os dados de esforço são dados lançados manualmente por os membros da equipe, normalmente em horas, e não obrigatoriamente logo ao término de cada atividade.
Os aspectos subjetivos e manuais de coleta tornam a avaliação da qualidade dos dados coletados uma tarefa complicada.
Tal tarefa exige a consideração de diversos fatores inerentes ao PDS para um julgamento menos propenso a falhas.
Outro aspecto que torna a avaliação de qualidade de dados de esforço uma tarefa não tão simples é a existência de uma variável que influência diretamente na avaliação:
O real conceito de &quot;bom «ou &quot;ruim».
Os dados estão inseridos em diferentes contextos, formatos, e sob diferentes propósitos de coleta, cujos aspectos adicionam relevante complexidade na definição do que pode ser considerado um dado bom ou um dado ruim.
São encontrados na literatura alguns trabalhos que visam a avaliação de qualidade de dados em diferentes contextos.
De entre eles se destacam:
AIQM (A methodology for in-formation quality assessment), ASM-IQS (Assessment of software mea-surement an information quality study), DQA (Data quality assess-ment), DQMIM (A data quality measurement informa-tion model), ORME-DQ (A framework and a methodology for data quality assessment and monitoring), MDB:
PA (Measuring data be-lievability:
A provenance approach) e Pedi (Process-embedded data integrity).
No entanto, nenhum de eles contempla de maneira adequada a avaliação da qualidade desses dados, obtidos especificamente de métricas de esforço de software, considerando as características inerentes a um PDS e atendendo às necessidades de gestores desse meio como, por exemplo, rapidez na avaliação e apontamento das potenciais causas dos problemas eventualmente encontrados.
Esta pesquisa visa oferecer suporte à essa avaliação de dados referentes a métricas de esforço de software atendendo às reais necessidades do processo de tomada de decisão desse contexto.
Além disso, focaliza em oferecer funcionalidade adequada e objetividade, e apontando causas de problemas através do uso de dimensões de qualidade e através da apresentação de níveis de qualidade (ótima, boa, razoável, muito ruim).
Este trabalho está organizado da seguinte forma.
O Capítulo 2 apresenta o referencial teórico necessário para o entendimento dos conceitos e técnicas adotados por esta pesquisa.
Fundamentação teórica Este capítulo tem o objetivo de oferecer um melhor entendimento sobre alguns conceitos envolvidos na avaliação de qualidade de dados no contexto de métricas coletadas durante o desenvolvimento de software.
Tendo em vista que o presente trabalho está focalizado em dados oriundos de medições de PDS, é apresentada uma breve descrição sobre:
PDS juntamente com modelos de processos;
Mensuração de PDS envolvendo a justificativa de utilização de métricas em modelos de maturidade como o CMMI e aprofundando conceitos de medições e metodologias utilizadas na validação dessas medições, Conceitos de qualidade de dados abordando questões de confiabilidade e validade e Conceitos que podem estar envolvidos em avaliação de qualidade de dados como lógica fuzzy e data provenance.
De maneira genérica, Pfleeger em entende um processo como um conjunto de tarefas ordenadas, ou seja, uma série de etapas que envolvem atividades, restrições e recursos para alcançar a saída desejada.
Mais especificamente, quando um processo envolve a elaboração de um produto de software, este processo (PDS) pode ser entendido como ciclo de vida do mesmo.
Isso porque este tipo de processo descreve a vida do produto começando da concepção, passando por a implementação, entrega, utilização e manutenção.
Sommerville em define que um PDS é um conjunto de atividades que resultam num produto de software.
Essas atividades podem envolver o desenvolvimento de software desde o início ou partir da extensão e modificação de sistemas existentes, configurando e integrando componentes específicos ao produto desejado.
O autor afirma que não há um processo ideal e muitas organizações desenvolvem seus próprios processos, buscando assim atender as suas necessidades específicas.
Existem diversos PDSs.
Entretanto algumas atividades são fundamentais e comuns a todos os processos, tais como:
Validação, o software é validado para assegurar que este faz o que o cliente deseja e (d) Evolução, onde o software deve evoluir conforme as mudanças que o cliente necessita.
Rocha em diz que um software consiste em conjuntos de informa ções em diferentes níveis de abstração, transformações e decisões relacionadas a essas transformações.
Também enfatiza que vários aspectos são fundamentais no desenvolvimento de software, mas para que um processo seja de qualidade, um requisito básico é que a atividade seja sistemática e passível de repetição, independente de quem a execute, bem como há a necessidade da qualidade do produto de software ser independente de quem o produziu.
O desenvolvimento deste trabalho é baseado nos conceitos de PDS de Sommerville e também na visão de qualidade de PDS de Rocha Sommerville em define modelo de processo como uma representação abstrata do processo de software.
Cada modelo representa um processo sob uma visão particular e assim fornece informações parciais sobre este.
Pfleeger em classifica modelos em dois tipos específicos e diferentes entre si:
Modelos que indicam um caminho que o desenvolvimento de um software deveria seguir e modelos que são descrições de como realmente é feito um software.
O autor entende que esses dois tipos de modelos deveriam ser semelhantes ou iguais;
Porém não é o que acontece na prática.
Modelar um processo resulta em importantes benefícios, pois o registro da descrição do desenvolvimento de software possibilita o entendimento sobre os recursos, atividades e restrições envolvidas no projeto.
A presença de um modelo auxilia na detecção de inconsistências, redundâncias e futuros problemas na execução das atividades do processo pois, conforme o modelo vai sendo desenvolvido, é possível analisar as atividades com mais detalhes e com enfoque maior nos seus objetivos.
De entre os modelos existentes, alguns são conhecidos como genéricos.
Esses são abordados a seguir de acordo com.
Modelo Cascata: O modelo Cascata foi o primeiro modelo proposto derivado de processos de engenharia de sistemas.
As atividades comuns a todos os processos, citadas anteriormente, são organizadas em fases:
Definição e Análise de Requisitos, Projeto, Implementação e Teste Unitário, Teste de integração e sistema e Manutenção.
O resultado de cada fase é um ou mais documentos que são aprovados.
Uma fase não deve começar antes que a anterior tenha terminado.
Modelo Evolutivo Iterativo: O modelo evolutivo é baseado na idéia de desenvolver uma implementação inicial, expor- la para o usuário comentar, para que assim o sistema seja refinado em várias versões até que o sistema mais apropriado seja desenvolvido.
O ciclo é baseado na execução de atividades de Especificação, Desenvolvimento e Validação, as quais são intercaladas e não ocorrem separadamente.
Modelo Espiral: O modelo espiral ao invés de representar o processo de desenvolvimento de software como uma seqüência de atividades com iterações entre atividades, o processo é representado como um espiral.
Cada volta no espiral representa uma fase do processo.
Cada volta desse modelo é composta por quatro etapas:
Determinar objetivos, Avaliar alternativas, identificar e resolver riscos, Desenvolver, verificar produto do próximo nível e Planejar a próxima fase.
Cada etapa é representada por um quadrante de um diagrama cartesiano.
O raio do espiral representa o custo acumulado do projeto.
Processo Unificado: O Processo Unificado (UP) é um modelo de processo de desenvolvimento de software que define quem, o que, quando e como é desenvolvido o software.
O RUP (Rational Unified Process) é baseado na linguagem de modelagem UML (Unified Modeling Language), com algumas diferenças.
O RUP combina a forma disciplinada do modelo Cascata e a responsabilidade dinâmica do modelo iterativo.
Por esse motivo, Sommerville afirma que RUP é um bom exemplo de modelo de processo híbrido, já que este une elementos de todos os modelos genéricos.
Em tempos de alta competitividade, a qualidade é fator essencial como vantagem competitiva e por isso as organizações buscam aprimorar seus processos para que estes resultem em produtos de qualidade, já que a qualidade do produto está diretamente relacionada à qualidade do processo por o qual este é desenvolvido.
Este comportamento depende da evolução e melhoria contínua do PDS, alcançadas através do uso de modelos de maturidade que guiam as organizações na obtenção do controle de seus processos e na evolução para uma cultura de excelência em gestão.
Os autores tratam esta iniciativa de melhoria contínua como modelos de SPI (Software Process Improvement) que objetiva a evolução do PDS.
As organizações buscavam a certificação em algum padrão logo no início da abordagem de qualidade de software.
Inicialmente o padrão mais utilizado foi ISO9000.
Porém este não tinha objetivos específicos para a área de software, surgindo então o modelo CMM que era voltado às organizações de software.
O CMM foi desenvolvido por o Sei (Software Engineering Institute) ligado à Universidade Canergie Mellon.
O objetivo foi estabelecer um padrão de qualidade para o software desenvolvido para as forças armadas americanas, pois estes eram em sua maioria ineficientes e entregues em prazos totalmente inesperados.
Esses modelos de maturidade indicam a coleta de métricas como meio para controlar e analisar os processos e obter essa melhoria contínua acima mencionada.
Atualmente um dos modelos de maturidade mais difundidos é o CMMI, estudado com maior detalhe a seguir.
O CMM foi substituído por o modelo CMMI onde a aplicabilidade não é restrita a processos de desenvolvimento de software.
O CMMI possui dois modos de representação:
Em est ágios (staged) e (2) contínuo (continuous) e para ambos, o elemento-chave é a PA (Área de Processo).
A representação em estágios é a mesma utilizada no SW-CMM que é o modelo do CMM voltado para software.
Esta representação considera os processos da organização como um todo e provê um caminho de melhorias com grupos de Pas, estabelecendo fácil migração do SW-CMM para o CMMI.
A Figura 2.1 ilustra os componentes do CMMI na representação em estágios, onde as Pas são organizadas em níveis de maturidade.
Dentro de as Pas estão definidos os objetivos genéricos e específicos, assim como as atividades ou práticas genéricas e específicas.
As características em comum (Common Features) definem as práticas genéricas.
O nível Inicial 1 indica um estado caótico, onde a organização tem um controle informal dos processos podendo estes resultarem ou não, produtos de qualidade.
O CMMI considera que toda a organização está, por definição, no nível 1.
O nível 2 é o gerenciado, focado nos projetos, sugerindo o gerenciamento dos processos de cada projeto, podendo estes serem diferentes entre si.
O nível 3, Definido, é focado na organização, ou seja, na padronização dos processos da organização através da análise dos processos dos projetos onde os mais bem sucedidos são institucionalizados.
O nível 4, Gerenciado Quantitativamente, sugere o gerenciamento quantitativo através da coleta e análise de métricas dos processos, de entre outras práticas, sendo esta a mais importante.
E por fim, o nível 5 é o Otimizado, onde prevê a melhoria contínua dos processos da organização.
A representação contínua utiliza níveis de competência para indicar a maturidade do processo, visa a melhoria individual de áreas de processos, escolhidas de acordo com as necessidades específicas da organização.
A Figura 2.2 ilustra os componentes do CMMI na representação contínua, onde para cada PA são definidos objetivos especí-ficos, os quais possuem práticas específicas.
Os níveis de competência são caracterizados por PA.
Em a representação contínua existem 6 níveis de competência numerados de 0 à 5, como pode ser visualizado na Tabela 2.2.
Cada nível de competência corresponde a um objetivo genérico e a um conjunto de práticas genéricas e específicas.
Cada PA é um grupo de atividades relacionadas que buscam atender um grupo de objetivos.
Em este contexto, processos dizem o que fazer &quot;ao invés de «como fazer».
A PA especifica objetivos e atividades que, ao serem executadas, espera- se que atinjam estes objetivos.
Alguns objetivos e práticas são específicos à PA, outros são genéricos e aplicados em todas as Pas.
Estes objetivos genéricos descrevem maneiras essenciais para que os processos possam ter um bom grau de repetição, padronização e controle adequado.
Independentemente do modo de representação, o modelo CMMI indica a coleta de métricas na PA de MA (Mensuração e Análise).
O papel desta PA é definir a coleta e análise de dados e relatar a informação aos respectivos projetos, proporcionando um retorno aos interessados.
A MA abrange 3 objetivos, sendo que destes, 2 são específicos e 1 é genérico.
Alinhar atividades de MA é o primeiro objetivo específico, com as seguintes práticas específicas: (
a) estabelecer objetivos de mensuração, (b) especificar as métricas, (c) especi- ficar procedimento de coleta e armazenamento de dados e (d) especificar procedimentos de análise.
Em outras palavras, preocupar- se em por quê/ o quê/ como medir e o que fazer com os resultados obtidos.
O segundo objetivo específico é prover resultados de mensuração.
As práticas específi-cas são: (
a) coletar dados da mensuração, (b) analisar dados da mensuração, (c) armazenar dados e resultados e (d) comunicar os resultados.
A preocupação está em seguir o planejamento e enfatizar a necessidade de comunicar os resultados aos interessados.
O terceiro objetivo, que é o genérico, é institucionalizar processo gerenciado com as seguintes práticas genéricas: (
a) estabelecer uma política organizacional, (b) planejar o processo, (c) prover recursos e (d) atribuir responsabilidades.
Este objetivo busca, portanto, a institucionalização da MA, diferentemente do que acontecia no CMM, onde MA era um componente para institucionalizar os processos:
Em o CMMI, MA é um processo a ser institucionalizado.
Esta institucionalização envolve treinamento de pessoas, gerenciamento de configurações, identificação e envolvimento de stakeholders e monitoração e controle de processos.
Esta PA recomenda que os dados específicos de cada projeto sejam armazenados num repositório.
O CMMI define que quando esses dados são largamente compartilhados entre os projetos, então esses dados devem ficar num repositório geral da organização.
Esse repositório é o resultante da integração de um plano de métricas organizacional e do OSSP (Organizational Standard Set of Processes) que é o conjunto de processos que toda a organização adota e considera como padrão para suas atividades.
O repositório contém medidas de processo e produto relatadas no OSSP, onde estão as informações dos processos que guiam as atividades da organização.
A melhoria do processo e do produto de software visa reduzir a probabilidade de erro, aumentar a produtividade e facilitar manutenção, entre outros objetivos.
São vários os fatores utilizados para verificar a qualidade de um software, como por exemplo a manutenibilidade, confiabilidade, flexibilidade, entre outros.
No entanto estes fatores são de caráter qualitativo e para se melhorar um processo é preciso medir- lo quantitativamente para eliminar a subjetividade desta atividade.
O uso de métricas de software reduz esta subjetividade para o alcance à qualidade do software, provendo uma base quantitativa para a tomada de decisão sobre o desenvolvimento do software.
Não obstante, o uso de métricas de software não elimina a necessidade de julgamento humano, ou seja, não elimina a necessidade da interpretação e criatividade das pessoas na avaliação do produto e processo Segundo Kan em, as métricas de software podem ser classificadas em três categorias:
Produto, processo e projeto.
As de produto descrevem as características do produto (e.
g tamanho, complexidade, peculiaridades do projeto, desempenho e nível de qualidade).
As de processo podem ser utilizadas para melhorar o desenvolvimento e a manutenção do PDS (e.
g defeitos encontrados e o esforço despendido para a sua correção, eficiência de remoção de defeitos).
As de projeto representam as características do projeto e da sua execução (e.
g número de recursos, custo, cronograma e produtividade).
O padrão da IEEE 1061 classifica as métricas de software em medidas diretas e medidas indiretas, e define que uma métrica direta é uma métrica que não depende da medida de nenhum outro atributo, nem necessita de validação.
Por sua vez, uma métrica indireta depende de outros atributos e necessita ser validada.
O padrão IEEE 1061 definiu os termos aplicados em medição de software a fim de padronizar- los, e alguns destes.
Os mais utilizados neste trabalho, estão listados a seguir:
Atributo: Uma propriedade física ou abstrata mensurável de uma entidade;
Fator de qualidade:
Um tipo de atributo, ou seja, um atributo orientado a gestão de software que contribui para sua qualidade;
Métrica: É uma função mensurável;
Métrica direta:
Uma métrica que não depende da medida de outro atributo;
Métrica de qualidade:
Uma função cujas entradas sejam dados do software e cuja saída seja um único valor numérico que possa ser interpretado como o grau que o software possui;
Métrica validada:
Uma métrica cujos valores estão sendo estatísticamente associados com o fator de qualidade correspondente;
Subfator de qualidade:
Uma decomposição do fator de qualidade ou um subfator para seus componentes técnicos;
Validação de métrica:
O ato ou processo de assegurar que uma métrica é confiável e realmente prediz ou avalia o fator de qualidade.
O padrão IEEE 1061 diz que uma métrica direta é pressupostamente válida.
Diferentemente, aponta que todas as métricas devem ser validadas.
Exemplos de métricas diretas são:
Número de defeitos, tamanho e número de linhas de código.
Exemplos de métricas derivadas são:
Produtividade $= número de linhas de código/ tempo· Densidade de defeitos $= números de defeitos/ tamanho· Estabilidade dos requisitos $= número de requisitos inicial/ número total de requisitos Em este trabalho, são adotados os termos de métrica e medida como sinônimos, como descritos acima.
Conforme KAN, alguns tipos de medidas são usadas com maior frequência em desenvolvimento de software, são elas:
Relação O resultado de uma relação é a divisão de uma quantidade por outra.
O numerador e o denominador são de populações distintas e são mutuamente exclusivos -- se um membro pertence à população referente a o numerador, então esse membro não pertencerá à população referente a o denominador.
Exemplo de métrica:
DensidadeDefeitosEntregues $= NumeroDefeitosEncontradosNoCliente TamanhoAtualProduto O numerador &quot;NumeroDefeitosEncontradosNoCliente «pertence à população &quot;Defeitos «e o denominador &quot;TamanhoAtualProduto «pertence à população &quot;Tamanho».
Proporção A proporção é diferente da relação porque o numerador é parte do denominador.
Exemplo de uma métrica baseada nesse tipo de medida é métrica derivada EfiCiência de Remoção de Defeitos:
EficienciaDeRemocão $= NumeroDefeitosInternos Note que o númerador &quot;NumeroDefeitosInternos «faz parte do denominador que é a soma da quantidade de defeitos internos com a quantidade de defeitos externos.
Porcentagem Uma proporção ou fração se transforma em porcentagem quando é expressa em termos de &quot;por 100 unidades».
Alguns autores recomendam que sejam medidas no mínimo 50 casos para se obter resultados sem distorções usando porcentagem.
Dependendo do número de categorias, Kan recomenda no mínimo 30 casos, que, segundo este mesmo autor, é o menor tamanho de população para análises estatísticas.
Um dos fatores mais importantes e críticos na utilização de métricas é a maneira por a qual os resultados são avaliados, o que depende substancialmente da validação das métricas.
O padrão IEEE define que, para uma métrica ser válida, esta deve ter um alto grau de associação com os fatores de qualidade que ela representa.
Em a documentação do padrão IEEE são descritos, detalhadamente, critérios para verificar quando uma métrica é válida ou não.
Em dezembro de 1992 foi aprovado o padrão IEEE 1061, IEEE Standard for a Soft-ware Quality Metrics Methodology, que é uma metodologia para identificar, implementar, analisar e validar métricas de qualidade de software, processos e produtos.
É importante salientar que o padrão é um processo e não especifica quais métricas utilizar.
Dentro de 5 anos um padrão deve ser revisado ou reafirmado, este trabalho é baseado na versão do padrão aceito em 1998 e reafirmado em 2004.
O padrão define que qualidade de software é um degrau através de o qual o software possui uma combinação de atributos.
Definir a qualidade de um sistema é equivalente a definir uma lista de atributos de qualidade &quot;exigidos «por o sistema.
Para medir estes atributos um conjunto apropriado de métricas de software deve ser estabelecido.
Entre outros aspectos, o uso do padrão IEEE 1061 auxilia uma organização a:
Estabelecer requisitos de qualidade para um sistema e suas saídas, Alcançar os objetivos de qualidade, Avaliar o nível de qualidade alcançada de acordo com os requisitos estabelecidos, Detectar anomalias ou potenciais problemas no sistema e Validar um conjunto de métricas.
Para isso, um conjunto de métricas deve estar representado num plano de métricas.
A norma define uma metodologia de validação de métricas através da definição de limites, critérios e um procedimento para adotar- los.
O apêndice A contém uma sumarização do padrão IEEE 1061 que pode ser acessado para maiores informações e detalhamento sobre suas definições para limites, critérios e aplicação do seu próprio procedimento de validação.
De acordo com o padrão, se uma métrica não passa em todos os critérios de validação, ela deve ser usada somente de acordo com o critério que ela passou.
Por exemplo, se uma métrica é considerada válida quanto o critério Rastreabilidade, ela deve ser usada somente para acompanhar a evolução (ou involução) do processo ou produto em questão.
O padrão também diz que é importante ressaltar que a validação das métricas deve ser aplicado em ambientes estáveis de desenvolvimento, ou seja, onde não muda com freqüência a linguagem de implementação, as ferramentas de desenvolvimento e outras especificidades do projeto.
Além disso, a aplicação e validação de métricas deve ser implementada durante as mesmas fases do ciclo de vida em diferentes projetos.
Por exemplo, se uma métrica A é coletada durante a fase de design do projeto P e tem relação com o fator de qualidade F, então uma outra métrica B seria coletada durante a fase de design do projeto Q e teria relação com o fator de qualidade F. Alguns pesquisadores também estudam questões relativas à validação de métricas.
Kaner em sugere um framework para avaliar métricas.
Este framework é composto por 10 perguntas.
O autor entende que respondendo essas perguntas, vários aspectos quanto a a definição das métricas serão esclarecidos, garantindo assim sua validade.
As perguntas a serem respondidas para cada métrica do plano de métricas de uma organização são as seguintes:
Qual a proposta da métrica?
Qual o escopo da métrica?
Que atributo estamos tentando medir?
Qual a escala natural do atributo que estamos tentando medir?
Qual a variabilidade natural do atributo?
O que é a métrica? (
a função atribuida ao valor do atributo) Que instrumento de medição usamos para fazer a medição? (
Exemplos de instrumentos:
Contagem, comparação, etc) Qual a escala natural da metrica?
Qual a variabilidade natural de leitura do instrumento? (
Erro de medição) Qual a relação do atributo com o valor da métrica? (
Questão da validade) Quais os efeitos naturais de usar esse instrumento?
Em o estudo desse framework foi aplicado com exemplos de métricas de defeitos.
Florac em também sugere uma avaliação de dados coletados de métricas.
Para essa avaliação o autor indica que os critérios estabelecidos por ele devem ser satisfeitos:
Critério 1: Dados devem ser verificados· Critério 2: Dados devem estar sincronizados· Critério 3: Dados devem ser consistentes· Critério 4: Dados devem ser válidos O critério 1 indica que os dados devem ser examinados para se assegurar que foram coletados de acordo com as especificações e não contêm erros.
Essa verificação avalia se os dados:
O critério 2 diz respeito à medições de atributos de um processo, produto e recursos para descrever o desempenho do processo.
Um exemplo de situação freqüentemente encontrada é quando relatos de esforço são preparados mensalmente enquanto que medidas de tamanho são relatadas em intervalos de semanas, sendo que as duas medições são utilizadas numa métrica.
O critério 3, de consistência, tem relação com o tempo, cuidados com definições que podem mudar de projeto para projeto, reclassificações de categorias e atividades durante o processo de medição.
O critério 4, de validade, verifica se os valores usados para descrever um atributo verdadeiramente descrevem o atributo de interesse.
Para isso, a definição das medições devem ser muito bem feitas.
Sem definições adequadas e consistentes para todos os momentos do processo de medição, nenhum dado pode ser considerado para representar um assunto de interesse.
Todas essas propostas descritas anteriormente estão preocupadas em garantir que as métricas possuam qualidade quanto a o seu propósito de uso, ou seja, se a métrica de defeito realmente contabiliza os problemas encontrados no produto e não apenas melhorias solicitadas.
De entre as propostas apresentadas nesta seção a Norma 1061 se mostra como a mais completa e detalhada através da definição de limites e critérios.
Enquanto que as outras não apresentam todo esse detalhamento e até mesmo maturidade em sua utilização.
Em a próxima seção são tratadas questões referentes à qualidade de dados que representam as métricas.
Diferentemente das propostas anteriores, a preocupação está em afirmar o quão verdadeiro é o valor associado à métrica, mesmo que já validada.
A qualidade de dados é uma questão crítica nas organizações devido a a grande quantidade de dados que elas armazenam.
Cada vez mais, a qualidade dos dados tem recebido atenção especial dos grupos de garantia de qualidade devido a os benefícios de alta confiabilidade que se pode alcançar.
Dados de alta qualidade podem melhorar a satisfação dos clientes se tratados estratégicamente como vantagens competitivas no mercado.
De forma generalizada, Strong Entendem dados de alta qualidade como dados acessíveis e &quot;amigáveis «para os seus usuários, sendo este um conceito amplamente adotado, inclusive por este trabalho.
Em este sentido, o autor classificou algumas características de dados de alta qualidade em quatro categorias baseadas em características:
Intrinsic DQ -- Categoria de aspectos intrínsecos, Accessibility DQ -- Categoria de aspectos de acessibilidade, Contextual DQ -- Categoria de aspectos de contexto e Representational DQ -- Categoria de aspectos de representação.
Cada categoria possui dimensões relacionadas ao tipo de aspecto tratado, como mostra a Tabela 2.3.
Strong em[ Strong et al.,
1997] sugere que um problema de DQ é qualquer dificuldade encontrada numa ou mais dimensões que endereçam a completude ou a falta de usabilidade do dado.
Em pode ser encontrada uma melhor definição do que é tratado em cada uma das categorias da Tabela 2.3, tais como:
Categoria &quot;Intrinsic DQ&quot;:
Onde são discutidas os tipos mais comuns de problemas, tratando- se de acurácia e objetividade do dado fonte;
Categoria &quot;Acessibility DQ&quot;:
Onde são discutidas as questões referentes à habilidade de se obter dados necessários num tempo adequado de maneira eficiente;
Categoria &quot;Contextual DQ&quot;:
Onde são discutidas questões de dados faltantes, incompletos ou definições inadequada dos dados;
Categoria &quot;Representational DQ&quot;:
Onde são discutidas questões de interpretabilidade, facilidade de entendimento e representação concisa e consisstente de dados.
O uso de dimensões para a observação da qualidade dos dados é um consenso entre a comunidade científica.
O que não é objetivo e rígido é a definição de quais dimensões devem ser consideradas, ficando esta decisão por conta dos analistas dos dados ligados ao seu contexto de uso.
Inicialmente os usuários dos dados, geralmente gestores, não sabiam a qual fonte atribuir os problemas de qualidade de dados, visto que agrupamentos inadequados de dados de fontes distintas eram, e ainda são, causas comuns de problemas tipo intrínsecos.
Os usuários apenas sabiam que o dado era conflitante e que a produção de ele podia ter um caráter subjetivo, evidenciando assim os problemas de confiabilidade.
Com o tempo, a questão evoluiu para a avaliação da acurácia das diferentes fontes, identificando assim uma baixa reputação das mesmas.
Essas fontes com baixa reputação começaram a ser vistas como fontes que agregavam pouco valor à organização, resultando então numa redução significativa do uso dos dados.
A Figura 2.3 ilustra esse raciocínio.
Como mostra a Figura 2.3, julgamento no processo de produção dos dados envolvendo a subjetividade é outra causa comum de problemas desta categoria.
Por exemplo, um dado interpretado é considerado de menor qualidade que um dado bruto, ou seja, um dado que não passou por interpretação e interferência humana.
Outro problema em potencial é a objetividade do dado, sobre os quais não se questionavam apenas os usuários que tinham conhecimento sobre o processo de produção dos dados.
O resultado geral de todos esses problemas é a redução de uso desses dados, tidos como suspeitos.
Gopal em realizou um estudo sobre fatores determinantes para o sucesso em medição de PDS.
Em este estudo o autor discute sobre a importância de uma implementa ção cuidadosa de um programa de métricas e a necessidade de se construir credibilidade sobre as métricas para o sucesso na melhoria de processo de software.
Em seu estudo, o autor propõe um modelo de fatores determinantes, com basicamente dois tipos:
O contexto organizacional -- o nível de comprometimento da organização com as métricas e ambiente técnico -- quão &quot;amigável «é o programa de métricas.
O ambiente em que as métricas estão inseridas deve ser não intrusivo, ou seja, o esforço gasto para coleta das informações das métricas não pode adicionar carga de trabalho significativa à organização.
De entre os fatores que compõem o ambiente técnico estão: (
a) Análise das métricas, (b) Treinamento, (c) Uso de Ferramentas automatizadas, (d) Coleta de métricas, (e) Procedimento de coleta de métricas e (f) Qualidade das métricas coletadas.
Esses estão diretamente envolvidos em fazer com que as métricas tenham maior acurácia, estejam no tempo certo e sejam fáceis de usar.
Destacamos os fatores referentes a: (
d) Coleta de métricas, que trata dos tipos de métricas que são coletadas e com qual frequência; (
e) Procedimento de coleta de métricas, que quanto mais otimizado, mais positivo é o impacto na qualidade das métricas, ou seja, melhora a acurácia e confiabilidade do dado e, conseqüentemente, aumenta a confiança por parte de os usuários que tomarão decisões com base nesses dados; (
f) Qualidade das métricas coletadas, onde a grande maioria de usuários de métricas concordam que estas devem ter precisão, relevância e deve prover informação útil.
Através do seu estudo, Gopal identificou que os gerentes de software precisam primeiro focar nos fatores do ambiente técnico e prover incentivos para os desenvolvedores de software usarem as informações de métricas nas suas ações diárias.
Uma vez usadas as métricas, então os gerentes podem focar no contexto organizacional buscando a melhoria de desempenho da organização.
Dadas as colocações acima, a qualidade dos dados é a questão que merece destaque por sua relevância identificada por vários pesquisadores, e também por ser objeto de estudo neste trabalho.
Em a visão de, confiabilidade e validade são os dois critérios mais importantes de qualidade de métrica.
Confiabilidade se refere à consistência de um número de medições usando o mesmo método de medição sobre o mesmo assunto.
Se medições repetidas são altamente consistentes ou até semelhantes, então o método de medição ou as definições operacionais de medição têm um alto grau de confiabilidade.
Por outro lado, se as variações entre medições repetidas são grandes, então a confiabilidade é baixa.
A confiabilidade pode ser expressa em termos de o desvio-padrão das medições repetidas.
Quando duas variáveis são comparadas, geralmente o índice de variação é usado.
Quanto menor o IV, mais confiável a medição.
É dado por a Equação 2.1: Ainda na visão de, validade existe quando uma medição ou métrica realmente mede o que se espera que ela esteja medindo.
Em alguns casos onde a medição não envolve um alto grau de abstração, por exemplo, a medição da altura do corpo, a validade é simplesmente a acurácia.
O autor ainda afirma que é importante esclarecer a diferença entre confiabilidade e validade.
Medições confiáveis não são necessariamente válidos e vice-versa.
A Figura 2.4 mostra graficamente a diferença entre os 2 critérios.
Se a proposta da medição é estar no centro do alvo, vemos que a confiabilidade parece um padrão não muito rígido de qual círculo estar, isso porque a confiabilidade é uma função de consistência.
Validade, por outro lado, é uma função de busca da &quot;mosca».
Em termos estatísticos, se o valor esperado (ou a média) está no centro do alvo, então é válido, se a variação é pequena em relação a o círculo todo, então é confiável.
Confiabilidade e validade são considerados termos de acurácia e exatidão respectivamente, quando não envolvem conceitos abstratos.
De acordo com existem diversas maneiras para avaliar a confiabilidade de medições empíricas, entre elas: (
a) método teste/ reteste, (b) método alternativa/ forma, (c) método divisão/ metades e (d) método da consistência interna.
De entre esses, o método adotado por o autor para realizar seus experimentos foi o teste/ reteste, por ser este o mais simples e que auxilia na interpretação das métricas de software, segundo o autor.
Apesar de a existência desses métodos, suas características puramente estatísticas não atendem algumas especificidades da avaliação de qualidade de dados num contexto espec ífico como na coleta de métricas de esforço.
Essas especificidades que tornam essa tarefa não trivial são discutidas com detalhe no Capítulo 3.
Por hora, estudamos outras alternativas para uma avaliação de qualidade dos dados de esforço adequada ao seu contexto, as quais são descritas na próxima seção.
De posse do conhecimento do impacto que uma organização pode ter como consequência do uso de dados de baixa qualidade, é possível inferir que uma avaliação dessa qualidade se faz necessária para apontar os pontos falhos dos dados para que a tomada de decisão não seja prejudicada por a falta de confiabilidade nos mesmos.
No entanto, essa avaliação é um processo que está diretamente relacionado ao ambiente de inserção do dado e aos seus usuários.
Ainda de acordo com, um dado pode ser usado para diferentes usos e, consequentemente, pode estar bem adequado para algum uso, nem tanto para um segundo e definitivamente inadequado para um terceiro uso.
Por isso, rotular a qualidade de um dado não é uma tarefa trivial, estando intimamente relacionada ao encontro de necessidades específicas de usuários também específicos.
Alguns usuários consideram que um dado é problemático se não possui acurácia, se não é atual, se o dado é de difícil interpretação e assim por diante.
Muitos usuários vêem dados com algum problema como de baixa qualidade;
Por outro lado é difícil encontrar um dado que não tenha pelo menos algum desses problemas.
Por isso, as estratégias de avaliação que permitam essa flexibilidade no apontamento de problemas são mais adequadas no contexto de DQ.
Essa flexibilidade diz respeito ao julgamento definitivo e rotulador, afirmando que o dado é bom ou ruim independente dos problemas que tem, ou dos que não tem, e nem na proporção e no impacto real do problema para o uso do dado.
Em se tratando de métricas de software, esforço mais especificamente, a necessidade dessa flexibilidade se mantém.
Uma estratégia que pode ser considerada, neste caso, é o uso de lógica já que ela lida com aproximações e níveis de pertinência, isenta de caráter rotulador.
A próxima seção introduz o assunto através de conceitos básicos, teoria de conjuntos e operações realizadas com essa lógica, para o processamento do conhecimento através da mesma.
O termo fuzzy em inglês pode ter diferentes significados dependendo do contexto abordado, mas o conceito básico está geralmente ligado a algo vago, indistinto, incerto.
Em a língua portuguesa ainda não existe um consenso sobre a melhor tradução, mas os termos difuso e nebuloso são os mais utilizados e portanto os mais populares na comunidade.
Por este motivo, neste trabalho é considerado o termo em inglês (fuzzy).
Em a lógica clássica bivalente, um elemento x do universo de discurso U é pertencente a um dado conjunto A se esse satisfaz determinada propriedade e não pertence se não a satisfaz, conforme pode ser visualizado na Função 2.2: De acordo com, em situações reais de avaliação, existem propriedades que são vagas, incertas ou imprecisas e impossíveis de serem caracterizadas por predicados da lógica clássica bivalente.
A teoria dos conjuntos fuzzy pode ser vista como uma extensão da teoria clássica e foi criada para tratar graus de pertinência intermediários entre a total e a não-pertinência.
Assim, na lógica fuzzy, o grau de pertinência de um elemento em relação a um conjunto é definido por uma função que assume como valor qualquer número real pertencente ao intervalo fechado, algebricamente mapeada por 2.3: Em o processo de tomada de decisão (ou avaliação) os humanos relacionam seu conhecimento a fatos com características incertas sem definir um resultado exato, por exemplo o uso de &quot;mais quente «ou &quot;menos quente «no lugar da bivalente classificação quente/ frio.
Para um melhor entendimento de como funciona a representação fuzzy de conhecimento é interessante abordar três diferentes tipos de incertezas que podem estar caracterizadas no processo de decisão humano: (
a) estocástica; (
b) léxica e (c) lingüística.
A incerteza estocástica é um princípio de incerteza matemática que lida com a incerteza quanto a a ocorrência de um determinado evento, por exemplo:
&quot;A probabilidade de acertar o alvo é de 0,8».
O evento &quot;acertar o alvo «é bem definido e a incerteza está em quando isto ocorrerá ou não.
Essa incerteza é quantificada por um grau de probabilidade, podendo ser assim processada usando métodos estocásticos como por exemplo redes Bayesianas.
Outro tipo de incerteza é a léxica que trata com a imprecisão presente na maioria dos conceitos humanos que derivam suas conclusões, como exemplo &quot;homem alto», &quot;dia quente», onde não existe uma definição fixa do que seja um homem alto ou um dia quente.
Além disso, a realidade do humano que está concluindo tais situações tem um papel importante já que para uma criança o conceito de homem alto é diferente de uma pessoa adulta.
Até mesmo para pessoas adultas o conceito de &quot;homem alto «pode variar.
Um exemplo de afirma ção baseada nesse tipo de incerteza é &quot;Provavelmente nós teremos um ano financeiro de sucesso».
Em um primeiro momento esta afirmação pode parecer muito similar à primeira (estocástica), no entanto, existem diferenças significativas, tais como:
O evento propriamente dito não está definido (para algumas companhias um ano financeiro de sucesso pode ter um sentido e para outras, outro); (
2) nessa afirmação não existe uma quantificação probabilística, logo essa probabilidade é percebida mas não matematicamente explícita como na estocástica.
Afirmações do tipo léxica têm um importante papel no processo humano de tomada de decisão, apesar de não apresentarem conteúdos quantitativos.
Em muitos casos, a incerteza adicionada através do uso de palavras adicionam um certo grau de flexibilidade.
Essa flexibilidade e a sua definição são características da incerteza linguística, que na verdade, é uma caracterização da incerteza léxica com uma certa flexibilidade de conceitos.
Essa flexibilidade pode ser notada no seguinte exemplo:»
Sindicatos e indústrias querem alcançar o mesmo objetivo:
Um adequado aumento de salários.
O problema está quando cada um deve se manifestar sobre o que é um salário adequado».
A lógica fuzzy permite a representação do conhecimento humano considerando a incerteza léxica fazendo uso de artifícios linguísticos e de marcações matemáticas através de cálculo de probabilidades.
Para essa representação fuzzy de conhecimento são utilizadas regras de produção fuzzy, variáveis linguísticas, funções de pertinência, operações fuzzy (aqui são exploradas apenas as básicas) e modelos de inferência fuzzy.
Cada um desses conceitos é apresentado a seguir.
Uma base de conhecimento fuzzy geralmente é construída por meio de regras de produ ção fuzzy.
Uma regra de produção é normalmente formada de duas partes principais se antecedente\&gt; então conseqüente\&gt;, onde o antecedente é composto por um conjunto de condições que, quando satisfeitas (mesmo que parcialmente), determinam o processamento do conseqüente da regra por um mecanismo de inferência fuzzy.
O conseq üente é composto por um conjunto de ações que são geradas com o disparo da regra (satisfação do antecedente).
Como exemplo de regras de produção considere um controlador de força de um guindaste:
SE Distancia $= baixaENTÃO Força $= alta Em a construção das regras de produção, devem ser definidas variáveis lingüísticas que fazem parte dos antecedentes e dos conseqüentes.
As variáveis lingüisticas são entidades utilizadas para representar de modo impreciso, e portanto, lingüístico, um conceito ou uma variável de um dado problema.
Em o exemplo utilizado acima (controlador da cabeça de um guindaste) as variáveis lingúisticas utilizadas são Distância e Força.
Para cada variável lingüistica devem ser definidos os possíveis valores que ela pode assumir, chamados termos.
Considerando ainda o exemplo anteriormente citado, podem ser definidas as seguintes variáveis linguisticas (Distância, Ângulo e Força) com os seus respectivos termos da seguinte forma de acordo com:
A função de pertinência é a valoração dos termos definidos para cada variável lingüística.
Para um dado elemento x do universo de discurso, o valor de pertinência µA, calculado por a função de pertinência, representa o quanto este elemento satisfaz o conceito representado por o conjunto fuzzy A. A Figura 2.5 mostra a definição gráfica de uma função de pertinência para o exemplo utilizado acima.
Para cada entrada do sistema de inferência fuzzy são calculados todos os valores de pertinências daquela entrada para cada função de pertinência.
Por exemplo, se o sistema de inferência recebe como entrada 12 metros para a variável linguistica Distância a produção das regras terá o seguinte processamento através das funções de pertinência:
A Distância de 12 metros é um membro de cada termo no grau de 0.1 para MuitoLonge, no grau de 0.9 para Médio, no grau de 0 para Perto, no grau de 0 para Zero e no grau de 0 para Longe, conforme pode ser visualizado na Figura 2.5 onde está a linha pontilhada, onde a abcissa (eixo x) é a distância em metros e a ordenada (eixo y) é o valor fuzzy.
Estudos referentes a Data Provenance (Proveniência de dados) abordam a problemática de conhecer o processo de obtenção de dados, no sentido de fonte, tempo e confiabilidade.
Domínios científicos usam diferentes formas de DP (Data Provenance) e com diferentes propósitos.
Dependendo desse domínio, DP pode ser descrita de maneira diferente.
Em Ciência da Computação, provenance -- também chamada de &quot;lineage «ou &quot;pedigree «-- descreve a fonte e/ ou a derivação do dado.
Em o contexto de Banco de Dados, Buneman define DP como a descrição das origens do dado e o processo por o qual este foi produzido para que chegasse na base de dados.
O autor ainda afirma que é essencial na avaliação da qualidade de uma base de dados o conhecimento de onde vem os dados de origem.
Já no contexto de SIG (Sistemas de Informações Geográficas), Lanter citado por caracteriza como linhagem informações que descrevem as transformações que o dado sofreu.
Outros autores acrescentam que DP pode ser vista como metadados que registram o processo de experimentos e as anotações sobre a execução dos mesmos.
Para os propósitos da presente pesquisa, DP é considerada como toda informação sobre o dado que contribui na determinação do processo de derivação e/ ou transformação do mesmo, abrangendo toda influência externa sofrida desde a sua forma original até a sua forma considerada final.
A Figura 2.8 mostra uma taxonomia das características de DP, criada por Simhan em, que facilita a compreensão do seu uso nos diferentes contextos.
A taxonomia acima ilustrada mostra os principais propósitos de uso de DP (Use of Provenance) podendo ser voltada à preocupação com qualidade de Dados, auditorias, como recurso de replicação de dados, ou de propósito de atribuições de dados e/ ou de informações.
Em a presente pesquisa o uso que se aplica para é para Data Quality (qualidade de dados).
O domínio aplicado de DP (Subject of Provenance) pode estar orientado tanto a dados (Data Oriented) quanto a processo (Process Oriented), o que é indicado por o círculo pontilhado na Figura acima.
Independente dessa orientação, a granularidade de coleta e armazenamento dos dados é uma decisão importante no uso de DP, podendo esta ser mais detalhada (Fine Grained) ou menos detalhada (Coarse Grained).
Esse nível de detalhamento pode influenciar diretamente na utilidade dos dados coletados e no custo de manter a proveniência dos dados.
De acordo com Simhan em, o custo pode ser inversamente proporcional à granularidade de coleta, ou seja, quanto menor a granularidade, mais custoso é o processo de rastrear DP.
A granularidade mais abrangente é onde se registra um histórico completo da derivação de um determinado conjunto de dados.
Essa abordagem é chamada &quot;workflow «ou &quot;coarse-grain provenance».
Outra granularidade possível de ser utilizada é chamada &quot;fine-grain provenance», um cálculo da derivação de parte do dado resultante.
De acordo com essa abordagem tem algumas vantagens com relação a a coarse-grained nos seguintes sentidos:
Em a maioria dos casos, o workflow inteiro pode ser bastante complicado de se rastrear;
Todo workflow pode não estar disponível;
A mais simples caracterização de um workflow pode ser o log ou registro de ações em componentes individuais da base de dados.
Dentro de a abordagem &quot;fine-grain provenance», descreve diferC entes tipos de proveniência:
Where-- provenance (onde-proveniência) e why-- provenance (por que- proveniência).
Where-- provenance se refere à justificativa para um elemento aparecer no resultado de uma consulta, no sentido de esclarecer de onde o dado surgiu.
Considerando o seguinte exemplo extraído de:
Assumindo (Kim, Cs) como resultado da execução da query, o where-- provenance de &quot;Kim «é o atributo name de alguma tupla de Emp onde o valor é Kim.
A identidade de qual tupla de Emp cujo valor é &quot;Kim «é feito através do why-- provenance de (Kim, Cs).
Why-- provenance de (Kim, Cs) envolve a query SQL e mais a tupla de Emp e a tupla de Dept com as seguintes propriedades: (
a) satisfaz a cláusula where da query, (b) o atributo name da tupla de Emp é &quot;Kim «e (c) o atributo dname da tupla de Dept é &quot;Cs».
Para a representação, armazenamento e visualização das informações referentes a proveni ência dos dados existem diferentes técnicas.
A escolha da melhor maneira de coletar e armazenar a proveniência está relacionada ao uso pretendido dos dados, o custo, a granularidade e até mesmo os usuários finais desses dados.
Algumas dessas técnicas são apresentadas abaixo:
Chimera é um sistema que provê um catálogo de dados virtuais para representar os dados derivados e o procedimento de derivação dos mesmos.
As principais motivações para desenvolver essa aplicação foi a possibilidade de encontrar erros em dados derivados e manter um registro da história de construção dos dados.
Para testar a aplicação, foram usados dados científicos na área de Física e Astronomia, os quais encontravam- se armazenados em arquivos.
Após os dados serem coletados desses arquivos, transformados, calculados e gerados foi criada uma base de dados réplica contendo o processo (lineage -- linhagem) por o qual cada dado passou, o que agregou um custo e tempo no registro da proveniência dos dados.
Esta aplicação é baseada na abordagem do tipo coarse-grain provenance e utiliza Annotation, que é uma técnica geralmente adotada em abordagens desse tipo.
Poesia é uma aplicação de DP que utiliza ontologias para correlacionar escopos dos dados e a granularidade dos dados estimando, assim, a proveniência dos dados.
Essa abordagem expressa os dados e os efeitos das opera ções de integrações entre conjuntos de dados.
A motivação para o desenvolvimento dessa aplicação foram problemáticas relacionadas a aplicações da agricultura, que se utiliza de dados de medições meteorológicas, muitos de os quais são produzidos por cálculos.
O conhecimento das fontes originais do dado derivado e o processo de derivação são determinantes na confiabilidade das análises das medições, pois estes dados são oriundos de bases de dados heterogêneas e são sumarizados num data warehouse, podendo neste processo de transação serem inseridos ruídos nos valores.
A abordagem fine-grain provenance foi o tipo de DP considerada.
Trio é uma aplicação de DP que rastreia informações de proveniência em dados armazenados em Data Warehouse, que considera acurácia e dados de proveniência como componentes inerentes.
A autora deste trabalho defende que atividades de mineração e atualização em dados de DW motivam a rastreabilidade da proveniência dos dados, e que qualquer sistema científico que usa queries em bases de dados, funções de workflow e atividades de transformações de dados podem aplicar técnicas de DP.
Para cada um dos ítens do Trio (dado, acurácia e proveniência) são definidos tipos de informações que são rastreadas e em qual granularidade essa rastreabilidade corre.
Esta aplicação é baseada na abordagem fine-grain provenance.
Foi testada num exemplo específico de Biologia, que consiste na coleta e armazenamento de dados relacionados a observações da localização dos pássaros numa época específica do ano usado para entender tendências nas populações de pássaros e correlacionar a vida dos pássaros com condições do ambiente.
Em a implementação, os dados coletados são modelados de acordo com uma linguagem de modelagem especí- fica para suportar o Trio e para serem consultados via query-- inverse que é um tipo de consulta específica dessa área de DP, onde são utilizados recursos de bancos de dados para utilizar- la.
Em esse trabalho a autora ressalta a aplicabilidade de fine-grain provenance em domínios como Biologia, Química e Astronomia.
Buneman Aplicam técnicas de DP para capturar linhagem dos dados quando estes são copiados manualmente de várias bases de dados fonte para uma base destino.
Essa aplicação captura a linhagem do dado paralelamente à alteração do dado na base destino dentro de o domínio de Bioinformática.
Os fatores motivacionais para aplicação de DP foram situações onde pesquisadores de biologia copiavam dados de diversas fontes, muitas vezes online, e colavam numa base única para prover visão unificada dos dados, e passado determinado tempo, os pesquisadores precisavam ter conhecimento de como determinados dados foram gerados, o que não era possível.
Além disso, como alternativa, os pesquisadores tentavam buscar novamente os dados nas fontes, e estes já haviam sofrido atualizações.
Esta é uma aplicação baseada na abordagem fine-grain provenance e implementa provenance queries para o registro da linhagem.
Em esse trabalho o autor afirma que a aplicação de DP é uma proposta que melhora a conabilidade e a transparência de registros de dados cientícos.
A Tabela 2.6 ilustra as principais diferenças entre as técnicas considerando o domínio de aplicação e algumas das características presentes na taxonomia anteriormente apresentada, entre elas:
O propósito de uso (Use of Provenance na taxonomia), a granularidade (Subject of Provenance na taxonomia) e a técnica adotada (Provenance Dissemination na taxonomia) para a representação, armazenamento e visualização de DP.
A análise da tabela possibilita observar que em diferentes domínios os propósitos podem coincidir ou não e as granularidades também podem variar.
De entre outros propósitos, na tabela é possível concluir que os mais comuns são:
Qualidade de dados:
Qualidade e a confiabilidade do dado pode ser baseada nos dados fontes e nas transformações que estes sofreram para produzir um dado final;
Auditoria de dados:
Para a auditoria de dados é útil o rastreamento do dado, a determinação das fontes usadas e a detecção de erros na geração dos dados;
Replicação de dados:
O conhecimento detalhado sobre a proveniência do dado pode permitir a repetição da derivação do dado, ajudando a manter a acurácia do mesmo Tendo em vista todas as características aqui apontadas, Data Provenance se mostra uma alternativa bastante promissora na área de Qualidade de Dados e é portanto, efetivamente adotada na proposta de solução para o problema a seguir apresentado.
Em este capítulo foram expostos conceitos envolvidos na medição de PDS.
Foram descritos alguns modelos de processos e ilustramos a idéia de medição de software a partir de um modelo de maturidade largamente utilizado e conceituado como o CMMI.
Além disso, foram estudadas propostas de validação de métricas utilizadas na medição de PDS cuja abordagem é voltada ao funcionamento eficiente das métricas.
Também foram estudados conceitos relacionados a lógica fuzzy e data provenance que servem de base para o entendimento da sua utilização como alternativa na solução do problema abordado por o trabalho.
Portanto, todos os conceitos estudados nesta Fundamentação Teórica contribuem para o entendimento do desenvolvimento seguinte do trabalho apresentado nos Capítulos seguintes.
A seguir é discutido o cenário de pesquisa onde a abordagem é voltada à eficiência e confiabilidade dos dados que representam as métricas, estando elas validadas ou não.
Após a apresentação do cenário, é exposto o problema corrente e os requisitos de uma solução que elimina ou ameniza os problemas encontrados no cenário.
Cenário Este capítulo descreve o cenário onde a pesquisa está inserida.
Apresenta o fluxo de coleta de métrica de esforço num projeto real e descreve uma análise estatística realizada para observar os problemas existentes nesse fluxo.
Uma vez apresentados esses problemas, são apontadas algumas características essenciais para que uma solução sirva de alternativa para amenizar ou eliminar esses problemas.
Essas características, que chamamos de requisitos da solução, nortearão o estudo de trabalhos relacionados (Capítulo 4) e, conseqüentemente, o desenvolvimento da solução proposta (Capítulo 5).
A Hp (Hewlett--Packard Company Brasil Ltda) -- empresa de renome internacional, possui no TECNOPUC -- Parque Tecnológico da PUC em Porto Alegre RS, uma operação de software, onde são realizadas atividades de desenvolvimento de software.
Visando atrair mais clientes e adquirir maior credibilidade nesse segmento, em 2002 a organização iniciou a busca por o CMM3.
Em novembro de 2005, após um grande esforço por parte de a organização e seus colaboradores, foram alcançados os requisitos para elevar- la ao nível 3 do CMM, sendo parte deste esforço em parceria com a PUCRS.
Um dos aspectos que merece destaque com relação a esses requisitos, é a utilização de um repositório central de métricas coletadas de diferentes projetos da organização, chamado de BO (Base Organizacional).
Este repositório central tem por objetivo prover uma visão unificada da realidade dos projetos, visando auxiliar os gestores no processo de tomada de decisão.
A pesquisa do presente trabalho está focada nas circunstâncias de coleta e armazenamento das métricas.
Para isso são utilizados dados referentes a um projeto de manutenção específico da Hp, que chamaremos de P1 por questão de sigilo.
O PDS do projeto P1 é o modelo cascata orientado a entregáveis.
O projeto P1 utiliza quatro ferramentas em seu gerenciamento, sendo que para o lançamento de horas de esforço é utilizado um software desenvolvido localmente.
A Figura 3.1 ilustra o fluxo que a métrica percorre desde o seu lançamento até o seu armazenamento no repositório central nos projetos da organização.
Os colaboradores atuantes nos respectivos projetos obtêm as métricas no PDS ilustrado como a primeira etapa na Figura.
Como segunda etapa ocorre o registro dessas métricas nas diferentes ferramentas que o projeto tem para apoio no gerenciamento, são elas:
MS Project da Microsoft e Project and Portfolio Management (PPM) da Mercury (cronograma);
IBM Rational Clear Quest da IBM (acompanhamento e controle de defeitos) e IBM Rational RequisitePro da IBM (requisitos).
Em a terceira etapa ocorre a construção de pacotes onde são agrupados os dados mensais, proporcionando uma visão do projeto naquele período.
A quarta etapa é ilustrada por o armazenamento dos pacotes no repositório central para, na quinta e última etapa, serem extraídos e apresentados aos gerentes e gestores.
Através de um estudo realizado nessa operação de software foi possível identificar algumas dificuldades em relação a a confiabilidade dos dados de esforço.
Para evidenciar esses problemas de qualidade dos dados foi executada uma avaliação estatística por Berardi em, baseada em correlação linear para observar se existia ruptura na homogeneidade dos dados ao longo de as medições realizadas no projeto P1.
Como resultados dessa avaliação, identificou- se uma mudança de comportamento dos dados quando estes eram coletados por processos de coletas distintos.
A avaliação executada está descrita abaixo.
A métrica escolhida para essa avaliação em questão foi a de esforço de retrabalho pré-release, que registra o número de horas que a equipe do projeto gastou para realizar consertos de defeitos detectados antes da entrega da release.
Esta métrica apresenta informações bastante relevantes para a organização.
Por exemplo, tempo gasto em retrabalho significa prejuízo para a organização no sentido de que a equipe não está produzindo, e tempo que não é gasto para produzir é sinônimo de desperdício para a organização.
Além disso, o conhecimento do valor dessa métrica pode apontar falta de qualidade na equipe do projeto.
Tendo em vista a importância dos dados dessa métrica, é de total interesse da organização ter confiança nos respectivos dados, independente da forma de coleta dos mesmos.
A falta de qualidade desses dados pode ter impacto significativo dentro de a organização, e ter como conseqüência o desuso do repositório central de métricas, o que geralmente acontece com os sistemas que dão esse tipo de suporte à decisão.
Conforme discutido na Fundamentação Teórica (Capítulo2), existem vários métodos para se avaliar a confiabilidade de dados de medições, e o utilizado neste caso é o teste/ reteste.
O teste/ reteste consiste em obter uma medição de um conjunto de indivíduos e, após todos os indivíduos serem medidos, obtém- se outra medição imediamente após a primeira (ou ainda, algum tempo depois), sendo que esta distância entre a primeira medição e a segunda deve ser igual para todos os indivíduos.
De posse dos dois conjuntos de medições dos mesmos indivíduos, calcula- se o índice de correlação entre os valores desses conjuntos.
O índice de correlação indicará o grau de confiabilidade dos dados, ou seja, quanto maior o índice, maior a confiabilidade.
Baseado nesse método, avaliamos o comportamento dos dados da métrica de retrabalho pré-release, comparando os índices de correlação obtidas com medições de cada processo de coleta utilizado (no caso, chamaremos de processo de coleta A e processo de coleta B).
O objetivo é rejeitar ou aceitar a seguinte hipótese:
O índice de correlação entre os dados das métricas deve se manter alto quando esses são coletados através do mesmo processo e através de processos diferentes.
Se a hipótese for aceita, ou seja, se o índice de correlação entre os dados de medições se manter alto, independente do processo de coleta, então significa que o comportamento dos dados se mantém homogêneo.
Por outro lado, se a hipótese for rejeitada, ou seja, se o índice de correlação diminuir conforme o processo de coleta se altera, então é possível inferir que a qualidade dos dados no que diz respeito a confiabilidade está dependendo do seu método de coleta, o que pode ser uma característica prejudicial para os gestores na tomada de decisão, pois esta fica dependente da eficiência da coleta dos dados das métricas e conseqüentemente dos dados coletados.
Para realizar essa comparação, foram definidos os seguintes passos:
Definir os indivíduos de os quais são utilizadas as medições para avaliação.
Em o contexto de mensuração de PDS, os indivíduos podem ser de diferentes releases em que a equipe de projeto está envolvida.
Para o processo de coleta A:
Obter uma medição da métrica de esforço de retrabalho pré-release para cada um dos indivíduos definidos.
Obter outra medição (medição 2) da métrica de esforço de retrabalho pré-release para os mesmos indivíduos em (a).
Calcular o índice de correlação entre os dados desses dois conjuntos de medições.
Para o processo de coleta B:
Obter uma medição (medição 3) da métrica de esforço de retrabalho pré-release para cada um dos indivíduos definidos.
Obter outra medição (medição 4) da métrica de esforço de retrabalho pré-release para os mesmos indivíduos em (a).
Calcular o índice de correlação entre os dados desses dois conjuntos de medições.
Calcular o índice de correlação entre os dados das medições obtidas através do processo de coleta A com os dados das medições obtidas através do processo de coleta Através do passo 1 são definidos os objetos de medição de acordo com as especificidades do contexto, garantindo assim que ao longo de toda a avaliação são considerados os mesmos indivíduos.
O objetivo dos passos 2 e 3 é avaliar se o comportamento dos dados é homogêneo quando se utiliza o mesmo processo de carga.
Já no passo 4, o objetivo é avaliar se os dados, coletados de diferentes maneiras, apresentam homogeneidade e por conseqüência, confiabilidade.
Para realizar a comparação dos dados da métrica de retrabalho pré-release, coletados com os diferentes processos de coleta, foram efetuadas duas avaliações.
Como pode ser visualizado na Tabela 3.1, para utilizar o método Teste/ RETESTE foram utilizados pares de medições da métrica em questão.
A primeira avaliação consiste em utilizar três releases do projeto P1, duas medições que foram realizadas através do processo de coleta A -- as quais ocorreram nos meses X e Y -- e, as duas medições que foram realizadas através do processo de coleta B -- que ocorreram nos meses P e Q. Em a segunda avaliação (Avaliação 2) são utilizadas as mesmas releases com a adição de duas novas e por causa de essa adição foi necessário utilizar diferentes medições (no caso, mês Z e W) pois nas medições anteriores essas releases não tinham medições coletadas.
As duas medições realizadas através do processo B são as mesmas utilizadas na Avaliação 1.
Decidiu- se realizar a segunda avaliação para obter um conjunto maior de medições, visto que a Avaliação 1 foi feita baseada em 3 pares de medições, e ao mesmo tempo, confirmar ou levantar novos resultados obtidos com a primeira avaliação.
Os dados das métricas para as avaliações (medições) foram extraídos da BO.
Para atingir o objetivo de aceitar ou rejeitar a hipótese de que &quot;o índice de correlação entre os dados das métricas deve se manter alto quando esses são coletados através do mesmo processo e através de processos diferentes «efetuamos a análise dos resultados de acordo com as seguintes etapas:
Observação dos índices de correlação entre os dados obtidos com o mesmo processo de coleta;
Observação dos índices de correlação entre os dados obtidos com processos de coleta distintos;
Observando os índices de correlação calculados entre os dados obtidos com o &quot;Tipo de processo de coleta «Igual, pode- se inferir, de acordo com o método teste/ reteste, que a confiabilidade dos dados pode ser considerada alta uma vez que os índices de correlação apresentam valores bastante próximos de 1 e, como é sabido, o valor 1 corresponde ao índice máximo de correlação.
Por outro lado, observando os índices de correlação entre os dados obtidos com o &quot;Tipo de processo de coleta «Diferente, nota- se que há diferença bastante significativa em relação a os índices de &quot;Tipo Igual», e além de diferentes, são muito baixos, onde os dois primeiros índices são quase próximos a 0 e, como é sabido, o valor 0 corresponde ao índice mínimo (ou até mesmo inexistente) de correlação.
Apesar de os dois últimos índices do &quot;Tipo Diferente», não serem tão próximos de zero não significa que estes tenham índices altos se comparados com os índices de &quot;Tipo Igual».
Com base nesses resultados, pode- se rejeitar a hipótese inicial, pois o índice de correla ção entre os dados das métricas não se manteve alto quando esses foram coletados através de processos distintos.
Como o objetivo motivador para a execução da Avaliação 2 é de confirmar os resultados da Avaliação 1 ou até mesmo levantar novos fatores, os índices de correlação na Tabela 3.3 foram observados buscando identificar características que ratificam os primeiros resultados ou que mostram novos fatores.
Como foi feito na primeira avaliação, observando os índices de correlação calculados entre os dados obtidos com o &quot;Tipo de processo de coleta «Igual, pode- se reafirmar que a confiabilidade dos dados é alta devido a os altos índices de correlação encontrados, de acordo com o método teste/ reteste.
Contudo, observando os índices de correlação entre os dados obtidos com &quot;Tipo de processo de coleta «Diferente, identificam- se valores de r mais baixos, no entanto não tão baixos quanto os encontrados na Avaliação 1.
Entretanto, apesar de serem consideravelmente maiores, em outras palavras, mais próximos de 1, continuamos afirmando que os índices não são altos com base no que afirma Triola em.
Triola fala que eventualmente a interpretação do índice de correlação r com a utilização dos critérios &quot;próximo de «0 ou 1 é vaga, e em casos onde os pares de dados amostrais são pequenos esta interpretação pode ser mais complicada ainda.
Com o intuito de facilitar a interpretação dos índices com amostras de pares pequenos, o autor construiu uma tabela com valores usuais de r calculados com amostras genéricas de dados e definiu para cada quantidade de pares um valor crítico para r, o qual pode ser considerado nas interpretações de cálculo de índice de correlação com amostras pequenas.
Em o contexto da Avaliação 2, onde são utilizados 5 pares de dados (duas medições para cada uma das cinco releases), o autor indica que um índice de correlação só será considerado alto se este for maior que 0,878 com uma precisão de 95% de chance de não ocorrer erros nesse cálculo.
Para que se tenha uma precisão de 99% de chance de não ocorrer erros, a correlação será alta se for maior que 0,959.
Portanto, como praticamente todos os índices de correlação entre os dados obtidos de processos diferentes foram menores que 0,878 (com uma exigência de exatidão menorcom uma exigência de exatidão maior), consideramos baixos e portanto com confiabilidade duvidosa.
Por outro lado, analisando um dos índices de correlação, entre os dados da Medição 2 (Processo A) e 3 (Processo B), o qual foi calculado em r $= 0, 8188 é possível identificar um outro tipo de comportamento dos dados quando o processo de coleta sofre alteração.
Diferente do que aconteceu na Avaliação 1, na Avaliação 2 os índices com dados de processos diferentes não foram tão baixos, o que indica que quando há modificação no processo de coleta, a qualidade do dado referente a a confiabilidade pode até ser alta, mas este é um fator que pode apresentar aleatoriedade.
Mesmo com este novo fator identificado no comportamento dos dados dessa métrica no projeto P1, não é possível inferir que os dados mantém sua qualidade preservada quando coletados de maneiras diferentes.
Com a realização deste estudo apresentado no Trabalho Individual II evidenciou- se que mudanças no processo de coleta de dados de métricas da organização estão, numa primeira análise refletindo negativamente na qualidade dos dados.
Com isso, mostrou- se que realmente os dados dessa base podem ter sua qualidade duvidosa, pois a qualidade do dado está sendo sensível a fatores externos.
Além disso, observou- se os dados armazenados no repositório central e inferiu- se que, em coletas do passado, existem distorções em algum destes quando comparados os valores da métrica na base fonte e no repositório central.
Tendo em vista esta última consideração, identificou- se que no processo de coleta manual desses dados, são adicionados ruídos nos mesmos.
Esses ruídos estão na forma de inconsistências, incompletudes e desatualizações.
Com a execução dessa avaliação preliminar pode- se concluir alguns aspectos com relação a a qualidade dos dados.
No entanto, a forma de avaliação utilizada tem característica estatística e genérica, não considera aspectos inerentes à medições que são de software e, além disso, não aponta potenciais causas para o problema encontrado de homogeneidade dos dados.
Essa técnica estatística é eficiente para confirmar ou descartar hipóteses de problemas.
No entanto, ela só aponta que pode existir o problema, mas não é capaz de mostrar onde e por quê ele ocorre.
Para a análise da qualidade de uma métrica igualmente importante, como esforço de trabalho, que é a medição do quanto está sendo investido para a construção de um produto de software novo, é essencial a consideração de vários aspectos e não somente estatísticos, sendo necessário então um outro mecanismo de avaliação que não apresente essas falhas comentadas.
A seguir são mencionados e discutidos os principais aspectos para construir uma proposta de avaliação eficiente no que diz respeito a contexto e apontamento de causas de problemas de qualidade, quando evidenciados.
Para sistematizar a busca por uma solução sobre a avaliação de qualidade de dados de métricas de esforço de software, optou- se por esquematizar alguns aspectos envolvidos nessa tarefa, os quais foram notados no ambiente real durante a avaliação descrita anteriormente, visto que alguns gestores participaram ativamente desse processo, por demonstrarem total interesse em avaliar a qualidade dos dados dos seus projetos.
Esses aspectos envolvidos são:
Granularidade; Funcionalidade;
Não-dicotomicidade; Objetividade;
Contextualização. Granularidade:
A solução deve ser capaz de prover uma avaliação da qualidade para cada lançamento realizado, pois esse nível de detalhe proporciona melhor identificação de problemas e, conseqüentemente, de melhorias que podem ser realizadas.
Uma avaliação que aponta a qualidade de conjuntos de dados mas não aponta claramente quais os motivos para tal avaliação não é eficaz neste contexto.
Além disso, não apontar quais dados oriundos de quais membros estão com problemas também é uma falha no processo de avaliação.
Atualmente o processo de melhoria e conscientização é geral envolvendo todos os colaboradores da equipe por não se saber um ponto de foco, onerando assim membros que poderiam estar se dedicando às suas atividades.
Funcionalidade: A solução deve permitir praticidade na análise dos dados possibilitando rápido retorno aos usuários quanto a a avaliação da qualidade dos mesmos.
Um resultado de avaliação disponibilizado depois de um certo tempo pode não ser mais útil para o gestor e dificulta a tomada de ação para busca de melhorias.
Atualmente, a avaliação é realizada no momento da coleta de dados por os próprios gestores e quando algum dado é suspeito (baseado em simples suspeitas e suposições) é exigido um prazo de no máximo 24 horas para se confirmar ou descartar as suspeitas, apontando assim uma certa urgência no conhecimento sobre o estado dos dados.
Não-dicotomicidade: A solução deve ser capaz de evitar a dicotomicidade, onde uma classificação exclui diretamente outra.
Afirmar que um dado é estritamente ruim pode estar sendo desconsiderado o fato de que ele é pouco ruim, por exemplo.
A dicotomicidade inibe a observação da evolução da qualidade dos dados, uma vez que esse tipo de avaliação não permite se ter a noção de que um dado era ruim e que após certas ações ele está menos ruim.
Atualmente no ambiente real, há uma demanda por parte de os gestores de equipes de desenvolvimento de software por o acompanhamento da evolução de qualidade de dados.
Objetividade: A solução deve apontar os motivos que levaram o dado a obter determinada avaliação.
Essa é uma característica crítica para o processo de melhoria da qualidade, pois é o que permite verificar o que faz com que o dado tenha tal avalia ção.
A gestão de equipe de software exige dinamicidade e praticidade uma vez que o contato com o cliente é permanente e direto.
Indicar que há um problema se torna um problema se as causas não são apontadas.
Os gestores necessitam da visibilidade do seu foco de ação e assim acompanhar a evolução nesses aspectos, sem onerar- los com ações genéricas sem foco e com possibilidade de falha no sucesso.
Contextualização: A solução deve considerar as características inerentes ao ambiente de PDS, o processo de coleta desses dados e o processo de análise dos mesmos.
A coleta de métricas é um dos aspectos específicos desse contexto por não possuir um mecanismo automático de medição, o que torna esse ambiente mais complexo e não trivial.
Todos os requisitos são de grande e igual importância para que o problema seja resolvido da melhor forma possível.
Portanto uma solução não é aplicável caso não abranja todos os critérios.
É fato que a definição desses critérios é baseada na análise de um único ambiente real de desenvolvimento de software, colocando em dúvida se em outros ambientes reais desse tipo seriam notados os mesmos requisitos.
No entanto, acreditamos que na ausência de critérios (pois nenhum outro trabalho aborda esse problema dessa maneira), a definição de um conjunto de requisitos, mesmo que pequeno e restrito a um cenário, é benéfica pois permite direcionar os objetivos do trabalho.
Em a observação de uma proposta quanto a esses requisitos é possível inferir que o requisito é totalmente atendido por a proposta, parcialmente ou definitivamente não atendido.
A proposta atenderá totalmente o requisito se contém todas as características descritas do respectivo requisito e poderá atender parcialmente quando apenas algumas dessas caracter ísticas são atendidas e não atenderá se nenhuma das características descritas no requisito for observada.
Trabalhos relacionados Este capítulo apresenta os trabalhos relacionados ao tema de pesquisa encontrados na literatura que propõem soluções para avaliar qualidade de dados em diversos contextos, de entre eles estão:
AIQM (A methodology for information quality assessment), ASM-IQS (Assessment of software measurement an information quality study), DQA (Data quality assessment), DQMIM (A data quality measurement information model), ORME-DQ (A framework and a methodology for data quality assessment and monitoring), MDB:
PA (Measuring data believability:
A provenance approach) e Pedi (Process-embedded data integrity).
O estudo foi feito em duas etapas devido a a possibilidade de classificação dos trabalhos em dois grupos distintos e ao final de cada etapa é apresentada uma tabela de comparação desses trabalhos observando a abrangência ou não de eles com relação a os requisitos da solução apontados no De acordo com Lee Em, para a avaliação de qualidade de dados existem diversas técnicas, onde as principais são: (
a) pesquisa sobre a qualidade dos dados (data quality survey); (
b) aplicação de métricas (data quality metrics) e (c) análise de integridade de dados em bancos de dados (data integrity analysis).
Os grupos são os seguintes:
Primeiro grupo:
Análise dos trabalhos que aplicam as técnicas apontadas como principais por Lee Em.
Esse grupo é analisado focando a abrangência (forte, fraca ou nenhuma) de todos os requisitos apontados para uma solução adequada, observando sua total utilização como solução para o problema.
Segundo grupo:
Análise dos trabalhos que aplicam técnicas distintas às apontadas por Lee Em.
Esse grupo é analisado focando a abrangência total do maior número de requisitos possível, observando sua adequação enquanto abordagem para a elaboração de uma solução para o problema.
Trabalhos que aplicam Data Quality Survey são:
Lee (AIMQ) em propõem uma metodologia que se utiliza de questionários para avaliar a qualidade de dados em qualquer contexto.
Essa metodologia é composta por três componentes:
Definição das dimensões para avaliação coleta de dados através de questionários e técnicas de análise para identificar problemas de qualidade nas informações;
Berry (ASM-IQS) em propõem a aplicação de dois diferentes tipos de instrumentos de avaliação, onde um é um questionário genérico e o outro é um questionário voltado para dados de medição de software.
O foco dessa avaliação é para melhoria do processo de medição e não para melhoria dos dados propriamente ditos.
Os trabalhos que aplicam a segunda técnica Data Quality Metrics como forma de avaliação são:
Pipino (DQA) em propõem um conjunto de métricas baseadas nas dimensões de formuladas através de três tipos de cálculos (razão, min/ max e média ponderada) que indicam a qualidade de um conjunto de dados genéricos.
Os autores ainda apresentam como essas métricas podem ser combinadas a uma avaliação subjetiva para melhorar a qualidade dos dados;
Caballero (DQMIM) em definem um modelo de medição que a partir de as necessidades dos usuários define o que, como e por quê deve ser medido e ainda, quem realiza a medição.
Com base nessas definições é construído um conjunto de métricas para avaliar a qualidade de conjunto de dados genéricos;
Batini (ORME-DQ) em propõem uma metodologia para avaliação, composta de quatro fases análise do ambiente, seleção de conjunto de dados críticos, avaliação qualitativa e quantitativa da qualidade e avaliação constante de qualidade.
Em a fase 3 é onde são aplicadas as métricas, cujo formato depende da respectiva dimensão (simples comparação entre strings, cálculo de distância, etc).
Prat e Madnick (MDB:
PA) em propõem uma abordagem para medir a confiabilidade de um conjunto de dados.
Os autores definem um conjunto de métricas a ser aplicadas com dados armazenados num modelo de Banco de Dados que guarda a proveniência dos dados.
Tanto as métricas quanto o modelo de proveniência são genéricos.
Os trabalhos que aplicam a segunda técnica Data Integrity Analysis como forma de avalia ção são:
A pesquisa de Lee (Pedi) em propõe um processo iterativo de melhoria de qualidade através da adoção de regras de integridade onde, dependendo da regra, algumas dimensões de são atacadas:
Integridade de coluna:
Acurácia e interpretabilidade,· integridade de entidade:
Acurácia e completude,· integridade de usuário:
Consistência Constantemente as regras são avaliadas através de freqüentes definições, medições, análises e redefinições para um ciclo de melhorias.
A análise dos trabalhos específicamente com relação a a abrangência dos requisitos da solução buscada (Capítulo 3) está ilustrada na Tabela 4.1.
Células com preenchimento de cor mais escura significam que o trabalho abrange o requisito conforme desejado para a solução, a cor mais clara indica que o trabalho apresenta o requisito, mas não conforme o esperado para a solução e a ausência de preenchimento indica a não abrangência ao requisito.
Não foi encontrado trabalho que, de acordo com os requisitos selecionados, pudesse ser aplicado adequadamente na avaliação de qualidade de dados no contexto de métricas de esforço de software.
Em esse sentido, é possível observar com essa análise que, para o problema abordado neste trabalho, a técnica de survey (data quality survey) não é suficientemente completa para a solução, pois apesar de abranger alguns requisitos, a forma de aplicação e de análise de resultados dificulta consideravelmente sua aplicação.
O tempo despendido na coleta de respostas sobre a qualidade de dados prejudica a sua funcionalidade, assim como o tempo necessário para a análise dos resultados obtidos com as respostas.
Apesar de os trabalhos referentes à técnica que se utiliza de métricas (data quality metrics) serem os que apresentam as maiores funcionalidades, o problema de granularidade dificulta sua utilização para a solução aqui buscada.
As métricas geralmente apresentam uma avaliação dicotômica (bom/ ruim) levando em consideração grupos de dados (por exemplo, 70% do conjunto de dados X apresenta problema).
Já o trabalho de análise de integridade de dados (data integrity analysis) não apresenta grande aderência ao que se procura, apesar de utilizar uma granularidade mais detalhada (por exemplo, colunas numa tabela de banco de dados).
O mesmo não satisfaz as reais necessidades da solução uma vez que o alto detalhismo desconsidera outros fatores que podem afetar a qualidade dos dados no contexto estudado, pois de acordo com os próprios autores, a análise de integridade de coluna, por exemplo, não tem como garantir que os dados presentes são corretos.
O trabalho desenvolvido por Liebchen Em é utilizada a técnica de classificação para avaliar a qualidade dos dados.
A falta de adequação dessa técnica para o atual problema está relacionada às manipulações que os dados sofrem na preparação prévia à aplicação do algoritmo de classificação.
Não parece coerente avaliar dados que têm a qualidade afetada justamente por manipulações ao longo de o seu ciclo, através de uma técnica que também está sujeita à inserção de erros nos dados.
Além disso, mesmo que a preparação seja isenta de problemas, uma avaliação que desconsidere características de contexto (talvez subjetivas), muitas vezes não mensuráveis, também pode apresentar falhas.
A pesquisa desenvolvida por Caro considera o conhecimento emp írico de usuários de portais web para identificar critérios para avaliação de qualidade de dados obtidos através desses portais.
Esse trabalho utiliza critérios sugeridos por os usuários para modelar uma rede Bayesiana considerando a incerteza dos usuários no momento da avaliação, o que pode ser uma abordagem interessante.
No entanto, a rede Bayesiana aponta uma probabilidade do dado ser bom ou ruim, mas não aponta quão bom ou ruim o dado pode ser considerado.
Data Provenance é uma alternativa que tem sido utilizada na busca por a qualidade de dados, onde tem se defendido que, através do conhecimento do processo de obtenção dos dados e/ ou de suas fontes, é possível inferir se o dado tem ou não qualidade.
A abordagem se mostra interessante para ajudar a evidenciar problemas de qualidade de dados através da sua proveniência, e pode ser encontrada em diferentes domínios de aplicação.
Em Chimera é utilizado Data Provenance para aplicações nas áreas de Astronomia, Biologia e Física.
Buneman também a adota com dados da Biologia.
Em Poesia, por sua vez, é aplicada tal abordagem no contexto de Agricultura, enquanto que, em Trio, no de Biologia.
Até onde se sabe, não foi utilizado Data Provenance no contexto de métricas de software.
A análise dos trabalhos do segundo grupo com relação a a abrangência dos requisitos da solução buscada (Capítulo 3) está ilustrada na Tabela 4.2.
O preenchimento de cor mais escura significa que o trabalho abrange o requisito conforme desejado para a solução, a cor mais clara indica que o trabalho apresenta o requisito, mas não conforme o esperado para a solução e a ausência de preenchimento indica a não abrangência ao requisito.
Fica claro na Tabela 4.2 que o trabalho de Buneman é o que mais atende os requisitos.
Contudo, como discutido no Capítulo 3, tal trabalho tem pouca aplicabilidade por não atender os requisitos de Não-dicotomicidade e Contextualização.
A propósito, o critério de não-dicotomicidade não é atendido por nenhum dos trabalhos analisados, o que caracteriza uma proposta inovadora e desafiadora da presente pesquisa.
As questões de objetividade e contextualização também são críticas porque apenas um de cada três trabalhos preencheu o requisito.
Tendo identificado através dos trabalhos relacionados que o tema é promissor e não está totalmente resolvido, definiu- se a estratégia de pesquisa.
De acordo com as características observadas, a pesquisa a ser desenvolvida é classificada como exploratória, onde o objetivo é examinar um tema ou problema de pesquisa pouco estudado ou que não tenha sido abordado da mesma forma anteriormente na literatura.
Entende- se que essa estratégia é adequada, pois o levantamento bibliográfico mostrou que não existem propostas abrangentes na área, e os estudos realizados até o momento não aprofundam o problema de estudo.
Este capítulo descreve a solução proposta para avaliar a qualidade de dados de métrica de esforço de software.
É uma arquitetura que comporta o rastreamento e armazenamento da proveniência dos dados de métricas de esforço e através dessa proveniência avalia por um sistema de inferência qual o nível de qualidade do dado variando de 0 a 1.
Em esse sentido, as próximas seções contemplam:
Descrição da arquitetura proposta, o processo de avaliação suportado por a arquitetura e descrição dos componentes que compõem a arquitetura juntamente com o processo de criação dos mesmos.
A arquitetura proposta para suportar a avaliação da qualidade dos dados de esforço, ilustrada na Figura 5.1 é composta por quatro principais blocos componentes de ação: (
a) Componente de Proveniência, (b) Componente de Inferência, (c) Componente de Quali-dade e (d) Componente de Análise.
Antes que esses dados sejam mostrados aos gestores, o colaborador os submete a uma máquina de inferência para receberem uma avaliação de nível de qualidade, cujo resultado é armazenado num base de dados chamada Quality Table.
Após, os dados podem ser disponibilizados ao processo de tomada de decisão com o nivel de qualidade atribuído.
A próxima e última etapa desse processo é a carga histórica da qualidade desses dados, que ocorre num banco dimen-sional estruturado na forma de um Data Warehouse que tem as bases Provenance, Efort Source e Quality Table como fonte desses dados.
O processo acima descrito é considerado bem comportado, podendo variar alguns as- pectos principalmente no que diz respeito à periodicidade com que os membros registram o esforço.
No entanto, a arquitetura e seus componentes comportam as diferentes possibil-idades de mudança do processo nesse sentido.
A seguir cada componente da arquitetura é descrito juntamente com o seu processo de construção.
A composição dos quatro blocos é o que permite à organização ter conhecimento do que está acontecendo no presente, mostrando os pontos de falha e de investimento de mudança para melhorias.
Cada bloco tem sua importância na arquitetura, sendo que um bloco sozinho não traz tantos benefícios quanto o uso em conjunto dos mesmos.
O bloco do Componente Proveniência, (a) na Figura 5.1, consiste no armazenamento da rastreabilidade de dados que dizem respeito às circunstâncias atuais de registro na base de dados Efort Source.
Como foi discutido anteriormente (Subseção 4.2, Capítulo 4) ex-istem várias técnicas para fazer uso de Data Provenance (DP).
Em este trabalho, o uso de DP foi através da construção de uma base de dados auxiliar convenientemente modelada e estruturada para capturar todas as informações consideradas pertinentes no processo de obtenção e derivação dos dados de esforço.
A Figura 5.2 mostra o diagrama de classes UML do banco de dados desse componente de proveniência.
As classes Event, Project, Release, Phase, Resource têm a função de registrar a proveniência do dado originalmente lançado, armazenando seu valor inicial (Efort Value), o dia em que o lançamento está ocorrendo (CurrentDay) e o dia em que está sendo registrado o esforço (RegisterDay).
A classe UpdateEvent tem a função de registrar a proveniência do dado quando este está sendo modificado por atualizações ou transformações, armazenando seu novo valor (NewEffortValue), quando isso está ocorrendo (UpdateDay) e quem está realizando essa alteração (Id-custodian).
E a classe Analysis tem a função de registrar as datas de análises que os dados sofrem para observar eventuais mudanças de qualidade num mesmo registro.
Para a construção desse modelo foram utilizados conceitos de DP e também conceitos consolidados na área de Data Quality, como o uso de dimensões de qualidade.
A proposta deste trabalho é inovadora no sentido em que utiliza DP como variáveis na composição de dimensões de qualidade discutidas anteriormente na Fundamentação Teórica através de as quais o dado é avaliado.
A seguir são expostas as dimensões consideradas nesta arquitetura e como foi o processo de composição das mesmas através de DP.
A avaliação de qualidade de dados é composta por a avaliação de diversas dimensões e cabe a cada organização determinar quais dimensões são mais importantes de acordo com o contexto em que os dados estão inseridos.
Além disso, deve- se também definir quais vari áveis estão envolvidas na composição de cada dimensão.
Um aspecto novo deste trabalho é justamente compor cada dimensão com informações referentes à proveniência dos dados avaliados, pois acreditamos que essa composição já exista e é conC hecida e utilizada no dia-a-dia das organizações, só que de forma empírica e intuitiva.
Além disso, apesar de atualmente a avaliação ser subjetiva, os envolvidos se utilizam de critérios e também relacionam variáveis de acordo com esses critérios para julgar os dados que lhes são disponibilizados.
Assim, para fazer essa composição é necessário que se consiga captar o conhecimento subjetivo e inerente às pessoas, e transformar num conhecimento de posse da organização adequadamente estruturado e acessível para realização de avaliações de qualidade independente de quem está envolvido no processo.
Para isso, identificamos a necessidade de estudo e análise de um ambiente real de desenvolvimento de software, para se identificar quais dimensões e variáveis podem ser consideradas essenciais num ambiente de desenvolvimento de software.
Esse estudo e análise permitem que os componentes sejam construídos da melhor forma possível, refletindo as necessidades mais evidentes num ambiente desse tipo.
É importante salientar que as dimensões refletem as necessidades dos envolvidos com a qualidade dos dados e, como não é possível abranger todas as necessidades, é preciso fazer escolhas.
De essa forma, para a identificação de quais dimensões podem ser consideradas essenciais no ambiente de desenvolvimento de software, foram utilizados resultados da análise estatística de qualidade nos dados da operação de software parceira demonstrada no Capítulo 3, onde foram identificados problemas críticos durante o ciclo dos dados de esforço.
Essa análise nos permitiu a definição de quais dimensões poderiam ser utilizadas de entre o conjunto proposto por.
Este trabalho tem o enfoque nas dimensões relativas a diferentes categoria estudadas no Capítulo 2, tais como Intrinsc DQ e Contextual DQ, pois identificamos que as questões abordadas nestas categorias vão ao encontro de as necessidades de um dado de métrica de software, que precisa ter objetividade, confiabilidade e excelente precisão e reputação.
A tabela 5.1 apresenta as dimensões definidas.
A descrição de cada dimensão está relacionada ao contexto em que ela é aplicada.
Em o contexto de análise de dados de esforço de software, as dimensões tem as seguintes interpretações:
Acurácia Diz respeito ao quanto o dado está correto.
Essa dimensão avalia se o dado contém o valor mais próximo de o esperado possível.
Atualidade Diz respeito a quão atualizado está o dado para o que ele vai ser analisado.
Essa dimensão pode ter duas interpretações:
O dado não está atual porque o colaborador não o registrou no período que devia (está atrasado) ou 2) o dado não está atual porque ele foi coletado num período e a análise está sendo feita muito tempo depois desse período.
A interpretação adotada pela presente solução está relacionada à primeira.
Completude Diz respeito à falta de informação, valores faltantes.
Essa dimensão avalia se o dado não está completo devido a a falta de registro ou o registro existente contém falhas com relação a o esperado.
Consistência Diz respeito ao sentido do dado ao longo de o ciclo.
Essa dimensão avalia se o dado sofreu inserção de erro ao sair de sua origem (base fonte), passar por todo o processo de extração e transformação e não sofrer alteração (modificação) significativa em seu valor.
A proposta deste trabalho é que cada dimensão seja constituída de variáveis referentes à proveniência dos dados, isso porque se acredita que rastreando o dado de esforço durante todo seu ciclo é possível inferir níveis de qualidade.
Para isso, a imersão num ambiente real se faz, novamente, necessária e totalmente pertinente para a construção de componentes que refletem a realidade do contexto.
Entretanto, para a definição das variáveis, não basta realizar análises sobre problemas históricos, como foi feito na definição das dimens ões, já que esse é um conhecimento empírico baseado em conhecimentos e experiências acumuladas na organização.
Portanto, nessa etapa foi necessário definir uma metodologia de pesquisa para aquisição de conhecimento.
Para a aquisição de conhecimento foi escolhida a metodologia de entrevista semi-estruturada (que pode ser acessada no Apêndice B através do Projeto exposto), uma vez que a metodoloC gia que se utiliza de questionário não seria capaz de capturar detalhes necessários para a composição dos componentes da arquitetura.
A estrutura da entrevista facilita a aquisição, pois a presença do entrevistador possibilita que novas questões sejam formuladas como conseqüência de uma resposta fornecida.
O instrumento desenvolvido para apoio à entrevista (apresentado no Apêndice A) possui uma organização visando à construção dos componentes da arquitetura.
Em este momento trataremos do aspecto da entrevista relacionado à construção do Componente de Proveniência, sendo os outros aspectos tratados na descrição de cada componente correspondente.
A parte da entrevista que contribuiu para a modelagem do Componente de Proveniência está ilustrada na Figura 5.3, onde foi disponibilizada uma lista de variáveis (informações de DP possíveis de serem rastreadas) para que o entrevistado selecionasse e relacionasse essas variáveis a cada uma das dimensões (acurácia, atualidade, completude e consistência).
Para construir o modelo apresentado na Figura 5.2, foram utilizadas as variáveis que tiveram maior número de seleção por parte de os entrevistados e também variáveis que não estavam na lista inicialmente e que os entrevistados adicionaram.
A Tabela 5.2 ilustra as variáveis envolvidas para cada dimensão de acordo com o que foi coletado com os entrevistados, onde:
Dr-DL é a diferença entre o dia do registro (Dr) e o dia do lançamento (Dl), onde o dia do registro é o dia para o qual o colaborador quer lançar o esforço e o dia do lançamento é a data em que ele está fazendo isto;
DFF-DR é a diferença entre o dia em que finalizou a fase de lançamento (DFF) e o dia do registro (Dr);
VO-VM é a diferença entre o valor original (VO) que é o primeiro registro do esforço e o valor modificado (VM) caso o registro seja alterado por motivo de atualização ou engano;
Em o momento, a proveniência contemplada por o modelo diz respeito a circunstâncias de lançamento e coleta dos dados.
Outros tipos de proveniência (fonte/ cálculos) não são abordados porque a pesquisa está focada em dados de esforço que não são obtidos por mais de uma fonte e não sofrem cálculos.
Conforme indica a metodologia de aplicação de entrevistas estruturadas, antes da realização oficial das mesmas com os colaboradores num ambiente real, foi realizado um pré-teste, cujos entrevistados são membros do grupo de pesquisa que possuem vivência na operação de software parceira e portanto conhecem a realidade dos reais entrevistados.
Além disso, para o pré-teste foram escolhidos esses membros por terem um certo conhecimento do contexto de dados de esforço podendo assim colaborar com a melhoria do instrumento de auxílio à aquisição de conhecimento e com o próprio processo de condução da entrevista.
Após a aplicação do pré-teste, foram convidados 10 colaboradores da operação de software, com 3 diferentes perfis:
Membro de equipe de desenvolvimento, gestor e organizador dos dados, conforme indica.
Com a participação de diferentes perfis na aquisição de conhecimento é possível entender melhor os problemas relacionados a dados de esforço considerando diferentes tipos de papéis e responsabilidades com relação a os dados.
Além disso, a abrangência de diferentes papéis é um fator importante na conscientização da organização como um todo quanto à importância da qualidade dos dados.
De entre esses 10 participantes, 4 são gestores, 4 são membros de equipe de desenvolvimento e 2 são organizadores de dados para análise.
Acreditamos que apesar de não ser um número muito alto de entrevistados, para o tipo de uso de respostas a amostra é satisfat ória.
O propósito não é realizar análises estatísticas, mas sim construir uma agregação de conhecimentos de acordo com diferentes pontos de vista para alcançar uma adequada abrangência de critérios a serem considerados na arquitetura.
Em esse sentido, a escolha dos entrevistados foi rigorosa e não aleatória, buscando colaboradores que possuíssem alto grau de experiência em PDS, alto nível de envolvimento e comprometimento com dados de esforço e alto grau de responsabilidade.
Detectar as necessidades dos envolvidos é uma tarefa muito difícil, mas é a melhor forma de entender o domínio e traz resultados bem interessantes de acordo com.
O bloco do Componente de Inferência, (b) na Figura 5.1, apresenta uma máquina de infer-ência munida de uma base de regras previamente construída.
Com a freqüência definida por a organização (semanal /quinzenal/mensal), a máquina de inferência recebe como entrada os dados de proveniência, referentes ao período desejado e, através do conjunto de regras, avalia a qualidade de cada lançamento considerando- o desde seu primeiro registro e todas as modificações sofridas durante o período (se existirem) até a última alteração.
A Figura 5.4 mostra um exemplo de arquivo de entrada com os dados de proveniência para o processamento da máquina de inferência para a dimensão acurácia onde o arquivo mostra para cada registro (representado por cada linha) na primeira coluna os valores da variável&amp; Dr-DL, na segunda coluna os valores da variável&amp; DFF-DR e na terceira coluna os valores da variável&amp; VO-VM.
A máquina possui dois níveis de processamento, onde no primeiro ela processa, através das regras e da função de pertinência, o grau de aderência do dado com relação a cada dimensão, cujos valores estão no intervalo de acordo com a lógica fuzzy.
O segundo nível de processamentoc (2 nd Inference Level result na Figura 5.5) utiliza o resultado de grau de aderência calculado para cada dimens ão (Quality fuzzy rule base na Figura 5.5) para calcular o grau de aderência do dado a diferentes graus de qualidade (Low, Reasonable, Acceptable ou High), conforme ilustrado na Fig. 5.5.
A Figura 5.6 mostra um exemplo de resultado de avaliação com o processamento da máquina de inferência onde cada linha (identificado por Id) representa um lançamento de esforço, a coluna grupo mostra a fase de lançamento, a coluna chv_ ati é a atividade correspondente ao esforço mostrado na coluna hora_ at.
O restante das colunas diz respeito às avaliações, já defuzzyficadas, com as respectivas variáveis e o respectivo resultado, sendo diferenciadas por as cores.
A última coluna (quali) apresenta o resultado de avaliação final defuzzyficado.
Para conhecer a avaliação linguística a Figura 5.37 pode ser acessada na seção seguinte.
As regras e as respectivas funções de agregação foram consolidadas através das entrevistas.
Primeiramente, o entrevistado relatou qual a importância (baixa, média ou alta) de cada dimensão para determinado grau de qualidade (Muito baixa qualidade, Razoável qualidade, Boa qualidade e Ótima qualidade), conforme pode ser visualizado na Figura 5.7.
Por exemplo, se um entrevistado relata que:
&quot;se o dado possui baixa acurácia e consist ência esse será considerado como de muito baixa qualidade, não importando se possui atualidade e completude baixa ou média, pois não adianta o dado ser atual e completo se não possuir adequada acurácia e consistência», nesse caso uma regra é constituída da seguinte maneira:
Se acurácia $= baixa E Consistência $= baixa E Se atualidade $= baixa Ou atualidade $= media E Se completude $= baixa Ou completude $= media Então Qualidade $= Muitobaixa.
Esse conhecimento é adquirido através da marcação do entrevistado na folha (Figura e de anotações realizadas durante a entrevista.
Com as variáveis de DP selecionadas, o entrevistado as relacionou com cada dimensão através de regras, definindo o que caracteriza um dado de baixa, média ou alta acurá- cia /atualidade/completude/ consistência.
Por exemplo, se o entrevistado relata que:»
se o membro de equipe (quem lança o dado) não lançou suas horas ao final do dia de trabalho, então provavelmente o dado tem acurácia duvidosa e dependendo de quanto tempo ele demorou para registrar, essa dúvida tende a aumentar, nesse caso as variáveis de proveniência utilizadas são dia do lançamento, dia do registro (y) e a diferença entre x e y (z), e o bloco de regras a ser criada é o seguinte:
Se z $= grande então acurácia $= baixa Se z $= médio então acurácia $= média Se z $= pequeno então acurácia $= alta A Figura 5.8 mostra um exemplo de marcação por um entrevistado.
Em seguida, cada entrevistado definiu, de acordo com a sua experiência e conhecimento empírico, o que pode ser considerado um valor para z grande, médio e pequeno e a intersecção de níveis, como pode ser mostrado no exemplo de gráfico de uma função de pertinência na Figura 5.9.
A Figura 5.9 ilustra um exemplo de função de pertinência para uma declaração do tipo:
&quot;até uns 2 dias de diferença pode- se considerar um valor pequeno, no entanto até o 4º dia é uma diferença média, mas de 6 dias em diante é considerada uma diferença grande.»
A principal vantagem do processamento em regras através de funções de pertinência que resultam valores entre é a flexibilidade na classificação do dado podendo fazer uso da incerteza utilizada no processo de tomada de decisão humana.
No caso de a métrica de esforço, os valores fuzzy indicam quão aderente está um dado com cada nível de qualidade, e não somente indica uma probabilidade do dado ser classificado em cada nível.
Por exemplo, a informação de que o dado é de baixa qualidade, mas que ele é quase de boa qualidade é uma informação mais rica e útil do que simplesmente dizer que o dado é de baixa qualidade.
Além disso, a avaliação fuzzy se mostra mais adequada ao contexto de PDS, cuja complexidade e dinamismo são características que não podem ser ignoradas no momento de uma avaliação, tornando assim uma avaliação dicotômica mais passível de erros.
É interessante observar que não foram encontradas pesquisas que utilizam lógica fuzzy para avaliação de qualidade de dados no contexto de métricas de software.
Entretanto, lógica fuzzy tem sido amplamente utilizada nesse contexto em outras áreas, como estimativas de software e gerenciamento de qualidade de software, Para a implementação da máquina de inferência foi utilizado a ferramenta Matlab versão 12 que possui um simulador fuzzy onde é possível implementar as funções de pertinência e o bloco de regras.
Através da construção de um script com sufixo.
M o recurso lê como entrada um arquivo com os dados a serem avaliados e grava em arquivo a avaliação de cada dado, conforme pode ser visualizado abaixo o script implementado.
A seguir é descrita a implementação de cada dimensão de acordo com os conceitos de sistemas de inferência fuzzy estudados na Fundamentação Teórica (Cápítulo 2), onde devem ser definidos:
Variáveis lingüísticas, termos e funções de pertinência.
Acurácia A Figura 5.10 mostra o modelo de inferência para o cálculo da dimensão Acurácia com as variáveis de entrada nas caixas amarelas e a variável de saída na caixa azul.
A caixa branca representa o passo da inferência onde as regras são executadas.
Variáveis linguísticas e termos:
Funções de pertinência As funções de pertinência das variáveis envolvidas no cálculo da dimensão acurácia, implementadas no Matlab, são expostas a seguir.
A Figura 5.11 ilustra a implementação da variável&amp; Dr-DL para a dimensão Acurá- cia.
Em esta figura a variável está descrita na forma de&quot;&amp; Dr-DL/ ac porque essa variável também é considerada no cálculo de outras dimensões e o&quot;/ ac indica que esta função é especificamente para acurácia.
Assim também ocorre com a nomenclatura das outras variáveis&quot;&amp; DFF-DL/ ac e&quot;&amp; VO-VM/ ac.
A função de pertinência na Figura 5.11 é representada no eixo x por &quot;dias «e no eixo y por &quot;valor fuzzy».
Em este caso, a função permite uma variação de até &quot;10 dias», o que pode ser alterado dependendo do contexto de aplicação ou de teste.
Essa representação também ocorre com a variável&amp; DFF-DR na Figura 5.13.
Diferentemente, a função de pertinência da variável&amp; VO-VM/ ac é representada no eixo x por &quot;porcentagem «e no eixo y por &quot;valor fuzzy».
Essa porcentagem pode ter uma variação de até 100%.
A variável de saída Acurácia na Figura 5.14 possui uma função de pertinência onde o eixo y é o valor fuzzy eixo x é um valor defuzzyficado num sistema de &quot;nota «que varia de 0 a 10.
Essa saída servirá de entrada para o segundo nivel de processamento da máquina de inferência.
Regras de produção As regras de produção foram inseridas no Matlab através de um recurso dessa ferramenta e podem ser vistas na Figura 5.15.
Essas regras são resultado do processo de aquisição de conhecimento já apresentado.
Atualidade A Figura 5.16 mostra o modelo de inferência para o cálculo da dimensão Atualidade com as variáveis de entrada nas caixas amarelas e a variável de saída na caixa azul.
A caixa branca representa o passo da inferência onde as regras são executadas.
Variáveis linguísticas e termos:
Funções de pertinência As funções de pertinência das variáveis envolvidas no cálculo da dimensão atualidade, implementadas no Matlab, são expostas a seguir.
A Figura 5.17 ilustra a implementação da variável&amp; Dr-DL para a dimensão Atualidade.
Em esta figura a variável está descrita na forma de&quot;&amp; Dr-DL/ at &quot;porque essa variável também é considerada no cálculo de outras dimensões e o&quot;/ at «indica que esta função é especificamente para atualidade.
Assim também ocorre com a nomenclatura da outra variável&quot;&amp; DC-DA/ at».
A função de pertinência na Figura 5.17 é representada no eixo x por &quot;dias «e no eixo y por valor fuzzy.
Em este caso, a função permite uma variação de até &quot;10 dias», o que pode ser alterado dependendo do contexto de aplicação ou de teste.
A representação da variável&amp; DC-DA na Figura 5.18 também é no eixo x é por dias e no eixo y por valor fuzzy.
Essa porcentagem pode ter uma variação de até 30 dias.
A variável de saída Atualidade na Figura 5.19 possui uma função de pertinência onde o eixo y é o valor fuzzy eixo x é um valor defuzzyficado num sistema de &quot;nota «que varia de 0 a 10.
Essa saída servirá de entrada para o segundo nivel de processamento da máquina de inferência.
Regras de produção As regras de produção foram inseridas no Matlab através de um recurso dessa ferramenta e podem ser vistas na Figura 5.20.
Essas regras são resultado do processo de aquisição de conhecimento já apresentado.
Variáveis linguísticas e termos:
Funções de pertinência As funções de pertinência das variáveis envolvidas no cálculo da dimensão completude, implementadas no Matlab, são expostas a seguir.
A Figura 5.22 ilustra a implementação da variável&amp; Dr-DL para a dimensão Completude.
Em esta figura a variável está descrita na forma de&quot;&amp; Dr-DL/ com &quot;porque essa variável também é considerada no cálculo de outras dimensões e o&quot;/ com «indica que esta função é especificamente para completude.
Assim também ocorre com a nomenclatura da outra variável&quot;&amp; DFF-DR/ at».
A função de pertinência na Figura 5.22 é representada no eixo x por &quot;dias «e no eixo y por valor fuzzy.
Em este caso, a função permite uma variação de até &quot;10 dias», o que pode ser alterado dependendo do contexto de aplicação ou de teste.
A representação da variável&amp; DFF-DR na Figura 5.23 também é no eixo x é por dias e no eixo y por valor fuzzy.
Essa porcentagem pode ter uma variação de até 10 dias.
A variável de saída Completude na Figura 5.24 possui uma função de pertinência onde o eixo y é o valor fuzzy eixo x é um valor defuzzyficado num sistema de &quot;nota «que varia de 0 a 10.
Essa saída também servirá de entrada para o segundo nivel de processamento da máquina de inferência.
Regras de produção As regras de produção foram inseridas no Matlab através de um recurso dessa ferramenta e podem ser vistas na Figura 5.25.
Essas regras são resultado do processo de aquisição de conhecimento já apresentado.
Variáveis linguísticas e termos:
Funções de pertinência As funções de pertinência das variáveis envolvidas no cálculo da dimensão consistência, implementadas no Matlab, são expostas a seguir.
A Figura 5.27 ilustra a implementação da variável&amp; VO-VM para a dimensão Consist ência.
Em esta figura a variável está descrita na forma de&quot;&amp; VO-VM/ con &quot;porque essa variável também é considerada no cálculo de outras dimensões e o&quot;/ con «indica que esta função é especificamente para consistência.
A função de pertinência na Figura 5.27 é representada no eixo x por porcentagem e no eixo y por valor fuzzy.
Em este caso, a função permite uma variação de até 100 porcento, o que pode ser alterado dependendo do contexto de aplicação ou de teste.
A variável de saída Consistência na Figura 5.28 possui uma função de pertinência onde o eixo y é o valor fuzzy eixo x é um valor defuzzyficado num sistema de &quot;nota «que varia de 0 a 10.
Essa saída também servirá de entrada para o segundo nivel de processamento da máquina de inferência.
Regras de produção As regras de produção foram inseridas no Matlab através de um recurso dessa ferramenta e podem ser vistas na Figura 5.29.
Essas regras são resultado do processo de aquisição de conhecimento já apresentado.
Variáveis linguísticas e termos:
Funções de pertinência Em a Figura 5.31 a entrada é um escalar defuzzyficado no primeiro nivel de processamento, que está representado num sistema de notas como mostra o eixo x da figura.
Em o eixo y é o valor fuzzy correspondente à entrada.
O mesmo ocorre com as outras funções de pertinência do restante das dimensões Atualidade, Completude e Consistência.
A Figura 5.35 mostra a função de pertinência que calcula o valor final retornando um escalar representado num sistema de notas em que pode ser entendido de forma linguistica através do acesso à Figura 5.37.
Regras de produção Como resultado dos cálculos da máquina de inferência utilizando a proveniência dos dados, é gerada essa tabela para guardar a avaliação de cada dimensão e para cada nível de qualidade.
Este é um componente bastante simples servindo como saída das regras e, também, como fonte para o Componente de Análise.
A Tabela 5.3 exemplifica o registro de uma avaliação de qualidade de um dado de esforço identificado por o Id $= 05, onde:
Acur. É o valor defuzzyficado correspondente à avaliação do primeiro nível de processamento da máquina de inferência da dimensão Acurácia;
At. É o valor defuzzyficado correspondente à avaliação do primeiro nível de processamento da máquina de inferência da dimensão Atualidade;
Compl. É o valor defuzzyficado correspondente à avaliação do primeiro nível de processamento da máquina de inferência da dimensão Completude;
Cons. É o valor defuzzyficado correspondente à avaliação do primeiro nível de processamento da máquina de inferência da dimensão Consistência;
Ba, R, Bo e O dizem respeito à avaliação dos níveis de qualidade Baixo, Razoável, Boa e Ótima qualidade, resultado do segundo nivel de processamento onde a entrada foi o valor defuzzyficado das dimensões e a saída é um valor defuzzyficado de qualidade, representado por a coluna Quali na Figura 5.3.
Os valores Ba, R, Bo, O são valores fuzzyficados intermediários, utilizados entre o cálculo das dimensões e o cálculo final da Qualidade.
Id Acur At Com Cons Ba R Bo O Quali O bloco do Componente de Análise, (d) na Figura 5.1, diz respeito à construção de uma base analítica dimensionalmente modelada para proporcionar análises completas voltadas às necessidades dos gestores.
A Base de Proveniência e Qualidade possibilita registrar o histórico da qualidade dos lançamentos de esforço num projeto/ organização.
Isso possibilita análises não só do presente, mas também do passado e, conseqüentemente, de tendências futuras.
O componente que mantém o histórico das avaliações da organização e que possibilita análises de tendências para apontar possíveis pontos de melhoria é estruturado na forma de um data warehouse, modelo estrela.
O componente, chamado aqui de DW-Q, é composto por uma tabela Fato (Event) que registra os valores de avaliação de cada dimensão de qualidade, também o resultado final de níveis de qualidade, além de medidas como UpdateRate (taxa de modificação dos dados), RegisterLatency (latência do registro dos dados, ou seja, o quanto os recursos demoram para registrar suas horas trabalhadas).
Essa tabela fato é caracterizada por as tabelas dimensão When, What, Timeline, Who, e Where.
A tabela dimensão When é responsável por possibilitar a análise de quando ocorrem os registros de esforço através da observação de ano, mês e data geradora do registro.
A tabela dimensão What mantém o valor original do esforço e informações de acontecimentos posteriores com o dado, quantas vezes ele foi modificado e o seu valor final que possibilita a análise de vulnerabilidade dos dados de esforço.
A dimensão Who disponibiliza a observação de colaboradores que possuem maior incid ência de alteração de comportamento nos dados, permitindo aos gestores concluir se a equipe recebeu um bom treinamento antes de inserção no projeto e se existe alguém que pode estar deteriorando a qualidade dos dados, por exemplo.
A dimensão Where podem apontar as versões mais críticas de um projeto, com relação a a qualidade de seus dados.
Através de uma análise mais detalhada das versões de um projeto é possível observar aspectos e ações das equipes e até mesmo de clientes que podem influenciar na qualidade dos dados.
Por exemplo, pode ser observado que uma versão que apresenta muito paralelismo nas atribuições de atividades apresenta um alto índice de baixa qualidade nos dados.
Finalmente, a tabela dimensão Timeline mantém o registro histórico de análises dos dados, mostrando a freqüência de avaliação.
O componente DW-Q usa como fonte o componente de proveniência (Provenance), componente de qualidade (Quality DataBase) e a fonte de esforço do projeto (Efort Source), sendo disponibilizado para os gestores na freqüência definida por a organização.
O DW é apropriado com a periodicidade que a organização define (semanalmente, quinzenalmente ou mensalmente).
A única condição para essa definição é que os dados avaliados sejam referentes ao período corrente e que não sejam dados &quot;ativos «no processo, ou seja, que não necessitem de atualizações.
Isso porque o modelo proposto não dá suporte a avaliações parciais;
Cada dado deve ter finalizado o seu ciclo.
Em este Capítulo foi apresentada a arquitetura proposta como solução juntamente com o detalhamento da construção de seus componentes.
É importante ressaltar que a etapa de aquisição de conhecimento foi bastante trabalhosa e não trivial, exigindo bastante tempo e planejamento.
No entanto, acreditamos que acrescentou significativamente valor à qualidade da solução por capturar um conhecimento real inerente às pessoas envolvidas num ambiente de software.
Em o próximo Capítulo a solução é testada com o intuito de observar sua utilidade e aplicabilidade num ambiente real trazendo benefícios em algumas tarefas relacionadas ao processo de tomada de decisão.
Testes da solução proposta Este capítulo relata o conjunto de testes realizados sobre a solução proposta no Capítulo testes;
E e a análise dos mesmos.
Os testes têm como objetivo principal averiguar se a avaliação esquematizada por a arquitetura consegue atender aos requisitos (Capítulo 3) inicialmente colocados como essenciais para uma solução adequada para o problema de avaliação de qualidade de dados de métricas de esforço de software.
Para isso, simulamos a utilização da arquitetura numa situação real de tomada de decisão que é o processo de estimativas através da utilização de algoritmos de mineração de dados.
Tendo em vista esse objetivo de uso de métricas de esforço, lançamos a seguinte hipótese:
Os índices de erros diminuem quando os dados são previamente avaliados e preparados antes da construção do modelo preditivo.
Se a hipótese for aceita, significa que a arquitetura contribui com o processo de tomada de decisão e ainda que a avaliação da arquitetura está calibrada para executar uma avaliação correta.
Por outro lado, se a hipótese for rejeitada significa que a arquitetura pode até contribuir com o processo, mas ela ainda não está calibrada para executar uma avaliação correta.
Para obter mais de uma possibilidade de observação da hipótese construimos três modelos preditivos, onde o primeiro serve de referência pois é construído sem submeter os dados a avaliação e portanto não sofreram preparação, no segundo e no terceiro os dados de entrada de cada um são previamente avaliados e preparados de acordo com critérios de níveis de qualidade diferentes.
Uma observação importante é com relação a o processo bem comportado que a arquitetura prevê e que não é considerado no teste devido a impossibilidade de testar alguns componentes da arquitetura separadamente.
Basicamente no teste é considerado o componente de inferência que recebe como entrada a proveniência dos dados de esforço que são simulados e não rastreados efetivamente devido a a impossibilidade de teste no ambiente inicialmente projetado.
Outro fator importante a ser considerado no teste é que a granularidade das informações avaliadas é diferente da granularidade utilizada nos modelos preditivos que se pretende construir.
Em outras palavras, a construção dos modelos preditivos prevê a utilização do atributo Tamanho (que pode ser em linhas de código, pontos de função ou casos de uso) e não existe o controle de Tamanho por membro de equipe.
Logo, para a construção dos modelos preditivos a qualidade é sumarizada por média, para cada Os.
Tendo em vista estas restrições, para realizar essa análise, foram definidos os seguintes passos:
Definir uma amostra de dados de esforço coletados de um ambiente real a serem submetidos à avaliação;
Atribuir proveniência desses dados através de simulação (dedução/ intuitivamente);
Para a construção do modelo preditivo 1: Sumarizar os dados por Os (sem submeter à avaliação);
Executar o algoritmo de mineração de dados para a construção do modelo preditivo 1;
Para a construção do modelo preditivo 2: Submeter todos os dados à avaliação de qualidade em cada dimensão;
Submeter todos os dados à avaliação de qualidade total;
Preparar os dados que receberam nível de qualidade abaixo de Boa (Razoável e Muito Baixa) substituindo- os por a média dos bons, baseado num dos processos de preparação de dados apontados por Tan em de acordo com a Figura 5.37;
Sumarizar os dados por Os;
Executar o algoritmo de mineração de dados para a construção do modelo preditivo 2;
Para a construção do modelo preditivo 3: Submeteder todos os dados à avaliação de qualidade em cada dimensão;
Submeter todos os dados à avaliação de qualidade total;
Preparar os dados que receberam nível de qualidade abaixo de Razoável (Muito Baixa) substituindo- os por a média dos bons, baseado num dos processos de preparação de dados apontados por Tan em de acordo com a Figura 5.37;
Sumarizar os dados por Os;
Executar o algoritmo de mineração de dados para a construção do modelo preditivo 3;
Realizar a comparação dos índices de erros entre o modelo preditivo 1 e modelo preditivo 2;
Realizar a comparação dos índices de erros entre o modelo preditivo 1 e o modelo preditivo 3.
Passo 1 do procedimento.
Para a execução dos dois testes foi escolhido o mesmo grupo de dados para que os dados escolhidos não influenciassem no sucesso da avaliação.
Por questão de coerência, os dados utilizados nesse teste são dados semelhantes aos envolvidos na análise estatística do Trabalho Individual II, ou seja, os registros de esforço do projeto P1 da versão 11.00, que contém ordens de serviço (Os) envolvidas apenas nesta versão.
Houve um passo, então, de filtragem buscando os dados de OSs que fazem parte apenas desta versão, pois uma Os pode estar envolvida em mais de uma versão.
Em o total, foram selecionadas 40 OSs, fechando o valor de 1041 registros de esforço distribuídas entre elas.
Passo 2 do procedimento Tendo definidos os dados, foi executado o passo que trata da atribuição da proveniência dos dados.
Abaixo são discutidas a forma de obtenção das variáveis de proveniência.
Como na base fonte original do projeto real não possui rastreamento do dia de lançamento (Dl), optou- se por obter essa diferença através da observação do crescimento da identificação da atividade e o descrescimento da data de registro (Dr).
A Tabela mas em alguns momentos que tenderia a crescer ela decresce e volta a crescer novamente, nesses casos a diferença é calculada de acordo com a tendência.
No caso de as atividades 1, 2, 3 e 4 a Dr está respeitando uma ordem natural de registro então a&amp; Dr-DL é 0.
Já no caso de a atividade 5, por a tendência ela deveria ter a Dr como 2/1/2007 ou maior e no entanto ela decresce para 1/1/2007 resultando então em 1 dia de diferença no mínimo.
Assim como ocorre com a atividade 7 que por a tendência deveria ter a Dr como 3/1/2007 ou maior e no entanto ela decresce para 1/1/2007 indicando então um atraso de registro de pelo menos 2 dias no mínimo.
Para a obtenção dessa variável foi feita a diferença entre a data em que a fase finalizou (DFF) e a data do registro (Dr).
Sobre essa diferença foi feita a seguinte análise:
Essa análise é necessária porque a variável&amp; DFF/ DR busca captar quanto o dado está atrasado em relação a o término da fase, não interessando os dados registrados antes do término nem no dia do término.
Em esse sentido, a Tabela 6.2 mostra um exemplo de atribuição dessa variável de acordo com a análise descrita acima.
No caso de a atividade 2 o dado foi registrado dentro de o prazo de fechamento da fase (DFF), logo a variável&amp; DFF-DR recebe o valor 0, o que igualmente acontece com a atividade 3.
Já a atividade 4 mostra um caso em que o dado foi lançado numa fase já finalizada (2 dias após) então a variável&amp; DFF-DR recebe o valor 2 indicando que está 2 dias fora de o prazo.
Como na base fonte original do projeto real não possui rastreamento do valor original (VO), optou- se por obter essa diferença através da observação de uma tabela de log onde alguns registros da base oficial de lançamento tinham seus respectivos lançamentos nessa tabela log.
Então consideramos o valor oficial de lançamento como o VM, ou seja, o valor definitivo.
Não é confirmado que o projeto utiliza essa tabela para este fim, mas foi uma opção para simular essa variável de proveniência.
A Tabela logo entende- se que o valor não sofreu modificação, recebendo então&amp; VOM-VM o valor 0 porque essa variável tem o intuito de observar as modificações que o dado sofre.
Em a atividade 2 existe registro de log, logo é possível observar que o dado foi modificado em 0,5 hora, o que ocorre de forma similar com a atividade 3 onde o dado foi modificado em 1 hora.
A diferença foi para menos;
Contudo, a variável&amp; VO-VM não faz essa distinção através de sinais positivo e negativo porque o interesse é saber o quanto foi modificado e não se foi para mais ou para menos.
A obtenção dessa variável foi feita através do cálculo entre a data de coleta dos dados (DC) e a data de análise (De a).
A Tabela 6.4 ilustra essa atribuição.
O modelo preditivo foi construído utilizando o algoritmo de mineração M5P, através da ferramenta Weka utilizando os seguintes atributos:
Tamanho, pf ajustados, estimado projeto, estimado server, estimado client, estimado integração, real projeto, real server, real client, real integração, n docs, total estimado e total real.
Esses atributos foram consideramos porque são os principais atributos envolvidos no registro de métricas de esforço abrangendo o tamanho medido em kloc ou pontos de função, o valor estimado por um especialista para cada fase (estimado projeto, estimado client, estimado integração), o valor real de cada fase registrado por os colaboradores (real projeto, real server, real client e real integração), número de documentos envolvidos num requisito representado por o atributo n docs e total estimado e total real que são somas dos estimados e reais de cada fase.
O atributo Total estimado é o atributo classe.
Os atributos referentes à valores estimados para as fases projeto, client, server e integração são considerados porque eles melhoram as estimativas, o que mostra que o especialista que estima atualmente não pode ser eliminado com a utilização de algoritmos de mineração.
Portanto, a mineração é utilizada para aprimorar a prática dentro de a organização junto ao especialista.
Realizando o método de teste chamado avaliação cruzada, foram obtidos índices de erros através da análise estatística realizada por a própria ferramenta Weka.
Esses índices de erros são mostrados nos passos 6 e 7 que também mostra os índices de erros do modelo preditivo 2 (com os dados com qualidade inferior a Boa substituídos por a média dos bons) e do modelo preditivo 3 (com os dados com qualidade inferior a Razoável substituídos por a média dos bons).
Passo 4 do procedimento O modelo preditivo foi construído utilizando os mesmos atributos utilizados na execução do passo 3 do procedimento.
Os dados foram submetidos à avaliação da máquina de inferência e obtiveram resultados de qualidade para cada dimensão e também receberam um nível de qualidade total baseado nesses resultados de dimensões.
Conforme indica a etapa (c) do passo 4 os dados que receberam nível de qualidade abaixo de Boa (Razoável e Muito Baixa) foram substituídos por a média dos bons, conforme indica um dos processos de preparação de dados apontados por Tan em.
O processo utilizado está descrito abaixo e a Tabela 6.5 ilustra a execução do mesmo.
Seleciona a Os;
Seleciona o critério de qualidade;
Calcula a média de esforço dos registros com qualidade acima de o critério para cada fase;
Substitui o valor de esforço dos registros com qualidade abaixo de o critério para cada fase;
Observação sobre o processo:
Quando a fase não possuir registros com qualidade acima de o critério definido e, portanto, não for possível calcular média, optou- se por manter os registros mesmo com qualidade prejudicada.
Em a Tabela 6.5 podem ser visualizados exemplos das situações que podem ser encontradas na preparação.
As atividades 1, 2, 3 e 4 da Os A são da fase Projeto onde um desses registros recebeu avaliação inferior a Boa.
De acordo com o processo de preparação descrito anteriormente foi realizada uma média dos registros com qualidade acima de o critério, no caso, com os registros das atividades 1, 2 e 3 resultando em 3,6 sendo assim, o registro da atividade 4 que tinha o valor 1,5 horas passou a ter 3,6 horas.
Outra situação ilustrada ocorre na fase Client onde dos 2 registros, um recebeu qualidade inferior a Boa, sendo este substituído por o valor do outro (3,75) já que só possui um registro com qualidade superior a Boa.
Em a fase Server, só existe uma atividade e que recebeu qualidade inferior a Boa, neste caso, de acordo com a observação apontada no processo de preparação, o registro de esforço se mantém igual.
Por último, na fase Teste não foi necessário realizar nenhuma substituição já que todos os dados foram avaliados com qualidade superior a Boa.
Após essa preparação o algoritmo M5P foi executado e o modelo preditivo foi construído e pode ser acessado no Apêndice C. A Tabela com os resultados é mostrada no passo 6 do procedimento, conforme anteriormente descrito.
Passo 5 do procedimento Para a construção do modelo preditivo 3 foram executados os mesmos passos do passo 4 com a mudança no critério de qualidade.
Em este modelo preditivo os dados que receberam qualidade inferior a Razoável (6 6), ou seja Muito Baixa foram substituídos por a média dos registros de qualidade superior a Razoável em cada fase.
A Tabela com os resultados é mostrada no passo 7 do procedimento, conforme anteriormente descrito e pode ser acessado no Apêndice C. Passo 6 do procedimento A Tabela 6.6 mostra os índices de erros obtidos na construção dos dois modelos preditivos, o primeiro é sem submeter os dados a avaliação e o segundo submete os dados à avaliação e prepara os que tem qualidade inferior a Boa (6 7).
Como pode ser observado na Tabela 6.6, com a execução desse passo a hipótese foi rejeitada, pois todos os índices de erros não melhoraram com a submissão dos dados à avaliação e sua posterior preparação.
Uma análise aprofundada sobre as causas e a interpreta ção desse resultado é apresentada no final deste capítulo.
Passo 7 do procedimento A Tabela 6.7 mostra os índices de erros obtidos na construção dos dois modelos preditivos, o primeiro é sem submeter os dados a avaliação e o segundo submete os dados à avaliação e prepara os que tem qualidade inferior a Razoável (6 6).
Como pode ser observado na Tabela 6.7, com a execução desse passo a hipótese foi aceita, pois todos os índices de erros melhoraram com a submissão dos dados à avaliação e sua posterior preparação, além de o coeficiente de correlação (correlation Coeficient) ter aumentado.
Uma análise aprofundada sobre as causas e a interpretação desse resultado é apresentada no final deste capítulo.
Observando a Tabela 6.6 os resultados não são animadores visto que os índices de erro aumentaram e o índice de correlação diminuiu.
Uma interpretação mais detalhada sobre esses resultados mostra que isto aconteceu devido o critério de qualidade ser razoavelmente exigente (6 7), onde foram mantidos apenas os dados com qualidade superior a 7 e todos os outros foram manipulados por a preparação.
Essa manipulação pode ter sido excessiva à medida que uma grande porcentagem dos dados foi avaliada como abaixo de 7 (22% dos dados), e assim refletindo na qualidade do modelo preditivo construído a partir de eles.
Por outro lado foi bastante animador o resultado obtido com a segunda preparação já que todos os índices de erros foram melhorados e o índice de correlação aumentou.
Assim, como no primeiro resultado, acreditamos que o sucesso foi devido a a baixa exigência com relação a os dados, não se fazendo necessário grande manipulação já que foram preparados apenas os que foram avaliados como muito baixa qualidade.
Já que esse resultado se mostrou promissor com a análise realizada na ferramentaWeka, realizamos outro teste estatístico bastante indicado por os autores da área de mineração de dados:
MMRE (Mean Magnitude of Relative Error) e PRED (25).
A o mostrar o resultado desse teste estatístico esclarecemos a sua interpretação. Quanto
menor o MMRE melhor é a estimativa, logo o modelo preditivo 3 é o melhor modelo de entre esses dois por apresentar o menor MMRE.
Além disso, os valores de PRED indicam que apesar de ambos errarem bastante em suas estimativas, os erros do modelo preditivo 3 estão mais dentro de a faixa de erro (55%) enquanto que os erros do modelo preditivo 1 estão apenas 37% dentro de a margem de erro permitida.
Em esse sentido, acreditamos que a hipótese inicial de que os índices de erros diminuem quando os dados são previamente avaliados e preparados antes da construção do modelo preditivo pode ser aceita uma vez que encontramos um caso de sucesso de confirmação da hipótese, que foi a execução do passo 5 do procedimento de teste com a construção do modelo preditivo 3.
Como os benefícios de uma avaliação de qualidade de dados não são apenas em mineração de dados, outras análises foram realizadas para observar a utilidade e funcionalidade da solução proposta por este trabalho.
Em esse sentido foram realizadas análises que podem ser executadas por os gestores para observar tendências e pontos que merecem atenção para melhorar a qualidade dos dados.
Para isso foram construídos os gráficos mostrados a seguir.
Esta visão de membro por equipe (Server) permite a observação do usuários que estão contribuindo com a baixa qualidade dos dados, podendo o gestor tomar atitudes em relação a a treinamentos e até mesmo em processo de concientização.
Outro aspecto que é interessante é a abertura das causas da baixa qualidade (apontado na coluna quali) da Tabela 6.9 através da avaliação das dimensões onde, ac é a dimensão acurácia, at é dimensão atualidade, com é a dimensão completude e cons é a dimensão consistência.
Figura 6.1 -- Gráfico de qualidade de membro de equipe no mês Março na equipe Server Observando o Gráfico 6.1 é possível notar que a maioria dos usuários contribui para uma qualidade considerada entre Razoável e Boa, com a exceção dos usuários a e b que apresentam a menor qualidade, abaixo de razoável, e o usuário a e c que apresentam como Razoável.
Através da construção do Gráfico 6.2 o gestor pode ter a visualização de que forma o membro de equipe está prejudicando a qualidade dos dados.
Por exemplo, o membro b está apresentando o maior problema com relação a a completude de seus registros, já o membro j está apresentando problemas com relação a a consistência de seus registros.
É importante salientar que essa análise por membro de equipe não tem o intuito de prejudicar- lo mas sim de conseguir contribuir para a sua melhoria dentro de a equipe como um todo.
Também é possível perceber grandes oscilações de qualidade nas dimensões, o que ocorre com os usuários h, i e j onde algumas dimensões são positivamente valoradas enquanto que outras deixam a desejar como na dimensão consistência.
Figura 6.3 -- Gráfico de qualidade do mês de Fevereiro por equipe e dimensão Através da construção do Gráfico 6.3 com a Tabela 6.10 o gestor tem à sua disposição a visualização de equipes que apresentam problemas.
Por exemplo, no gráfico do mês de fevereiro a equipe de teste apresenta grandes problemas com relação a a acurácia e consist ência.
A utilização das dimensões facilita o apontamento de questões que necessitam de melhoria, e este gráfico mostra bem isto.
Em uma análise geral as fases projeto e server não possuem grandes variações de qualidade nas dimensões.
Enquanto que as outras fases, principalmente teste e client não possuem homogeneidade em seus registros de esforço.
Figura 6.4 -- Gráfico de média de qualidade por equipe no mês de Fevereiro Com a construção do Gráfico 6.4 e da visualização de sua Tabela 6.4 é possível observar o estado da qualidade de forma geral entre as equipes no mês de Fevereiro.
Confirmando o que outros gráficos já haviam demonstrado, no mês de fevereiro a equipe de Teste apresenta problemas.
Mais uma vez as fases projeto e server são as que apresentam seus resultados de forma positiva.
Figura 6.5 -- Gráfico de qualidade geral no ano de 2007 A construção do Gráfico 6.5 e a visualização da sua Tabela 6.12 permitem ao gestor uma análise mais genérica observando a evolução da qualidade ao longo de o ano.
É possível que seja notado algum mês como problemático e como estratégia para o ano seguinte, observar com maior atenção e cuidado aquele mês para que os dados possuam melhor qualidade e consequentemente as decisões tomadas não sejam prejudicadas por a falta de confiabilidade nos dados.
Em este gráfico os meses janeiro e agosto podem ser notados como problemáticos, no entanto a falta de dados como pode ser observado também na Tabela problemáticos pois a única fase que possuem dados já apresentam uma qualidade não tão boa.
A falta de dados é explicada por a escolha dos indivíduos que fazem parte do escopo deste teste.
As fases aqui tidas como 0 (zero) possuem dados de esforço registrados em outras Os e outras versões que não foram escolhidas para fazer parte deste teste por os motivos inicialmente expostos.
Figura 6.6 -- Gráfico de qualidade de equipe no ano de 2007 por dimensões Já com a construção do Gráfico 6.6 e com a visualização de sua Tabela 6.13 o gestor pode observar as causas de problemas em determinados meses e tomar ações preventivas para que isto não ocorra no ano seguinte.
Foi atingido o objetivo principal dos testes de averiguar se a avaliação atende aos requisitos, simulando testes num ambiente real e fazendo uso de dados reais.
Os resultados obtidos e as análises realizadas mostraram que a pesquisa apresenta resultados satisfatórios e é promissora.
A etapa de preparação dos dados foi uma etapa que demandou tempo e trabalho mas que foi muito benéfica para mostrar a contribuição da pesquisa no processo de tomada de decisão no sentido de estimativas.
Em o próximo e último Capítulo são apresentadas as conclusões e os trabalhos futuros tendo em vista o que foi alcançado até aqui.
Conclusão Este trabalho propõe uma arquitetura computacional para dar suporte à avaliação de qualidade de dados referentes à medições de esforço de software.
Essa questão foi levantada através de uma análise realizada num ambiente de desenvolvimento de software real, relatada no Capítulo 3, onde por técnicas estatísticas constatou- se que a qualidade de dados é uma questão em aberto nas organizações que coletam métricas desse tipo.
Em esse estudo pôde- se observar que uma vez verificado o problema de qualidade de dados é necessária uma ação para melhorar- los.
No entanto, sem uma avaliação adequada, essa ação se torna inócua.
Para verificar a existência de uma solução adequada foi realizado um estudo sobre o estado da arte em avaliação de qualidade de dados, onde foram encontrados vários trabalhos, relatados e comparados no Capítulo 4 de acordo com os requisitos de uma solução adequada definidos no Capítulo 3.
Através dessa comparação, foi possível verificar que não há presente na bibliografia atual um trabalho que possa ser considerado como solução para o problema em questão.
Os trabalhos são bem interessantes e se esforçam para colaborar com essa tarefa que é tão difícil:
Avaliar a qualidade dos dados.
Contudo, os trabalhos não atendem às reais necessidades de gestão no contexto de desenvolvimento de software.
A Tabela 7.1 mostra a comparação final entre os trabalhos relacionados estudados e o trabalho desenvolvido durante esta pesquisa e como pode ser observado, o presente trabalho preenche totalmente todos os requisitos.
A Tabela 7.2 mostra de que maneira cada requisito é preenchido.
A arquitetura proposta como solução é composta por 4 principais componentes:
Componente de Proveniência, Componente de Inferência, Componente de Qualidade e Componente de Análise.
Os componentes foram construídos com base em uma aquisição de conhecimento realizada num ambiente operacional utilizando como instrumento de coleta de dados uma entrevista semi-estruturada, cujo formato possibilitou a construção adequada de cada componente buscando atender de forma completa os requisitos da solução, como mostra a Tabela 7.2.
Para avaliar esta solução foi efetuado um conjunto de testes, descrito no Capítulo 6, seguindo uma seqüência de passos para aceitar ou rejeitar a hipótese inicial de melhoria.
Com os resultados foi possível verificar que a contribuição da avaliação com uma das técnicas utilizadas para realização de estimativas através de um dos testes, a efetividade da utilização de dimensões uma vez que a avaliação contribui com o processo de tomada de decisão através diferentes visões e granularidades que ela permite visualizar, facilitando assim mecanismo de treinamento e conscientização dentro de a equipe de projeto, a eficácia da avaliação através da utilização da lógica fuzzy no componente de inferência evitando eliminar dados com grau de qualidade não tão alto mas que pode ser útil no processo de tomada de decisão e o sucesso na composição das dimensões através de proveniência de dados, proposta inovadora deste trabalho.
Essa solução possui uma importante contribuição em relação a trabalhos anteriores de avaliação de qualidade de dados por apresentar uma arquitetura específica e adequada para avaliação de qualidade no contexto de métricas de esforço de software.
Essa contribuição foi publicada na forma de pôster num importante evento na área de data quality que acontece no MIT (Massachusett Institute of Technology) em Cambridge, Massachusetts, Usa, chamado International Conference on Information Quality A continuidade da pesquisa visa primeiramente implementar os componentes que ainda não foram testados individualmente.
Em segundo lugar, a continuidade da pesquisa está em melhorar os componentes ou até mesmo analisar outras propostas para substituir algum de eles.
Em esse sentido, pretende-se:
Agregar novas dimensões de qualidade para que atenda outros tipos de problemas que podem ser encontrados e assim agregar novas variáveis lingüísticas e funções de pertinência.
Estudar outras alternativa para utilização no mecanismo de inferência como redes Bayesianas e redes neurais ou até mesmo sistemas NeuroFuzzy.
Possibilitar a análise de dados em termo real e assim permitir uma avaliação intermedi ária.
Atualmente a arquitetura considera um período de coleta e após, análise, não permitindo análise de evolução ou involução individual dos dados.
Integrar a arquitetura ao ambiente SPDW desenvolvido no grupo de pesquisa.
