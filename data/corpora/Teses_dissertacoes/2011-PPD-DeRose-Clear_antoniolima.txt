A análise numérica de modelos Markovianos é relevante para a avaliação de desempenho e análise probabilística do comportamento de sistemas em diversas áreas da engenharia, bioinformática e economia.
A Multiplicação Vetor--Descritor (MVD) é a operação utilizada para obter soluções estacionárias e transientes de modelos representados por meio de uma estrutura compacta, denominada descritor Markoviano.
Algoritmos que realizam a MVD normalmente geram um alto custo computacional e, por isso, alternativas como o processamento paralelo são utilizadas para produzir resultados em menos tempo.
Esse trabalho introduz um conjunto de implementações paralelas para máquinas com memória compartilhada de um algoritmo híbrido para a manipulação de descritores Markovianos e uma análise detalhada de desempenho com base em quatro modelos Markovianos reais.
As implementações são baseadas em diferentes estratégias de escalonamento utilizando OpenMP, técnicas de balanceamento de carga e decomposição presentes na literatura.
O estudo sobre o desempenho contém análise do fator de aceleração, sobrecarga de sincronização e escalonamento, políticas de escalonamento e afinidade de memória em máquinas Em uma.
Por meio de as implementações desenvolvidas, obteve- se um fator de aceleração de até oito em oito núcleos com tecnologia Intel Hyper--Threading, o que tem um impacto direto na solução de grandes modelos Markovianos.
Os resultados e lições apresentadas neste trabalho podem ser usadas por outros pesquisadores que estejam trabalhando com aplicações criadas a partir de modelos de programação similares.
Palavras-chave: Algoritmos Paralelos, OpenMP, Programação concorrente, Arquiteturas Em uma, Descritores Markovianos, Rede de Autômatos Estocásticos, Avaliação de Desempenho.
A modelagem de sistemas reais realizada por meio de formalismos Markovianos apresenta- se como uma ferramenta importante para compreender problemas de diversas áreas, como, por exemplo, economia, engenharia e bioinformática, permitindo ainda a previsão do comportamento desses sistemas em determinadas situações.
Grande poder de processamento e armazenamento são normalmente necessários para possibilitar que sistemas reais de grande complexidade possam ser descritos e analisados numericamente.
Descritores Markovianos são estruturas baseadas em propriedades e conceitos da álgebra tensorial e devido a a sua forma compacta de armazenamento são utilizados para representar grandes sistemas Markovianos.
Uma variedade de formalismos estruturados que fazem uso de descritores Markovianos estão disponíveis para a comunidade científica, como:
Redes de Petri Estocásticas (Stochastic Petri Nets -- SPN), Álgebra de Processos (Process álgebra -- PEPA) e Redes de Autômatos Estocásticos (Stochastic Automata Networks -- SAN), entre outros.
A extração de resultados de modelos Markovianos é normalmente efetuada por meio de técnicas de simulação e métodos numéricos iterativos.
Como descritores Markovianos são estruturas de representação diferentes das utilizadas em sistemas Markovianos tradicionais, algoritmos especializados precisam ser desenvolvidos.
Para possibilitar que soluções estacionárias de modelos Markovianos sejam obtidas, algoritmos que implementam o processo conhecido por Multiplicação Vetor--Descritor (MVD) são aplicados.
São exemplos desses algoritmos, o algoritmo Sparse algoritmo Shuffle e o algoritmo Split.
A MVD é responsável por multiplicar um vetor de probabilidades por o descritor Markoviano que é composto por um conjunto de termos produto- tensoriais.
A solução numérica é tradicionalmente empregada por meio de métodos iterativos que executam a MVD repetidas vezes até a convergência.
Entre vários métodos, os mais utilizados são:
O Método da Potência (Power Method), Arnoldi e GMRES (Generalized Minimal Residual).
Juntamente com a evolução dos algoritmos, o poder de processamento e armazenamento verificado em máquinas atuais tem possibilitado a avaliação de modelos Markovianos de grande complexidade.
Entretanto, devido a solução desses modelos constituir- se de um processo iterativo, onde a MVD é repetida diversas vezes, o poder de processamento atual não é suficiente para obter- se resultados em tempo hábil.
Portanto, como a maioria das máquinas atuais possuem arquiteturas paralelas, baseadas principalmente na tecnologia multinúcleo, o desenvolvimento de soluções paralelas para acelerar a MVD torna- se essencial.
O objetivo principal deste trabalho é otimizar o desempenho da Multiplicação Vetor--Descritor realizada com o algoritmo Split por meio de o uso de técnicas de paralelização com enfoque em arquiteturas paralelas com memória compartilhada.
Este trabalho introduz um conjunto de implementações paralelas do algoritmo Split para máquinas com memória compartilhada e uma análise detalhada de desempenho com base em quatro modelos Markovianos reais.
Essas implementações são baseadas em diferentes estratégias de escalonamento usando OpenMP (Open Multi--Processing), técnicas de balanceamento de carga e decomposição presentes na literatura.
O estudo sobre o desempenho das implementações consiste numa análise do fator de aceleração (speedup), sobrecarga (overhead) de sincronização e escalonamento, políticas de escalonamento e afinidade de memória.
A dissertação está organizada da seguinte forma:
O Capítulo 2 apresenta uma revisão dos principais conceitos e fundamentos sobre o formalismo de Redes de Autômatos Estocásticos, álgebra tensorial e o descritor Markoviano, bem como apresenta alguns exemplos de modelos Markovianos reais.
Em o Capítulo 3, aborda- se a MVD e os principais algoritmos para MVD, além de apresentar uma descrição das estratégias de paralelização da MVD e o conjunto de implementações paralelas desenvolvidas.
Após, no Capítulo 4, uma avaliação de desempenho detalhada das implementações é descrita, onde os modelos apresentados no Capítulo 2 são utilizados como estudos de caso.
A dissertação é finalizada por o Capítulo 5, onde são descritas as principais conclusões.
Este capítulo inicia com a apresentação dos conceitos relacionados ao formalismo estruturado de Redes de Autômatos Estocásticos, após descreve- se a estrutura utilizada para representação de modelos, o descritor Markoviano, e conceitos básicos da álgebra tensorial.
Por fim, exemplos de modelos Markovianos reais são descritos.
Os conceitos abordados são utilizados para o entendimento dos algoritmos para Multiplicação Vetor--Descritor.
Os modelos apresentados são utilizados posteriormente como casos de estudo na avaliação de desempenho das implementações paralelas desenvolvidas.
Redes de Autômatos Estocásticos (SAN).
O formalismo de Redes de Autômatos Estocásticos (Stochastic Automata Networks -- SAN), proposto na década de 80 por Plateau, tem base nas propriedades e conceitos encontrados no formalismo de Cadeias de Markov (Markov Chains).
Logo, todo sistema real que pode ser representado por meio de Cadeias de Markov, pode ser representado também em SAN.
Devido a o poder de modelagem proporcionado por o formalismo, sistemas reais complexos podem ser modelados.
Assim sendo, SAN tem grande aplicabilidade na previsão de desempenho de aplicações paralelas, bem como na avaliação de serviços presentes em sistemas distribuídos, entre outros.
Uma determinada realidade modelada por meio de Cadeias de Markov apresenta- se na forma de um único autômato estocástico constituído por todos os estados possíveis do sistema, as transições existentes entre esses estados e taxas correspondentes às frequências (tempo) de cada transição.
Esse autômato, ou a Cadeia de Markov, pode ser descrito matematicamente por meio de uma matriz de transição de estados (MT), formalmente chamada de Gerador Infinitesimal Q cuja ordem é dada por o número total de estados do autômato.
De essa forma, um dos problemas que tenta- se amenizar com a utilização de Redes de Autômatos Estocásticos é o da explosão do espaço de estados, pois com a representação do funcionamento de um dado sistema por Cadeias de Markov, o qual SAN tem equivalência, gera- se computacionalmente uma matriz esparsa de ordem tal que pode impossibilitar o seu armazenamento e, consequentemente, tornar a solução de um modelo inviável.
Sendo assim, SAN caracteriza- se por possibilitar que sistemas com um grande espaço de espaços possam ser modelados de uma forma mais compacta e modular.
Essa representação é realizada por intermédio de um conjunto de autômatos estocásticos, quase independentes entre si, onde cada autômato é formado por um conjunto finito de estados e transições entre esses estados.
Em SAN, cada autômato de um modelo possui sua própria matriz de transição de estados, onde cada elemento aij representa a taxa de transição do estado linha i para o estado coluna j.
Em a diagonal principal da matriz é necessário fazer um tratamento, de maneira que cada elemento aij, onde i $= j, deve conter o valor negativo da soma dos outros elementos de mesma linha i, ou seja, os elementos diagonais farão com que a soma de todos os elementos de uma determinada linha i, seja igual a zero.
Por exemplo, a matriz de transição de estados do autômato A seria igual a uma matriz de ordem n1, igual ao número de estados presentes em A, exemplificada na Equação 2.1.
Estado global é constituído por o conjunto dos estados locais de todos os autômatos do modelo, representando o que seria um estado da Cadeia de Markov equivalente.
O produto tensorial, também chamado de produto de Kronecker é identificado por o operador e representa a multiplicação tensorial entre duas matrizes reais A e B de dimensões e, respectivamente.
Como resultado da operação, tem- se uma matriz (tensor) C de dimensões (1 1 × 2 2).
Por exemplo, pode- se definir duas matrizes A e B de dimensões 2 x 2 e 3 x 4, respectivamente:
Tendo as matrizes definidas, o produto tensorial entre elas C $= A B é igual a:
O produto tensorial consiste basicamente em multiplicar cada elemento aij da matriz A por a matriz B, resultando numa matriz C cujo espaço de estados é dado por a multiplicação das dimensões de A e B. Entretanto, na formação do descritor Markoviano, a matriz C não é gerada.
De essa forma, se um elemento de C precisa ser acessado, o mesmo é calculado e não armazenado.
Sendo a matriz C formada por quatro blocos (número de elementos de A) de 12 elementos, se o elemento c39 é desejado significa que ele é o terceiro do último bloco (a22 b13).
Todo produto tensorial formado por N matrizes pode ser decomposto em N fatores normais.
Sendo um produto tensorial A B formado por as matrizes quadradas A e B, pode- se derivar dois tipos de fatores normais: (
A InB) e (InA B), onde InA e InB correspondem à matrizes identidades com ordens iguais à ordem de A e B, respectivamente.
De essa forma, sejam as matrizes:
Pode- se formar os fatores normais A InB e InA B:
A soma tensorial equivale a soma convencional dos fatores normais de duas matrizes A e B, ou seja, soma- se o resultado do produto tensorial de uma matriz A por uma matriz InB por o resultado do produto tensorial de uma matriz InA por uma matriz B.
Nota- se que essa operação é calculada somente para matrizes quadradas, ao contrário de o produto tensorial que é para qualquer dimensão de matrizes.
Dado as matrizes:
A soma tensorial dessas matrizes pode ser representada a partir de a Equação 2.2.
A B $= (A InB)+ (InA B) o que resulta na matriz C. Logo, C $= A B é igual a:
De essa forma, soma- se as matrizes quadradas obtidas através das operações A InB e InA B, Em a Equação 2.4 que equivale a Equação 2.3, cada linha constitui um termo produto-tensorial, ou simplesmente, termo tensorial formado por uma matriz de transição de estados Q e N -- 1 matrizes identidade Ini, onde a ordem n de cada matriz identidade corresponde a ordem da matriz Q de identificação i.
O descritor Markoviano (DM) apresenta- se como uma forma compacta de representar o gerador infinitesimal correspondente à Cadeia de Markov associada ao modelo SAN, isto é, por meio de operações tensoriais entre matrizes, ou tensores, que por sua vez representam as transições locais ou sincronizantes entre os estados de cada autômato, constrói- se matematicamente uma representação da matriz de transição de estados completa do modelo SAN.
Desta forma, pode- se dividir a representação do DM numa parte local e outra sincronizante:
Parte Local (Equação 2.5):
Composta por a soma tensorial das matrizes de transição Ql de cada autômato A (i), onde cada matriz contém as taxas das transições locais de cada autômato;
Parte Sincronizante (Equação 2.6):
Formada por um conjunto de matrizes positivo e outro negativo para cada evento de um autômato.
De essa forma, dado um valor E de eventos sincronizantes e um valor N de autômatos, a quantidade total de matrizes da parte sincronizante do DM é igual a..
Como pode ser visto na Equação 2.6, existem dois produtos tensoriais para cada evento sincronizante, um para a parte positiva Qe+ 2 e outro para a negativa Qe-3.
Uma informação importante refere- se à que as taxas de ocorrência de eventos sincronizantes estarão presentes somente na matriz do autômato que inicia o evento (tipicamente a primeira matriz do termo produto-tensorial), nas outras matrizes as taxas serão iguais a 1 (um).
Esse conceito vale tanto para a parte positiva quanto para a negativa.
Além disso, as matrizes positivas e negativas dos autômatos que não sofrem influência de determinado evento sincronizante serão matrizes identidade.
De as equações 2.5 e 2.6, pode- se formar a Equação 2.7 que representa o descritor Markoviano completo.
Com as matrizes de transição locais, pode- se decompor a soma tensorial Ql Ql normais (Ql In2)+ (In1 Ql), ou seja:
Para a parte sincronizante do descritor Qs, deve- se formar as matrizes de acordo com os eventos sincronizantes.
Considerando os autômatos A e o único evento sincronizante e1 do modelo:
Em essa seção, apresenta- se o modelo (Figura 2.3) que descreve o padrão de interação de uma equipe de desenvolvimento de software.
Esse modelo tem por finalidade analisar as probabilidades de períodos de espera para solucionar questões de projeto por diferentes participantes do projeto.
O modelo SDT é composto por um equipe central, contendo autômatos de dois estados que representam a disponibilidade da equipe central em cooperar com N participantes:
O autômato CT Availability (com estados A e U, ou seja, Available e Unavailable respectivamente, relacionados a sobreposição de fuso horário num dia de trabalho da equipe de desenvolvimento de software.
E o autômato CT Activities (com estados M e C, ou seja, gerenciamento e colaboração respectivamente).
Além disso, o modelo contém uma equipe de desenvolvimento de software composta por N autômatos de três estados:
Estado W significa que um componente está trabalhando, completando tarefas ou colaborando com outros membros da equipe;
O estado S representa que o componente está procurando por uma solução específica, informação, documentação, fontes de dados ou, ainda, aprendendo alguma questão técnica sozinha;
O estado C significa que o componente está colaborando com a equipe central para solucionar questões técnicas.
A Figura 2.3 ilustra a rede de autômatos estocásticos correspondente ao cenário descrito.
O comportamento de um membro da equipe descreve que quando membros estão trabalhando, eles podem parar enquanto (evento e) procuram uma solução (sf) ou preferencialmente movem- se para cooperar com a equipe central (evento com o) e, após isso, retornar para o trabalho (evento s).
O descritor do modelo apresenta geralmente (2 × N) eventos sincronizantes, totalizando (4 × N) termos produto- tensoriais com 2+ N matrizes.
O espaço de estados é dado por.
Essa seção descreve um modelo (Figure 2.4) para uma rede de filas aberta composta por quatro filas (A, A e A) com capacidades finitas K1, K2, K3 e K4, respectivamente.
Em o padrão de roteamento dos clientes, eles chegam em A e A com taxas constantes 1 e 2, respectivamente.
Os clientes podem deixar de A para A, se e, somente se, existir espaço na fila (comportamento bloqueante), considerando que os clientes podem deixar de A para A se houver espaço ou deixar o modelo de outra forma (comportamento de perda).
Os clientes podem também deixar de A para A com um comportamento bloqueante.
Enquanto A, A e A possuem um comportamento único de atendimento, ou seja, considerando a mesma taxa média de atendimento para todos os clientes, fila A possui um comportamento baseado em padrão alternado de atendimento (Alternate Service Pattern (ASP)).
A taxa de atendimento para essa fila varia de acordo com P padrões de atendimento diferentes.
A podem mudar seus padrões de atendimento simultaneamente com o final do atendimento de um cliente.
Portanto, quando um cliente é atendido por o padrão Pi, A pode continuar atendendo o próximo cliente no mesmo padrão com a probabilidade ii ou pode alternar para um padrão diferente Pj, com a probabilidade de ij (para todos os padrões de atendimento Pi:
Essa seção apresenta um modelo para a avaliação de uma implementação paralela mestre-escravo do algoritmo de Propagação, considerando comunicação assíncrona.
O modelo (Figura 2.5) é composto de um autômato Master de três estados (transmitting, receiving e idle), S autômatos escravos com três estados cada (idle, processing e transmitting).
Ainda, existe um autômato Buffer de K+ 1 posições.
O autômato Master é responsável por distribuir o conjunto de tarefas aos escravos e por analisar os resultados calculados por eles.
Um evento sincronizante chamado up envia um conjunto inicial de tarefas para todos os escravos e o evento sincronizante down finaliza uma execução da aplicação.
A ocorrência desse evento indica que todos os autômatos devem trocar seus estados locais para o estado inicial.
O evento sincronizante si representa o envio de uma nova tarefa para o i-ésimo escravo.
O autômato Master consome o conteúdo do Buffer por meio de o evento sincronizante c..
Finalmente, o autômato Slave termina a tarefa através da ocorrência do evento local pi.
O evento sincronizante ri representa a recepção de tarefas completadas por o Buffer.
O descritor do modelo apresenta (3 S+ 3) eventos sincronizantes, no total de (6 S+ 6) termos produto- tensoriais.
O espaço de estados é dado por.
Descritores são estruturas que possuem um grau de complexidade de manipulação extremamente alto, devido a sua estrutura tensorial, embora possuam grande vantagem em termos de eficiência de armazenamento.
De essa forma, o comportamento de sistemas de grande complexidade de tamanho pode ser avaliado e descrito por meio de formalismos estruturados, porém ao preço de um alto custo de processamento.
A Multiplicação Vetor--Descritor (MVD) é a operação chave para obter os índices de desempenho de modelos representados por meio de descritores Markovianos e consiste na multiplicação do vetor de probabilidades por o descritor.
Com base na estrutura do descritor Markoviano de SAN, sendo o vetor de probabilidades, N o número de autômatos, E o número de eventos sincronizantes do modelo e T $= N+ 2E tem- se:
Em a Equação 3.1, apresenta- se a multiplicação do vetor de probabilidades por a matriz de transição de estados Q que é equivalente a multiplicação de por o somatório do produto tensorial das matrizes locais e sincronizantes do descritor Markoviano.
Por intermédio de propriedades da álgebra tensorial, ao invés de multiplicar por o descritor completo (tornando- o uma matriz única) pode- se multiplicar- lo por cada termo tensorial, acumulando os resultados, como na Equação 3.2.
De essa forma, pode- se trabalhar com o descritor em sua forma compacta.
Qj Métodos numéricos para a MVD A fim de obter soluções transientes e estacionárias de modelos Markovianos, torna- se necessário a utilização de métodos numéricos específicos sobre a MVD.
Por meio de uma aproximação inicial ou arbitrária do vetor solução, métodos iterativos buscam refinar o resultado a cada iteração.
Por outro lado, métodos diretos alcançam num passo a solução exata, se ela existir.
Entretanto, métodos iterativos são mais adequados para a MVD, pois eles conseguem trabalhar com a forma compacta do descritor Markoviano.
Ao contrário, métodos diretos normalmente requerem que o descritor Markoviano seja resolvido numa matriz única Q, alterando- a por meio de operações de fatoração, por exemplo.
Alguns dos métodos numéricos iterativos mais utilizados para solucionar sistemas Markovianos são:
O método da Potência (Power Method), GMRES e Arnoldi.
Os métodos GMRES e Arnoldi podem convergir em menos iterações que o método da Potência.
Entretanto, para a execução desses métodos são necessários mais vetores de tamanho do espaço de estados global do que o método da Potência, fazendo que os requisitos de memória sejam consideravelmente elevados.
Por isso, o método numérico utilizado para os experimentos deste trabalho é o método da Potência que realiza a multiplicação de um vetor de probabilidades por Q, sucessivamente, aproximando- se a cada iteração da solução dita estacionária1.
O método termina quando um erro aceitável (resultado da diferença entre o vetor atual e o anterior) é atingido.
Um outro critério de parada adotado é a definição de um limite de iterações.
O processo pode ser demonstrado na Equação 3.3, onde é o vetor inicial (n) é o vetor solução.
O tamanho do vetor de probabilidades a ser multiplicado por o descritor Markoviano é dado por o espaço de estados global do modelo, isto é, o número total de estados possíveis.
De essa forma, o tamanho de é calculado por o produtório da ordem da matriz de transição de estados de cada autômato N ni.
Em modelos SAN de complexidade de tamanho alta, o espaço de estados pode i $= 1 variar de 100 a cerca de 300 milhões de estados.
Em métodos para a MVD normalmente utilizase dois vetores de probabilidades e, na presença de eventos sincronizantes, utiliza- se um terceiro, devido a uma otimização no processo.
Em esse sentido, a quantidade de memória necessária torna- se um ponto crítico.
Por outro lado, o armazenamento do descritor Markoviano não possui um custo de armazenamento equivalente.
Os conceitos discutidos nessa seção aplicam- se a todos os algoritmos para MVD apresentados na Seção 3.2.
A o realizar a MVD, algoritmos especializados são necessários para possibilitar que os resultados de modelos Markovianos sejam extraídos a partir de o descritor em sua forma compacta.
A multiplicação de por o descritor Markoviano pode ter um custo de processamento e armazenamento diferente que é dependente do algoritmo utilizado e de características do modelo, como por exemplo:
Número de elementos não-nulos que compõem as matrizes do descritor, número de autômatos e eventos sincronizantes.
A multiplicação vetor-matriz é uma operação usualmente utilizada na solução de sistemas por métodos numéricos iterativos, sendo largamente discutida na literatura.
Devido a a características das matrizes que compõem o descritor Markoviano, normalmente esparsas, abordagens e técnicas empregadas para a solução de sistemas esparsos podem ser adaptadas para a MVD, dando origem ao algoritmo Sparse.
Além de o algoritmo Sparse, constituemse algoritmos distintos para MVD:
O algoritmo Shuffle e, mais recentemente, o algoritmo Split que combina as abordagens apresentadas por os algoritmos Sparse e Shuffle.
A seguir, os algoritmos Sparse, Shuffle (Seção 3.2.2) e Split (Seção 3.2.3) são apresentados.
A criação da estrutura A numa fase de pré-processamento do método numérico é adequada para reduzir os custos de processamento, sendo que do contrário cada elemento seria gerado em tempo de execução a cada iteração do método.
Sendo assim, o custo da solução de um termo tensorial através do Algoritmo 3.1 em termos de multiplicações em ponto flutuante equivale a:
Considerando que é necessário executar a geração dos escalares somente uma vez durante todo o método numérico (normalmente milhares de iterações), o seu custo equivalente a N -- 1 N nzk de multiplicações em relação a o custo total da solução acaba não k $= 1 sendo substancial.
De essa forma, o algoritmo Sparse é caracterizado por ser eficiente em tempo de processamento, apresentando uma complexidade pessimista igual a O (n) (linhas 11 a 12), enquanto que pode tornar a solução inviável devido a o custo de armazenamento da estrutura A gerada.
Em contrapartida, na Seção 3.2.2 apresenta- se o algoritmo Shuffle que possui características que o tornam eficiente em utilização de memória com relação a o algoritmo Sparse.
3.2.2 Algoritmo Shuffle O algoritmo Shuffle corresponde a um dos algoritmos tradicionais para a MVD, considerado eficiente em termos de utilização de memória.
Essa característica é reconhecida no algoritmo Shuffle, pois ele executa a MVD sem a necessidade de resolver o descritor Markoviano ou criar qualquer estrutura adicional de armazenamento.
Esse fim é somente alcançado através da manipulação do descritor Markoviano por propriedades da álgebra tensorial, mais especificamente, da propriedade de decomposição de um termo produto-tensorial em fatores normais (Seção 2.2).
De essa forma, o algoritmo Shuffle (Algoritmo 3.2) consiste em multiplicar sucessivamente o vetor de probabilidades por cada fator normal.
Como o descritor não é resolvido, torna- se necessário realizar o cálculo de indices que determinam a localização dos elementos no tensor resultante Q. Isso é feito através de duas variáveis de deslocamento:
Nlef tk e nrightk.
Essas variáveis armazenam o valor correspondente ao produto das ordens das matrizes que estão esquerda e à direita, respectivamente, da matriz de transição Q (k) de ordem nk.
Em relação a o mapeamento dos valores, nlef t e nright indicam também quantas vezes (k) cada elemento q (i, j) da matriz Q (k) aparece no tensor resultante.
Existem três principais algoritmos utilizados para efetuar a Multiplicação Vetor--Descritor.
Entre eles, o algoritmo Sparse que apesar de ser considerado eficiente em tempo de processamento, pode inviabilizar a solução de modelos por os seus requisitos de memória.
Por outro lado, um algoritmo eficiente em memória, o algoritmo Shuffle, possui um custo de processamento consideravelmente alto.
O algoritmo Split representa uma alternativa que, por meio de heurísticas apropriadas, busca efetuar a MVD com o menor custo possível de processamento, respeitando os limites de armazenamento.
Além disso, por ser um algoritmo híbrido, o Split pode se comportar exatamente igual a solução Shuffle e a solução Sparse, se necessário.
Experimentos recentes realizados para comparar o desempenho desses três algoritmos, demonstram que o algoritmo Split é o mais promissor com relação a os algoritmos Shuffle e Sparse.
De essa forma, considerou- se o algoritmo Split como o objeto de estudo desse trabalho.
Atualmente consegue- se analisar numericamente modelos Markovianos de grande porte.
Entretanto, devido a a solução desses modelos constituir- se de um processo iterativo, onde a MVD é repetida diversas vezes, o poder de processamento atual utilizado de forma sequencial não é suficiente para obter- se resultados em tempo hábil.
Como a maioria das máquinas atuais possuem arquiteturas paralelas, o desenvolvimento de soluções paralelas para acelerar a MVD torna- se essencial.
Diante de isso, implementações paralelas do algoritmo Shuffle e Split foram desenvolvidas para execução em agregados computacionais.
Bons ganhos de desempenho foram alcançados com a implementação paralela do algoritmo Shuffle, entretanto o algoritmo Shuffle apresentou- se com baixos níveis de paralelismo, sendo que uma quantidade pequena de tarefas grandes são geradas por de meio de técnicas de decomposição.
O que, além de prejudicar a escalabilidade da solução paralela, ainda torna difícil a tarefa de balancear a carga entre os processadores.
Já na implementação paralela do algoritmo Split, realizada por meio de primitivas do padrão MPI (Message Passing Interface), verificou- se que a estratégia de paralelização utilizada trabalhou numa granularidade muito fina do algoritmo, o que resultou numa elevada sobrecarga de sincronização entre processos por a rede, o que foi intensificado por a utilização de uma rede de alta latência.
De essa forma, a implementação obteve ganhos de desempenho muito baixos.
Em ambas as implementações paralelas (Split e Shuffle), houve a necessidade de sincronização de resultados em cada iteração, tal operação foi realizada por meio de rotinas do padrão MPI.
Em esse sentido, com o crescimento da complexidade de tamanho dos modelos, ou seja, com o aumento do espaço de estados, o crescimento da sobrecarga de sincronização de resultados torna- se inevitável.
Diferentes estratégias de particionamento de dados para o algoritmo Split foram investigadas com experimentos em agregados computacionais, mostrando que um alto nível de paralelismo pode ser alcançado.
Entretanto, mesmo otimizando etapas de sincronização de dados em relação a forma como era feito anteriormente, a abordagem paralela utilizada usando rotinas MPI apresentou baixa escalabilidade, devido principalmente ao custo de atualização de vetores a cada iteração via rede.
Esses resultados motivaram o desenvolvimento de implementações paralelas para máquinas com memória compartilhada.
O modelo de programação por memória compartilhada permite a obtenção de paralelismo sem necessitar da replicação de grandes estruturas, pois todos os fluxos de execução concorrentes (threads) enxergam o mesmo espaço de endereçamento.
Além de proporcionar redução de utilização de memória, os processos de sincronização de dados também são menos custosos em comparação com o paradigma de programação por troca de mensagens (MPI).
De essa forma, ao invés de sincronizar dados via rede, o que possui um alto custo, os dados são sincronizados por o acesso/ escrita à dados compartilhados via barramento.
Além disso, como a maioria das máquinas atuais são máquinas de arquitetura multinúcleo, algoritmos desenvolvidos por meio de modelos de programação por memória compartilhada tornam- se cada vez mais utilizados.
Logo na próxima seção, apresenta- se novas soluções paralelas para o algoritmo Split com base em um modelo de programação por memória compartilhada.
A fim de obter paralelismo na MVD, deve- se primeiramente considerar o uso de estratégias de decomposição de dados.
Decomposição de dados corresponde a um método de derivação de concorrência em algoritmos que operam sobre grandes estruturas de dados e, ainda, quando operações similares são executadas sobre diferentes dados.
Essa técnica trata- se de uma forma natural de obtenção de paralelismo, onde, numa primeira fase, o dado sobre o qual os cálculos são realizados é particionado.
Em uma segunda fase, esse particionamento é utilizado para induzir o particionamento dos cálculos em tarefas.
Logo, essas tarefas podem ser executadas em paralelo sobre diferentes partições de um dado.
Em essa seção, apresenta- se estratégias de particionamento de dados na MVD com o algoritmo Split, uma descrição dos custos computacionais das tarefas geradas em cada estratégia e, ainda, as implementações paralelas do algoritmo Split.
Através do particionamento do descritor Markoviano, pode- se gerar um conjunto de tarefas independentes e, dessa forma, explorar paralelismo na MVD.
Existem duas formas de derivação de concorrência na solução de modelos Markovianos com o algoritmo Split:
Particionamento por termo produto-tensorial e particionamento por AUNF.
Essa seção apresenta essas duas abordagens, descrevendo o número de tarefas e custos computacionais envolvidos em cada abordagem.
Particionamento por termo produto-tensorial Uma possível abordagem de particionamento tem base no conjunto de termos produto- tensoriais que compõem o descritor Markoviano.
Em esse caso, os termos tensoriais representam o conjunto de tarefas a serem mapeadas (ou escalonadas) aos processadores disponíveis.
A computação de cada termo tensorial pode ser efetuada de forma independente, acessando diferentes dados e sem a necessidade de etapas de sincronização resultantes de uma interação entre processos.
Isso significa que a computação de uma tarefa (termo tensorial) não exige que outra tarefa ou resultado tenha sido previamente computado.
O número de termos tensoriais é dado por T $= N+ 2 E, onde N corresponde ao número de autômatos e E ao número de eventos sincronizantes do modelo.
Cada tarefa gerada na abordagem de particionamento por termo tensorial pode gerar também diferentes custos de processamento.
Como a geração de tarefas é efetuada de forma estática, isto é, todas as tarefas são conhecidas antes do começo da fase de solução, pode- se estimar os seus custos antes do inicio do programa paralelo.
Isso tem grande importância, visto que o conhecimento do número de tarefas e seus respectivos custos podem ajudar a produzir um melhor mapeando das tarefas aos processadores.
Sendo que uma distribuição igualitária de carga pode conduzir a melhores ganhos de desempenho do programa paralelo.
Esses custos podem ser estimados teoricamente através do cálculo do número de multiplicações necessário para computar cada tarefa, tipo de operação aritmética fundamental no algoritmo Split.
O custo computacional em termos de número de multiplicações em ponto flutuante relacionados a cada termo é dado por, onde nzj corresponde ao número total de elementos não-nulos da i-ésima matriz do termo j e N j+ 1 nj é o tamanho da partição do vetor i $= de probabilidades a ser multiplicado.
O número total de tarefas a ser processado em paralelo é dependente das características do modelo e de estratégias de solução utilizadas.
Como apresentado, o custo computacional de cada termo produto-tensorial é definido principalmente por o número de elementos não-nulos e por o valor do parâmetro de corte.
Em essa abordagem, se as tarefas possuem custos muito diferentes e existem num número limitado, torna- se difícil obter um balanceamento de carga eficiente e, ainda, pode limitar a escalabilidade da solução paralela.
Particionamento por AUNF Uma abordagem diferente de particionamento refere- se considerar cada AUNF como uma tarefa, distribuindo assim a computação dos AUNFs, ou um conjunto de eles, para cada processador.
Em o algoritmo Split, todo termo produto-tensorial é formado por um conjunto de AUNFs que representam tarefas menores (com custos menores) e que podem ser computados de forma independente.
Todos os K AUNFs do j-ésimo termo possui o mesmo custo, e se somados, o resultado equivale ao custo total do termo.
A computação de cada AUNF representa uma tarefa independente, onde uma fatia do vetor de probabilidades inicial é separada para ser multiplicado por o AUNF e, em seguida, o resultado é acumulado no vetor resultado.
O número total de AUNFs é dado por a equação.
A abordagem de particionamento por AUNF é possível, pois todo termo tensorial possui ao menos um AUNF.
Em comparação com a abordagem anterior, tem- se um número total de tarefas maior com custos computacionais menores, possibilitando que um melhor balanceamento de carga seja efetuado, pois essa situação proporciona que uma distribuição igualitária de carga seja mais facilmente alcançada.
O conceito de balanceamento de carga refere- se à realizar o mapeamento das tarefas aos processadores de forma que o conjunto de todas as tarefas seja completado no mais curto espaço de tempo possível, o que significa também buscar a minimização do tempo ocioso dos processadores.
Por outro lado, uma quantidade de tarefas maior com custos menores pode conduzir também a uma melhor escalabilidade, pois a partir de uma granularidade mais fina, o nível de paralelismo também aumenta.
Escalabilidade em aplicações paralelas significa que ao envolver mais processadores no cálculo, o ganho de desempenho paralelo também aumenta e em proporções similares.
OpenMP (Open Multi--Processing) corresponde a uma API (Application Program Interface) para programação paralela em máquinas com memória compartilhada, introduzida no final da década de 90.
A API OpenMP consiste de um conjunto de diretivas de compilador, biblioteca de rotinas e variáveis de ambiente utilizadas para a paralelização de programas em Fortran e C/ C+.
Alguns termos do padrão OpenMP são utilizados com frequência neste trabalho, como:
Diretiva: Uma diretiva OpenMP em C+ corresponde a um pragma que passa uma informação ao compilador e afeta a porção de código que o segue.
Construtor: Corresponde a uma diretiva executável do OpenMP, ou seja, uma diretiva que apresenta- se num contexto de execução e não de declaração (por exemplo, como a diretiva&amp; define do C/ C+).
Construtores são associados a instruções, laços ou blocos estruturados e podem suportar uma ou mais cláusulas.
Cláusula: Refere- se a um parâmetro adicional passado ao construtor que pode especificar comportamentos diferentes durante a execução.
A biblioteca de rotinas do OpenMP é acessada através do cabeçalho omp.
H em C/ C+.
A utilização das diretivas é efetuada por meio de um pragma.
Como outras diretivas de pré-processador, o pragma em OpenMP inicia com um carácter&amp;, logo a sintaxe de uso das diretivas pode ser descrita como:
Desenvolveu- se três implementações paralelas do algoritmo Split para máquinas com memória compartilhada utilizando a API OpenMP e a linguagem C+.
As implementações diferem em suas estratégias de particionamento de dados e de escalonamento de tarefas.
Em o início de cada iteração do método numérico, uma região paralela é criada.
O Split corresponde a um algoritmo desenvolvido com base em laços que iteram sobre os termos tensoriais e os AUNFs.
De essa forma, a paralelização foi realizada por meio de a distribuição das iterações do laço entre as threads.
O vetor de probabilidades foi tratado como uma variável compartilhada que é atualizada ao final da computação de cada tarefa.
Logo, o acesso a essa variável deve ser protegido a fim de evitar que condições de corrida provoquem a geração de resultados incorretos.
Para possibilitar que múltiplas threads possam atualizar simultaneamente o vetor compartilhado, utilizou- se o construtor atomic que corresponde a uma alternativa eficiente ao construtor critical.
Escalonamento dinâmico As duas primeiras implementações utilizam o construtor for do OpenMP para distribuir as iterações entre as threads.
Por meio de a cláusula schedule, especifica- se como as iterações do laço são atribuídas (escalonadas) às threads.
A sintaxe de utilização dessa cláusula é a seguinte:
Schedule (tipo) De entre os possíveis tipos de escalonamento para a cláusula schedule, definiu- se a utilização de um escalonamento dinâmico, indicado pela palavra dynamic.
Além disso, pode- se definir a granularidade de distribuição da carga, chamada aqui de chunk-size, que corresponde a um pedaço contíguo dentro de o espaço de iterações do laço.
Em esse caso, o chunk-size escolhido foi igual a 1.
Em a estratégia de escalonamento final definida), uma iteração por vez é atribuída a cada thread e no momento em que uma thread processa uma iteração, ela requisita outra e, assim por diante, até não existir mais iterações a processar.
O tipo de escalonamento dynamic pode ser mais adequado para cargas de trabalho não-balanceadas, principalmente quando o custo das tarefas é desconhecido.
Além disso, ao distribuir dinamicamente as tarefas às threads, consegue- se mapear possíveis interações de processadores com à memória ou disputa por outros recursos computacionais, o que dificilmente pode ser identificado previamente à execução paralela.
Alguns exemplos de custos gerados durante a execução paralela são:
Custos do mecanismo de coerência de caches em máquinas Em uma, espera para ter acesso a uma variável envolvida numa atualização atômica, acessos à memória remota (maior custo), disputa por o barramento, entre outros.
O Algoritmo 3.4 descreve a primeira implementação que utiliza uma estratégia de particionamento por termo tensorial.
Uma região paralela é criada com a diretiva&amp; pragma omp parallel e o laço é paralelizado via o construtor for (linha 2).
Em essa implementação, existem T termos a serem distribuídos entre as threads seguindo uma estratégia de escalonamento dinâmico.
Por meio de a cláusula private, especifica- se que cada thread possui sua própria cópia das variáveis j, k e do vetor.
Como múltiplas threads podem simultaneamente escrever em mesmas posições de, tratou- se a região (linha 7) com o construtor atomic.
O final do bloco paralelo ocorre após a linha 7.
O Algoritmo 3.5 usa uma estratégia de particionamento por AUNF e funciona de forma similar ao Algoritmo 3.4.
Entretanto, para realizar o particionamento por AUNF, o Algoritmo 3.5 possui uma lista global de AUNFs e contém apenas um laço para iterar sobre as tarefas.
Portanto, existe um conjunto de tarefas consistindo de todos os AUNFs gerados.
Escalonamento estático Como o escalonamento estático disponibilizado por o OpenMP (cláusula schedule (static)) não considera os custos de processamento das tarefas ao distribuir- las as threads, implementou- se um escalonamento estático de forma manual com base na solução worst-fit decreasing para o problema bin packing.
O escalonamento implementado sorteia as tarefas em ordem decrescente de custo computacional e, então, escalona uma por uma, iniciando por a thread menos carregada.
De essa forma, por meio de essa estratégia busca- se a obtenção de um balanceamento de carga eficiente, sendo executado de forma estática, ou seja, numa etapa de pré-processamento.
Os experimentos consideram quatro modelos e três variações de tamanho para cada modelo (ver Seção 2.3).
A principal diferença entre os modelos é a heterogeneidade e o número de tarefas envolvidas na computação.
Logo, esses modelos foram subdivididos com base em seus padrões de tarefas:
Tarefas homogêneas, tarefas mistas e tarefas heterogêneas.
Uma classificação com base na granularidade do problema também pode ser feita.
O número e o tamanho/ custo das tarefas geradas em cada forma de decomposição define a granularidade.
A decomposição que resulta num número grande de tarefas pequenas é chamada de grão fino (granularidade fina) e a decomposição que resulta num número pequeno de tarefas grandes é chamada de grão grosso (granularidade grossa).
Os modelos diferem em termos de número de tarefas e seus custos computacionais que são iguais entre a granularidade grossa e fina.
Por exemplo, o tamanho pequeno do modelo RS possui um conjunto de 44 tarefas na granularidade grossa.
O custo total em multiplicações é igual a 369.098.752, enquanto que na granularidade fina, existe um conjunto de 176 tarefas com esse mesmo custo.
As principais características de cada modelo e seu tipo de tarefa podem ser vistos na Tabela 4.1.
Além disso, os dados de execução dos modelos RSA, SDT, ASP e MSA gerados com a solução Split sequencial, juntamente com o método da Potência, são apresentados na Tabela 4.2, Tabela 4.3, Tabela 4.4 e Tabela 4.5, respectivamente.
Para cada modelo realizou- se 500 iterações do método da Potência divididas em 5 execuções para 2, 4, 8 e 16 threads a fim de obter dados de fator de aceleração, eficiência e desvio padrão.
Sendo que para a configuração de 16 threads, todos os processadores lógicos são utilizados, o que significa que existem duas threads rodando em cada núcleo físico.
A tecnologia Intel Hyper--Threading tem o objetivo de maximizar a utilizar de cada núcleo e, dessa forma, o cálculo de eficiência foi realizado, considerando até 8 núcleos de processamento.
O fator de aceleração (Facel) corresponde a razão entre o tempo de execução sequencial (Ts) e o tempo de execução paralelo (Tp) (Equação 4.1), em outras palavras, mostra quantas vezes a execução paralela foi mais rápida que a execução sequencial.
A eficiência (Ep) corresponde à medida de utilização dos processadores, definida por a razão entre o fator de aceleração obtido e o número de processadores (nprocs), apresentada por a Equação 4.2.
Através da metodologia apresentada avaliou- se as três implementações abaixo:
TP-Dyn (Algoritmo 3.4):
Particionamento por termo produto-tensorial (tarefas em grão grosso) seguindo uma estratégia de escalonamento dinâmico.
AUNF-Dyn (Algoritmo 3.5):
Particionamento por AUNF (tarefas em grão fino) com escalonamento dinâmico.
AUNF-Man (Algoritmo 3.6):
Escalonamento estático de tarefas utilizando particionamento por AUNF.
Em essa seção, apresenta- se os principais resultados das três implementações OpenMP do algoritmo Split para quatro modelos Markovianos e suas variações (Seção 2.3).
Após apresentar os resultados, na Seção 4.3.4 e Seção 4.3.5 discute- se o impacto da política de alocação de memória interleaving e a análise de sobrecarga de escalonamento e sincronização, respectivamente.
A Figura 4.2 apresenta os gráficos do fator de aceleração com o aumento do número de threads para as três implementações:
TP-Dyn (Algoritmo 3.4), AUNF-Dyn (Algoritmo 3.5) e AUNFMan (Algoritmo 3.6).
Como pode- se observar, as implementações apresentaram um comportamento similar, pois as tarefas distribuídas possuem o mesmo custo em ambas as granularidades, não gerando complicações no balanceamento de carga.
A diferença máxima entre o mais alto e o mais baixo fator de aceleração foi de aproximadamente 8% (por exemplo, para o tamanho pequeno).
A implementação AUNF-Dyn obteve os melhores resultados para todos os tamanhos de modelo, obtendo um fator de aceleração de até 6.8 para uma eficiência de 85%.
Esse comportamento ocorreu, pois AUNF-Dyn trabalha numa granularidade mais fina se comparada com a implementação TPDyn, o que permite a obtenção de um maior nível de paralelismo e, consequentemente, uma melhor escalabilidade.
Além disso, AUNF-Dyn utiliza um escalonamento dinâmico, diferente da abordagem AUNF-Man, e, dessa forma, a implementação AUNF-Dyn consegue mapear possíveis interações dos núcleos com à memória, por exemplo, ou disputa por outros recursos computacionais, o que não é identificado no cálculo de custos computacionais efetuado previamente à execução na implementação AUNF-Man..
Essa seção discute os resultados de desempenho para os modelos SDT e ASP.
De forma geral, esses modelos possuem tarefas com diferentes custos computacionais.
Entretanto, um conjunto considerável dessas tarefas tem um mesmo custo, o que gera um baixo nível de heterogeneidade de tarefas nesses modelos.
Portanto, ASP e SDT foram classificados como modelos do tipo misto de tarefas.
A Figura 4.4 apresenta os dados de desempenho coletados para o modelo ASP.
A implementação AUNF-Man alcançou o melhor fator de aceleração, valor aproximado igual a 7.4 e uma eficiência de 92%.
Para o tamanho pequeno de modelo existem poucas tarefas a distribuir e, portanto, não suficiente para obter- se uma boa escalabilidade com 16 threads na granularidade grossa (TP-Dyn).
Como podemos ver nos resultados de desempenho de and, a implementação TP-Dyn obteve uma melhor escalabilidade, mas ainda não satisfatória com 16 threads.
AUNF-Dyn obteve o mais baixo fator de aceleração, pois o número de iterações (tarefas) é suficientemente grande para gerar uma sobrecarga de escalonamento dinâmico.
AUNF-Man trabalha na mesma granularidade que AUNF-Dyn, mas com um estratégia de escalonamento estático que não gera a mesma sobrecarga que AUNF-Dyn.
O modelo MSA é composto por um conjunto de tarefas heterogêneas em ambas as granularidades e, ainda, em grande quantidade se comparado com os outros modelos.
A Figura 4.5 apresenta os resultados de desempenho para o modelo MSA.
A implementação AUNF-Dyn não obteve bons resultados de fator de aceleração por o mesmo motivo que o modelo ASP.
Isto é, o número de tarefas a distribuir entre as threads é consideravelmente alto.
Além disso, o aumento no número de threads torna- se um complicador para o escalonamento e a cláusula schedule pode produzir sobrecarga nessa situação.
Isto é observado na comparação entre o desempenho das três implementações com 16 threads, onde a diferença dos resultados da implementação AUNF-Dyn para as outras é maior.
Outra questão a se destacar é que a heterogeneidade do conjunto de tarefas do modelo torna o processo de escalonamento mais complexo, gerando um fraco balanceamento de carga.
Nota- se que a implementação AUNF-Man deveria suprir esses problemas, entretanto, como é realizado um escalonamento estático com base em custos teóricos, custos relacionados a disputa por recursos durante a execução (como o acesso ao barramento) não são mapeados. Como
pode- se observar, a implementação TP-Dyn obteve um comportamento similar a AUNF-Man para o tamanho pequeno.
Com o crescimento de tamanho do modelo, a quantidade de dados armazenados aumenta e, consequentemente, a disputa por recursos.
De essa forma, para o tamanho médio e grande, a implementação AUNF-Man obteve um desempenho inferior a implementação TP-Dyn, que por utilizar um escalonamento dinâmico, consegue mapear melhor outros custos gerados durante a execução.
Apesar disso, as implementações obtiveram um fator de aceleração de até 5.1 para uma eficiência de 64%.
Para melhorar o desempenho e escalabilidade de aplicações paralelas em máquinas Em uma, é importante levar em conta questões como afinidade de memória e thread.
Alocação de dados e mapeando de threads a processadores/ núcleos tornam- se importantes aspectos, pois o acesso à memória local é mais rápido que o acesso à memória remota e o OpenMP 2.5 não tem suporte para controlar afinidade.
Entretanto, existem bibliotecas e recursos oferecidos por o sistema operacional que podem auxiliar na aplicação de afinidade de memória e thread.
Bem conhecidas estratégias para realizar alocação de dados são as técnicas first-touch e next-touch e rotinas da API Em uma, por exemplo.
O suporte ao controle de afinidade de thread é encontrado em bibliotecas de escalonamento de processos em sistemas Linux (cabeçalho sched.
H), API Em uma, compiladores como o Intel C+ (variável de ambiente KMP_ AFFINITY), entre outros.
Considerações sobre afinidade de thread Inicialmente, afinidade de thread é aplicada para evitar que o escalonador do sistema operacional migre threads para outros processadores/ núcleos automaticamente durante a execução, o que pode implicar na perda de localidade de dados e mal uso das caches.
Outra situação possível, por exemplo, num sistema com tecnologia SMT (Simultaneous Multithreading), também conhecida por HT (Hyper--Threading, da Intel), é do sistema operacional escalonar duas threads para processadores lógicos2 diferentes, porém para o mesmo núcleo.
De essa forma, essas threads compartilham recursos do núcleo, além de possivelmente sobrecarregar- lo.
A tecnologia SMT permite que um maior nível de utilização do núcleo seja alcançado, possibilitando que dois processos executem concorrentemente num mesmo núcleo por meio de a replicação de alguns componentes de hardware, como por exemplo, registradores.
Outros recursos como caches L1 e L2, unidade lógica aritmética (ULA) são recursos compartilhados.
De essa forma, o mapeamento thread/ núcleo utilizado para a execução dos experimentos foi o ilustrado por a Figura 4.6.
Em a Figura 4.6, cada processador possui um conjunto de quatro núcleos de processamento, para cada núcleo é possível obter dois níveis de paralelismo, devido a tecnologia SMT.
De essa forma, o sistema operacional enxerga 16 processadores virtuais.
A política de afinidade de thread utilizada, intercala o mapeamento das threads aos processadores lógicos por CPU e núcleo físico.
Assim sendo, na configuração apresentada com 16 threads, a primeira thread é mapeada para o primeiro núcleo do primeiro processador, a segunda para N 0/ CPU1, a terceira para N 1/ CPU0, a quarta para N 1/ CPU1 e assim por diante.
Essa estratégia visa maximizar o uso das caches, sendo que numa configuração com duas threads, cada thread executaria num processador diferente, resultando no uso privado da cache L3.
Por outro lado, também visa o não sobrecarregamento dos núcleos, sendo que uma segunda thread é mapeada para um mesmo núcleo somente a partir de a nona thread, fazendo com que em configurações de threads de até 8, recursos do núcleo (por exemplo, cache L2) não sejam compartilhados.
Afinidade de memória A implementação sequencial do algoritmo Split realiza um acesso estático à memória, ou seja, existe um padrão no acesso à memória, onde cada tarefa acessa sempre os mesmos dados a cada iteração do método numérico.
Para o caso de implementações paralelas executadas em máquinas Em uma, tal fato possibilita que otimizações de acesso à memória para uma menor latência sejam efetuadas.
Esse tipo de otimização é realizado inserindo os dados relativos a cada tarefa no módulo de memória local do processador que a executa.
Contudo, existe um complicador no caso de o algoritmo Split, onde muitas tarefas podem ler do e escrever no mesmo dado durante a execução, não tornando possível tomar completa vantagem da alocação local de dados.
Além disso, essa abordagem é somente eficiente quando a aplicação paralela em questão utiliza- se de estratégias de escalonamento estático, ou seja, o conjunto de tarefas que cada thread deve processar é definido previamente à execução.
Por esses motivos não foi possível efetuar otimizações para latência de dados nas implementações paralelas do algoritmo Split.
Por outro lado, existe a possibilidade de otimizar o desempenho de acesso à memória para uma melhor vazão.
A maioria das aplicações tendem a preferir menor latência, entretanto, como no caso deste trabalho, a otimização para latência não ajusta- se ao problema, devido a a que tarefas manipulam dados iguais durante a execução e, ainda, ao padrão de acesso irregular à memória causado por o escalonamento dinâmico.
Portanto, otimizou- se as implementações paralelas do algoritmo Split para vazão utilizando a política interleaving, onde cada página de memória é atribuída seguindo uma abordagem round-robin sobre os módulos de memória.
O comportamento do método numérico para a MVD com o algoritmo Split paralelo com e sem o modo interleaving é ilustrado por a Figura 4.7.
Comparação entre a política de alocação de memória padrão e a política interleaving no método de solução com o algoritmo Split paralelo.
A Figura 4.7, ilustra no comportamento das implementações paralelas do algoritmo Split sem qualquer otimização de acesso à memória para máquinas Em uma.
Em essa situação, a thread mestre é responsável por alocar todos os dados necessários para a fase de solução, onde os principais dados acessados correspondem à lista de AUNFs e aos vetores de probabilidades.
Como a política de alocação de memória padrão corresponde a uma alocação local3, a lista de AUNFs e os vetores são alocados por a thread mestre no módulo de memória M0.
Sendo assim, a grande parte de acessos à memória durante uma execução paralela do método será direcionada a um único módulo de memória, gerando um alto tráfego de dados e disputa por acesso a recursos.
Além disso, essa situação é similar à que ocorre em máquinas SMP (Symmetric Multiprocessor) tradicionais, não tomando proveito da arquitetura Em uma disponibilizada e causando uma degradação do desempenho da aplicação.
Figura 4.7 corresponde ao cenário em que foi realizado os experimentos desse trabalho, onde existe a aplicação da política interleaving.
Em esse caso, numa execução paralela do método numérico, a thread mestre aloca os dados de forma distribuída entre os módulos de memória, fazendo com que os dois módulos de memória sejam utilizados.
De essa forma, os controladores de memória são utilizados em paralelo.
Ocorre também uma minimização do tráfego de dados e, assim, consequentes ganhos de desempenho.
Contudo, os ganhos de desempenho dependem da velocidade do barramento de interconexão e da latência de acesso local e o remoto.
Isto significa que em máquinas Em uma grandes, onde a latência de acesso à memórias remotas é alto, um aplicação mais cautelosa da técnica deve ser efetuada.
Essa noção pode ser obtida através do cálculo do fator Em uma.
O fator Em uma (FN), calculado por a Equação 4.3, corresponde a razão entre a latência de acesso à memória remota (LacRem) e a local (LacLoc).
A máquina utilizada nos experimentos deste trabalho possui um fator Em uma de aproximadamente 1.5, o que representa um fator baixo, considerando que geralmente pode variar de 1.2 a 3.0 dependendo da arquitetura da máquina.
Para a maioria dos experimentos realizados com a política interleaving, o desempenho de acesso à memória foi otimizado.
Um resumo das melhoras obtidas é apresentada na Tabela 4.6.
Tabela 4.6 ­ Impacto do modo interleaving com 16 threads para o tamanho médio de modelos.
Como apresentado na Tabela 4.6, por meio de a estratégia interleaving obteve- se melhoras de até 25%.
Entretanto, no caso de o modelo ASP (implementação TP-Dyn) e MSA (implementação AUNFMan), não ocorreu uma melhora considerável.
Por outro lado, com o modelo ASP (implementação AUNF-Man) e o modelo MSA (implementação TP-Dyn), obteve- se resultados negativos produzindo uma pequena perda de desempenho.
A razão é que cada implementação induz a um padrão de acesso à memória diferente.
De essa forma, algumas implementações geraram um quantidade de acessos à memória remota maior que o esperado.
No caso de os modelos ASP e MSA, especificamente, o problema foi intensificado devido a o tamanho da lista de AUNFs ser consideravelmente maior que no caso de os modelos RS e SDT, sendo uma estrutura frequentemente acessada durante a execução.
Análise da sobrecarga gerada por diferentes estratégias de escalonamento dinâmico com aumento do número de iterações.
Com a finalidade de avaliar as cláusulas de escalonamento foi desenvolvido um benchmark contendo um laço que executa uma soma.
A paralelização foi efetuada através do construtor for, onde todas as threads atualizam uma variável privada.
Além disso, o construtor for não foi combinado com o construtor parallel a fim de poder coletar os tempos de execução sem influência de sobrecarga de criação de threads.
Em a Figura 4.8, apresenta- se o tempo de processamento em segundos do benchmark sequencial, paralelo usando a cláusula schedule e a cláusula schedule para 16 threads.
Os resultados demonstram que a sobrecarga da estratégia de escalonamento dynamic, 1 é relacionado ao aumento do número de iterações.
Com um número pequeno de iterações, a sobrecarga gerada pode ser considerada baixa.
Além disso, usando a estratégia de escalonamento guided, 1, a sobrecarga de escalonamento é severamente minimizada.
Portanto, a utilização do tipo de escalonamento guided é uma boa solução para melhorar os resultados da implementação AUNF-Dyn com os modelos ASP e MSA, pois possuem um grande número de iterações na granularidade fina.
Contudo, usar o escalonamento guided com cargas de trabalho heterogêneas não é uma tarefa simples.
Grande pedaços de tarefas são inicialmente distribuídos entre as threads, logo, se as primeiras tarefas atribuídas possuírem custos altos e as próximas tarefas atribuídas possuírem custos baixos, algumas threads podem ficar sobrecarregadas, resultando num fraco balanceamento de carga.
De essa forma, realizou- se experimentos utilizando a cláusula schedule com a implementação AUNF-Dyn por meio de o sorteio das tarefas com base em seus custos computacionais nas ordens ascendente e descendente.
Tabela 4.7 ­ Comparação entre os fatores de aceleração alcançados com os escalonamentos dynamic, 1 e guided, 1 e diferentes estratégias de ordenamento de tarefas para 16 threads.
A Tabela 4.7 apresenta os resultados do experimento com escalonamento guided para diferentes estratégias de ordenamento de tarefas.
Os resultados mostram como a aplicação é influenciada por sobrecargas de escalonamento dinâmico.
Além disso, sorteando as tarefas em ordem descendente obteve- se uma considerável perda de desempenho para o modelo MSA.
O mesmo não ocorre com o modelo ASP, pois ele possui uma menor heterogeneidade de tarefas que o modelo MSA, minimizando os efeitos de um fraco balanceamento de carga.
Sincronização de dados Normalmente, sobrecargas dependem de uma série de fatores, como do compilador utilizado, da biblioteca, do hardware, entre outros.
Com a finalidade de medir a sobrecarga em implementações paralelas com memória compartilhada, pode- se fazer uma comparação entre o tempo gasto para executar o programa paralelo utilizando 1 thread e o tempo gasto para executar o programa sequencial, sendo que no caso ideal os dois tempos devem ser iguais.
A fórmula utilizada para o cálculo da sobrecarga é apresentada por a Equação 4.4.
A partir de a Figura 4.9, observa- se uma alta sobrecarga de sincronização para os modelos MSA e SDT comparado com os modelos RS e ASP, onde foi obtido melhores ganhos de desempenho na avaliação de desempenho anteriormente apresentada.
A fim de verificar a causa dos altos níveis de sobrecarga, computou- se o número de acessos executados a regiões atômicas.
Os resultados, observados na Tabela 4.8, mostram que não existe relação entre o número de acessos à regiões atômicas com o nível de sobrecarga encontrado.
Tabela 4.8 ­ Relação entre o aumento do número de acessos a regiões atômicas e a sobrecarga medida.
Como a execução do programa ocorre com 1 thread, a causa das sobrecargas não corresponde à condições de corrida ou problemas relacionados à coerência de caches.
Durante os experimentos, observou- se que os altos níveis de sobrecarga ocorrem numa parte específica do algoritmo, onde atualizações atômicas são realizadas sobre um mesmo vetor, acessando- o por completo e com grande frequência.
Logo, o padrão de acesso é a principal causa.
Comparação entre os fatores de aceleração para 16 threads obtidos em 5 iterações e 500 iterações para as três implementações e para todos os modelos e suas variações.
Em a Tabela 4.9, apresenta- se os resultados do experimento.
Como pode- se observar, para a maioria dos resultados a melhor implementação para 5 iterações é também a melhor (números em negrito) implementação para 500 iterações para quase todos os modelos Markovianos.
Quando os resultados não batem, a diferença entre os valores são mínimos, o que significa que qualquer implementação pode ser usada.
No entanto, existe um custo associado para escolher a melhor implementação.
Como pode- se executar cada implementação, uma após a outra, sem necessariamente reiniciar o método numérico, pode- se manter os últimos resultados obtidos e continuar a execução depois de escolher a melhor implementação.
O custo é basicamente a soma do tempo de execução das duas piores implementações para 5 iterações menos o tempo de execução da melhor para 10 iterações.
A Tabela 4.10 resume o custo para encontrar a melhor implementação em relação a uma execução do método numérico até sua convergência.
A estratégia de seleção automática consiste em executar 5 iterações de cada implementação, calculando- se os fatores de aceleração para cada uma e, então, continuar a execução a partir de os últimos resultados obtidos com a implementação que apresentou o melhor fator de aceleração.
Por exemplo, o tamanho médio do modelo ASP demora cerca de 6.756 segundos para completar 987 iterações com a melhor implementação.
A partir de a estratégia de seleção automática, seriam executadas 5 iterações para a implementação TP-Dyn, AUNF-Dyn e AUNF-Man..
No caso de o modelo ASP, a melhor implementação foi a AUNF-Dyn que iria continuar a execução a partir de a 16o iteração até completar 987 iterações, o que demoraria 6.722 segundos.
Logo, o tempo total gasto com o uso da estratégia seria equivalente ao tempo para as 15 primeiras iterações com as três implementações somadas ao tempo gasto para o restante das iterações com a melhor implementação, totalizando 6.843 segundos.
Como apresentado na Tabela 4.10, o custo gerado equivale a cerca de 0.01%.
Somente com o modelo RS obteve- se um custo considerável, com todos os outros modelos o custo gerado foi muito pequeno.
O custo para o modelo RS é significativo, pois esse modelo necessita de poucas iterações para convergir se comparado com os outros modelos.
Este trabalho apresentou um conjunto de implementações paralelas da MVD com o algoritmo Split para a solução de modelos Markovianos.
Essas implementações foram desenvolvidas usando OpenMP para máquinas com memória compartilhada.
Uma série de experimentos foi executada utilizando quatro tipos de modelos Markovianos reais e três variações de tamanho para cada um.
Um conjunto importante de análises foi realizado, incluindo análise do fator de aceleração, sobrecarga de sincronização e escalonamento, políticas de escalonamento e afinidade de memória.
A partir de os experimentos realizados obteve- se um fator de aceleração de até oito usando oito núcleos com a tecnologia Intel Hyper--Threading.
Observou- se que a escolha da implementação depende do tamanho e das características do modelo a ser avaliado.
De essa forma, como utiliza- se de métodos iterativos para a solução desses modelos, executando- se poucas iterações com as três implementações é possível automaticamente selecionar a melhor de elas e, logo, acelerar a obtenção de resultados.
A diferença entre as implementações corresponde às estratégias de escalonamento e granularidade em a qual elas trabalham.
Duas das implementações utilizam estratégias de escalonamento disponibilizadas por a API OpenMP e a terceira tem base num escalonamento de tarefas manual.
Para o modelo que consiste de tarefas homogêneas, a estratégia de escalonamento dinâmico com tarefas de grão fino é mais adequada que a estratégia de escalonamento estático.
O motivo é que o número de tarefas é mínimo para gerar uma sobrecarga com a cláusula schedule.
Além disso, para o modelo composto por poucas tarefas na granularidade grossa, a escalabilidade da execução paralela não é tão boa quanto na granularidade fina para o mesmo modelo.
Isso ocorre, pois não existem tarefas suficientes a serem distribuídas entre os núcleos para obter- se escalabilidade.
Para os modelos compostos por um grande número de tarefas, a sobrecarga imposta por a cláusula schedule produz efeitos negativos nos resultados de desempenho.
Para minimizar esse efeito, o tipo de escalonamento dynamic pode ser trocado por o tipo guided.
Entretanto, o escalonamento guided pode provocar um fraco balanceamento de carga sem a aplicação de estratégias de ordenamento de tarefas.
Considerando que a cláusula guided inicialmente distribui grandes pedaços de trabalho, torna- se importante sortear as tarefas em ordem ascendente para melhorar o balanceamento de carga e, então, obter melhores ganhos de desempenho.
Outra fonte de sobrecarga encontrada por meio de os experimentos corresponde ao uso do construtor atomic, que dependendo do padrão de utilização, pode ter um impacto considerável no tempo de execução.
Além disso, aplicando a política de alocação de memória interleaving obteve- se melhoras no desempenho de acesso à memória, o que representa uma alternativa a ser utilizada principalmente em aplicações onde não é possível tomar completa vantagem da técnica first-touch.
Por exemplo, em aplicações que utilizam estratégias de escalonamento dinâmico, ou seja, o padrão de acesso à memória torna- se irregular.
As implementações apresentadas neste trabalho obtiveram altos ganhos de desempenho, o que tem um impacto direto na solução de grandes modelos Markovianos representados por descritores Markovianos.
As discussões apresentadas neste trabalho podem também ser usadas por pesquisadores que estão trabalhando com modelos de programação similares.
Além disso, este trabalho corresponde a mais um exemplo de sucesso do uso da API OpenMP para paralelização de aplicações científicas.
