Atualmente, a tecnologia de Realidade Virtual permite utilizar computadores com um grau de interação superior às interfaces tradicionais, baseadas apenas no teclado e no mouse, através de dispositivos que permitem inserir o usuário num ambiente gerado em computadores.
Em este ambiente virtual, o usuário pode visualizar em três dimensões os objetos que o compõe, com a possibilidade de interagir com os mesmos de maneira semelhante ao que ocorre no mundo real.
O grau de interatividade num ambiente virtual é influenciado por a capacidade do ambiente de rastrear determinadas partes do corpo, como a cabeça, a mão, ou até mesmo o corpo inteiro.
Também é importante que este ambiente proporcione a execução de determinadas operações, que permitam o usuário interagir com um objeto virtual como se fosse um objeto real, promovendo a sensação de que o primeiro estava imerso em outra realidade.
Infelizmente, o custo de tais equipamentos e a quantidade de fios necessários para conectar- los ao corpo do usuário, além de outras restrições, limitam a utilização da Realidade Virtual na vida diária.
Este trabalho objetiva apresentar uma alternativa ao rastreador de posição e orientação utilizado para rastrear a mão.
A sua contribuição é fazer uso das técnicas de Processamento de Imagens e Visão Computacional para implementar um rastreador de mão baseado em imagens de câmeras de vídeo.
Para isto, o projeto foi divido em três fases distintas.
A primeira fase detecta a mão numa imagem através da segmentação de pele.
Em esta fase, quatro algoritmos de segmentação de pele são implementados e vários testes são realizados, utilizando dois espaços de cores e dois modelos de cores.
A segunda fase determina a posição da mão.
Em esta fase dois algoritmos são implementados e testados.
Em a terceira a fase, a orientação da mão é determinada através de uma técnica conhecida na Visão Computacional denominada Momentos de Imagem.
Em seguida, através da análise do contorno da mão, algumas características são detectadas, como as pontas dos dedos, os vales entre os dedos e o pulso, as quais podem ser utilizadas para calcular a posição e orientação da mão em 3D.
A o longo deste volume descreve- se em maiores detalhes cada fase de desenvolvimento do projeto juntamente com as técnicas utilizadas e seus respectivos resultados.
Atualmente a forma de interação mais comum entre humanos e computadores é realizada por meio de dispositivos físicos, como teclado, mouse, caneta óptica e etc..
Poucas são as aplicações com capacidade de interpretar dados no domínio do som ou vídeo.
Entretanto a complexidade de certas aplicações cresce cada vez mais, tornando insuficientes as formas atuais de interação como, por exemplo, a visualização de uma quantidade massiva de dados em indústrias como a automobilística e em setores como a medicina, ou ainda, a manipulação de objetos 3D com o mouse, visto que este dispositivo tem seus movimentos limitados a 2D.
Entretanto, algumas pesquisas estão sendo realizadas para prover interfaces intuitivas e inteligentes de forma a melhorar a comunicação entre humanos e máquinas, tornando a interação entre os mesmos uma atividade mais natural.
Um dos objetivos dos estudos em IHC (Interação Humano-- Computador) é transformar o corpo humano ou parte de ele, em dispositivos de entrada para o computador.
A pesquisa nesta área, atualmente concentra- se no reconhecimento de gestos, reconhecimento de linguagens de sinais, rastreamento de mão, entre outros.
Em contrapartida, os progressos na área de Realidade Virtual (RV) permitem atualmente simular ambientes do mundo real, a partir de computadores.
A principal idéia que move a RV é a possibilidade de unir num ambiente virtual usuários humanos e computadores, interagindo da mesma forma como no mundo real.
Um exemplo de tal integração seria a possibilidade de projetar e manipular um objeto virtual podendo ver- lo, manipular- lo e analisar- lo, como se o mesmo existisse fisicamente, inclusive podendo sentir o objeto através da emulação do tato.
O surgimento da tecnologia de RV inaugurou um novo paradigma na simulação e interação através de computadores.
Um ambiente virtual pode ser considerado como uma interface visual em terceira dimensão.
No entanto, novos problemas surgem, pois embora se atue num mundo 3 D, muitas pessoas têm dificuldade de utilizar os sistemas que tentam imitar- lo de alguma forma, pois muitas condições e restrições para a atuação no ambiente real atualmente não podem ser representadas num ambiente virtual.
Embora a RV possua algumas limitações, várias áreas do conhecimento sofreram fortes influências com a utilização desta tecnologia.
Riva argumenta que os ambientes virtuais possibilitam pela primeira vez na história um meio que permite um grande entendimento da dinâmica dos processos cerebrais, bem como podem ser utilizados para tarefas de treinamento de novos médicos, em as quais as simulações de situações reais de emergência são utilizadas para auxiliar o novato em medicina a tomar decisões certas em situações reais a partir de várias fontes de informação.
Outro exemplo que se pode citar é o trabalho de Benes e Bueno, em o qual um Simulador de Hepatectomia permite que virtualmente um médico-aprendiz treine exaustivamente a ressecção de fígado, Figura 1.1.
Em tratamentos psicológicos, atualmente a RV é geralmente utilizada para o tratamento de fobias, em o qual o paciente é colocado num ambiente virtual que simula situações que ajudam o indivíduo a superar seu problema.
A VRT (Virtual Reality Therapy), tem sido utilizada com sucesso em terapias como medo de voar ou medo de alturas.
Maiores detalhes sobre o uso da VRT para tratamentos de distúrbios psicológicos podem ser encontrados em.
Em áreas como arquitetura e engenharia, algumas empresas e indústrias adotaram a RV como parte do processo de desenvolvimento de seus produtos, sob a justificativa de que esta tecnologia permite melhorar a qualidade da produção. Como
exemplo pode- se citar a Embraer que possui um centro de Realidade Virtual (CRV), que permite reduzir o tempo de desenvolvimento de novas aeronaves.
O CRV está equipado com um avançado hardware gráfico, que garante aos engenheiros da empresa visualizar, em três dimensões, toda a estrutura de uma aeronave em fase de projeto, Figura 1.2.
Além disso, o cliente pode visualizar o avião, na fase de produção, avaliar a configuração do mesmo e personalizar o produto conforme suas necessidades.
No caso de a arquitetura, a RV é utilizada para se ter uma idéia de como será uma determinada obra, principalmente como será o interior da mesma, sem a necessidade de construir uma maquete real.
Em o Departamento de Expressão Gráfica da Faculdade de Arquitetura da UFRGS a realidade virtual foi utilizada para modelar os prédios de um campus universitário, com a possibilidade de navegação no interior dos mesmos, bem como permite acessar informações relativas às pessoas que freqüentam tais prédios e acervos bibliográficos, entre outros.
Este ambiente virtual foi implementado através da linguagem VRML, e está disponível na Internet.
Em a Figura 1.3, tem- se um exemplo de modelagem de um prédio do campus da PUCRS) através de uma maquete virtual).
Para que fosse viável tamanho grau de interação, novos dispositivos de entrada e saída foram desenvolvidos.
Os principais dispositivos de entrada usados em RV são os rastreadores de movimento e as luvas de entrada de dados, vista na Figura 1.4.
A eficiência no rastreamento dos movimentos do usuário é um dos principais determinantes da maior ou menor sensação de imersão do usuário num ambiente virtual.
É a partir de a captação dos movimentos do usuário que é possível interpretar comandos e executar- los de maneira adequada.
Para este fim, utilizam- se os dispositivos de rastreamento, ou tracking devices, cuja principal função é fornecer a posição e/ ou orientação de uma parte do corpo do usuário.
Tais dispositivos baseiam- se em algum princípio da física e podem ser classificados nos seguintes tipos:
Rastreadores mecânicos:
São dispositivos articulados que rastreiam um determinado ponto do corpo do indivíduo, denominado ponto de referência, em relação a outro ponto fixo conhecido, tomado como base.
Em a Figura 1.5 (a) tem- se um exemplo de um rastreador mecânico, construído por Ivan Sutherland que possibilita rastrear a cabeça do usuário, e na Figura 1.5 (b) tem- se um rastreador da MicroScribe, utilizado para modelagem 3D de objetos;
Rastreadores Acústicos: São rastreadores que localizam a posição de um determinado objeto por meio de a reflexão do som.
A posição é obtida através do cálculo de distância em função de o tempo decorrido entre a emissão de um som e sua recepção, logo após ser refletido por o alvo;
Rastreadores Magnéticos: É o tipo de tecnologia de rastreador mais utilizada atualmente em ambientes virtuais.
O princípio de funcionamento desses dispositivos é que quando um fio elétrico é submetido a um campo magnético, surge uma corrente elétrica induzida.
Através de um receptor colocado no objeto a ser rastreado e da análise das correntes induzidas sobre ele é possível calcular a posição do objeto em relação a um emissor de campos magnéticos;
Rastreadores Ópticos: São rastreadores que utilizam imagens para determinar a posição de um ponto.
Existem dois tipos de rastreadores ópticos, aqueles que utilizam câmeras instaladas no ambiente que captam a imagem do usuário, obtendo a sua posição através da identificação de pontos marcados no corpo do mesmo e aqueles que utilizam câmeras acopladas no próprio corpo do usuário, identificando pontos marcados no ambiente.
Motivação e Objetivos do Trabalho A necessidade de equipamentos caros acoplados ao corpo limita muito a utilização da RV em tarefas do cotidiano.
No caso de a mão, que é o principal meio de manipulação de objetos no mundo real, a sua utilização num ambiente virtual se dá com a utilização das luvas de entrada de dados, bem como de outros dispositivos utilizados para detectar sua posição e orientação.
Devido a o preço de tais equipamentos, e à quantidade de fios que necessitam ser ligados a mão ou ao braço, causando um certo desconforto e limitando os movimentos do usuário, é extremamente útil e economicamente interessante eliminar a necessidade de qualquer dispositivo acoplado às mãos, mas continuar a permitir ao usuário manipular qualquer objeto dentro de o ambiente virtual de maneira semelhante ao que é feito no mundo real.
Portanto, neste trabalho pretende-se substituir o rastreador de posição e orientação por uma ou duas câmeras que captam a imagem da mão e, a partir de elas, obter a posição e orientação da mão.
Para localizar a mão numa imagem e separar- la do resto dos objetos podem ser utilizadas técnicas de Visão Computacional (VC).
Em seguida, a posição e a orientação da mão podem ser calculadas por meio de a localização de determinados pontos, denominados pontos de controle, localizados sobre a mão, ou ainda pode- se utilizar informações obtidas através da linha de contorno da mão.
Como o escopo deste trabalho não inclui o reconhecimento de poses da mão, qualquer configuração dos dedos e da própria mão é aceitável.
Portanto, neste trabalho a determinação da posição e orientação da mão subdivide- se em três etapas:
Segmentar a mão de uma imagem do mundo real;
Determinar a posição 3D da mão;
Determinar a orientação 3D da mão.
Embora alguns autores, como Rehg e Kanade, considerem o rastreamento visual (visual tracking), o ato de recuperar o estado da mão através de imagens, neste trabalho a palavra rastrear é utilizada para designar o ato de determinar a posição e a orientação da mão através de imagens, não importando o estado da mesma.
O conceito de Tempo Real, neste trabalho, significa executar uma determinada função ou conjunto de funções tão rápido que o usuário não perceba que está havendo processamento nas imagens obtidas por a câmera.
Além disso a palavra segmentar será utilizado no sentido de separar ou detectar, como por exemplo, segmentar a mão significa separála do fundo da imagem ou detectar- la numa imagem.
Organização da Dissertação Este documento é composto de 8 capítulos incluindo a Introdução e está organizado da seguinte forma:
Em o Capítulo 2 são mostrados alguns projetos relacionados na área de IHC e RV.
Em o Capítulo 3 são mostrados os algoritmos de segmentação de pele implementados para separar a mão do fundo da imagem.
Em seguida, é fornecida uma análise em termos de tempo e qualidade da resposta de cada algoritmo.
Em o Capítulo 4 são mostrados os métodos utilizados para refinar a qualidade da segmentação de pele e localização de região da mão.
Também são descritos dois algoritmos para obtenção da posição da mão, bem como a avaliação em termos de tempo e qualidade da resposta de cada algoritmo.
Em o Capítulo 5 é descrita a técnica denominada Momentos de Imagens, muito utilizada em VC para descrever propriedades geométricas de objetos presentes em imagens, que neste trabalho foi utilizada para calcular a orientação da mão.
Em o Capítulo 6 são descritas algumas técnicas para detectar características específicas na superfície da mão, como a pontas dos dedos, os vales entre os dedos e os pulsos.
Em o Capítulo 7 são descritas algumas técnicas que aproveitam os resultados obtidos na detecção de características da mão para resgatar informações em 3D e obter a posição da mão no espaço.
Em o Capítulo 8 são apresentadas as considerações finais e os possíveis trabalhos futuros.
A Mão como Dispositivo de IHC A maior parte dos computadores pessoais possui algum tipo de interface gráfica com o usuário, todas elas baseadas no modelo WIMP (Window-Icon-- Mouse-Pointer).
Este modelo foi concebido através da metáfora da mesa de trabalho (desktop), onde se tem uma mesa com objetos, documentos, planilhas, etc, que são manipulados através do mouse.
A diferença é que, na mesa de trabalho real, o usuário pode escrever seus textos utilizando uma caneta, ao mesmo tempo abrir um documento, guardar outros, usar uma calculadora ou qualquer outra atividade de uma rotina de trabalho normal.
Tais atividades no computador requerem o uso do mouse para selecionar algum objeto e mover- lo.
Caso existam vários documentos abertos em janelas, algumas deverão ser minimizadas, outras arrastadas até o usuário clicar e &quot;pegar «o documento que deseja, enquanto que numa mesa real o usuário afasta os demais documentos, mesmo que este esteja empilhado com outros documentos e pega o que lhe interessa.
Essa liberdade de manipulação, existente numa mesa de trabalho real, o mouse infelizmente não fornece.
De acordo com o que foi dito na Introdução, a comunidade de RV, apesar de ter concebido equipamentos e métodos de interação que permitem construir interfaces que estão cada vez mais próximas do modelo real de área de trabalho, ainda enfrenta problemas com relação a o hardware utilizado, devido a a quantidade de fios, peso dos equipamentos, limitação de movimentos, etc..
Com o objetivo de diminuir ou até eliminar a necessidade de qualquer hardware acoplado à mão do usuário, muitas pesquisas, apoiando- se na Visão Computacional, estão sendo realizadas de forma a permitir a utilização da mão humana como dispositivo de computador através do rastreamento da mão por câmeras de vídeo.
O arranjo básico de tais sistemas é ilustrado na Figura executa uma determinada ação.
Projetos Relacionados Vários projetos científicos ou comerciais foram desenvolvidos com o objetivo de tornar as interfaces de computadores mais intuitivas e fáceis de aprender.
No que diz respeito à utilização da mão, a maioria dos projetos subdivide- se em reconhecimento de gestos ou o uso da mão como mouse virtual.
Em as seções 2.1.1 e 2.1.2, serão descritas algumas dessas aplicações.
Reconhecimento de Gestos e Poses O uso de reconhecimentos de gestos se constitui num dos mais atrativos campos de pesquisa para o desenvolvimento de interfaces inteligentes devido a a grande quantidade de movimentos possíveis, e ao modo intuitivo de como se faz uso desses gestos.
Algumas aplicações em reconhecimentos de gestos incluem reconhecimento de linguagens de sinais e reconhecimento de pose, que neste caso limitam- se a reconhecer estados da mão, como:
Mão fechada, aberta, dedo apontando para cima, para baixo, etc..
Em áreas como a robótica o reconhecimento de gestos é utilizado para controlar robôs móveis à distância.
Através de uma interface de reconhecimento de gestos alguns eletrodomésticos também podem ser controlados remotamente, conforme afirma Freeman.
Em alguns ambientes de RV, gestos e poses são utilizados para implementar a navegação dentro de ambientes virtuais, como mostrado em Sato e Saito, onde é apresentado um sistema reconhecimento de poses e gestos, que permite o usuário navegar dentro de um ambiente virtual gerado através de um grande display imersivo, mostrado na Figura 2.2.
Em as apresentações multimídia, comuns em conferências e reuniões, geralmente o palestrante é obrigado a se deslocar para o computador e utilizar o mouse ou o teclado para poder navegar por os slides.
Para facilitar a navegação, sem a necessidade de utilizar o mouse ou o teclado, Berárd, construiu um sistema que por meio de a identificação de gestos, pode- se executar os comandos &quot;next-slide «e &quot;previous-slide».
Outro sistema, com este mesmo objetivo, pode ser encontrado no trabalho de Licsár.
Wu apresenta um quadro negro virtual 3 D, destinado a ser usado em salas de aula e que permite construir objetos através de gestos.
Geralmente os sistemas de apresentações multimídia baseiam- se na arquitetura câmera-projetor, mostrada na Figura 2.3.
Em esta arquitetura a câmera é utilizada para captar a mão e seus movimentos e por meio de reconhecimento de poses ou gestos executar uma determinada função do programa que está sendo exibido por meio de o projetor.
Uso da mão como mouse Algumas pesquisas realizam esforços para substituir o mouse por algo bem mais fácil de usar.
Uma opção muito comum é utilizar um dedo como o cursor do mouse.
Bérard apresenta um sistema denominado FingerMouse que permite controlar o cursor do mouse através da mão nua.
O usuário simplesmente movimenta a mão em frente a uma câmera para posicionar o cursor do mouse na tela e os clicks do mouse são gerados quando o usuário deixa um dedo esticado por mais de 1 segundo.
Este sistema utiliza um projetor para exibir imagens do monitor numa parede, e o FingerMouse permite controlar aplicativos Windows como o Internet Explorer e o (Figura 2.4-a) ou faz desenhos utilizando um dedo no lugar do mouse (Figura 2.4-b).
Outra aplicação interessante do rastreamento de dedos no lugar do mouse, destinada a sessões de brain storm, comuns em universidades, é apresentada por Bérard.
Em seu sistema cada participante da sessão utiliza um teclado sem fio para colocar novas idéias num display fixado numa parede e utiliza os dedos para selecionar ou arrastar cada item exibido.
A o final da sessão, o resultado pode ser salvo e distribuído por a Internet.
Além de o mouse, dispositivos como joysticks, utilizados principalmente em jogos estão também sendo substituídos por a mão.
Outro exemplo, na área de jogos, é o trabalho de Parker, que substitui o mouse por a mão permitindo que o usuário utilize os mesmos movimentos que faria se estivesse jogando o jogo Solitaire com cartas reais.
Segmentação de Pele Introdução O primeiro passo para determinar a posição da mão do usuário, através de imagens, é segmentar a mão do resto do fundo.
Em muitas atividades como detecção e rastreamento de faces, detecção de mãos em imagens digitais e reconhecimento de gestos, detecção de imagens pornográficas, são utilizados algoritmos de detecção de pele.
A possibilidade de detectar a pele em sistemas de rastreamento de faces e mãos é justificada principalmente por a característica invariante da mesma, ou seja, independente do ponto de vista e das deformações ocorridas devido a rotações e perspectiva, a pele não muda suas características como cor e textura.
A cor da pele basicamente é determinada por uma substância denominada melanina.
Embora exista variação na concentração de tal substância nas diferentes raças humanas, o que permite existir peles com cores diferentes, a cromaticidade da mesma é uma característica que permite identificar- la com precisão.
Com relação a a pele humana foi constatado que, independente de suas variações (branca, negra, amarela, etc) elas tendem a formar um cluster (aglomerado) no espaço de cores, denominado cluster de pele, como aquele mostrado na Figura 3.1 (b), obtido a partir de uma imagem contendo somente pele como aquela mostrada na Figura 3.1 (a).
Em um sistema de detecção de pele, três problemas devem ser solucionados, nesta ordem:
Escolher o espaço de cores;
Modelar o que é a pele no espaço de cores escolhido;
Definir algoritmo de classificação do que é pele;
A escolha do espaço de cores define a forma como uma cor é representada numericamente.
A modelagem da pele consiste em construir um conjunto de regras ou uma estrutura de dados que define, de entre todas as cores possíveis de um espaço de cores, quais cores provavelmente são de pele e quais não são.
Os algoritmos de classificação, por sua vez, de posse das informações contidas nos modelos de pele decidem se um determinado pixel numa imagem de entrada é um pixel de pele ou um pixel que pertence ao fundo da imagem.
Em as próximas seções deste trabalho serão apresentados os espaços de cores utilizados, bem como os modelos de pele e os algoritmos de segmentação de pele implementados.
Espaço de Cores Os espaços de cores mais conhecidos e utilizados são o RGB, RGB normalizado e o HSV.
Albiol et al demonstra que o espaço de cores utilizado não tem grande influência sobre algoritmos de segmentação de pele, mas para um determinado algoritmo de segmentação, o seu desempenho pode ser maximizado quando se utiliza um espaço de cores específico.
A fim de determinar qual o melhor espaço de cor, bem como determinar qual o melhor algoritmo de segmentação para este trabalho, alguns testes foram realizados com 4 algoritmos de segmentação utilizando os espaços de cores RGB, RGB-normalizado e o HSV, respectivamente.
O modelo RGB está presente na maioria dos dispositivos de captura de vídeo sendo o espaço de cores mais utilizado em imagens digitais.
Em ele a cor é representada por a combinação de três valores que correspondem ao nível de vermelho, verde e azul, respectivamente.
Devido a o fato de que o RGB mistura os dados de cor e luminosidade, é pouco recomendado utilizar este espaço de cor em algoritmos de detecção de pele.
Em contrapartida, o espaço de cores RGB normalizado consiste em retirar uma das componentes de cor do RGB e normalizar as demais conforme as Equações 3.1 e 3.2.
Em as Equações 3.1 e 3.2 a componente B (blue/ azul) é retirada, por não possuir nenhuma informação relevante, além de diminuir a influência da iluminação e permitir a redução da dimensionalidade dos dados de 3 para 2 dimensões.
Além de o RGB, o HSV (Hue, Saturation e Value) é um espaço de cores muito utilizado em sistemas de detecção de pele.
A componente H descreve a matiz da cor, S a saturação ou a quantidade de preto na cor e finalmente, V denota o brilho ou a quantidade de branco (luz) presente na cor.
O principal motivo de se utilizar este espaço de cores é a possibilidade de amenizar os efeitos da iluminação simplesmente retirando a componente V, pois diferente do RGB este espaço de cores não mistura as cores com a luz.
Em este trabalho os três sistemas de cores foram testados e avaliados e, uma vez que se tenha definido o espaço de cores a ser utilizado o próximo passo é definir um mais modelos de pele, que serão vistos na 3.3.
Modelos de Pele Como as cores da pele humana tendem a formar um cluster no espaço de cor a classificação um pixel se resume em determinar se a cor do mesmo pertence ou não ao cluster de pele.
Para que seja possível implementar algoritmos de classificação, deve- se, num primeiro momento, modelar (definir, representar) o que é e o que não é pele.
Em este trabalho foram utilizados dois modelos de pele:
O Modelo Baseado em Regras e o Modelo Probabilístico, explicados em detalhes a seguir.
Modelo Baseado em Regras A maneira mais simples de modelar os pixels de pele consiste em analisar as cores que a pele costuma assumir, num espaço de cores específico, e representar- las explicitamente através de um conjunto de regras pré-definido que determina os intervalos ou conjunto de valores de cor que a pele pode assumir.
Este conjunto de regras servirá para decidir se um determinado pixel deve ser classificado como pixel pele ou fundo.
Os classificadores baseados em regras geralmente são rápidos, embora apresentem como principal dificuldade a necessidade de utilizar um espaço de cores específico e um conjunto de regras eficiente.
Modelo Probabilístico O Modelo probabilístico, adotado neste trabalho, é representado por uma distribuição de probabilidades que permite calcular a probabilidade de um determinado pixel ser de pele.
Em esta abordagem, surge o conceito de dados de treinamento, pois para construir o modelo de pele necessita- se previamente de imagens contendo apenas pele.
A partir de as imagens de treinamento, como aquela mostrada na Figura 3.2, constrói- se a distribuição de probabilidade da seguinte maneira:
O segundo modelo probabilístico utilizado neste trabalho baseia- se na construção de uma distribuição de intervalos de probabilidades, denominada bin-histograma.
A construção deste modelo consiste em particionar o espaço de cores em intervalos iguais, chamados de bins, conforme apresentado por Ahmad.
A geração do bin-histograma consiste em contar o número de ocorrências de pontos num determinado bin.
A o ler um pixel da imagem de treinamento, suas coordenadas de cor são utilizadas para determinar a qual intervalo de pixels (bin) ele pertence e posteriormente a freqüência do seu respectivo bin é atualizada.
Após determinar a freqüência de ocorrência de cada bin, divide- se as freqüências por a maior freqüência e normaliza- se os resultados dessas divisões entre 0 e 255.
A geração das tabelas LUT e do bin-histograma é denominada fase de treinamento.
Uma vez que se tenha definido o modelo de pele, o próximo passo é construir um algoritmo que classifique os pixels de uma imagem qualquer, de acordo com as informações contidas no modelo adotado.
A seguir serão mostrados os algoritmos de segmentação de pele avaliados neste trabalho.
Algoritmos de classificação Os modelos de pele contém somente informações sobre o que provavelmente é pele ou não.
A estratégia de classificação consiste de um algoritmo que analisa os pixels de uma imagem de entrada e de acordo com os dados contidos nos modelos de pele determina se o mesmo é de pele ou fundo.
Os quatro algoritmos implementados foram projetados e testados para classificar pixels no espaço de cores RGB-normalizado e HSV.
Alguns destes algoritmos utilizam também um modelo de fundo, sendo que este é construído da mesma maneira que o modelo de pele.
A única diferença é que as imagens, agora, são imagens de fundo que não possuem nenhuma ocorrência de pixels de pele.
Algoritmo Baseado em Regras Como visto anteriormente, a estratégia mais simples de segmentação de pele são os algoritmos baseados em regras.
Um pseudocódigo usando este conceito é mostrado no Algoritmo baseando- se na premissa de que a área ocupada por os pixels de pele é bem menor do que a área ocupada por os pixels de fundo.
Um teste com este algoritmo foi realizado utilizando um conjunto de imagens contendo somente pele e outro conjunto de imagens contendo somente imagens de fundos.
Se algoritmo não apresenta nenhuma falha de detecção de pele, então para uma imagem contendo somente pele a imagem de saída será igual a imagem de entrada, ou seja, todos os pixels de pele serão detectados.
O contrário ocorrá caso seja aplicado ao algoritmo uma imagem contendo somente fundo, pois nenhum pixel de pele será detectado.
Entretanto, como qualquer algoritmo de segmentação de pele, este também apresenta falhas de detecção de pele e, pode- se calcular a taxa de detecção de pele dividindo- se o número de pixels de pele da imagem de saída por o número de pixels de pele da imagem de entrada.
Essa taxa quanto mais alta melhor é o algoritmo.
De maneira semelhante, pode- se calcular a taxa de detecção de fundo, dividindo- se o número de pixels de fundo da imagem de saída por o número pixels de fundo da imagem de entrada.
Mas, ao contrário de taxa de detecção de pele, esta, quanto mais baixa melhor é o algoritmo, pois significa que o algoritmo está descartando os pixels de fundo presente na imagem de entrada.
Sendo assim, o Algoritmo 1 foi testado com 50 imagens de pele e fundo e, para cada uma de elas foi calculada a taxa de detecção de pele e fundo respectivamente.
Obteve- se 71% de detecção de pele e 32% de detecção de fundo, com tempo médio de processamento de 0.0043 segundos.
Em a Figura 3.3 temos um exemplo de uma imagem processada por o Algoritmo 1, que apresenta um desempenho razoável em termos de detecção de pele, mas deixa muito a desejar devido a alta taxa de falsos positivos, ou seja, pixels de fundo erroneamente classificados como pele.
Algoritmo 1 Algoritmo Regras (R, G, B) se (B\&gt; 160 and R 180 (G\&gt; 160 and R 180 and (B 100 and R 100 and or or (B/ (R+ G+ B)\&gt; 0.40) or (G/ (R+ G+ B)\&gt; 0.40) or (R 102 and G\&gt; 100 and B 160) então é fundo senão é pele;
Algoritmo Limiar Simples O algoritmo denominado Limiar Simples é um dos mais rápidos.
Em ele, um pixel de pele é classificado como sendo de pele se a sua freqüência no histograma de pele, ou no bin-histograma, for maior que um limiar (threshold).
Em o Algoritmo 2 temos a listagem do algoritmo Limiar Simples e na Figura 3.4, alguns resultados obtidos com esta técnica.
Algoritmo da Maior Freqüência Semelhante a o algoritmo anterior, este utiliza a tabela de pele e a de fundo.
O critério de seleção é simples:
Se a freqüência de um determinado pixel é maior no histograma de pele do que no histograma de fundo, então o pixel tem grande probabilidade de ser pele, portanto, classificado como tal, e vice-versa.
Em o Algoritmo 3, temos a listagem do algoritmo da Maior Freqüência e, na Figura 3.5 exemplos de imagens processadas por ele.
Algoritmo 3 Algoritmo Maior Freqüência (pixel, modelo de pele, modelo de fundo) se pele.
Frequencia\&gt; fundo.
Frequência é pele;
Algoritmo Baseado no Teorema de Bayes O quarto algoritmo construído também utiliza os modelos de pele e fundo, sendo que a probabilidade da cor do pixel ser de pele ou fundo é calculada por a Equação 3.3.
Em a Equação 3.3 p (c| pele) representa a probabilidade de ocorrer a cor c, sabendo- se que a mesma é de pele.
Os elementos p (pele) e p(¬ pele), representam a probabilidade da cor ser de pele e de fundo respectivamente, determinadas diretamente dos modelos de pele e fundo (LUT ou bin-histograma).
Essas probabilidades são calculas por as Equações 3.4 e 3.5.
Testes e Resultados Os testes foram realizados com 50 imagens do conjunto de treinamento, utilizando os espaços de cores RGB-normalizado e HSV e os modelos de cores LUT e bin-histograma.
O critério para calcular as taxas de detecção de pele e fundo é o mesmo que foi utilizado para testar o Algoritmo baseado em Regras, descrito na seção 3.4.1.
As Figuras 3.7 a 3.9 apresentam gráficos com o desempenho dos algoritmos de detecção.
A métrica de desempenho é a taxa de detecção correta dos pixels de pele e fundo.
Em os gráficos, quanto mais próximas de 1 são as taxas de detecção de pele e fundo, mais eficiente é o algoritmo.
Como um dos objetivos deste trabalho é detectar a mão numa imagem através da segmentação da pele, o algoritmo ideal seria aquele que detectasse apenas a pele humana, separando- a completamente do fundo da imagem.
Figura 3.6: Exemplos de resultados obtidos utilizando o Teorema de Bayes, com limiar $= 0.5.
Mas, como sempre existe um erro de classificação, a avaliação dos algoritmos testados leva em consideração a taxa de detecção de pele e fundo.
Portanto, quanto mais altos e próximos os valores numéricos das duas taxas melhor é o algoritmo.
Para o caso da taxa de detecção de fundo, quanto maior o valor da mesma menor o número de pixels de fundo classificados como pele, o que facilita na localização da mão, pois tem- se pequenas áreas de pixels de fundo em contraste com uma grande área formada por pixels de pele.
Em as próximas seções, além de a análise dos algoritmos em termos de eficiência na detecção de pele e fundo, também uma análise de tempo de processamento de cada algoritmo é apresentada.
Algoritmo Limiar Simples O algoritmo Limiar Simples tem seu desempenho mostrado na Figura 3.7, onde é possível notar que este, quando utiliza o limiar $= 5, atinge uma taxa alta de detecção de pele tanto para o modelo de pele LUT, como para bin-histograma, bem como no espaço de cores RGB e HSV.
Observando os resultados dos testes, pode- se afirmar que à medida que se aumenta o limiar tem- se uma queda na taxa de detecção de pele e um aumento na taxa de detecção de fundo, ou seja, existe uma quantidade grande de pixels de pele sendo classificados erroneamente como pixels de fundo.
Algoritmo da Maior Freqüência A Figura 3.8 mostra o desempenho do algoritmo Maior Freqüência, que utiliza os modelos de pele e fundo, classificando um determinado pixel como sendo pele ou fundo de acordo com a freqüência com que o mesmo ocorre nos dois modelos, ou seja, o pixel somente é considerado um pixel de pele se sua freqüência é maior no modelo de pele, e vice-versa.
Observando a Figura Limiar Simples.
Além disso, é visível que este algoritmo é mais eficiente que o anterior, pois as taxas de detecção de pele e fundo são mais equilibradas do que no algoritmo Limiar Simples, pois os valores numéricos das duas taxas são próximos.
Este algoritmo apresenta melhor eficiência quando utiliza o espaço de cores HSV, apresentando as melhores taxas de detecção de pele e fundo, principalmente quando se utiliza o modelo de pele e fundo bin-histograma com bin de tamanho 16.
Teorema de Bayes O algoritmo que utiliza o Teorema de Bayes também apresentou um desempenho semelhante ao algoritmo Maior Freqüência.
Em a Figura 3.9 (a) pode- se notar que o algoritmo, com limiar $= 0.31, também apresenta melhores resultados quando utiliza o espaço de cores HSV.
Para 1 Significa rior a 30%.
Novamente para o modelo de cores bin-histograma, o espaço de cores HSV mostra- se superior ao RGB, apresentando taxas de detecção de pele e fundo melhores, exceto quando se utiliza um bin de tamanho 32, conforme a Figura 3.9 (a).
Tempo de Execução De acordo com análise feita na seção anterior, o espaço de cores HSV demonstrou bons resultados principalmente no algoritmo da Maior Freqüência e o algoritmo baseado no Teorema de Bayes.
Entretanto, antes de classificar o pixel com pele ou fundo, existe a necessidade de converter a cor do pixel de RGB para HSV.
Como o presente trabalho exige o funcionamento destes algoritmos em tempo real, o tempo de conversão de um espaço de cores para outro deve ser considerado, bem como o tempo de processamento de cada algoritmo.
Para mensurar os tempos consumidos por os algoritmos, aplicou- se 50 imagens em cada um de eles, e o tempo gasto em cada imagem foi armazenado.
Posteriormente a média de tempo de cada algoritmo foi calculada.
Em a Figura 3.10, tem- se um gráfico com a média de tempo de cada algoritmo, medida em segundos.
Em este gráfico pode- se notar que trabalhar com RGB é mais rápido e que o pior algoritmo em termos de tempo é o algoritmo que utiliza o Teorema de Bayes no espaço de cores HSV, embora leve menos de 0.1 seg para processar uma imagem.
Testes Utilizando Imagens com Fundo Controlado O conjunto de treinamento utilizado nos exemplos anteriores consistia de imagens contendo somente pele (Figura 3.2) e imagens que não possuíam nenhuma ocorrência de pele, conforme mostrado na Figura 3.11 (a).
Como a utilização dos algoritmos de detecção de pele neste projeto não será feita em ambientes com fundo complexo como mostrado na Figura 3.11 (a), um novo conjunto de treinamento foi construído utilizando imagens como a imagem mostrada na Figura 3.11 (b), onde o fundo não tem uma grande variação de cores.
Após a construção dos modelos de cores, com imagens de fundo controlado, foram testados os algoritmos Maior Freqüência e Teorema de Bayes, por terem apresentado os melhores resultados.
Em a Figura 3.12, é possível notar a melhora no desempenho dos algoritmos Maior Freqüência e Teorema de Bayes.
Além de aumentar as taxas de detecção de pele e fundo, os algoritmos apresentaram resultados mais eficientes, uma vez que essas taxas de detecção de pele e fundo apresentam valores numéricos muito próximos.
Pode- se notar que para o espaço de cores HSV, utilizando o modelo de cor bin-histograma com bin de tamanho 4, tem- se a melhor eficiência dos dois algoritmos.
Entretanto o algoritmo Maior Freqüência é considerado superior, pois apresenta o mesmo desempenho que o algoritmo Teorema de Bayes, mas com tempo de processamento menor, conforme visto na Figura 3.10.
Portanto neste projeto o algoritmo de segmentação de pele adotado é o Algoritmo da Maior Freqüência, utilizando o espaço de cores HSV e o modelo de cores bin-histograma com bin de tamanho 4 ou 16.
Determinando a Posição da Mão Introdução Em este capítulo são descritas as técnicas utilizadas para construir uma estratégia que forneça a localização da mão do usuário, dada uma imagem de entrada.
Também serão mostrados alguns algoritmos utilizados para corrigir falhas resultantes da segmentação da pele, bem como serão mostrados os dois algoritmos construídos para determinar a posição da mão, seguidos de seus respectivos resultados.
Segmentação da Mão Devido a os problemas da segmentação, não se obtém uma separação total entre os pixels de pele e fundo, como pode ser visto na Figura 4.1.
Portanto é necessário refinar o resultado obtido da segmentação de pele obtendo o máximo de informação possível sobre a mão do usuário, para que se possa determinar sua posição com precisão.
Observando a Figura 4.1 pode- se notar as seguintes imperfeições da segmentação:
Pixels de fundo classificados erroneamente como pele;
Pixels de pele classificados como fundo, gerando buracos na superfície da mão.
Para contornar este problema e melhorar o resultado da segmentação da mão, foram construídos algoritmos auxiliares que corrigem tais falhas.
Em as próximas seções serão discutidos cada algoritmos e sua função.
Através de exemplos, serão mostrados os resultados obtidos com a aplicação dos mesmos.
Retirando pixels de fundo A estratégia utilizada neste trabalho para localizar a mão e separar- la do fundo baseia- se no fato de que a imagem resultante da segmentação de pele é composta de pequenas áreas de fundo classificadas como pele em contraste com uma grande área de pele, como mostrado na Figura Seguindo este raciocínio, a maior área contígua de pele é selecionada como sendo a mão.
Tal área é obtida através do algoritmo recursivo floodfill, em o qual dado um ponto da imagem, ele retorna o número de pixels conectados a ele, ou seja, o número de pixels que compõe a área contínua a qual ele pertence.
Em o Algoritmo 4 temos a listagem do pseudocódigo do algoritmo floodfill e, onde a variável global contador armazena o número de pixels de pele diretamente conectados ao pixel (x, y) passado como argumento para a função.
A variável contador é sempre iniciada de zero para cada pixel analisado.
Em a Figura 4.2, tem- se um exemplo da aplicação do algoritmo floodfill sobre uma imagem de entrada.
Eliminando Buracos na Superfície da Mão Conforme visto anteriormente, a segmentação produz alguns buracos na superfície da mão, como mostrado na Figura 4.1.
Para corrigir este problema, usou- se novamente o algoritmo floodfill, para preencher a região externa à mão, pintando o fundo de uma determinada cor.
Em a Figura preenche o fundo com uma determinada cor, enquanto os pixels que não pertencem ao fundo são pintados de outra cor (Figura 4.3 (b)) e finalmente, retira os pixels de fundo, deixando apenas a mão sem falhas em sua superfície (Figura 4.3 (c)).
Como é possível notar, na Figura 4.3 todo este procedimento é realizado dentro de uma região, sem a necessidade de processar a imagem inteira.
Obtendo a Posição da Mão Após corrigir as falhas presentes na superfície da mão, já é possível utilizar algum método para calcular a posição da mesma.
Considera- se como posição da mão a posição de um ponto que se encontra no centro da palma da mão.
Descobrir este ponto pode não ser uma tarefa trivial, uma vez que a mão é um objeto articulado e muda sua forma de acordo como sua orientação e a configuração dos dedos.
Para identificar a posição da mão duas estratégias foram estudadas e avaliadas, conforme será visto a seguir.
Centro de Massa A primeira estratégia utilizada para calcular a posição da mão foi determinar o centro de massa, obtido por as Equações 4.1 e 4.2, aplicadas sobre a superfície da mão.
Em as esquações n corresponde ao número de pixels da área da mão.
Xi xc $= i $= 1 yi yc $= i $= 1 Apesar de ser um algoritmo simples de implementar e rápido o suficiente para ser utilizado em tempo-real, este não apresentou o resultado esperadoem determinados casos.
Dependendo da pose da mão e da presença do antebraço na cena, o resultado pode ser um ponto fora de a região da palma da mão.
Em a Figura 4.4 temos alguns exemplos de resultados positivos obtidos.
Entretanto na Figura 4.5 tem- se situações onde o centro de massa não corresponde ao centro da palma da mão e, por meio de testes foi possível notar que este fenômeno ocorre quando se tem uma presença marcante do antebraço na cena, fazendo com que o ponto calculado se desloque em sua direção.
Os melhores resultados obtidos com esta técnica ocorre quando se tem a palma da mão paralela ao plano da câmera com os dedos esticados e com o antebraço aparecendo discretamente na cena, como visto na Figura 4.4.
Transformada da Distância A outra técnica avaliada baseia- se na Transformada da Distância, utilizada por Morris e Deimel para localizar o centro da palma da mão.
A transformada da distância é normalmente aplicada sobre uma imagem binária, cujo resultado é outra imagem, denominada imagem de distâncias.
O valor da intensidade dos pixels desta imagem é o valor da distância ao limite mais próximo de o mesmo, sendo que o limite considerado pode ser o fundo da imagem ou o contorno de um objeto.
Um exemplo de transformada da distância aplicada à uma imagem binária (Figura 4.6 (a)) pode ser visto na Figura 4.6, onde é possível perceber que quanto mais interno são os pixels dentro de o objeto, maior é o valor de suas intensidades, ou seja, maior o valor da distância em relação a o fundo da imagem, como pode ser percebido ao se observar a Figura 4.6 (b).
Em uma primeira tentativa de determinar o centro da mão, foi implementado um método que toma o contorno da mão e calcula a transformada da distância dos pontos internos à mão com relação a este, sendo a Distância Euclidiana utilizada como cálculo de distância entre dois pontos, da seguinte maneira:
Alguns resultados podem ser vistos na Figura 4.7, mostrando que este método funciona bem com diferentes configurações da mão, sendo robusto com relação a as falhas da detecção do contorno.
Em a Figura 4.7 o centro do círculo corresponde ao centro da mão.
Embora exista a possibilidade de ocorrerem situações como visto na Figura 4.8, estas não se constituem num problema grave, pois melhorando qualidade da segmentação e detecção do contorno da mão pode- se calcular o centro da mão através da transformada de distância com uma ótima precisão.
Entretanto o maior problema deste método reside no custo computacional do mesmo, devido a procura global da menor distância num dado momento e da maior distância entre as menores (passo 3), ou seja para cada pixel P pertencente à mão calcula- se a distância d (P, Q) em relação cada pixel Q pertencente ao contorno.
Além disso, o cálculo de distância é obtido por a Distância Euclidiana, que utiliza operações complexas, como potências.
Portanto, devido a estas desvantagens este método não serve para aplicações de tempo-real.
Felizmente, existe uma alternativa de calcular a transformada de distância sem utilizar a Distância Euclidiana e sem precisar utilizar o contorno da mão.
De acordo com Butt é possível utilizar uma aproximação da Distância Euclidiana de forma que o cálculo de distância entre pontos tenha baixo custo computacional e o mínimo de erro, tornando possível utilizar a transformada de distância em aplicações de tempo-real.
Segundo Gunilla, existem várias aproximações para a Distância Euclidiana, sendo as mais conhecidas as distâncias City Block, Chamfer-3- 4, Chamfer-5- 7-11 e Chessboard.
Para o cálculo do centro da mão utilizou- se a distância Chamfer-3- 4 ou DCT (Distance Chamfer Transform) que, de acordo com Morris e Deimel, apresenta os melhores resultados em termos de desempenho e precisão, devido a o uso de informações locais ao pixel analisado, como as distâncias dos pixels da vizinhança que já foram analisados.
O algoritmo para calcular a DCT consiste em deslizar uma máscara numa imagem binária, sendo que o tamanho e os valores dessa máscara variam de acordo com a aproximação utilizada.
O algoritmo realiza dois passos sequenciais denominados forward e backward respectivamente.
Em o passo forward a imagem é varrida da esquerda para direita e de cima para baixo, utilizando a máscara mostrada na Figura 4.10 (a) que mostra em destaque quais os pixels utilizados para analisar o pixel central (quadrado pintado na Figura 4.10).
Em esta fase a menor distância é calculada com relação a os pixels que estão a esquerda e aos pixels que estão acima de o pixel analisado.
Em o passo backward a imagem é varrida de baixo para cima, da direita para esquerda, utilizando- se a máscara mostrada na Figura 4.10 (b) e, a menor distância é calculada com relação a os pixels abaixo e aos pixels do lado direito do pixel analisado.
Em o Algoritmo 5 tem- se a listagem do pseudocódigo do algoritmo para calcular a DCT.
Após obter a imagem de distâncias seleciona- se o pixel de maior intensidade como o centro da mão.
Apesar desses problemas, o cálculo da transformada da distância utilizando a DCT não sofre interferência alguma devido a a presença do antebraço na cena, como acontece com o centro de massa, sendo que este método apresentou os melhores resultados.
Portanto, para este projeto a transformada da distância, utilizando a DCT como aproximação da Distância Euclidiana se constitui na melhor opção para localizar a mão do usuário, podendo ser utilizada numa aplicação de tempo-real além de ser a técnica mais robusta com relação as diversas poses da mão.
No entanto, vale lembrar que se a pose da mão for restrita a pose mostrada na Figura 4.4, o centro de massa se torna a melhor escolha para calcular a posição da mão devido a o seu desempenho, em termos de tempo, ser superior aos demais algoritmos.
Em a Tabela 4.1 tem- se a listagem do tempo médio de cada algoritmo implementado, onde é possível observar que o centro de massa é o método mais rápido, mas devido a os problemas discutidos anteriormente, ele não se mostrou uma boa opção para determinar a posição da mão.
E, apesar de os bons resultados, em termos de obtenção do centro da mão, a transformada da distância utilizando a Distância Euclidiana é o método mais lento, levando 0.5 segundos para calcular a posição da mão.
Portanto o método que possui a melhor relação entre qualidade e tempo de resposta é a transformada da distância utilizando a DCT.
Os tempos mostrados na Tabela 4.1, referem- se somente aos algoritmos de obtenção da posição da mão, sem considerar o tempo gasto em etapas anteriores como, a segmentação de pele, localização da mão e preenchimento de buracos na superfície da mão.
Além disso são mostrados, também, os tempos relativos dos algoritmos em relação a o centro de massa com o objetivo de fornecer uma noção da diferença de desempenho entre os mesmos, independente do hardware utilizado.
Pode- se notar, através da Tabela 4.1 que a DCT é 20 vezes mais lerda que o centro de massa e que a transformada da distância com a Distância Euclidiana é 538 vezes mais lerda que o centro de massa.
O tempo de processamento de algoritmo depende da quantidade de pixels analisados e, quanto maior a área de pele detectada, mais tempo cada algoritmo necessita para calcular a posição da mão.
Os momentos de imagem permitem calcular determinadas propriedades geométricas de objetos presentes numa imagem como posição, tamanho e orientação.
Tais propriedades são interessantes na implementação de propostas de reconhecimento de gestos, rastreamento de objetos e extração de características para reconhecimento de padrões, onde há a necessidade de encontrar descritores de objetos que sejam invariantes com relação a translação, rotação e escala.
Segundo Rocha e Gonzales, para uma função bidimensional f (x, y) os momentos de ordem (p+ q) são definidos por a Equação 5.1.
Como estamos aplicando a Equação 5.1 sobre um objeto binário numa imagem digital, a função f (x, y) será igual a 1 para todos os pixels que pertencem ao objeto, portanto podemos simplificar- la para a Equação 5.2: Onde A é a área do objeto, ou simplesmente o número de pixels que o compõe, e corresponde ao momento de ordem zero M00.
A partir de a Equação 5.2, pode- se obter informações como o centróide do objeto e sua orientação.
O centróide do objeto é obtido por o cálculo dos momentos de primeira ordem definidos por as Equações 5.3 e 5.4, que em última análise correspondem as equações do centro de massa apresentadas no Capítulo 4.
Em a seção 5.2 será descrito como calcular a orientação da mão numa imagem utilizando os momentos de imagens e os problemas ocorridos com o uso desta técnica.
Cálculo da Orientação da mão Para facilitar a análise de objetos que possuem um formato complexo, pode- se utilizar uma elipse equivalente que melhor descreve sua forma, como ilustrado na Figura 5.1.
A elipse equivalente permite determinar algumas das propriedades do objeto em análise, como sua posição, a qual pode ser calculada por as Equações 5.3 e 5.4, que corresponde ao centróide do objeto e também ao centro da elipse.
Além de a posição, a orientação do objeto pode ser obtida através da orientação da elipse.
Em este trabalho seguiu- se a mesma de idéia de Rocha, Freeman e Bradski, que consideram a orientação de um objeto a orientação do maior eixo principal da elipse equivalente.
A orientação deste eixo pode ser calculado por a Equação 5.5 arctan a-c onde corresponde ao menor ângulo entre o maior eixo e a horizontal e, a, b e c são calculados por as seguintes equações:
Os tamanhos dos eixos principais da elipse podem ser calculados através das seguintes 5.9 e 6 (a+ c -- b2+ (a -- c) 2) 6 (a+ c+ b2+ (a -- c) 2) onde W corresponde ao menor eixo e L corresponde ao maior eixo.
Em alguns testes notou- se que a presença do antebraço na cena prejudica o cálculo da orientação da palma da mão, pois a orientação do eixo principal da elipse é influenciada por a orientação do antebraço, uma vez que a área de ele também é utilizada no cálculo dos momentos de imagem.
Em a Figura 5.3 pode- se notar a influência do antebraço no cálculo da orientação, através da observação da orientação do maior eixo da elipse, que não corresponde a orientação da palma da mão.
Devido a este fenômeno, adotou- se a condição de que somente a mão aparece na cena, enquanto o antebraço permanece oculto por o uso de roupas de manga comprida ou qualquer outro objeto que o impeça de ser segmentado junto com a mão.
O uso de uma fita no pulso também é possível desde que área da mão seja a maior área de pele presente na imagem.
De acordo com o que foi mencionado em capítulos anteriores, uma das necessidades deste projeto é que o mesmo deve ser executado em tempo real.
Como a orientação da mão é calculada aplicando- se a Equação 5.5 sobre a área da mão, um teste foi realizado aplicando- se esta mesma equação somente sobre o contorno da mão obtido a partir de uma modificação do algoritmo floodfill, cujo pseudocódigo é listado no Algoritmo 6.
Em esta nova versão, o algoritmo floodfill quando utilizado para preencher a área da mão a fim de eliminar buracos (ver seção 4.2.2 do último pixel de pele processado por ele numa lista, obtendo dessa maneira o contorno da mão.
Para cada câmera utilizada deve- se saber sua pose (posição e orientação) no espaço e suas características internas, como a distância focal e fator de distorção radial.
Essas informações são representadas numa matriz denominada matriz da câmera que determina como um ponto do espaço 3D é representado no plano da imagem da câmera em 2 D, ou seja, essa matriz define o mapeamento (projeção) de um ponto 3D para um ponto 2D.
A obtenção destes parâmetros é feita utilizando métodos de calibração de câmera que, por meio de imagens conhecidas permitem calcular a matriz da câmera de cada câmera.
Estabelecer a correspondência entre os pares de pontos P1 e P2 na Figura 6.1, ou seja garantir que estes pontos correspondem a um mesmo ponto do espaço Pw, para que se possa recuperar a informação 3D.
Um método de extração de características, muito comum, utilizado no DigitEyes e no sistema de reconhecimento de gestos Mime é a detecção das pontas dos dedos, das juntas entre os dedos e do pulso (Figura 6.2).
Para localizar os dois primeiros elementos, pode- se analisar o contorno da mão em busca de porções do contorno onde há mudança brusca na curvatura (mínimos e máximos locais) do contorno.
No caso de o pulso, a estratégia mais comum é a busca por regiões de largura constante.
Em as seções a seguir, primeiramente será apresentado um algoritmo para gerar a seqüência de pontos que define o contorno da mão.
Este passo é necessário, pois originalmente o contorno é apenas um conjunto de pontos sobre uma imagem, sem nenhuma relação de ordem entre os pontos.
Em seguida, serão apresentados alguns algoritmos para detectar os mínimos e os máximos locais do contorno e classificar estes elementos em ponta de dedo ou vale entre os dedos.
Ordenação do Contorno Como os métodos de detecção de características, implementados neste trabalho, aplicam- se sobre o contorno da mão, há a necessidade de que o mesmo esteja ordenado de forma que se possa percorrer- lo continuamente, ou seja, partindo- se de um ponto inicial qualquer, passa- se por o seu vizinho, e assim, sucessivamente até encontrar novamente o ponto de partida.
Entretanto devido a a natureza do algoritmo floodfill, utilizado para detectar o contorno, conforme explicado no Capítulo 5, os pixels não estão numa ordem seqüencial.
Para resolver este problema foi implementado um algoritmo, cujo pseudocódigo é listado no Algoritmo 7 com o intuito de obter um contorno contínuo.
Ele recebe como entrada uma imagem binária contendo somente o contorno da mão, como um conjunto de pontos sobre uma imagem, que a partir de um ponto inicial, será percorrido em o qual pixel, vizinho ao pixel analisado, é o próximo ponto do contorno, sendo que a ordem de procura por pixels não processados é mostrada na Figura 6.3.
O algoritmo pára quando encontra por a segunda vez o ponto inicial.
O Algoritmo 7 funciona sem problemas se o contorno não tiver &quot;bifurcações», uma vez que o algoritmo floodfill retorna um contorno fechado e com 1 pixel de largura.
Mas, devido a as falhas na segmentação de pele, o contorno da mão não é uma linha com um caminho único.
Em a maioria dos casos o contorno apresenta bifurcações, como mostrado na Figura 6.4.
Se o Algoritmo 7 chegar no ponto destacado na Figura 6.4, ele não terá para onde ir e, portanto, falhará.
Uma vez que se tenha ordenado os pontos do contorno de forma que seja possível percorrer- lo como uma linha contínua, já é possível aplicar os métodos que analisam o mesmo e detectam os pontos característicos.
Em a próxima seção são apresentadas as estratégias testadas neste trabalho para determinar os pontos máximos e mínimos locais presentes no contorno, bem como os métodos empregados para a classificação dos mesmos em pontas de dedos ou vales entre os dedos.
Localizando Mínimos e Máximos Locais Em esta seção serão apresentadas duas técnicas de detecção de mínimos e máximos locais, que correspondem às pontas dos dedos e aos vales entre os dedos.
Curva de Distâncias Uma técnica utilizada para detectar os mínimos os máximos locais de uma curva baseia- se em determinar a distância de cada ponto do contorno a um determinado ponto conhecido.
Para o caso da mão pode- se utilizar o centro da mão.
Em as Figuras 6.6 (a) e 6.6 (b) tem- se dois exemplos da aplicação de tal técnica, tomando- se o centro da mão como ponto de referência e percorrendo o contorno no sentido anti-horário, a partir de o ponto mais à direita e mais abaixo.
Em a Figura 6.6 (a), pode- se notar que existem dois máximos locais na curva que correspondem às pontas dos dedos e apenas um vale entre os picos, correspondendo ao vale entre os dedos.
A mesma análise pode ser feita com relação a o gráfico da Figura 6.6 (b), onde existem quatro máximos locais devido a a presença de quatro dedos e três mínimos locais correspondendo aos vales entre os dedos.
Também é possível observar, através da Figuras 6.6 (a) e 6.6 (b) que os mínimos locais, que correspondem aos vales da mão, aparecem entre dois máximos locais.
Em a seção 6.2.2 será apresentada uma técnica mais sofisticada que permite encontrar os pontos de interesse da mão utilizando diretamente o contorno.
A outra técnica usada para determinar os pontos característicos da mão é denominada kcurvatura ou k--curva, proposta por Segen e Kumar apud.
Com esta técnica é possível localizar as pontas dos dedos e os vales entre os dedos sem a necessidade de calcular a distância de cada ponto do contorno com relação a um ponto conhecido, pois utiliza apenas o contorno.
Exemplos de uso desta técnica podem ser encontrados no sistema Mime de reconhecimento de gestos e no trabalho de Wu, que utilizou a k--curvatura para detectar os dedos.
A definição de k--curva para um determinado ponto do contorno, é o ângulo formado entre os vetores V1 $= e V2 $ , onde k é uma constante e P a lista de pontos do contorno da mão apud.
Através da utilização de valores apropriados de k pode- se obter os mínimos e máximos locais analisando o ângulo entre os vetores V1 e V2, como mostrado na Figura 6.7 (a-b).
Em esta figura observa- se que ângulos pequenos correspondem a pontos onde existe uma mudança acentuada no gradiente da curvatura do contorno, ao passo que a Figura 6.7 (c) ilustra uma situação contrária, onde não existe mudança significativa no gradiente da curva.
A obtenção deste ângulo pode ser feita através do produto escalar entre os vetores V1 e V2.
Embora seja possível identificar os pontos de inflexão no contorno, ainda não se sabe a que característica cada um se refere.
Como somente o valor do ângulo não é suficiente para realizar a classificação destes pontos, utilizam- se as direções dos vetores V1 e V2, para determinar se o ponto analisado é um vale da mão ou uma ponta do dedo.
Considerando que neste projeto a mão estará sempre com os dedos abaixo de o centro da mão, como mostrado na Figura 6.9 (b), e que o contorno será processado no sentido anti-horário pode- se concluir que se os vetores V1 e V2 tem direção para baixo) diz- se que o ponto analisado é um vale entre os dedos, por outro lado, se V1 e V2 tem direção para cima) o ponto corresponde à ponta de um dedo.
Para calcular os ângulos entre os vetores o primeiro algoritmo implementado analisa o cosseno do ângulo, dado por a Equação 6.1 num determinado ponto do contorno.
Em este caso, quanto maior o valor do cosseno menor o ângulo.
Portanto as características da mão corresponderão aos pontos com maiores cossenos.
Em a Figura 6.9 (a), tem- se um gráfico de uma k--curva construído a partir de o valor dos cossenos em cada ponto do contorno da Figura 6.9 (b).
Como pode ser observado, os máximos locais da curva, correspondem aos pontos característicos da mão.
Uma alternativa para calcular o valor de cada ponto da k--curva, apresentada por Heckenberg, é baseada na diferença entre os ângulos dos vetores V1 e V2 com relação a a horizontal, como ilustrado na Figura 6.10, calculada através da Equação 6.2, sendo que os ângulos de cada vetor com a horizontal são obtidos através das Equações 6.3 e 6.4.
Através da Equação 6.2 é possível identificar onde ocorrem os pontos de inflexão no contorno, pois esta equação mede a variação no gradiente da curvatura do contorno num determinado ponto.
Em este caso, valores grandes de corresponderão aos pontos de inflexão do contorno.
Além disso, com esta técnica é fácil classificar os pontos entre ponta de dedo ou vale entre os dedos através do sinal do valor resultante da Equação 6.2, sendo que os valores positivos correspondem às pontas dos dedos e os valores negativos aos vales entre os dedos.
Para entender melhor como isto funciona é necessário analisar qual das duas situações, mostradas na Figura 6.11, ocorre quando um ponto característico é detectado.
Para o caso da ponta do dedo os vetores V1 e V2 estão dispostos da maneira como é mostrada na Figura 6.11 (a), ou seja, com V1 apontando para baixo e V2 apontando para cima.
Levando em conta que o cálculo dos ângulos+ k e com relação a a horizontal, é obtido com o uso da tangente, Equações quadrantes.
Então, para o caso das pontas dos dedos o ângulo do vetor V1 (K) é negativo enquanto que o ângulo do vetor V2(+ k) é positivo, como mostrado na Figura 6.12.
Utilizando estes ângulos na Equação 6.2 obtêm- se um resultado positivo.
De a mesma maneira os vales entre os dedos podem ser detectados através do valores negativos da Equação 6.2.
Observando a Figura 6.11 (b) é possível perceber que o ângulo do vetor V1 (k) é positivo, pois o vetor pertence ao primeiro quadrante, e o ângulo do vetor V2(+ k) será negativo, como mostra a Figura 6.13.
Portanto a Equação 6.2 resulta valores negativos quando o ponto analisado é um vale entre os dedos.
Em a Figura 6.14 (a), tem- se um gráfico dos valores da k--curva obtidos para cada ponto do contorno da pose mostrada na Figura 6.14 (b), onde pode- se observar que os máximos locais da curva correspondem às pontas dos dedos e os mínimos locais correspondem aos vales entre os dedos.
Detecção dos Vales entre os Dedos Em este projeto somente os vales entre os dedos são detectados.
Para selecionar os pontos de interesse um algoritmo de limiar é utilizado para obter os pontos que possivelmente sejam os vales entre os dedos.
No caso de o primeiro método, que utiliza o cosseno, seleciona- se os pontos cujo valor da função é maior do que um limiar l, de tal forma que somente alguns pontos candidatos, próximos aos máximos locais sejam selecionados, como visto na Figura 6.9 (a).
Em contrapartida, no segundo método, os pontos selecionados são aqueles cuja k--curva é menor que um limiar l, garantindo desta maneira que somente os pontos próximos aos mínimos locais sejam detectados, como mostrado na Figura 6.14 (a).
Devido a o uso de um limiar vários pontos são detectados, pois possuem um valor semelhante de k--curva.
Felizmente os pontos selecionados tendem a ocupar regiões bem definidas no contorno, formando um aglomerado de pontos em cada vale a ser detectado, como ilustrado na Figura 6.15.
Para selecionar o ponto correto em cada vale entre os dedos, seleciona- se em cada aglomerado aquele que estiver mais próximo de o centro da mão.
Em a Figura 6.16 tem- se um exemplo do resultado da detecção de vales entre os dedos.
Embora a k--curva tenha- se mostrado uma técnica eficiente na detecção de pontos característicos, alguns cuidados devem ser tomados com relação a a escolha do valor de k.
Através de testes foi possível verificar que um valor de k entre 20 e 30 produz uma curva suave, como mostrado na Figura 6.14 (a).
Mas alguns problemas ocorrerão se a mão estiver muito afastada da câmera ou devido a a escala utilizada, pois tais situações resultam num contorno com poucos pontos e, um k com valor muito alto não permitirá detectar todos os vales da mão.
Detectando o Pulso Outros dois pontos que podem ser úteis para implementar um método de posição e orientação 3D da mão, devido a o fato de estarem presentes na maioria das poses, são os pontos extremos do pulso.
Para detectar- los considera- se que a largura do antebraço é constante, e que a partir de um determinado momento, começa a aumentar quando se vai em direção a o centro da mão, como mostrado na Figura 6.18.
O pulso, então, é o lugar onde ocorre essa mudança de largura.
O algoritmo desenvolvido detecta os pontos P1 e P2 no antebraço e calcula a distância dos mesmos.
Em seguida calcula- se as distâncias dos pontos subseqüentes a P1 e P2 de cada lado do contorno, (na Figura são chamados de P3 e P4) e calcula- se a diferença entre essas distâncias.
Isso vai se repetindo para os demais pontos até que a diferença entre essas distâncias consecutivas esteja acima de um limiar d..
Para facilitar a detecção destes pontos e o cálculo das distâncias a mão é rotacionada de forma que fique alinhada com o eixo vertical.
Em a Figura 6.19 temos exemplos de alguns resultados da detecção de pulso.
Vale lembrar que este método é o primeiro passo para a implementação de um procedimento automático de separação da mão do antebraço, semelhante ao método criado por Deimel.
Obtendo a Posição da Mão em 3D A partir de os pontos característicos obtidos sobre a imagem da mão (centro da mão, pontas dos dedos, vales entre os dedos e o pulso), o próximo passo para o processo de rastreamento tridimensional é a obtenção da posição espacial de cada ponto identificado.
Este processo comumente chamado de reconstrução 3D é realizado a partir de imagens captadas por uma ou duas câmeras.
Em as seções a seguir, são apresentados dois algoritmos baseados em duas câmeras e um algoritmo baseado na imagem de apenas uma câmera.
Posição 3D Utilizando Duas Câmeras Para inferir a posição 3D de um ponto pode- se utilizar imagens de duas câmeras, ortogonais ou não, por meio de suas projeções nos planos das imagens.
Para aplicar estes métodos é necessário conhecer o mesmo ponto nas duas imagens além de um ponto de referência que tenha uma coordenada previamente conhecida.
Uso de Câmeras Ortogonais Em o caso de se utilizar câmeras ortogonais, na projeção gerada em cada câmera obtém- se um par de coordenadas.
De acordo com o sistema de eixos apresentado na Figura 7.1, uma câmera superior pode gerar as coordenadas (x, y) do ponto e uma câmera frontal, as coordenadas (x, z).
Embora este método seja mais simples de implementar, ele apresenta alguns problemas para o caso da mão.
O primeiro de eles diz respeito diretamente à mão e o segundo, ao processo de geração da projeção da cena na imagem.
O primeiro problema acontece por que nem sempre é possível obter os mesmos pontos nas duas vistas.
Visto que as câmeras são ortogonais pode ocorrer a situação mostrada na Figura 7.2, onde na vista superior foram detectados os vales entre os dedos, mas na vista frontal não se tem os pontos correspondentes.
O segundo problema com este método é causado por as distorções criadas por o processo de geração da projeção perspectiva realizado por a câmera.
Por exemplo, na Figura 7.3 observa- se que dois pontos A e B, que possuem coordenadas x iguais no modelo do objeto, terão, na projeção frontal, coordenadas diferentes.
Este problema é sempre minimizado se a distância dos pontos até a câmera é maior que a distância de eles entre si e agravado se esta fica muito próxima dos mesmos.
Uso de Câmeras Não-Ortogonais Uma das idéias que ocorre quando se deseja obter medidas em 3D através de imagens é utilizar uma analogia do sistema visual estéreo do ser humano.
Com o uso de duas câmeras, com configuração semelhante aos olhos humanos, ou seja, lado a lado e espaçadas entre si, como ilustrado na Figura 7.4, é possível calcular a profundidade relativa dos pontos.
Para isso é necessário que as câmeras estejam calibradas (que se saiba a posição e a orientação de elas do espaço) e que o mesmo ponto no espaço seja visto por ambas.
Este último quesito é atingido mais facilmente do que com as câmeras ortogonais pois no caso em questão, como as câmeras estão próximas, elas tem quase a mesma visão do cenário.
Em este projeto, pontos específicos na mão são identificados nas duas imagens, como mostrado na Figura 7.5.
A partir de aí pode- se utilizar alguns destes pontos para localizar a posição da mão no espaço por meio de algoritmos de reconstrução 3 D, como por exemplo o algoritmo de triangulação apresentado por Shapiro.
Esta estratégia não foi implementada devido a restrições de tempo, as quais este projeto foi submetido.
Posição 3D Utilizando Uma Câmera Como foi mencionado na seção 7.1.2, o algoritmo de reconstrução pressupõe que o mesmo ponto pode ser obtido nas duas imagens.
Se esta correspondência, entretanto, não for obtida com precisão, os dados de entrada do algoritmo estarão incorretos e o resultado acabará não satisfazendo os requisitos do rastreamento.
Além disso, deve- se considerar que problemas de iluminação podem suprimir pontos numa imagem, embora eles estejam presentes na outra.
Uma alternativa para calcular a posição 3D da mão é utilizar apenas uma câmera e detectar mais de um ponto sobre a imagem.
A idéia baseia- se num algoritmo chamado Warp.
Este algoritmo parte de um quadrado que está numa posição conhecida e que ao ser &quot;visto «por uma câmera, transforma- se num quadrilátero, em face de sua nova posição em relação a esta câmera e também por causa de a transformação perspectiva que é gerada por a projeção.
Um exemplo destas transformações pode ser visto na Figura 7.6.
Em (a) observa- se o quadrado em sua posição original, em (b) este quadrado sofre uma rotação e uma escala.
Em (c) o quadrado sofre, adicionalmente, uma transformação perspectiva.
A partir de esta nova imagem do quadrilátero, o algoritmo de Warp infere a matriz de transformação geométrica e de perspectiva, MT, capaz de levar os pontos do quadrado aos pontos do quadrilátero, através da Equação 7.1.
Em esta equação o termo P2 é um vetor-coluna e referese as coordenadas (x, y, z) dos pontos que compõe o quadrilátero e P1 refere- se aos pontos do quadrado.
Ressalta- se aqui que o algoritmo não gera explicitamente as coordenadas dos pontos capturados por a câmera e nem a orientação destes no espaço.
Entretanto, de posse da matriz de transformação é possível aplicar- la num objeto que esteja sobre o quadrado original, de forma que este objeto sofra a mesma transformação ocorrida no quadrado.
Este algoritmo é inclusive utilizado na ferramenta ARToolkit que permite o rastreamento de objetos usando apenas uma câmera.
Esta ferramenta, entretanto, exige a colocação de marcadores sobre o objeto a ser rastreado.
A biblioteca segmenta a imagem e identifica os pontos extremos do marcador como pontos característicos a serem usados no algoritmo de Warp.
Em a Figura 7.7 pode- se observar dois exemplos do uso do ARToolkit como rastreador.
Note- se que na imagem da direita a mão é coberta por uma luva branca a fim de facilitar a segmentação.
Em o presente trabalho por outro lado, os pontos característicos são obtidos diretamente da mão por meio de a identificação dos vales existentes entre os dedos, sendo que a marca pode ser substituída por uma &quot;marca virtual «como mostrado na Figura 7.8, utilizando também os limites do pulso para compor o quadrilátero, a partir de o qual se infere a matriz de transformações.
Vale lembrar que para usar este algoritmo é preciso que se tenha um quadrado como forma original e não um quadrilátero.
Entretanto, como no início do rastreamento não se consegue inferir um quadrado na mão do usuário, a partir de os pontos característicos e sim um quadrilátero, como visto na Figura 7.8 e assim durante todo o rastreamento.
Diante deste cenário, optou- se por implementar um algoritmo que realiza um mapeamento de quadrilátero para quadrilátero, apresentado por Heckbert apud.
Este algoritmo realiza tal mapeamento através da utilização de um quadrado intermediário, ou seja, um quadrilátero inicial é mapeado para um quadrado e, em seguida, o quadrado é mapeado para o quadrilátero final, conforme ilustrado na Figura 7.9.
Devido a restrições de tempo do projeto implementou- se a solução mais simples, ou seja, em 2 D, mas que permite verificar que a técnica funciona para os casos de translação, rotação e escala.
No caso de o rastreamento da mão, o primeiro passo é obter a MT do quadrilátero para o quadrado padrão.
Este procedimento é denominado calibração e requer que o usuário permaneça com a mão imóvel numa determinada posição do espaço por um breve intervalo de tempo.
Esta operação é realizada apenas uma vez, ou seja, a calibração consiste na determinação da MT que leva os pontos do quadrilátero inicial para o quadrado e em seguida mapeia- se o quadrado para os novos quadriláteros, obtidos nos quadros seguintes.
Com o objetivo de verificar o funcionamento do algoritmo de warping, tomou- se um losângulo, formado por os pontos médios das arestas do quadrilátero inicial que serve para analisar se o algoritmo consegue inferir, através da MT, as mudanças na translação, rotação e escala.
O losângulo é obtido ainda na fase de calibração e seus pontos são mapeados para o quadrado padrão, de tamanho 100x100 pixels que, durante o rastreamento, estes novos pontos são mapeados para os novos quadriláteros obtidos.
A medida que a mão se movimenta, rotaciona, aproxima ou se afasta da câmera o mesmo ocorre com o losângulo, lembrando que estas informações estão codificadas na MT.
Em a Figura 7.11 tem- se um exemplo da translação, onde é possível notar que o losângulo é mapeamento corretamente em cada quadro.
Em seguida na Figura 7.12, temos um exemplo de que a técnica funciona quando ocorre a rotação da mão.
Em a Figura 7.12 (a) temos a posição inicial da mão e na Figura 7.12 (b) temos a posição final da mão, após a rotação, onde o losâgulo é distorcido para refletir tal mudança.
E, finalmente na Figura 7.13, temos um exemplo de mod-ificação na escala devido a aproximação da mão em direção a a câmera, pois é possível notar a diferença de tamanho do losângulo nas Figuras 7.13 (a) e 7.13 (b).
Embora esta técnica tenha apresentado bons resultados, alguns erros ocorrem devido a má qualidade na detecção de algumas características, principalmente o pulso, pois ao longo de as imagens captadas por a câmera, ora o pulso está mais próximo de a região da mão e ora está mais próximo de a região do antebraço, como visto nas Figuras 7.13 (a) e 7.13 (b).
Além disso, erros na detecção do contorno fazem com que os vales entre os dedos sejam detectados fora de os limites da mão, como visto na Figura 7.11 e na Figura 7.12 (b).
Considerações Finais Em este trabalho foi apresentado um sistema de rastreamento de mão destinado a ser utilizado em ambientes virtuais, embora possa ser utilizado para implementação de interfaces intuitivas, como a arquitetura câmera-projetor ou um menu virtual, mencionados no Capítulo 2.
A principal meta foi construir uma alternativa aos rastreadores magnéticos, muito utilizados em Realidade Virtual, para obter a posição e a orientação da mão do usuário.
As vantagens do rastreador de mão baseado em imagens compreendem a sua facilidade de uso, sem a necessidade de uma carga de aprendizado grande para utilizar- lo e o conforto de não obrigar o usuário a usar algum tipo de hardware na mão.
O projeto foi dividido em três fases e, em cada uma de elas, buscou- se soluções nas áreas de Processamento de Imagens e Visão Computacional, testando- se cada solução quanto a a qualidade de resposta e tempo de processamento, uma vez que o rastreador de mão deve operar em temporeal.
Em a primeira fase deste projeto foram estudados os dois espaços de cores (RGB e HSV) mais utilizados para segmentação de pele, bem como foram implementados e testados 4 algoritmos de segmentação de pele e seus resultados apresentados.
O uso da segmentaçãoimage de pele justifica- se por a necessidade de separar a mão do ambiente de trabalho, e por as vantagens deste método que permite localizar a mão mesmo que ela mude de forma de uma cena para outra.
Apesar de o processo de segmentação de pele apresentar algumas falhas de classificação, estas foram superadas através da utilização de um algoritmo simples de preenchimento de áreas (floodfill).
Em seguida, na segunda fase, duas estratégias de obtenção da posição da mão foram apresentadas, o centro de massa e a Transformada da Distância utilizando a distância Chamfer-3- 4 (DCT).
Em este projeto a DCT foi escolhida como estratégia de obtenção da posição da mão por fornecer respostas mais precisas mesmo com a presença do antebraço na cena e com diversas configurações da mão.
Entretanto, recomenda- se o uso do centro de massa, caso se deseje trabalhar somente com a mão, descartando o antebraço e a mão em poses semelhantes aquelas apresentadas no Capítulo 3 (Figura 4.4).
Além disso, este método é simples de implementar e rápido em termos de desempenho.
Quanto a a técnica de momentos de imagens utilizada para calcular a orientação da mão, esta pode ser utilizada com várias câmeras, tornando possível o conhecimento da orientação da mão em vários planos como, por exemplo, para o caso da configuração de câmeras ortogonais, mostrada na Figura 7.2, onde pode- se obter a orientação da mão nos planos xy e xz, sem nehuma modificação no algoritmo.
Além disso, neste trabalho utilizou- se os momentos de imagens para calcular a orientação da mão a partir de seu contorno, ao contrário de muitos trabalhos que utilizam a área do objeto de interesse.
Desta forma o número de pontos utilizados por o algoritmo cai drasticamente, culminando num aumento significativo de desempenho do mesmo.
Tempo de execução de cada fase do algoritmo de rastreamento.
Fase do algoritmo.
Tempo de Processamento (seg) Tempo Relativo Segmentação Seleção da Maior área Preenchimento de Buracos Detecção da Região de Interesse Detecção da Borda da Mão Distância Chamfer Ordenação do Contorno Orientação Detecção do Pulso Detecção dos Vales Com os resultados obtidos, conclui- se que a idéia de implementar um rastreador baseado em imagens é viável, além de ser uma alternativa mais econômica, pois o hardware utilizado consiste de um computador com uma ou mais câmeras captando o movimento da mão do usuário.
Embora não se tenha concretizado o objetivo inicial de rastrear a mão em 3 D, devido as restrições de tempo, as quais este projeto se submeteu, algumas aplicações podem ser desenvolvidas utilizando- se alguns dos algoritmos implementados.
Exemplos de tais aplicações são mostradas a seguir:
Menu virtual:
Um menu virtual semelhante ao wearable menu pode ser implementado utilizando- se um algoritmo de segmentação de pele para localizar a mão numa imagem do mundo real.
Considerando que esteja sempre com o polegar virado para cima, pode- se detectar as pontas dos dedos e associar à cada uma de elas um item do menu.
O acionamento de um determinado item do menu pode ser feito detectando- se a flexão do dedo correspondente através da análise presença ou ausência de uma determinda ponta do dedo, permitindo desta maneira, que um item do menu seja utilizado por meio de flexão dos dedos;
Mouse virtual:
Considerando que apenas um dedo corresponderá ao ponteiro do mouse e o dedo indicador corresponde ao mesmo:
Usando algum algoritmo de segmentação de pele localiza- se a mão do usuário.
Em seguida, por meio de a k--curva detecta- se a ponta do dedo.
O click do mouse pode ser implementado por meio de a detecção da flexão do dedo, como feito por Zhu, ou por a permanência do mesmo por um determinado tempo em cima de o objeto ou menu que se deseja utilizar.
Esta aplicação poderia ser utilizada em ambientes de realidade aumentada ou sessões de brain-storm;
Joystick virtual:
Este dispositivo pode ser implementado de maneira semelhante ao mouse virtual, mas com outro objetivo, por exemplo, pode- se utilizar um dedo esticado para apontar a direção, dentro de um ambiente virtual, para onde o usuário deseja se deslocar.
Através de duas câmeras é possível descobrir a orientação da mão nos planos xy e xz, e permitir que o usuário se desloque em várias, que não sejam somente para cima e para baixo.
Pode- se utilizar o joystick virtual em jogos de carta, onde todas as operações realizadas com as cartas consistem em pegar- las e arrastar- las com a mão.
O ato de pegar uma carta pode ser implementado fazendo com que o usuário deixe por um determinado tempo a mão sobre a carta desejada, ou esticar um dedo, que será detectado através da k--curva.
E, soltar- las pode ser feito da mesma maneira, mas num local diferente de onde a carta estava inicialmente.
Um exemplo de joystick virtual pode ser encontrado nos trabalhos de Freeman, que desenvolveu um sistema que permite controlar um carro de um simulador de fórmula 1 com a mão.
Como algumas fases do projeto contam com mais de um algoritmo para resolver o mesmo problema, na Tabela 8.2 listam- se quais as vantagens de desvantagens de determinados algoritmos implementados neste trabalho, com algumas observações com relação a o uso dos mesmos.
Trabalhos Futuros Em o desenvolvimento desta pesquisa, várias idéias surgiram com intuito de refinar ainda mais a qualidade do sistema implementado.
A seguir, algumas idéias para estender este trabalho:
Melhorar a segmentação de pele levando em contas os vários tipos de pele, e melhorar o algoritmo de segmentação de forma que este se torne mais robusto às variações de iluminação.
Pesquisas neste sentido podem ser encontradas no trabalho de Veznhevets;
Além de melhorar o algoritmo de segmentação de pele com relação a os problemas de iluminação seria interessante que este utilizasse alguma estratégia de aprendizado sobre a pele do usuário, em tempo de execução, melhorando a qualidade da segmentação e não obrigando- o a utilizar somente informações estáticas provenientes de tabelas e arquivos;
Melhorar a detecção do contorno da mão, uma vez que a má qualidade deste é a principal fonte de erros gerados por os algoritmos de detecção de características;
Desenvolver uma ou várias aplicações que utilizem a mão como dispositivo, com objetivo de testar os algoritmos deste projeto;
Inserir mais uma câmera no sistema para que seja possível utilizar visão estéreo ou coletar informações sobre a mão com mais de uma vista;
Permitir o uso das duas mãos, o que torna necessário o desenvolvimento de um modelo de mão, que diferencie a mão direita da esquerda.
Atualmente este projeto não possui nenhum conhecimento sobre o que é mão, pois considera- se que, logo após a segmentação da pele, a maior área que permanece na imagem é a mão;
Reimplementar o algoritmo warp para trabalhar com corrdenadas 3D permitindo obter os mesmos resultados que o ARToolkit, mas sem a necessidade de marcadores sobre a mão do usuário.
