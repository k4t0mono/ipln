Em razão de o surgimento de redes de comunicação de alta velocidade, tais como Myrinet e SCI, a construção de arquiteturas baseadas em máquinas comuns (PCs e estações de trabalho) conectadas por esse tipo de rede -- o que se denomina agregado (ou cluster) -- tornou- se viável.
Tais arquiteturas vêm se consolidando como plataformas alternativas para a execução de aplicações paralelas complexas, principalmente devido a a relação custo/ benefício que oferecem.
Esse avanço das tecnologias de redes possibilita também a agregação de clusters, formando uma estrutura de cluster de clusters, como uma única máquina paralela.
Um dos principais problemas no uso de cluster de clusters é o software utilizado para o desenvolvimento de aplicações paralelas, visto que cada agregado envolvido na estrutura possui certas características que precisam ser tratadas por a linguagem ou ambiente de programação, visando o alcance de alto desempenho.
Esta dissertação tem como objetivo apresentar uma ferramenta de programação paralela por troca de mensagens que executa sobre uma estrutura de cluster de clusters:
O MDX-cc.
A ferramenta foi concebida tendo como base o sistema MDX e uma primeira versão foi implementada oferecendo suporte à comunicação em agregados com redes SCI, Myrinet e Fast-Ethernet.
O principal objetivo do MDX-cc é oferecer recursos de comunicação e sincronização de processos que rodam em agregados interligados.
Por sua arquitetura modular e abstração do uso de protocolos de comunicação dedicados a cada tecnologia de rede, o MDX-cc oferece uma interface de programação simples, com um conjunto reduzido de primitivas, e provê transparência total na comunicação entre processos que executam em nós de clusters com tecnologias de rede distintas.
Palavras-chave: Programação paralela, cluster de clusters, avaliação de desempenho, troca de mensagens.
O Processamento de Alto Desempenho é considerado uma ferramenta fundamental para as áreas da ciência e tecnologia.
Sua importância estratégica é demonstrada por a quantidade de iniciativas em pesquisa e desenvolvimento nestas áreas, financiadas por governos de todo o mundo.
Encontra- se, principalmente nas áreas da ciência como física, meteorologia e química, maior demanda por alto desempenho.
O Processamento de Alto Desempenho, por sua vez, depende fundamentalmente de técnicas de processamento paralelo, capazes de prover o desempenho necessário para estas aplicações.
Em função de a popularização das redes Fast-Ethernet, e do surgimento de redes de comunicação de dados extremamente rápidas, tais como Myrinet e SCI, plataformas de hardware compostas por computadores comuns (PCs e estações de trabalho) conectados por esse tipo de rede começaram a ser utilizadas como alternativa de arquitetura de máquina paralela, surgindo daí a computação baseada em agregados (cluster computing).
De uma maneira geral, o principal objetivo da computação baseada em agregados (clusters) é o emprego de hardware barato e de ambientes de programação dedicados para permitir a execução de aplicações paralelas e distribuídas, oferecendo alto desempenho com um custo bastante reduzido.
Com os avanços das tecnologias de redes, há ainda a possibilidade da interligação de agregados, formando uma estrutura de cluster de clusters, que cooperam na resolução de um problema.
A interligação de clusters, por meio de uma rede local ou via internet, visa principalmente alcançar o máximo desempenho na execução de aplicações paralelas.
Uma vez que o número de máquinas aumenta, o poder computacional alcançado também é maior.
Desta forma, aplicações complexas, que necessitam de um grande potencial de recursos, podem ser distribuídas nos agregados envolvidos na estrutura, de forma a conseguir uma melhora de desempenho.
Atualmente, o principal fator limitante no uso de cluster de clusters como plataforma de alto desempenho é o software utilizado para o desenvolvimento de aplicações.
Os ambientes mais utilizados até então são úteis à utilização de um único agregado, com determinadas características (rede, SO, etc.).
Os agregados, na sua maioria, são homogêneos, o que facilita a sua utilização.
Surge a necessidade de ambientes que suportem a execução de programas paralelos em agregados interligados, estrutura que pode ser vista como uma única máquina paralela heterogênea.
Dentro desse contexto, este trabalho vem apresentar um ambiente de programação paralela aplicado a cluster de clusters, o MDX-cc.
Esse ambiente tem como objetivo facilitar a programação e a execução de aplicações paralelas em agregados interligados, gerenciando a comunicação entre processos, independentemente do cluster onde os mesmos se encontram executando.
Este trabalho não se preocupa com problemas avançados resultantes da heterogeneidade de um cluster de clusters, como balanceamento de carga e utilização de mais de um modelo de programação paralela (troca de mensagens e memória compartilhada) numa mesma aplicação.
Apenas se preocupa com a implementação de uma ferramenta que ofereça infra-estrutura para a sua utilização.
Este documento está dividido da seguinte maneira.
O segundo capítulo apresenta aspectos gerais de programação paralela, como arquiteturas de máquinas paralelas utilizadas atualmente, modelos de programação paralela e bibliotecas desenvolvidas.
O Capítulo 3 apresenta a estrutura de cluster de clusters, à qual é aplicado o ambiente MDX-cc.
Em o testes de desempenho realizados com o MDX-cc são demonstrados no Capítulo 7 e, por fim, o sugestões de trabalhos futuros relacionados a este.
Programação Paralela Este capítulo apresenta diferentes aspectos ligados à programação paralela, desde aspectos de hardware, como arquiteturas de máquinas usadas atualmente, até aspectos ligados diretamente à programação, como formas de paralelização de programas e modelos de programação paralela.
Máquinas paralelas vêm se tornando mais populares em função de a demanda sempre crescente por poder computacional.
Infelizmente, os sistemas que oferecem a capacidade de processamento para satisfazer esta demanda, muitas vezes, ainda têm custo elevado, são difíceis de programar, ou ambos.
Como a programação de aplicações paralelas ainda exige o conhecimento de características específicas da máquina para a obtenção de desempenho, conhecimentos sólidos sobre arquiteturas de máquinas paralelas auxiliam no desenvolvimento de programas paralelos, desde sua modelagem até a fase de depuração e otimização.
Diversas taxonomias para arquiteturas paralelas foram propostas.
Em essa seção serão abordadas a classificação genérica de Flynn e a classificação segundo o compartilhamento de memória.
Classificação de Flynn Para uma classificação inicial de máquinas paralelas, pode ser usada a classificação genérica de Flynn.
Baseando- se no fato de uma máquina executar uma seqüência de instruções sobre uma seqüência de dados, diferencia- se o fluxo de instruções (instruction stream) e o fluxo de dados (data stream).
Dependendo da multiplicidade desses fluxos, e da combinação das possibilidades, são propostas quatro classes de máquinas:
SISD, MISD, SIMD e MIMD Em uma máquina da classe MIMD (Multiple Instruction Multiple Data), cada unidade de controle C recebe um fluxo de instruções próprio e o repassa para sua unidade de processamento P, para que seja executado sobre um fluxo de dados próprio.
De essa forma, cada processador executa o seu próprio programa sobre seus próprios dados de forma assíncrona.
Sendo assim, o princípio MIMD é bastante genérico, pois qualquer grupo de máquinas, se analisado como uma unidade, pode ser considerado da classe MIMD.
Para que o processamento nas diferentes posições de memória possa ocorrer em paralelo, a unidade M não pode ser implementada como um único módulo de memória.
Em essa classe, enquadramse servidores com múltiplos processadores (dual, quad), redes de estações e máquinas como Cm-5, nCUBE, Intel Paragon e Cray T3D.
A maioria das máquinas paralelas atuais, incluindo os agregados abordados neste trabalho, se enquadram na classe MIMD.
Classificação segundo o compartilhamento de memória Um outro critério para a classificação de máquinas paralelas é o compartilhamento de memória.
Quando se fala em memória compartilhada (shared memory), existe um único espaço de endereçamento que será usado de forma implícita para comunicação entre processadores, com operações load e store.
Quando a memória não é compartilhada, existem múltiplos espaços de endereçamento privados (multiple private address space), um para cada processador.
Isso implica na comunicação explícita através de troca de mensagens com Memória distribuída (distributed memory), por sua vez, refere- se a localização física da memória.
Se a memória é implementada com vários módulos, e cada módulo é colocado próximo a um processador, então a memória é considerada distribuída.
Outra alternativa é o uso de memória centralizada (centralized memory), ou seja, a memória encontra- se à mesma distância de todos os processadores, independentemente de ter sido implementada com um ou vários módulos.
De acordo com o uso ou não de uma memória compartilhada, podese diferenciar máquinas paralelas da seguinte forma:
Multiprocessadores: Todos os processadores acessam, através de uma rede de interconexão, uma memória compartilhada.
Existe apenas um espaço de endereçamento e os processos comunicam através da memória compartilhada.
Em relação a o tipo de acesso à memória, multiprocessadores podem ser classificados como:
Acesso uniforme à memória (uniform memory access, Uma):
Memória centralizada e localizada à mesma distância de todos os processadores.
Acesso não-uniforme à memória (non-uniform memory access, Em uma):
Memória distribuída, implementada com módulos associados a diferentes processadores (distâncias diferentes).
Multicomputadores: Cada processador P possui uma memória local M, à qual só ele tem acesso.
A comunicação entre processos é feita através de troca de mensagens por a ede de interconexão.
Em relação a o tipo de acesso à memória do sistema, multicomputadores podem ser classificados como:
Sem acesso à memória remota (non-remote memory access, Norma):
Cada processador só consegue endereçar sua memória local.
Em esta seção serão apresentados os principais modelos de máquinas paralelas, que constituem atualmente as principais tendências para construção desses sistemas.
Multiprocessadores Simétricos Multiprocessadores simétricos (SMP) são sistemas constituídos de processadores comerciais, também denominados &quot;de prateleira», conectados a uma memória compartilhada (caracterizando essas máquinas como Uma) através de um barramento (Figura 2.2).
A comunicação entre processos se dá por memória compartilhada.
A escalabilidade de uma máquina SMP é comprometida por o fato da ligação com a memória ser através de um barramento.
Como o barramento só permite uma transação por vez, o desempenho da máquina cai a medida que o canal é disputado por um maior número de processadores.
São exemplos de SMP o IBM R50, SGI Power Challenge e o Hp/ Convex Exemplar X--Class.
Máquinas com Memória Compartilhada Distribuída Máquinas com memória compartilhada distribuída (DSM) são sistemas em que, apesar de a memória encontrar- se fisicamente distribuída, todos os processadores podem endereçar todas as memórias.
Essa implementação pode ser feita em hardware, software ou, ainda, a combinação dos dois.
A distribuição da memória pode ser resultado da escolha entre uma arquitetura multiprocessada com memória distribuída (máquina Em uma) ou uma arquitetura de multicomputador com memória distribuída (máquina Norma).
São exemplos de DSM o Stanford DASH, o Cray T3D e estações de trabalho rodando TreadMarks.
Redes de Estações de Trabalho Redes de estações de trabalho (NOW) são sistemas constituídos por várias estações de trabalho interligadas por uma rede tradicional, como Ethernet e ATM.
Em a prática, uma rede local pode ser vista como uma máquina paralela em que vários processadores, com suas memórias locais, são interligadas por uma rede, constituindo uma máquina paralela Norma de baixo custo.
Uma vez interligadas por redes como Ethernet e ATM, uma máquina NOW possui baixo desempenho na troca de mensagens, pois essas redes utilizam protocolos pesados na comunicação (TCP/ IP), resultando numa latência1 muito grande.
Máquinas Agregadas Com as novas tecnologias de redes locais de alta velocidade, o uso de redes de estações de trabalho como máquinas paralelas está se tornando cada vez mais atrativo.
Em este contexto, tem se investido numa nova classe de máquinas paralelas, que são as máquinas agregadas Máquinas agregadas, como as NOW, são constituídas por várias estações de trabalho interligadas.
A diferença é que foram projetadas especialmente para a execução de aplicações paralelas.
Desta forma, as estações podem ser otimizadas (tanto em hardware como em software) para este fim.
Em a maioria dos casos, as máquinas que servem de nó de uma COW não possuem monitor, teclado e mouse, e seu sistema operacional presta apenas serviços que serão úteis ao processamento paralelo de sistemas.
Latência: Tempo necessário para o envio de uma mensagem de tamanho zero.
Encontra- se, atualmente, duas tendências na construção de máquinas agregadas Agregados interligados por redes padrão:
Esta tendência é impulsionada por os grandes fabricantes como IBM e Hp, que estão interessados na construção de máquinas paralelas poderosas agregando milhares de estações de trabalho de baixo custo (low end).
Para a interligação de tantas máquinas, investir numa rede especial aumenta muito o custo.
Assim, utiliza- se para este fim uma rede padrão como a Fast-Ethernet.
Para melhorar a latência da rede são usados grandes chaveadores (switches), em contraste aos hubs, que funcionam como grandes barramentos, solução mais comum em redes locais por causa de o menor custo.
O enfoque reside na obtenção de desempenho com muitos nós de pequeno poder computacional e, preferencialmente, com aplicações que não tenham muita necessidade de comunicação.
Estas máquinas são claramente Norma, já que não é possível o acesso a memória de nós remotos e o paradigma de comunicação utilizado é a troca de mensagens.
Agregados interligados por redes de baixa latência:
Esta tendência é impulsionada por pequenas empresas que fabricam placas de interconexão especificamente para máquinas agregadas.
Estas placas implementam protocolos de rede de baixa latência otimizados para as características de comunicação de aplicações paralelas.
Como o custo destas placas é mais alto do que placas de rede padrão, é muito oneroso construir máquinas com muitos nós (centenas de nós).
Para compensar o menor número de nós, são usados nós mais poderosos, muitas vezes servidores de médio porte com vários processadores (SMP).
A máquina resultante fica mais equilibrada na relação poder de processamento do nó e desempenho da rede, obtendo um bom desempenho, mesmo com aplicações que necessitem de muita comunicação.
A maioria destas máquinas é Norma já que, em princípio, não é possível o acesso a memória de nós remotos.
Porém, algumas das placas implementam um espaço de endereçamento global entre os nós permitindo o acesso de memórias remotas.
Em este caso a máquina resultante é Em uma e são suportados tanto a troca de mensagens como o acesso a áreas de memória compartilhada.
A utilização de máquinas agregadas traz algumas vantagens como:
Ótima relação custo-benefício:
As máquinas agregadas, por serem construídas com componentes &quot;de prateleira», que, por sua vez, são produzidos em grande escala, têm custo reduzido.
Quanto a o desempenho, este é aproximado ao de um sistema MPP que, por exemplo, pode custar até duas vezes mais, além de possuir um dos menores custos por MFlops/ s (milhões de instruções de ponto flutuante por segundo) em comparação às outras arquiteturas paralelas.
Baixo custo de manutenção:
A arquitetura possui um baixo custo de manutenção, pois os &quot;componentes de prateleira «utilizados são facilmente encontrados no mercado, podendo ser feita a substituição de peças, igualmente, com um baixo custo.
Devido a esta ótima relação custo-benefício, fabricantes de MPPs estão se utilizando de alguns conceitos da construção de agregados para diminuir o custo de seus produtos.
Sendo assim, as diferenças entre estas máquinas diminuíram bastante ao ponto de ficar difícil sua classificação.
Escalabilidade: Com a utilização destas máquinas, tem- se a capacidade de expansão mantendo as mesmas características com configurações de baixo custo e que podem crescer na medida das necessidades, com a inclusão de nós adicionais.
A arquitetura possui, também, um alto grau de configurabilidade, por se tratar de uma arquitetura aberta.
A definição de seus componentes (nós e rede de interconexão) é generalizada, possibilitando diversas configurações.
Com relação a a interconexão entre os nós deste tipo de máquina, existem vários padrões, que serão apresentados a seguir.
Fast-Ethernet: Fast-Ethernet é uma extensão da Ethernet, utilizada na maioria das LANs (Local Area Networks).
Possui uma latência bem menor na comunicação e um aumento significativo na largura de banda.
Utiliza as mesmas tecnologias do padrão Ethernet, permitindo uma implementação e interoperabilidade mais fáceis.
A switch Fast-Ethernet possui uma vazão na ordem de 100 Mbits/ s e, por o fato de ser uma placa convencional, a implementação das camadas de rede deve ser feita em software, o que compromete de forma significativa a latência.
Em as outras tecnologias isto não se verifica, pois a interconexão das camadas é implementada em hardware.
Além de ser utilizada em clusters com um grande número de nós, Fast-Ethernet também é utilizada em agregados interligados por rede de baixa latência, como rede secundária para avaliação e monitoração da máquina.
ParaStation: Consiste numa emulação de sockets UNIX e de ambientes amplamente utilizados para programação paralela, como PVM.
Isto permite portar uma grande quantidade de aplicações paralelas e cliente/ servidor para a ParaStation.
Algumas implementações iniciais atingem uma latência em torno de 2 microssegundos e uma largura de banda de 1,5 Gbits/ s por canal de comunicação.
Uma rede ParaStation utiliza uma topologia baseada numa malha de duas dimensões, mas para sistemas pequenos uma topologia em anel é suficiente.
Em este padrão, a rede é dedicada a aplicações paralelas, não substituindo LANs comuns, de modo que os protocolos das LANs comuns podem ser eliminados.
Isto permite utilizar propriedades mais especializadas na rede, como protocolos ponto-a-ponto e controle da rede em nível de usuário, sem interação com o SO.
O protocolo ParaStation implementa múltiplos canais lógicos de comunicação numa ligação física.
Em contraste com outras redes de alta velocidade, não há custo adicional para componentes de switch central.
ParaStation é utilizada na construção de agregados interligados por rede de baixa latência.
Myrinet: Myrinet é uma tecnologia de chaveamento e comunicação de pacotes de alto desempenho (latência de cerca de 5 microssegundos) de custo relativamente baixo, que está sendo amplamente utilizada para interconectar máquinas baseadas em agregados.
É um padrão que utiliza uma tecnologia baseada em comunicação através de pacotes.
Desenvolvida por a Myricom, pode ser utilizada tanto em máquinas agregadas de SANs (Small Area Networks) como em máquinas agregadas de LANs.
Em comum com as LANs, os nós de uma máquina baseada em agregados que utilizam uma rede Myrinet, enviam e recebem os dados na forma de pacotes.
Qualquer nó pode enviar um pacote para qualquer outro nó, porém, em contraste com as LANs comuns, uma rede Myrinet possui altas taxas de transferência.
Uma ligação Myrinet é composta por um par de canais full-duplex que permite uma taxa de transferência em torno de 2 Gbit/ s cada um.
As características que tornam a Myrinet uma rede de alto desempenho incluem o desenvolvimento de canais robustos de comunicação com controle de fluxo, pacotes e controle de erro, baixa latência, interfaces que podem mapear a rede, rotas selecionadas, tradução de endereços da rede para estas rotas, bem como manipulação do tráfego de pacotes e software que permite comunicação direta entre os processos em nível de usuário e rede.
Uma rede Myrinet utiliza normalmente topologias regulares, tipicamente malhas de duas dimensões, embora permita a utilização de uma topologia arbitrária, uma vez que um cabo Myrinet pode conectar hosts entre si, ligar cada placa a um switch ou, ainda, dois switches entre si.
Ao contrário de uma LAN típica, em que o tráfego de pacotes compartilha um mesmo canal físico, uma rede Myrinet com uma malha bidimensional pode ser considerada escalável, pois a capacidade dos agregados cresce com o número de nós devido a o fato de que muitos pacotes podem trafegar de forma concorrente por diferentes caminhos da rede.
Uma rede Myrinet tem switches de múltiplas portas, que podem ser conectados por ligações para outros switches e para outros hosts em topologias variadas.
Myrinet é utilizada na construção de agregados interligados por rede de baixa latência.
SCI (Scalable Coherent Interface):
O padrão SCI foi originalmente designado para proporcionar a interconexão entre sistemas de memória compartilhada com coerência de cache.
Especifica um hardware inovador e protocolo para conexão de até 64k nos nós, numa rede de alta velocidade com características de comunicação de alto desempenho.
A característica de coerência de cache é implementada com uma lista de módulos SCI.
Cada nó é acoplado a um módulo.
Quando um processador atualiza seu cache, o estado deste é propagado por todos os outros módulos, compartilhando a mesma cache.
Esta lista para caches coerentes distribuídos é escalável devido a um sistema robusto poder ser criado por a inserção de mais módulos SCI na lista.
Proporciona uma latência de menos de 1 microssegundo e alta largura de banda numa conexão ponto-a-ponto, o que também evita a limitação física dos barramentos, não havendo, deste modo, maiores dificuldades com relação a a escalabilidade.
O SCI define serviços de barramento, oferecendo soluções distribuídas para a sua realização.
Esses serviços proporcionam um espaço de endereçamento físico de 64 bits entre os nós, o que permite transações de escrita, leitura e a criação de áreas de memória compartilhada entre estes.
De os 64 bits, 16 bits são utilizados para endereçar os 64K nós possíveis e os 48 bits restantes são utilizados para o endereçamento em cada nó.
A placa SCI permite construir máquinas com características Em uma, uma vez que permite acessos à memória remota, ou seja, um nó pode acessar a memória local de outro nó, realizados por o hardware.
No entanto, estes acessos são mais lentos que os acessos locais, o que caracteriza acessos não uniformes à memória.
A construção básica de blocos SCI é feita por pequenos anéis.
Sistemas maiores podem ser obtidos através da criação de anéis, interconectados via switches SCI.
Desta forma, além de permitir a troca de mensagens utilizando um hardware especial, o SCI ainda possui a capacidade de implementar, via hardware, uma memória compartilhada distribuída (DSM), através de operações de escrita e leitura em regiões de memória mapeadas em memórias remotas.
Isto resulta numa baixa latência, num ambiente baseado em agregados.
Atualmente, a construção de máquinas agregadas tornou- se uma tendência, já existindo algumas com centenas de nós, como, por exemplo, a máquina iCluster, com 225 nós e pico de 165.9 GFlops, instalada em 2001 no laboratório Id-IMAG/ INRIA, em RhoneAlpes, Grenoble, França.
Em a programação paralela, existe a necessidade de comunicação entre os processos, pois os mesmos cooperam na resolução de um problema.
A comunicação entre processos pode ser efetuada através de memória compartilhada ou por troca de mensagens.
Modelos de programação baseados em memória compartilhada permitem implementações com menor complexidade em relação a os modelos com troca de mensagens.
Entretanto, leituras e escritas a um mesmo dado compartilhado não podem ser feitas simultaneamente, exigindo a utilização de uma seção crítica envolvendo o acesso a memória compartilhada.
O modelo que utiliza memória compartilhada distribuída apresenta uma menor complexidade para o programador, se comparado ao modelo por troca de mensagens, devido a a semelhança com um programa seqüencial.
O programador não tem nenhuma preocupação com a comunicação e com o gerenciamento do particionamento do conjunto de dados, podendo focar toda sua atenção única e exclusivamente no desenvolvimento dos algoritmos.
Três abordagens têm sido utilizadas na implementação de sistemas com memória compartilhada:
Implementação por hardware:
Estendem técnicas tradicionais de caching para arquiteturas escaláveis;
Implementação de bibliotecas e por o SO:
O compartilhamento e a coerência de memória são obtidos através de mecanismos de gerenciamento de memória virtual;
Implementação por o compilador e bibliotecas:
Acessos compartilhados são automaticamente convertidos em primitivas de coerência e sincronização.
Os sistemas com memória compartilhada, como o IVY e TreadMarks, provêem uma abstração de memória compartilhada em redes de estações de trabalho.
A facilidade que esses sistemas de memória compartilhada proporcionam para o programador tem um custo que é, normalmente, um desempenho inferior aos dos sistemas com troca de mensagens.
Outro problema enfrentado por este modelo é a baixa escalabilidade, pois fica difícil conseguir um bom desempenho à medida que aumenta o número de processadores do sistema.
A Figura 2.3 mostra um sistema de memória compartilhada distribuída, onde cada processador acessa uma memória compartilhada que pode ser local ou distante (Em uma).
Em o modelo de programação paralela baseado em troca de mensagens, os processadores encontram- se distribuídos fisicamente sobre uma rede e podem acessar somente sua memória local (Norma).
Através da rede, os processos podem enviar e receber mensagens, de acordo com a Figura 2.4.
Que ilustra o envio de uma mensagem de um processo a outro.
A comunicação por troca de mensagens envolve, pelo menos, dois processos:
O transmissor e o receptor, sendo que pode haver vários receptores e transmissores.
A enviadas e recebidas, respectivamente.
Atualmente, este modelo de programação é o mais usado para programação paralela sobre uma rede de estações de trabalho.
Diversas bibliotecas foram desenvolvidas, como PVM, MPI.
Com a troca de mensagens, a falta de uma memória compartilhada é completamente exposta para o programador, que precisa decidir e conhecer a localização dos processos e dos dados, bem como efetuar explicitamente a comunicação entre os mesmos, enviando e recebendo mensagens e sincronizando cada um dos processos.
Isto torna o modelo de programação bastante complexo.
Dada a diversidade de arquiteturas paralelas, foram propostas diversas linguagens, compiladores e bibliotecas visando gerenciar programas paralelos.
O paralelismo num programa pode ser explorado implicitamente através de compiladores paralelizantes, como o explicitamente, através de primitivas inseridas no programa, ou, ainda, através de uma linguagem de programação, como C Concorrente ou Orca.
A experiência tem mostrado que a programação paralela é significativamente mais difícil que a programação seqüencial, principalmente porque envolve a necessidade de sincronização entre tarefas e a análise de dependência de dados.
A utilização de um sistema paralelizante minimiza essas dificuldades, e permite também o reaproveitamento de programas seqüenciais já implementados.
Por outro lado, o paralelismo explícito permite que fontes não passíveis de detecção por um sistema paralelizante possam ser exploradas.
O paralelismo expresso por o usuário pode ser especificado num programa através de comandos específicos de uma linguagem de programação paralela, por chamadas a rotinas de uma biblioteca que gerência os recursos de paralelismo da máquina ou por o uso de diretivas do compilador.
Em os agregados, por causa de a necessidade de troca de mensagens, as bibliotecas específicas têm sido uma forma muito utilizada de implementar programas paralelos.
São exemplos de bibliotecas PVM ­, MPI, TreadMarks e MDX Em este capítulo serão apresentadas duas bibliotecas de programação paralela por troca de mensagens, sendo uma para comunicação sobre redes Myrinet e outra para comunicação sobre redes SCI.
Bibliotecas de memória compartilhada não serão apresentadas por não fazerem parte do escopo do trabalho.
GM é uma API de comunicação para redes Myrinet.
Tem como características o baixo overhead na troca de mensagens, baixa latência e alta vazão.
Adicionalmente, possui outras características:
A comunicação em GM é feita através de estruturas chamadas &quot;portas».
Este modelo de comunicação determina que não seja necessária a conexão entre clientes que se comunicam.
Basta que um cliente monte a mensagem e a envie, através de uma porta, para qualquer outra porta na rede.
O cliente destino então consome a mensagem da porta à qual foi enviada a mesma (Figura 2.5).
O envio da mensagem é regulado por tokens, de maneira semelhante ao FM.
O emissor só pode enviar mensagens para uma porta quando possuir um token para aquela porta.
Durante o envio de uma mensagem, o emissor libera o token e o recebe de volta somente quando a mensagem é corretamente entregue.
Analogamente, o recebimento de mensagens também é regulado por tokens;
Porém, é possível ao receptor especificar diversas primitivas de recebimento, de acordo com o tamanho e a prioridade das mensagens que deseja receber.
Essa característica fornece bastante flexibilidade, pois somente serão recebidas as mensagens que satisfizerem o tamanho e a prioridade especificadas em cada receive ().
E importante salientar que a prevenção de deadlocks é tarefa do programador:
Deve haver sempre um token disponível para cada tamanho e prioridade de mensagem que se deseja receber numa determinada porta.
A ordenação é garantida somente para mensagens de mesma prioridade, vindas do mesmo emissor e enviadas ao mesmo receptor.
Não existe dependência entre mensagens de prioridades diferentes, o que permite que mensagens com prioridades mais baixas possam ser processadas antes de mensagens com prioridades mais altas.
As principais primitivas da API GM são:
Gm_ init () Inicia o ambiente GM.
Deve ser a primeira função chamada numa aplicação GM.
Abre a porta de comunicação número port da placa de rede identificada por device.
As identificar a porta.
Serve unicamente para depuração de programas.
Retorna o identificador GM associado ao nó host_ name.
Aloca length bytes de memória DMA.
Copia len bytes de from para to.
Envia msg, de tamanho len, com prioridade priority, para a porta target_ port_ id do nó target_ node_ id..
Assim que o processo de envio é completado, a função callback (port, context, status) é chamada, com status indicando que a operação de envio foi executada com sucesso.
Permite ao GM receber uma mensagem de tamanho size, com prioridade priority, na variável msg.
Retorna um evento de recebimento de mensagem em port..
Caso nenhum evento esteja pendente, GM_ NO_ RECV_ EVENT é retornado.
Caso haja uma mensagem a ser recebida, gm_ close (struct gm_ port* port) Fecha porta de comunicação.
Finaliza o ambiente GM.
Deve ser a última função chamada numa aplicação GM.
MPI consiste numa interface padrão para troca de mensagens (comunicação direta) definida por um comitê que reuniu várias instituições de ensino, pesquisa e fabricantes de máquinas paralelas.
MPI é um sistema complexo, pois sua completa especificação abrange 129 funções.
Muitas destas possuem numerosos parâmetros e variáveis.
Entretanto, a maioria das aplicações pode ser programada com apenas seis destas funções.
Um programa MPI, compreende um ou mais processos que comunicam através de funções da biblioteca MPI para enviar e receber mensagens.
Em MPI, um número fixo de processos é criado durante a fase de inicialização.
Os processos podem executar diferentes programas.
Por esta razão, o modelo de programação MPI é algumas vezes referenciado como um Multiple Program Multiple Data (MPMD), diferentemente do modelo Single Program Multiple Data (SPMD) em que cada processador executa um trecho de código de um mesmo programa.
A comunicação entre processos pode ocorrer de duas formas:
Comunicação ponto- aponto (envolve apenas o emissor e o receptor da mensagem) ou comunicação coletiva (envolve um grupo de processos).
Como citado anteriormente, MPI contém um número muito grande de funções.
Contudo, pode- se resolver um grande número de problemas usando- se somente seis funções básicas.
Além de as seis funções básicas, será apresentada outra, usada para sincronização dos processos:
É a primeira chamada que deve ser efetuada num aplicação.
Dispara a inicialização do ambiente MPI.
Os parâmetros argc e argv são requeridos somente na linguagem C, e utiliza os argumentos do programa main.
MPI_ Finalize () Essa função é usada para terminar uma aplicação.
Esta função retorna o número total de processos no grupo do comunicador.
Comunicador é um conjunto de processos que podem trocar mensagens entre si.
Quando o comunicador usado for o comunicador global pré-definido MPI_ COMM_ WORLD, então esta função indica o número total de processos envolvidos no programa.
Comm identifica o comunicador, size é o número de processos no grupo do comunicador.
Esta função retorna o identificador, isto é, o número de criação do processo atual no comunicador.
Cada comunicador contém um grupo de processos, que são identificados por um rank que começa em 0.
Comm é o comunicador, rank é o identificador do processo atual no grupo do comunicador.
Utilizada por um processo para receber uma mensagem.
Se não existir uma mensagem, o processo fica bloqueado.
Buf é o endereço inicial do receive buffer, count é o número de elementos do receive buffer, datatype é o tipo dos dados do receive buffer;
Source é o identificador do processo origem.
Tag é a tag da mensagem, comm é o comunicador, e status é o status do objeto.
MPI_ Barrier (MPI_ Comm comm) Esta função tem como objetivo bloquear o processo que a chama até que o número (quorum) de processos participantes desta barreira seja alcançado.
Ou seja, os processos que a chamarem ficarão bloqueados até que o último processo execute esta operação.
Quando isso ocorrer, todos os processos envolvidos na barreira serão desbloqueados e continuarão nas execuções.
O parâmetro comm identifica o comunicador.
A aplicação apresentada por o Algoritmo 2.1 tem como objetivo enviar uma mensagem de teste do processo com identificador 0 para o processo com identificador 1.
Inicialmente, inicia- se a aplicação em MPI com a primitiva MPI_ Init.
Esta retorna se a operação ocorreu ou não com sucesso.
Em seguida, recupera- se o número total de processos envolvidos no comunicador MPI_ COMM_ WORLD através da primitiva MPI_ Comm_ size.
Com a primitiva MPI_ Comm_ rank, recupera- se o número do identificador do processo.
Caso o identificador seja 0, é enviada uma mensagem, cujo conteúdo é &quot;Mensagem de teste «e está armazenada em buf, através da primitiva MPI_ Send ().
Caso o identificador do processo seja 1, este recebe uma mensagem e armazena na variável buf.
Em seguida, imprime o seu identificador e a mensagem recebida.
Por último, finaliza- se a aplicação em MPI e termina- se o programa.
O YAMPI (Yet Another Message Passing Interface) consiste num sub-set de comandos do MPI, que é a API de troca de mensagens mais utilizada, para uma plataforma Linux com interface de interconexão Dolphin SCI.
A proposta de API feita neste trabalho procura satisfazer os dois pontos principais deste cenário:
Uma API de troca de mensagens sobre um ambiente de memória compartilhada e com desempenho próximo a o uso de transações nativas.
Esta API foi concebida especificamente para o ambiente cluster SCI para ser utilizada por desenvolvedores de middleware.
Ela leva em conta as vantagens e limitações da interface SCI, com opções restritas e, portanto, com menor complexidade.
O YAMPI utiliza uma variação do Active Messages quanto a o tratamento das mensagens curtas.
Estas são endereçadas a regiões de memória do nó destino sem que o mesmo esteja em modo de recepção.
O uso de uma interface de troca de mensagens sobre SCI tem por objetivo oferecer a boa performance de comunicação encontrada no SCI, especialmente sua baixa latência de mensagem, sem adicionar um grande overhead de software.
A Figura 2.6 situa esta API dentro de um ambiente operacional que utiliza outras ferramentas de programação distribuída.
Os principais objetivos do projeto YAMPI são:
Eficiência: Este projeto é totalmente voltado para as capacidades da interface SCI, na tentativa de se obter valores de performance próximos a transferência de dados de memória obtidas de maneira nativa, que têm valores entre 3 e 5 µs.
De este modo as aplicações desenvolvidas poderão ser capazes de tirar proveito das larguras de banda extremamente altas oferecidas por a interconexão SCI.
Similaridade com padrão estabelecido:
A API desenvolvida deve possuir interface similar ao padrão determinado por o MPI.
Adequação ao modelo de threads:
O resultado deste projeto deve naturalmente ser compatível com ambientes multi-thread.
Este comportamento poder ser configurado em tempo de execução, uma vez que a adição de locks irá adicionar overhead na execução das operações.
Portabilidade: Todo o projeto é baseado na API SISCI, para que possa ser rapidamente portado para outros ambientes.
O YAMPI contempla seis funções básicas do MPI, sendo elas:
Este comando inicia um processo YAMPI e deve sempre ser a primeira rotina a ser chamada.
YAMPI_ Finalize () Finaliza o processo YAMPI.
Deve ser a última rotina a ser chamada numa aplicação Identifica um processo YAMPI dentro de o grupo.
O identificador é retornado em rank.
Identifica o número de processos YAMPI dentro de o grupo.
O número total de processos é retornado em size.
Rotina básica para envio de mensagens no YAMPI, e que utiliza o modo de comunicação bloqueante (blocking send).
Em este modo a finalização da chamada depende da recepção, por o nó destino, da mensagem enviada.
Em este caso, o dado deve ter sido enviado com sucesso ao processo dest, indicando que o buffer buf pode ser reutilizado.
Rotina básica para a recepção de mensagens no YAMPI, e que utiliza o modo de comunicação bloqueante.
Em este modo a finalização da chamada depende do envio, para o nó remetente, da sinalização de recebimento da mensagem.
Em este caso o dado, proveniente do processo source, deve ter sido armazenado em buf, estando pronto para ser utilizado.
A aplicação apresentada no Algoritmo 2.2 tem como objetivo enviar uma mensagem de teste do processo com identificador 0 para o processo com identificador 1.
Inicialmente, inicia- se a aplicação em YAMPI com a primitiva YAMPI_ Init.
Esta retorna se a operação ocorreu ou não com sucesso.
Em seguida, recupera- se o número total de processos envolvidos no comunicador YAMPI_ COMM_ WORLD através da primitiva YAMPI_ Comm_ size.
Com a primitiva YAMPI_ Comm_ rank, recupera- se o número do identificador do processo.
Caso o identificador seja 0, é enviada uma mensagem, cujo conteúdo é &quot;Mensagem de teste «e está armazenada em buf, através da primitiva YAMPI_ Send ().
Caso o identificador do processo seja 1, este recebe uma mensagem e armazena na variável buf.
Em seguida, imprime o seu identificador e a mensagem recebida.
Por último, é finalizada a aplicação com a primitiva YAMPI_ Finalize ().
O sistema MDX implementa um ambiente de execução paralela num cluster de estações de trabalho rodando sistema operacional Linux.
A principal característica do MDX é que ele permite os dois modelos de programação paralela:
Troca de mensagens e memória compartilhada.
Este sistema é implementado sobre o sistema operacional nativo (Linux) de cada nó do sistema.
Um núcleo de comunicação disponibiliza ao usuário transparência na execução dos programas paralelos, encaminhando as solicitações de serviços aos servidores especializados de forma transparente.
Os servidores especializados do sistema são:
Servidor de Nomes, Servidor de Memória, Servidor de Sincronização, Servidor de Comunicação e Servidor de Nomes de Portas.
O sistema foi organizado numa arquitetura multithread, onde threads especializadas são responsáveis por a execução dos serviços oferecidos por o ambiente.
O MDX serviu de base para várias dissertações de mestrado e trabalhos de conclusão de curso, sendo que, atualmente, o modelo baseado em troca de mensagens encontra- se mais desenvolvido.
Quanto a o modelo baseado em memória compartilhada, estão em fase de depuração os mecanismos que implementam a sincronização, a criação local e distante de threads e o balanceamento de carga.
Detalhes da implementação do sistema MDX são descritos em.
Uma implementação do MDX para redes ATM é descrita em.
Núcleo de Comunicação O núcleo de comunicação tem a função de receber todas as mensagens de requisição de serviços de clientes, identificar o servidor a que se destina a mensagem, descobrir a localização do mesmo e enviar a requisição do serviço.
O servidor então, processa a mensagem e a reenvia ao núcleo de comunicação que a entrega ao cliente.
As diferentes localizações dos servidores não são percebidas por os clientes, pois estas sempre enviam suas requisições de serviços para o núcleo local.
Sendo assim, este núcleo fica responsável por a tarefa de localizar o servidor e realizar o envio e a recepção de mensagens distantes.
O núcleo é composto por duas camadas:
Protocolo e encaminhamento de mensagens.
A camada protocolo é responsável por implementar um modelo cliente/ servidor de base.
Essa camada oferece quatro funções básicas:
Clientes enviam requisições, servidores recebem requisições, servidores enviam respostas e clientes recebem as respostas.
Esse protocolo é uma variante do protocolo usado no modelo cliente/ servidor e permite a chamada de um procedimento numa máquina por um processo em execução em outra ­ RPC (Remote Procedure Call).
A camada encaminhamento de mensagens é responsável por disponibilizar transparência na localização dos clientes e servidores.
É composta por dois processos que fazem a recepção, o roteamento e o envio de mensagens Em caso de cliente local, a mensagem é repassada diretamente a ele, enquanto que no caso distante a mensagem é repassada para o núcleo do cliente em questão, que a encaminha ao servidor destino.
Servidores Especializados O Sistema MDX é composto, além de o Núcleo de Comunicação, por servidores especializados, responsáveis por prestar serviços aos processos clientes.
Esses servidores recebem requisições dos clientes, processam essas requisições, e enviam as respostas aos processos que solicitaram os serviços.
Os servidores podem ser centralizados ou distribuídos.
No caso de servidor distribuído, instâncias do servidor são disparadas em diferentes nós do ambiente e cooperam para oferecer os serviços aos processos clientes.
Desta forma, pode se obter uma melhor performance na execução dos serviços.
No caso de servidor centralizado, apenas uma instância do servidor é disparada num nó do ambiente.
Em este caso, pode- se obter atomicidade na realização dos serviços.
Quando um servidor é executado, este fica num loop infinito onde recebe requisições dos clientes, executa o serviço e envia resposta ao cliente.
O servidor é finalizado quando uma requisição de término é recebida.
O MDX oferece ao usuário as seguintes primitivas Inicia uma task num determinado nó do sistema.
O parâmetro task_ name indica o nome do programa a ser executado e address indica o nó onde a task será executada.
Cria uma barreira de sincronização.
O parâmetro label indica o rótulo da barreira no sistema.
Barrier retorna os dados da barreira e quorum especifica o número de processos que irão sincronizar através da estrutura criada.
MDX_ Barrier (barrier_ id barrier) Bloqueia o processo na barreira barrier até que o número de processos bloqueados indicado na sua criação seja atingido.
Assim que este número é atingido, todos os processos são desbloqueados e continuam executando.
MDX_ Barrier_ Destroy (barrier_ id barrier) Finaliza a barreira indicada por barrier.
Todos os processos que estão bloqueados nessa barreira são desbloqueados.
MDX_ Barrier_ LookUp (barrier_ id* barrier, char* label) Retorna o identificador de uma barreira previamente criada com um rótulo indicado por label.
O retorno desta primitiva indica se a operação obteve sucesso (a barreira foi encontrada) ou não (não existe barreira associada ao label especificado).
Em o sistema MDX, programas que usam troca de mensagens se comunicam de forma indireta, através de portas de comunicação.
As portas são criadas por os processos e são gerenciadas por o sistema.
Uma porta possui um identificador único e os processos enviam e recebem mensagens através deste mecanismo.
As portas são criadas de forma explícita em qualquer nó do sistema.
Esta possibilidade de criação de portas locais ou remotas implica na necessidade de um controle dos custos de comunicação relacionados à localização da porta.
A recepção de mensagens numa porta não é restrita ao processo que a criou nem a processos localizados no mesmo nó em que se encontra a porta.
Desta forma, o custo de uma operação receive numa porta localizada remotamente é maior, uma vez que o envio da requisição e o recebimento da mensagem são feitos entre diferentes nós do ambiente.
As portas de comunicação armazenam as mensagens enviadas numa fila (FIFO) de mensagens, de onde são consumidas por os processos que executam receive.
As primitivas de comunicação do MDX são:
Cria uma porta num nó do sistema.
Assim que esta primitiva é executada, port armazena os dados da porta criada no nó address.
MDX_ Port_ Destroy (port_ id port) Primitiva usada para destruir uma porta de comunicação.
É passado como parâmetro o identificador único da porta a ser destruída.
Envia uma mensagem para uma porta de comunicação.
O parâmetro port indica a porta a qual se destina a mensagem.
O parâmetro msg armazena o conteúdo da mensagem e size representa o número de bytes a serem enviados.
Primitiva usada para receber uma mensagem de uma porta.
O parâmetro port indica a porta de a qual deve ser recebida a mensagem.
O parâmetro msg armazenará o conteúdo recebido e size representa o tamanho da mensagem.
Esta primitiva é bloqueante, ou seja, caso não haja mensagens na porta indicada, o processo fica bloqueado até que uma mensagem seja enviada para ela.
MDX_ Port_ Register (port_ id port, char* label) Associa um rótulo (label) a uma porta de comunicação do sistema.
O retorno dessa primitiva indica o sucesso ou não da execução do serviço.
O primeiro parâmetro, port, armazena a estrutura da porta a ser registrada no sistema.
O segundo parâmetro, label, guarda o rótulo a ser associado à porta.
MDX_ Port_ LookUp (port_ id* port, char* label) É usada para retornar os dados de uma porta registrada no sistema.
O primeiro parâmetro retornará o identificador da porta.
O segundo parâmetro é o rótulo por o qual a porta será localizada.
Em o modelo de memória compartilhada, variáveis compartilhadas são declaradas globais ao programa e são gerenciadas por o servidor de memória do sistema MDX.
Usando este modelo de programação, o programador não especifica onde os processos são criados.
Esta decisão é tomada por o sistema, através de algoritmos de balanceamento de carga.
A única primitiva de memória compartilhada é:
MDX_ shared var_ name Declara uma variável global que é compartilhada por os diferentes processos do mesmo programa paralelo.
O sistema suporta os tipos de dados básicos da linguagem C, assim como arrays multidimensionais.
Implementação O MDX possui algumas implementações já analisadas e documentadas em.
Partindo da implementação original, otimizações foram feitas e novas versões foram produzidas.
Em este capítulo será apresentada a implementação que obteve melhor desempenho, MDXv1.
MDX­v1 Em é descrita a implementação de uma versão otimizada do sistema MDX, chamada MDX-v1, que também roda em redes ATM.
Esta implementação visa solucionar os problemas do MDX, analisados no mesmo trabalho.
Os problemas apontados são:
Núcleo de comunicação complexo, o que resulta num maior número de chamadas ao núcleo do sistema operacional.
A estrutura do núcleo, com threads que fazem o roteamento de mensagens entre processos comunicantes, torna o caminho da mensagem cliente-servidor-cliente muito longo (6 trocas de mensagens na versão original, para o pior caso).
Overhead de controle na troca de mensagens, ou seja, pacotes de requisição e de resposta carregam muitos dados para controle.
Esta implementação tem como objetivo reduzir a complexidade do núcleo de comunicação, diminuindo ao máximo a comunicação entre clientes e servidores, e mantendo disponíveis as primitivas das versões anteriores.
A seguir será detalhada a implementação desta nova arquitetura para o MDX.
O núcleo de comunicação foi reduzido a uma tabela de local de nomes (NLT), a qual armazena a localização dos servidores na rede de processadores e é compartilhada por os processos clientes.
Desta forma, processos clientes podem se comunicar diretamente com servidores, sem que a mensagem passe por o núcleo.
Para enviar uma solicitação de serviço a um servidor, o cliente consulta a NLT e obtém a localização do servidor e o socket através de o qual o mesmo recebe as requisições.
Caso a NLT não contenha o socket de comunicação com o servidor, este é criado por o cliente e a conexão com o servidor é estabelecida.
O socket a ser usado na comunicação com o servidor é adicionado à tabela.
Através deste mecanismo, o cliente se comunica diretamente com o servidor, enviando requisições e recebendo respostas.
A busca por informações na NLT e o estabelecimento da conexão com os servidores são transparentes para o programador, sendo realizados por as primitivas MDX.
A Figura 2.7 mostra a arquitetura do sistema MDX-v1, onde núcleo de comunicação e cliente compartilham a NLT, e a comunicação do cliente com o servidor é direta.
As funções do núcleo de comunicação são as seguintes:
Criar a área de memória compartilhada onde será armazenada a NLT.
Carregar os dados da NLT, tendo como fonte um arquivo de configuração do ambiente.
A NLT consiste numa tabela residente na memória através de a qual os processos podem localizar rapidamente o endereço do destino da mensagem.
Em o esquema do MDX, o núcleo recebe todos os pedidos de clientes e executa uma consulta na NLT em busca da localização do servidor.
Como os processos do núcleo, dos servidores e dos clientes estão separados, há a necessidade da troca de informações contidas na NLT entre esses processos.
Para isso existem algumas alternativas de IPC (Interprocess Communication):
Troca de mensagens, memória compartilhada, sinais, semáforos, dutos (pipes) e RPCs.
A alternativa de memória compartilhada é que mais eficiência proporciona na implementação do compartilhamento da NLT, pois não há necessidade de cópia entre os processos nem de operações de entrada/ saída.
Em o MDX-v1, a NLT reside na memória compartilhada, uma região de memória alocada por o núcleo e mapeada por os processos clientes e servidores.
A implementação dessa área compartilhada é feita via chamadas de sistema:
Shmget (criação), shmat (mapeamento) e shmctl (controle da área).
Em a NLT, o campo identificador do processo é um número retornado por a chamada de sistema getpid ().
O motivo dessa identificação deve- se ao fato de cada cliente ter um socket para envio e recebimento de mensagens.
Se o cliente for enviar uma requisição a um servidor, ele deve primeiramente consultar a NLT, na entrada correspondente ao seu número de processo.
Os campos de identificação e endereço do servidor especificam a localização do servidor no ambiente paralelo e seu endereço.
O campo socket de envio permite que o cliente, quando transmite sua segunda requisição, envie diretamente para a conexão já estabelecida na primeira vez.
De o lado do servidor, a resposta é encaminhada por o mesmo socket onde foi feita a leitura.
O campo de identificador do nó se aplica a servidores distribuídos, ou seja, replicados na rede de processadores.
Uma vez criada a área de memória compartilhada, é necessário carregar esta tabela com os dados gravados num arquivo de configuração.
Encerrado o preenchimento da NLT, o núcleo apenas aguarda até que o usuário o finalize.
O usuário encerra o núcleo pressionando qualquer tecla.
Em o MDX-v1, quando um processo cliente deseja se comunicar com um servidor, ele deve estabelecer uma conexão com o mesmo.
Cada servidor do MDX-v1 aguarda conexões numa determinada porta.
Visando paralelismo no atendimento de requisições de diferentes clientes, assim que um cliente estabelece conexão com o servidor, é criada uma thread para atender exclusivamente a este cliente.
A partir de então, o socket através de o qual o cliente efetuou a conexão e a thread criada são dedicados exclusivamente à comunicação e ao atendimento de requisições daquele cliente.
Desta forma, o cliente estabelece conexão com o servidor apenas uma vez.
O socket desta conexão é armazenado no campo socket na NLT.
A thread criada para atender a determinado cliente possui a mesma funcionalidade dos servidores da versão original do MDX.
É um processo que fica em loop aguardando mensagens (receive) do cliente, tratando as requisições e devolvendo respostas.
A eliminação do núcleo de comunicação no processo de envio de requisições, também influência no momento da localização do servidor.
Para isso, os processos clientes dispõem do número da porta padrão em que cada servidor MDX-v1 aguarda requisições.
De posse desta informação, basta localizar, na NLT, o endereço do servidor destino.
Esta forma de comunicação também diminui o overhead de controle nas requisições, pois não é necessário o cliente especificar na mensagem qual o servidor ao qual se destina a requisição, apenas o serviço e os dados relevantes para a realização do mesmo.
Em o MDX-v1, além de as primitivas de troca de mensagens através de portas de comunicação, foi também implementada a comunicação direta entre processos.
Em este novo formato de comunicação, cada cliente, ao iniciar seu funcionamento, registra- se no servidor de comunicação com um identificador único.
A partir desse registro, os processos podem enviar mensagens diretamente para outro, indicando o identificador do processo destino.
As novas primitivas de comunicação são:
Essa primitiva cria um socket para futura comunicação e envia uma requisição de registro de cliente (REG_ CLIENT) ao servidor de comunicação.
As informações do cliente (localização e socket) são registradas com o identificador passado em id..
Localiza um cliente no sistema para futura comunicação.
Esta primitiva envia ao servidor de comunicação uma requisição de localização de cliente (LOOKUP_ CLIENT), informando o identificador do mesmo, passado por parâmetro em id..
O servidor devolve como resposta as informações (endereço e socket) do cliente procurado.
Com essas informações, a primitiva adiciona à tabela local de nomes (NLT) o cliente localizado e retorna 1.
Caso algum erro aconteça, o valor 0 é retornado.
Envia uma mensagem de forma direta a um outro processo do sistema.
As informações (enderece e socket) do cliente destino, passado por parâmetro em id_ dest, são localizadas na NLT.
O conteúdo da mensagem, passado em msg, com tamanho msg_ length, é enviado.
Esta primitiva efetua um receive no socket criado para comunicação em MDX_ RegisterClient ().
Caso não haja mensagens no socket, o processo fica bloqueado aguardando a chegada da primeira mensagem.
São passados como parâmetro a variável onde será armazenada a mensagem recebida, msg, e o número de bytes a serem recebidos, msg_ length.
Após a apresentação das bibliotecas de programação, pode- se verificar que cada uma de elas possui suas características próprias, na sintaxe das primitivas, ou ainda na forma de programação.
Por exemplo, enquanto o MPI comunica de forma direta por chamadas MPI_ Send () e MPI_ Recv (), o GM utiliza portas de comunicação para troca de mensagens.
Isto dificulta a utilização de mais de uma biblioteca de forma conjunta, como, por exemplo, para executar programas paralelos numa estrutura de cluster de clusters onde um cluster utiliza rede Myrinet e biblioteca GM para programação, e outro utiliza rede SCI e biblioteca Cluster de Clusters.
Conforme visto anteriormente, máquinas agregadas (COW ­ cluster of workstations) constituem a tendência mais atrativa na construção de máquinas paralelas, devido a o avanço nas tecnologias de redes locais (Myrinet, Fast-Ethernet, SCI) e de processadores, e ao baixo custo em relação a outras arquiteturas.
Esse avanço das tecnologias de processadores e de redes possibilita também a agregação de clusters, formando uma estrutura de cluster de clusters, como uma única máquina paralela.
Esta interligação de agregados, por meio de uma rede local ou via internet, visa principalmente alcançar o máximo desempenho na execução de aplicações.
Uma possibilidade de ganho de desempenho está no aumento do número de nós disponíveis.
Uma vez que este número aumenta, o poder computacional alcançado também é maior.
Aplicações complexas, que necessitam de um grande potencial de recursos, são distribuídas nos nós dos agregados envolvidos na estrutura de forma a conseguir uma melhora de desempenho.
Outra possibilidade de ganho de desempenho está no mapeamento de processos.
De acordo com as características de cada agregado, como, por exemplo, a velocidade da rede utilizada e o poder computacional dos nós, partes da aplicação podem executar de forma mais eficiente num determinado cluster.
Por exemplo, uma aplicação poderia ser mapeada da seguinte maneira:
Processos que realizam um grande número de trocas de mensagens executam em nós de um agregado com rede mais rápida enquanto que, processos que realizam operações mais complexas e com menos comunicação rodam num cluster cujos nós tenham um poder de processamento maior.
A Figura 3.1 mostra um exemplo de interligação de clusters.
O agregado C1, com doze nós interligados por rede primária Myrinet está ligado aos clusters C2, com seis nós e rede primária SCI, e C3, com oito nós e rede primária Fast-Ethernet.
Os nós dos agregados C1 e C2 possuem como rede secundária Fast-Ethernet, através de a qual estão ligados aos nós dos outros agregados por um switch Fast-Ethernet.
Atualmente, pesquisas vêm sendo desenvolvidas numa área chamada Grid Computing, que visa a interligação de um grande número de máquinas, que podem ser clusters, geograficamente distribuídas, de maneira a formar um metacomputador e tirar o máximo proveito do poder computacional alcançado.
A utilização de cluster de clusters pode ser um passo importante nessa direção.
A Figura 3.2 mostra uma arquitetura de cluster de clusters formada por quatro agregados.
O agregado C1 possui doze nós interligados por rede Myrinet, e está ligado por uma rede local a um agregado C2, que possui seis nós interligados por rede SCI.
C2 está conectado, via internet, aos agregados C3 e C4, sendo que o primeiro possui quatro nós interligados por tecnologia Ethernet, e o último é formado por cinco nós interconectados por rede Myrinet.
Evidentemente, o uso de uma estrutura de cluster de clusters esbarra em alguns problemas.
Um dos principais é a complexidade de programação, principalmente para que seja explorada a possibilidade de ganho de desempenho por o mapeamento de processos.
Deve haver uma análise das necessidades de cada processo, bem como das características de cada agregado.
Por exemplo, pode ser interessante que processos que necessitam compartilhar grande quantidade de dados executem num cluster com rede SCI, que fornece mecanismo de memória compartilhada em hardware.
Também pode ser interessante que processos que consomem muita CPU rodem num cluster com nós mais poderosos, mesmo que tecnologia de rede utilizada seja mais lenta.
Esse mapeamento torna a programação bastante complicada, uma vez que pode envolver mais de um modelo de programação (troca de mensagens e memória compartilhada), além de ser necessário o desenvolvimento de um algoritmo eficiente de mapeamento de processos.
Por sua complexidade, a exploração da possibilidade de ganho de desempenho por o mapeamento de processos não faz parte do escopo deste trabalho.
A exploração da possibilidade de ganho de desempenho por o aumento do poder computacional esbarra em problemas mais simples, como a necessidade de uma ferramenta de programação que ofereça suporte à heterogeneidade de tecnologia de rede.
Em esse caso, o modelo de programação pode ser único, mas a ferramenta deve fornecer mecanismos de comunicação entre processos que executam em clusters com tecnologias de rede distintas.
Atualmente, existe um grande número de bibliotecas de comunicação destinadas à programação sobre agregados com uma determinada tecnologia de rede.
Podem ser citadas como exemplos a biblioteca YAMPI (seção 2.4.3) para redes SCI, GM (seção 2.4.1) para redes Myrinet, e MPI (seção 2.4.2), que possui diferentes implementações para redes SCI, Myrinet e Fast-Ethernet.
Surge a necessidade de uma ferramenta única, que ofereça suporte à comunicação por essas redes e, além disso, ofereça possibilidade de comunicação entre processos que executam em agregados diferentes, mesmo que esses utilizem tecnologias de rede distintas.
Sistemas Relacionados Em este capítulo, serão apresentados sistemas de programação paralela que atacam os problemas decorrentes do uso de uma estrutura de cluster de clusters como uma máquina paralela.
O projeto MultiCluster, desenvolvido por o Grupo de Processamento Paralelo e Distribuído da Universidade Federal do Rio Grande do Sul (UFRGS), objetiva um ambiente de desenvolvimento de aplicações paralelas voltado à utilização de agregados de clusters.
A idéia principal do projeto é permitir a integração de agregados baseados em tecnologias de comunicação diferentes, visando permitir a combinação de modelos de programação distintos (troca de mensagens e memória compartilhada).
A biblioteca Deck serve de base para o modelo MultiCluster.
Para melhor entendimento do modelo, ele pode ser dividido em aspectos de hardware e de software.
A escolha da tecnologia usada para interligar os nós dos clusters, bem como daquela utilizada para interligar clusters, depende muito das necessidades de cada aplicação.
Para o uso do MultiCluster, independe a tecnologia que interligue os nós e os agregados (FastEthernet, Myrinet ou SCI).
Basta que um dos nós de cada cluster faça o papel de gateway para ligação com outro agregado.
A camada de software do modelo MultiCluster segue algumas definições conceituais para viabilizar a integração de maquinas agregadas.
Nós Lógicos e Nós Físicos.
Um nó físico corresponde a uma máquina disponível em qualquer dos agregados envolvidos na estrutura.
Nós lógicos correspondem ao conjunto de máquinas disponíveis do ponto de vista da aplicação.
No caso de agregados que utilizam troca de mensagens um nó lógico corresponde a um nó físico.
No caso de agregados que utilizam memória compartilhada, um nó lógico pode corresponder a mais de um nó físico.
Comunicação Intra-nós e Inter- nós As aplicações trabalham apenas com a visão de nós lógicos.
Desta forma, é relativamente fácil adaptar diferentes modelos de programação:
Dentro de um mesmo nó lógico a comunicação é feita por memória compartilhada;
Entre nós lógicos, a comunicação é feita por troca de mensagens.
De o ponto de vista do usuário, o modelo de comunicação entre processos é único, e a camada de comunicação se encarrega de implementar um ou outro.
Heterogeneidade Embora seja um problema menos freqüente, a heterogeneidade pode aumentar de acordo com as características dos clusters que serão interligados.
Em este ponto, são consideradas as diferentes representações de dados e a necessidade de indicar ao destinatário da mensagem qual a arquitetura do processo remetente.
Esse problema deve ser tratado implicitamente por a camada de comunicação.
Para diminuir qualquer queda de performance que possa haver numa integração deste tipo, é dada ao usuário a possibilidade de definir a melhor localização para as tasks do programa, criando mecanismos próprios de comunicação para cada uma.
Com essa facilidade, a comunicação entre processos em clusters diferentes pode ser balanceada e diminuída tanto quanto possível, diminuindo o tráfego na rede que interliga os agregados.
Deck (Distributed Execution and Communication Kernel) é uma biblioteca de que provê abstrações básicas para aplicações paralelas, como threads e caixas postais, além de serviços mais complexos, como serviço de nomes e comunicação coletiva.
Deck possui duas camadas.
A camada de baixo é chamada de µDECK e é responsável por as abstrações básicas:
Threads, semáforos, mensagens, caixas postais e memória compartilhada.
A camada mais acima é a camada de serviços, serviços estes que podem ser escolhidos em tempo de compilação.
Dois serviços devem ser analisados de forma mais completa:
Serviço de nomes e RCD (Remote Communication Daemon).
O servidor de nomes é um processo dedicado que executa no primeiro nó de cada cluster.
Por exemplo, pode haver um servidor de nomes rodando no nó &quot;verissimo «e outro no nó &quot;scliar».
Cada servidor de nomes é responsável por registrar caixas postais criadas no agregado onde estão rodando e é executado automaticamente no início da aplicação.
A camada RCD é descrita mais detalhadamente na seção 4.1.6.
O Deck funciona, atualmente, sobre redes Myrinet e SCI.
A parte de multithread, em ambas as implementações, utilizam chamadas POSIX Threads.
Para implementação da comunicação sobre redes Myrinet, Deck utiliza a biblioteca Bip ­ Basic Interface for Parallelism, principalmente por sua simplicidade e desempenho.
Para seguir esse modelo, Deck utiliza um protocolo de negociação entre processos onde o remetente envia mensagens de requisição pequenas antes de enviar uma mensagem grande.
Essas requisições são tratadas, no lado do receptor da mensagem, por uma thread dedicada, a rv-daemon, criada no início do processo.
Quando um processo envia uma mensagem pequena, esta é colocada diretamente na caixa postal destino, pois Bip suporta o armazenamento de mensagens pequenas em buffers internos.
No caso de uma mensagem grande, uma requisição é enviada ao rv-daemon do lado do receptor.
Essa thread verifica se há espaço nos buffers para armazenar a mensagem e responde.
O remetente permanece enviando a requisição ao rv-daemon do receptor até receber uma resposta positiva, quando, enfim, envia a mensagem.
A implementação do Deck/ SCI é feita sobre duas bibliotecas de programação para SCI:
Yasmin, que provê mecanismos para criação, mapeamento e sincronização de segmentos compartilhados, e Sthreads, que oferece chamadas POSIX Threads sobre o Yasmin.
São oferecidos, por a camada µDECK, serviços de criação, denominação, mapeamento e locking de segmentos compartilhados.
Diferentemente de Myrinet, SCI oferece uma fácil implementação dos dois modelos de programação.
Desta forma, Deck/ SCI oferece caixas postais e troca de mensagens, além de mecanismos de memória compartilhada.
É necessário lembrar que segmentos de memória só podem ser compartilhados por processos localizados num mesmo nó lógico.
Para oferecer suporte ao modelo MutliCluster, o RCD (Remote Communication Daemon) foi implementado como um serviço do Deck para realizar a comunicação entre clusters.
Como cada cluster possui um nó que faz o papel de gateway, o RCD é executado, neste nó, automaticamente no início da aplicação.
O RCD atua em duas ocasiões:
Quando a aplicação procura por um nó localizado em outro agregado e quando mensagens são enviadas a uma caixa postal remota (em outro cluster).
Quando uma primitiva Deck falha na busca por uma caixa postal num servidor de nomes local, ela contata o RCD, que envia uma mensagem broadcast aos outros RCDs no sistema e aguarda por uma resposta, retornando- à a primitiva que o chamou.
Em o segundo caso, quando uma primitiva Deck necessita enviar mensagem a uma caixa postal remota, ela contata o RCD, que encaminha a mensagem ao RCD responsável por a comunicação remota no agregado onde a caixa postal destino está localizada.
Desde a publicação do padrão MPI, um grande número de implementações MPI de alta qualidade vêm sendo disponibilizados.
De entre essas implementações, podem ser Argonne National Laboratory.
Uma vez que apenas a funcionalidade do MPI é especificada por o padrão, cada implementação é otimizada para uma determinada arquitetura.
Desta forma, não é possível que versões diferentes do MPI cooperem na execução de uma aplicação.
Para resolver este problema, foi formado o Interoperable MPI Steering Committee (IMPI).
Este comitê elaborou uma proposta de padrão para a interoperabilidade entre diferentes implementações MPI Um dos principais objetivos do IMPI é oferecer uma interface de programação idêntica a do MPI, mas utilizar múltiplas versões do MPI para executar uma única aplicação.
Assim, uma aplicação que executa corretamente com MPI não necessita qualquer alteração para rodar com o IMPI.
O padrão IMPI é dividido em quatro partes:
Protocolos de startup/ shutdown, protocolo de transferência de dados, algoritmos coletivos e metodologia de testes.
O padrão IMPI usa o termo &quot;implementações MPI «para referir universos MPI que são utilizados para executar uma aplicação.
Não significa necessariamente implementações diferentes do MPI, mas também múltiplas instâncias da mesma implementação.
Por exemplo, o IMPI pode utilizar duas instâncias de uma versão LAM/ MPI.
Existem quatro entidades IMPI:
Server, clients, hosts and procs.
Essas entidades são apresentadas por a Figura 4.1 e são descritas a seguir.
O server é o ponto de encontro entre implementações MPI.
Há um client por implementação e esse client é o representante da implementação no server.
Pode haver um máximo de 32 clients para executar uma aplicação IMPI.
Cada client pode ter um ou mais hosts.
Cada host representa um grupo de processos (procs).
Dois passos são necessários para disparar uma aplicação IMPI.
O primeiro passo é iniciar o server, que fica aguardando conexões dos clients.
Um client, ao conectar o server, envia ao mesmo as suas informações, como o número de hosts que ele representa, o número de procs de todos os clients e as envia a todos os clients.
Desta forma, cada client obtém as informações referentes a todos os outros clients e pode passar- las a seus hosts.
Com base nessas informações, os clients decidem de forma será feita a comunicação entre eles.
Por exemplo, cada client envolvido numa comunicação compara o tamanho máximo de pacote suportado por os dois e opta por o menor valor.
Este processo é chamado &quot;negociação», embora as decisões sejam tomadas de forma independente e distribuída.
Após o startup, tem início o segundo passo.
Os hosts devem criar conexões TCP entre eles, e essa conexão será usada para troca de mensagens entre implementações MPI.
Após a criação dessas conexões, o server e os clients ficam simplesmente aguardando o fim da execução da aplicação.
O processo de shutdown ocorre da seguinte maneira.
Cada proc envia uma mensagem ao seu host indicando a seu término.
Assim que um host recebe essa indicação de todos os seus procs, ele transmite essa mensagem para seu client.
De forma semelhante, quando o client recebe mensagens de término de todos os seus hosts, ele repassa a mensagem ao server.
Finalmente, o server termina quando receber mensagem de término de todos os clients.
Mensagens trocadas entre processos executando numa mesma implementação MPI utilizam os mecanismos otimizados desenvolvidos para aquela implementação.
O IMPI só interfere na comunicação entre processos que rodam em diferentes implementações.
Caso um processo deseja enviar uma mensagem a outro que executa em outra implementação, a mensagem deve ser passada do host local, que a repassa ao host remoto.
O host remoto deve transmitir a mensagem ao processo destino.
Como as comunicações por TCP são lentas, se comparado à troca de mensagens numa única implementação MPI (para redes Myrinet, por exemplo), os algoritmos coletivos, como MPI_ BARRIER, por exemplo, foram desenvolvidos de forma a minimizar a comunicação entre diferentes implementações.
A maioria desses algoritmos tem uma fase local e uma fase global.
Por exemplo, a execução de uma barreira entre múltiplas implementações possui duas fases.
Em a primeira, todos os processos executando numa mesma implementação sincronizam por sua barreira local.
A segunda fase é a sincronização das barreiras locais por uma barreira global.
Está disponível uma ferramenta de teste do IMPI.
Um applet Java, que pode ser encontrado qualquer número de hosts e procs.
Através desse simulador, uma implementação do IMPI pode ser testada.
Uma vez passando esses testes, uma implementação IMPI teoricamente está pronta para cooperar com qualquer outra implementação IMPI que também tenha passado por os testes.
O objetivo do MPICH/ Madeleine é oferecer uma versão do MPICH com suporte eficiente e simultâneo a diferentes tecnologias de rede.
Esta versão foi produzida sobre uma biblioteca de comunicação que já oferece suporte a múltiplas tecnologias de rede, o Madeleine, que é o módulo de comunicação da ferramenta de programação PM².
A interface de programação Madeleine provê um pequeno conjunto de primitivas orientadas à troca de mensagens.
Basicamente, oferece primitivas para envio e recebimento de mensagens, bem como primitivas que permitem que o usuário defina de que maneira os dados serão inseridos ou extraídos das mensagens.
O Madeleine gerência o uso de diferentes protocolos de rede ao mesmo tempo, além de gerenciar o uso de múltiplos adaptadores de rede para cada um desses protocolos.
Isso permite que a aplicação do usuário escolha dinamicamente um determinado protocolo, de acordo com suas necessidades.
Este controle é oferecido por dois objetos básicos.
O objeto channel define um universo fechado de comunicação.
Cada channel é associado a um protocolo de rede, um adaptador de rede correspondente e um conjunto de objetos connection.
Cada connection representa uma conexão ponto-a-ponto entre dois processos.
A definição do tamanho do buffer de mensagens é dependente das tecnologias disponíveis para comunicação.
Os pontos de buffer para rede TCP/ Fast-Ethernet, SISCI/ SCI e Bip/ Myrinet são, respectivamente, 64 Kbytes, 8 Kbytes e 7 Kbytes.
No caso de o uso de mais de uma tecnologia de rede, o tamanho do buffer que armazena a mensagem deve ser igual ao maior ponto de escolha entre as tecnologias disponíveis.
Por exemplo, no caso de um agregado que utiliza rede SCI e Fast-Ethernet/ TCP, o buffer deve ser de 64 Kbytes.
Caso a mensagem seja enviada por rede SCI, o buffer nunca será completado e vários valores null serão enviados, prejudicando o desempenho da comunicação.
Diferentemente de outras abordagens, que utilizam conexões TCP para conectar múltiplas implementações MPI, o MPICH/ Madeleine permite às aplicações utilizar clusters Myrinet, SCI e Ethernet conectados por qualquer dessas tecnologias de rede.
No entanto, há a necessidade de que todos os nós da estrutura estejam conectados diretamente.
Atualmente, encontra- se em desenvolvimento um mecanismo de conexão que utiliza nós que fazem o papel de gateway, de maneira semelhante àquela implementada no projeto Multicluster (seção Após apresentados os sistemas de programação, pode ser montada a seguinte tabela de comparação entre eles:
O Projeto Multicluster teve seu desenvolvimento interrompido, e ainda nenhum protótipo foi disponibilizado.
O padrão IMPI foi implementado apenas uma vez como um protótipo para validar o padrão.
Este protótipo não contempla todas as definições do padrão.
De a mesma forma, se encontra disponível apenas um protótipo do MPICH/ Madeleine, em o qual estão sendo feitas alterações no sentido de melhorar sua performance.
Ambiente MDX-cc Este trabalho propõe e implementa um ambiente de programação paralela para rodar sobre uma arquitetura de cluster de clusters, oferecendo os recursos necessários para o aproveitamento de uma estrutura desta natureza.
Este capítulo descreve esse ambiente, em termos de recursos e funcionalidades, e apresenta detalhes de implementação do mesmo.
O projeto aqui descrito foi motivado por alguns aspectos importantes.
O principal de eles é a necessidade de um melhor aproveitamento da infra-estrutura oferecida por a universidade.
Hoje, a PUCRS conta com um importante centro de pesquisa em processamento de alto desempenho, o CPAD.
Esse centro dispõe de quatro agregados, sendo que dois de eles utilizam rede Fast-Ethernet, um usa rede SCI e outro rede Myrinet.
O CPAD tem interesse em executar aplicações paralelas utilizando todo o poder computacional disponível.
Para isso, é interessante a utilização, por uma mesma aplicação, de todos os agregados interligados, formando uma estrutura heterogênea de cluster de clusters.
A utilização dessa estrutura requer um ambiente que possibilite a programação e a execução de programas paralelos em cluster de clusters.
O Grupo de Processamento Paralelo e Distribuído do curso de Pós-graduação em Ciência da Computação da PUCRS vem desenvolvendo, há alguns anos, uma ferramenta de programação paralela, o Sistema MDX, que executa sobre máquinas agregadas.
A idéia deste trabalho é unir a experiência no desenvolvimento do MDX com a experiência na programação sobre clusters, adquirida no CPAD, para implementar um ambiente de programação paralela para rodar sobre cluster de clusters:
O MDX-cc.
O MDXcc é baseado no Sistema MDX, em sua implementação MDX-v1 (seção 2.4.4).
No entanto, oferece apenas o paradigma de programação por troca de mensagens (o MDX oferece também o paradigma de memória compartilhada).
A programação por memória compartilhada foi deixada de lado neste primeiro momento devido a o tempo disponível para este trabalho e ao grande número de modificações que se fizeram necessárias na estrutura do ambiente original para adaptar o seu uso a uma estrutura de cluster de clusters (seção 5.3).
O MDX-cc executa processos de um mesmo programa paralelo, no modelo SPMD, em vários clusters, ligados um ao outro por rede Fast-Ethernet.
Esses clusters devem, internamente, ter seus nós conectados por rede Fast-Ethernet, SCI ou Myrinet.
A idéia principal é oferecer ao usuário total transparência na comunicação entre os processos.
A troca de mensagens deve ocorrer independentemente da tecnologia de rede disponível no nó onde receive, não se preocupando com qual tecnologia será usada para comunicação.
O MDX-cc provê algumas funcionalidades básicas para executar aplicações paralelas sobre agregados heterogêneos interligados.
Essas funcionalidades são:
Detecção das tecnologias de rede disponíveis em cada nó Tendo em vista que processos podem rodar em nós de agregados que usam, internamente, redes distintas, cada processo MDX-cc identifica, de forma automática, as tecnologias de rede disponíveis no nó onde está executando.
Com base nessa informação, entre outras, o MDX-cc vai decidir por qual rede a comunicação com outro processo será realizada.
Protocolo de Comunicação O MDX-cc fornece os mecanismos de troca de mensagens entre processos.
A tecnologia de rede utilizada na comunicação deve ser transparente ao usuário, de forma que ele apenas faça melhor rede a ser utilizada.
Sincronização por Barreiras Técnicas de sincronização (semáforos, monitores, barreiras entre outros) são geralmente utilizadas na programação paralela para máquinas com memória comum, sendo que os processos paralelos interagem através de memória compartilhada.
Todavia, as máquinas sem memória comum necessitam de um mecanismo de sincronização determinado por um modelo de programa paralelo:
Processos comunicantes ou cliente/ servidor.
O MDX-cc implementa um mecanismo de barreiras de sincronização.
As barreiras são gerenciadas por um servidor especializado, o Servidor de Sincronização.
Podem ser criadas e executadas barreiras a fim de sincronizar um grupo de processos.
Memória Compartilhada Distribuída Seguindo o projeto inicial do Sistema MDX, que prevê também o paradigma de memória compartilhada, um módulo de memória compartilhada deve oferecer primitivas de criação, escrita, leitura e remoção de variáveis compartilhadas.
Em clusters com rede SCI, os mecanismos de memória compartilhada do hardware SCI devem ser utilizados.
Para agregados com as outras tecnologias de rede, deve haver uma abstração, por meio de um servidor de memória compartilhada distribuída.
Este serviço não foi implementado nesta primeira versão do MDX-cc.
Balanceamento de Carga Visando evitar a sobrecarga de processos em nós na execução de um programa paralelo, um serviço de balanceamento de carga deve avaliar o estado atual de carga de cada um dos nós no momento do disparo de cada processo.
Desta forma, deve evitar que um determinado nó receba processos mais pesados que outros, tornando aquele nó um ponto de degradação de desempenho.
Este serviço não foi implementado.
O MDX-cc usa um modelo de round-robin simples no disparo dos processos nos nós envolvidos no sistema.
Um arquivo de nomes de máquinas armazena os nós onde os processos devem ser disparados.
No caso de o arquivo apresentado na Figura 5.2, é disparado um processo em cada um dos seis nós, começando por o nó &quot;amazonia01», seguindo a ordem apresentada no arquivo, até o nó &quot;tropical02».
Caso haja mais processos do que nós disponíveis, o disparo recomeça no nó &quot;amazonia01 «e assim segue até que o número total de processos tenha sido disparado.
Comunicação em Grupo Um mecanismo interessante em programas paralelos é o de criação de grupos de processos.
O MDX-cc deve prover mecanismos de envio de mensagens para grupos de processos (Multicast) e envio de mensagens a todos os processos da aplicação (Broadcast).
Também deve oferecer rotinas de recebimento genérico, ou seja, sem especificar a origem da mensagem.
Este serviço não foi implementado.
O MDX-cc oferece somente mecanismos de comunicação ponto-a-ponto entre processos, onde deve ser informado a origem (no caso de um receive) ou o destino (no caso de uma primitiva send) da mensagem.
Para que o MDX execute sobre uma estrutura de cluster de clusters, alterações na estrutura do sistema se fizeram necessárias.
Esta seção apresenta as principais modificações realizadas.
O MDX, em sua implementação MDX-v1, possui um Núcleo de Comunicação, que executa em todos os nós do ambiente, e é responsável única e exclusivamente por criar uma área de memória compartilhada, para onde é carregada a Tabela Local de Nomes (NLT).
A Figura 5.3 ilustra a estrutura do MDX.
O sistema MDX oferecia poucas ferramentas de infra-estrutura, como, por exemplo, aplicativos de configuração da ferramenta, programas que facilitem o início e o término do ambiente, scripts de disparo de aplicações e ferramenta de compilação de programas.
Para o MDX-cc, foram implementados utilitários que auxiliam na programação e na execução de aplicações.
Para a configuração do ambiente, foi implementado o mdxconf.
Para a compilação de programas, a ferramenta desenvolvida é o mdxcc (seção 5.4.6) e o mdxshutdown (seção 5.4.6).
Em o MDX-v1, tarefas eram criadas dinamicamente, com a primitiva MDX_ Task_ Create ().
Em uma estrutura de cluster de clusters, o número de tarefas criadas dinamicamente pode ser muito alto, dado o aumento dos recursos computacionais disponíveis.
A Figura 5.5 ilustra a criação de tarefas no MDX.
Em o exemplo apresentado, o processo1, que executa no nó 1, cria o processo2, no nó 2, através de uma chamada à primitiva MDX_ Task_ Create ().
Para facilitar a programação sobre cluster de clusters, o MDX-cc oferece suporte à programação SPMD.
Agora, a aplicação é distribuída nos nós disponíveis, e cada processo, ao se registrar no Servidor de Comunicação, recebe, na variável MDX_ id um identificador único, e, na variável MDX_ np o número de processos disparados.
Até a sua versão MDX-v1, o MDX oferecia suporte às tecnologias Fast-Ethernet, por protocolo UDP, e ATM.
As tecnologias mais usadas atualmente na construção de agregados são Fast-Ethernet, Myrinet e SCI.
Em esse contexto, para o MDX-cc, foi deixada de fora a implementação com suporte a rede ATM e foi implementado versões com suporte às tecnologias Fast-Ethernet TCP, Myrinet e SCI.
Com a implementação dessas versões, o MDX passa a oferecer suporte à comunicação por rede Fast-Ethernet (TCP e UDP), Myrinet e SCI.
Em o MDX, a estrutura de armazenamento de informações de servidores e de processos era única e muito simples, a NLT.
Ela armazenava informações sobre a localização dos servidores, bem como as informações para comunicação com os processos da aplicação (identificador, endereço do nó onde executa e porta de comunicação).
A Figura 5.6 ilustra essa estrutura.
Com a implementação do suporte a outras tecnologias de rede, há um aumento no número de informações necessárias para comunicação entre os processos.
Isto torna impossível o armazenamento das informações dos processos na NLT.
Por isso, essa estrutura foi modificada completamente e dividida em duas:
NLT e LocalClientTable.
A NLT foi simplificada e armazena informações somente sobre a localização dos servidores do ambiente.
A LocalClientTable é uma estrutura mais complexa e armazena informações sobre os processos com os quais há comunicação.
A nova estrutura é ilustrada por a Figura 5.7.
Um dos objetivos do MDX-cc é oferecer ao usuário uma interface de programação simples.
A fim de atingir este objetivo, alguns mecanismos oferecidos por o MDX foram modificados.
Em o MDX, o mecanismo de comunicação é o de portas, onde uma mensagem é enviada a uma determinada porta e consumida da mesma.
Esse mecanismo apresentou um péssimo desempenho, sendo por isso deixado de fora de os mecanismos de comunicação do MDX-cc.
Para a comunicação direta, o número de primitivas foi reduzido a dois, e essas primitivas são apresentadas na seção 5.4.4.3.
O uso de barreiras de sincronização também foi simplificado.
As primitivas de sincronização foram reduzidas a duas (seção 5.4.4.2) e são de mais fácil utilização.
Com essa simplificação e eliminação de serviços, alguns servidores especializados do MDX também ficaram de fora de o MDX-cc.
Além de essas alterações, uma série de avaliações de desempenho e de otimizações foram realizadas no decorrer de o trabalho.
Essas otimizações e avaliações são apresentadas em A arquitetura do MDX-cc é baseada no modelo cliente-servidor, onde processos servidores atendem requisições de processos clientes.
Esses processos clientes são partes da aplicação paralela que executa sobre o ambiente.
O MDX-cc oferece suporte a comunicação através de redes Fast-Ethernet, Myrinet e SCI.
Para a implementação dos mecanismos de comunicação sobre essas tecnologias, foram utilizadas as bibliotecas sockets para rede Fast-Ethernet, GM para redes Myrinet, e YAMPI para rede SCI.
A Figura 5.9 ilustra a estrutura externa do MDX-cc, onde o ambiente é implementado sobre as bibliotecas de comunicação citadas anteriormente.
A biblioteca de sockets executa sobre o sistema operacional, enquanto as bibliotecas YAMPI e GM acessam diretamente as interfaces de rede SCI e Myrinet, respectivamente.
Sobre o MDX-cc executam as aplicações paralelas.
O MDX-cc possui um arquivo de configuração (nlt.
Conf) que armazena a localização dos servidores do ambiente.
Esse arquivo armazena apenas o identificador do servidor (sincronização ou comunicação) e o endereço IP do nó onde o mesmo deve executar (Tabela sistema serão executados os servidores de comunicação e sincronização do MDX-cc.
Para modificação desse arquivo, um aplicativo foi implementado, o mdxconf.
Esse aplicativo permite verificar a configuração atual do ambiente, bem como alterar essa configuração, de forma simples.
A Figura 5.10 apresenta a tela inicial do mdxconf, onde aparecem opções para configurar um novo ambiente, adicionar um servidor ao sistema ou listar a configuração atual do ambiente.
Cada processo de uma aplicação MDX-cc possui uma Tabela Local de Nomes, a NLT.
A o ser iniciado, cada processo carrega a sua NLT com os dados do arquivo de configuração nlt.
Conf. Essa tabela será usada na localização dos servidores no sistema, para o envio de requisições de serviços.
Por exemplo, quando um processo cliente efetua uma chamada de criação de barreira de sincronização, a primitiva localiza o servidor de sincronização na NLT e envia uma requisição de criação de barreira.
A Tabela 5.2 apresenta os campos da NLT, bem como a informação armazenada por cada um de eles.
As informações armazenadas nos primeiros dois campos são carregadas do arquivo nlt.
Conf no início do processo.
O campo serv_ socket é preenchido no estabelecimento da conexão com o servidor, e armazena o socket através de o qual o processo se comunica com o mesmo.
A comunicação entre processos clientes e servidores é detalhada a seguir.
A comunicação entre processos clientes e servidores ocorre de forma implícita nas primitivas do MDX-cc.
Quando uma primitiva (por exemplo, de execução de barreira de sincronização) necessita de algum serviço da camada de serviços do MDX-cc, ela localiza, na NLT, o servidor que atenderá a requisição, envia a solicitação ao mesmo e recebe a resposta.
A Figura 5.11 ilustra, de forma simplificada, o sistema de comunicação entre processos clientes e servidores.
O processo cliente consulta a NLT, localiza o servidor, envia uma requisição e recebe uma resposta.
Uma seqüência de passos é necessária para a comunicação entre os processos clientes e os servidores do MDX-cc:
É importante ressaltar que a comunicação entre processos clientes e os servidores do MDX-cc sempre se dá através da rede Fast-Ethernet, de maneira a preservar a rede rápida (SCI ou Myrinet, quando utilizada) para a comunicação entre os processos da aplicação.
A estrutura e o funcionamento dos servidores do MDX-cc são descritos de forma detalhada a seguir.
Em o MDX-cc, cada servidor especializado foi implementado como um programa independente, que executa em algum dos nós do sistema, conforme configurado através do mdxconf.
Quando um processo cliente deseja se comunicar com um servidor, ele deve estabelecer uma conexão com o mesmo, através de uma chamada connect.
Cada servidor do MDX-cc aguarda conexões, por uma chamada accept, numa porta diferente, conforme mostra a Tabela 5.3.
Visando um maior paralelismo no atendimento de requisições de diferentes clientes, os servidores são programas multithread.
Em o momento que um cliente estabelece conexão com o servidor, é criada uma nova thread.
A partir de então, o socket através de o qual o processo efetuou a conexão e a thread criada são dedicados exclusivamente à comunicação e ao atendimento de requisições daquele cliente.
Assim, o cliente efetua a conexão com o servidor apenas uma vez, através de uma chamada connect, no endereço e porta de comunicação do servidor.
O socket desta conexão é armazenado no campo serv_ socket na NLT (Tabela 5.2) do processo cliente.
O Algoritmo 5.1 mostra um esqueleto de código de um servidor.
A thread criada para atender a um cliente consiste num processo que fica em loop aguardando mensagens do cliente, tratando as requisições e devolvendo respostas.
O Algoritmo 5.2 mostra um esqueleto de código de uma thread que atende um determinado cliente.
Algoritmo 5.1: Servidor de Sincronização O Servidor de Sincronização é um servidor do MDX-cc que é disparado automaticamente no início da aplicação paralela e executa em apenas um nó do ambiente.
A função do Servidor de Sincronização é o gerenciamento dos mecanismos de sincronização do ambiente.
A sincronização de processos é realizada através de barreiras.
Estas são implementadas como uma lista encadeada simples, a BarrierList.
Barreira é uma estrutura que contém quatro campos:
Identificador, Quorum, Count e ProcessList, conforme mostra a Figura 5.12.
Quorum expressa o número de processos que sincronizam através da barreira e o campo Count tem como objetivo a verificação do número de processos bloqueados, isto é, que já executaram a barreira.
Cada vez que um processo executa a barreira este campo é incrementado.
Quando seu valor atingir o Quorum todos os processos bloqueados são liberados.
A ProcessList de uma barreira é uma lista que contém todos os processos bloqueados.
A estrutura que armazena informações dos processos é composta por dois campos:
ProcId, que armazena o identificador do processo, e ProcSocket, que armazena o identificador do socket de comunicação do servidor com o processo.
Quando o servidor recebe uma mensagem, este identifica e trata o serviço solicitado.
Quando um serviço de criação de barreira (BarrierCreate) é identificado, o Servidor de Sincronização dispara a função BarrierCreate para tratar- lo.
Esta função cria uma nova barreira na sua lista de barreiras e atribui a mesma o Identificador e o Quorum recebidos.
Em seguida, esta devolve uma mensagem contendo um indicativo de sucesso.
Caso a criação não tenha ocorrido, esta devolve um indicativo de insucesso.
Após o envio da mensagem de resposta, encerra- se a função.
Quando um serviço de execução de barreira (Barrier) é identificado, o Servidor de Sincronização dispara a execução da função Barrier para tratar- lo.
Esta inclui o processo (identificador e socket de comunicação) na lista de processos (ProcessList) da barreira, e incrementa o campo Count.
Em seguida, verifica- se se o processo foi incluído na ProcessList com sucesso.
Se a inclusão não ocorreu, um indicativo de insucesso é enviando ao cliente.
Caso contrário, verifica- se se o Quorum foi alcançado (Count $= Quorum).
Se isto ocorrer, envia- se um indicativo de sucesso para todos os processos contidos na ProcessList, desbloqueando os mesmos.
A o final desta operação, encerra- se a execução da função.
Existem duas barreiras criadas quando o servidor é disparado, que são InitBarrier e FinalBarrier.
Essas duas barreiras servem para sincronizar o início e o fim de cada processo.
Servidor de Comunicação O Servidor de Comunicação é um servidor do MDX-cc que é disparado automaticamente no início da aplicação paralela e executa em apenas um nó do ambiente.
A principal função do Servidor de Comunicação é o gerenciamento das informações de todos os processos da aplicação paralela.
Esse servidor também é responsável por atribuir um identificador único a cada processo.
O armazenamento das informações dos processos é feito numa tabela, chamada de ClientTable.
Essa tabela é implementada como um vetor de np posições, onde np é o número de processos da aplicação paralela.
Cada posição desse vetor armazena as informações de um processo, cujo identificador é dado por a posição do vetor.
Por exemplo, as informações do processo 15 estão localizadas na posição 15 da ClientTable.
A estrutura da ClientTable é ilustrada na Figura 5.13.
Sobre um processo são armazenados os seguintes campos:
Info_ ok:
Indica se as informações do processo estão preenchidas has_ MYR:
Informa se o processo pode comunicar por rede Myrinet myr_ info:
Ponteiro para uma estrutura que armazena informações para a comunicação com o processo por tecnologia Myrinet.
Essa estrutura contém os seguintes campos node_ id:
Identificador do nó, atribuído por a biblioteca GM port_ id:
Porta de comunicação GM, criada por o processo GM_ port:
Estrutura usada por a biblioteca GM para comunicar com outros processos has_ SCI:
Indica se o processo pode comunicar através da tecnologia SCI sci_ info:
Ponteiro para uma estrutura que armazena informações para comunicação com o processo através da tecnologia SCI.
Essa estrutura possui os seguintes campos:
YAMPI_ id:
Identificador do processo, usado para comunicação por a biblioteca YAMPI has_ ETH:
Informa se o processo pode comunicar através de rede Fast-Ethernet_ info:
Ponteiro para uma estrutura que armazena informações para comunicação com o processo através da tecnologia Fast-Ethernet.
Essa estrutura possui os seguintes campos:
Quando o servidor recebe uma mensagem, este identifica e trata o serviço solicitado.
O Servidor de Comunicação atende a dois serviços do MDX-cc:
RegisterClient e LookupClient.
O primeiro é o serviço de registro de informações no servidor.
Cada processo MDX-cc registra suas informações no Servidor de Comunicação.
Essas informações serão recuperadas por outros processos para futura comunicação.
O segundo serviço é o de recuperação de informações de um processo.
A o ser identificada uma requisição de RegisterClient, o servidor dispara a função AddClient, passando as informações do processo, recebidas junto com a requisição.
Essa função gera um identificador i, único para o processo, e armazena as informações recebidas na posição i da ClientTable.
Em seguida, o identificador gerado é enviado ao processo requisitante, bem como o número total de processos da aplicação.
Caso o processo não tenha sido registrado na tabela de clientes, um indicativo de erro é enviado como resposta.
Quando uma requisição de LookupClient é identificada, o servidor dispara a função LookupClient, passando o identificador do processo, que foi enviado com a requisição Essa função localiza as informações do processo, que estão na posição i da ClientTable, e envia essas informações ao processo requisitante.
Caso as informações do processo não estejam registradas na tabela, um indicativo de erro é enviado como resposta.
O MDX-cc, em sua concepção, prevê um conjunto reduzido de primitivas.
Isso proporcionou a implementação de uma interface de programação bastante simples.
Esta seção apresenta as primitivas do MDX-cc, dividindo- as em Primitivas de Controle, Primitivas de Sincronização e Primitivas de Comunicação.
Primitivas de Controle As primitivas de controle são usadas para inicio é término de programas MDX-cc.
Essas primitivas são apresentadas por a Figura 5.14.
A primitiva MDX_ Init () deve ser chamada no início de um programa MDX-cc.
Essa primitiva, inicialmente, chama a função MDX_ EnvDetect (), para detecção das tecnologias de rede disponíveis no nó onde o processo executa.
Essa função detecta, através dos arquivos /proc/pci e /proc/modules do Linux, quais as interfaces de rede que estão instaladas no nó Fast-Ethernet (obrigatório) e SCI e/ ou Myrinet.
Uma vez detectadas as interfaces de rede disponíveis, o processo pode iniciar os ambientes através de os quais ele poderá comunicar.
Se a tecnologia SCI está disponível no nó, é chamada a função YAMPI_ Init (), que é a primeira função a ser chamada numa aplicação YAMPI.
Caso a tecnologia Myrinet tenha sido detectada, são chamadas duas funções:
A função gm_ init (), que inicia um processo GM, e a função gm_ open (), que serve para abrir uma porta de comunicação GM, estrutura que será usada para enviar e receber mensagens via rede Myrinet.
Para a comunicação por rede Fast-Ethernet são criados dois sockets, um para comunicação por protocolo TCP e outro para comunicação por protocolo UDP.
Depois de iniciados os ambientes através de os quais o processo poderá comunicar, as informações devem ser registradas no Servidor de Comunicação.
Para isso, é chamada a função MDX_ RegisterClient ().
Esta função envia uma requisição de serviço RegisterClient ao Servidor de Comunicação, enviando também as informações sobre o processo e sobre as tecnologias de rede detectadas.
Como resposta, em caso de sucesso no registro das informações, o processo recebe o seu identificador e o número de processos disparados na aplicação.
O identificador é armazenado na variável MDX_ id, e o número de processos na variável MDX_ np.
Finalmente, o processo executa a barreira InitBarrier, que sincroniza o início de todos os processos da aplicação.
A outra primitiva de controle do MDX-cc é a MDX_ Finalize ().
Essa primitiva executa a barreira FinalBarrier, a fim de sincronizar o término de todos os processos da aplicação.
Em seguida, são finalizados os ambientes de programação iniciados em MDX_ Init ().
Caso o ambiente YAMPI tenha sido iniciado, é chamada a função YAMPI_ Finalize ().
Caso o ambiente GM tenha sido iniciado, é chamada a função gm_ finalize ().
Finalmente, o processo é encerrado.
Primitivas de Sincronização A sincronização de processos no MDX-cc é feita através do mecanismo de barreiras, gerenciado por o Servidor se Sincronização.
São oferecidas ao usuário duas primitivas de sincronização, ilustradas na Figura 5.15.
A primitiva MDX_ BarrierCreate () envia ao Servidor de Sincronização uma requisição de criação de barreira (BarrierCreate), enviando também o Identificador da barreira e o Quorum, que indica o número de processos que sincronizam através da barreira.
O servidor envia como resposta um indicativo de sucesso ou de insucesso na operação.
A outra primitiva, MDX_ Barrier (), envia uma requisição de execução de barreira (Barrier) ao Servidor de Sincronização, enviando também o Identificador da barreira a ser executada.
Em seguida, A primitiva fica bloqueada aguardando a resposta do servidor.
Essa resposta indica que todos os processos já executaram a barreira e podem seguir sua execução.
Primitivas de Comunicação A comunicação entre processos no MDX-cc é feita através de duas primitivas:
MDX_ Send () e MDX_ Recv () (Figura 5.16).
A primitiva MDX_ Send () envia a mensagem contida em msg, de tamanho tam, ao processo id_ dest.
O campo mode indica uma preferência do usuário sobre a tecnologia de rede empregada na comunicação (Fast-Ethernet, Myrinet ou SCI).
As alternativas são:
MDX_ USE_ TCP:
Dar preferência à utilização de rede Fast-Ethernet, com protocolo MDX_ USE_ UDP:
Dar preferência à utilização de rede Fast-Ethernet, com protocolo MDX_ USE_ MYR:
Dar preferência à utilização de rede Myrinet.
MDX_ USE_ SCI:
Dar preferência à utilização de rede SCI.
MDX_ USE_ ANY:
Indica que o usuário não quer dar preferência a nenhuma das tecnologias.
A primitiva MDX_ Recv () recebe uma mensagem de tamanho tam, proveniente do processo id_ from.
A mensagem é armazenada em msg.
O campo mode indica uma preferência do usuário sobre a tecnologia de rede empregada na comunicação.
As alternativas são as mesmas da primitiva MDX_ Send ().
As primitivas de comunicação devem oferecer transparência ao usuário sobre qual a tecnologia de rede é utilizada na comunicação.
Para tanto, deve haver uma análise das alternativas e uma decisão sobre a rede usada.
Este protocolo de comunicação será detalhado no Capítulo 6.
Em esta seção serão apresentados alguns exemplos simples de programas MDX-cc, que mostram a utilização das primitivas apresentadas na seção anterior.
O programa apresentado por o Algoritmo 5.3 é uma aplicação bem simples, onde uma mensagem de hello é enviada do processo zero ao processo com identificador 1.
Inicialmente, é definida uma variável, buffer, de 30 caracteres, que armazenará a mensagem.
A primeira primitiva MDX-cc chamada é a MDX_ Init (), a fim de iniciar o processo e receber, em MDX_ id e MDX_ np, o seu identificador único e o número de processos da aplicação.
Em seguida, é colocada em buffer o conteúdo da mensagem.
O processo com identificador zero envia a mensagem (buffer), com tamanho 30, para o processo com identificador 1, através da primitiva MDX_ Send ().
O processo com identificador 1 chama a primitiva MDX_ Recv (), passando como remetente o processo zero.
A mensagem é armazenada na variável buffer, com 30 caracteres.
Recebida a mensagem, o processo 1 a imprime na tela.
Para finalizar a aplicação, é chamada a primitiva MDX_ Finalize (), que sincroniza os processos e termina a aplicação.
Algoritmo 5.3: Exemplo de helloworld no MDX-cc.
Char buffer;
Em essa aplicação, em todas as chamadas MDX_ Send () e MDX_ Recv () está indicada a preferência do usuário por a utilização de rede Fast-Ethernet, com protocolo TCP, na comunicação, através do parâmetro MDX_ USE_ TCP.
O programa apresentado por o Algoritmo 5.4 é uma aplicação de pipeline, onde uma mensagem é gerada num processo, e essa mensagem passa por todos os outros processos, até ser enviada novamente ao processo que a gerou.
Inicialmente, é definida uma variável, buffer, de 1024 caracteres, que armazenará a mensagem.
A primeira primitiva MDX-cc chamada é a MDX_ Init (), a fim de iniciar o processo e receber, em MDX_ id e MDX_ np, o seu identificador único e o número de processos da aplicação.
Em seguida, é atribuído à variável prox o identificador do processo ao qual a mensagem deverá ser passada.
Uma vez que o processo tem seu identificador, pode ser iniciado o processo de envio da mensagem.
O processo &quot;mestre», com identificador zero, passa a mensagem (buffer), com tamanho 1024, para o processo com identificador 1, através da primitiva MDX_ Send ().
Concluído o envio, o processo &quot;mestre «aguarda, numa chamada MDX_ Recv (), o retorno da mensagem, que será enviada por o último processo, ou seja, com maior identificador.
Algoritmo 5.4: Exemplo de pipeline no MDX-cc.
Char buffer;
Os processos com identificador diferente de zero recebem a mensagem do processo anterior e a repassam ao próximo.
Para isso, é chamada a primitiva MDX_ Recv (), indicando como remetente o processo anterior, ou MDX_ id-1.
A mensagem é armazenada na variável buffer, com 1024 caracteres.
Recebida a mensagem, o processo deve repassar- la ao próximo.
Isso é feito através de uma chamada MDX_ Send (), passando como identificador do processo destino a variável prox, o tamanho da mensagem, 1024, e variável buffer, que armazena a mensagem.
Para finalizar, é chamada a primitiva MDX_ Finalize (), que sincroniza os processos e termina a aplicação.
Em essa aplicação, em todas as chamadas MDX_ Send () e MDX_ Recv () está indicada a preferência do usuário por a utilização de rede Myrinet na comunicação, através do parâmetro O programa apresentado por o Algoritmo 5.5 é uma aplicação de ordenação de vetor, onde um vetor é gerado num processo, e esse vetor sofre uma reordenação em cada um dos outros processos, em sistema de pipeline, até ser enviado, totalmente ordenado, novamente ao processo que a gerou.
O vetor gerado possui np posições, onde np é o número de processos da aplicação.
A primeira primitiva MDX-cc chamada é a MDX_ Init (), a fim de iniciar o processo e receber, em MDX_ id e MDX_ np, o seu identificador único e o número de processos da aplicação.
Em seguida, pode ser iniciado o processo de ordenação.
O processo &quot;mestre», com identificador zero, gera o vetor com números aleatórios, através da chamada gera_ vetor () e passa o vetor gerado para o processo com identificador 1, através da primitiva MDX_ Send ().
Concluído o envio, o processo &quot;mestre «aguarda, numa chamada MDX_ Recv (), o retorno do vetor, que será enviada por o último processo, ou seja, com maior identificador.
A o receber o vetor, este é mostrado na tela através da chamada mostra_ vetor ().
Os processos com identificador diferente de zero recebem o vetor do processo anterior, executam um processo de ordenação, e o repassam ao próximo.
Para isso, é chamada a primitiva MDX_ Recv (), indicando como remetente o processo anterior, ou MDX_ id-1.
O vetor é armazenado na variável buffer.
Recebido o vetor, o processo deve ordenar- lo e passar- lo ao próximo processo.
Isso é feito através de uma chamada à função ordena_ vetor (), seguida de uma chamada à primitiva MDX_ Send (), à qual é passado como identificador do processo destino a expressão% MDX_ np, o tamanho da mensagem, e a variável buffer, que armazena o vetor.
Para finalizar, é chamada a primitiva MDX_ Finalize (), que sincroniza os processos e termina a aplicação.
Em essa aplicação, em todas as chamadas MDX_ Send () e MDX_ Recv () está indicado que o usuário não tem preferência por a utilização de qualquer tecnologia de rede na comunicação.
Essa opção é expressa no parâmetro MDX_ USE_ ANY.
Algoritmo 5.5: Exemplo de ordenação de vetor em paralelo no MDX-cc.
Para compilar programas MDX-cc, deve ser chamado o aplicativo mdxcc.
Esse aplicativo compila o código-fonte nome_ prog.
C, passando os parâmetros e incluindo as bibliotecas necessárias ao compilador gcc para geração do executável de nome_ prog.
Para executar aplicações numa máquina paralela, o MDX-cc oferece um aplicativo, o mdxrun.
Esse aplicativo dispara, no modelo SPMD, np cópias do programa nome_ prog nos nós disponíveis, que se encontram no arquivo de máquinas (Figura 5.2).
O mdxrun dispara os servidores do ambiente conforme configuração feita por o aplicativo mdxconf.
Em seguida, dispara as cópias da aplicação nos nós disponíveis, presentes no arquivo de máquinas.
Após o término da aplicação, os servidores ficam ativos no ambiente, para futuras execuções de programas.
Para terminar a execução dos mesmos, deve ser chamado o aplicativo mdxshutdown.
Esse aplicativo localiza os servidores e envia uma mensagem de encerramento aos mesmos.
Os servidores, ao receberem a mensagem, terminam sua execução.
Para rodar o MDX-cc, alguns requisitos de hardware devem ser observados.
É necessário no mínimo um agregado, e este agregado deve, no mínimo, ser interligado por rede FastEthernet.
Se o agregado utiliza como rede primária Myrinet ou SCI, ele deve utilizar rede secundária Fast-Ethernet.
Esta foi escolhida como tecnologia comum a toda a estrutura por ser amplamente utilizada e de mais baixo custo em relação a as demais tecnologias.
Uma estrutura de cluster de clusters, para o MDX-cc, consiste num grande conjunto de nós interligados por rede Fast-Ethernet, onde existem subconjuntos de nós, que são os agregados, interligados por uma rede rápida.
A Figura 5.20 apresenta uma estrutura de cluster de clusters na visão do MDX-cc.
Essa estrutura é composta por quatro agregados, sendo que dois de eles utilizam rede primária Myrinet, um utiliza SCI e outro usa Fast-Ethernet.
Todos os quatro agregados usam como rede secundária Fast-Ethernet, através de a qual seus nós estão interligados com os nós dos outros clusters.
O MDX-cc usa esses agregados interligados como uma única máquina com 42 nós interligados por rede Fast-Ethernet.
Internamente aos clusters, a comunicação entre processos deve ser otimizada de maneira a utilizar a tecnologia empregada como rede primária do agregado.
O MDX-cc roda sobre sistema operacional Linux e necessita algumas bibliotecas para compilação e execução.
Para clusters com rede Myrinet, o MDX-cc requer que seja instalada a biblioteca GM em todos os nós.
Para agregados baseados em rede SCI, a biblioteca SISCI deve ser instalada em todos os nós.
Comunicação entre Processos Clientes Em a comunicação entre processos clientes no MDX-cc, um problema se torna evidente:
A forma como é tratada a comunicação entre processos que executam em nós de agregados que utilizam como rede primária tecnologias distintas.
Um protocolo de comunicação de decida qual tecnologia deve ser usada na troca de mensagem entre esses clientes se faz necessária.
Em a busca por um melhor desempenho na comunicação, a negociação entre processos que desejam se comunicar foi implementada de maneira a não envolver nenhuma troca de mensagens.
Ambos os processos devem executar um mecanismo de análise que decida através de qual rede a mensagem deve ser enviada ou recebida.
Para poder realizar essa análise, cada processo deve ter acesso a informações sobre aqueles com os quais deseja se comunicar.
A busca dessas informações é detalhada a seguir.
Em o MDX-cc, a análise da comunicação terá como elemento base as informações sobre os processos envolvidos.
Essas informações estão registradas no Servidor de Comunicação (seção 5.4.3.2).
Cada processo deve solicitar ao servidor os dados referentes aos clientes com os quais deseja se comunicar.
Em três momentos distintos essas informações podem ser solicitadas.
Essas três alternativas possuem vantagens e desvantagens e são detalhadas a seguir.
Em essa implementação, as informações sobre o processo com o qual haverá comunicação (processo par) são solicitadas ao Servidor de Comunicação no momento da troca de mensagem, ou seja, em toda chamada MDX_ Send () ou MDX_ Recv ().
Esse modelo apresenta algumas vantagens.
Uma de elas é o fato do processo sempre obter informações atualizadas sobre o par, pois busca as informações sempre que uma comunicação com o mesmo ocorre.
Outra vantagem é que as informações sobre o processo par podem ser descartadas após a comunicação, não sendo necessária nenhuma estrutura de armazenamento.
A principal desvantagem desse modelo é que a comunicação entre um par de processos é prejudicada por a solicitação de informações ao Servidor de Comunicação.
Antes de enviar uma mensagem, o processo deve solicitar ao servidor as informações do processo destino.
O processo destino, para receber a mensagem corretamente, deve obter as informações do processo origem.
Isso implica numa troca de mensagem extra a cada chamada às primitivas de comunicação.
A Figura 6.1 apresenta o modelo descrito.
Primeiro, o Processo 1 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 2, que efetua uma chamada MDX_ Recv () para receber- la.
Em ambas as chamadas, o primeiro passo, antes de enviar ou receber a mensagem, é buscar as informações do processo par no Servidor de Comunicação (busca_ info()).
O servidor envia resposta com as informações solicitadas e, enfim, a mensagem é enviada/ recebida.
Após essa comunicação, o Processo 2 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 1, que efetua uma chamada MDX_ Recv () para receber- la.
Novamente, as informações do processo par são solicitadas ao servidor de comunicação (busca_ info()).
De posse dessas informações, a comunicação é concluída com o envio da mensagem do Processo 2 ao Processo 1.
Em esse modelo, cada processo, ao ser iniciado, solicita ao Servidor de Comunicação as informações sobre todos os outros processos da aplicação.
Essas informações são armazenadas e utilizadas para comunicação no decorrer de a sua execução.
Esse modelo é semelhante ao utilizado por o IMPI (seção 4.2).
A vantagem desse modelo é que as primitivas de comunicação não são prejudicadas por a solicitação de informações, como no modelo anterior.
A o enviar ou receber uma mensagem, o processo busca as informações sobre o processo par numa estrutura de armazenamento em sua memória local, por exemplo, uma tabela de processos.
Esse modelo apresenta algumas desvantagens.
Uma de elas é que cada processo armazena informações sobre todos os outros processos da aplicação.
Desta forma, podem ser guardadas informações de processos com os quais não haverá comunicação.
Outra desvantagem é o fato de todos os processos necessitarem de uma estrutura de armazenamento das informações.
Uma terceira desvantagem pode ser destacada:
Como as informações são solicitadas ao servidor apenas no início de cada processo, podem ser armazenadas informações desatualizadas sobre alguns processos.
A Figura 6.2 apresenta o modelo de busca no início do processo.
A o ser iniciado, em MDX_ Init (), cada processo busca as informações sobre todos os processos da aplicação, e o servidor envia resposta com as informações solicitadas.
Essas são armazenadas por os processos e, nas chamadas às primitivas de comunicação, são usadas para enviar ou receber uma mensagem.
Em a Figura 6.2, o Processo 2 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 1, que efetua uma chamada MDX_ Recv () para receber- la.
Em seguida, o Processo 1 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 2, que efetua uma chamada MDX_ Recv () para receber- la.
Em ambas as comunicações, não foi necessário solicitar ao Servidor de Comunicação as informações do processo par.
Basta enviar ou receber a mensagem.
A terceira alternativa de implementação consiste na busca de informações sobre um determinado processo apenas na primeira comunicação com o mesmo.
Por exemplo, se o processo com identificador 1 deseja enviar uma mensagem ao processo com identificador 15, o primeiro solicita as informações do processo 15 ao Servidor de Comunicação.
Essas informações são mantidas numa estrutura de armazenamento na memória local.
Daí em diante, sempre que o processo 1 desejar comunicar com o processo 15, são utilizadas as informações contidas na memória local, não sendo necessário buscas- las novamente no Servidor de Comunicação.
Uma vantagem desse modelo é que são armazenadas informações apenas sobre os processos com os quais há comunicação.
Outra característica é que apenas a primeira comunicação é afetada por a busca de informações no servidor.
Essa característica pode ser considerada uma vantagem em relação a a primeira alternativa, em a qual todas as comunicações são prejudicadas, enquanto que se torna uma desvantagem em relação a a segunda alternativa, onde nenhuma comunicação é afetada por a busca de informações, que ocorre apenas no início de cada processo.
Podem ser citadas como desvantagens desse modelo o fato de ser necessária uma estrutura de armazenamento das informações e a possibilidade de um processo armazenar informações desatualizadas, pois as mesmas são solicitadas ao servidor apenas na primeira vez que se fazem necessárias.
A Figura 6.3 ilustra o modelo apresentado.
Inicialmente, o Processo 1 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 2, que efetua uma chamada MDX_ Recv () para receber- la.
Como é a primeira vez que há comunicação entre o Processo 1 e o Processo 2, cada um de eles deve buscar as informações do processo par no Servidor de Comunicação (busca_ info()).
O servidor envia resposta com as informações solicitadas e, enfim, a comunicação é concluída.
As informações do processo par são armazenadas.
Após essa primeira comunicação, o Processo 2 faz uma chamada MDX_ Send () para enviar uma mensagem ao Processo 1, que efetua uma chamada MDX_ Recv () para receber- la.
Ambos os processos já possuem as informações um sobre o outro, de forma que basta que a mensagem seja enviada do Processo 2 para o Processo 1.
O mesmo ocorre na comunicação subsequente, onde uma mensagem é enviada do Processo 1 ao Processo 2.
Após uma análise das vantagens e desvantagens de cada uma das alternativas de implementação da busca de informações sobre os processos e das características do MDX-cc, decidiu- se por a implementação da busca de informações apenas na primeira comunicação Em esse modelo, não há armazenamento de dados desnecessários, pois são recuperadas informações apenas sobre os clientes com os quais há comunicação.
Além disso, no ambiente alvo, as informações sobre as tecnologias de rede não são alteradas no decorrer de o processo, eliminando o problema da inconfiabilidade dos dados.
O problema da perda de performance, apontado por o modelo de busca de informações a cada comunicação, é diminuído, uma vez que apenas a primeira comunicação com cada um dos processos é afetada.
Em o MDX-cc, cada processo possui uma estrutura de armazenamento de informações de processos.
Essa estrutura é idêntica à ClientTable, tabela usada por o Servidor de Comunicação para gerenciar as informações dos processos, detalhada na seção 5.4.3.2, e é denominada LocalClientTable.
O preenchimento dessa tabela é feito por as primitivas de comunicação MDX_ Send () e MDX_ Recv ().
O primeiro passo executado por as primitivas de comunicação é localizar as informações do processo par (origem da mensagem, no caso de um MDX_ Recv (), e destino, no caso de um MDX_ Send()) na sua LocalClientTable.
Caso as informações do processo par não se encontrem na tabela, é chamada a função MDX_ LookupClient ().
Essa função envia uma requisição de busca de informações (LookupClient) ao Servidor de Comunicação, passando também o identificador do processo a ser localizado.
O servidor envia resposta com as informações solicitadas e essas são armazenadas na posição i da LocalClientTable, onde i é o identificador do processo par.
Em as próximas vezes que houver comunicação com o processo i, as informações do mesmo já se encontram na tabela local, não sendo necessário solicitar- las ao servidor.
De posse das informações sobre o processo com o qual haverá comunicação, as primitivas podem executar o processo de decisão sobre qual tecnologia de rede será utilizada.
Esse processo é detalhado a seguir.
Em o MDX-cc, a negociação entre processos não envolve troca de mensagens.
Ambos os processos realizam uma análise e decidem através de qual rede ocorrerá a comunicação.
Essa análise se baseia em três itens:
Inicialmente, é feita a verificação de quais as tecnologias de rede estão disponíveis aos dois processos.
De entre essas, é dada a preferência à tecnologia mais rápida e confiável, na seguinte ordem:
SCI, Myrinet, Fast-Ethernet TCP e Fast-Ethernet UDP.
Este último somente é considerado no caso de o tamanho da mensagem ser menor que 65507 bytes.
Em seguida, é adicionada ao processo de análise a preferência do usuário (parâmetro mode das primitivas de comunicação MDX_ Send () e MDX_ Recv()).
No caso de o usuário ter expressado alguma preferência de tecnologia a ser usada na comunicação (parâmetro mode com valor diferente de MDX_ USE_ ANY), é verificado se a mesma se encontra entre as disponíveis aos dois processos.
Em caso afirmativo, essa será a tecnologia usada na comunicação.
Caso contrário, a tecnologia usada será aquela de melhor desempenho e maior confiabilidade disponível.
O exemplo apresentado por a Figura 6.4 mostra a negociação entre dois processos, onde o Processo 1 dispõe de redes SCI e Fast-Ethernet, enquanto o Processo 2 dispõe de redes Myrinet e Fast-Ethernet.
Ambos os processos expressam, nas chamadas MDX_ Send () e MDX_ Recv (), a preferência por a utilização de rede Fast-Ethernet e protocolo UDP.
O primeiro critério, que verifica as tecnologias disponíveis a ambos os processos, deixa como possibilidades as tecnologias Fast-Ethernet/ TCP e Fast-Ethernet/ UDP.
O segundo critério, que dá preferência à tecnologia mais rápida e confiável, eliminou a possibilidade de comunicação por Fast-Ethernet/ UDP, selecionando Fast-Ethernet/ TCP para comunicação.
No entanto, o último critério de análise, que envolve a preferência do usuário, no caso, FastEthernet/ UDP, alterou essa decisão, uma vez que a tecnologia desejada por o usuário se encontra entre aquelas selecionadas por o primeiro critério de análise.
A rede selecionada para comunicação é Fast-Ethernet/ UDP.
Em o exemplo apresentado por a Figura 6.5, ambos os processos dispõe de redes SCI e Fast-Ethernet, e ambos expressam, nas chamadas MDX_ Send () e MDX_ Recv (), a preferência por a utilização de rede Myrinet na comunicação.
O primeiro critério, que verifica as tecnologias disponíveis a ambos os processos, deixa como possibilidades as tecnologias FastEthernet/ TCP, Fast-Ethernet/ UDP e SCI.
O segundo critério, que dá preferência à tecnologia mais rápida e confiável, selecionou a rede SCI para comunicação.
O último critério de análise, que envolve a preferência do usuário, no caso, Myrinet, não altera essa decisão, uma vez que a tecnologia desejada por o usuário não se encontra entre aquelas selecionadas por o primeiro critério de análise.
A rede selecionada para comunicação é SCI.
Se a rede selecionada por os critérios de análise é Fast-Ethernet/ TCP, é verificada a existência de uma conexão TCP entre os processos envolvidos.
Caso essa conexão não exista, a mesma deve ser estabelecida.
Em a primitiva MDX_ Recv (), é feita uma chamada à função MDX_ AcceptConn (), que aguarda uma conexão TCP do processo origem da mensagem.
Em a primitiva MDX_ Send () ocorre uma chamada à função MDX_ RequestConn (), que solicita uma conexão TCP no endereço e porta onde o processo receptor está aguardando.
Este processo de estabelecimento de conexão TCP é realizado apenas uma vez, sendo que a conexão estabelecia será usada nas próximas comunicações entre os dois processos.
Já existindo a conexão, a mensagem pode ser enviada ou recebida.
A primitiva MDX_ Send () envia a mensagem via conexão TCP chamando a função MDX_ RSend ().
De o outro lado, a primitiva MDX_ Recv () recebe a mensagem por uma chamada à função MDX_ RRecv (), função de recebimento de mensagem via conexão TCP.
Caso a tecnologia selecionada seja Fast-Ethernet com protocolo UDP, a mensagem é simplesmente transmitida entre os processos, não sendo necessária o estabelecimento de conexão.
A primitiva MDX_ Send () envia a mensagem para o endereço e porta do processo par, através da função MDX_ SendTo ().
De o lado do receptor, a primitiva MDX_ Recv () chama a função MDX_ RecvFrom (), que recebe uma mensagem por a rede Fast-Ethernet e protocolo A Figura 6.7 mostra a comunicação entre dois processos através de tecnologia FastEthernet com protocolo UDP.
De a mesma forma que na comunicação por TCP, na primeira comunicação, antes de enviar/ receber a mensagem, ambos os processos devem solicitar as informações do processo par no Servidor de Comunicação.
Em seguida, a mensagem é transmitida.
Em as próximas comunicações, a busca de informações não é necessária.
Basta enviar/ receber a mensagem.
A comunicação via rede Myrinet acontece através das portas de comunicação GM, criadas no início de cada processo.
A primitiva MDX_ Send () envia, através da sua porta, uma mensagem à porta criada por o processo par.
Isso é feito por a função MDX_ GM_ Send ().
Essa função efetua uma cópia da mensagem para uma área de memória DMA, utilizada por a biblioteca GM.
O processo receptor consome a mensagem da sua porta de comunicação através de uma chamada à função MDX_ GM_ Recv ().
Essa função copia a mensagem, recebida numa área de memória DMA, para a área de memória passada em MDX_ Recv ().
Essas cópias de mensagem prejudicam o desempenho da comunicação.
A Figura 6.8 mostra a comunicação entre dois processos através de tecnologia Myrinet.
Em a primeira comunicação, antes de enviar/ receber a mensagem, é realizada a busca de informações do processo par no Servidor de Comunicação.
Em seguida, a mensagem é transmitida.
Em as próximas comunicações, a busca de informações não mais é necessária.
Via rede SCI, a comunicação é feita através da biblioteca YAMPI.
A primitiva MDX_ Send () efetua uma chamada à função MDX_ YAMPI_ Send (), que envia uma mensagem de um processo YAMPI a outro.
De o outro lado, a primitiva MDX_ Recv () recebe essa mensagem chamando a função MDX_ YAMPI_ Recv ().
A Figura 6.9 mostra a comunicação entre dois processos através de tecnologia SCI.
Em a primeira comunicação, antes de enviar/ receber a mensagem, é feita a busca de informações do processo par no Servidor de Comunicação.
Em seguida, a mensagem é transmitida.
Em as próximas comunicações, a busca de informações não mais é necessária.
Basta enviar/ receber a mensagem.
Avaliação de Desempenho do MDX-cc O desempenho de uma aplicação que é executada numa plataforma paralela é um dos pontos principais a serem avaliados por uma equipe de desenvolvimento.
Embora predizer a performance que será obtida por uma aplicação num ambiente paralelo seja uma tarefa difícil, uma boa avaliação de desempenho do sistema de comunicação sobre o qual a mesma está baseada é de grande ajuda.
Este capítulo apresenta os testes de desempenho realizados com o MDX-cc.
O ambiente de teste utilizado para avaliar a performance do MDX-cc é o CPAD (Centro de Processamento de Alto Desempenho).
Este centro é o resultado de uma parceria entre a Pontifícia Universidade Católica do Rio Grande do Sul e a Hp do Brasil.
A Hp possui atividades conjuntas com universidades e centros de pesquisa reconhecidos, objetivando a continuidade de um amplo processo de cooperação nacional através de parcerias.
O CPAD se propõe a ser um Centro de Pesquisa dedicado a investigar arquiteturas de software e hardware para processamento de alto desempenho.
Para este fim o CPAD conta atualmente com quatro máquinas agregadas onde servidores Hp são interligados através de redes de alta velocidade.
Esses agregados são:
Amazonia Agregado com 16 nós Hp-E60 com dois processadores Pentium III 550 MHz, com 256 MB de RAM e HD de 20 GB (sem monitor e sem teclado).
Esses nós utilizam rede primária Myrinet e rede secundária Fast-Ethernet, e rodam sistema operacional Linux.
Tropical Agregado com 8 nós Hp-E-PC com dois processadores Pentium III 1 GHz, com 256 MB de RAM e HD de 20 GB (sem monitor e sem teclado).
Esses nós possuem rede Fast-Ethernet e rodam sistema operacional Linux.
Pantanal Cluster com 4 nós Hp-Vectra VEI8 com um processador Pentium III 550 MHz, com 256 MB de RAM e HD de 9 GB (sem monitor e sem teclado).
Esses nós rodam com sistema operacional Linux, possuem rede primária SCI e rede secundária Fast-Ethernet.
Ombrofila Agregado com 16 nós Hp-E-PC com um processador Pentium III 1 GHz, com 256 MB de RAM e HD de 20 GB (sem monitor e sem teclado).
Esses nós possuem rede Fast-Ethernet e rodam sistema operacional Linux.
A interligação dos nós desses agregados através de rede Fast-Ethernet forma a estrutura de cluster de clusters exigida por o MDX-cc (seção 5.5).
Sobre essa estrutura foram realizados os testes de desempenho do ambiente.
As características e limitações de um sistema de troca de mensagens têm diferentes impactos sobre diferentes aplicações.
Tendo isto em vista, é fundamental a comparação entre diferentes métodos para que se possa escolher o mais adequado para uma determinada aplicação.
A maioria das medidas de performance em sistemas de comunicação é dada em termos de dois parâmetros:
Latência e largura de banda (vazão), referenciados neste trabalho como L e B (bandwidth) respectivamente.
O primeiro diz respeito à semântica de sincronismo de uma troca de mensagens e o segundo à semântica de transferência de dados.
O propósito da avaliação da latência é caracterizar a velocidade em que o sistema de comunicação consegue sincronizar, através de troca de mensagens, dois processos cooperativos. Quanto mais
rápido o sistema é capaz de executar esta tarefa, melhor será a performance de aplicações altamente síncronas.
Por o descrito acima é possível definir latência como o tempo necessário para se enviar uma mensagem da origem ao destino, ou seja, do instante no processo origem inicia uma operação de envio até o instante em que o processo destino é notificado sobre o recebimento desta mensagem.
Os processos &quot;origem «e &quot;destino «são aplicações executadas em diferentes nós de um agregado ou de uma estrutura de cluster de clusters.
A maneira mais comum de se computar a latência é por o uso de uma avaliação do tipo ping-pong.
Em esta avaliação são utilizados dois processos, chamados aqui de &quot;ping «e &quot;pong».
O processo ping envia uma mensagem para o processo pong e espera a resposta, sendo que esta é apenas uma cópia da mensagem recebida por o processo pong.
O tempo gasto na execução desta transação é medida por o processo ping e consiste no tempo gasto desde a transmissão da mensagem até a sua recepção.
Esta medida caracteriza o quão rápida é a transferência de dados ocorre entre o transmissor e o receptor.
Para esta medida também foram considerados dois processos, chamados também de &quot;ping «e &quot;pong».
Se o tempo t necessário para se transferir um bloco de dados com S bytes, medido a partir de o instante que o processo ping inicia a operação de envio até o recebimento da mensagem por pong, então o valor da largura de banda B, será calculado segundo a equação mostrada por a Figura 7.2.
Qualquer medida de largura de banda requer transferência de grandes quantidades de dados, para isto foram considerados três métodos para obter esta medida:
Single-message: Envio de uma mensagem muito longa.
Esta técnica é a mais simples mas só é adequada quando o sistema troca de mensagens permite o uso de mensagens muito grandes.
Stream: Envio de uma única, e longa, mensagem.
Esta técnica é possível em sistemas de comunicação que suportam transferência de dados em modo stream, como sockets Burst:
Envio de uma rápida seqüência de mensagens de tamanho fixo.
Esta técnica é utilizada quando o tamanho máximo mensagem permitido por o sistema de troca de mensagens não é grande o suficiente.
Esta técnica não é útil quando se necessita de uma avaliação do comportamento de um sistema de mensagens segundo o tamanho da mensagem (per-message).
Em este caso deve- se emular um mensagem grande por a fragmentação da mesma numa longa seqüência de mensagens de tamanho fixo, que devem utilizadas para se reconstruir a mensagem original no receptor.
Para se ter uma avaliação mais correta, os processos de fragmentação e reconstrução da mensagem devem ser incluídos no cálculo da largura de banda.
É interessante se fazer uma avaliação per-message de um sistema de mensagens pois desta maneira pode- se avaliar a eficiência do uso das características desta interface.
Sendo assim, para este trabalho, adotou- se a técnica de single-message pois o tamanho de mensagens permitido no sistema é suficiente para realização dos testes.
Para a medição da largura de banda, foi utilizada a mesma técnica ping-pong descrita na seção 7.2.1 para medir o intervalo entre o envio da mensagem e sua recepção (round-trip).
O processo ping envia uma mensagem com S bytes de tamanho para o processo pong, que deve enviar os mesmos dados de volta.
O tempo de round-trip (designado aqui como tRT) necessário para se executar as duas operações é medido.
O cálculo da largura de banda média para um tamanho de mensagem é dada como a média aritmética das larguras de bandas obtidas, como mostra a Figura 7.4 O processo descrito é repetido 500 vezes para cada tamanho de mensagem entre 1 byte e 1 Mbyte.
O tempo necessário para se enviar uma mensagem de tamanho S é calculado como a metade de tRT, e a vazão para cada repetição é dada por a equação mostrada na Figura 7.3.
O Algoritmo 7.1 apresenta o código da aplicação de teste de latência e largura de banda:
Algoritmo 7.1: Programa de avaliação do MDX-cc.
O primeiro teste apresentado será o da comunicação sobre rede Fast-Ethernet.
A avaliação foi feita com a aplicação ping-pong executando em dois nós do cluster Tropical.
Em esta seção são mostrados os resultados obtidos com a utilização dos protocolos TCP e UDP.
Am ambos os casos é feita uma comparação com os resultados obtidos por sockets.
Rodando sobre rede Fast-Ethernet, com a utilização do protocolo TCP, a aplicação de avaliação do MDX-cc obteve os seguintes resultados:
Figura 7.5: Gráfico de latência do MDX-cc sobre Fast-Ethernet protocolo TCP.
Os resultados apresentados por o teste de latência (Figura 7.5) indicam um desempenho bastante satisfatório, próximos ao desempenho obtido por sockets TCP puro.
Isto indica que o MDX-cc não inclui um grande overhead na comunicação.
A troca da primeira mensagem, de tamanho 1 byte, apresenta uma latência mais alta devido a a necessidade da busca das informações do processo par no Servidor de Comunicação (seção 6.2).
As demais trocas de mensagem apresentam um melhor desempenho, pois cada processo já tem a sua disposição as informações do processo par, não sendo necessária a busca das mesmas no Servidor de Comunicação.
A partir de a segunda troca de mensagem, a latência da comunicação aumenta conforme há o aumento do tamanho da mensagem.
Os valores apresentados por o teste de latência se refletem nos resultados do teste de vazão, apresentados por a Figura 7.6.
O gráfico mostra uma proximidade entre os valores obtidos por o MDX e por sockets.
Em a troca da primeira mensagem, de 1 byte, a vazão é bastante baixa.
Para as outras trocas de mensagem, os valores vão ficando maiores à medida que o tamanho da mensagem também aumenta.
Rodando sobre rede Fast-Ethernet, com a utilização do protocolo UDP, o MDX-cc obteve os resultados mostrados na Tabela 7.2.
Os resultados apresentados por o teste de latência do MDX-cc sobre Fast-Ethernet (Figura 7.7) com protocolo UDP indicam um desempenho razoável, mas um pouco distante do desempenho obtido por o uso de sockets UDP.
Essa diferença pode estar na forma como é montada a estrutura (socket) com os dados do socket destino.
Em o MDX-cc, essa estrutura é montada em toda comunicação.
Isto acontece na função MDX_ SendTo () (seção 6.2.2), de envio de mensagens por UDP.
Já na aplicação utilizada para testar o uso de sockets puro, essa estrutura é montada uma única vez, sendo utilizada a mesma para todas as trocas de mensagem.
Os testes do protocolo UDP foram feitos apenas com mensagens menores que 65507 bytes, tamanho máximo suportado por esse protocolo no MDX-cc.
De a mesma forma que no teste de latência apresentado anteriormente, a troca da primeira mensagem, de tamanho 1 byte, apresenta uma latência alta e as demais trocas de mensagem apresentam um melhor desempenho.
Para mensagens maiores, a latência da comunicação aumenta conforme há o aumento do tamanho da mensagem.
Figura 7.7: Gráfico de latência do MDX-cc sobre Fast-Ethernet protocolo UDP.
Os resultados do teste de vazão, apresentados por a Figura 7.8, são conseqüência dos números do teste de latência.
Os números obtidos são mais baixos em relação a os obtidos por sockets.
Em a troca da primeira mensagem, de 1 byte, a vazão é bastante baixa.
Em as outras trocas de mensagens, os valores vão ficando maiores à medida que o tamanho da mensagem também aumenta.
De a mesma forma que no teste de latência, há uma leve vantagem do protocolo UDP para mensagens de até 4 Kbytes.
A avaliação do MDX-cc rodando sobre rede Myrinet foi feita com a aplicação ping-pong rodando em dois nós do cluster Amazonia.
Esta seção apresenta e comenta os resultados obtidos.
Esses resultados são comparados com os obtidos por a ferramenta MPI.
A Tabela 7.3 mostra os valores obtidos por a aplicação de teste do MDX-cc sobre rede Myrinet:
Os resultados apresentados por o teste de latência do MDX-cc sobre Myrinet (Figura desempenho obtido por MPI.
Esse resultado é reflexo das cópias de mensagem de e para áreas de memória DMA realizadas por as funções MDX_ GM_ Send () e MDX_ GM_ Recv () (seção mensagem apresenta um valor elevado e as demais trocas de mensagem apresentam um melhor desempenho.
Para mensagens maiores, a latência da comunicação aumenta conforme há o aumento do tamanho da mensagem.
Os resultados do teste de vazão, apresentados por a Figura 7.10, mostram o mesmo comportamento apresentado por o teste de latência.
Em a troca da primeira mensagem, de 1 byte, a vazão é bastante baixa, enquanto que, para mensagens maiores, os valores vão aumentando à medida que o tamanho da mensagem também aumenta.
Importante ressaltar a capacidade do hardware Myrinet.
Apesar de os números apresentados por o teste estarem abaixo de o esperado, o desempenho ainda é muito superior àquele obtido por o ambiente executando sobre rede Fast-Ethernet, tanto por protocolo UDP quanto por protocolo TCP.
Os testes do MDX-cc sobre rede SCI foram realizados em dois nós do cluster Pantanal.
Os resultados obtidos na avaliação são apresentados e comentados a seguir.
A Tabela 7.4 mostra os valores obtidos por a aplicação de teste do MDX-cc sobre rede Myrinet, juntamente com aqueles obtidos por o MPI.
Os resultados apresentados por o teste de latência do MDX-cc sobre SCI indicam um desempenho dentro de o esperado, próximo, mas ainda abaixo de o obtido por a biblioteca MPI.
Para mensagens com tamanho entre 128 bytes e 32 Kbytes o MDX-cc obteve até um desempenho melhor.
Entretanto, para mensagens maiores, o MPI apresenta um melhor desempenho.
A Figura 7.12 apresenta o gráfico do teste de vazão do MDX-cc sobre rede SCI.
Esse gráfico é um reflexo do comportamento apresentado por o teste de latência.
O MDX-cc obtém uma maior vazão para mensagens entre 128 bytes e 32 Kbytes.
Em a troca da primeira mensagem a vazão é bastante baixa, enquanto que, para mensagens maiores, os valores vão aumentando à medida que o tamanho da mensagem também aumenta.
A comunicação através de tecnologia SCI foi a que apresentou o melhor desempenho no MDX-cc.
Os gráficos apresentados por a Figura 7.11 e por a Figura 7.12 mostram uma menor latência e, consequentemente, maior vazão da comunicação sobre SCI em relação a a comunicação sobre as tecnologias Fast-Ethernet TCP, Fast-Ethernet UDP e Myrinet.
Para avaliar o comportamento do MDX-cc sobre uma estrutura de cluster de clusters, com uma aplicação envolvendo todos os agregados disponíveis, foi implementado um protótipo baseado na aplicação descrita por o Algoritmo 5.4.
Esta aplicação passa uma mensagem entre os processos, formando um anel, onde a mensagem é enviada por o processo mestre, com identificador 0, para o processo 1, este repassa a mensagem para o processo 2, que repassa ao processo 3, e assim sucessivamente, até que a mensagem chegue ao processo com maior identificador.
Este processo devolve a mensagem ao processo mestre.
Esta aplicação não avalia o desempenho do MDX-cc, mas sim o funcionamento da ferramenta sobre uma estrutura heterogênea, avaliando o comportamento do protocolo de comunicação implementado.
Para a execução da aplicação, foram alocados quatro nós do cluster Amazonia, quatro nós do agregado Tropical, dois nós do cluster Ombrofila e dois nós do cluster Pantanal.
Não foram utilizados todos os nós de todas as máquinas a fim de facilitar a ilustração da execução.
O arquivo de nomes de máquinas ficou configurado conforme mostra a Figura 7.13.
Disparada a aplicação, com 12 processos, estes ficaram distribuídos da seguinte maneira:
A troca de mensagens entre cada par de processos ocorreu através da tecnologia selecionada por o protocolo de análise apresentado na seção 6.2.2.
Não foi passada preferência nenhuma nas primitivas de comunicação.
Com base na rede utilizada em cada troca de mensagem, pode- se montar o gráfico apresentado por a Figura 7.15.
A comunicação entre os processos 0 e 1, e 1 e 2, se dá através de rede Myrinet, uma vez que ambos os nós dispõem dessa tecnologia.
Os processos 2 e 3 se encontram em nós de agregados diferentes (Amazonia e Tropical), onde o agregado Amazonia dispõe de redes Myrinet e Fast-Ethernet, e o Tropical dispõe somente de rede Fast-Ethernet.
Desta forma, esse processos se comunicam por Fast-Ethernet/ TCP.
O mesmo ocorre para a comunicação entre os processos 8 e 9.
A comunicação entre os processos 3 e 4 se dá através da única tecnologia disponível, que é Fast-Ethernet, por protocolo TCP.
Assim como o cluster Tropical, o Ombrofila também utiliza somente rede FastEthernet.
Consequentemente, a comunicação entre os processos 4 e 5, 5 e 6, 6 e 7, e 7 e 8 ocorre através dessa tecnologia.
Os processos 9 e 10 estão localizados em nós de clusters diferentes, Amazonia e Pantanal.
O cluster Pantanal utiliza redes SCI e Fast-Ethernet, enquanto o agregado Amazonia dispõe de redes Myrinet e Fast-Ethernet.
A comunicação entre os dois processos se dá através da tecnologia Fast-Ethernet/ TCP, comum a ambos os agregados.
O mesmo ocorre para a troca de mensagem entre os processos 11 e 0.
Entre os processos 10 e 11, ambos localizados no cluster Pantanal, se dá através de rede SCI.
A execução desta aplicação mostra que o MDX-cc facilita a utilização de uma estrutura de cluster de clusters, uma vez que gerência a comunicação entre processos localizados em agregados diferentes.
Além disso, torna mais fácil a programação, pois oferece um protocolo de comunicação que decide através de qual tecnologia de rede acontece a troca de mensagens Desta forma, o usuário não se preocupa com a localização dos processos nos nós da estrutura.
Conclusão Em os dias de hoje, é crescente a busca por alto desempenho na execução de aplicações, principalmente nas áreas da ciência e tecnologia.
Em esse contexto, os clusters surgem como alternativa viável na construção de máquinas com maior poder computacional.
Em função de o surgimento de redes de comunicação extremamente rápidas, surge a possibilidade da interligação de agregados, formando uma estrutura de cluster de clusters.
A interligação de clusters visa melhorar o desempenho na execução de aplicações paralelas, através do aumento do número de máquinas interligadas.
Um dos principais problemas no uso de cluster de clusters é o software utilizado para o desenvolvimento de aplicações paralelas, visto que cada agregado envolvido na estrutura possui certas características que precisam ser tratadas por a linguagem ou ambiente de programação, visando o alcance de alto desempenho.
Esta dissertação apresentou um ambiente de programação paralela aplicado a cluster de clusters, o MDX-cc.
Foram apresentados detalhes de projeto e de implementação da ferramenta, as primitivas oferecidas por o ambiente e exemplos de programas.
Também foram apresentados os testes de desempenho realizados com o MDX-cc, e os números obtidos nos testes foram interpretados.
Alguns problemas surgiram no decorrer de o trabalho.
A necessidade de modificações na estrutura do MDX original pode ser citada como o principal de eles.
Outros problemas que podem ser citados são os encontrados na documentação e na utilização da biblioteca GM, além de a necessidade de correção de alguns bugs da biblioteca YAMPI.
A implementação da comunicação sobre rede Myrinet apresentou alguns obstáculos.
A biblioteca GM, utilizada por o MDX-cc, apresenta um modelo de programação complicado, com a utilização de tokens de acesso à rede e de eventos de recebimento de mensagens.
A documentação disponível sobre a biblioteca é bastante incompleta, o que dificultou a implementação das funções de comunicação do MDX-cc via rede Myrinet.
A biblioteca YAMPI, por o fato de estar em fase inicial de desenvolvimento, apresentou alguns problemas na troca de mensagens longas.
Este fato atrasou um pouco o desenvolvimento das funções de comunicação via rede SCI.
Após a correção desses bugs, a implementação do MDX-cc seguiu seu curso normal.
Apesar de as dificuldades citadas, o trabalho obteve resultados interessantes.
Esses resultados são apresentados a seguir.
Pode- se dizer que o trabalho atingiu seu objetivo, uma vez que o resultado do mesmo é uma primeira versão do MDX-cc já implementada e funcionando de forma estável.
O principal objetivo da ferramenta proposta, que era de oferecer ao programador transparência total na comunicação entre processos, independentemente das tecnologias de rede disponíveis a cada um de eles, foi alcançado.
Em o MDX-cc, o usuário faz a troca de mensagens entre processos através de chamadas MDX_ Send () e MDX_ Recv ().
Essas primitivas executam o protocolo que decide através de qual tecnologia a mensagem será enviada ou recebida.
Desta forma, o usuário não precisa se preocupar com a localização dos processos de uma aplicação paralela e com as tecnologias de rede disponíveis a cada um de eles.
Os testes de desempenho mostraram que o MDX-cc apresenta um desempenho satisfatório para comunicação por rede Fast-Ethernet (seção 7.3) e por rede SCI (seção 7.5).
Em a comunicação por redes Myrinet, o MDX-cc apresentou uma performance abaixo de a esperada (seção 7.4).
Esse fato se deve às cópias de mensagem realizadas por as funções MDX_ GM_ Send () e MDX_ GM_ Recv ().
Em todos os testes de desempenho, a latência da primeira troca de mensagem é elevada.
Esse fato era esperado e ocorre devido a o mecanismo de negociação entre os processos, que necessita buscar as informações do processo par no Servidor de Comunicação Finalmente, pode- se concluir que o MDX-cc é uma ferramenta interessante no auxílio à programação paralela sobre cluster de clusters.
Esse trabalho representou apenas o primeiro passo no desenvolvimento do ambiente, com o projeto e a implementação de uma primeira versão do mesmo.
Com base nos resultados dessa dissertação, possibilidades de trabalhos futuros aparecem e são apresentadas a seguir.
Apesar de ter obtido um resultado satisfatório, esta dissertação pode sugerir alguns trabalhos a serem desenvolvidos no futuro.
Os serviços propostos, mas não implementados nesse trabalho, podem ser citados como exemplo.
Pode ser sugerido o desenvolvimento de um serviço de memória compartilhada entre processos.
Esse serviço pode utilizar, para processos localizados num mesmo agregado com rede SCI, os mecanismos de memória compartilhada oferecidos por o hardware SCI.
Para processos executando em nós com outras tecnologias, um modelo de memória virtual distribuída pode ser implementado, como, por exemplo, o modelo Linda.
A fim de alcançar um maior desempenho para as aplicações MDX-cc, um mecanismo de balanceamento de carga pode ser estudado e desenvolvido.
Esse mecanismo, além de evitar a sobrecarga de nós, pode levar em consideração o hardware de cada um dos agregados da estrutura, alocando processos com maior número de chamadas a primitivas de comunicação em agregados com rede mais rápida.
A fim de tornar a comunicação no MDX-cc mais completa, podem ser estudados mecanismos de comunicação em grupo.
Podem ser desenvolvidas primitivas de criação de grupos de processos e de envio de mensagens a determinado grupo de processos (ou a todos os processos da aplicação).
Além disso, uma primitiva de recebimento genérica, ou seja, de mensagens vindas de qualquer outro processo, também pode ser implementada.
Além desses trabalhos de implementação dos serviços já propostos nesse trabalho, podem ser sugeridos também trabalhos de otimização do MDX-cc.
O estudo de outro mecanismo de negociação entre processos e a implementação de comunicação por rede Myrinet sem cópias de mensagem não podem ser descartados e tornariam o MDX-cc mais completo e interessante.
