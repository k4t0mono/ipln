Este trabalho apresenta um estudo exploratório da Indexação Semântica Latente (Latent Semantic Indexing ou LSI) e do rendimento que têm as funções peso aplicadas a este método.
O LSI permite encontrar uma estrutura semântica associada a uma coleção de documentos.
Um fator importante que influência nos resultados obtidos por uma consulta são as funções peso.
É por meio de estas funções que se consegue distinguir um documento dentro de toda a coleção, pois a função peso associa uma relevância para cada termo de índice.
Para esta pesquisa foram escolhidas as funções peso Ltu, Okapi e atc, as mais utilizadas nas pesquisas de recuperação de informação.
Um estudo de caso foi realizado com a coleção de documentos MEDLINE através da implementação de um protótipo.
Os resultados obtidos mostram que uma boa recuperação vai depender da função peso e também das dimensões (nível de k) escolhidas para a matriz de representação.
Os sistemas de recuperação de informação (Ri) ganharam importância quando surgiram as bibliotecas digitais:
A necessidade de encontrar informação específica a perguntas em geral booleanas era o objetivo destes sistemas de Ri.
Com a chegada da Internet estes sistemas se tornaram mais populares e passaram a exigir mais recursos, já que a informação disponibilizada versara era mais sobre uma área específica de informação num dado formato, mas sim sobre diversos assuntos e em diferentes formatos (textos, imagens, vídeo, etc) Esta informação, que é disponibilizada por meio de Web sites, blogs, etc. cresceu de forma inesperada e com ela cresceu também a necessidade de encontrar informação específica na Internet.
Devido a a variedade de formatos de informação existentes, utilizam- se distintos formatos para mostrar a informação disponível (por exemplo:
Html1, páginas dinâmicas, rss2, etc) Esta heterogeneidade de recursos e de informação disponível levou os sistemas de Ri, conhecidos comumente como ferramentas de busca, a novos desafios:
Indexar a maior quantidade de páginas na Web e fornecer respostas às consultas feitas ao sistema por os usuários.
As tecnologias para mostrar a informação, apoiadas na criação de novos conteúdos, facilitaram também o desenvolvimento de sítios Web sem standards e possibilitaram o uso do spam3 para melhorar seu posicionamento nas diversas ferramentas de busca, o que leva a uma resposta que em geral não satisfaz a consulta do usuário.
Por a diversidade da informação on-line e dos domínios existentes na Web, entre outros fatores, as ferramentas de busca tiveram de melhorar seus algoritmos de indexação, busca e Ri, pois a necessidade do usuário cresceu e, com ela, a concorrência entre as diversas ferramentas de busca.
Esta necessidade levou a pesquisar- se áreas como clustering, indexação de diversos formatos de arquivos, interação humano-computador e semântica da informação, entre as mais destacadas.
Em o presente trabalho de pesquisa nosso interesse é trabalhar com o conteúdo da informação, especificamente com a semântica da informação, considerada na atualidade como sendo uma das áreas que pode levar a satisfazer as exigências do usuário já que, quando se faz uma consulta, deseja- se obter uma resposta, a mais exata possível, para a consulta que se formulou.
É nesse sentido que o método Latent Semantic Indexing (LSI) trabalha, ajudando a encontrar uma relação semântica entre uma consulta e os termos indexados.
Desta forma, embora as palavras não sejam iguais na relação, diminui a informação redundante.
Este método é relativamente novo e visa melhorar os resultados, sobretudo, trabalhando com problemas tais como sinonímia e polissemia.
A empresa GoogleTM demonstrou interesse por este método, incorporando o LSI em seu sistema de publicidade AdSenseTM 4.
Outra forma de melhorar o processo de Ri na obtenção de uma resposta que satisfaça ao usuário é dotar o sistema de Ri de critérios de discriminação entre os termos indexados, isto é, destacar quais são os termos mais relevantes dentro de a coleção.
Estes critérios de discriminação são chamados de &quot;funções peso».
Estas funções determinam um valor numérico, chamado &quot;peso», para cada termo de índice da coleção de tal forma que, na realização de uma consulta, a relevância entre a consulta e os termos de índice vai ser determinada por os termos que possuam maior peso.
Em geral estes pesos vão depender do objetivo do sistema de Ri e da coleção que está sendo utilizada.
Existem diversas pesquisas sobre estas funções;
Entre elas, uma das propostas mais estudadas é a do sistema OKAPI.
É nesta linha que o presente trabalho de pesquisa aborda as duas áreas descritas anteriormente:
O estudo do método LSI e o rendimento das funções peso aplicadas a este método.
Quando se trabalha com uma coleção contendo uma quantidade abrangente de documentos, existe a dificuldade de encontrar, ou recuperar, documentos que sejam relevantes a uma necessidade expressa numa consulta.
Essa necessidade é suprida por as ferramentas de busca, que recuperam documentos a partir de uma consulta fornecida por um usuário, mas os documentos recuperados, em geral, são documentos que não satisfazem plenamente à consulta do usuário, por os seguintes motivos:
O usuário exige cada vez mais das ferramentas de busca, com o menor esforço possível, sobretudo quanto a os resultados obtidos para a consulta que forneceu, tanto no tempo de resposta, quanto na exatidão (precisão) do rankeamento5.
Em resumo, o usuário busca ferramentas inteligentes que possam &quot;entender «sua consulta e bem resolver- la.
Em o âmbito da Web, a falta de padronização na criação de páginas por parte de os programadores, webmasters e outros, e a sobrecarga de &quot;informação relevante «(uso de técnicas de spam e técnicas de posicionamento SEO6) fazem que se percam diferenças no conteúdo dos documentos.
A maioria das ferramentas de busca recuperam documentos por similaridade entre as palavras do documento e a consulta fornecida.
Observa- se que os três motivos anteriores estão relacionados a um único problema central:
A falta de alguma análise semântica no tratamento da informação.
Por rankeamento entenda- se a classificação dos documentos mostrados como resposta.
Os Seos (do inglês Search Engine Optimizator) são pessoas ou entidades que se dedicam à otimização de sítios Web.
Por otimização entende- se o processo desenvolvido nos Web sites para que estes sejam posicionados nos primeiros lugares nas ferramentas de busca para algumas palavras-chave.
Embora o método LSI seja relativamente novo, existem pesquisas sobre este método aplicadas a distintas áreas.
Em esta seção são elencadas as publicações referentes a LSI mais relevantes para esta pesquisa.
Apresentam- se também os trabalhos de pesquisa referentes às funções peso, que são amplamente aplicadas em diversas áreas.
Dado que o LSI é um tema relativamente novo, ainda existem poucas referências a esse tema.
A seguir relacionam- se as referências consideradas como relevantes.
Cross- language:
Em e os autores mostram como o LSI auxilia na tradução (representação) de documentos multilingües.
Tal trabalho tem como resultado gráficos de comparações, entre as consultas feitas num idioma e documentos similares recuperados em outros idiomas, onde destaca- se que a consulta fornecida não precisa ser traduzida para recuperar os documentos.
O trabalho não contribui para a presente pesquisa, já que o foco é o Cross- language Information Retrieval.
Autotutor: É um sistema tutor inteligente para a Web, auspiciado por a Universidade de Memphis e desenvolvido por uma equipe interdisciplinar.
Este sistema conta com vários módulos, entre eles o LSI, e seu objetivo é utilizar agentes inteligentes, mediante uma conversa em linguagem natural, para ensino aos estudantes sobre um tema em particular.
A forma de avaliar se os estudantes respondem corretamente faz uso do LSI:
Os pesquisadores fazem uma comparação entre as respostas esperadas (consideradas como &quot;boas&quot;) e as respostas fornecidas por os usuários.
Devido a o grau de abrangência que uma resposta pode ter, ao utilizar o LSI chega- se a avaliar de melhor forma esta resposta, já que é considerada a semelhança que existe entre a resposta do usuário e a resposta do sistema.
O foco do trabalho volta- se a um sistema tutor.
A relação que o mesmo tem com Ri é muito reduzida e pouco contribui ao trabalho de pesquisa realizado.
Michael W Berry Descrevem em e o LSI aplicado à Ri mostrando vantagens, relacionadas à sinonímia e à polissemia, para citar algumas.
Em particular, os autores Michael W. Berry e Susan T Dumais, são os que deram início às pesquisas sobre o LSI e possuem diversas publicações aplicando o LSI à Ri.
Em o decorrer deste documento, utilizam- se tais trabalhos como base teórica para a pesquisa realizada.
Descrevendo o estado-da-arte, encontram- se os seguintes trabalhos de pesquisa relacionados às funções peso:
PageRankTM: O conhecido algoritmo de rankeamento do GoogleTM, que serve para rankear páginas segundo a popularidade (determinada por este algoritmo) que a página possua, foi fruto de um estudo bastante completo voltado ao rankeamento de páginas Web, considerando variáveis tais como links de ingresso e saída de uma página.
Diversas pesquisas foram desenvolvidas em torno deste PageRankTM.
Entre estas encontrase o trabalho de Taher H. Haveliwala e Sepandar D. Kamvar, que mostra como detectar o spam de links e calcular de forma mais rápida o PageRankTM, entre outros aspectos.
Este trabalho contribui à presente pesquisa oferecendo uma visão da variabilidade que as funções peso podem ter:
Distintas variáveis que fazem parte da função vão depender do objetivo que o sistema de Ri queira atingir.
Em, os autores abordam as funções peso clássicas tais como a norma e função booleana.
Os autores não apresentam um estudo intenso sobre este tema, apenas fornecem uma conceitualização inicial do comportamento destas funções.
Este livro dá a base inicial da pesquisa para descrever as funções peso.
O autor Stephen Robertson parece ser o pesquisador mais destacado neste tema.
Diversas pesquisas sobre as funções peso levaram- no, com sua equipe, a criar a função peso Okapi, uma das funções mais utilizadas em Ri.
Em é relatada a utilização dessa função, junto com outras três, para mostrar uma forma alternativa de avaliar os documentos recuperados, sem utilizar abrangência e precisão.
Em mostra- se como aplicar a função peso BM25, uma das variações da função Okapi, em documentos estruturados em Html ou XML.
Tal artigo faz uma ponderação dos documentos em si, e das tags existentes nos mesmos.
A presente dissertação utiliza os resultados das diversas pesquisas desenvolvidas por Stephen Robertson.
Cross- language.
Em é apresentado um estudo dos pesos na tradução de idiomas, especificamente na recuperação de documentos em inglês traduzidos ao árabe.
Os pesquisadores utilizam como função peso uma variação da função inverse document frequency.
As contribuições trazidas por esta pesquisa são empregadas no presente trabalho, no sentido de fazer modificações nas funções peso para obter uma melhor recuperação.
Embora o método LSI seja testado em coleções com uma quantidade de documentos relativamente grande, não foi aplicado (pelo menos, não foi encontrado registro de aplicação na revisão bibliográfica realizada até esta data) em sistemas de recuperação com um volume de informação considerável, associado a funções peso distintas, para fins de estudo.
A única referência de aplicação encontrada foi a da empresa GoogleTM, que aplicou o método num serviço específico (AdSenseTM).
Assim, o objetivo principal desta dissertação é:
Fazer um estudo exploratório sobre o método LSI e a aplicação de funções peso associadas ao uso do método.
Os objetivos secundários são:
Observar o comportamento que têm as funções peso neste contexto;
Determinar qual (is) função peso melhor ajudam a recuperar documentos.
Em a seguinte seção descreve- se a metodologia adotada para o desenvolvimento do trabalho.
A metodologia adotada para o desenvolvimento do presente trabalho constou inicialmente de uma pesquisa bibliográfica nas áreas de recuperação de informação de forma geral, aprofundada nas áreas concernentes ao método LSI e às funções peso.
Após terminada a revisão bibliográfica estudou- se o método LSI, abrangendo desde a criação da matriz termo-documento (matriz inicial) até os métodos que podem ser utilizados para atualizar a estrutura semântica encontrada.
No que se refere às funções peso, foi estudado o modo como estas funções são criadas e como afetam os termos de índice.
Em particular, foram escolhidas e analisadas as funções peso mais utilizadas encontradas na bibliografia.
De modo a analisar o comportamento das funções peso escolhidas para associação com o método LSI, constatou- se a necessidade de desenvolver um protótipo de modo a aplicar estas funções junto a uma coleção de documentos (previamente avaliada), cuja estrutura semântica foi obtida com o uso do método LSI.
A determinação de a (s) função (ões) que possuem melhor rendimento foi feita com uso das consultas pré-definidas da coleção empregada, e os resultados foram avaliados quanto a abrangência e precisão.
O texto da dissertação divide- se em cinco capítulos, precedidos desta introdução e seguidos de Bibliografia e Anexo.
Em o Capítulo 1 se faz uma introdução aos problemas existentes, discutem- se trabalhos de pesquisa realizados e apresentam- se trabalhos correlatos das duas áreas que são abordadas neste documento, o método LSI e as funções peso, destacando os trabalhos cujas contribuições são utilizadas nesta pesquisa.
Em o Capítulo 2 se faz uma introdução à Ri destacando os pontos concernentes à pesquisa.
Em o Capítulo 3 é estudado em detalhe o método LSI, explicando- se a forma de representar os documentos e termos indexados e apresentando- se, o método matemático que é utilizado para reduzir a dimensão da matriz e realizar o matching entre uma consulta e os termos indexados.
Em o Capítulo 4 são detalhadas as funções peso, a importância das mesmas e o modo como influenciam nos resultados de uma consulta.
Já no Capítulo 5 apresenta- se o estudo de caso realizado, a modelagem do protótipo implementado, e o estudo de caso trabalhado no contexto da dissertação, através de o qual foram analisadas quatro diferentes funções peso numa coleção de documentos com consultas pré-definidas.
Por último, no Capítulo 6, apresentam- se as conclusões da pesquisa realizada e considerações quanto a trabalhos futuros.
Entre os modelos algébricos alternativos, uma abordagem apresentada em é o LSI.
A maioria dos sistemas de Ri utilizam este modelo por o fato de que o mesmo é capaz de representar e trabalhar com a informação de forma matemática e estatística.
Esta abordagem gerou diversas pesquisas tanto na maneira como trabalhar cada elemento do vetor, quanto no modo de fazer o matching entre uma consulta e os documentos representados.
Cada vetor representa um documento d, e este vetor possui como elementos os termos ti.
Este vetor possui a seguinte representação:
Onde j é o número do documento d na coleção e cada ti é um termo, sendo que pertence ao intervalo 1 i n, sendo n o total de termos do documento.
Com respeito a o matching, quando um usuário faz uma consulta q esta pode ser representada em forma de vetor da mesma maneira como é representado um documento.
A forma de saber se uma consulta q está próxima a um documento d é mediante o produto ponto, ou produto interno entre os dois vetores:
Este produto serve para medir o grau de similaridade entre um documento armazenado e a consulta do usuário.
Fazendo uma ordenação por os pesos dos documentos recuperados, como resultado se tem um ranking de um conjunto de documentos recuperados, obtendo uma resposta ordenada.
A maneira como se designa um valor, denominado &quot;peso», para cada elemento do vetor, é explicada na seção 4.12.
O tipo de avaliação geralmente depende do objetivo do sistema:
A idéia é avaliar a performance da recuperação.
Van Rijsbergen aponta a existência de diversos indicadores para a avaliação.
Entre eles, os mais aceitos nas pesquisas de Ri são abrangência e precisão, dado que seu propósito é medir a efetividade de um sistema de Ri.
A seguir serão descritos em detalhe estes dois indicadores e será brevemente comentado um terceiro, denominado fallout.
Os modelos de abrangência e precisão podem ser utilizados como modelos de avaliação da Ri na Web.
Apresentam- se na Tabela 2.2 os conceitos inerentes à abrangência e precisão (numa visão de teoria dos conjuntos).
Para entender melhor a Tabela 2.2 apresenta- se graficamente na Figura 2.1 cada um dos conceitos de abrangência e precisão.
A seguir explicam- se estes dois conceitos:
Abrangência. Representa uma porcentagem dos documentos que foram recuperados.
É uma parte dos documentos, o subconjunto dos documentos relevantes (R), entre os documentos que foram recuperados.
Onde, nr representa os n primeiros documentos relevantes recuperados e dr representa os documentos relevantes a uma determinada consulta.
Precisão. Representa uma porcentagem dos documentos que foram recuperados.
É uma parte dos documentos recuperados, o conjunto A, constituído por os documentos que foram recuperados e que são realmente relevantes.
Onde nr representa os n primeiros documentos relevantes recuperados e n o total de documentos.
A Figura 2.2 mostra que os sistemas de Ri em Internet possuem um terceiro fator de medida, que é a velocidade.
Estes sistemas on-line precisam ter velocidade na recuperação de documentos.
O usuário, particularmente, exige velocidade na recuperação dos documentos e precisão destes documentos com respeito a a consulta que forneceu.
Para trabalhar com textos considera- se que existem palavras mais relevantes que outras no mesmo texto, o que significa que aquelas palavras são mais representativas que as outras (no que se refere aos modelos de recuperação, estas palavras possuem peso maior).
Estas palavras são chamadas termos de índice.
Para obter resultados melhores, pode- se recorrer à desambigüização do texto, uma vez que se pretende identificar o sentido de uma palavra num determinado contexto e num conjunto de palavras candidatas.
Para atingir tal objetivo se faz uma análise lexical.
Esta análise é o processo de conversão de um conjunto de caracteres, do texto do documento, numa cadeia de palavras, candidatas a ser termos de índice.
O objetivo de fazer uma análise lexical é identificar as palavras importantes dentro de um texto.
Este processo vai estar sujeito a conhecimentos adicionais como distinguir letras maiúsculas de minúsculas, conhecer a importância dos números como termos de índice, etc..
Palavras que aparecem no texto repetidas vezes são boas candidatas a termos de índice.
Porém, existem palavras que, mesmo aparecendo com alta freqüência, não apresentam tal significância (exemplo:
Artigos, preposições, etc) Este conjunto de palavras não significantes é conhecido como &quot;stopwords».
A eliminação de stopwords evita que palavras não significantes interfiram no processo de recuperação, ajudando a reduzir o tamanho do texto e, com isto, potencialmente reduzir a abrangência.
Quando se eliminam as stopwords se reduz o texto e o tamanho do documento, o que facilita o armazenamento dos documentos.
Quando o usuário faz uma consulta através de uma palavra, pode ser que uma variante dessa palavra (plural, palavra adicionada de sufixos, etc) esteja num documento relevante, ou seja, uma variação dessa palavra pode permitir encontrar termos de índice e, desse modo, melhorar o resultado da busca.
Para operar esta melhora se substitui a palavra por a respectiva raiz (stem), e este processo de substituição é conhecido como &quot;stemming».
A raiz é a parte da palavra que resta, depois de eliminados seus afixos.
É importante utilizar o stemming para melhorar a performance da recuperação porque esta operação reduz as palavras a um termo núcleo, além de o que os afixos de uma palavra não constituem a essência semântica da mesma.
Existem diversas técnicas de stemming descritas por Baeza-Yates e Ribeiro-Neto:
Retirada de afixos, tabela de lookup, variação do sucessor e N­gramas.
A eliminação de sufixos às vezes pode trazer erros:
Só se remove um sufixo quando o contexto está correto.
Entenda- se por &quot;correto «o contexto representado nas seguintes situações:
O comprimento restante do stem excede a um valor;
Usualmente este valor é 2;
2. A letra final do stem satisfaz a uma condição (por exemplo:
Que não termine com a letra g).
Por exemplo, a palavra estadual, eliminado o sufixo ual leva à cadeia estad, mas para a palavra igual, se eliminado o sufixo ual, restaria ig (situação 1) e a palavra terminaria com a letra g (situação 2), levando a uma incorreção no stemming.
Como já explicado anteriormente, um termo de índice pode derivar do texto de um documento ou pode ser independente.
Os termos de índice a escolher podem ser identificados por um especialista ou ser identificados de forma automática.
Para se ter um melhor controle da linguagem, pode- se prever uma hierarquia que relacione os termos de índice.
Existem dois fatores importantes para a efetividade de uma linguagem indexada7, que são exaustividade (exhaustivity), que é o número de diferentes temas indexados, e especificidade (specificity), a facilidade com que se descrevem temas precisamente.
Pensado numa procura na Web, não é viável o uso deste modelo.
Como alternativa podemos indexar termos do texto construindo uma estrutura a qual possa manter- se atualizada.
É claro que isto vai ocorrer quando os índices não tiverem que ser inseridos constantemente (por exemplo, a cada segundo).
Mas este exemplo de inserções constantes é que caracteriza a representação das máquinas de busca da Internet.
Baeza-Yates e Ribeiro-Neto descrevem as seguintes técnicas de indexação:
Arrays de sufixos (Suffix arrays).
Servem para dar respostas mais precisas a consultas mais complexas.
Esta técnica pode ser utilizada para indexar palavras sem stopwords.
Cada posição no texto é considerada como texto sufixo.
Linguagem indexada é a linguagem que descreve documentos e consultas.
Com respeito a a busca, Kang e Kim classificam em três grupos os tipos de consultas que o usuário fornece na Internet:
Temas de relevância (de informação), busca de homepages (navegacional) e busca de serviços (transacional).
Esta classificação ajuda a ter um filtro de busca para obter resultados mais exatos.
Em os autores explicam que as limitações da atual indexação automática são causadas, principalmente, por três fatores:
A maneira como os termos de índice são identificados é incompleta.
Quando se escolhem palavras para servirem de índices, geralmente estas palavras são escolhidas por um grupo de pessoas que selecionam os termos mais freqüentes dentro de um documento, para evitar a sinonímia.
Uma solução alternativa é a construção de tesauros (listas de palavras com seus significados e as relações entre elas;
Normalmente restritos a um domínio específico de conhecimento), só que esta alternativa de solução exige muito trabalho humano.
Existe carência de modelos automáticos para trabalhar com a polissemia.
Este problema pode ser tratado de duas formas:
Utilizar um humano que atue como tradutor.
Os dois modelos anteriores não são totalmente efetivos, além de serem custosos.
Os termos são trabalhados de forma isolada.
De esta maneira se perde a relação existente entre as palavras numa mesma frase.
A existência dos fatores anteriores levou ao desenvolvimento e pesquisa de outros modelos para melhorar essas falhas.
O LSI trabalha rompendo as limitações com os três fatores mencionados anteriormente.
A Indexação Semântica Latente (do inglês Latent Semantic Indexing, LSI) tenta superar as deficiências da recuperação por combinação de termos, tratando a falta de confiabilidade dos dados associados a uma relação termo-documento ou documentodocumento, como um problema estatístico.
Este método assume que há uma estrutura semântica oculta (latente), subjacente aos dados.
Esta semântica é esquecida parcialmente por a aleatoriedade da escolha da palavra no que se refere à recuperação, por o fato de que se escolhem palavras individuais para serem recuperadas, indexadas, etc..
Utiliza- se no LSI um modelo matemático para estimar esta estrutura latente, que liberta do &quot;ruído «constituído por a polissemia e por a sinonímia existente nos documentos.
A descrição dos termos e dos documentos baseados na estrutura semântica latente é utilizada tanto para a indexação como para a recuperação.
Entende- se por &quot;estrutura semântica «a estrutura de correlação entre as palavras individuais que aparecem nos documentos;
&quot;semântico «implica o fato de que os termos, num documento, possam ser tomados como referentes ao documento ou ao assunto desse documento.
Esta técnica de análise da semântica de palavras em distintos documentos é automática:
Essa é a diferença principal que existe entre o LSI e os outros modelos existentes.
O modelo matemático que se utiliza para criar a estrutura semântica é a decomposição Single Value Decomposition (SVD).
O resultado da aplicação deste modelo, após realizadas operações matriciais, é uma matriz aproximada à matriz original.
Esta matriz original é a matriz que representa uma relação, podendo ser esta relação termodocumento, termo-termo ou documento-termo.
Matematicamente, este resultado pode ser interpretado como uma configuração espacial em a qual o produto co-seno entre vetores representa a similaridade estimada entre dois documentos e, na área de Ri, SVD é interpretada como uma técnica para gerar um conjunto de indexações não correlacionadas de variáveis ou fatores;
Cada relação (por exemplo, a relação termo-documento) é representada por seu vetor de valores.
O modelo vetorial é o mais aceito por os pesquisadores para a Ri.
Este modelo é representado da seguinte forma:
De a equação se tem que:
A: É a matriz original com a relação termo-documento, de dimensões m x n;
U: É a matriz ortogonal de dimensões m x k;
K é um valor intermediário entre m e n.:
É a matriz de tamanho k x k com elementos positivos ou nulos na diagonal principal.
Os elementos da diagonal principal recebem o nome de &quot;valores singulares «e estão ordenados de forma decrescente, sendo nulos nas últimas posições da matriz;
VT: É a matriz ortogonal de tamanho k x n, onde T denota tratar- se de uma matriz transposta.
A decomposição SVD representa a matriz original A nas matrizes U, descreve cada uma dessas equações:
E VT.
Cada uma das associações que existem para construir a matriz A possui uma equação.
A seguir se Comparação termo-termo.
O produto co-seno entre dois vetores-linha de Ak mostra a extensão em a qual dois termos têm um padrão semelhante de ocorrências, num conjunto de documentos.
A matriz resultante é uma matriz quadrada simétrica:
Comparação documento-documento.
O produto co-seno entre duas colunas mostra a extensão em a qual dois documentos têm um perfil similar entre eles:
Comparação termo-documento.
O produto das matrizes resultantes de uma matriz que é aproximadamente igual à matriz A, sendo esta matriz de nível k.
Assim o produto resultante é a matriz Ak:
Esta última equação da relação termo-documento vai ser trabalhada com detalhe nas próximas seções.
O SVD é realizado em matrizes reais ou complexas (condição inicial para aplicar a decomposição nas matrizes).
Em descrevem- se duas formas de fazer a decomposição:
Uma forma reduzida e outra completa.
Os exemplos abordados no presente trabalho de pesquisa utilizam matrizes reais e utilizam a decomposição SVD completa.
A decomposição SVD gera uma matriz Ak de nível k (rank-k) relacionada à matriz original A, uma matriz aproximada à matriz original A. Este nível k é representado como rk.
Esta matriz Ak expressa a melhor representação da estrutura semântica de certo domínio, podendo ser este domínio uma coleção de documentos ou um banco de dados.
A escolha de qual o melhor nível de aproximação vai ser feita por testes empíricos, já que os modelos para encontrar esta matriz ainda estão em discussão.
As primeiras rA linhas de VT formam a base do &quot;espaço linha «de A, enquanto que as primeiras rA colunas de U formam a base do &quot;espaço coluna «de A. Obtendo a aproximação à matriz A de nível k, onde k é matriz diagonal de tamanho k x k contendo os k maiores valores O conteúdo semântico de um documento é geralmente determinado por a freqüência relativa de termos e, para descrever este conteúdo semântico de uma coleção de textos, podem ser utilizados os vetores base da matriz original A (os espaços coluna de A).
A aplicação da decomposição SVD ajuda, principalmente, a reduzir o tamanho de m e n, já que em geral os dados da matriz A são altamente esparsos, isto porque nem todos os termos aparecem em todos os documentos.
Após aplicar a decomposição SVD à matriz original e encontrar a matriz AK aproximada à matriz original, resta ver como se faz o matching8 entre uma consulta fornecida por o usuário e a matriz aproximada.
Apresenta uma equação simples para fazer o matching entre um vetor e a coleção de documentos:
A soma dos vetores-termo de dimensão k é refletida por qTUK na equação anterior, e esse resultado multiplicado por diferência os pesos em dimensões separadas.
O resultado desta equação são as coordenadas dentro de um plano cartesiano;
Esta equação é mostrada visualmente na Figura 3.3 (Seção 3.2.3) após realizada a consulta ao sistema.
Por matching entenda- se o uso de um critério de comparação entre um recurso e outro, podendo ser estes recursos palavras, frases, documentos, etc..
Em geral, para fazer o matching entre uma consulta fornecida ao sistema e a coleção de documentos (neste caso, a matriz aproximada de nível k) é utilizado o método de comparação do co-seno.
O objetivo de utilizar o co-seno é medir a semelhança existente entre uma consulta e os documentos da coleção.
Esta equação é apresentada em da seguinte forma:
Onde ej é um vetor que representa a coluna j de uma matriz identidade de tamanho n x n, q representa o vetor de consulta do usuário e exemplo, q é a norma euclidiana do vetor.
Assim, por representa a norma euclidiana do vetor q..
Como critério de discriminação, em utiliza- se para cos j o valor mínimo de 0.5.
Isto indica que os valores iguais ou maiores (pesos dos documentos recuperados) que este valor são relevantes para a consulta q..
Uma equação alternativa para encontrar o matching entre a consulta e os documentos, com um custo computacional mínimo segundo, é:
Onde, para documentos escalados, o vetor s j $= k VkT e j.
Para todos os vetores documento^ (sj) se cumpre que cos j inicialmente.
Tem- se de considerar que estes cálculos (e para que se obtenha um custo computacional baixo) são previstos para matrizes de tipo esparsas.
37 cos j.
Isto implica que, às vezes, se recuperam mais documentos relevantes utilizando esta equação, do que com a equação proposta Em apresenta- se uma outra equação alternativa para o matching entre consultas.
Para tanto, primeiro a consulta q é projetada sobre a estrutura semântica encontrada, da seguinte forma:
Após encontrar q realiza- se o matching através da seguinte equação:
O objetivo de projetar a consulta q nas matrizes e UK é que a consulta faça o matching e U de nível k) e, entre os termos de nível k da coleção (a multiplicação com as matrizes uma vez encontrado o resultado, se faz a comparação com a matriz VK (os documentos de nível k da coleção).
Note- se que, ao fazer o matching entre a consulta e os termos, esta operação é calculada uma vez só.
O resultado (que é o vetor q) é comparado com todos os documentos da matriz VK, e isto leva a ter um ganho no processo da consulta, já que não é refeito o cálculo em cada iteração.
Uma vez encontrada a estrutura semântica, as matrizes U, e V de nível k, existe a possibilidade de querer inserir novos termos ou novos documentos na coleção de documentos, na matriz AK.
A forma como teriam que ser inseridos os novos q termos e p documentos na matriz AK é mostrada nas figuras 3.1 e 3.2, correspondentemente.
Para fazer uma atualização na estrutura semântica, o caminho mais simples seria criar uma nova matriz A com os novos termos ou documentos e calcular novamente a matriz Ak.
Esta forma de atualização, embora seja a mais precisa, tem custo computacional elevado.
Como alternativa de solução existem dois métodos para fazer a atualização:
O folding-in e o SVD-Updating, ambos descritos em e.
Em as próximas seções são abordados estes dois métodos.
AK é que sejam representados em forma de vetores.
A adição de documentos numa estrutura de LSI já existente se dá por meio de a seguinte equação:
Assim, após ser calculado o resultado da equação anterior, é inserido o novo documento na matriz VK.
De igual forma, para adicionar novos termos na matriz Uk segue- se a equação:
Esta forma de atualizar é rápida e com custo computacional baixo.
A desvantagem de utilizar esta técnica de atualização é que a mesma reproduz uma representação inexata da atualização dos novos termos ou documentos, ou seja, quanto maior a quantidade de termos ou documentos adicionados, maior a variação nos resultados.
O SVD-Updating é outra forma de atualizar a coleção de documentos;
Além de permitir inserir novos documentos e novos termos, tem a possibilidade de fazer correções nos pesos dos termos.
No que se refere ao método de cálculo, seja D a representação dos p novos vetores documentos a serem processados, que será uma matriz esparsa9 m x p (lembre- se que m representa o número de termos existentes na coleção).
D é incorporada às colunas da matriz A de nível k, e para tal efeito são calculados os correspondentes valores e vetores singulares da equação:
A forma de calcular os novos valores para UB, SVD (B) $= U B BVBT.
Para inserir novos termos, seja T a coleção de novos termos que serão anexados na coleção de documentos.
Esta matriz esparsa T é de dimensão q x n (já que n representa o número de documentos existentes na coleção).
Esta matriz é incorporada na matriz AK, então os novos valores e vetores singulares são calculados por meio de a seguinte equação:
Afirma- se que esta matriz é esparsa já que nem todos os termos vão aparecer em todos os documentos.
Os novos valores para UC, então:
Se H $= são:
Então os valores correspondentes para UC.
Já inclusos os novos termos ou documentos na coleção, procedemos às modificações nos pesos dos termos da coleção.
Para trocar o valor do peso em j termos, segue- se o processo:
Seja Yj uma matriz de dimensão n x j, esta matriz é uma matriz unitária de nível j.
Seja a matriz Zj de dimensão n x j onde as colunas especificam a diferença atual entre os pesos antigos e novos, para cada um dos j termos.
A equação para calcular o novo peso é:
Para calcular os novos pesos dos termos e documentos procede- se da seguinte forma.
Seja então:
Então os valores para UW e VW são:
J Maiores detalhes sobre os métodos de atualização explicados anteriormente encontram- se em, onde o autor faz um estudo detalhado da atualização de termos ou documentos numa estrutura semântica já calculada.
Referentes ao SVD, existem estudos de melhoria ou variantes ao uso desta decomposição.
Entre eles encontram- se:
Decomposição Semidiscreta.
A decomposição SDD (do inglês Semidiscrete Decomposition) e a decomposição QR10 são descritas em como dois métodos alternativos de decomposição de matrizes.
Em geral as decomposições SVD e SDD estão baseadas na decomposição QR, e este processo de decomposição é descrito em.
Riemannian SVD (R-SVD).
Este é um dos métodos propostos como melhoria ao método LSI.
Em os autores apresentam o método Riemannian-SVD aplicado ao LSI.
O objetivo desta decomposição é criar as matrizes U, e V sem precisar da decomposição SVD.
Estas matrizes, criadas por a decomposição ULV, possuem um nível k menor.
Desta forma o cálculo da matriz A de nível k é mais rápido.
Em são estudados dois métodos de low-rank approximation, métodos alternativos ao método QR.
Em a pesquisa os autores apresentam algoritmos para calcular o sparse pivoted QR approximation (SPQR) e o sparce column-row (SCR), duas formas de obter um nível k mais baixo que o obtido por a decomposição SVD.
A decomposição QR de uma matriz é a decomposição da matriz numa matriz ortogonal e triangular.
Os métodos citados anteriormente não vão ser considerados no presente trabalho de pesquisa, dadas as limitações de tempo inerentes à duração do mestrado.
No que diz respeito a variantes sobre o LSI, existem as seguintes linhas de pesquisa:
Probabilistic Latent Semantic Indexing (PLSI).
O autor Thomas Hofmann descreve em que um dos fatores que afetam o desempenho do LSI é a falta de fundamentação estatística.
Em este sentido o autor acrescenta um fator de análise probabilístico ao LSI, e define um modelo gerador apropriado dos dados.
Em os autores apresentam um estudo sobre a melhoria de desempenho no LSI, que consiste em identificar valores críticos na decomposição SVD e, após, remover- los (elementos deixados com valor igual a zero).
Isto ajuda que a matriz termo-documento seja mais esparsa.
Estas variantes visam aprimorar ao rendimento do método LSI tanto nos resultados quanto no processo da decomposição.
Em o presente trabalho de pesquisa, porém, é estudado o método LSI clássico.
Em esta seção é apresentado um exemplo de aplicação do método LSI a uma coleção de documentos.
Será desenvolvido o processo de decomposição e, após serem geradas as matrizes de nível k, será efetuado o matching entre uma consulta e a estrutura semântica encontrada.
Note- se que esta coleção serve somente como exemplo para entender este método.
Em o Capítulo 5 mostram- se os resultados da aplicação do método num estudo de caso, a coleção de documentos MEDLINE.
O enunciado do exercício aqui apresentado foi retirado de.
Em a Tabela 3.2 mostra- se a coleção de documentos a serem utilizados.
Estes documentos são títulos de livros.
As palavras sublinhadas, na coluna Títulos, são as palavras-chave de cada título.
O critério para que uma palavras seja indexada é que apareça mais de duas vezes na coleção.
As palavras &quot;for», &quot;and», etc..
Não são utilizadas por serem consideradas stopwords.
Após serem identificados os termos de índice na coleção de documentos, o que pode ser o passo seguinte é montar a matriz termo-documento com as ocorrências respectivas.
Esta matriz é visualizada na Tabela 3.3.
Observe- se que, na matriz da Tabela 3.3, não se encontra a ocorrência de um termo mais de uma vez.
Em outros casos as ocorrências nas colunas podem ter quantidades maiores que 1 (várias vezes a mesma palavra no mesmo documento).
Os termos da matriz anterior são normalizados à norma euclidiana de cada coluna.
Esta norma euclidiana é definida por a seguinte equação:
Denota- se por Â a primeira matriz de zeros e uns:
Em seguida, designam- se pesos a cada elemento da matriz.
Note- se que no exemplo descrito em não é utilizada função peso mas, para o presente exemplo, a função peso escolhida é:
A matriz gerada após atribuir o valor correspondente a cada elemento da matriz Â é:
Agora, aplicando a decomposição SVD à matriz A pesada (matriz anterior), encontram- se as matrizes U, e V:
Pode- se observar que o nível de A é 14, pois a matriz zero na diagonal principal (considerando a matriz possui 3 linhas com valor como matriz quadrada).
Para este exemplo vai- se trabalhar com uma matriz Ak de nível igual a 2.
Para isso, são utilizadas as primeiras k linhas e colunas das matrizes U, Uk, e V para formar as matrizes e Vk com nível igual a 2.
Estas matrizes estão dentro de o quadro assinalado em cada uma das matrizes respectivas.
Para gerar a matriz Ak $= 2 multiplicam- se as matrizes anteriores utilizando a equação.
A matriz A de nível 2 resultante é:
Esta primeira etapa termina ao encontrar- se a estrutura semântica (a matriz aproximada e as matrizes Uk, e Vk).
A segunda etapa é o matching entre uma consulta fornecida ao sistema e a coleção de documentos.
Seja q a seguinte consulta fornecida:
A consulta q possui a seguinte configuração em forma de vetor coluna:
Após ser descrita a consulta q em forma de vetor aplica- se a equação:
Por exemplo, para fazer o matching entre o documento B5 (coluna 5) e a consulta q, a aplicação da equação anterior é a seguinte:
Fazendo o matching entre a consulta e a matriz, na Tabela 3.4 mostra- se o ranking para todos os documentos da coleção.
Se a recuperação fosse realizada por um matching literal os documentos recuperados seriam:
B3, B11, B12 e B17.
Utilizando LSI, acrescentaram- se os documentos B5, B6, B7 e B16 aos resultados anteriores.
Os documentos restantes apresentam um nível baixo de relevância, como pode ser apreciado na Tabela 3.4.
Uma outra forma de visualizar os resultados da consulta é no plano cartesiano.
Para representar os termos no plano cartesiano multiplica- se a primeira coluna da matriz U2 por o primeiro valor singular da matriz para o eixo das abscissas, e a segunda coluna da matriz U2 por o segundo valor singular da matriz o eixo das ordenadas.
De a mesma forma, para representar os documentos no eixo das abscissas a primeira coluna da matriz V2 é multiplicada por o primeiro valor singular da matriz já para as ordenadas multiplica- se a segunda coluna da matriz por o segundo valor singular da matriz Após ter realizado a consulta se obtêm as coordenadas da consulta q, mediante a equação q $= q T U k 1.
O resultado são as coordenadas da consulta no plano cartesiano q $ .
Estas coordenadas são projetadas na Figura 3.3 sob o título Consulta.
As coordenadas dos termos e documentos utilizados na Figura 3.3 são mostrados na Tabela 3.5.
Em a Figura 3.3 o vetor Consulta encontra- se projetado sobre os documentos e sobre os termos de índice da coleção.
Para apreciar melhor a diferença entre as duas matrizes, na Figura 3.4 é mostrado o gráfico gerado por os métodos de abrangência e precisão.
Em o gráfico da Figura 3.4 pode- se observar que não se apresentam curvas suavizadas porque a avaliação se fez só com uma única consulta, os resultados mostram que tanto a matriz com peso quanto a matriz sem peso possuem o mesmo grau de precisão e abrangência.
Já no que ser refere à área abaixo de a função, a matriz sem peso possui maior área.
A conclusão sobre esta área ocupada seria que a matriz sem peso é melhor que a matriz que utilizou uma função peso.
Mas, devido a o exemplo trivial e à falta de maior quantidade de consultas para fazer uma análise mais aprofundada, não é possível tecer conclusões sobre esta observação.
O objetivo deste exemplo foi mostrar que o método LSI tem a capacidade de recuperar documentos mesmo nos casos em que não se encontrem palavras idênticas às da consulta.
Em o Capítulo 5, especificamente na Seção 5.5, se faz um estudo detalhado sobre as funções peso e os resultados de abrangência e precisão para cada função utilizada.
Cada documento dentro de uma coleção possui termos-chave.
A escolha destes termos-chave vai depender do objetivo do sistema.
Em geral estes termos são palavras soltas, e não frases.
Para cada documento, após aplicar- se os procedimentos de operações em textos (descritos na Seção 2.4) a listagem resultante dá origem a termos de índice de cada um dos documentos da coleção.
É por meio destes termos que um documento vai ser indexado e ser diferenciado dos outros, ou seja, estes termos de índice servem como identificadores dos documentos.
Note- se que, numa coleção de documentos, um termo de índice pode aparecer em vários ou até mesmo em todos os documentos.
Isto faz que não possa ser diferenciado um documento de outro, ou que não se possa distinguir entre dois ou mais documentos que possuam os mesmos termos de índice.
Por outro lado, os termos de índice que aparecem em poucos documentos podem ser considerados como relevantes para identificar tais documentos.
Dadas estas dificuldades, é necessário designar um nível de relevância entre os termos de índice selecionados;
A forma de designar esta relevância será quantitativa.
Para tal efeito, designa- se um valor a cada termo de índice, e este valor associado ao termo de índice é conhecido como peso.
Em se explica que o objetivo de pesar termos é melhorar o desempenho referente a a habilidade de recuperar documentos relevantes e deixar de lado a informação irrelevante.
Um parâmetro para a Ri é utilizar a freqüência com que uma palavra ocorre num documento:
Esta é uma boa técnica para medir a significância de uma palavra e aplicar uma função que pondere esta palavra com relação a o documento e à coleção de documentos em geral.
Em a Figura 4.1 pode- se apreciar a freqüência de ocorrência de várias palavras numa posição do texto (f) relacionada à posição no ranking (r).
Os dois cortes da Figura 4.1, denominados corte superior e corte inferior, servem para delimitar as palavras mais utilizadas ou mais significantes, o que se pode apreciar na área hachurada.
Esta técnica da palavra mais significante é utilizada no modelo vetorial e no modelo probabilístico.
Embora seja possível trabalhar sem usar termos pesados facilitando o processo da indexação, e simplesmente designar uma função Booleana, não será possível distinguir entre diferentes consultas feitas ao sistema.
Não se utilizando uma função que atribua peso a cada termo pode se dar o caso em que sejam fornecidas duas consultas diferentes e as respostas sejam semelhantes ou até iguais, isto por o fato que existe pouca precisão, donde a necessidade de uma distinção entre os termos indexados, atribuindo- se uma relevância a cada termo.
Uma das funções mais estudadas e aplicadas em sistemas de Ri é a função term frequency (tf).
Uma das primeiras funções a aparecer, esta função é denotada por:
Onde fij indica a quantidade de vezes que o termo i ocorre no documento j;
Ou seja, o termo que ocorre mais vezes no documento é o termo mais importante.
A título de exemplo, considerem- se como documentos os títulos de livros apresentados na Tabela 4.1.
Em a Tabela 4.1 observa- se que, para o primeiro documento, as palavras information e retrieval são mais significantes que Algorithms e Heuristics_ Para o segundo documento, todos os termos de índice possuem o mesmo valor de significância.
Note- se que, se fosse utilizado o critério de indexação do exemplo da seção 3.2.3 (termos que se repetem mais de uma vez na coleção) as palavras indexadas seriam information e retrieval para os dois documentos.
Com este critério resulta impossível distinguir os dois documentos, pois os termos de índice são iguais, embora o tf do primeiro termo da consulta seja maior que o segundo.
Dependendo da aplicação e do objetivo do sistema de Ri, utilizar a função tf pode resultar muito simples:
Só a freqüência de uma palavra não chega a ter um desempenho que satisfaça.
Por este motivo, diversos pesquisadores estudaram a forma de melhorar esta função.
Um exemplo é o trabalho de Christina Christophi onde são mostradas as variações apresentadas por esta função peso.
Imagine- se que os títulos dos livros da Tabela 4.1 são os títulos de uma página Web.
Como acontece na realidade, as páginas podem estar cheias de spam e o título de uma das páginas pode possuir mais ocorrências dos termos de índice do que as outras.
Se só for analisado o tf de cada documento, o peso do documento com maior número de ocorrências seria mais relevante que os outros.
O que não necessariamente é verdade, pois a repetição da palavra leva a distorcer a informação.
Para tratar este problema é necessário analisar o peso global (e não só o peso local como faz o tf) de um termo dentro de a coleção de documentos.
A seguir explica- se a forma de pesar globalmente.
Outra equação muito utilizada, que complementa a equação anterior, é a inverse document frequency (idf), esta última denotada por:
O objetivo desta função é designar a importância de um termo considerando toda a coleção, ou, dito de outro modo, é distinguir os documentos que existem na coleção, diminuindo o peso das palavras que ocorrem em vários documentos.
Em a Tabela 4.1 pode- se observar que os termos de índice information e retrieval ocorrem nos dois documentos.
Isto faz que não se possa distinguir os dois documentos dentro de a coleção, pois ambos possuem quase que as mesmas palavras.
Para que os documentos sejam identificados é necessário aplicar as funções peso de forma geral.
A seguir explica- se esta forma de pesar os termos.
Em as duas seções anteriores, observa- se a existência de dois tipos de pesos:
Locais (termo (s) com relevância no documento) e globais (termo (s) com relevância em toda a coleção de documentos).
Existe uma terceira função, de normalização, que é um fator de ponderação que compensa os distintos comprimentos dos documentos que existem na coleção.
Por o explicado anteriormente, a função peso geral é formada por o peso local (lij), peso global (gi) e o fator de normalização de documentos (dj) 11.
Em esta equação é representada por:
Observe- se que neste caso dj representa o fator de normalização de documentos e não o j-ésimo documento da coleção.
Levando esta notação a uma forma mais geral, se tem a representação utilizada por o sistema SMART:
Onde A representa o peso local, B o peso global e C a normalização.
As três primeiras letras são para a coleção de documentos e as três últimas letras são para a consulta fornecida.
Existem distintas funções peso para cada forma de pesar, nas Tabelas 4.2, 4.3 e 4.4 mostram- se algumas destas funções.
Em as Tabelas 4.2, 4.3 e 4.4 são descritas as funções peso mais utilizadas em aplicações e em pesquisa.
Devemos destacar que as funções apresentadas podem ter distintas variações, por o fato que cada pesquisa estuda uma função e faz diversos ensaios até chegar a uma fórmula mais estável para seus objetivos.
Como exemplo, utilizando as três tabelas anteriores e utilizando a equação, se tem a função peso &quot;lnc», onde l representa o peso local log, n representa a ausência de peso (alguns autores utilizam o &quot;x «em vez de o &quot;n&quot;), neste caso do peso global, e c é a normalização de documentos.
Note- se que neste exemplo a equação, incluindo a consulta, seria &quot;lnc_ Nnn», ou seja, que não é considerado o peso na consulta.
De maneira formal, a associação de peso é assim apresentada em:
&quot;Seja t o número de termos de índice no sistema e seja ki um termo de Em esta designação de pesos, assume- se que se têm pesos independentes.
Quer dizer que, conhecendo o peso wi, j associado ao par (ki, dj), nada se sabe sobre o peso wi+ 1,, j associado ao par (ki+ 1, dj).
Pode- se observar que a importância de designar pesos é fundamental:
É por meio de estas funções que um documento ou um grupo de documentos pode ser reconhecido como significante e ser recuperado, quando uma consulta é fornecida ao sistema.
Este capítulo descreve o estudo de caso realizado no contexto da dissertação:
A coleção de documentos que foi selecionada para gerar a matriz com a relação termodocumento (matriz original A), as funções peso escolhidas, a modelagem e a implementação do protótipo utilizado.
Após apresentado o protótipo, são analisados os dados de saída obtidos ao longo de as execuções.
Estes dados de saída são os resultados das consultas aplicadas a cada uma das matrizes geradas.
A forma de analisar os resultados obtidos é por meio de abrangência e precisão, descritos na Seção 2.3.
Foram empregadas quatro distintas funções peso, cujos resultados foram comparados.
Também foi construída uma análise entre os resultados obtidos, com os diversos níveis de k escolhidos para a matriz original A, numa comparação entre matrizes com a mesma função peso, só que com o nível k distinto.
O objetivo de variar o nível k é observar como se alteram os resultados de um nível a outro.
Descreveu- se no Capítulo 4 que as funções peso têm um papel importante juntos aos termos de índice da coleção, para diferenciar um termo de outro.
Desta forma, para obter um resultado de comparação foram escolhidas as funções peso mais utilizadas nas pesquisas de Ri.
Trabalhos de pesquisa como entre outros mostram que as funções escolhidas têm um bom rendimento se comparadas a outras funções peso existentes.
A seguir apresentamos cada uma das quatro funções peso escolhidas.
Esta função está composta por os três pesos descritos na equação.
A função atc é bastante utilizada por os pesquisadores, e o motivo principal do destaque desta função é que utiliza uma variação do tf.
A composição desta função é a seguinte:
Peso Local (a):
O log é a função logaritmo em base 10, N é o número total de documentos da coleção e df (document frequency) é o número de documentos em que ocorre o termo i.
Peso Global (t):
O tf é igual à equação, max_ tf é o tf de valor máximo.
Normalização (c):
Esta é a equação do co-seno para a normalização de documentos (ver Tabela 4.4).
64 No que se refere a o valor das constantes desta função, os valores de 0.5 foram testados por Robertson Em as diferentes pesquisas do sistema OKAPI se mostram como os que melhor se ajustam para esta função.
O trabalho de Chris Buckley Propõe uma alternativa às propostas de Stephen Robertson e seu grupo de pesquisa.
Esta função segue a notação do SMART:
L é o peso local, t o peso global e u a normalização de documentos.
A equação desta função é:
A equação anterior está composta por:
Peso Local (L):
Onde o Log é a função logaritmo em base 10.
O objetivo de somar 1 no log (tf) é evitar a perda de dado se acontecer um Log (1), pois existe uma ocorrência de termo só que ao aplicar o logaritmo o valor é 0.
Peso Global (gi):
Este peso global é o idf tradicional.
Log é a função logaritmo em base 10, N é o número total de documentos da coleção e df (document frequency) é o número de documentos em que ocorre o termo i.
Normalização: A diferença da função anterior é que esta função utiliza o Pivoted Unique Normalization para a normalização de documentos.
O dl (document length) é o comprimento do documento j na coleção, avg_ dl (average document length) é a média dos comprimentos dos documentos na coleção e os valores 0.8 e 0.2 são constantes com valor predefinido em (a equação genérica está descrita em).
Interessados na pesquisa de Ri, Stephen Robertson e seu grupo estudaram a forma de melhorar os resultados do sistema Okapi.
Inicialmente se começou com a função expandida do tf:
Em a equação anterior, r representa o número de documentos relevantes que contêm o termo i, R representa o número de documentos conhecidos que são relevantes para um tópico específico, n é o número de documentos que contêm o termo i e N é o número de documentos indexados.
Com vistas a melhorar os resultados obtidos por a função continuou- se testando e apresentando diversas melhoras na equação, tanto de forma teórica quanto empírica.
Uma das observações feitas foi considerar o tamanho da coleção de documentos, já que numa coleção de documentos nem todos os documentos possuem o mesmo tamanho, porém a freqüência de uma palavra vai depender do tamanho do documento.
Por exemplo, entre dois ou mais documentos, aquele que tenha maior tamanho vai possuir maior quantidade de palavras.
A partir de esta observação é que a equipe de pesquisa propõe uma nova função peso:
Esta nova fórmula, em essência, é uma variação da função tf* idf;
Pois o w é a equação e k1 é uma constante desconhecida, dl é o tamanho do documento (document length) e avg_ dl é o comprimento médio da coleção de documentos.
Já no TREC-312 apresenta- se a função BM25, que até hoje é uma das mais utilizadas em sistemas de Ri devido a os bons resultados que se obtêm.
Esta função possui a seguinte equação:
Os novos termos utilizados na equação anterior são:
Q é a consulta com T termos, qtf é a freqüência do termo dentro de a consulta Q e K é k1+ utilizado.
Onde k1, b e k3 são parâmetros que vão depender do tipo de consulta e possivelmente do banco de dados De o inglês Text REtrieval Conference (TREC), conferência realizada a cada ano com o objetivo de avaliar técnicas e sistemas relacionados à Ri.
Os pesquisadores indicam, para essas variáveis, os seguintes valores por default:
K1 $= 1.2, b $= 0.75 e k3 é ajustado freqüentemente a 7 ou a 1000.
Em indica- se que, para a variável b, valores menores às vezes podem ser vantajosos.
Esta função, aqui denominada Básica, é uma função peso simples, já que somente considera a norma e o comprimento do documento, sem considerar toda a coleção nem considerar a freqüência das palavras.
A equação desta função é:
Já escolhidas as quatro funções peso a serem avaliadas, descreve- se a coleção de documentos utilizada para o estudo de caso.
Para realizar um estudo de caso dessa natureza é necessária uma coleção previamente organizada, que possua uma quantidade considerável de termos de índice, documentos, consultas e uma listagem dos termos que são relevantes para documentos específicos.
Em a pesquisa realizada foram encontradas as seguintes coleções de documentos:
Folha-RiCOL. É uma coleção de referência cujos documentos foram derivados do corpus FolhaNot.
O corpus FolhaNot é um subconjunto da parte jornalística do corpus NILC/ São Carlos, compilado por a equipe do NILC (Núcleo Interinstitucional de Lingüística Computacional) da Universidade de São Paulo, em São Carlos.
A coleção de referência Folha94 contém 4156 artigos (tratados como documentos) de 229 edições do jornal Folha de São Paulo do ano de 1994.
De estes documentos foram indexados 37267 termos.
Esta coleção tem disponíveis 50 tópicos de consulta.
TodoBR. Páginas Web do motor de busca TodoBR.
Esta coleção refere- se ao período do mês de novembro do ano 1999.
Inclui páginas Web indexadas e as consultas feitas ao motor de busca TodoBR nesse período.
O número de documentos é de 5939061, os termos indexados fazem um total de 2669965 e as consultas disponíveis são 33154.
O inconveniente de trabalhar com esta coleção é que, do total de consultas disponibilizadas, somente 50 foram avaliadas e, do total de documentos, somente 4117 foram avaliados, reduzindo assim consideravelmente a coleção.
MEDLINE (Coleção Medlars).
Esta coleção possui resumos de jornais especializados em medicina.
Esta é uma coleção de documentos padrão utilizada em pesquisas em Ri.
Esta coleção consta de 1033 documentos e 5831 termos.
O número de consultas disponíveis são 30.
Esta coleção, disponibilizada na Internet, já conta com a matriz termo-documento, a listagem dos termos de índice, as consultas tanto em forma de vetor quanto em linguagem natural e a listagem dos documentos que são relevantes com relação a os termos de índice.
CRAN (Coleção Cranfield Aerodynamics).
Esta coleção possui sumários de artigos de jornais da área da engenharia aeronáutica.
Semelhantemente à coleção MEDLINE, é uma coleção de documentos padrão utilizada em pesquisas de Ri.
Esta coleção consta de 1398 documentos e 4612 termos.
O número de consultas disponíveis são 225.
Esta coleção, disponibilizada na Internet, conta com a matriz termo-documento, a listagem dos termos de índice, as consultas tanto em forma de vetor quanto em linguagem natural e a listagem dos documentos que são relevantes aos termos de índice.
CISI (Information Science Abstracts).
Esta coleção de sumários sobre artigos de Ciência da Informação consta de 1460 documentos e 5609 termos, o número de consultas disponíveis são 112.
Esta coleção, disponibilizada na Internet junto com as outras duas coleções anteriores, conta com a matriz termo-documento, a listagem dos termos de índice, as consultas tanto em forma de vetor quanto em linguagem natural e a listagem dos documentos que são relevantes aos termos de índice.
Entre as coleções descritas anteriormente, foi escolhida a coleção MEDLINE, que tem a matriz termo-documento de maior tamanho, com relação a as coleções CISI e CRAN.
Nos Web sites dos grupos de pesquisa de LSI existem recursos disponíveis para trabalhar com este método, alguns de eles utilizando recursos nativos do sistema operacional (por exemplo Unix).
Outros recursos disponibilizados são bibliotecas que ajudam na manipulação de matrizes (criação de matrizes unitárias, cálculo de valores singulares, etc) Existe uma implementação do método LSI em MatLab13 disponibilizada para pesquisa, o problema desta implementação é que se compõe de arquivos pré-compilados onde não é possível analisar nem fazer modificações no código.
Esta implementação utiliza a função peso tf* idf por default, evitando a possibilidade de utilizar outras funções peso.
O objetivo do estudo de caso realizado é analisar o comportamento do método LSI e das funções peso aplicadas a este método, porém foram descartados temas como formas de implementar o LSI, redução no tempo de recuperação de documentos ou processamento de um maior número de documentos.
Foi escolhido o MatLab para implementar o protótipo, já que este ambiente de programação é voltado às aplicações matemáticas, entre outras.
Em particular, o MatLab possui bibliotecas para a criação e manipulação de matrizes e de decomposição de matrizes, que entre estas encontra- se a decomposição SVD.
O tempo de programação é reduzido notavelmente por ter estas bibliotecas prontas, pois para utilizar- las MatLab é uma linguagem de alto nível focada no desenvolvimento de algoritmos, computação numérica, visualização e análise de dados.
O protótipo se apresenta com duas entradas de dados:
A coleção de documentos e a consulta ao sistema.
De forma geral, em cada uma das suas fases as entradas vão gerar saídas que chegam a se transformar em entradas dos outros processos e, na fase final, a saída gerada é o ranking dos documentos como resposta à consulta fornecida ao sistema.
Identificadas as entradas iniciais do protótipo, o passo seguinte é representar estas entradas em forma de vetor (para a consulta fornecida) e em forma de matriz com a relação termo-documento (a matriz original A da coleção de documentos é de tamanho 5831 x 1033).
Uma vez formada a matriz original A o passo seguinte é pesar a matriz, neste passo a matriz original é pesada com cada uma das funções peso escolhidas, gerando como saída uma matriz Â.
Note- se que esta matriz é distinta para cada função utilizada.
Em a fase seguinte utiliza- se a matriz Â para encontrar as matrizes U, V mediante a decomposição SVD.
Uma vez feita esta decomposição, é calculado o nível k e criada a estrutura semântica (as matrizes AK, UK, e VK).
Geradas as matrizes de nível k, o seguinte passo é começar a trabalhar com as consultas que são a segunda entrada de dados do sistema.
As consultas são representadas em forma de vetor.
Em a fase do matching se recebe como entrada as consultas e as matrizes e VK e aplica- se a equação do matching.
A saída que este processo produz é um ranking dos documentos recuperados para a consulta fornecida;
Este ranking está listado de forma decrescente sendo o primeiro documento da lista o mais relevante para a consulta fornecida.
O processo descrito anteriormente é mostrado na Figura 5.1 e a implementação encontra- se em anexo.
Em a Figura 5.2 observa- se que a função Básica tem a menor área inferior à curva entre as funções peso.
Comparando os resultados de abrangência e precisão entre as funções observa- se que esta função é a que possui a menor precisão e a menor abrangência.
Isto significa que, neste nível, esta função não tem um bom rendimento.
No que se refere à função atc, embora esteja acima de a curva da função Básica, não tem muita diferença desta função peso, o que pode ser melhor apreciado nos resultados de abrangência e precisão.
No que diz respeito à abrangência, é melhor em quase 5%, mas na precisão e no MAP possui uma diferença mínima.
As funções Ltu e Okapi possuem um desempenho melhor que as outras duas funções (possuem maior área embaixo de a curva).
Tanto na Figura 5.2 quanto nos dados da Tabela 5.1 observa- se que estas duas funções têm uma diferença mínima entre elas:
Os resultados de abrangência, precisão e MAP mostram uma diferença menor que 1%.
Esta pequena diferença entre as funções peso mostra que, estatisticamente, as duas funções não possuem uma diferença significativa, não se podendo estabelecer qual das duas funções é melhor a este nível de k.
Em a Figura 5.3 é exibido o gráfico gerado com o nível de k $= 774.
As quatro curvas geradas mostram, em geral, uma pequena melhora na recuperação em relação a o nível anterior;
As características continuam semelhantes:
A função Básica, neste nível, continua tendo o pior rendimento entre as quatro funções, a função atc tem um incremento de quase 4% na área do gráfico mas ainda o rendimento desta função é menor no que se refere às funções Ltu e Okapi.
Estas últimas duas funções continuam com o melhor rendimento, mas ainda continuam sem mostrar uma diferença significativa entre ambas.
Em particular, na Figura 5.4 a função Básica não mostra um incremento significativo no resultados de abrangência e precisão no que diz respeito às outras funções;
A função atc tem uma melhora significativa.
Os resultados de precisão mostram uma semelhança entre a função Ltu e Okapi, mas nas medidas de abrangência e MAP mostra- se que a função atc ainda não tem um rendimento maior que as funções Ltu e Okapi.
O rendimento entre estas últimas duas funções ainda continua semelhante.
Em a Figura 5.5 mostra- se o último nível escolhido.
Em este gráfico observa- se que as quatro funções diminuíram quase um 50%, a variação na área do gráfico deste nível respeito ao nível anterior é significativa.
Embora as funções Ltu e Okapi mantiveram seu rendimento melhor que as outras duas funções diminuíram também o nível de abrangência e precisão.
Conclui- se que as funções Ltu e Okapi são as funções com melhor rendimento, o rendimento de ambas funções chega ser parelho, porém não se pode concluir qual das duas funções é melhor.
Com o objetivo de analisar cada função em cada um dos quatro níveis de k escolhidos, foram reunidos os resultados de uma mesma função no mesmo gráfico.
A seguir, na Figura 5.6, mostra- se o gráfico para cada uma das funções peso.
A o igual que a função Ltu, o gráfico da Figura 5.9 mostra um desempenho quase constante da função Okapi.
O nível 10 possui o desempenho mais baixo.
Entre os níveis restantes, o nível 258 apresenta melhores resultados de desempenho que o nível 774.
Esta diferença pode ser apreciada, sobretudo, na abrangência e na área abaixo de a curva.
Por os quatro últimos gráficos conclui- se que diminuir o nível de k ajuda a melhorar a abrangência e precisão mas que esta diminuição tem um limite, pois o ultrapassar este limite leva piorar a abrangência e precisão.
No decorrer de a realização do presente trabalho de pesquisa, estudou- se em detalhe o método LSI e o comportamento das funções peso aplicadas a este método.
Em o primeiro capítulo identificou- se o problema da falta de alguma análise semântica da informação, apresentaram- se os trabalhos de pesquisa realizados em Ri e em outras áreas trazendo o estado-da-arte no uso deste método.
Em o segundo capítulo foi apresentada uma visão geral da Ri, oferecendo o embasamento teórico para esta pesquisa.
Em o terceiro capítulo foi apresentado o método LSI, se fez um estudo detalhado mostrando como o mesmo funciona, como se aplica e como pode ser atualizada a estrutura semântica encontrada.
A título de ilustração se desenvolveu um exemplo aplicando este método a uma pequena coleção de documentos.
Em o capítulo quatro se aprofundou o estudo das funções peso, mostrando a forma como são utilizadas e como são geradas estas funções para, depois, serem aplicadas a uma coleção de documentos.
Já no capítulo cinco foi apresentado o estudo de caso desenvolvido para analisar o método LSI e o comportamento das funções peso.
A seguir apresentam- se as experiências e conclusões obtidas com a pesquisa, e os trabalhos futuros que poderiam dar seqüência ao presente trabalho.
Este trabalho de dissertação teve seu início com uma revisão bibliográfica preliminar (Trabalho Individual II).
A pesquisa inicial enfocou a Web Semântica (origens, estado- da arte, etc) e, como área complementar, a Ri na Web.
Posteriormente, no Plano de Estudo e Pesquisa foi proposto o estudo de um método de análise semântica para os documentos da Web Semântica, ou seja, para uma coleção de documentos que usasse uma notação adotada por a Web Semântica.
O primeiro problema encontrado na realização dessa proposta preliminar foi a inexistência de coleções de documentos significativas em RDF ou RSS para fazer avaliações consideráveis.
Além de possuir um número pequeno de documentos para fazer a avaliação estas coleções não estavam preparadas nem possuiam consultas relatadas, levando a um problema maior na hora de avaliar os resultados.
Como alternativa considerou- se trabalhar com uma coleção de documentos para a Web, sendo este o ambiente mais próximo de a Web Semântica.
A procura de uma coleção de documentos Web levou a encontrar a coleção da ferramenta de busca TodoBR, disponibilizada para pesquisa.
Esta coleção possuía um total de 5939061 documentos e 2669965 termos indexados.
O número de consultas à coleção é de 33154.
O problema encontrado nesta coleção foi que, do total de consultas feitas, só 50 foram avaliadas e, do total de documentos disponibilizados, só foram avaliados 4117, diminuindo consideravelmente a quantidade de documentos.
Um outro problema desta coleção era que a mesma não estava preparada (ou seja, os documentos ainda não estavam sistematizados para uso do LSI) e não foi encontrada referência bibliográfica de pessoas ou grupos de pesquisa que tenham utilizado esta coleção, que permitisse uma comparação de resultados.
Já que não se tinha parâmetro algum para utilizar esta coleção, decidiu- se então utilizar a coleção Folha-RiCOL.
Foi interessante esta alternativa já que, além de possuir um corpus em português tinham- se resultados de avaliação desta coleção.
Para os objetivos desta pesquisa, faltava ser criada a matriz termo-documento e a representação vetorial das consultas, então começou- se a criar a matriz.
Encontraram- se dois problemas neste processo:
O menos importante, a falta de disponibilidade de um computador altamente performante e dedicado a gerar a matriz e os vetores;
O segundo, o tempo de processamento no cálculo da decomposição.
Calculou- se a decomposição SVD com uma matriz esparsa de 10000x1000 termos (menos da metade do total da coleção), e o tempo de processamento foi de três dias, sem chegar- se ao término do processo.
Pensou- se trabalhar fracionando a Resource Description Framework (RDF):
Modelos ou fontes formais de dados baseados em XML.
Recomendado por a W3C para a descrição do conteúdo de um Web site em metadados.
Paralelamente à atividade de encontrar a coleção de documentos, foi- se estudando o método LSI.
Em a bibliografia detectamos a ênfase na possibilidade de fazer uma análise semântica da informação, aplicável em diversas áreas.
Esta forma de fazer a análise de uma relação termo-documento foi o incentivo para estudar este método.
Em uma primeira instância observou- se que, para entender o funcionamento deste método, era necessário primeiro entender o funcionamento da decomposição SVD, pois o eixo principal deste método está nesta decomposição.
Encontra- se referência à decomposição SVD na área de álgebra linear numérica, em.
Em encontram- se os conceitos da decomposição.
A explicação está baseada em métodos numéricos de decomposição de matrizes (por exemplo, a decomposição QR).
Em encontra- se o algoritmo e a implementação desta decomposição na linguagem de programação C. O método LSI é explicado detalhadamente em.
Em os exemplos utilizados e nas fórmulas utilizadas no livro, é empregada a notação do MatLab, donde surgiu a idéia de estudar este método sobre este ambiente de programação.
Observou- se que, ao aplicar a decomposição SVD, o custo computacional crescia:
Quanto maior o tamanho da matriz, maior o tempo de processamento, porém o tempo para calcular as matrizes U, e V depende do tamanho da matriz.
É por este motivo que alguns autores assinalam que este método é inviável para uma aplicação em Internet, porque o tamanho da matriz de uma 86 recupera documentos por a semântica comum entre ambos, obtendo, desta forma, uma abrangência maior na recuperação.
Com respeito a as funções peso, a maioria dos trabalhos de pesquisa nesta área referência à função Okapi.
Esta função foi o resultado da melhora da função atc.
Em é estudada a forma de avaliar resultados sem os dados de abrangência e precisão, tendo sido utilizadas quatro funções peso:
Tf, atc, Okapi e Ltu.
Utilizando como referencial para nosso trabalho de pesquisa, foram escolhidas as funções atc, Okapi e mais uma função, chamada de básica, que serviu como base de comparação entre as outras três funções.
Depois de ter implementado as funções peso, não existe dúvida que o uso destas funções é fundamental para melhorar os resultados das aplicações onde se trabalhe com corpus de dados (podendo ser este corpus:
Coleções de documentos, conjuntos de dados, etc) As funções Okapi e Ltu foram as que determinaram de melhor forma os pesos nos elementos da matriz termo-documento.
Um dos motivos do bom rendimento é que estas funções consideram várias características da coleção de documentos e que estas duas funções possuem constantes calculadas de forma empírica, que ajudam a melhorar a qualidade dos resultados.
Mostrou- se também que a função atc possui um rendimento médio com relação a as outras três funções.
Em particular, o rendimento desta função em relação a a função Okapi é menor, confirmando assim a melhora que teve a função Okapi em relação a esta função.
A vantagem maior de ter implementado o protótipo em MatLab foi poder obter resultados de forma rápida, poupando tempo de implementação de funções que manipulem matrizes e sobretudo implementando a decomposição SVD.
O tempo ganho foi utilizado para estudar em detalhe o método LSI e gerar resultados para depois analisar- los.
A maior desvantagem desta forma de implementação foi o tempo e o recurso computacional consumidos para realizar a decomposição SVD.
Acredita- se que, se o protótipo houvesse sido implementado em outra linguagem de programação (por exemplo C ou C+), o consumo de memória seria menor, e em tal caso se poderia trabalhar com uma coleção de documentos maior.
Embora o aprendizado deste método tenha sido difícil e o mesmo demande um custo computacional elevado para sua implementação, a possibilidade de aplicação em áreas onde a necessidade de comparar e descobrir alguma relação entre dados seja importante, ou crítica, leva a considerar- lo como uma boa alternativa para descobrir essa relação que é procurada.
As funções peso são fundamentais em problemas que trabalhem com um corpus de dados, onde aplicar uma função peso adequada ao ambiente leva a obter melhores resultados.
Observou- se nos resultados do estudo de caso que, quando a função peso considera a maior quantidade de características possíveis do corpus, os resultados a serem obtidos tendem a ser melhores.
Os resultados do estudo de caso mostraram que o sucesso de um sistema de Ri que utilize LSI vai depender de encontrar um nível de k adequado onde se represente de melhor forma a estrutura semântica da coleção de documentos.
No que se refere a sistemas de Ri em geral, a função peso que seja utilizada vai levar a recuperar, da melhor forma, os documentos da coleção.
É claro que os resultados encontrados nesta pesquisa são ainda escassos para poder concluir se este método é melhor que outros métodos existentes ou que as funções Okapi e Ltu são as funções mais recomendadas para utilizar em Ri.
Nosso objetivo foi fazer um estudo exploratório, não se pretendendo tecer conclusões mais expressivas nesta etapa Para poder ter resultados mais expressivos destaca- se a necessidade do uso de outras coleções de documentos, que permitirão visão da melhora ou piora quanto a os resultados obtidos.
Se fossem utilizadas coleções com maior quantidade de termos de índice e documentos (por exemplo, as coleções Folha-RiCOL ou TodoBr) poderia se ter como objetivo analisar o desempenho do LSI em coleções de maior tamanho.
Também poderiam ser estudadas formas de implementar este método.
Outra forma de ver o desempenho deste método seria aplicar as pesquisas descritas na Seção 3.2.2, em especial o trabalho de April, Kontostathis Onde são identificados valores críticos na decomposição SVD.
Para saber se a semântica inerente, encontrada por o método LSI numa coleção de documentos, é mais abrangente, teríamos que comparar este método com outros em Ri.
Seria interessante utilizar outro tipo de corpus de dados para ver se existem diferenças de resultados;
Como exemplo, poderia ser aplicado este método em áreas como ontologias, clustering ou categorização de textos, já que os corpora utilizados nestas áreas diferem de uma coleção de documentos clássica.
