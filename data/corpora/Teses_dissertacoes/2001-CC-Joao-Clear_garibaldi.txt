Os avanços da tecnologia de informação vieram garantir a possibilidade das empresas manipularem grandes volumes de dados, com o uso das redes viabilizando operações em nível mundial.
Diariamente, dados sobre os mais variados aspectos dos negócios da empresa são gerados e armazenados e passam a fazer parte dos recursos de informação da organização.
Em geral, estes dados estão armazenados em representação multidimensional, não permitindo de imediato uma representação física para os mesmos.
A mineração de dados (data mining) envolve a exploração em bancos de dados para descobrir relações que não estão armazenadas explicitamente.
Por outro lado, muitos esforços tem sido dedicados no sentido de integrar a visualização a este processo.
A técnica de coordenadas paralelas é uma metodologia já estabelecida para representação de tuplas multidimensionais em duas dimensões.
Porém, para um conjunto volumoso de tuplas sua representação fica comprometida por o acúmulo e sobreposição de linhas.
Outro aspecto que se faz notar com o uso desta metodologia é a não utilização de uma dimensão disponível para representar os dados, ou seja, trabalha- se somente com representação em duas dimensões, perdendo- se portanto a terceira dimensão.
Visando desenvolvemos tornar a representação uma nova de coordenadas que paralelas mais clara, neste trabalho abordagem representará variáveis multidimensionais em três dimensões.
Com esta abordagem procuramos, num primeiro momento, resolver o problema da sobreposição de linhas que acontece quando representamos os dados em duas dimensões e que, se não tratados prévia e corretamente, continuará existindo na representação em três dimensões.
Para tanto, nos apoiamos em conceitos da área de estatística no sentido de identificar padrões de comportamento (similaridades, proximidade, padrões, etc) entre as tuplas trabalhadas.
Feito isto, as tuplas passarão por um processo de classificação a fim de que as possamos manipular ordenadamente, procurando, desta forma, eliminar a sobreposição de linhas e tornar a sua representação em três dimensões mais harmoniosa.
A representação em três dimensões torna- se então viável tendo em vista que as tuplas passarão a ser projetadas ordenadamente criando uma representação com profundidade.
Para tanto, desenvolvemos um protótipo para que testes pudessem ser efetuados no sentido de buscarmos comprovação prática da teoria abordada ao longo de esta dissertação.
Utilizamos da para isto de algumas sua tuplas padrão, no sentido ao de permitir o acompanhamento evolução representação paralelamente embasamento teórico, bem como exemplos que nos permitissem trabalhar mais intensivamente com a ferramenta.
Palavras-Chave: Coordenadas Paralelas, Visualização, Coeficientes Correlação, Dados Multidimensionais.
Procuraremos multidimensionais como nos ater à com presente trabalho visualização atenção à de dados de suporte decisão, dedicando especial área administração e gerenciamento de negócios.
Para isso, nossa pesquisa inicia a partir de a técnica de coordenadas paralelas desenvolvida por Alfred Inselberg,, técnica esta utilizada para a representação de variáveis multidimensionais em duas dimensões.
Inicialmente procuramos demonstrar alguns aspectos que se fazem notar com o uso desta técnica quando de a representação de conjuntos volumosos de dados.
Em estes casos, em virtude de a grande quantidade de linhas que são projetadas, a técnica apresenta algumas restrições principalmente no que diz respeito à sobreposição e acúmulo destas linhas no gráfico.
Esta sobreposição nos leva a uma drástica redução e, em alguns casos, eliminando totalmente a possibilidade de extrairmos qualquer informação significativa dos dados que estão sendo projetados.
Observa- se também que a técnica utiliza somente duas dimensões na representação dos dados, perdendo com isto a terceira dimensão.
Em vista de o exposto procuramos desenvolver uma nova abordagem para esta técnica, no sentido de encontrarmos uma alternativa para agregar a terceira dimensão para a representação de variáveis multidimensionais através de uma generalização do conceito de coordenadas paralelas.
Procuramos, com esta abordagem, projetar cada uma das linhas (tuplas) multidimensionais em profundidade (perspectiva), uma após a outra, representando desta forma graficamente cada uma de elas uma espécie de fatia.
Destacamos que esta forma de representação também poderia ser feita utilizando a técnica tradicional de coordenadas paralelas (em duas dimensões), porém sem nenhum ganho na sua representação uma vez que manteríamos a sobreposição das linhas (num mesmo plano) e, consequentemente, as mesmas dificuldades para a extração de informação.
Representando as tuplas em profundidade estamos agregando à técnica de coordenadas paralelas a terceira dimensão que é a proposta deste trabalho.
Com a projeção em três dimensões, além de suavizar a visualização, passamos a contar, por exemplo, com a possibilidade de rotacionar a imagem e a partir de isso, observála por diversos ângulos aumentando, desta forma, a capacidade de percepção dos dados.
Passamos também a contar com uma melhor utilização do espaço uma vez que podemos representar sobreposição de uma grande quantidade acontece na de tuplas num mesmo gráfico em duas sem a linhas que visualização tradicional dimensões através da técnica de coordenadas paralelas no plano.
No decorrer de o trabalho constataremos que os dados, se não forem previamente trabalhados, continuarão sendo representados com sobreposições, persistindo desta forma o problema já existente quando de a representação em duas dimensões.
Em a tentativa de encontrarmos uma alternativa para a resolução deste problema, nos valemos de conceitos desenvolvidos na área de estatística que permitissem identificar padrões de comportamento entre os dados que estiverem sendo tratados.
Constatamos que, uma vez identificados estes padrões, principalmente no que se refere a proximidades e similaridades, podemos aplicar métodos de classificação rearranjar os dados para representar- los mais claramente em três dimensões.
Com esta abordagem, através do desenvolvimento de um protótipo buscou- se generalizar a técnica de coordenadas paralelas e demonstrar a viabilidade do projeto de agregar a terceira dimensão no processo de visualização.
A apresentação das características do protótipo será precedida de embasamento teórico necessário para um melhor entendimento da metodologia adotada no desenvolvimento do mesmo.
Para tanto;
Esta dissertação está organizada em capítulos como segue.
Com o capítulo 2, sobre tratamento de dados em ambiente empresarial, procuramos abordar aspectos relacionados à tecnologia da informação nas organizações, onde as ferramentas para exploração em bases de dados têm- se transformado em aliado para a obtenção de sucesso no gerenciamento dos negócios.
Procuramos destacar também neste capítulo a necessidade do desenvolvimento de novas ferramentas que facilitem a visualização destes dados.
O capítulo 3 trata de aspectos relativos a visualização científica e apresenta, além de alguns conceitos, um breve histórico do que é visualização científica, bem como procura relatar a sua evolução ao longo de os últimos anos.
Em o capítulo 4, o enfoque é em relação a coordenadas paralelas.
Em ele analisamos a técnica já desenvolvida e largamente utilizada para a representação de dados multidimensionais.
Destacamos, no decorrer de o capítulo, que a esta ser metodologia investigado nos apresenta algumas restrições que trabalho.
Em o capítulo 5, sobre tratamento estatístico dos dados, são abordados aspectos estatísticos no que se refere a métodos utilizados por esta ciência no manuseio de variáveis multidimensionais.
Imperativa foi uma incursão neste domínio, tendo em vista a necessidade de nos apoiarmos em alguns dos métodos e conceitos já desenvolvidos e sedimentados nesta área.
Com o capítulo 6, procuramos descrever a metodologia aplicada na implementação do protótipo, onde apresentamos a seqüência de passos percorridos para tornar viável a representação de variáveis multidimensionais em três dimensões a partir de a técnica de coordenadas paralelas descrita no capítulo 4.
Apresentamos, no capítulo 7, os testes efetuados para efeitos de validação do protótipo desenvolvido, bem como análise a respeito de os experimentos.
Os dados utilizados para estes testes encontram- se disponíveis nos Anexos do trabalho.
Para finalizar, no capítulo 8 apresentamos para as conclusões e obtidas com o desenvolvimento do trabalho, a obtidos com a proposta.
A a medida em que a economia se afasta rapidamente da &quot;Era Industrial «para a &quot;Era da Informação e do Conhecimento», caracterizada por uma concorrência global, é necessário que as empresas reformulem as idéias e processos que as levam a competir no mercado.
Os últimos 30 anos testemunharam o surgimento de novas e avançadas tecnologias que vieram revolucionar o tratamento das informações.
Algumas empresas têm adotado avançados sistemas de informação, os quais fornecem aos seus gestores detalhes rápidos e precisos sobre as preferências e o comportamento do mercado em que operam.
Em a realidade, as pessoas que diariamente têm que tomar decisões nas empresas, necessitam urgentemente de informação precisa e de qualidade sobre os vários setores de atividade da empresa.
O processo de tomada de decisão numa empresa quase nunca é um processo dos monolítico.
Setores Em esse de sentido, é importante conjugar vendas informações e mercados, provenientes diversos atividade, nomeadamente financeiro e de pessoal, logística e produção, de forma que se possa criar uma &quot;imagem «completa e atualizada do seu negócio.
Entre as tecnologias da informação estão surgindo sistemas chamados por os americanos de business intelligence e que fazem parte de uma nova categoria dos Sistemas de Apoio à Decisão.
Business intelligence (Bi), ou inteligência de negócios, tem como objetivo fornecer informações estratégicas que possam suportar decisões bem informadas.
Para tanto, Bi abrange uma série de ferramentas e metodologias.
Por trás do desenvolvimento desta área, há tanto fatores tecnológicos que têm tornado possível o surgimento de novas ferramentas capazes de prover informação deste nível, quanto fatores relacionados com o próprio mercado, com os negócios, ou a necessidade de informação de tal natureza.
Estas pressões do mercado determinam as aplicações de Bi, que pode servir para planejamento estratégico, marketing, gerenciamento de materiais e diversas outras áreas de decisão.
A informática e a tecnologia da informação evoluíram muito nas últimas décadas.
No entanto, apenas recentemente estas novas tecnologias puderam dar um melhor apoio ao mundo dos negócios.
As empresas estão se modernizando e se tornando mais criativas e eficientes na solução de seus problemas.
A automação das atividades administrativas e dos processos produtivos trouxe grandes benefícios para a área operacional e retorno financeiro aos investimentos realizados.
Porém, especificamente para a área de negócios e para a alta gerência da empresa, somente nos últimos anos é que a tecnologia da informação começou a trazer benefícios.
Algumas das tecnologias da informação se transformaram em ferramentas estratégicas para a obtenção de sucesso no gerenciamento dos negócios.
Destacamos, como as mais importantes:
Data warehouse (DW) e data mining (DM).
Segundo Inmon, considerado um pioneiro no tema, um data warehouse é uma coleção de dados orientada por assuntos, integrada, variante no tempo, e não volátil, que tem por objetivo dar suporte aos processos de tomada de decisão.
Já para Cazarini, é um grande banco de dados voltado para dar o suporte às necessidades gerenciais de informações.
Os dados que o compõem são derivados dos diversos bancos de dados operacionais utilizados na empresa, e estão armazenados num local diferente dos referidos bancos de dados operacionais.
Segundo Cielo, nada mais é do que um banco de dados criado especialmente para dar suporte à decisão, carregado com as informações da empresa, que permite aos diretores, gerentes e decisores, garimparem informações conforme o que eles acharem melhor e devem estar acessíveis através de comandos simples.
O data warehouse permite que sejam feitas consultas e análises bastante eficazes, transformando dados esparsos em informações que podem ser convertidas em estratégias de negócios.
O principal benefício proporcionado com a sua utilização é a diminuição do tempo que os gerentes levam para obter as informações necessárias aos seus processos decisórios com a eliminação de tarefas operacionais como pesquisa e identificação dos dados necessários.
Estas bases de dados contém gerentes para realizarem as referidas tarefas.
Em as próximas décadas o data warehouse deve tornar- se uma ferramenta somente os dados necessários aos imprescindível para os gerentes conseguirem administrar seus negócios.
Como o ambiente de negócios está se tornando cada vez mais dinâmico, é extremamente necessário que as regras de negócios sejam incorporados às aplicações, que as estruturas dos sistemas se ajustem aos negócios e que o tempo de resposta dos sistemas seja cada vez menor.
Qualquer sistema de data warehouse só funciona pode ser utilizado plenamente, com boas ferramentas de exploração.
Com o surgimento do data warehouse, a tecnologia de data mining (mineração de dados) também ganhou a atenção do mercado.
Como o data warehouse possui bases de dados bem organizadas e consolidadas, as ferramentas de data mining ganharam grande importância e utilidade.
Essa técnica, orientada a mineração de dados, oferece uma poderosa alternativa para as empresas descobrirem novas oportunidades de negócio e, acima de tudo, traçarem novas estratégias para o futuro.
Para potencialmente Cazarini úteis a data de mining é o processo estão de extrair informações num data partir dados &quot;brutos «que armazenados warehouse ou nos bancos de dados dos sistemas transacionais.
Data tendências mining as é o processo de descoberta empresa, de novas correlações, da análise padrões e entre informações uma através de grandes quantidades de dados armazenados em bancos de dados usando técnicas estatísticas e matemáticas para reconhecimento de padrões.
Segundo Feldens tais ferramentas, através de algoritmos baseados em redes neurais, estatística ou algoritmos de aprendizado, são capazes de explorar grandes bases de dados em busca de informações que possam ajudar os administradores de empresas a decifrar o comportamento do consumidor, a compra de suprimentos ou ainda administrar as áreas comerciais e financeira de forma transparente.
Estas ferramentas são capazes hipóteses de e aprender a partir de os dados, gerando e validando para o enormes volumes de destacando conhecimento interessante usuário, eventualmente descobrindo conhecimento novo, útil e importante.
O propósito da análise de dados é descobrir previamente características dos dados, sejam relacionamentos, dependências ou tendências desconhecidas.
Tais descobertas tornam- se parte da estrutura informacional em que decisões são formadas.
Uma típica ferramenta seleção de análise dados e da dados iniciar ajuda uma os usuários finais na para definição geração do de a problema, na apropriada análise informação, que ajudará a resolver problemas descobertos por eles.
Em outras palavras, o usuário final reage a um estímulo externo, a descoberta do problema por ele mesmo.
Se o usuário falhar na detecção do problema, nenhuma ação é tomada.
A premissa do data mining é uma argumentação ativa, isto é, em vez de o usuário definir o problema, selecionar os dados e as ferramentas para analisar ferramentas do data mining pesquisam automaticamente os mesmos tais dados, a procura as de anomalias e possíveis relacionamentos, identificando assim problemas que não tenham sido identificados por o usuário.
Portanto, as ferramentas de data mining analisam os dados, descobrem problemas ou oportunidades escondidas nos relacionamentos, e então diagnosticam o comportamento dos negócios, requerendo a mínima intervenção do usuário, assim ele se dedicará somente a ir em busca do conhecimento e produzir mais vantagens competitivas.
Como podemos ver, as ferramentas de data mining, baseadas em algoritmos, inteligência artificial, redes neurais, regras de indução, e lógica de predicados, somente facilitam e auxiliam o trabalho dos analistas de negócio das empresas, ajudando as mesmas se tornar mais competitivas e maximizar seus lucros.
O processo de descobrimento realizado por o data mining pode ser utilizado a partir de sistemas transacionais.
Porém, é muito mais eficiente utilizar- lo a partir de um data warehouse onde os dados já estão sem erros, sem duplicidade, são consistentes e habilitam descobertas mais abrangentes e precisas.
As técnicas de visualização não são propriamente técnicas de data mining, mas sim meios de analisar e observar os dados de uma determinada base de dados de forma gráfica.
A visualização fornece meios de obter sumários visuais de uma base de dados.
As ferramentas de visualização podem ainda ser usadas como um mecanismo de compreensão da informação extraída por meio de as técnicas de linhas de e data mining (com Características difíceis de detectar por a simples observação colunas valores numéricos) podem se tornar óbvias se forem observadas graficamente.
Por meio de visualização podem ser utilizadas técnicas interativas que permitam rápida e facilmente alterar o tipo de informação analisada, bem como o método usado (histogramas, gráficos de dispersão, etc).
Também é útil para a percepção de características que se aplicam a pequenos subconjuntos dos dados e que poderiam passar despercebidas se fossem utilizados somente meios estatísticos, pois estes consideram basicamente características genéricas.
Através destas técnicas podem ser encontradas características ou fenômenos pouco comuns ou interessantes sem que se esteja diretamente procurando por eles;
A partir de estas constatações, pesquisas tornam- se necessárias, no sentido de desenvolver e quem sabe, agregar novas ferramentas às técnicas de DW e DM que facilitem a visualização desta massa de dados e que permitam, com facilidade, de elas extrair informações para que possam ser analisadas da forma que for necessária;
Permitindo a usuários pouco especializados entender- los da maneira fácil e sem margem a dúvidas.
O campo da pesquisa científica é, muitas vezes, caracterizado por a análise de grandes quantidades de dados.
Para obter um melhor entendimento dos dados com os quais estiver trabalhando um pesquisador geralmente precisa recorrer a recursos específicos da matemática, estatística, simulação e outras técnicas.
Cada técnica representa e interpreta os dados de um modo particular e contribui, assim, para sua análise.
Mesmo sabendo que a informação sobre o perfil do cliente típico ou do produto de sucesso de uma empresa encontra- se de alguma forma entre os dados de marketing e de vendas armazenados nos bancos de dados da empresa, ainda pode existir um longo caminho a ser percorrido até que esta informação esteja, de fato, disponível.
O uso da matemática clássica é a maneira tradicional para examinar dados científicos.
Porém, esta abordagem muitas vezes não é satisfatória para análise de grandes quantidades de dados, tendo em vista que os dados brutos podem facilmente confundir o pesquisador.
A sua &quot;extração «eficaz, de modo a poder subsidiar decisões, depende da existência de ferramentas especializadas que permitam a captura de dados relevantes, e que manipulem esta massa de maneira que tornem possível a sua visualização através de várias dimensões que permitam análises significativas dos dados, de tal maneira a transformar dados brutos em informação útil para os processos estratégicos da empresa.
O caminho encontrado para a solução deste problema foi a representação visual dos dados que passa a ser uma aproximação mais intuitiva e pode ajudar o pesquisador no exame de características complexas e na justaposição dessas características.
Dá- se o nome de visualização científica ao processo de extrair as características destes dados e sua representação através de abordagens gráficas.
Para Miladinov, sistemas de visualização são usados para analisar dados complexos e críticos.
Assim, é importante representar os dados de um modo verdadeiro e aceitável, de forma que o usuário possa deduzir conclusões corretas sobre os mesmos.
A visualização científica é também parte integrante do processo de simular fenômenos naturais, onde a principal meta é entender como a natureza funciona.
Para isto o cientista segue, através de vários passos, observando um evento ou fenômeno natural para analisar os resultados.
A representação visual destes dados é freqüentemente indispensável para que se possa obter uma melhor compreensão do processo envolvido.
Segundo características Domik dados, e a usar visualização um científica para é o processo de extrair destes computador representar- los visualmente.
Também para McCormick, a disciplina de visualização científica é relativamente nova e sua definição e metas foram estabelecidas inicialmente em 1987 por o relatório Visualização em Computação Científica, que serviu como base tendo em vista as definições das exigências primárias e as especificações dos primeiros sistemas de visualização.
Muitos dos mais recentes sistemas de visualização herdaram as exigências iniciais de software e introduziram um novo conjunto de exigências.
Estas novas exigências foram adicionadas, como uma extensão, para atender novas técnicas de visualização que não eram conhecidas dos projetistas dos antigos sistemas.
Também tiveram o propósito de modificar algumas exigências antigas que não foram bem entendidas por os projetistas da época.
Este processo interativo de engenharia de software produziu os sistemas de visualização utilizados atualmente.
Conforme Miladinov, como uma disciplina nova que evolui a passos rápidos, a visualização científica não se sujeitou, rigorosamente, às exigências requeridas por a engenharia de software.
É muito difícil implementar, na prática, as exigências de software no campo da visualização científica, por as seguintes razões:
Melhores e novas técnicas de visualização são desenvolvidas rapidamente em várias instituições acadêmicas, tornando obsoletas algumas das técnicas atualmente utilizadas nos sistemas de visualização;
Visualização conhecimento científica de um nas esforço áreas multidisciplinar de química, que utiliza cientistas física, matemática, estatística, engenharia e ciência da computação, passando a ser um desafio estabelecer comunicação clara dentro de uma comunidade tão diversa como esta.
Principalmente, sabendo- se que o produto final de um sistema de visualização é uma imagem, torna- se extremamente difícil definir padrões e exigências para esta imagem, uma vez que existirá sempre uma relação de dependência dos dados a serem visualizados.
Por este motivo a visualização científica funciona caso a caso.
Como no processo de projetar uma ferramenta de visualização para dados dirigidos, o desenvolvedor deve preocupar- se, principalmente, em entender a complexidade destes dados, se as exigências para os dados mudarem, então as exigências para a ferramenta de visualização também deverão mudar.
Diferentemente de McCormick, para Carmo e Earnshaw, a visualização científica não é uma área nova de pesquisa.
Ao contrário, é um termo antigo que passou a receber maior interesse por parte de a comunidade da área da computação e tem evoluído muito devido, principalmente, à evolução dos computadores com recursos gráficos de alto padrão assim como a disseminação dos supercomputadores, o que tem permitido a evolução de áreas que exigem um alto desempenho computacional.
Uma de elas é a visualização interativa de dados provenientes da solução numérica de problemas de grande volume e complexidade.
Citam ainda como exemplo, os autores Martinez e Earnshaw, exames não destrutivos e não invasivos de estruturas internas de organismos vivos, efeitos da turbulência em fluxo de fluidos e engenharia genética.
São todos exemplos que chamaram a atenção pública e onde a visualização científica trouxe benefícios significativos.
Isto porém, é apenas um aspecto do vasto campo de aplicação desta ciência.
A visualização de campos vetoriais recebe freqüentemente o nome de visualização de fluxos e tem se tornado um poderoso aliado para numerosas aplicações científicas e tecnológicas.
São aplicações importantes da visualização de campos vetoriais:
A análise dos resultados de testes de estruturas aero e hidrodinâmicas (como aviões, carros e navios) em túneis de vento e água, estudos de transmissão de calor em máquinas e motores, estudos de bioindicadores transportados por o fluxo do sangue no corpo humano, estudos climáticos e oceanográficos, entre outros.
Estes constituem apenas alguns dos exemplos que evidenciam a relevância desta área de pesquisa.
Os resultados de muitos destes estudos ou não podem ser avaliados sem o auxilio da computação ou se beneficiam grandemente através de ela.
A visualização da informação em geral e a visualização científica em particular, são áreas de pesquisa dinâmica, em constante evolução.
Como conseqüência, surgem novas técnicas e seus métodos estão em constante aperfeiçoamento.
O sucesso da visualização depende da aplicação da técnica mais adequada aos dados em estudo e a que melhor atenda aos objetivos da visualização, inexistindo, portanto, ferramentas de visualização prontas para todas as crescentes necessidades do mercado.
Segundo McCormick, a definição clássica de visualização é como segue:
A visualização é um método computacional.
Ela transforma o simbólico no geométrico, permitindo a pesquisadores observar as simulações.
A visualização disponibiliza um método para ver o que não pode ser visto.
Ela enriquece o processo de descoberta científica e disponibiliza conhecimento profundo e inesperado.
Para Aref, a visualização científica é um campo novo da ciência computacional em franca expansão tendo em vista o crescimento rápido da tecnologia computacional, particularmente em relação as workstations e softwares gráficos.
As ferramentas de visualização estão começando a participar de nossas vidas diárias através do seu uso nas artes e particularmente, em animação de filmes.
Passa a ser igualmente uma grande promessa para pesquisa científica e aplicações educacionais.
Quando gráficos gerados por computador são aplicados a dados científicos com o propósito de transmitir conhecimento e testar hipóteses, estamos falando de visualização científica.
Já Foley, considera que uma definição útil de visualização, poderia ser a ligação (ou mapeamento) de dados para uma representação que pode ser percebida.
As ligações, poderiam ser visuais, audíveis ou táteis, quem sabe, até uma combinação destes.
Conforme Gershon, visualização é mais do que um método computacional.
Visualização é um processo para transformar informação numa forma visual, permitindo aos usuários observar a informação.
A exibição visual resultante permite ao cientista ou engenheiro perceber características escondidas nos dados, porém necessárias para análises exploratórias visuais dos mesmos.
Torna- se apresentação.
Importante de diferenciar entre estão visualização preocupados, científica gráficos com da Gráficos apresentação principalmente, comunicação da informação de resultados já conhecidos, de modo que estes sejam melhor compreendidos.
Maneira para Utilizando- se técnicas de visualização científica, busca- se os uma melhor representar, entender e, por conseqüência, interpretar dados.
Porém, freqüentemente os dois métodos são entrelaçados.
A explosão da nave espacial Challenger, em 1986, ilustra bem as conseqüências de uma conclusão mal deduzida de uma técnica de visualização imprópria.
Conforme Miladinov, a explosão foi causada por dois anéis de borracha que vazaram devido a o tempo frio.
Em a noite anterior ao lançamento da nave, os engenheiros de propulsão da NASA usaram uma ferramenta de visualização para determinar se existiria qualquer correlação estatística entre tempo frio e um possível dano aos anéis.
Eles acharam um padrão visual no gráfico dos dados que indicava não haver nenhuma correlação.
Assim, concluíram ser seguro prosseguir com o lançamento.
Infelizmente eles estavam errados e o erro de eles aconteceu devido a o fato que só foram plotados uma parte dos dados, enquanto ignoravam o restante.
Em este caso particular, a representação visual dos dados era precisa e não ambígua, mas a técnica de visualização usada para aquela parte particular de dados era imprópria.
Este exemplo ilustra a importância de usar a configuração correta de uma ferramenta de visualização para tomar decisões cruciais, principalmente sabendo- se que uma ferramenta não pode decidir.
A decisão caberá sempre à pessoa que estiver analisando os dados gerados.
O exemplo citado, reforça nossa afirmação anterior de que a escolha de uma técnica de visualização é altamente dependente das características dos dados que serão analisados.
Em outras palavras, a dedução de exigências de software para uma correta visualização deve começar com a análise dos dados.
Habber e McNabb descreveram o processo de visualização composto por três transformações principais.
A meta destas transformações é converter a informação representada por os dados numa imagem visual que seja facilmente compreensível por o sistema de percepção humano, sem perder o significado original transmitido por a informação:
A primeira transformação descrita como enriquecimento de dados.
Utilizando os dados brutos reais ou de uma simulação e convertendo- os num formato que esteja pronto para as operações de visualização subseqüentes;
O segundo passo, chamado mapeamento da visualização, é um passo muito importante que mapeia os dados para um objeto objetos têm atributos como geometria, cor, de visualização.
Estes tempo, transparência, luminosidade e reflexão, que serão usados para retratar o significado dos dados ao espectador;
O terceiro passo faz a renderização e aplica operações como translação, rotação, escurecimento e sombreando aos objetos de visualização criados no passo anterior.
Este modelo captura problema essencial no projeto de um sistema de visualização.
O cientista recebe os dados num formato bruto e se utiliza de um sistema de visualização para analisar- los.
O sistema de visualização executa certas transformações nestes dados e os exibe através de uma representação visual.
Enquanto na fase de projeto do sistema, é trabalho do desenvolvedor da visualização definir as exigências básicas para assegurar que as técnicas de transformação e de visualização a serem aplicadas aos dados sejam as apropriadas no sentido de que transmitam um alto grau de percepção.
Este passo é importante nas exigências do processo, porque utilizando- se de técnicas de visualização impróprias para analisar os dados poderia- se- tirar conclusões errôneas, como visto no caso de a Challenger.
O foco do problema é que os cientistas só conhecem o significado subjacente dos dados e eles se valem do processo de análise visual para melhor entender- los e de eles extrair as principais características e, por conseqüência, as mais importantes.
Então, o desenvolvedor do projeto de visualização não pode definir completamente as exigências para o sistema de visualização antes de começar a implementação do mesmo.
Além disso, o desenvolvedor trabalha com um conjunto incompleto de exigências, e o processo de projetar um sistema de visualização torna- se uma colaboração interativa entre o cientista e o desenvolvedor do mesmo.
Depois de revisar vários protótipos visuais e analisar os dados, cientista e desenvolvedor podem convergir a um conjunto fixo de exigências de software.
Este tipo de colaboração é implementado no National Science Foudation (NSF) e Engineering Research Center (ERC) na Mississippi State University.
O Engineering Research Center tem três áreas principais:
A área de Grid Generation (Geração de Malhas), a Computational Fluid Dynamics (Dinâmica de Fluídos) e a área de Visualização.
Os cientistas na geração de malhas, geram a malha para um fenômeno físico particular, sendo que os cientistas que trabalham com a dinâmica de fluidos transformam aquela malha, para quantidades numéricas.
Por sua vez, área de visualização tem a incumbência de criar as imagens dos dados recebidos.
Para trazer à tona exigências em tal ambiente cooperativo, o desenvolvedor da visualização precisa estar ciente dos conceitos importantes nos campos da dinâmica de fluidos e geração de malhas.
O desenvolvedor precisa também estar atento às diferentes culturas e linguagens dentro de estas diferentes comunidades.
É importante notar que, dentro de esta multidisciplinaridade, o desenvolvedor deverá ter o máximo de agilidade no sentido de atender a todos, da melhor forma possível, com o seu trabalho.
Ganhando familiaridade com os conceitos socio-lingüísticos e etnológicos o desenvolvedor estará melhor equipado para filtrar informações sem importância e extrair somente informações pertinentes.
O desenvolvedor pode enfocar então no material pertinente (método de zoom) como sugerido por Gougen e Linde.
Outro assunto não menos importante no desenvolvimento de um sistema de visualização destacado por Miladinov, é a audiência esperada.
Em a visualização individual, somente o cientista que gerou os dados é a audiência esperada.
Em este ambiente, o cientista está familiarizado com os dados e ele não precisa de nenhum contexto como etiquetas ou legendas para melhor entender- los.
Isto significa que a visualização é preparada para a percepção visual entendida por somente uma pessoa e poderá não fazer sentido para observadores estranhos à aplicação.
A visualização de grupo deve ser a meta de todo cientista que queira compartilhar a análise dos dados com outros colegas.
Este tipo de visualização, além de ter um contexto mais acabado uma vez que inclui a descrição visual dos dados, agrega aos mesmos legendas e cores.
Porém, este tipo de visualização parte do princípio que o espectador já esteja familiarizado com os conceitos científicos e com os dados que estão sendo trabalhados.
O terceiro tipo de visualização é para o público, este sim, dirigido ao público em geral.
A visualização &quot;de grupo «assume que o espectador não tenha muita familiaridade com os dados e nem com conceitos científicos que estejam por trás disto e usa descrições elaboradas e técnicas de visualização que se destinam para o público geral.
A audiência de um sistema de visualização é um fator importante nas exigências de software.
É relativamente fácil incluir alternativas de visualização &quot;individual», porque o desenvolvedor só terá que satisfazer as exigências de um cientista.
A inclusão de exigências de visualização &quot;para o público «é bastante difícil, porque o desenvolvedor tem que unir as exigências de diferentes cientistas e tentar usar as técnicas apropriadas que apelem ao sistema de percepção visual destas várias pessoas.
Por exemplo, um cientista pode ser chamado a examinar dados em duas dimensões, enquanto outro poderia preferir um visão tridimensional dos mesmos dados.
Isto representa um problema, porque o sistema cognitivo do indivíduo é treinado para analisar dados de um certo modo, sendo difícil mudar sua maneira de ver.
Em estes casos, o desenvolvedor passa a enfrentar muita inércia quando tenta apresentar novas técnicas de visualização.
Uma alternativa para reduzir um pouco este problema é o desenvolvedor trabalhar de perto com o cientista na transição para o novo modo de visualizar os dados.
Outra opção seria preparar os sistemas com alternativas para visões múltiplas dos dados, apropriando, desta forma, o sistema para vários modelos visuais preferidos por os cientistas.
Ajustar as exigências de visualização &quot;para o público «é um processo mais difícil.
Em este caso, o desenvolvedor tem que extrair exigências do público geral, que não tem experiência com técnicas de visualização e não entende a complexidade dos dados científicos.
A técnica mais eficiente para a definição de exigências em tal contexto é a prototipação.
Desenvolvendo um protótipo que ilustre e se utilize de várias técnicas de visualização, o desenvolvedor pode se utilizar de alguns dos conceitos de visualização científica para o usuário não científico.
Assim, este usuário poderá escolher uma técnica apropriada para o protótipo que mais se identifique com o seu sistema cognitivo.
Como visto na seção anterior, há um número grande de problemas com a dedução de exigências para um sistema de visualização científica.
Os problemas são devido a a natureza multidisciplinar do sistema, à complexidade dos dados, e o desenvolvimento constante e rápido de novas técnicas de visualização.
Christel e Kang apresentam um modelo que agrupa problemas de dedução de exigências em três categorias:
Problemas de escopo, problemas de compreensão e problemas de volatilidade.
Problemas de escopo:
Problemas de compreensão:
Os cientistas têm uma compreensão incompleta das suas necessidades;
Os cientistas têm pouca compreensão das limitações em técnicas de visualização;
O desenvolvedor têm pouco conhecimento do domínio de problema;
O desenvolvedor e os cientistas utilizam- se de diferentes terminologias para o mesmo conceito;
Visões contraditórias de diferentes cientistas;
As exigências são freqüentemente muito vagas;
Os cientistas não podem descrever figuras utilizando- se de palavras.
Problemas de volatilidade:
Os as exigências evoluem com o passar do tempo (o cientista não está seguro do que ele quer até que se tenha produzido uma visualização);
Novas técnicas tornam- se, rapidamente obsoletas.
Principais problemas de dedução de exigências para um sistema de visualização científica ajustam- se bem dentro de estas categorias.
Porém, estas categorias não respondem por o fato de que pessoas diferentes percebem informação visual diferentemente.
É muito difícil para um desenvolvedor extrair exigências de um cientista cujo sistema cognitivo visual não esteja afinado ao do desenvolvedor.
Uma solução para este problema é estarem familiarizados entre si em relação a os seus sistemas cognitivos e aprender como eles percebem, individualmente, a informação visual.
A visualização tradicional em duas dimensões e a plotagem de linhas estão entre as técnicas mais comuns de visualização utilizadas para dados com reduzido número de variáveis, porém, podemos mapear as variáveis utilizando gráficos em diferentes cores, tamanhos, formas e localizações.
Para dados volumosos (maiores que o número de pixels da tela), poderiam ser visualizadas uma certa porção exemplo, no restante destes dados e permitir Wong que, o a usuário navegasse, de por interativamente.
Segundo é um visualização da dados multivariáveis científica.
Foi e multidimensionais (MDMV) estudado separadamente subcampo e importante visualização antes da por estatísticos psicólogos desde computação ter sido transformada numa disciplina.
O aparecimento de computadores pessoais de baixo custo e as estações de trabalho (workstations) na década de 80 trouxeram nova vida dentro de a análise gráfica dos dados MDMV.
Paralelamente à evolução dos computadores, este tópico de pesquisa passou a fazer parte das metas, a curto prazo, do seminário de Visualização Científica Computacional ocorrido em 1987 e patrocinado por a NSF.
Os questionamentos sobre técnicas efetivas e eficientes de visualização de MDMV se expandiram desde então.
Schmid e Hinterberger citam em seu trabalho que o principal objetivo da visualização MDMV passou a ser encontrar, a partir de dados multidimensionais, maneiras de reduzir para estruturas de dimensão menos complexas estes conjuntos de dados, procurando com isto permitir que sejam encontradas mais facilmente as tendências e os relacionamentos entre as variáveis que estão sendo utilizadas.
Diferentes propriedades, estruturas e características dos dados podem mudar o modo de visualização, porém não os objetivos.
Infelizmente, existe muito pouco conhecimento sobre como encontrar regras práticas e úteis que ajudem na escolha de um método para representação gráfica de um determinado conjunto de dados multidimensional.
Muito esforço tem sido desenvolvido no sentido de aperfeiçoar métodos para visualização deste tipo de dado, mas o questionamento sobre quantos diferentes métodos de visualização são conhecidos surpreendentemente, recebeu pouca atenção.
Normalmente não é possível selecionar, de imediato, um método específico de exibição, muito menos uma combinação satisfatória de exibições e escolher uma melhor visão para um determinado conjunto de dados tendo em vista principalmente os aspectos individuais das pessoas que devem ser considerados quando de a decodificação da informação que está sendo representada graficamente.
Um pesquisador poderá identificar um fenômeno em particular nos dados somente se estes forem mostrados de uma maneira especial.
Outro pesquisador poderá não identificar o fenômeno da mesma forma, necessitando para isto talvez se utilizar de outra técnica para visualizar os mesmos dados e encontrar resultados equivalentes.
Conclui- se, portanto, que o conceito de visualização envolve idéias de diferentes áreas de pesquisa, tais como:
Computação gráfica, estudo da percepção humana, geometria, análise de dados e ciência cognitiva.
A visualização científica ou visualização em computação científica, nos auxilia na extração de informações úteis a partir de um complexo e/ ou volumoso conjunto de dados utilizando- se de imagens e/ ou gráficos.
O tratamento de grandes volumes de informação requer a existência de meios para reduzir a quantidade de informação a visualizar, quer eliminando informação (filtragem), quer simplificando a forma de apresentar (escolha de representação).
Os resultados das pesquisas em visualização afetam diretamente os aspectos quantitativos e qualitativos do trabalho de usuários, mas a utilidade geral de informação visualmente exibida está afetando crescentemente outros aspectos de trabalho, tais como supercomputadores, satélites, scanners médicos, microscópios, radiotelescópios, sensores geofísicos, informação sobre rodovias, geometria e modelos computacionais.
Todos estes geram, em princípio, grande quantidade de dados de engenharia e dados médicos.
As muitas técnicas que nos ajudam a analisar e compreender estes dados, provendo representações visuais, estão incluídas na definição de visualização.
A visualização inclui, além de técnicas que auxiliam o usuário a interagir com a representação visual de dados científicos, o hardware, o software e também sistemas que suportem o modo interativo.
A partir deste ponto do trabalho passaremos a analisar o desenvolvimento conceituar as ocorrido na área de visualização científica nos últimos anos, bem como principais técnicas existentes na MDMV.
Segundo Nielson as últimas três décadas de desenvolvimento da visualização de MDMV podem ser caracterizadas, grosseiramente, nas seguintes fases:
A análise exploratória clássica de dados;·
seminário de Visualização em Computação Científica da NSF 1987;
A Conferência em 1991 sobre Visualização de IEEE' 91.
Nos relata Nielson, cada uma destas fases possui Também, conforme algumas características importantes e que devem ser destacadas conforme segue.
A primeira fase estava primariamente preocupada com a apresentação gráfica de uma ou duas variáveis.
Esta fase foi dominada por a análise de dados exploratória de Tukey, quando cientistas começaram a encarar dados em representação gráfica com uma perspectiva diferente.
Embora a maioria dos gráficos ainda permanecesse na forma bidimensional, os cientistas puderam codificar dados com parâmetros múltiplos, isto é, multivariados, com plotagem bidimensional.
O reconheceu a impulso deste de a trabalho continuou de por a próxima fase O quando a NSF dos importância visualização dados em envolvimento cientistas em computação acelerou o crescimento da pesquisa, conseguindo, através do uso dos computadores alterar as idéias tidas até então, gerando com isto o desenvolvimento de muitos conceitos novos.
A missão foi definida formalmente e foram desenvolvidos muitos novos e promissores conceitos durante os anos subseqüentes.
O estágio final (atual) se preocupa com a elaboração e determinação de técnicas de visualização de MDMV.
Resta ser visto se os conceitos de visualização de MDMV existentes podem nos levar para uma melhor visualização de um problema e o melhor entendimento básico desta ciência.
Esta discussão de visualização MDMV está ainda longe de ser completa.
Cientistas usam visualização de múltiplas variáveis desde quando Crome, utilizando- se de símbolos em forma de pontos, procurou mostrar a distribuição geográfica de 56 comunidades na Europa.
Em 1950, Gibson começou a pesquisa em percepção visual de textura.
Posteriormente, Pickett e White propuseram o mapeamento de um conjunto de dados num gráfico artificial de objetos compostos por linhas.
Este trabalho de mapeamento por textura foi, mais tarde, investigado por Pickett, e tendo sido posteriormente computadorizado.
Chernof apresentou sua seqüência de faces de múltiplas variáveis em 1973.
Em esta famosa técnica, são mapeadas a forma das faces e suas características faciais, incluindo o nariz, boca e olhos.
Estas faces são exibidas então num gráfico bidimensional.
Este estágio da pesquisa pode ser caracterizado por conjuntos de dados com tamanho relativamente pequeno e ferramentas para sua visualização que normalmente consistiam de lápis de cor papel para gráficos.
A geração de gráficos era exibida, principalmente, em duas dimensões.
Estatísticos eram a força de pesquisa dominante durante este período.
Foram usados gráficos para extrair as características principais dos dados, sugeriram métodos de análise estatísticos que seriam aplicados aos dados e apresentariam as conclusões.
A análise exploratória de dados de Tukey significou uma nova era para a visualização científica de dados.
A análise exploratória de dados é mais do que uma ferramenta:
É uma maneira de pensar.
Ensina para as pessoas como decodificar, dos dados, uma informação visual.
Com o aparecimento do computador pessoal, este se tornou a ferramenta mais poderosa do cientista.
A partir de aí foi possível visualizar dados além de duas dimensões interativamente.
O trabalho desenvolvido até então, com longos cálculos, tornou- se disponível em tempo real.
Estatísticos podiam visualizar dados durante cada fase da análise sem ter de aguardar até que os resultados finais estivessem disponíveis.
A disponibilidade de computadores com recursos para exibição de cores em alta resolução também trouxe novas oportunidades para o estudo de visualização de MDMV.
Durante esta fase, dados espaciais em duas e três dimensões foram os dados mais comuns que começaram a ser estudados.
Satélites enviados da terra estão de transmitindo continuamente dados.
Ainda, nesta fase, Asimov apresentou a técnica visualização por projeções de dados de múltiplas variáveis em planos de duas dimensões.
O seminário da NSF em 1987, formalmente declarou a necessidade da visualização espacial de objetos em duas e três dimensões.
A projeção em duas dimensões de conjuntos de dados com múltiplas variáveis também é incluída como um dos objetivos potenciais a curto prazo nas pesquisas de visualização científica.
Uma vez tendo sido definida a missão, os cientistas começaram trabalhando na representação e visualização de dados MDMV.
O limite da disponibilidade de hardware de alta velocidade para gráficos durante o estágio anterior foi gradualmente sendo conquistado.
A maioria das pesquisas foram dirigidas não levando em conta o desenvolvimento de ferramentas exploratórias de análise de dados que se baseiam, pesadamente, em medidas estatísticas que exigem computações de alta velocidade.
Alguns dos conceitos de visualização MDMV, desenvolvidos nesta fase, incluem:
Coordenadas paralelas, que serão estudadas em maior profundidade no capítulo 4 deste trabalho, iconografia, mundos dentro de mundos, empilhamento de dimensões, eixo hierárquico e várias outras idéias a respeito.
Muitas destas técnicas tentaram mostrar todas as dimensões todas as variáveis visualmente numa única exibição, entretanto, outras visavam a manipulação direta de gráficos, em os quais o usuário interativamente seleciona subconjuntos para exibição usando, por exemplo, um dispositivo como o mouse.
A realidade virtual começa, nesta época, a aparecer na literatura da visualização de MDMV.
Ainda segundo Nielson, em 1990 e 1991 haviam, pelo menos, quatorze trabalhos relacionados a MDMV publicados nas conferências de visualização do IEEE.
Um total de sete foram publicados nas quatro conferências de visualização desde então, até 1997.
Algumas das ferramentas mais recentemente desenvolvidas são aperfeiçoamentos, cada um destes em caminhos diferentes de trabalho, das fases desenvolvidas anteriormente.
Por exemplo: HyperSlice é uma tentativa para combinar uma matriz de painel de uma matriz de scatterplot.
AutoVisual é uma versão estendida de mundos dentro de mundos com interfaces baseadas em novas regras.
XmdvTool integra quatro ferramentas de visualização de MDMV existentes:
Empilhamento de dimensões, matriz de scatterplot, glyphs, e coordenadas paralelas num sistema com ênfase na pintura n-dimensional.
As pesquisas em visualização MDMV tem sido diversificadas e incluindo nesta tarefa colaboradores multidisciplinares.
Tentativas de combinar sons com gráficos estão também sendo feitas.
Um dos mais recentes assuntos de pesquisa em visualização de MDMV é a necessidade de avaliar a regularidade, efetividade e utilidade das técnicas de visualização de MDMV.
Preocupações semelhantes também aparecem nos outros campos de pesquisa de visualização.
Infelizmente, a literatura de MDMV sofre com uma terminologia inconsistente.
O termo dimensionalidade especialmente é sobrecarregado.
Matemáticos consideram dimensão o número de variáveis independentes numa equação algébrica.
Engenheiros interpretam dimensão como medidas de qualquer tipo (largura, duração, altura, densidade).
Até mesmo o prefixo multi freqüentemente é substituído por o prefixo hiper.
Em literatura estatística, o prefixo multi significa dois ou mais, indicando um ponto de ruptura natural entre uma e duas dimensões em métodos probabilísticos.
Para o ponto de ruptura entre três e quatro (ou além de), o prefixo hiper é usado.
Por acharmos mais coerente, no presente trabalho, adotaremos o prefixo multi para nos referir a dimensionalidade maior do que um.
A tornando- se utilização cada vez de mais conjuntos comum de e dados isto com variáveis a multidimensionais cada vez está a com passa- se sentir, mais, necessidade de novas técnicas para visualizar, filtrar, analisar e interpretar estes dados.
Como vivemos num universo em que temos três dimensões espaciais, estamos acostumados com esse tipo de representação e pouca atenção dedicamos a outras formas de mostrar conjuntos de dados.
Coordenadas paralelas são uma metodologia para visualização geométrica em n-dimensões para problemas multivariáveis.
A idéia desta técnica foi apresentada por Alfred Inselberg na Universidade de Illinois em 1959, que tem trabalhado em ela desde então.
É uma técnica de visualização onde as dimensões são representadas como uma série de eixos paralelos uns aos outros e com igual espaçamento entre eles em os quais os valores estão representados,.
Cada eixo representa uma coordenada na dimensão correspondente.
Desenhando- se uma linha ligando os eixos uns aos outros, podemos representar pontos e planos em n-dimensões.
Desta forma, cada variável é representada em seu próprio eixo e os valores das demais variáveis nos eixos adjacentes.
Assim, um ponto num espaço n-dimensional transforma- se numa linha poligonal disposta através de n eixos paralelos com n-1 segmentos de linha conectando os n valores de dados.
Em sendo assim, a representação de um vetor através da representação de até a representação de Nn na coordenada 1, coordenada Nn, será obtida na coordenada 2 e assim por diante forma, a partir de a representação n..
De esta resultante, podemos tirar conclusões a respeito, por exemplo do relacionamento existente entre as variáveis representadas.
Um grupo de linhas projetadas bastante próximas uma das outras nos indicará um grau de relacionamento positivo entre as tuplas que as compõem.
Outra vantagem deste método de visualização é que a representação de todos os vetores num mesmo gráfico nos permite efetuar comparações visuais entre vetores.
Pontos em espaços representação em coordenadas Euclidianos são paralelas.
O representados em de linhas poligonais podem na ser número dimensões que visualizadas com esta técnica é extremamente grande e limitado, em princípio, apenas por a resolução horizontal da tela.
Entretanto, à medida que o número de dimensões cresce, as coordenadas deverão ser representadas próximas umas das outras gerando, consequentemente maiores dificuldades para a percepção de padrões.
A técnica de coordenadas paralelas contrasta, portanto, com o tradicional sistema de coordenadas cartesianas onde todos os eixos são mutuamente perpendiculares.
O uso de coordenadas paralelas para a análise de dados multidimensionais é ideal, tendo em vista a facilidade que esta metodologia nos proporciona no sentido de permitir a visualização de tendências e correlações existentes entre um conjunto grande de variáveis.
Representando uma reta em coordenadas cartesianas, a partir de o conjunto de pontos $= --!
N, teríamos um formato conforme pode ser visto na Figura 1.
Com a representação dos mesmos dados através da técnica de coordenadas paralelas, passamos a ter uma representação conforme a Figura 2.
Nota- se que todas as linhas (representando os pontos nas coordenadas cartesianas), em coordenadas paralelas cruzam um ponto comum.
Considerando as variáveis tupla de valores de (5, um 3, 4, 2, 0, 3, sete e que estas estejam representadas por a com a Figura 3, de exemplo da apresentamos representação paralelas.
Uma característica importante das coordenadas paralelas consiste na possibilidade da permutação entre os eixos.
Isto significa que as dimensões, em sendo necessário para melhorar a análise dos dados, poderão ser apresentadas fora de a seqüência original da tupla.
Com a Figura 4, procuramos demonstrar esta propriedade, para tanto, utilizamos a mesma tupla já representada na Figura 3.
De entre as vantagens da representação através de coordenadas paralelas, destaca- se, principalmente, a possibilidade de trabalhar com diferentes escalas, sendo uma para cada coordenada forma, a bem como de representar variáveis dados em qualquer e suas dimensão, relações, permitindo desta visualização multidimensionais vantagens estas não encontradas nos métodos tradicionais em duas ou três dimensões.
No decorrer de a pesquisa em relação a coordenadas paralelas, foram detectadas duas restrições em relação a esta metodologia:
A representação de múltiplas variáveis num mesmo gráfico gera acúmulo e sobreposição de linhas plotadas que contribuem substancialmente para congestionar o gráfico não nos permitindo a extração de qualquer tipo de informação, nem mesmo dedutiva, a respeito de os dados.
Outro aspecto que se faz notar com o uso desta metodologia não utilização de uma dimensão para representar os dados, ou seja, trabalha- se somente com representação em duas dimensões, perdendo- se portanto a terceira dimensão.
Procuramos, com o exemplo a seguir, demonstrar as restrições referenciadas.
Para tanto utilizaremos como modelo parte de um pedido de mercadoria colocado numa empresa produtora de calçados a partir de um cliente.
Com a representação, procuramos reforçar que são tuplas que por série uma de armazenam série de tipicamente (tuplas), cada variáveis cada uma de elas multidimensionais.
Pedido representando um este, e composto por uma linhas (eixos), produto colunas representando as quantidades de pares de calçados em cada um dos números (coluna 2 a 7) sendo que a primeira coluna identifica o produto.
Produtos. ECKH $= -- Representação de um pedido de calçados em 3D Com a Figura 6, representamos em 3D os mesmos dados do pedido projetado na Figura 5, através da abordagem de coordenadas paralelas.
Conforme pode ser observado, mesmo com esta nova representação a dificuldade para extrair informação dos dados, continua existindo.
Para dimensões a que possamos representar do estes dados de multidimensionais paralelas, em três partir da generalização conceito coordenadas torna- se importante o tratamento dos dados através de um processamento prévio dos mesmos no sentido de criarmos condições para uma representação mais harmoniosa e que nos possa transmitir a informação que estamos buscando.
Para tanto, entendemos ser necessário nos valer de algumas abordagens já sedimentadas em outras áreas, principalmente a estatística.
Passam a ser importantes algumas premissas básicas no sentido de analisarmos e identificarmos, por exemplo, similaridades, padrões, associações, relações, correlações e proximidades entre as tuplas, bem como entre os dados que as compõem para que possam ser encontrados os padrões de comportamento entre o conjunto de dados que está sendo trabalhado.
Em um segundo momento as tuplas e eixos serão classificados, ou seja, tornase necessário o seqüenciamento entre eles a fim de que os possamos manipular ordenadamente, tornando assim sua representação em três dimensões visualmente mais harmoniosa nos permitindo, desta forma, antever a possibilidade de projetarmos tendências futuras a partir de os dados utilizados.
Para atingir estes objetivos, utilizamos métricas no sentido de encontrar distâncias entre as tuplas e os dados, por semelhança, proximidade, associação, enfim uma métrica que nos permita identificar o grau de relacionamento existente entre os dados que estão sendo analisados.
Quando Vale aqui ressaltar que aspectos deste em tipo duas tornam- se dimensões, desnecessários estamos trabalhando com representação porém passam a ser bastante importantes quando os dados são representados em três dimensões.
A representação em três dimensões passa a se tornar viável no momento em que as tuplas e os dados tenham sido submetidos aos processos descritos e aplicados os conceitos desenvolvidos neste trabalho.
Em o principalmente próximo no capítulo analisamos métodos utilizados por a e estatística análise de manuseio, agrupamentos, classificação, aproximações variáveis multidimensionais.
É importante um estudo também nesta área relaciona- se precisamos diretamente nos apoiar com no assunto objeto deste trabalho, no já uma vez que de o qual e decorrer alguns desses métodos/ conceitos desenvolvidos sedimentados.
Como a estatística é a ciência que procura encontrar relações entre indivíduos de um mesmo grupo, ou em grupos distintos,.
Nos apoiaremos em ela para estabelecer um rápido panorama em relação a o tratamento de variáveis multidimensionais.
No caso de uma única variável ter sido medida em espécimes de uma amostra, a análise de tais dados é chamada de univariada.
Se porém valores de diversas variáveis forem obtidas em cada um dos espécimes dessa mesma amostra, as técnicas para a análise desses dados são chamadas de multivariadas ou multidimensionais.
Tal análise de mensurações múltiplas efetuadas sobre uma amostra fornece um melhor entendimento na razão direta do número de variáveis utilizadas e permite considerar, simultaneamente, a variabilidade existente nas diversas propriedades medidas.
Ainda, segundo Landin, a estatística de dados multidimensionais ajuda o pesquisador a resumir dados e reduzir o número de variáveis necessárias para descrevêlas.
A estatística de variáveis multidimensionais normalmente é empregada para:
A aproximação estatística pode ser usada entre um grande número de variáveis e explicar as dimensões subjacentes comuns destas variáveis.
Além de cumprir as metas de descobrir a estrutura subjacente e redução de dados, a escala de multidimensional espaço podemos de dados (MDS) que proporciona, facilitar «um a para o investigador, e pode de uma representação relações.
Então, definir conjunto multivariado métodos estatísticos para calcular os parâmetros e avaliar o ajuste de vários modelos de distância de espaço para proximidade de dados».
A expressão análise de correlação multivariada ou correlação múltipla é utilizada quando mais de duas variáveis estão envolvidas.
A análise de correlação multivariada é um conceito mais amplo do que a múltipla correlação.
Como o próprio nome já diz, este tipo de correlação está envolvendo a relação existente entre diversas variáveis.
O pré-requisito para determinarmos o relacionamento entre duas variáveis é obter as medidas num conjunto de objetos que digam respeito a estas duas variáveis.
Estendendo este conceito para correlação multivariada mostramos com a Tabela 2, o formato necessário numa coleção de dados para avaliarmos o grau de relacionamento ao longo de estas variáveis, denominando a cada conjunto de dados dos objetos (tuplas) como variáveis,.
Nn Objeto 2 Objeto 3 Dados observados:
Cada medida numa variável Objeto m Em o exemplo, os objetos podem representar um conjunto de cidades as variáveis poderiam incluir índices criminais, índices de desemprego, população, níveis de chuva.
Poderiam também os objetos estarem representando itens (produtos) de um pedido de calçados e as variáveis armazenando a identificação do produto, a cor, as quantidades de pares comprados por tamanho, etc..
Em os exemplos citados, a lista poderá ser estendida como quisermos.
Entretanto, quando coletamos dados, nós normalmente o fizemos com um propósito.
Mesmo com a possibilidade de calcularmos o grau de relacionamento entre todas as variáveis que fazem parte dos nossos objetos, normalmente estamos preocupados em encontrar o grau relacionamento entre alguns conjuntos destas variáveis.
De No caso de o pedido de calçados, por exemplo, poderíamos num determinado momento apurar o grau de relacionamento entre os produtos e a cor, em outro momento, entre a cor e a quantidade total de pares do item, num terceiro momento poderíamos estar interessados em calcular o grau de relacionamento entre as quantidades de pares por número.
Destacamos que esta última alternativa estará sendo utilizada para a demonstração da metodologia e cálculos no decorrer deste capítulo.
Especificamente, neste trabalho, utilizamos para nossas demonstrações modelo de pedidos de calçados onde, neste caso, nossos objetos são tratados como itens de pedido (tuplas) enquanto as variáveis X, representarão a quantidade de pares de cada tamanho produzido do produto (dimensões ou eixos).
Segundo Triola, entende- se por centróide o ponto de dados emparelhados c) Métodos por similaridade mútua:
Procuram agrupar observações que tenham uma similaridade comum com outras observações.
Inicialmente uma matriz n x m de similaridades entre todos os pares da observação é calculada.
Em seguida, Colunas as similaridades entre colunas de são repetidamente recalculadas.
Tenderão representando membros um único agrupamento apresentar membros.
Métodos por agrupamentos hierárquicos:
São as técnicas mais comumente usadas.
Para o seu desenvolvimento parte- se de uma matriz simétrica de coeficientes de associação entre itens e para a combinação dos mesmos, segundo níveis hierárquicos de similaridade, utiliza- se de um procedimento aglomerativo de tal modo que cada ciclo de agrupamento obedeça a uma ordem sucessiva no sentido do decréscimo de similaridade.
Embora diversas medidas de similaridade tenham sido propostas, somente duas são geralmente usadas:
Coeficiente de correlação e coeficiente de distância.
O coeficiente de correlação mede o grau de associação entre valores por a representação de pontos num sistema de coordenadas e suas respectivas posições em relação a uma linha reta.
O coeficiente de distância expressa o grau de similaridade como distância num espaço multidimensional.
Quanto maior a distância, maior o grau de similaridade e viceversa.
Baseados pesquisadores controversos exatamente e estes pouco nestas abordagens de concluímos que, se para e a estatísticos e métodos agrupamento, para o seu classificação temos similaridade impressão de são ser contribuem trabalho, vez este o ponto inicial deste projeto uma que trabalhando com variáveis multidimensionais estas deverão, de alguma forma, passar por um processo de classificação prévio antes de sua visualização.
Levando ainda em conta as considerações de Landin e Davis adotamos, para o nosso trabalho, esta linha de raciocínio, ou seja, entendemos que o método por agrupamentos hierárquicos através do coeficiente de correlação de Pearson conforme descrito acima, melhor se ajusta às nossas necessidades.
A escolha desta metodologia está centrada, principalmente na necessidade de, num primeiro momento, agrupar as tuplas por similaridade e, num segundo momento ordenar- las para que as possamos projetar em profundidade agregando, desta forma a terceira dimensão ao conceito de coordenadas paralelas.
Descartamos o coeficiente de distância tendo em vista que seu método não se adapta as nossas necessidades uma vez que as medidas calculadas são exatamente o oposto das que estamos procurando.
Considerando os aspectos acima abordados, passa a ser de fundamental importância o tratamento estatístico dos dados no decorrer de o processo de sua visualização.
O estudo da correlação tem por objetivo medir e avaliar o grau de relação existente entre duas variáveis aleatórias.
Assim como a média serve para determinar uma medida de tendência central e o desvio-padrão serve para determinar uma medida de dispersão, o coeficiente de correlação () é uma medida que, através de um único número, identifica o nível de correlação entre duas variáveis.
Um coeficiente de correlação não é, necessariamente, uma medida de casualidade, mas serve para indicar a força de uma relação.
O coeficiente de correlação pode variar entre 1,0 e 1,0.
Em assim sendo, podemos medir a relação entre duas variáveis determinar se é forte, fraca ou nula, através da disposição dos pontos em torno de uma reta.
O sinal do coeficiente de correlação indica a direção da relação entre:
E, enquanto o valor absoluto do coeficiente indica a extensão da relação.
Observa- se com a Figura 7, que embora representando tuplas com valores não próximos entre si, a similaridade de comportamento entre ambas é idêntico, caracterizando desta forma um alto (e positivo) coeficiente de correlação.
Deduz- se que o coeficiente de correlação é uma descrição sumária da extensão da associação linear sistemática entre valores de duas variáveis.
Se todos os valores das variáveis satisfazem exatamente uma equação, diz- se que elas estão perfeitamente correlacionadas ou que há correlação perfeita entre elas.
Uma das grandes vantagens do coeficiente de correlação é a facilidade com que podem ser relacionadas variáveis em escalas completamente diferentes e em diferentes unidades.
Procuramos, com a Figura 9, exibir exemplos de representação gráfica para diferentes coeficientes de correlação entre variáveis.
Desde que o coeficiente de correlação H é uma medida da relação linear entre duas variáveis, sua definição leva em conta cada par de valores,.
Baseados na definição de H e tomando como base o modelo de pedido de calçados, demonstramos a seqüência dos cálculos necessários para encontrarmos, por exemplo, o coeficiente de correlação entre as quantidades de pares dos tamanhos 33 a 38 para a amostra de 10 produtos.
Para tanto, devemos calcular os coeficientes de cada tamanho em relação a os demais, ou seja, o tamanho 33 em relação a os tamanhos 34, 35, 36, 37 e 38, representado na Figura 10.
O passo seguinte é calcular os coeficientes do tamanho 34 em relação a o 33, 35, 36, 37 e 38, repetindo o processo até o final da seqüência de tamanhos.
Para o cálculo do coeficiente de correlação, que segundo Johnson e Triola, é uma medida de relacionamento linear entre um par de valores, podemos definir a seqüência de cálculo da seguinte forma:
Onde: Como o coeficiente de correlação indica o grau de similaridade entre conjuntos de variáveis, por exemplo, e é calculado através da divisão do valor da covariância pela raiz quadrada do produto dos desvios-padrão dos conjuntos de dados a e b, para isto devemos, num primeiro momento, calcular a média e a variância da quantidade de pares de cada numero (Tabela 3) para, em seguida, calcular os coeficientes de correlação.
Como podemos trabalhar com um grande número de variáveis, gerando desta forma também uma grande quantidade de coeficientes de correlação, torna- se conveniente sistematizar a organização destes coeficientes.
Isto é feito através da matriz de correlação conforme exemplo apresentado na Tabela 4.
Esta matriz representa um arranjo tabular de todos os coeficientes de correlação que possam ser calculados a partir de todos os pares de valores representados por as variáveis Nn.
Destacamos que a matriz com os índices de correlação entre os elementos é quem vai nos permitir, no próximo passo, a definição da melhor seqüência para a visualização das coordenadas.
Em a matriz de coeficientes, estes representam o grau de semelhança entre pares de objetos e os mesmos deverão ser arranjados de acordo com os respectivos graus de similaridade de modo a ficarem agrupados segundo uma disposição hierárquica.
Os resultados quando organizados em gráfico mostrarão as relações entre os dados.
Destaca- se representa a variável que toda a matriz de correlação quadrada.
N, a segunda linha representa a N variável e assim sucessivamente.
Similarmente, a primeira coluna da matriz também representa a variável coluna representa a variável a segunda, seguindo assim até a coluna final.
Cada célula da matriz é ocupada por um coeficiente de correlação entre as variáveis representadas por uma linha e coluna em particular que esta célula ocupa.
Podemos, a partir de as matrizes apresentadas na Tabela 5 e Tabela 6, observar duas características importantes:
Coeficientes diagonais:
Os coeficientes da diagonal principal, destacados na matriz, apresentam todos o valor 1 (correlação perfeita), e representam o coeficiente de correlação de uma determinada coluna com si mesma.
Simetria: A matriz de correlação apresenta simetria a partir de sua diagonal, ou seja, ji.
Considerando- se por exemplo, a Tabela 7, o coeficiente das células superior direita e inferior esquerda, ambas apresentam o coeficiente 0,4597.
Entende- se que este espelhamento deva existir uma vez que, o primeiro caso representa a correlação entre as variáveis enquanto que no segundo caso, representa a correlação entre as variáveis Observando- se cada um dos outros pares correspondentes de variáveis, acima e abaixo de a linha diagonal, veremos que todos seguem o mesmo princípio.
O número de correlações:
A simetria na matriz de correlações resulta num substancial grau de redundância.
Podemos então descartar a porção da matriz que se encontra acima ou abaixo de a diagonal e não perderemos nenhuma informação.
Podemos também descartar a diagonal uma vez que sabemos que o coeficiente de correlação de uma variável com ela mesma é igual a um.
Com a Figura 13 apresentamos um procedimento fácil para determinar o número de coeficientes.
Simplesmente devemos descartar as células da diagonal da matriz e dividir as células restantes ao meio.
Por exemplo, numa matriz 4 x 4, representando a correlação entre 4 variáveis, teremos um total de 16 células.
Eliminando as células da diagonal, termos na como parte resultante superior um da total de e 12 a células, outra a metade na destas parte estão representadas diagonal metade inferior.
Apesar de o fato de que o conjunto de coeficientes acima e abaixo de os elementos diagonais estejam duplicados, torna- se interessante os manter na apresentação da matriz, uma vez que a leitura da mesma fica mais fácil de ser feita.
A seqüência de passos descrita neste capítulo está diretamente relacionada com a montagem da matriz de correlação, matriz necessária para sistematizar os coeficientes de correlação entre os elementos que estão sendo trabalhados para, num segundo momento definir a melhor seqüência das coordenadas e, finalmente, representar os dados em 3D.
Descrevendo genericamente o algoritmo utilizado para a solução do problema proposto podemos dizer que procuramos seqüenciar eixos e tuplas maximizando os coeficientes de correlação entre os elementos da seqüência, para isto utilizando como base a matriz de correlação.
Em um primeiro momento representamos, com a Figura 14, um esboço dos módulos que compõem o protótipo desenvolvido.
Em o esboço, os programas estão Originais), em forma de matriz com as tuplas a -- Tuplas serem processadas e visualizadas.
O primeiro registro do arquivo contém a quantidade de tuplas que fazem parte do arquivo e a quantidade de elementos em cada tupla.
Este programa calcula e gera um arquivo (os eixos) com os coeficientes de correlação Coeficientes de correlação entre as colunas (eixos) da matriz lida, entre conforme demonstrado na Tabela 7.
ProgramaP2 Encontra a melhor seqüência dos eixos:
Este programa procura, a partir de o arquivo()) com correlação, os coeficientes da melhor seqüência para visualização das colunas, agrupando os coeficientes em ordem decrescente.
Em feito isto, gera um arquivo com a melhor seqüência (Seqüência para visualização dos eixos).
Programa 2!
Calcula os coeficientes de correlação entre os eixos:
Os procedimentos deste programa são idênticos aos descritos no programa P1, ou seja, faz também a leitura do arquivo texto (Tuplas Originais), em forma de matriz com as tuplas a serem processadas e visualizadas.
A diferença está em relação a a matriz a ser trabalhada, este programa processa uma matriz transposta em relação a a trabalhada no programa P1.
Desta forma, calcula e gera um arquivo com os coeficientes de correlação entre as tuplas da matriz lida (demonstrado na Tabela 10.
Coeficientes de correlação entre as tuplas), conforme ProgramaP4 Encontra a melhor seqüência das tuplas:
Este coeficientes de programa correlação repete os procedimentos por o programa do P3, programa arquivo P2, agora com os gerados A4, agrupando estes coeficientes em ordem decrescente.
Em feito isto, gera um arquivo, com a melhor seqüência para visualização das tuplas ()&amp; Seqüência para visualização das tuplas).
ProgramaP5 Gera arquivo para visualização dos eixos e tuplas:
Este programa gera um arquivo no formato apropriado do programa de visualização (Gnuplot) com a seqüência correta das tuplas, eixos e seus devidos valores a serem por ele processados (Seqüência para visualização eixos e tuplas).
Para isto, utiliza- se do arquivo inicial A1, bem como dos arquivos A3 e A5 com a melhor seqüência para a visualização das colunas e tuplas.
Programa P6 Gnuplot:
A partir de o arquivo (A6), encarrega- se de visualizar os dados iniciais já processados e ordenados.
Para este procedimento nos utilizamos do programa Gnuplot.
Os demais programas descritos foram desenvolvidos em Pascal.
A divisão do protótipo numa quantidade maior de programas quando, em princípio, poderíamos ter- lo desenvolvido em menos unidades, deveu- se aos seguintes fatos:
Exemplificando, podemos a qualquer momento visualizar em 3 D:
As tuplas originais, sem nenhum processamento -- Figura 15;
somente os eixos ordenados como apresentado na Figura 16;
somente as tuplas ordenadas;
Com as eixos e tuplas ordenadas -- Figura 18.
Em as seções que se seguem procuramos identificar algumas características e funcionalidades do protótipo desenvolvido.
Para tanto, apresentamos uma descrição detalhada do protótipo apresentado na Figura 14.
Utilizamos neste detalhamento o modelo de pedido utilizado como base neste trabalho e representado através da Tabela 1.
Em um primeiro momento, trabalhamos com as colunas (numeração dos calçados) onde são calculadas a média e a variância da quantidade de pares de cada numero, em seguida, são calculados os coeficientes de correlação (H) entre as colunas.
Em este caso devemos calcular o coeficiente de correlação de cada uma das colunas em relação a as demais tendo, como produto final, a matriz de correlação.
Em a matriz de correlação trabalhamos com todos os pares de variáveis e calculamos um coeficiente de correlação para cada par.
Com a Tabela 7, representamos a matriz de correlação calculada a partir de as colunas (tamanhos) do modelo de pedido.
Referenciando ao contexto da seção 5.2 -- Análise de agrupamento, uma vez calculada e montada a matriz de correlação torna- se necessária a sua ordenação (decrescente) através dos coeficientes de correlação encontrados, Em esta ordenação, além de os coeficientes, deve- se também levar em conta a sua localização na matriz (linha e coluna), uma vez que estes elementos serão necessários quando de o processo para definição da ordem de visualização das coordenadas.
Em vista de isto apresentamos, na Tabela 8 os coeficientes de correlação constantes da matriz de correlação apresentada na Tabela 7 já ordenados, representando desta forma, o resultado final deste processo.
A definição da seqüência para visualização das coordenadas (seleção dos eixos).
Uma vez ordenados os fatores de correlação, devemos encontrar a melhor seqüência para a disposição dos eixos no gráfico, no sentido de tornar a visualização das tuplas a mais harmoniosa possível.
Este procedimento é efetuado tendo- se como ponto de partida a lista ordenada de índices gerada a partir de a matriz de correlação.
O processamento é feito linha a linha da lista procurando por a melhor seqüência a ser utilizada para a visualização das tuplas e eixos, colocando lado a lado aqueles que tenham afinidade entre si (por proximidade ou vizinhança), ou seja, compartilhem linha ou coluna.
Exemplificando, num determinado momento, será lido um par da lista que estaria representado por as variáveis, par representando as variáveis i, j num segundo momento poderíamos ler outro j, k com estas duas seqüências estaria caracterizada a proximidade entre as variáveis que poderiam passar a ser representadas na seqüência i, j, k.
Em este ponto encontramos uma afinidade entre o par lido neste momento (5,6), com a linha dois do momento 2, através do valor 5, podemos então colocar o segundo valor lido no final da linha 2, mantendo desta forma a afinidade entre todos os elementos.
Momento 4, leitura do par:
Este par deve ser ignorado no processamento, uma vez que tanto o primeiro valor quanto o segundo valor já se encontram numa mesma tupla.
Momento 5, leitura do par:
Em este ponto encontramos uma nova afinidade entre o par lido neste momento, com a linha 1 da matriz, através do valor 1, podemos então colocar o segundo valor lido no início da linha 1 mantendo desta forma a afinidade entre ambos.
Momento 6, leitura do par:
Com o par lido (2,6) podemos, neste ponto do processo, unir as duas linhas uma vez que passou a existir esta possibilidade tendo em vista a afinidade do primeiro valor lido (2) com o último valor da linha 2.
Observe- se que neste caso também passamos a ter uma afinidade entre o segundo valor lido com o primeiro valor da linha 1.
Esta é uma característica importante do algoritmo e que deve ser ressaltada uma vez que sempre, ou independente rejeitados do número foi o de pares do lidos e independente 4, todos os dos pares em ignorados como caso momento valores processamento serão apresentados numa única tupla, ordenados, com demonstrado, por suas proximidades.
Após trabalharmos com os eixos, todo sentido de repetir os procedimentos com as o processo (produtos) deverá para ser os retomado no também tuplas quais devemos calcular a média e a variância da quantidade de pares de cada produto e, em seguida, calcular os coeficientes de correlação (H) entre as tuplas.
Em este caso devemos calcular o coeficiente de correlação de cada uma das tuplas em relação a as demais tendo, como produto final, a segunda matriz de correlação.
Para este cálculo utilizaremos do mesmo pedido agora trabalhado como uma matriz transposta.
Como apresentado para a numeração dos calçados, a Tabela 9 nos mostra os valores da média e da variância para cada produto do pedido.
Variância Média na Tabela 10 apresentamos a matriz de correlação calculada a partir de as tuplas (produtos) do modelo de pedido representado na Tabela 1, utilizado como base para o desenvolvimento do trabalho.
Apresentamos, com a Figura 17, o gráfico em 3D do pedido original (dados brutos) com o intuito de permitir uma melhor avaliação do resultado do processo descrito neste capítulo.
Após o processamento do algoritmo apresentamos, com a Figura 18, a visualização final do pedido utilizado como modelo até este ponto do trabalho.
Entendemos que, mesmo com os dados utilizados para acompanhamento do desenvolvimento do raciocínio, é perceptível a alteração ocorrida na visualização dos dados.
Produtos. Apresentamos com este capítulo os experimentos feitos com protótipo desenvolvido no sentido de documentar e validar os algoritmos propostos.
Para tanto, nos utilizamos de conjuntos de dados conforme detalhado em cada experimento.
Procuramos utilizar para os testes dados de diferentes origens e formatos onde, para o experimento 1, utilizamos números reais gerados através de algoritmo, tratando- se portando de dados fictícios.
Em o experimento 2 utilizamos dados reais que representam a quantidade de veículos, por tipo, distribuídos nos 36 municípios do Vale do Rio dos Sinos.
Para o experimento 3 foram utilizados dados também reais e provenientes da Universidade do Arizona que, a partir de uma pesquisa em 40 residências mensurou, por tipo, a quantidade de lixo descartada durante uma semana.
Com o quarto e último experimento utilizamos como base as médias finais por disciplina de uma turma de alunos do ensino médio, tratando- se portanto de dados também reais.
Dados para teste gerados através de algoritmo.
Análise dos gráficos:
Dados originais em coordenadas paralelas.
Tendo em vista a quantidade de tuplas que estão sendo projetadas observa- se com a Figura 19 uma representação bastante comprometida, não nos permitindo extrair facilmente informação.
Valores Eixos. ECKH $ ' -- Experimento 1 -- Dados originais em coordenadas paralelas Dados originais em 3D.
Pode- se notar que a Figura 20, gerada a partir de os dados originais, nos apresenta um gráfico a partir de o qual é possível extrairmos mais informação.
Com os dados neste formato a plotagem apresenta uma similaridade, nota- se de uma evolução cíclica dos mesmos.
Dados ordenados em 3D.
Tendo em vista a dificuldade de representarmos no gráfico a legenda identificando todas as tuplas, na Figura 21 estamos apresentado- as em intervalos.
Porém, para efeitos de clareza, a seqüência completa para a visualização é:
A partir de os dados processados já é possível identificarmos alguns fenômenos bastante interessantes, os quais comentamos a seguir:
As coordenadas nos oito eixos tem basicamente o mesmo comportamento, nota- se claramente que as tuplas ocorrem em pares praticamente iguais, que se repetem ao longo de a representação.
Imaginando- se um corte longitudinal, no gráfico dos dados não processados é possível chegar a esta conclusão, porém com os dados agrupados torna- se bem mais simples a identificação deste fenômeno.
Em relação a o fenômeno acima, e observando- se mais detalhadamente a seqüência identificar da que representação estes pares das tuplas, é possível a quase seguem uma tendência periódica, totalidade de eles mantém uma evolução uniforme de 21 unidades.
Com este fato novo poderíamos reconhecer que a cada 21 unidades de tempo os dados tendem a repetir- se.
Conclusão da análise:
Com este experimento, entendemos que o agrupamento efetuado por o algoritmo funcionou e que tenha atendido as expectativas.
Fazendo referência ao embasamento teórico, a partir de a periodicidade observada, torna- se perfeitamente possível projetarmos para o futuro uma nova ocorrência do fato.
Número de veículos em 1998 na região do Vale do Rio dos Sinos, classificado em ordem alfabética de município.
Análise dos gráficos:
Dados originais em coordenadas paralelas.
Em esta representação (Figura 22) deixamos de utilizar uma área bastante grande do gráfico, tendo em vista a escala para a representação dos automóveis ser bastante desproporcional em relação a escala para representar, por exemplo os ônibus e microônibus.
Este fato faz com que a representação dos dados fique comprometida.
Dados originais em 3D.
A Figura 23 plotada a partir de os dados originais nos apresenta um gráfico com os maiores valores representados de maneira bastante esparsa porém mais inteligível comparando- se com a representação em coordenadas paralelas.
A o representarmos os dados em problema da disparidade entre os números destacado na representação anterior continua e parte do gráfico é representado num nível mais baixo (achatado), fato este que depende apenas dos dados de entrada, e não do algoritmo de visualização.
Tipo s de ve ículo Município s!
Experimento 2 -- Dados originais em 3D Dados ordenados em 3D.
Embora a visualização tenha melhorado (Figura 24), observa- se que o problema da disparidade entre os números destacado na representação em coordenadas paralelas e também com os dados originais em 3D mantém- se para esta representação.
A diferença que pode ser observada é que os dados deixaram de estar tão dispersos e passaram a ser representados em grupos mais ou menos organizados.
Em o sentido de demonstrarmos alternativas disponibilizadas por a ferramenta de visualização, apresentamos, com a Figura 25, exemplo de rotação do gráfico.
Tendo em vista a dificuldade de representarmos no gráfico a melhor seqüência das tuplas identificando os municípios, na Figura 24 deixamos de apresentar- las.
Porém, para efeitos de clareza dos dados, a seqüência completa para a visualização dos mesmos, na ordem é:
Conclusão da análise:
Observamos que o algoritmo melhorou a visualização no sentido de que tornou mais harmoniosa sua representação, porém não corrige o problema da disparidade dos valores apresentados.
Este aspecto poderia ser resolvido através de um pré-processamento dos dados, que eliminaria o efeito apresentado.
Fica portando ao arbítrio do usuário a decisão de fazer ou não este pré-processamento.
Universidade do Arizona. Conteúdo:
Pesos (em libras) de resíduos descartados por residência numa semana.
Análise dos gráficos:
Dados originais em coordenadas paralelas.
Com a representação através deste método pode- se observar, a partir de a que uma grande quantidade de tuplas encontra- se sobreposta na projeção.
Algumas poucas, com valores mais elevados destacam- se, podendo ser identificadas.
Tipos de lixo -- Experimento 3 -- Dados originais em coordenadas paralelas Dados originais em 3D.
A Figura 27 nos apresenta um gráfico em o qual encontramos dificuldades em extrair alguma de informação.
Valores que Podemos estão acompanhar, de maneira e se razoável, ao a primeira da seqüência sendo representados referem tamanho residência analisada, mas em relação a os demais dados, não nos é possível fazer qualquer análise mais específica devido a o acúmulo e sobreposição de linhas em sua representação.
Dados Ordenados em 3D.
Mesmo tendo melhorado a visualização de forma significativa com a representação da Figura&amp;, continuamos com dificuldade para extrair algum fato novo.
Um dos motivos para isto pode ser atribuído à não familiaridade com os dados que estão sendo representados.
Porém, utilizando os recursos da ferramenta de visualização efetuamos uma pequena rotação no gráfico, conforme nos mostra a Figura 29, no sentido de, quem sabe, por outro ângulo, descobrir algo interessante.
A partir deste fato, realmente passamos a observar alguns aspectos até então escondidos na representação anterior em função de o ângulo de visão que tínhamos da mesma.
A seguir citamos alguns destes aspectos:
As residências de maior tamanho mantém um padrão de resíduos de alimentos compatível com aquelas com menor número de moradores;
Algumas poucas residências mantém um volume bastante diferenciado das demais no que se refere a resíduos de jardinagem, apesar tamanho da moradia manter- se praticamente na média das demais.
Os resíduos de vidro mantém, para a quase totalidade das residências um valor significativo, independendo do tamanho da residência É surpreendente a quantidade residual de papéis e a pouca influência que sofre em função de o tamanho da residência.
Para efeitos de clareza dos dados, a seqüência completa para a visualização das residências é, na ordem:
Conclusão da análise:
Observamos que o algoritmo melhorou a visualização dos dados, porém, no momento em que rotacionamos o gráfico passamos a observar de maneira mais clara novos fatos que nos permitiram tirar conclusões apesar de estarmos mantendo, pela primeira vez, contato com estes dados.
Com este experimento comprovamos mais uma vez que o método utilizado é superior explique ao método tradicional de coordenadas paralelas, embora parte deste fato se por as facilidades manipulação da representação disponibilizadas por a ferramenta utilizada para visualização dos dados.
plotada a partir de os dados originais nos apresenta um gráfico com os principais valores representados de maneira bastante irregular porém identificando claramente os alunos com médias maiores e menores.
Alunos Inglês Biologia Química Física Matemática Português História.
ECKH $ ! Experimento 4 -- Dados originais em 3D Dados Ordenados em 3D.
A o representarmos os dados em 3D (Figura 32), o problema da representação irregular entre os números destacado na representação anterior desaparece, suavizando, em parte a sua representação.
Embora a visualização tenha melhorado observamos dois aspectos que chamam a atenção:
Por a representação da Figura 32, as duas últimas disciplinas que estão sendo mostradas parecem ser as que possuem as melhores médias, porém, identificando- as veremos que tratam- se de Física e Matemática, disciplinas estas que normalmente fazem parte daquelas com menores médias em qualquer classe escolar, principalmente no ensino médio.
Procurando identificar a origem desta contradição, rotacionamos o gráfico e com esta transformação (conforme nos mostra a Figura 33), passamos a observar uma parte da de as médias, principalmente de Física e Matemática que, por estarem encobertas na primeira representação, nos levam a uma falsa impressão de médias altas.
Com este procedimento, pode- se observar que as médias destas disciplinas são realmente mais baixas.
O gráfico nos apresenta alguns alunos (identificados na Figura 32), que possuem notas parecidas e que esperaríamos estar com suas tuplas sendo projetadas próximas uma da outra, o que não aconteceu.
A explicação para este fato está diretamente relacionada à maneira de medir as distâncias e similaridades (as métricas) entre os objetos.
Como nosso algoritmo utiliza a métrica do coeficiente de correlação que aproxima os objetos por a similaridade das suas flutuações, constata- se uma maior similaridade, neste caso, entre alunos com notas consideradas boas e alunos com notas não tão satisfatórias.
A partir de estas constatações, deduzimos que para os dados que estão sendo trabalhados a métrica utilizada no algoritmo pode não ser a melhor.
Procuramos, a partir de esta situação, utilizar recurso disponível no protótipo que, por ter sido desenvolvido em módulos conforme referenciado na seção 6.1, nos permite, com pouco esforço, utilizar outras métricas para calcular as distâncias entre os dados.
Utilizaremos inicialmente como métrica o desvio-padrão, calculado para cada uma das disciplinas.
Com a Figura 34, representamos os dados ordenados por o desviopadrão e pode- se notar que esta representação já nos mostra uma maior harmonia entre os dados.
Destacamos que nesta figura as disciplinas estão representadas em ordem crescente do desvio padrão de cada uma de elas.
Desta forma, observa- se que a disciplina de Geografia é a que representa o menor desvio-padrão, enquanto a que tem o maior valor está sendo representada por a disciplina de Matemática.
Conclusão da análise:
A partir de a primeira constatação, concluímos que encontramos uma dependência dos dados em relação a a forma de representar- los.
Como a ferramenta de visualização nos permite ver os dados em diversos ângulos, caberá ao usuário escolher aquele que melhor se adapte a cada situação.
Em métrica relação à para segunda se constatação, houve possível a necessidade informação de alterarmos clara a da utilizada que tornasse extrair mais representação.
Isto nos leva novamente a concluir que um processo de visualização é dependente dos dados que estão sendo utilizados.
Comprova- se assim conforme Freitas que o processo de análise de dados é mais facilmente efetuado sobre representações gráficas das características e propriedades das entidades em estudo.
Em determinadas situações podemos ser forçados a explorar os recursos da ferramenta de visualização (como por exemplo:
Rotacionar o gráfico) e em outros momentos talvez seja necessário utilizar métricas diferentes procurando aquela que melhor se ajuste aos dados.
Conforme proposta inicial, procuramos com este trabalho desenvolver apresentar uma nova abordagem para a visualização de variáveis multidimensionais.
A partir de uma generalização da técnica de coordenadas paralelas, agregamos a possibilidade da representação de dados multidimensionais em três dimensões.
Procuramos demonstrar através do embasamento teórico, da descrição do algoritmo e dos exemplos apresentados que o método escolhido funciona bem, e que a transformação ocorrida na representação dos dados é significativa obtendo- se resultados que podem ser considerados satisfatórios.
Por ser um processo de mudança de representação simples e natural, a transformação de duas para três dimensões é fácil de ser assimilada e, com um mínimo de observação, os dados passam a ser identificados na nova representação.
Consideramos este fato como mais um aspecto favorável à metodologia uma vez que os resultados poderão ser analisados até por usuários com pouca experiência nesta área.
Ressaltamos que esta facilidade em analisar a representação por usuários com pouca experiência foi uma das proposições iniciais do trabalho.
Identificamos, no decorrer de as pesquisas a necessidade de encontrar soluções para a reorganização dos dados, sem o qual este processo de mudança de representação não seria tão útil.
Este processo de organização prévia dos dados torna- se necessário para a representação em três dimensões uma vez que as tuplas e eixos originais não estão necessariamente numa seqüência que produza superfícies suaves e de melhor reconhecimento.
A reorganização tenta, na medida das possibilidades, agrupar os dados similares (tanto reordenando tuplas quanto eixos) procurando, desta forma, obter uma superfície tão regular quanto possível, a fim de que o usuário não venha a ter dificuldades na percepção do comportamento dos dados.
Para resolver este problema procuramos embasamento na estatística no sentido de encontrar uma técnica que nos permitisse agrupar estes dados.
Passamos a utilizar no desenvolvimento do trabalho o método de classificação através do coeficiente de correlação, que é uma medida que identifica o nível de correlação entre duas variáveis.
A partir de a definição do método de reorganização, nos dedicamos ao algoritmo final, que contempla aspectos relacionados principalmente ao processo de organização dos eixos e tuplas, além de gerar automaticamente os dados para a ferramenta de visualização, no caso o gnuplot.
É interessante notar que a representação em 3D disponibiliza uma gama maior de alternativas e recursos dinâmicos ao usuário final da ferramenta através da manipulação da representação.
Com estas disponibilidades a visualização passa a ser utilizada como um mecanismo mais rico do que as coordenadas paralelas originais, pois se oferecem, por exemplo, facilidades de rotação e manipulação.
Isto fornece uma melhor capacidade de compreensão da informação, onde o usuário pode escolher a representação apropriada ao seu sistema de percepção visual, aumentando substancialmente sua capacidade para extrair informações pertinentes.
Por outro lado, na representação em coordenadas paralelas os dados são essencialmente estáticos não permitindo qualquer interferência do usuário final, que deixa de interagir com a ferramenta de visualização, reduzindo desta forma drasticamente sua capacidade de percepção.
Foi possível comprovar, através dos experimentos utilizados com a ferramenta, que Miladinov está correto ao afirmar que como o produto final de um sistema de visualização é uma imagem, sempre existirá uma relação de dependência entre os dados a serem visualizados, sendo este um dos motivos por os quais a visualização científica funciona caso a caso, ou seja, para diferentes conjuntos de dados teremos diferentes representações onde, para algumas representações, poderemos ter maiores ou menores facilidades na interpretação do gráfico e a eficiência do processo está diretamente relacionada com os dados que estão sendo utilizados.&amp;&amp;&amp;
Em este ponto se faz necessário referenciarmos alguns aspectos que puderam ser observados no decorrer de o trabalho e dos experimentos efetuados com o protótipo desenvolvido que relacionamos a seguir:
Quando utilizamos uma quantidade maior de tuplas passamos a encontrar dificuldades para identificar no gráfico cada uma das ocorrências, isto porém não é um caso particular.
Trata- se, isto sim, de um problema comum a este tipo de representação.
Em um conjunto de dados onde as escalas não sejam homogêneas, representação em coordenadas paralelas nos apresenta uma sobreposição de linhas enquanto que na representação em 3D com os dados ordenados a representação tende a ser mais harmoniosa.
Utilizamos como método de classificação o coeficiente de correlação.
Se faz necessário um estudo a respeito de outros métodos no sentido de utilizar classificações diferenciadas dependendo das características dos dados.
Importante também utilização de outras ferramentas de visualização agregando novos recursos e efeitos visuais, como por exemplo, efeito de cores.
Cumpre- nos destacar que, devido restrições do protótipo desenvolvido, trabalhamos com conjuntos amostrais relativamente pequenos, porém isto já nos permite afirmar que a técnica mostrou- se eficaz com os experimentos apresentados.
Passa a ser interessante que se busque, em trabalhos futuros, implementar uma ferramenta genérica onde a partir de os dados de entrada, sejam utilizados módulos diversos onde cada módulo poderia ser responsável por a classificação destes dados em diferentes técnicas podendo gerar os dados de saída também em diversos formatos, formatos estes compatíveis com outras ferramentas de visualização para que o usuário possa decidir sobre a forma de representação que melhor se ajuste ao seu nível de percepção visual.
