MPSoCs (do inglês, Multiprocessor System On Chip) constituem uma tendência no projeto de sistemas embarcados, pois possibilitam o melhor atendimento dos requisitos da aplicação.
Isso se deve ao fato de que a arquitetura desses sistemas é composta por vários processadores, módulos de hardware dedicados, memória e meio de interconexão, fornecendo um maior poder computacional quando comparados a sistemas monoprocessados equivalentes.
No entanto, estratégias que possibilitem o aproveitamento da capacidade de processamento destas arquiteturas precisam ser mais bem entendidas e exploradas.
Para isso, é necessário dispor de infra-estruturas de hardware e software que habilitem gerenciar a execução de tarefas no MPSoC.
A partir de estas infra-estruturas deve ser possível, por exemplo, mapear tarefas dinamicamente nos processadores, balanceando a carga de trabalho do MPSoC através de estratégias de alocação dinâmica de tarefas.
O estado da arte da bibliografia no tema explora estratégias de alocação estática e dinâmica de tarefas sobre MPSoCs e avalia a viabilidade e eficiência das mesmas.
Contudo, a necessidade de criação das infra-estruturas de hardware e software para viabilizar a exploração destas estratégias constitui- se um gargalo no avanço desta tecnologia.
Adicionalmente, a maioria dos trabalhos utiliza plataformas modeladas em níveis muito abstratos de modelagem para avaliação das abordagens pesquisadas, reduzindo a confiabilidade dos resultados relatados.
A principal contribuição do presente trabalho é a proposta e implementação de uma plataforma MPSoC denominada HMPS (Hermes Multiprocessor System).
HMPS conta com uma infra-estrutura de hardware e uma infra-estrutura de software, capazes de gerenciar a execução de tarefas no sistema.
A plataforma HMPS é baseada em multiprocessamento homogêneo, e possui uma arquitetura de processadores mestre-escravo.
A plataforma utiliza como meio de interconexão uma rede intra-chip (NoC) e possibilita que tarefas possam ser alocadas estática e/ ou dinamicamente no sistema.
Com isso, várias estratégias de alocação distintas podem ser implementadas e avaliadas.
HMPS deverá ser um ponto de partida para vários trabalhos, contribuindo para a pesquisa na área de MPSoCs.
Este documento apresenta a proposta e a implementação da plataforma HMPS.
Para a infra-estrutura de hardware utilizou- se a NoC Hermes, desenvolvida por o grupo de pesquisa GAPH, e o processador de código aberto Plasma disponível no site OpenCores.
Módulos de hardware foram desenvolvidos e alterações no código do Plasma foram realizadas, visando conectar o processador à NoC e realizar a alocação de tarefas na memória do processador.
Para a infraestrutura de software, foi desenvolvido um microkernel multitarefa que executa em cada processador escravo e a aplicação de alocação de tarefas que executa no processador mestre.
São exploradas duas estratégias de alocação de tarefas:
Uma estática e uma dinâmica.
Palavras Chave: MPSoCs, multiprocessamento, multitarefa, NoC, alocação de tarefas.
Sistemas multiprocessados em chip (MPSoC, do inglês Multiprocessor System-on-Chip) são um dos recursos chave (uma das aplicações-chave) da tecnologia VLSI (do inglês Very LargeScale Integration) e uma tendência no projeto de sistemas embarcados.
Um MPSoC consiste de uma arquitetura composta por recursos heterogêneos, incluindo múltiplos processadores embarcados, módulos de hardware dedicados, memórias e um meio de interconexão.
Um número crescente de aplicações embarcadas possui requisitos estritos de desempenho, consumo de potência e custo.
Exemplos dessas aplicações são encontrados nas áreas de telecomunicações, multimídia, redes e entretenimento.
O atendimento de cada um desses requisitos separadamente é uma tarefa difícil e a combinação de eles constitui um problema extremamente desafiador.
MPSoCs possibilitam a adaptação da arquitetura do sistema de forma a melhor atender os requisitos da aplicação:
A atribuição de poder computacional onde é necessário possibilita atender a requisitos de desempenho;
A remoção de elementos desnecessários reduz o custo e o consumo de potência.
Isso mostra que MPSoCs não são apenas chips multiprocessadores, os quais apenas utilizam da vantagem da alta densidade de transistores para colocar mais processadores num único chip e sem visar as necessidades de aplicações específicas.
Ao contrário, MPSoCs são arquiteturas personalizadas que fazem um balanço entre as restrições da tecnologia VLSI com os requisitos da aplicação.
MPSoCs, além de núcleos, contêm um meio de interconexão.
Existem diferentes meios de interconexão em MPSoCs:
Conexão ponto a ponto, barramento único, barramento hierárquico e redes intra-chip.
O meio de interconexão mais tradicional é o barramento.
Contudo, não são escaláveis além de algumas dezenas de núcleos.
Uma moderna forma de interconexão de.
Essas redes vêm sendo pesquisadas com o intuito de resolver problemas relacionados à comunicação de dados entre os componentes do sistema.
De entre esses problemas, encontra- se a baixa escalabilidade e a ausência de paralelismo, uma vez que a interconexão através de um barramento compartilhado permite que apenas uma comunicação possa ser estabelecida num dado momento.
A motivação do presente trabalho reside na importância de as arquiteturas MPSoCs para sistemas embarcados atuais e na necessidade de desenvolver arquiteturas escaláveis, uma vez que a tendência de MPSoCs é que eles sejam um &quot;mar de processadores», isto é, uma arquitetura constituída por uma grande quantidade de núcleos interconectados por uma NoC.
Além disso, existe a necessidade do desenvolvimento de camadas de software básico para processadores embarcados que compõem o MPSoC, com a finalidade de gerenciar a execução das tarefas no sistema.
De essa forma, tarefas podem ser enviadas em tempo de execução para processadores que serão capazes de receber, armazenar, inicializar e executar as mesmas.
Essa abordagem possibilita uma flexibilidade na distribuição da carga nos processadores do MPSoC, de forma que os benefícios oferecidos por estas (e.
g paralelismo) possam ser melhor aproveitados.
Dado o contexto descrito nos parágrafos anteriores, este trabalho visa o desenvolvimento de uma infra-estrutura multiprocessada e multitarefa, que permita explorar a alocação dinâmica de tarefas.
Os núcleos desta infra-estrutura são interconectados por uma NoC.
Alocação dinâmica é o envio de novas tarefas a nodos escravos durante a execução do sistema.
Esse envio pode ser mediante decisão de um nodo mestre ou a partir de a requisição de novas tarefas por nodos escravos.
Todos os nodos que compõem a infra-estrutura são todos processadores idênticos, constituindo assim uma plataforma com processamento homogêneo.
Dentro de o contexto de alocação dinâmica está a migração de tarefas.
Este mecanismo suspende a execução de uma tarefa num nodo x e a envia para um nodo y, onde sua execução é retomada.
A migração de tarefas é um tema bastante explorado na área de Sistemas Paralelos e Distribuídos.
A aplicação desta abordagem em MPSoCs está sendo estudada de forma a empregar seus benefícios na área de sistemas embarcados.
A migração de tarefas pode ser justificável para otimizar o desempenho do sistema, considerando critérios como, por exemplo, carga de trabalho nos processadores e a carga da comunicação nos enlaces da rede.
A Figura 1 apresenta a arquitetura conceitual desta dissertação:
Um MPSoC com processamento heterogêneo interligado por uma NoC, onde um nodo mestre deverá alocar as tarefas no MPSoC (nodos escravos).
Um nodo escravo poderá ser um processador embarcado de propósito geral (CPU) ou um módulo de hardware reconfigurável.
De essa forma, as tarefas podem ser tanto de software (executarão em CPUs) ou de hardware (um bistream de reconfiguração parcial).
Cada nodo escravo componente do sistema deverá ser capaz de receber uma tarefa, armazenar todas as informações referentes a esta tarefa em memória e executar- la.
Para os processadores, faz- se necessário o desenvolvimento de uma camada de software, chamada microkernel (µkernel), que forneça os serviços básicos de um sistema operacional para o controle da execução das tarefas no ambiente multiprocessado e multitarefa.
Para os módulos reconfiguráveis, faz- se necessária a inclusão de um controlador de configuração e infra-estrutura para conexão destes módulos.
O escopo da presente dissertação é o desenvolvimento da infra-estrutura de hardware e de software para a inclusão dos processadores numa NoC.
Desta forma, ao final deste trabalho será obtido um MPSoC com processamento homogêneo, ou seja, todos os núcleos são processadores de propósito geral idênticos.
A inclusão de módulos de hardware reconfigurável ao sistema é tema de trabalhos futuros, já havendo diversos trabalhos desenvolvidos no grupo de pesquisa em o qual a presente dissertação insere- se:
Os objetivos são divididos em duas classes:
Objetivos estratégicos e objetivos específicos.
Os objetivos estratégicos compreendem:
Domínio da tecnologia de sistemas multiprocessadores em chip (MPSoC);
Domínio da tecnologia de redes intra-chip (NoC).
Os objetivos específicos compreendem:
Criação de um MPSoC homogêneo, composto por uma infra-estrutura de hardware e uma infra-estrutura de software;
Conexão do processador selecionado à Hermes, desenvolvendo- se a interface com a NoC (Ni ­ Network Interface) e as camadas básicas de comunicação (drivers);
Seleção de processador alvo a ser conectado na NoC Hermes;
Desenvolvimento de um microkernel multitarefa para ser executado em cada processador selecionado;
Dotar o microkernel com serviços para receber e executar novas tarefas, serviço este denominado no contexto do presente trabalho, de alocação de tarefas.
Ainda, são descritas as mudanças realizadas no processador e os mecanismos que foram implementados e agregados ao sistema de forma a atingir os objetivos almejados.
A infra-estrutura de hardware gerada nesta etapa é a primeira contribuição deste trabalho.
A alocação de tarefas é um problema clássico em sistemas multiprocessados e recentemente têm recebido atenção de vários pesquisadores no âmbito de MPSoCs.
Em a literatura, a alocação de tarefas é também referenciada como mapeamento de tarefas.
Alguns trabalhos utilizam alocação estática, a qual é desenvolvida em tempo de projeto e não muda ao longo de a execução do sistema.
Outros utilizam a alocação dinâmica, em a qual emprega- se uma alocação inicial com migração de tarefas e/ ou dados em tempo de execução.
A alocação estática é considerada atraente (do inglês, attractive) para sistemas em os quais a solução é computada uma única vez e é aplicada durante a vida inteira desses sistemas.
É o caso de, por exemplo, sistemas dedicados a um algoritmo específico para processamento de sinal (e.
g filtro de imagens) e aplicações multimídia.
Ruggiero et al.
Apresentam um ambiente para alocação de tarefas, onde uma plataforma MPSoC é utilizada.
O problema é dividido em dois subproblemas:
Alocação de tarefas em processadores e escalonamento de tarefas.
O subproblema da alocação é resolvido com Programação Linear Inteira (ILP, do inglês Integer Linear Programming) e é assim definido:
&quot;dado um grafo de tarefas, o problema consiste em alocar n tarefas em m processadores, de forma que a quantidade total de memória alocada, para cada processador, não exceda o tamanho de cada memória local».
O subproblema de escalonamento é resolvido através de Programação por convergindo a uma solução ótima.
Os grafos de tarefas são modelados como um pipeline, abordagem utilizada em certas aplicações multimídia, onde as tarefas ocorrem seqüencialmente no tempo.
A arquitetura alvo utiliza memória distribuída com troca de mensagens.
Os processadores são todos iguais (ARM7) interligados por barramento.
Cada processador tem uma memória local e pode acessar memórias remotas.
O sistema operacional utilizado é o RTEMS.
Um estudo de caso demonstra uma maior eficiência computacional da abordagem que utiliza a interação de ILP com CP em relação a as abordagens tradicionais (puramente ILP ou CP).
Virk e Madsen apresentam um ambiente para modelagem em nível de sistema de plataformas MPSoCs heterogêneas, cujo meio de interconexão de núcleos é uma NoC.
Este ambiente tem por objetivo desenvolver modelos para uma ampla classe de projetos e possui uma biblioteca de modelos de componentes abstratos contendo características físicas, de desempenho e de potência.
É necessária a modelagem do sistema e de todas as inter-relações entre os processadores, processos, interfaces físicas e interconexões.
Uma aplicação é composta de um conjunto de tarefas, cada uma de as quais podendo ser decomposta numa seqüência de segmentos.
Cada tarefa possui parâmetros, como deadline, período, tempo de troca de contexto, entre outros.
A plataforma multiprocessada é modelada como uma coleção de PEs (do inglês, Processing Elements) e dispositivos interconectados por um canal de comunicação.
Cada PE é modelado em termos de os serviços do SO fornecidos para as tarefas da aplicação.
Três serviços são modelados:
Escalonamento, alocação de recursos e sincronização.
A NoC é modelada como um processador de comunicação e um evento de comunicação é modelado como uma tarefa de mensagem, que deve ser sincronizada, ter seus recursos alocados e escalonada.
Cada implementação de NoC possui uma base de dados contendo informações de todos os seus recursos.
O alocador da NoC tenta minimizar os conflitos de recursos.
O escalonador da NoC executa as tarefas de mensagens de acordo com o requisito do serviço da rede.
Além disso, ele tenta minimizar a ocupação dos recursos.
Para a validação do ambiente foi modelado um dispositivo multimídia portátil.
A abstração do código da aplicação, bem como a extração dos parâmetros do grafo de tarefas e mapeamento das tarefas na NoC foram realizados manualmente.
São utilizados 4 processadores heterogêneos cada um com sua memória local e todos interconectados por uma NoC com topologia toro.
Hu e Marculescu apresentam um algoritmo para escalonamento de tarefas e comunicação em NoCs com núcleos heterogêneos, sob restrições de tempo real, minimizando o consumo de energia.
O algoritmo automaticamente atribui tarefas a diferentes PEs e então escalona os mesmos.
Os escalonamentos da comunicação e computação acontecem em paralelo.
Uma aplicação é descrita como um CTG (Communication Task Graph), onde as tarefas podem ter dependência de controle (uma não pode iniciar antes que outra termine) ou de dados (comunicação inter-tarefas durante execução).
Uma arquitetura é descrita como um ACG (Architecture Characterization Graph), indicando nodos e conexões entre eles.
Dados os CTG e ACG, o problema consiste em decidir como escalonar as tarefas e as comunicações na arquitetura alvo.
A solução para este problema tem impacto no consumo de energia do sistema, pois:
Devido a a heterogeneidade da arquitetura, atribuir a mesma tarefa para diferentes PEs pode levar a diferentes consumos de energia na computação;
Para diferentes alocações de tarefas, o volume de comunicação inter-tarefas e o caminho de roteamento pode mudar significativamente, causando diferentes consumos de energia na comunicação.
A arquitetura alvo é uma NoC com topologia malha 2D.
Cada núcleo é composto por um PE e um roteador.
Os PEs são heterogêneos, pois os mesmos podem ser processadores diferentes ou módulos de hardware específico.
O escalonamento é não-preemptivo, estático e tenta minimizar o consumo de energia da aplicação.
Para isso é preciso:
Determinar quais tarefas executarão em quais PEs;
Em PEs que executam mais de uma tarefa, determinar qual é executada primeiro;
Determinar o tempo para todas as comunicações.
Experimentos mostram que o algoritmo proposto, comparado com o algoritmo de escalonamento EDF (Earliest Deadline First), consome 55% e 39% menos de energia (duas categorias de benchmarks).
Marcon et al.
Propõem e avaliam um conjunto de modelos de computação voltados para a solução do problema de mapeamento de núcleos de propriedade intelectual sobre uma infra-estrutura de comunicação.
Três modelos são propostos (CDM, CDCM e ECWM) e comparados, entre si e com três outros disponíveis na literatura (CWM, CTM e ACPM).
Embora os modelos sejam genéricos, os estudos de caso restringem- se a infra-estruturas de comunicação do tipo rede intra-chip.
Dada a diversidade de modelos de mapeamento, é proposto o metamodelo Quantidade, Ordem, Dependência (QOD), que relaciona modelos de mapeamento, usando os critérios expressos na denominação QOD.
Considerando o alto grau de abstração dos modelos empregados, julga- se necessário prover uma conexão com níveis inferiores da hierarquia de projeto.
Em este sentido, são propostos modelos de consumo de energia e tempo de comunicação para redes intra-chip.
Visando demonstrar a validade dos modelos propostos, foram desenvolvidos métodos de uso destes na solução do problema de mapeamento.
Estes métodos incluem algoritmos de mapeamento, estimativas de tempo de execução, consumo de energia e caminhos críticos em infraestruturas de comunicação.
É também proposto um framework denominado CAFES, que integra os métodos desenvolvidos e os modelos de mapeamento em algoritmos computacionais.
É criado ainda um método que habilita a estimativa de consumo de energia para infra-estruturas de comunicação e sua implementação como uma ferramenta computacional.
A alocação estática de tarefas não condiz com a realidade atual dos MPSoCs, onde estes são projetados para suportar diferentes aplicações e inclusive novas aplicações após o sistema ter sido fabricado.
Como exemplo, citam- se telefones celulares 3 G, que suportam inúmeros perfis de aplicações, e sistemas set-top box para televisão digital, com padrões para tratamento de imagem em constante evolução.
A alocação dinâmica de tarefas considera uma alocação inicial, que é ajustada ao longo de a execução da aplicação.
Esse ajuste é efetuado mediante o envio de novas tarefas a PEs.
Ainda, a alocação dinâmica habilita a execução de ocorrer migração de tarefas.
Este mecanismo suspende a execução de uma tarefa num nodo x e envia esta tarefa para um nodo y, onde sua execução é retomada.
A migração de tarefas têm sido largamente empregada na área de Sistemas Paralelos e Distribuídos, de forma a melhorar o desempenho geral dos sistemas, distribuindo a carga de trabalho entre os PEs.
Pesquisas recentes têm explorado a migração de tarefas no contexto de MPSoCs com vários propósitos, como por exemplo:
Diminuir o volume de comunicação entre as tarefas e conseqüentemente o consumo de energia total do sistema;
Balancear a carga de trabalho entre processadores, melhorando o desempenho do sistema;
Aumentar a tolerância a falhas de um sistema.
Assim, os benefícios da migração de tarefas podem ser utilizados em MPSoCs de forma a melhor atender os requisitos de aplicações embarcadas.
A viabilidade da migração de tarefas em MPSoCs é estudada por Bertozzi et al.,
onde é proposta uma infra-estrutura de software para gerenciamento de tarefas.
O mecanismo de migração de tarefas proposto é baseado em checkpoints no código cuja estratégia é definida por o usuário.
A arquitetura utilizada é um MPSoC composto por processadores ARM7 interligados por barramento, cada um com uma memória local.
Uma cópia do sistema operacional µClinux executa em cada processador.
Uma memória compartilhada é usada para comunicação entre processos e para manter a informação do estado global dos processos executando em cada processador (tabela de migração de processos).
O sistema é organizado com um processador mestre e um número arbitrário de escravos.
O mestre desempenha o controle de admissão de tarefas e a alocação inicial de tarefas nos processadores escravos.
Foi desenvolvido um middleware no nível de usuário sobre o µClinux, composto por um daemon e uma biblioteca de mensagens, fornecendo suporte à migração por o uso de pontos de migração através de checkpoints no código.
O programador especifica explicitamente o estado mínimo da tarefa a ser salvo em cada ponto de migração.
O middleware permite salvar o estado da tarefa e restaurar- lo no processador destino.
Um daemon mestre (que executa no processador mestre) implementa a estratégia de balanceamento de carga e controla os eventos de migração de forma centralizada.
Um daemon escravo (que executa em cada processador escravo) é responsável por criar processos no processador local quando lhe for requisitado por o mestre.
Quando uma tarefa deve ser migrada, o daemon mestre ativa a entrada da tarefa selecionada na tabela de migração.
Quando a tarefa atinge o ponto de migração, ela confirma a requisição de migração, desativando sua entrada na tabela de migração.
A seguir, o daemon mestre requisita a criação de um novo processo no processador destino.
As tarefas encontram- se replicadas em cada memória privada, evitando assim, a transferência de código.
A Figura 2 expressa T_ migration, o tempo gasto entre a chamada de um tempo usado por a API de checkpoints para verificar a requisição de migração, salvar o contexto e sair (T_ shutdown);
tempo gasto esperando o daemon ser ativado (T_ activation); (
iii) tempo necessário para restaurar a tarefa migrada (T_ reboot).
Para medir o desempenho do sistema e caracterizar os custos de migração, diversos testes foram feitos.
O overhead dessa abordagem foi caracterizado via simulação funcional e sua viabilidade no contexto de MPSoCs foi demonstrada.
Ozturt et al.
Propõem um esquema de migração seletiva, o qual decide por migrar uma tarefa (código) ou um dado, de maneira a satisfazer um requisito de comunicação.
A escolha entre as duas opções é feita em tempo de execução, baseada em estatísticas coletadas em tempo de compilação.
O objetivo é reduzir o consumo de energia de um MPSoC durante a comunicação entre processadores.
A abordagem de migração seletiva é composta por três componentes:
Profiling, Code Annotation e Selective Code/ Data Migration.
O primeiro e o segundo são realizados em tempo de compilação.
A técnica de Profiling coleta estatísticas de custos de energia para a migração de código e dado.
Esses custos são calculados para cada unidade de migração de código e para cada unidade de migração de dado.
Code Annotation indica a seqüência de comunicações envolvidas para um determinado código.
O processo de migração de dados ou código é realizado em tempo de execução e baseia- se nos custos de energia calculados estaticamente.
A abordagem proposta considera requisitos de comunicação e tenta tomar decisões globalmente ótimas (ou seja, considerando múltiplas comunicações e não apenas a comunicação corrente).
A arquitetura MPSoC utilizada no trabalho é composta por 8 processadores interligados por um barramento.
Cada processador possui uma memória local de 32 KB de capacidade.
A comunicação entre os processadores é realizada através de troca de mensagens.
A migração seletiva foi implementada e comparada com dois esquemas alternativos:
Um que sempre migra dado e outro que sempre migra código.
Os resultados coletados indicam que a migração seletiva reduz o consumo de energia e o tempo de execução.
Em é proposto um algoritmo de particionamento HW/ SW para redes reconfiguráveis, o qual otimiza a alocação dinâmica de tarefas de forma que defeitos na rede possam ser tratados e o tráfego seja minimizado.
Uma vez que a alocação estática não é tolerante a falhas e a redundância de nodos e canais possui alto custo, esta abordagem separa a funcionalidade do HW físico e realoca tarefas de nodos defeituosos em nodos sem defeitos, tornando- se tolerante a falhas.
Para determinar a alocação de tarefas em recursos da rede, é usado o algoritmo de particionamento HW/ SW executado durante a operação do sistema.
Nodos da rede são FPGAs (dispositivos reconfiguráveis) e CPUs, podendo assim atribuir tarefas de HW e SW para os mesmos, respectivamente.
Os nodos são conectados ponto-a-ponto.
A rede é composta por nodos computacionais C, sensores S, atuadores A e canais de comunicação.
Tarefas são distinguidas por tarefas de sensor, de controlador e atuador.
Sensores produzem dados que são processados por controladores e atuadores consomem dados.
São atributos de uma tarefa necessários para o particionamento:
Tempo de execução, deadline, LUTs (área), células de memória, tamanho da migração e taxas de transmissão de dados.
O particionamento é executado em duas fases:
Reparo rápido:
Quando um nodo da rede falha, as tarefas que estavam neste nodo são alocadas em outro.
Réplicas de tarefas se tornam tarefas principais (originais).
Novas rotas para comunicação são estabelecidas;
otimização: Esta fase tenta encontrar uma alocação de tarefas de forma que o tráfego de dados seja minimizado.
De forma a tolerar outros defeitos do nodo, tarefas replicadas precisam ser criadas e alocadas.
A abordagem foi implementada na seguinte infra-estrutura:
Rede de 4 placas de FPGAs reconfiguráveis, sistema operacional microC-OS II.
Um gerenciador de tarefas decide onde alocar as tarefas.
Foi implementado um modelo comportamental dessa rede com diferentes cenários, variando o número de tarefas e o número de nodos.
Os resultados mostram que o tráfego é reduzido em pelo menos 20%.
Ngouanga et al.
Exploram a alocação dinâmica de tarefas numa arquitetura reconfigurável de grão grande.
O trabalho considera uma plataforma (Apaches) composta por PEs homogêneos conectados por uma NoC.
O número de nodos é parametrizável.
Nodos de processamento (ou escravos) executam tarefas e são agrupados em clusters;
Nodos mestres ou controladores de rede gerenciam clusters e desempenham a alocação de tarefas nos mesmos.
Há um nodo mestre para cada cluster.
A Figura 4 apresenta o funcionamento da plataforma Apaches.
Os nodos M e I/ O são respectivamente o controlador de rede e o controlador de I/ O. O restante são nodos escravos.
Apenas o nodo mestre (M) é responsável por desenvolver o mapemanto de tarefas.
Em a Figura 4 (a), uma aplicação está executando na plataforma.
Quando uma nova aplicação deve ser mapeada na rede, o sistema realiza seqüencialmente as seguintes operações:
Uma nova alocação é computada por o nodo mestre;
Se a nova alocação implica realocar algumas tarefas de aplicações que já estão executando, o mestre move essas tarefas para suas novas localizações.
Para isso é enviada uma requisição para cada PE que vai ter uma tarefa realocada (Figura 4 (b)).
Os códigos das tarefas que foram realocadas e das tarefas da nova aplicação são enviados para os PEs aos quais elas foram atribuídas (Figura 4 (c) e Figura 4 (d)).
O sistema retoma a execução de tarefas interrompidas e inicia o processamento da nova aplicação mapeada na rede (Figura 4 (e)).
Cada PE executa um microkernel multitarefa.
São usados dois algoritmos de alocação de tarefas:
Simulated annealing e force directed.
O primeiro é baseado na simulação de um fenômeno físico, o qual muda aleatoriamente a posição das tarefas na rede, estima custos e aceita ou rejeita mudanças de acordo com um critério de temperatura.
O segundo calcula o posicionamento das tarefas, levando em conta a força resultante produzida por a atração entre tarefas comunicantes.
Essa força de atração é proporcional ao volume de comunicação entre tarefas interligadas.
Resultados mostram que a alocação dinâmica traz benefícios quando comparados com uma alocação estática ou aleatória.
Nollet et al.
Desenvolveram um esquema para o gerenciamento de recursos (computação e comunicação) numa plataforma heterogênea multiprocessada cujos núcleos são interconectados por uma NoC.
Estes núcleos são monotarefa e podem ser núcleos de hardware reconfigurável.
Um gerente (centralizado) é responsável por alocar a quantidade certa de recursos, de forma a acomodar tarefas de uma aplicação em tempo de execução.
Isso basicamente significa:
Alocar recursos de computação, gerenciar recursos de comunicação na NoC e tratar de várias questões relacionadas à migração de tarefas em tempo de execução na NoC.
A heurística de gerenciamento de recursos considera propriedades específicas do hardware reconfigurável para a alocação de tarefas.
Quando ocorre um mapeamento falho ou quando os requisitos do usuário mudam, a heurística de gerenciamento de recursos pode utilizar a migração de tarefas.
Como em, uma tarefa migra para outro PE apenas em determinados pontos no código (pontos de migração definidos por o usuário).
Um problema importante considerado na migração de tarefas é assegurar a consistência da comunicação durante o processo de migração.
Esta questão é originada do fato de que, depois de receber uma requisição de migração, a quantidade de tempo e mensagens de entrada que uma tarefa requer até atingir o ponto de migração são desconhecidos.
Isso significa que as tarefas produtoras de mensagens (i.
e pares comunicantes) devem manter o envio de mensagens até que a tarefa migrante informe que um ponto de migração foi atingido e que ela parou o consumo de mensagens.
Contudo, pode haver mensagens armazenadas e não-processadas nos canais de comunicação entre tarefas produtoras e tarefas migrantes.
De essa forma, Nollet et al.
Apresentam dois mecanismos de migração de tarefas em NoCs, que asseguram a consistência de mensagens no processo de migração de tarefas.
A plataforma multiprocessada foi emulada interconectando um processador StrongARM (PE mestre) a um FPGA contendo PEs escravos.
Um sistema operacional executa no PE mestre e contém a heurística de gerenciamento de recursos e mecanismo de migração de tarefas.
O objetivo deste trabalho é o desenvolvimento de uma plataforma multiprocessada e multitarefa em chip que permita a alocação dinâmica de tarefas sobre os processadores.
Esta plataforma é um MPSoC interligado por uma NoC.
Todos os nodos do MPSoC são processadores embarcados de propósito geral idênticos.
Um nodo mestre aloca as tarefas nos nodos escravos e cada escravo componente do sistema é capaz de receber uma tarefa, armazenar todas as informações referentes a esta tarefa em memória e executar- la.
Para tanto, foi implementado um microkernel que fornece os serviços básicos de um sistema operacional necessários para o controle da execução das tarefas no ambiente multiprocessado e multitarefa.
O nodo mestre contém as informações de alocação de todas as tarefas a serem executadas, relacionando um conjunto de tarefas por nodo escravo.
Em a inicialização do sistema, ele carrega as tarefas de um repositório, enviando- as aos nodos escravos designados para executar- las.
Como este envio de tarefas ocorre apenas na inicialização do sistema, denomina- se esta alocação como alocação estática.
Esta alocação inicial pode ser realizada aleatoriamente, manualmente ou então utilizar um dos algoritmos de alocação estática apresentados aqui.
O foco principal do trabalho é a alocação dinâmica, em o qual o nodo mestre envia novas tarefas aos nodos escravos mediante requisição destes, durante a execução do sistema.
São contribuições deste trabalho:
Definição e implementação de uma plataforma MPSoC, com processadores interconectados por uma NoC (contribuição de hardware); (
ii) desenvolvimento de um microkernel para alocação estática e dinâmica de tarefas, permitindo um melhor uso dos processadores (contribuição de software).
A Tabela 1 resume as características dos trabalhos apresentados nesta Seção e posiciona o presente trabalho frente a o estado da arte.
As características apresentadas na tabela incluem:
Este trabalho inclui conceitos da área de Arquitetura de Computadores, Sistemas Digitais e Sistemas Operacionais, caracterizando assim sua constituição multidisciplinar.
Este Capítulo apresenta os conceitos envolvidos neste trabalho:
NoCs, Processadores Embarcados e Sistemas Operacionais.
Uma maneira de minimizar os problemas oriundos da arquitetura de barramentos é através da utilização de redes intra-chip, também denominadas NoCs.
Redes intra-chip utilizam conceitos originados da área de redes de computadores e de comunicação de dados, as quais empregam a organização da transferência de informação em camadas e protocolos.
As principais características das NoCs que motivam o seu estudo são:
Confiabilidade e eficiência no gerenciamento de energia;
Escalabilidade da largura de banda em relação a arquiteturas de barramento reusabilidade;
Decisões de roteamento distribuídas.
A escalabilidade refere- se à capacidade de se interconectar núcleos adicionais de hardware, sem diminuir significativamente o desempenho global do sistema.
No caso de o barramento, o acréscimo de novos núcleos implica aumentar o comprimento total deste, reduzindo o desempenho global do sistema.
Em NoCs, o acréscimo de um elemento da rede leva ao aumento do número de canais de comunicação, melhorando de uma maneira global o seu desempenho.
Devido a a multiplicidade de caminhos possíveis em NoCs, é possível explorar também o paralelismo na comunicação entre os núcleos, ou seja, pares de núcleos distintos podem realizar transações simultaneamente.
Em barramento, apenas uma transação pode ser efetuada num dado momento, limitando assim a largura de banda utilizada por um módulo.
A reusabilidade de estruturas de comunicação é um conceito suportado tanto por estruturas de interligação por barramento como por rede.
Esta característica diz respeito a se utilizar uma mesma estrutura de comunicação, sem modificar- la, em projetos distintos.
A interligação por barramento utiliza fios longos, ocasionando um maior consumo de energia que em NoCs.
Estas, por sua vez, fazem uso eficiente do consumo de energia, uma vez que a comunicação é feita ponto a ponto, utilizando fios de tamanho reduzido, devido a a pouca distância existente entre os módulos.
Uma rede é composta por nodos de processamento e nodos de chaveamento.
A Figura 5 mostra uma rede em anel, uma topologia bastante simples e econômica.
Cada nodo de chaveamento possui ligações para outros dois nodos de chaveamento vizinhos e para um nodo de processamento local.
Os nodos de processamento (Figura 6 (a)) contém um núcleo capaz de processar informação, podendo ser tanto um processador com memória local quanto um núcleo dedicado a uma tarefa específica, como por exemplo realizar a Transformada Discreta do Co-seno (DCT, do inglês, Discrete Cosine Transform) num tratamento de imagem.
Os nodos de chaveamento, denominados roteadores, (Figura 6 (b)) realizam a transferência de mensagens entre os nodos de processamento.
Em geral, eles possuem um núcleo de chaveamento (ou chave), uma lógica para roteamento e arbitragem (R &amp; A) e portas de comunicação para outros nodos de chaveamento e, dependendo da topologia, para um nodo de processamento local.
As portas de comunicação incluem canais de entrada e saída, os quais podem possuir, ou não, buffers para o armazenamento temporário de informações.
As informações trocadas por os nodos são organizadas sob a forma de mensagens, as quais, possuem, em geral, três partes:
Um cabeçalho (header), um corpo de dados (payload) e um terminador (trailer).
Em o cabeçalho estão incluídas informações de roteamento e controle utilizadas por os nodos de chaveamento para propagar a mensagem em direção a o nodo destino da comunicação.
O terminador, por sua vez, inclui informações usadas para a detecção de erros e para a sinalização do fim da mensagem.
Tipicamente, as mensagens são quebradas em pacotes para a transmissão.
Um pacote é a menor informação que contém detalhes sobre o roteamento e seqüenciamento dos dados e mantém uma estrutura semelhante a de uma mensagem, com um cabeçalho, um corpo e um terminador.
Um pacote é constituído por flits1, cuja largura depende da largura física do canal.
As redes de interconexão para multiprocessadores empregam muitos conceitos empregados em redes de computadores.
Um de eles é a organização em camadas que encapsulam funções equivalentes àquelas definidas no modelo OSI (do inglês, Open System Interconnection), um padrão internacional de organização de redes de computadores proposto por a Iso (do inglês, International Organization for Standardization).
A arquitetura de uma rede que segue o modelo OSI é formada por níveis, interfaces e protocolos.
Cada nível oferece um conjunto de serviços ao nível superior, usando funções realizadas no próprio nível e serviços disponíveis nos níveis inferiores.
Os nodos de chaveamento de redes de interconexão que são estruturados em camadas hierárquicas implementam algumas das funções dos níveis inferiores (físico, enlace, rede) do modelo OSI, descritas abaixo:
Nível Físico:
Realiza a transferência de dados em nível de bits através de um enlace.
Nível de enlace:
Efetua a comunicação em nível de quadros (grupos de bits).
Preocupa- se com o enquadramento dos dados e com a transferência desses quadros de forma confiável, realizando o tratamento de erros e o controle do fluxo de transferência de quadros.
Nível de rede:
Faz a comunicação em nível de pacotes (grupos de quadros).
Responsável por o empacotamento das mensagens, roteamento dos pacotes entre a origem e o destino da mensagem, controle de congestionamento e contabilização de pacotes transferidos.
Quanto a a topologia, as redes de interconexão para multiprocessadores são definidas por a conexão de seus roteadores e podem ser agrupadas em duas classes principais:
As redes diretas e as redes indiretas.
Em as redes diretas, cada nodo de chaveamento possui um nodo de processamento associado, e esse par pode ser visto como um elemento único dentro de a máquina, tipicamente referenciado pela palavra nodo, como mostra a Figura 7.
Topologias de redes diretas estritamente ortogonais mais utilizadas são a malha ndimensional), o toróide (Figura 8 (b)) e o hipercubo (Figura 8 (c)).
Flit é a menor unidade de transferência de dados.
Em as redes indiretas os nodos de processamento possuem uma interface para uma rede de nodos de chaveamento baseados em chaves (roteadores).
Cada roteador possui um conjunto de portas bidirecionais para ligações com outros roteadores e/ ou nodos de processamento.
Somente alguns roteadores possuem conexões para nodos de processamento e apenas esses podem servir de fonte ou destino de uma mensagem.
A topologia dessa rede é definida por a estrutura de interconexão desses roteadores.
Duas topologias clássicas de redes indiretas se destacam:
O crossbar e as redes multiestágio.
Para conexão indireta de N nodos de processamento, o crossbar é a topologia ideal, pois consiste de um único roteador NxN capaz de ligar qualquer entrada com qualquer saída.
A desvantagem desta topologia é o custo, o qual cresce quadraticamente com o número de nodos de processamento.
MPSoCs são projetados tendo por base processadores de propósito geral (MPUs, do inglês, Microprocessor Units), processadores de sinais digitais (DSPs, do inglês, Digital Signal Processor) e co-processadores de propósito específico.
De essa forma, o entendimento de suas arquiteturas fornece um contexto para a escolha dos processadores adequados, em termos de desempenho, custo, potência e outros requisitos.
Processadores embarcados de propósito geral possuem aspectos diferentes de processadores de alto desempenho utilizados em PCs (do inglês, Personal Computer).
Um PC deve suportar uma ampla gama de tipos de aplicação:
Ferramentas de produtividade (email, processadores de texto, apresentações), ferramentas de projeto (CAD, do inglês, Computer-Aided Design), jogos, multimídia e o sistema operacional em si.
Por outro lado, os sistemas embarcados executam um conjunto específico de tarefas.
De essa forma, um processador de propósito geral embarcado, deve suportar diferentes aplicações embarcadas, cada uma de as quais com um conjunto específico de tarefas.
A arquitetura do conjunto de instruções (ISA, do inglês, Instruction-Set Architecture) de um processador modela as funcionalidades deste e permite suporte eficiente para linguagens de alto nível, como C/ C+.
A micro-arquitetura (i.
e, a organização do hardware) reflete a natureza genérica do ciclo busca/ execução das instruções.
Ela não é concebida para um algoritmo particular ou conjunto de algoritmos, como acontece em processadores de aplicação específica (e.
g As próximas subseções apresentam alguns processadores embarcados com arquitetura RISC 32-bits.
Em este trabalho, optou- se em utilizar esta classe de processadores por estes serem utilizados largamente em sistemas embarcados, como por exemplo o processador ARM7.
Processadores ARM são utilizados, por exemplo, em telefones celulares e PDAs (do inglês, Personal Digital Assistant).
Sua arquitetura tem evoluído e, atualmente, existem cinco famílias:
ARM7, ARM9, ARM9E, ARM10 e ARM11.
A sucessão de famílias representa mudanças na micro-arquitetura.
Além disso, a ISA é freqüentemente melhorada, de forma a suportar novas funcionalidades.
Essas melhorias funcionais incluem Thumb, um conjunto de instruções de 16 bits para código compacto;
DSP, um conjunto de extensões aritméticas para aplicações DSP;
E Jazelle, uma extensão para execução direta de bytecode Java.
Outra melhoria da ISA é o suporte à unidade de gerenciamento de memória (MMU, do inglês, Memory Management Unit).
O processador PowerPC 440 da IBM foi concebido para uma variedade de aplicações embarcadas e integra um pipeline superescalar de sete estágios, com suporte à duas instruções por ciclo.
Um diagrama de blocos do PowerPC 440 é apresentado na Figura 10.
Esse processador possui:
Registers); Múltiplos timers, uma unidade de gerenciamento de memória (MMU, do inglês, Management Memory Unit), suporte à depuração e gerenciamento de potência;
As famílias de processadores MIPS, desenvolvidos por a MIPS Techonologies Inc., têm sido licenciadas e integradas nos projetos de várias companhias.
A tecnologia oferecida por a MIPS tem sido usada para roteadores Cisco, modems à cabo e ADSL, smartcards, impressoras a laser, set-top boxes, palmtops e o PlayStation2 da Sony Inc..
A Tabela 2 apresenta as especificações das famílias dos processadores MIPS.
A primeira CPU MIPS comercial foi o R2000, anunciado em 1985.
Ele possui instruções de multiplicação e divisão e trinta e dois registradores de 32 bits de propósito geral.
O R2000 também possui suporte a até quatro co-processadores, um desses podendo ser a Unidade de Ponto O R3000 sucedeu o R2000 em 1988, adicionando caches de 32 KB para dados e instruções, e provendo suporte à coerência de cache para multiprocessadores.
O R3000 também incluiu uma MMU.
Como o R2000, o R3000 pode ser conectado à FPU R3010.
A série R4000, lançada em 1991, estendeu o conjunto de instruções MIPS para uma arquitetura de 64 bits, absorveu a FPU, criando um sistema em único chip, operando até 100 MHz.
Com a introdução do R4000, logo apareceram versões melhoradas, incluindo o R4400 em 1993, o qual incluiu caches de 16 KB e um controlador para outra cache externa (de nível 2) de 1 MB.
Uma vez que a tecnologia MIPS pode ser licenciada, a companhia Quantum Effect Devices (QED), projetou o R4600, o R4700, o R4650 e o R5000.
O R4600 e o R4700 foram usados em versões de baixo custo das estações de trabalho da companhia SGI (do inglês, Silicon Graphics Incorporation).
O R4650 foi usado em set-top boxes para WebTV.
A QED posteriormente projetou a família RM7000 e RM9000 de dispositivos para o mercado embarcado, como dispositivos de rede e impressoras a laser.
O R8000 foi o primeiro projeto MIPS superescalar, capaz de executar duas instruções aritméticas e duas operações de memória por ciclo de clock.
O projeto foi distribuído sobre seis circuitos integrados:
Uma unidade de inteiros, uma unidade de ponto-flutuante, duas para o controle dos tags das caches nível 2, uma para monitorar o barramento (para coerência de cache), e um controlador de cache.
Em 1995 o R10000 foi apresentado.
Esse processador possui velocidade de relógio mais alta do que o R8000, tendo caches de dados e instruções de 32 KB.
Ele também é superescalar, mas sua maior inovação inclui execução das instruções fora de ordem.
Projetos recentes têm sido baseados no R10000.
O R12000 utiliza processo de fabricação mais atual que o R10000, permitindo aumento da velocidade do relógio.
O R14000 permite suporte à DDR SRAM na cache fora de o chip e barramento de dados (FSB, do inglês, Front-Side Bus) mais rápido.
As últimas versões compreendem o R16000 e o R16000A os quais apresentam velocidade de relógio mais alta, cache L1 adicional e um chip menor comparado com os anteriores.
MIPS32 24 Kf O MIPS32 24 Kf é um processador sintetizável para aplicações embarcadas.
Seu diagrama de blocos é apresentado na Figura 11.
Possui uma arquitetura com pipeline de oito estágios.
Sua MMU possui quatro entradas para instruções e oito para dados.
Apresenta uma unidade de ponto flutuante (FPU) que suporta instruções de precisão simples e dupla e uma unidade de multiplicação/ divisão (MDU, do inglês, Multiple/ Divide Unit) de alto desempenho.
O bloco ser configuradas em 0, 16, 32 e 64 KB.
A unidade de interface com o barramento (BIU, do inglês, Bus Interface Unit) implementa o padrão OCP (Open Core Protocol).
Interfaces opcionais suportam blocos externos como co-processadores.
O módulo EJTAG fornece suporte à debug.
Sistemas operacionais são programas intermediários responsáveis por controlar os recursos existentes num computador (processadores, memórias, dispositivos de E/ S, etc.), além de fornecer a base para o desenvolvimento de programas de aplicação.
As aplicações são compostas por tarefas, sendo uma tarefa um conjunto de instruções e dados com informações necessárias à sua correta execução num processador.
Além disso, os sistemas operacionais podem ser vistos como uma camada de software que provê um ambiente com uma interface mais simples e conveniente para o usuário.
Os sistemas operacionais dispõem de diversos tipos de serviços.
No entanto, levando em consideração a maioria das implementações de sistemas operacionais existentes atualmente, pode- se dizer que os principais serviços implementados no kernel do sistema operacional são Escalonamento de tarefas:
Este serviço é fundamental em qualquer sistema operacional, pois é necessário para gerenciar a utilização de recursos, bem como a ordem de execução das tarefas constituintes de sistema, o que é realizado de acordo com a política de escalonamento adotada.
Troca de contexto:
Sempre que uma nova tarefa é escalonada, é necessário realizar uma troca de contexto no sistema, ou seja, o sistema operacional deve salvar as informações da tarefa que está executando e carregar as informações necessárias para a execução da nova tarefa.
As trocas de contexto ocorrem em intervalos de tempo determinados, chamados timeslice.
Comunicação entre tarefas:
Este serviço é responsável por o gerenciamento da troca de informações entre tarefas cooperantes, ou seja, tarefas que necessitam de resultados de outra (s) tarefa (s) durante sua execução.
Esta troca pode ser realizada através de memória compartilhada ou troca de mensagens.
Tratamento de interrupções:
Interrupções são mecanismos através de os quais outros módulos (E/ S, memória) podem interromper o processamento normal do processador, e sempre que isto acontece, o processador deve interromper sua execução e iniciar a execução de uma rotina específica a fim de tratar esta interrupção.
Gerenciamento de memória:
Este serviço consiste na tarefa de conhecer as partes da memória que estão ou não em uso, alocar memória para as tarefas quando necessário e liberar memória quando estas estiverem com sua execução finalizada.
Como um subgrupo de sistemas operacionais, encontram- se os sistemas operacionais embarcados.
Estes vêm tornando- se bastante populares, visto que implementam apenas as funcionalidades necessárias à aplicação que será executada.
O tamanho de um sistema operacional embarcado (espaço de memória ocupado) tende a ser menor que o de um sistema operacional convencional, reduzindo o kernel a um microkernel (µkernel), o que é desejável para aplicações embarcadas, como por exemplo, aplicações para celulares.
Assim como os sistemas operacionais convencionais, os sistemas operacionais embarcados oferecem mecanismos de escalonamento, tratamento de interrupções, gerenciamento de memória entre outros.
Exemplos de sistemas operacionais embarcados são o µClinux, o eCos e o EPOS.
O µClinux é um sistema operacional derivado do Linux usado em microcontroladores que não possuem unidades de gerenciamento de memória (MMUs).
A grande contribuição do µClinux é o fato de possuir um código modular e pequeno, reduzindo o consumo de energia e o uso de recursos computacionais.
Contudo, não oferece bom suporte para multitarefas.
O eCos é um sistema operacional portável para arquiteturas 16, 32 e 64 bits e pode ser usado tanto em microprocessadores como microcontroladores.
Sua biblioteca possui uma camada de abstração ao hardware, fornecendo suporte a diversas famílias de processadores.
O eCos permite diversas políticas de escalonamento, mecanismo de interrupções, primitivas de sincronização e comunicação específica para a aplicação do usuário.
Além disso, o eCos suporta gerenciamento de memória, tratamento de exceções, bibliotecas, drivers, etc..
Ele possui ferramentas de configuração e construção, além de compiladores e simuladores da Gnu.
O EPOS é um sistema operacional orientado a aplicação, i.
e, se adapta automaticamente aos requisitos da aplicação que o usuário elabora.
É concebido para aplicações dedicadas e portado para diversas arquiteturas de processadores.
A maioria dos sistemas operacionais embarcados existentes apresenta consumo de memória elevado;
Serviços não necessários ao escopo deste trabalho (por exemplo, pilha TCP/ IP, alocação dinâmica de memória) e (iii) elevada complexidade.
Assim, optou- se por implementar um sistema operacional de tamanho reduzido contendo apenas os serviços necessários para alcançar os objetivos do trabalho (estes serviços são apresentados no Capítulo 5).
O tamanho do sistema operacional deve ser levado em conta, uma vez que a quantidade de memória disponível em sistemas embarcados é restrita.
Em este trabalho, são utilizados 64 KB de memória para cada processador (espaço em o qual reside o sistema operacional e as tarefas).
Cada BRAM (Block RAM) possui capacidade de 2 KB, resultando em 32 BRAMs por processador.
Uma vez que a plataforma criada utiliza 6 processadores, são necessárias 192 BRAMS.
Para prototipar este sistema nas plataformas disponíveis no grupo GAPH, as quais contém dispositivos XC2 VP30, XC2 V4000 e XC4 LX25 (72 BRAMs), será avaliado um MPSoC com 4 processadores.
Este Capítulo apresenta a primeira contribuição deste trabalho:
O desenvolvimento da infra-estrutura de hardware da plataforma MPSoC.
Esta infra-estrutura possui os seguintes componentes, que são mostrados na Figura 12 e na Figura 13: NoC Hermes:
Realiza a interconexão dos núcleos e o roteamento de pacotes entre os mesmos Plasma:
Nodo processador que executa a aplicação.
Cada processador possui uma memória local, a qual não é acessível a outros processadores);
Ni (Network Interface):
Faz a interface entre o processador e a rede.
DMA (Direct Memory Access):
Transfere para a memória do processador o código-objeto das tarefas enviadas por um nodo mestre.
CPU (arquitetura MIPS), DMA e RAM.
A NoC Hermes e o processador Plasma são módulos IP não desenvolvidos no escopo do presente trabalho.
A contribuição refere- se principalmente à integração dos diversos núcleos (processadores Plasma) à rede e no desenvolvimento dos módulos Ni e DMA.
As próximas seções discutem cada um dos componentes desta infra-estrutura.
Para a interconexão dos núcleos do MPSoC este trabalho utiliza a NoC Hermes, desenvolvida no grupo de pesquisa GAPH.
A razão desta escolha é em virtude de as vantagens que redes intra-chips proporcionam ao projeto de MPSoCs, bem como o domínio dessa tecnologia por o grupo.
A NoC Hermes possui um mecanismo de comunicação denominado chaveamento de pacotes, em o qual os pacotes são roteados individualmente entre os nodos sem o estabelecimento prévio de um caminho.
Este mecanismo de comunicação requer o uso de um modo de roteamento para definir como os pacotes devem se mover através dos roteadores.
A NoC Hermes utiliza o modo de roteamento wormhole, em o qual um pacote é transmitido entre os roteadores em flits.
Apenas o flit de cabeçalho possui a informação de roteamento.
Assim, os flits restantes que compõe o pacote devem seguir o mesmo caminho reservado por o cabeçalho.
A NoC Hermes utiliza uma topologia malha, ilustrada na Figura 14.
Essa topologia é justificada em função de a facilidade de desenvolver o algoritmo de roteamento, inserir núcleos e gerar o leiaute do circuito.
A interface de comunicação entre roteadores Hermes vizinhos é apresentada na Figura 15.
Os seguintes sinais compõem a porta de saída:
Clock_ tx:
Sincroniza a transmissão de dados;
Tx: Indica disponibilidade de dado;
Lane_ tx:
Indica o canal virtual transmitindo dado;
Data_ out:
Dado a ser transmitido;
Credit_ in:
Informa disponibilidade de buffer no roteador vizinho, para cada canal virtual.
O número de canais virtuais (l lanes) e a largura do barramento de dados (n bits) são parametrizáveis em função de os recursos de roteamento disponíveis e memória disponível para esquemas de buferização.
O roteador Hermes possui uma lógica de controle de chaveamento centralizada e 5 portas bidirecionais:
East, West, North, South e Local.
A porta Local estabelece a comunicação entre o roteador e seu núcleo local.
As demais portas ligam o roteador aos roteadores vizinhos.
Cada porta unidirecional (entrada ou saída) do roteador corresponde a um canal físico.
O modo de chaveamento wormhole adotado no roteador Hermes permite que cada canal físico seja multiplexado em n canais virtuais (VCs).
A Figura 16 apresenta o roteador Hermes com dois VCs por canal físico.
A cada porta de entrada é adicionado um buffer para diminuir a perda de desempenho com o bloqueio de flits.
A perda de desempenho acontece porque quando um flit é bloqueado num dado roteador os flits seguintes do mesmo pacote também são bloqueados em outros roteadores.
Com a inserção de um buffer o número de roteadores afetados por o bloqueio dos flits diminui.
O buffer inserido no roteador Hermes funciona como uma fila FIFO (First In First Out) circular, cuja profundidade p é parametrizável.
Quando o canal físico é dividido em n VCs, uma fila FIFO de profundidade p/ n é associada a cada VC.
Por exemplo, o roteador apresentado na Figura 16 possui um espaço de armazenamento de 8 flits por porta de entrada, sendo multiplexado em dois VCs, com cada buffer associado a cada VC com profundidade de 4 flits.
A porta de entrada é responsável por receber flits de pacotes e armazenar- los no buffer do VC correto.
A seleção do buffer é realizada através do sinal lane_ rx, que informa a qual VC pertence o flit no canal físico.
O sinal lane_ rx possui n bits, onde n é o número de VCs.
Após a seleção do buffer correto, o flit é armazenado e o número de créditos do VC (espaços livres para armazenamento) é decrementado.
Quando o flit é transmitido por a porta de saída, o flit é removido do buffer e o número de créditos incrementado.
A informação de crédito disponível é transmitida ao roteador vizinho através do sinal credit_ o.
O sinal credit_ o também possui 1 bit para cada VC e é interpretado de forma análoga ao sinal lane_ rx.
A lógica de controle de chaveamento implementa uma lógica de arbitragem e um algoritmo de roteamento.
Quando um roteador recebe um header flit, a arbitragem é executada e se a requisição de roteamento do pacote é atendida, um algoritmo de roteamento é usado para conectar o flit da porta de entrada à correta porta de saída.
Cada roteador deve ter um endereço único na rede.
Para simplificar o roteamento na rede, este endereço é expresso nas coordenadas XY, onde X representa a posição horizontal e Y a posição vertical.
Um roteador pode ser requisitado para estabelecer até nxm+ 1 conexões simultaneamente, onde n representa o número de portas de entrada, m representa o número de VC e 1 representa a porta local.
Uma lógica de arbitragem centralizada é usada para garantir acesso a um VC de entrada quando um ou mais VCs requerem simultaneamente uma conexão.
A política de arbitragem RoundRobin é usada no roteador Hermes.
Essa política utiliza um esquema de prioridades dinâmicas, proporcionando um serviço mais justo que a prioridade estática, evitando também starvation.
A prioridade atribuída a um VC é determinada por o último VC a ter uma requisição de acesso atendida.
Em o roteador Hermes, o algoritmo de roteamento XY é utilizado.
O algoritmo XY compara o endereço do roteador atual (XLYL) com o endereço do roteador destino (XTYT) do pacote, armazenado no header flit.
Os flits devem ser roteados para a porta Local quando o endereço XLYL do roteador atual é igual ao endereço XTYT do pacote.
Se esse não for o caso, o endereço XT é primeiro comparado ao endereço (horizontal) XL.
Os flits serão roteados para a porta East quando XL\ XT, para West quando XL\&gt; XT e se XL $= XT o header flit já está alinhado horizontalmente.
Se esta última condição é a verdadeira, o endereço (vertical) YT é comparado ao endereço YL.
Os flits serão roteados para a porta North quando YL\ YT, para South quando YL\&gt; YT.
Se a porta escolhida está ocupada, o header flit também como todos os flits subseqüentes do pacote serão bloqueados.
Em o roteador Hermes com canais físicos multiplexados em VCs, uma porta (canal físico) é considerada ocupada quando todos os VCs estão ocupadas.
Em o algoritmo de roteamento XY não é necessário estabelecer uma ordem entre os VCs, porque a restrição de percorrer primeiro a coordenada X e depois a coordenada Y é suficiente para evitar a ocorrência de deadlock.
Quando o algoritmo de roteamento encontra um canal de saída livre para uso, a conexão entre o VC de entrada e o VC de saída é estabelecido e a tabela de chaveamento é atualizada.
A tabela de chaveamento é composta por três vetores:
In, out e free.
O vetor in conecta um VC de entrada a um VC de saída.
O vetor out conecta um VC de saída a um VC de entrada.
O vetor free é responsável por modificar o estado do VC de saída de livre para ocupado.
Os número da porta e do número do VC.
Depois que todos os flits do pacote forem transmitidos, a conexão deve ser encerrada.
Isto pode ser feito de dois modos diferentes:
Por um trailer ou usando um contador de flits.
Um trailer requer um ou mais flits para serem usados como terminador de pacote e uma lógica adicional é necessária para detectar o trailer.
Para simplificar o projeto, o roteador possui um contador para cada VC de entrada.
O contador de um canal específico é inicializado quando o segundo flit do pacote é recebido, indicando o número de flits que compõem o payload.
O contador é decrementado a cada flit transmitido com sucesso.
Quando o valor do contador alcança zero, a posição do vetor free correspondente ao VC de saída vai para um, encerrando a conexão.
Em o contexto do presente trabalho a configuração da rede adotada é:
Topologia malha 3 x 2, suportando 6 nodos de processamento;
Largura de flit igual a 16 bits (optou- se por uma largura de flit inferior ao tamanho da palavra do processador (32 bits) para reduzir a área da rede);
Profundidade dos buffers igual a 8 flits;
Roteamento XY e arbitragem Round Robin;
Canais virtuais não são utilizados, simplificando o projeto do hardware.
Trabalhos futuros, em redes maiores, poderão incluir canais virtuais para redução da congestão na rede.
O Plasma é um processador RISC de 32 bits com um subconjunto de instruções da arquitetura MIPS.
Seu código (VHDL) é aberto, disponível através do OpenCores.
O pipeline de instruções do Plasma contém três estágios:
Busca, decodificação e execução.
Diferentemente da definição original do processador MIPS, a organização de memória do Plasma é Von Neumann e não Harvard.
Além disso, o Plasma oferece suporte ao compilador C (gcc) e tratamento de interrupções.
O diagrama da Figura 17 apresenta a interconexão do processador Plasma (mlite_ cpu) a alguns periféricos que acompanham a distribuição do processador:
Para atingir o objetivo pretendido neste trabalho, foram necessárias modificações no Plasma original.
Estas modificações dizem respeito à criação de novos mecanismos, exclusão de módulos e registradores mapeados em memória, bem como inclusão de novos módulos e novos registradores mapeados em memória.
Estas mudanças são discutidas a seguir.
Para tornar possível a execução de múltiplas tarefas sobre a mesma CPU, foi criado um mecanismo de paginação.
As mudanças realizadas afetam a geração de endereços.
Em o Plasma original o endereço é composto por 13 bits, fazendo com que a memória tenha uma capacidade de 8192 bytes.
O tamanho desse endereço foi aumentado, no contexto deste trabalho, em 3 bits, totalizando em 65536 bytes de memória.
A memória é dividida em quatro páginas de 4096 palavras.
Conforme mostra a Figura 18, os 2 bits mais significativos do endereço indicam a página e o restante indicam o deslocamento dentro de a mesma.
A o banco de registradores do processador (entidade Reg_ bank) foi adicionado um registrador, page, referente a a página da memória.
A configuração da página é realizada através da instrução mtc0 reg, 10.
Em esta instrução, o endereço inicial de uma tarefa na memória (offset) contido no registrador reg é carregado para o registrador 10 do CP0, que corresponde ao registrador page.
Mem_ ctrl) gera um endereço (mem_ address_ wop) que não contém a página, apenas o endereço lógico de deslocamento dentro de a mesma.
De essa forma, o endereço físico gerado por a CPU é composto por a concatenação do endereço lógico (mem_ address_ wop, fornecido por o controlador de memória) com a informação de página (page), como mostra a Figura 19.
Este mecanismo de paginação oferece segurança no acesso à memória evitando violação de endereços.
Isso quer dizer que uma tarefa residente na página px nunca conseguirá acessar qualquer endereço na página py (sendo x y), uma vez que todo endereço lógico gerado por a tarefa será concatenado com a página px.
O Plasma original não oferece suporte a interrupções de software.
No entanto, a aplicação pode fazer chamadas de sistema, requisitando um serviço ao sistema operacional como, por exemplo, envio e recebimento de mensagens.
De essa forma, foi necessária a inclusão da instrução syscall, prevista na arquitetura MIPS, ao conjunto de instruções do Plasma.
Esta instrução gera uma interrupção desviando o fluxo de execução do processador para um endereço pré-estabelecido, onde a chamada de sistema vai ser tratada.
Chamadas de sistema são abordadas na Seção 5.7.
Foi também desenvolvido um módulo responsável por fazer a interface entre o processador e a NoC, a Network Interface (Ni).
Outro módulo, DMA, foi desenvolvido para transferir o código-objeto de tarefas que chegam na Ni para a memória do processador.
Estas duas entidades são detalhadas nas próximas subseções.
A uart, contida no Plasma original, não se faz mais presente.
A Figura 20 mostra o diagrama do módulo Plasma, com os respectivos componentes internos, após as modificações realizadas.
Ainda, foram excluídos os registradores mapeados em memória gpioO_ reg e gpioA_ reg e foram acrescentados novos registradores destinados à comunicação entre as entidades Plasma, Ni e DMA.
São eles:
Mlite_ CPU Ni:
Configuration; Mlite_ CPU Ni e DMA Ni:
Status_ Read, Status_ Send, Read_ Data, Write_ Data, Packet_ ACK, Packet_ NACK, Packet_ END;
Visando facilitar a avaliação de desempenho do sistema, foi criado outro registrador mapeado em memória, chamado Tick_ Counter.
Este registrador acumula os ciclos de relógio durante a execução do sistema e pode ser lido por o microkernel ou por a aplicação através de uma chamada de sistema (ver Seção 5.7).
Com isso, pode- se medir o número de ciclos de relógio gastos para uma determinada operação do microkernel como, por exemplo, troca de contexto, escalonamento, ou para uma operação da aplicação.
Cada um dos registradores são abordados na seqüência do Capítulo, A Ni foi desenvolvida para realizar a interface entre o processador e a NoC Hermes.
Ela é responsável por:
Enviar pacotes para a rede, segmentando os dados deste pacote em flits.
Em este trabalho, a Ni recebe do processador um pacote cujos dados possuem 32 bits de tamanho e os divide em segmentos de 16 bits (tamanho do flit da NoC);
Receber pacotes da rede armazenando- os num buffer.
Quando existir um pacote completo no buffer ou quando o buffer estiver cheio, a Ni interrompe o processador para que este receba os dados.
Os segmentos de 16 bits referentes ao conteúdo do pacote recebido da rede são agrupados e repassados para o processador com palavras de 32 bits;
Repassar o código-objeto de tarefas recebido da rede, através do DMA, para a memória;
Informar ao processador (microkernel) qual a sua localização na rede (netAddress).
Um pacote que trafega na rede possui o seguinte formato:
Size indica o tamanho, em flits, do conteúdo do pacote;
Payload indica o conteúdo do pacote.
Os campos target e size possuem tamanho de 16 bits.
O campo payload é constituído por:
Todos os pacotes são montados por os drivers de comunicação presentes no microkernel do processador, fazendo parte da infra-estrutura de software da plataforma.
Estes drivers, além de montar os pacotes, os enviam à Ni e recebem pacotes da Ni.
A comunicação entre drivers e Ni acontece através de escrita/ leitura em/ de registradores mapeados em memória.
Estes registradores são descritos na Tabela 4.
Além destes registradores, existe ainda o registrador Configuration, cujo endereço é 0x20000140 e é lido por o microkernel de forma a conhecer a localização do processador na rede (endereço de rede -- netAddress).
A Figura 21 mostra os sinais da Ni que fazem interface com a NoC e os sinais que fazem interface com a CPU.
A Tabela 5 descreve cada um destes sinais.
Dado recebido da CPU a ser enviado para a NoC (registrador write_ data) data_ read Saída Dado recebido da NoC e repassado para a CPU (registrador read_ data) config Saída Informa à CPU seu endereço de rede (registrador Conforme já citado no início desta Seção, a NoC recebe e envia dados com tamanho de 16 bits e o processador escreve e recebe os dados com tamanho de 32 bits.
De essa forma, a Ni deve agrupar os dados provenientes da NoC repassando para o processador e dividir os dados provenientes do processador e repassar para a NoC.
O dado que a Ni recebe do processador é armazenado num buffer de 32 bits (buffer_ out).
O dado enviado para a NoC é a metade mais significativa deste buffer, como mostra a Figura 23 (a).
Para enviar a metade menos significativa, esta é deslocada para a metade mais significativa (Figura 23 (b)).
A Figura 24 apresenta a máquina de transição de estados de Ni que faz o envio de um pacote para a NoC.
A máquina de estados inicia no estado Starget.
Em este estado é enviado para a NoC o destino do pacote.
Uma vez que o destino do pacote possui 16 bits, a metade menos significativa do dado recebido do processador é copiada para a metade mais significativa de buffer_ out..
O estado avança para Ssize se o processador deseja enviar mais um dado A instrução nop é importante, pois a arquitetura MIPS tem a característica delayed-- branch, a qual executa as instruções após os saltos.
E se a NoC pode receber o dado.
Em Ssize é enviado para a NoC o tamanho, em flits, do pacote.
Assim como o destino, o tamanho do pacote também possui 16 bits.
Então, a metade menos significativa do dado recebido do processador é copiada para a metade mais significativa de buffer_ out..
O estado avança para Spayload se o processador deseja enviar mais um dado e se a NoC pode receber o dado.
Em Spayload é enviado todo o conteúdo do pacote para a NoC, cujos dados possuem 32 bits.
Aqui, os dados recebidos do processador são armazenados inteiramente em buffer_ out fazendo o deslocamento da metade menos significativa para a parte mais significativa.
A Figura 25 ilustra a segmentação de pacotes realizada por a Ni.
Em a Figura 25 (a) têm- se um pacote de requisição de mensagem montado por o driver.
Em a Figura 25 (b) têm- se o pacote segmentado em flits de 16 bits para o envio à NoC.
Cada campo do payload, que possui tamanho de 32 bits é divido em dois flits.
A máquina de transição de estados de Ni que faz o recebimento de um pacote é mostrada na Figura 27.
O estado inicial é Swait, em o qual espera- se receber o primeiro flit do pacote (correspondente ao destino).
Se existem mais dados a serem recebidos e tem espaço no buffer para armazenar- los, o estado avança para Ssize.
Em Ssize, o apontador LAST é incrementado e o tamanho do pacote é armazenado no buffer e também num sinal (size_ in) para o controle do recebimento.
Novamente, se existem mais dados a serem recebidos e tem espaço no buffer para armazenar- los, o estado avança para Swasting.
Em este estado, os flits do payload são armazenados no buffer e a cada flit armazenado, LAST é incrementado e size_ in é decrementado.
Quando o último flit do pacote deve ser armazenado no buffer, o estado passa a ser Sending.
Em Sending, além de armazenar o último flit do pacote no buffer, LAST aponta para a próxima linha do buffer.
A Figura 28 mostra como o driver recebe um dado da Ni.
Os endereços dos registradores Status_ Read e Read_ Data são carregados para os registradores 8 e 9, respectivamente (linhas 4 e 5).
Verifica- se se o dado está disponível através da leitura do endereço especificado por 8 (Status_ Read) (linha 8).
Se o dado não está disponível, é porque o buffer de Ni que armazena os dados advindos da NoC está vazio.
Então, o driver aguarda dentro de o laço por a chegada do dado no buffer para a leitura do mesmo.
Se o dado está disponível, ele é lido do endereço contido no registrador 9 (Read_ Data) (linha 12).
O DMA foi desenvolvido para transferir o código objeto de uma tarefa para a memória do processador permitindo que este continue sua execução.
O código objeto de uma tarefa é enviado por a rede para um determinado processador.
O DMA é responsável por transferir este código objeto para a memória do processador.
Assim como qualquer outro pacote, quem recebe o código objeto da tarefa é a Ni, armazenando- o no buffer de recebimento.
Uma vez o pacote recebido, as seguintes operações são realizadas:
A Ni interrompe a CPU informando a chegada de um pacote;
O microkernel, que executa na CPU e faz parte da infra-estrutura de software do sistema obtém o identificador da tarefa e o tamanho do código objeto da mesma e verifica a disponibilidade de página livre na memória.
A CPU informa ao DMA o endereço da memória a partir de o qual o código objeto deve ser transferido e o tamanho do código objeto;
O DMA faz acessos à Ni para ler o código objeto e acessos à memória para escrever o mesmo.
Quando o código já estiver alocado, o DMA interrompe a CPU informando que uma nova tarefa está na memória;
O microkernel faz as inicializações da tarefa, e partir disso a tarefa executará quando for escalonada.
Este mecanismo permite que o processador realize a execução das suas tarefas em paralelo com a recepção de novas tarefas.
A comunicação entre o microkernel e o DMA ocorre através de registradores mapeados em memória.
A Tabela 6 descreve estes registradores.
A Figura 29 mostra os sinais do DMA que fazem interface com a CPU, a Ni e a memória.
A Tabela 7 descreve cada um desses sinais.
Indica o endereço da memória a partir de o qual o código objeto deve ser transferido (registrador Set_ DMA_ Address) Indica o tamanho do código objeto a ser transferido (registrador Set_ DMA_ Size) Indica que a transferência deve ser iniciada (registrador Start_ DMA) Informa a Ni que recebeu um dado (um flit) Indica se tem dado disponível para leitura na Ni.
Interrompe o processador quando termina a transferência Indica que o processador já reconheceu a interrupção (registrador DMA_ ACK) Informa à memória o endereço em que o dado deve ser escrito Informa à memória que deseja escrever Indica que não pode escrever na memória.
Informa a memória o dado a ser escrito Dado recebido da Ni que vai ser enviado para a memória do processador.
O DMA comunica- se com a Ni através dos registradores mapeados em memória que a CPU utiliza para comunicar- se com a Ni (Status_ Read e Read_ Data).
A Figura 30 ilustra a máquina de transição de estados do DMA.
O estado inicial é Swait, em o qual são conhecidos o endereço da memória a partir de o qual o código objeto vai ser transferido e o tamanho do código objeto.
Também neste estado a interrupção é desativada, após a CPU informar que reconheceu a interrupção.
Se a CPU informa ao DMA que pode iniciar a transferência, o estado avança para Scopy.
Em este estado, cada dado referente a o código objeto é buscado na Ni e escrito na memória do processador.
A cada escrita, o endereço da memória é incrementado e o tamanho do código objeto é decrementado.
Quando o tamanho do código objeto é zero, o estado avança para Send, onde a escrita na memória é desabilitada e a CPU é interrompida.
O estado passa a ser Swait novamente.
Este Capítulo descreve os serviços do microkernel desenvolvido para o gerenciamento de tarefas na plataforma MPSoC.
Cada processador escravo componente do sistema possui uma cópia deste microkernel.
As principais funções do microkernel são o suporte à execução de múltiplas tarefas e a comunicação entre tarefas.
Este microkernel corresponde à segunda contribuição deste trabalho.
Um sistema operacional multitarefa permite que várias tarefas compartilhem o uso de uma mesma CPU, ou seja, várias tarefas são executadas concorrentemente.
Essa abordagem requer gerenciamento de memória e proteção de memória, uma vez que várias tarefas compartilharão o mesmo espaço de armazenamento;
Escalonamento, visto que as tarefas irão concorrer por a mesma CPU e;
Mecanismos de comunicação entre as tarefas.
Conforme mencionado no Capítulo 4, cada processador possui uma memória local.
Esta memória é dividida em páginas.
O microkernel reside na primeira página e as tarefas residem nas páginas subseqüentes.
De essa forma, a gerência de memória empregada neste trabalho, preocupa- se apenas em determinar a página em a qual reside a tarefa que está executando.
A geração de endereços físicos é realizada por o mecanismo de paginação visto na Seção 4.2.
O escalonamento de tarefas é preemptivo.
O algoritmo utilizado é o Round Robin, em o qual as tarefas são escalonadas de forma circular e cada uma de elas executa durante uma fatia de tempo (timeslice).
Sempre que uma tarefa nova deve ser escalonada, o contexto da tarefa que estava executando deve ser salvo.
Após o escalonamento, o contexto da nova tarefa é restaurado.
O acesso à memória é caracterizado como Norma (Em o Remote Memory Access), ou seja, a memória local de um processador não é acessível a outro.
Assim, a comunicação entre tarefas ocorre através de troca de mensagens.
Para tanto, são utilizados pipes de mensagens.
Cada tarefa possui um pipe, em o qual são armazenadas todas as mensagens que esta tarefa envia a outras.
Para enviar e receber mensagens, tarefas fazem chamadas de sistema.
Quando tarefas localizadas em processadores diferentes desejam se comunicar, é necessária a comunicação entre processadores.
Drivers de comunicação montam e desmontam pacotes contendo as informações das mensagens que são enviadas através da rede de interconexão.
A estrutura do microkernel multitarefa é mostrada na Figura 31.
Ela é composta por diferentes níveis de serviços.
Em o nível mais abaixo encontra- se o serviço de inicialização do sistema (Boot), onde são inicializados os ponteiros para dados globais (gp) e para pilha (sp), a seção de dados estáticos e as estruturas de dados para gerenciamento das tarefas.
Após a inicialização, o serviço de boot aciona o escalonador.
Em o nível acima de o nível de boot (Nível 2), encontram- se os drivers de comunicação.
O último nível (Nível 3) é composto por os serviços de tratamento de interrupções, escalonamento, comunicação entre tarefas e chamadas de sistema.
Os serviços do microkernel multitarefa foram implementados parte em linguagem C e parte em linguagem de montagem (assembly).
Os drivers de comunicação e operações de tratamento de interrupção, como salvamento e recuperação de contexto, são implementados em assembly devido a o acesso a registradores facilitado por a linguagem;
Estruturas de dados e funções como escalonamento, chamadas de sistema e comunicação entre tarefas são implementadas em linguagem C, pois é uma linguagem de alto nível facilitando a programação destas funções.
Conforme mencionado no início deste Capítulo, a gerência de memória empregada aqui preocupa- se apenas em determinar a página onde a execução acontece.
Não existe o serviço de alocação dinâmica de memória, pois, além de aumentar o tamanho e complexidade do microkernel, este serviço não se faz necessário para atingir o objetivo pretendido neste trabalho.
O tamanho do microkernel desenvolvido é 6,6 KB, utilizando 41% da página, que possui capacidade de 16 KB.
Antes de abordar os serviços que compõem o microkernel, é necessário conhecer as estruturas de dados utilizadas por o mesmo para o gerenciamento das tarefas.
Para gerenciar a execução das tarefas, o microkernel mantém um TCB (Task Control Block) para cada tarefa, em o qual estão contidos, entre outros dados, os valores de 30 registradores do Plasma.
A Figura 32 apresenta, em linguagem C, a estrutura TCB.
Os registradores salvos no TCB são os registradores temporários, registradores salvos, registradores de argumentos, registradores de retorno, endereço de retorno(ra), o ponteiro para a pilha de dados(sp), o ponteiro para dados globais(gp) e os registradores utilizados nas operações de multiplicação e divisão(hi elo).
Além destes registradores, é armazenado, para cada tarefa, o contador de programa (pc), o offset que indica seu endereço inicial na memória, seu identificador (id) e seu status.
O status de uma tarefa pode ser:
Pronta (READY) ­ quando está pronta para executar;
Executando (RUNNING) ­ quando está utilizando a CPU;
Terminada (TERMINATED) ­ quando terminou sua execução;
Esperando (WAITING) ­ quando requisita uma mensagem e aguarda a resposta;
Livre (Free) ­ quando o TCB da tarefa está livre e pode ser alocado;
Alocando (ALLOCATING) ­ quando o TCB está sendo alocado.
É mantido também um pipe global de mensagens.
Um pipe é uma área de memória destinada à comunicação entre tarefas.
Em ele ficam armazenadas (até serem consumidas) as mensagens que as tarefas enviam a outras tarefas.
Associados ao pipe, estão pipe_ order, que indica a ordem de chegada das mensagens no pipe e pipe_ ocupation que indica quais posições do pipe estão ocupadas.
O pipe possui capacidade para armazenar 10 mensagens.
Uma mensagem (Message) armazenada no pipe possui um tamanho (length), uma tarefa destino (target), uma tarefa fonte (source) e o conteúdo (msg).
O conteúdo de uma mensagem possui tamanho máximo de 128 inteiros.
É mantida também, uma tabela chamada task_ location que estabelece a localização de todas as tarefas do sistema:
Qual tarefa está localizada em qual processador.
Esta tabela é consultada no momento de uma comunicação entre tarefas, conforme será explicado adiante, na Seção 5.6.
O microkernel gerência a execução de até três tarefas:
T1, t2 e t3.
Esta limitação é imposta apenas no contexto deste trabalho, por restrições de memória, sendo possível parametrizar o código de forma a suportar um número maior de tarefas.
Existe ainda, uma tarefa inativa (idle).
Em a prática, a tarefa idle é apenas uma função localizada dentro de o microkernel, que executa um laço infinito.
Ela é um artifício utilizado para permitir que o microkernel fique aguardando por interrupções provenientes da NoC, quando não tem tarefas a serem escalonadas e executadas.
De essa forma, enquanto tarefa idle está executando, as interrupções permanecem habilitadas e o microkernel pode receber e enviar pacotes.
É mantido um vetor de TCBs com quatro posições:
Cada índice do vetor corresponde a uma tarefa.
A tarefa idle está localizada no último índice do vetor.
Cada processador escravo componente do MPSoC possui uma cópia do microkernel.
Quando sua execução é iniciada, são atribuídos valores iniciais a algumas variáveis do sistema e estruturas de dados.
Os registradoresgp esp do Plasma são inicializados, respectivamente, com o ponteiro para dados globais e ponteiro para a pilha referentes ao microkernel.
Estes valores são conhecidos por o microkernel, para que, sempre que este retome a execução do sistema (ocorrência de interrupções ou chamadas de sistema), ele possa reconfigurar os registradoresgp esp.
Isto é necessário, pois antes do fluxo de execução passar para o microkernel, uma tarefa pode estar sendo executada e, assim sendo, os registradoresgp esp possuem, respectivamente, o ponteiro para dados globais e ponteiro para a pilha da tarefa.
Inicialmente, nenhuma tarefa está alocada na CPU.
Assim, o microkernel inicializa o vetor de TCBs, indicando que estão todos livres (status $= FREE).
A cada TCB é associada uma página da memória, atribuindo- o um offset, como mostra a Figura 33.
Em a primeira página encontra- se o microkernel.
A tabela de localização de tarefas (task_ location) é inicializada indicando que todas suas entradas estão livres.
A estrutura pipe_ ocupation também é inicializado indicando que todas as posições do mesmo estão desocupadas.
Os endereços das rotinas que tratam dos diferentes tipos de interrupção são armazenados num vetor de ponteiros para funções (ISR).
Estas rotinas são registradas, atribuindo seus respectivos endereços a uma posição deste vetor.
As interrupções registradas são interrupções advindas da NoC, de um contador de timeslice e do DMA.
Por fim, a máscara de interrupções é configurada para conter cada uma dessas interrupções.
Este processo é explicado na próxima Subseção.
Após as inicializações, a tarefa idle é escalonada.
Enquanto esta tarefa está em execução, o sistema aguarda por uma interrupção da NoC (chegada de pacotes) ou do DMA (nova tarefa na memória).
Um processador componente do MPSoC pode ser interrompido via hardware ou via software.
Assim, uma interrupção pode ser proveniente:
De a Ni, para que ele receba pacotes advindos da NoC (interrupção de hardware);
Deve ser escalonada.
Timeslice é uma fatia de tempo durante a qual uma tarefa é executada (interrupção de hardware);
No caso de as interrupções de hardware, o processador mantém um registrador contendo a máscara das interrupções (irq_ mask_ reg) e outro registrador contendo o status das interrupções (irq_ status), ou seja, quais as interrupções ativas.
O registrador de status é composto por os bits mostrados na Figura 34.
Assim, cada fonte de interrupção de hardware contém uma máscara conhecida por o microkernel.
Essas fontes e suas respectivas máscaras são mostradas abaixo:
Em a inicialização do sistema, o microkernel registra uma rotina de tratamento de interrupção para cada tipo de interrupção:
Os_ InterruptRegister (IRQ_ COUNTER, Scheduler);
Os_ InterruptRegister (IRQ_ NOC, DRV_ Handler);
Os_ InterruptRegister (IRQ_ DMA, DMA_ Handler);
Registrar significa armazenar num vetor de ponteiros para função (ISR) o endereço da rotina responsável por o tratamento de determinada interrupção, associando a posição neste vetor com a máscara.
Assim, quando ocorrer uma interrupção cuja fonte é o contador de timeslice, por exemplo, e cuja máscara é definida por IRQ_ COUNTER, a função Scheduler entrará em execução.
Após associar as rotinas de tratamento de interrupção, a máscara de interrupções é configurada, habilitando as três interrupções:
Os_ InterruptMaskSet;
Com isso, a máscara possui o valor 00111000.
O processador é interrompido quando a operação And entre a máscara de interrupções e o status das interrupções possui um resultado diferente de Zero.
Quando isso ocorre, o fluxo de execução salta para um endereço fixo:
O endereço 0x3C, onde o tratamento é iniciado.
Enquanto uma interrupção é tratada, não podem ocorrer novas interrupções.
Assim, as interrupções são desabilitadas (via hardware) no início do tratamento e realibilitadas ao término (via software).
O tratamento de interrupções pode ser mais bem explicado numa seqüência de passos, conforme a seguir.
O primeiro passo do tratamento de uma interrupção é o salvamento de contexto da tarefa que estava sendo executada.
O microkernel mantém numa variável global, current, o endereço do TCB da tarefa vigente.
Em este TCB são salvos os registradores e o pc da tarefa.
Antes da interrupção, os registradoressp egp do processador são referentes à tarefa em execução.
De essa forma, eles são configurados com valores referentes ao microkernel;
É chamada uma rotina (Os_ InterruptServiceRoutine) responsável por verificar qual a causa da interrupção e, conseqüentemente, chamar a função designada a tratar a interrupção;
Após a interrupção tratada, o contexto da tarefa é restaurado.
Se a interrupção foi causada por o contador de timeslice, uma nova tarefa foi escalonada e começa a ser executada.
Senão, a tarefa que estava executando antes de interrupção retoma sua execução.
A Figura 35 mostra uma interrupção advinda do contador de timeslice.
Os sinais irq_ mask_ reg e irq_ status indicam, respectivamente, a máscara de interrupções (quais interrupções estão sendo esperadas) e o status das interrupções (quais interrupções estão ativas).
O sinal intr_ enable indica quando as interrupções estão habilitadas.
O sinal intr_ signal indica quando o processador foi interrompido e page indica a página da memória em que se encontra o fluxo de execução.
As legendas da Figura são explicadas a seguir.
O processador executa a tarefa que está na página 1;
O registrador de status tem o seu 4º bit configurado em 1;
A operação And entre a máscara de interrupções e o registrador de status resulta num valor diferente de zero;
Uma interrupção é gerada e novas interrupções são desabilitadas;
O microkernel entra em execução, salva o contexto da tarefa suspendida e verifica que a interrupção foi causada por o contador de timeslice;
A rotina chamada para tratar essa interrupção é a rotina Scheduler, que faz o escalonamento de uma nova tarefa:
A tarefa que se encontra na página 2 (page $= 2).
Uma interrupção advinda da NoC é mostrada na Figura 36.
As legendas da Figura são explicadas a seguir.
O registrador de status tem o seu 6º bit configurado em 1;
A operação And entre a máscara de interrupções e o registrador de status resulta num valor diferente de zero;
Uma interrupção é gerada e novas interrupções são desabilitadas;
O microkernel entra em execução, salva o contexto da tarefa suspendida e verifica que a interrupção foi causada por a chegada de pacotes por a NoC.
A rotina chamada para tratar essa interrupção é a DRV_ Handler, que trata o pacote;
De a mesma forma que acontece nas interrupções de hardware, nas interrupções de software, o fluxo de execução também salta para um endereço fixo:
O endereço 0x44 e tem as seguintes operações:
Salvamento parcial de contexto, ou seja, apenas alguns registradores são salvos:
Registradores de argumento, endereço de retorno(ra), ponteiro para dados globais(gp) e ponteiro para a pilha(sp);
Configuração de osp egp do microkernel;
Chamada da função Syscall, que trata a chamada de sistema;
Recuperação do contexto parcial da tarefa que causou a interrupção e retomada de sua execução;
O serviço de chamadas de sistema é abordado mais adiante, na Subseção 5.7.
O escalonamento utilizado por o microkernel é preemptivo e sem prioridades.
A política de escalonamento é Round Robin, em a qual as tarefas são escalonadas de maneira circular.
A função de escalonamento é mostrada na Figura 37.
Os_ InterruptMaskSet (IRQ_ COUNTER);
MemoryWrite; O microkernel mantém nas variáveis globais current e rounRobin, o endereço do TCB da tarefa que está executando e o índice do vetor de TCBs correspondente a esta tarefa, respectivamente.
O status da tarefa que foi interrompida passa a ser READY (linha 6).
O escalonador procura a próxima tarefa a ser executada:
Se rounRobin indica que a tarefa que estava executando é a última do vetor1, então a tarefa a executar deve ser a primeira;
Em este caso, a última tarefa do vetor significa a tarefa que está no índice MAXLOCALTASKS-1, a qual corresponde a uma tarefa da aplicação.
Em a prática, a que última tarefa do vetor (a que se encontra no índice MAXLOCALTASKS), é a tarefa idle, que só é escalonada quando nenhuma tarefa da aplicação está pronta para executar.
A Figura 38 ilustra o escalonamento circular de três tarefas:
T1 que se encontra na página 1, t2 na página 2 e t3 na página 3.
A tarefa t1 executa durante seu timeslice;
Uma interrupção é gerada, novas interrupções são desabilitadas e o microkernel entra em execução;
O contexto da tarefa que foi suspendida, t1, é salvo e o escalonador escalona a próxima tarefa:
T2. O contexto de t2 é carregado e o fluxo de execução salta para a página 2.
As interrupções são habilitadas;
A tarefa t2 executa durante seu timeslice.
Uma interrupção é gerada, novas interrupções são desabilitadas e o microkernel entra em execução;
O contexto da tarefa que foi suspendida, t2, é salvo e o escalonador escalona a próxima tarefa:
T3. O contexto de t3 é carregado e o fluxo de execução salta para a página 3.
As interrupções são habilitadas.
Uma troca de contexto totaliza 589 ciclos e uma tarefa executa durante 8194 ciclos.
Sistemas operacionais em geral, utilizam um timeslice 10 vezes maior do que a troca de contexto.
Aqui, o timeslice é parametrizável e é aproximadamente 14 vezes maior do que o tempo de troca de contexto, pois tem- se por objetivo reduzir a sobrecarga de processamento por parte de o microkernel.
A Tabela 8 mostra o tempo gasto para cada operação da troca de contexto:
Salvamento de contexto, escalonamento e recuperação de contexto.
Há casos em que o escalonador deve entrar em execução sem que ocorra uma interrupção do contador de timeslice.
Isto acontece quando:
Uma tarefa termina sua execução (Seção 5.7);
Uma tarefa tenta receber uma mensagem que está em outro processador, gerando um pacote de requisição e aguardando sua resposta (Seção 5.6);
A tarefa idle está executando e uma nova tarefa é alocada na memória do processador (Seção 6.2).
Em estes quatro casos, uma variável global, needTaskScheduling, é configurada para 1 indicando que uma nova tarefa deve ser escalonada.
Esta variável é sempre configurada para 0 antes do tratamento de uma interrupção (de hardware ou software) e verificado após de o tratamento, pois é apenas durante o tratamento que ela é configurada para 1, ou seja, apenas um evento que causou uma interrupção pode ocasionar um escalonamento.
Tarefas podem cooperar em tempo de execução, trocando informações entre si.
A comunicação entre tarefas ocorre através de pipes.
Segundo Tanenbaun, um pipe é um canal de comunicação em o qual mensagens são consumidas na ordem em que são armazenadas no mesmo.
Em o contexto deste trabalho, um pipe é uma área de memória pertencente ao microkernel reservada para troca de mensagens entre tarefas, onde as mensagens são armazenadas de forma ordenada e consumidas de acordo com a ordem.
A Figura 39 apresenta três formas de implementar a comunicação entre tarefas através de pipes.
Em a primeira), é mantido um pipe único, em o qual são armazenadas todas as mensagens de tarefas locais enviadas para quaisquer tarefas.
Em a segunda), cada tarefa possui um pipe exclusivo associado a ela, em o qual ficam armazenadas todas as mensagens recebidas de outras tarefas.
De essa forma, se a tarefa t3 enviar uma mensagem (msg) para t2, a mensagem será armazenada no pipe de t2 (pipe2).
Quando t2 desejar receber esta mensagem, ela será buscada no pipe2.
Assim como na segunda forma de implementação da comunicação, na terceira), cada tarefa possui um pipe exclusivo associado a ela.
Contudo, neste pipe ficam armazenadas as mensagens que ela envia a outras tarefas.
Assim, se a tarefa t3 enviar uma mensagem (msg) para t2, a mensagem será armazenada no pipe de t3 (pipe3).
Quando t2 desejar receber esta mensagem, ela será buscada no pipe3.
A abordagem utilizada neste trabalho é a de pipe global.
Contudo, antes disso, foram implementadas e testadas as outras duas abordagens, em as quais foram encontrados problemas conforme descrito a seguir.
Write_ pipe(&amp; msg, 2);
Write_ pipe(&amp; msg, 2);
Write_ pipe(&amp; msg, 2);
T2 t2 Read_ pipe(&amp; msg, 3);
Read_ pipe(&amp; msg, 3);
Read_ pipe(&amp; msg, 3);
Read_ pipe(&amp; msg, 2);
Read_ pipe(&amp; msg, 2);
Em a abordagem da Figura 39 (b), se o pipe de uma tarefa estiver cheio, o processador não pode mais receber mensagens e a rede pode ficar bloqueada.
A Figura 40 ilustra esta situação.
Suponha que duas tarefas t1 e t2, respectivamente em P8 e P4 enviem, cada uma, uma mensagem à t3 em P6.
A tarefa t3 aguarda as mensagens nesta ordem:
Primeiro de t1 e depois de t2.
Suponha então, que a mensagem de t2 chegue primeiro) e o pipe de t3 fique cheio.
Quando a mensagem de t1 chegar, não vai haver espaço no pipe) e, uma vez que t3 não consegue consumir a mensagem esperada, a rede vai ficar bloqueada.
Assim, esta forma de comunicação torna o sistema suscetível a situações de bloqueio facilmente.
Read t1 t3 Read t pipe cheio!
Este problema de bloqueio pode ser resolvido com a abordagem da Figura 39 (c), em o qual uma mensagem é escrita no pipe da tarefa fonte e só enviada através da rede mediante requisição por a tarefa destino.
Quando a mensagem requisitada não está disponível, é enviado um pacote de controle indicando que não existe mensagem ainda.
A desvantagem desta abordagem é o uso não otimizado da área de memória dos pipes, no processo de escrita de mensagens:
Uma tarefa pode ser altamente cooperante escrevendo muitas mensagens, enquanto outra raramente escreve mensagens.
De essa forma, o pipe global otimiza o uso da área de memória destinado à comunicação entre tarefas e evita também situações de bloqueio na rede, uma vez que, da mesma forma que na abordagem da Figura 39 (c), as mensagens são enviadas através da rede mediante requisições.
As tarefas se comunicam através de duas primitivas.
Para o envio de uma mensagem, uma tarefa utiliza a primitiva:
WritePipe(&amp; mensagem, id_ destino) onde&amp; mensagem especifica o endereço lógico (dentro de a página onde está a tarefa) em que está armazenada a mensagem e id_ destino é o identificador da tarefa para a qual a mensagem está sendo enviada.
Para o recebimento de uma mensagem é utilizada a primitiva:
ReadPipe(&amp; mensagem, id_ fonte) onde&amp; mensagem especifica o endereço lógico (dentro de a página onde está a tarefa) em que a mensagem será armazenada e id_ fonte é o identificador da tarefa que enviou a mensagem.
A comunicação pode acontecer entre tarefas que residem em processadores diferentes, como mostra a Figura 41.
Quando uma tarefa t5, no processador Proc2, deseja receber uma mensagem de uma tarefa t2, o microkernel verifica qual a localização de t2 (task_ location, citado na Seção 5.2).
Se t2 encontra- se no processador local, o microkernel copia a mensagem do pipe para a página de t5, como mostra a Figura 39 (a).
Em este exemplo, t2 encontra- se num processador remoto, Proc1.
De essa forma, a comunicação ocorre numa seqüência de passos:
O microkernel em Proc2 monta um pacote de requisição (request_ msg) e envia a Proc1, requisitando uma mensagem de t2 para t5);
A tarefa t5 é colocada em espera (status $= WAITING) e uma nova tarefa é escalonada em Proc2;
O microkernel em Proc1 recebe a requisição e verifica no pipe se existe uma mensagem para t5.
Se sim, o pacote contendo as informações e o conteúdo da mensagem é enviado a Proc2, como mostra a Figura 41 (b).
Se não, é enviado um pacote informando que não há mensagem (em o_ message);
Quando Proc2 recebe a resposta da requisição, t5 passa a ter status READY e pode ser novamente escalonada para continuar sua execução.
Se a resposta contiver a mensagem esperada, ela é copiada para o endereço especificado por&amp; msg, na primitiva de comunicação.
Este endereço é localizado dentro de a página da tarefa.
Se a resposta for em o_ message, t5 pode tentar novamente receber a mensagem.
Para isso, o recebimento da mensagem, na aplicação, deve ser implementado num laço:
É importante salientar que uma tarefa entra em estado de espera apenas quando ela faz ReadPipe de uma mensagem cuja tarefa está num processador remoto.
Ou seja, no momento em que ela requisita uma mensagem e deve aguardar o pacote de resposta.
Caso contrário, isto é, quando a mensagem é de uma tarefa local, não é preciso aguardar resposta.
A primitiva chamada retorna o resultado da operação e a tarefa decide se quer tentar novamente executar ReadPipe.
Assim como a primitiva ReadPipe pode não ser concluída com sucesso, ou seja, quando a mensagem esperada não está disponível, o envio de mensagens também pode falhar.
Isso acontece quando o pipe está cheio e, portanto, não há mais espaço para novas mensagens.
De essa forma, o envio de uma mensagem também pode ser implementado dentro de um laço:
As mensagens são ordenadas no momento em que são armazenadas nos pipes.
Para cada mensagem é associado um número inteiro indicando sua ordem (para isso é utilizado o vetor pipe_ order (visto na Seção 5.2).
Em o envio de uma mensagem, o pipe é percorrido verificando se já existem mensagens da tarefa fonte para a tarefa destino.
Se afirmativo, é verificado qual a maior ordem existente.
A nova mensagem é armazenada no pipe indicando em pipe_ order que sua ordem é a maior ordem encontrada mais 1.
Quando uma tarefa desejar receber uma mensagem, a mensagem repassada a ela será a que tiver menor ordem.
As primitivas de comunicação entre tarefas (WritePipe e ReadPipe) ocasionam chamadas de sistema, de forma que o microkernel assume o controle, gerenciando a leitura e escrita nos pipes, bem como a leitura e escrita de mensagens em endereços de memória de diferentes páginas.
As chamadas de sistema são tratadas a seguir.
Uma chamada de sistema é uma interrupção gerada por o software com intuito de requisitar um serviço do sistema operacional.
Diversos sistemas operacionais possuem chamadas de sistema para diferentes propósitos, entre eles, gerenciamento de processos, gerenciamento de arquivos, proteção, gerenciamento de tempo, realizar operações de E/ S, entre outros.
Em este trabalho, chamadas de sistemas são utilizadas para realizar a comunicação entre tarefas, para terminar a execução de uma tarefa e para informar a uma tarefa o valor do contador de ciclos de relógio (Tick_ Counter).
Em a prática, as primitivas de comunicação WritePipe e ReadPipe são definidas como sendo uma função denominada SystemCall, que recebe como primeiro parâmetro um inteiro especificando o serviço.
Além desse primeiro parâmetro, ela recebe os outros dois argumentos advindos da primitiva, como mostra a Figura 42.
A Figura 43 mostra a função SystemCall.
Ela é implementada em assembly e gera a chamada de sistema através da instrução syscall (linha 5), logo após retornando à execução da tarefa (linha 7).
Conforme já visto na Seção 4.2, a instrução syscall foi acrescentada ao conjunto de instruções do processador Plasma, pois na sua distribuição original, esta instrução não era reconhecida.
Também, foi visto na Seção 5.4 que, quando a aplicação executa uma primitiva que gera uma chamada de sistema:
O contexto da tarefa é salvo parcialmente;
É chamada uma função, em C, denominada Syscall responsável por realizar a chamada de sistema retornando o resultado da operação para a tarefa;
O contexto parcial da tarefa é recuperado e;
A tarefa retoma sua execução.
A rotina implementada em C, responsável por tratar a chamada de sistema é mostrada na Figura 44.
Uma chamada de sistema espera 3 argumentos.
O primeiro indica o serviço desejado:
1 para WritePipe;
2 para ReadPipe;
0 para Exit e 3 para GetTick.
Exit não é uma chamada de sistema é gerada passando como argumento o valor 0.
O procedimento tomado para este serviço é discutido na Seção 6.1.3.
Se o serviço desejado for GetTick, dois argumentos seguintes são ignorados e o valor do relógio é informado à tarefa.
Se o serviço solicitado for WritePipe, os dois parâmetros seguintes possuem, respectivamente, o endereço lógico em o qual se encontra a mensagem e o identificador da tarefa destino (linhas 3 a 5).
Se for ReadPipe, os dois parâmetros seguintes possuem, respectivamente, o endereço lógico para onde a mensagem vai ser copiada e o identificador da tarefa fonte.
Em estes dois últimos serviços é retornado 0 à tarefa para indicar que a operação não pôde ser efetuada e 1 para indicar que foi bem sucedida.
Conforme visto na Seção 4.3 os drivers fazem o envio de pacotes para a NoC e o recebimento de pacotes da NoC.
Foram vistos também os serviços que um pacote carrega e o formato do pacote para cada tipo de serviço.
Para cada serviço, existe um tratamento.
De essa forma, a Figura 45 apresenta a função que é chamada quando acontece uma interrupção proveniente da chegada de pacotes por a NoC.
Esta função faz chamadas aos drivers de comunicação necessários para o tratamento dos pacotes.
Todos os drivers são implementados em assembly.
Em esta seção são discutidos apenas os drivers de comunicação para os serviços de troca de mensagens.
Os drivers de comunicação para os serviços de alocação de tarefas são objetos de discussão do Capítulo 6.
DRV_ ReadService(&amp; service);
Switch (service) case REQUEST_ MESSAGE:
DRV_ DeliverMessage (netAddress);
Break; Quando chega um pacote por a NoC, a primeira informação a se saber é o serviço que este pacote carrega.
Assim, é chamado um driver (DRV_ ReadService) que lê da Ni o serviço (linha 4).
Se o serviço for REQUEST_ MESSAGE, é chamado o driver DRV_ DeliverMessage, responsável por fazer a devolução da mensagem requisitada, passando o endereço de rede do processador local (netAddress) como parâmetro para a montagem do pacote (linha 9).
Este driver lê da Ni o processador fonte, a tarefa destino e a tarefa fonte da mensagem.
Com base nestas duas últimas informações, a mensagem é procurada no pipe.
Se ela existe, o pacote com a mensagem é montado como segue:
Se a mensagem não existe, é enviado um pacote com o serviço Em o_ MESSAGE, indicando que a mensagem não existe.
O pacote é montado da seguinte forma:
Se o serviço que o pacote carrega é DELIVER_ MESSAGE, é chamado o driver DRV_ ReadMessage, responsável por ler a mensagem e transferir- la para a aplicação.
Este driver lê da Ni o processador fonte, a tarefa destino e a tarefa fonte da mensagem.
O endereço da memória para onde a mensagem deve ser copiada é buscado no TCB da tarefa:
Registradora1, que contém o primeiro parâmetro da chamada de sistema que a tarefa realizou (ReadPipe(&amp; msg, source_ id).
Este endereço encontra- se na página onde a tarefa que fez a chamada ReadPipe está alocada.
Então, o driver lê da Ni o tamanho e o conteúdo da mensagem copiando estas informações para o endereço especificado por a tarefa.
O driver retorna 1 para a tarefa indicando que a chamada ReadPipe foi concluída com sucesso e o status da tarefa é configurado para READY.
Uma tarefa que faz uma chamada ReadPipe requisitando uma mensagem que está num processador remoto é colocado em estado de espera (status $= waiting), não sendo mais escalonada até receber a resposta da requisição.
De essa forma, após receber a resposta, é verificado se a tarefa que estava executando no momento da chegada desta resposta era a tarefa idle.
Se sim, a variável needTaskScheduling indica que é preciso escalonar uma nova tarefa Se o serviço for Em o_ MESSAGE, é chamado o driver DRV_ NoMessage.
Este driver lê da Ni o processador fonte, a tarefa destino e a tarefa fonte da mensagem.
O driver retorna 0 para a tarefa indicando que a chamada ReadPipe não foi concluída com êxito e o status da tarefa passa a ser READY.
O serviço Em o_ MESSAGE, assim como o DELIVER_ MESSAGE é uma resposta do serviço REQUEST_ MESSAGE.
Este, por sua vez, gerado por a chamada ReadPipe (com mensagem remota).
Desta forma, assim como acontece com DELIVER_ MESSAGE, após receber a resposta, é verificado se a tarefa que estava executando antes de chegar esta resposta é a tarefa idle.
Se sim, a variável needTaskScheduling indica que é preciso escalonar uma nova tarefa.
Este Capítulo apresenta a estratégia de alocação de tarefas implementada neste trabalho.
Foram desenvolvidas duas estratégias de alocação:
Alocação estática e alocação dinâmica.
Este mestre possui uma interface com um repositório de tarefas, responsável por o armazenamento do código executável de todas as tarefas que devem ser executadas no sistema.
A Seção 6.2 apresenta como as tarefas são recebidas e alocadas nos nodos escravos.
A arquitetura é apresentada de forma simplificada na Figura 46.
Diferentemente dos nodos escravos, o nodo mestre executa apenas a aplicação de alocação.
A aplicação de alocação de tarefas compreende estruturas de dados necessárias para gerenciar a alocação, drivers de comunicação com escravos e tratamento de interrupções provenientes da NoC.
O mestre possui ainda uma interface com um repositório de tarefas, o qual é uma memória externa com códigos objetos de tarefas a serem alocadas no MPSoC.
Trabalhos futuros compreendem tornar a aplicação de alocação uma tarefa genérica, permitindo que ela compartilhe tempo de execução com as demais aplicações do sistema.
O repositório de tarefas fisicamente é uma memória de grande capacidade, externa ao sistema MPSoC.
Ele está conectado diretamente ao mestre, e não à rede, visando reduzir o tráfego na rede e aumentar o desempenho do sistema.
A interface entre o processador mestre e o repositório de tarefas é composta por os sinais address, que indica o endereço de leitura na memória e data_ read, que indica o dado lido.
Conforme mostra a Figura 17 (página 46), no Plasma, a memória externa possui endereçamento de 0x10000000 a 0x1 f.
A Figura 47 mostra a estrutura do repositório de tarefas, com duas tarefas.
Para cada tarefa armazenada no repositório são conhecidas as seguintes informações:
Identificador (id), tamanho do código objeto da tarefa (size) e endereço inicial do código objeto (initial_ address).
Estas informações estão nos primeiros endereços do repositório, constituindo um cabeçalho.
A partir deste cabeçalho encontram- se os códigos objetos das tarefas.
O número de tarefas presentes no repositório é uma informação contida na aplicação de alocação.
Associado ao vetor slaves, utiliza- se uma estrutura denominada free_ pages, a qual indica quantas páginas livres cada escravo possui.
Inicialmente, todos têm 3 páginas livres.
Sempre que uma tarefa for alocada num escravo S, este tem o número de páginas livres decrementada.
Uma tabela, static_ allocation, contém as informações de alocação estática.
Em ela, os campos task e slave indicam, respectivamente, que a tarefa task deve ser alocada no processador slave.
A tabela é criada como mostra a Figura 48.
As chamadas à função InsertStaticAllocation inserem três entradas na tabela indicando, respectivamente, que a tarefa com identificador 1 deve ser alocada no escravo SLAVE1;
A tarefa com identificador 2 deve ser alocada no escravo SLAVE2 (linha 4) e;
A tarefa com identificador 3 deve ser alocada no escravo SLAVE3 (linha 7).
Após atribuir uma tarefa a um escravo S, a função OccupiedPage, decrementa o número de páginas livres de S.
InsertStaticAllocation; OccupiedPage;
InsertStaticAllocation (2, SLAVE2);
OccupiedPage (SLAVE2);
InsertStaticAllocation (3, SLAVE3);
OccupiedPage (SLAVE3);
A definição das tarefas iniciais do sistema requer a compilação da aplicação de alocação para cada nova aplicação.
Esta recompilação pode ser evitada se a lista de tarefas iniciais também for incluída no repositório de tarefas.
A função de alocação estática, TasksAllocation, é mostrada na Figura 49.
O mestre utiliza um ponteiro para uma estrutura TaskPackage (linha 3) de forma a percorrer o cabeçalho de informações das tarefas no repositório.
TaskPackage possui a mesma estrutura de cabeçalho utilizada no repositório:
Id, size e initial_ address.
De forma a acessar a memória externa, é atribuído a este ponteiro o endereço 0x10000000 (linha 4).
A tabela static_ allocation é percorrida de forma a alocar todas as tarefas em ela contidas (linha 6).
A variável slave indica o escravo em o qual a tarefa vai ser alocada (linha 7).
O cabeçalho de informações no repositório é percorrido e quando a tarefa a ser alocada é encontrada, é chamado o driver DRV_ AllocationTask, passando como parâmetros o escravo (slave) onde a tarefa deve ser alocada e o endereço para a estrutura com as informações da tarefa(&amp; task) (linha 10).
Este driver lê as informações do repositório montando e enviando um pacote com os seguintes campos:
Após alocar a tarefa, o mestre deve informar aos outros escravos que uma nova tarefa está alocada no sistema.
Então, é chamado o driver DRV_ AllocatedTask passando como parâmetros o escravo onde a tarefa foi alocada (slave), o identificador da tarefa (task.
Id) e o escravo que está sendo informado (slaves) (linha 13).
Dado que a rede não possui serviço de multicast, esta operação é realizada por várias transmissões unicast.
Este driver monta e envia um pacote com os seguintes campos:
Após realizar alocação estática, mestre executa driver DRV_ FinishedAllocation, que monta um pacote com o serviço FINISHED_ ALLOCATION e envia a todos os escravos, indicando que a alocação estática foi concluída.
Então, ele habilita as interrupções provenientes da NoC de forma a aguardar pacotes de comunicação dos escravos, para realizar a alocação dinâmica de tarefas.
TaskPackage* task;
DRV_ AllocationTask (slave,&amp; task);
DRV_ AllocatedTask (slave, task.
Id, slaves);
Break; A alocação dinâmica de tarefas compreende o envio de uma tarefa (ti) a um nodo escravo mediante requisição de outra tarefa (tj).
Ou seja, o nodo mestre realiza a alocação dinâmica quando uma tarefa que está executando (tj) requisita a alocação de outra tarefa que se encontra no repositório (ti).
É importante destacar que as requisições de alocação são, na prática, transparentes a tj, sendo requisitadas por o microkernel quando tj tentar enviar uma mensagem a ti e esta não estiver alocada no sistema.
O nodo mestre, após a execução da alocação estática, aguarda interrupções provenientes da rede com solicitação de serviços.
Os serviços contidos nos pacotes podem ser:
Requisição de alocação de tarefa;
Notificação de término de execução de tarefa.
O fluxograma da Figura 50 ilustra quando tarefas são requisitadas, no lado do nodo escravo.
Quando a tarefa tj tenta enviar uma mensagem à tarefa ti (WritePipe(&amp; msg, ti), o microkernel do nodo escravo verifica na tabela de localização de tarefas (task_ location) se ti está alocada.
Se afirmativo, a chamada de sistema é concluída com sucesso.
Senão, verifica- se se a alocação estática já está terminada.
Se afirmativo, ti é requisitada chamando o driver DRV_ RequestTask.
Senão, armazena- se a requisição numa tabela de requisições de tarefas, requestTask.
Esta tabela contém os campos requested e requesting, que correspondem, respectivamente, ao identificador da tarefa requisitada e o identificador da tarefa que está requisitando.
Em os dois últimos casos (3 e 4), a tarefa que está requisitando, é colocada em espera (status $= WAITING).
WritePipe(&amp; msg, ti) Escreve mensagem no pipe Sim ti está Não Alocação estática terminada?
Enquanto o processo de alocação estática não termina, tarefas que já foram alocadas podem solicitar tarefas que ainda deverão ser alocadas.
De essa forma, as requisições são armazenadas na tabela requestTask e na medida que as tarefas requisitadas são alocadas, as tarefas que fizeram as requisições são desbloqueadas e podem ser escalonadas novamente.
Suponha que, no caso de a Figura anterior, a requisição de tj por ti tenha sido armazenada em requestTask.
A Figura 51, mostra o procedimento tomado quando ti é alocada no sistema.
A tarefa é inserida na tabela de localização de tarefas, task_ location.
Se existe requisição de ti na tabela requestTask, esta requisição vai ser removida e tj vai ser desbloqueada ti é alocada no sistema Insere ti em Não task_ location ti está em requestTask?
Quando o processo de alocação estática termina, verifica- se, em requestTask, se ainda existem requisições pendentes.
Se sim elas deverão ser alocadas.
Este tratamento é explicado na Seção 6.2.
Quando uma tarefa deve ser requisitada, o driver DRV_ RequestTask monta e envia um pacote com os seguintes campos:
O fluxograma da Figura 52 ilustra o procedimento realizado quando uma tarefa ti termina sua execução.
Uma chamada de sistema é gerada com o serviço EXIT.
Se não existem mensagens no pipe escritas por ti, o status da tarefa passa a ser Free, liberando o TCB que ela está ocupando;
Ti é removida da tabela tasks_ location;
É chamado o driver DRV_ TerminatedTask, que informa ao mestre que a tarefa ti terminou sua execução e pode ser liberada.
Se ainda existem mensagens de ti no pipe, o status desta tarefa passa a ser TERMINATED, a variável needTaskScheduling é configurada para 1 e só após todas as mensagens de ti serem consumidas, é informado ao mestre que a tarefa pode ser liberada.
Isso se deve ao fato de que, quando uma tarefa é liberada, ela é removida da tabela de localização de tarefas de todos os escravos.
Se tiver alguma tarefa tentando ler uma mensagem da tarefa liberada, esta não vai ser encontrada na tabela.
Exit (ti) ti.
Status $= Free RemoveTask (ti) Não DRV_ Term inatedTask (t i, netAddress) Mensagens de ti no pipe?
Para informar ao mestre que uma tarefa está terminada e pode ser liberada, o driver DRV_ TerminatedTask monta e envia um pacote com os seguintes campos:
O nodo mestre aguarda interrupções provenientes da NoC, geradas por a recepção dos pacotes REQUEST_ TASK e TERMINATED_ TASK, descritos acima.
Quando uma interrupção é gerada, o contexto do mestre é salvo na pilha e o fluxo de execução salta para a função que trata a interrupção.
Esta função é mostrada na Figura 53.
O serviço que o pacote carrega é lido da Ni por o driver DRV_ ReadService e armazenado no endereço do parâmetro service (linha 7).
Se o serviço for REQUEST_ TASK, é chamado o driver DRV_ RequestTask, que lê da Ni o identificador da tarefa requisitada e o escravo que está requisitando.
O cabeçalho de informações do repositório de tarefas é percorrido e quando a tarefa requisitada for encontrada, procura- se um escravo com página livre para alocar a tarefa.
Então, é chamado o driver DRV_ AllocationTask.
Após alocar a tarefa, é enviado um pacote a todos os outros escravos informando que uma nova tarefa foi alocada.
Se o serviço for TERMINATED_ TASK, é chamado o driver DRV_ TerminatedTask que lê da Ni o identificador da tarefa que terminou a execução e o escravo onde ela está alocada (linha 25).
Então, o driver DRV_ DeallocatedTask monta e envia um pacote para outros escravos informando que uma tarefa deve ser liberada.
Este pacote contém os seguintes campos:
TaskPackage* task;
DRV_ ReadService(&amp; service,&amp; size);
DRV_ RequestTask(&amp; taskID,&amp; slave);
DRV_ AllocationTask (slave,&amp; task);
OccupiedPage (slave);
If (slaves! $=
slave) DRV_ AllocatedTask (slave, task.
Id, slaves);
DRV_ TerminatedTask(&amp; taskID,&amp; slave);
If (slaves! $=
slave) DRV_ DeallocatedTask (taskID, slaves);
Uma importante função do código da Figura 53 é a SearchSlaveAvailable.
Em o contexto do presente trabalho, a distribuição de carga procura uniformizar o número de tarefas por processador.
Sugere- se para trabalhos futuros incluir funções mais elaboradas de distribuição de carga, que levem em conta parâmetros como volume de comunicação entre as tarefas, posicionamento das tarefas na rede de forma a minimizar o congestionamento na rede, o tempo de execução das tarefas, de entre outros.
Conforme explicado na Seção 5.3, os nodos escravos, quando são inicializados, contém em sua memória apenas o microkernel, executando a tarefa idle.
De essa forma, as tarefas são transferidas para a memória do processador, através da rede, em dois momentos:
Recepção das tarefas iniciais (podendo não haver tarefas inicias designadas para o nodo escravo), caracterizando a alocação estática;
E recepção de novas tarefas em tempo de execução, caracterizando a alocação dinâmica.
A Figura 54 complementa a Figura 45, apresenta no Capítulo 5, com os serviços executados por o microkernel.
Quatro serviços foram incluídos, para permitir a alocação e liberação de tarefas.
Estes serviços são executados mediante uma interrupção causada por a recepção de um pacote oriundo da rede.
DRV_ ReadService(&amp; service,&amp; size);
Switch (service) case REQUEST_ MESSAGE:
Os_ InterruptMaskClear (IRQ_ NOC);
DRV_ StartAllocation (tcbs.
Offset); DRV_ DeallocatedTask(&amp; task);
RemoveTask (task);
DRV_ RequestTask (requestTask.
Requested, netAddress);
Break; Se o serviço que o pacote carrega é TASK_ ALLOCATION, significa que o código objeto de uma tarefa está sendo transferido do repositório para a Ni e deve ser alocado na memória do processador.
De essa forma, para alocar a tarefa, é procurado um TCB livre 1.
O status deste TCB passa a ser ALLOCATING e o pc é configurado para 0.
Uma variável global, allocatingTCB contém o endereço do TCB que está sendo utilizado para alocar a tarefa.
As interrupções provenientes da NoC são desabilitadas.
O driver DRV_ StartAllocation é chamado passando como parâmetro o endereço (página) a partir de o qual a tarefa vai ser alocada (linha 22).
Este driver lê da Ni o identificador da tarefa armazenando- o no TCB referenciado por allocatingTCB e o tamanho do código objeto da tarefa.
Então, ele informa ao DMA o tamanho do código objeto da tarefa (escrevendo no registrador Set_ DMA_ SIZE), o endereço da memória a partir de o qual o código deve ser transferido (escrevendo no registrador Set_ DMA_ ADDRESS) e ativa o DMA (escrevendo no registrador Após ter recebido e tratado um pacote com o serviço TASK_ ALLOCATION, a CPU do escravo continua sua execução em paralelo com o DMA, que realiza a transferência do código objeto para a memória.
O controlador de DMA interrompe a CPU quando a transferência estiver concluída.
A função que trata a interrupção do DMA é mostrada na Figura 55.
A tarefa cujo código acaba de ser transferido para a memória é colocada na tabela task_ location (linha 3).
A tarefa, que está ocupando o TCB referenciado por allocatingTCB, passa a ter status READY (linha 4).
O microkernel avisa ao DMA que a interrupção foi aceita (linha 5).
Se a tarefa que estava executando antes da interrupção era a tarefa idle, needTaskScheduling é colocada em 1 indicando a necessidade do escalonamento de uma nova tarefa.
As interrupções provenientes da NoC são habilitadas.\&gt;
MemoryWrite; Se o serviço é ALLOCATED_ TASK, significa que uma tarefa foi alocada no sistema.
Para tratar este pacote, é chamado o driver DRV_ AllocatedTask (linha 25 da Figura 54).
Este driver lê da Ni o endereço do processador em o qual a tarefa foi alocada e o identificador da tarefa.
Estes dois dados são inseridos na tabela de localização de tarefas (task_ location).
Se serviço é DEALLOCATED_ TASK, significa que uma tarefa deve ser liberada.
De essa Sempre haverá retorno de TCB livre, pois o mestre mantém o controle de páginas livres em todos os processadores escravos.
Se afirmativo, o driver DRV_ RequestTask é chamado passando como parâmetros a tarefa requisitada e o endereço do escravo que está solicitando (linha 35 da Figura 54).
A validação dos procedimentos de alocação de tarefas, estática e dinâmica, é apresentada no Capítulo seguinte.
Este Capítulo está estruturado como segue.
Inicialmente apresenta- se a validação do microkernel, ilustrando- se por simulação funcional cada serviço implementado.
A Seção seguinte apresenta a utilização do sistema para a execução de uma aplicação paralela simples, Merge Sort, procurando- se mostrar o impacto do posicionamento das tarefas no desempenho da aplicação.
A terceira parte do Capítulo apresenta os resultados da execução de aplicações paralelas sobre a plataforma.
A quarta e última parte apresenta parte da codificação MPEG, composta por 5 tarefas:
Algorithm), IDCT (Inverse Discrete Cosine Transform) e recepção dos dados.
Os resultados serão apresentados na forma da taxa de recepção dos dados por a última tarefa.
Por MPEG ser uma aplicação tempo real, é importante a avaliação da vazão que o sistema pode fornecer.
Os serviços avaliados nesta Seção compreendem:
Escalonamento de tarefas, comunicação entre tarefas na mesma CPU, comunicação entre tarefas posicionadas em CPUs distintas, e a alocação (estática e dinâmica) de tarefas.
A Figura 56 apresenta a alocação inicial de três tarefas, t1, t2 e t3 num dado processador.
São mostrados os sinais da Ni, CPU (o processador) e DMA.
Em a Ni, data_ in indica os dados recebidos da NoC e interrupt_ plasma indica que a Ni está desejando interromper o processador para receber pacotes.
Em a CPU, page indica a página da memória de a qual o código objeto é executado.
O código objeto do microkernel encontra- se na página 0.
Em as páginas 1, 2 e 3 residem códigos objetos de tarefas.
Os sinais sinal intr_ enable e intr_ signal indicam, respectivamente, quando as interrupções estão habilitadas e quando o processador é interrompido.
Em o DMA, start representa quando o DMA começa a transferência do código objeto de uma tarefa para a memória do processador.
O sinal interrupt indica quando o DMA comunica que o código já está completamente da memória.
A seguir, são mostrados os passos do processo de alocação inicial de tarefas num processador.
Os números abaixo têm correspondência com os números da Figura 56.
A CPU inicia executando o microkernel que, após completar o boot, chama a tarefa idle, habilitando as interrupções.
O mestre envia um pacote para o processador com o serviço TASK_ ALLOCATION, indicando que uma tarefa será alocada em sua memória.
A Ni recebe o pacote e interrompe a CPU.
As interrupções são desabilitadas.
O microkernel trata a interrupção lendo o pacote da Ni e verificando que uma tarefa deve ser transferida para a memória.
Após o tratamento da interrupção, a CPU sinaliza o DMA para que este comece o processo de transferência.
O microkernel habilita as interrupções e continua sua execução enquanto o DMA faz a transferência.
O DMA termina a transferência dos dados e interrompe a CPU.
O microkernel desabilita as interrupções, faz a inicialização da tarefa e a escalona para executar.
As interrupções são habilitadas e t1 entra em execução.
A região destacada na Figura mostra que a tarefa t1 é executada por um perído de tempo muito pequeno, pois um pedido de interrupção é atendido.
Outro pacote indicando nova tarefa (t2) é recebido por a Ni que interrompe a CPU.
De a mesma forma que antes, a CPU sinaliza o DMA para iniciar a transferência.
A CPU retoma a execução da tarefa t1.
O DMA interrompe a CPU indicando que existe uma nova tarefa na memória.
O mesmo processo se repete para t3.
Mesmo com a chegada de novas tarefas, t1 é executada até completar seu timeslice.
Após ter completado a alocação inicial, o mestre envia um pacote ao processador com o serviço FINISHED_ ALLOCATION.
A Ni recebe este pacote e interrompe a CPU.
A seqüência de operações descritas acima demostram a correta operação dos mecanismos de interrupção, DMA e alocação estática de tarefas.
Uma vez as tarefas alocadas na CPU, o microkernel passa a escalonar- las.
A Figura 57 mostra a continuação da simulação apresentada na Figura 56, apresentando o escalonamento de tarefas.
Agora t1, t2 e t3 concorrem por o uso da CPU.
Os eventos apresentados na Figura 57 são descritos como segue:
Uma interrupção é gerada por o contador de timeslice.
As interrupções são desablitadas.
O microkernel entra em execução, salva o contexto de t1, escalona t2 e carrega o contexto desta nos registradores.
As interrupções são habilitadas.
O fluxo de execução é desviado para página 2, onde t2 está alocada.
As interrupções são desabilitadas.
O microkernel entra em execução, salva o contexto de t2, escalona t3 e carrega o contexto desta nos registradores.
As interrupções são habilitadas.
O fluxo de execução é desviado para a página 3, onde t3 está alocada.
Quando uma tarefa termina sua execução, ela deve ser removida da lista de tarefas a serem escalonadas.
A Figura 58 mostra o término da tarefa t2.
O sinal intr_ signal representa quando acontece uma chamada de sistema.
O microkernel entra em execução, atribui TERMINATED para o status de t2 e escalona a próxima tarefa, t3.
A Figura 59 mostra a comunicação entre duas tarefas, t1 e t2, que residem na mesma CPU.
São mostrados os sinais do microkernel (page e intr_ syscall), uma parte da área de memória de t1 correspondente à mensagem a ser enviada e uma parte da área de memória de t2 (Tarefa 2) correspondente à mensagem que vai ser recebida.
Os passos do processo de comunicação entre tarefas na mesma CPU são apresentados a seguir.
O microkernel entra em execução e copia a mensagem para o pipe.
O timeslice de t1 termina e t2 é escalonada.
O microkernel copia a mensagem do pipe para a área de memória de t2.
T2 volta a executar.
A Figura 60 apresenta os tempos, em ciclos de relógio, para a escrita e a leitura de uma mensagem de tamanho 3.
A escrita da mensagem no pipe gasta 357 ciclos.
Este número compreende o intervalo entre a chamada de sistema executada por t1 (WritePipe) e o término do tratamento desta chamada por o microkernel.
As operações executadas neste intervalo são:
Salvamento parcial de contexto de t1;
verificação do serviço resquisitado;
Escrita da mensagem no pipe;
Restauração do contexto parcial de t1.
A leitura da mensagem gasta 365 ciclos.
Este número compreende o intervalo entre a chamada de sistema executada por t2 (ReadPipe) e o término do tratamento desta chamada por o microkernel.
As operações executadas neste intervalo são:
Salvamento parcial de contexto de t2;
verificação do serviço resquisitado;
Leitura da mensagem no pipe e escrita da mesma na página de t2;
restauração do contexto parcial de t2.
A Figura 63 apresenta a comunicação entre duas tarefas que residem em processadores diferentes.
A tarefa t1.
Reside em PROC1 e a tarefa t2, em PROC2.
Ambas tarefas estão alocadas na página 1 da memória de cada processador.
Os passos para a comunicação são descritos a seguir.
Em PROC1, t1 executa e monta uma mensagem cujo tamanho (length) é 3 e cujo conteúdo é (msg), (msg) 3 (msg).
O microkernel entra em execução e copia a mensagem para o pipe do microkernel.
Não há envio de dados para t2.
Posteriormente, t2 (em PROC2) executa e faz uma chamada de sistema pedindo a leitura da mensagem de t1).
Este evento é totalmente assíncrono em relação a a geração da mensagem por parte de a tarefa t1.
O microkernel verifica que t2 está alocada em PROC1, monta um pacote de requisição de mensagem e envia este pacote ao processador PROC1.
O pacote de requisição chega em PROC1 e a Ni interrompe a CPU.
O microkernel monta a mensagem de resposta e envia a PROC2.
A mensagem de resposta chega em PROC2 e a Ni interrompe a CPU.
O tempo para a escrita no pipe é o mesmo da comunicação entre tarefas na mesma CPU, pois a escrita é sempre local.
No entanto o tempo de leitura é superior, pois envolve um número superior de passos.
A Figura 64 apresenta os intervalos de tempo envolvidos nesta comunicação.
O tempo total para a operação de read, com tamanho da mensagem igual a 3 e hops $= 1, é igual a 1066 ciclos de relógio.
Cada intervalo é detalhado a seguir:
ReadPipe por a tarefa t2, até o início do envio do pacote de requisição à PROC1.
2º intervalo ­ Corresponde a latência de rede e varia de acordo com o número de saltos na mesma.
Em este exemplo, o número de saltos é mínimo 1 hop:
PROC1 é vizinho de PROC2.
O número de operações de arbitragem e roteamento envolvidas é igual à distância dos roteadores mais um, pois o roteador destino deve encaminhar o pacote para a porta local.
Logo, no presente caso, hops $= 1, temos 2 operações de arbitragem e roteamento.
A comunicação remota é, em média, três vezes mais lenta que a comunicação local.
O projetista das aplicações que executarão no sistema deverá considerar este tempo no momento do posicionamento das tarefas.
Uma aplicação cujas tarefas são altamente cooperantes possui melhor desempenho quando estas tarefas são alocadas no mesmo processador ou em processadores vizinhos.
O último serviço a ser validado é a alocação dinâmica de tarefas.
As simulações abaixo apresentam a execução das tarefas t1 e t2, não havendo a tarefa t3 no sistema.
Em o momento que a tarefa t2 tentar enviar uma mensagem para t3, t3 será alocada dinamicamente no mesmo processador.
A Figura 66 apresenta a alocação dinâmica da tarefa t3 que é solicitada por t2.
São apresentados alguns sinais do DMA não foram mostrados na Figura 56, que trata da alocação estática.
Estes sinais são:
Set_ size, que indica que o DMA está recebendo o tamanho do código objeto;
Set_ address, que indica que o DMA está recebendo o endereço da memória a partir de o qual o código deve ser transferido;
Data_ read significa cada dado lido da Ni;
Data_ write é cada dado escrito na memória e;
Address_ write significa o endereço da memória onde está sendo escrito um dado do código objeto.
As etapas envolvidas na alocação dinâmica de tarefa compreendem:
A tarefa t2 faz uma chamada de sistema desejando enviar uma mensagem para t3 O microkernel verifica que t3 não está alocada.
Segue- se então o envio de um pacote para o mestre com o serviço REQUEST_ TASK solicitando a alocação desta tarefa (a transmissão do pacote deve ser observada nas transições que ocorrem no sinal data_ out).
O mestre envia um pacote ao processador com o serviço TASK_ ALLOCATION e o código objeto de t3.
A Ni recebe o pacote e interrompe a CPU.
O microkernel verifica que uma nova tarefa deve ser alocada.
Então, envia ao DMA o tamanho do código objeto da tarefa e o endereço a partir de o qual o código deve ser escrito e sinaliza para que este inicie a transferência da tarefa para a memória.
A tarefa t1 continua a executar enquanto o DMA realiza a transferência.
O DMA termina a transferência e interrompe a CPU.
O microkernel faz as inicializações da tarefa.
T1 volta a executar até completar seu timeslice.
A Figura 67 apresenta a continuação do processo descrito na Figura 66, mostrando que, após ser alocada, t3 passa a ser escalonada.
Processo descrito na Figura 66.
T2 é escalonada e refaz a chamada de sistema enviando a mensagem para t3.
O processo de alocação dinâmica interfere minimamente no desempenho dos processadores, pois a transferência dos códigos objetos é feita por DMA.
O número de ciclos de relógio gastos para a alocação de uma tarefa t, cujo tamanho é 3,19 KB, é 11.700.
A alocação corresponde ao intervalo entre o início da transmissão da tarefa por o mestre e interrupção da CPU escrava por o DMA, momento em que a transferência do código da tarefa foi completada.
Porém, a tarefa que requisitou a alocação tem seu desempenho reduzido, pois a mesma é bloqueada no momento da chamada de sistema (WritePipe) que gerou a requisição de alocação.
Justifica- se esta ação, pois as sucessivas escritas no pipe para uma tarefa ainda não alocada preencheriam o pipe, impedindo que as outras tarefas no processador comuniquem- se com tarefas já alocadas.
A decisão de reduzir o desempenho localmente (bloquear a tarefa que requisitou a alocação) evita a degradação global de desempenho (o não bloqueio da tarefa que requisitou a alocação poderia bloquear inúmeras outras tarefas).
O projetista deve avaliar o volume de comunicação entre as tarefas, a fim de decidir por políticas de alocação estática (quando tarefas comunicam- se com muita freqüência) ou alocação dinâmica (pouca comunicação entre as tarefas).
O objetivo deste experimento é ilustrar a operação do sistema MPSoC, através de um aplicação paralela simples, Merge Sort.
A aplicação é descrita por um grafo, ilustrado na Figura 68 (a), sendo função do mestre do sistema alocar as tarefas.
A tarefa t1 tem por função apenas enviar os elementos a serem ordenados para as tarefas t2 e t3, e depois receber e unir (operação merge) os resultados parciais.
As tarefas t2 e t3 realizam o ordenamento (algoritmo bubble sort) dos dados recebidos.
As três tarefas são alocadas estaticamente.
A Tabela 12 apresenta o tempo de execução para ordenar um vetor de 400 elementos, utilizando- se diferentes posicionamentos para as tarefas.
Os resultados para os sistemas monoprocessado e multiprocessado (mapeamento M4, tempo total igual a 574.643 ciclos de relógio), correspondem ao esperado, ou seja, redução do tempo de processamento (speedup igual a 2,14 ­ divisão entre Tendt1 de M1 por Tendt1 de M4).
Esperaria- se- tempos semelhantes para os mapeamentos M1 e M2 e para M3 e M4.
Em os mapeamentos M1 e M2 as ordenações parciais ocorrem no mesmo processador, caracterizando uma operação seqüencial.
Para os mapeamentos M3 e M4 as ordenações parciais ocorrem em processadores distintos, com a conseqüente operação em paralelo dos algoritmos bubble sort.
Para a análise destes resultados é preciso compreender como ocorre a comunicação entre as tarefas.
A Figura 69 ilustra parte do código da tarefa t1.
Como pode ser observado na Figura, t1 envia os dados para as tarefas de ordenação e depois aguarda a recepção dos dados.
Observando os tempos de execução dos mapeamentos M3 e M4, o resultado esperado seria um desempenho do mapeamento M3 próximo a o desempenho de M4, pois as rotinas de ordenação estão em processadores distintos.
Em este mapeamento, M3, toda vez que a tarefa t1 é escalonada, há uma tentativa para leitura de dados da tarefa t2.
Uma vez que t2 ainda não concluiu, a tentativa de leitura apenas consome ciclos, não realizando trabalho efetivo.
Dado que t2 conclui em 782.797 (Tendt2), percebe- se uma alta sobrecarga do microkernel devido a leituras que não retornam dados.
Analisando a Tabela 12, pode- se inferir os seguintes tempos (aproximados):
Ti -- tempo de inicialização (alocação estática das tarefas):
20.000 ciclos de relógio Ts -- tempo de ordenação parcial:
300.000 ciclos de relógio (Tendt3 -- Tstartt3, nos mapeamentos M3 e M4).
Tm -- tempo de merge dos resultados:
150.000 ciclos de relógio, em todos os mapeamentos).
De posse destes tempos, pode- se calcular o tempo ideal de execução para os mapeamentos seqüencial e paralelo, como ilustrado na Tabela 13.
A diferença entre os tempos total e ideal é explicada por dois fatores:
A) Leituras sem sucesso geram chamadas de sistema sobrecarregando a rede e/ ou o processador.
A forma utilizada nos experimentos para minimizar esta sobrecarga foi adicionar um atraso entre cada solicitação de leitura:
A solução efetiva para minimizar a sobrecarga é realizar a leitura de forma bloqueante, não escalonando a tarefa novamente enquanto não forem enviados dados à esta tarefa.
Esta otimização está entre as atividades a serem realizadas em trabalhos futuros.
B) No caso de o mapeamento M1 também há uma sobrecarga devido a o escalonamento da tarefa t1, quando esta requer um dado de t2.
Tornando a leitura bloqueante este escalonamento não ocorrerá enquanto não forem enviados dados à t2.
A Figura 70 detalha a execução das tarefas, através do sinal page, no mapeamento paralelo, M3.
Em esta Figura deve- se observar: --
O número de ciclos de relógio reduzido para configurar as tarefas (Ti). --
A sobrecarga por as operações de leitura sem sucesso.
Este tempo pode ser observado entre o término da tarefa t3 e o início de execução do merge, correspondendo a 110.000 ciclos de relógio.
Aumentando- se o tamanho do vetor a ordenar para 1000 posições, o speed-up manteve- se aproximadamente igual.
Este experimento permitiu validar a plataforma MPSoC para uma aplicação multi-tarefa, com diferentes mapeamentos.
A transferência de mensagens através da rede tem baixo impacto no desempenho, dado o elevado tempo de processamento das tarefas.
Este experimento também mostrou que o mecanismo de comunicação entre tarefas pode ser otimizado, tornando a leitura bloqueante.
O objetivo deste experimento é validar a correta execução de aplicações paralelas com tarefas comunicantes sob a plataforma.
São utilizadas três aplicações, sendo duas a aplicação Merge Sort, apresentada na Seção anterior.
A terceira é uma aplicação simples, composta por 4 tarefas.
O grafo desta aplicação é mostrado na Figura 71.
As tarefas A e B enviam, respectivamente, 3 e 4 vetores de 30 elementos para a tarefa C. Esta, por sua vez, repassa estes vetores para a tarefa D. Foram utilizados três diferentes mapeamentos, mostrados na Figura 72.
As tarefas do primeiro Merge Sort são referenciadas por AM1, BM1 e CM1.
As tarefas do segundo Merge Sort são referenciadas por AM2, BM2 e CM2.
Por fim, as tarefas da terceira aplicação são referenciadas por AC, BC, CC e DC.
O mapeamento M1 caracteriza- se por o compartilhamento dos recursos por as duas aplicações Merge Sort;
O mapeamento M2 caracteriza- se por a independência de recursos para as aplicações Merge Sort;
O mapeamento M3 compartilha parcialmente as tarefas do Merge Sort.
A terceira aplicação é utilizada como &quot;ruído «sobre as aplicações Merge Sort.
A Tabela 15 apresenta os resultados da execução das três aplicações com os diferentes mapeamentos.
Os resultados correspodem ao número de ciclos de relógio gastos para a execução de cada aplicação.
Em o mapeamento M1, as duas aplicações Merge Sort possuem o mesmo mapeamento, concorrendo por os mesmos recursos.
De essa forma, seus resultados são praticamente iguais.
A terceira aplicação possui um tempo de vida curto, comparado às aplicações Merge Sort.
Em o mapeamento M2, o segundo Merge Sort teve seu tempo de execução reduzido, quase igual ao tempo mínimo observado na Tabela 12 (574.643), pois as tarefas AC, BC e CC da terceira aplicação, com as quais ele compartilha processamento, terminam sua execução deixando os processadores disponíveis exclusivamente para a execução das tarefas do segundo Merge Sort (AM2, BM2 e CM2).
A terceira aplicação teve seu desempenho degradado:
Em o mapeamento M1 seu tempo de execução foi de 194.301 ciclos de relógio;
Em o mapeamento M2, foi de 232.513.
Isso se deve ao fato de que, no M1, a tarefa BC compartilha o processamento com AC, e ambas terminam cedo.
Além disso, a tarefa CC executa sozinha no processador onde está alocada.
Já em M2, todas as tarefas da terceira aplicação compartilham processamento com tarefas das aplicações Merge Sort, as quais possuem um tempo de vida maior e, conseqüentemente, continuarão a utilizar o processador enquanto a terceira aplicação não termina sua execução.
Ainda no mapeamento M2, o primeiro Merge Sort teve seu desempenho melhorado em relação a o mapeamento M1.
A razão disso é que, em M1, ele compartilha os mesmos recursos com o segundo Merge Sort.
Em M2, as tarefas AM1 e BM1 compartilham o mesmo processador, e a tarefa CM1 compartilha o processador com a tarefa CC, da terceira aplicação, que termina sua execução mais cedo.
Em o mapeamento M3, as aplicações Merge Sort tiverem tempos praticamente iguais, pois as tarefas B e C compartilham recursos com tarefas da terceira aplicação que, mais uma vez, terminam sua execução cedo fazendo com que as tarefas B e C das aplicações Merge Sort utilizem o processador com exclusividade.
A alocação das tarefas A no mesmo processador, contribui também para os resultados próximos.
A simulação de cada experimento gera um relatório para cada processador.
Este relatório contém os resultados da execução de cada tarefa alocada no processador.
Através destes relatórios, pôde- se obsrevar a correta execução das tarefas.
Esta Seção apresenta a execução de parte da codificação MPEG sob a plataforma implementada.
A aplicação é composta por 5 tarefas, mostradas na Figura 73: T1 envia a t2 8 vetores, cada um composto por 128 bytes;
T2 recebe os vetores, executa o algoritmo IVLC e envia os resultados à t3;
t3 recebe os vetores de t2, executa o algoritmo IQUANT e envia os resultados à t4;
t4 recebe os vetores de t3, executa o algoritmo IDCT e envia os resultados à t5.
Uma vez que MPEG é uma aplicação de tempo real, é importante a avaliação da vazão que o sistema pode fornecer.
De essa forma, os resultados desta Seção são apresentados na forma da taxa de recepção dos dados por a última tarefa (t5).
Ou seja, é conhecido o instante de tempo Ti, em o qual a tarefa t5 recebe o primeiro byte de dados (primeiro byte do primeiro vetor) e o instante de tempo Tf, em o qual t5 recebe o último byte de dados (último byte do oitavo vetor).
Conhecendo estes tempos pode- se calcular a vazão do sistema, para a recepção de dados por t5.
A Tabela 16 apresenta estes instantes de tempo para três diferentes mapeamentos.
Em o mapeamento M1, as tarefas t1, t3 e t5 compartilham o processador P1 e as tarefas t2 e t4 compartilham o processador P2.
Para este mapeamento, é utilizada a alocação estática de tarefas.
Em os mapeamentos M2 e M3 todas as tarefas estão distribuídas em 5 processadores.
Em M2 utiliza- se alocação estática de tarefas e em M3 utiliza- se alocação dinâmica.
A Tabela 17 apresenta a vazão do sistema para os três mapeamentos.
Os dados que chegam na tarefa t5 totalizam em 1024 bytes.
De essa forma, a vazão é calculada por a expressão:
1024/ (Ti ­ Tf).
O mapeamento M1 apresentou menor vazão, pois as cinco tarefas da aplicação concorrem por o uso de dois processadores.
De essa forma, os resultados demoram mais tempo para chegar na tarefa t5.
Os mapeametos M2 e M3 obtiveram vazões equivalentes à quase o triplo da vazão obtida no mapeamento M1.
Isso se deve ao fato de que as tarefas executam em paralelo e não concorrem por recursos.
SLAVE2 e t5 em SLAVE1.
Observa- se ainda, que a tarefa t1 não é interrompida no início de sua execução, pois a tarefa t2 ainda não foi alocada e, portanto, não faz requisições de mensagem à t1.
O término da execução das tarefas para o mapeamento M1 é mostrado na Figura 75.
Observa- se a conclusão de todas as tarefas em SLAVE2.
Em SLAVE1, a tarefa t5 (page $= 3) termina sua execução fazendo a impressão dos resultados.
A Figura 76 apresenta o processo de alocação das tarefas para o mapeamento M2.
Observase na linha do tempo, a alocação seqüencial das cinco tarefas:
T1 em SLAVE1, t2 em SLAVE2, t3 em SLAVE3, t4 em SLAVE4 e t5 em SLAVE5.
Em este mapeamento, a tarefa t2 é interrompida várias vezes, desde o início de sua execução, por a tarefa t3 que faz requisições de mensagens ainda não disponíveis.
O término das tarefas no mapeamento M2 é mostrado na Figura 77.
A tarefa t1 finaliza sua execução bastante tempo antes das outras tarefas, não aparecendo na Figura.
Observa- se o término seqüencial das tarefas.
A última tarefa a finalizar a execução é a tarefa t5 em SLAVE5, com a impressão dos resultados.
A Figura 78 apresenta o processo de alocação das tarefas para M3.
Este mapeamento é igual ao utilizado em M2, contudo, a alocação é dinâmica.
Isto significa que:
A tarefa t2 será alocada apenas quando for requisitada por t1;
a tarefa t3 será alocada apenas quando for requisitada por t2;
a tarefa t4 será alocada apenas quando for requisitada por t3;
por fim, a tarefa t5 será alocada apenas quando for requisitada por t4.
Diferentemente do mapeamento M2, em o qual a tarefa t2 é interrompida sucessivas vezes por a tarefa t3, no mapeamento M3, t2 executa um tempo sem interrupções e requisita t3.
Após t3 ser alocada e iniciar sua execução, a t2 passa a ser interrompida (legenda 2 da Figura).
O término das tarefas para o mapeamento M3 é o mesmo para M2, mostrado na Figura 77.
Sistemas multiprocessados em chip, MPSoC, são uma tendência no projeto de sistemas embarcados.
Entretanto, não há plataformas completas disponíveis em domínio público para realizar pesquisas no tema.
Freqüentemente, os temas são tratados de forma isolada.
Por exemplo, há inúmeros trabalhos situados dentro de os temas de rede intra-chip, processadores embarcados e sistemas operacionais embarcados.
Contudo, trabalhos contendo a integração de todo o sistema e avaliando seu desempenho, são escassos.
A contribuição maior desta Dissertação foi o desenvolvimento de uma plataforma MPSoC completa, multiprocessada e multitarefa, com interconexão por rede intra-chip.
Esta plataforma será de domínio público, disponível aos grupos de pesquisa que trabalham nas áreas de NoC, MPSoC, sistemas operacionais embarcados, entre outros.
O trabalho iniciou a partir de as descrições VHDL da rede Hermes e do processador Plasma.
O desenvolvimento concomitante do hardware para integrar os componentes principais e do software para suportar as aplicações concorrentes resultaram nas seguintes contribuições:
De hardware:
Arquitetura do nodo de processamento ­ composto por a interface de rede, DMA, processador;
De software:
Desenvolvimento do microkernel para suporte a aplicações concorrentes e alocação estática e dinâmica de tarefas.
A plataforma MPSoC desenvolvida foi descrita em VHDL sintetizável (nível RTL), e validada por simulação VHDL.
Apesar de o tempo de simulação ser elevado, esta estratégia permite:
Simular a arquitetura que realmente executará no hardware, não sendo um modelo abstrato, como muitas vezes encontra- se na literatura;
Avaliar com precisão, no nível de ciclo de relógio, o desempenho dos mecanismos empregados na comunicação, transferência de dados, chaveamento de contexto, entre outros.
Enumera- se abaixo um conjunto de trabalhos que podem ser executados na seqüência do desenvolvimento desta plataforma MPSoC.
Prototipação do sistema.
Para a execução de tarefas mais complexas, o tempo de simulação torna- se muito elevado.
Visando avaliar o desempenho do sistema para aplicações complexas, deve- se prototipar o mesmo em FPGA.
Leitura bloqueante.
A leitura não bloqueante de mensagens empregada neste trabalho faz com que, no caso de mensagem inexistente, muitos pacotes de requisição e resposta negativa (em o_ message) sejam gerados e enviados por a rede.
Além de congestionar a rede, muito processamento da CPU é gasto para tratar estes pacotes, degradando o desempenho do sistema.
Visando solucionar este problema, a leitura de mensagens deverá ser bloqueante, fazendo com que uma tarefa seja suspensa no momento em que ela requisita uma mensagem e volte a ser escalonada somente após a mensagem ser recebida.
Canais virtuais.
A NoC utilizada para realizar a interconexão dos módulos não possui canais virtuais, pois simplifica o projeto do hardware.
Contudo, em redes maiores, haverá congestão nos canais da rede.
Dado que já existe implementação da Hermes com canais virtuais, pode- se empregar esta rede para redução da congestão da rede.
MPSoC heterogêneo.
A plataforma desenvolvida neste trabalho é um MPSoC com processamento homogêneo, ou seja, todos os núcleos são processadores embarcados de propósito geral idênticos (processador Plasma).
Como trabalho futuro está a inserção de módulos de hardware reconfigurável na rede, tornando o processamento heterogêneo.
Em o repositório, além de tarefas de software, existirão bistreams de reconfiguração parcial.
Para tanto, faz- se necessária a inclusão de um controlador de configuração e infra-estrutura para conexão destes módulos.
Mecanismos de avaliação de carga.
Visando distribuir a carga de trabalho do sistema, devem ser implementados mecanismos de avaliação de carga.
Estes mecanismos podem considerar a carga de trabalho local de processadores e o tráfego nos canais da rede.
Tornar a aplicação de alocação uma tarefa genérica.
Em a plataforma desenvolvida, um nodo mestre é designado a executar a aplicação de alocação de tarefas.
Trabalhos futuros compreendem tornar a aplicação de alocação uma tarefa genérica, permitindo que ela compartilhe tempo de execução com as demais aplicações do sistema.
Além disso, sugere- se incluir na aplicação de alocação, funções mais elaboradas de distribuição de carga, que levem em conta parâmetros como volume de comunicação entre as tarefas, posicionamento das tarefas na rede de forma a minimizar o congestionamento na rede, o tempo de execução das tarefas, consumo de energia, de entre outros.
A migração de tarefas é um mecanismo a ser explorado dentro de o contexto da alocação dinâmica.
Este mecanismo suspende a execução de uma tarefa num nodo x e a envia para um nodo y, onde sua execução é retomada.
A migração de tarefas é um tema bastante explorado na área de Sistemas Paralelos e Distribuídos.
Contudo, a aplicação desta abordagem em MPSoCs está sendo estudada de forma a empregar seus benefícios na área de sistemas embarcados.
A migração de tarefas pode ser justificável para otimizar o desempenho do sistema, considerando critérios como, por exemplo, carga de trabalho nos processadores e a carga da comunicação nos enlaces da rede.
Coerência de cache.
Uma vez que tarefas paralelas podem trabalhar sobre os mesmos dados, é necessária a funcionalidade de coerência de cache.
Com isso, diferentes tarefas poderão compartilhar dados, fazendo com que as alterações realizadas por uma tarefa sejam conhecidas por todas as outras.
Gerência de Memória. A gerência de memória empregada neste trabalho dispõe apenas de um mecanismo de paginação, o qual faz a geração de endereços físicos de acordo com a página onde se encontra o fluxo de execução.
Sugere- se incluir a funcionalidade de alocação e liberação dinâmica de memória.
Uma interface gráfica com o usuário está sendo desenvolvida de forma a facilitar o projeto e execução de aplicações sobre a plataforma.
Até o presente momento, esta interface possibilita carregar diferentes aplicações e alocar cada uma de suas tarefas escolhendo os nodos escravos.
As dimensões da rede são fixas (3x2) bem como o número de processadores (6).
Sugere- se permitir ao usuário criar uma plataforma com uma rede de tamanho parametrizável, escolhendo o núcleo a ser conectado em cada nodo da rede:
Um processador ou um módulo de hardware reconfigurável.
