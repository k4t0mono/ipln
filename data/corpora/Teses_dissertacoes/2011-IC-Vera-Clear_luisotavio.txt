Este trabalho estuda a aplicação de técnicas de aprendizado de máquina (agrupamento e classificação) à pesquisa de jurisprudência, no âmbito do processo judicial eletrônico.
Discute e implementa alternativas para o agrupamento dos documentos da jurisprudência, gerando automaticamente classes que servem ao posterior processo de categorização dos documentos anexados ao processo jurídico.
O algoritmo TClus de Aggarwal, Gates e Yu é selecionado para desenvolvimento de exemplo de uso, com propostas de alteração no descarte de documentos e grupos, e passando a incluir a divisão de grupos.
A proposta ainda introduz um paradigma &quot;bag of terms and law references «em lugar de o &quot;bag of words», quando utiliza, na geração dos atributos, os tesauros do Senado Federal e da Justiça Federal para detectar termos jurídicos nos documentos e expressões regulares para detectar referências legislativas.
Em o exemplo de uso, empregam- se documentos oriundos da jurisprudência do Tribunal Regional Federal da 4a Região.
Os resultados dos agrupamentos¯ e submetidos aos testes de significânforam avaliados por as medidas Relative Hardness e cia de Wilcoxon e contagem de vitórias e derrotas.
Os resultados da categorização foram avaliados por avaliadores humanos.
A discussão e análise desses resultados abrangeu a comparação do sucesso e falha na classificação em relação a a similaridade do documento com o centróide no momento da categorização, à quantidade de documentos nos grupos, à quantidade e tipo de atributos nos centróides e à coesão dos grupos.
Discute- se, ainda, a geração dos atributos e suas implicações nos resultados da classificação.
Contribuições deste estudo:
Confirmação da possibilidade de uso do aprendizado de máquina na pesquisa jurisprudencial, evolução do algoritmo TClus ao eliminar os descartes de documentos e grupos e ao implementar a divisão de grupos, proposta de novo paradigma &quot;bag of terms and law references», através de prototipação do processo proposto com exemplo de uso e avaliações automáticas na fase de clustering, e por especialista humano na fase de categorização.
Palavras chave:
Categorização, agrupamento, hard clustering, bag of terms and law references, Relative Hardness Measure, Measure, Wilcoxon signed-- ranks test, teste de contagem de vitórias e derrotas, direito, jurisprudência.
Por determinação do Conselho Nacional de Justiça, até o final do ano de 2010, todo o Poder Judiciário brasileiro teve que implantar o processo eletrônico, finalizando o trâmite de documentos em papel.
Atividades de rotina em gabinetes dos magistrados incluem a pesquisa por decisões proferidas em casos julgados anteriormente, a jurisprudência.
Quando o juiz encontra um caso semelhante ao que está estudando, tem a oportunidade de, concordando com os argumentos apresentados, aproveitar a fundamentação exposta, reduzindo drasticamente o tempo gasto elaborando a fundamentação de sua decisão.
Para agilizar este trabalho, sistemas usando recursos de Processamento da Linguagem Natural e Mineração de Dados para classificação e recuperação de documentos podem representar uma melhoria nos procedimentos de pesquisa.
Este estudo propõe o uso de processos de agrupamento e categorização de textos jurídicos, descritos no Capítulo 4, Seções 4.5.1 e 4.5.2.
O processo proposto é constituído de uma fase de agrupamento dos documentos que compõem a jurisprudência, gerando um conjunto de classes correspondentes aos grupos encontrados e outra fase que, quando os litigantes enviarem peças processuais em forma digital através de upload no sistema processual eletrônico, categoriza os documentos enviados e retorna aos usuários os documentos integrantes do grupo que gerou a respectiva classe.
Para tanto, nossa revisão dos fundamentos de aprendizado de máquina, constante do categorização, Seção 2.2.
O estudo de trabalhos correlatos, investigando a evolução do emprego de categorização precedida por agrupamento, encontra- se no Capítulo 3.
Ressaltamos que somente um destes trabalhos, apresentado na Seção 3.3.5 utilizou documentos do domínio jurídico do conhecimento.
O pré-processamento dos documentos usa uma mescla de tesauros jurídicos mantidos por o Senado Federal e por o Conselho da Justiça Federal, descritos na Seção 4.4.2.1, para extrair termos dos documentos e compor vetores de atributos, abandonando o paradigma num exemplo de uso, foram construídos um corpus, descrito no Capítulo 4, Seção 4.4.1, uma base lexical, descrita no Capítulo 4, Seção 4.4.2.2, um parser e um tagger, descritos no Capítulo 4, Seção 4.4.3.
Resultados são descritos no Capítulo 5.
A avaliação dos resultados, exposta no Capítulo 5, compreendeu o cálculo de medidas internas dos agrupamentos realizados, apresentados na Seção 5.3.1, para a fase de teste, e na Seção 5.8, para a fase de operação;
E validação da categorização, na Seção 5.3.2, para a fase de teste, e Seção 5.9, para a fase de operação, através de especialista humano.
De a análise destas avaliações emergem, então, nossas conclusões, expostas no Capítulo 5.11.
Nossas contribuições, não se limitam a confirmar a possibilidade de uso de técnicas de aprendizado de máquina para realizar pesquisa de jurisprudência.
Incluem, também, proposta de evolução deste algoritmo, avaliada mediante prototipação do algoritmo com variações onde se eliminaram os descartes de documentos e grupos e implementou- se a reconhecedor de referências legislativas, mesclamos 3 dicionários, 2 lematizadores e 2 tesauros jurídicos, utilizados no pré-processamento do corpus jurídico, que construimos com a jurisprudência do Tribunal Regional Federal da 4a Região.
Considerações Iniciais Desde os primórdios da informática, o computador vem sendo empregado na solução de problemas de complexidade crescente.
Alguns destes problemas, são solucionáveis através de um método conhecido, determinado previamente.
São exemplos disto, sistemas administrativos, cálculos, etc..
Outros problemas não têm sua solução pré-determinada.
Conhece- se, apenas, uma certa quantidade de informações relacionadas ao problema.
Em este caso, segundo Mitchell podemos empregar métodos de aprendizado de máquina.
Em função de as informações que dispomos a respeito de o problema, temos duas possibilidades:
Entre as informações disponíveis, encontramos soluções para situações específicas.
Em este caso, podem ser empregados métodos que buscam solucionar problemas novos, guiando- se por as soluções já conhecidas.
Este é o aprendizado supervisionado, também chamado de categorização ou, ainda, classificação.
Faremos um breve estudo destes métodos na Seção 2.2;
Entre as informações disponíveis, não contamos com soluções prévias.
Em este caso, é empregada a abordagem de aprendizado não supervisionado, ou clustering, ou, em português, agrupamento, que será estudado na Seção 2.3.
Aprendizado supervisionado ­ Categorização Segundo Tan, Steinbach e Kumar, este tipo de aprendizado consiste em analisar um conjunto de situações, denominadas &quot;instâncias «ou exemplos, e suas características, denominadas &quot;atributos».
Entre os atributos, aquele que apresenta a solução previamente conhecida é denominado &quot;atributo alvo «ou &quot;rótulo de classe».
Os valores do atributo alvo constituem as soluções previamente conhecidas e, portanto, compõem o conjunto de possíveis soluções.
Busca em Espaço de Estados Quando estão presentes as soluções para casos específicos, formulam- se as hipóteses informações, responde com o valor da característica faltante.
Duas abordagens clássicas propostas para solucionar este problema são:
O algoritmo Find-S, que busca todas as hipóteses que incluam todos os resultados positivos e excluam todos os resultados negativos, selecionando, de entre estas hipóteses, a mais específica;
O algoritmo candidate-Elimination, que busca todas as hipóteses que incluam todos os resultados positivos e excluam todos os resultados negativos, selecionando, de entre estas hipóteses, a mais genérica.
A Figura 2.1 permite observar a diferença entre as soluções adotadas por os dois algoritmos:
A área delimitada por a linha pontilhada representa a hipótese mais específica do algoritmo Find-S e a área delimitada por a linha contínua, a hipótese mais genérica do algoritmo candidate-Elimination.
A área entre estas duas linhas compreende possíveis hipóteses intermediárias entre a mais específica e a mais genérica.
Se, conforme a Figura 2.2 (esquerda), uma nova instância, com atributos que incidam nesta área intermediária, apresentar um resultado positivo, deverá- se- descartar a hipótese mais específica do algoritmo Find-S e adotar, em seu lugar, a hipótese mais específica que englobe esta nova instância.
De a mesma maneira, conforme a Figura 2.2 (direita), se uma nova instância, com atributos que incidam nesta área intermediária, apresentar um resultado negativo, deverá- se- descartar a hipótese mais genérica do algoritmo CandidateElimination e adotar, em seu lugar, a hipótese mais genérica que exclua esta instância.
Suponha, agora, a ocorrência de uma nova instância com resultado negativo, dentro de a área delimitada por uma hipótese mais específica, gerada por o Find-S. Isto leva a uma situação em a qual não será mais possível obter uma função que determine o resultado sem erros.
Teremos a mesma situação, se uma instância com resultado positivo ocorrer além de a área delimitada por uma hipótese mais genérica, gerada por o algoritmo candidate-Elimination.
Vide Figura 2.3.
Em outras palavras, é vazio o conjunto de hipóteses consistentes com a solução.
Os algoritmos Find-S e Candidate-Elimination trabalham com a idéia da busca no espaço de hipóteses.
No entanto, a quantidade de hipóteses mesmo para uma pequena quantidade de atributos e uma pequena quantidade de possíveis valores para estes atributos, facilmente atinge uma enorme gama de hipóteses.
Suponha o exemplo dos caixas do banco.
Considerando que numa dada hipótese, cada atributo pode ter um de seus possíveis valores, caso específico, ou ter seu valor indeterminado, caso genérico.
De esta maneira, voltando ao exemplo dado na Seção 2.2.1, teremos 3* 3* 6 $= 162 possibilidades, 163 se contarmos a hipótese vazia.
Se considerarmos o atributo &quot;feriado «como tendo 3 valores possíveis (&quot;véspera de feriado», &quot;retorno de feriado «e &quot;nenhum feriado&quot;), a quantidade de hipóteses sobe para 217.
Se dobrarmos a quantidade de atributos, teremos 3* 3* 6* 3* 3* 6+ 1 $= 26.245 hipóteses!
E a maioria dos problemas reais se representa com muito mais que 8 atributos.
Assim, buscar no espaço de hipóteses facilmente se torna inviável.
Além de isto, estes algoritmos geram soluções na forma de expressões conjuntivas, ou seja:
Ai Reli Vi i $= 1 onde A é um atributo, V é um valor e Rel é um relação, como` $ ',` 'ou`'.
As árvores de decisão são uma forma de se obter uma solução que preveja a disjunção.
Suponha que o novo conjunto de treino seja o da Tabela 2.2.2.
Uma possível árvore de decisão para classificar as instâncias poderia ser a da Figura 2.4.
Os nodos folha com o valor &quot;S «indicam as situações em que se precisa aumentar a quantidade de caixas no atendimento, os nodos folha com o valor &quot;N «indicam que não há necessidade disto.
De esta árvore geramos a expressão:
I $= s (I $= n F $= v) (I $= n F $= v V $= s D $= 2 F $= N D $= 3) que, ainda, pode ser simplificado para:
I $= s (I $= n F $= v) (I $= n F $= v V $= s D $= 2) (I $= n F $= r V $= s D $= 3) (2.2) onde I é o início do mês;
F é o feriado;
V é o verão;
6. V, r são os valores &quot;véspera de feriado «e &quot;retorno de feriado&quot;;
Redes Bayesianas Uma outra abordagem para o aprendizado supervisionado, é o uso da teoria das probabilidades.
Conforme Luger uma rede bayesiana é representada por um grafo acíclico dirigido, conforme exemplo da Figura 2.5, onde as relações indicam um certo grau de causalidade.
Cada nodo representa um evento e tem uma probabilidade conhecida de ocorrência.
Os nodos pais têm probabilidades de ocorrência independentes e os nodos filhos têm probabilidade de ocorrência influenciada por a ocorrência dos nodos pais.
Conforme o exemplo da Tabela 2.3, o nodo pai A tem 20% de probabilidade de ocorrer, enquanto o nodo B tem 77% de probabilidade de ocorrer.
Já o nodo C tem 14% de probabilidade de ocorrer se somente A tiver ocorrido, 10% se somente B tiver ocorrido, 8% se ambos tiverem ocorrido e probabilidade 0%, ou seja, não ocorre, se nenhum de eles tiver ocorrido.
Note que a probabilidade de A e B ocorrerem simultaneamente é dada por e a probabilidade de ocorrência de &quot;C «é dada por e a probabilidade de P (A B| C) é dada por isolando P (C A B), obtemos substituindo este resultado na relação P (C| A B), obtemos o teorema de Bayes:
A forma geral do teorema de Bayes é dada por:
P (Hi| E) $= P (E| Hi) × P (Hi) k $= 1 P (E| Hk) × P (Hk) onde E é uma determinada evidência (nodo pai, ou causador);
Em este caso, é possível que exista um hiperplano com uma margem maior com erros dentro de o limite de erros aceitáveis.
A Figura 2.7 apresenta exemplos de casos de separabilidade de instâncias.
Já a Figura 2.8 apresenta exemplos de dados não linearmente separáveis em que é preciso mapear o espaço original para um novo espaço onde seja possível separar as instâncias por meio de um hiperplano linear.
Metodologia De acordo com Mitchell, no processo de categorização, temos 3 fases:
Fase de treino:
Em a qual se analisam as instâncias conhecidas, procurando determinar formas de se chegar ao &quot;atributo alvo», ou seja, a solução geral, aplicável quando fornecidas novas instâncias onde não se conheça previamente o valor deste atributo.
Fase de teste:
Em a qual se avalia a qualidade do treino, seja em virtude de o algoritmo, seja em face de as instâncias utilizadas para o treinamento.
Em esta fase, aplicamos a solução geral determinada na fase anterior a um conjunto de instâncias cujos valores dos atributos alvo são conhecidos.
As soluções produzidas para estas instâncias são comparadas com as soluções que previamente dispúnhamos e em as quais confiamos que estejam corretas.
De esta maneira, medimos a precisão do resultado, bem como as características específicas dos erros cometidos, tais como quantidade de &quot;falsos positivos «e &quot;falsos negativos&quot;;
Fase de operação:
Tendo determinado uma solução de qualidade aceitável, passa- se à fase de operação, quando não mais se conhecem as soluções para as situações.
Avaliações serão feitas nesta fase, sendo comum descobrir, na prática, taxas de erros significativamente superiores aos encontrados na fase de teste.
Isto pode ocorrer devido a a solução ser demasiadamente específica para o conjunto de treino.
Tal situação é denominada &quot;overfitting&quot;;
Aprendizado Não-Supervisionado ­ Clustering Todas as abordagens estudadas até aqui contam com o fato de se conhecer previamente, no conjunto de treino, para cada instância, o valor do atributo alvo, atuando, assim, como &quot;professor «da máquina, ou seja, o aprendizado é supervisionado.
Há casos, porém, em que não se conhece, com antecedência, o valor do atributo alvo.
O processo de aprendizado, portanto, é não supervisionado por não dispor de um atributo alvo.
De este problema emergem dois desafios:
Agrupar as instâncias utilizando- se de critérios que determinem a semelhança entre as instâncias de um mesmo grupo e que as diferenciem dos demais grupos;
Rotular os grupos, uma tarefa opcional1, cuja necessidade depende da aplicação e, freqüentemente, realizada através de intervenção manual.
É importante notar que os métodos de agrupamento descritos a seguir têm sido utilizados para solucionar problemas nos mais diversos campos do conhecimento e a natureza dos dados, por esta razão, varia consideravelmente.
Os dados podem ser as linhas de uma tabela num banco de dados, os pixels de uma imagem, os frames de um vídeo, os documentos num sistema de arquivos, entre outros.
Em o âmbito deste estudo, passaremos corpus são as instâncias e as palavras que os compõem são os atributos destas instâncias ou, na maioria das vezes, se tornam a matéria-prima de a qual obtemos os atributos.
Segundo Tan, várias são as estratégias empregadas nos algoritmos de clustering.
Elas podem variar segundo:
A quantidade de grupos ao quais uma instância pode ser atribuída: (
a) os algoritmos que atribuem cada instância a um único grupo são denominados algoritmos de hard clustering;
A relação entre os grupos: (
a) os algoritmos hierárquicos organizam os grupos em forma de árvores de categorias, denominadas dendogramas, onde os nós folhas representam as instâncias, a raiz é um único cluster mais genérico que os demais e os nós intermediários representam clusters de variado grau de especificidade.
Há três maneiras populares de medição da similaridade entre os clusters, conforme ilustrado na Figura 2.9: P, q Q.
Equilíbrio entre para os e contras das medidas anteriores.
Observando- se a Figura 2.9, nota- se que a opção por o Average Linkage eleva a complexidade do algoritmo.
A completude das atribuições: (
a) os algoritmos que descartam instâncias são chamados parciais;
O critério de atribuição de instâncias aos grupos: (
a) os algoritmos baseados em protótipo são aqueles que representam o grupo mediante um indivíduo ideal, que pode ser uma das instâncias (medóide) ou calculado a partir de as instâncias integrantes do grupo (centróide);
O Algoritmo K--Means Entre os algoritmos clássicos de hard clustering e, ainda, muito popular, está o K--means, proposto por MacQueen, que pressupõe que a relação de pertinência entre as instâncias e grupos obedece a funções de distribuição de probabilidade.
O K--means busca descobrir os centróides de cada grupo estimando as médias geradoras de cada grupo/ distribuição e quais instâncias foram geradas por quais distribuições.
Para tanto, realiza- se iterações em 2 passos:
Atribui cada instância ao centróide mais próximo/ semelhante;
Recalcula os centróides como o ponto médio das instâncias a ele atribuídas;
Estas iterações encerram- se quando de a convergência dos centróides.
Visto que desejamos atuar no ramo do Direito, ressaltamos que o entrelaçamento de diferentes assuntos é reconhecidamente a maioria dos casos.
Por esta razão, espera- se que soluções de hard clustering tenham mais efetividade apenas ao se processar documentos que discorram acerca de um único tema.
Algoritmo Expectation--Maximization (Em) O algoritmo Em, proposto por Dempster, Laird e Rubin, assim como o K--Means, pressupõe que os grupos são determinados por distribuições de probabilidade gaussianas.
No entanto, enquanto no K--Means as distribuições de probabilidade compartilham as mesmas variâncias, o Em admite a possibilidade de múltiplas variâncias, conforme se pode verificar ao se comparar as Figuras 2.10 e 2.11.
Em a verdade, o algoritmo K--Means é um caso especial do algoritmo Em.
Sendo um algoritmo de soft clustering, o Em admite que as instâncias possam estar vinculadas a mais de um grupo.
Ele inicializa o processo usando o K--Means para estimar os grupos e suas médias iniciais e passa a calcular a probabilidade de que as instâncias estejam nos demais grupos.
Para tanto, realiza iterações em 2 passos com a seguinte forma geral:
Maximization-Step: Calcula- se novas médias, maximizando- se as probabilidades do Expectation-Step;
Pode se repetir as iterações até a convergência dos parâmetros ou até que a sua mudança seja inferior a um valor limite especificado.
Este algoritmo apresenta, no entanto, sérias dificuldades em convergir ou converge para uma solução inadequada quando o conjunto de dados é muito grande ou inicializado erroneamente.
Suas variações FREM, on-line Em e Scalable Em também são altamente problemáticas em presença de grande volume de dados.
Agrupamento Semi--Supervisionado De acordo com Grira, Crucianu e Boujemaa, o agrupamento semi-supervisionado é uma forma de agrupamento em a qual se impõe alguma restrição, normalmente nas formas must-link ou cannot-link que provê supervisão, embora limitada.
Acrescentam ainda que o conhecimento representado por estas restrições é insuficiente para uso em aprendizado supervisionado.
Assim, a combinação entre a aplicação da função de similaridade e alguma restrição guiam o procedimento de atribuição de instâncias aos clusters.
Ainda segundo Grira, Crucianu e Boujemaa, os algoritmos de clustering semisupervisionado se dividem em dois tipos:
Aqueles que aplicam as restrições na função de similaridade e, aqueles que aplicam as restrições no algoritmo do clustering propriamente dito.
O algoritmo semi-supervisionado de Aggarwal, Gates e Yu, estudado na Seção 3.4, aplica sua restrição no algoritmo de clustering.
Mas, o faz apenas na inicialização dos dados, atribuindo cada instância a um grupo.
As iterações subseqüentes não são influenciadas por qualquer restrição.
Pré-Processamento Para que se possa classificar ou agrupar documentos, é preciso que as instâncias contenham as informações na forma adequada para a realização de operações de comparação.
Se os atributos das instâncias se compõem de colunas numa tabela de um banco de dados, é mais provável que não seja necessário nenhum processamento prévio.
No entanto, é altamente provável que haja necessidade de formatação de dados em outros casos.
Em nosso estudo, as instâncias se compõem de textos em linguagem natural e, portanto, deverão ser formatadas.
Alguns destes procedimentos são muito conhecidos e usados largamente:
Parsing, consiste em recortar o texto, de ele extraindo as palavras que o compõem, etiquetando- as e realizando a análise sintática, identificando os grupos constituintes de acordo com uma gramática;
Stemming é o processo de normalização por o qual buscamos reverter palavras, derivadas ou flexionadas para uma forma normal comum a todas as suas variações.
Esta forma normal pode ser a raiz da palavra, &quot;altamente «se reverterá para &quot;alto», por exemplo;
Ou pode ser o seu stem, &quot;alt», neste caso.
A priori não importa se a reversão deverá remeter à raiz ou ao stem, o mais importante é que as várias flexões/ derivações sejam mapeadas para uma mesma partícula;
Lematização é o processo de normalização em que se converte uma palavra inflexionada para uma forma não flexionada:
O lema ou lexema correspondente.
Diferenças semânticas não são levadas em consideração.
Os seguintes benefícios podem ser obtidos ao substituir as palavras dos documentos por os seus lemas:
Eliminar ambigüidades léxicas, evitando contabilizar sob um mesmo atributo palavras com grafias iguais mas de sentidos diversos;
E contabilizar sob um mesmo atributo palavras com grafias diferentes que, por apresentarem sentidos muito próximos (por exemplo o mesmo verbo em diferentes flexões) compartilham o mesmo lema contabilizando- as sob um mesmo atributo e, assim, propiciando, não apenas a redução de sua dimensionalidade, mas, também, elevando a semelhança entre os documentos em os quais estas palavras ocorrem;
Esta abordagem reconhece apenas uma relação entre as palavras, que consiste no fato de se encontrarem no mesmo documento e limita- se a valorar a freqüência de suas ocorrências.
A mera contabilização de freqüências pode, no entanto, induzir a erros de super/ subvalorização de palavras.
Considere, por exemplo, um texto de 10.000 palavras, onde encontramos 10 ocorrências da palavra &quot;recorrer «e compare com um outro texto, composto de 200 palavras, onde detectamos 5 ocorrências da palavra &quot;divisão».
Ora, o cálculo da freqüência absoluta indica que &quot;recorrer «desempenha um papel mais preponderante no conjunto de dados, a despeito de representar 0,1% do texto em face de os 2,5% representados por &quot;divisão».
Assim, o cálculo de freqüência de palavras, normalmente é acompanhado por algum método de normalização.
O cálculo do percentual, aqui exposto é um método simples de se atingir tal objetivo.
Outros métodos foram propostos, sendo o TF-IDF largamente utilizado.
Este método leva em consideração, não apenas a freqüência dos termos em cada documento, mas, também, a quantidade de documentos em que o termo ocorre.
De esta maneira, temos o cálculo de freqüência de um dado termo k num documento j:
T F (tk, d j) $= se&amp; (tk, d j) 0 A freqüência inversa de documento, que exprime a relevância do termo, dada a quantidade de documentos em que ele ocorre, é dada por:
IDF $= log&amp; D (tk) Finalmente, temos que:
T F -- IDFi j $= T Fi j.
IDFi Funções de Proximidade Durante o processo de agrupamento, ou em outras circunstâncias, é necessário realizar o cálculo de proximidade semântica entre pares de termos ou documentos.
De acordo com Tan, Steinbach e Kumar, a proximidade pode ser a diferença ou a similaridade entre as instâncias.
Duas abordagens populares são:
Distância euclidiana é uma medida de diferença muito popular onde os termos são a distância entre eles será dada por i $= 0 onde n é a quantidade de dimensões.
Quanto menor a distância entre duas instâncias, maior a probabilidade de que sejam atribuídas a um mesmo grupo.
Quanto maior a similaridade entre duas instâncias, maior a probabilidade de que sejam atribuídas a um mesmo grupo.
Métodos de Validação de Classificação Para aferir a qualidade dos resultados, utilizam- se várias medidas que expressam o grau de qualidade destes métodos.
Segundo Tan, Steinbach e Kumar, as medidas mais comuns são compostas dos seguintes elementos básicos:
Verdadeiros Positivos (Vp):
Instâncias corretamente classificadas como pertencentes a uma classe específica;
Verdadeiros negativos (VN):
Instâncias corretamente classificadas como não pertencentes a uma classe específica;
Falsos positivos (FP):
Instâncias erroneamente classificadas como pertencentes a uma classe específica;
Falsos negativos (FN):
Instâncias erroneamente classificadas como não pertencentes a uma classe específica.
Ainda conforme Tan, Steinbach e Kumar, baseadas nas contagens destes elementos, destacamos as seguintes medidas:
A acurácia ou precisão:
É o percentual de acertos, V P+ FP+ V N+ FN, ou seja, a proporção das instâncias corretamente obtidas por o total de instâncias;
A abrangência ou recall:
V P+ FP, ou seja, a quantidade das instâncias corretamente obtidas por a quantidade de instâncias realmente pertencentes à classe alvo;
A medida F ou F-measure:
É uma média entre precisão e abrangência, podendo atribuir pesos valorando uma ou outra medida, sendo que, mais freqüentemente, é usada a fórmula:
F $= (1+, onde P é a precisão, R é o recall e é o peso, definido por:
Peso precisão é mais valorizada Recall é mais valorizado Métodos de Validação de Grupos De acordo com Tan, Steinbach e Kumar, os métodos de validação são taxonomicamente divididos em:
Supervisionados, também chamados de índices externos, quando se utilizam de informação adicional além de a presente no conjunto de dados, são detalhados na Seção 2.7.1;
Não supervisionados, também chamados de índices internos, quando utilizam, exclusivamente, informação contida no conjunto de dados, são detalhados na Seção Relativos, métodos supervisionados ou não supervisionados quando usados para comparar diferentes experimentos.
Métodos de Validação de Grupos Supervisionados Segundo Tan, Steinbach e Kumar, de entre os métodos de validação supervisionados, baseados no pressuposto de que a um grupo corresponda uma classe, encontramos:
Os métodos orientados a classificação (a) métodos que qualificam a presença/ ausência das classes nos grupos através da acurácia, abrangência e F-Measure;
Jaccard: F01+ f f10+ f11 (a) a estatística Rand:
De entre os métodos de validação não supervisionados, encontramos medidas baseadas em coesão interna do grupo, ou seja, a proximidade entre as instâncias de mesmo grupo e separação dos grupos, o grau de afastamento entre os grupos.
Para grupos baseados em grafos, as medidas implicam comparações entre instâncias.
Em grupos baseados em protótipo a coesão resulta de comparação entre as instâncias do grupo e o protótipo e a separação apenas entre protótipos.
A Tabela 2.4 apresenta o cálculo de medidas de coesão e separação de agrupamentos, onde f é a função de proximidade.
Coeficiente de Silhueta O coeficiente de silhueta, baseado em grafos, combina coesão e separação para determinar se uma instância está bem inserta no grupo ou se está em região intercluster.
Este coeficiente permite a visualização gráfica da qualidade dos grupos.
O coeficiente de uma instância é dado por si $= (bi -- ai) max (ai, bi) onde ai $= j $= i, j Ck f (xi, y j)| Ck| bi $= minkC j Ck/ Ck f (xi, y j)| Ck| o coeficiente de silhueta médio de um grupo é dado por sC j si $= i $= 1| C j| e o coeficiente de silhueta médio de todo o agrupamento é dado por sC sC $= i $= 1 i A família de Índices Dunn A família de índices Dunn apud, também pondera coesão e separação dos grupos, apresentando a seguinte fórmula geral:
Dunn, a distância entre clusters é a distância entre os vizinhos mais próximos (single linkage) e o diâmetro é a maior distância entre uma instância e o respectivo centróide.
Bezdek apresentou experimentos demonstrando resultados superiores ao usar average linkage para determinar a distância entre os clusters e calcular o diâmetro como o dobro da distância média entre as instâncias e seus respectivos centróides.
Índice Davies--Bouldin O índice Davies-Bouldin utiliza a razão entre a dispersão interna do grupo e a separação entre os grupos, dada por:
Ri $= max j $= i 1 j| C| s (Cn) $= i $= 1 Ri s (Ci)+ s (C j) f (Ci, C j) xCn f (x, cn)| Cn| para Cn $= n-ésimo cluster e cn $= centróide do n-ésimo cluster.
Medida Stein, Eissen e Wissbrock ressaltam que os índices da família Dunn e o índice Davies-Bouldin assumem o modelo baseado em protótipo, pressupondo clusters de forma esférica e, assim, a aplicabilidade do índice torna- se questionável quando este modelo não se aplicar.
A medida, proposta por Stein e Niggemann adota modelo baseado em grafo e considera a densidade dos clusters no cálculo dado por| Ci|.
I i $= 1 onde i $= min w (u, v) E E é um conjunto de arcos tal que o Ci seja desconexo e w (u, v) é o peso do arco que conecta u e v..
Medida¯ ou medida de densidade esperada, proposta por Stein, Eissen e Potthast A medida, também adota modelo baseado em grafo e pondera a densidade dos clusters em relação a a densidade do agrupamento no seu cálculo, dada por| Ci| w (Ci)| C|.|
Ci| i $= 1 onde k é a quantidade de classes e ln (w (C)) ln(| C|) w (C) $ | C|+ f (x, y) x $= y x, y C Sendo a densidade baseada na soma das similaridades entre as instâncias, quanto maior for a densidade dos grupos em relação a a densidade do agrupamento, maior será o valor desta medida e, portanto, melhor será a qualidade do agrupamento.
Medida Relative Hardness A medida relative hardness (Rh), de Pinto e Rosso, concatena os documentos de cada categoria, obtendo um único vetor por categoria, ou seja, é baseada em protótipo, e é dada por i, j $= 1, i\ j f (CATi, CAT j) n/ 2 onde n é a quantidade de categorias e CATi é a i-ésima categoria.
Esta medida é uma soma das similaridades intra-cluster e, portanto, como afirmado por os seus autores, quanto menor o seu valor, melhor é a qualidade do agrupamento.
Métodos de Comparação de Algoritmos de Aprendizado de Máquina Segundo Demsar, ao realizar- se comparativos entre as performances de algoritmos de aprendizado de máquina é necessário certificar- se de que as diferenças de performance aferidas nos testes sejam realmente significativas e não pequenas diferenças aleatórias resultantes de características específicas do conjunto de dados utilizado.
Em seu estudo, Demsar enfoca a comparação de algoritmos de aprendizado supervisionado.
García Usam teste de significância para comparar algoritmos genéticos e Cappelleri O usam para avaliar análise de tratamento de diabetes.
O teste de significância que aplicamos em nosso exemplo de uso segue a linha de Mukhopadhyay agrupamentos e realizam o teste de significância para determinar que os índices de qualidade dos agrupamentos obtidos por os algoritmos por eles propostos são significativamente superiores.
Demsar apresenta vários métodos estatísticos de teste de significância.
Por aplicarem- se à comparação de pares de algoritmos, não serem paramétricos e serem facilmente implementáveis, destacamos os seguintes testes:
Teste de contagem de vitórias, derrotas e empates ­ Teste de Sinal[ She04, Sal97 Dem06 Dem06].
Em este teste, calcula- se um escore E contabilizando- se a quantidade de conjuntos de dados em as quais o algoritmo proposto teve melhor performance.
Os empates contam como 0,5 e o escore final é truncado para o inteiro imediatamente inferior.
Se, para N $= quantidade de conjuntos de dados, então a diferença entre os resultados sendo comparados é significativa, com 5% de confiança;
T a menor das somas de ranks.
Por fim, calcula- se z $= sqrt N onde N é a quantidade de conjuntos de dados e, para z 1, 96, pode- se rejeitar a hipótese nula com 5% de confiança.
Considerações Finais Revisamos a abordagem do Aprendizado Supervisionado, que só permite obter soluções se dispusermos de um atributo alvo e somente o conjunto de valores em ele encontrados.
Se, uma solução como d, ainda que esta seja a solução correta.
O aprendizado consiste em determinar a forma de se chegar a uma solução específica, dado um conjunto de possíveis soluções.
O objetivo deste estudo é trabalhar com documentos jurídicos.
Assim, nossas instâncias serão documentos contendo grande quantidade de termos, versando sobre ampla gama de assuntos.
Uma vez que o trabalho não é determinar se um dado caso tem veredito favorável ou não, mas saber se o caso é semelhante a um ou mais casos já analisados e quais seriam estes, não temos um conjunto fixo e conhecido de atributos alvo e, portanto, necessitamos, primeiramente, determinar quais os possíveis valores que o atributo alvo poderá assumir.
Para tanto, estudamos, na seção anterior, os algoritmos de aprendizagem não supervisionada, que, através do agrupamento de instâncias semelhantes, buscam determinar quais são os possíveis valores que pode assumir o atributo alvo.
Foram aqui apresentadas algumas abordagens para realização de agrupamento, já com foco no agrupamento de documentos.
Em o próximo capítulo, revisamos trabalhos relacionados, que apresentam processo em que as instâncias são agrupadas e os grupos obtidos determinam o conjunto de possíveis atributos-alvo para uso de categorizador.
Considerações Iniciais Buscando a bibliografia pertinente, percebe- se que ainda se dispõe de poucos relatos de experimentos utilizando clustering como auxiliar de algoritmos de categorização de documentos não previamente rotulados.
A Seção 3.3.5 apresenta um relato neste sentido.
A maioria dos trabalhos relacionados foi agrupada na Seção 3.2 e na Seção 3.3, de acordo com o algoritmo categorizador utilizado.
A Seção 3.4 apresenta um trabalho relacionado cujos algoritmos empregados distingüem- se dos anteriores, sendo, assim, apresentado separadamente.
Alguns trabalhos não utilizaram documentos contendo texto em linguagem natural em seus experimentos.
No entanto, uma vez que utilizam processo em que se realiza clustering para auxiliar algoritmo categorizador, considerou- se relevante revisar- los.
Os experimentos relatados neste capítulo, bem como seus resultados, não foram reproduzidos, tendo sido empreendidos por os autores dos respectivos artigos.
Trabalhos Baseados em Classificadores Bayesianos Em esta seção estão agrupados os trabalhos que utilizam classificador Bayesiano.
Além de isto, a maioria de eles usa o algoritmo Em na fase de clustering.
Os primeiros trabalhos da Subseção 3.2.1 não apresentaram experimentos com documentos contendo textos em linguagem natural sem formatação específica.
Somente o último destes trabalhos tratou de documentos textuais.
No entanto, os trabalhos relatados nesta subseção foram selecionados tendo em vista o processo de clustering como auxiliar da categorização proposto por os autores.
Ainda que divirjam no tipo de dado tratado no próximo capítulo, propõem metodologias que podem ser, também, utilizadas com documentos textuais.
Gerando Redes Bayesianas usando o Algoritmo em Friedman, propõe o uso do algoritmo Em para alterar redes bayesianas, melhorando sua performance.
Seu estudo continua em parceria com Elidan e Lotner e Koller.
Em seu primeiro trabalho, Friedman apresenta o Model Selection Em, MS-EM e o Alternate Model Selection Em, AMS-EM, que difere daquele por evitar convergência prematura para máximos locais.
Estes algoritmos aprendem novas redes bayesianas na presença de variáveis ocultas ou de valores faltantes.
Além de a descoberta de variáveis, promove, também, inserção e remoção de arcos.
Friedman ressalta que trabalhos anteriores realizam estas operações fora de o Em e, a cada alteração, chamam o Em para estimar os parâmetros da rede.
Por esta razão, o Em recalcula todos os parâmetros da rede a cada chamada.
Em a proposta de Friedman, uma vez que as modificações estruturais da rede são realizadas dentro de o Em, o recálculo dos parâmetros é limitado aos nodos afetados por a alteração.
Em seu segundo trabalho, Friedman rebatiza o MS-EM para Structural Em, Sem.
Sua nova proposta, o Bayesian Structural Em, BSEM, difere do Sem, que realiza uma busca no espaço de estruturas × parâmetros, por realizar uma busca apenas no espaço de estruturas.
Além de isto, o Sem busca valores aproximados e o BSEM busca valores exatos.
Em a busca por valores exatos, o BSEM precisaria repetir muitos cálculos intermediários.
Assim, os cálculos intermediários são armazenados em cache e a quantidade efetiva de cálculos é significativamente reduzida.
Em o terceiro trabalho, o estudo se concentra na descoberta de variáveis ocultas que interagem com variáveis observadas.
O método proposto realiza uma busca na rede por subestruturas, chamadas de &quot;semi-cliques «por os autores, que podem indicar a presença de uma variável oculta.
Segundo os autores, um semi-clique é um relaxamento no número de vizinhos, definido como um conjunto de variáveis tais que cada variável tenha arestas conectando- a com, pelo menos, metade das demais variáveis do conjunto.
A o encontrar um semi-clique, o algoritmo realiza uma inserção de uma nova variável, quebrando o semi-clique.
Se, após um processo de aprendizado, a nova estrutura apresentar melhores resultados que a original, a nova variável é aceita na estrutura.
Em o quarto trabalho, Elidan e Friedman propõem método para descobrir a dimensionalidade de variáveis ocultas.
Para cada nova variável oculta H, seria necessário realizar muitas execuções do Em, variando a cardinalidade desta variável.
Por isto, os autores consideraram que o Em teria um custo computacional muito elevado e propõem novo algoritmo inspirado por clustering aglomerativo e técnicas de fusão de modelos bayesianos.
Além de isto, os autores destacam que quando há muitas variáveis ocultas a serem determinadas, o custo de processamento escala rapidamente devido a a influência da alteração de uma variável oculta sobre as demais variáveis ocultas.
Por esta razão, o algoritmo trata primeiramente as variáveis ocultas com menor impacto sobre outras.
Os experimentos acima descritos foram realizados sobre vários conjuntos de dados.
Em os dois primeiros, Friedman utilizou dados gerados artificialmente:
1 rótulo de classe) e os demais atributos representavam as 99 palavras mais freqüentes do vocabulário, exceto as stopwords, que foram removidas.
Em esta série de estudos vemos a evolução do trabalho de Friedman, buscando gerar redes bayesianas com o auxílio do Em.
Em seus primeiros trabalhos não foram utilizados documentos em linguagem natural sem formatação específica.
Porém no último trabalho foi utilizado o corpus 20 Newsgroups.
Os autores reportam apenas o ganho de performance tendo como baseline a rede bayesiana original.
Classificação de Texto num Modelo de Mistura Hierárquico para Pequenos Conjuntos de Treino Toutanova Propõem estender o categorizador Naïve Bayes utilizando um modelo de mistura hierárquica de tópicos, diferenciando termos de acordo com sua especificidade/ generalidade.
Os autores assumem uma hierarquia de tópicos pré-definida e buscam gerar automaticamente um modelo de probabilidade para documentos.
Os parâmetros do modelo são aprendidos mediante o algoritmo Em, que busca maximizar a verossimilhança nos dados de treino.
Uma vez aprendidos os parâmetros, novos documentos são classificados, utilizando- se Naïve Bayes, computando e maximizando probabilidades de categorias em função de as palavras contidas no documento, representadas como vetor de freqüências.
Este modelo hieráquico foi inspirado no modelo de Redução Hierárquica, Hierarchy Shrinkage.
Aqui, porém, os nodos intermediários da hierarquia representam níveis de abstração das palavras contidas nos documentos.
Os autores assumem que cada palavra num documento é gerada por um nodo (nível de abstração) no caminho entre a classe do documento (nodo folha) e a raiz da hierarquia.
Esta representação resulta numa mistura onde as probabilidades dos termos são compartilhadas por múltiplas classes (nodos folha).
O nível de abstração de cada palavra é desconhecido e, assim, modelado como variável oculta.
A construção da árvore utiliza o modelo Cluster-Abstraction que gera modelos hierárquicos a partir de dados não rotulados utilizando o algoritmo Em.
Diferentemente deste modelo, o modelo proposto por os autores usa uma hierarquia pré-definida e dados rotulados para estimar os parâmetros do Em.
Em o expectation step, o algoritmo calcula a probabilidade P (v| C, wi) do nível v hieráquico, dadas a palavra wi e a classe C, para cada par (C, v).
Em o maximization step, o algoritmo calcula a probabilidade P (wi| v) da palavra wi, dado o nível v da hierarquia e a probabilidade P (v| C) do nível v dada a classe C. Como resultado, palavras mais genéricas têm probabilidades maiores em nodos mais próximos à raiz e palavras mais específicas de certas classes terão probabilidades maiores próximo a os respectivos nodos folha.
Segundo os autores, dados empíricos mostram que basta realizar em torno de 2 a 5 iterações do Em para obter uma árvore com boas estimativas.
Acima de isto verificou- se a ocorrência de overfitting.
Obtido o modelo hieráquico, os autores apresentam duas maneiras de se utilizar- lo:
Para associar um único rótulo a cada documento, o algoritmo bayesiano seleciona a classe com a maior probabilidade dadas as palavras do documento, P (c| d);
Para associar múltiplos rótulos sugerem o uso de valores de corte, recuperando as classes de maior probabilidade.
Para avaliar a performance do método proposto, os autores usaram dois corpora:
Reuters 21578 e 20 Newsgroups.
Os algoritmos comparados foram:
Naïve Bayes, Probabilistic Latent Semantic Analysis, Hierarchical Shrinkage, KNN e SVMs.
O corpus 20 Newsgroups possui, aproximadamente, 20.000 documentos, divididas em 20 grupos de, aproximadamente, 1.000 documentos.
Foram selecionados 15 grupos para tornar o experimento semelhante ao relatado em.
Estes grupos possuem muitas semelhanças entre si e aproximadamente 4% dos documentos estão presentes em mais de um grupo.
Os 15 grupos estão organizados em 5 categorias gerais e esta organização hieráquica foi adotada como modelo para o experimento.
O assunto da postagem foi incluído no documento.
As letras foram convertidas para minúsculas.
Não foi utilizado nenhum método de normalização.
Foram removidas as stopwords e todas as palavras com freqüência inferior a 4 ocorrências no corpus.
Vários treinos foram realizados, a cada treino variou- se a quantidade total de documentos, dividido igualmente por grupo.
O Modelo de Mistura Hierárquica proposto por os autores obteve as melhores performances em 6 dos 8 experimentos, obtendo a segunda melhor performance nos demais.
A diferença de performance entre os classificadores reduziu significativamente à medida que o conjunto de documentos aumentou de tamanho.
O segundo experimento utilizou o particionamento ModApte1 do corpus Reuters-21578 com as modificações usadas por Yang e Liu em.
Estas modificações consistem em selecionar somente documentos classificados em categorias que ocorrerem tanto no conjunto de treino quanto no de teste.
As letras foram convertidas para minúsculas.
Não foram removidas as stopwords.
Foi realizada redução de dimensionalidade selecionando as Os autores consideraram que as categorias deste corpus não estavam apropriadas ao Modelo de Mistura Hierárquica proposto, pois 4 das 8 categorias previstas pertenciam ao domínio das finanças.
Assim, decidiram usar um algoritmo aglomerativo para gerar novas categorias.
Como resultado, obtiveram 4 categorias intermediárias e 90 categorias finais (nodos folha).
Usando este corpus, o classificador baseado em SVM apresentou a melhor performance em todas as medições.
O Modelo de Mistura Hierárquica foi o segundo melhor em 4 das 7 aferições.
Os autores passaram a estudar a hipótese de que a baixa performance decorra da fase de pré-processamento, pois não realizaram a normalização das palavras e acreditavam que houvesse diferenças nas listas de stopwords e nos esquemas de peso dos termos.
Nota- se que os autores apresentam informação aparentemente contraditória em seu artigo, pois afirmam que não realizaram a remoção de stopwords.
Em comparação com o Modelo de Redução Hierárquica, obteve melhor performance em 6 das 7 medições.
Superou o Naïve Bayes em todas as medições realizadas.
Classificação de Textos Semi--Supervisionada Usando em Particional Cong, Lee, Wu e Liu, questionam o pressuposto comumentemente assumido em processos de aprendizado semi-supervisionado, em o qual espera- se que os grupos gerados tenham correspondência um-para-um com as categorias constantes dos dados pré-rotulados.
De esta maneira, propõem processo de agrupamento hierárquico usando hard clustering e, após, aplicam o Em em cada partição.
Por fim, usam os dados rotulados para podar a árvore de maneira que os nodos restantes da árvore satisfaçam o pressuposto de correspondência um-para-um com as categorias.
Os autores afirmam que o Em tem má performance na presença de mais de 2 distribuições.
Assim, empregam o particionamento recursivo dos dados de maneira a garantir que haja apenas duas distribuições em cada partição.
Para tanto, usam um algoritmo de hard clustering, que consiste em:
Biparticionar os dados randomicamente;
Treinar um classificador bayesiano para corrigir a distribuição dos documentos entre as partições;
Repetir recursivamente os passos acima para cada partição.
Os autores prevêem a iteração de passos do algoritmo acima até a convergência.
A condição de parada da recursividade é a presença de, no máximo, 2 documentos rotulados.
Após o particionamento, o algoritmo proposto realiza a poda da árvore.
Há 2 razões para a poda:
Eliminar partições muito pequenas e eliminar problemas de overfitting.
Para tanto, o algoritmo poda a árvore começando por os nodos folha e caminhando em direção a a raiz.
Sempre que a soma dos erros de classificação dos nodos filhos for maior que os erros de classificação do nodo pai, poda- se os nodos filhos e retorna os erros de classificação do nodo pai.
Em caso contrário, retorna- se a soma dos erros de classificação dos nodos filho.
Obtendo a árvore, o algoritmo inicia a execução do Em em cada partição, tendo, agora, como pressuposto, que exista uma correspondência um-para-um das distribuições com as categorias dos documentos.
No entanto, os autores relatam que, ainda assim, em alguns casos, esta correspondência não ocorre e isto é detectado quando aumenta o número de iterações do Em.
Por isto, a cada iteração, é realizada uma classificação com algoritmo bayesiano usando cross validation e, verificando- se que a acurácia diminui, encerra- se o Em sem esperar por sua convergência.
A partir de então, é possível realizar a classificação de novos documentos.
O processo de classificação é descrito por os autores em dois passos:
O documento a ser classificado é agrupado hierarquicamente, iniciando pela raiz da árvore e seguindo até um nodo folha;
A o atingir um nodo folha, o documento é classificado usando um classificador bayesiano, com os parâmetros obtidos por o Em, usando a equação| d| P (C j| di) $= P (C j) k $= 1 P (wdi.
K| C j Cr wdi.
K| Cr) onde C j é uma classes e di é um documento e wdi.
K é a palavra na posição k do documento di.
A probabilidade P (wi| C j) é substituída por P (wi| C j), conforme a equação P (wi| C j) $= 1+ i $= 1 N (wi, di) P (C j| di) -- N (wi, dv) P (C j| dv)| V|+ g $= 1 P (C j| di) -- N (wi, dv) P (C j| dv) cujos parâmetros foram obtidos através do Em.
Para os experimentos, os autores selecionaram os corpora:
20 Newsgroups:
Contendo 19.997 artigos divididos quase igualmente entre 20 categorias.
De este corpus foram derivados 2 grupos:
Reuters 21578: Contendo 12.902 artigos divididos em 135 tópicos.
Também foram derivados dois grupos deste corpus:
O pré-processamento não envolveu stemming.
Foram removidas as stopwords e as palavras que apareciam em menos de 3 documentos2.
Para cada um dos grupos, os autores realizaram 10 experimentos, selecionando randomicamente os documentos que compunham o conjunto de documentos rotulados.
Todos os resultados apresentados são médias dos resultados dos 10 experimentos.
Para os grupos 20A e 20 B, o conjunto de teste continha 4.000 documentos, o conjunto de treino com documentos não rotulados continha 10.000 documentos e o conjunto de treino com documentos rotulados variou de tamanho entre 40 e 6.000 documentos.
Para os grupos Ra e Rb, o conjunto de teste se compunha de 3.299 documentos, o conjunto de treino com documentos não rotulados continha 8.000 documentos e o conjunto de treino com documentos rotulados variou de tamanho entre 20 e 1.200 documentos.
Os algoritmos utilizados para comparação com o proposto foram:
Naïve Bayes, Em com interrupção das iterações3 antes da convergência, M-EM, proposto por Nigam et Os autores utilizaram a medida F com $= 1 para avaliação de performance dos algoritmos.
O algoritmo proposto apresentou performance superior aos demais algoritmos em todos os experimentos.
A diferença de performance foi maior quando o conjunto de documentos rotulados era pequeno.
Analisando a Efetividade e Aplicabilidade do Co--Training Nigam e Ghani realizaram experimento analisando algoritmo de Co-Training, proposto por Blum e Mitchell para conjuntos de dados disjuntos4, como, por exemplo, um documento web e as palavras que ocorrem nos hyperlinks que o referem, para aumentar a performance de algoritmos de aprendizado quando se dispõe de dados rotulados e não rotulados.
Os autores demonstraram que algoritmos que lidam com uma natural disjunção dos atributos do modelo de espaço vetorial obtêm melhor performance.
Além de isto, demonstram a possibilidade de tornar explícita uma disjunção dos atributos quando esta existir, mas for desconhecida.
Em seu artigo, os autores questionaram se realmente é possível pressupor esta disjunção dos atributos em dados reais e, para responder a essa questão, realizaram experimento comparando o Co-Training com o Em, escolhido em vista de se dispor de 2 Considerando- se o total de documentos em ambos os corpora, 20 newsgroups e Reuters 21578.
Para o experimento com o Em, utilizaram, inicialmente, o Naïve Bayes para gerar os parâmetros do Em, usando, apenas, os dados rotulados.
Durante o expectation step é calculada a probabilidade P (C j| di) de ocorrência da classe C j dado o documento não rotulado di.
O maximization step estima os novos parâmetros para o classificador.
Para o Co-Training, treinou- se dois classificadores Naïve Bayes com porções distintas dos atributos.
Para tanto, faz- se uma iteração que inicia treinando os dois classificadores com o conjunto de dados rotulados e, para cada classe Ci, cada classificador rotula o documento com a maior confiança de que pertença à classe Ci.
Espera- se, assim, que, no retreino, o novo documento rotulado por o primeiro classificador forneça melhores dados de treino para o segundo classificador e, da mesma forma, o novo documento rotulado por o segundo classificador forneça melhores dados de treino para o primeiro classificador.
Para os experimentos, foram utilizados dois corpora:
WebKB Course 5:
Uma coleção de documentos web dos departamentos de Ciência da Computação de 4 universidades6.
O objetivo do experimento era descobrir quais documentos são páginas iniciais de cursos acadêmicos.
Foram separados 25% dos documentos para a fase de teste.
O classificador Naïve Bayes foi executado independentemente tanto com 100% dos documentos rotulados quanto com apenas 12, para que se pudesse aferir o ganho proporcionado por os dois algoritmos.
O Em e o CoTraining foram executados com 12 documentos rotulados.
O Co-Training apresentou performance inferior ao Em.
Os autores elencam algumas hipóteses para tanto:
News 2x2:
Para garantir a disjunção dos dados os autores organizaram um corpus baseado no 20 Newsgroups com documentos dos grupos comp..
Os. Ms--windows.
Misc e talk.
Politics. Misc para compor o conjunto dos documentos rotulados como positivos e comp..
Sys. Ibm.
Pc. Hardware e talk.
Politics. Guns para os documentos negativos.
Assim, 6 Universidades de Cornell, do Texas, de Washington e do Wisconsin.
De a análise destes resultados, os autores consideraram ainda não estar provado que a disjunção dos atributos traga benefício para o aprendizado com dados rotulados e não rotulados.
Restava, ainda, determinar se a boa performance se deve realmente à disjunção dos atributos ou se ao fato de que o Co-Training realiza um aprendizado incremental, incluindo um documento por classe a cada iteração, enquanto o Em trata todos os documentos a cada iteração.
Assim, foi realizado novo experimento que demonstrou a efetividade de se tratar a disjunção dos atributos.
Em este experimento, o Em e o Co-Training foram hibridizados, resultando no Co-EM e self-training, com as seguintes características:
Co-EM: Realiza as iterações rotulando todos os documentos.
Mas, tratando separadamente os atributos disjuntos;
Self-Training: Manteve o aprendizado incremental, rotulando um documento por classe a cada iteração.
Mas, usa apenas um classificador que atua sobre a totalidade do atributos.
Tendo, com o novo experimento, provado o benefício de se explorar a disjunção dos atributos, os autores buscaram descobrir se seria possível obter benefícios semelhantes em conjuntos de dados que não apresentem tal disjunção, ao menos não de forma conhecida.
Para tanto, foi realizado novo experimento, ainda utilizando o corpus News 2x2.
Porém, a divisão foi realizada de maneira aleatória.
Os autores obtiveram resultados inferiores aos aferidos quando se conhecia a exata disjunção de atributos, mas superiores em relação a o tratamento dos atributos de forma indistinta.
No entanto, ao se utilizar o corpus News5 que contém os documentos dos grupos comp..
De o corpus 20 Newsgroups, verificou- se que, apesar de não haver uma disjunção natural nos atributos, o Co-Training ainda teve uma taxa de erro 10% inferior ao Em.
Os autores ressaltam que o ganho ocorreu ao se realizar uma separação randômica de atributos e pretendem elaborar um algoritmo de separação baseado em informação mútua.
Cabe ressaltar que, neste trabalho, Nigam e Ghani não propõem categorização auxiliada por clustering, objeto do presente estudo.
No entanto, considerou- se relevante revisar este experimento tendo em vista que sua validação ocorreu por comparação com processo de categorização bayesiana auxiliada por clustering usando o algoritmo Em e que os resultados demonstraram que o Co-Training obteve performance superior.
Trabalhos Baseados em Classificadores SVM ou Derivados do SVM Em esta seção estão agrupados os trabalhos que utilizam classificador SVM ou de ele derivado.
Como na seção anterior, o trabalho da Subseção 3.3.3 também não apresenta experimentos com textos em linguagem natural sem formatação específica.
De a mesma maneira, sua inclusão neste estudo considera as metodologias propostas que podem ser utilizadas com documentos textuais.
Combinando Clustering e Co-Training para Melhorar a Classificação de Textos Usando Dados Não Rotulados Raskutti, Ferrá e Kowalczyk, buscam solucionar uma limitação do Co-Training, que pressupõe que os atributos dos dados possam ser divididos em 2 grupos distintos, cada qual usado no treinamento de um classificador diferente.
Eles apresentam uma proposta em que os atributos derivados do pré-processamento dos textos constituem o grupo usado no treinamento do primeiro classificador, denominado classificador W P e, para treinar o segundo classificador, denominado Cf, propõem o uso de Clustering dos documentos para gerar novos atributos, contendo informações tais como medidas de similaridade.
O processo de clustering empregado por os autores tem complexidade O (r2), para r sendo a quantidade de amostras de textos rotulados e não rotulados usadas no treinamento.
Para evitar que o tempo de treino escale a níveis que exijam demasiados recursos computacionais, os autores dividem as amostras em S partições antes de executar o clustering.
De cada partição, são selecionados, apenas, os N maiores clusters.
Cada cluster Ci gera os seguintes novos atributos para os documentos:
Uma flag indicando se este é o cluster mais próximo de o documento;
A similaridade com o centróide de Ci;
A similaridade com o centróide dos documentos não rotulados de Ci;
Para cada classe q, a similaridade com o centróide dos documentos de q presentes em Ci.
Assim, a quantidade de novos atributos é de Sn (q+ 3).
Os classificadores usados no Co-Training utilizam o algoritmo SVM da seguinte maneira:
Treinam- se os classificadores W P e Cf;
Usa- se o Cf para rotular o conjunto de treino e seleciona- se alguns destes documentos para integrar o novo conjunto de treino do W P;
Usa- se o W P para rotular o conjunto de treino e selecionam- se alguns destes documentos para integrar o novo conjunto de treino do Cf;
Treinam- se novamente os classificadores W P e Cf, agora denominados W Pco e CFco.
Originalmente, os autores propuseram iterar os passos 2 a 4 acima.
No entanto, após realizarem os primeiros testes, concluíram que os melhores resultados eram decorrentes de uma única etapa de co-training e, assim, descartaram tal iteração.
Para os experimentos, os autores dividiram o conjunto de treino em 5 partições e, de cada partição, selecionaram os 20 maiores clusters para a geração de novos atributos.
Assim, S $= 5 e N $= 20.
Os autores utilizaram os seguintes corpora:
WebKB: As 4 categorias mais populosas, excluindo a categoria others e as páginas de redirecionamento de navegador, totalizando 4.108 páginas.
Eles selecionaram, randomicamente, 225 documentos para o treino e 800 para o teste.
Os demais documentos constituíram o conjunto de dados não rotulados.
Após o pré-processamento, os autores obtiveram 87.601 atributos derivados das palavras extraídas dos documentos e 700 gerados por o clustering.
Reuters 215787:
O modApte split, com 9.603 documentos de treino e 3.299 de teste.
Os autores selecionaram as 10 categorias mais populosas e as dividiram em conjuntos rotulados e não rotulados.
Embora tenham dividido as categorias em diferentes proporções, não foram informados o critério de divisão nem o tamanho de cada conjunto.
Após o pré-processamento, os autores obtiveram 20.197 atributos derivados das palavras extraídas dos documentos e 1.300 gerados por o clustering.
20 Newsgroups:
Mensagens de 20 newsgroups, totalizando 18.828 documentos, sem os documentos redundantes, divididos homogeneamente entre os grupos.
A maior parte dos cabeçalhos foi removida.
Não foi informado quais cabeçalhos permaneceram.
Os autores selecionaram, randomicamente, 2.000 documentos para o conjunto de dados rotulados de treino e 8.000 para o conjunto de dados não rotulados.
O restante foi utilizado para teste.
Após o pré-processamento, os autores obtiveram 26.362 atributos derivados das palavras extraídas dos documentos e 2.300 gerados por o clustering.
Os autores reportam utilizar pré-processamento compatível com o relatado em, e.
Não se obteve acesso a este último artigo para conferência destes dados.
Quanto a os dois primeiros, há concordância em relação a o pré-processamento realizado sobre o WebKB, que não passa por stemming ou remoção de stopwords.
Porém, quanto a o Reuters 21578, removem stopwords mas não fazem stemming.
Já faz o stemming e a remoção de stopwords.
De a mesma maneira, quanto a o 20 Newsgroups, removem stopwords mas não fazem stemming.
Não utiliza o corpus 20 Newsgroups.
Os algoritmos comparados foram os utilizados no Co-Training, todos baseados em SVMs com kernel linear.
Dois de eles foram treinados com os atributos extraídos das ocorrências das palavras, com ou sem Co-Training, W Pco e W P, respectivamente.
Os outros dois foram treinados com os atributos gerados por o clustering dos documentos, com ou sem Co-Training, CFco e Cf, respectivamente.
Após os experimentos, os autores verificaram que o W Pco apresentou a melhor performance na maioria dos experimentos.
Assim, consideraram este seu classificador final.
O CFco raramente apresentou melhor performance que o W Pco e, na maioria das vezes em que apresentou boa performance, o W Pco apresentou performance superior, demonstrando- se bastante sensível à influência do Co-Training.
Os autores ainda fazem algumas considerações acerca de o pequeno ganho apresentado por o CFco em função de a baixa qualidade dos atributos gerados por o clustering como, por exemplo, atributos binários.
No entanto, percebe- se que estes atributos trazem um ganho significativo para o classificador W Pco.
Este ganho, porém, ocorre somente na primeira iteração do Co-Training.
Analisando a natureza dos atributos gerados por o clustering, verifica- se que, exceto por os dois primeiros atributos de cada cluster, os demais, um para cada classe e um para os não rotulados, são dependentes de centróides das respectivas classes (ou dos documentos não rotulados).
Ora, após a primeira iteração do Co-Training, alguns documentos são rotulados e, portanto, mudam os centróides citados.
Não há menção a novas execuções do passo de clustering, nem a qualquer recálculo de distância de centróides.
De esta maneira, supõe- se que não há alteração dos valores dos atributos utilizados no treinamento do CFco, e, conseqüentemente, estes atributos tornam- se cada vez menos representativos comprometendo, assim, a performance do CFco.
CBC: Classificação de Texto Baseada em Clustering Requerendo Mínimos Dados Rotulados Zeng Propõem classificação de dados não rotulados utilizando algoritmo de clustering guiado por um pequeno conjunto de dados rotulados.
Os dados não rotulados são, então, rotulados de acordo com o cluster ao qual foram associados.
Dispondo, então, de um conjunto maior de dados rotulados, realiza- se o treinamento do classificador.
Os autores ressaltam que, embora a técnica descrita acima não seja uma proposta nova, o método por eles proposto objetiva solucionar dificuldades de classificação quando a quantidade de dados rotulados é extremamente pequena, como, por exemplo, menos de 10 amostras para cada rótulo.
Enquanto outras propostas enfocam na classificação auxiliada por dados não rotulados, o enfoque, aqui, é no clustering auxiliado por dados rotulados.
Conforme os autores, os métodos de clustering são menos sensíveis a tendências causadas por dados esparsos iniciais que os de categorização.
Além de isto, o método de clustering proposto é, de fato, um classificador baseado numa distribuição de probabilidade e, assim, conforme demonstrado por Ng e Jordan, atinge sua performance assintótica mais rapidamente que os modelos discriminativos.
Para a fase de clustering foi utilizada a de classes existentes nos dados rotulados.
Para a fase de categorização foi utilizado o Transductive SVM, TSVM.
Os algoritmos de clustering e categorização são sucessivamente invocados através de sucessivas iterações auxiliando- se mutuamente no ato de rotular os dados:
Em o passo de clustering, calculam- se os centróides de cada classe considerando- se somente os dados rotulados.
Estes centróides são usados como semente inicial do KMeans.
Após a convergência do K--Means, somente um percentual p dos documentos não rotulados mais próximos dos centróides recebem o rótulo atribuído ao respectivo centróide.
Os demais documentos permanecem sem rótulo.
Em o passo de categorização, realiza- se o treino do TSVM com todos os dados (rotulados e não rotulados) e, de cada classe, seleciona- se o mesmo percentual p de documentos não rotulados com a maior margem e aplica- se o rótulo da respectiva classe.
Esta iteração entre os algoritmos repete- se até que não restem documentos sem rótulo.
Os autores realizaram 3 experimentos com os seguintes corpora:
20 Newsgroups, Reuters-21578 e páginas web do Open Directory Project (ODP).
Para o Cluster Based Categorization, CBC, foram extraídos, de cada documento, um vetor de atributos para o título e outro para o corpo do documento.
Para os demais algoritmos, foi extraído apenas um vetor de atributos para o corpo e o título de cada documento.
Foi feita a stemmização das palavras e a remoção das stopwords e das palavras que ocorreram em no máximo 3 documentos.
Também foram removidas as palavras que ocorriam apenas no conjunto de teste, mas não no conjunto de treino.
Os atributos dos vetores receberam o TF-IDF das palavras restantes.
Foi necessário reduzir o número de classes em cada corpus devido a o tempo de treino do TSVM escalar em função de a quantidade de classes.
Assim, foram utilizadas:
20 Newsgroups:
As mesmas 5 classes comp..
Utilizadas em Nigam e Ghani, contando, cada uma, com, aproximadamente, 1.000 documentos, 80% na fase de treino e 20% na fase de teste.
Após o pré-processamento, restaram 14.171 palavras distintas, 14.059 no corpo dos documentos e 2.307 no título;
Reuters-21578: As 10 maiores classes, earn, acq, money-fx, grain, crude, trade, interest, ship, wheat e corn, particionadas de acordo com o ModApte, havendo 6.649 documentos de treino e 2.545 de teste.
Após o pré-processamento, restaram 7.771 palavras distintas, 7.065 no corpo dos documentos e 6.947 no título;
Open Directory Project: As 6 maiores classes do segundo nível do diretório, Business/ Management, Computers/ Software, Shopping/ Crafts, Shopping/ Home &amp; Garden, Society/ Religion &amp; Spirituality e Society/ Holidays.
Foram utilizados 50% dos documentos para o treino e 50% para o teste.
Após o pré-processamento, restaram 17.050 palavras distintas, 16.818 no corpo dos documentos e 3.729 no título;
Os autores utilizaram o pacote SVM--Light8 para as categorizações SVM e TSVM, com kernel linear.
O percentual de documentos não rotulados eleitos para recepção de rótulos tanto no passo de clustering quanto no de categorização do CBC foi de 1%.
Como a classificação envolvia múltiplas classes, foi necessário treinar vários categorizadores SVM um-contra- todos.
A métrica de avaliação foi micro-averaging F1, que se constitui numa média ponderada de cada F-Measure, com $= 1.
Em todos os corpora o CBC apresentou performance significativamente superior quando o conjunto de dados rotulados era pequeno, perdendo esta diferença e tornando- se aproximadamente equivalente ao SVM, TSVM e Co-Training à medida que aumentou o conjunto inicial de documentos rotulados.
Também foi avaliado o impacto de diferentes percentuais de seleção de documentos para aplicação de rótulo ao final das fases de clustering e categorização.
Os autores perceberam que o percentual de 100%, ou seja, apenas uma iteração de clustering e categorização, foi claramente superior a percentuais inferiores.
Pretendem estudar as razões de tal comportamento, que acreditam ocorrer em função de o categorizador não conseguir acrescentar documentos que contribuam com informação significativa ao algoritmo de clustering.
Li, Chi, Fan e Xue propõem o algoritmo Support Cluster Machine.
Os autores argumentam que o SVM sofre de problemas de escalabilidade e acrescentam uma fase de pré-processamento ao SVM buscando reduzir a quantidade de instâncias usada para treinar o classificador.
Diferentemente de outras propostas, o objetivo do clustering não é a seleção das instâncias mais representativas para treinamento do classificador.
O classificador é treinado com os centróides obtidos por o clustering.
Este modelo parte do pressuposto que os dados seguem uma distribuição estatística e, assim, a escolha dos centróides para o treinamento é mais adequada que a de instâncias representativas por preservar com maior exatidão o perfil estatístico do conjunto de dados original.
Desta forma, os autores reportam que obtiveram acurácia equivalente ao SVM com significativa redução de custo computacional.
Os autores ainda relatam que a mesma função de medida de distância usada para comparar os clusters entre si na fase de clustering é, também, usada na categorização, comparando documentos com os clusters.
Não definem um algoritmo de clustering específico a ser empregado no pré-processamento:
De entre as sugestões apresentadas encontram- se o K--Means, conforme implementado em Hartigan e Won, Em e clustering hierárquico.
Em os seus experimentos, os autores optaram por o algoritmo Threshold Order Dependent, TOD apud.
Este algoritmo, para cada documento, verifica se este está a uma distância acima de um valor de corte do ponto mais próximo.
Se estiver, um novo cluster é formado com o documento como centro.
Senão, associa- o ao cluster mais próximo.
Os autores optaram por usar este algoritmo por sua complexidade linear e por ser capaz de lidar com dados seqüenciais com uma complexidade espacial insignificante.
Para fins de comparação, o Em também foi utilizado.
Os conjuntos de dados utilizados por os autores foram:
Toydata, gerado randomicamente por os autores, 2) um banco de dados com imagens de números escritos a mão, divididos em 10 classes, obtido do MNIST9 e 3) o banco de dados Adult10, com informações sobre renda, construído a partir de dados de censo.
Classificação SVM Hierárquica Baseada em Support Vector Clustering e sua Aplicação na Categorização de Documentos Hao, Chiang e Tu relatam experimento de categorização de documentos usando uma hierarquia de classes obtida através de clustering.
Os autores optaram por utilizar SVM, Support Vector Machine, como algoritmo de classificação por entenderem tratar- se do estado da arte na categorização de documentos.
No entanto, preocupando- se com a dificuldade de se obter um bom classificador capaz de dintingüir entre múltiplas classes em face de a facilidade de se obter um que trate apenas duas classes, decidiram gerar múltiplos classificadores binários.
Duas estratégias podem ser utilizadas para se reconhecer diversas classes usando múltiplos classificadores binários:
A estratégia um-contra- todos consiste em gerar um classificador binário para cada classe, ou seja, um classificador que decide se o documento pertence a uma determinada classe ou não;
A estratégia um-contra-um consiste em gerar um classificador binário para cada par de classes.
Desta forma, para decidir se um documento pertence à classe xi, é n é o número de classes.
De esta maneira, encontraram um novo problema:
A escalabilidade.
Além de a grande quantidade de atributos típicos da categorização de documentos, ter de lidar com uma grande quantidade de classes eleva o custo computacional a patamares proibitivos.
Para lidar com esta questão, adotaram um modelo hierárquico de classes.
Conforme os autores, um dado documento poderá ser classificado numa classe folha ou numa classe mais genérica, representada por um nodo intermediário da árvore.
O processo de classificação consiste, então, em realizar categorizações flat iniciando na raiz da árvore de classes, descendo recursivamente através de um ou mais ramos.
A cada nível, a categorização ocorre usando um dos seguintes métodos:
Se o nodo se dividir em dois ramos, é utilizada uma classificação binária SVM;
Se o nodo possuir mais de dois ramos, são utilizados múltiplos classificadores.
A decisão entre estratégia um-contra- todos ou um-contra-um tem como base a acurácia obtida em cada estratégia no nodo corrente.
Devido a a dificuldade de se obter documentos pré-rotulados por especialista humano para as fases de treino e teste, os autores utilizaram SVC, Support Vector Clustering, para gerar a hierarquia automaticamente.
Inicialmente, cria- se a raiz representando todos os documentos num único grupo e vai se subdividindo os grupos através da variação de parâmetros do SVC11, gerando os diversos níveis da hierarquia de classes.
Para decidir quando parar a divisão dos grupos, os autores utilizaram a medida Cs para fuzzy clustering, que leva em conta tanto o grau de compactação dos documentos de cada grupo, quanto o grau de separação dos grupos entre si.
O pré-processamento dos documentos compreendeu a remoção de stopwords e palavras com menos de quatro ocorrências no corpus.
Ainda assim, as instâncias contavam com, aproximadamente, 10.000 atributos.
De esta maneira, os autores reduziram a dimensionalidade utilizando ganho de informação, Learning Vector Quantization e Latent Semantic Indexing.
Foi utilizado o corpus Reuters-2157812.
O treino utilizou 9.603 documentos e o teste utilizou 3.299 documentos.
Foram eliminadas classes que não continham documentos para treino ou teste e documentos que não estavam ligados a nenhuma classe.
O conjunto resultante tinha 90 categorias.
De esta maneira, os autores realizaram seus experimentos com o mesmo conjunto de documentos utilizados nos experimentos descritos por Joachims.
Os autores mediram a performance usando F-Measure com $= 1.
Selecionaram as 10 categorias mais freqüentes e compararam os resultados obtidos com Decision Tree C4.
5, KNN e SVM não hierárquico.
O método proposto obteve melhor resultado em 6 categorias.
Verificaram que nas 4 categorias em que não alcançou a melhor performance, possuíam grande número de subcategorias e os seus grupos geradores não tinham boa separação, sendo comum que se intercalassem.
Mineração de Textos de Decisões da Suprema Corte Administrativa Austríaca Feinerer e Hornik, relatam experimento de clustering e classificação de documentos contendo jurisprudência da Suprema Corte Administrativa Austríaca, no subdomínio do Direito Tributário, no período de 2.000 a 2.004, tendo em vista a importância de seus efeitos no setor comercial.
O objetivo deste estudo foi comparar os agrupamentos formados com estudos anteriores, da década de 1.980, acerca de jurisprudência no mesmo subdomínio, a fim de averiguar os efeitos das mudanças sociais do corpo normativo tributário Austríaco.
Os autores usaram um corpus de treino composto de 994 documentos textuais, cada um contendo uma decisão da corte em língua Alemã.
As palavras foram stemmizadas e receberam 2 medidas de peso:
TF e TF-IDF.
Especialistas do domínio realizaram, manualmente, a divisão dos textos em 3 grupos, classificando- os com os rótulos &quot;VA «Tax», &quot;Income Tax «e &quot;outros».
Em o primeiro experimento, os autores usaram o algoritmo K--means.
No entanto, embora os testes tenham durado poucos minutos em seus equipamentos, os autores afirmam estarem cientes de que a escalabilidade dos dados demandaria demasiados recursos computacionais.
Assim, um novo experimento de agrupamento foi realizado.
Desta vez, cada cluster foi manualmente configurado par ter um conjunto específico de palavras-chave do subdomínio.
Assim, cada documento foi analisado tendo em vista a similaridade com o conjunto de palavras-chave.
Este método foi denominado Keyword Based Clustering Method, ou Método de Agrupamento Baseado em Palavras-Chave.
Ambos experimentos foram avaliados por meio de os índices Rand e cRand.
O Keyword Based Clustering Method superou o K--Means, aumentando o índice Rand de 0,52 para 0,66 e o cRand de 0,03 para 0,32.
O grupo &quot;Income Tax «teve 100% de precisão e Recall.
Posteriormente, os autores realizaram experimentos de classificação de documentos de jurisprudência em 2 grupos:
Documentos que tratam de matéria fiscal Austríaca e documentos que não tratam desta matéria.
Foi utilizada a classificação &quot;C-SVC «com Support Vector Machines.
Foram utilizados 200 documentos para o treino e 50 para o teste.
O treino da SVM levou um dia e a classificação de cada documento foi calculada em 15 minutos num computador com processador de 2.6 GHz e 2 Gb de memória principal.
Assim, os autores decidiram utilizar a abordagem de matriz termo-documento, com pesos baseados em TF e TF-IDF.
Novamente utilizaram 200 documentos para o treino e 50 para o teste.
Obtiveram índices Rand em torno de 0.6 e cRand em 0.2, tidos como altamente promissores por os autores e indicativos de que o uso de SVM para a classificação de textos tem um grande potencial.
Em este trabalho, Feinerer e Hornik não propõem novos algoritmos.
Apenas o uso de algoritmos já conhecidos.
Além de isto, o processo de classificação após o clustering apenas determinava se o documento classificado fazia parte do domínio ou não.
Tem- se como maior contribuição o uso do Keyword Based Clustering Method e, uma vez que, se comparado à situação brasileira, estão disponíveis, entre nós, o Vocabulário Controlado Básico, publicado por o Senado Federal Brasileiro e o Tesauro Jurídico da Justiça Federal Brasileira.
Pretende-se utilizar- los na fase de pré-processamento dos exemplos de uso propostos em nosso trabalho, tal como descrito no Capítulo 4.
Aprendizagem Ativa Usando Pré-Clustering Nguyen e Smeulders propõem a utilização de clustering para auxiliar algoritmo de Active Learning, proposto por Lewis e Gale.
O algoritmo original baseia- se em treinar um classificador com um conjunto de dados rotulados iniciais e, então, realizar iterações executando o classificador sobre dados não rotulados para selecionar os n documentos que o classificador tenha a menor certeza de qual rótulo aplicar.
Estes documentos são, então, rotulados por especialista humano.
O classificador realiza novo treino incluindo os novos documentos rotulados.
Esta iteração se repete enquanto o especialista humano estiver disposto a realizar classificações.
Em esta proposta, os autores usam um algoritmo de clustering para 1) realizar a seleção de documentos a rotular e 2) rotular os documentos sem a intervenção humana.
Para tanto, o algoritmo de soft clustering agrupa os documentos rotulados e não rotulados, selecionando, primeiramente, os documentos mais representativos dos clusters para rotular e aplica o rótulo do documento rotulado mais próximo no mesmo cluster.
A partir de então, o algoritmo passa por cada cluster, iniciando por os mais densos, selecionando dois tipo de amostras:
Os documentos não rotulados mais representativos do cluster e 2) os documentos não rotulados mais próximos dos limites entre clusters.
Os documentos que pertencem a um único cluster recebem o rótulo do documento rotulado mais próximo e os que pertencem a mais de um cluster são atribuídos ao cluster de maior probabilidade e, então, recebem rótulo do documento rotulado mais próximo que esteja neste cluster.
Quando a margem de classificação atinge a borda dos clusters, é executado um novo reagrupamento com um valor de limiar menor, a fim de obter mais clusters de menor tamanho, refinando, assim, a qualidade da classificação.
Foram realizados dois experimentos:
O primeiro buscou detectar imagens que contivessem rostos humanos.
Os autores utilizaram 2.500 imagens com tamanho 20×20, obtidas conforme experimentos referidos em artigo anterior de Pham, Worring e Smeulders.
No entanto, destaca- se que o referido artigo relata a construção de um banco de dados contendo 33.360 faces e 360.000 padrões não faciais.
O processo por o qual os autores selecionaram 2.500 imagens deste conjunto anterior não foi informado.
O segundo buscou identificar números escritos a mão, separando imagens de um determinado dígito das demais.
As imagens foram obtidas do banco de dados MNIST13.
Para comparações, foram implementados três outros algoritmos de Active Learning, todos usando SVM linear para classificação.
O primeiro seleciona os dados de treino randomicamente.
O segundo seleciona as instâncias mais próximas da borda de classificação.
O terceiro usa os medóides dos clusters próximos à margem de o SVM.
Usando Supervisão Parcial para Categorização de Textos Aggarwal, Gates e Yu propõem método de categorização utilizando classes definidas por algoritmo de clustering parcialmente supervisionado.
Em o experimento realizado, os autores usaram a taxonomia do Yahoo14, para gerar as sementes iniciais para o algoritmo de clustering.
O algoritmo de clustering proposto emprega junção de clusters cujos centróides sejam muito próximos, além de remoção de clusters com pequena quantidade de documentos.
Por esta razão, após o clustering, as classes geradas divergem da taxonomia do Yahoo, embora mantenham coerência com estas, segundo avaliação humana realizada.
Uma vez obtidas as classes, a categorização de novos documentos é feita por algoritmo também proposto por os autores, que emprega a mesma medida de distância utilizada no clustering.
Por esta razão, os autores afirmam que a categorização pode, teoricamente, obter acurácia perfeita e, portanto, a qualidade da categorização passa a depender exclusivamente da qualidade do clustering.
De acordo com os autores, o clustering sem qualquer tipo de supervisão é capaz de gerar grupos de boa qualidade somente quando há uma pequena quantidade de grupos, aproximadamente 50.
O experimento realizado gerou 1.167 clusters.
Ambos os algoritmos utilizam o modelo de espaço vetorial para representação dos documentos e a distância de cosseno como medida de similaridade.
A fase de pré-processamento utiliza o cálculo do Índice Normalizado Gini para o descarte de atributos.
Para tanto, calcula- se a presença fracional de uma dada palavra numa classe i, dada por fi/ ni, onde fi é a freqüência da palavra na classe i e ni a contagem de palavras na classe i, o desvio fracional pi é definido por pi $= fi/ ni j $= 1 f j/ n j onde K é o número de classes.
O Índice Normalizado Gini normalizado de uma dada palavra é dado por:
G $= 1-p2 i $= 1 Assim, conforme inlustrado na Figura 3.1, à medida que se equilibra a distribuição de uma palavra através de diferentes classes, o Índice Normalizado Gini se aproxima de 1 -- 1/ K. Por outro lado, conforme a palavra se demonstrar muito particular de uma dada classe, o Índice Normalizado Gini decresce significativamente.
O algoritmo de clustering parte de um conjunto inicial de centróides e realiza sucessivas iterações, divididas em 4 fases:
Atribuição de Documentos:
Cada documento é associado ao cluster cujo centróide esteja mais próximo.
Documentos cuja distância até o centróide mais próximo esteja acima de um valor de corte são descartados como ruído.
A o final da fase é calculado um novo centróide para cada grupo;
Seleção de Atributos: As palavras com o menor peso na definição dos centróides são descartadas.
Este descarte deve ocorrer gradualmente, a cada iteração, e não numa única vez para que não ocorra a perda de atributos importantes em função de centróides ainda não muito bem refinados;
A condição de parada das iterações é baseada na quantidade de atributos.
A o atingir um valor de corte mínimo, as demais fases da iteração corrente são executadas e, então, encerram- se as iterações.
O algoritmo de categorização, a exemplo do algoritmo de clustering, poderia simplesmente classificar os novos documentos buscando o centróide mais próximo, utilizando a mesma função de similaridade.
No entanto, os autores ressaltam que há a possibilidade de que um documento seja mal classificado quando houver um documento muito próximo a diferentes clusters.
Para distingüir entre assuntos muito próximos, os autores empregam um método proposto originalmente por Chakrabarti, adaptado a um modelo não-hierárquico.
O algoritmo de categorização seleciona os k clusters com centróides mais próximos e seleciona o cluster de maior dominação.
Assim, sejam, por exemplo, dois grupos G1 e G2, dois documentos d1 e d2 a classificar, conforme a Figura 3.2, um valor de limiar l e uma função de similaridade sim (Gi, d j).
Se a similaridade sim\&gt;+ l), ou seja d1 está muito mais próximo de G1 que de G2, G1 é dominante em relação a d1, que é classificado como pertencente à classe representada por G1.
No entanto, embora sim\&gt; sim (G2, d2), sim (G1, d2) (sim (G2, d2)+ l), ou seja, d2 encontra- se numa região intercluster.
Em este caso, a dominância é definida através de uma segunda função de similaridade simdi f (Gi -- G j, dk), que calcula a similaridade desconsiderando, para tanto, os atributos não nulos de G j, ou seja, descartam- se de Gi, todos os atributos zerados em seu centróide e todos os atributos diferentes de zero em G j.
Por exemplo, supondo que os vetores de atributos sejam compostos por os atributos &quot;A «a &quot;G», conforme apresentado na Figura 3.3, a função simdi f irá calcular a distância de d2 ao centróide de G1 considerando, somente, os atributos &quot;D «e &quot;E».
Já a função simdi f irá calcular a distância de d2 ao centróide de G2 considerando, somente, os atributos &quot;C «e &quot;F».
Em o experimento realizado, os autores tentaram, primeiramente, realizar o clustering totalmente não supervisionado.
No entanto, as classes obtidas eram muito genéricas fazendo a mistura de assuntos como &quot;arte por computador», &quot;artesanato «e &quot;museus», resultando no assunto demasiadamente genérico &quot;arte».
Assim, decidiram por a utilização da taxonomia do Yahoo, na versão de novembro de 1.996, para gerar as sementes iniciais do algoritmo de clustering.
Para tanto, a árvore de assuntos do Yahoo foi truncada, totalizando, então, 1.463 nodos folha.
Através desta, obtiveram o corpus de, aproximadamente, 167 mil documentos.
Durante o pré-processamento dos documentos, foram utilizadas as seguintes reduções de dimensionalidade dos documentos:
Descarte das palavras que ocorriam em, no máximo, 7 documentos, reduzindo de 700 mil palavras distintas no corpus, para 87 mil;
Descarte das 10 mil palavras com o maior Índice Normalizado Gini, resultando em, aproximadamente, 77 mil palavras;
Os parâmetros utilizados para o clustering foram:
Condição de Parada das Iterações:
Redução da dimensionalidade para 200 palavras;
Fator de Redução da Dimensionalidade: 0,7;
Limiar para Descarte de Grupo:
8 documentos;
Limiar para Agregar Grupos:
Similaridade entre os centróides superior a 0,95.
Em o algoritmo de classificação, para detectar se C1 é dominante em relação a C2, a primeira condição testada verificava se a similaridade de um dado documento d tinha uma similaridade superior a de C2 em relação a d em, no mínimo l. Em o experimento, foi utilizado um limiar l de 0,025.
Para avaliar a performance do processo, uma vez que as categorias não eram iguais às do Yahoo, não era possível usar estas classes para verificar a correta pertinência de documentos.
Assim, procedeu- se a uma avaliação empírica que consistiu em separar uma amostra de 141 documentos dos clusters obtidos e entrevistar 10 pessoas que responderam, para cada documento, uma das 5 opções:
Categorização do Yahoo é melhor;
Esta categorização é melhor;
Ambas estão igualmente corretas;
Nenhuma está correta (6%);
Não sabe.
O trabalho de Aggarwal, Gates e Yu destaca- se não apenas por propor algoritmos de agrupamento e de categorização, mas, também, por diversas proposições, tais como o uso do Índice Normalizado Gini e avaliação do peso das palavras na definição dos centróides dos clusters na seleção de atributos e a função de similaridade do categorizador, que nem sempre decide por a mera verificação de distância simples, mas faz seleção de atributos quando um documento está próximo de mais de um cluster.
Ressalte- se que esta função de similaridade é tida por os autores como uma das contribuições do artigo.
A outra contribuição, que determinou o título do artigo, é a conclusão dos autores que o clustering sem supervisão gera classes muito genéricas quando é grande o número de clusters.
Verifica- se, no entanto, que o algoritmo de clustering proposto itera 4 fases e, na terceira fase, realiza a aglomeração de clusters que estejam muito próximos.
Isto é um indicativo de que tal problema possa ser solucionado diminuindo- se o valor de limiar usado para definir a união de clusters.
Embora tenha apresentado bons resultados, não foi localizada, até o presente, uma continuidade para o mesmo.
Foram encontradas 18 citações a este trabalho, mas nenhuma que aproveitasse as metodologias em ele apresentadas.
Alguns dos trabalhos da Seção 3.3 são mais recentes e, assim, constituem indicativo de que haja uma certa tendência atual em investigar o processo de categorização auxiliada por clustering usando categorizadores da &quot;família «SVM.
Uma das possíveis causas disto pode ter sido a escolha do corpus para os experimentos.
O corpus usado por os autores era composto de documentos obtidos através do Yahoo, diferentemente da maioria dos trabalhos aqui apresentados, que deram preferência a corpora mais comumentemente usados em pesquisas de aprendizado de máquina, tais como o 20 Newsgroups, Reuters2157815 e WebKB Course16.
De esta maneira, torna- se mais difícil comparar diferentes propostas.
Além de isto, a forma como o método proposto foi validado também dificulta comparações:
Não foram utilizadas medidas de avaliação dos resultados tais como a medidas de coesão/ separação de clusters, apenas avaliação humana sob critérios subjetivos tais como &quot;esta classificação é melhor /pior/equivalente àquela classificação».
O mesmo experimento, com os mesmos resultados, avaliado por outro grupo poderia obter, a nosso ver, avaliação bem distinta.
Considerações Finais Foi revisada a literatura pertinente e, além de reunir- se conhecimento para empreender uma solução para problema nessa área, percebeu- se uma mudança de tendência no processo de categorização auxiliado por clustering, usando redes bayesianas e Em para categorizadores SVM ou derivados, não havendo preferência clara por qualquer algoritmo de clustering.
Detectou- se o trabalho da Seção 3.4, entitulado &quot;Usando Supervisão Parcial para Categorização de Textos», como alternativa isolada, indicativo, talvez, de possibilidades de aprofundamento de estudos.
A Tabela 3.1 apresenta comparação dos trabalhos relacionados.
A coluna &quot;Processo Proposto «apresenta os algoritmos de agrupamento e categorização adotados por os respectivos autores em suas proposições.
A coluna &quot;Validação «apresenta os algoritmos usados para comparar os resultados, validando seus respectivos processos.
Toutanova E Hao, Chiang e Tu, a compararam exclusivamente com experimentos com classificadores.
Feinerer e Hornik e Aggarwal, Gates e Yu não realizaram comparações, preferindo avaliação por especialista humano.
Quanto a a redução de atributos, note- se que, apesar de a maioria dos trabalhos estudados usarem stemming para esta atividade, optou- se por a lematização, tendo em vista as perdas decorrentes de um processo de stemming em línguas mais ricas em inflexões que o inglês, conforme visto por Korenius em textos finlandeses e Gonzalez em textos em português.
Quanto a o objetivo de cada trabalho notamos que, embora haja um processo de clustering seguido de categorização, apenas os trabalhos de Feinerer e Hornik e Aggarwal, Gates e Yu utilizam o clustering para descobrir as classes a serem utilizadas por o categorizador.
Hao, Chiang e Tu utilizam o clustering para descobrir a hierarquia das classes.
A maioria dos trabalhos utiliza o clustering para melhorar a performance dos classificadores, mantendo o conjunto de classes original.
São exemplos disto, os trabalhos de Zeng E Nigam e Ghani que buscam aumentar o conjunto de treino;
Toutanova, que buscam gerar os parâmetros bayesianos;
Cong, Lee, Wu e Liu, que buscam garantir a relação um-para-um entre os grupos e as classes;
E Raskutti, Ferrá e Kowalczyk, que buscam prover disjunção de atributos, requerida por o Co-training.
Quanto a a avaliação de suas propostas, novamente, Feinerer e Hornik e Aggarwal, Gates e Yu se distingüem, realizando avaliação humana, em contraste com os demais que avaliam por comparação com estudos anteriores.
Dois fato-res contribuem para tal ocorrência:
A construção dos corpora utilizados e a proposta de geração de classes.
Ao dispor de conjunto de documentos e classes distinto dos encontrados em outros estudos, ficam sem possibilidade de usar- los para comparação.
Restam- lhes duas alternativas:
Realizar nova execução dos algoritmos a comparar ou avaliar através de especialista humano.
Semelhante situação ocorre em nosso estudo.
No entanto, devido a as dificuldades de se conseguir realizar avaliação humana de uma grande quantidade de dados, optamos por reexecutar o algoritmo de Aggarwal, Gates e Yu utilizando os mesmos dados de nosso estudo, conforme veremos no Capítulo 3, para avaliar os resultados do clustering, restringindo a avaliação humana aos resultados da categorização.
Usa 2 classificadores, um para cada conjunto disjunto de atributos.
Aumentar o conjunto de treino.
Prover relação 1x1 entre grupos e classes.
Descoberta classes Geração de árvore de classificadores Aumentar o conjunto de treino.
Provê disjunção de atribs.
Descoberta classes Nigam Cong Feinerer Hao Zeng Raskutti Aggarwal de Parâmetros bayesianos $= hidden values estimados e maximizados por o Em.
Toutanova Gerar parâmetros bayesianos Centróides pré-definidos por taxonomia.
Modifica a taxonomia.
Descarta docs/ grupos.
Iterações do SVC dividem em grupos cada vez menores.
Centróides pré-definidos por taxonomia.
Iterações rotulam subset dos documentos.
Gera segundo grupo de atributos através de clustering Termos pré-definem centróides, descarta demais tokens.
Classificadores podam árvore gerada por clustering hierárquico.
Método Referência Objetivo Assign Flat 1 pass TClus Naïve Bayes Naïve Bayes hard.
KMeans CoTrain.
Processo Proposto Clust. Classif.
Naïve Bayes Flat 1 pass Aval.
Humana CoTrain.
Validação Clust. Classif.
Naïve Bayes, CoNaïve Bayes SelfTrain.
Atualmente, o trabalho de pesquisa jurisprudencial realizado por os Operadores do Direito demanda demasiado tempo em virtude de as limitações das ferramentas de pesquisa disponíveis.
De entre as limitações encontradas neste sistema, destacamos:
Escopo da Pesquisa:
A jurisprudência é composta de documentos cuja classificação é um preâmbulo, denominado ementa composto de uma seqüência de termos jurídicos e um resumo do tema abordado no texto.
A Figura 4.1 apresenta uma visão geral da estrutura do texto jurisprudencial.
As seções 1, 2, 3 e 6 do documento representam informação do caso específico, não do tema debatido no texto.
A seção 4 é o caput da ementa, composto de seqüências de termos, simples ou compostos, separados por caracteres de ponto`.',
destacados em vermelho.
A seção 5 é o corpo da ementa, que apresenta um resumo dos temas abordados, sem termos específicos que os identifiquem.
A seção 7 é o relatório dos fatos, seguida da cognição do juiz.
Esta última seção é a que apresenta o conteúdo textual de interesse do usuário que realiza a pesquisa;
Os sistemas de pesquisa oferecidos por os tribunais, não realizam a busca do argumento de pesquisa no inteiro teor do documento, limitando seu escopo à ementa.
Infelizmente, é notório no meio jurídico que, freqüentemente, a precisão da classificação encontrada nas ementas é deficitária.
Por vezes estão incompletos os termos descritores constantes da ementa.
Outras vezes encontram- se, ali, termos descritores referentes a assuntos que não são objeto do debate registrado no inteiro teor do documento;
Argumento de Pesquisa: Os sistemas de pesquisa oferecidos por os tribunais recuperam documentos que contenham as palavras digitadas no argumento de pesquisa operadores agreguem o benefício de permitir pesquisas mais específicas, muitos usuários não conseguem assimilar sua lógica e, por sentirem- se desconfortáveis com tal interface, não se beneficiam dos recursos disponíveis.
Além de isto, tal sistemática não propicia a possibilidade de encontrar- se documentos que não contenham algum dos argumentos de pesquisa mas que versem sobre assunto semelhante a documentos que contenham tal argumento.
Analogamente, esta sistemática pode recuperar documentos que contenham o argumento de pesquisa, mas cujo tema seja diverso daquele buscado por o usuário.
Tendo em vista a implantação do processo eletrônico desde janeiro de 2010, pretendese utilizar os documentos anexados ao processo por as partes como argumento de pesquisa.
Ou seja, submete- se o documento ao classificador treinado com a jurisprudência e recuperase os documentos que compõem o conjunto de treino da respectiva classe.
Além de isto, o escopo da pesquisa não será limitado às ementas, mas ampliado, abrangendo o inteiro teor do documento.
A recuperação de documentos utilizando solução baseada em aprendizado de máquina aqui descrita, permitirá:
Recuperar documentos que versem sobre o tema pesquisado, ainda que não contenham as palavras constantes do argumento de pesquisa;
Considerando as deficiências da classificação ementária, bem como a grande quantidade de documentos que compõem a jurisprudência de cada corte e, por sua vez, a grande quantidade de cortes em nosso país, não há como prover um conjunto de documentos devidamente rotulados para treinar um classificador.
Optamos, então, a exemplo dos trabalhos de Feinerer e Hornik e Aggarwal, Gates e Yu, revisados no Capítulo 3, por experimentar um processo de agrupamento de documentos para prover as classes a serem utilizadas por o categorizador.
No entanto, em nossa revisão bibliográfica, vimos que algoritmos de agrupamento clássicos, como o K--Means, necessitam pré-configuração da quantidade de grupos a serem gerados.
Uma vez que não se conhecem, a priori, nem os temas debatidos nem a sua quantidade, buscamos implementar um processo de geração de classes através de agrupamento de documentos de jurisprudência para treinar um categorizador, atendendo aos seguintes quesitos:
Reduzir os problemas advindos dos erros de classificação encontrados nas ementas dos documentos que compõem a jurisprudência;
Descobrir as classes a serem utilizadas por o categorizador sem exigir que se configure previamente a sua quantidade.
Aporte Teórico Utilizado O algoritmo proposto por Aggarwal, Gates e Yu revisado na Seção 3.4, pressupõe que, dado um corpus composto integralmente de documentos previamente classificados, seja possível, partindo desta classificação inicial, obter automaticamente um novo conjunto de classes que, sob julgamento humano, seja qualitativamente equivalente ou superior à taxonomia original.
Tendo em vista as deficiências da classificação ementária, o problema do agrupamento e classificação de documentos jurídicos a ser tratado neste estudo apresenta características muito semelhantes àquelas tratadas no experimento de Aggarwal, Gates e Yu, onde os documentos estão previamente classificados, mas acredita- se que o conjunto de atributos alvo possa ser melhorado.
No entanto, há que se notar uma diferença, posto que ignorar tal divergência implica em resultados cujo impacto pode ser determinante para o fracasso deste estudo:
O algoritmo de agrupamento daqueles autores realiza descartes, a título de ruído, de documentos, no passo de atribuição de documentos, e de grupos, no passo eliminação de grupos.
Considere- se, hipoteticamente, a situação de um réu preso sendo que a única forma de convencer um juiz a soltar- lo é a argumentação de outro juiz libertando um outro réu numa situação equivalente.
Considere- se, também, a hipótese de que não haja nenhum outro caso semelhante a este.
Se tal documento único que faça a diferença entre manter- se preso ou libertar o réu for descartado como ruído, uma vida será, definitivamente, arruinada.
Por estas razões, optamos por adotar o algoritmo de Aggarwal, Gates e Yu, propondo sobre o mesmo algumas evoluções, tendo em vista seu uso em corpus jurídico, a saber:
Eliminar o descarte de documentos e o descarte de clusters;
Incluir uma operação de divisão de clusters;
Testar variações no limiar para união de clusters.
Note- se que este algoritmo adota o pressuposto de relacionamento um-para-um entre os grupos gerados e as classes utilizadas para treinar classificador.
Embora Cong, Lee, Wu e Liu questionem este pressuposto e apresentem algoritmo para solucionar este problema, tem como requisito o uso de um conjunto de dados rotulados que guiem o particionamento dos dados não rotulados para garantir que, em cada partição, haja um mapeamento um-para-um entre grupos e classes.
Apesar de os dados que dispomos estarem rotulados, questiona- se a qualidade desses rótulos e em nossa proposta buscamos melhorar este conjunto de rótulos.
Assim, não podemos aplicar o algoritmo de Cong, Lee, Wu e Liu pois este se baseia numa confiança, que não dispomos, nos rótulos de classe pré-existentes.
Visão Geral da Solução Adotada Conforme ilustrado na Figura 4.2, neste estudo propomos experimentar processo composto de duas fases, &quot;A «e &quot;B», em o qual:
Submetem- se documentos obtidos do corpus de jurisprudência J, a um processo de agrupamento, gerando grupos, Si, que determinam as classes a serem usadas por o categorizador;
Novos documentos pj são submetidos ao categorizador, que utiliza as classes geradas por o agrupamento, Si, para classificar- los.
Embora omitido na Figura 4.2 visando a sua clareza, uma vez determinada a classe do novo documento, recuperam- se os documentos constantes da jurisprudência que compõem o grupo correspondente a esta classe.
Os códigos identificadores dos documentos de jurisprudência jk Ci que compõem o grupo correspondente à classe obtida são registrados no Processo Eletrônico pj, permitindo que o usuário consulte o inteiro teor da respectiva jurisprudência jk.
Após o julgamento do processo jurídico, o magistrado produz um novo documento, contendo a decisão judicial e o integra à jurisprudência.
Por esta razão, a Fase &quot;A «deve ser executada novamente.
A periodicidade em que esta fase deva ser reexecutada é um ajuste que os administradores deste sistema poderão adequar às suas necessidades específicas1 e não faz parte do escopo deste estudo.
Detalhamento da Solução Adotada na Figura 4.3 detalhamos os processos de agrupamento para geração de classes, Fase &quot;A», descrito na Seção 4.5.1, e classificação de novos documentos, Fase &quot;B», descrito na Seção 4.5.2.
Em a Fase &quot;A», temos:
Pré-Processamento: Obtêm- se os documentos ji com decisões em processos judiciais, do corpus de jurisprudência J e realiza- se o pré-processamento, conforme descrito na Seção 4.4.2, onde, de cada documento ji obtem- se um vetor de atributos ji $= e ain é o enésimo atributo extraído de ji.
Obtem- se, assim, um conjunto 1 Diariamente, semanalmente, etc..
Detecção de Classes:
Seleciona entre os atributos de ji, como rótulo de classe Ca0 inicial, o primeiro atributo a0 obtido das respectivas ementas.
Considera- se cada classe como grupo inicial Sa0;
Redução de atributos:
São descartados de J, os atributos ai que tenham os maiores Índices Normalizados Gini, ou que ocorram em apenas um documento;
De os atributos ai restantes presentes nos documentos ji J calculam- se os centróides Si;
Agrupamento: Agrupam- se os documentos de J, usando como sementes iniciais dos grupos os centróides Si, conforme descrito na Seção 4.5.1, alterando a relação documento/ grupo, ou gerando novos grupos, produzindo novo conjunto de centróides Si.
Em a Fase &quot;B», temos:
Pré-Processamento: Obtêm- se os documentos pj juntados aos Processos Eletrônicos P e realiza- se o pré-processamento, conforme descrito na Seção 4.4.2 onde, de cada Classificação:
Os vetores de atributos pj são classificados numa das classes Ci definidas por os grupos Si gerados na Fase &quot;A», utilizando algoritmo descrito na Seção Processo Eletrônico pj, permitindo que o usuário consulte o inteiro teor da respectiva jurisprudência jk.
À medida que os magistrados julgarem os processos, novos documentos pl serão incluídos no Corpus de Jurisprudência e, conseqüentemente, serão necessárias, de tempos em tempos, novas execuções do agrupamento, fase &quot;A».
Para implementar nossos exemplos de uso, foi montado corpus composto de documentos obtidos através do site do Tribunal Regional Federal da 4a Região, conforme descrito na Subseção 4.4.1.
O pré-processamento destes documentos consiste em:
Extração de palavras e referências legislativas usando Parser desenvolvido conforme descrito na Seção 4.4.3;
2) lematização usando lematizador híbrido probabilístico e baseado em regras;
E 3) identificação de termos usando tesauros jurídicos, conforme a Seção 4.4.2.2.
Com redução da dimensionalidade dos atributos, foram eliminados aqueles com maior Índice Normalizado Gini ou que ocorriam em, apenas, um documento, vide Seção 3.4.
Em as subseções a seguir, apresentamos detalhamento da implementação da solução proposta.
Composição do Corpus O corpus foi construído com documentos de jurisprudência do Tribunal Regional Federal da 4a Região através do seguinte caminho de hyperlinks:
&quot;Jurisprudência «&quot;TRU4 e Turmas Recursais «Consulta Jurisprudência da TRU4 e Turmas Recursais», que leva desta página foi preenchido marcando- se os campos &quot;Acórdãos», &quot;Súmulas «e &quot;Decisões Monocráticas a partir de 08/2006 «e selecionando- se o período de 9 de janeiro de 2.006 a 27 de maio de 2.009.
De esta maneira, foram selecionados todos os documentos proferidos por estas turmas no período.
Isto resultou num conjunto composto de 43.806 documentos.
Alguns documentos referiam- se a processos protegidos por o sigilo judicial e continham, apenas, uma mensagem informando a existência desta proteção.
Após eliminar- los, restaram 43.704 documentos.
No entanto, Aggarwal, Gates e Yu propuseram em algoritmo de Hard Clustering como forma de definir novas classes para posterior categorização de documentos.
Considerando- se que algoritmos deste tipo assumem o pressuposto de que cada documento trata de um único tema e que é comum que os litígios judiciais abordem múltiplos temas igualmente relevantes, considerou- se a necessidade de, neste estudo, descartar do corpus os documentos que versassem acerca de múltiplos temas, a fim de evitar que a presença de termos característicos de temas distintos num único documento torne- se fonte geradora de erros de agrupamento/ classificação.
De esta maneira, considerando- se que:
A classificação expressa na ementa dos documentos segue a terminologia padronizada no Tesauro da Justiça Federal2, mantido por o Conselho da Justiça Federal3;
A o redigir a ementa, o judiciário transcreve toda a hierarquia dos termos, desde a raiz até o termo mais específico, realizando um caminhamento em profundidade sobre a árvore do tesauro;
Tal redação apresenta uma sintaxe regular, separando os termos por um ponto`.'
ou um hífen`';
Pré-Processamento de Documentos Estruturas Terminológicas Para poder identificar a ocorrência de termos jurídicos nos documentos, foram utilizados dois tesauros especializados no domínio:
O Vocabulário Controlado Básico, VCB;
E o Tesauro da Justiça Federal, TJF, também conhecido como Vocabulário Controlado da Justiça, VCJ.
O Senado Federal Brasileiro4 é o mantenedor do VCB, que abrange vários domínios do conhecimento, com foco no domínio do Direito, que representa 3.400 termos.
Ele está estruturado como um tesauro.
Assim, há indicações de equivalência de termos.
Sendo, portanto, possível verificar que &quot;crime por computador «é semanticamente equivalente a &quot;crime de informática «e o mesmo se aplica a &quot;Uniformização de jurisprudência e «&quot;Súmula vinculante».
Este tesauro está disponível em formato PDF e, para que um programa possa utilizar seus dados, foi preciso converter- lo para formato textual e extrair as informações com um parser.
Apesar de, originalmente, tratar- se de texto livre, obedece a uma estrutura sintática bastante regular, com algumas irregularidades quando um campo usa mais de uma linha.
Em esse caso, foi necessário juntar manualmente as linhas para que cada campo estivesse inteiramente definido numa única linha.
Em a Tabela 4.1, pode- se ver um excerto do conteúdo do Tesauro.
As linhas 1, 12, 14 e 16 iniciam a definição de um termo.
Em as linhas 2, 3 e 4, após as palavras &quot;NÃO USE «encontram- se definições não oficiais equivalentes do termo.
Em a linha 5, após o &quot;TG «(termo geral), encontra- se o hiperônimo do termo.
Em a linha 6 encontra- se um hipônimo, logo após o &quot;Te «(termo específico).
Em as linhas 7, 8, 9 e 10, após o &quot;TR «encontram- se os termos relacionados, mas não equivalentes.
Em as linhas 13, 15 e 17, após o &quot;Use «encontram- se os termos equivalentes (e oficiais).
Finalmente, na linha 11, após o &quot;CDD», encontra- se o código de classificação das bibliotecas.
Apesar de possuir esta estrutura hierárquica, está longe do Tesauro ser uma única grande árvore.
De acordo com Jaegger, está, na verdade, muito fragmentado, apresentando uma grande quantidade de sub-árvores desconectadas.
Assim, esta estrutura hierárquica do Tesauro foi ignorada neste estudo.
Focamos nas relações de equivalência, com vistas a a normalização dos termos buscando reduzir a dimensionalidade dos atributos.
As relações &quot;TR «são &quot;dicas «para a existência de alguma forma de relação entre os termos, mas não há informações mais detalhadas a respeito de a natureza de tais relações.
Assim, também foram ignoradas.
O TJF5, assim como o VCB, está no formato PDF e também foi preciso converter- lo para formato textual e extrair as informações com um parser.
A sintaxe utilizada é muito semelhante à do VCB.
Assim, &quot;TG «indica um hiperônimo e &quot;Te «indica um hipônimo.
Figura 4.4 ­ Exemplo de Estrutura de grafo presente no Tesauro da Justiça Federal o termo &quot;delito», são alcançáveis por dois caminhos distintos.
A Figura 4.4 mostra mais claramente estas relações.
Em o presente estudo, são utilizadas, apenas, as indicações de equivalência de termos.
Como apresentado na Tabela 4.1, o VCB utiliza a expressão &quot;NÃO USE «para indicar termos equivalentes ao constante da linha 1.
De a mesma maneira, na Tabela 4.3, vemos que o TJF utiliza a expressão «UP' para indicar a equivalência dos termos.
A expressão regular, no padrão POSIX, «(Não Use| UP)(+) «é utilizada para detectar estas relações de equivalência em ambos os vocabulários.
Para minimizar o ruído nos dados nos processos de clustering e categorização, busca- se identificar diferentes formas de expressão, gerando atributos únicos, bem como identificar ambigüidades no texto, gerando atributos distintos.
Para tanto, vários métodos de normalização têm sido utilizados na fase de pré-processamento de textos.
Optou- se por a lematização, evitando ambigüidades introduzidas por o stemming, conforme verificado por Korenius Em experimentos com textos finlandeses, e Gonzalez, com a língua portuguesa.
Assim, foi construída uma base lexical para auxiliar a lematização.
Foram importados e mesclados 3 dicionários:
O Dicionário de Português Brasileiro Unitex (Unitex-PB6) organizado por Muniz e as versões portuguesa7 e latina8 do Wiktionary, um projeto da Wikimedia Foundation9.
Decidiu- se por a importação deste último, uma vez que muitos termos jurídicos estão em latim10.
Após importar os dicionários, e montar uma estrutura unificada, detectou- se a ausência de muitas das palavras que ocorriam nos Tesauros e nos documentos do corpus.
Assim, foi necessário importar estas novas palavras para o dicionário unificado.
Desenvolveu- se, então, um parser e um lematizador, descritos na Seção 4.4.3, para lematizar os termos dos tesauros.
As seqüências de lemmata obtidas dos tesauros foram armazenadas na base lexical.
Termos equivalentes, como &quot;crime por computador «e &quot;crime de informática», receberam mesma identificação.
Arquitetura do Pré-Processamento A Figura 4.7 mostra a arquitetura do pré-processamento proposto.
Os documentos do corpus construído estão em formato Html e constituem a entrada para o pré-processamento.
Possuem a estrutura geral apresentada na Figura 4.6, onde se vêem metadados como, por exemplo, o número do processo e a ementa, de especial interesse e cujo caput apresenta uma classificação composta de termos padronizados constantes do Tesauro da Justiça Federal, dispostos em seqüência consoante a hierarquia do referido tesauro.
O conteúdo textual a ser processado encontra- se entre tags &quot;DIV «cujas classes CSS são &quot;caputEmenta», &quot;ementa», &quot;paragrafoNormal «e &quot;citacao», e é extraído aplicando expressões regulares.
Para construir os vetores de atributos, que são compostos de termos dos documentos, foram eliminados metadados e extraídos parágrafos dos documentos em Html, usando expressões regulares que buscaram as tags com classes CSS indicando seu conteúdo.
Isto resultou num array de parágrafos com texto puro, exceto por a presença de tags &quot;B», &quot;I», &quot;U «e &quot;DD».
As três primeiras são indicadores de negrito, itálico e sublinhado;
A última é indicadora de itemização.
Embora não sejam utilizadas neste estudo, sendo descartadas após a extração de termos, optou- se por não excluir- las para que, em futuros trabalhos, sejam mais uma alternativa à disposição para o pré-processamento.
As tags de negrito e sublinhado indicam que o autor do documento confere maior relevância ao trecho em destaque.
Isto poderia ser levado em consideração para dar maior peso aos atributos gerados.
O itálico poderia auxiliar o tagger a, por exemplo, delimitar um termo.
&quot;artigo 34 do decreto-lei n 8192 de fevereiro de 1972 «&quot;decreto-lei n 8192 de fevereiro de 72, por meio de os artigos 34, 35 e 36 «&quot;dl 8192/1972», &quot;dl 8192/1972 art. «dl 8192/1972 art. «dl 8192/1972 art..
O parser desenvolvido extrai tokens usando expressões regulares que reconhecem palavras, números, pontuação, URLs, e-mails, datas, números de processo e referências legislativas.
A Tabela 4.4 apresenta exemplos de normalização de referências legislativas produzidas, mostrando que referências a um ou mais artigos de uma norma, resultam num token para a norma, além de um token para cada par norma-artigo.
O processo visa aumentar mais a similaridade entre os documentos que abordem o mesmo par artigo-norma e aumentar um pouco a similaridade aqueles que referenciarem artigos diferentes de uma mesma norma.
Evita, ainda, aumento de similaridade indesejado entre documentos que façam referência a artigos de mesmo número em normas distintas.
Lematizador O lematizador pesquisa na base lexical os tokens identificados como palavras, recuperando os respectivos lemmata.
Os tokens não encontrados na base são descartados.
Havendo mais de um lemma relacionado a um token, faz- se a desambiguação lexical.
As únicas alternativas de desambiguação são os lemmata obtidos da base lexical11.
O desambiguador itera entre dois desambiguadores não gulosos.
Um baseado em regras e o outro probabilístico.
Ambos desambiguadores podem decidir 1) por a desambiguação, 2) eliminar uma das ambigüidades ou 3) não realizar nenhuma operação.
A eliminação de uma das alternativas, proporciona ao próximo desambiguador melhores condições de decisão.
Em a primeira iteração, o desambiguador probabilístico assume comportamento não guloso, desambiguando somente ante grandes diferenças de probabilidades entre as alternativas de desambiguação.
Em a segunda invocação, o desambiguador probabilístico se torna guloso, selecionando a alternativa de maior probabilidade.
Para o desambiguador baseado em regras importou- se 69 regras do ELAG (Elimination of Lexical Ambiguities by Grammars) providas por Muniz12.
Foram acrescentadas mais 271 novas regras inspiradas nas regras gramaticais apresentadas por Ricardo Sérgio 11 Por exemplo:
O token &quot;par», em função de o sufixo &quot;ar», poderia ser etiquetado como verbo por um desambiguador guiado por o sufixo.
No entanto, não há um lemma etiquetado como verbo para este token na base lexical.
O desambiguador probabilístico usa tabela de probabilidades do tagger denominado Forma de Gonzalez, que decide com base em sufixos do token.
Em este estudo, seu algoritmo sofreu duas modificações:
São considerados apenas os lemmata presentes na base lexical, e, 2) na primeira iteração, assume um comportamento não guloso.
Reconhecimento de Termos Após a lematização, identificam- se os termos dos tesauros.
A Figura 4.8 apresenta um modelo de vetor de atributos, gerado ao fim do pré-processamento, resultante do seguinte texto:»
1. Trata- se de Ação Ordinária proposta por a empresa Porto de Cima Rádio e Televisão Ltda, objetivando a desclassificação das empresas Rádio e Televisão Rotioner Ltda (ROTIONER) e Rádio e Televisão Canal 29 do Paraná Ltda (SESAL) da Concorrência Pública no 150j97-SSRjMC, Descarte de atributos Além de isto, implementou- se, também, uma variação no pré-processamento dos documentos.
A seleção de atributos empregada por os autores baseou- se no índice Gini, descartando as palavras que tinham distribuição muito homogênea entre as classes e, também, descartando palavras que ocorriam em poucos documentos.
Em este estudo avaliou- se, alternativamente, uma combinação da solução dos autores com a proposta de Feinerer e Hornik, que apresenta o Keyword Based Clustering.
No entanto, em, definem- se manualmente os termos que compõem os centróides dos clusters.
Em nosso exemplo de uso, a alternativa de pré-processamento que se propõe é a mudança da representação dos documentos no modelo de espaço vetorial de bag of referências legislativas com a distribuição mais desigual ou que tenham ocorrido em apenas um documento.
Para tanto, foram utilizados tanto os termos do TJF quanto do VCB.
Note- se que o documento com a decisão judicial, embora seja redigido por um juiz, contém citações de textos produzidos por as partes do processo, que podem adotar o VCB.
O Ministério Público Federal, por exemplo, adota, oficialmente, o VCB.
Considerando- se que não é incomum que o juiz transcreva diversos parágrafos da argumentação do Ministério Público, acrescentando, ao final, frases como &quot;é a minha decisão «ou &quot;decido de acordo com o Ministério Público», percebe- se que, nestes casos, os termos relevantes para o pré-processamento serão, necessariamente, oriundos do VCB.
Processo de Agrupamento e Classificação Agrupamento Conforme exposto no caput deste capítulo, o problema de agrupamento e classificação de documentos jurídicos a ser tratado neste estudo apresenta características muito semelhantes àquelas tratadas no experimento de Aggarwal, Gates e Yu, excetuando- se, no entanto, o descarte de documentos e grupos, já ressalvado, também, no referido caput.
O critério de parada do processo de clustering estabelecido por Aggarwal, Gates e Yu baseou- se na redução de atributos.
Quando a quantidade de atributos era inferior a 200 encerrava- se o clustering.
Em o estudo por nós desenvolvido, os vetores de atributos possuem dimensionalidade bem inferior ao dos referidos autores em função de cada atributo representar termos jurídicos ou referências legislativas e não as palavras dos documentos.
Por esta razão, após alguns testes, decidiu- se estabelecer o limite mínimo de 20 atributos como critério de parada.
Em seu artigo, Aggarwal, Gates e Yu, abordam a questão da divisão de clusters em grupos menores.
Informam que o algoritmo não suporta tal operação por crerem que esta seria não-supervisionada e que isto poderia gerar incoerências com os rótulos de classe original.
Em este estudo optou- se por implementar esta operação e analisar a validade de tal ponderação.
Foram definidas e experimentadas duas alternativas de algoritmos para implementar esta operação, apresentados nas Subsubseções a seguir.
Algoritmo de Divisão Este algoritmo acrescenta um passo de divisão na iteração principal do algoritmo de Aggarwal, Gates e Yu, em o qual os clusters podem ser divididos em grupos menores.
Para tanto, definiu- se que seriam selecionados para divisão os clusters que apresentem muita variação de similaridade entre seus respectivos centróides e documentos, baseado na hipótese de que esta seja uma característica encontrada em grupos que contenham subgrupos razoavelmente bem separados.
Assim, sejam intra-cluster e Cn, n o n-ésimo cluster e o desvio-padrão de suas similaridades internas.
Divide- se todo Cn se n\&gt; algoritmo proposto por Aggarwal, Gates e Yu, em o qual se realiza um processo de subclustering aglomerativo, definido abaixo e detalhado no Algoritmo 1.
O cluster é fracionado em subclusters contendo um único documento.
Faz- se uma redução de atributos14 usando o índice Gini;
Realiza- se uma iteração semelhante à iteração principal, porém sem o passo de divisão de clusters.
Os dois primeiros passos não são executados na primeira iteração porque cada cluster contém um único documento.
O conjunto de clusters resultante é retornado para o algoritmo principal, substituindo o cluster de onde se originaram.
Algoritmo de Divisão Implícita Conforme descrito no Capítulo 5, a inclusão do passo de divisão detalhado no Algoritmo 1 teve um impacto significativo na velocidade do processamento.
Por esta razão, buscou- se uma alternativa que viabilizasse a divisão dos clusters sem causar tão grande impacto no custo do processamento.
Inspirando- se no algoritmo TOD apud, foi alterado o passo de atribuição de documentos a clusters proposto no algoritmo original de Aggarwal, Gates e Yu, substituindo- se o descarte de documentos por a criação de um novo cluster contendo unicamente o documento outrora selecionado para descarte, conforme detalhado no Algoritmo 2.
Embora o novo cluster seja composto de apenas um documento, novas iterações poderão atrair documentos dos grupos mais próximos.
Conforme ilustrado na Figura 4.9, 1) o documento &quot;A «está além de o limiar de similaridade do cluster e, assim, 2) cria- se um novo cluster 14 Esta redução é utilizada exclusivamente no escopo do processamento do subclustering, após a finalização deste passo de divisão.
Categorização Novos documentos a classificar são submetidos ao mesmo pré-processamento para obtenção de vetores de atributos usados no clustering.
No entanto, não será feito novo cálculo do índice normalizado Gini, serão descartados os mesmos atributos descartados na fase de pré-processamento dos documentos a agrupar, garantindo que os vetores sejam compostos por os mesmos atributos.
A categorização utiliza classes obtidas assumindo o pressuposto de relação um-para-um com os grupos obtidos na fase de clustering e foi determinada usando o mesmo processo de determinação da classe de maior dominância proposto por Aggarwal, Gates e Yu, conforme descrito na Seção 3.4.
Este processo baseia- se no cálculo de proximidade do documento a classificar em relação a cada um dos centróides dos grupos correspondentes às classes, usando a mesma função de similaridade de cosseno usada na fase de agrupamento.
No entanto, se na fase de agrupamento há um limiar mínimo de similaridade para atribuição de um documento ao grupo15, na fase de categorização não há limite mínimo de similaridade para a classificação.
Há, porém, mais um procedimento, inexistente na fase de agrupamento, onde se determina a dominância de uma classe sobre o documento a ser categorizado, descrita na Seção 3.4, em o qual, quando um documento se encontra numa região limítrofe entre k classes, se faz novo cálculo de proximidade em relação a os respectivos centróides desconsiderando- se, agora, os atributos que sejam não nulos nos centróides envolvidos.
Considerações Finais Foi apresentada aqui a arquitetura de nossa proposta, vide Seções.
Detalhando, então os passos empreendidos em sua implementação.
Foram, então, executados exemplos de uso de nossa reimplementação do algoritmo original de Aggarwal, Gates e Yu, e das variações propostas para evolução deste algoritmo, utilizando, para tanto, o corpus jurídico organizado conforme descrito na Seção 4.4.1, a fim de descobrir se o uso de algoritmos de aprendizado de máquina podem ser utilizados satisfatoriamente para acelerar o processo de pesquisa de jurisprudência.
Em o próximo capítulo, apresentamos relato da avaliação e análise dos resultados obtidos ao executarmos nossos exemplos de uso.
Foi apresentada aqui a arquitetura de nossa proposta, vide Seções 4.2, 4.3 e 4.4.
Detalhando, então, os passos empreendidos em sua implementação, apresen- tamos, na Seção 4.4.2.2, construímos nosso dicionário mediante importação de dois dicionários em língua portuguesa e um em latim.
Desenvolvemos um parser que reconhece os tokens constantes deste dicionário e referências legislativas, apresentado na Seção 4.4.3, e um lematizador híbrido, que alterna a aplicação de regras gramaticais e cálculo de probabilidade, visto na Seção 4.4.3.2.
Desenvolvemos, também, um extrator de atributos que reconhece os termos jurídicos dos vocabulários controlados e as referências legislativas, apresentado na Seção 4.4.3.3.
E completamos o pré-processamento realizando o descarte de atributos baseado no Índice Normalizado Gini, além de atributos pouco freqüentes, conforme exposto na Seção 4.4.3.4.
Construímos um corpus com jurisprudência baixada do 15 Exceto nas variantes do algoritmo em que não há descarte de documento e não se realiza a divisão implícita de grupos.
Tribunal Regional Federal da 4a Região, conforme descrito na Seção 4.4.1 que, após o préprocessamento, proveu os vetores de atributos para executar os exemplos de uso de nossa reimplementação do algoritmo original de Aggarwal, Gates e Yu, e das variações propostas para evolução deste algoritmo, descritas na Seção 4.5.1, a fim de descobrir se o uso de algoritmos de aprendizado de máquina podem ser utilizados satisfatoriamente para acelerar o processo de pesquisa de jurisprudência, e, em especial, atendendo os quesitos de não realizar descarte de documentos ou grupos e implementando duas alternativas de operação de divisão de grupos, descritas nas Seções 4.5.1.1 e 4.5.1.2.&amp;&amp;&amp; Por fim, categorizamos documentos usando classes obtidas assumindo o pressuposto de uma relação um-para-um com os grupos gerados por o algoritmo evoluído, conforme exposto na Seção usando linguagem PHP16.
Em o próximo capítulo, apresentamos relato da avaliação e análise dos resultados obtidos ao executarmos nossos exemplos de uso.
Considerações Iniciais Para averiguar a efetividade desta proposta junto ao exemplo de uso, avaliando, assim, a aplicabilidade e, portanto, perspectivas de sua implantação em ambiente real, realizaramse experimentações, detalhadas na Seção 5.2.
A Seção 5.3 descreve a avaliação dos resultados obtidos, que foi dividida em dois momentos:
Avaliação do agrupamento, descrita na Seção 5.3.1 e avaliação da categorização, descrita na Seção 5.3.2.
As Seções 2.7 e 2.6 apresentam uma rápida revisão de métodos comumentemente utilizados para avaliação de agrupamentos e de classificação, respectivamente.
Parâmetros Adotados na Validação Dividimos os documentos do corpus em 3 conjuntos:
Treino: 716 documentos, para realizar o agrupamento;
Teste: 238 documentos, para a primeira classificação;
Operação: 238 documentos, para a classificação final;
O procedimento adotado, como critério de divisão, consistiu em selecionar, seqüencialmente, 3 documentos para o conjunto de treino, 1 para o conjunto de teste e um para o conjunto de operação, reiniciando o processo até que se esgotassem os documentos.
De esta maneira, a divisão dos codcumentos ficou ligada à ordem em que os documentos ingressaram no corpus.
Essa, por sua vez, seguiu a ordem em que foram realizados os downloads dos documentos.
Conforme descrito na Seção 4.4.1, foram buscados os documentos através de pesquisa por data no site do Tribunal Regional Federal da 4a Região, compreendendo o período de 9 de janeiro de 2.006 a 27 de maio de 2.009.
Foram, inicialmente, baixados em ordem cronológica crescente, os documentos do ano de 2.009.
Em seguida os documentos do ano de 2.008.
Após, os de 2.007 e, finalmente, os de 2.006.
Foram selecionados os 716 documentos do conjunto de treino para agrupar e gerar as classes.
Foram extraídos, ao todo, 1.255.266 tokens destes documentos.
Cada documento apresenta uma média de 1.753,16 tokens.
Após a extração dos atributos, obteve- se, por documento, uma média de 138,54 atributos.
O parsing e a desambigüação levaram em torno de 1h30 min e a detecção de atributos consumiu em torno de 15 minutos.
Assim, o tempo médio de pré-processamento é de menos de 9s por documento.
Desabilitados todos os descartes e habilitado o passo de divisão Desabilitados todos os descartes e habilitada a divisão implícita Obtidos os atributos, foram determinados os grupos/ classes iniciais.
Para tanto, o primeiro atributo obtido da ementa de cada documento foi usado como rótulo de classe.
Em o Apêndice B, vemos a Tabela B. 2 que apresenta um resumo dos grupos iniciais obtidos e a quantidade de documentos associada a cada um.
Descartaram- se, então, os atributos via Índice Normalizado Gini, listados na Tabela D. 1, no Apêndice D. Decidiu-se por o descarte dos 50 atributos com o maior Índice Normalizado Gini.
Não foi possível descartar mais atributos devido a alguns documentos e grupos ficarem com poucos atributos.
Foram descartados, também, todos os atributos que ocorriam somente num documento.
A Tabela B. 1, encontrada no Apêndice B, apresenta as dimensionalidades iniciais dos grupos.
Para melhor observar o efeito das alterações propostas, executamos o algoritmo de agrupamento várias vezes, ativando, seletivamente, cada alteração proposta e, posteriormente, ativando- as em conjunto, conforme indicado na Tabela 5.1.
O limiar de similaridade utilizado foi de 50%.
O limiar de descarte de grupos foi de 4 documentos.
Limiares de similaridade e descarte superiores a estes resultavam em descarte de todos os documentos no algoritmo original de Aggarwal, Gates e Yu, pois a exigência de maior similaridade aumentava o descarte de documentos e diminuia a quantidade de documentos no grupos, fazendo com que os grupos atingissem o limiar de descarte e fossem, tambem, descartados.
As iterações iniciaram com, no máximo, 200 atributos nos centróides e encerraram- se com, no mínimo, 24 atributos.
Cada algoritmo de agrupamento levou entre 30 min e 1h30 min de execução, exceto por o algoritmo que implementou o passo de divisão de grupos, que levou em torno de 3h30 min para executar.
O algoritmo de categorização classificou, em média, um documento a cada 2,02s. Depois de executados os agrupamentos, foram calculados dois índices internos de qualidade dos agrupamentos de cada um dos conjuntos de grupos gerados, detalhados na Seção 5.3.1.
Selecionou- se, então, o conjunto gerado por o algoritmo evoluído sem descartes de documentos ou grupos e com divisão implícita de grupos, por apresentar a melhor performance média dos índices internos para prover as classes utilizadas em todos exemplos de uso de classificação, descritos na Seção 5.3.2.
Os 238 documentos do conjunto de teste foram categorizados em 161 das 465 classes correspondentes ao grupos obtidos no agrupamento realizado através do algoritmo evoluído selecionado.
Os resultados da categorização foram submetidos a validação por especialista humano.
Após a validação por especialista humano, conforme descrito na Seção 5.3.2, analisaram- se os resultados obtidos, verificando que obteve- se uma precisão de, aproximadamente, 57%.
Analisou- se, também, a relação entre os verdadeiros/ falsos positivos e diversos parâmetros, tais como quantidade de documentos no grupo, quantidade de atributos no centróide e no documento categorizado, quantidade de palavras nos atributos originados de termos jurídicos, etc..
De esta análise não se identificou qualquer relação entre estes parâmetros e o sucesso/ insucesso na categorização.
Por esta razão, suspeitando de que tal relação não tivesse raízes nestes parâmetros, procedeu- se a uma análise mais detalhada, nas Seções 5.4.1 e 5.5, dos casos extremos:
Os falsos positivos categorizados com alta similaridade e os verdadeiros positivos com baixa similaridade.
Em esta análise, percebeu- se que, em muitos centróides os atributos de maior peso tinham semântica muito genérica e, assim, formulou- se a hipótese de que poderia- se- minimizar este problema dando pesos proporcionais à semântica dos atributos, conforme detalhado na Seção 5.6.
Também percebeu- se que o passo de projeção, onde se faz o recálculo dos centróides, não reconhecia a presença de novos atributos não nulos decorrentes da inclusão de novos documentos.
Procedeu- se à implementação de novo exemplo de uso, retornando ao ponto da detecção dos atributos nos documentos.
Desta vez, atribuiu- se pesos proporcionais à especificidade dos atributos.
A informação do grau de especificidade dos termos foi obtida a partir de os tesauros, e as referências legislativas receberam pesos arbitrados, conforme critérios detalhados na Seção 5.7.
Os demais procedimentos de pré-processamento seguiram o mesmo rito, descartando- se os 50 atributos com o maior Índice Normalizado Gini e os atributos que ocorriam em somente um documento.
Repetiu- se a execução dos algoritmos de agrupamento, descrito na Seção 5.8, conforme o algoritmo original e as cinco variações do algoritmo evoluído.
O passo de projeção foi alterado, permitindo que novos atributos não nulos ingressem no centróide em decorrência da inclusão dos atributos dos novos documentos no centróide.
Em o Apêndice C, vemos a Tabela C. 2 que apresenta um resumo dos grupos finais obtidos e a quantidade de documentos associada a cada um.
Foi realizado novo cálculo dos índices de qualidade dos agrupamentos e, desta vez, o algoritmo que descarta documentos e não descarta grupos apresentou a melhor performance média.
No entanto sua performance média superou a performance média do algoritmo de divisão implícita em, apenas, 2% e, sendo tão pequena a diferença e por não realizar descartes, preferimos selecionar o conjunto de grupos gerado por este último para prover as classes usadas na fase de categorização.
Não foi possível utilizar os 238 documentos do conjunto de operação devido a a indisponibilidade de tempo para validação por especialista humano.
Em o tempo que dispúnhamos, a única maneira que encontramos de avaliar ao menos 100 categorizações foi o emprego de dois especialistas humanos.
Cada especialista humano avaliou um conjunto de 55 categorizações, composto de um conjunto de 50 categorizações distinto do conjunto recebido por o outro avaliador, e de um conjunto de 5 categorizações iguais às do conjunto de 5 categorizações recebido por o outro avaliador.
Totalizando, assim, 105 categorizações distintas.
Os 105 documentos selecionados aleatoriamente do conjunto de operação, foram categorizados em 74 das 453 classes correspondentes ao grupos obtidos no agrupamento realizado através do algoritmo evoluído selecionado.
Após a validação por os especialistas humanos, conforme descrito na Seção 5.9, analisou- se os resultados obtidos, verificando que obteve- se uma precisão de, aproximadamente, 50,5%.
Também repetiu- se a análise da relação entre os verdadeiros/ falsos positivos e os diversos parâmetros analisados anteriormente.
Desta vez, pôde- se identificar que alguns destes parâmetros mantêm um razoável grau de relação com o sucesso na categorização.
Avaliações Realizadas A avaliação dos exemplos de uso implementados compreendeu duas fases:
Avaliação do agrupamento:
Procedeu- se à análise dos agrupamentos utilizando o algoritmo originalmente proposto por Aggarwal, Gates e Yu e as evoluções aqui propostas, conforme detalhado na Seção 5.3.1;
Avaliação da classificação:
De entre os métodos de agrupamento implementados selecionou- se aquele que apresentou melhor combinação de avaliação por índices internos com velocidade de processamento.
Realizou- se, então, categorização dos documentos do conjunto de teste utilizando as classes obtidas através do agrupamento selecionado e procedeu- se a uma avaliação por especialista humano, detalhada na Seção 5.3.2.
Análise dos Agrupamentos Para comparar os agrupamentos obtidos nas execuções dos vários algoritmos, buscamos medidas de qualidade de agrupamentos que, embora não exigissem validação por especialista humano, face a a indisponibilidade de tempo, oferecessem resultados que se aproximassem daqueles obtidos mediante sua validação.
Ingaramo, Pinto, Rosso e Errecalde realizaram experimento através de o qual, após a geração dos clusters, uti- lizando os corpora CICLing-20021, R82 e os corpora do WSI SemEval apud, compararam as medidas Measure, Measure, Índice Dunn3, Índice Davies--Bouldin e Relative Hardness Measure, buscando detectar quais índices apresentavam resultados semelhantes à avaliação humana por meio de a F-Measure.
Os autores demonstraram que as me¯ didas Measure e Relative Hardness Measure apresentaram resultados muito semelhantes à avaliação humana.
Ressaltaram, porém, que os corpora se caracterizam por conterem textos pequenos e que sua avaliação não deve ser estendida, sem maiores investigações, a contextos diferentes.
Embora os experimentos de Ingaramo, Pinto, Rosso e Errecalde, tratem de documentos pequenos e os documentos de nosso exemplo de uso sejam mais extensos, o pré-processamento utilizado, conforme descrito na Seção 4.4.2, realizou grande redução da dimensionalidade dos atributos, obtendo, por exemplo, um vocabulário médio de 22,65 termos/ referências legislativas por documento, enquanto que no WSI SemEval o vocabulário médio é de 47,65 palavras.
Assim, por apresentarem resultados que se aproximam bastante de resultados obtidos por avaliação humana e por considerar- se que há aplicabilidade destas medidas em nosso contexto, optamos por avaliar os agrupamentos obtidos através do cálculo das medidas Measure e Relative Hardness Measure.
A Tabela 5.2 apresenta um comparativo das medidas aferidas.
As medidas do algoritmo original foram destacadas em azul.
Para cada medida, reportamos o valor absoluto aferido e a sua variação percentual em relação a o algoritmo original.
A seta aponta para cima em caso de melhoria e para baixo em caso contrário.
A melhor performance foi destacada em vermelho e a segunda melhor performance, em negrito.
Os algoritmos 2 e 5 obtiveram a melhor performance numa das medidas.
Mas, obtiveram performances muito baixas em outra das medidas.
A última coluna da Tabela 5.2 apresenta uma média dos percentuais de variação e indica que o algoritmo 6, segunda melhor performance nas duas aferições, teve o melhor desempenho médio.
Além de isto, 1 Composto de 48 resumos dos artigos apresentados na Conferência CICLing 2002.
Para verificar se o resultado da aferição representa melhoria significativa particionamos aleatoriamente o conjunto de treino em 8 conjuntos disjuntos, 4 conjuntos contendo 86 documentos e 4 contendo 85 documentos.
Não foi possível dividir em maior quantidade de conjuntos pois o algoritmo de Aggarwal, Gates e Yu, em sua forma original, realiza grande quantidade de descartes e, por reduzir- se o tamanho do conjunto, a divisão em mais de 8 conjuntos implicou em descarte de 100% dos documentos na maioria dos conjuntos.
Os demais conjuntos terminavam com um único grupo e, assim, também não era possível realizar o cálculo de qualquer medida, pois:
Em o cálculo da medida Relative Hardness, onde n é a quantidade de categorias, temos uma divisão por zero em vista de a expressão n × no denominador e;
Em o cálculo da medida, então simplificamos o cálculo da medida para assim para quaisquer documentos no grupo, independentemente da similaridade entre eles.
Deixa de fazer sentido uma medida de densidade independizada da similaridade entre as instâncias.
Também não foi possível reduzir ainda mais o limiar de similaridade pois o passo de aglomeração do algoritmo acabava por unir todos os grupos num único grupo.
Em a iteração seguinte, com o centróide recalculado, a maioria dos documentos era descartada.
Para alguns dos conjuntos, restava um único grupo ao final da iterações e para outros, após o descarte de documentos, a quantidade de documentos restantes estava abaixo de o limiar de descarte de grupos, implicando no descarte do último grupo.
Alg. Para cada partição executamos novo agrupamento utilizando o algoritmo original de Aggarwal, Gates e Yu[ AGY04] e a evolução proposta por o algoritmo 6, onde não se descartam documentos nem grupos e realiza- se a divisão implícita de grupos.
Estabeleceram- se as hipóteses nulas H0RH:
RH1 $= RH6 e H0¯:
1 $= 6, onde Rh é a medida Relative Hardness,¯ é a Medida Esperada de Densidade e RHi e¯ i são as aferições das respectivas medidas em relação a o i-ésimo algoritmo.
Conforme as Tabelas 5.3 e 5.4, as evoluções que propusemos ao algoritmo de Aggarwal, Gates e Yu, usando a variante que realiza a divisão implícita, superaram o algoritmo original em todas as partições tanto por a medida Relative Hardness quanto por a medida Measure.
Para que se possa considerar que a melhoria de performance seja significativa com 5% de confiança, o teste de sinal apud utilizado exige que, nas 8 medições, o algoritmo proposto obtenha, no mínimo, 7 vitórias.
Análise da Classificação Para validar a classificação dos documentos do conjunto de teste, submetemos os resultados ao exame de especialista humano com experiência em pesquisa e classificação de documentos jurídicos, atuante no Ministério Público Federal.
Para tanto foi desenvolvido um programa apresentado na Figura 5.1, onde o especialista visualiza duas colunas:
A da esquerda apresenta o inteiro teor do documento classificado e a da direita apresenta o inteiro teor dos documentos que compõem o grupo que gerou a classe correspondente.
Durante o processo de agrupamento, foram gerados rótulos para os grupos.
No entanto, estes rótulos foram gerados para depuração durante o desenvolvimento dos programas e não foram apresentados à especialista.
Uma vez que o objetivo é realizar uma pesquisa de documentos utilizando as metodologias de aprendizado de máquina, a avaliação da especialista deve ser focada no resultado final sob o ponto de vista de usuário.
O programa de validação permite, apenas, que a especialista indique se o documento foi bem ou mal classificado.
A especialista foi orientada a considerar que o documento classificado, visualizado na coluna da esquerda, seja um processo jurídico em andamento e que os documentos visualizados na coluna da direita seriam os retornados por um aplicativo de pesquisa de documentos.
Assim, para cada classificação, a especialista foi orientada a «verificar se os resultados da pesquisa continham, absolutamente, toda a informação necessária para que o jurista faça suas referências à jurisprudência quando redigir sua argumentação, dispensando, portanto, a realização de novas pesquisas na jurisprudência, marcando a pesquisa com um sinal de.
Caso contrário, a pesquisa deve ser marcada com um sinal de».
A o final da validação, 136 documentos (57%) foram considerados verdadeiros positivos (Vp) e 100 documentos (42%) foram considerados falsos positivos (FP) por a especialista.
Note- se que, na fase de agrupamento, o limiar de similaridade utilizado para associar um documento a um grupo foi de 0,5.
Já na fase de classificação, não existe limite mínimo de similaridade.
Os documentos foram todos categorizados em alguma das classes geradas por o agrupamento.
A Figura 5.2 apresenta a quantidade de verdadeiros positivos (Vp) e de falsos positivos (FP) tabulados em faixas de similaridade, iniciando por os categorizados com mais de 50% de similaridade com a classe, seguidos de faixas de 5% até um mínimo de 10% de similaridade.
Para melhor analisar a validação da categorização, foram obtidos de cada classificação, os seguintes indicadores:
Simcateg: Similaridade entre o documento e a classe;
Qtdoc: Quantidade de documentos no grupo correspondente à classe;
Coesao: Coesão do grupo correspondente à classe;
Simmean: Similaridade média dos documentos do grupo correspondente à classe;
Maxsim: Similaridade do documento de maior similaridade com o centróide do grupo correspondente à classe;
Minsim: Similaridade do documento de menor similaridade com o centróide do grupo correspondente à classe;
Qtattseed: Quantidade de atributos no centróide do grupo correspondente à classe;
Qtattdocs: Soma da quantidade de atributos nos documentos do grupo correspondente à classe;
Maxattdoc: Quantidade de atributos do documento com mais atributos no grupo correspondente à classe;
Minattdoc: Quantidade de atributos do documento com menos atributos no grupo correspondente à classe;
Meanattdoc: Média de atributos nos documentos do grupo correspondente à classe;
Maxngram: Quantidade de palavras do termo com o maior número de palavras no grupo correspondente à classe;
Minngram: Quantidade de palavras do termo com o menor número de palavras no grupo correspondente à classe;
Meanngram: Quantidade média de palavras dos termos no grupo correspondente à classe;
Qtterm: Quantidade de atributos originados de termos dos vocabulários jurídicos do grupo correspondente à classe;
Qtrefleg: Quantidade de atributos originados de referências legislativas do grupo correspondente à classe;
Qtmerge: Quantidade de uniões de grupos realizadas no grupo correspondente à classe;
Maxattcomm: Maior quantidade de atributos em comum entre os documentos e o centróide do grupo correspondente à classe;
Minattcomm: Menor quantidade de atributos em comum entre os documentos e o centróide do grupo correspondente à classe;
Meanattcomm: Quantidade média de atributos em comum entre os documentos e o centróide do grupo correspondente à classe;
Não foi detectada nenhuma evidência de relação entre estes atributos e o sucesso/ falha na classificação, exceto por uma fraca relação com a similaridade entre o documento classificado e a classe, já evidenciada na Figura 5.2.
Informação Não Extraída dos Documentos Após avaliação dos dados obtidos por os após a execução dos exemplos de uso, percebeuse que o grau de similaridade dos documentos categorizados com os centróides dos grupos geradores das classes apresenta algum nível de relação com o sucesso/ falha da classificação.
No entanto, esta relação é insuficiente para explicar satisfatoriamente os resultados da classificação.
Assim, procedemos a uma revisão mais detalhada dos casos extremos.
Analisamos, então, os 11 documentos erroneamente classificados cuja similaridade com os centróides supera 45%.
Analisamos, também, os 14 documentos corretamente classificados cuja similaridade com os centróides é inferior a 25%.
Falsos Positivos com Alta Similaridade Percebe- se que uma combinação de dois fatores muito contribuiu para a incidência dos falsos positivos estudados:
Atributos com alta freqüência e atributos com semântica demasiadamente genérica.
O documento 554, por exemplo, foi classificado na classe correspondente ao grupo 15449, rotulado como &quot;crime».
Tem como atributos os listados na Tabela 5.5.
Percebese a predominância do atributo &quot;crime&quot;:
Quase o triplo do segundo atributo mais freqüente e mais que o triplo do terceiro atributo.
Além deste, os atributos &quot;código penal», &quot;justiça do trabalho», &quot;multa», &quot;legislação penal «e &quot;circunstância atenuante «são, também, demasiadamente genéricos.
Nota- se, também, que não há atributos originados de referências legislativas.
O grupo é composto de 5 documentos, cujos temas podem ser vistos na Tabela 5.6.
Percebe- se que, em verdade, não há identificação de temas entre quaisquer dois documentos do grupo.
Além de isto, não se pode falar em sanar este problema aumentando a quantidade de iterações do algoritmo na expectativa de que o passo de projeção elimine mais atributos do centróide, pois os atributos eliminados seriam os de maior especificidade semântica.
A título de comparação, o oposto ocorre com a classificação do documento 1039 no grupo 15447, rotulado como &quot;estação de rádio».
A classificação ocorreu com similaridade de 72%, a mais alta de entre as classificações em nosso exemplo de uso.
Tanto o documento classificado como os 3 documentos agrupados versam sobre atraso na autorização para operação de emissora de rádio.
A o observar- se os atributos do centróide, percebe- se que os atributos de maior peso têm alta especificidade semântica.
Além de isto, tanto os documentos agrupados, como o documento classificado têm pelo menos um atributo não nulo em comum com o centróide originado de referência legislativa não genérica4.
O grupo 19018, rotulado como &quot;dano&amp; indenização», é resultante de uma divisão implícita, que iniciou- se com o documento 61 na segunda iteração, recebendo mais 3 documentos nas duas última iterações.
Este grupo tem como tema a indenização por danos morais.
Durante o teste de classificação, 5 documentos foram categorizados na classe correspondente a este grupo.
De estes, 3 documentos são verdadeiros positivos, 2 são falsos positivos.
De os falsos positivos, um foi categorizado com baixa similaridade, 29,57%, tendo apenas um atributo não nulo em comum com o centróide, &quot;indenização&quot;;
O outro documento 4 Por referência legislativa genérica, entenda- se uma referência a uma legislação ampla, como a Constituição Federal ou os Códigos Civil e Penal, sem especificar um artigo.
Já o documento 979 foi erroneamente categorizado, com similaridade de 53,3%, na classe correspondente ao grupo 19116, rotulado como &quot;crédito tributário&amp; multa», resultante de divisão implícita.
Este grupo contêm apenas um documento e somente três atributos em seu centróide, listados na Tabela 5.9.
De os atributos não nulos do documento, somente &quot;multa «e &quot;crédito tributário «também não são nulos no centróide.
A escassez de atributos, agravada por o fato do atributo &quot;multa «ser demasiadamente genérico, acabou por determinar a errônea categorização deste documento.
Foram avaliadas as classificações que, embora tenham sido corretas, tiveram muito baixa similaridade entre o documento e o centróide do grupo correspondente.
Foi observado que os centróides e estes documentos tinham poucos atributos não nulos em comum.
O documento 989, por exemplo, classificado com 19,5% de similaridade na classe correspondente ao grupo 15556, rotulado como &quot;l 9289/1996 art. 7», tem apenas dois atributos não nulos em comum com o centróide deste grupo:
&quot;contador «e &quot;renda», ambos muito genéricos.
O documento 769, classificado com 19,3% de similaridade na classe correspondente ao grupo 19067, rotulado como &quot;período de carência&amp; ação ordinário», tem apenas dois atributos não nulos em comum com o centróide deste grupo:
&quot;renda «e &quot;ação ordinário», ambos muito genéricos.
Em relação a o problema gerado por atributos de semântica muito genérica com maior peso nos centróides que os de semântica mais específica, elencamos algumas alternativas, descritas nas Seções 5.6.2.1, 5.6.2.2 e 5.6.2.3.
Descarte de Nodos Não Terminais A maneira mais simples de evitar que atributos de semântica muito genérica ganhem demasiada relevância nos centróides dos grupos, conseqüentemente agrupando documentos com temática diversa, como ocorreu com o grupo 15449 ­ &quot;crime», é descartar estes atributos.
Embora tenhamos descartado as informações de hierarquia dos termos dos tesauros utilizados, esta poderia ser utilizada para selecionar somente os termos mais específicos, ou seja, apenas os nodos folha, como ilustrado na Figura 5.3, seriam reconhecidos no préprocessamento dos documentos.
Atribuição de Pesos aos Termos Um possível problema que poderá surgir se adotada a alternativa apresentada na Seção pode ocorrer é que documentos que tratem de assuntos semelhantes apresentem termos distintos, porém filhos de um mesmo nodo-pai, também presente no texto, mas descartadão, não sejam reconhecidos como mais similares entre si que documentos de assuntos &quot;muito distantes», ou seja, de grande distância entre os respectivos nodos.
Uma forma de lidar com este problema seria permanecer reconhecendo os termos genéricos, mas, atribuir pesos de acordo com o nível de especificidade.
Se o peso semântico for um inteiro indicando o nível de profundidade na hierarquia, ao somar- lo ao atributo, que se insere no intervalo (0;
1], estaremos garantindo, em qualquer caso, que termos mais específicos ganhem mais relevância no cálculo de similaridade.
Além de isto, no passo de projeção, o descarte de atributos selecionará sempre os atributos mais genéricos, em detrimento de os mais específicos.
A Tabela 5.10 apresenta um exemplo de como o peso semântico poderia influir na relevância dos atributos do Grupo &quot;dano&amp; indenização».
A exemplo do trabalho de Toutanova[ TCP+ 01], revisado na Seção 3.2.2, poderia ser implementado um algoritmo de agrupamento hierárquico onde cada nodo da árvore representaria uma classe, e os termos jurídicos e referências legislativas estariam relacionados com níveis diferentes desta árvore, conforme suas respectivas especificidades.
Dife- rentemente do experimento de Toutanova, o nível dos atributos não é desconhecido, mas obtido conforme referenciado na Seção 5.6.2.2 e detalhado na Seção 5.7.
Atualização dos Tesauros Se observarmos a Tabela 5.7, veremos que ali consta o termo estupro.
Considerado o contexto dos demais termos, percebe- se que tal termo não parece ter a menor relação com os demais, sugerindo a possibilidade de problemas na extração dos termos.
Em exame mais detalhado dos documentos, encontramos, no documento 636, as expressões &quot;violação ao princípio da eficiência e da razoabilidade «e &quot;Violação A os ARTIGOS 6o De a LEI 9612/ 98 E 9o, Inciso II, De o DECRETO 2615/ 98».
O termo &quot;violação», como unigrama, é sinônimo do termo &quot;estupro».
Expressões como &quot;violação de /do/a/ao direito/ referência legislativa «são comuns, mas não se encontram em nenhum dos vocabulários utilizados.
Estes vocabulários contêm expressões específicas, como &quot;violação de direito autoral», ou &quot;violação de direito de propriedade «e, assim, são reconhecíveis como tal e não como sinônimo de estupro.
No entanto, como as expressões utilizadas não existiam nos tesauros, o pré-processamento reconheceu apenas o unigrama &quot;violação», que foi normalizado para estupro.
Tanto Sordi quanto Jaegger ressaltam que seus tesauros estavam incompletos.
Tal defasagem agrava- se, tendo em vista o tempo transcorrido desde a época de sua publicação até o presente.
Há diversas iniciativas de geração automatizada de tesauros e ontologias.
Em português encontramos a ferramenta ExatoLP de Lopes, que automatiza a criação de estruturas ontológicas e poderia auxiliar neste processo.
Agrupamento Semi--supervisionado por Referências Legislativas Uma outra possibilidade seria adaptar o algoritmo de agrupamento para usar uma função de similaridade que considerasse, tão somente, os atributos com origem em referências legislativas.
De esta maneira, garantiría- se- que cada grupo contivesse somente documentos que referenciassem as mesmas normas.
Alguns problemas devem, no entanto, ser levantados:
As partes dos processos, podem, eventualmente, não realizar pesquisa jurisprudencial suficientemente abrangente para determinar que normas se aplicam ao caso em questão e, conseqüentemente, os documentos por elas anexados ao processo jurídico não conterão referências legislativas que gerem atributos necessários à boa classificação.
Tal problema, no entanto, pode ser minimizado utilizando- se uma função de similaridade diferente para a classificação, onde seriam considerados todos ou, pelo menos, alguns atributos com origem em termos jurídicos;
Os documentos podem conter, apenas, referências legislativas muito genéricas, como a Constituição Federal;
As referências legislativas podem ser redigidas de forma que não sejam detectadas por o parser.
Exemplos disto não faltam:
&quot;Magna Carta», &quot;Lei Orgânica da Magistratura», &quot;Lei da Mordaça», &quot;Lei Maria da Penha», Estatuto da Criança e do Adolescente», etc..
Seria necessário, portanto, construir- se um dicionário de nomes populares de normas jurídicas.
Atribuição de Pesos Semânticos aos Termos e referências Legislativas De entre as alternativas elencadas nas seções anteriores, descartamos a opção da Seção demandariam muito mais tempo do que dispúnhamos para serem implementadas.
De esta maneira, optamos por concentramos- nos na opção da Seção 5.6.2.2.
Atribuímos, então, um peso semântico a cada termo extraído dos vocabulários jurídicos.
Este peso é um inteiro indicando o nível de profundidade na hierarquia que foi somado ao atributo.
Para obter o nível de profundidade, retornamos aos tesauros.
No caso de o TJF, a relação hierárquica é uma informação completa, ou seja, todos os termos apresentam a indicação de todos os seus hiperônimos e a sua distância dos mesmos5 e, assim, bastou selecionar o maior n dos respectivos TGn de cada termo e usar- lo como peso do termo.
No caso de o VCB, a estrutura de árvore está fragmentada e há muitos termos específicos sem indicação de hiperônimo.
No entanto, a maioria dos termos do VCB apresenta a Classificação Decimal de Direito (CDD), composta por um número de 3 dígitos que pode ser seguido de um ponto e um número variável de dígitos.
A quantidade de dígitos após o ponto indica o grau de especificidade do termo e foi usada como peso do termo.
Restaram 2016 termos do VCB que não apresentavam o CDD e, assim, receberam o peso mínimo, ou seja, zero.
Quanto a as referências legislativas, atribuiu- se peso quatro às referências que não continham especificação de artigo e peso seis a qualquer referência legislativa acompanhada de especificação de artigo.
As únicas normas sem referência de artigo que não receberam peso 4 foram os Códigos Civil e Penal, por serem leis extensas, que receberam, então, peso 3 e as Constituições Federal e estaduais em virtude de suas abrangências, incluindo matérias cíveis e penais, que receberam peso dois.
Determinados os pesos para os atributos, realizou- se nova extração de termos e referências legislativas dos documentos, gerando os respectivos vetores de atributos.
As Seções 5.8 e 5.9, a seguir, descrevem as análises dos resultados da execução dos agrupamentos e da classificação destes novos vetores com pesos semânticos.
Obtidos os novos vetores de atributos, agora com pesos semânticos, realizou- se nova rodada de execuções dos vários algoritmos de agrupamento, conforme a Tabela 5.1.
O limiar de similaridade teve de ser reduzido para 40% porque com o aumento de atributos os documentos tornaram- se mais distintos, reduzindo a similaridade e, assim, novamente o algoritmo original de Aggarwal, Gates e Yu descartava muitos documentos e, posteriormente, descartava grupos.
Mas, com esta redução, os centróides foram considerados muito similares e todos os grupos foram aglomerados num único grupo na primeira iteração.
Em a segunda iteração, após o recálculo do centróide, a maioria dos documentos era descartada pois ficaram muito distantes do centróide e, em seguida, o grupo era descartado em função de o limiar de descarte de grupos.
A solução foi iniciar as iterações com, no máximo, 70 atributos nos centróides, o que garantiu diferenciação a eles, impedindo que se unissem num único grupo.
Isto acarretou numa diminuição da quantidade de iterações, o que não garantia um bom refinamento dos grupos e, assim, fixou- se o mínimo de 15 atributos para encerramento das iterações.
Os algoritmos 2 e 3 obtiveram a melhor performance numa das medidas.
O algoritmo 3 teve o melhor desempenho médio.
O algoritmo 6 teve o segundo melhor desempenho médio, perdendo para o algoritmo 3 por, apenas, 2,20%.
Tabela 5.13 ­ Ranks de Relative Hardness para o cálculo do Wilcoxon Sign Test entre os algoritmos 3 e 6 Partição rank apresentado na Tabela 5.12.
Verificou- se que, não se pode rejeitar a hipótese nula H0RH:
RH3 $= RH6, ou seja, não há diferença significativa entre a performance dos dois algoritmos em relação a a medida Relative Hardness.
Procedeu- se, então ao cálculo do Wilcoxon signed-- ranks test, cujos ranks podem ser verificados na Tabela 5.13: T $= min (R3, R6) $= 13 z $= z $= demonstrando que z\&gt; 1, 96 e, portanto, o Wilcoxon signed-- ranks test também não permite rejeitar a hipótese nula H0RH e, assim, não se verifica superioridade significativa da performance do algoritmo 3 sobre o algoritmo 6, em relação a a medida Relative Hardness.
Vê- se, portanto, que, há controvérsia entre os testes de significância em relação a as duas medidas de qualidade interna dos agrupamentos obtidos com os algoritmos 3 e 6.
Em vista de isto, e por não realizar descartes, que, conforme elencado na Seção 4.2, é uma característica desejada num sistema de pesquisa jurisprudencial, selecionamos, novamente, o algoritmo 6 para aprofundar nossos estudos.
Passamos, então à comparação entre o algoritmo 1 e o algoritmo 6.
Para verificar se o resultado da aferição representa melhoria significativa em relação a a medida Rh e se não¯ realizamos o mesmo particionamento representa piora significativa em relação a a medida, do conjunto de treino em 8 subconjuntos e repetimos os agrupamentos segundo os dois algoritmos em cada uma das partições.
Estabeleceram- se as hipóteses nulas H0RH:
RH1 $= RH6 e H0 Para verificar se o algoritmo evoluído, usando a variante que realiza a divisão implícita, superou o algoritmo de Aggarwal, Gates e Yu, realizamos o teste de contagem de vitórias e derrotas, tendo em vista que este é o teste que menos rejeita a hipótese nula.
Conforme a Tabela 5.15, nosso algoritmo superou o original em 7 das 8 partições, permitindo rejeitar a hipótese nula e concluir que a performance do algoritmo evoluído é significativamente superior, com respeito a a medida Relative Hardness.
Por outro lado, para aferir se o algoritmo de Aggarwal, Gates e Yu tem performance significativamente superior à performance de nosso algoritmo, usando a variante Alg.
A Tabela 5.17 apresenta os ranks das diferenças de performance obtidas em cada conjunto de dados, usados no cálculo do Wilcoxon signed-- ranks test, T $= min $= 8 z $= z $= demonstrando que z\&gt; 1, 96 e, portanto, o Wilcoxon signed-- ranks test também não permite rejeitar a hipótese nula e, portanto, não se verifica superioridade significativa da performance do algoritmo de Aggarwal, Gates e Yu sobre o algoritmo evoluído, em relação a a medida Measure.
Separamos, então, aleatoriamente 105 documentos do conjunto de operação, que foram divididos em 3 subconjuntos disjuntos.
Os subconjuntos A e B contém 50 documentos e o subconjunto C contém 5 documentos.
O especialista humano 1, cujo perfil detalhamos no Apêndice E, recebeu, para avaliação, as categorizações dos subconjuntos A e C. O especialista humano 2, cujo perfil detalhamos no Apêndice F, recebeu para avaliação, as categorizações dos subconjuntos B e C. Utilizou-se o mesmo programa de validação e, ao final da validação, verificou- se que, dos 5 documentos que foram validados por ambos especialistas, apenas 1 foi objeto de discordância.
O Anexo A apresenta o inteiro teor do documento Em o 50, cuja classificação no grupo o caso mais semelhante ao do documento Em o 50.
Ambos possuem as seguintes características:
Discussão entre o INSS e o segurado, que requerem a concessão de auxílio-doença ou mesmo aposentadoria por invalidez;
A condição de segurado e o período de carência foi devidamente provada no processo;
O fim da invalidez afigura- se permanente ou impossível de determinar.
Os casos divergem por as seguintes características:
O INSS é vencedor no caso de o documento Em o 17 e derrotado no caso de o documento Em o 50;
Difere o argumento da decisão judicial, no documento Em o 50 o segurado tinha doença congênita, portanto, não gozava de condições para o trabalho na data em que ingressou na condição de segurado;
E, no caso de o documento Em o 17, o juízo sequer discute a existência ou não da capacidade laborativa à epoca do ingresso na condição de segurado.
Ressaltando que a vitória ou derrota desta ou aquela parte no processo não é objeto tratado em nossa proposta de uso do aprendizado de máquina na pesquisa jurisprudencial e que, desta maneira, focamos apenas em averiguar se há identificação temática entre o documento ora classificado e os documentos do grupo correspondente a esta classe.
A divergência, portanto, se resume a um ponto:
A razão sobre a qual se embasa a decisão do juiz.
Não nos cabe, todavia, entrar no mérito do entendimento dos especialistas humanos.
Cabe, no entanto, decidir a respeito de a divergência de avaliação.
Em o caso específico, entendemos que a comprovação da validade da classificação realizada em nosso exemplo de uso deve manter- se incontroversa.
Havendo dúvida, deve ser reputada como falha e, portanto, 53 documentos foram considerados verdadeiros positivos (Vp) e 52 documentos (49,5%) foram considerados falsos positivos (FP) por ao menos um especialista humano.
Os documentos foram categorizados em 74 das 453 classes geradas por o agrupamento.
A Figura 5.4 apresenta a quantidade de verdadeiros positivos (Vp) e de falsos positivos (FP) tabulados em faixas de similaridade, iniciando por os categorizados com mais de 40% de similaridade com a classe, seguidos de faixas de 5% até um mínimo de 5% de similaridade.
Verifica- se, aqui, que com similaridades mais altas, é maior a probabilidade de se obter um verdadeiro positivo.
O contrário também se verifica.
Acima de 30% de similaridade não ocorreram falsos positivos.
Verificamos, também, que 26 dos 53 verdadeiros positivos ocorreram em classes que correspondem a grupos com menos de 4 documentos, sendo 18 em classes correspondentes a grupos com um único documento.
Ressaltando que ajustamos para 4 documentos o limiar para descarte de grupos, utilizados por algumas das variações do algoritmo, percebemos importância de não realizar tal descarte.
Note- se que em ambiente de produção tal proporção, provavelmente, não se verificará.
Pois, em nossos exemplos de uso, lidamos com um corpus bem menor6 do que o real montante disponível na instituição judiciária7 e, conseqüentemente, o tamanho médio dos grupos aumentará.
Tribunal Regional Federal da 4a Região, desde sua fundação em 1988 já julgou mais de 3 milhões de processos, a maioria, no entanto, só existe em meio eletrônico na forma de imagem digitalizada, não sendo viável tratar- las textualmente.
Não tão evidente é o fato de que a relação mais fraca é a do indicador qtterm e a mais forte é a do indicador qtrefleg.
Estes indicadores estão, no entanto, relacionados entre si.
Pois, ambos são contagens do tipo de origem dos atributos e, assim, quanto maior o qtrefleg, tanto menor o qtterm.
Verificamos que os atributos com origem em referências legislativas nos centróides iniciais representavam cerca de 4,9% dos atributos e que nos centróides finais eles representam cerca de 15,17%, percebe- se que este tipo de atributo 8 No caso de o indicador qtdoc, por exemplo, 0% representa grupos com um único documento e 100% representa grupos com 25 documentos.
Além de isto, se considerarmos que, em produção, teremos um conjunto de documentos muito maior, a quantidade de documentos em cada grupo vai aumentar e, conseqüentemente, também aumentará a quantidade de atributos não nulos nos centróides.
Espera- se que, com isto, aumente a incidência de verdadeiros positivos, quando na fase de categorização.
Impressões dos Especialistas Humanos De acordo com o especialista humano 1, as falhas de classificação ocorreram quando o documento a ser classificado versava sobre matéria de muita especificidade.
Citou o exemplo do documento 20, onde o tema era &quot;prescrição intercorrente «e os documentos retornados versavam sobre &quot;prescrição», mas não &quot;intercorrente».
Tal documento apresenta 12 ocorrências do termo &quot;prescrição intercorrente», mas apresenta 10 ocorrências do termo &quot;prescrição», além de 1 de &quot;extinção do processo «e 1 de &quot;imprescritibilidade», termos estes, que ocorrem em documentos atribuidos ao grupo gerador da classe em a qual o documento 20 foi classificado.
Além de isto, reproduzindo parcialmente o seguinte trecho:
&quot;Lei 11.051, percebemos que a última referência a «prescrição», considerado o contexto, é, na verdade, uma referência a &quot;prescrição intercorrente «e não ao termo mais genérico &quot;prescrição».
A correta detecção do termo, expresso como &quot;prescrição», mas com o sentido de &quot;prescrição intercorrente», extrapola nossa proposta, demandando a implementação de desambigüação semântica.
Este especialista foi questionado a respeito de a utilidade de um sistema que realize pesquisa jurídica automaticamente, com base nos documentos anexados ao processo eletrônico, retornando documentos com a jurisprudência correlata, com a precisão ora aferida.
Manifestou- se dizendo que tal retorno aceleraria seu processo de pesquisa de jurisprudência.
Informou que utilizando um sistema padrão de pesquisa de jurisprudência, a saber pesquisa por palavras-chave na ementa dos documentos, costuma ter que ler uma média de 35 documentos antes de encontrar aquele que lhe traga informação jurídica necessária para sua argumentação no caso em que está trabalhando.
A respeito de as ementas, informa que em torno de 30% das ementas ou estão erradas ou não suficientemente específicas.
O especialista humano 2 manifesta que costuma realizar buscas utilizando combinação de parâmetros ou expressões muito específicas e que comumentemente se depara com resultados extremos:
Ou não retorna nenhum documento ou retornam centenas ou milhares, inclusive para combinação de vários termos.
Ressaltou que uma pesquisa usando as tecnologias ora propostas não trarão benefício nos casos em que retorna muitos documentos e nenhum trata do problema específico.
Mas, ressalta que se um único de eles estiver relacionado ao tema buscado é mais útil que as buscas convencionais que retornam &quot;dezenas ou mesmo centenas de casos muito genéricos».
Ressalta, ainda, que impressionou- se muito quando obteve poucos documentos (5, no máximo) e todos ou quase todos eram úteis para a solução de seu problema.
Atentou, também, para o fato de que as buscas convencionais falham em reconhecer documentos quando usam expressões diferentes da buscada, e entende ser este o maior problema das pesquisas textuais.
Por fim, este especialista ainda ressaltou que o benefício social da implantação de um sistema baseado na metodologia proposta não se restringiria à aceleração da tramitação processual, mas, também, na relocação de recursos humanos para tarefas mais especializadas, citando, como exemplo, os estagiários e advogados iniciantes, comumentemente alocados para realização de pesquisa jurisprudencial.
Note que o especialista humano 1 aponta como maior causa dos erros de classificação a extrema especificidade do documento classificado.
Já o especialista humano 2 entendeu que, comparando com os sistemas de busca convencionais, o &quot;sistema «ora avaliado conseguiu retornar documentos com maior grau de especificidade.
Embora pareçam visões conflitantes, note- se que o especialista humano 1 não emitiu seu parecer fazendo comparação com os sistemas convencionais.
Simplesmente frisou que os erros que ocorreram foram em virtude de o maior grau de especificidade do documento classificado.
Considerações Finais Em este capítulo avaliamos os resultados das execuções do algoritmo original de Aggarwal, Gates e Yu, e das variações propostas para evolução deste algoritmo.
Tendo em vista não encontrarmos relações entre os indicadores extraídos dos resultados da validação por especialista humano, e após análise mais detalhada de algumas classificações, verificamos a necessidade de valorizar termos cuja semântica fosse mais específica, em detrimento de termos muito genéricos, como &quot;crime».
Adotados os procedimentos da Seção 5.6.2.2, executamos novamente, nosso exemplo de uso, gerando novos agrupamentos.
Recalculamos as medidas internas destes agrupamentos para selecionar aquele que proveria as classes para categorização.
Descobrimos que, do ponto de vista das medidas internas, o agrupamento gerado por o algoritmo 3, que descarta documentos mas não descarta nem divide grupos, obteve melhor performance.
No entanto, tendo em vista que sua performance média superou por meros 2% a performance média do algoritmo 6, que não realiza descartes e realiza a divisão implícita, optamos por trabalhar com este último.
A classificação desta segunda execução de nosso exemplo de uso foi avaliada por dois especialistas humanos que, além de identificarem verdadeiros e falsos positivos, apresentaram breve relato de suas impressões sobre nossa proposta de pesquisa de jurisprudência, sinalizando que o uso de aprendizado de máquina para este fim pode contribuir para acelerar o processo de cognição no meio judicial brasileiro.
Aprofundamos tais ponderações no próximo capítulo, onde concluímos nosso estudo.
Conclusão Em o decorrer deste estudo, revisamos os fundamentos teóricos na área do aprendizado de máquina.
Em a área de aprendizado não supervisionado, estudamos o K--Means, o mais popular algoritmo flat para hard clustering e o algoritmo Em, clássico algoritmo flat para soft clustering.
Realizamos, também, breve revisão de outras metodologias nesta área, como agrupamentos hierárquicos e agrupamentos semi-supervisionados.
Em a área de aprendizado supervisionado, destacamos, de nossa revisão, as redes Bayesianas e o algoritmo SVM.
Aprofundamos nosso estudo revisando trabalhos relacionados ao que aqui desenvolvemos.
Vários destes trabalhos apresentaram soluções calcadas em processo de agrupamento baseado em modificações do Em visando a geração de redes bayesianas precedido de diferentes algoritmos de agrupamento.
O estudo dos trabalhos relacionados conclui aprofundando a proposta de Aggarwal, Gates e Yu.
Percebemos que esta proposta, de partir de uma prévia classificação para gerar um conjunto de classes melhorado, identifica- se com a classificação ementária que encontramos na jurisprudência brasileira.
Além de isto, sua proposta de conduzir as iterações reduzindo a dimensionalidade dos centróides e, com isso, acelerar o processamento a cada iteração, sinalizou a viabilidade do uso deste algoritmo em ambiente de produção.
O Tribunal Regional Federal da 4a Região, que forneceu o conjunto de documentos que compôs nosso corpus de estudo, em maio de 2009, quando iniciamos os procedimentos de download da jurisprudência, contava com mais de 3 milhões de documentos em sua jurisprudência e concluía, em média, 700 processos por dia.
Os processos conclusos tornamse nova jurisprudência e, assim, é preciso que se reexecute o agrupamento sobre o corpus.
Tal execução necessita estar conclusa no dia seguinte, quando nova leva de processos serão julgados.
Em este contexto, conta muito a seleção de algoritmos que se destaquem na velocidade de processamento.
Além de isto, o categorizador proposto utilizou função de similaridade comparando os novos documentos com os centróides dos grupos geradores das classes e, assim, o seu treinamento se constitui do próprio processo de agrupamento.
Já os trabalhos que se utilizam do SVM ou de seus derivados, demandam, ainda, o treinamento do classificador após a execução do agrupamento e, portanto, agravam o problema de incluir novos documentos no corpus e, eventualmente, o problema da descoberta de novas classes.
Apresentamos, então, nossa proposta de adaptação do algoritmo de Aggarwal, Gates e Yu para uso em corpus jurídico.
Verificamos ser necessária a adaptação do al-goritmo, conforme exposto na Seção 5.9, e não meramente sua utilização em corpus com características distintas daquele empregado originalmente por seus autores.
Isso porque, em sua forma original, o algoritmo realiza descartes de documentos e grupos, a título de ruído.
Ressaltamos que no contexto de nosso estudo, o da pesquisa jurídica, tais &quot;ruídos «são valorizados.
É o caso de temas novos, sem precedentes, tal como o exemplo de recente exposição dedicado à questão das células-tronco.
Objeto este de forte disputa, chegando a mobilizar setores de nossa sociedade.
Por esta razão propusemos modificar o algoritmo evitando todo e qualquer descarte.
Outra modificação proposta, seguindo discussão suscitada por os próprios autores, foi a de implementar um passo de divisão de grupos, que não chegou a ser explorada por Aggarwal, Gates e Yu.
Inicialmente implementamos a divisão de grupos como um passo extra da iteração.
No entanto, percebemos que, por não descartar documentos no passo de atribuição, obtínhamos, obviamente, grupos de baixa densidade que, conseqüentemente, se tornavam os maiores candidatos à divisão.
Assim, inspirados no algoritmo TOD apud, que cria um novo grupo quando um documento está muito distante de qualquer centróide, experimentamos realizar a divisão dos grupos dentro de o próprio passo de atribuição;
Denominamos tal procedimento de divisão implícita, ao invés de criar um passo próprio para tanto.
De esta maneira, economizouse o custo computacional imposto por o passo de divisão que necessita avaliar a variância de similaridade intra-cluster de cada grupo para decidir se o grupo deve ser dividido.
Outra economia computacional advinda desta modificação foi a eliminação das sub-iterações realizadas no passo de divisão de grupos.
Verificamos, também, que o algoritmo usando a divisão implícita obteve melhor performance média que o algoritmo que implementou passo de divisão de grupos, com relação a as medidas de qualidade internas Relative Hardness Measure e Measure, em face de o conjunto de treino utilizado.
O algoritmo que usou a divisão implícita não apresentou, no entanto, melhor performance média que a variante que descarta documentos, não descarta grupos e não realiza qualquer divisão de grupos.
Porém, consideramos que tal diferença, 2,2%, era um prejuízo aceitável se considerado o fato de que estávamos elegendo uma versão que não realizava descarte de documentos.
Preferimos grupos um pouco menos densos e um pouco menos distintos entre si a arriscarmos a possibilidade de que alguém, por exemplo, não seja liberto porque a argumentação capaz de convencer um juiz de sua soltura existia, mas não foi encontrada.
Além disso, o especialista humano 2, ao manifestar suas impressões acerca de os resultados de nosso exemplo de uso, declarou ser preferível a recuperação de um conjunto pequeno de documentos9, desde que, obviamente, eles contenham a informação buscada.
Ressalte- se que o limiar para descarte de grupos utilizado originalmente por Aggarwal, Gates e Yu era de 8 documentos e, em nosso exemplo de uso, tivemos de reduzir para 4.
Ainda assim, um a menos que o máximo expressado como ideal por o especialista humano 1.
Finalmente, ao analisarmos a classificação, verificamos que foi obtida uma precisão de 50,5%.
Embora pareça, a princípio, que esta precisão é muito baixa, ressaltamos que não se pode, por exemplo, comparar- la com performances obtidas por classificadores binários.
Nosso classificador obteve esta performance em face de 453 possíveis classes e não de duas como o fazem os classificadores binários.
A Tabela 5.18 apresenta a quantidade máxima de categorias usadas nos experimentos de classificação discutidos nos trabalhos relacionados.
Exceto por o algoritmo em o qual baseamos nosso trabalho, que superou em pouco mais de 150% o quantitativo de classes obtidas em nosso exemplo de estudo, os demais trabalhos lidaram com quantidade bem inferior de classes.
O único trabalho que reporta a precisão de seus resultados é o de Toutanova, que, usando 15 classes, obteve resultados variando de cerca de 58% a 88%.
Em relação a o custo computacional, entendemos ser viável a implantação de sistema baseado em nossa proposta para atender a carga de processos no ritmo de seu crescimento num ambiente de produção.
Embora não tenhamos feito aferições exaustivas, todos os exemplos de uso executaram num computador com processador AMD Athlon64 X4@ 2.6 Ghz, com 4 Gb de memória.
Já o equipamento recém adquirido por o Ministério Público Federal para uso com o processo eletrônico é um Dual Hexa-Core HT@ 2.6 Ghz, que expõe 24 processadores para o sistema operacional e dispõe de 32 Gb de memória.
Desde o parsing até o agrupamento, gastou- se menos de 3 horas e isto considerando- se que cada um destes procedimentos foi implementado em programas separados e, assim, neste tempo, deve- se levar em conta a repetição da inicialização de variáveis que, por carregarmos o banco de dados lexical em memória, representa parcela significativa deste tempo.
A carga do tesauro representa em torno de 12 minutos, por exemplo.
A o unificar estes procedimentos num só programa, o tempo de execução sofrerá redução significativa.
Enfim, nesta nossa primeira proposta de uso de aprendizado de máquina para recupe-ração de jurisprudência, cremos haver contribuído com respeito a:
Uso do aprendizado de máquina na pesquisa jurisprudencial:
Apresentamos processo de categorização auxiliado por agrupamento baseado em algoritmo, selecionado em virtude de (a) partir de um conjunto de classes previamente determinado e gerar um novo conjunto de classes melhorado e, assim, reduzir os erros de classificação encontrados nas ementas da jurisprudência, além de descobrir as classes sem que seja necessário pré-configurar sua quantidade antes da execução do algoritmo;
Evolução do algoritmo selecionado:
Adaptamos este algoritmo para as necessidades específicas da pesquisa de jurisprudência, realizando com sucesso modificações que (a) eliminaram os descartes de documentos e grupos, que poderiam impedir que fossem encontrados documentos relativos a casos sem precedentes que, se apresentados ao juiz do caso em andamento, podem fazer a diferença entre o sucesso ou insucesso da respectiva demanda;
Prototipação do processo proposto:
Implementamos protótipo do algoritmo proposto e executamos nosso exemplo de uso em computadores de uso pessoal, utilizando linguagem interpretada, sem focar na otimização do código e, ainda assim, o pré-processamento consumiu 9s por documento, o agrupamento levou 1h30 min para agrupar 716 documentos e o algoritmo de categorização classificou um documento a cada 2s.
Verificamos, portanto, a viabilidade de sua utilização em ambiente de produção.
Além de isto, elencamos como contribuições secundárias Merge de Dicionários:
Unificação dos dicionários Unitex-PB e Wiktionary em língua portuguesa e Wiktionary em latim, produzindo um dicionário mais completo;
Corpus: Organização de corpus jurídico em língua portuguesa, contendo jurisprudência do Tribunal Regional Federal da 4a Região;
Parser: Codificação de parser para a língua portuguesa, reconhecendo palavras constantes do dicionário, e, também, referências legislativas, tal como explicado na Seção Tagger:
Codificação de tagger que lematiza os tokens extraídos por o parser, baseandose em dicionário, utilizando método iterativo alternando o uso de regras gramaticais e probabilidades;
Merge de Tesauros: Unificação dos tesauros jurídicos do Senado Federal e do Conselho da Justiça Federal, identificando automaticamente termos iguais e, através de especialista, termos equivalentes;
Extrator de termos:
Codificação de reconhecedor de termos jurídicos na seqüência de lemas obtida através do tagger, usando o tesauro jurídico unificado.
Em nosso exemplo de uso, separamos 1.192 documentos dos 43.704 obtidos do Tribunal Regional Federal da 4a Região.
Conforme informado, este procedimento foi necessário, tendo em vista que o algoritmo de Aggarwal, Gates e Yu atribui documentos a um único grupo.
Por discutirem acerca de múltiplos temas, tais documentos foram descartados.
No entanto, verificando que nosso exemplo de uso tratou, apenas, 2,73% dos documentos obtidos, percebemos que, em trabalhos futuros, será necessário dedicar nossos esforços a acrescentar novas evoluções nesse algoritmo a fim de habilitar- lo a agrupar e classificar documentos em múltiplos grupos/ classes.
Uma das possibilidades consideradas é incorporação da técnica de segmentação de documentos, proposta por Tagarelli e Karypis.
Também merece mais atenção o papel que as referências legislativas representam na qualidade dos resultados da categorização.
Em trabalhos futuros, consideramos alterar o algoritmo proposto aumentando o seu grau de semi-supervisionamento.
Atualmente, esse algoritmo é considerado semi-supervisionado por partir de grupos gerados em função de classificação prévia.
Mas, após a inicialização dos grupos, não há quaisquer restrições nas operações de atribuição de documentos e de aglomeração e divisão de grupos.
Poderíamos, assim, aplicar restrição na função de similaridade do algoritmo de agrupamento, levando- à considerar apenas os atributos oriundos de referências legislativas.
Os atributos originados de termos jurídicos, embora ignorados, continuariam presentes nos vetores dos documentos e dos centróides.
De esta maneira, posteriormente, a função de similaridade do algoritmo de categorização livre dessa restrição, utilizaria tanto atributos oriundos de termos jurídicos quanto atributos originados de referências legislativas.
Isso poderá permitir que os documentos produzidos por as partes possam ser categorizados mesmo na eventualidade de seus advogados desconhecerem uma ou mais legislações pertinentes ao caso em questão.
Um problema que decorrerá desta técnica é uma redução drástica dos atributos, que poderão, em muitos casos, atingir patamares mínimos, como 5 ou menos atributos.
Em conseqüência de isto, poderá ficar comprometida a performance desse algoritmo, por basearse na quantidade de atributos para regular suas iterações e determinar seu encerramento.
Em este caso, as iterações poderiam ser regidas por outra variável, como a coesão, por exemplo;
Outros pontos que podem ser desenvolvidos em trabalhos futuros:
Categorização dos 133 documentos do conjunto de operação que restaram sem ser categorizados.
Tal classificação poderia ser validada, inclusive, perante conjuntos de classes oriundos de vários algoritmos de agrupamento, não somente aqueles prototipados em nosso exemplo de uso, mas, também, outros que, na ocasião, se considere oportuno acrescentar ao conjunto de estudo;
Paralelização do algoritmo, tendo em vista que nos últimos anos tem- se observado que a arquitetura dos computadores disponíveis no mercado vem abandonando o modelo monoprocessado.
A o executar os protótipos dos algoritmos, em nosso exemplo de uso, observou- se constantemente, que a taxa de uso de uma das CPUs subia para 100%, equanto as demais CPUs oscilavam entre 0% e 20%.
Este é um claro indicativo do benefício que se pode obter através da paralelização (a) dos procedimentos de parsing e desambigüação que pode ocorrer tanto em nível de documento quanto em nível de parágrafo;
Tal conclusão leva em conta, principalmente, a consideração de que equipamentos de ambiente corporativo, como o Ministério Público Federal, têm maior disponibilidade de processadores que equipamentos de uso pessoal.
