O desenvolvimento de fármacos é um dos grandes desafios da ciência atual por se tratar de um processo onde os custos e o tempo envolvido são elevados.
Um dos problemas mais interessantes nessa área é a predição da conformação e da energia envolvida na interação entre ligantes e suas proteínas-alvo ou receptores.
É nos experimentos de docagem molecular que essa interação é avaliada.
É muito comum que durante a docagem molecular se façam simplificações onde o receptor é tratado como rígido.
Porém, proteínas são inerentemente sistemas flexíveis e essa flexibilidade é essencial para a sua função.
A inclusão da flexibilidade do receptor em experimentos de docagem molecular não é uma tarefa trivial, pois, para permitir mobilidade a certos átomos do receptor, há um aumento exponencial do número de graus de liberdade a serem considerados.
Há atualmente diversas alternativas para contornar esse problema, entre elas, a que se optou neste trabalho:
Considerar a flexibilidade explícita do receptor por meio de a execução de uma série de simulações de docagem molecular, utilizando em cada um de eles uma conformação diferente da trajetória dinâmica do receptor, gerada por uma simulação por dinâmica molecular (DM).
Um dos maiores problemas desse método é o tempo necessário para executar- lo.
Sendo assim, o objetivo desse trabalho é contribuir para a seleção de conformações do receptor de forma a acelerar a execução de experimentos de docagem molecular com o receptor completamente flexível.
Além de o mais, o trabalho apresenta novas metodologias para a análise da interação receptor-ligante em simulações de docagem deste tipo.
Para alcançar esses objetivos, é aplicado um processo de descoberta de conhecimento.
A primeira etapa consistiu no desenvolvimento de um banco de dados para armazenar informações detalhadas sobre o receptor e suas conformações, ligantes e experimentos de docagem molecular, chamado FReDD.
Com os dados organizados no FReDD, foi possível a aplicação de diferentes técnicas de mineração de dados.
O primeiro conjunto de experimentos foi realizado utilizando o algoritmo de classificação J48.
O segundo conjunto de experimentos foi executado com o algoritmo de regressão M5P, onde apesar de resultados interessantes, a utilização direta para seleção de conformações em futuros experimentos de docagem molecular não se mostrou promissora.
Finalmente, foram executados os experimentos de agrupamento com 10 diferentes algoritmos, com entradas variadas.
Para os algoritmos de agrupamento foram desenvolvidas diferentes funções de similaridade onde os resultados finais utilizados em conjunto com o padrão de dados P-MIA permitiu a redução efetiva da quantidade de experimentos de docagem.
Palavras-chave: Mineração de Dados, Docagem Molecular, Receptor Flexível, Dinâmica Molecular.
Um dos grandes desafios da ciência hoje é sem dúvida o desenvolvimento de novos fármacos.
Trata- se de um processo complexo e interdisciplinar, dirigido por esforços combinados da indústria farmacêutica, companhias de biotecnologia, autoridades reguladoras, pesquisadores acadêmicos e outros setores privados e públicos.
Além disso, os custos envolvidos são muito altos:
Em média um bilhão de dólares até a aprovação de um novo fármaco, assim como o tempo despendido é em torno de 10 a 15 anos.
Existe uma grande variedade de abordagens computacionais que podem ser aplicadas aos diferentes estágios do processo de desenvolvimento de novos fármacos.
Em os primeiros estágios, o foco está em reduzir o número de ligantes cuja interação com o receptor provavelmente será favorável, enquanto que, nos passos finais, os esforços estão direcionados em diminuir os custos experimentais e reduzir o tempo de execução dos experimentos.
Enquanto alguns pesquisadores de novos fármacos focam em soluções alternativas para otimizar o processo, outros trabalham na melhoria dos protocolos já existentes.
Essas melhorias podem ser direcionadas para incorporar a flexibilidade da proteína em simulações de docagem molecular, explorar extensivamente as conformações do ligante dentro de o sítio ativo, refinar e estabilizar o complexo receptor-ligante final, estimar as energias livres de ligação, entre outras.
Segundo Broughton, um dos desafios mais interessantes na área de desenvolvimento de novos fármacos está justamente relacionado com a predição da geometria e da energia envolvida na interação entre ligantes em suas proteínas-alvo.
É nas simulações de docagem molecular que a interação entre um receptor1 e um ligante2 é analisada.
É muito comum que durante a docagem molecular se façam simplificações drásticas, onde tipicamente, o receptor é tratado como rígido.
Porém, quando somente uma conformação do mesmo é considerada, de acordo com Totrov &amp; Abagyan, os algoritmos atuais de docagem molecular predizem erroneamente o local de ligação para 50 a 70% dos ligantes analisados.
Ademais, proteínas são sistemas inerentemente flexíveis sendo essa flexibilidade essencial para determinar sua função.
Além disso, de todas as possíveis conformações que uma proteína pode apresentar em determinado intervalo de tempo, não é possível conhecer a priori qual ou quais dessas conformações será adotada em resposta a ligação a determinado ligante ou como desenhar um ligante para uma conformação ainda desconhecida A inclusão da flexibilidade do receptor em simulações de docagem molecular não é uma tarefa trivial.
Para permitir mobilidade a certos átomos do receptor há um aumento exponencial no 1 Em o presente trabalho macromolécula, proteína e receptor são termos tratados com o mesmo significado.
Há atualmente diversas alternativas para contornar esse problema.
Por exemplo: Pode- se utilizar bibliotecas de rotâmeros para as cadeias laterais, diferentes estruturas cristalográficas do receptor, uma estrutura média baseada num conjunto de estruturas ou ainda um conjunto de conformações do receptor resultantes de uma simulação por dinâmica molecular (DM).
Alguns trabalhos estão relacionados somente com a consideração do movimento de cadeias laterais do sítio de ligação no receptor, por o algoritmo de docagem ou por meio de a indicação do usuário, de quais cadeias laterais, ou partes da cadeia principal, ele deseja considerar como flexíveis durante as simulações de docagem, entre muitas outras alternativas revisadas por Alonso, Cozzini, B-Rao E Wong.
Entretanto, um dos métodos mais promissores para o tratamento da flexibilidade do receptor é por meio de a geração de subconjuntos de conformações do mesmo por a simulação por DM.
Esse é considerado hoje o método mais acessível de se produzir muitas conformações da proteína com um custo razoável.
Com essa metodologia uma série de conformações, denominada trajetória dinâmica do receptor, é gerada e utilizada em estudos de docagem molecular[ LIN02, LIN03].
De essa maneira, pode ser executado um conjunto de simulações de docagem molecular, utilizando- se em cada uma, uma conformação diferente da trajetória dinâmica.
Em este trabalho, foi adotada a nomenclatura de modelo FFR (Fully-- Flexible Receptor) para simulações de docagem com a consideração explícita da flexibilidade do receptor.
Um dos maiores problemas desse método é o tempo necessário para executar- lo.
Um exemplo do tempo de execução para a consideração do modelo FFR de receptores em docagem molecular seria:
Dado um banco de dados de pequenas moléculas (ligantes) como o ZINC que atualmente tem mais de 20 milhões de compostos disponíveis.
A análise da interação de todos esses possíveis candidatos, mesmo que in-silico, com uma determinada proteína-alvo se torna inviável de ser executada, pois, estima- se que seriam necessários em torno de 650.000.000 hs (mais de 74 mil anos) até o término da execução desse experimento (onde cada experimento proteína-ligante é executado em aproximadamente 1 minuto numa máquina Core2Quad, 2,4 GHz, 8 GB de memória RAM).
Por essa razão, é essencial que se pesquisem maneiras menos custosas de incorporar a flexibilidade dos receptores nas simulações de docagem molecular.
Para alcançar os objetivos propostos, como primeira etapa, tornou- se necessário o desenvolvimento do FReDD (Flexible Receptor Docking Database), um banco de dados para armazenar informações sobre o receptor e suas conformações, ligantes e simulações de docagem molecular.
Assim, os dados foram organizados de tal maneira que se tornou possível utilizar- los em experimentos com diferentes técnicas de mineração de dados, objetivando encontrar padrões entre as conformações do receptor que indicassem quais eram as mais promissoras.
Foram executados experimentos de mineração de dados com técnicas como classificação, regressão e agrupamento, que permitiram um melhor entendimento da interação receptor-ligante e avançaram no sentido de reduzir o espaço conformacional a ser considerado nas simulações de docagem molecular com o modelo FFR.
Motivação O processo de docagem considerando a flexibilidade explícita do receptor, a partir de uma série de execuções de simulações de docagem onde, em cada uma, uma conformação do receptor é considerada, é computacionalmente muito custoso.
Por outro lado, a consideração da flexibilidade de receptores é muito importante, uma vez que proteínas não permanecem rígidas em seu ambiente celular, sendo a flexibilidade essencial para exercer sua função.
Assim, a principal motivação do trabalho está em contribuir para a redução do conjunto de conformações a serem consideradas, agrupando- as por diferentes critérios de similaridade, mas ainda mantendo as características explícitas de flexibilidade do receptor tornando a busca de novos inibidores mais realista e abrangente, já que a flexibilidade do receptor estará sendo considerada.
A utilização de mineração de dados em Bioinformática é uma linha de pesquisa em evidência atualmente.
As técnicas de preparação de dados biológicos desenvolvidas neste trabalho poderão ser utilizadas em outras áreas de pesquisa em Bioinformática que necessitem da aplicação de técnicas para a descoberta de conhecimento em grandes bancos de dados.
Outra motivação para o desenvolvimento deste trabalho está relacionada ao receptor investigado, a enzima InhA do Mycobacterium tuberculosis.
A tuberculose é uma doença infecciosa que, embora curável, representa um problema de saúde pública de proporção mundial.
Dados da Organização Mundial da Saúde (OMS) reportam que 9,2 milhões de pessoas no mundo desenvolveram tuberculose no ano de 2006, com um total de 1,7 milhões de mortes relacionadas a esta doença.
A isoniazida (INH), um dos principais fármacos utilizados no tratamento da tuberculose tem como alvo a enzima InhA.
Este fármaco apresenta uma poderosa atividade antibactericida, porém, devido a o tratamento longo, prescrição imprópria do medicamento, e muitas vezes falta de colaboração do paciente, surgiram cepas de Mycobacterium tuberculosis resistentes a um ou mais fármacos hoje existentes no mercado.
Por os motivos apresentados, torna- se essencial a busca de inibidores alternativos para essa enzima.
E para isso, há a necessidade da utilização e desenvolvimento de novas técnicas que agilizem o processo de Planejamento Racional de Fármacos (do inglês, Rational Drug Design -- RDD).
Objetivos O objetivo geral dessa Tese é de contribuir para o entendimento da importância da flexibilidade do receptor em simulações de docagem molecular e para a redução do tempo necessário para a execução desse tipo de experimento a partir de a aplicação de um processo de descoberta de conhecimento em Banco de Dados.
Desenvolver um Banco de Dados para armazenamento de resultados de docagem molecular com o modelo FFR e preparar os dados para os diferentes algoritmos de mineração.
Executar experimentos de mineração de dados com as técnicas de classificação e regressão de forma a melhorar o entendimento sobre a importância da flexibilidade de receptores em simulações de docagem molecular.
Agrupar conformações do modelo FFR obtidas de uma simulação por a DM.
Para tal, novas funções de similaridade são desenvolvidas e utilizadas por diferentes algoritmos de agrupamento de conformações.
Contribuir para a redução do tempo de execução das simulações de docagem molecular com o modelo FFR com o auxílio do padrão de dados para workflows científicos P-MIA.
Organização da Tese Esta Tese de Doutorado está organizada da seguinte forma:
O Capítulo 2 apresenta os principais conceitos importantes para o entendimento de todo o trabalho.
Em este capítulo são descritos:
O Planejamento Racional de Fármacos ou Rational Drug Design (RDD), incluindo as suas principais etapas, o processo de Docagem Molecular, as diferentes abordagens para a consideração da Flexibilidade do Receptor em docagem molecular e a metodologia de Dinâmica Molecular.
Em o Capítulo 3 são descritos todos os materiais e métodos utilizados.
Em ele inclui- se a descrição das principais ferramentas direta e indiretamente aplicadas:
O workflow científico FReDoWS, o software de docagem molecular AutoDock3.
0.5, o software de dinâmica molecular AMBER9, o software para análise de interação receptor-ligante LigPlot, o sistema gerenciador de banco de dados PostGreSQL, a linguagem de programação Python e a plataforma para mineração de dados WEKA.
Além de as ferramentas, esse capítulo descreve o receptor e os ligantes investigados, assim como, as simulações por DM e docagem molecular que originaram todos os dados desta Tese.
O Capítulo 4 é uma continuação do anterior.
Em ele são abordados os conceitos de Mineração de Dados e das principais etapas do processo de descoberta de conhecimento em banco de dados.
São também descritas neste capítulo as técnicas de mineração de dados utilizadas e os respectivos algoritmos.
Em o Capítulo 5 é apresentado o primeiro resultado desta Tese, o Banco de dados FReDD (Flexible Receptor Docking Database), desenvolvido para armazenar os resultados de docagem molecular com o modelo FFR, assim como as informações sobre as conformações do receptor e os ligantes.
Este capítulo também apresenta como o conteúdo armazenado no FReDD foi preparado para ser utilizado com as técnicas de mineração de dados onde é descrito o algoritmo desenvolvido para gerar essas entradas.
A o final desse capítulo, a partir de as entradas preparadas para mineração é descrita uma análise preliminar sobre esses dados.
O Capítulo 6 apresenta o segundo conjunto de resultados desta Tese, a aplicação da técnica de mineração de dados Classificação com árvores de decisão utilizando o algoritmo J48.
Uma das principais contribuições desse capítulo é a metodologia proposta de discretização do atributo alvo dos arquivos de entrada utilizados.
Essa metodologia proposta é então comparada com 2 métodos de discretização clássicos com base no impacto dos mesmos no resultado das árvores de decisão geradas.
O Capítulo 7 descreve o terceiro conjunto de resultados desta Tese que estão relacionados com os experimentos realizados com a técnica de mineração de dados de regressão por árvores modelo, utilizando o algoritmo M5P do WEKA.
Em ele são comparados os resultados obtidos com árvores modelo para diferentes formas de pré-processamento dos arquivos de entrada.
Além disso, é descrita uma metodologia de pós processamento dos resultados das árvores modelo que permitiu a indicação das conformações mais promissoras nesses experimentos.
O Capítulo 8 contém o último conjunto de resultados desta Tese, que compreendem os experimentos com a técnica de mineração de dados Agrupamento.
São apresentados os testes realizados com 10 algoritmos de agrupamento implementados em para diferentes entradas e com diferentes funções de similaridade, incluindo as funções desenvolvidas nesta Tese.
Em o final deste Capítulo são descritas análises utilizando o P-MIA que comparam as funções de similaridade e mostra um estudo de caso efetivo do ganho de processamento obtido com a utilização do P-MIA em conjunto com os resultados de Agrupamento.
Em o Capítulo 9 relaciona alguns trabalhos já publicados com o conteúdo desta Tese.
Estes incluem trabalhos sobre Bancos de Dados para Planejamento Racional de Fármacos, trabalhos sobre a execução de docagem molecular com o receptor flexível e seleção de conformações e trabalhos sobre a utilização de algoritmos de agrupamento com dados de DM.
O Capítulo 10 apresenta as considerações finais desta Tese, com sugestões para trabalhos futuros.
Este capítulo apresenta conceitos importantes para o entendimento de todo o trabalho.
A primeira seção descreve o Planejamento Racional de Fármacos ou Rational Drug Design (RDD), incluindo as suas principais etapas.
A seguir, a seção Docagem Molecular explica este processo que constitui o princípio do RDD.
A seção Consideração da Flexibilidade do Receptor apresenta algumas das diferentes abordagens que podem ser adotadas para incorporar a flexibilidade do receptor em simulações de docagem molecular.
Em seguida, é explicada brevemente a metodologia de Dinâmica Molecular.
Esta metodologia para a geração de conformações de um receptor faz parte da abordagem de incorporação da flexibilidade do mesmo durante o processo de em docagem adotado neste trabalho.
Por fim, são apresentadas as considerações finais deste capítulo.
O Planejamento Racional de Fármacos A indústria farmacêutica tem investido constantemente em novas tecnologias para melhorar a qualidade dos compostos candidatos a fármacos.
Paralelo a isso, os avanços da biologia molecular e de ferramentas de simulação in-silico, o planejamento de medicamentos passou a ser feito de maneira mais lógica, o que é chamado de Planejamento Racional de Fármacos (RDD).
Esse processo consiste basicamente de quatro etapas descritas em e representadas no fluxograma da Figura 2.1: A primeira etapa consiste em isolar um alvo específico ou receptor (proteínas, receptores de membrana, DNA, RNA e outros).
A partir de a análise computacional da estrutura tridimensional (3 D) dessa proteína determinada por modelagem comparativa ou cristalografia por difração de raios-X ou por ressonância magnética nuclear, e armazenada num banco de dados estrutural como o Protein Data Bank -- PDB, é possível apontar prováveis regiões de ligação onde uma pequena molécula (ligante) pode se ligar a esse receptor (atividades 1 e 2 do fluxograma da Figura 2.1).
Baseado nas prováveis regiões de ligação identificadas na etapa anterior, é selecionado um conjunto de possíveis candidatos, chamados ligantes, que podem se ligar a essa região no receptor.
Usualmente ligantes podem ser buscados em bancos de dados de compostos como o ZINC.
As diferentes conformações e orientações que determinado ligante pode assumir dentro de o sítio de ligação de uma determinada proteína são simuladas por software de docagem molecular como AutoDock 3.0.5, FlexX e DOCK4.
0. Os ligantes que teoricamente obtiveram melhores resultados nas simulações são experimentalmente sintetizados e avaliados através de ensaios biológicos e pré-clínicos (atividade 5 do fluxograma).
Baseado nos resultados experimentais, o medicamento é gerado ou o processo retorna ao início.
As quatro etapas do processo de RDD descritas por Kuntz compreendem a fase de pesquisa e desenvolvimento.
Em esta fase do desenvolvimento de um novo medicamento, que é divididos em duas etapas:
Testes pré-clínicos (onde são escolhidos os modelos animais para servir como cobaias) e testes clínicos em humanos.
Os testes clínicos em humanos são divididos em 4 fases:
Fase I:
Em esta fase é avaliada a segurança do novo fármaco, se os efeitos colaterais do mesmo são suportáveis, definir a melhor forma de administração do medicamento e analisar como o organismo reage.
Os testes são aplicados em 20 a 100 voluntários saudáveis;
Fase II: Em esta fase é analisada a efetividade da droga.
São determinados os efeitos colaterais e outros aspectos relacionados a segurança e toxicidade do composto.
Durante esse período também é determinada a dosagem a ser administrada baseada numa amostra de algumas centenas de pacientes;
Fase III (2-6 anos):
Estabelece a eficácia e efeitos colaterais após um longo período de uso do medicamento baseado numa amostra maior de pacientes.
O registro da nova droga é então submetido para o órgão responsável do país (por exemplos nos Estados Unidos, a US Food and Drug Administration -- FDA) e após aprovado para comercialização ainda são necessários alguns meses para o acerto de questões burocráticas.
Docagem Molecular O entendimento detalhado das interações entre receptores biológicos e seus ligantes é muito importante para aplicações médicas e industriais, sendo essencial para a interpretação de uma série de fenômenos bioquímicos, constituindo a base do RDD.
A docagem molecular é um processo que possibilita que pequenas moléculas sejam posicionadas numa configuração favorável para a formação de um complexo receptor-ligante estável.
Esse método tem se mostrado muito efetivo no estudo de interações proteína-ligante e essa informação estrutural obtida do complexo modelado teoricamente pode auxiliar na definição de como a estrutura de um ligante pode ser modificada para melhorar sua função biológica ou para o desenho de novos compostos.
A docagem molecular é executada por algoritmos de docagem, que são capazes de gerar um grande número de complexos receptor-ligante, avaliando- os em termos de a energia livre de ligação Em a Figura 2.2 é apresentado um exemplo do processo de docagem molecular.
Em cinza, está representado parte do sítio ativo da molécula receptora (a proteína InhA).
Em ciano a conformação inicial do ligante Triclosano antes da execução do algoritmo de docagem molecular, e em magenta, o ligante em sua posição final determinada por o algoritmo de docagem como sendo a de melhor encaixe.
A proteína é representada na forma de superfície, em cinza, e o ligante em palitos.
A conformação inicial do ligante aparece em ciano, e a conformação do ligante ao final de um experimento de docagem molecular, em magenta.
O processo de analisar a interação receptor-ligante não é simples pois é influenciado por muitos fatores entrópicos e entálpicos, como por exemplo, a mobilidade do ligante e do receptor, o efeito do ambiente no receptor, a distribuição de cargas no ligante, e outras interações dos mesmos com a água que dificultam muito a descrição desse processo.
De essa forma, independente da natureza dos complexos receptor-ligante, algumas questões precisam ser contempladas por os algoritmos de docagem molecular e estas podem ser resumidas como a combinação da estratégia de busca com uma função de avaliação.
A estratégia de busca da melhor conformação/ orientação do ligante precisa explorar exaustivamente todas as formas de ligação entre ligante e receptor, o que inclui tanto a exploração de todos os seis graus de liberdade translacionais e rotacionais do ligante, quanto os graus de liberdade conformacionais do receptor.
Em os primeiros algoritmos de docagem molecular desenvolvidos, o método de aproximação mais comum era o tratamento de ligantes e receptores como corpos rígidos, baseado no modelo de reconhecimento molecular do tipo &quot;chave e fechadura «proposto por Emil Fisher em 1894.
Entretanto esta é uma simplificação muito drástica e limita os resultados para aqueles próximos à conformação experimentalmente observada para o complexo receptor-ligante.
Além de o mais, a geometria molecular pode mudar muito na associação receptor-ligante uma vez que ambas são moléculas flexíveis, sendo mais apropriada uma analogia ao modelo mão-e-luva uma vez que durante a docagem molecular ligante e receptor ajustam suas conformações de forma a encontrar um melhor encaixe, o chamado encaixe induzido ou induced-fit, proposto inicialmente por Koshland Jr..
Em 1958.
A flexibilidade do ligante é atualmente explorada por a maioria dos software de docagem molecular, uma vez que não envolve um grande esforço computacional.
Entretanto, a consideração da flexibilidade de receptores continua sendo um grande desafio.
Modelar diretamente os movimentos de uma proteína associado com a flexibilidade do sítio ativo da mesma representa um problema significante devido a o desafio duplo da alta dimensionalidade do espaço conformacional e a complexidade da função de energia envolvida.
Esses fatores tornam a exploração de todos os graus de liberdade do receptor impraticável.
Porém, ao mesmo tempo, não considerar a flexibilidade dos receptores nos experimentos de docagem molecular induz a uma predição errônea do local de ligação de 50 a 70% dos ligantes.
Além de o mais, proteínas não permanecem rígidas em seu ambiente celular e essa flexibilidade é essencial para exercerem suas funções.
Consideração da Flexibilidade do Receptor Atualmente existe um grande número de alternativas para incorporar, ao menos, parte da flexibilidade do receptor revisadas nos últimos anos por Teodoro e Kavraki, Totrov e de docagem molecular publicados no ano de 2009 têm especificamente endereçado a questão da flexibilidade dos receptores.
Segundo Alonso, a primeira solução chamada Soft Docking lida com a flexibilidade de maneira simples e indireta.
Em essa abordagem, um pequeno grau de sobreposição entre o ligante e o receptor é permitido através de interações de van der Walls suavizadas.
Isso permite, por exemplo, que um determinado ligante possa se encaixar num sítio de ligação onde supostamente só uma molécula menor poderia.
Segundo B-Rao A maior desvantagem dessa abordagem é que somente movimentos nas cadeias laterais podem ser analisados mas não na cadeia principal ou outras mudanças mais significativas.
Entretanto, Soft Docking tem a vantagem de ser computacionalmente eficiente e de fácil interpretação, sendo por isso também explorado em Segundo Wong o primeiro método a incorporar explicitamente a flexibilidade do receptor em simulações de docagem foi o método apresentado por Leach.
Em essa abordagem, durante a docagem molecular são explorados os graus de liberdade conformacionais das cadeias laterais de alguns resíduos do sítio ativo do receptor, enquanto sua cadeia principal é mantida rígida.
Uma extensão desse trabalho é apresentada em onde o espaço conformacional de toda a proteína é explorado.
Assim como a abordagem apresentada por Leach, muitas outras metodologias de incor poração da flexibilidade do receptor em docagem molecular se utilizam da inclusão da mobilidade das cadeias laterais do sítio ativo do receptor como, por exemplo:
Schnecke e Kuhn apresentam o algoritmo Slide em que um fragmento do ligante é inicialmente posicionado, seguido por a adição de outros fragmentos.
Após, conflitos entre o ligante e o receptor são resolvidos por rotações em partes do ligante e nas cadeias laterais do receptor;
Shaffer e Verkhivker descrevem um algoritmo que utiliza uma biblioteca de rotâmeros para executar uma busca otimizada de todas as possibilidades de combinações de conformações das cadeias laterais do sítio ativo do receptor.
Apostolakis Apresenta uma abordagem que utiliza uma conformação do receptor em a qual um ligante é posicionado aleatoriamente em seu sítio ativo e a energia do complexo é minimizada para remover eventuais sobreposições.
Isto é repetido 1000 vezes, gerando uma estrutura diferente do sítio ativo a cada execução.
Os melhores resultados são submetidos a um refinamento da minimização de energia.
De essa forma, o conjunto de conformações do sítio ativo representa parte da flexibilidade do receptor.
Mais recentemente, o programa AutoDock4 passou a modelar completamente a flexibilidade de certas porções da proteína.
Isto é realizado a partir de cadeias laterais do receptor selecionadas por o usuário que serão tratadas como flexíveis durante a simulação de docagem molecular, utilizando os mesmos métodos aplicados por o AutoDock4 para tratar a flexibilidade do ligante.
Outro trabalho mais recente, que incorpora a flexibilidade do receptor ao processo de docagem a partir de a mobilidade do sítio ativo do receptor, é o algoritmo MADAMM.
Em MADAMM, a proteína é flexibilizada utilizando- se uma biblioteca de rotâmeros de cadeias laterais de aminoácidos.
MADAMM aumenta a capacidade do algoritmo de docagem introduzindo essa flexibilidade das cadeias laterais selecionadas por o usuário através de um processo de docagem com múltiplos estágios incluindo uma etapa de modelagem molecular.
Em este trabalho foi demonstrado que a orientação de resíduos particulares do receptor tem uma influência crucial na forma como receptor-ligante interagem durante a docagem molecular.
Segundo B-Rao, a maior desvantagem desse método é que o mesmo somente considera mudanças conformacionais nas cadeias laterais.
Para simular o processo de ligação receptor-ligante o mais detalhado possível e evitar limitações de outras abordagens de modelo de flexibilidade, algumas metodologias se utilizam dos métodos de simulação por a DM, muitas vezes, em etapas pós-docagem.
A principal vantagem da representação por DM em estudos de docagem é que essa se mostra muito acurada e pode explicitamente modelar todos os graus de liberdade do receptor.
Infelizmente essas metodologias apresentam custos computacionais muito altos.
O detalhamento deste tipo de técnica está fora de o escopo deste trabalho.
Em os artigos de revisão além de as técnicas classificadas em alguma das categorias de acordo com Teodoro e Kavraki há diversas metodologias que não se enquadram em nenhum dos outros 4 grupos.
Um exemplo é a técnica inovadora apresentada por Zhão e Sanner, o software FLIPDock (Flexible Ligand Protein Docking).
Esse software permite a execução automática de docagem molecular considerando ligantes e sítios ativos do receptor como flexíveis descrevendo os mesmos por meio de Flexibility Tree (FT), uma estrutura de dados que codifica o subespaço conformacional de moléculas biológicas utilizando um pequeno número de variáveis e reduzindo muito o custo computacional de modelar moléculas flexíveis.
A FT é utilizada para descrever tanto ligantes quanto receptores.
Segundo Zhão e Sanner a maior vantagem do FLIPDock é sua versatilidade para a descrição com a FT das moléculas envolvidas e a combinação com métodos de busca e como as funções de escore.
Assim é possível que novas funções possam ser incorporadas a qualquer momento ao programa.
O FLIPDock também permite ao usuário controlar a alocação de recursos computacionais para a representação de movimentos específicos.
Além de os métodos apresentados acima, que utilizam somente uma conformação do receptor, há um grande número de abordagens para incorporação da flexibilidade do receptor na docagem molecular que utilizam um conjunto de conformações do mesmo.
Esse conjunto de conformações pode ser determinado experimentalmente por difração de raios X ou por NMR ou gerados por métodos computacionais como simulações utilizando DM.
Muitos trabalhos focam na descrição da flexibilidade do receptor através da combinação de um conjunto de estruturas em grids.
Geralmente esses grids representam uma média das estruturas.
Knegtel Foram os pioneiros nessa abordagem em a qual condensaram as estruturas do receptor num grid simples com o objetivo de reduzir o tempo de execução da docagem molecular.
Eles avaliaram duas diferentes maneiras de combinar muitas estruturas determinadas experimentalmente numa representação média:
Uma considerando a média de energia de interação entre receptor-ligante e outro baseado na variação de posição dos átomos do receptor.
Osterberg Oferecem uma representação discreta do sítio ativo do receptor através do cálculo de grids de três maneiras diferentes:
Um grid médio que corresponde a uma média simples ponto- aponto dos valores de todos os grids que representam as estruturas;
Um grid mínimo que considera o valor mínimo entre todos os grids e por último um grid ponderado por a energia envolvida, quanto mais negativa a energia, maior o peso do ponto no grid médio.
Broughton também utiliza grids para representar a flexibilidade do receptor, entretanto as estruturas são obtidas de uma simulação por a DM.
Ao invés de combinar diferentes estruturas num grid simples, o programa FlexE cria uma descrição única para a proteína alvo.
As características estruturais mais conservadas são sobrepostas numa estrutura média rígida.
Para as regiões que variam, são consideradas explicitamente diferentes estruturas mantidas como um conjunto que é explorado combinatoriamente durante a docagem.
Mais recentemente, Bottegoni Têm trabalhado no desenvolvimento do 4 DDocking, um novo protocolo para execução de docagem molecular, em que a conformação do receptor é a quarta dimensão.
Em este protocolo, múltiplos grids representam múltiplas conformações e cada uma destas é considerada como uma variável na otimização global.
Essa abordagem se mostrou bastante eficiente para a modelagem da flexibilidade de receptores em docagem molecular, reduzindo o tempo de execução desse tipo de experimento e mantendo a acurácia.
Receptor Segundo Alonso, a abordagem mais abrangente para a inclusão da flexibilidade de receptores consiste em executar simulações de docagem molecular do ligante contra cada estrutura de um conjunto de estruturas do receptor geradas por simulação por a DM.
Lin Apresentam o método chamado RCS -- Relaxed Complex Scheme para acomodar a flexibilidade do receptor na busca por a conformação receptor-ligante mais correta.
Primeiro os autores executaram uma simulação por a DM do receptor sem o ligante e então docaram ligantes às estruturas geradas durante a simulação por a DM.
Após, aplicaram o RCS para encontrar a conformação receptorligante mais correta.
Mais recentemente, no trabalho apresentado por Amaro, os autores apresentam melhorias no método RCS incluindo uma redução prévia do conjunto de conformações do receptor, buscando um conjunto menor, porém representativo (os trabalhos de serão melhor detalhados no Capítulo de Trabalhos Relacionados).
Entre todas as metodologias que foram brevemente apresentadas neste trabalho, optamos por considerar a flexibilidade do receptor utilizando a combinação de docagem molecular com resultados de simulação por DM.
Como explicado na Introdução desta Tese, foi adotado o termo modelo FFR (Fully-- Flexible Receptor) para essa metodologia.
Este consiste na execução de uma sequência de simulações de docagem molecular, em cada uma é empregada uma conformação diferente da trajetória da DM.
Apesar de essa metodologia aumentar a chance de se encontrar um receptor num estado conformacional correto para acomodar um ligante em particular, ela tem como maior desvantagem o tempo necessário para executar cada diferente simulação de docagem receptorligante.
Dinâmica Molecular Em condições fisiológicas, as biomoléculas experimentam vários tipos de movimentos e de mudanças conformacionais muitas vezes cruciais para suas funções.
Com o avanço de técnicas experi- mentais tornou- se possível uma visão mais detalhada de diversos processos biológicos por o acesso a propriedades atômicas de macromoléculas biológicas, como proteínas.
O acesso a esse tipo de informação permitiu o desenvolvimento de estudos de simulação por DM que tem por objetivo simular o comportamento microscópico de átomos em proteínas, fundamentada nos princípios básicos da mecânica clássica.
Com simulações por a DM é possível estudar o efeito explícito de ligantes na estrutura e estabilidade das proteínas, considerando os efeitos do solvente e os diferentes parâmetros termodinâmicos envolvidos (pressão, temperatura, volume, etc.), incluindo energias de interação e entropias.
De essa forma, a simulação por DM é uma das técnicas computacionais mais amplamente aplicada no estudo de macromoléculas biológicas.
Essa técnica é importante para o entendimento do comportamento dinâmico das proteínas em diferentes intervalos de tempo, o que permite o estudo desde movimentos internos rápidos até mudanças conformacionais mais lentas.
Por essas razões, atualmente a simulação por DM é considerada a melhor técnica para obter um conjunto mais completo de conformações de uma proteína.
De acordo com Cozzini A técnica de simulação por a DM representa o método mais acessível e com um custo mais razoável para a geração de conformações de um receptor.
Por esse motivo, a metodologia de incorporação da flexibilidade do receptor em simulações de docagem molecular considerada neste trabalho se utiliza de conformações geradas por simulação por a DM.
Considerações Finais Este capítulo apresentou os principais conceitos envolvidos no trabalho e que são essenciais para o seu entendimento:
Planejamento de fármacos e suas principais etapas, a docagem molecular e a simulação por DM.
Também está descrito neste capítulo as diferentes abordagens que podem ser utilizadas para a consideração da flexibilidade de receptores em simulações de docagem molecular.
Entre todas a metodologias que foram brevemente apresentadas, a que escolhemos neste trabalho foi a combinação de docagem molecular com resultados de simulação por DM.
Para isso, é executada uma seqüência de simulações de docagem molecular onde em cada uma é empregada uma diferente conformação da trajetória da DM (modelo FFR).
A execução automática dessa série de simulações de docagem para o modelo FFR do receptor é feita utilizando- se o workflow científico FReDoWS descrito em.
De acordo com Alonso Essa metodologia aumenta a chance de se encontrar um receptor num estado conformacional correto para acomodar um ligante em particular.
Entretanto, o tempo necessário para executar- la ainda é considerável, sendo esta a principal desvantagem desta abordagem Outros trabalhos desenvolvidos no LABIO (Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas) utilizaram do modelo FFR, como os trabalhos, demonstrando a importância desse tipo de abordagem.
Em o trabalho descrito em, a utilização das estruturas instantâneas geradas nas simulações por DM permitiu a simulação da flexibilidade da enzima InhA de Mycobacterium tuberculosis durante a docagem molecular no estudo da afinidade por o NADH desta proteína e 3 diferentes mutantes da mesma.
Em este trabalho foi demonstrado que as mutações causam instabilidades conformacionais ao longo de as trajetórias dinâmicas, sendo este um conhecimento importante para a busca de novos inibidores para esta proteína.
Essas informações não teriam sido obtidas sem considerar o receptor e seus mutantes como moléculas flexíveis.
Em o trabalho descrito por Cohen Em, foi investigado o efeito da flexibilidade explícita também da enzima InhA através da realização de simulações de docagem molecular em cada uma das diferentes conformações do seu modelo FFR, com os inibidores etionamida (ETH), triclosan (TCL) e isoniazida-pentacianoferrato II (PIF).
O modelo FFR utilizado neste estudo mostrou que diferentes modos de ligação dos ligantes ETH, utilizada.
A análise apresentada nestes trabalhos revelou, por exemplo, que para o complexo InhA-ETH apenas 5 resíduos da proteína interagem com este ligante na estrutura cristalina, enquanto que ao longo de a trajetória no seu modelo FFR, 80 diferentes resíduos fazem contatos com o ligante ETH.
O mesmo também foi observado nos estudos com o ligante TCL, onde apenas 2 resíduos deste receptor interagem com o ligante na estrutura cristalina, enquanto que no modelo FFR, 46 diferentes resíduos interagem com o TCL.
Efeito semelhante é observado para o complexo indica que quando a plasticidade do receptor é considerada em simulações de docagem molecular é permitido que sejam explorados novos espaços no sítio de ligação do receptor, que não seriam possíveis de outra forma.
Além desses trabalhos descritos brevemente acima, um estudo detalhado da importância do modelo FFR na docagem esta descrito em e será detalhado nos próximos capítulos.
Em este trabalho foi analisado os resultados da docagem molecular também do modelo FFR da InhA espécie ligante NADH, em sua estrutura cristalográfica, 22 resíduos do receptor interagem com o ligante, enquanto que utilizando o modelo flexível deste, 185 diferentes resíduos interagem.
A mesma diferença é observada para os outros ligantes, demonstrando novamente a importância da consideração da flexibilidade do receptor, pois com o uso de um modelo rígido não seria possível a obtenção desse tipo de informação, que é muito importante na busca de novos inibidores para esta enzima.
O próximo capítulo apresenta os materiais e métodos utilizados neste trabalho.
Serão detalhadas todas as ferramentas aplicadas no desenvolvimento desta Tese.
Este próximo capítulo também inclui a descrição do receptor e dos ligantes utilizados, assim como das simulações de docagem molecular que originaram todos os dados.
Em este capítulo são descritos todos os materiais e métodos utilizados no desenvolvimento deste trabalho.
A primeira seção descreve todas as ferramentas direta e indiretamente aplicadas no presente trabalho, que incluem:
O workflow científico FReDoWS desenvolvido no LABIO e utilizado para execução de simulações de docagem molecular com o receptor flexível, o software de docagem molecular AutoDock3.
0.5, o software de dinâmica molecular AMBER9, o software para análise de interação receptor-ligante LigPlot, o sistema gerenciador de banco de dados PostGreSQL, a linguagem de programação Python e a plataforma para mineração de dados WEKA.
As seções seguintes apresentam a descrição do receptor e ligantes utilizados assim como as simulações por dinâmica molecular e docagem molecular que originaram todos os dados utilizados nesta Tese.
Por fim são feitas as considerações finais deste capítulo.
Os dados descritos neste capítulo e utilizados como material na Tese estão, em parte, em artigos publicados durante o desenvolvimento deste trabalho:
O artigo completo publicado no LNBI-LNCS durante o evento Brazilian Symposium on Bioinformatics de 2007;·
os resumos publicados e artigos submetidos ao x-meeting 2010 e em avaliação (2 rodada) para publicação no BMC Bioinformatics;
O resumo publicado e apresentado durante o evento ISCB-Latin America em 2010;
Ferramentas utilizadas no desenvolvimento deste trabalho Para a execução automática de simulações de docagem molecular, considerando o modelo FFR foi desenvolvido um workflow científico, o qual denominamos de FReDoWS (Flexible-Receptor Docking Workflow System).
O artigo descrito em é uma extensão significativa do trabalho introdutório apresentado em e que inclui uma etapa de seleção de conformações do receptor a serem utilizadas nas simulações de docagem molecular.
A Figura docagem molecular.
Em o modelo do FReDoWS cada tipo de atividade do workflow corresponde a uma cor diferente.
As atividades em verde-escuro são executadas por o usuário, as atividades em roxo representam subflows, sub-processos compostos por atividades, transições e aplicações próprias.
As atividades em verde-claro são executadas por o sistema sem intervenção do usuário e podem invocar uma ou mais aplicações externas.
As atividades em rosa são utilizadas para sincronização no modelo e para transações com condições mais complexas, nenhuma ação é efetivamente executada por elas.
As principais etapas da execução do FReDoWS são resumidas a seguir:
Preparação dos arquivos da macromolécula.
Em esta etapa, a DM já foi executada e os arquivos do receptor são preparados para uso na docagem molecular.
No caso de os programas utilizados por o FReDoWS, essa etapa inclui a execução do módulo Ptraj do software AMBER (que será detalhado a seguir) para transformar os arquivos resultados da DM em arquivos PDB;
Preparação do ligante:
Em essa etapa o ligante é posicionado onde se deseja que seja sua conformação inicial nas simulações de docagem molecular;
O usuário seleciona o tipo de experimento, se é Exaustivo ou Seletivo.
Em o experimento Exaustivo, todas as conformações do modelo FFR são utilizadas.
Em o experimento Seletivo é estabelecido um critério de seleção de conformações, onde o total a ser utilizado na docagem é estabelecido por o usuário.
Essa etapa de seleção será detalhada no Capítulo de Trabalhos Relacionados;
Independente de ser Exaustiva ou Seletiva, a execução dos experimentos de docagem é realizada da mesma forma, por o subflow Execute Docking.
Como para isso utilizamos o programa AutoDock3.
0.5, essa etapa de execução do workflow será descrita na próxima seção.
O AutoDock3.
0.5, programa de docagem molecular empregado neste trabalho, consiste num conjunto de programas de código e acesso livres (Addsol, AutoTors, AutoGrid e AutoDock) desenvolvidos por Olson Para a predição do modo de ligação de ligantes com receptores macromoleculares.
Ele combina um método baseado em malhas (do inglês, grid) para a avaliação da energia do complexo, pré-calculando as energias de interação receptor-ligante par-a-par e utilizando estas para otimizar o cálculo da energia final a cada iteração.
As principais etapas envolvidas na execução de docagem molecular com o AutoDock3.
0.5 são:
Preparação dos arquivos do receptor e do ligante.
Para a preparação das cargas atômicas parciais do receptor, considerando que o mesmo já é um arquivo.
PDBQ (arquivo.
PDB com cargas), é executado o módulo do AutoDock chamado addsol.
O addsol especifica parâmetros de solvatação atômica para cada átomo da macromolécula, gerando um arquivo.
Pdbqs. A preparação do ligante, considerando que o mesmo já esteja num formato.
MOL2, compreende a execução do módulo deftors, onde podem ser definidos os ângulos de torção do ligante.
A segunda etapa consiste na execução dos módulos mkgpf3 e mkdpf3, responsáveis por gerar os arquivos de parâmetros para os módulos Autogrid e Autodock respectivamente.
É executado o módulo Autogrid.
Esse módulo define uma malha de afinidade para cada um dos tipos de átomos do ligante:
Tipicamente carbono, oxigênio, nitrogênio, enxofre e hidrogênio, mais uma malha de potencial eletrostático.
Essa malha corresponde a uma matriz 3D de pontos igualmente espaçados, centrado em alguma região de interesse do receptor em estudo (Figura 3.2 (a)).
Exemplo de uma malha de afinidade num receptor.
Em esta figura trata- se do receptor InhA, onde Cada ponto dentro de a malha armazena a energia potencial de um átomo de prova em relação a todos os átomos no receptor.
Assim, durante a docagem esses valores de energia são utilizados para reduzir os cálculos que são necessários para se chegar ao valor final da FEB.
Em a última etapa é então executado o módulo Autodock.
Para a execução do Autodock é escolhido um dos 3 algoritmos descritos a seguir.
Adicionalmente, independente da função de busca utilizada para encontrar o melhor encaixe receptor-ligante, é calculada a FEB do complexo conforme detalhado em.
Algoritmo SA (Simulated Anneling):
Em a exploração conformacional utilizando a técnica de Monte Carlo com SA, a proteína permanece estática durante a simulação e a molécula do ligante faz um movimento aleatório dentro de o sítio de ligação.
A cada passo da simulação é aplicada uma modificação pequena e aleatória em cada grau de liberdade do ligante.
Esta modificação resulta numa nova configuração, cuja energia é avaliada utilizando a malha de afinidade previamente calculada por o AutoGrid.
Esta nova energia é comparada com a energia da etapa anterior.
Se a nova energia for menor, a nova configuração é imediatamente aceita.
Se a nova energia for maior, o resultado é tratado probabilisticamente em função de a temperatura.
Algoritmo Ga (Genetic Algorithm):
Em a aplicação de Ga para a docagem molecular, uma certa posição do ligante no sítio ativo do receptor (definida por um conjunto de valores que descrevem a translação, orientação e conformação de este), determinam seu estado.
Cada variável de estado do ligante corresponde a um gene.
O estado do ligante corresponde ao genótipo, onde suas coordenadas atômicas são seu fenótipo.
O Ga do AutoDock3.
0.5, então inicia sua execução por a criação aleatória de uma população com um número de indivíduos definidos por o usuário.
A criação da população inicial é seguida de ciclos sucessivos de gerações até que o número máximo de gerações ou o número máximo de avaliações de energia seja alcançado.
A avaliação de energia é calculada baseada numa função de energia resultante da interação receptor-ligante.
Durante esse processo, pares aleatórios de indivíduos passam por um processo de crossover (recombinação), em o qual os novos indivíduos herdam os genes dos pais e/ ou mutação randômica, em a qual os genes sofrem uma alteração aleatória.
Algoritmo LGA com um algoritmo de busca local (do inglês, Local Search -- Ls).
Em o LS são aplicadas pequenas alterações rotacionais e conformacionais, atuando no genótipo.
Em o LGA as adaptações provenientes do Ls são como adaptações em função de o ambiente, sendo então possíveis de serem transferidas para as próximas gerações se apresentarem uma melhor avaliação de energia.
Em o AutoDock é executado um determinado número de runs, que correspondem ao número de diferente tentativas que serão executados por o algoritmo selecionado, de forma a encontrar a melhor energia de interação entre o receptor-ligante (FEB).
De essa forma, como resultado da execução de uma simulação de docagem utilizando o Autodock3.
0.5 tem- se um arquivo de saída que lista as coordenadas do ligante, FEB, RMSD (com relação a posição inicial) e outros valores para cada uma das tentativas executadas (runs), ordenadas ascendentemente por FEB.
O Amber é uma coleção de programas que permitem aos seus usuários a execução de simulações por DM de biomoléculas.
Para a execução de uma simulação por DM com o AMBER são necessários:
As coordenadas cartesianas de todos os átomos do sistema, a topologia (determina a conectividade entre os átomos, seus nomes, etc.), o campo de força e a lista de comandos, que determinarão os parâmetros da simulação.
O Amber não é um programa de acesso livre e código aberto.
Entretanto, alguns de seus módulos são, como por exemplo, seus campos de força, o módulo Ptraj, etc..
A DM que deu origem as conformações utilizadas neste trabalho foi executada com o AMBER6.
Os arquivos resultantes dessa DM foram processados com o módulo Ptraj do AMBER9, que transforma esses arquivos em.
PDBs. Em esta versão do Ptraj, foram incluídos algoritmos de agrupamento de estruturas e os mesmos foram aplicados neste trabalho, conforme será detalhado no Capítulo 8.
O LigPlot é um programa que gera esquemas da interação entre um receptor e um ligante a partir de um arquivo PDB.
Sua saída é um arquivo PostScript (ps) que contém uma representação das interações do complexo, incluindo ligações de hidrogênio e contatos hidrofóbicos.
Em é descrito o algoritmo deste programa.
O LigPlot, assim como o AutoDock3.
0.5 é um programa de código e acesso livre.
O LigPlot se utiliza dos contatos calculados previamente por o programa HBPLUS.
Então, como entrada para a execução do LigPlot são fornecidos:
Um arquivo.
PDB único do receptor e ligante, os arquivos gerados por o HBPLUS (arquivo.
Hhb que contém a lista de ligações de hidrogênio, o arquivo.
Nnb com a lista de contatos hidrofóbicos e um arquivo de configuração com os parâmetros para a plotagem).
Como saída, os principais arquivos gerados por o LigPlot são (Figura 3.3):
LigPlot. Ps -- Figura 3.3 (a).
Em esse arquivo, as ligações de hidrogênio são indicadas por linhas tracejadas entre os átomos envolvidos.
Os contatos hidrofóbicos são marcados nos resíduos por um desenho de um arco com traços ao redor, em direção a o átomo do ligante que faz esse contato.
Em o exemplo apresentado na figura foi executado o LigPlot considerando como entrada um arquivo.
PDB com um receptor, a enzima InhA e com um ligante, o substrato THT presente na estrutura cristalográfica desta enzima.
LigPlot. Pdb:
Contém o.
PDB que está no arquivo.
Ps (contém o ligante e os resíduos do receptor que estabelecem contato com o mesmo);
LigPlot. Hhb -- Figura 3.3 (b):
Arquivos no formato LigPlot que contém as ligações de hidrogênio do.
Hhb original.
Em Figura 3.3 (b) nota- se que há somente uma ligação de hidrogênio encontrada e a mesma é entre átomos do próprio THT (por essa razão não aparece no ligplot.
Ps); LigPlot.
Nnb -- Figura 3.3 (c):
Arquivos no formato LigPlot que contém os contatos hidrofóbicos do.
Nnb. A Figura 3.3 (c) mostra parte desse arquivo, onde se pode ver por exemplo, que o THT estabelece 5 contatos com o resíduo do receptor Ile (Isoleucina) 214 (na última coluna desse arquivo é listada a distância entre os átomos do contato).
Esses contatos podem ser verificados na Figura 3.3 (a);
LigPlot. Sum:
É um arquivo que sumariza as informações dos arquivos.
Hhb e.
Nnb, conforme mostra a Figura 3.3 (d).
Em este arquivo estão listados somente os totais de contato de cada resíduo do receptor, sem mostrar qual é exatamente o átomo que estabeleceu o contato e a respectiva distância do mesmo.
O LigPlot foi aplicado neste trabalho para a determinação dos contatos entre o receptor e seu substrato, o que foi utilizado como entrada para auxiliar no agrupamento das estruturas obtidas da DM, conforme será detalhado no Capítulo 8.
O PostgreSQL é um sistema gerenciador de banco de dados (SGBD) baseado no POSTGRES desenvolvido por o Departamento de Ciência da Computação da Universidade da Califórnia em Berkeley.
O PostgreSQL é desenvolvido em código fonte aberto, com acesso livre, e suporta grande parte do padrão SQL além de funcionalidades como:
Comandos complexos, chaves estrangeiras, gatilhos, visões, etc..
O Banco de Dados FReDD, que será descrito no Capítulo 5, foi desenvolvido utilizando esse SGBD.
A linguagem de programação Python foi adotada neste trabalho para o desenvolvimento de todos os scripts para tratamento dos dados, tanto para inserir dados no banco de dados desenvolvido, quanto para gerar as entradas para os algoritmos de mineração.
Também foi utilizada na escrita de programas para o processamento dos resultados obtidos, em diferentes etapas do desenvolvimento desta Tese.
Python é uma linguagem de programação de alto nível, interpretada, imperativa e de tipagem dinâmica e forte.
É simples de aprender, orientada a objetos e que contém estruturas de dados de alto nível.
Por suas características, é a linguagem ideal para o desenvolvimento de aplicações rápidas para diferentes áreas e plataformas.
Para o desenvolvimento de alguns scripts, foi utilizada a biblioteca Biopython.
Essa biblioteca é composta por um conjunto de funcionalidades para biologia computacional, como por exemplo suporte a dados de diferentes formatos (Fasta, GenBank, saída do BLAST, etc.), interface com o BLAST, ClustalW, entre outras.
O Waikato Environment for Knowlodge Analysis -- WEKA foi criado com a tarefa de unir diferentes algoritmos de aprendizagem de máquina numa plataforma única.
Em o início dos anos 90, esses algoritmos eram feitos para uso em diferentes plataformas e operavam com uma variedade de formatos de dados.
Sendo assim, o WEKA, escrito em código aberto, fornece um conjunto de algoritmos de aprendizagem, e permite também que pesquisadores desenvolvam novos com o apoio de uma infra-estrutura para manipulação dos dados e os incluam neste ambiente.
Atualmente, o WEKA tem ampla aceitação nos meios acadêmicos e empresariais e foi utilizado durante todo o desenvolvimento desta Tese para a execução dos experimentos de mineração de dados (detalhados nos próximos capítulos).
Receptor Investigado:
Proteína InhA de Mycobacterium tuberculosis A proteína em estudo é a InhA, a enzima 2-trans-enoil ACP (CoA) Redutase de Mycobacterium tuberculosis (Mtb) cuja estrutura 3D está apresentada na Figura 3.4.
A InhA caracteriza- se por uma folha de 7 fitas-paralelas, contornadas por 8 hélices-, conectadas por alças e voltas, formando o sítio de ligação desta enzima com a coenzima NADH (nicotinamida adenina dinucleotídeo, forma reduzida), em azul na Figura 3.4.
A InhA é uma enzima importante no mecanismo de ação da tuberculose pois é responsável por a biossíntese de ácidos graxos, um importante componente da parede celular da micobactéria, e, consequentemente, uma das estruturas essenciais para a sua sobrevivência.
Por esse motivo, desperta atenção especial como alvo atraente para o desenvolvimento de novos fármacos para a tuberculose.
Em vermelho as hélices.
Em amarelo as fitas.
Em cinza as voltas e alças.
Em azul, na forma Licorice (software VMD), a coenzima NADH.
O ligante Isoniazida Pentacianoferrato (IPF) -- Figura 3.6 é composto por 28 átomos antes da preparação do ligante para a docagem molecular e 24 átomos, após.
Em este trabalho uma molécula inibidora da InhA, desenvolvida por Oliveira e colaboradores para ser um inibidor sem ativação prévia.
Consiste na molécula de Isoniazida (INH) acrescido de um grupamento pentacianoferrato com o centro metálico acoplado.
Para a docagem com esse ligante flexível foram selecionados três ângulos de rotação, entre os átomos N3 e Fe, entre C8 e C13 e entre N14 e N16 (ligações em verde na Figura 3.6).
O ligante Triclosano (TCL) (código Zinc:
2216), Figura 3.7, é uma molécula pequena, formada por 24 átomos (antes da preparação para a docagem molecular, após, o ligante tem 18 átomos).
De acordo com o banco de dados Zinc, a molécula apresenta dois ângulos de rotação, o primeiro entre os átomos de carbono C3 e O2 e o segundo entre os átomos O2 e C7 (ligações em verde na Figura 3.7).
O AutoDock detecta um terceiro ângulo de rotação entre os átomos C4 e O1 mas este não é tratado quando este ligante é considerado como flexível nas simulações de docagem, sendo considerados somente os outros 2 ângulos mencionados.
A Etionamida ou ETH é uma pequena molécula composta por 21 átomos e descrita na Figura 3.8.
Simulações por a DM do Receptor InhA Os estudos de simulação por DM da InhA de Mtb, que originaram as conformações utilizadas nesse trabalho, foram realizados com a InhA complexada com a coenzima NADH utilizando o software AMBER6.
0 por um período de 3.100 ps e estão descritos na Figura 3.9, onde em (a) a flexibilidade é evidenciada por as cadeias laterais de 3 conformações diferentes, a cristalográfica e as estruturas nos instantes 1.000 e 2.000 ps da simulação e em (b) tem- se diferentes conformações da InhA, onde cada cor também representa a conformação num instante diferente no tempo.&amp;&amp;&amp;
Simulações de Docagem molecular Durante o desenvolvimento desta Tese foram executados 2 conjuntos de simulações de docagem molecular, denominados Experimentos-Fase 1 e Experimentos-Fase 2.
Os resultados dos Experimentos Fase 1 foram os utilizados durante a maior parte deste estudo.
Porém, ao executar a última parte deste trabalho foi necessária uma modificação na forma de extrair as conformações da DM, o que causou uma modificação nas mesmas.
Por esse motivo, as simulações de docagem precisaram ser reexecutadas, conforme descrito na Subseção a seguir.
Em magenta, ciano e amarelo 10 cadeias laterais de aminoácidos da proteína cristalográfica e nos instantes de tempo 1.000, 2.000 ps da simulação por DM, respectivamente. (
b) Em laranja a estrutura cristalográfica, em ciano a estrutura média de 0 a 500 ps, em azul de 550 a 1.000 ps, em magenta de 1.050 a 1.500 ps e em verde e 1.550 a 2.000 ps..
Em a tabela 3.1, as linhas 1, 3, 5 e 7 correspondem aos resultados considerando os 10 runs de execução da docagem para os ligantes NADH, PIF, TCL e ETH respectivamente.
As linhas 2, 4, 6 e 8 mostram os resultados considerando somente o run de melhor FEB durante a execução da docagem de cada arquivo de saída do AutoDock3.
0.5 para os ligantes NADH, PIF, TCL e ETH respectivamente.
A primeira coluna da tabela descreve o tipo de resultado (se é considerando todos os runs ou somente o de melhor FEB).
Em a segunda coluna está a média de FEB e o desvio padrão em kcal/ mol, considerando somente os valores de FEB negativos.
O total de simulações de docagem válidas descritos na terceira coluna corresponde ao total de simulações que convergiram para um valor de FEB negativo.
Em a quarta coluna é descrito o melhor valor de FEB para cada simulação (FEB mínima).
As colunas 5 e 6 contém os valores de FEB máximo e moda respectivamente.
Os valores de FEB mínima, máxima e moda estão relacionados somente com os resultados de docagem válidos.
É importante ressaltar que o valor de FEB mínima para todas as simulações não se encaixa dentro de o limite determinado por a média da FEB e desvio padrão.
Isso ocorre porque a FEB apresenta em seu histograma, para ambos os ligantes, um comportamento bimodal, ou seja, há um valor de moda em o qual a maioria dos demais valores de FEB se concentra, e um segundo valor de moda, onde algumas instâncias tem seu valor de FEB.
Como a média foi calculada considerando todas as instâncias com valor de FEB negativas há essa diferença no seu desvio padrão que não inclui então o valor da melhor FEB.
Em esse conjunto de simulações os melhores resultados foram do ligante NADH, principalmente se somente o melhor run for considerado, sendo também para esse ligante a maior variação de FEB.
O resultado para o ligante ETH apresentou a pior média de FEB (6,4 Kcal/ mol), porém ainda aceitável, conforme mostra a discussão apresentada em.
Para a etapa final deste trabalho foram reexecutadas as simulações de docagem molecular para o mesmo receptor e os mesmos 4 ligantes.
Essas simulações precisaram ser reexecutadas pois durante a utilização dos algoritmos de agrupamento, a DM precisou ser sobreposta na primeira estrutura (até o momento utilizávamos as estruturas no mesmo sistema de referência, mas não exatamente sobrepostas).
Por esse motivo, as conformações foram alteradas em relação as utilizadas até o momento e armazenadas no banco de dados FReDD (descritos no Capítulo 3).
Além de o mais, havia a necessidade de avaliação dos resultados de docagem molecular com o ligante também flexível e com o algoritmo do AutoDock3.
0.5 mais utilizado, o LGA.
As novas docagens também utilizaram o AutoDock3.
0.5 e foram executados com o seguinte protocolo (descrito em detalhe em):
O algoritmo de execução do AutoDock3.
0.5 selecionado foi o LGA;
Considerou- se 25 runs de execução com 500 mil avaliações em cada run;
Com exceção do NADH, os ligantes foram considerados flexíveis durante a execução do AutoDock3.
0.5, sendo:
­ TCL com 2 ângulos de torção:
Entre os átomos C3_ O2 e O2_ C7;
A Tabela 3.2 resume os resultados dos Experimentos Fase 2.
A primeira coluna mostra o ligante;
A segunda coluna contém a média e desvio padrão do valor de FEB para o resultado de melhor FEB, a terceira coluna descreve o total de simulações válidas de cada ligante (aqueles cujo valor de FEB é negativo) e a última coluna apresenta o valor de FEB mínima para cada ligante.
Analisando a Tabela 3.2 é possível concluir que nesse segundo conjunto de simulações de docagem, os melhores resultados foram com o ligante TCL, sendo os piores para o ligante NADH, que assim como para a Fase 1, na Fase 2 também apresenta maior variação de FEB.
Comparando as Tabelas 3.1 e 3.2 podese concluir que há diferenças entre os resultados, mesmo que o modelo FFR e ligantes sejam os mesmos para ambos experimentos.
Essas diferenças ocorrem principalmente devido a os experimentos terem sido executados com algoritmos diferentes (SA e LGA), que conforme descrito na Seção Experimentos Fase 2.
Considerações Finais Este capítulo apresentou as principais ferramentas utilizadas no desenvolvimento desta Tese assim como a descrição do receptor e ligantes considerados e as simulações por DM e docagem molecular que originaram todos os dados empregados.
Conforme apresentado na seção Ferramentas, com exceção do software utilizado na geração da DM do receptor de estudo (AMBER), todas as demais ferramentas são software livre e com código aberto que executam no sistema operacional Linux.
Em o trabalho de Schroeder Foi demonstrada a flexibilidade do receptor InhA (Figura alvo da Tuberculose, tornaram o estudo desse receptor interessante.
Além de o mais, outros trabalhos do LABIO mostraram que o modelo FFR da InhA se reflete nos resultados de docagem molecular, conforme discutido no final do Capítulo 2, o que também justifica o uso deste tipo de receptor no presente trabalho.
Em o próximo capítulo serão apresentados os conceitos sobre a área de Mineração de Dados, descrevendo as técnicas de mineração aplicadas neste trabalho:
Classificação, regressão e agrupamento e os respectivos algoritmos utilizados.
Esse capítulo é uma continuação do anterior que descreve os Materiais e Métodos.
Por compreender uma série de itens e para facilitar a leitura o seu conteúdo foi apresentado em separado.
São abordados os conceitos de Mineração de Dados e das principais etapas do processo de descoberta de conhecimento em banco de dados.
Além de o mais, são descritas as técnicas de mineração de dados utilizadas, assim como, os algoritmos de cada técnica aplicados nesta Tese.
Para finalizar esse capítulo, são apresentadas as considerações finais e uma breve descrição do capítulo seguinte.
Em essas considerações finais é apresentado o primeiro trabalho desenvolvido utilizando técnicas de mineração de dados aplicadas à dados de docagem molecular com o modelo FFR.
Esse trabalho foi o ponto de partida de todos os resultados apresentados nos capítulos seguintes desta Tese, tendo sido publicado:
Como resumo na conferência x-meeting em 2007 onde ganhou o prêmio de 3 melhor pôster do evento;
Como artigo completo no LNBI-LNCS durante o evento Brazilian Symposium on Bioinformatics de 2008.
Descoberta de Conhecimento em Bancos de Dados e Mineração de Dados A mineração de dados é uma parte integral da descoberta de conhecimento em bancos de converter dados crus em informação útil, como pode ser visto na Figura 4.1.
O processo de KDD consiste de uma série de passos de transformação, desde o pré-processamento dos dados até o pós-processamento dos resultados da mineração de dados e compreende as seguintes etapas:
O pré-processamento é a transformação dos dados de entrada num formato apropriado para uma análise subsequente.
Os passos envolvidos no pré-processamento incluem a integração dos dados de múltiplas fontes, a limpeza dos dados para remoção de ruídos e dados duplicados e a seleção de registros e características que serão relevantes na etapa de mineração de dados.
Para Tan, o pré-processamento é a etapa mais trabalhosa e que ocupa mais tempo no processo de KDD.
A etapa de mineração de dados cujas técnicas podem ser utilizadas para investigar grandes bancos de dados como o objetivo de encontrar padrões previamente desconhecidos, úteis e relacionados entre os dados.
Um conjunto de fatores motivou o desenvolvimento de técnicas para a mineração de dados, como, por exemplo, a necessidade diária de se lidar com bancos de dados de tamanhos na ordem de gigabytes ou terabytes que compreendem centenas ou milhares de atributos, como os bancos de dados de expressão de genes, sendo necessárias técnicas para o tratamento dessa alta dimensionalidade.
Outros problemas dizem respeito à complexidade e heterogeneidade dos dados, como os de estrutura sequencial e tridimensional do DNA ou proteínas, e aos dados distribuídos que precisam ser analisados mesmo não estando armazenados fisicamente em conjunto.
O pós-processamento é a etapa que integra os resultados da mineração de dados a um sistema de suporte à decisão, permitindo que somente os resultados úteis sejam incorporados a esse sistema.
De acordo com Tan, um exemplo dessa etapa consiste na visualização dos resultados da mineração de dados ou na aplicação de medições estatísticas para desconsiderar resultados não legítimos.
Adaptado de Han e Kamber Atualmente, muitas pesquisas na área de mineração de dados estão voltadas para o desenvolvimento de ferramentas que lidam com diversos métodos escaláveis e eficientes para procurar padrões de interesse em grandes bancos de dados.
Ao mesmo tempo em que o progresso na biologia e na ciência médica tem aumentado a necessidade de se lidar com o acúmulo de grandes quantidades de dados.
A questão levantada por Han diz respeito a como integrar essas duas áreas:
Mineração de dados e Bioinformática, permitindo que a mineração em dados biológicos seja realizada com sucesso.
Métodos avançados, eficientes e escaláveis de mineração de dados ainda precisam ser desenvolvidos, e, dentro desse contexto, os seguintes tópicos são citados como possibilidades de pesquisa nessa área:
Procura por similaridade e comparação de dados biológicos:
Dados biológicos geralmente apresentam ruídos e dados faltantes, sendo importante o desenvolvimento de algoritmos de mineração que tratem esse tipo de problema;
Análise de associações:
Identificação de recorrência de sequências biológicas (de proteínas, por exemplo) ou outros padrões relacionados.
Métodos de análise de associação e correlação podem auxiliar na determinação dos tipos de genes ou proteínas que podem ocorrer em amostras alvo.
Essas análises facilitam o descobrimento de grupos de genes ou proteínas e o estudo da interação e relação entre eles;
Padrões frequentes em grupos:
A maioria dos algoritmos que realizam agrupamento são baseados em distâncias Euclidianas ou densidade.
Entretanto, dados biológicos geralmente consistem de vários atributos que formam um espaço de dimensões muito grande, sendo necessário descobrir padrões que permitam um correto agrupamento desses dados;
Análise de caminhos:
Unir informações sobre genes e proteínas em diferentes estágios da evolução de uma doença, pois os genes e proteínas influenciam cada um desses estágios de evolução.
Se uma sequência de atividades dentro de cada estágio for estabelecida é possível desenvolver fármacos específicos com alvo nos diferentes estágios separadamente;
Visualização de dados e mineração de dados visual:
Estruturas complexas de sequências de genes e proteínas são mais bem representadas em grafos, árvores, cubos e cadeias por vários tipos de ferramentas de visualização.
Essa forma de visualização dos dados facilita o entendimento de padrões, descobrimento de conhecimento e exploração de dados interativamente;
Preservação da privacidade dos dados biológicos:
Embora a troca de informações seja importante, hospitais e institutos de pesquisa permanecem relutantes em disponibilizar seus dados biológicos, sendo por isso importante desenvolver métodos de mineração de dados que preservem a privacidade dos dados.
Pré-processamento Segundo Wang, dados biológicos são gerados, na maioria das vezes, em locais geograficamente diferentes, com uma variedade de recursos e por a aplicação de diferentes técnicas.
Sendo assim, para extrair informações úteis, esses dados precisam ser agrupados, caracterizados e limpos.
Essa etapa de pré-processamento dos dados pode consumir muito tempo se for necessário pesquisar muitos bancos de dados distribuídos para garantir a qualidade dos dados.
O principal objetivo do pré-processamento é garantir a qualidade dos dados e, de acordo com Tan, suas principais abordagens envolvem:
Agregação: É a combinação de dois ou mais objetos num objeto único permitindo que os conjuntos de dados sejam reduzidos, tornando o processamento dos mesmos mais rápido.
O maior problema da agregação é uma possível perda de detalhes importantes.
Em atributos quantitativos, corresponde a operações de soma ou média, e em atributos qualitativos, corresponde a valores que podem ser resumidos ou organizados num conjunto.
Amostragem: Seleciona um subconjunto do banco de dados, assim, ao utilizar somente esse subconjunto, são obtidos os mesmos resultados com um custo computacional reduzido.
Redução de dimensionalidade:
Em Bioinformática, essa abordagem é muito importante, já que muitos bancos de dados biológicos apresentam centenas e até milhares de atributos.
A principal vantagem em reduzir a dimensionalidade dos dados está no fato de que a maioria dos algoritmos de mineração de dados trabalha melhor se o número de atributos não for muito grande, o que permite a geração de modelos mais compreensíveis.
Seleção de um subconjunto de atributos:
É outra maneira de reduzir a dimensionalidade dos dados.
A eliminação de atributos redundantes (cujas informações já estão contidas em outros atributos) ou irrelevantes (dependendo da análise a ser realizada, alguns atributos não precisam ser considerados) torna as atividades de classificação ou agrupamento dos dados mais eficiente.
Existem três abordagens para esse tipo de seleção dos dados:
Ou está inserida na etapa de mineração de dados, ou acontece antes como uma tarefa independente, ou é tratada como uma caixa preta dentro de a etapa de mineração de dados.
Criação de atributos:
Segundo Tan, muitas vezes é interessante criar, a partir de os atributos originais, um novo conjunto de atributos que capture informações importantes dos conjuntos de dados de forma mais eficaz.
E, se esse número for menor que o original, haverá uma redução de dimensionalidade.
Discretização e binarização:
Muitos algoritmos de mineração de dados, principalmente para determinação de padrões de associação entre os atributos precisam que os dados sejam binários, enquanto que algoritmos para classificação dos dados precisam que os atributos sejam categóricos, assim, essas transformações são muito utilizadas.
Transformações de variáveis:
Refere- se a transformações aplicadas a todos os valores de uma variável, ou seja, para cada objeto, a transformação é aplicada ao valor da variável do objeto.
Uma técnica muito utilizada de transformação é a normalização.
Técnicas de Mineração de Dados na Figura 4.2 é apresentado o banco de dados para o armazenamento de informações sobre a taxonomia de alguns indivíduos, como por exemplo, a ordem e família a que pertencem juntamente com algumas de suas características.
A partir desse banco de dados, por meio de a aplicação de técnicas de mineração de dados, pode- se analisar sua árvore genealógica, agrupar indivíduos da mesma espécie, fazer uma predição da família a que pertencem com base nas suas características e antecedentes entre outras possibilidades, sendo esses somente alguns exemplos de informações que podem ser extraídas de bancos de dados a partir de o uso de técnicas como associação, classificação e agrupamento.
Geralmente as tarefas de mineração de dados são divididas em duas categorias:
Técnicas Preditivas:
O objetivo desse tipo de técnica é predizer um atributo baseado na análise dos valores de outros atributos.
Existem dois tipos de modelos de predição:
Classificação e regressão.
Técnicas Descritivas: Têm por objetivo encontrar padrões entre os dados (correlações, grupos, anomalias, etc.) que explicam alguma relação entre os mesmos.
Geralmente esse tipo de técnica necessita de pós-processamento para validar e explicar os resultados.
As técnicas de mineração de dados descritivas são:
Associação, agrupamento e sumarização.
A seguir são apresentadas as técnicas de mineração de dados aplicadas neste trabalho:
Classificação, Regressão, Agrupamento e Associação.
Para Wang, dados biológicos consistem de múltiplos atributos e a relação/ interação entre esses atributos pode ser muito complicada de ser estabelecida.
Em Bioinformática, classificação é uma das ferramentas mais populares e utilizadas para entender a relação entre características de vários objetos, pois trata- se de um processo que encontra propriedades comuns entre um conjunto de dados e os organiza em diferentes classes, de acordo com um modelo de classificação.
Para Tan, uma técnica de classificação, ou classificador, pode ser definida como uma abordagem sistemática para construir modelos de classificação a partir de dados de entrada onde aplica- se um algoritmo de aprendizado para induzir o modelo que melhor identifica as relações entre atributos.
Os dados de entrada numa tarefa de classificação são um conjunto de registros, ou instâncias, caracterizado por uma tupla (x, y), onde x é o conjunto de atributos e y o atributoclasse.
A classificação consiste então na tarefa de aprender uma função-alvo f que mapeie cada conjunto de atributos x para um dos atributos-classe y, que deve ser categórico.
Essa função-alvo f é um modelo de classificação.
Exemplos de abordagens de classificação incluem árvores de decisão, classificadores baseados em regras, redes neurais, redes Bayesianas, etc..
Entre essas diferentes abordagens para classificação, nesse trabalho serão apresentados somente classificadores baseados em árvores de decisão.
Segundo Chen, esse método de classificação é um método de aprendizado supervisionado que constrói árvores de decisão a partir de um conjunto de exemplos.
A qualidade da árvore depende da exatidão da classificação e do tamanho da árvore.
Cada árvore de decisão é composta por:
Nodo raiz:
Não apresenta arestas chegando e zero ou mais arestas saindo;
Nodos internos:
Cada um possui uma aresta chegando e duas ou mais saindo;
Folhas ou nodos terminais:
Cada nodo-folha possui uma aresta chegando e nenhuma saindo.
Esse tipo de modelo de aprendizagem, uma árvore de decisão, é composta por um nodo inicial, ou nodo raiz, a partir de o qual vão sendo associados os nodos internos que contêm condições que testam os valores dos atributos, enquanto que os nodos-folhas estão associados a um determinado valor do atributo classificador (Figura 4.3).
Sendo assim, a classificação de um registro de teste inicia por o nodo raiz, onde aplica- se um teste relacionado ao valor do atributo associado ao nodo, dependendo desse valor, determina- se o próximo nodo a ser analisado.
A partir desse outro nodo interno, para o qual uma nova condição é avaliada, é definido qual é o nodo no próximo passo, e assim acontece até se chegar a um nodo terminal ou folha, que definirá a que classe determinado registro pertence.
Esses modelos seguem uma aproximação do tipo divisão-e-conquista uma vez que a medida que se vai percorrendo a árvore em direção as folhas, o número de possibilidades de resolução do problema vai diminuindo até se chegar à uma única solução, o que torna a classificação correta.
Em o exemplo da Figura 4.3 é apresentada uma árvore de decisão para indicar qual é a classe de FEB (atributo classe:
Bom, Regular ou Ruim) que determinado experimento de docagem molecular apresenta dependo da distância de alguns resíduos do receptor para o ligante (atributos preditivos).
Então, percorrendo essa árvore de decisão pode- se verificar, por exemplo, que se a distância do resíduo do receptor HIE 92 para o ligante for maior do que 9,61 Å, a FEB será muito ruim.
A partir de um mesmo conjunto de atributos há milhares de árvores que podem ser determinadas.
Porém, encontrar a árvore que melhor representa a relação entre os atributos tem um alto custo computacional.
De essa forma, têm sido desenvolvidos muitos algoritmos para resolver esse problema.
Geralmente esses algoritmos empregam uma estratégia que constrói uma árvore tomando uma série de decisões sobre qual atributo considerar para particionar os dados localmente ótimos.
Um desses algoritmos, o Algoritmo de Hunt, é a base de muitos algoritmos de árvores de decisão, incluindo o ID3, C4.
5 e o CART.
Em o algoritmo de Hunt, uma árvore de decisão cresce de uma forma recursiva por o particionamento de registros de treino em sucessivos subconjuntos.
Dado Dt, um conjunto de registros de treino, como:
Se todos os registros em Dt pertencem à mesma classe em y, então t é um nodo folha chamado yt;
Se Dt contém registros que pertencem à mais de uma classe, então uma condição de teste é aplicada a um atributo interior selecionado para particionar os registros em subconjuntos menores.
Um nodo filho é criado a cada resultado da condição de teste e os registros em Dt são distribuídos nos nodos filhos baseados nos resultados da condição de teste.
O algoritmo é então recursivamente aplicado para cada nodo filho.
Muitas considerações devem ser feitas por o algoritmo de árvore de decisão, que envolve etapas bem mais detalhadas, principalmente, no que diz respeito a escolher o atributo que vai dividir os dados e, dependendo do tipo de atributo (binário, nominal, ordinal ou contínuo) definem como a classificação deve ser feita.
Para avaliar os modelos gerados com árvores de decisão, quando não se tem um conjunto de teste disponível, uma abordagem comum é executar uma validação cruzada (do inglês, cross-validation).
Em a validação cruzada com 10 partições os dados são divididos randomicamente em 10 partes de tamanhos iguais, onde 9 das 10 partes são utilizadas para aprendizagem e 1 parte para teste.
Esse procedimento é repetido 10 vezes, onde a cada execução uma parte diferente do conjunto de entrada é utilizada como conjunto de teste.
Então, é calculada a média dos 10 valores das métricas estimados a cada execução para os conjuntos de teste e esses valores servem para estimar a performance dos modelos produzidos.
Para árvores de decisão algumas de as· Acurácia (Acc):
É a taxa de instâncias que foram classificadas corretamente durante o processo de validação cruzada.
Valores maiores de acurácia indicam modelos melhores;
Root-Mean Squared Error (RMSE):
Valores menores indicam modelos melhores· Mean Absolute Error (MAE):
Calcula a média da magnitude dos erros individuais, sem considerar seus sinais.
Valores menores indicam modelos melhores.
P recisão, p $= Recall, r $= F -- score, F $= rp r+ p A regressão é uma técnica de modelagem preditiva onde o atributo-classe é contínuo.
Modelos de regressão lineares e não-lineares são utilizados em várias áreas como biologia, medicina, agronomia, engenharias.
Como exemplos de aplicações para regressão pode- se citar:
A previsão de um índice na bolsa de valores, a projeção de vendas de uma empresa, a previsão de quantidade de precipitação, etc..
Segundo Han e Kamber análises de regressão podem ser aplicadas para modelar a relação entre uma ou mais variáveis independentes (variáveis preditoras) e uma variável dependente (que é uma variável contínua, chamada de variável resposta).
Este relacionamento pode ser por uma equação linear ou uma função não linear.
Em o contexto de mineração de dados, as variáveis preditoras são os atributos de interesse que descrevem as tuplas.
Esses valores, em geral, são conhecidos.
A variável resposta é a variável que se deseja predizer o valor.
Além de o atributo-alvo ser contínuo, a regressão também é uma boa alternativa quando todos os valores dos atributos preditores são contínuos.
A análise de Regressão Linear envolve a variável resposta y e uma simples variável preditora, x.
Em a forma mais simples de regressão, modela- se y como uma função linear de x:
A Regressão não-linear é aplicada quando não há uma dependência linear entre os dados.
Regressão polinomial é geralmente aplicada quando há somente uma variável preditora, o que é modelado adicionando- se termos ao modelo linear.
Aplicando- se transformações as variáveis pode- se converter um modelo não-linear num linear.
Além de regressão linear, regressão linear de múltiplas variáveis e regressão não-linear, há outras formas de regressão que não serão detalhadas neste trabalho, como por exemplo, regressão logística, regressão de Poisson, entre outros.
Além disso, há 2 tipos principais de árvores para predição numérica:
Árvores de regressão e árvores modelo.
Em a árvore de regressão cada nodo-folha armazena um valor contínuo, que é na verdade uma média dos valores dos atributos preditores das tuplas de teste.
Em as árvores modelo, cada nodo-folha contém um modelo de regressão que corresponde a uma equação com múltiplas variáveis para a predição do atributo.
O algoritmo M5P foi desenvolvido por Witten e Frank baseado no algoritmo M5 minute, uma implementação otimizada do clássico algoritmo M5.
Esse algoritmo pode manipular tarefas que envolvem arquivos de entrada de alta dimensionalidade e atributos que podem ser numéricos.
O resultado da execução do M5P são as chamadas árvores modelo (Exemplo na Figura 4.4).
Esse tipo de árvore é construída por o algoritmo, inicialmente, como uma árvore de decisão comum.
Uma vez que essa árvore básica foi obtida, o algoritmo concentra- se em podar a árvore.
A diferença da árvore modelo para outros tipos de árvores de decisão é que os nodos-folha são substituídos por um plano de regressão em vez de um valor constante, como mostra o exemplo de modelo linear a direita na Figura 4.4 que quantifica a contribuição dos atributos preditivos na determinação do atributo-alvo (FEB).
Em o modelo linear, cada parte da equação corresponde a um dos resíduos do receptor multiplicados por um valor constante que quantifica sua contribuição no valor final do atributo-classe (FEB).
Quando a árvore modelo é utilizada para predizer o valor para uma determinada instância-teste a árvore é percorrida da raiz para as folhas de maneira normal, utilizando os valores dos atributos das instâncias para tomar decisões de rota em cada nodo.
O nodo folha conterá um modelo linear ou Linear Mode (LM) que quantifica a contribuição de cada atributo na predição do atributoclasse.
A maioria das métricas de avaliação de árvores modelo são as mesmas das árvores de decisão, como por exemplo as métricas RMSE e MAE.
Porém, além de as métricas apresentadas anteriormente, é comum para árvores modelo se utilizar a métrica Correlação, que mede a correlação estatística entre a e p, onde valores maiores indicam modelos melhores.
Correlação (Correl) é definida por:
Correl $= Spa Sp Sa onde, Spa, Sp e Sa:
Spa $= Sp $= Sa $= i (pi -- p) (ai -- a) n-1 i (pi -- p) n-1 i (ai -- a) n-1 Segundo Chen, o processo de agrupar objetos físicos ou abstratos em classes de objetos similares é chamado agrupamento (do inglês, clustering).
Então os objetos chamados pontos dentro de um grupo (do inglês, cluster) tem alta similaridade entre si e alta dissimilaridade de objetos que estejam em outros grupos.
Essa medida de similaridade entre objetos é feita baseada nos valores dos atributos que os descrevem.
O agrupamento é uma técnica de aprendizado não supervisionado que permite a identificação de regiões densas e esparsas no espaço de um objeto, descobrindo padrões de distribuição e correlações interessantes entre os dados.
Os grupos apresentam diversas classificações, como por exemplo quanto a a maneira como são feitos, que pode ser por a semelhança dos dados ou por dados com mesmo significado (Tan Há na literatura muitos algoritmos de agrupamento.
Apesar de as características dos diferentes métodos de agrupamento muitas vezes se sobreporem, é importante que sejam organizados em diferentes categorias.
Em geral, segundo Han e Kamber, os métodos podem ser classificados em:
Dado k, o total de partições, esse método utiliza uma técnica de realocação interativa, que busca melhorar o particionamento movendo objetos de um grupo para outro.
Os algoritmos de agrupamento mais comuns que utilizam esse método são:
Means, onde cada grupo é representado por o valor médio dos objetos no grupo;
E K--medoid, onde os grupos são representados por um dos objetos localizados próximo de o centro do mesmo.
A seguir serão descritos os algoritmos de agrupamento utilizados.
A descrição dos mesmos é feita principalmente de acordo com suas implementações do trabalho de Shão, aplicados durante o desenvolvimento desta Tese.
Shão Classifica os algoritmos de agrupamento implementados em da seguinte forma:
Divisivo ou Top-Down:
Correspondem aos algoritmos hierárquicos divisivos;
Aglomerativo ou Botton-Up:
São os algoritmos hierárquicos aglomerativos;
Os diferentes algoritmos aglomerativos diferem entre si na forma de escolha de que pares de grupos serão combinados a cada execução.
Uma vantagem desse método é que as informações sobre como os grupos são combinados a cada execução pode ser salva, possibilitando que uma única execução possa desfazer algum passo já executado;
Refinado: Em os algoritmos definidos por como refinados, inicia- se por grupos aleatórios e iterativamente os membros de um grupo vão sendo refinados.
Em esta classificação estão os algoritmos classificados por como métodos de particionamento e métodos baseados em modelos;
O algoritmo Complete Linkage, um algoritmo hierárquico aglomerativo, utiliza a técnica conhecida como Farthest Neighbor ou vizinho mais distante.
A seguir é descrito o algoritmo básico de agrupamento hierárquico aglomerativo:
Algoritmo 4.1: Algoritmo Hierárquico Aglomerativo básico.
Inicia n grupos C, cada um com um ponto Calcule uma matriz M de proximidades de acordo com uma função F Repita Localize os grupos Ci e Cj com menor distância em M Construa um novo grupo Cij combinando Ci e Cj Atualiza M para refletir a proximidade entre o novo grupo Cij e os grupos originais Até que todos os pontos estejam num grupo ou que o número desejado de grupos seja alcançado Para o algoritmo Complete Linkage a distância entre 2 grupos é definida como a maior distância entre um par de pontos individuais, cada um pertencendo a um dos grupos.&amp;&amp;&amp;
De essa forma, a função de distância F entre o grupo combinado Cij e qualquer outro grupo Cl é calculada com a maior distância entre os pontos integrantes de ambos grupos:
F (Cl, (Ci, Cj) $= max (F (Cl, Ci), F (Cl, Cj) É um dos algoritmos hierárquicos aglomerativos mais simples, descritos por Johnson.
Esse algoritmo utiliza a técnica do vizinho mais próximo, onde, ao contrário de o algoritmo Complete Linkage, a distância entre um grupo e outro é definida como a menor distância entre um par de pontos pertencentes a cada um dos grupos.
A cada iteração os grupos mais próximos são combinados até que o número de grupos desejado seja atingido.
De essa forma, a função F do algoritmo F (Cl, (Ci, Cj) $= min (F (Cl, Ci), F (Cl, Cj) As estratégias de distância mínima e máxima, Edge e Complete Linkage representam dois extremos em termos de distância entre grupos.
Como a maioria dos procedimentos que envolvem esses extremos, esses algoritmos tendem a ser altamente sensíveis a outliers.
Por essa razão foi desenvolvido o algoritmo Average Linkage, com o objetivo de ser uma abordagem intermediária.
De essa forma, nesse algoritmo de agrupamento hierárquico aglomerativo a distância utilizada para combinar 2 grupos a cada execução é medida por a média de todas as distâncias calculadas entre pontos individuais dos 2 grupos, sendo então F do algoritmo 4.1 definida como:
F (Cl, (Ci, Cj) $= (F (Cl, Ci)+ F (Cl, Cj) O Linkage é um algoritmo de agrupamento aglomerativo muito similar ao Edge Linkage.
Porém no Linkage, a distância entre os grupos é definida como a distância entre os centróides dos mesmos.
Uma desvantagem desse algoritmo está relacionada ao tamanho dos grupos selecionados para serem combinados a cada execução.
Se esse tamanho dos grupos for muito diferente, o centróide do novo grupo será semelhante ao do grupo de maior tamanho, o que ocasionaria a perda de informações do menor grupo.
A função F do algoritmo 4.1 é então definida para o Linkage como, onde n corresponde ao total de pontos pertencentes a determinado grupo:
F (Cl, (Ci, Cj) $= nj ni nj ni F (Cl, Ci)+ F (Cl, Cj) -- F (Ci, Cj) ni+ nj ni+ nj (ni+ nj) 2 Esse é um algoritmo de agrupamento hierárquico aglomerativo derivado do algoritmo Cure.
Em esse algoritmo cada cluster é representado por 5 pontos representativos.
A escolha dos pontos representativos é feita selecionando os 5 pontos distantes ao máximo dentro de o grupo.
Em seguida, os pontos representativos são aproximados do centróide do grupo por meio de um fator de encolhimento, que neste caso, é realizado movendo- se cada ponto 1/4 em direção a o centróide do grupo, gerando novos pontos representativos.
Esse movimento centrípeto em direção a o centróide tem por objetivo tornar o algoritmo menos sensível a outliers.
A cada passo de iteração, o par de grupos com os representativos mais próximos são combinados (mesma estratégia utilizada no algoritmo Edge Linkage), porém considerando apenas os pontos representativos (e não todos os pontos do grupo) e novos pontos representativos são calculados.
A escolha de 5 pontos representativos e do movimento de 1/4 são escolhas arbitrárias da implementação Esse algoritmo é uma variação do algoritmo Centripetal onde a estrutura do algoritmo é a mesma, porém a cada passo de iteração, o par de grupos com os representativos com maior distância (ou seja, o contrário do algoritmo Centripetal) são combinados.
É o único algoritmo divisivo utilizado, sendo o de execução mais rápida.
Ele inicia sua execução associando todos os pontos a um grande grupo.
Então, iterativamente eles dividem esse grande grupo em 2 sub-grupos a cada estágio.
Um contador que controla o número de grupos é aumentado de 1 a cada iteração até alcançar o número estabelecido, sendo sensível a outliers.
Em a implementação desse algoritmo apresentada por Shão Foi estabelecido que o diâmetro de um grupo é a distância máxima entre quaisquer 2 pontos neste grupo.
Em cada ciclo de execução é encontrado o grupo com maior diâmetro.
Este é então dividido em 2 pontos contidos no diâmetro deste grupo.
Os pontos são divididos entre esses 2 grupos, de acordo com o diâmetro mais próximo a cada ponto.
Agrupamentos hierárquicos podem produzir grupos com tamanhos diferentes, mas não podem produzir grupos com diâmetros muito diferentes.
Esse algoritmo foi definido por Shão Como do tipo Refinado.
Sua execução inicia com grupos aleatórios (definidos como sementes) com centróides também aleatórios.
Os grupos são então refinados utilizando um algoritmo Em (Expectation-Maximization) (descrito a seguir por ser a base da implementação do Bayesian por Shão).
De acordo com os autores uma série de execuções desse algoritmo devem ser realizadas, com diferentes sementes iniciais para que resultados consistentes sejam obtidos.
Algoritmo 4.2: Algoritmo Em utilizado em.
Define grupos semente e escolhe aleatoriamente centróides para esses grupos Repita Passo Expectation:
Para cada ponto é calculada a probabilidade deste pertencer a cada grupo Passo Maximization:
Dadas as probabilidades reestima os grupos e centróides de acordo com os pontos reais de forma a maximizar as probabilidades 5: Até que os parâmetros não mudem ou que seja atingido um certo limiar O algoritmo K--means, também classificado por Shão Como do tipo Refinado, tem como parâmetro de entrada o número de grupos, k, onde um conjunto de n pontos é particionado em k grupos de forma que a similaridade entre os pontos dentro de um mesmo grupo seja alta e entre grupos diferentes seja baixa.
A similaridade é medida de acordo com o valor médio dos pontos no grupo, chamado de centróide ou centro de gravidade.
O algoritmo que descreve o K--means é o seguinte:
Algoritmo 4.3: Algoritmo K--means básico.
Selecione K pontos como centróides Repita Forme K grupos atribuindo cada objeto ao centróide mais próximo Calcule o centróide de cada grupos Até que os centroides não se alterem Para a atribuição de um ponto ao centróide mais próximo, é necessário uma medida de similaridade que quantifique a noção de que centróide é mais perto de cada objeto.
Uma das funções de similaridade mais utilizadas é a distância Euclidiana.
Geralmente essa medida de similaridade adotada por o K--means é simples, uma vez que o algoritmo calcula repetidamente a proximidade dos pontos para os centróides.
Esse algoritmo é classificado por como do tipo Refinado.
Resumidamente, agrupamento utilizando o algoritmo Som consiste dos passos descritos no Algoritmo 4.4.
Algoritmo 4.4: Algoritmo de agrupamento utilizando Som.
Inicializa os centróides aleatoriamente Repita Selecione o próximo ponto Determine o centróide mais próximo desse ponto Atualize esse centróide e todos os outros centróides próximos deste, numa vizinhança especificada Até que os centroides não se alterem ou que um limiar seja excedido Associe cada ponto ao centróide mais próximo e retorne os centróides dos grupos.
Mapas Auto-- organizáveis, do inglês Self-Organizing Maps (Som) é uma técnica para agrupamento e visualização de dados baseada num ponto de vista de redes neurais.
Assim como outros algoritmos de agrupamento baseados em centróide, o objetivo do Som é encontrar um conjunto de centróides e associar cada ponto do conjunto de dados ao centróide que apresentar a melhor aproximação deste ponto.
Assim como o K--means, os pontos são processados um por vez e o centróide mais próximo é atualizado.
Uma característica diferencial desse algoritmo é que o mesmo impõe uma organização topográfica (espacial) dos centróides.
De essa forma, apesar de semelhante ao K--means, diferencia- se por essa relação topográfica entre os centróides.
Durante o processo de treinamento, o Som utiliza cada ponto para atualizar o centróide mais próximo e os demais centróides próximos topograficamente.
Segundo Shão Não há uma métrica de avaliação de agrupamento aceita universalmente.
Em este trabalho implementado por são utilizadas principalmente 2 métricas, o Davies-Bouldin Index (DBI) e o pseudo F-statistic (pSF).
O DBI é definido por:
A métrica pSF é baseada na comparação de variância entre os grupos para a variância residual sobre todos os pontos e é definida por:
Essas métricas são imperfeitas.
Por exemplo, baixos valores de DBI podem ser resultados de agrupamentos que apresentam muitos grupos com somente um ponto.
E, pSF tende a resultar em valores altos quando todos os grupos tem aproximadamente o mesmo tamanho, mesmo que estes sejam mal formados.
O ideal é utilizar essas métricas em conjunto com uma inspeção visual nos resultados.
A metodologia de Associação é uma técnica de mineração útil para a descoberta de relações interessantes escondidas em grandes bases de dados.
Essas relações podem ser representadas na forma de regras de associação.
Por exemplo, utilizando o mesmo contexto dos exemplos de árvore de decisão e árvores modelo acima, e considerando o seguinte conjunto de transações, onde cada linha corresponde aos resíduos do receptor que interagem com determinado ligante, após um experimento de docagem molecular:
HIE92, GLY207, THR100 b) ARG76, THR100 c) HIE92, GLY207 d) HIE92, THR100 e) HIE92, GLY207, ARG76 A o se utilizar Associação pode- se obter regras como:
Em esse exemplo, a regra de associação indica que há uma forte relação entre esses 2 atributos e que muitas vezes em que o resíduo Histidina 92 do receptor interage com determinado ligante, o resíduo Glicina 207 também interage.
De essa forma, Tan Define regras de associação como uma expressão na forma X Y, onde X e Y são conjuntos disjuntos.
Para avaliar as regras geradas há duas métricas:
Suporte e confiança.
Suporte determina o quão frequente uma regra é aplicável a um conjunto de dados, enquanto que Confiança determina o quão frequente os itens em Y aparecem em transações com X.
Ou seja, essas métricas refletem o grau de utilidade e o grau de certeza respectivamente.
Tipicamente, regras de associação são consideradas interessantes se elas satisfazem um limiar mínimo de suporte e confiança definidos por o usuário de acordo com os dados.
O algoritmo Apriori foi proposto por Agrawal Em para minerar itens frequentes em bases de dados para a determinação de regras de associação entre os itens.
Para isso, o Apriori executa múltiplas iterações sobre a base de dados de transações.
Em a primeira iteração é contabilizado o suporte de cada item.
Aqueles itens com um suporte individual maior que o suporte mínimo são considerados os itens mais frequentes.
Em cada uma das iterações seguintes, sendo k o número da iteração, os itens mais frequentes na iteração k -- 1 são agrupados em conjuntos de k itens, sendo esses considerados itens candidatos.
Para os itens candidatos é então contabilizado o seu suporte, e se o mesmo for maior que o suporte mínimo, esses itens candidatos são considerados frequentes.
O processo continuará até que o conjunto de itens frequentes seja um conjunto vazio.
Considerações Finais Esse capítulo é uma continuação do anterior sobre Materiais e Métodos.
Em ele foram apresentados conceitos sobre a área de mineração de dados.
Inicialmente são descritas duas definições diferentes para mineração de dados, assim como uma ilustração que mostra onde essa etapa está inserida dentro de o processo de KDD.
Em seguida, são exemplificadas algumas aplicações de mineração de dados na Bioinformática.
Em a continuação do capítulo são explicadas as técnicas de mineração de dados, e seus respectivos algoritmos, aplicados neste trabalho.
Com os exemplos apresentados durante esse capítulo, juntamente com a seção sobre préprocessamento é possível concluir que esta é uma etapa importante do processo de KDD e que despende muito tempo para ser executada.
Além de o mais, se os dados não estão organizados, essa etapa pode ser muito mais dispendiosa.
Um dos objetivos do desenvolvimento deste trabalho foi, desde o início, um estudo aprofundado da importância da flexibilidade de receptores em docagem molecular, para, após, utilizar esse conhecimento de forma a acelerar esse processo.
Por esse motivo decidimos minerar os dados de resultados de docagem molecular com o modelo de receptor FFR.
Esses dados, após gerados, estão em diferentes arquivos de saída e sem organização nenhuma.
Sendo assim, foi desenvolvido o trabalho descrito em que foi o ponto de partida de todos os resultados apresentados nos capítulos seguintes desta Tese.
Em esse trabalho de Machado 40 resultados de docagem com o modelo FFR são processados (receptor InhA e ligante NADH), um número considerado suficiente para testar a metodologia apresentada.
Para armazenar esses resultados, assim como as 40 conformações do receptor, foi desenvolvido um banco de dados (BD) descrito na Figura 4.5 (a).
Em esse BD, todas as informações dos resultados de docagem (FEB, RMSD, etc.) são armazenados na Tabela Docking, informações estruturais dos ligantes estão na Tabela Ligand_ atoms e Coord_ ligand_ atoms_ docking e informações estruturais do receptor nas tabelas Coord_ protein_ atoms_ docking e Protein_ atoms.
Foram então realizados experimentos com os dados armazenados no BD e a técnica de mineração Associação (algoritmo Apriori implementado no WEKA).
Para isso foram geradas entradas apropriadas ao WEKA, onde para cada experimento de docagem armazenado é analisada a ocorrência de cada tipo de aminoácido com uma distância máxima de 4,0 Å do ligante.
Se alguns dos resíduos do receptor de determinado tipo (por exemplo, glicina) em algum momento estiver a menos do que 4,0 Å do ligante, é atribuído o valor &quot;Y «para esse tipo de aminoácido, caso contrário é atribuído &quot;N».
Assim, nesse arquivo de entrada para o WEKA as instâncias correspondem aos resultados de docagem e os atributos à indicação se determinado tipo de aminoácido interagiu ou não como o ligante naquele experimento.
De essa análise é possível então descobrir quais tipos de aminoácidos mais interagem com o ligante nos 40 resultados analisados.
Essa informação então pode ser utilizada para a busca de novos inibidores para esse receptor.
Com esse arquivo preparado, ao abrir- lo no WEKA, ainda sem aplicar o Apriori, já é possível identificar o número de simulações de docagem que cada um dos tipo de aminoácidos interage com o ligante, conforme mostrado na Figura 4.5 (b).
A seguir, com o Apriori, conseguimos extrair regras como as de exemplo mostradas na Figura 4.5 (c).
Como esse trabalho inicial foi possível perceber que há muita informação importante sobre a interação receptor-ligante que, sem um processo de KDD, é muito difícil (senão impossível) de extrair- las diretamente dos arquivos de saída da docagem molecular e da DM.
Com a continuação desse trabalho percebeu- se que esse BD desenvolvido não era o modelo mais apropriado para armazenar grandes quantidades de experimentos de docagem com o FFR, considerando por exemplo todos os runs de cada docagem, diferentes ligantes, diferentes FFR, etc..
Para isso então é proposto o banco de dados FReDD -- Flexible Receptor Docking Database para armazenar todos esses resultados de docagem molecular e conformações da trajetória por simulação por a DM que serve para diferentes FFR e diferentes ligantes.
O FReDD será descrito no próximo capítulo.
Este capítulo apresenta o Banco de dados FReDD (Flexible Receptor Docking Database), desenvolvido neste trabalho para armazenar os resultados de docagem molecular com o modelo FFR.
A partir de os dados armazenados nesse BD foi possível a utilização de diferentes técnicas de mineração de dados conforme será descrito nos próximos capítulos.
Esse BD é uma extensão do modelo análises preliminares nos seus dados foram publicados durante o desenvolvimento desta Tese nos seguintes trabalhos:
Como resumo expandido no LNBI-LNCS durante o evento Brazilian Symposium on Bioinformatics de 2009;·
como artigo completo na conferência IADIS International Conference Applied Computing de· como capítulo do livro Tópicos em sistemas colaborativos, multimídia, web e banco de dados de 2010.
Em esse capítulo de livro são apresentados, de forma bem resumida, vários tópicos presentes nesta Tese, incluindo o BD FReDD.
O mesmo foi apresentado como no minicurso intitulado &quot;Processo de KDD aplicado à Bioinformática durante o Simpósio Brasileiro de Banco de Dados em 2010».
Além de o FReDD, esse capítulo também apresenta como o conteúdo armazenado no FReDD foi preparado para ser utilizado com as técnicas de mineração de dados onde é descrito o algoritmo desenvolvido para gerar essas entradas.
A o final desse capítulo, a partir de as entradas preparadas para mineração é descrita uma análise preliminar nesses dados, conforme apresentado no artigo.
O modelo do Banco de dados FReDD é mostrado na Figura 5.1 (desenhado com ferramenta Microsoft Visio).
Atualmente no FReDD estão armazenados todos os resultados dos experimentos de docagem Fase 1.
Os resultados de docagem Fase 2 somente foram executados ao final do desenvolvimento desta Tese, por esse motivo ainda não tem seus dados no FReDD.
Esse banco de dados é composto atualmente por 17 tabelas contendo um total de 15.814.183 registros.
Conforme já mencionado em Materiais e Métodos, o FReDD foi implementado utilizando o SGBD PostgreSQL num ambiente Linux numa máquina Core 2 Quad com 8 GB de memória RAM.
Os dados armazenados em todas tabelas do FReDD foram inseridos através de scripts Python.
O FReDD foi desenvolvido para armazenar:
Dados referentes a átomos e aminoácidos existentes;
Dados sobre o receptor e ligantes utilizados;
Dados referentes as conformações do receptor utilizado;
Dados referentes aos resultados de docagem molecular utilizando as conformações do receptor armazenadas.
É importante ressaltar que os resultados de todas as execuções dentro de cada experimento de docagem molecular foram considerados.
As tabelas Atom e Residue_ Prot são denominadas de conteúdo fixo.
Elas foram definidas dessa forma porque contém informações que tem validade para quaisquer receptores ou ligantes que venham a ser inseridos no FReDD, uma vez que correspondem aos dados referentes aos átomos contidos na tabela periódica e aos aminoácidos naturais e suas variações de nomenclatura de acordo com os software utilizados.
A tabela Atom contém os dados referentes a todos os átomos da tabela periódica, identificados por o campo Symbol -- símbolo do átomo.
Ela é constituída de 96 registros (os átomos raros não foram incluídos).
Essa tabela é importante pois permite que sejam recuperadas informações químicas sobre qualquer átomo que esteja contido no receptor ou num dos ligantes armazenados, como por exemplo, o grupo e período do átomo na tabela periódica (Group_ a e Period), o nome do mesmo (Name) entre outras informações.
Essa tabela contém os dados referentes aos 20 aminoácidos naturais identificados por o campo Code3 que corresponde ao código de 3 letras de cada aminoácido.
Essa tabela é constituída de 27 registros pois, alguns dos 20 aminoácidos naturais são identificados por o AMBER6.
0 por diferentes códigos de 3 letras.
Por exemplo, o aminoácido Histidina pode ser representado por o código HIS, HIE, HIP e HID de acordo com os diferentes estados de protonação desse resíduo.
Assim como a tabela Atom, essa tabela é importante caso seja necessário a recuperação de informações químicas sobre determinado aminoácido.
Alguns de seus principais campos são:
Code1 -- o código de uma letra que representa o aminoácido;
Name e Original_ name -- o nome do aminoácido em português e o nome original respectivamente;
Classification -- o tipo do aminoácido, como por exemplo, polar, apolar, básico, etc..
Outras informações sobre o resíduo, como por exemplo:
N_ atoms, Strucuture, density, surface, volume, solubility e mass.
A tabela Protein contém informações sobre os receptores armazenados no banco de dados.
A chave primária dessa tabela é o campo PDBCode que corresponde ao código PDB do receptor exatamente como é depositado no repositório de estruturas tridimensionais de proteínas PDB.
Para armazenar esses dados no FReDD foi desenvolvido um script em Python utilizando uma biblioteca de funções chamada Biopython.
Atualmente essa tabela contém 1 registro, que corresponde aos dados referentes a proteína de estudo nesse trabalho, a InhA.
Os principais campos contidos nessa tabela são:
Name -- nome da proteína;
Resolution -- a resolução com que esta estrutura foi determinada;
Missing_ residues -- se esse campo contém o valor 1, há resíduos de determinada estrutura que não foram identificados experimentalmente, se for 0, a estrutura está completa;
Missing_ atoms -- se conter 1, há átomos não identificados, se o valor for 0, todos os átomos estão descritos no PDB;
Hetatom -- o valor 1 indica que há um ligante nessa estrutura enquanto que Hetatom igual a 0 significa que a estrutura contém somente átomos da própria proteína;
Hetatom_ name -- se há um ligante na estrutura que está sendo armazenada, o nome do mesmo é armazenado nesse campo;
Informações sobre a obtenção da estrutura:
Deposition_ date, Structure_ method, Author, Source_ organism, Source_ expression_ system, Journal_ reference;
Outras informações contidas no cabeçalho do PDB original do receptor como:
Keywords, Head, Compound_ molecule, Compound_ synonym e Access_ date;
Parte de um arquivo PDB que descreve uma conformação da proteína está descrito na Figura sobre como a estrutura foi obtida, onde está publicada, etc..
A primeira linha da Figura 5.2 não existe no arquivo, porém está descrita na Figura 5.2 para auxiliar no entendimento do formato PDB.
Cada proteína é composta por resíduos, que por sua vez, são compostos por átomos.
No caso de a InhA, essa proteína é composta por 4.008 átomos (os números e nomes de alguns desses átomos estão descritos nas colunas 2 e 3 da Figura 5.2 respectivamenteo nome e o número de alguns resíduos estão descritos nas colunas 4 e 5 da Figura 5.2).
O mesmo átomo pode aparecer mais de uma vez dentro de uma proteína e até mesmo, mais de uma vez dentro de o mesmo resíduo, por isso, é sempre identificado por um nome mais um número.
O mesmo acontece com os resíduos.
O mesmo resíduo pode aparecer inúmeras vezes dentro de uma proteína portanto identificado por o seu código de 3 letras e por um número.
Os dados sobre os átomos e resíduos de cada conformação da proteína são armazenados nas tabelas descritas a seguir.
A tabela Conformation_ Prot armazena informações referentes às conformações do receptor.
Essa tabela é identificada por os campos PDBCode e N_ Conf_ Prot que correspondem ao código PDB do receptor e ao número da conformação.
Essa tabela é composta por os seguintes campos que armazenam:
PDBCode e N_ Conf_ Prot· Type_ conf_ prot -- Tipo de conformação:
Se é cristalográfica ou resultante de uma simulação por a DM;
Filename -- Nome do arquivo;
Date, Software, Size -- dados referentes a conformações resultantes de uma simulação por a DM, Date armazena a data que a simulação foi executada, Software indica que software foi utilizado para a execução da simulação e Size armazena o tamanho da mesma.
Essa tabela contém atualmente 3.100 registros, que correspondem às 3.100 conformações do receptor InhA utilizadas nesse trabalho resultantes da simulação por a DM descrita no Capítulo 2.
Essa tabela contém uma lista de todos os resíduos contidos em cada proteína e os relaciona com os aminoácidos naturais armazenados na tabela Residue_ Prot.
Atualmente contém 268 registros que correspondem aos 268 resíduos que a única proteína armazenada contém.
Esse total de resíduos pode ser visto na Figura 5.2 na coluna 5, onde o PDB inicia por o Resíduo 1, uma Alanina (Ala) e termina por o resíduo Leucina (Leu), que corresponde ao resíduo 268.
Os principais campos dessa tabela são:
N_ res_ prot (número do resíduo na proteína), PDBCode (o código identificador da proteína) e Code3 (o código de 3 letras de cada resíduo).
Essas duas tabelas servem para relacionar a proteína que está sendo armazenada com os átomos contidos na mesma, assim como relacionar os átomos com os resíduos.
A tabela Atom_ Prot armazena as informações contidas nas colunas 2, 4 e 5 do arquivo PDB descrito na Figura 5.2 (Número do átomo, Código de 3 letras do resíduo e Número do Resíduo, respectivamente) e as relaciona com o código PDB da proteína.
Como cada átomo aparece somente uma vez dentro de cada proteína, cada registro nessa tabela é identificado por o código PDB da proteína e por o número do átomo na mesma (PDBCode, N_ atom_ prot).
Essa tabela é necessária porque muitas vezes o mesmo átomo aparece inúmeras vezes dentro de um mesmo PDB da proteína e a cada vez que ele aparece corresponde a um diferente átomo.
Então, essa tabela faz esse tipo de relação, do número do átomo com o número do resíduo, para uma determinada proteína.
Ela contém atualmente 4.008 registros, que correspondem aos 4.008 átomos da proteína InhA, a única proteína armazenada até o momento.
A tabela Atom_ Residue_ Prot relaciona os átomos contidos em cada tipo de resíduo.
Por exemplo, entre as Alaninas contidas nos PDBs que utilizamos nesse trabalho, o número máximo de átomos que cada uma continha era 12 ou menos, os que estão listados na Figura 5.2 das linhas 1 a 12.
A diferença no número de átomos de um resíduo dentro de um mesmo arquivo PDB ocorre porque alguns átomos do resíduo podem estar ligados a outros átomos dependendo da posição em que se encontram dentro de a estrutura no momento que a simulação por a DM.
Portanto a tabela relaciona cada resíduo com o nome e número de átomos que o mesmo pode conter.
A tabela contém atualmente 345 registros pois cada resíduo é cadastrado somente uma vez.
Para inserir dados nas tabelas Atom_ Prot e Atom_ Residue_ Prot foram desenvolvidos scripts em Python.
Esses scripts são executados somente 1 vez para cada receptor, uma vez que independente da conformação, um receptor conterá sempre o mesmo número de átomos e de resíduos e a relação que há entre átomos e resíduos não se altera da mesma forma.
Essa tabela é utilizada para armazenar as coordenadas de todos os átomos de cada conformação da proteína.
Seus principais campos são:
PDBCode, N_ Conf_ Prot e N_ Atom_ Prot que armazenam o código PDB da proteína, o número de sua conformação e o número do átomo, respectivamente.
Esses campos são utilizados em conjunto como chave primária e permitem o relacionamento desta tabela com as tabelas Conformation_ Prot e Atom_ Prot;
Coord_ X, Coord_ Y e Coord_ Z armazenam as coordenadas x, y e z de cada átomo.
Um exemplo pode ser visto nas colunas 6, 7 e 8 da Figura 5.2;·
Charge1, Charge2, Occupancy e BFactor são campos utilizados no armazenamento de estruturas cristalográficas porque não constam em PDBs resultantes de uma simulação por a DM.
Atualmente somente as conformações resultantes da DM é que estão sendo armazenadas no FReDD.
Essa tabela é composta atualmente 12.424.800 de registros, que correspondem às 3.100 conformações da proteína InhA multiplicadas por os 4.008 átomos contidos em cada conformação.
Para a inserção de dados nessa tabela foi desenvolvido um script em Python para inserir os dados referentes a cada conformação, primeiro armazenando os dados sobre a conformação na tabela Conformation_ Prot e após sobre as coordenadas de cada átomo da mesma na tabela Coord_ Atom_ Prot.
Em a Figura 5.1 que descreve o Modelo final do banco de dados FReDD a tabela Docking está relacionada às simulações de docagem molecular enquanto que as tabelas Tabelas Ligand, Composition_ Lig, Residue_ Lig, Atom_ Res_ Lig, Atom_ DLG_ Lig, Conformation_ Lig e Coord_ Atom_ Lig estão relacionadas com dados sobre os ligantes e suas conformações.
A Tabela Ligand é utilizada para armazenar informações referentes aos ligantes.
Cada registro é identificado por o campo LIGCode, que corresponde a um código de até 10 letras que identifica o ligante, semelhante ao código PDB que identifica a proteína.
Porém, no caso de o ligante, esse código não permite uma busca direta do ligante num banco de dados de pequenas moléculas pois, muitos ligantes são identificados com um mesmo código e uma busca por o ligante deve ser feita utilizando uma combinação de parâmetros.
Além de o LIGCode, essa tabela contém os campos· Name -- Nome do ligante;
Classification -- os ligantes podem ser classificados de acordo com suas características eletrônicas e da interação;
A data de acesso, o nome do arquivo orginal e o tipo do arquivo original (pode ser por exemplo PDB ou MOL2;·
Formul -- A fórmula química do ligante.
A Tabela Ligand é uma versão inicial já que o foco deste trabalho não é seleção de ligantes e atualmente estamos trabalhando com um número reduzido (somente 4 ligantes).
Porém, o laboratório LABIO tem como um de seus objetivos trabalhar com Virtual Screening e para isso está sendo estudado por outro membro do laboratório técnicas de seleção de ligantes, trabalho que possivelmente atualizará essa tabela tornando a mesma mais completa e apropriada.
Atualmente a tabela Ligand contém atualmente 4 registros, um para cada ligante utilizado nesse trabalho:
NADH, PIF, TCL e ETH (descritos no Capítulo 2).
O banco de dados foi modelado de forma a ser o mais flexível possível, sendo assim, poderá armazenar ligantes compostos por um ou mais resíduos.
Essa informação é armazenada nas tabelas Composition_ Lig e Residue_ Lig.
A tabela Composition_ Lig é composta por os campos LIGCode (código identificador do ligante), N_ res_ lig (número do resíduo no ligante) e Code3 (código identificador do resíduo do ligante) e contém uma lista dos resíduos que compõem cada ligante.
A tabela Residue_ Lig armazena informações sobre os resíduos de cada ligante e contém os campos Code3 (código identificador do resíduo do ligante), Name e Description (armazenam informações mais detalhadas sobre os resíduos que compõem os ligantes).
Contudo, atualmente os 4 ligantes de trabalho do LABIO que estão armazenados no FReDD são compostos somente por um resíduo cada.
Sendo assim, as tabelas Composition_ Lig e Residue_ Lig são compostas atualmente somente por 4 registros cada uma e para facilitar, esses resíduos receberam como identificador Code3 o próprio LIGCode de cada ligante.
As tabelas Atom_ Res_ Lig, Atom_ Lig e Atom_ DLG_ Lig armazenam informações sobre os átomos que compõem cada ligante, suas relações com os resíduos (ou resíduo) e a relação que há entre os átomos do ligante antes (arquivo MOL2 do ligante) e após a preparação do mesmo para docagem molecular.
A tabela Atom_ Lig tem como chave primária o número de cada átomo do ligante (campo N_ atom_ lig) e o LIGCode.
Também armazena o nome do átomo (Name_ atom_ res) e o código de três letras do resíduo de cada átomo (Code3).
Essa tabela contém atualmente 144 registros, 71 estão relacionados aos átomos que compõem o NADH, 24 registros correspondem aos 24 átomos do ligante TCL em sua versão original (MOL2), 28 registros estão relacionados aos 28 átomos da Como esses ligantes contém somente 1 resíduo cada, a tabela Atom_ Res_ Lig que relaciona os átomos aos tipos de átomos (Tabela Atom) e aos resíduos (através dos campos Symbol, Code3 e Name_ atom_ res) contém o mesmo número de registros da tabela que contém somente os átomos, totalizando então os mesmos 144 registros.&amp;&amp;&amp;
O arquivo original do ligante TCL está descrito na Figura 5.3 como um exemplo de arquivo MOL2 de um ligante, nesse caso, obtido a partir de o banco de dados de pequenas moléculas ZINC.
Atualmente, a maioria dos ligantes já podem ser obtidos no formato MOL2, o formato ideal, pronto para ser utilizado para execução de docagem molecular.
Esse formato consiste no formato PDB mais uma coluna que contém a carga de cada átomo do ligante (corresponde a penúltima coluna da Figura 5.4), coluna essencial para a execução de docagem molecular.
Antes da execução das simulações de docagem molecular é necessário preparar os arquivos do ligante.
Para isso, é executado por o programa deftors do AutoDock3.
0.5. O arquivo PDBQ do ligante, preparado para a docagem molecular, está descrito na Figura 5.4.
Como é possível analisar, comparando o arquivo do ligante antes da preparação para a docagem e após (Figuras 5.3 e 5.4), há uma diferença em relação a o número de átomos e ao nome de cada átomo.
De um total de 24 átomos antes da preparação para a docagem molecular (Figura ligante constam no banco de dados, é necessário que a relação que há entre os átomos antes e após a preparação da docagem molecular seja também armazenada.
Para isso, desenvolveu- se a Tabela Atom_ DLG_ Lig, composta por o número de cada átomo do ligante na versão antes da preparação para a docagem molecular (N_ atom_ lig) e o correspondente número e nome de cada átomo (N_ atom_ dlg e Name_ atom_ dlg) após a preparação do mesmo (essa numeração dos átomos do PDBQ é a mesma que aparece nos arquivos de saída do AutoDock3.
0.5). Assim como as tabelas Atom_ Res_ Lig e Atom_ Lig, a tabela Atom_ DLG_ Lig contém 144 registros, 71 relacionados ao NADH, 24 ao TCL, 28 ao PIF, e 21 à ETH.
A Tabela 5.1 mostra uma parte do conteúdo da tabela Atom_ DLG_ Lig para exemplificar como ocorre a relação entre o arquivo do ligante antes e após a preparação para a docagem.
Em a primeira coluna da tabela está o código do ligante (LIGCode), ou seja, sua identificação dentro de a tabela Atom_ Lig.
Em a segunda coluna está o campo Code3, campo identificador das tabelas Atom_ Res_ Lig, Atom_ Lig e Atom_ DLG_ Lig.
A terceira e quarta colunas listam os campos N_ atom_ lig e Name_ atom_ res que correspondem ao número e nome do átomo do ligante em sua versão original, respectivamente.
A quinta coluna mostra o campo N_ atom_ dlg, que se refere ao número do átomo no arquivo DLG.
Como pode- se verificar com os dados descritos na Tabela 5.1, alguns átomos são os mesmos em ambos arquivos do ligante, por exemplo os átomos 1 e 2 são iguais no arquivo original do ligante e no arquivo após a preparação para a docagem (Coluna 5 na Tabela molecular foram compactados num átomo no arquivo do ligante final, o átomo 18.
Por isso, na tabela Atom_ DLG_ Lig esse átomo do ligante final se relaciona com 2 átomos do arquivo do ligante original.
Os dados das tabelas Ligand, Composition_ Lig e Residue_ Lig foram inseridos utilizando scripts Python.
Esses scripts foram executados somente 1 vez para cada ligante armazenado.
A tabela Docking armazena informações sobre os experimentos de docagem molecular executados, atribuindo um número sequencial a cada experimento inserido no banco de dados e relacionando esse número com o número da conformação do receptor utilizado.
Os principais campos dessa tabela são:
N_ Docking -- é um número sequencial utilizado como campo identificador desta tabela;
PDBCode e N_ Conf_ Prot -- são os campos que estabelecem o relacionamento da tabela Docking com a tabela de conformações do receptor, indicando que conformação do receptor foi utilizado em cada uma das simulações de docagem armazenadas;
Software, Algorithm, Data_ Execution, Total_ Time, Author, Local_ execution, Docked e Filename_ docking -- informações técnicas sobre cada experimento de docagem importantes para registro sobre os experimentos que poderá ser útil em futuras comparações entre resultados;
FReDoWs -- esse campo indica se o experimento de docagem foi executado utilizando o workflow científico descrito em ou foi executado com o auxílio de scripts;
Docking_ Type -- armazena o tipo de experimento de docagem executado, se foi exaustivo, considerando todos as conformações do receptor ou seletivo, onde somente uma parte das conformações é considerada;
Total_ N_ runs -- armazena o total de runs de cada experimento;
Essa tabela contém 12.400 registros, 3.100 registros para cada um dos ligantes NADH, PIF, TCL e ETH.
Os scripts Python desenvolvidos para armazenar os dados da tabela Docking foram executados e os dados das tabelas Conformation_ Lig e Coord_ Atom_ Lig foram inseridos durante a execução deste mesmo script, uma vez que eles se referem aos dados dos mesmos arquivos de saída (os arquivos DLG resultantes da docagem).
As tabelas Conformation_ Lig e Coord_ Atom_ Lig armazenam os dados referentes aos resultados de cada experimento de docagem molecular.
Para um melhor entendimento do conteúdo dessas tabelas, uma parte do arquivo de saída do software AutoDock3.
0.5 que corresponde a 1 run executado, está descrito na Figura 5.5.
Esse arquivo é resultante da execução do experimento de docagem molecular considerando- se a conformação 2 do receptor InhA e o ligante TCL.
É na tabela Conformation_ Lig que estão armazenados os dados marcados em rosa na Figura campos dessa tabela são:
LIGCode e N_ Conf_ Lig -- campos utilizados como identificadores.
Se N_ Conf_ Lig for igual a 0 (zero) indica que determinado registro de conformação do ligante corresponde a original· N_ docking -- relaciona a conformação do ligante que está sendo armazenada com o experimento de docagem onde a mesma foi obtida.
Se for a conformação inicial de um determinado ligante, o valor de N_ docking é vazio;
Type_ conf_ lig -- indica o tipo de conformação do ligante, se for 0, trata- se de uma conformação inicial, se for 1, uma conformação resultante de docagem molecular;
Run e Rank de cada run executado dentro de cada experimento de docagem molecular;
RMSD (Root Mean Square Deviation) -- corresponde à distância do centro de massa do ligante na sua posição inicial, antes da execução da docagem molecular e o centro de massa do ligante após a execução da docagem;
FEB -- Energia de interação final;
Ki -- Constante de inibição;
É necessário que a cada conformação do ligante armazenada na tabela Conformation_ Lig estejam relacionadas as coordenadas de todos seus átomos.
Para isso foi desenvolvida a tabela Coord_ Atom_ Lig cujos principais campos são:
LIGCode, N_ conf_ lig e N_ atom_ dlg -- são os campos que formam a chave primária de cada registro.
Caso a conformação que esteja sendo armazenada seja a conformação inicial do ligante, o N_ atom_ dlg terá o mesmo valor do N_ atom_ lig da tabela Atom_ DLG_ lig;
Coord_ X, Coord_ Y e Coord_ Z -- que correspondem as coordenadas x, y e z de cada átomo;
A Tabela 5.2 resume o conteúdo de Conformation_ Lig e Coord_ Atom_ Lig.
Em a primeira coluna tem- se o nome dos ligantes, a segunda coluna descreve o total de átomos originais de cada ligante enquanto que a terceira coluna contém o total de átomos de cada ligante após a preparação para a docagem molecular.
A quarta coluna apresenta o total de registros da Tabela Conformation_ Lig e a última coluna o total de registros da Tabela Coord_ Atom_ Lig.
Para cada ligante o total de registros na Tabela Conformation_ Lig é calculado da forma: (resultados de docagem molecular que convergiram para valores válidos para esse ligante)* 10 (Total de runs executados em cada uma das simulações de docagem molecular)+ 1 conformação inicial do ligante.
Após a descrição de todo o modelo do FReDD assim como todo o seu conteúdo, é necessário descrever como esses dados foram pré-processados para os experimentos de mineração de dados.
Todos os passos envolvidos nesse pré-processamento são ilustrados na Figura 5.6: Um dos objetivos desde o início do desenvolvimento deste trabalho foi de analisar qual é a importância da flexibilidade dos receptores em docagem molecular.
Para isso, foi proposto um ambiente que permitisse um estudo sobre como ocorrem as interações receptor-ligante em simulações de docagem com o modelo FFR.
De essa forma, para alcançar esse objetivo, decidiu- se utilizar nas análises as distâncias mínimas entre os átomos do ligante e os átomos dos resíduos do receptor e o valor de FEB de cada experimento de docagem molecular executado.
Ao contrário de o trabalho descrito no final do Capítulo 3, onde os atributos eram indicações se havia ou não interação entre determinado resíduo do receptor e o ligante, nos arquivos de entrada a partir desse momento são consideradas os valores das distâncias.
Um exemplo de arquivo de entrada do complexo InhA-ETH.
Passos intermediários no préprocessamento necessários para a aplicação de algumas técnicas de mineração de dados.
Os arquivos de entrada para cada uma das técnicas de mineração aplicadas.
Para obter as distâncias mínimas (em Å) entre todos os átomos do ligante em sua posição final após a docagem e os átomos dos resíduos do receptor, que serão então os atributos preditivos nos arquivos de entrada, é preciso combinar os campos das tabelas Coord_ Atom_ Prot e Coord_ Atom_ Lig de forma a obter uma tabela de distâncias como a mostrada na Figura 5.6.
Em esse exemplo é considerado o ligante ETH, com seus 13 átomos após a preparação para a docagem molecular, colunas na tabela de exemplo, cuja combinação totaliza 91 distâncias a serem calculadas para verificar qual é a distância mínima (marcada em vermelho na Figura).
Ao lado de a tabela, a ilustração desse ligante mostra algumas dessas distâncias calculadas.
A partir de as distâncias mínimas calculadas na etapa do pré-processamento, é necessário combinar- las num arquivo de entrada único que contém as distâncias mínimas entre todos os resíduos do receptor e o ligante para cada uma das conformações do ligante (que podem ser todos os resultados de docagem ou somente aqueles runs de melhor FEB).
Para a obtenção desta é necessário que sejam efetuados 3.024.112 cálculos de distâncias mínimas para o NADH, 8.152.560 para o PIF, 7.603.160 para o TCL e 8.155.240 para a ETH.
Esse arquivo único está exemplificado em parte na Figura 5.6 para o ETH.
Inicialmente tentou- se gerar essa tabela a partir de a execução de uma única consulta SQL que calculasse todas essas distâncias para todos os ligantes ao mesmo tempo.
Porém, essa consulta não foi processada por o PostGreSQL, uma vez que, envolvia o cálculo em memória de quase 27 milhões distâncias mínimas (considerando os 4 ligantes).
Em a tentativa de que as instâncias da tabela fossem obtidas a partir de uma única consulta SQL buscou- se otimizar o FReDD, criando diferentes índices e tabelas intermediárias, porém nenhuma das consultas executadas dessa forma obteve sucesso já que o aplicativo PGAdim (que administra BD desenvolvidos para o PostGreSQL) parava sua execução após, em média, 48 horas ininterruptas.
Devido a esse problema, optou- se por o desenvolvimento de um script Python que conectasse ao FReDD executando uma sequência de consultas, considerando em cada uma, uma conformação e um resultado de docagem a ser analisado e escrevendo os resultados num arquivo de saída para cada ligante, ao final de cada consulta.
Durante a execução das consultas são utilizadas tabelas intermediárias criadas para otimizar- las.
A tabela Conf_ docking_ 1 ENY reune as informações de todos os átomos de todos os resíduos de todas as conformações do receptor InhA.
Assim, a consulta nesses dados não é mais executada a partir de junções de várias tabelas mas diretamente acessando os registros desta tabela intermediária.
As outras tabelas intermediárias foram Conf_ docking_ NADH_ 1 ENY, Conf_ docking_ ETH_ 1 ENY, Conf_ docking_ TCL_ 1 ENY e Conf_ docking_ PIF_ 1 ENY.
Essas tabelas armazenam os registros de coordenadas de todos os átomos de todas conformações do respectivo ligante.
Assim, a consulta nos dados de coordenadas dos ligantes é feita diretamente nessas tabelas, não sendo necessária a junção de várias outras, o que envolveria um custo computacional bem maior.
O algoritmo a seguir resume o script Python escrito para gerar essas entradas.
Para facilitar o entendimento do algoritmo, serão utilizadas variáveis e não os nomes das tabelas e os nomes dos campos.
Sendo: T otalConf ormacoesR:
Corresponde ao número total de conformações do receptor;
T otalConf ormacoesL:
Corresponde ao número total de conformações de um determinado Ligante, considerando todos os runs;
T otalResiduosR:
Armazena o número total de resíduos do receptor;
ResiduoR: É um dado resíduo do receptor;
T otalAtomosResiduoR:
Armazena o total de átomos do ResiduoR;
XR, yR e zR:
Correspondem as coordenadas espaciais de cada átomo de determinado ResiduoR;
LiganteL é um determinado ligante;
T otalAtomoLiganteL:
Armazena o total de átomos do LiganteL;
XL, yL e zL:
Correspondem as coordenadas espaciais de cada átomo de determinado LiganteL;
DistMi, j:
Matriz que armazena todas as distâncias);
ResultMt, r:
Matriz que armazena todos os resultados);
M inimaDist:
Armazena o valor mínimo da matriz DistMi, j.
Algoritmo 5.1: Algoritmo de cálculo de distâncias entre átomos do ligante e átomos de resíduos do receptor.
Adaptado de.
Para t $= 1 até T otalConf ormacoesL Para r $= 1 até T otalResiduosR Para i $= 1 até T otalAtomosResiduoR Para j $= 1 até T otalAtomoLiganteL DistMi, j $= (xRi -- xLj) 2+ (yRi -- yLj) 2+ (zRi -- zLj) 2 Se DistMi, j M inimaDist então M inimaDist $= DistMi, j Fim Se Fim Para Fim Para ResultMt, r $= M inimaDist Fim Para Fim Para 16: Fim Para Durante essa etapa de preparação dos dados foram então gerados os arquivos ARFF (AttributeRelation File Format) para serem utilizados no WEKA e cujo conteúdo corresponde à matriz ResultMt, r.
Um arquivo ARFF é composto por duas seções distintas:
Um cabeçalho que contém o nome da relação, a lista de atributos e seus tipos, e a seção de dados, onde cada linha corresponde a uma instância e os atributos são delimitados por vírgulas e devem aparecer na ordem com que foram declarados no cabeçalho, conforme mostra o exemplo na Figura 5.7.
ARFF da Figura 5.7 está pronto, pois o atributo-alvo para esse tipo de técnica é um valor numérico.
Entretanto, para os experimentos com classificação aplicando árvores de decisão é necessário discretizar o valor da FEB, pois para esses experimentos o atributo-alvo deve ser categórico (Capítulo 6).
Além de o mais, para experimentos com Associação e para análises sobre as interações entre o receptor-ligante, onde não há atributo-alvo, os atributos preditivos precisam ser binarizados, onde:
Atributo $= 1 se DistanciaRL 4, 0 Å 0 se DistanciaRL\&gt; 4, 0 Å Onde:
DistanciaRL $= distância mínima entre um determinado resíduo do receptor e um ligante.
O valor de 4,0 Å foi determinado de forma a estar associado a categorização de distâncias entre átomos doadores-aceitadores em ligações de hidrogênio e são classificadas em:
Forte -- equivale a distâncias entre os átomos de 1,9 Å a 2,5 Å;
Moderada -- distâncias entre os átomos de 2,5 Å a 3,2 Å;
Fraca -- distâncias entre 3,2 Å e 4,0 Å;
Distâncias maiores do que 4,0 Å indicam que determinado resíduo não estabelece nenhum contato com nenhum átomo do ligante em determinado resultado de docagem.
Análises preliminares com o FReDD Um dos objetivos desse trabalho é investigar a importância da flexibilidade do receptor em suas interações intermoleculares com pequenas moléculas ou ligantes.
Para isso, a partir de os arquivos de entrada de distâncias entre resíduos do receptor e ligantes, foram realizadas análise preliminares nos dados com o objetivo de verificar quais são os resíduos que permanecem em contato na maior parte de todas as execuções de cada simulação de docagem molecular com o FFR para cada um dos ligantes.
O resultado que descreve os 10 resíduos que mais interagem com cada um dos 4 ligantes, mostrados na Figura 5.8.
A união dos Top 10 de cada ligante é a lista final de 25 resíduos descrita na Tabela 5.3 onde as células em destaque indicam os resíduos que mais interagiram com cada um dos ligantes.
A Tabela 5.4 apresenta esse total de resíduos do receptor que interagem com o modelo FFR (coluna 2 na tabela) em pelo menos um dos resultados e os compara com o total de resíduos que interagem com cada ligante num resultado de docagem com a estrutura cristalográfica do receptor InhA, o qual chamamos de modelo rígido do receptor (RR -- Rigid Receptor), análise realizada por uma inspeção visual com um programa visualizador de estruturas de macromoléculas (coluna 3 na tabela).
Além disso, na coluna 4 da tabela é apresenta quantos resíduos do modelo RR são comuns a seleção dos Top 10 para cada ligante.
A Tabela 5.4 confirma que é muito importante considerar a flexibilidade em simulações de docagem molecular.
Por exemplo, para o NADH, somente 22 resíduos do modelo RR interage com este ligante.
Quando sua flexibilidade é considerada, existem 185 resíduos interagindo em pelo menos um run com o modelo FFR.
Para esse ligante, 9 dos 10 Top 10 aparecem na seleção do modelo RR.
Isto acontece porque o NADH é o ligante natural e sua região de ligação nesse receptor é bem conhecida mas ainda assim a flexibilidade apresenta um papel muito importante em mediar mesmos (Tabela 5.4).
Isto acontece porque a região de ligação para esses 2 ligantes é a mesma, próxima a onde liga- se o substrato.
Para o TCL, 139 resíduos do modelo FFR interagem com o ligante enquanto que somente 12 do modelo RR, e destes, não mais do que 5 aparecem na lista de Top 10 para esse ligante.
Isto significa que há outros 5 resíduos que interagem muitas vezes no dobro do tamanho do ligante ETH, esperava- se que os mesmos não interagissem na mesma região do receptor, o que é indicado por os 7 dos Top 10 resíduos serem diferentes para o ETH.
Tabela 5.4: Análises de interações intermoleculares entre modelo FFR-ligantes e modelo RR-ligantes.
Ligante Interações FFR-Ligante Interações RR-ligante RR Top 10 Considerações Finais Esse capítulo descreveu o banco de dados FReDD e todas as suas tabelas.
Para cada tabela foram descritos os principais campos, como que seu conteúdo foi obtido, de que forma os dados foram armazenados e o total de registros.
Uma grande parte dos dados armazenados no FReDD foram utilizados nos experimentos de Mineração de Dados que serão descritos nos próximos capítulos.
Até o momento não foi encontrado nenhuma base de dados que tenha sido desenvolvida com o mesmo propósito do FReDD.
Há uma plataforma de integração de dados para Triagem Virtual que é detalhado no Capítulo 9.4, mas esta não armazena informações como resultados de docagem molecular nem conformações de uma trajetória de DM.
Em as seções que descrevem o pré-processamento é apresentada uma metodologia de preparação dos dados de docagem molecular para a geração de entradas para mineração de dados pode ser aplicada a diferentes complexos com o objetivo de descoberta de conhecimento sobre as interações FFR-ligante.
Além de o mais, as análises preliminares sobre os dados armazenados no FReDD mostram informações que não seriam obtidas se o modelo rígido do receptor fosse considerado ou sem uma preparação apropriada de todos os dados resultantes de simulações de docagem molecular com o modelo FFR.
Não foi encontrado até o momento trabalhos que tenham investigado a flexibilidade dos receptores em docagem molecular da forma como apresentado nesta Tese.
Essas análises descritas na Seção 5.5 podem ser muito importantes tanto para o entendimento de como é a interação do receptor InhA com ligantes, quanto para a busca de novos inibidores para essa enzima.
Em esse sentido, já esta em finalização um trabalho de mestrado desenvolvido por o aluno Christian Quevedo no LABIO-GPIN para a busca de ligantes considerando informações da trajetória de receptores.
Em o próximo capítulo são descritos os experimentos de classificação com árvores de decisão, realizados também para entendimento das simulações de docagem molecular com o FFR, de forma a determinar a relação dos resíduos do receptor em contato com o ligante com o valor de FEB.
Em este capítulo serão apresentados os resultados obtidos com a aplicação de classificação com árvores de decisão utilizando o algoritmo J48 do WEKA.
Uma das principais contribuições desse capítulo é a metodologia proposta de discretização do atributo-alvo, a FEB, que se utiliza dos valores de moda e desvio padrão da mesma.
Essa metodologia proposta é então comparada com 2 métodos de discretização clássicos, por frequência e por intervalos de tamanho igual.
A comparação dos métodos é feita com base no impacto dos mesmos no resultado das árvores de decisão geradas.
De essa forma é possível indicarmos, ao final desse capítulo, qual é o método de discretização que mais se aplica a esse tipo de dados de docagem molecular.
Além de o mais, a partir de as árvores geradas nós apresentamos uma outra metodologia para análise das interações FFR-ligante, diferente da apresentada nas análises preliminares do Capítulo 5.
Os resultados apresentados nesse capítulo estão publicados:
Como resumo expandido no LNBI-LNCS durante o evento Brazilian Symposium on Bioinformatics de 2010;·
como artigo na conferência IADIS International Conference Applied Computing de 2010· como capítulo do livro Tópicos em sistemas colaborativos, multimídia, web e banco de dados.
Como uma parte do artigo que está na 3 rodada de revisão para publicação no WIREs Data Mining and Knowledge Discovery que consiste num resumo de todos os experimentos de mineração realizados durante o desenvolvimento desta Tese.
O principal objetivo da execução de experimentos de mineração de dados, no contexto desse trabalho, é a obtenção de modelos que, de alguma forma, selecionam quais as conformações do receptor se mostram mais promissoras, de forma que futuros experimentos de docagem molecular com diferentes ligantes utilizem não todas as conformações do receptor, mas sim somente aquelas que, para os ligantes analisados nesse trabalho, obtiveram os melhores resultados.
De essa forma, o tempo de execução de cada experimento de docagem molecular considerando um diferente complexo receptor-ligante será reduzido consideravelmente.
Em esse sentido, o primeiro trabalho que foi realizado para a geração de modelos que relacionassem os valores de distância entre os resíduos do receptor e o ligante e os valores finais de FEB consistiu na utilização da técnica de mineração de dados Classificação utilizando o WEKA para a execução dos experimentos.
Entre os diferentes algoritmos de classificação, neste trabalho, foi escolhido o algoritmos de classificação por árvores de decisão.
De acordo com Freitas A árvore de decisão tem a vantagem de sua saída graficamente representar a descoberta de conhecimento e indicar a importância dos atributos utilizados para predição.
E, como este trabalho está inserido num contexto interdisciplinar, era necessário a escolha de um algoritmo de classificação onde a saída fosse facilmente entendível e não uma caixa preta como outros algoritmos de classificação como Support Vector Machines (SVM) ou redes neurais.
O WEKA implementa vários algoritmos de classificação baseados em árvores de decisão, com por exemplo o ADTree, NBTree, J48 entre outros.
Em esta Tese, optou- se por o J48, implementação do WEKA para o algoritmo C4.
5. Todos os experimentos com esse algoritmo foram realizados considerando como arquivo de entrada somente o melhor run de cada execução de docagem, o que chamamos de experimentos com Best FEB, e todos os resultados utilizados são dos experimentos de docagem molecular Fase 1.
O resumo dos resultados desse experimento estão descritos na Tabela 3.1 do Capítulo 3.
Conforme descrito no Capítulo 4, na classificação, o atributo-alvo precisa ser uma classe, ou seja, o mesmo não pode ser um valor discreto como é o valor da FEB.
Sendo assim, é necessário discretizar- lo, sendo esse um dos passos do pré-processamento dos dados, descrito na próxima seção deste capítulo.
Discretização do Atributo Classe Segundo Tan Quando se decide discretizar um atributo contínuo transformando- o em categórico é necessário se decidir em quantas categorias isso será realizado e em como será feito o mapeamento.
O processo de discretização segue principalmente 2 passos:
Os dados contínuos são organizados de alguma forma, e então divididos em n intervalos especificados por n -- 1 pontos de divisão.;
É definido como todos os valores de um intervalo serão mapeados para um mesmo valor categórico.
Para as abordagens testadas neste trabalho, os dados foram discretizados em n $= 2, n $= 3, n $= 4 e n $= 5 intervalos.
Porém os resultados se mostraram mais promissores com 5 classes e devido a isso, são apresentados somente esses resultados utilizando n $= 5: Excelente, Bom, Regular, Ruim e Muito_ Ruim.
Há diferentes abordagens para a realização da discretização, entre as quais nesse trabalho foram aplicadas:
Discretização por frequência, discretização por grupos de mesmo tamanho (Método 2) e discretização por moda e desvio padrão (Método 3).
As Figuras 6.1 e 6.2 mostram os histogramas de distribuição do valor de FEB para os 4 ligantes, mostrando o resultado do número de instâncias que permaneceram em cada classe, de acordo com os 3 métodos de discretização aplicados.
Uma das abordagens mais simples é a discretização dos dados em conjuntos de mesmo tamanho, onde os dados são divididos num número específico de intervalos com aproximadamente o mesmo número de instâncias.
Então, considerando k o número de intervalos definidos por o usuário e m o total de instâncias, esse método divide a variável contínua em k intervalos onde cada intervalo contém aproximadamente m k instâncias.
Como resultado, descritos nas Figuras 6.1 (a) e 6.2 (a), é possível verificar que as classes ficam balanceadas, todas com aproximadamente o mesmo número de instâncias.
De acordo com Dougherty Este é o método de discretização mais simples, embora seja vulnerável a outliers.
Em essa abordagem, para cada atributo discreto a ser discretizado, seus valores são ordenados e então divididos em k intervalos, onde cada intervalo tem o mesmo tamanho.
Sendo k o número de intervalos, o tamanho dos intervalos de um determinado atributo x é definido por:
Xmax -- Xmin O resultado desse método de discretização está descrito nas Figuras 6.1 (b) e 6.2 (b).
É importante ressaltar que nas Figuras não está representada toda a distribuição de FEB, pois para intervalos que continham poucos valores, os mesmos foram condensados por o programa Microsoft Excel (onde foram feitos os histogramas) para melhorar a visualização.
De essa forma, principalmente para os histogramas dos ligantes TCL e PIF, tem- se a impressão de que os intervalos não têm o mesmo tamanho.
Como resultado desse método de discretização, as classes não ficaram balanceadas, em especial para os resultados dos ligantes TCL e PIF, como pode ser observado nas Figuras 6.1 (b) e 6.2 (b).
Considerando os valores da Tabela 3.1 do Capítulo 3 para os resultados Best FEB, para o TCL a FEB varia de 10.0 Kcal/ mol até 4.9 kcal/ mol, porém o valor de Moda é 9.0 kcal/ mol, um valor muito próximo de o valor de FEB máximo, o que divide então a maioria das instâncias como dos primeiro ou segundo intervalos.
O mesmo acontece na distribuição de FEB do ligante PIF, onde o valor da Moda é muito mais próximo de o valor máximo de FEB do que de seu valor mínimo, causando o mesmo efeito que ocorre no ligante TCL.
Esse método de discretização é proposto nesta Tese e está publicado em.
Ele considera os valores de moda e desvio padrão do atributo que está sendo discretizado.
O objetivo deste método de discretização proposto era de que atributos nas extremidades ficassem num mesmo intervalo, por exemplo, que os melhores valores de FEB permanecessem num intervalo e os piores em outro intervalo diferente.
Esse método de discretização segue a Equação classes e deve ser modificada se um número de classes diferente for considerado.
Em essa Equação F EB e representa o desvio padrão do atributo F EB.
Classe $= Excelente Bom Regular Ruim M uito_ Ruim se se se Me o -- 2 Me o -- Me o+ Me o+ 2 como se pode verificar nas Figuras 6.1 e 6.2 essa discretização para alguns ligantes é balanceada, porém a maioria das instâncias, para os 4 ligantes, ficou classificada como Regular.
Entretanto, o objetivo deste método de discretização foi alcançado para os 4 ligantes, permanecendo os melhores valores de FEB num grupo diferente dos piores valores, para os 4 ligantes.
Resultados com o Algoritmo J48 Para a execução dos experimentos com o algoritmo J48 do WEKA foram geradas entradas diferentes para os 4 ligantes e para os 3 métodos de discretização, totalizando 12 arquivos de entrada, onde os atributos preditivos são as 268 distâncias mínimas dos resíduos do receptor para cada ligante e o atributo-alvo é a classe de FEB de cada um dos resultados de docagem molecular considerados.
Os resultados dos experimentos utilizando o algoritmo J48 estão descritos na Tabela 6.1.
Para que os modelos gerados fossem mais legíveis, a maioria dos parâmetros do algoritmo J48 permaneceram com seus valores default, com exceção do parâmetro minN umObj o qual foi atribuído o valor de 50.
Esse parâmetro está relacionado com o número de instâncias mínimo em cada nodo folha.
Foram executados experimentos com esse parâmetro com valores de 30, 50, 75 e 100, sendo os melhores resultados obtidos da execução com minN umObj $= 50.
A avaliação dos modelos gerados é feita com a validação cruzada com 10 partições, conforme explicado no Capítulo na Tabela 6.1 de resultados tem- se:
Em a primeira coluna a descrição do método de discretização utilizado, na segunda, o nome do ligante, na terceira a Acurácia (Acc.)
de o conjunto de teste da validação cruzada, na quarta coluna é apresentado o tamanho da árvore final resultante em cada experimento (Tree Size -- Ts).
Em as colunas 5 e 6, os percentuais de MAE e RMSE respectivamente.
A sétima coluna contém o valor de F-measure (FM) de cada modelo gerado.
Para detalhes sobre as métricas Acc.,
Ts, MAE, RMSE e FM consultar o Capítulo 4.
Considerando o Método 1, por frequência, foram obtidos os piores resultados para todos os ligantes, o que mostra que esse tipo de discretização, para esse tipo de dado, não é apropriado.
O Método 2, por tamanho de intervalo igual, obteve melhores resultados para o PIF.
Entretanto para esse ligante, esse método tem 99,31% das instâncias classificadas como Excelente ou Bom (IEGC).
Isso significa que o modelo gerado por PIF-Método 2 não é útil para extração de conhecimento sobre os resíduos do receptor envolvidos em bons valores de FEB, pois a maioria das instâncias está classificada numa mesma categoria.
Para o TCL, o método 2 foi melhor em 3 das 5 métricas avaliadas.
Entretando, assim como ocorreu com o PIF, para o TCL-Método 2, 99,79% das instâncias estão classificadas como Excelente ou Bom.
De essa forma, se o valor de IEGC for considerado, mesmo com melhores valores de acurácia, esses modelos são distorcidos.
O Método 3, proposto em, obteve melhores resultados em todas as métricas para os ligantes NADH e ETH e em 2 das 5 métricas para o TCL.
Embora os valores não sejam melhores para todas as métricas, os modelos gerados com esse método de discretização foram mais legíveis, ou seja, permitem uma melhor interpretação por serem árvores com poucos nodos.
Consequentemente, esses modelos são mais aplicáveis, permitindo que mais informação sobre a interação receptor-ligante seja extraída dos modelos gerados.
Com o objetivo de tentar melhorar os modelos gerados por árvore de decisão, decidiu- se executar um segundo conjunto de experimentos de classificação, onde os atributos preditivos dos arquivos de entrada foram selecionados.
Essa seleção de atributos foi realizada para eliminar todos os atributos de distâncias mínimas de resíduos que em nenhum resultado de docagem molecular estabeleceram contato com o ligante, ou seja, que a distância mínima considerando todas as simulações de docagem foi maior do que 4,0 Å.
Assim, o total de atributos (preditivos mais o atributo-alvo) de cada arquivo de entrada para esse segundo conjunto de experimentos é de 70, 81, 66 e 88 para os ligantes NADH, PIF, TCL e ETH respectivamente.
Os resultados do segundo conjunto de experimentos com árvores de decisão estão resumidos na Tabela 6.2, cuja descrição das colunas e linhas é a mesma da Tabela 6.1.
Os resultados obtidos com esse segundo conjunto de experimentos com a seleção de atributos foi muito próximo de o primeiro conjunto considerando todos os atributos preditivos de distâncias mínimas.
A maior diferença ocorreu para o ligante TCL, que para esse segundo conjunto de experimentos obteve melhores resultados com o Método 3.
Para o Método 1 foram obtidos os piores modelos.
O Método 2 obteve melhores resultados para o PIF, mas assim como para o primeiro conjunto de experimentos, para esse ligante a maioria das instâncias foram classificadas como Excelente ou Bom.
O Método 3, utilizando moda e desvio, obteve os melhores resultados para a maioria das métricas para os ligantes NADH, TCL e ETH.
Considerando os resultados para o segundo conjunto de experimentos e com a discretização por o Método 3, as árvores de decisão geradas são analisadas a seguir.
A árvore obtida para o NADH-Método 3 está descrita na Figura 6.3.
As árvores para os ligantes PIF, TCL e ETH estão no Apêndice A. A raiz da árvore de decisão NADH-Método 3 é o resíduo THR 100.
Como podese observar analisando essa árvore de decisão, a distância desse resíduo do receptor para o NADH é determinante para definir se um resultado de docagem obteve bons valores de FEB (classes E, B e Re) ou valores ruins (R e MR).
Uma inspeção visual na estrutura cristalográfica desse receptor mostra que esse resíduo não é diretamente relacionado ao sítio ativo desse receptor.
A informação de que esse resíduo é importante para a determinação de bons ou ruins valores de FEB não teria sido obtida sem um processo de KDD.
Bom (E e B).
Em vermelho, as Classes Ruim e Muito Ruim (R e MR).
Em branco, a classe Regular (Re).
Para um melhor entendimento dos modelos gerados com o J48, podem ser extraídas regras de decisão a partir de as árvores de decisão.
De essa forma, a partir de as árvores para o NADH descrita na Figura 6.3 e para o PIF, TCL e ETH descritas nas Figuras do Apêndice A, pode- se extrair:
Considerações Finais Esse capítulo apresentou os experimentos de classificação com árvores de decisão executados durante o desenvolvimento desta Tese.
Para a utilização dessa técnica de mineração de dados foi necessária a discretização do atributo-alvo FEB.
Foram comparados 3 métodos de discretização, por frequência, por intervalos de tamanho igual (Método 2) e o método proposto utilizando moda e desvio padrão da distribuição de FEB dos resultados de docagem para os 4 ligantes (Método 3).
A comparação entre os métodos de discretização foi feita baseada na execução de dois conjuntos de experimentos:
Em o primeiro conjunto foram utilizados todos os atributos preditivos de distâncias mínimas entre os resíduos do receptor e os ligantes, no segundo conjunto foi aplicada uma seleção de atributos onde foram excluídos todos os atributos de distâncias mínimas onde o valor para todas as instâncias era maior do que 4,0 Å.
Os resultados para os 2 conjuntos de experimentos foram aproximados:
Para a maioria das métricas de avaliação o Método 1 apresentou resultados ruins, o TCL, NADH e ETH.
De essa forma, baseado nos resultados apresentados, o método de discretização que se mostrou mais apropriado para ser utilizado em resultados de docagem molecular foi o Método 3, que se utiliza dos valores de média e desvio da distribuição de FEB.
Além de o método de discretização proposto, a análise dos modelos induzidos obtidos da execução do algoritmo J48 do WEKA é uma outra contribuição deste trabalho, onde uma nova forma de análise da interação receptor-ligante e suas relações com os valores de FEB é apresentada.
Apesar de obter modelos interessantes, e permitir que fossem extraídos conhecimentos sobre a interação receptor-ligante, a utilização das árvores de decisão para a seleção direta de conformações do receptor para utilização em simulações de docagem com ligantes diferentes não é possível de ser feita diretamente.
Isso ocorre porque as conformações do receptor com melhor FEB são diferentes para os 4 ligantes, não sendo possível selecionar um conjunto único de conformações mais promissoras.
Além de o mais, devido a os resultados obtidos não serem promissores para todos os ligantes, acrescido da percepção de que, por causa de a variação de FEB ser muito sutil, a determinação de que uma instância pertencia a uma classe ou a outra era determinada por uma diferença de apenas 0, 1 kcal/ mol, optou- se por a busca de alternativas para o algoritmo J48.
Assim, para prosseguir a pesquisa e a busca de modelos que indicassem características importantes para serem utilizadas na seleção de conformações do receptor, se tornou necessário optar por o uso de outro algoritmo que não necessitasse que o atributo-classe fosse discretizado, aceitando o valor real da FEB.
O algoritmo encontrado com tais características foi o algoritmo de regressão Em esse capítulo são descritos os experimentos realizados com a técnica de mineração de dados de regressão por árvores modelo, utilizando o algoritmo M5P do WEKA.
As principais contribuições desse capítulo estão relacionadas à aplicação de regressão para esse tipo de dado de docagem molecular e a comparação dos resultados obtidos com árvores modelo para diferentes formas de preprocessamento desses dados.
Os resultados obtidos com o desenvolvimento deste trabalho estão nas seguintes publicações:
Os resultados apresentados nesse capítulo estão publicados:
Como artigo completo publicado no periódico BMC Genomics em 2010;
Bioinformatics; Como 2 resumos publicados e apresentados durante o evento ISCB-Latin America[ MAC10d, WIN10c] em 2010;·
como parte do capítulo do livro Tópicos em sistemas colaborativos, multimídia, web e banco de dados de 2010.
Em esse capítulo de livro é apresentado um exemplo de utilização de árvores modelo para o NADH;
Como uma parte do artigo que está na 3 rodada de revisão para publicação no WIREs Data Mining and Knowledge Discovery que consiste num resumo de todos os experimentos de mineração realizados durante o desenvolvimento desta Tese.
Como todos os atributos são valores numéricos e de acordo com Han e Kamber a abordagem mais abrangente para a predição de valores numéricos é a regressão, sendo então este um dos motivos de termos explorado essa técnica de mineração de dados.
Além disso, como temos interesse em entender os modelos gerados devido a importância de conhecermos a relação entre a menor distância receptor-ligante e o valor da FEB, estes modelos devem ser tão compreensíveis quanto possível.
Desconsiderando que não há um consenso na literatura sobre mineração de dados sobre qual a tarefa de mineração que fornece o resultado mais compreensível, há um acordo de que representações como árvores de decisão e conjunto de regras são melhores de entender do que uma &quot;caixa preta «como o resultado de SVM (Support Vector Machine) ou redes neurais.
Conforme descrito no Capítulo 4, para a predição numérica há dois tipos de árvores:
Árvores de regressão e árvores modelo.
A principal diferença entre ambas é sobre o conteúdo dos nodos-folha.
Cada nodo-folha numa árvore de regressão armazena um valor contínuo que corresponde a média do valor do atributo predito para as tuplas de teste enquanto que os nodos-folha nas árvores modelo contém modelos de regressão -- uma equação com múltiplas variáveis.
Devido a o tipo de resultado apresentado nos nodos-folha optamos por utilizar Árvores modelo (do inglês, Model Trees).
O algoritmo de árvore modelo utilizado neste trabalho foi o M5P disponível no pacote WEKA.
Como resultado da execução do algoritmo M5P tem- se uma árvore modelo e um conjunto de Linear Models (LMs).
Cada nodo da árvore modelo é um resíduo e um valor de distância determina a rota a percorrer na árvore.
Os nodos folhas todos são LMs.
Os LMs são equações que, a partir de uma séria de termos, cada um com seu coeficiente, mais um valor constante, determinam o valor do atributo-classe.
Todos os nodos da árvore modelo terminam num LM.
Em o contexto desse trabalho, os LMs são equações que determinam o valor da FEB a partir de um conjunto de resíduos e seus coeficientes somados a um valor constante.
A aplicação do algoritmo M5P, para todos os ligantes, é realizada considerando todos os runs de cada experimento de docagem molecular da Fase 1.
De essa forma, o total de instâncias de cada arquivo de entrada para o algoritmo M5P é de 11.284, 30.420, 28.370 e 30.430 para o NADH, PIF, TCL e ETH respectivamente.
Pré-processamento O primeiro trabalho desenvolvido utilizando o algoritmo M5P está descrito em[ WIN10c, WIN11].
Em este trabalho, a partir de os arquivos de entrada que contém os 268 atributos de distâncias mínimas entre os 268 resíduos do receptor e cada um dos ligantes e o atributo-alvo FEB, foram gerados novos arquivos de entrada a partir de diferentes estratégias de seleção de atributos.
A primeira estratégia de seleção de atributos é proposta em e é baseada no contexto.
É a mesma estratégia utilizada nos experimentos de classificação descritos do Capítulo Å.
Assim, essa estratégia de seleção de atributos baseada no contexto adota que distâncias maiores do que 4,0 Å correspondem a resíduos do receptor que não estabelecem contato com nenhum átomo do ligante em nenhuma das simulações de docagem consideradas, e podem, por esse motivo, serem removidos dos arquivos de entrada.
Sendo assim, ao aplicarmos essa estratégia de seleção de atributos baseada no contexto os arquivos de entrada permaneceram com o total de atributos descritos na segunda coluna da Tabela 7.1.
Onde: MS é a heurística de um subconjunto S que contém k atributos;
Rcf é a média de correlação entre determinado atributo f e o atributo-alvo;
Rf f é a média de correlação entre 2 atributos.
Além de as duas abordagens descritas acima para seleção de atributos, foi gerada uma terceira entrada que combina os atributos da seleção de atributos baseada no contexto e com o CFS.
O resultado dessa terceira abordagem de seleção de atributos está descrita na coluna 4 da Tabela 7.1.
Primeiro Conjunto de Experimentos Utilizando o Algoritmo M5P Foram executados experimentos de regressão com o M5P considerando 4 diferente entradas para cada ligante:
O arquivo de entrada inicial, com 268 atributos de distâncias mínimas mais o atributo-alvo, a FEB;
O arquivo de entrada após a aplicação da seleção de atributos baseada no contexto;
O arquivo de entrada após a aplicação do algoritmo CFS para seleção de atributos;
E O arquivo de entrada com os atributos resultantes da união dos atributos da seleção baseada no contexto e da aplicação do CFS.
O algoritmo M5P tem uma série de parâmetros, de entre eles, concentrou- se em encontrar um árvore modelo e com o número de LMs (Linear Modes).
O valor que utilizou- se nos experimentos foi de 1.000, pois com esse valor foram obtidas árvores modelo legíveis ao mesmo tempo que não tão pequenas que não fornecessem nenhuma informação relevante.
Tentou- se a execução do serem compreendidas, uma vez que continham milhares de nodos e LMS, 100, que também gerou árvores muito grandes, 1.000, o valor escolhido, 2.000 e 3.000 que começaram a gerar árvores muito pequenas e por isso com pouca informação.
É importante mencionar que nos experimentos descritos a seguir eliminou- se todas as instâncias cujo valor de FEB era positivo, uma vez que considerou- se essas instâncias como outliers.
Essa avaliação baseada no contexto tem por objetivo avaliar os modelos de acordo com os resíduos que estão presentes tanto nos nodos internos quanto nos LMs dos nodos-folha, entre outros) para verificar se os mesmos são resíduos dentro de o sítio de ligação do receptor, que realmente contribuem para o cálculo da FEB.
Para essa análise, um especialista definiu os resíduos do sítio de ligação do receptor, o valor definido como ESR, que corresponde a Seleção de Resíduos do Especialista, que para o receptor InhA totalizou 52 resíduos.
Os resíduos que aparecem nas árvores-modelo e nos LMs são definidos como M R (Model-tree Resíduos).
Para o cálculo do F -- score (Equação 4.5 do Capítulo 4), são utilizados as fórmulas de precisão e recall definidas em Han Para avaliação em mineração de texto:
ESR e os atributos recuperados correspondem aos atributos M R, que aparecem nas árvores ou nos LMs.
Analisando as Tabelas 7.2 e 7.3 é possível ver que a terceira estratégia de pré-processamento (algoritmo CFS) não é melhor para nenhum dos experimentos realizados.
Considerando as métricas preditivas (Tabela 7.2), os arquivos de entrada completo (1) e com seleção de atributos baseado recall, r.
Tabela 7.3: Avaliação baseada no contexto das árvores-modelo do primeiro conjunto de experimentos com o M5P.
Ligante Estratégia Pré-processamento Avaliação Precisão Recall F--score Todas as métricas, enquanto que para os demais ligantes, os melhores resultados alternam entre as estratégias 1 e 2.
Por outro lado, as métricas baseadas no contexto (Tabela 7.3) mostram que os Para analisar esses resultados em termos de suas significância estatística, foi aplicado o teste de Friedman com um nível de significância de $= 0, 05 em ambas as Tabelas.
Foram avaliados os valores MAE e RMSE da Tabela 7.2 e o F-score da Tabela 7.3.
Em a Tabela 7.3 foi avaliada se a estratégia 3 (CFS) era significativamente pior que as demais.
Foram obtidos os valores de p $= 0, 04 para o MAE e p $= 0, 054 para o RMSE, o que indica que provavelmente essa estratégia é realmente pior que as estratégias 1, 2 e 4.
Em relação a o F-score da Tabela 7.3, foi avaliado se a estratégia 2 é significativamente melhor que as outras.
Em este caso, podemos afirmar que é verdade, já que foi obtido o valor de p $= 0, 014.
Assim, é possível inferir que a abordagem de seleção de atributos baseada no contexto melhora os resultados iniciais.
E, como o interesse nessa pesquisa são os modelos induzidos, a métrica de avaliação também baseada no contexto se mostra como mais adequada.
Segundo Conjunto de Experimentos Utilizando o Algoritmo M5P O segundo conjunto de experimentos utilizando o M5P tem por objetivo efetivamente selecionar conformações mais promissoras do modelo FFR do receptor.
Para isso, foi gerado um novo arquivo de entrada, que seguiu a estratégia de seleção de atributos baseada no contexto, indicada por os resultados do primeiro conjunto de experimentos com o M5P como a melhor estratégia de seleção de atributos, mas utilizou uma margem maior de distância entre resíduos do receptor e ligante, de 5,0 Å para abranger um número maior de resíduos do receptor.
Assim, o total de atributos de distância considerados para cada ligante foi de 106, 122, 121 e 128 para o NADH, PIF, TCL e ETH respectivamente.
Os resultados das métricas preditivas para os 4 ligantes no segundo conjunto de experimentos com o M5P estão descritos na Tabela 7.4.
A descrição dessa tabela é mesma da Tabela 7.2.
ETH, em torno de 60%, que pode ser explicada por este ligante ser uma pró-droga, ele se liga ao receptor InhA como um aduto com o NADH (ETH-NADH), e nestes experimentos ele foi considerado sem o NADH.
De essa forma, o ETH explora uma região do sítio de ligação do receptor que na verdade não está acessível por a presença do NADH.
Não considerando essa diferença de correlação se comparada com os demais ligantes, 60% é um valor de correlação satisfatório para validar o modelo obtido.
As métricas preditivas consideradas mostram a qualidade dos modelos obtidos.
Entretanto, como o objetivo dos experimentos utilizando o M5P era de selecionar conformações do receptor, foi preciso estabelecer um critério de seleção de LMs e uma metodologia de análise das árvores modelo obtidas.
Essa metodologia consiste em identificar quais são os melhores LMs de cada modelo, para então percorrer as árvores e selecionar as conformações que pertencem os LMs selecionados.
As instâncias classificadas nas LMs selecionadas indicam as conformações do receptor mais promissoras para serem utilizadas em docagem molecular com outros ligantes.
Os 3 principais passos da metodologia desenvolvida são:
Percorre- se as árvores modelo utilizando- se o conjunto de teste para identificar que instâncias pertencem a cada um dos LMs;
Cuidadosamente define- se um critério de seleção dos LMs mais representativos;
Avalia- se se as conformações selecionadas são de fato promissoras.
O conjunto de teste considerado é o de instâncias BEST FEB, ou seja, das instâncias com somente o melhor valor de FEB de cada simulação de docagem.
Esse conjunto foi escolhido pois cada conformação do receptor está relacionada com somente uma instância no conjunto de teste.
Cada ligante tem seu conjunto de teste.
Sendo assim, iniciou- se por a implementação de scripts Python que mapeassem as instâncias dos conjuntos de teste para os respectivos LMs das árvores modelo de cada ligante (Figura 7.3 (a)).
Esses scripts (um para cada árvore modelo), ao serem executados, verificam a qual LM cada uma das instâncias do conjunto de teste pertence, gerando uma lista que relaciona conformação com LM (Figura 7.3 (b)).
A seguir, as conformações que pertencem ao mesmo LM são agrupadas e a média de FEB de cada grupo é calculada (Figura 7.3 (c)).
Por fim, tem- se condições de indicar quais LMs são mais representativos, e então utilizar- las para a seleção de conformações.
Os LMs mais representativos são aquelas cuja média de FEB das instâncias do LM é menor do que a média de todas as instâncias do conjunto de teste (Figura 7.3 (d)).
Decidiu- se por esse critério de seleção pois, se a média de FEB do grupo relacionado com determinado LM é menor do que a média do todo é porque agrupou instâncias com valores de FEB bem negativos, ou seja, justamente os que queremos selecionar.
Um exemplo desse processo está na Figura 7.3 para o conjunto de teste e árvore modelo do ligante NADH que ao final do processo somente o LM11 é selecionado.
Para exemplificar a metodologia de seleção de LMs considerou- se os resultados para o ligante PIF.
A Tabela 7.5 apresenta esses resultados onde as colunas 1 e 4 contém os LMs, as colunas 2 e 5 o total de instâncias em cada LM e nas colunas 3 e 6, a média de FEB das instâncias de cada LM.
Os LMs selecionados estão destacados na tabela.
Kcal/ mol, os LMs selecionados para esse ligante são LM1, LM2, LM3, LM5 e LM7.
Os resultados dos LMs selecionados para cada um dos demais ligantes estão descritos nas Tabelas Por exemplo, para o NADH, a média de FEB é de 12,9 kcal/ mol, o que seleciona somente o LM11 como promissora.
Tabela 7.8: Análise dos LMs geradas para o ligante ETH.
Total Instâncias Média FEB Kcal/ mol Para verificar se as conformações selecionadas são realmente as conformações com melhores resultados de docagem molecular, os seus valores de FEB foram cuidadosamente avaliados.
Para isso, as instâncias dos conjuntos de teste de cada ligante foram organizadas numa ordem ascendente por FEB.
Então, compararam- se as conformações no topo da lista ordenada (ou seja, os de FEB mais negativa) com as conformações dos LMs selecionadas.
Os resultados obtidos estão descritos na Tabela 7.9.
Em a coluna 1 tem- se os ligantes, nas colunas 2, 3 e 4 o número total de conformações selecionadas que estão no TOP 10, 100 e 1000 da lista ordenada por FEB, respectivamente.
A coluna 5 mostra o total de conformações selecionadas para cada ligante.
Baseado nos dados descritos na Tabela 7.9 nota- se que as conformações selecionadas são conformações que obtiveram bons resultados em docagem molecular para os 4 ligantes.
Para o NADH e PIF, dos 10, 100 e 1.000 Top melhor FEB, a metodologia proposta selecionou quase 100% das melhores conformações.
Para a ETH, foram selecionados os 10 melhores, 92% dos 100 melhores e 617 dos 1.000 melhores, porém esse ligante foi o que selecionou menos conformações.
Os piores resultados foram para o TCL, onde das 1.780 conformações selecionadas, somente 610 estão entre as 1000 melhores para esse ligante.
De essa forma, nesse segundo conjunto de experimentos com o algoritmo M5P a maior contribuição é a estratégia de seleção de conformações que foi capaz de selecionar as conformações mais promissoras.
Além disso, as análises das árvores modelo indicam quais são os resíduos do receptor mais importantes para a determinação de bons e ruins valores de FEB.
Por exemplo, a partir de a árvore do NADH e do LM selecionado para esse ligante (Tabela 7.6) é possível observar que todas as conformações do receptor cuja distância do resíduo THR100 é maior do que 11,49 Å são consideradas conformações promissoras.
A discussão de como essa informação será utilizada para a busca de novos inibidores para essa enzima está fora de o escopo desse trabalho e consiste em trabalhos futuros a serem realizados.
Considerações Finais Em este trabalho, a partir de os arquivos de entrada descritos no Capítulo 5, que contém os 268 atributos de distâncias mínimas entre os 268 resíduos do receptor e cada um dos ligantes e o atributo-alvo FEB, foram gerados novos arquivos de entrada a partir de diferentes estratégias de seleção de atributos.
A primeira estratégia de seleção de atributos é proposta em e é baseada no contexto.
A segunda estratégia utiliza o algoritmo de aprendizagem de máquina CFS (Correlation based Feature Selection) para a seleção automática de atributos.
E a terceira estratégia de seleção combina as duas primeiras.
São então comparados os resultados do algoritmo M5P para as 3 diferentes entradas utilizando as métricas clássicas RMSE, MAE e Correlação assim como utilizando métricas também baseadas no contexto.
Analisando estatisticamente os resultados obtidos com o algoritmo M5P com o Teste de Friedman, observou- se que a abordagem baseada no contexto melhorou significativamente as métricas dos modelos gerados a partir de as diferentes entradas, enquanto que a seleção de atributos com o algoritmo CFS obteve os piores resultados em relação as métricas de avaliação preditivas.
De essa forma, os resultados do primeiro conjunto de experimentos com o M5P mostram o quanto é importante o pré-processamento para a obtenção de modelos mais acurados e interpretáveis.
Como trabalho futuro, pretende-se utilizar as informações dos melhores modelos gerados para a seleção de novos compostos candidatos baseado em como o modelo FFR interage com os ligantes já estudados.
Para o segundo conjunto de experimentos com o M5P foram utilizados arquivos de entrada com seleção de atributos baseada no contexto mas com uma distância de 5,0 Å do ligante.
Os resultados do segundo conjunto de experimentos com o M5P foram modelos que de acordo com as métricas de avaliação preditivas são bons modelos.
Com estes modelos, foi aplicado um pós-processamento nas árvores modelo geradas onde, para cada LM foi calculado a média de FEB das instâncias associadas a esse LM.
A partir desses valores foi determinado que um LM é representativo se a média de FEB é menor ou igual a média de FEB do conjunto de teste.
As instâncias nos LMs selecionados são então consideradas como mais promissoras, o que totalizou 1.521 conformações para o NAD, 1.780 permitiu o desenvolvimento de um critério de seleção de LMs, que por sua vez, foram capazes de selecionar esse conjunto de conformações do receptor mais promissoras.
Sendo assim, as maiores contribuições desse capítulo dizem respeito ao pré-processamento e avaliação dos modelos baseados no contexto, que produziram melhores modelos e a metodologia de pós-processamento que permitiu a indicação de conformações do receptor mais promissoras para cada um dos ligantes.
Apesar de os resultados de todos os experimentos com o M5P serem interessantes, a utilização dos mesmos, diretamente para seleção de conformações em futuros experimentos de docagem molecular não é promissora.
O maior problema encontrado foi de que as conformações mais promissoras eram diferentes para cada um dos ligantes, o que dificulta a utilização das mesmas para análises de interação com novos compostos obtidos de bancos de compostos como o ZINC.
Ou seja, não é possível, a partir desses resultados, estabelecer um conjunto único de conformações mais relevantes.
Outro problema encontrado é que, para se utilizar os modelos gerados para predizer o valor de FEB de novos ligantes é necessário saber as distâncias mínimas dos mesmos para os resíduos do receptor, informação que somente é obtida após a execução da docagem molecular, o que também dificulta a utilização dos modelos gerados com o M5P para efetivamente selecionar conformações do receptor para compostos ainda não testados.
Por os motivos descritos acima optou- se por não mais se utilizar como entrada nos experimentos com mineração de dados, os resultados de docagem molecular e sim, diretamente, as conformações do receptor FFR.
E, como não será mais utilizado os resultados de docagem, não tem- se mais um atributo-classe FEB.
Assim, a técnica de mineração a ser aplicada deverá ser de aprendizado nãosupervisionado, em a qual a classe de cada instância é desconhecida assim como o total de grupos e a estrutura dos mesmos.
A técnica de aprendizado não-supervisionado escolhida é a de Agrupamento.
Para a utilização dessa técnica de mineração de dados não foi utilizado os algoritmos de agrupamento implementados no WEKA e sim os algoritmos de agrupamento descritos em, que estão implementados no módulo Ptraj9.
Os experimentos de agrupamento serão descritos no próximo capítulo.
Este capítulo descreve os experimentos realizados com a técnica de mineração de dados nãosupervisionada Agrupamento.
O principal objetivo desse conjunto de experimentos é de agrupar conformações mais similares do modelo FFR do receptor, onde a entrada dos algoritmos de agrupamento são as próprias conformações.
Os agrupamentos gerados nesses experimentos são então utilizados por o padrão P-MIA, proposto em para ser utilizado em Bioinformática com o propósito de reduzir a quantidade total de conformações a serem processadas em experimentos de docagem molecular com o modelo FFR e garantindo que as melhores conformações continuariam a ser consideradas.
Assim, a partir de os grupos de conformações obtidos, após a execução do agrupamento, e utilizando o P-MIA, há um ganho em relação a quantidade de conformações que não precisam ser processadas, o que é identificado dinamicamente, sem interferência do usuário e realizado em paralelo, aumentando o desempenho desse tipo de experimento e permitindo que novos compostos sejam testados com um tempo de processamento reduzido.
A técnica de agrupamento já foi utilizada em vários trabalhos para o agrupamento de conformações do receptor resultantes de DM.
Um exemplo é o trabalho apresentado por Torda e van Gunsteren onde 2 algoritmos clássicos de agrupamento Single Linkage e Hierarquical são aplicados a um subconjunto de átomos que representam as conformações de uma trajetória de simulação por a DM.
Mais recentemente, Shão Implementaram 11 algoritmos de agrupamento (Average Linkage, Bayesian, Centripetal, Centripetal Complete, COBWEB, Complete Linkage, Edge Linkage, Hierarchical, K--means, Linkage e Som) onde comparam seus resultados e os utilizam para entender os dados de simulações por a DM.
Os algoritmos de agrupamento utilizam diferentes funções de similaridade para determinar a proximidade dos dados do conjunto de entrada.
O tipo de função de similaridade deve estar de acordo com os dados de entrada.
Em os trabalhos e, a medida de similaridade utilizada por todos os algoritmos de agrupamento foi a de RMS das coordenadas cartesianas dos átomos considerados.
Essa medida, definida por Dab RM S, corresponde a soma dos quadrados das distâncias sobre todos os pares ij de N átomos que estão sendo considerados das conformações a e b (dij é a distância tridimensional entre os átomos i e j):
De a, b RM S $= (da -- db ij) N i\ j ij Além de a execução de experimentos de agrupamento visando a redução do custo computacional de docagem com o FFR, nesta Tese propõe- se a definição de novas funções de similaridade desenvolvidas com o objetivo de agrupar as conformações do receptor de forma mais eficaz.
Essas novas funções de similaridade são definidas neste capítulo juntamente com a descrição de todos os experimentos de agrupamento executados.
Esses foram realizados com diferentes conjuntos de átomos de entrada uma vez que, de acordo com Shão, os resultados dos algoritmos de agrupamento são fortemente dependentes da escolha de átomos para a comparação par-a-par da função de similaridade.
Esse capítulo compreende:
Os testes realizados com os algoritmos implementados por para a determinação do número de grupos;
A descrição de como os dados de saída do programa LigPlot foram preparados para serem utilizados nas novas funções de similaridade e como essas foram definidas.
Após são descritos todas as configurações de experimentos de agrupamento executados, onde os resultados obtidos com a função clássica RM S são comparadas com os das funções desenvolvidas.
Por fim, são apresentadas análises utilizando o P-MIA que comparam a função RM S com uma das funções desenvolvidas mostrando efetivamente o ganho de processamento obtido com a utilização do P-MIA em conjunto com os resultados dos algoritmos de Agrupamento.
Determinação do Número de Grupos O primeiro conjunto de experimentos de Agrupamento aplica os 10 diferente algoritmos implementados em com a função de similaridade RM S definida por Dab RM S.
Estes foram executados com o objetivo de melhorar o entendimento a respeito de a implementação destes algoritmos assim como, para estabelecer o total de grupos ideal.
Os 10 algoritmos utilizados foram:
Average Linkage (Average), Bayesian, Centripetal Complete (Centripetal_ Comp), Complete Linkage (Complete), Edge Linkage (Edge), Hierarchical, Linkage, K--means e Som.
Em todos os experimentos de agrupamento desta Tese a DM utilizada como entrada é a de 3.100 ps do receptor InhA descrita na Seção 3.4 do Capítulo 3.
Em estes primeiros experimentos, os 10 algoritmos foram executados com 2 conjuntos de átomos de entrada:
ALL -- Considera os átomos de Carbono- dos 268 resíduos do receptor.
Ou seja, nos experimentos de agrupamento com essa entrada, a função de similaridade entre duas conformações Vinte cinco_ RES ou 25_ RES -- Considera os átomos de Carbono- dos 25 resíduos selecionados na análise descrita no Capítulo 5 como os que mais interagem com os ligantes estudados.
Em esse caso, a função é calculada sobre N $= 25 átomos de cada conformação considerada.
Esse estudo inicial tinha por objetivo analisar se as entradas ALL e 25_ RES para os algoritmos causavam diferentes resultados, principalmente em relação as métricas de avaliação de Agrupamento DBI e pSF descritas no Capítulo 4, Seção 4.3.3.11.
A Figura 8.1 mostra os gráficos da métrica DBI e a Figura 8.2 contém os mesmos gráficos para a métrica pSF.
Em as Figuras 8.1 e 8.2 têm- se os resultados para experimentos de agrupamento com 10, 20, 30, 40, 50, 60, 70, 80 90 e 100 grupos, para os 10 algoritmos.
Esses resultados mostram que:
A partir de aproximadamente 30 grupos não há mais modificação nos valores de ambas as métricas.
Em relação a métrica pSF não há muita diferença entre os valores para os conjuntos de átomos de entrada ALL e 25_ RES, com exceção para os algoritmos Centripetal, Linkage e Edge que apresentam valores próximos de zero para a 25_ RES.
Para a métrica DBI, as entradas diferem mais para os 10 algoritmos, porém alternam em melhores resultados.
Logo, não é possível afirmar o melhor conjunto de átomos de entrada para os experimentos de agrupamento.
Assim, ambas as entradas serão utilizadas nos experimentos subsequentes.
Em esses resultados, os melhores resultados em relação a métrica DBI foram obtidos para os algoritmos Centripetal, Linkage e Edge.
Entretanto, os 3 algoritmos apresentam valores de pSF ruins, principalmente para a entrada 25_ RES.
De essa forma, foi decidido continuarmos utilizando os 10 algoritmos nos demais experimentos, pois somente com este primeiro conjunto de resultados não há como afirmar quais algoritmos apresentam melhor comportamento para os diferentes números de grupos e entradas.
Para a métrica DBI, a partir de 20 grupos, os valores tendem a se manter iguais.
Em relação a pSF, os melhores valores, que correspondem aos maiores valores de pSF se mantém até no máximo 30 grupos, sendo melhores para até 20 grupos.
Assim, foi estabelecido que para mais de 20 grupos não há mais ganho nos agrupamentos para ambas as entradas em todos os algoritmos.
Para um entendimento mais detalhado dos resultados, para agrupamentos de até 20 grupos, foi realizado o segundo conjunto de experimentos com os mesmos conjuntos de átomos de entrada ALL e 25_ RES, mas com 2 até 20 grupos, variando de 1 em 1.
Ou seja, foram realizados testes grupos.
Os resultados estão descritos nos gráficos das Figuras 8.3 e 8.4 para as métricas DBI e pSF respectivamente.
Em preto nos gráficos têm- se os resultados para entrada ALL e em vermelho os resultados para a entrada 25_ RES.
como se pode perceber analisando os resultados dos gráficos, os valores de DBI foram em média menores, atingindo um máximo de 3,5 e os valores de pSF foram bem maiores, atingindo valores máximos de aproximadamente 1.000 quando o máximo com os experimentos de 10-100 grupos foi de 300 para essa métrica.
Essa análise indica que os resultados com até, no máximo, 20 grupos apresentam melhores valores de DIB e pSF do que a análise inicial com um mínimo de 10 grupos.
DBI e pSF entre ambas as entradas.
Com esse estudo mais detalhado dos resultados para 2-20 grupos é possível concluir que com mais do que 10 grupos não se tem mais ganho em ambas as métricas, os valores de DBI se mantém iguais e os valores de pSF tendem somente a diminuir para os 10 algoritmos.
De essa forma, a partir desses gráficos das Figuras 8.3 e 8.4 decidiu- se estabelecer o máximo de 10 grupos para os experimentos subsequentes.
Apesar de os algoritmos Centripetal, Edge e Linkage apresentarem valores ruins de pSF independente do total de grupos, isso somente ocorre para a entrada 25_ RES.
Como decidiu- se continuar a utilização de ambos conjuntos de átomos de entrada, esses algoritmos não foram descartados por apresentarem bons resultados para a entrada ALL.
Assim, resumidamente, estabeleceu- se que nos experimentos seguintes de agrupamento:
Devem ser testados agrupamentos de 2 até no máximo 10 grupos;
Serão utilizadas ambas as entradas ALL e 25_ RES;
Os 10 algoritmos devem ser considerados nos experimentos.
Funções de Similaridade Para as novas funções de similaridade decidiu- se incorporar ao valor de distância De a, b RM S informações adicionais que permitissem uma melhora na determinação da similaridade entre duas conformações.
Para isso, optou- se por utilizar os resultados de processamento do software LigPlot.
O LigPlot, ao receber como entrada um arquivo.
PDB com o receptor e o ligante, fornece como saída uma lista de contatos hidrofóbicos e ligações de Hidrogênio estabelecidos entre o complexo receptor-ligante.
Portanto, a partir de o resultado do LigPlot, tem- se uma lista de quantos contatos cada conformação estabeleceu com determinado ligante.
Essa informação é incorporada ao valor de distância entre conformações para que a forma como estas estabelecem contatos com um mesmo ligante auxilie na determinação da similaridade entre as mesmas.
As novas funções de similaridade, assim como a descrição de como foram preparados os resultados do LigPlot para serem utilizados em conjunto com o valor de De a, b RM S são descritas nas próximas seções.
Para realizar essa análise sobre como cada conformação estabelece contatos, foi necessário escolher qual ligante seria mais apropriado.
Foram escolhidas 2 entradas diferentes para o LigPlot:
Uma entrada que considera o substrato THT e outra que utiliza o substrato THT e o ligante NADH.
O substrato THT foi escolhido pois ele encontra- se na mesma região do sítio de ligação onde possíveis inibidores da InhA se ligam.
E a entrada com o THT+ NADH foi utilizada pois a região ocupada por estes na InhA compreende a maior parte do sítio de ligação deste receptor.
Isso significa que, analisando como o THT ou o THT+ NADH interagem com cada uma das conformações, pode- se ter uma visão de como outros compostos podem estabelecer contatos com este receptor.
Como explicado na Seção 3.1.4 do Capítulo 3, como entrada para o LigPlot é fornecido um arquivo PDB que contém o receptor e o (s) ligante (s).
Sendo assim, para analisar os contatos estabelecidos entre cada uma das 3.100 conformações da DM da InhA e o substrato THT é necessário preparar 3.100 PDBs que contenham as estruturas da DM juntamente com a estrutura do THT, estando este posicionado corretamente no sítio de ligação do receptor.
Para isso foram utilizados os seguintes arquivos:
A estrutura da enzima InhA com código PDB:
1 BVR obtida no Protein Data Bank.
Essa estrutura da InhA foi determinada com o NADH e o THT e apresenta 6 cadeias A, B, C, D, E e F. Para a preparação dos arquivos para o LigPlot foi utilizada somente a Cadeia C deste arquivo PDB;
A primeira estrutura da DM da InhA, denominada conformação_ 1.
DM. Utilizando o software VMD de visualização de estrutura de moléculas biológicas executou- se os seguintes passos:
As duas estruturas PDB foram abertas no VMD:
1 BVR_ cadeiaC e conformação_ 1.
DM; A estrutura 1 BVR_ cadeiaC é sobreposta na conformação_ 1.
DM. Assim, o substrato THT fica posicionado corretamente na conformação_ 1.
DM e consequentemente nas demais conformações da DM.
O arquivo PDB do THT posicionado é armazenado;
A Figura 8.5 mostra o resultado desse posicionamento.
A estrutura da 1 BVR está em magenta, a estrutura conformação_ 1.
DM está em vermelho.
Em verde está o THT posicionado.
Para a preparação da segunda entrada para o programa LigPlot utilizou- se os seguintes arquivos:
A cadeia C da estrutura da InhA com o código PDB:
1 BVR;
A estrutura da InhA como código PDB:
1 ENY.
Essa estrutura contém o ligante NADH e foi utilizada como estrutura inicial na DM considerada nesta Tese;
A primeira estrutura da DM -- conformação_ 1.
DM; Utilizando o software VMD preparou- se esses arquivos da mesma forma que o InhA+ THT, mas neste segundo a estrutura 1 ENY também foi sobreposta na conformação_ 1.
DM, posicionando o ligante NADH corretamente na conformação_ 1.
DM e consequentemente nas demais da DM.
Os arquivos do THT e NADH posicionados são armazenados.
A Figura 8.6 mostra o resultado desse posicionamento onde a estrutura da 1 BVR está em magenta, a estrutura conformação_ 1.
DM está em vermelho e a estrutura da 1 ENY está em azul.
Em verde está o THT posicionado e em amarelo o NADH.
A estrutura da 1 BVR está em magenta, a estrutura conformação_ 1.
DM da DM está em vermelho, ambas na forma de New_ cartoon.
Em verde está o THT na forma de VDW.
DM está em vermelho, a estrutura da 1 ENY está em azul, todas na forma de New_ cartoon.
Em verde está o ligante THT na forma de VDW e em amarelo o ligante NADH também na forma de Para a execução do LigPlot com cada uma das 3.100 entradas foi desenvolvido um script que finaliza a preparação da entrada, executa o LigPlot e processa a saída.
Esse script segue a estrutura do fluxograma descrito por a Figura 8.7 que utiliza como exemplo o THT.
Para cada estrutura da DM, é concatenado no seu final o PDB do THT ou NAD+ THT já posicionado.
Esse PDB é então processado por o LigPlot;
O resultado do LigPlot é um conjunto de arquivos, entre eles:
LigPlot. Hhb, LigPlot.
Nnb, LigPlot.
Sum, LigPlot.
Ps. De estes arquivos são armazenados o LigPlot.
Sum e o LigPlot.
Ps.. O LigPlot.
Sum é então processado para que os contatos estabelecidos entre cada conformação da DM e o THT ou THT+ NADH sejam contabilizados em 2 tabelas de resultados:
A Tabela HHB, que contém os totais de ligações de hidrogênio e a Tabela NNB com os totais de contatos hidrofóbicos.
Como exemplo das tabelas HHB e NNB é descrita a Tabela 8.1.
Em essa tabela cada linha corresponde a uma conformação processada (InhA+ THT ou InhA+ THT+ NADH) e cada coluna é um dos 268 resíduos do receptor.
Em cada célula é armazenado o total de contatos que determinado resíduo do receptor estabeleceu com o ligante (THT ou NAD+ THT).
Em a última coluna tem- se a soma dos contatos que determinada conformação estabeleceu com o ligante.
As duas primeiras funções de similaridade desenvolvidas, chamadas de T CN e T CN_ M ult2 utilizam os valores de totais de contatos estabelecidos por cada conformação com as 2 entradas analisadas:
THT e THT+ NAD.
O objetivo das funções T CN e T CN_ M ult2 é de modificar o valor de distância entre 2 conformações baseado em quantos contatos as mesmas estabeleceram.
A preparação dos dados resultantes do processamento do LigPlot para utilização na função T CN compreende as seguintes etapas (estes passos são executados para as 2 entradas separadamente):
As tabelas NNB e HHB foram analisadas para a determinação de quantos resíduos do receptor estabeleceram contato com o THT e com o THT+ NADH.
Todos os resíduos que não estabeleceram nenhum contato ao longo de os 3.100 resultados avaliados foram descartados, o que resultou nos resíduos descritos na Tabela 8.2.
Os valores destacados nessa Tabela correspondem aos resíduos que são coincidentes com os Top 25 resíduos (análise descrita no Capítulo 5 dos resíduos que mais interagem com os ligantes estudados).
Foram criadas 2 tabelas NNB+ HHB, uma para cada uma das entradas THT e THT+ NADH, que correspondem a soma dos contatos da tabela NNB com os contatos da tabela HHB.
Para cada uma das tabelas NNB+ HHB foi calculado o total de contatos estabelecidos por cada conformação, somando- se os contatos de todos os resíduos.
A partir de os valores dessa coluna com totais de contato, foi calculado o valor máximo de contatos.
Com esse valor máximo, os demais valores de totais de contatos foram normalizados, conforme mostra a Tabela 8.3 de exemplo, que é resultado do processamento da entrada THT.
Em essa tabela têm- se nas linhas as conformações do modelo FFR e nas colunas os resíduos do receptor, sendo a penúltima coluna, o totais de contatos de cada conformação e a última coluna corresponde ao total de contatos normalizado (V_ TCN), baseado no valor máximo de contatos, que neste exemplo é o valor de 146.
Para a função T CN foram utilizados os valores da primeira coluna (número da conformação) e da última coluna (totais de contato normalizados).
Sendo a função RM S determinada por Dab RM S, a T CN utiliza como função de similaridade o valor de Dab T CN, que consiste em:
De a, b T CN $= De a, b RM S V_ T CNa+ V_ T CNb Onde, V_ T CNa é o total de contatos normalizado da conformação a e V_ T CNb é o valor total de contatos normalizados da conformação b.
Essa função foi pensada dessa forma tentando agrupar conformações que tenham mais contatos, diminuindo a distância entre ambas e aumentando a distância entre conformações que apresentem ambas poucos contatos.
Por exemplo, sendo conformação_ 1, conformação_ 2 e conformação_ 4 três conformações que determinado algoritmo de agrupamento está verificando a similaridade.
Supondo que o valor de D1, 2 RM S e D1, 4 RM S entre as conformações seja de 1,0 Å.
Sendo os valores de V_ T CN das conformações os descritos na Tabela 8.3, tem- se:
Com este exemplo, é possível ver que, como a conformação_ 1 tem um V_ TCN igual a 0, 49 e a conformação_ 2 apresenta um V_ TCN igual a 0, 26, a distância entre essas conformação antes de 1,0 Å, com a função D1, 2 T CN passa a ser de 1,33 Å.
Ou seja, a distância entre essas duas conformações é aumentada devido a o baixo V_ TCN da conformação_ 2.
Entretanto, com o exemplo com a conformação_ 1 e conformação_ 4, a distância entre as conformações não se altera pois ambas as conformações apresentam um mesmo valor de V_ TCN.
Para cálculos de D1, 2 T CN entre conformações com altos valores de V_ TCN a distância entre as mesmas diminui, o que aumenta a possibilidade de ambas conformações permanecerem num mesmo grupo ao final da execução do algoritmo de agrupamento.
Como uma variação da função T CN, definiu- se a função T CN_ M ult2.
Sendo a função RM S definida por o valor de Dab RM S, a função T CN_ M ult2 é definida por:
De a, b T CN_ M ult2 $= 2 De a, b RM S V_ T CNa+ V_ T CNb Onde, V_ T CNa é o total de contatos normalizado da conformação a e V_ T CNb é o valor total de contatos normalizados da conformação b.
A única diferença da função T CN_ M ult2 para a T CN é o 2 no numerador multiplicando o valor de De a, b RM S. Esse valor 2 foi escolhido devido a os valores de V_ TCN do denominador da função no máximo poderem somar 2.
De essa forma, somente no caso extremo de ambas as conformações terem valores de V_ TCN próximos ao máximo, o valor da De a, b T CN_ M ult2 não modificaria o valor da Dab RM S, em todos os outros casos, essa valor é influenciado por os valores de V_ TCN.,
divide- se o valor de De a, b RM S por o valor de CORRELACAOa, b, obtendo- se como resultado o valor de distância entre as conformações agora de 1, 10 Å.
Isso significa que os valores de CORRELACAO entre as conformações determinam o quanto a distância original RM S deverá ser aumentada para refletir a relação que há entre as conformações em relação a os totais de contatos estabelecidos.
É importante ressaltar que nas funções CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 não estão sendo considerados somente os totais de contatos, mas também quais resíduos estão estabelecendo os contatos.
A partir de a definição de que seriam utilizados esses valores da matriz CORRELACAO, foi necessário a definição de como esses valores seriam compostos com o valor de De a, b RM S. Assim, foram desenvolvidas as 3 funções descritas a seguir.
A função mais simples é a CORREL_ V 1: De a, b RM S De a, b CORREL_ V 1 $= De a, b RM S CORRELACAOa, b se CORRELACAOa, b 0, 3 se CORRELACAOa, b\&gt; 0, 3 O teste do valor de CORRELACAO é necessário porque alguns valores da matriz são negativos ou muito pequenos, próximos de zero.
Esses valores, quando utilizados nos algoritmos de agrupamento, causavam erros na execução pois geravam valores de distância muito grandes, não tratáveis por os algoritmos (com exceção dos algoritmos Bayesian, Som e K--means que terminavam sua execução mesmo quando os valores da matriz CORRELACAO eram utilizados diretamente).
As funções CORREL_ V 2 e CORREL_ V 3 foram elaboradas para compor o valor de De a, b RM S de maneira diferente da CORREL_ V 1.
Foram testadas diversas funções para determinar essa composição e os melhores resultados foram obtidos com as fórmulas:
De a, b CORREL_ V 2 $= 10 De a, b RM S 10CORRELACAOa, b De a, b CORREL_ V 3 $= e De a, b RM S a, b Para melhorar o entendimento, o gráfico da Figura 8.8 mostra os valores de distância entre 2 conformações obtidos com as funções CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3, supondo sempre que o valor de De a, b RM S seja igual a 1, 0 Å.
Em esse gráfico os valores de distância De a, b CORREL_ V 1, De a, b CORREL_ V 2 e De a, b CORREL_ V 3 estão em Y e os valores da matriz CORRELACAO estão em X, onde foram exemplificados valores de correlação maiores do que 0,15.
Como é possível ver com esse gráfico, a função CORREL_ V 2 modifica mais drasticamente o valor de De a, b RM S, enquanto que a função CORREL_ V 3 modifica esse valor de forma mais sutil.
O valor de CORREL_ V 1 não muda até que a correlação atinja o valor de 0,3.
Com essas 3 diferentes funções utilizando os valores da matriz CORRELACAO buscou- se analisar o impacto de uma função de similaridade linear e duas exponenciais (CORREL_ V 2 e CORREL_ V 3) nos algoritmos de agrupamento.
Resultados dos Experimentos de Agrupamento Os experimentos com os 10 algoritmos de agrupamento foram divididos em 2 grupos para facilitar a discussão dos resultados obtidos:
RM S X T CN:
Apresenta os resultados para as funções RM S comparadas com os resultados das funções T CN e T CN_ M ult2 que utilizam os totais de contatos normalizados estabelecidos entre as conformações do receptor e o THT ou THT+ NADH;
RM S X CORREL:
Descreve os resultados de RM S, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 para ambas as entradas THT e THT+ NADH Para os 2 grupos de experimentos foram utilizadas as seguintes conjuntos de átomos de entrada:
ALL e 25_ RES:
Consideram os átomos de carbono- dos 268 resíduos do receptor e dos Top 25 resíduos, respectivamente;
20_ RES:
Considera os átomos de carbono- dos 20 resíduos do receptor que aparecem estabelecendo ao menos 1 contato (HHB+ NNB) com o THT.
Esses 20 resíduos estão descritos na Tabela 8.2.
Essa entrada somente foi considerada em experimentos com as funções de similaridade quando executadas com a entrada THT;
46_ RES:
São utilizados nesta entrada os átomos de carbono- dos 46 resíduos do receptor que estabelecem contatos HHB ou NNB com o THT+ NADH.
Essa entrada de algoritmo de agrupamento somente foi considerada em experimentos com funções de similaridade quando a entrada da mesma eram os resultados de THT+ NADH;
Esse primeiro grupo de resultados correspondem aos experimentos com as funções RM S, T CN e T CN_ M ult2 para os 10 algoritmos de agrupamento e subdividiu- se em 2 sub-grupos:
Os experimentos onde a entrada para as funções de similaridade T CN e T CN_ M ult2 foram os resultados dos contatos do receptor-THT e os experimentos cuja entrada para as funções são os resultados de contatos do receptor-THT+ NADH.
É importante diferenciar os conjuntos de átomos de entrada para os algoritmos de agrupamento, que correspondem aos átomos considerados para os cálculos de similaridade entre 2 conformações, das entradas para as funções de similaridade, que indicam qual tabela de contatos resultante do LigPlot foi aplicada em cada momento (THT ou THT+ NADH).
As Figuras 8.9 e 8.10 mostram os resultados da métrica DBI para os 10 algoritmos, onde nas funções T CN e T CN_ M ult2 foi aplicada a entrada THT+ NADH.
As Figuras 8.11 e 8.12 descrevem os resultados para da métrica pSF para essa mesma configuração de experimento.
Essa configuração de experimento utilizou como conjunto de átomos de entrada ALL, 46_ RES e 25_ RES.
Em as Figuras 8.9, 8.10, 8.11 e 8.12 os gráficos de cada linha correspondem a um mesmo algoritmo, e as colunas são os diferentes conjuntos de átomos de entrada.
Os resultados dessa mesma configuração de experimento porém com a entrada para as funções de similaridade sendo os resultados para o THT estão no Apêndice C. Em esta seção e nas próximas são descritos os resultados para os as funções de similaridade com entrada THT+ NADH.
Os resultados para as entradas THT são descritas sempre nos Apêndices desta Tese.
Som executados com as funções RM S, T CN e T CN_ M ult2 com entrada THT+ NADH.
Sobre os valores de DBI, das Figuras 8.9 e 8.10, a primeira consideração é que para os algoritmos Average, Complete, Edge, Hierarchical e K--means os valores de TCN e TCN_ Mult2 são iguais.
Em relação a essa métrica, cujos menores valores correspondem a melhores resultados, para os algoritmos Average, Complete e Edge a função T CN_ M ult2 apresenta valores melhores que a RM S para as diferentes entradas e os diferentes número de grupos.
Os algoritmos Hierarchical e K--means, apesar de apresentarem valores aproximados, a função T CN_ M ult2 é melhor em maior parte dos diferentes números de grupos e conjuntos de átomos testados.
Os algoritmos Linkage e Som apresentam valores muito próximos, sendo difícil a indicação de qual das funções têm melhores resultados.
Bayesian tem muita variação entre qual das funções apresenta melhores valores de DBI.
Centripetal tem melhores resultados para a função T CN_ M ult2 para as entradas ALL e 46_ RES, e para a função RM S com a entrada 25_ RES.
O contrário ocorre para o algoritmo Centripetal_ Comp, em que T CN_ M ult2 tem melhores resultados para 25_ Res e para as entradas ALL e 46_ Res os melhores valores de DBI variam entre as funções RM S e T CN.
Em relação a métrica pSF, onde maiores valores indicam melhores resultados, os algoritmos Bayesian e Som apresentam valores muito aproximados, dificultando a verificação de qual das funções foi melhor.
Os algoritmos Average, Complete, K--means e Hierarchical obtiveram melhores valores para a função RM S. Edge e Linkage foram os piores algoritmos para todas as funções analisadas, com exceção da execução do Edge, para a função RM S e entrada ALL, que apresenta bons resultados e do Linkage para 5 grupos ou mais, onde ALL gerou melhores resultados para RM S enquanto que 46_ RES e 25_ RES para as funções T CN e/ ou T CN_ M ult2.
Centripetal obteve melhores valores de pSF para as entradas ALL e 46_ RES, intercalando as funções RM S e T CN, enquanto que a entrada 25_ RES somente gerou valores maiores do que zero para a função T CN.
Centripetal_ Comp apesar de gerar valores de pSF bem próximos para os átomos de entrada ALL e 25_ RES, pode- se perceber alguns resultados melhores para T CN e T CN_ M ult2.
É importante ressaltar que os melhores valores de pSF, para a maioria dos algoritmos, aparecem para agrupamentos de até 5 grupos, após, os valores tendem a não variar muito.
Os resultados dos experimentos com as funções RM S X T CN para o THT (Apêndice C) foram muito parecidos com os resultados discutidos para THT+ NADH.
O padrão observado para a métrica DBI se manteve.
Em relação a métrica pSF houveram alguns resultados diferentes, como por exemplo, para THT o algoritmo Complete e Centripetal_ Comp apresentam resultados aproximados como o Som, o Centripetal apresenta resultados ruins como o Edge e Linkage.
Som executados com as funções RM S, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 com entrada THT+ NADH.
Os resultados apresentados nas Figuras 8.13 e 8.14 sobre a métrica DBI mostram que, para os algoritmos Bayesian, Centripetal_ Comp, Linkage e Som os valores são muito aproximados, não sendo possível a identificação das funções com melhores resultados.
Para os algoritmos Complete e Hierarchical a função RM S é sempre melhor, independente dos átomos de entrada e do número de grupos.
O mesmo ocorre para o Average e K--means, entretanto, para esses algoritmos, em alguns casos, a função CORREL_ V 3 se aproxima dos valores de RM S. Centripetal e Edge são algoritmos cujos valores de DBI apesar de parecidos, mostram para a entrada ALL melhores valores para a função CORREL_ V 3.
Os valores de pSF das Figuras 8.15 e 8.16 mostram que, novamente, os algoritmos Bayesian (ALL e 46_ RES), Som e Centripetal_ Comp (ALL) têm valores muito aproximados, sendo possível destacar somente Bayesian--CORREL_ V 3 com melhores valores para o conjunto de átomos de entrada 25_ RES.
Os algoritmos Average, Complete, Hierarchical e K--means têm melhores valores de pSF para a função RM S em todos os casos estudados, com exceção dos resultados para até 3 grupos com a função CORREL_ V 3 que se aproximam de RM S. Os algoritmos Edge e Linkage apresentam os piores resultados entre os algoritmos, com exceção do Edge-RM s, que apresenta bons valores de pSF com entrada ALL e Linkage--CORREL_ V 3 e entrada 25_ RES.
A função CORREV_ V 3 apresenta melhores valores de pSF para os experimentos com Bayesian (25_ RES) e Centripetal_ Comp (46_ RES e 25_ RES).
Os resultados ruins em relação as métricas DBI e pSF para as funções CORREL_ V 1 e CORREL_ V 2 possivelmente foram causados por essas funções permitirem que sejam obtidos valores muito altos de distância entre 2 conformações, o que pode prejudicar a execução dos algoritmos de agrupamento.
Os resultados para a mesma configuração de experimento mas para a entrada com THT estão no Apêndice D. De maneira geral, para as métrica DBI e pSF os resultados obtidos foram muito parecidos entre as entradas THT e THT+ NADH.
Avaliações das Médias de Desvio Padrão (DP) de FEB Dentro de Cada Grupo De acordo com Shão As métricas de avaliação DBI e pSF são imperfeitas e o ideal é utilizar- las em conjunto e realizar uma inspeção visual nos resultados para conclusões sobre os melhores agrupamentos.
Com base nos resultados apresentados, é difícil a indicação de um melhor algoritmo e de uma melhor função pois os melhores valores das métricas variam muito entre as diferentes configurações de experimentos de agrupamento executadas.
Por esses motivos, neste trabalho decidiu- se, além de avaliar os grupos com as métricas clássicas DBI e pSF implementadas em, verificar os resultados dos agrupamentos na aplicação que os mesmos serão utilizados.
O objetivo dos experimentos de agrupamento desde o início é utilizar- los em docagem molecular com o modelo FFR de receptor, de forma a acelerar experimentos desse tipo onde somente parte das conformações de cada grupo serão consideradas.
Para essa verificação dos resultados de agrupamentos na aplicação, num primeiro momento foi necessário a reexecução dos experimentos de docagem molecular (Experimentos Fase 2 -- Seção 3.5.2 do Capítulo 3) pois, para utilização das conformações nos algoritmos de agrupamento foi necessária a sobreposição de todas as estruturas na primeira da DM, modificando as que foram utilizadas na Docagem-Fase 1.
Essa análise foi realizada para os resultados de docagem com os 4 ligantes:
NADH, PIF, TCL e ETH.
A partir destes, foram calculadas as médias de desvios padrão (DP) de FEB de cada agrupamento obtido para as diferentes configurações de experimentos.
Os melhores valores nessa avaliação de média de DP são para as configurações com os menores valores.
Estas menores médias de DP indicam que determinado agrupamento colocou conformações que apresentaram resultados de docagem mais similares em mesmos grupos.
Por exemplo, tem- se experimentos de agrupamento quaisquer com 2 grupos cada, que obtiveram os seguintes valores de DP de FEB:
Experimento 1: Grupo 1 com DP de FEB de 0,5 Kcal/ mol e Grupo 2 com DP de 1,5 kcal/ mol.
A média de DP para esse agrupamento é de 1,0 kcal/ mol;
Experimento 2: Grupo 1 com DP de FEB de 2,5 Kcal/ mol e Grupo 2 com DP de 1,5 kcal/ mol.
A média de DP para esse agrupamento é de 2,0 kcal/ mol;
Em este exemplo simples, o melhor resultado é para o Experimento 1, uma vez que o mesmo apresenta menor variação de FEB entre seus grupos.
Para realizar essa análise foi necessário cruzar as informações sobre FEB de cada conformação para cada ligante com os agrupamentos.
Para isso, foi desenvolvido um pequeno Banco de Dados chamado Docagem_ Agrupamentos que armazena todos os resultados de agrupamentos para as configurações executadas (6 tabelas, uma para cada função utilizada) e os resultados de docagem molecular de cada ligante (4 tabelas, uma para cada ligante).
Assim, o cálculo de média de DP de FEB, utilizando os dados armazenados neste BD, foi aplicado para os 4 ligantes e:
Todas as funções de similaridade variando suas entradas entre THT e THT+ NADH;
Número de grupos variando de 2 a 10;·
para os conjuntos de átomos de entrada 20_ RES, 25_ RES, 46_ RES e ALL;
Para os 7 algoritmos de agrupamento com melhores resultados para as métricas DBI e pSF:
Average, Bayesian, Centripetal_ Comp, Complete, Hierarchical, K--means e Som.
Os resultados dos agrupamentos com os algoritmos Centripetal, Edge e Linkage, especialmente para a métrica pSF, são ruins para quase todas as configurações de experimentos realizados, incluindo aqueles primeiros experimentos com de 10-100 grupos e para de 2-20 grupos.
Em este capítulo são descritos os resultados desta análise aplicado nos resultados de e 8.18) e THT (Figuras 8.19 e 8.20).
Para os demais ligantes, os resultados estão apresentados no Apêndice E, considerando a entrada THT.
T CN, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 (entrada THT+ NADH) para os algoritmos Average, Bayesian e Centripetal_ Comp (ALL, 25_ RES e 46_ RES).
T CN, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 (entrada THT+ NADH) para os algoritmos Complete, Hierarchical, K--means e Som (ALL, 25_ RES e 46_ RES).
RM S, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 (entrada THT) para os algoritmos Average, Bayesian e Centripetal_ Comp.
RM S, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 (entrada THT) para os algoritmos Complete, Hierarchical, K--means e Som.
Os resultados descritos nas Figuras 8.17, 8.18, 8.19 e 8.20 são discutidos considerando todas as funções para as diferentes configurações de experimentos:
Novamente os algoritmos Bayesian e Som apresentam valores aproximados para as diferentes funções.
E isso ocorre para THT e THT+ NADH.
As funções com menores valores de DP variam muito para os diferentes número de grupos, não sendo possível indicar quais tem melhor resultado para esses algoritmos.
O Average-THT+ NADH apresenta menores valores de médias de DP para a função T CN_ M ult2 para os átomos de entrada ALL e 46_ RES, sendo superada em poucos casos por a função RM S. O mesmo algoritmo com a entrada para as funções THT também apresenta melhores valores para T CN_ M ult2 na maioria dos casos, com exceção para ALL e 20_ RES onde CORREL_ V 3 ou RM S algumas vezes são melhores.
Os melhores valores de DP para o algoritmo Centripetal_ Comp-THT+ NADH variam entre as funções CORREL_ V 2 (para as 3 entradas), T CN (ALL) e T CN_ M ult2 (25_ RES e 46_ RES).
Para Centripetal_ Comp-THT, CORREL_ V 2 é melhor para as entradas ALL e 25_ RES enquanto que CORREL_ V 1 tem menores valores de DP de FEB para 46_ RES.
Complete-THT+ NADH apresenta menores valores de DP para a função T CN_ M ult2 (ALL e 25_ RES) enquanto que para a entrada 46_ RES há uma variação entre as funções.
Menos valores de DP para Complete-THT ocorrem na maioria dos casos também para a função T CN_ M ult2;·
O algoritmo Hierarchical apresenta os menores valores de DP de FEB para as funções CORREL_ V 1, T CN_ M ult2 e CORREL_ V 2 para ambas as entradas THT+ NADH e THT.
De maneira geral, o algoritmo K--means apresenta menores valores de DP para a função T CN_ M ult2 para a maioria das configurações de experimento realizadas com ambas entradas THT e THT+ NADH.
Resumindo os resultados discutidos, juntamente com os resultados para os demais ligantes (Apêndice E) pode- se afirmar que a função T CN_ M ult2 apresenta os menores valores de DP de FEB para as diferentes configurações de experimento.
Além disso, também foi verificado que os melhores algoritmos foram Average e K--Means.
Confirmou- se os resultados das métricas DBI e pSF em relação a os algoritmos Bayesian e Som, que não tem seus resultados muito alterados independente da configuração de experimento executada.
As funções relacionadas a correlação (CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3) novamente não se mostram muito promissoras, como já havia sido verificado com as métricas DBI e pSF, com exceção de poucos casos.
Avaliação com o P-MIA O P-MIA é um padrão de múltiplas instâncias autoadaptáveis, um padrão de dados para workflows científicos desenvolvido durante a Tese de Doutorado de Patrícia Hübler.
Esse trabalho foi realizado com o objetivo de contribuir com a redução da quantidade de docagens a serem executadas, via a definição de um padrão capaz de executar a seleção de conformações do receptor de forma dinâmica, onde não exista a necessidade de execuções exaustivas.
Para a utilização do P-MIA, a etapa preliminar consiste na execução do agrupamento, separandose as conformações em diferentes grupos (não importando qual a configuração do experimento de agrupamento).
De essa forma, a cada conformação são relacionadas as seguintes informações:
O grupo ao qual pertence, o lote e o status, que identifica a situação sobre o processamento da conformação, podendo ser Ativo (A), Finalizado (F), Descartado (D) ou Prioridade do grupo alterada (P).
Essa informação de status é fundamental para determinar se uma conformação será considerada para docagem ou não (somente conformações com status (A) são processadas).
O P-MIA também utiliza os valores de quantidade mínima de conformações (QM) a serem processados e o percentual da amostragem (PA) que formam cada lote, definidos por o usuário.
Após a separação das conformações em grupos, o P-MIA subdivide os grupos em lotes.
A quantidade de lotes é definida em tempo de execução baseada nos valores de QM e PA.
Estudos descritos no trabalho de Patrícia mostram que a análise de quantidades menores de dados (lotes) fornece melhores resultados.
Um lote é formado por a quantidade de conformações indicadas por PA.
As conformações de um determinado grupo que não entram num lote formam o chamado lote residual (que pode ser processado ao final da execução com as conformações do lotes ou não).
A seguir, cada grupo é separado em lotes e inicia- se a execução individual de cada conformação num programa de um workflow científico.
Como resultado obtém- se o chamado &quot;Resultado Execução», que neste caso trata- se da FEB.
Essa valor numérico é armazenado de alguma forma (arquivo, tabela num Banco de Dados, etc.) e avaliado com base no intervalo[ M elhor_ valor, P ior_ valor], que corresponde ao melhor e pior valor de FEB, respectivamente.
As conformações cujo &quot;Resultado Execução «se aproximam ou são menores do que o M elhor_ valor são as conformações com maior probabilidade de sucesso.
Para o processamento dos lotes por o workflow, são utilizados os seguintes parâmetros:
Numero, que corresponde a quantidade de conformações já processadas de um lote, total_ resultado, é o somatório dos resultados individuais de um grupo, resultado_ snapshot, é o valor final do processamento de determinada conformação, total_ lote, total de conformações de determinado lote, melhor_ valor e pior_ valor, que correspondem ao melhor e pior valor a ser atingido.
A partir desses parâmetros, o P-MIA calcula uma série de médias, como a média de FEB das conformações já processadas, o ponto médio de FEB do intervalo e a média amostral estimada, que considera as conformações ainda não processadas, utilizando para esse cálculo os valores de desvio padrão de FEB do grupo e do lote para as conformações já processadas.
A fórmula da média amostral estimada está detalhada em e é uma das principais contribuições do modelo P-MIA e corresponde ao valor principal utilizado para indicar se determinado lote será descartado ou continuará sua execução.
Com o auxílio da Patrícia Hübler, que implementou algumas funcionalidades do P-MIA para os testes descritos em, o P-MIA foi aplicado a dois diferentes agrupamentos para a verificação se uma das funções de similaridade propostas nesta Tese apresenta ganho efetivo no processamento das conformações utilizados na docagem, comparando com a função RM S padrão.
Como no trabalho o objetivo era o padrão, para a verificação do mesmo, foram implementados os passos descritos acima com o auxílio de planilhas eletrônicas, sendo boa parte do trabalho feito manualmente.
Por esse motivo, de todos os agrupamentos gerados nas mais diferentes configurações, a análise do P-MIA foi aplicada somente a um destes.
O agrupamento escolhido foi com a função T CN_ M ult2, executada com a entrada THT+ NADH, para o algoritmo K--means, com conjunto de átomos de entrada ALL, com o total de grupos igual a 6.
A mesma configuração de agrupamento um dos ligantes testados em.
Antes da aplicação do P-MIA os dados foram preparados, onde para ambos agrupamentos foram associados os valores de FEB a suas respectivas conformações, dentro de os diferentes lotes e dos diferentes grupos, utilizando para isso o BD Docagem_ Agrupamentos.
Após a separação das conformações em lotes e a associação dos resultados de FEB obtidos, o P-MIA calcula os valores de média e média estimada para a determinação de continuidade ou não do processamento.
A Tabela 8.5 contém o total de conformações que compõem cada um dos grupos, gerados para as configurações de agrupamento K--means-ALL-6_ grupos-THT+ NADH com as funções de similaridade RM S e T CN_ M ult2.
Esses grupos foram então divididos em lotes com QM $= 50 e PA $= 30%.
Para a identificação de qual seria o percentual a ser utilizado para a definição de continuidade ou descarte de um lote, foram analisados valores de 20%, 30%, 50%, 70% e 80%.
As Figuras 8.21, 8.22 mostram exemplos das análises realizadas para 30% onde as colunas referem- se a lote de cada grupo (C_ L); (
2) quantidade total de conformações do lote (Quant);
Média aritmética de FEB das conformações do lote até o momento da análise;
Média estimada de FEB das conformações restantes até o momento da análise;
Quantidade de conformações processadas até o momento (Proc); (
6) quantidade total de conformações processadas (ProcFinal) e quantidade de conformações não processadas (Ganho).
A análise dos resultados de médias aritmética e estimada de FEB se inicia quando 20% das conformações já foram processadas.
Após, é determinado qual é o status de cada lote.
Se, ao analisar os valores de média aritmética e média estimada de determinado lote, ambos os valores forem piores do que o valor médio utilizado como parâmetro, o lote é Descartado (D).
Os lotes com essa característica são sombreados nas Figuras 8.21, 8.22.
São geradas tabelas como as exemplificadas nas Figuras 8.21, 8.22 para a avaliação do ganho com 20%, 30%, 50%, 70% e 80% das conformações processadas utilizando a abordagem do P-MIA para a determinação do status de cada lote a medida que vai avançando o processamento dos mesmos.
Considerando as Figuras 8.21, 8.22 onde as análises começaram a ser feitas quando 30% das conformações de cada lote haviam sido processadas, para a função RM S, 1.446 conformações foram descartadas, ou seja, um ganho de 47%.
A análise com os resultados da função T CN_ M ult2 1.376 conformações foram descartadas, o que corresponde a um ganho de 44%.
O gráfico da Figura 8.23 mostra o ganho obtido a medida que as análises com 20%, 30%, 50%, 70% e 80% foram sendo realizadas.
Em o gráfico da Figura 8.23 pode- se observar que a melhor alternativa é de iniciar a análise o quanto antes, ou seja, com 20% das conformações processadas, onde para a função RM S 1.
648 conformações foram descartadas, o que corresponde a um ganho de 53% e para a função T CN_ M ult2, o estudo do ganho obtido com o uso do P-MIA e das funções de similaridade, foi verificado se as conformações com melhores resultados foram contempladas, ou seja, se foram processados a medida que as análises eram realizadas.&amp;&amp;&amp;
O gráfico da Figura 8.24 apresenta essa análise, onde Melhores 10% referem- se as 310 conformações em que no experimento exaustivo descrito na Seção 3.5.2 do Como pode- se ver na Figura 8.24, somente com 20% das conformações processadas, para função RM S, 239 das 310 conformações foram contempladas (77%) e com a função T CN_ M ult2, 254.
Os resultados do processamento com o P-MIA mostram que este padrão de workflow utilizando em conjunto com os resultados dos experimentos de agrupamento apresenta um ganho muito importante na execução de simulações de docagem molecular com o modelo FFR.
Considerações Finais Este capítulo apresentou todos os experimentos de agrupamento executados com diferentes configurações.
As funções de similaridade desenvolvidas são descritas e seus resultados são comparados com a função original.
A o final do capítulo foi realizada uma análise dos resultados obtidos utilizando para isso o padrão de workflow P-MIA para efetivamente acelerar as simulações de docagem molecular com o receptor flexível.
Os primeiro experimentos executados somente com a função RM S mostraram que mais do que 20 grupos não causavam modificações nas métricas de avaliação dos grupos (DBI e pSF).
Para um estudo mais detalhado, foram executados os experimentos de 2-20 grupos, mas variando de 1 em 1.
Em este estudo, decidiu- se que mais do que 10 grupos não eram necessários e, além disso, verificou- se que os dois conjuntos de átomos testados (ALL e 25_ RES) apresentavam resultados aproximados, não sendo possível indicar qual era o melhor.
Para o desenvolvimento das novas funções de similaridade foram utilizados os resultados do processamento com o programa LigPlot, que analisa os contatos estabelecidos entre determinado complexo receptor-ligante.
As análises com o LigPlot foram feitas com duas entradas diferentes, considerando as conformações do receptor e o substrato THT e o receptor com o THT+ NADH.
A partir destes, foram desenvolvidos 5 funções de similaridade divididas em 2 grupos:
As funções T CN e T CN_ M ult2, que utilizam os valores de totais de contatos entre receptor-ligante e as funções CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 que consideram matrizes de correlação entre as conformações obtidas a partir de como cada conformação estabeleceu seus contatos.
De essa forma, foram executados experimentos com as seguintes configurações:
Funções de similaridade:
RM S, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e· entrada paras as funções de similaridade:
THT e THT+ NADH;
Algoritmos: Average, Bayesian, Centripetal_ Comp, Complete, Edge, Hierarchical, Linkage, K--means e Som;
Número de grupos:
De 2, 3, 4, 5, 6, 7, 8, 9 e 10;·
conjuntos de átomos de entrada:
ALL, 25_ RES, 20_ RES e 46_ RES.
A partir de os resultados obtidos para todos esses experimentos, incluindo a análise de média de DP de FEB dentro de cada grupo, pode- se concluir:
É muito difícil o desenvolvimento de funções que geram bons resultados para todos os testes executados, já que muitas variações foram realizadas, trata- se de muitos e diferentes algoritmos, como diferentes conjuntos de átomos de entrada para as diferentes funções de similaridade.
Não houveram muitas diferenças entre as entradas THT e THT+ NADH.
Acredita- se que isso se deve ao fato de mesmo o LigPlot tendo sido executado com diferentes entradas, o padrão de contatos estabelecidos entre as conformações e THT ou THT+ NADH se manteve o mesmo, principalmente ao comparar- se os valores de totais normalizados.
Considerando a comparação RM S X T CN, de maneira geral, considerando somente DBI, as funções T CN e/ ou T CN_ M ult2 apresentam ou valores aproximados ou melhores valores do que RM S para a maioria dos algoritmos, diferentes números de grupos e diferentes conjuntos de átomos de entrada.
As funções CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 não se mostram muito promissoras nem em relação as métricas nem em relação as médias de DP de FEB dos grupos.
Em relação a os algoritmos de agrupamento:
Segundo Shão As métricas de avaliação DBI e pSF são imperfeitas.
Assim, não é possível, somente com base nos valores destas indicar quais são as configurações de agrupamento mais promissoras.
Por esse motivo foram feitas as análises de médias de DP de FEB dos grupos, que mostraram que as funções desenvolvidas nesta Tese tendem a diminuir os valores de DP, o que indica que houve melhora nos agrupamento em relação a aplicação dos mesmos em docagem molecular.
Em especial para a função de similaridade T CN_ M ult2, que se mostrou entre as funções desenvolvidas ser a mais promissora.
A função RM S aparece em poucos resultados desta análise de média de DP com melhores valores, e isso ocorre para os 4 ligantes estudados;
Em relação a o tempo de processamento utilizando a função RM S e as funções desenvolvidas T CN e T CN_ M ult2, não houve aumento nesse tempo, tendo os algoritmos despendidos o mesmo tempo.
As funções CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 em relação a função RM S tiveram seu tempo de execução aumentado somente no início de cada experimento, para a leitura da matriz CORRELACAO.
A execução do LigPlot para a geração dos dados utilizados nas funções desenvolvidas despendeu em torno de 8 horas para as duas entradas, num computador Core2Duo, 2 GB RAM, mas esse procedimento só é necessário de ser executado uma vez.
A análise com o P-MIA mostrou bons resultados de ganhos para ambas as funções RM S e T CN_ M ult2.
Apesar de a função RM S apresentar em torno de 5% mais ganho do que a T CN_ M ult2, em relação a o número de conformações descartadas a cada porcentagem de análise, a função T CN_ M ult2 contempla maior número de conformações das 10% melhores, e a diferença sobre a função RM S se mantém para todas as análises feitas em torno de 5%.
Ou seja, mesmo que T CN_ M ult2 tenha descartado um número menor de conformações, contemplou conformações comprovadamente mais promissoras.
A análise com o padrão P-MIA mostra um ganho de processamento muito importante, utilizando tanto a função RM S quanto a função T CN_ M ult2, uma vez que com somente 20% das conformações processadas, houve ganhos de aproximadamente 50%, o que possibilitaria a execução dos experimentos de docagem num tempo consideravelmente mais reduzido.
Além de o mais, com os mesmos 20% de processamento, 77% (RM S) e 82% (T CN_ M ult2) das melhores conformações foram consideradas.
Ou seja, com 20% do tempo de um experimento exaustivo, 80% das melhores conformações já foram consideradas.
Isso significa por exemplo que, um experimento que antes precisava em torno de 12 horas num computador QuadCore com 8 GB RAM, utilizando o P-MIA com os agrupamentos gerados ele despende aproximadamente 1/5 desse tempo, 2 horas e 24 minutos.
Ainda são necessárias análises com o P-MIA para outras das configurações de agrupamento executadas, mas para isso há a necessidade da implementação do padrão num workflow cientifico, o que já está sendo realizado por um aluno de Mestrado da PUCRS.
Com os resultados das análises do P-MIA apresentados nesta Tese e em pode- se concluir que sua utilização é muito promissora para a redução do tempo de execução das simulações de docagem molecular com o modelo FFR, mantendo as características dessa flexibilidade.
Este capítulo apresenta os trabalhos relacionas ao desenvolvido nesta Tese, que incluem:
Um trabalho sobre um Banco de Dados para triagem virtual de compostos, que pode ser comparado ao FReDD;
Trabalhos relacionados a execução de docagem molecular com o receptor flexível e seleção de conformações;
E· trabalhos relacionados ao agrupamento de trajetórias de DM.
Banco de Dados para RDD ou Docagem Molecular Em esse trabalho é apresentado um BD desenvolvido utilizando a plataforma de integração Ondex para relacionar dados sobre a descoberta de novos candidatos a fármacos in-silico.
A motivação dos autores em desenvolver essa base de dados é de encontrar exemplos de moléculas que devem ter uma proposta terapêutica adicional as já existentes para determinado alvo.
A plataforma Ondex utilizada para essa integração de dados endereça esse problema representando os muitos tipos de dados com nodos, que contém o que os autores chamam de Concepts e conexões representam o que os autores definem como Relations.
A integração dos dados é então representada como uma rede de nodos (Concepts) interconectados (Relations) na forma de um grafo que pode ser enriquecido semanticamente com metadados, dessa forma, múltiplas fontes de informação são colocadas significativamente juntos num grafo.
Em esse trabalho, foram integrados diversos BD, onde foram considerados como Concepts por exemplo:
O DrugBank (para a obtenção de compostos), UniProt (Receptor), BLAST, OpenBaBel.
Essas bases são então combinadas utilizando o workflow descrito na Figura 9.1 (a).
Essas fontes de dados heterogêneas são então convertidas para o formato Ondex, mapeadas e transformadas por essa plataforma para criar novas relações entre os dados.
Em a Figura 9.1 (b) adaptada do trabalho de, é mostrado um grafo gerado com a plataforma Ondex a partir de os dados integrados, mostrando as conexões entre os Concepts.
Em o trabalho é descrito um estudo de caso para a busca de novos fármacos para uma proteína específica, mostrando a eficiência da base de dados integrada e como esta pode revelar conhecimentos para a descoberta de novos usos terapêuticos de drogas.
Compostos. (a) Workflow de integração de dados desenvolvido em. (
b) Uma parte do grafo gerado por a plataforma Ondex sobre os dados integrados em.
A Execução de Docagem Molecular com o Receptor Flexível e Seleção de Conformações O primeiro trabalho desenvolvido por o LABIO com o objetivo de reduzir o número de conformações e acelerar as simulações de docagem com o modelo FFR está descrito em e consiste num módulo de seleção de conformações que foi inserido no workflow FReDoWS.
O critério de seleção utilizado em baseou- se na idéia de que, se um resultado de docagem com determinada conformação obteve um bom valor de FEB e RMSD, é possível que esta mesma conformação, ao interagir com um ligante parecido com o primeiro, também irá apresentar bons valores de FEB e RMSD.
Para realizar essa seleção de conformações, são executadas as seguintes etapas:
É solicitado ao usuário que informe o total de conformações que ele deseja selecionar, um valor de RMSD máximo e uma tabela de resultados exaustivos para o modelo FFR;
A tabela de resultados informada é ordenada de forma crescente por FEB;
Essa tabela ordenada é separada em duas tabelas de acordo com o valor de RMSD máximo informado;
Se o total de conformações na tabela com valores dentro de o limite de RMSD ultrapassar ou for igual ao total de conformações solicitas por o usuário, a lista de conformações a serem utilizadas na docagem Seletiva está pronta para uso, caso contrário, são adicionadas conformações cujo resultado de docagem excede o valor máximo de RMSD indicado.
Em este trabalho foram realizados estudos de caso para verificar a eficiência do critério de seleção desenvolvido.
Em os resultados apresentados nesse critério se mostrou eficiente principalmente quando realizadas comparações entre os resultados exaustivos e seletivos para um mesmo ligante, a seleção representou adequadamente o modelo FFR.
De essa forma, segundo Machado, para ligantes de uma mesma classe não é necessária a utilização de todas as conformações do FFR, sendo o conjunto selecionado já eficiente na representação do todo.
Esse trabalho foi o passo inicial para todo o trabalho apresentado nesta Tese.
O trabalho apresentado em Lin É uma descrição detalhada do trabalho resumido em.
Em esses trabalhos foi proposta uma abordagem computacional para o tratamento da flexibilidade de receptores:
O RCS -- relaxed-complex scheme.
Esse método reconhece que ligantes devem se ligar a conformações do receptor que ocorrem raramente numa dinâmica.
Em o RCS, num primeiro estágio é executada uma simulação por DM do receptor sem estar com nenhum ligante, o que gera uma amostra de conformações do mesmo.
Em o segundo estágio do RCS, é executada uma docagem rápida de mini bibliotecas de candidatos a inibidores a um grande conjunto de conformações do receptor geradas no estágio 1 do processo.
Para executar a DM foi utilizado o programa AMBER e a duração da simulação foi de 2 ns.
A docagem foi executada utilizando o programa AutoDock3.
0.5 com o algoritmo LGA.
Um procedimento automático foi desenvolvido tanto para preparar os arquivos para o AutoDock quanto para executar a docagem molecular.
E quando esse procedimento é utilizado em conjunto com as simulações por a DM permite a acomodação direta da flexibilidade do receptor.
Em o estudo de caso apresentado foram utilizados as conformações de 10 em 10 ps da DM de 2 ns.
A distribuição da FEB variou em torno de 3 kcal/ mol, indicando a sensibilidade dos resultados de docagem para as diferentes conformações da DM.
Segundo Lin existem inúmeros métodos que podem ser aplicados para o cálculo da energia livre de ligação (FEB) do complexo, mas a maiorias destes necessitam de um grande esforço computacional.
O esquema aplicado em RCS, o Mm/ PBSA é um método de pós-processamento para avaliar as energias livres de moléculas ou de complexos com um custo computacional mais baixo que outros métodos.
Esse esquema combina energias mecânicas moleculares (Mm), energia de solvatação e entropia estimada do complexo e estão detalhadas em.
Utilizando o esquema Mm/ PBSA (molecular-Mechanics Poisson--Botzmann Surface Area) e ranqueando por FEB os diferentes modos de ligação receptor-ligante dos complexos minimizados, é demonstrado por que os melhores resultados estão de acordo com a estrutura cristalográfica, o que mostra que com esse esquema Mm/ PBSA para determinação da FEB foram obtidos complexos mais estáveis.
A primeira melhora incluída no RCS por Amaro Consiste no uso do AutoDock versão 4.0.
Em esta versão do programa AutoDock foram incluídas melhorias em relação a como o ligante é considerado, onde por exemplo, um maior número de tipos de átomos podem estar presentes nos ligantes, é possível adicionar as cargas aos ligantes durante seu preparo para a execução da docagem molecular, entre outras.
O segundo conjunto de melhorias propostas por consiste na consideração tanto de efeitos locais, quanto de efeitos globais na determinação da FEB de determinado complexo receptorligante.
Essa melhoria esta relacionada com a inclusão de novas funções para a determinação da FEB final, num estágio de pós-processamento da docagem.
O terceiro conjunto de melhorias define dois algoritmos para a redução do número de conformações da trajetória da DM:
A utilização do método Fatoração QR e de um algoritmo de agrupamento de conformações disponíveis no GROMACS, que se utiliza dos valores de RMSD entre as estruturas.
Essa terceira melhoria proposta por ao método RCS é a que mais interessa nesta revisão pois, assim como o trabalho descrito nesta Tese, foi apresentada uma metodologia para a redução do número de estruturas da DM.
Em o RCS original as simulações de docagem molecular foram executadas utilizando conformações extraídas da DM em intervalos de tempo iguais, de 10 em 10 ps..
O problema é que atualmente as DM são executadas para intervalos de tempo maiores, e por esse motivo, o número de conformações a serem utilizadas aumentou muito.
A primeira técnica, chamada de Fatoração QR, foi desenvolvida inicialmente para outros propósitos, e neste trabalho de Amaro Foi utilizada para a redução do conjunto de conformações da DM a serem utilizadas na docagem (Figura 9.3.
Para isto, essa técnica segue os seguintes passos:
Executa um alinhamento estrutural múltiplo utilizando as estruturas da DM de 50 em 50 ps..
Para a determinação desse alinhamento múltiplo, todos os possíveis alinhamentos par-a-par são determinados;
É realizada um análise a partir de um agrupamento hierárquico, que é computado baseado numa medida de similaridade estrutural utilizada para o alinhamento múltiplo.
A medida de similaridade aplicada mede a distância entre todos os pares de Carbonos ao longo de as estruturas alinhadas;
O alinhamento estrutural é então armazenado numa matriz multidimensional, onde cada conformação é uma coluna e cada linha corresponde a um alinhamento;
É aplicado à matriz gerada no passo anterior o método Fatoração QR, gerando como resultado uma lista reordenada das conformações baseada na similaridade entre as mesmas.
Esse agrupamento hierárquico reordenado é mostrado à esquerda na Figura 9.3.
Ele permite então a definição de um conjunto de conformações não-redundantes de acordo com algum limiar estabelecido por o usuário.
A direita na Figura 9.3, um exemplo da aplicação do método de Fatoração QR, de um conjunto inicial de 400 estruturas foi obtido um conjunto de 33 estruturas não redundantes.
Detalhes em.
Também no trabalho de é apresentado um método alternativo para o agrupamento de estruturas baseado numa matriz de RMSD par-a-par das estruturas.
Essa matriz de valores de RMSD é dividida em lotes de estruturas mais similares utilizando um algoritmo de agrupamento contido no programa GROMACS.
Esse agrupamento permitiu que a docagem fosse executada num reduzido número de estruturas consideradas mais significativas.
Agrupamento de trajetórias de DM Em o trabalho de Torda 2 algoritmos de agrupamento tradicionais foram aplicados a uma trajetória por a DM e os mesmos foram comparados com 2 conjuntos de dados de teste.
Em, primeiro os autores escolheram um subcojunto de todos os átomos pesados da cadeia principal de 12 resíduos de regiões bem conhecidas do receptor para representar as conformações.
Após, foi selecionado todos os átomos pesados da cadeia principal dos 64 resíduos do receptor de estudo.
Foram aplicados 2 algoritmos de agrupamento:
Um hierárquico aglomerativo, o single linkage, e um hierárquico divisivo.
Independente do algoritmo de agrupamento, o mesmo precisa de uma medida de similaridade para diferenciar os pontos uns dos outros.
Em os algoritmos utilizados na medida considerada foi a de RMSD das coordenadas cartesianas dos átomos considerados, sendo definida por Dab e descrita na Equação 8.1 do Capítulo 8.
Em esse trabalho foram considerados 64 resíduos de cada uma das 2.000 conformações.
Com base nos resultados obtidos, os autores concluem que o algoritmo hierárquico divisivo parece produzir resultados mais significantes do que o algoritmo single linkage.
Segundo os autores, o single linkage falha porque é baseado na distância mínima entre os pontos.
Em relação a os resultados com os 2 conjuntos de entradas os autores optam por o conjunto menor, de 12 resíduos.
Essa escolha foi feita pois ao considerar todos os resíduos a diferença entre qualquer par de estruturas raramente refletia propriedades conformacionais individuais das mesmas.
Em vez de isso, esse conjunto refletia mudanças em muitas regiões simultâneas e parcialmente independentes.
É importante salientar que neste trabalho não havia o objetivo de utilizar as estruturas agrupadas em simulações de docagem molecular.
O artigo de Shão É um dos trabalhos mais relacionados com o trabalho apresentado nesta Tese, principalmente em relação a o Capítulo 8.
Em esse artigo os autores apresentam um conjunto de 11 algoritmos de agrupamento de diferentes tipos:
Average--Linkage, Single--Linkage, Complete--Linkage, Linkage, Centripetal, Centripetal-Complete, Hierarchical, K--Means, Bayesian, Som, COBWEB implementados e comparados com dados de diferentes simulações por DM.
Esses algoritmos foram implementados na linguagem C e foram incorporados ao módulo Ptraj do AMBER9.
Todos os 11 algoritmos foram executados com a mesma função de similaridade, a RMSD descrita em e reproduzida por a equação Os testes iniciais apresentados em foram realizados no plano 2D com dados aleatórios, somente para análise dos algoritmos.
A seguir os algoritmos foram utilizados considerando como entrada duas trajetórias de 500 ps, em que de cada uma foram extraídos conformações de 5 em 5 ps, totalizando 100 conformações a serem agrupadas.
De esses resultados os autores concluíram que o número ideal de grupos é 5.
Os autores também realizam experimentos com uma trajetória do mesmo receptor porém agora de 36 ns, em que as conformações foram consideradas de 10 em 10 ps, totalizando 3.644 estruturas a serem agrupadas.
Para essa trajetória os testes para definir o número de grupos foi de 2 a 20 grupos onde foram considerados somente alguns resíduos específicos do receptor.
De os resultados obtidos com esse último e mais completo experimento os autores concluem:
A performance dos algoritmos é altamente dependente da escolha do número de grupos e dos átomos utilizados na entrada;
O algoritmo single-- linkage é o mais frágil a presença de outliers.
Embora esse algoritmo consiga lidar com grupos de diferentes tamanhos, geralmente gera resultados ruins quando os pontos são muito próximos;
Os algoritmos complete-- linkage e centripetal complete são algoritmos hierárquicos aglomerativos que não apresentam grupos com somente um objeto;
O centripetal apresenta resultados similares ao linkage, onde apesar de produzirem ótimos valores de DBI, tem muitos grupos com somente um ponto;
O linkage e average-- linkage apresentam bons resultados para as métricas DBI e pSF.
Eles produzem grupos com tamanhos variados;
O algoritmo K--means tende a produzir grupos de tamanhos similares;
O Bayesian produz bons resultados, mas que começam a piorar a medida que o número de grupos aumenta.
Para produzir bons resultados, ele deve ser executado muitas vezes, o que gera um alto custo computacional;
O Som produz também bons resultados porém apresenta dificuldade em produzir grupos de diferentes tamanhos;
COBWEB apesar de um algoritmo promissor também necessita de múltiplas execuções para a obtenção de bons resultados;
O algoritmo Hierarchical foi o mais rápido, sendo muito sensível a outliers;
Resumindo, de maneira geral, os autores apontam os algoritmos K--means, average-- linkage e Som com os de melhor performance durante os experimentos.
Considerações Finais A base de dados apresentada na primeira seção foi o único trabalho relacionado ao FReDD que foi encontrado até o momento.
Apesar de o trabalho de consistir numa plataforma de integração de bases de dados públicas, o seu propósito, é o mesmo do FReDD, de auxiliar na descoberta de novos fármacos.
A principal diferença é que o FReDD tem um modelo e dados próprios armazenados, enquanto que no trabalho de todos os dados provém de bases de dados de acesso público.
Além de o mais, o auxilia o RDD de forma mais direta que o FReDD pois ele já indica possíveis candidatos à fármacos, enquanto que o FReDD será utilizado no futuro com esse propósito.
O módulo de seleção de conformações do workflow FReDoWS se mostrou eficiente para a utilização com conjuntos de ligantes de mesma classe.
Porém, um dos objetivos dos trabalhos desenvolvidos no Laboratório de Bioinformática, Modelagem e Simulação de Biossistemas (LABIO) é no futuro realizar Triagem Virtual (do inglês, Virtual Screening -- Vs) com o receptor flexível.
De essa forma, a proposta de seleção de conformações apresentadas em não serviria para esse propósito.
Além de o mais, neste trabalho não foi realizado nenhuma investigação para o entendimento da interação receptor-ligante, o que está incluído no trabalho descrito nesta Tese.
Outro diferencial é que nesta Tese a seleção de conformações foi realizada aplicando- se diferentes técnicas de mineração de dados, incluindo informações do contexto.
Esse tipo de análise não está no trabalho de.
Em os trabalhos descritos por é proposta uma abordagem computacional para o tratamento da flexibilidade de receptores:
O RCS.
Em o RCS, é executada uma simulação por DM do receptor e a seguir é executada uma docagem molecular de mini bibliotecas de candidatos a inibidores a um grande conjunto de conformações do receptor geradas na DM.
A diferença desta metodologia de execução de docagem com receptor flexível para a empregada no nosso trabalho está no pós-processamento executado no RCS, que utilizando o esquema Mm/ PBSA, os diferentes modos de ligação receptor-ligante obtidos com a docagem são novamente ranqueados.
Outra diferença significativa do trabalho de para o descrito nesta Tese é que, pelo menos uma vez, executamos as simulações de docagem molecular utilizando todas as conformações da DM.
Isto permitiu um mapeamento detalhado da interação receptor-ligante com o receptor flexível.
Em o trabalho de, as estruturas são utilizadas de 10 em 10 ps, não havendo nenhum tipo de análise nas estruturas entre esses intervalos.
O mesmo ocorre no trabalho de.
A redução do número de conformações a serem utilizadas é feito com base em estruturas obtidas de 50 em 50 ps da DM.
As estruturas entre esses intervalos são ignoradas e não foram analisadas no trabalho.
Nós acreditamos que, não realizar nenhum tipo de análise pelo menos uma vez de todas as estruturas pode ocasionar na perda de informações importantes, e principalmente, podem não ser analisadas estruturas que poderiam ter uma melhor afinidade com determinado ligante.
Também no trabalho de, apesar de os métodos de agrupamento de conformações serem interessantes, não é demonstrado no trabalho detalhes sobre os mesmos.
Além disso, não é apresentada na conclusão do trabalho, qual das duas técnicas de seleção de conformações se mostrou mais efetiva e causou a menor perda de informações.
Tanto para o trabalho de, quanto para o trabalho de, a etapa de pósprocessamento aplicada aos resultados de docagem não está disponível para utilização.
Sendo assim, não foi possível uma comparação dos resultados desta Tese com os resultados com o método RCS.
Em relação a esses trabalhos de, a nossa grande diferença está no estudo detalhado das interações receptor-ligante.
Também apresentamos a utilização de um conjunto de 10 algoritmos de agrupamento e não somente um, conforme descrito em.
Além disso, em é sempre utilizado como parâmetro para agrupamento de estruturas o valor do RMSD.
Em o nosso trabalho, modificamos a função de similaridade para também incluir informações do contexto.
O trabalho de Torda É um dos primeiros trabalhos relacionados ao agrupamento de conformações de trajetórias de DM.
A principal diferença deste trabalho para o trabalho descrito nesta Tese é que neste somente foram estudados 2 algoritmos de agrupamento, ambos com a função de similaridade RMSD.
Além de o mais, neste trabalho os autores não tinham o objetivo de utilizar as estruturas agrupadas para simulações de docagem molecular.
Uma característica importante do trabalho de que foi utilizado nesta Tese consiste na análise dos agrupamentos utilizando como entrada diferentes conjuntos de resíduos do receptor.
A diferença é que para nossos resultados não foram encontradas diferenças significativas para as diferentes entradas analisadas, enquanto que nos autores indicam ser melhor utilizar conjuntos menores de resíduos a todos os resíduos do receptor de estudo.
O trabalho de Shão, apesar de ter sido utilizado como base no desenvolvimento dos experimentos de agrupamento desta Tese, há diferenças significativas entre os trabalhos.
A principal é que nesta Tese foram analisadas diferentes funções de similaridade para calcular a distância entre as conformações que estão sendo agrupadas.
Essas diferentes funções consideram informações do contexto para melhorar os agrupamentos.
Além de o mais, no presente trabalho são comparados os resultados ao se considerar diferentes átomos na entrada dos algoritmos, enquanto que em para os experimentos mais importantes foram todos realizados considerando somente alguns resíduos definidos por os autores como mais importantes.
Além de o mais, em não há a intenção de utilizar os agrupamentos para a docagem molecular.
Este documento apresentou todas as etapas do trabalho desenvolvido com os objetivos de melhorar o entendimento sobre a importância da flexibilidade de receptores em docagem molecular e de selecionar conformações do receptor de forma a acelerar esse processo.
Como método para alcançar esses objetivos aplicou- se um processo de KDD, em que diferentes técnicas de mineração de dados foram utilizadas.
A maioria dos resultados obtidos nesta Tese já está publicada em artigos, resumos, capítulo de livro ou estão em artigos sob revisão:
COH10, COH11, MAC11a, MAC10c, MAC10b, MAC10d, WIN10c, MAC11b, MAC10a, WIN11].
Os Capítulos 3 e 4 apresentam os materiais e métodos utilizados para o desenvolvimento deste trabalho.
A o final deste capítulo é descrito o primeiro trabalho que originou todos os resultados posteriores.
Em este foi desenvolvido um BD inicial para armazenamento dos resultados de docagem e das conformações da DM, e a partir desses dados, foram executados os primeiros experimentos de mineração de dados com a técnica de Associação.
Como esse modelo de BD não suportava diferentes simulações de docagem, este foi evoluído para o modelo descrito no capítulo seguinte.
Assim, no Capítulo 5, é descrito o primeiro resultado desta Tese, o BD FReDD[ WIN09, WIN10a, WIN10b], que armazena os resultados de conformações do receptor e do ligante e de docagem molecular.
A partir de os dados armazenados no FReDD, uma etapa de preparação para a mineração foi realizada, onde foi utilizado principalmente as distâncias entre os resíduos do receptor e os 4 ligantes estudados.
A o final deste capítulo é realizada uma análise preliminar nos resultados armazenados no FReDD que selecionam um conjunto de 25 resíduos do receptor que mais interagem com os 4 ligantes.
Esses resíduos, chamados de Top 25, são utilizados no Capítulo 8 para as análises com a técnica de agrupamento.
MAC10b, MAC11b, WIN10b].
Uma das principais contribuições dessa capítulo é a metodologia proposta de discretização do atributo-alvo dos arquivos de entrada utilizados.
Essa metodologia proposta é comparada com 2 métodos de discretização clássicos com base no impacto dos mesmos no resultado das árvores de decisão obtidas.
Os resultados com a Classificação apesar de gerar modelos interessantes e permitir que fossem extraídos conhecimentos sobre a interação receptor-ligante, a utilização para a seleção de conformações do receptor em docagem com ligantes diferentes não é possível de ser feita diretamente pois as conformações do receptor com melhor FEB são diferentes para os 4 ligantes, não sendo possível selecionar um conjunto único de conformações mais promissoras.
Além de o mais, a discretização não é precisa uma vez que a variação dos valores de FEB entre as instâncias de entrada é muito sutil, prejudicando a determinação de que uma instância pertencia a uma classe ou a outra.
Assim, optou- se por o uso de um algoritmo onde não fosse necessária a discretização do atributo-classe FEB:
O algoritmo escolhido foi o de regressão M5P.
Os resultados com a aplicação da técnica de mineração de Regressão com o algoritmo de árvores modelo M5P são resumidos no Capítulo 7.
As principais contribuições deste capítulo estão relacionadas ao pré-processamento dos dados baseado no contexto e a metodologia de pós processamento dos resultados das árvores modelo que permitiu a indicação das conformações mais promissoras nesses experimentos.
Apesar de os resultados com o M5P serem interessantes, assim como para Classificação, a utilização dos mesmos, diretamente para seleção de conformações em futuras simulações de docagem molecular não é promissora.
O principal problema encontrado é que as melhores conformações são diferentes para cada ligante.
Ou seja, não é possível, a partir desses resultados, estabelecer um conjunto único de conformações mais relevantes.
Outro problema encontrado é que, para se utilizar os modelos induzidos para predizer o valor de FEB de novos ligantes é necessário saber as distâncias mínimas dos mesmos para os resíduos do receptor, informação que somente é obtida após a execução da docagem molecular, o que também dificulta a utilização dos modelos com o M5P para efetivamente selecionar conformações do receptor para compostos ainda não testados.
Por esses motivos, optou- se por não mais se utilizar como entrada nos experimentos de mineração os resultados de docagem molecular e sim, diretamente as conformações do receptor.
E, como não será mais utilizado os resultados de docagem, não tem- se mais um atributo-classe FEB.
A técnica de aprendizado não-supervisionado escolhida foi a de Agrupamento.
Em este estudo de caso, com somente 20% das conformações processadas, houve ganhos de aproximadamente 50% o que possibilita a execução dos experimentos de docagem num tempo consideravelmente mais reduzido.
Além de o mais, com os mesmos 20% de processamento, 77% (RM S) e 82% (T CN_ M ult2) das melhores conformações foram consideradas.
Ou seja, com 20% do tempo de um experimento exaustivo, 80% das melhores conformações já foram consideradas.
KDD nesse tipo de resultado de docagem molecular com o FFR são os diferenciais deste trabalho.
Com base em todos os resultados apresentados, desde o BD FReDD até os experimentos com Classificação, Regressão e Agrupamento, este trabalho contribuiu para melhorar a eficiência da seleção de conformações do receptor utilizando um processo completo de KDD, uma vez que os dados foram preparados, a mineração de dados foi aplicada e os resultados foram pós-processados.
Com a implementação do P-MIA e com as novas DM que estão sendo executadas no LABIO, possivelmente todas as contribuições deste trabalho serão efetivamente utilizadas para a busca de novos compostos para a InhA e para outros receptores que venham a ser alvo de estudo no laboratório.
Principais Contribuições As principais contribuições obtidas com o desenvolvimento desta Tese atendem ao principal objetivo da mesma:
Contribuir para o entendimento da importância da flexibilidade do receptor em simulações de docagem molecular e para a redução do tempo necessário para a execução desse tipo de experimento a partir de a aplicação de um processo de descoberta de conhecimento em Banco de Dados:
O modelo do BD FReDD desenvolvido para armazenamento de resultados de docagem com o FFR e de conformações resultantes de DM.
Não foi encontrado outro BD que apresentasse um modelo para este mesmo tipo de dado biológico.
O algoritmo desenvolvido para a preparação dos dados de docagem com o FFR para utilização nas técnicas de mineração de dados.
A utilização de distâncias mínimas entre os resíduos do receptor e os ligantes como atributos preditivos, obtidas a partir de os dados armazenados no FReDD, é uma contribuição interessante desta Tese pois pode ser aplicada como uma nova forma de análise dos resultados de interação receptor-ligante (como a análise apresentada ao final do Capítulo 5).
A aplicação de técnicas de mineração de dados em resultados de docagem molecular com o receptor flexível para a extração de conhecimento sobre a interação do complexo receptorligante.
O método de discretização proposto, que utiliza os valores de Média e Desvio Padrão do atributo-alvo para a determinação das classes.
Para o atributo-alvo FEB, esse método de discretização foi o mais promissor a gerou as melhores árvores de decisão.
A seleção de atributos baseada no contexto, que nos experimentos com o algoritmo M5P de Regressão melhoraram os modelos gerados.
O método de pós-processamento das árvores modelo geradas com o M5P que permitiram que fossem selecionados conjuntos de conformações mais promissoras para cada ligante.
As novas funções de similaridade para os algoritmos de Agrupamento:
T CN, T CN_ M ult2, CORREL_ V 1, CORREL_ V 2 e CORREL_ V 3 e a comparação das mesmas em relação a os resultados obtidos para métricas clássicas como DBI e pSF e em relação a aplicação, considerando as médias de DP de FEB dos agrupamentos para diferentes configurações de experimentos.
Os agrupamentos gerados (independente da função) que, utilizados em conjunto com o PMIA, permitem um ganho no processamento de experimentos de docagem com o modelo FFR do receptor.
Trabalhos Futuros Como sugestões para trabalhos futuros:
Executar os experimentos de mineração de dados a partir de os resultados de DM mais longas.
Atualmente, estão sendo produzidas no LABIO DM de 10 até 100 ns, o que está gerando um número muito maior de conformações a serem consideradas em docagem molecular com o modelo FFR.
Expandir o BD FReDD, disponibilizando acesso ao mesmo por a Web, para que outros grupos de pesquisa tenham acesso aos dados armazenados.
Com a implementação do P-MIA, que está em fase de execução, analisar outras configurações de agrupamentos e seu impactos no ganho de processamento, o que também permitirá que novas funções de similaridade sejam desenvolvidas e testadas.
Paralelizar a execução dos experimentos de agrupamento, pois para a DM analisada, de 3,1 ns, por exemplo o algoritmo K--means despende em torno de 2 horas para executar, considerando somente uma determinada configuração de experimento.
Para a utilização de Agrupamento nas DM que estão sendo executadas no LABIO, será necessária essa paralelização dos algoritmos.
Executar outras técnicas de mineração de dados diretamente com as coordenadas cartesianas das conformações, como o trabalho que está sendo desenvolvido por a doutoranda Ana Winck.
