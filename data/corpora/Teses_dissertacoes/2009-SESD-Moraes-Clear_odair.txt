O aumento do número de transistores num único chip trouxe novos desafios de projeto, entre eles como aproveitar efetivamente este elevado número de componentes.
Uma das formas encontradas é através do reuso de módulos de hardware.
Quando estes módulos de hardware são processadores, temos multiprocessamento em chip, resultando nos MPSoCs (Multiprocessor System on Chip).
Os MPSoCs estão se tornando elementos comuns em sistemas embarcados, devido a o alto desempenho e à flexibilidade que eles trazem ao projeto.
Em os últimos anos universidades e empresas vem desenvolvendo grandes projetos em multiprocessamento.
O presente trabalho tem por objetivo desenvolver um MPSoC homogêneo, com interconexão por NoC.
A motivação para a adoção de NoCs reside no maior paralelismo e escalabilidade desta infraestrutura de comunicação, quando comparado a barramentos.
O desempenho do MPSoC desenvolvido é avaliado, demonstrando- se os benefícios em se utilizar este tipo de arquitetura no projeto de sistemas embarcados.
Palavras Chave: MPSoCs, multiprocessamento, NoC, multicore.
Design And Performance EVALUATION Of A NoC-BASED HOMOGENEOUS MPSoC A indústria de Circuitos Integrados (Ci) vem crescendo consideravelmente em função de o aumento da demanda do mercado de consumo e, com isso, a evolução dos processadores também vem se acentuando.
Quando falamos em demanda de consumo podemos dar inúmeros exemplos de produtos, tanto para consumidores diretos ou indiretos.
Um dos mercados de maior crescimento é o de telecomunicações, que também é bastante amplo, envolvendo desde telefonia móvel, internet, HDTV até GPS.
Outra área em expansão é a de segurança, não só residencial, mas também industrial, e temos produtos como rastreamento via satélite, aviões, automóveis, etc..
Todos estes exemplos têm pelo menos um processador internamente.
Quando são agregados mais características a este produto, a possibilidade de maior consumo, maior processamento e miniaturização são inerentes a sua característica.
Por isso, tornar os CIs cada vez menores é um dos grandes desafios das indústrias de circuitos integrados.
Em 1965 o então Presidente da Intel predizia que o poder de processamento aumentaria 100% a cada 18 meses, recebendo o nome de &quot;Lei de Moore».
Já em 1975 ele revisou sua predição e aumentou para 2 anos.
A tecnologia que nos permite criar processadores cada vez mais rápidos, conforme predito na Lei de Moore necessita de novos métodos de concepção e projeto para resolver importantes desafios, como integração de bilhões de transistores num único circuito integrado e dissipação de potência.
Em 2005 a International Technology Roadmap for Semiconductors já citava a existência de circuitos integrados com mais de 100 milhões de transistores, e que a previsão era que na próxima década circuitos integrados seriam fabricados com bilhões de transistores.
Hoje já podemos constatar que em algumas indústrias isso já é possível.
Temos como exemplo, o processador Itanium da Intel com mais de 2 bilhões de transistores.
Atualmente a principal barreira que os projetistas estão encontrando para desenvolver CIs com bilhões de transistores é a questão das propriedades físicas, ou seja, a miniaturização de um transistor está chegando ao seu limite.
Os projetos de circuitos eletrônicos sofreram avanços, e se direcionam para uma tecnologia que possibilita podermos incluir num mesmo circuito integrado, módulos analógicos e digitais.
É comum encontrarmos ao menos um microprocessador e uma memória nos CIs atuais, além de processadores de sinais, filtros, amplificadores, etc..
Esta quantidade de componentes no interior de um chip denomina- se SoC (System-on-chip).
SoC é portanto, um Ci complexo que integra os principais elementos funcionais de um produto completo.
A integração de vários componentes em SoCs traz avanços comparado com a integração em PCBs (Printed Circuit Board), que utiliza diversos circuitos integrados para executarem várias funções.
Entre estes avanços podemos destacar:
Maior velocidade de operação do sistema, uma vez que na integração intra-chip o fluxo de dados entre o processador e outros componentes também intra-chip podem ser maximizados;
Potência consumida reduzida, devido a as baixas tensões requeridas devido a o alto grau de miniaturização;
Tamanho e complexidade reduzidas dos produtos para os usuários finais, uma vez que o número de componentes adicionais cai drasticamente;
Aumento da confiabilidade do sistema.
Enquanto um único processador pode ser suficiente para aplicações de baixo desempenho, um crescente número de aplicações necessita de mais processadores para atingir seus requisitos de desempenho.
Por esta razão, é crescente o uso de MPSoCs para a criação de sistemas integrados complexos.
Um MPSoC é mais do que um cluster dentro de um chip, pois requisitos de aplicação e restrições de implementação fazem com que os projetistas criem arquiteturas especializadas e heterogêneas.
O desenvolvimento de Sistemas Multiprocessados em Chip, ou MPSoCs (Multiprocessor System-on-chip), é uma tendência no projeto de sistemas embarcados, pois estes sistemas permitem uma melhor utilização dos transistores disponíveis.
Em 2005 estima- se que 75% dos projetos dos sistemas embarcados utilizavam pelo menos três processadores.
MPSoCs são arquiteturas que buscam um compromisso entre restrições da tecnologia VLSI e as necessidades da aplicação.
MPSoCs representam uma evolução do conceito de SoC, e com isso se beneficiam das mesmas vantagens adquiridas no projeto de SoCs.
Geralmente esse projeto é baseado no reuso de núcleos de hardware (ou simplesmente IPs), ou seja, módulos préprojetados e pré-validados.
A maioria das indústrias que utilizam sistemas on- chip hoje em dia, precisam se preocupar com o desempenho e o consumo de energia.
Preocupados com isso essas indústrias visam não só a miniaturização de seus equipamentos, mas também como reduzir o esforço de projeto e consequentemente o time- to-- market.
Adicionalmente, MPSoCs são tendência porque amenizam a crise de projeto VLSI, através da redução do espaço entre a disponibilidade de transistores por circuito integrado nas tecnologias atuais e a capacidade de projeto de SoCs.
Por essas características já citadas, os sistemas multiprocessados num único chip se tornam num vasto campo para pesquisas.
A maioria dos sistemas embarcados (e.
g Celulares, PDAs, HDTVs, set-top box) hoje em dia já utilizam um sistema em único chip e todos possuem diversos tipos de chips, cada um com suas características específicas.
Podemos dizer então que a maioria dos projetos de MPSoCs tendem a ser heterogêneos, podendo ter vários PEs (Processing Elements), memórias, etc..
Todos esses elementos terão suas próprias características de frequência, potência e voltagem diferentes entre eles.
A forma mais utilizada para interligar os núcleos de um hardware é através de arquiteturas por barramento.
Podemos citar o CoreConect da IBM e o WISHBONE da Silicore.
Este último foi adotado por a organização OpenCores como padrão de conectividade em seus núcleos de hardware.
Implementações por barramento podem ser &quot;simples», quando se trata de implementação, porém apresentam algumas desvantagens:
Apenas uma troca de dados pode ser realizada por vez, pois o meio físico é compartilhado por todos os núcleos de hardware, reduzindo o desempenho global do sistema;
Necessidade de mecanismos inteligentes de arbitragem do meio físico para evitar desperdício de largura de banda;
A escalabilidade é limitada, ou seja, o número de núcleos de hardware que podem ser conectados ao barramento é muito baixo, tipicamente na ordem da dezena;
O uso de linhas globais num circuito integrado com tecnologia submicrônica impõe sérias restrições ao desempenho do sistema devido a as altas capacitâncias e resistências parasitas inerentes aos longos fios.
Uma alternativa ao uso dos barramentos são as redes intra-chip, ou NoCs (Network onChip).
Uma rede intra-chip possui as mesmas características de uma rede de computadores interconectados paralelamente e pode ser definida como um conjunto de roteadores e canais ponto-a-ponto que interconectam os núcleos de um sistema integrado de modo a suportar a comunicação entre esses núcleos.
As principais características das NoCs que motivam o seu estudo são:
Confiabilidade; Eficiência no gerenciamento de energia;
Escalabilidade da largura de banda em relação a arquiteturas de barramento reusabilidade;
Decisões de roteamento distribuídas.
O modelo de comunicação utilizado é o da troca de mensagens, sendo que a comunicação entre núcleos é feita através do envio e recebimento de mensagens de requisição e de resposta.
Cada pacote é constituído geralmente por:
Cabeçalho (header), o qual sinaliza o início do pacote e inclui informações necessárias à sua transferência por a rede;
Carga útil (payload), a qual inclui o conteúdo do pacote;
E (iii) terminador (trailer), o qual sinaliza o final do pacote e pode ser até a última palavra da carga útil, desde que haja um bit especial ativado apenas no final do pacote.
Redes intra-chip podem ser caracterizadas como segue:
Controle de fluxo:
Define a alocação de buffers e canais necessários para um pacote avançar por a rede, realizando a regulação de tráfego nos canais.
Topologia: É o arranjo dos nodos e canais sob a forma de um grafo.
Não está diretamente associada aos roteadores, mas sim como eles estão interligados.
De acordo com estão divididos em redes diretas e indiretas.
A Figura 1b ilustra uma topologia direta, do tipo malha.
Roteamento: Define o caminho a ser realizado por um pacote para atingir o seu destino.
Determina como um pacote escolhe um caminho deste grafo.
Arbitragem: Resolve conflitos internos na rede, quando duas ou mais mensagens competem por um mesmo recurso (buffer ou canal de saída).
Determina qual canal de entrada pode utilizar um determinado canal de saída.
Chaveamento: Define como um pacote é transferido da entrada de um roteador para um de seus canais de saída.
Os chaveamentos mais utilizados são por circuito e por pacotes Tratamento de contenção:
Determina o esquema de filas utilizado para armazenar um pacote bloqueado na rede quando um canal de saída por ela requisitado já está alocado para outro pacote.
Motivações do Trabalho Tendo presente o aumento da demanda de processamento e da utilização de vários processadores para execução de várias tarefas, o uso de sistemas multiprocessados num único chip está se tornando cada vez mais relevante.
A implementação de projetos MPSoC é também uma importante linha de pesquisa do Grupo de Apoio ao Projeto de Hardware (GAPH).
Podemos citar três desses projetos:
MultiNoC, MPSoC-H e HeMPS.
Objetivos do Trabalho O presente trabalho tem por objetivo estratégico o domínio das tecnologias MPSoC e NoC, desenvolvendo- se um novo MPSoC, com ênfase na integração rede­processador de forma estruturada.
De entre os objetivos específicos cita- se:
A) Desenvolver módulos de hardware responsáveis por a interface entre a rede intra-chip e o processador;
B) Definir as políticas de acesso à rede, desenvolvendo os drivers necessários para atender a estas políticas;
C) Validar a estrutura de hardware-- software com pelo menos um processador RISC.
Organização do documento O presente documento está organizado como segue.
O Capítulo 2 explora o estado da arte em MPSoC e NoCs, relacionando alguns tipos de MPSoCs acadêmicos e comerciais e alguns exemplos de arquiteturas NoC.
O Capítulo 3 descreve a arquitetura de hardware e a infraestrutura utilizada para o desenvolvimento do MPSoC proposto.
O capítulo 4 apresenta a infraestrutura de software utilizada para o desenvolvimento da implementação do tratamento de interrupções, dos drivers de comunicação e a interface de rede para conexão entre o hardware e a NoC.
O resultados obtidos e a validação de toda a infraestrutura do MPSoC são apresentados no Capítulo 5, em que a validação se divide em dois momentos, a validação do sistema sem a implementação de interrupção e a validação do sistema com a implementação de interrupção com a inclusão de um aplicação paralela.
Este Capítulo apresenta arquiteturas de MPSoCs, tanto comerciais quanto acadêmicas.
A segunda parte deste Capítulo revisa arquiteturas de redes intra-chip, como:
Xpipes, Hermes, SoCIN, QNoC, AEThereal.
MPSoCs A MultiNoC desenvolvida por baseia- se na rede Hermes de tamanho 2x2 com controle de fluxo baseado em handshake, dois processadores R8 com 1024 palavras de memória cache, uma memória compartilhada de 1024 palavras e uma interface serial com o computador hospedeiro.
A Figura 2 mostra os módulos que compõe o sistema MultiNoC.
O processador R8 utilizado foi desenvolvido por o grupo GAPH, possui arquitetura load/ store de 16 bits, endereçamento a palavra, banco de registradores com 16 registradores de uso geral, 4 flags de estado e execução de instruções em 3 ou 4 ciclos.
A memória (memory IP) possui 4 BlockRAMs, resultando em 1024 palavras de 16 bits.
A interface serial possui protocolo RS-232 e fornece uma comunicação bidirecional com o computador hospedeiro.
A MultiNoC é uma arquitetura Em uma (Non-Uniform Memory Access), em que cada processador tem sua memória local, mas pode ter acesso à memória de outros processadores.
Pode ser estendido para qualquer número de processadores assim como o número de memórias.
Também pode ser adaptado a protocolos de comunicação mais rápidos, como USB, PCI e Firewire.
O projeto desenvolvido por implementa um MPSoC utilizando uma rede Hermes a partir de o sistema MultiNoC.
A Figura 3 ilustra a arquitetura do MPSoC-H (MPSoC-HERMES).
As principais diferenças entre o sistema MultiNoC e o sistema MPSoC-H são:
Rede HERMES 3 x3 com controle de fluxo baseado em créditos;
8 processadores R8 com 12K de palavras de memória cache;
Ausência de memória compartilhada.
A principal função da interface serial é montar e desmontar pacotes.
Quando recebe informações do computador hospedeiro, a interface serial as encapsula em pacotes e envia para os módulos através da NoC.
Suas principais funcionalidades são:
Carga e dump das memórias do sistema, ativação dos processadores, E/ S de dados do sistema e debug do sistema.
Foi realizado a criação de um wrapper que possibilita a integração entre o processador e o MPSoC.
Este wrapper tem por função, controlar a execução do processador provocando pausas quando o mesmo executar instruções de leitura, escrita, de entrada ou saída e proporcionar um conjunto de serviços que serve de suporte para comunicação com os demais módulos.
Também inclui dentro deste wrapper, uma memória que serve como cache para o processador.
A memória cache foi implementada a partir de 12 módulos de BlockRAMs de 2048 palavras.
Todos os serviços do wrapper foram mapeados em memória, que são efetuados usando as instruções load/ store de acesso à memória.
De a mesma forma que foi feita com o processador, a interface serial também foi encapsulada dentro de um wrapper para facilitar a integração com o MPSoC-H..
Os serviços implementados no wrapper serial são invocados através de um software que acessa a porta serial do computador hospedeiro.
Outro projeto que está relacionado ao tema MPSoC é o de alocação e comunicação entre tarefas em MPSoCs, implementados na plataforma HeMPS.
Para a infraestrutura de hardware foi utilizado a NoC Hermes, e o processador de domínio público Plasma.
Módulos de hardware foram desenvolvidos, visando conectar o processador à NoC e realizar a alocação de tarefas na memória do processador.
Para a infraestrutura de software, foi desenvolvido um microkernel multitarefa que é executado em cada processador escravo e a aplicação de alocação de tarefas que é executada no processador mestre.
São exploradas duas estratégias de alocação de tarefas:
Uma estática e outra dinâmica.
A Figura 4 mostra a arquitetura proposta por este trabalho.
A arquitetura CellBE (Cell Broadband Engeni Architecture) (Figura 5), foi proposta em 2000 por um grupo denominado STI (Sony, Thoshiba, IBM), para estudar a possibilidade de criar um novo console para jogos.
Nota- se que mesmo sendo um processador de uso geral, ele também é direcionado para aplicações em dispositivos embarcados.
O projeto da arquitetura CellBE deveria conciliar, além de a velocidade de processamento, a redução do consumo de potência.
A solução para resolver este problema foi criar um processador com uma arquitetura multicore heterogênea.
Esta arquitetura permite alcançar alto desempenho, utilizando uma arquitetura paralela entre processadores de características diferentes.
Baseado na arquitetura do processador PowerPC, o CellBE é constituído de nove núcleos, divido num núcleo PowerPC e oito núcleos SPE (Synergistic Processor Element), ambos de propósito geral.
O PPE (PowerPC Processing Element) é a unidade central de processamento, enquanto os SPEs são responsáveis por acelerar aplicações específicas.
Além de os nove núcleos, a arquitetura possui um barramento principal EIB (Element Interconect Bus), um controlador de memória DEMAC (Direct Memory Acces Controller), um controlador de memória XDRAM, e uma entrada e saída de Rambus FlexIO.
A Figura 6 apresenta o layout do processador Cell, fabricado inicialmente em tecnologia de 90 nm,], oito camadas de metal, 234 milhões de transistores, e área de 221 mm2.
O Power Processor Element (PPE) é um processador RISC baseado na arquitetura POWER PC, possuindo as mesmas funcionalidades de um processador de propósito geral.
Seu principal objetivo é atribuir tarefas para as demais unidades de processamento (SPEs), funcionando como um controlador.
Ele é também o responsável por executar o sistema operacional e a maioria das aplicações mais simples, sendo as atividades de computação intensiva atribuídas aos processadores SPEs.
O PPE é um processador de 64 bits.
O PPE possui 64 KB em sua memória cache L1, sendo essa dividida em 32 KB de cache para instruções e outros 32 KB para dados, além de o SMT (Simultaneous Multithreading) que é um tecnologia similar ao Hyper Threading1.
Há também uma cache L2 de 512 KB, que foi reduzido para que não ocupasse uma área muito grande, e assim pudesse ser ocupado por outros processadores.
Para compensar esta perda são utilizadas memórias XDR da Rambus (Figura 6) de uso externo.
A IBM também inclui uma unidade VMX (Altivec) para fazer cálculos de precisão dupla e trabalhar com duplo processamento (dual issue), o que significa que pode executar duas instruções simultaneamente por ciclo.
Devido àa sua arquitetura RISC, os PPEs do Cell tem um consumo menor de energia do que o PowerPC, inclusive em altas taxas de relógio.
Além de a cache L2, o PPE ainda possui uma PXU (PowerPC eXecution Unit), que capaz de executar instruções de duas threads simultaneamente.
Esta PXU é dividida em três outros Unit).
A Iu é responsável por o controle do PPE e cuida de tarefas como fetch de instruções, decodificação e branch prediction.
A XU é responsável por a execução das instruções que envolvem números inteiros e de instruções do tipo Load e Store.
A VSU é responsável por a execução de instruções SIMD e também atua como unidade de ponto flutuante.
O SPE foi criado para preencher uma lacuna existente entre os processadores de propósito geral e os de propósito específico (melhor desempenho numa única aplicação).
O objetivo principal é garantir o desempenho no processamento de grande quantidade de dados de jogos, aplicações multimídia e sistemas de grande largura de banda de dados, como áudio, vídeo, criptografia e simulação de fenômenos físicos.
Cada um dos oito SPEs é um microprocessador RISC SIMD (Single Instruction Multiple Ddata) independente constituído de um núcleo de processamento SPU (Synergistic Processor Unit) e uma memória local de 256 KB (ele não possui uma memória cache própria) e um MFC (Memory Flow Controller).
O SPE, assim como PPE é capaz de fazer processamento duplo e uma unidade de SPE pode calcular até 25,6 GFlops, resultando num total de até 200 GFlops.
As SPUs possuem bancos de 128 registradores com 128 bits cada.
De essa forma cada Hyper Threading -- tecnologia desenvolvida por a Intel, utilizada em processadores que o faz simular dois processadores tornando o sistema mais rápido quando se usa vários programas ao mesmo tempo.
Sabe- se que o acesso constante à memória causa um custo muito grande para uma melhor performance, tentando minimizar este problema o SPE possui uma memória própria (cache), evitando assim o acesso direto à memória do sistema, ou seja, cada SPE possuí uma região de memória chamada de Ls (Local Store) com um tamanho de 256 KB.
Esta memória é extremamente rápida, e é utilizada para guardar quantas instruções forem necessárias.
Com essa implementação houve uma redução do tamanho do chip, pois ocorreu consequentemente a redução do número de transistores utilizados.
A responsabilidade de gerenciar o movimento de dados entre a memória principal e a Ls fica a cargo de o software.
O MFC (Memory Flow Controller) serve como uma interface para o sistema e o SPU, criando uma polarização do sistema de transferência de dados, proteção e sincronia entre a memória principal e a área local do SPU.
Ela é responsável por a comunicação com a memória principal por DMA, onde através de filas fazem a transferência de dados com a memória principal, que pode ser iniciada tanto por os PPEs quanto por os SPEs.
O EIB (Element Interconect Bus) é o barramento responsável por a interconexão entre os SPEs, o PPE e o controlador de I/ O, que permite aos núcleos efetuar leitura e gravação de forma independente e simultânea com relação a os outros núcleos.
Foi implementado na forma de quatro anéis que circulam em sentidos contrários em pares.
Este barramento trabalha com a metade do ciclo de clock do processador, sendo capaz de transmitir 16 bytes a cada dois clocks do sistema.
Devido a esta característica este barramento é definido como barramento de oito bytes por ciclos.
O TILE64 foi desenvolvido por a Tilera Corporation baseado numa arquitetura capaz de gerenciar centenas de núcleos num mesmo encapsulamento.
Inicialmente foi projetado para trabalhar como chip em redes inteligentes e equipamentos de distribuição de mídia.
Empresas como 3 Com, Codian e GoBackTV já utilizam esta tecnologia.
Comparando com o desempenho de um Xeon-Dual Core para sistemas embarcados numa mesma situação, ele atinge um pico 10 vezes maior.
Este processador possui uma rede de 64 núcleos (tiles) de 32 bits (Figura 7) com características idênticas, conectado por uma rede malha, responsável por a comunicação entre os núcleos e com o mundo exterior.
Cada tile é um processador, incluindo uma memória cache integrada com L1 e L2 que interliga o tile à malha.
A rede malha é de fato constituída por cinco sub-redes independentes, duas redes são totalmente gerenciadas por o hardware e podem transportar diferentes tipos de dados de e para cada tile ou transferência via DMA, as outras três redes estão disponíveis para uso da aplicação, permitindo a comunicação entre os núcleos e dispositivos I/ O (Figura 7).
A rede IMesh (Intelligent Mesh) permite uma comunicação sem interromper as aplicações que estão rodando nos tiles.
Isso facilita a transferência de dados entre os tiles, contendo um total controle para cada conexão de rede.
Ao invés de um barramento centralizado, cada núcleo tem sua própria memória, podendo compartilhar dados com quatro núcleos vizinhos.
Como mostra a Figura 8, a memória está dividida em duas caches L1 de 8 KB e uma L2 de 64 KB, com L3 simulado até 5 MB quando necessário.
O Ci também integra uma série de interfaces incluindo XAUI, gigabit Ethernet, UARTs, Flexibel I/ O e quatro controladores DDR2.
A arquitetura do processador TILE64 incorpora um array bidimensional homogêneo de núcleos de propósito geral.
Para cada processador há um roteador que interliga o núcleo à rede.
A combinação de um núcleo e um roteador forma a construção básica de um tile.
Cada núcleo é um processador totalmente funcional capaz de executar sistemas operacionais completos.
Cada núcleo é otimizado para fornecer o maior desempenho com a menor potência, podendo trabalhar a velocidades entre 500 MHz e 1 GHz, com consumo de energia a um nível tão baixo como 170 mW numa simples aplicação.
A Intel vem realizando pesquisas em processadores com vários núcleos.
Uma das últimas pesquisas em processadores da Intel é o chamado Polaris ou Intel Core-80, que possui 80 núcleos.
Este processador, de acordo com a Intel, é capaz de realizar um trilhão de operações em ponto flutuante por segundo, ou 1 Teraflop.
Fazendo uma breve comparação, tomemos como exemplo o supercomputador ASCI Red, que em 1996 para atingir os mesmos 1 Teraflop utiliza 10 mil processadores Pentium Pro1 de 200 MHz, seu consumo era de 500 kW e outros 500 kW somente para refrigerar a sala onde se encontrava.
Cada um dos 80 núcleos possui 1,2 milhões de transistores ocupando uma área de 3 mm2, totalizando 100 milhões de transistores para uma área total de 275 mm2.
Todos os núcleos compartilham uma região de cache de 4 MB.
Seu processo de fabricação é de 65 nanômetros.
A Figura 9 apresenta a estrutura do processador e a estrutura de cada tile.
Em uma primeira versão do processador Polaris, sua frequência de trabalho era de 3,16 GHz, cada núcleo trabalha com uma tensão de 0,95 V e com um consumo total de 62 W. Para obter este consumo a Intel utilizou a tecnologia denominada de fine-Grain Power Management, que coloca em estado ocioso os núcleos não utilizados.
O Pentium Para o internamente funciona como se fosse três processadores em paralelo e é capaz de executar até três instruções por pulso de clock.
Possui 5,5 milhões de transistores, largura de Bus de 64 bits e cache L2 de 256K e 512K e co-processador interno.
A arquitetura NoC do processador Polaris é apresentada na Figura 10 que contém 80 tiles arranjados numa rede com uma topologia malha 2D de 10x8 e operando à uma frequência de 4 GHz.
Cada tile consiste de um PE (processing engine) com interface mesócrona que direciona os pacotes entre os tiles e está conectado a um circuito de roteamento com 5 portas de comunicação, com alta vazão e baixa latência para a troca de mensagens.
O roteador da NoC possui chaveamento do tipo wormhole e dois canais físicos para transmissão de pacotes.
A NoC do Polaris habilita uma largura de banda de 256 GB/ s.
Todos os PEs possuem duas unidades de processamento de ponto flutuante (FPMAC), uma memória de instrução de ciclo único com 3 KB (IMEM) e uma memória de dados com 2 KB (DMEM).
Uma VLIW (Very Long Instruction Word) de 96 bits codifica até 8 operações por ciclo.
Com um arquivo de registro de 10 portas (6- ler, 4- escrever), a arquitetura permite a programação de ambas FPMACs, carregar e armazenar a DMEM simultaneamente, enviar/ receber pacotes da rede malha, controle de programa, e dynamic sleep instructions.
Um bloco de interface de roteamento (RIB) trata o encapsulamento do pacote entre o PE e o roteador.
Uma arquitetura totalmente simétrica permite a qualquer PE enviar/ receber instruções e pacotes de dados de qualquer outro tile.
NoCs A rede Xpipes foi proposta por Bertozzi et.
Al. Para SoCs multiprocessados.
Esta NoC possui roteamento wormhole e faz uso do algoritmo de roteamento estático denominado street sign routing.
As rotas são obtidas através da interface de rede, acessando uma tabela que tem por entrada o endereço destino.
Este algoritmo de roteamento permite uma implementação simples do roteador porque nenhuma decisão dinâmica tem que ser tomada nos nodos de processamento.
Uma das principais preocupações no projeto da Xpipes foi a confiabilidade na comunicação.
Isto foi alcançado por meio de detecção de erro distribuída, ou seja, a cada roteador.
Embora a detecção de erro distribuída cause uma sobrecarga de área nos nodos da rede, se comparada com uma solução fim-a-fim, ela é mais bem capacitada para conter os efeitos da propagação de erro, por exemplo impedindo que um cabeçalho corrompido seja transmitido para um caminho errado.
A rede intra-chip Xpipes possui alto grau de parametrização.
A parametrização inclui o tamanho do flit, o espaço de endereçamento dos núcleos, o número máximo de roteadores entre dois núcleos, o número máximo de bits para controle de fluxo fim-a-fim, a profundidade do buffer, o número de canais virtuais por canal físico, entre outros parâmetros.
Em a Figura 11 é apresentado o esquema do roteador, o qual possui 4 entradas, 4 saídas e 2 canais virtuais multiplexados sobre o mesmo canal físico.
O roteador adota buferização de saída e a arquitetura resultante consiste de múltiplas replicações do mesmo módulo de saída apresentado na Figura 12, um para cada porta de saída do roteador.
Os sinais de controle de fluxo gerados em cada módulo (tal como ACK e NACK para flits de entrada) são coletados por uma unidade centralizada do roteador, a qual transmite ao roteador fonte apropriado.
Como pode ser observado na Figura 12, cada módulo de saída tem 7 estágios de pipeline para maximizar a frequência de operação de clock do roteador.
Os decodificadores CRC para detecção de erro trabalham em paralelo com a operação do roteador, desse modo ocultando sua latência.
O primeiro estágio do pipeline verifica o cabeçalho dos pacotes nas diferentes portas de entrada para determinar se os pacotes têm que ser roteados através de determinada porta de saída.
Somente pacotes compatíveis com a porta de saída são enviados para o segundo estágio, em o qual são resolvidas disputas baseadas numa política round robin.
A arbitragem é realizada quando o flit terminador do pacote antecessor é recebido, de modo que todos os outros flits do pacote possam ser propagados sem disputa.
O terceiro estágio possui apenas um multiplexador, o qual seleciona a porta de entrada prioritária.
O estágio seguinte de arbitragem guarda o status do registrador de canal virtual e determina se flits podem ser armazenados no registrador ou não.
Um flit de cabeçalho é enviado para o registrador com o maior número de posições livres, seguido por sucessivos flits do mesmo pacote.
O quinto estágio é o estágio de buferização, e a resposta ACK/ NACK neste estágio indica se um flit foi armazenado corretamente ou não.
O estágio seguinte cuida do envio do controle de fluxo:
Um flit é transmitido para o próximo roteador somente quando a porta de saída do roteador destino possuir posições livres disponíveis no registrador adequado.
O último estágio de arbitragem multiplexa os canais virtuais no enlace do canal físico.
A interface de rede Xpipes fornece uma interface padronizada OCP para conectar aos núcleos.
A Ni dos núcleos que iniciam a comunicação (iniciadores) precisa transformar o OCP em pacotes para serem transmitidos através da rede.
Ele representa o lado escravo de uma conexão OCP fim-a-fim, e, por isso, é chamado de network interface slave (NIS).
Sua arquitetura é visualizada na Figura 13.
A rede Hermes foi desenvolvida por o grupo de pesquisa GAPH.
A rede Hermes possui um mecanismo de comunicação denominado chaveamento de pacotes, em o qual os pacotes são roteados individualmente entre os nodos sem o estabelecimento prévio de um caminho.
Este mecanismo de comunicação requer o uso de um modo de roteamento para definir como os pacotes devem se mover através dos roteadores.
A Rede Hermes utiliza o modo de roteamento wormhole, em o qual um pacote é transmitido entre roteadores em flits, cada pacote é transmitido flit a flit por os canais físicos.
Apenas o flit de cabeçalho possui a informação de roteamento.
Assim, os flits restantes que compõe o pacote devem seguir o mesmo caminho reservado por o cabeçalho.
A rede Hermes adota a topologia malha, a qual é justificada em função de a facilidade de desenvolver o algoritmo de roteamento, inserir núcleos e gerar o layout do circuito.
O roteador da rede Hermes possui uma lógica de controle de chaveamento centralizada e 5 portas bidirecionais:
East, West, North, South e Local.
A porta Local estabelece a comunicação entre o roteador e seu núcleo local.
As demais portas ligam o roteador aos roteadores vizinhos.
O modo de chaveamento wormhole adotado no roteador Hermes permite que cada canal físico seja multiplexado em n canais virtuais (VCs).
A Figura 15 apresenta o roteador Hermes com dois VCs por canal físico.
A lógica de controle de chaveamento implementa uma lógica de arbitragem e um algoritmo de roteamento.
Quando um roteador recebe um header flit, a arbitragem é executada e se a requisição de roteamento do pacote é atendida, um algoritmo de roteamento é usado para conectar o flit da porta de entrada à correta porta de saída.
Cada roteador deve ter um endereço único na rede.
Para simplificar o roteamento na rede, este endereço é expresso nas coordenadas XY, onde X representa a posição horizontal e Y a posição vertical.
A rede SoCIN (System-on-Chip Interconnection Network), foi desenvolvida por o Programa de Pós-graduação em Computação da Universidade Federal do Rio Grande do Sul (PPGC-UFRGS) A rede SoCIN possui topologia direta podendo ser configurada uma malha 2-D ou toróide 2D.
Possui controle de fluxo do tipo handshake, roteamento do tipo fonte e determinístico, chaveamento por pacote tipo wormhole, arbitragem dinâmica distribuída e memorização de entrada.
É baseado num soft- core de um roteador parametrizável denominado RASoC (Router Architecture for SoC).
Esse soft- core é descrito em VHDL e possui quatro parâmetros básicos:
Número de portas de comunicação (até 5);
largura da parte de dados do canal físico (n);
profundidade dos buffers de memorização (p); (
iv) e largura da informação utilizada para roteamento das mensagens (m), a qual determina a dimensão máxima da rede.
O enlace da rede SoCIN possui dois canais unidirecionais em oposição.
Cada canal possui n sinais de dado, dois sinais de enquadramento do pacote (bop e eop) e dois sinais de controle de fluxo (val e ack).
O sinal bop (begin-of-- packet) marca o início do pacote, enquanto que o eop (end-of-- packet) marca o seu final.
O sinal val (valid) é utilizado por o emissor para sinalizar a presença de um dado válido no canal, enquanto que o sinal ack (acknowledgement) é utilizado por o receptor para confirmar o recebimento do dado.
Os n sinais de dado e os sinais de enquadramento constituem o phit (n+ 2 bits).
O pacote do protocolo da rede SoCIN é composto por um flit de cabeçalho e por um número irrestrito de flits que compõem a carga útil do pacote, sendo destinados ao enquadramento do pacote que o último flit da carga útil é também o terminador do pacote.
Cada flit possui n+ 2 bits, de os quais n bits são reservados ao transporte de informação (ou dado) e dois bits são destinados ao enquadramento do pacote.
A arquitetura Quality-of--Service NoC (QNoC) proposta por Bolotin et al.
Está baseada numa topologia malha com chaveamento de pacotes wormhole e com controle de fluxo baseado em créditos.
O roteamento wormhole reduz a latência e o requerimento de filas nos roteadores.
A QNoC oferece quatro classes de serviço:
Sinalização: Nível de serviço com a maior prioridade na rede, para assegurar baixa latência.
É utilizado por interrupções e sinais de controle.
Tempo-real: Nível de serviço que garante largura de banda e latência para aplicações de tempo real.
Leitura/ Escrita (RD/ WR):
Nível de serviço projetado para suportar acessos curtos a memórias e registradores.
Transferência de blocos:
Nível de serviço usado para transferência de mensagens longas e blocos grandes de dados, tal como conteúdo de cache e transferências DMA.
Os pacotes transportam as informações, command e payload.
Em a Figura 17 podemos visualizar o formato básico do pacote.
O campo TRA (Target Routing Address) contém o endereço solicitado por o roteamento.
O campo command indentifica o payload, especificando o tipo de operação.
O resto da informação é o payload, de comprimento arbitrário.
O pacote é dividido em flits que são classificados dentro de os seguintes tipos:
FP (Full último flit do pacote.
O tipo do flit e o nível de serviço são indicados em fios separados de controle.
A Tabela 1 descreve os sinais de entrada e saída da porta de saída.
A Figura 18 ilustra a arquitetura do roteador.
O roteador possui até cinco conexões:
Quatro para roteadores vizinhos e uma para o núcleo local.
O roteador transfere pacotes das portas de entrada para as portas de saída.
Dados são recebidos em flits.
Cada flit que chega é primeiro armazenado em buffers de entrada.
Existem buffers separados para cada um dos quatro níveis de serviço.
O algoritmo de roteamento determinístico XY é invocado quando o primeiro flit do pacote é recebido.
O primeiro flit contém o endereço destino do pacote e é utilizado para determinar a qual porta de saída o pacote é destinado.
O número da porta de saída selecionada para a transmissão do pacote de cada nível de serviço é armazenado na Tabela de Roteamento Atual (CRT, do inglês Current Routing Table).
Quando um flit é enviado da porta de entrada para a porta de saída, um buffer torna- se disponível e um crédito de buffer é enviado ao roteador anterior.
Cada porta de saída do roteador é conectada a uma porta de entrada do próximo roteador.
A porta de saída gerência o número de posições disponíveis no buffer de cada nível de serviço da próxima porta de entrada.
Estes números são armazenados no Estado do Próximo Buffer (NBS, do inglês Next Buffer State).
O número é decrementado quando um flit é transmitido e incrementado quando um crédito de buffer do próximo roteador é recebido.
A porta de saída escalona a transmissão de flits de acordo com a disponibilidade de buffers no próximo roteador, com a prioridade do nível de serviço e com a arbitragem round-robin das portas de entrada esperando transmissão de pacotes dentro de o mesmo nível de serviço.
O número de buffers disponíveis no próximo roteador é armazenado na tabela NBS de cada nível de serviço de cada porta de saída.
A prioridade dos níveis de serviço é fixa, ordenada com Signalling tendo a maior prioridade, real-time tendo a segunda, RD/ WR a terceira e o Block--Transfer a última.
O estado atual da arbitragem round-robin é armazenado na tabela do número da Porta de Entrada Atualmente Servida (CSIP, do inglês Currently Serviced Input Port number) para cada nível de serviço de cada porta de saída.
Este número avança quando a transmissão de um pacote é finalizada ou se nenhuma transmissão de uma determinada porta de entrada e nível de serviço existe.
Goossens et.
Al. Propuseram a rede intra-chip.
Ela oferece serviços diferenciados com conexão.
Uma conexão descreve a comunicação entre um núcleo origem e um ou mais núcleos destino, com um nível de serviço associado.
Conexões devem ser criadas expressando o nível de serviço requisitado.
A aceitação da conexão pode incluir a reserva de recursos na rede, tais como buffers ou o percentual de largura de banda do canal.
Depois do uso, a conexão é finalizada e os recursos liberados.
Diferentes conexões podem ser criadas ou finalizadas independentemente.
A rede intra-chip possui um roteador que combina vazão garantida (GT) e melhor esforço (Be), como ilustrado na Figura 19.
De esse modo é garantido o comportamento de pior caso aliado com uma boa média de uso de recursos.
O serviço GT oferece uma latência fim-a-fim fixa e tem a mais alta prioridade, forçada por o árbitro.
O serviço Be usa toda a largura de banda que não está reservada ou não é usada por o tráfego GT.
Recursos são sempre utilizados quando existem dados disponíveis.
As conexões GT são criadas ou removidas através de pacotes Be que reservam um caminho na rede, dependendo da disponibilidade de recursos.
Os pacotes Be usam roteamento wormhole na origem, e os fluxos GT usam chaveamento por circuito com STDM.
Roteadores usam filas virtuais de saída para o tráfego Be.
Este Capítulo apresenta a primeira contribuição desta Dissertação.
Em este Capítulo são apresentados os ambientes de desenvolvimento utilizados para o projeto do hardware, a descrição do processador, assim como os acréscimos realizados na descrição no escopo do trabalho.
Esta arquitetura é um MPSoC homogêneo, constituído de vários núcleos de processamento, compostos por processadores MR4 e conectados através da NoC Hermes com uma topologia do tipo malha 2 D, como ilustrado na Figura 20.
Para o desenvolvimento deste MPSoC forma reutilizados módulos já desenvolvidos por o grupo GAPH, os quais compreendem o processador MR4 e a NoC Hermes.
Para o desenvolvimento deste projeto a rede Hermes foi parametrizada utilizando os seguintes parâmetros:·
Controle de Fluxo:
Credit Based· Sem canais virtuais· Dimensão:
3x3· Largura do flit:
32· Profundidade dos buffers:
8· Algoritmo de roteamento:
XY Processador MR4 O processador MR4 utilizado neste trabalho foi desenvolvido por o GAPH e é um dos variantes da família MRx, que tem como características gerais:
Arquitetura Harvard, load/ store;
Dados e endereços de 32 bits;
Endereço de memória orientado a byte, onde cada palavra ocupa 4 posições consecutivas de memória;
O banco de registradores possui 32 registradores de uso geral de 32 bits que vão de 0 à Formato de instrução regular.
Todas as instruções possuem o mesmo tamanho, ocupando uma 1 palavra em memória.
Para efetuar a troca de informações entre o processador e as memórias, o MR4 utiliza os seguintes sinais:·
i_ adress:
Barramento unidirecional de 30 bits;·
instruction: Barramento unidirecional de 32 bits;·
d_ adress:
Barramento unidirecional de 32 bits;·
data: Barramento bidirecional de 32 bits.
Em a Figura 23 temos a organização do bloco de dados.
A execução de qualquer instrução requer de 3 à 5 ciclos, busca de instrução, decodificação e leitura dos registradores, operações com a ALU (unidade lógico-aritmética), (iv) acesso a memória (v) atualização do banco de registradores (write-back).
O bloco de dados necessita de 21 sinais de controle, organizados em quatro classes:·
habilitação de escrita em registradores:
Wpc, CY1, wreg, CY2, walu, wmdr;·
controle de leitura/ escrita na memória externa:
Ce, rw e bw;·
as operações que a unidade lógico-aritmética, executa;·
a seleção da operação do comparador;·
os controles dos multiplexadores, resultantes da decodificação da instrução.
A execução das instruções é feita através de uma máquina de estados de controle.
A Figura 24 mostra a máquina de estados, seus estados e os registradores que são modificados durante sua execução.
As funções dos estados são descritas a seguir.
Idle: Estado inicial após o reset, serve para garantir que a primeira borda de subida de relógio após este sinal defina o início da operação do processador MRx;
Sfetch: Primeiro ciclo, busca de instrução;
SReg: Segundo ciclo, leitura dos registradores fonte;
SAlu: Terceiro ciclo, operação com a ALU e eventual comparação no comparador (em paralelo);
Wbk: Quarto ciclo para a maioria das instruções, onde se escreve o resultado no banco de registradores e atualiza- se o contador de programa (quinto ciclo para as instruções Lw e LBU);
Sld: Quarto ciclo das instruções Lw e LBU, onde se lê um dado da memória de dados externa;
Sst: Último ciclo das instruções SW e Sb, onde se escreve um dado na memória de dados externa;
Ssalta: Último ciclo das instruções de salto condicional ou incondicional, apenas atualiza valor do PC.
A principal modificação do processador MR4 para este trabalho inclui o tratamento de interrupção.
Esta modificação é descrita na Seção seguinte.
Inclusão de tratamento de interrupção no processador MR4 A principal justificativa para implementação da interrupção é evitar- se a perda de desempenho na recepção dos dados.
Não havendo um mecanismo de interrupção no processador devem- se verificar periodicamente bits de status para a detecção de dado, processo este denominado polling.
O polling é a forma mais simples para um dispositivo de E/ S se comunicar com o processador.
Em este caso, o principal problema para um processo de polling é a sobrecarga do processador, que realiza todo o trabalho de controle de envio e recepção dos dados.
Como veremos no Capítulo 5, um dos principais problemas de latência na rede é o tempo gasto por o processador, verificando se pode enviar dados à Ni.
De acordo com há duas formas de endereçar os dispositivos de E/ S, mapeamento em memória e instruções de E/ S especiais.
O método utilizado neste caso foi implementar a interrupção através de mapeamento em memória.
As principais mudanças na arquitetura do MR4 foram:
A inclusão de novas instruções para o tratamento da interrupção;
A inclusão de um novo estado na máquina de estados de controle;
Novos registradores.
Estas modificações, apresentadas na Figura 25, serão habilitar_ interrupção end_ mul detalhadas nas próximas Seções.
Em o módulo datapath foram adicionados um novo pino de saída, habilita_ interrupção e um novo registrador EPC.
Dentro de o módulo control_ unit foram incluídos dois pinos de entrada, módulo mrstd (descrição de mais alto nível na hierarquia do processador), foi adicionado o pino de entrada intr.
Além de essas implementações foi adicionado um novo controlador de escrita em registrador, a microoperação lepc.
Em a Figura 26 apresenta- se o bloco de dados completo com as inclusões realizadas durante o projeto.
O sinal habilita_ interrupção foi criado para indicar quando o estado da interrupção foi alterado.
Por default a interrupção inicia sempre ativa.
O processo de habilitar/ desabilitar deste sinal é realizado através da execução da instrução mtc0.
A o se iniciar o tratamento de uma interrupção, deve- se desabilitar o tratamento de outras interrupções.
No caso de ocorrer uma requisição de interrupção durante o atendimento de uma dada interrupção, o processador não será capaz de recuperar o contexto de suas operações, dado que não há mecanismo de empilhamento de chamadas de interrupção.
Com isso foi necessário fazer o mascaramento por o próprio processador.
Para realizar tal mascaramento foi criado o flip-flop em_ atendimento dentro de a entidade control_ unit e um registrador EPC, que armazena o endereço da última instrução que estava sendo executada quando ocorreu a interrupção.
O processador, ao iniciar a execução de um programa coloca o sinal interno em_ atendimento em zero.
Cada vez que o processador entrar no ciclo de busca de instrução é feito o teste:
INTR $= '1' and em_ atendimento $= '0' Se este teste for verdadeiro, passa- se para um novo estado na máquina de controle, denominado Sint, e realizam- se as seguintes funções:
1 na prática este é o endereço 4, dado que o espaço de instruções inicia no simulador Spim, utilizado no presente trabalho, em 0x400020);
Para simplificar a descrição VHDL do sistema, foi criado o módulo elemento de processamento (elemento_ pe.
Vhdl) ou PE, o qual possui a entidade do processador MR4 (MRStd), a entidade da memória de dados (Data_ mem) e a entidade da memória de instruções (Instr_ mem).
A Figura 28 apresenta as interfaces de cada elemento interno do módulo elemento_ pe, assim como os sinais para a comunicação com a interface de rede.
A conexão do elemento_ pe com a interface de rede compreende os seguintes sinais:
Além de as alterações para a inclusão das entidades dentro de o núcleo de processamento, foi necessário modificar o tamanho da memória dentro de o módulo elemento_ pe.
Em o projeto inicial a memória de dados possuía um tamanho de 2048 palavras.
Devido a a aplicação utilizada na validação, ordenação merge sort, o tamanho foi alterado para 20480 palavras.
O registradorSP (stack pointer) é também ajustado de acordo com o tamanho de memória.
O código objeto e os dados iniciais da aplicação que serão executados no processador são carregados no início da execução do sistema.
Para gerar o código objeto foi utilizado simulador/ montador PCSpim.
Interface de Rede Para o desenvolvimento da interface de rede (Ni, do inglês, Network Interface) foi elaborado primeiramente um módulo de transmissão de dados, denominada outcoming unit e um módulo de recepção de dados denominada incoming unit.
Posteriormente, as interfaces são integradas, formando assim a Network Interface.
A outcoming unit tem por objetivo enviar pacotes oriundos do processador para a NoC.
A outcoming unit possui um buffer (buf_ ni_ cpu) de até 1024 posições (parametrizável em tempo de projeto).
O processo de envio é feito em 3 etapas:
O processador começa a escrever os dados do pacote no buffer quando a faixa de endereçamento for fornecida entre Ni_ TX_ BASE_ ADDRESS e Ni_ TX_ HIGH_ ADDRESS.
O processador avisa a outcoming unit que terminou de escrever o pacote no buffer, ativando o sinal end_ pack.
Este sinal é ativado através de escrita no endereço Ni_ TX_ END_ PACKET e quando os sinais ce $= '1', rw $= '0' e do bit LSB do sinal data_ in_ cpu $ ` 1'.
Uma vez sinalizado o final de escrita do pacote cada palavra é enviada à NoC.
Através do sinal tx é informado à NoC que há dados a enviar.
Os estados desta máquina correspondem às seguintes funções:
SIdle. Estado Inicial.
A máquina estados inicia e permanece neste estado enquanto o reset estiver ativado em` 1', se não avança para o próximo estado.
SReceive. A máquina de estados permanece neste estado enquanto o sinal de end_ pack estiver desativado.
A máquina de estados avança para o estado STransmiting quando o sinal end_ pack for ativado.
STransmiting. Em este estado ativa- se o sinal tx e incrementa- se o contador_ flits.
A máquina de estados permanece neste estado enquanto o sinal de crédito credit_ i estiver ativo, indicando que pode ser enviado o pacote.
A máquina de estados avança para o estado SWaiting quando o sinal de credit_ i estiver desativado.
O sinal credit_ i é gerado por o controle de fluxo da NoC.
A o terminar de enviar para a NoC o pacote (controle por o número de flits transmitidos), a máquina retorna para o estado SReceive.
SWaiting: A máquina de estados permanece neste estado enquanto o sinal de credit_ i estiver desativado e avança para o próximo estado quando credit_ i for ativado.
Em este estado o sinal tx é desativado.
Durante os estados STransmiting e SWaiting um sinal denominado busy é ativado.
O processador deve ler o sinal busy antes de cada escrita no buffer para evitar que dados que estão sendo enviados sejam corrompidos por um novo pacote.
A incoming unit tem por objetivo receber pacotes da NoC, e enviar- los para o processador.
Esta interface é similar à anterior, possuindo também um buffer (buf_ ni_ noc) de até 1024 posições (parametrizável em tempo de projeto).
O pacote oriundo da NoC é escrito neste buffer.
A o final da escrita do pacote ativa- se um registrador indicando pacote no buffer.
O processador ao detectar este sinal ativo, realiza a leitura do buffer.
Os estados desta máquina correspondem às seguintes funções:
SIdle. Estado inicial.
Quando rx (aviso da NoC que há novo pacote) e credit_ o_ ni (interface de rede pode receber dados) estiverem ativos, a máquina de estado avança para o estado SReceive.
SReceive: Quando o estado atual estiver em SReceive e os sinais rx e credit_ o_ ni forem ativados, a máquina de estado avança para o próximo estado SReceiving.
Em este estado é zerado o contador cont_ in, que controla a ocupação do buffer_ ni_ noc.
SReceiving: O buffer realiza a recepção do pacote.
O contador cont_ in é incrementado neste estado, desde que rx seja igual a` 1'.
Quando todos os flits forem recebidos, avança- se para o estado SEnding.
SEnding: Este estado é responsável por ativar o sinal ready_ pack, que indica ao processador que há pacote para receber (ativa o sinal de interrupção intr).
O processador ao escrever no endereço de end_ receive indica à esta máquina que o pacote foi totalmente lido, permitindo a volta ao estado SIdle e a conseqüente recepção de um novo pacote.
Para que a Ni pudesse armazenar todos os flits enviados por o CPU e consequentemente da NoC, foi necessário criar dois buffers, um de recepção da CPU e outro para a recepção da NoC com tamanhos iguais de 1024 posições limitando o tamanho do pacote a 1022 palavras de payload (lembrando, dois flits são reservados para o header).
A Ni é responsável por integrar as unidades outcoming unit e incoming unit.
A Ni realiza a interface entre o processador MR4 e a NoC Hermes e tem por funções:
Enviar os pacotes para a NoC Hermes, sendo enviado por o processador, recebido por a Ni e depois enviado para a NoC;
Receber os pacotes da NoC Hermes, sendo recebido por a Ni e depois enviado para o processador.
A Figura 31 apresenta a interface externa da Ni.
Os sinais de interface com a CPU compreendem:
Ce: Quando ativado indica se está em curso uma operação com a memória de dados da CPU;
Rw: Quando ativado indica que está ocorrendo uma operação de leitura e quando desativado uma operação escrita;
D_ address:
Indica o endereço da memória para leitura ou escrita de dados; (
iv) data_ in_ cpu:
Dado recebido da CPU;
data_ out_ cpu:
Dado enviado para a CPU;
Intr: Sinal de interrupção que vai para a CPU.
Os sinais de interface com a NoC compreendem:
Clock_ tx:
Clock para sincronizar o envio de dados;
Tx: Informa à NoC que tem dado a enviar;
Data_ out_ noc:
Dado a ser enviado à NoC;
Credit_ i:
Indica se a NoC pode receber dados;
Clock_ rx:
Clock para sincronizar a recepção de dados;
Rx: Indica se tem dado a receber da NoC;
Data_ in_ noc:
Dado recebido da NoC;
Credit_ o:
Informa à NoC se pode receber dados.
Em este capítulo é descrito os ambientes para o desenvolvimento deste trabalho e os métodos para a comunicação entre os núcleos de processamento (elemento_ pe) e a NoC Hermes.
Para isso foram desenvolvidos os drivers de comunicação e a estrutura que trata das chamadas de interrupção.
Também é descrito as primeiras formas de envio de pacotes para teste de validação assim como o desenvolvimento de uma aplicação paralela para avaliação de desempenho da arquitetura.
Ambientes de Desenvolvimento Atlas é um ambiente para geração e avaliação de NoCs.
O projeto de NoC requer novas características, tais como suporte a modos de chaveamento, algoritmos de roteamento e tipos de escalonamento.
O grupo GAPH desenvolveu o ambiente Atlas para automatização dos processos que englobam os projetos de NoCs:
Geração de rede, geração de tráfego, simulação e avaliação de desempenho.
A geração da NoC permite as seguintes parametrizações:
Topologia, algoritmo de roteamento, largura dos canais de comunicação, profundidade dos buffers, número de canais virtuais, estratégias de controle de fluxo.
A partir de esta configuração é gerada a rede intra-chip que deve respeitar os requisitos de determinada aplicação.
Em a geração de tráfego, os cenários de tráfego são gerados para caracterizar aplicações que executarão sobre a rede.
Em a simulação, os dados de tráfego são injetados na rede.
Em a etapa de avaliação de desempenho é possível gerar gráficos, tabelas, mapas e relatórios para auxiliar na análise dos resultados obtidos.
A Figura 32 ilustra a interface principal do ambiente Atlas, a qual permite invocar as ferramentas que compõem as etapas do fluxo de projeto.
As principais ferramentas do ambiente Atlas estão descritos abaixo com suas respectivas funções:
NoC Generation ­ Maia:
Automatização do processo de interconexão dos roteadores.
Traffic Generation ­ Traffic Mpbs:
Gera os arquivos de tráfico que serão transmitidos através da rede durante a simulação.
NoC Simulation: Verifica o correto funcionamento da rede, através do simulador ModelSim.
Traffic Evaluation ­ Traffic Measurer:
Tem o objetivo de facilitar a análise dos resultados gerados durante a simulação da rede.
Para auxiliar no desenvolvimento, foram utilizados dois simuladores para linguagem de montagem, o PCSpim (Figura 33) para gerar o código objeto e o Mars (Figura 34) para simulação do código.
Inicialmente o código é escrito em linguagem de montagem e depois gerado o código objeto final por o PCSpim, logo após é realizada a adequação do código para executar no processador MR4, visto que é necessário a eliminação de algumas linhas de código para tal adequação.
As simulações do código assembly foram realizadas no Mars por ter uma estrutura de identificação de erro e um ambiente mais amigável.
A Figura 33 apresenta o simulador utilizado para gerar o código objeto.
O simulador Mars poderia ser também utilizado para gerar o código objeto, mas seria necessário modificar o endereço de inicio de leitura do arquivo objeto, sendo necessário modificar a estrutura vhdl do mrstd.
Vhd. Em a Figura abaixo é mostrado ao ambiente do simulador Mars.
Para validar a implementação do MPSoC desenvolvido utilizou- se o simulador Modelsim.
Este simulador possibilitou desenvolver os módulos de interface de rede, assim como o núcleo de processamento.
Além de a inclusão de novos módulos, permitiu- nos realizar toda a parte de simulação e debug do projeto.
Todos os gráficos e formas de onda foram gerados através do simulador Modelsim, assim como o acesso à memória em tempo de simulação deu- nos** uma visão ampla do conteúdo de cada registrador criado em memória.
Em a Figura 35 é ilustrado o ambiente que permitiu toda a parte de desenvolvimento de nosso projeto.
Tratamento de Interrupção De acordo com o que foi mencionado no Capítulo 3, quando ocorre uma interrupção o registrador EPC armazena PC+ 4, ou seja, o endereço da próxima instrução e PC recebe o endereço da rotina de tratamento de interrupção, denominada interrupt_ handler.
O endereço armazenado em EPC é novamente carregado quando a rotina interrupt_ handler é finalizada.
Os endereços de entrada e saída mapeados em memória utilizados por a interrupt_ handler compreendem:
Ni_ RX_ BASE_ ADDRESS:
Registrador que contém o endereço da interface de rede.
Packet_ base_ address:
Registrador que retorna o endereço em memória de pacote válido, ou seja, de pacote que já foi recebido e transferido para a memória de dados.
Ni_ RX_ END_ RECEPTION:
Registrador que informa o fim de recepção do pacote.
Para a implementação da rotina interrupt_ handler foram criadas quatro regiões em memória para armazenar os pacotes recebidos, endereçadas por PACKET_ BASE_ ADDRESS.
A rotina interrupt_ handler é executada no momento em que ocorre o salto para o endereço 0x40000024.
Depois de salvar o contexto de todos os registradores executa- se um laço (Figura 36), que busca o endereço de uma das quatro regiões criadas para armazenar o novo pacote.
Havendo área de memória disponível, pode- se iniciar a leitura dos dados da Ni (observar a linha 5), a partir de o endereço armazenado no registradort1.
Se não houver espaço para armazenar um novo pacote, a rotina desabilita a interrupção através da instrução da chamada à instrução mtc0 e depois restaura o contexto dos registradores.
A interrupção é posteriormente habilitada quando de o consumo de um pacote.
Em o momento em que é encontrado um local para armazenar o pacote, executa- se o código a partir de o rótulo read_ packet (Figura 37), ou seja, começa- se a receber os dados da Ni.
Depois de recebido todos os dados da Ni, é enviado um sinal de fim de recepção dos dados através de Ni_ RX_ END_ RECEPTION e restaurado o contexto dos registradores.
Em seguida depois da finalização da rotina de interrupção salta- se para o endereço armazenado no registrador EPC através da chamada à instrução mfc0.
Drivers de Comunicação Os drivers de comunicação são responsáveis por gerenciar a recepção e o envio de pacotes.
Foram criados dois drivers, um de transmissão de pacotes e outro de recepção de pacotes.
A estrutura do pacote compreende:
Primeiro flit -- endereço do Np destino;
Segundo flit -- tamanho do payload;
Demais flits -- payload.
Os objetivos dos drivers são montar e desmontar os pacotes, e o envio e o recebimento de pacotes.
Para que isso ocorra, são utilizado endereços de entrada/ saída mapeados em memória.
O driver de envio denominado como outcoming_ unit possui três registradores mapeados em memória.
Os endereços de saída mapeados em memória utilizados por o driver de envio compreendem:
Ni_ TX_ BASE_ ADDRESS:
Registrador que contém o endereço da outcoming unit;
Ni_ TX_ BUSY:
Registrador que indica se a Ni está ocupada;
Ni_ TX_ END_ PACKET:
Registrador que informa o fim de transmissão do pacote.
O código simplificado em linguagem de montagem para a transmissão de um pacote (driver de envio) de n flits é apresentado na Figura 39.
O código compreende em salvar o contexto, dois laços principais e recuperação do contexto:
Verificação se a Ni está livre, através da leitura do sinal busy, mapeado no endereço Ni_ TX_ BUSY;
Envio do pacote através da leitura da área de dados e escrita no buffer, no endereço Ni_ TX_ BASE_ ADDRESS.
A o final do envio do pacote avisase à Ni que há pacote disponível, escrevendo- se no endereço Ni_ TX_ END_ PACKET.
O driver de recepção denominado como incoming_ unit possui somente um registrador mapeado em memória.
O PACKET_ BASE_ ADDRESS é o registrador que retorna o endereço do pacote válido.
Este driver baseia- se basicamente na rotina bloqueante chamada get_ packet_ address.
O código em linguagem de montagem para a recepção de um pacote (driver de recepção) de n flits é apresentado na Figura 40.
Este código compreende dois laços principais:
Carrega o endereço do ponteiro do pacote e carrega o conteúdo do ponteiro, ou seja, o endereço do pacote mapeado no endereço PACKET_ BASE_ ADDRESS;
Restaura contexto.
Para auxiliar na validação do sistema foi criado um contador de ticks (ciclos de clock).
Os endereços mapeados em memória utilizados para computar o tempo transcorrido desde a inicialização do sistema incluem:
CONT_ TICK_ H:
Registrador que contém a parte alta de contagem dos ciclos de clock.
CONT_ TICK_ L:
Registrador que contém a parte baixa de contagem dos ciclos de clock.
Para o cálculo da latência, o driver de envio lê o tempo atual, armazenando- o no final do pacote.
O driver de recepção por sua vez subtrai o tempo atual do tempo armazenado no final do pacote recebido, computando assim a latência total do pacote.
As alterações ainda incluem a colocação de mais um pino de entrada (cont_ tick) para realizar a leitura dos ciclos de clock no elemento_ pe.
Vhdl e repassar os dados para a CPU através de data_ cpu.
Para verificar se os dados estavam sendo enviados ou recebidos corretamente por os drivers de envio e recepção, criou- se uma rotina para a impressão dos dados para um arquivo texto.
O código simplificado pode ser visualizado na Figura 41.
Para realizar esta impressão utilizou- se um registrador mapeado em memória, conforme descrito abaixo:
VET_ TEMP:
Contém os dados que serão impressos via debug.
Debug: Registrador que realiza o printf dos dados recebidos, para auxilio da validação do sistema.
A rotina consiste num loop principal que executa as seguintes tarefas salva contexto, carrega endereços dos registradores VET_ TEMP e Debug, faz a impressão dos dados a partir de o tamanho contido no header do pacote e restaura contexto.
Foram necessárias modificações na descrição do elemento_ pe.
Vhdl para inclusão do processo que gera o arquivo para debug dos dados.
Este processo lê os dados no endereço 0xFFFF1A08 e converte para serem escritos no arquivo de impressão.
Os resultados apresentados a seguir estão divididos em duas fases.
Uma fase onde se leva em consideração as primeiras implementações que tem como parte do desenvolvimento as duas interfaces de comunicação (transmissão e recepção), os dois drivers de comunicação (outcoming unit e incoming unit) e a NoC.
A segunda fase tem incluídos a implementação e o tratamento das interrupções.
Validação da Transmissão e Recepção de Pacotes Esta Seção apresenta os resultados obtidos na primeira fase do projeto, que tem como objetivo validar o envio e a recepção de pacotes.
Todas as simulações e validações foram realizadas em VHDL utilizando o Modelsim.
Para a validação, foram realizadas simulações que enviam pacotes com o tamanho de 32 flits.
O cenário a seguir demonstra a geração de um pacote de 32 flits, sendo os 2 primeiros flits o destino e o tamanho do pacote, seguidos de 28 flits de payload, e mais 2 flits com a contagem do tempo.
Este cenário abrange o envio e a recepção destes pacotes entre três processadores, sendo P1 e P3 os processadores de envio e P2 o processador de destino.
A Figura 42 apresenta os sinais gerados por o processador PE1 (pacote de dados) para a NI1.
Os dados gerados por o processador são armazenados no buffer da NI1, e uma vez o pacote armazenado no buffer, a NI1 o injeta na rede.
Os eventos gerados compreendem:
O processador PE1 gera o pacote com 28 flits;
É carregada a parte baixa do contador com valor 82 (52 H) em CONT_ TICK_ L;
Inicia- se o envio do pacote para o processador de destino por a rede;
O sinal busy permanece desabilitado, indicando que a Ni pode receber o pacote;
O pacote é injetado na rede.
A NI1 inicia a recepção do pacote, armazenando todos os flits no buffer da NI1;
O sinal end_ pack indica final de transmissão de pacote;
O sinal busy é habilitado, indicando que a Ni está ocupada;
O sinal credit_ i habilitado, indica que a NoC pode receber dados;
Processo de envio do pacote para a NoC.
A Figura 43 apresenta o gráfico dos tempos gerados por o envio do pacote por PE3, os sinais gerados por o processador e os sinais gerados por a NI3.
Os eventos gerados compreendem:
Pacote sendo gerado por o processador PE3 com tamanho de 28 flits;
O sinal busy está desabilitado indicando que a NI3 está pronta para receber dados.
Momento em que o pacote é enviado por o processador PE3 e injetado na NI3;
Quando injetado na Ni o pacote é armazenado no buffer (buf_ ni);
Depois de armazenado o pacote, o processador sinaliza a NI3 através do sinal end_ pack que enviou todo o pacote;
O sinal de tx está desabilitado indicando à NoC que não há dados para serem enviados.
Depois de sinalizado o final do pacote por end_ pack, a NI3 ativa tx, indicando que há pacote a ser enviado;
Como a NI3 está ocupada transmitindo o pacote, a NI3 habilita o sinal busy;
A NoC mantém o sinal credit_ i habilitado, informando a NI3 que pode receber dados;
O pacote é enviado para a NoC.
A recepção dos pacotes é realizada por o processador PE2 através da NI2.
A Figura 44 apresenta os sinais gerados por a NI2 e a recepção do pacote enviado por PE1.
Os eventos gerados compreendem:
Inicialmente credit_ o esta habilitado, permitindo assim o envio de pacotes da NoC para NI2.
A NI2 só está pronta para receber os dados assim que rx estiver habilitado.
Momento em que o pacote enviado por PE1 é recebido por a NI2 através de data_ in_ noc;
Momento em que o pacote é completamente armazenado no buffer da NI2;
Assim que o pacote é totalmente armazenado no buffer, a NI2 sinaliza ao processador PE2 que há pacotes a serem enviados através de ready_ pack;
O PE2 lê da NI2 o pacote armazenado no buffer;
Recepção do primeiro pacote por PE2;
Após recebimento de todo o pacote, PE2 sinaliza NI2 através de end_ receive que finalizou recebimento do primeiro pacote.
A Figura 45 ilustra os sinais gerados por a NI2 e a recepção do segundo pacote gerado por PE3.
Os eventos gerados compreendem:
A NI2 sinaliza à NoC que pode receber dados habilitando a porta credit_ o;
O sinal rx está habilitado, informando a NI2 que há dados a receber da NoC;
O pacote é inserido na Ni;
Momento em que o segundo pacote é completamente armazenado no buffer da Ni;
A NI2 sinaliza ao processador PE2 que há pacotes a serem lidos através de ready_ pack;
A NI2 inicia o envio do segundo pacote para o processador PE2 (destino);
Resultados de Latência Para a avaliação de latência foi utilizada uma configuração com 6 processadores gerando tráfego (pacotes de 32 flits) e 3 processadores recebendo este tráfego.
A Tabela 3 apresenta a distribuição espacial do tráfego.
Em a Figura 46 é apresentada as posições dos roteadores e dos núcleos de processamento na rede NoC.
Os resultados obtidos referem- se apenas às medidas de latência dos pacotes enviados por os processadores PE1 e PE3 ao processador PE2.
As medidas efetuadas levam em conta o tempo que cada instrução leva para ser executada.
Analisando o código assembly de transmissão podemos constatar o número de instruções executadas.
O laço para transmissão possui 7 instruções, como pode se visto na Figura 47.
Cada instrução leva 4 ciclos para ser executada, exceto a instrução lw que são 5 ciclos.
Assim este laço leva para executar (6* 4)+ (1* 5) $= 29 ciclos.
Como executamos 32 vezes este laço, temos 928 ciclos.
O laço de recepção tem por sua vez o mesmo número de instruções, mas executa só 30 vezes.
Logo, leva 870 ciclos.
A estes valores vamos adicionar pelo menos 15 instruções que são executadas antes e depois do laço, e assim temos aproximadamente:
A Figura 48 apresenta o início da geração do pacote por o PE1:
Primeiro flit gerado por o PE1, num total de 32 flits.
Tempo decorrido para gerar o início do pacote.
Início da injeção do pacote na rede.
Este pacote é injetado na rede com um tempo de 100 ciclos.
A Figura 49 mostra o instante em que o pacote é recebido por PE2:
Instante em que o primeiro pacote é recebido da NI2.
Tempo do instante em que o primeiro pacote está sendo recebido ­ tempo igual a 1042 ciclos.
Em a Figura 50 é apresentado o resultado da soma dos tempos em que o pacote leva para ser injetado na rede, mais o tempo que ele leva para receber o pacote:
Recepção do pacote.
Tempo total de recepção do pacote ­ tempo igual a 1900 ciclos.
Estes números conduzem à latência de 1800 ciclos de relógio para o primeiro pacote, de 32 flits.
Este valor de latência conduz a uma latência por flit igual a 56 ciclos de relógio, o que é um valor relativamente alto.
O segundo pacote deve esperar que todo o pacote 1 seja recebido.
Logo, a sua latência será o tempo de recepção do primeiro pacote, mais 858 ciclos, que é igual a 2758 ciclos de clock, que é aproximadamente o valor apresentado na Figura 51 ­ 2903 ciclos de clock, levando em conta que para iniciar a transmissão foi preciso gastar 100 ciclos de clock.
A latência do segundo pacote é igual a 2803 ciclos de clock, ou 88 ciclos de clock por flit.
Esta maior latência deve- se a colisão dos pacotes, como pode ser facilmente observado no diagrama da Figura 51.
A transmissão dos pacotes dos processadores PE1 e PE3 para o PE2 pode ser esquematizada temporalmente no diagrama da Figura 52.
Em este diagrama observa- se que os dois pacotes são simultaneamente inseridos nos buffers de suas interfaces de rede.
Devido a a arbitragem, um é transmitido primeiro, e na sequência o segundo é transmitido.
Validação da Interrupção Para efetuar a validação desta segunda fase foi utilizado um aplicação paralela do tipo merge sort (código disponível no Apêndice A) utilizando envio de pacotes com tamanhos que variam de 16 à 600 flits.
Para demonstrar a execução desta aplicação, foram utilizados cinco cenários, que têm por objetivo analisar o desempenho do MPSoC.
A aplicação merge sort engloba a divisão de um pacote armazenado em memória num tamanho que dependerá dos n processadores que serão utilizados nas simulações.
Estes pacotes serão enviados para n processadores de destino, os quais executarão a ordenação através de uma aplicação bubble sort (código disponível no Apêndice B) dos pacotes recebidos e retornam os pacotes ordenados para o processador de origem.
O processador de origem recebe os pacotes ordenados e aplica o merge para a ordenação final.
Este cenário compreende a execução da aplicação bubble sort num único processador.
O objetivo deste cenário é verificar os tempos gastos e utilizar- lo com métrica de comparação para os cenários que iremos analisar na seção de resultados.
Para esta verificação não foi utilizado em nenhum momento o acesso à NoC.
Para executar um bubble sort num pacote de 600 flits, foram consumidos 9.184.058 ciclos de clock.
O tempo total de execução foi praticamente o tempo de ordenação, desprezando assim o valor gasto para iniciar a rotina.
Este cenário compreende a execução da aplicação merge sort num único processador, através da divisão do vetor em duas partes.
Comparado o cenário 2 ao cenário 1, obtém a aceleração (speedup) por a otimização do algoritmo.
Para simplificar a análise de resultados analisamos somente a ordenação de um vetor (pacote) com 600 flits.
Em este cenário o vetor é dividido em dois outros vetores de tamanhos iguais.
A Figura 53 apresenta o gráfico de tempos após a simulação da aplicação merge sort executada no processador PE1.
1) O pacote armazenado em memória é dividido em duas partes, ou seja, dois pacotes de mesmo tamanho.
2) Ordenação da primeira parte do pacote.
3) Ordenação da segunda parte do pacote.
4) Depois de remontado os dois pacotes, é aplicado o merge para ordenação final.
Este cenário é semelhante ao cenário 2, execução da aplicação merge sort num único processador, porém divide- se o vetor em quatro partes.
A Figura 54 apresenta o gráfico de tempos após a simulação da aplicação merge sort executada no processador PE1.
1) Tarefa de ordenação dos dois primeiros pacotes.
2) Primeira tarefa de merge aplicada sobre os dois primeiros pacotes.
3) Tarefa de ordenação dos dois pacotes restantes.
4) Segunda tarefa de merge aplicada sobre os dois pacotes restantes.
A Figura 56 apresenta o gráfico de tempos após a simulação que compreende os três processadores, assim como as principais etapas durante o processo de envio dos pacotes.
Em esta figura podemos visualizar os pacotes sendo enviados para os processadores de destino PE2 e PE3.
Apesar de não visualizarmos o header de cada pacote, os endereços estão nos primeiros flits, assim como o tamanho do pacote.
Descreveremos agora as principais fases durante a transmissão dos dois vetores (pacotes) para os processadores de destino, que executarão o bubble sort dos dois vetores.
1) A NI1 sinaliza ao processador através do sinal busy que está desocupada e os dados podem ser enviados;
2) Os dados do primeiro pacote são enviados para NI1;
3) Os dados são recebidos por a NI1 e armazenados no buffer;
4) Após os dados serem enviados por o processador, o PE1 sinaliza através de end_ pack que finalizou a transmissão do primeiro pacote;
5) busy sinaliza ao processador que a Ni está ocupada, impedindo que o processador envie mais dados.
A partir deste ponto a Ni inicia o envio do pacote recebido;
6) Depois de armazenados no buffer os dados são injetados na rede;
7) Depois de enviado o primeiro pacote, busy sinaliza ao processador que a Ni está desocupada e pronta para receber um novo pacote;
8) Os dados do segundo pacote são enviados para Ni;
9) Os dados são recebidos por a Ni e armazenados no buffer;
A Figura 57 ilustra a forma como a interrupção atua no momento que é ativada por a Ni.
A sequência de eventos compreende:
1) Os dados são recebidos da NoC por a Ni e armazenados no buffer, como se trata da recepção de dados via rede, eles são armazenados em buff_ ni_ in;
2) A Ni sinaliza ao processador via interrupção que há dados a serem enviados;
3) Após ser sinalizado, o processador imediatamente copia o conteúdo do registrador PC pra o registrador EPC;
4) Dado que o sinail habilitar_ interrupção está ativo e o sinal em_ atendimento desativado, a máquina de controle vai para o estado INT.
6) Para sinalizar que o processador está executando a rotina de processamento é ativado o sinal em_ atendimento enquanto recebe os dados.
Depois de enviados os pacotes, eles são recebidos por os processadores que farão o sort dos flits desordenados.
Após ordenação eles são enviados para o processador de destino.
A Figura 58 apresenta a recepção do pacote logo após a requisição da interrupção através do sinal intr da Ni.
A requisição da interrupção fica ativa enquanto a Ni esta enviando dados para o processador.
Os eventos ocorridos para a recepção, ordenação e envio de um pacote são mais bem descritos a seguir:
1) O pacote é armazenado no buffer da Ni em buf_ ni_ in;
2) A Ni interrompe PE2 através do sinal intr;
3) PE2 atende a solicitação e habilita o sinal em_ atendimento;
4) Em data_ in_ ext, PE2 faz recepção do pacote;
5) Após finalização de recepção do pacote PE2 sinaliza a Ni, indicando fim de recepção;
6) O registradora1 é utilizado para indicar início e término da tarefa de ordenação;
7) PE2 verifica se a Ni está ocupada.
Envia pacote para Ni.
Armazena o pacote no buffer através de buf_ ni;
8) Depois de armazenado o pacote no buffer a Ni envia- o para a rede.
A Figura 59, análoga à Figura 58, ilustra a ordenação do vetor no processador PE3.
Para termos uma visão mais ampla do sistema podemos observar a Figura 60, a qual apresenta a aplicação merge sort executada em todos os processadores.
Em o envio dos pacotes verificamos que o Pacote 1 é injetado na rede para ser enviado ao processador PE2, somente após ter sido totalmente armazenado no buffer.
Com isso o Pacote 2 é recebido por PE3 somente após todo o Pacote 1 estar totalmente armazenado em PE2.
A sequência de eventos compreende:
1) Através de data_ out_ ext os pacotes são enviados para os processadores PE2 e PE3;
2) A Ni sinaliza o processador mediante o sinal de interrupção intr.
O processador habilita o sinal em_ atendimento para indicar que está atendendo a solicitação da Ni;
3) O pacote é armazenado no processador.
Após finalização da recepção do pacote, o processador sinaliza fim de recepção;
4) Duração da tarefa de ordenação aplicada aos pacotes recebidos;
5) O processador verifica se a Ni está ocupada e envia os pacotes ordenados através de data_ out_ ext;
6) A Ni sinaliza o processador PE1 mediante o sinal de interrupção intr.
O processador PE1 habilita o sinal em_ atendimento para indicar que está atendendo a solicitação da Ni;
7) Em data_ in_ ext ocorre a recepção dos pacotes;
8) Após os pacotes serem remontados é executada a rotina de merge sobre o pacote remontado, tendo como resultado a ordenação final.
Para realizar o próximo cenário foram incluídos mais dois processadores, PE8 e PE9.
Em esta simulação PE1 enviará quatro pacotes para os processadores executarem o bubble sort do pacote recebido e retornar para o processador de origem, neste caso o PE1.
A Figura 61 ilustra como os processadores estão distribuídos na rede.
Todas as fases durante a aplicação merge sort no sistema com 5 processadores (Figura 62) conectados a rede são descritas a seguir:
1) Injeção dos pacotes na rede.
Depois que cada pacote é enviado, envia- se um sinal de fim de transmissão do pacote, habilitando sinal de end_ pack da Ni;
2) Ativação da interrupção (intr).
Processador sinaliza que está atendendo à requisição habilitando em_ atendimento;
3) Recepção dos pacotes por os processadores;
4) Duração da tarefa de ordenação aplicada aos pacotes recebidos;
5) O processador verifica se a Ni está ocupada e envia os pacotes ordenados através de data_ out_ ext.
Pacotes 1 e 2;
6) O processador verifica se a Ni está ocupada e envia os pacotes ordenados através de data_ out_ ext.
Pacotes 3 e 4;
7) A Ni sinaliza o processador PE1 mediante o sinal de interrupção intr.
O processador PE1 habilita o sinal em_ atendimento para indicar que está atendendo a solicitação da Ni;
8) Em data_ in_ ext ocorre a recepção dos pacotes;
9) a) Primeiro merge executado sobre os pacotes 1 e 2.
B) Primeiro merge executado sobre os pacotes 3 e 4.
C) Ordenação final dos pacotes obtido através do merge final.
Resultados de Latência e Speedup Em esta avaliação de resultados temos como principais objetivos analisar os valores de latência e speedup dos cenários simulados durante a execução das aplicações merge sort e bubble sort.
Para todos os gráficos dos Cenários simulados, foram analisados os resultados com aplicação do merge sort e bubble sort em pacotes com 600 flits.
De acordo com o que vimos na seção anterior, a latência para executar uma ordenação de um pacote com 600 flits através de um algoritmo bubble sort foi de aproximadamente 9.184.058 ciclos de clock num sistema monoprocessado.
Para executar a mesma ordenação de um pacote de 600 flits, no mesmo processador tivemos um tempo de execução de 4.705.002 ciclos de clock (Cenário 2).
Em a Figura 63 são apresentados os tempos gastos para executar a tarefa de ordenação sobre cada pacote.
Podemos observar que dentro de o tempo de execução, o maior gasto foi com relação a a tarefa de bubble sort sobre os pacotes.
Convém destacar que neste cenário (Cenário 2) o processador apesar de estar incluído dentro de um MPSoC em momento algum ele faz acesso a rede ou se utiliza de interrupção.
Os tempos obtidos neste cenário serão utilizados em conjunto com o Cenário 4 para obtermos os valores de speedup.
Em a Figura 64 apresenta- se o tempo de ordenação para o Cenário 3 (sistema monoprocessado, com o vetor dividido em 4 partes iguais).
O tempo de execução para este cenário foi de 2.441.433 ciclos de clock.
A maior contribuição para a redução do tempo gasto deve- se à redução do tamanho de cada pacote, ou seja, em cada ¼ do pacote original.
Apesar de utilizarmos três chamadas ao merge, o tempo total entre o inicio do primeiro bubble sort e o segundo merge que é de 2.370.526 ciclos de clock, nos dá aproximadamente o tempo de execução.
Podemos destacar também o merge final que utilizou o mesmo tempo para ser executado em ambos os cenários.
Sendo o cenário 4 a versão multiprocessada doa Cenário 2, verificamos que o tempo para executar a ordenação dos pacotes são aproximadamente os mesmos.
Como estamos com os processadores agora conectados à rede, temos um grande tempo em que o processador PE1 fica aguardando que os dois processadores, que neste caso são os processadores PE2 e PE3, executem suas ordenações e enviem seus pacotes.
Em a Figura 65 podemos visualizar o tempo gasto por PE2 para executar o bubble sort no primeiro pacote e enviar- lo para PE1, que é o tempo que PE1 fica aguardando.
Podemos dizer que o impacto da comunicação da rede não teve influência direta na aplicação, porque se tomarmos o tempo de execução que foi de 2.428.641 ciclos de clock e os tempos de ordenação dos dois pacotes e o merge final com relação a os tempos que foram gastos com envio e recepção dos pacotes, temos aproximadamente 1,9% do tempo de execução.
A Tabela 4 apresenta os tempos de execução que foram obtidos através da simulação de ordenação de pacotes de diversos tamanhos.
Estes valores foram obtidos dos Cenários 2 e 4.
A Figura 66 apresenta os tempos de execução para uma aplicação merge sort ser executa num sistema com 1 processador e um outro sistema com 3 processadores.
O gráfico obtido refere- se aos valores contidos na Tabela 4.
Figura 66 ­ Gráfico de tempos de execução das aplicações merge sort e bubble sort em 1 CPU (Cenário 2) e 3 CPUs (Cenário 4).
A Figura 67 apresenta o gráfico de tempos do Cenário 5, que envolve um sistema que possui 5 processadores na rede.
Como o pacote é dividido em quatro pacotes assim como foi feito no cenário 3, os tempos de envio, ordenação e recepção são reduzidos.
O tempo de envio e recepção dos pacotes por PE1 leva o processador a ficar um tempo ocioso devido a o tempo que o processador PE2 leva para executar a ordenação que é de aproximadamente 574.818 ciclos de clock.
Logo após a recepção de todos os pacotes, há a primeira remontagem, que vem a ser a união dos pacotes 1 e 2 num único pacote, com o tempo de 9.009 ciclos de clock.
Depois de remontados os pacotes, é realizado o primeiro merge com um tempo total de 25.546 ciclos de clock.
Em a próxima etapa temos um tempo maior de 21.065 ciclos de clock.
Isso é devido a duas rotinas que são executadas, uma é a segunda remontagem sobre os pacotes 3 e 4 e a outra rotina é a montagem da parte alta do vetor para ser realizado o merge final.
Logo em seguida é executado o segundo merge sobre o segundo pacote que foi remontado.
Como os pacotes são de tamanhos iguais, obtivemos um tempo igual de 25.546 ciclos de clock.
Como não há mais pacotes a serem remontados, a próxima etapa a ser executada é a montagem da parte baixa do vetor.
Esta etapa leva 11.259 ciclos de clock.
Como última etapa temos o merge, que executa a ordenação final dos pacotes com tempo final de 50.896 ciclos de clock.
Somando os tempos gastos de envio e recepção dos quatro pacotes temos um tempo total de 44.930 ciclos de clock.
Analisando o tempo total de execução para efetuar a ordenação de todos os pacotes de aproximadamente 754.758 ciclos de clock, podemos verificar que tivemos quase 6% de tempo gasto de comunicação na rede.
Em a Tabela 5 apresentamos os valores obtidos em ciclos de clock para os tempos de execução para o Cenário 3, que utiliza um sistema monoprocessado e os valores obtidos para sistemas multiprocessado para 5 processadores (Cenário5).
A Figura 68 Figura 66 apresenta os tempos de execução para uma aplicação merge sort ser executa num sistema com 1 processador e um outro sistema com 5 processadores.
O gráfico obtido refere- se aos valores contidos na Tabela 5 Tabela 4.
Figura 68 -- Gráfico de tempos de execução das aplicações merge sort e bubble sort em 1 CPU (Cenário 3) e 5 CPUs (Cenário 5).
Para que possamos medir as vantagens de incluir num sistema vários núcleos de processamento, devemos utilizar métricas que mostrem os ganhos obtidos ao executar de forma paralela um algoritmo na arquitetura desenvolvida.
Os cenários que foram simulados abrangem essa questão de forma que foi executado o algoritmo de ordenação de forma sequencial e paralela.
Uma tarefa, teoricamente pode ser dividida em n tarefas com n processadores.
Podemos definir o ganho de desempenho como a razão entre o tempo de execução de forma sequencial num único processador e os n processadores.
Conforme, a métrica de speedup refere- se ao ganho de desempenho obtido por a razão entre o tempo de execução do melhor algoritmo sequencial e sua forma paralela.
Vale salientar que o speedup não deverá atingir um valor maior que o número de processadores utilizados.
Esta análise é definida como Lei de Amdahl.
Esta lei demonstra o fato de que o gargalo de desempenho de uma aplicação paralela é sua parte sequencial.
Por ela, à medida que a parte sequencial do programa aumenta o speedup consequentemente diminui.
Portanto para atingir um bom speedup, é necessário tornar a parte sequencial do programa a menor possível.
Ademais se um problema é constituído de partes paralelizáveis e sequenciais, é necessário fazer com que as partes maiores executem mais rápidas Os valores de speedup na utilização de 2 CPUs e 4 CPUs são apresentados na Tabela 6.
Em os valores obtidos para 2 processadores o ganho de desempenho foi praticamente 2, ou seja, o impacto da comunicação a da rotina merge foi muito baixo.
Já para 4 processadores, o speedup ficou abaixo de 4, devido a o tempo de execução das rotinas merge e da comunicação.
Os atuais projetos de hardware devem aliar as restrições clássicas de projeto, como alto desempenho e baixo consumo de energia, e um reduzido tempo para o produto chegar ao mercado.
Uma forma de atender a estas restrições conflitantes é através do reuso de módulos de propriedade intelectual, e em particular através do reuso de processadores, conduzindo assim aos MPSoCs.
Em os primeiros Capítulos foram abordados os principais conceitos relacionados à MPSoCs e NoCs.
Atualmente vários métodos vêm sendo desenvolvidos para dar mais poder de processamento aos processadores.
O principal de eles é colocar vários núcleos de processamento numa única pastilha de silício.
Em 1997 os projetistas da Intel já prediziam que em 2014 teríamos processadores com 8 núcleos, em 2009 isso já é quase uma realidade.
Claro que aqui estão sendo considerados processadores para usuários domésticos.
Porém se pensarmos que as telecomunicações, os produtos eletrodomésticos, internet, HDTV se utilizam cada vez mais dos recursos de processamento como, por exemplo, acesso a banda larga, onde temos acesso a 100 Mbps e que em alguns países já é possível assistir via internet filmes em alta definição, para que isso seja possível é necessário um grande poder de processamento.
Esse contexto comprova o que foi mencionado no Capítulo 1, o grande crescimento da utilização de processadores na indústria, seja ela, de produtos eletrodomésticos, automobilística, médica, telecomunicações, etc..
Todas de certa forma vão ter em futuro próximo a interligação de seus produtos.
O mecanismo com que os componentes dos sistemas vinham se comunicando não supre mais as necessidades atuais, de alta largura de banda e escalabilidade.
Dada as limitações dos barramentos, a interconexão nos MPSoCs está adotando NoCs.
O Capítulo 2 apresento diversos projetos de NoCs, e o que mais contribui para o desenvolvimento deste trabalho foi a rede Hermes.
As contribuições deste trabalho compreendem:
1) Desenvolvimento de um MPSoC homogêneo, com reuso dos módulos de processamento e NoC;
2) Desenvolvimento da interface de rede, tendo por característica o uso de buffers para injecção/ recepeção de dados.
Esta característica permite uma simples integração de IPs à rede;
3) Desenvolvimento das camadas de software básico (drivers) para a realização da comunicação;
4) Modificação do processador MR4, com a inclusão do mecanismo de interrupção.
5) Avaliação do desempenho do MPSoC proposto utilizando uma aplicação paralela.
A partir de os resultados obtidos através das simulações e das análises realizadas julga- se que os objetivos foram atingidos, ou seja, o desenvolvimento do MPSoc homogêneo, assim como a revisão bibliográfica nos temas abordados.
Como principais sugestões de trabalhos futuros podemos citar:
Integração de um módulo DMA e execução de simulações com um número maior de processadores para verificar o ganho de desempenho do sistema;
Inserir outro modelo de processador para analisar qual o impacto na mudança de arquitetura no sistema;
Pesquisar novos métodos para integrar num mesmo ambiente a integração da NoC Hermes com &quot;bibliotecas «de IPs diferentes, que venham a possibilitar a redução de tempo de integração de núcleos IPs e aumentar o tempo de análise de resultados.
