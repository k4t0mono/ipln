Tradicionalmente, a avaliação de IHC é aplicada nas fases finais do processo de desenvolvimento de um sistema interativo, quando existe, pelo menos, um protótipo do mesmo.
Executar esta avaliação antecipadamente ­ ainda na etapa de projeto ­ pode diminuir os custos com reparos de eventuais problemas.
Normalmente, este tipo de avaliação é feito sobre modelos e cenários de uso e não sobre um protótipo, e geralmente baseado em métodos formais e automáticos.
A pesquisa aqui apresentada trata de avaliação de IHC baseada em modelos propondo um método informal e não automático:
Diretrizes para Avaliação de IHC baseada em Modelos.
A idéia deste trabalho é não só antecipar a avaliação para a etapa de projeto, mas também apresentar um método de fácil uso e baixo custo, que pode ser utilizado tanto por profissionais de IHC, quanto por desenvolvedores com pouca experiência na área.
Palavras-Chave: IHC, modelos, avaliação de usabilidade, avaliação baseada em modelos.
Devido a a popularização dos computadores pessoais e a crescente diversidade de fins para os quais estes computadores são utilizados, torna- se cada vez mais necessário que os sistemas computacionais &quot;falem «a língua do usuário.
Considera- se que um sistema &quot;fala «a língua do usuário quando este possui interfaces com qualidade, permitindo uma boa interação entre o usuário e o sistema interativo em questão.
E a área de Interação Humano-- Computador (IHC) é a responsável por o estudo desta, como o próprio nome já diz, interação entre o ser humano e o computador.
Esta área tem por objetivo principal fornecer, aos pesquisadores e desenvolvedores de sistemas, explicações e previsões para fenômenos de interação usuário-sistema e resultados práticos para o projeto da interface com usuário.
A o projetar uma interface, o designer não deve presumir que todos os usuários são como ele próprio, assim como não deve presumir que apenas seguir uma série de diretrizes de usabilidade durante o projeto será a forma de garantir a qualidade de uso do sistema interativo em desenvolvimento.
Deve- se pensar que alguém irá avaliar a qualidade de uso do sistema, nem que seja apenas o usuário final do mesmo.
Segundo Preece, a avaliação de IHC é um processo de coleta de dados responsável por informar o modo como um determinado usuário ou grupo de usuários deve utilizar um produto para uma determinada tarefa num certo tipo de ambiente.
Seus principais objetivos são:
Para chegar a estes objetivos, existem vários métodos de avaliação a serem utilizados, os quais podem ser classificados ­ em termos gerais ­ como métodos de avaliação formativa e métodos de avaliação somativa.
A avaliação formativa é executada durante todo o processo de design, usando, como base para a avaliação, artefatos como cenários, modelos de tarefas, modelos de interação e storyboards, por exemplo.
Já a avaliação somativa é executada na etapa final do processo de desenvolvimento, para avaliar o sucesso de um sistema ou testar a sua conformidade com determinados padrões previamente estabelecidos.
De entre estes, os métodos mais utilizados são os métodos de avaliação somativa, pois, em sua maioria, são de fácil entendimento e aplicação, o que se deve também ao fato de serem aplicados quando já existe, pelo menos, um protótipo do produto a ser avaliado, tornando a avaliação mais palpável.
No entanto, apesar de ser relativamente bem utilizado, o fato deste tipo de avaliação ser aplicado apenas na fase final de desenvolvimento do sistema pode provocar um aumento dos custos com reparos de eventuais falhas.
Em este sentido, quanto mais cedo forem identificados os problemas de interação e/ ou interface, menor será o custo para reparar- los.
Assim, com a intenção de avaliar a usabilidade de um sistema ainda em tempo de projeto, propõe- se executar esta avaliação a partir de modelos.
A premissa básica deste trabalho é:
Se os modelos servem como base para a construção do sistema, avaliando estes modelos, estaria- se- avaliando o sistema antecipadamente.
Desta forma, o presente trabalho concentra- se, principalmente, em «prever a usabilidade do produto ou de algum aspecto de ele, focando na antecipação da identificação de problemas de usabilidade, levando a avaliação para a etapa de projeto, quando se possui apenas modelos e/ ou diagramas do sistema a ser desenvolvido, e não um protótipo funcional do mesmo.
Com o intuito de atingir este objetivo, de levar a avaliação para a etapa de projeto, foram investigados métodos de avaliação de IHC informais e empíricos, a fim de se encontrar um método que melhor se adequasse ao problema em questão.
Os métodos informais ­ ou por inspeção ­ são aqueles baseados em diretrizes e/ ou na experiência dos avaliadores, tais como a avaliação heurística e a verificação de diretrizes, por exemplo.
Já os métodos empíricos caracterizam- se por utilizarem usuários reais na avaliação do sistema interativo, podendo variar desde observações e entrevistas com usuários até testes do sistema com a participação de usuários executando determinadas tarefas, como, por exemplo, testes de usabilidade e comunicabilidade.
Estes métodos foram escolhidos justamente por serem métodos manuais, ou seja, métodos que dependem da intervenção humana, considerando as questões subjetivas inerentes a este tipo de avaliação.
A partir de os estudos realizados ­ e que serão posteriormente descritos neste trabalho ­ decidiu- se por propor um método informal de avaliação em IHC, mais especificamente listas de verificação de diretrizes, para uso sobre modelos tanto de IHC quanto de Engenharia de Software (ES).
A idéia central deste método é que o avaliador possa utilizar essas diretrizes para avaliar um sistema interativo ainda em sua fase de modelagem.
Em este âmbito, o capítulo 2 apresenta trabalhos relacionados ao uso de modelos em IHC, tanto para projeto quanto para avaliação, que foram utilizados para fundamentar a opção por utilização de métodos informais de avaliação sobre modelos de ES e IHC.
O capítulo 3 apresenta alguns estudos preliminares, que foram realizados como base prática para a construção do método de avaliação proposto.
O capítulo 4 apresenta o método proposto bem como sua aplicação e análise dos resultados obtidos.
E, por fim, o capítulo 5 apresenta as considerações finais sobre o trabalho e possibilidades de continuidade da pesquisa realizada.
Atualmente, uma forma reconhecida para o desenvolvimento de software é o uso do processo unificado (Rational Unified Process -- RUP), que, segundo Jacobson, envolve atividades como levantamento e análise de requisitos, projeto (modelagem), codificação, teste e implantação.
E, para a atividade de modelagem, no desenvolvimento orientado a objetos é utilizada como linguagem padrão a Unified Modeling Language (UML), que em sua versão 2.0, possui 13 diagramas.
O objetivo de se ter tantos diagramas é permitir visões de diferentes ângulos do sistema a ser desenvolvido, analisando- o e modelando- o sob diversos aspectos, permitindo, assim, que os diagramas se completem.
Segundo Campos &amp; Ribeiro, a UML consolidou- se como um padrão no que diz respeito ao desenvolvimento de software orientado a objetos, e segundo o mesmo autor, a orientação a objetos é uma abordagem interessante no que se refere ao desenvolvimento de sistemas interativos.
No entanto, tanto a UML enquanto linguagem de modelagem, como o RUP enquanto processo de desenvolvimento, são reconhecidamente deficientes no suporte ao desenvolvimento de sistemas interativos.
Assim, para suprir esta carência, costuma- se modelar a interação do usuário com o sistema através de outro conjunto de modelos e diagramas, aqui denominados de modelos de De o ponto de vista da IHC, normalmente projetam- se telas e elementos de interface com base em princípios genéricos e diretrizes empíricas, geralmente propostas por os fabricantes de sistemas operacionais em que o sistema é implementado.
Mas só isto não resolve todos os problemas, é necessário projetar (modelar) também a parte interativa do sistema e não apenas sua interface.
Entre os modelos de IHC mais difundidos, encontram- se os modelos de tarefa e os de interação.
A modelagem de tarefas tem como objetivo identificar as tarefas a serem executadas por o usuário num sistema e existem várias técnicas que podem ser utilizadas para apoiar- la, como, por exemplo, Goals, Operators, Methods and Selection Rules (GOMS), Concur Task Trees (CTT) e Hierarchical Task Analysis (HTA).
Já na modelagem de interação se pode projetar como estas tarefas serão realizadas no sistema, ou seja, como o usuário interagirá com o sistema para executar- las.
Para esta fase existem também diferentes técnicas, tais como, Task--Action Grammar (TAG), User-Action Notation (UAN) e Modeling Language for Interaction as Conversation (MOLIC).
Em este âmbito, a fim de discutir estes esforços de modelagem, têm surgido diversas propostas de extensão da UML a fim de adaptar- la ao desenvolvimento de sistemas interativos.
Algumas propõem a inclusão de novos diagramas na linguagem UML, como Paternò, já outras propõem a extensão da UML a fim de permitir a representação de aspectos relacionados a sistemas interativos, como da Silva&amp; Paton.
Outras iniciativas propõem processos de desenvolvimento que integrem modelos de tarefas e de sistema num mesmo framework formal e notações dedicadas a modelagem de navegação Web e como estas afetam o processo de design (Winckler et al, Algumas destas iniciativas serão apresentadas neste capítulo, e discutidas quando de a apresentação da proposta deste trabalho.
Segundo Paternò, a UML é a abordagem baseada em modelos mais bem sucedida para apoiar o desenvolvimento de software.
No entanto, durante a evolução da UML pouca atenção foi direcionada para apoiar o projeto e desenvolvimento de interfaces com usuário.
Ao mesmo tempo em que interfaces com usuário tornaram- se parte essencial na maioria dos projetos de software, o uso de modelos para capturar requisitos e expressar soluções de projeto tornou- se uma necessidade.
Em este trabalho, Paternò propõe a integração de um modelo de tarefas da área de IHC) à UML a fim de chegar a uma UML para sistemas interativos, discutindo três abordagens para isto.
A primeira abordagem consiste em representar elementos e operadores de um modelo CTT numa notação UML já existente, o Diagrama de Classes.
A segunda consiste em desenvolver conversores automáticos a partir de a UML para Modelos de Tarefas, para a qual ele cita uma proposta existente sobre utilizar informações existentes em diagramas da UML ­ casos de uso, diagramas de casos de uso ­ para desenvolver modelos de tarefas.
E, na terceira, o autor discute a criação de uma nova UML para sistemas interativos, onde haveria a inserção do CTT no conjunto de diagramas da UML, criando um mapeamento semântico dos conceitos do CTT para o meta-modelo da UML.
Isto englobaria identificar correspondências, tanto em nível conceitual quanto estrutural, entre elementos e conceitos do CTT e da UML, explorando mecanismos de extensão da UML que apóiam esta solução.
Esta terceira é a abordagem escolhida e explorada por Paternò em Paternò.
Em da Silva&amp; Paton é apresentada uma proposta de extensão da UML para a representação da interface com usuário, a UMLi.
Segundo o autor, embora a interface com usuário represente uma parte essencial dos sistemas computacionais, a UML parece ter sido desenvolvida com pouca atenção no que diz respeito a ela.
Em esta proposta, as tarefas são modeladas utilizando uma extensão dos Diagramas de Atividades ao invés de incorporar uma notação totalmente nova na UML, como pode ser visto na Figura 1, que apresenta um exemplo de um Diagrama de Atividades que modela o comportamento de uma interface, extraído de da Silva&amp; Paton.
O autor acredita que a inserção de notações de uma comunidade ­ de IHC ­ no contexto de outra ­ ES ­ pode facilmente levar a propostas complexas e deselegantes as quais compartilham capacidades sobrepostas e levam os usuários a desafios, tais como, quando e como utilizar diferentes notações.
Furtado &amp; Soares propõem uma adaptação do RUP para que este contemple aspectos de IHC integrados aos seus principais workflows, gerando o RUPi.
Segundo Furtado &amp; Soares, conceitos de IHC como fatores humanos, diretrizes, interfaces para todos e geração de interfaces com usuários a partir de modelos são capazes de contribuir para a construção de sistemas usáveis quando aplicados num processo de desenvolvimento de software.
No entanto, esta abordagem também sugere a inserção de novos artefatos durante o processo de desenvolvimento, não necessariamente novos diagramas, mas a adoção de melhores práticas no RUP, como, por exemplo, alguns novos workflows:
Segundo Campos &amp; Harrison, a necessidade de uma verificação de design antecipada aumenta conforme o sistema vai tornando- se mais complexo.
O desafio então, está em construir sistemas &quot;corretos por design».
Segundo este autor, o que se pode fazer é confirmar formalmente que a especificação possui determinados requisitos.
Para tentar superar este desafio, uma das possibilidades é levar a verificação para o início do processo de design, a fim de antecipar a avaliação, prevenindo possíveis problemas.
Trabalhos com verificação formal têm se preocupado com duas questões:
A verificação da implementação contra sua especificação e, em particular no contexto de sistemas concorrentes, que certas propriedades do sistema se cumpram (que o sistema não possua deadlock, que os axiomas sejam consistentes e assim por diante).
Em este âmbito, alguns autores defendem o uso de métodos de avaliação automática, os quais consistem em verificar de forma automática, e não manual, a usabilidade de um sistema, utilizando métodos formais.
Um exemplo de avaliação formal sobre modelos é o Model Checking, o qual foi proposto inicialmente como uma alternativa para o uso de Theorem Proving.
A premissa básica implica que uma máquina de estados finita possa ser objeto de exaustivas análises por todo o espaço de estados do sistema para determinar quais propriedades irão predominar.
Além disso, existem pesquisas com a intenção de expandir este tipo de avaliação sobre diferentes modelos em diferentes etapas do desenvolvimento de um website, o que é apresentado na próxima seção.
Em Xiong, é descrito um estudo sobre como melhorar a avaliação com base na inspeção ­ automática -- de diretrizes através do ciclo de vida de aplicações web, mapeando conceitos de diretrizes para diferentes artefatos durante o processo de desenvolvimento.
Em este âmbito, uma das vantagens de se empregar ferramentas automáticas para verificação de diretrizes é que não é necessário um amplo conhecimento em ergonomia, uma vez que a maioria dos aspectos técnicos estão nas ferramentas.
Em este trabalho, os autores apresentam uma visão mais ampla para inspeção automática de diretrizes, verificando vários artefatos produzidos em diferentes etapas ao longo de o ciclo de vida de aplicações Web.
Segundo Nielsen &amp; Mack, quanto antes os problemas de usabilidade forem identificados, menor será o custo para regularizar- los.
Porém, tradicionalmente, a maioria dos métodos de avaliação em IHC é utilizada após ter- se uma aplicação pronta, ou em sua fase final de desenvolvimento, o que acarreta um retrabalho e, conseqüentemente, um aumento dos custos com a reparação, onerando mais tempo da equipe e do processo de desenvolvimento como um todo.
Assim, com a intenção de antecipar esta identificação de problemas de usabilidade, a idéia deste trabalho é realizar a avaliação da interface/ interação ainda na etapa de projeto, utilizando, para isto, tanto o projeto do sistema (uso de modelos da área de ES) quanto o projeto da interação dos usuários com o sistema (uso de modelos da área de IHC).
Em relação a os modelos utilizados na área de ES, e sua relação com a construção de sistemas interativos, no capítulo anterior foi apresentado o resultado de uma pesquisa bibliográfica sobre desenvolvimento destes sistemas utilizando modelos.
Com este estudo notou- se que existem abordagens que propõem a inclusão de questões relativas a interação do usuário através de extensão de diagramas da UML, da inclusão de novos modelos na UML, e da adoção de novos workflows para o adequar o RUP ao desenvolvimento de sistemas interativos, por exemplo.
Porém, segundo Campos &amp; Ribeiro, a adoção destas propostas por parte de a comunidade tem- se mostrado lenta.
Este fato deve- se, por um lado, à natural resistência a face a o potencial de confusão que eles causam (por se tratar de novas extensões à notação).
No que diz respeito à avaliação de IHC baseada em modelos, notou- se que existem algumas propostas como a utilização de modelos e métodos formais para realizar esta avaliação, os quais tendem a avaliação automática.
A avaliação automática provê alguns benefícios, principalmente no que diz respeito a dados quantitativos (por exemplo, a partir de modelos navegacionais, apresentar o número de passos que o usuário precisaria dar para efetuar determinada tarefa/ ação), porém, um de seus pontos fracos é que ela não consegue capturar dados subjetivos da interação (por exemplo, saber se as mensagens fornecidas para os usuários estão corretas ou são úteis para eles).
Em este âmbito, Campos &amp; Harrison ressaltam a subjetividade inerente à avaliação.
Os autores dizem que é impossível provar que um projeto está correto/ exato porque corretude/ exatidão é um conceito relativo, subjetivo.
Segundo estes autores, o que se pode fazer é verificar formalmente se a especificação possui algumas propriedades solicitadas, seja através da verificação das implementações em relação a os requisitos ou através da verificação de que certas propriedades do sistema funcionam.
Porém, para sistemas interativos, os autores ressaltam que o lado humano também deve ser verificado:
Além de o comportamento do software, existe o comportamento ­ flexível, adaptável e não-determinístico ­ dos usuários.
E este tipo de comportamento não tem como ser mensurado e nem ­ automaticamente ­ capturado.
Assim, a fim de verificar como tratar a questão da subjetividade nas avaliações realizadas nas fases iniciais de projeto, foram realizados dois estudos preliminares, com a utilização de métodos manuais de avaliação de usabilidade sobre modelos de ES e de IHC.
Foram escolhidos métodos manuais por estes considerarem a questão da subjetividade, quando trazem ­ como componente dos métodos ­ a experiência do especialista em IHC e/ ou a experiência de uso do usuário real, por exemplo.
Em esta seção serão apresentados o sistema utilizado como base da modelagem utilizada bem como os métodos de avaliação e os modelos utilizados nos testes, e a justificativa para utilização de cada um de eles.
Sistema Utilizado Para estes estudos foi utilizada a modelagem (UML) de um sistema de Livraria Digital disponível em Guedes, e também construídos modelos de IHC para o mesmo sistema.
Este sistema permite a pesquisa de livros por título, autor ou ISBN, a visualização de detalhes de um livro e a inclusão do mesmo num carrinho de compras.
Além disso, o sistema permite que o cliente altere a quantidade ou exclua algum livro do carrinho e conclua seu pedido, selecionando a forma de pagamento e informando o local de entrega do pedido.
Por fim, o sistema permite que o usuário verifique a situação de seus pedidos.
Exceto para a funcionalidade de pesquisar livros, é necessário que o usuário seja identificado como usuário do sistema.
Para isso, é preciso que este efetue um cadastro, informando seus dados pessoais e definindo um login e uma senha.
As próximas seções apresentarão os métodos e modelos utilizados nestes testes bem como o detalhamento dos mesmos.
Métodos de Avaliação Utilizados Conforme citado anteriormente, neste trabalho foram utilizados métodos manuais de avaliação de IHC, especificamente métodos informais e empíricos.
De os métodos informais, foram selecionadas a avaliação heurística e a verificação de diretrizes e, dos métodos empíricos, os testes de comunicabilidade.
A avaliação heurística é um método de avaliação informal que foi desenvolvido como uma alternativa para, em pouco tempo e com baixo custo, encontrar- se problemas de usabilidade.
De três a cinco avaliadores examinam a interface individualmente, percorrendo a mesma no mínimo duas vezes, uma para ter uma visão geral e outra para examinar cada elemento de acordo com as dez heurísticas propostas por 1, e geram relatórios individuais.
Pode haver ainda a participação de um especialista no domínio para esclarecer eventuais dúvidas dos avaliadores.
Após as avaliações individuais, os avaliadores reúnem- se e geram um único relatório contendo os problemas encontrados para cada elemento da interface, relacionados à heurística que foi violada e a seu grau de severidade.
Já a avaliação por verificação de diretrizes ­ ou avaliação por checklists ­ é um método que consiste em vistorias baseadas em listas de verificação, através de as quais o avaliador examina aspectos de usabilidade de uma interface, verificando a conformidade dos mesmos com um conjunto de diretrizes (guidelines), dispostas na forma de uma lista de verificação (checklist).
Estas diretrizes podem variar desde prescrições altamente específicas, De entre as dez heurísticas propostas por Nielsen estão &quot;Visibilidade do Estado do Sistema», Correspondência entre o Sistema e o Mundo Real &quot;e «Consistência e Padronização», por exemplo.
É trabalho do avaliador afirmar ou discordar dos itens encontrados na lista.
Em este tipo de avaliação, ao contrário de a avaliação heurística, é a qualidade das ferramentas de avaliação, no caso, das diretrizes, e não a qualidade ­ experiência ­ dos avaliadores, que determinam o sucesso da avaliação.
Para estes experimentos, a avaliação heurística foi escolhida em virtude de ser um método bastante popular entre os profissionais da área de IHC, por ser relativamente fácil de se utilizar e também por ser mais abrangente e/ ou subjetiva que a verificação de diretrizes, possibilitando uma maior amplitude de resultados encontrados.
E a verificação de diretrizes foi escolhida por ser um método de avaliação rápido e direto, o qual não depende tanto da experiência (em IHC) do avaliador.
Em relação a os métodos empíricos, foi utilizado o teste de comunicabilidade, que têm como objetivo avaliar a interface em relação a a sua propriedade de comunicabilidade (propriedade de transmitir ao usuário, de forma eficaz e eficiente, as intenções e princípios de interação que guiaram o seu design).
Para isto, este método simula a comunicação do usuário para o designer, o que é feito através da análise e etiquetagem das ações do usuário utilizando- se um conjunto de interjeições que o usuário potencialmente utilizaria para se expressar numa situação onde acontece uma ruptura na sua comunicação com o sistema.
Um exemplo destas interjeições é o &quot;Cadê?»,
que é utilizado quando o usuário sabe a operação que deseja executar mas não a encontra de imediato na interface.
Optou- se por utilizar este método visto seu objetivo ser avaliar a interface com relação a a qualidade da comunicação do designer para os usuários ao passo que os testes de usabilidade visam quantificar o desempenho do usuário, o que envolve a medição do tempo e das ações dos usuários, por exemplo, o que seria difícil de mensurar com sua aplicação sobre os modelos de projeto.
Modelos Utilizados Segundo Campos, em ES, normalmente são utilizados modelos white box ­ modelos que priorizam como o sistema será desenvolvido e não como este aparecerá para o usuário ­ direcionados à implementação, que são os modelos que, de fato, descrevem como o sistema será codificado.
Por outro lado, os modelos de IHC normalmente são modelos conceituais que fornecem uma visão black box do sistema, não representando como o sistema funciona, mas, sim, como ele será apresentado ao usuário.
As próximas seções apresentarão os modelos de ES, ou modelos UML, mais especificamente, e os modelos de IHC utilizados nestes estudos.
Em este trabalho foi utilizado o Diagrama de Casos de Uso (Figura 2), que é o diagrama mais geral e informal da UML, normalmente utilizado nas etapas de levantamento e análise de requisitos do sistema.
Ele apresenta uma linguagem de fácil compreensão para que os usuários possam ter uma idéia geral de como o sistema irá se comportar.
Este diagrama concentra- se em dois itens principais:
Atores e Casos de Uso.
Os Atores representam os papéis desempenhados por os diversos usuários que poderão, de alguma maneira, vir a utilizar os serviços, tarefas ou funções do sistema, estes representados por os Casos de Uso.
Além de o Diagrama de Casos de Uso, foram utilizados também os Diagramas de Atividades, exemplificado na Figura 3 e de Seqüência, que, segundo Jacobson, apresentam uma visão mais dinâmica do sistema, enquanto os Diagramas de Casos de Uso apresentam uma visão mais estática e estrutural do mesmo.
O Diagrama de Atividades preocupa- se em descrever os passos a serem percorridos para a conclusão de uma atividade específica, concentrando- se no fluxo de controle de uma atividade.
Já o Diagrama de Seqüência, como o próprio nome já diz, representa a seqüência (ordem temporal) em que os objetos envolvidos num determinado processo trocam mensagens.
Um Diagrama de Seqüência, conforme apresentado na Figura 4 identifica o evento gerador do processo modelado e o ator responsável por este evento, e determina como o processo deve- se desenrolar e ser concluído por meio de chamadas de métodos, disparados por mensagens enviadas entre os objetos.
Assim, para os estudos aqui relatados, o Diagrama de Casos de Uso foi escolhido por apresentar uma visão mais estrutural do sistema, assim como os modelos de tarefas em IHC (que serão vistos na seqüência).
Já os Diagramas de Atividades e Seqüência foram escolhidos por apresentarem uma visão mais dinâmica, através de a qual pode ser possível identificar, de alguma forma, a interação entre usuário (ator) e o sistema modelado.
Em este trabalho foi utilizado um diagrama hierárquico que apresenta uma visão macro das metas que cada classe de usuários pode realizar (Figura 5).
E, para cada meta prevista, foi elaborado um modelo de tarefas, utilizando- se a adaptação do HTA proposta em de Paula, a qual representa a decomposição hierárquica de funções em seus componentes funcionais, indicando algumas relações temporais entre elas, conforme a Figura 6.
Em este modelo (Figura 6), por exemplo, para se atingir a meta &quot;Manipular carrinho «o usuário pode ou &quot;Alterar quantidade «ou &quot;Concluir pedido «ou &quot;Excluir item».
E, para executar a tarefa &quot;Concluir pedido», o usuário deve primeiro &quot;Selecionar forma de pagamento», depois &quot;Informar local de entrega «e em seguida &quot;Confirmar pedido».
Após a modelagem diagramática das tarefas associa- se às tarefas os signos2 correspondentes (o detalhamento da modelagem utilizada pode ser visto no Apêndice A).
Apesar de o modelo de tarefas descrever a estrutura das tarefas que serão apoiadas por o sistema, ele não é suficiente para dar forma à solução computacional tal como esta será percebida por o usuário.
Em este trabalho, além de a modelagem de tarefas, foi utilizada a MOLIC, que é um modelo para representação da interação como um conjunto de conversas que os usuários podem travar com o porta-voz do designer3, sem especificar ainda a interface propriamente dita.
Os elementos básicos da MOLIC são as cenas, os processos do sistema e as transições, conforme apresentado na Figura 7 Em este modelo (Figura 7), a expressão &quot;Solicitar inscrição», por exemplo, representa o tópico (assunto) da conversa entre o usuário e o porta-voz do designer.
A expressão&quot;[ fornecer dadosconfirmar] «representa uma fala do usuário e p:
Erro ­ informações inválidas ou falta de informações obrigatórias «Um signo é algo que representa alguma coisa para alguém;
Assim um signo da interface é tudo aquilo que tem um significado para alguém, no caso designer, usuário ou avaliador.
A MOLIC é baseada na Engenharia Semiótica onde o designer é representado por um portavoz do designer.
Representa uma fala do porta-voz do designer.
Já o quadrilátero preto representa um processo do sistema que origina a fala do porta-voz do designer, a qual muda o rumo da conversa.
Além de o diagrama, ainda deve ser feita sua especificação textual, a qual deve fornecer detalhes de cada conversa (Apêndice D).
A utilização da MOLIC, apesar de este ainda não ser um diagrama muito difundido, deve- se ao fato de acreditar- se que este é o diagrama que melhor representa a interação sistema-usuário, quando a trata como uma conversa entre o usuário e o sistema, princípio básico do conceito de interação.
Por conseqüência da escolha da MOLIC, foram utilizados o Diagrama de Metas e os Modelos de Tarefas, por estes serem utilizado como base para o desenvolvimento da MOLIC.
Considerando- se a necessidade de avaliação de sistemas interativos antes que os mesmos sejam concluídos e as considerações já feitas sobre modelos e métodos de avaliação sobre eles existentes, partiu- se para um primeiro experimento, em o qual foram utilizados diferentes métodos de avaliação de IHC sobre diferentes modelos, tanto de IHC quanto de ES.
Primeiro Experimento:
Métodos utilizados Em a avaliação heurística, os participantes deveriam percorrer os diagramas e modelos em busca de problemas de usabilidade baseando- se nas dez heurísticas propostas por Nielsen.
Como o objetivo do experimento era inicialmente verificar a aplicabilidade dos diferentes métodos de avaliação, não se achou necessário a consolidação dos resultados, analisando- se apenas as avaliações individuais.
Em a avaliação por verificação de diretrizes utilizou- se uma lista de verificação voltada para sistemas web, dado que o sistema modelado era de uma Livraria Digital.
Em esta avaliação, os participantes deveriam confrontar as informações apresentadas nos modelos com os itens existentes na lista de verificação.
E, nos testes de comunicabilidade, os participantes deveriam &quot;utilizar o sistema «enquanto eram observados por um avaliador.
Primeiro Experimento:
Modelos utilizados Em este experimento, foram avaliados tanto diagramas da UML quanto modelos de IHC a fim de verificar que tipos de informações conseguiam- se capturar com cada tipo de modelo.
De a modelagem UML optou- se por avaliar o Diagrama de Casos de Uso, o Diagrama de Atividades e o Diagrama de Seqüência.
Quanto a a modelagem de IHC, optou- se por trabalhar com o Diagrama de Metas, com os Modelos de Tarefas e com a MOLIC.
Primeiro Experimento:
Participantes Para escolha dos participantes do experimento foi delineado o seguinte perfil:
O usuário deveria ter conhecimento sobre modelagem UML, sobre modelagem de IHC e sobre avaliação de IHC.
Em este âmbito, foram selecionados doze usuários, os quais foram subdivididos em diferentes grupos:
Quatro usuários realizaram a avaliação heurística;
Quatro realizaram a avaliação por verificação de diretrizes e quatro participaram como usuários num teste de comunicabilidade sobre modelos.
Para não comprometer os resultados dos testes, metade da amostra avaliou primeiro os diagramas da UML e posteriormente, os modelos de IHC, e a outra metade o contrário, ou seja, primeiro avaliou os modelos de IHC e num segundo momento os diagramas da UML.
Primeiro Experimento:
Tarefas Os participantes dos testes tinham por tarefa, a ser executada a partir de os diagramas fornecidos, concluir o pedido de um livro e solicitar a entrega do mesmo em outro endereço que não o do próprio cliente.
Além de isto, deveria ser considerado que o usuário esqueceu sua senha e precisava recuperar- la.
Para isto, a todos os participantes foi distribuído um cenário, o qual descrevia a tarefa que eles necessitavam executar durante a avaliação.
Para execução desta tarefa, os participantes receberam os seguintes diagramas:
Um Diagrama de Casos de Uso contendo os Casos de Uso modelados no sistema (Pesquisa Livros, Visualizar Detalhes do Livro, Adicionar Livro ao Carrinho de Compras, Visualizar Carrinho de Compras, Concluir Pedido, Realizar Login e Verificar Pedidos);
Carrinho, Verificar Pedido e Realizar Login;
Quatro Modelos de Tarefas contendo a modelagem de tarefas das metas estabelecidas no Diagrama de Metas;
De posse dos diagramas recebidos, os usuários deveriam aplicar seus métodos de avaliação a fim de encontrar problemas de usabilidade, e enquanto realizavam as avaliações eram observados, a fim de se verificar os tipos de problemas que conseguiam capturar com os métodos de avaliação que estavam utilizando.
Todos os participantes do experimento assinaram um termo de consentimento livre e esclarecido, concordando com os termos do mesmo, além de responderem a um questionário para obtenção de seu perfil e outro para verificação da apreciação dos mesmos sobre o experimento.
Primeiro Experimento:
Resultados do estudo A seguir, serão apresentadas algumas considerações sobre os testes realizados e seus resultados.
Quanto a o perfil dos usuários, dos doze participantes dos testes, metade considerou- se principiante quanto a a modelagem UML e a outra metade considerou- se intermediária quanto a seu uso.
Já em relação a modelagem de IHC, dez dos doze participantes consideraram- se principiantes e dois consideraram- se em nível intermediário, não participando, também, nenhum usuário experiente.
Quanto a os problemas encontrados, a Tabela 1 apresenta uma breve descrição de cada problema e o número de usuários que o encontrou usando cada método/ modelo.
Cabe ressaltar que no problema &quot;Não há a possibilidade de recuperar a senha», dois usuários devem ser considerados como 100%, pois neste caso, dois usuários receberam o diagrama com a opção &quot;Recuperar senha «modelada e dois receberam a modelagem sem esta opção.
Ou seja, aqueles que receberam a modelagem sem a opção não devem ser considerados.
Em relação a a verificação de diretrizes, notou- se que a maioria dos problemas encontrados com este método foi identificada por todos usuários que o aplicaram.
Por exemplo, os problemas &quot;Não apresenta opções de Desfazer ou Cancelar «e &quot;Não apresenta Ajuda e Documentação «foram identificados por todos os usuários que aplicaram este método de avaliação.
Este resultado reflete a objetividade da lista de verificação, no uso de a qual o avaliador necessita apenas verificar se determinado item do sistema está ou não de acordo com as diretrizes da lista.
E, se a lista de verificação estiver bem elaborada, ela pode cobrir boa parte dos problemas de interação.
Por outro lado, este fato pode limitar a obtenção de resultados, pois o avaliador acaba atentando apenas para os itens existentes na lista, correndo assim, o risco de deixar &quot;escapar «outros problemas.
Após a visualização de detalhes do livro, como volto a navegar?
Não existe a possibilidade do usuário entrar em contato por email Mensagens de erro não descrevem ações para resolver o problema Total Avaliação Heurística Checklist Teste de Comunicabilidade Pode-se notar, por a tabela, que a avaliação heurística foi o método de avaliação que identificou o maior número de problemas.
Por ser um método mais subjetivo e dependente da experiência do avaliador, este método acaba cobrindo uma amplitude maior de problemas.
Se for comparada aos problemas detectados com a verificação de diretrizes, por exemplo, como a segunda é mais dependente da qualidade da lista de diretrizes do que da experiência do avaliador, pode ser que a mesma não tenha sido suficiente e/ ou adequada a uma avaliação sobre modelos (dado que a lista utilizada era voltada a sistemas web e não a modelos).
Notou- se, também, que nenhum dos problemas descritos foram encontrados apenas com os testes com usuários.
Isto pode ter ocorrido devido a a dificuldade de fazer com que os usuários interajam com modelos dessa natureza, sendo diferentes de efetuar a avaliação sobre protótipos ou storyboards do sistema.
Primeiro Experimento:
Discussão Em relação a a aplicação dos métodos de avaliação, confirmou- se que, para a verificação de diretrizes, não foi necessária uma maior experiência dos avaliadores, mas que estas diretrizes precisam ser mapeadas ­ adaptadas ­ para diretrizes específicas para modelos.
Notou- se, também, que ao utilizar a avaliação heurística nos diagramas de IHC, mais especificamente na MOLIC, os participantes conseguiram avaliar o fluxo de navegação do sistema bem como a qualidade das mensagens do sistema para o usuário.
Já nos diagramas UML foram identificados alguns problemas relacionados a mensagens inadequadas, principalmente por o fato de nenhum dos diagramas da UML conter campos ­ espaços ­ destinados a mensagens do sistema para o usuário.
Quanto a a utilização dos modelos, encontraram- se mais problemas com os diagramas da UML do que com os modelos de IHC.
É provável que isto esteja ligado ao fato dos modelos da UML não representarem todas as informações necessárias para a resolução do problema, considerando o quesito usabilidade.
Não se questiona aqui a correção destes diagramas:
Eles podem até estar modelados de forma ­ funcionalmente ­ correta, mas não contêm as informações necessárias para avaliar a usabilidade do sistema.
A grande maioria dos usuários encontrou dificuldades para trabalhar com os diagramas da UML, principalmente com os Diagramas de Seqüência.
Estes, segundo os participantes, apresentavam informações ­ internas do sistema ­ irrelevantes para o usuário, participantes acabavam por se perder ao tentar executar o fluxo neste diagrama.
Muitos participantes fizeram observações no sentido de que o Diagrama de Seqüência não era intuitivo para recuperar a senha, mas não notaram que, na verdade, o diagrama não apresentava esta opção (nenhum Diagrama de Seqüência foi modelado com a opção de Recuperar Senha, exatamente para verificar se esta opção seria observada no Diagrama de Atividades e se a falta de ela no Diagrama de Seqüência seria observada).
Em esse caso, talvez não fosse a estrutura do Diagrama de Seqüência que não era intuitivo e sim, seu conteúdo que não tratava a necessidade do usuário.
Outra informação interessante é que 50% dos participantes relacionaram o Diagrama de Casos de Uso com o Diagrama de Metas da modelagem de IHC, informando que o Diagrama de Casos de Uso proporciona uma visão geral do sistema, permitindo a visualização das tarefas (objetivos) que o usuário pode ter ao utilizar o sistema.
No entanto, em geral, os usuários sentiram falta de um diagrama que representasse todas as atividades (todos os Diagramas de Atividades reunidos num único diagrama), para que pudessem saber a partir de qual parte do sistema cada funcionalidade do mesmo poderia ser acessada.
Já 100% dos participantes consideraram o Modelo de Interação mais intuitivo do que qualquer diagrama da UML, o que já era esperado, dado o caráter comunicativo da MOLIC.
Considerando os problemas identificados, os modelos e os métodos utilizados, tanto a avaliação heurística quanto a verificação por diretrizes identificaram problemas por 31 vezes.
Já nos testes com usuários foram identificados problemas por 13 vezes.
Com base nestes dados, além de a dificuldade da realização dos testes com usuários a partir de modelos, decidiu- se utilizar apenas métodos de avaliação por inspeção (excluindo- se os métodos empíricos).
Em este âmbito pensou- se em definir um conjunto de heurísticas (mais abrangentes) e uma lista de verificação (de diretrizes) com itens mais objetivos para a avaliação de IHC baseada em modelos, sendo estes específicos para cada um dos quatro modelos já referenciados, tendo em vista as suas diferenças quanto a a representação de informações sobre IHC.
Assim, neste experimento, foi realizado um estudo inicial para verificação de quais seriam as diretrizes aplicáveis a modelos.
Segundo Experimento:
Métodos utilizados Em este experimento foram utilizadas listas de diretrizes relacionadas a cada uma das dez heurísticas de Nielsen.
As listas de diretrizes utilizadas foram compiladas por Camargo &amp; Meruvia, com base em Dias,, e.
Segundo Experimento:
Modelos utilizados Para este segundo experimento foram utilizados um Diagrama de Casos de Uso, um Diagrama de Atividades, um Modelo de Tarefas e um Modelo de Interação.
Optou- se por não trabalhar com o Diagrama de Seqüência visto as críticas feitas por os participantes do estudo de caso em relação a este diagrama.
Os modelos utilizados foram os mesmos do experimento anterior (Livraria Digital).
No entanto, desta vez, não houve diferença entre os diagramas, ou seja, todos apresentavam as mesmas funcionalidades modeladas, dentro de as características de cada modelo, é claro.
Segundo Experimento:
Participantes Dezesseis participantes contribuíram para este estudo de caso.
Os participantes eram concluintes da disciplina de IHC do curso de graduação em Ciência da Computação os quais já haviam visto na teoria e na prática tanto os métodos de avaliação quanto os modelos (de ES e de IHC) utilizados.
Segundo Experimento:
Tarefas Os participantes foram separados em duplas e cada dupla recebeu uma das listas de diretrizes (Apêndice D).
Além de as diretrizes em si, no início de cada lista também existia uma introdução sobre a atividade a ser realizada e, ao final, um termo de consentimento o qual os participantes deveriam assinar aceitando a participação e a utilização dos resultados para fins de pesquisa.
Cada dupla deveria verificar a aplicabilidade de cada diretriz da lista em cada um dos modelos.
Cada lista possuía as seguintes opções quanto a aplicação:
&quot;Sim», caso a diretriz fosse aplicável ao modelos;
&quot;Não», caso a diretriz não fosse aplicável;
&quot;Parcialmente», caso o participante achasse que a diretriz até poderia ser aplicada, desde que sofresse alguma alteração.
Caso o participante selecionasse a opção &quot;Parcialmente», ele deveria descrever o motivo da não aplicação total de tal diretriz sobre o modelo e sugerir alguma alteração na diretriz.
O participante também deveria identificar para qual modelo valia a sua resposta, visto estarem trabalhando com quatro modelos distintos.
Segundo Experimento:
Resultados do estudo e discussão Constatou- se que muitas diretrizes não poderiam ser aplicadas aos modelos, mesmo que sofressem alguma alteração no intuito de adaptar- las.
Por exemplo, no caso de a diretriz &quot;Em a entrada de dados numéricos, o usuário é liberado do preenchimento dos zeros fracionários desnecessários?»,
por exemplo, nenhum dos modelos utilizados no estudo de caso apresenta este nível de informação, portanto não há maneira de adaptar- la.
No entanto, existiam itens parcialmente aplicáveis e até itens diretamente aplicáveis.
Um exemplo de diretriz parcialmente aplicável é &quot;As opções do menu estão organizadas de modo lógico dando, ao usuário, o nome dos itens e as variáveis das tarefas?»,
a qual pode ser adaptada para &quot;As tarefas que devem ser realizadas estão organizadas logicamente?».
O item &quot;As mensagens de erro são compostas de forma que o sistema, e não o usuário seja responsável?»
é um exemplo de diretriz diretamente aplicável, pois se consegue avaliar este item com o Modelo de Interação, por exemplo, sem a necessidade de adaptações da diretriz.
A Tabela 2 apresenta algumas das diretrizes utilizadas e adaptadas (a lista completa encontra- se no Apêndice E).
A partir destes resultados, das sugestões dos participantes deste estudo de caso e da prévia identificação da necessidade de adaptação das diretrizes, deu- se início ao processo de adaptação, com a intenção de direcionar- las aos modelos, sempre atentando para avaliar a usabilidade do sistema e não a corretude do modelo.
Um ponto a destacar é que, a partir deste estudo, optou- se por elaborar apenas as listas de verificação de diretrizes e não heurísticas como tinha se pensado nos estudos anteriores.
Isto deve- se ao nível de detalhe das diretrizes elaboradas, que não estavam nem tão genéricas como as heurísticas de Nielsen (padrão inicial de comparação estabelecido) nem tão específicas como uma diretriz bastante direta (como &quot;O título está centralizado ou alinhado à esquerda.»,
por exemplo), o que inicialmente se esperava obter.
Conforme apresentado no capítulo anterior, após uma adaptação inicial de diretrizes conhecidas, e verificação das mesmas, chegou- se a uma lista de diretrizes específica para cada um dos quatro modelos utilizados:
Estas listas estão apresentadas na Tabela 3, categorizadas a partir de as heurísticas de Nielsen.
A fim de verificar a aplicabilidade destas diretrizes sobre modelos, foram realizados dois novos experimentos:
Um, nos mesmos moldes do segundo experimento relatado no capítulo anterior, com a aplicação destas diretrizes ­ adaptadas ­ sobre os mesmos modelos utilizados anteriormente;
E outro, com um sistema real, com a aplicação das listas sobre a modelagem (de ES e de IHC) do mesmo e sobre o protótipo do sistema.
Após a elaboração das listas de diretrizes apresentadas na tabela 3, efetuou- se, então, um novo experimento, a fim de verificar a aplicabilidade destas diretrizes sobre os modelos em questão e, a partir deste experimento, fazer os refinamentos/ ajustes que fossem necessários nas diretrizes.
Os participantes receberam os quatro diagramas previamente estabelecidos e suas respectivas listas de verificação de diretrizes, e deveriam verificar se o sistema estava de acordo com cada diretriz da lista baseando- se nos diagramas.
Modelos utilizados Conforme citado anteriormente, para este experimento foi tomado como base o mesmo sistema dos estudos anteriores (Livraria Digital) e, com isto, foram utilizados os mesmos diagramas dos estudos de caso anteriores:
Dois diagramas de IHC (Modelo de Tarefas e Modelo de Interação) e dois diagramas de ES (Diagrama de Casos de Uso e Diagrama de Atividades).
No entanto, para este experimento também foi entregue, aos participantes, a especificação textual do Modelo de Tarefas e do Modelo de Interação.
Como um dos tipos de problemas identificáveis no primeiro estudo de caso era a qualidade das mensagens de erro, desta vez os participantes receberam a especificação textual do Modelo de Interação, a qual representa bem estas mensagens.
Participantes Participaram deste estudo de caso dezesseis pessoas, alunos concluintes da disciplina de IHC do curso de graduação em Ciência da Computação e que já haviam visto, na teoria e na prática, tanto os métodos de avaliação quanto os modelos (de ES e de IHC) utilizados (cabe ressaltar que estes alunos não eram da mesma turma citada no segundo experimento do capítulo anterior).
Os participantes foram divididos em dois grupos:
Um grupo aplicou as diretrizes primeiro nos diagramas de IHC e depois nos de ES, enquanto o outro grupo fez o processo contrário, aplicou as diretrizes primeiramente nos diagramas de ES e em seguida nos de IHC.
Cada participante recebeu o Diagrama de Casos de Uso, o Diagrama de Atividades, os Modelos de Tarefas e o Modelo de Interação e uma lista de verificação para cada modelo.
Todos também receberam um documento em o qual constava uma introdução sobre o trabalho e o termo de consentimento.
Resultados Em geral, não foram detectadas diretrizes que não fossem aplicáveis aos modelos.
Apenas itens como, por exemplo, &quot;As abreviaturas são significativas?»
­ item da lista de verificação do Diagrama de Casos de Uso ­ em o qual 87% dos participantes que avaliaram primeiro os diagramas de IHC e depois de UML consideraram o item não aplicável e, 62% dos participantes que avaliaram primeiro os modelos de UML de depois de IHC também consideraram o item não aplicável.
No entanto, acredita- se que o item foi considerado não aplicável por o fato das modelagens não apresentarem abreviatura, logo, uma simples adaptação do item &quot;As abreviaturas são significativas?»
para &quot;Caso existam abreviaturas, estas são significativas?»
tornaria o item aplicável.
A partir de os resultados deste estudo de caso, partiu- se para o refinamento das listas de verificação, buscando tornar- las mais diretas.
Após o refinamento destas listas, a fim de verificar sua aplicabilidade por outro ângulo, deu- se, então, início a um novo processo, o qual consistia em avaliar os modelos do sistema e em seguida avaliar o sistema já implementado, com a intenção de verificar os problemas encontrados nas duas avaliações.
Esta etapa será descrita na próxima seção.
Para verificar a aplicabilidade das diretrizes elaboradas, optou- se por avaliar tanto os modelos de um sistema quanto o próprio sistema já implementado, verificando a similaridade dos problemas encontrados.
Sistema analisado Para este estudo de caso foi utilizada a ferramenta XenMaestro, desenvolvida no CPPH ­ Centro de Pesquisa PUCRS Hp.
O XenMaestro é uma ferramenta que instancia e controla domínios de sistemas virtuais, da plataforma Xen4, automaticamente, conforme a definição do ambiente.
Ele é executado a partir de um computador na rede e, ao ser iniciado com o arquivo de configuração indicado, busca computadores host com o Xen instalado e suas imagens em execução, fazendo uma conexão de rede com cada um de eles e criando novas imagens de sistema quando necessário.
A especificação do sistema XenMaestro pode ser encontrada no Apêndice F. Métodos utilizados Para a avaliação dos modelos do sistema XenMaestro foram utilizadas as listas de diretrizes elaboradas no decorrer de esta pesquisa.
Já para avaliar o sistema desenvolvido, além de a aplicação das listas de diretrizes elaboradas, optou- se, também, por utilizar a Avaliação Heurística.
Modelos utilizados Para este estudo de caso foram utilizados os diagramas para os quais as listas foram elaboradas:
Diagrama de Casos de Uso, Diagrama de Atividades, Modelo.
Xen é uma plataforma de virtualização livre que permite simular diferentes sistemas operacionais num mesmo hardware.
No entanto, foi feita uma engenharia reversa, ou seja, construíram- se os modelos a partir de o sistema já desenvolvido, ao invés de implementar o sistema com base nos modelos, visto o sistema já estar numa etapa avançada de desenvolvimento quando optou- se por utilizar- lo nesta pesquisa (e por não haver sido feita uma modelagem completa do mesmo até então).
Desenvolveu- se, então, o Diagrama de Casos de Uso e os de Atividades, os Modelos de Tarefas e de Interação, estes últimos com suas respectivas especificações textuais.
Para a avaliação foram selecionados alguns casos de uso do sistema, sendo requisito para isto que eles possuíssem interação entre o sistema e o usuário.
Os casos de uso selecionados foram:
Add Host, Create VM Domain, Edit Machine Information, Get Domain From Host e Remove Machine, e, para estes, foram desenvolvidos, então, os Modelos de Tarefas e de Interação correspondentes.
Participantes Esta avaliação contou com nove participantes:
Os participantes ou eram estudantes de mestrado em Ciência da Computação os quais também já haviam visto na teoria e na prática tanto os métodos de avaliação quanto os modelos (de ES e de IHC) utilizados, ou eram pesquisadores quais já haviam participado de alguns projetos de avaliação de IHC.
Resultados Em esta seção são discutidos os resultados alcançados com o estudo de caso referente a a aplicação das diretrizes propostas.
A Tabela 4 apresenta resultados referentes à avaliação do sistema com o Diagrama de Casos de Uso.
Cabe ressaltar que não serão discutidas, aqui, as diretrizes para as quais não tenham sido identificados problemas nos modelos, mas tenham sido identificados problemas no sistema, visto boa parte destes problemas estar ligada à interface e não a interação.
As tabelas existentes neste capítulo apresentam os resultados da aplicação das listas de diretrizes nos diagramas e no sistema, indicando &quot;Sim «se a diretriz foi satisfeita, &quot;Não «no caso de o sistema não contemplar a diretriz e &quot;N/ A «caso a diretriz tenha sido considerada como não aplicável.
As tabelas apresentam ainda um campo para a Avaliação Heurística, o qual deve estar selecionado, caso o problema tenha sido identificado com este método aplicado sobre o sistema.
A Tabela 5 apresenta alguns resultados da avaliação do sistema sobre o Diagrama de Atividades.
Em relação a as diretrizes para o Diagrama de Atividades, das 19 diretrizes elaboradas, todas foram aplicáveis ao diagrama, segundo os avaliadores.
No entanto, ao aplicar 2 destas diretrizes, foram identificados problemas no diagrama os quais não foram identificados quando as diretrizes foram aplicadas no sistema.
São elas:
&quot;O usuário pode sair do sistema a qualquer momento?&quot;:
A o aplicar a diretriz no sistema, os avaliadores consideraram que o usuário poderia, sim, sair do sistema a qualquer momento, desde que selecionasse o botão &quot;Fechar «da janela do sistema.
Entretanto, ao aplicar a diretriz no Diagrama de Atividades, não foram identificados pontos de saída do sistema.
Porém, houve comentários dos avaliadores no sentido de que não é característica do Diagrama de Atividades representar este tipo de informação e, sim, que o diagrama em questão representa estados finais para uma atividade, e não pontos de saída do sistema;
&quot;Se o usuário pode voltar a uma atividade anterior, ele pode mudar sua escolha anterior?&quot;:
A o aplicar esta diretriz sobre o sistema os avaliadores não identificaram problemas, pois, segundo eles, o usuário pode voltar a uma atividade anterior e alterar sua escolha face todas as atividades do sistema estarem na mesma tela.
Já na aplicação desta mesma diretriz sobre o Diagrama de Atividades, eles identificaram problemas, comentando que o usuário não poderia voltar a uma atividade anterior, logo, não poderia alterar sua escolha.
Tabela 5 ­ Resultados da Verificação com o Diagrama de Atividades (cont.)
Diretriz Diagrama de Atividades Em o diagrama Sim Não usuário necessita em cada etapa da seqüência de uma transação?
Sim Em o sistema Não Avaliação Heurística A Tabela 6 apresenta alguns resultados da avaliação do sistema com os Modelos de Tarefas.
Tabela 6 ­ Resultados da Verificação com o Modelo de Tarefas (cont.)
Diretriz são consistentes?
Modelo de Tarefas Em o diagrama Sim Não Sim Avaliação Heurística Em o sistema Não Em o que diz respeito a aplicação das diretrizes sobre o Modelo de Tarefas, das 17 diretrizes elaboradas para este modelo, todas foram consideradas como aplicáveis, segundo os avaliadores.
Houve um caso onde foi identificado problema a partir de o modelo mas este não foi identificado ao avaliar o sistema, com a diretriz &quot;O usuário pode sair do sistema a qualquer momento?»,
para a qual os avaliadores comentaram que não havia a representação de saída do sistema por meio de uma tarefa ubíqua no Modelo de Tarefas.
Já no sistema, os avaliadores comentaram que havia a opção de sair do sistema através do botão &quot;Fechar «da janela, mas que não havia uma opção de saída fornecida por o sistema.
A Tabela 7 apresenta alguns resultados da avaliação do sistema com o Modelo de Interação.
Tabela 7 ­ Resultados da Verificação com Modelo de Interação (cont ) Diretriz cada ação do usuário?
De as 33 diretrizes elaboradas para o Modelo de Interação, nenhuma foi considerada como não aplicável.
Como aconteceu também no Diagrama de Atividades, algumas diretrizes identificaram problemas a partir de o modelo avaliado mas não identificaram tal problema a partir de o sistema.
São elas:
&quot;O usuário pode sair do sistema a qualquer momento?&quot;:
Este problema foi identificado a partir de o modelo, visto não ter sido identificada uma opção de saída a qualquer momento a partir de o modelo, no entanto, os avaliadores consideraram que esta existia a partir de o sistema, pois havia a opção de &quot;Fechar «na janela do sistema;
&quot;Há acesso a tela principal de qualquer lugar do sistema?&quot;:
Este problema foi identificado a partir de o modelo e não foi identificado ao avaliar o sistema.
Os avaliadores consideraram que a cena principal do sistema deveria possuir a notação que identificasse um acesso ubíquo;
&quot;Se os usuário podem voltar a uma cena anterior, ele pode mudar sua escolha anterior?&quot;:
Foi considerado que no sistema o usuário poderia executar esta atividade, já no modelo, esta opção não foi identificada.
A o serem questionados por o motivo desta não identificação a partir de o modelo, os avaliadores mencionaram que, de acordo com o modelo, o usuário só poderia voltar a uma cena anterior a partir de o diálogo &quot;edit machine information», o que os fez considerar que, se não havia a opção nas outras cenas então esta opção não era apresentada no sistema.
Discussão Devido a o grande número de diretrizes que foram consideradas não aplicáveis para o Diagrama de Casos de Uso, acredita- se que, por a sua característica de informalidade, seja, de fato, um tanto quanto complexo avaliar a usabilidade de um sistema a partir de os Diagramas de Caso de Uso.
E que, conforme já comentado, definir padrões para a construção deste diagrama, não seria uma alternativa viável, visto que isto limitaria o diagrama, descaracterizando- o.
No que diz respeito aos Diagramas de Atividades, sobre a diretriz &quot;O usuário pode sair do sistema a qualquer momento?»,
acredita- se que uma alternativa para este tipo de problema teria sido utilizar o Diagrama de Seqüência para a avaliação, devido sua característica de representar um processo completo, e não apenas os passos a serem percorridos para a conclusão de um método ou algoritmo específico, que é o caso do Diagrama de Atividades.
Já sobre a diretriz &quot;Se o usuário pode voltar a uma atividade anterior, ele pode mudar sua escolha anterior?»
acredita- se que, dada a característica do Diagrama de Atividades de representar estados de ações dentro de um fluxo de controle, e que estes não podem ser decompostos, esta diretriz possua uma tendência a não aplicação.
O mesmo serve para a diretriz &quot;O usuário pode interromper, reiniciar e retomar uma atividade a qualquer instante?».
Em relação a o Modelo de Tarefas, o fato de alguns problemas não terem sido identificados a partir de os modelos, mas ao avaliar o sistema, o mesmo problema ser encontrado, considera- se natural, visto determinados problemas, principalmente no que diz respeito à interface (e não a interação) serem identificados, na maioria das vezes, com o sistema já desenvolvido.
Quanto a o Modelo de Interação, durante a aplicação da diretriz &quot;Há acesso à tela principal de qualquer lugar do sistema?»
foi identificado problema apenas sobre com o modelo, não sendo o problema identificado ao avaliar o sistema.
Pode ter ocorrido um problema em relação a a modelagem do sistema, onde não se representou o acesso de qualquer lugar do sistema por a notação do acesso ubíquo na cena, ou por este acesso não ter sido modelado devido a a característica do sistema de possuir todas as ações na mesma tela.
O fato da diretriz &quot;O usuário pode sair do sistema a qualquer momento?»
ter tido uma aplicação problemática em 3 dos 4 modelos avaliados pode ter acontecido devido os avaliadores terem considerado o botão &quot;Fechar «da janela como uma opção de saída do sistema, no entanto, acredita- se que não se deve considerar esta como uma opção de saída, visto não fazer parte do sistema propriamente dito, e sim do sistema operacional.
Houve também, casos em que o problema não foi identificado com o modelo, sendo identificado apenas com o sistema já desenvolvido, no entanto, acredita- se que isto não seja um grande problema, devido a, tipicamente, alguns problemas só serem apontados após a aplicação de um método de avaliação com o sistema pronto.
Em este sentido, constatou- se que em o (s):
Diagrama de Casos de Uso:
De os 13 problemas identificados a partir de o sistema desenvolvido, seja com a Avaliação Heurística ou com a Verificação de Diretrizes, 4 problemas foram identificados a partir de a avaliação do diagrama;
Diagramas de Atividades:
De os 13 problemas identificados a partir de o sistema desenvolvido, seja com a Avaliação Heurística ou com a Verificação de Diretrizes, 8 de eles foram identificados a partir de a avaliação dos diagramas;
Modelos de Tarefas: De os 15 problemas identificados a partir de o sistema desenvolvido, seja com a Avaliação Heurística ou com a Verificação de Diretrizes, 13 destes foram identificados a partir de a Verificação das Diretrizes a partir de os modelos;
Modelo de Interação: De os 23 problemas identificados a partir de o sistema desenvolvido, seja com a Avaliação Heurística ou com a Verificação de Diretrizes, 13 foram identificados a partir de a Verificação das Diretrizes a partir de o modelo.
O fato de todas as diretrizes para os modelos de IHC terem sido consideradas como aplicáveis pode ter ocorrido devido a a característica destes modelos de representação de interação, ou seja, estes modelos foram desenvolvidos com este objetivo.
Com exceção do Diagrama de Casos de Uso onde a maioria dos itens não foram aplicáveis, acredita- se que os outros diagramas tiveram resultados, no mínimo, razoáveis no que diz respeito a identificação de problemas a partir de modelos.
Considerando a necessidade da antecipação do processo de avaliação de IHC, a fim de identificar problemas de usabilidade ainda em tempo de projeto, reduzindo assim, os custos com reparação de eventuais problemas, pensou- se em propor um método de avaliação de IHC baseada em modelos.
Considerando, também, a ampla utilização da UML como linguagem de modelagem para o desenvolvimento de sistemas orientados a objetos e o pouco conhecimento das equipes de desenvolvimento sobre diagramas de IHC, pensou- se que o método proposto deveria poder ser utilizado tanto com modelos de IHC quanto com modelos de ES.
A partir de o levantamento bibliográfico feito para este trabalho, identificou- se a existência de alguns métodos de IHC que são aplicados sobre modelos, os quais, em sua maioria, são métodos de avaliação automática, deixando de capturar dados subjetivos.
A fim de trabalhar a questão da subjetividade na avaliação de IHC baseada em modelos, propôs- se, neste trabalho um método informal -- e não automático -- de avaliação de IHC baseada em modelos.
Para esta proposta foi necessário definir quais diagramas da UML e quais métodos de avaliação de IHC seriam utilizados por o método, para o qual foi feito um estudo de caso inicial, aplicando diferentes métodos de avaliação sobre diferentes diagramas.
A partir de esta etapa, definiu- se que seriam elaboradas listas de heurísticas e de diretrizes para avaliação de IHC sobre os diagramas de Caso de Uso e de Atividades da UML e sobre modelos de Tarefas e de Interação de IHC.
No entanto, identificou- se certa dificuldade para separar diretrizes de heurísticas, dado o nível de especificidade de ambas.
Assim, acabou- se optando por elaborar uma lista de diretrizes, algumas bastante diretas e outras nem tanto.
Como definiu- se que trabalharia- se- com Diagramas de Casos de Uso, Diagramas de Atividades, Modelos, deu- se início ao processo de elaboração de uma lista para cada um destes diagramas, atentando para suas características particulares.
Após definidas as diretrizes que seriam trabalhadas, tentou- se aplicar- las ­ em seu estado natural, sem adaptação ­ aos diagramas em questão.
Confirmou- se a idéia de que era necessária uma adaptação das diretrizes a fim de ajustar- las para que fossem aplicáveis sobre modelos.
E, após um processo inicial de adaptação das diretrizes, tentou- se aplicar- las sobre os diagramas em questão.
Feito isto, partiu- se para um refinamento das diretrizes e para a aplicação do método proposto.
Para a etapa de validação, aplicou- se as diretrizes sobre os diagramas de um determinado sistema e sobre o próprio sistema já desenvolvido.
Aplicou- se ainda o método de Avaliação Heurística sobre o sistema a fim de comparar os problemas encontrados tanto na modelagem quanto no sistema.
Em esta análise, procurou- se atentar para problemas que tenham sido identificados apenas nos modelos e não no sistema, o que pode ocorrer devido a o sistema não ter sido fiel a sua modelagem ou por problemas de ambigüidade da diretriz.
Notou- se que, para a aplicação do método, é necessário um mínimo de experiência em IHC, visto não ter- se elaborado diretrizes muito diretas.
Isto leva a outra discussão:
Será que diretrizes bastante diretas para modelos, não acabariam por avaliar o modelo e não o sistema a partir de sua modelagem?
E, estas sendo bastante diretas, não seria possível automatizar- las?
E, se fosse, será que conseguiriam tratar de questões subjetivas, o qual era um dos objetivos deste trabalho?
Estas são algumas questões a serem trabalhadas e discutidas futuramente, como, por exemplo, o poder de expressão de cada notação e a possibilidade de &quot;formalizar «a avaliação para cada modelo.
Acredita- se que determinadas diretrizes, como, por exemplo, «A quantidade de passos necessários para que o usuário alcance seu objetivo é minimizada? (
Recomenda- se não ultrapassar 4 passos) «possam ter sua verificação automatizada, desde que passem por um processo de adaptação e com a utilização de métodos formais.
No entanto, acredita- se que outras diretrizes, como, por exemplo, &quot;As mensagens colocam o usuário no controle do sistema?»
não possam ser automatizadas, devido a necessidade, pelo menos, por enquanto, do &quot;olho humano «para verificar a qualidade da mensagem.
Por fim, considera- se que este trabalho pode ser utilizado como uma avaliação &quot;quick and dirty».
Em este tipo de avaliação, os projetistas obtêm um feedback informal dos usuários ou avaliadores para confirmar que suas idéias estão de acordo com as necessidades dos usuários e que estão agradando;
Sendo sua ênfase a contribuição rápida e não descobertas cuidadosamente documentadas.
Ou seja, o método proposto neste trabalho se apresenta como uma alternativa viável ­ de baixo custo e de fácil aplicação ­ para profissionais de IHC ou com pouca experiência em IHC terem, de uma forma bastante rápida, feedback sobre o sistema que estão desenvolvendo.
Em o intuito de melhorar o presente trabalho acredita- se que pode- se ampliar a verificação e análise dos resultados, testando- o com um maior número de usuários e testandoo com um sistema realmente em construção avaliando modelos e posteriormente avaliando- o pronto.
No que diz respeito à avaliação sobre modelos, acredita- se ser interessante incluir no método uma lista de verificação para o Diagrama de Seqüência, face sua característica de representar um processo completo, conforme já comentado.
Outra contribuição neste sentido seria estender as listas de diretrizes, trabalhando com outras listas a fim de adaptar- las.
E uma outra contribuição ainda seria avaliar quais diretrizes são passíveis de automatização e como automatizar- las.
Por fim, uma outra idéia que surgiu no decorrer deste trabalho, foi de estender a avaliação de IHC, desenvolvendo- se um método de avaliação para diferentes etapas do processo de desenvolvimento do sistema, avaliando outros artefatos que não modelos, utilizando, se necessário, outros métodos de avaliação que não os informais.
