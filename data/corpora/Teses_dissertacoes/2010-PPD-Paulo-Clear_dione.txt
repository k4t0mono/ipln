Através de formalismos Markovianos é possível modelar diversos sistemas e resolver- los através de soluções computacionais específicas possibilitando prever ou avaliar seus padrões de comportamento.
O formalismo de Redes de Autômatos Estocásticos (SAN) permite descrever modelos Markovianos de forma compacta e modular.
Além disso, é utilizado para obter índices de desempenho de sistemas através de soluções numéricas iterativas que se baseiam num descritor e um vetor cujo tamanho é igual ao espaço de estados do modelo.
Dependendo do tamanho do modelo esta operação torna- se computacionalmente onerosa e muitas vezes impraticável.
Um método alternativo para calcular índices a partir de um modelo é a simulação, principalmente porque ela simplesmente exige a definição de um gerador de números pseudo-aleatórios e funções de transição entre estados que permitem a criação de uma trajetória.
O processo de amostragem pode ser diferente para cada técnica estabelecendo algumas regras para coleta de amostras para posterior análise estatística.
As técnicas de simulação, normalmente requerem muitas amostras para calcular índices de desempenho estatisticamente relevantes.
Este trabalho proporciona comparações da precisão dos resultados de alguns modelos Markovianos obtidos a partir de a execução de diferentes técnicas de simulação.
Além disso, propõe uma maneira distinta de simular modelos Markovianos usando um método baseado em estatística Bootstrap para minimizar o efeito dé comparado com reescolha das amostras.
A eficácia do método proposto, denominado Bootstrap simulation, e sultados da solução numérica para um conjunto de exemplos descritos por meio de o formalismo de modelagem SAN.
Palavras-chave: Avaliação de Desempenho;
Redes de Autômatos Estocásticos; Simulação;
Precisão. A o analisar um sistema é possível utilizar formalismos que facilitam sua representação permitindo descrever- lo de forma a evidenciar as diferentes configurações que ele pode apresentar.
Os formalismos são comumente utilizados na modelagem de sistemas computacionais, sistemas industriais ou sistemas que envolvam fenômenos da natureza.
A modelagem por formalismos permite a aplicação de soluções computacionais para resolver- los e possibilita sua análise a partir de os resultados obtidos.
Esta análise permite avaliar seus padrões de comportamento prevendo eventos ou cenários de maior ou menor probabilidade de ocorrência.
De entre as soluções utilizadas para a resolução de sistemas através de formalismos, destacamse principalmente os métodos analíticos e a simulação.
Os métodos analíticos consistem, normalmente, no emprego de métodos matemáticos iterativos, como o Método da Potência, o Método de Arnoldi e o Método GMRES (Generalized minimal residual method).
Estes métodos constituem- se, basicamente, na multiplicação de um vetor por uma matriz, denominada matriz de taxas de transição (escala de tempo contínua) ou matriz de probabilidades de transição (escala de tempo discreta).
A solução estacionária (vetor resultante), do processo iterativo, irá conter as probabilidades de permanência em cada estado do sistema modelado.
A obtenção da matriz, utilizada neste processo, se dá a partir de um grafo ou autômato que representa o sistema como um conjunto finito de estados possíveis e transições entre estes estados.
Os estados correspondem à s configurações do sistema e as transições identificam as mudanças possíveis de um estado para outro.
Estas transições são rotuladas por eventos que, em escala de tempo contínuo, apresentam as taxas ou frequências em que as mesmas ocorrem.
Esta matriz de transição de estados é derivada de um grafo também denominado Cadeia de Markov.
Cadeias de Markov é um formalismo analítico, não estruturado, amplamente utilizado por sua facilidade de representação e baixa complexidade de resolução.
Porém, uma de suas limitações éa ocorrência da explosão do seu espaço de estados, que acontece quando as configurações possíveis do sistema aumentam significativamente, tornando- as difíceis de serem tratadas através de uma única matriz de transição correspondente.
De entre as alternativas de modelagem conhecidas, que tentam minimizar este problema, sobressaem- se formalismos estruturados, como por exemplo, Stochastic Au-tomata Networks (SAN), Generalized Stochastic Petri Nets (GSPN) e Perfomance Evaluation Process Algebra (PEPA).
Estes formalismos apresentam soluções numéricas mais eficientes em memória, para grandes modelos, do que as Cadeias de Markov.
Outra solução bastante utilizada na resolução de sistemas é a simulação.
Esta solução, geralmente baseia- se na geração de números aleatórios que atuam nas decisões de comportamento e na interação entre os componentes do sistema analisado.
No entanto, a simulação consiste em aproximações, sendo necessário executar- la por um tempo suficientemente grande para que ela se aproxime da solução obtida com os métodos iterativos.
Em a simulação, além de uma matriz de transição, trabalha- se com uma função de transição que define a ocorrência dos eventos através de uma análise nas taxas discretizadas de cada linha da matriz.
Esta discretização corresponde à conversão dessas taxas em probabilidades e constitui o vetor, também de probabilidades, a partir de a coleta de amostras obtidas através de trajetórias disparadas no modelo.
A escolha da solução mais adequada numa determinada situação deve considerar critérios como a disponibilidade de tempo, recursos computacionais existentes e o nível de precisão requerido nos resultados.
Embora as soluções analíticas apresentem precisão nos resultados, as mesmas deparam- se com limitações que estão ligadas, principalmente, no que diz respeito ao armazenamento da matriz de transição e dos vetores utilizados no processo iterativo.
A utilização do formalismo SAN se sobressai à s Cadeias de Markov por apresentar uma forma de representação compacta em memória, permitindo assim modelar sistemas com maior número de configurações (estados).
Além disso, no que diz respeito a SAN algumas técnicas para otimizar a resolução de modelos têm sido aprimoradas com a finalidade de acelerar a convergência na resolução dos métodos iterativos.
Em este contexto pode ser mencionada a solução Split, a qual adota primitivas de ã lgebra tensorial bastante eficientes, combinadas com técnicas de armazenamento para matrizes esparsas.
No entanto, conforme diminui o nível de abstração da realidade modelada exigindo que ainda mais estados sejam representados, aumenta a carga computacional aplicada à s ferramentas numéricas que resolvem este formalismo.
Este fato faz com que a resolução do mesmo também alcance os limites computacionais.
Quando se deseja resolver modelos com um número de estados que ultrapassam a capacidade atual de resolução dos métodos iterativos, destaca- se então a simulação.
A simulação apresenta- se como uma alternativa viável por possibilitar a exploração de métodos e técnicas de armazenamento em memória de forma menos complexa que nos métodos iterativos.
Com isso, torna- se possível a resolução de modelos com um número de estados ainda maior.
A o resolver estes modelos através da simulação, um fator crítico é a precisão dos resutados obtidos através desta solução.
Mesmo assim, trabalhos que analisam especificamente a precisão dos resultados da simulação ainda são desconhecidos até o momento.
Uma análise minuciosa em relação a os resultados de diferentes técnicas de simulação pode evidenciar particularidades ainda não questionadas e apontar características específicas sobre o comportamento de cada uma de elas.
Além disso, os resultados obtidos por simulações podem ser comparados com os resultados obtidos por métodos iterativos, para um modelo equivalente, o que possibilita demonstrar o erro de cada uma das técnicas.
De entre as técnicas de simulação conhecidas para a resolução de formalismos Markovianos estão a Forward Simulation, a Backward Coupling Simulation e a simulação tradicional, chamada neste trabalho de Permanence Time Simulation.
O objetivo geral deste trabalho é evidenciar quantitativamente de forma comparativa a precisão dos resultados observados para alguns modelos Markovianos ao aplicar diferentes técnicas de simulação.
Esta comparação visa auxiliar na escolha da melhor técnica a ser adotada, de entre as que serão analisadas, sendo elas:
A) Forward Simulation;
B) Backward Coupling Simulation;
E c) Permanence Time Simulation.
Além disso, será realizado um estudo no método Bootstrap a fim de propor a adaptação do mesmo como uma nova técnica de simulação visando a obtenção de resultados mais precisos considerando também os tempos de processamento de cada técnica.
As técnicas de simulação serão analisadas comparando seus resultados com os da solução iterativa, obtida através do Método da Potência utilizando sistemas modelados com o formalismo SAN, equivalentes em ambas as abordagens.
Para resolver- los e obter o vetor de probabilidades com os resultados, desenvolveu- se um simulador que realiza trajetórias no estado global destes sistemas, o qual corresponde ao estado da Cadeia de Markov correspondente à SAN.
Vale destacar que isso pode ser realizado porque toda SAN possui uma Cadeia de Markov subjacente.
Como o objetivo e óbservar o impacto da simulação em relação a a precisão dos resultados, acredita- se que o formato esparso de representação dos modelos seja independente do formalismo.
Logo, optou- se por realizar a simulação desta forma, por a praticidade de tratar um Gerador Infinitesimal1 de uma Cadeia de Markov, do que tratar um Descritor Markoviano de SAN, o qual é composto por várias matrizes de transição de menor dimensão, porém com operadores tensoriais.
O foco das comparações e análises que serão realizadas neste trabalho será em relação a o vetor de probabilidades resultante ao aplicar as técnicas de simulação.
A partir de ele, a precisão será avaliada analisando o erro relativo entre o resultado obtido com a simulação e o resultado da solução iterativa.
Para identificar particularidades que possam causar discrepâncias nos resultados, de forma a acarretar perda de precisão, pretende-se observar outros pontos, como a relevância no tamanho da trajetória e na quantidade de amostras envolvidas no processo de simulação.
Para obter os resultados em tempo hábil foi utilizada também a paralelização na implementação das técnicas de simulação.
O Gerados Infinitesimal de Cadeias de Markov será visto no próximo capítulo.
Como propósito da paralelização, deseja- se reduzir o tempo de processamento na tentativa de obter resultados mais precisos, uma vez que, normalmente, uma das desvantagens da simulação comparado à s soluções iterativas é o tempo despendido até atingir uma aproximação da solução estacionária2.
Para a satisfação dos objetivos apontados é necessário o entendimento sobre os seguintes assuntos:
Formalismos Markovianos, abordados no Capítulo 2, onde é descrito o formalismo de Cadeias de Markov e de Redes de Autômatos Estocásticos (SAN), ilustrando suas respectivas representações e os modelos utilizados neste trabalho;
Simulação, abordado no Capítulo 3, onde são tratados assuntos sobre o processo de simulação e as diferentes técnicas utilizadas para a resolução de modelos Markovianos.
Em o Capítulo 4 são descritos os testes e os resultados obtidos com as técnicas de simulação conhecidas.
Em seguida, no Capítulo 5 é apresentada a nova técnica de simulação baseada no método Bootstrap e os respectivos resultados obtidos com ela.
Por fim, no Capítulo 6 são apresentadas a conclusão, a contribuição e os trabalhos futuros referentes a esta dissertação.
A solução estacionária do método iterativo pode ser obtida subtraindo os resultados do vetor da solução atual com os resultados da solução do vetor anterior.
Se o mesmo for zero, ou próximo de zero dado uma certa tolerância, significa que a solução estacionária foi então atingida.
Formalismos Markovianos Formalismos Markovianos são constantemente utilizados na modelagem de diversos sistemas.
O emprego destes formalismos unidos a soluções computacionais baseadas em métodos matemáticos para sua resolução, apresentam- se como boas práticas para avaliar o desempenho de sistemas e analisar seus padrões de comportamento.
A seguir são apresentados os formalismos de Cadeias de Markov e de Redes de Autômatos Estocásticos (SAN), demonstrando suas respectivas formas de representação.
Cadeias de Markov é um formalismo matemático utilizado para a modelagem de sistemas proposto por o matemático russo Andrei Andreyevich Markov em 1906.
Através deste formalismo o funcionamento dos sistemas pode ser descrito através de um conjunto de estados possíveis e de transições entre estes estados.
As transições são modeladas por um processo estocástico1 de tempo contínuo ou discreto, os quais são definidos por distribuições de probabilidade exponenciais ou geométricas respectivamente.
Este trabalho irá tratar apenas sistemas de tempo contínuo, porém com estados discretos.
Em um sistema com espaço de estados discreto, as variáveis que compõem seu estado mudam de valor instantaneamente.
Em este caso, podem ser mencionados os nodos de um grafo, sendo que é possível apenas estar num ou em outro estado (nodo) do grafo, não havendo um estado intermediário.
Para facilitar a compreensão da representação de um sistema modelado através do formalismo dé ilustrada uma cadeia na Figura 2.1.
Os nodos da figura representam os estados Cadeias de Markov e Estas setas são rotuladas por taxas, que são definidas por valores que determinam a frequência do disparo das transições.
A Cadeia de Markov apresentada na Figura 2.1 pode também ser representada através de uma matriz, denominada Matriz de Taxas de Transição, conforme pode ser observado na Tabela 2.1.
As taxas, na matriz, são distribuídas de forma que cada linha i e coluna j indicam a taxa de transição de um estado para outro (representado entre parênteses) na Cadeia de Markov, sendo a dimensão da matriz igual ao número de estados do modelo, ou seja| S|.
Em a resolução deste formalismo é necessário que esta matriz de transição seja um Gerador Infinitesimal de forma que a soma de cada linha i da matriz seja igual a zero.
Para realizar este ajuste, adiciona- se à diagonal principal, o complemento da soma de todos os elementos não diagonais de cada linha, resultando então no Gerador Infinitesimal Q da Tabela 2.2.
Feito isso, têm- se um sistema de equações, de forma que Q $= 0, onde deseja- se encontrar o vetor.
Como a resolução de um sistema deste tipo torna- se muito complexa com o aumento de estados do modelo, um dos métodos mais utilizados para a obtenção do vetor é o Método da Potência.
Para que este método possa ser aplicado, é necessário primeiramente transformar o Gerador Infinitesimal Q numa matriz chamada Matriz de Probabilidades de Transição ou Matriz Estocástica P (uma vez que a solução também corresponde a um vetor de probabilidades).
Feito isso, não podem haver valores negativos na matriz e a soma de todos os elementos de cada linha devem ser iguais a um.
Este processo corresponde à discretização da matriz Q que pode ser realizada dividindo toda a matriz por o seu maior elemento max em módulo, e somando a ela uma matriz identidade I de mesma dimensão, conforme a equação 2.1.
O método da potência consiste, basicamente, em sucessivas multiplicações de um vetor qualquer por uma matriz, de forma iterativa.
Em Cadeias de Markov esta matriz corresponde à matriz de probabilidades de transição P, sendo que no final do processo iterativo resulta- se num novo vetor, de forma que P $ , sendo a dimensão de igual ao número de estados do modelo.
Este vetor resultante conterá as probabilidades de permanência em cada estado sendo ele a solução estacionária que, neste caso, baseia- se num critério de parada que pode ser um erro aceitável, resultante da diferença dos vetores entre duas iterações.
A expressão a seguir ilustra o processo iterativo, sendo (n) a solução estacionária:
Um dos principais problemas das Cadeias de Markov é a ocorrência da explosão do seu espaço de estados, que ocorre conforme aumentam as configurações possíveis (nível de detalhe) do sistema que está sendo modelado.
Este fato torna as Cadeias de Markov difíceis de serem tratadas através de uma única matriz de transição correspondente pois aumenta também a dimensão da matriz envolvida no processo iterativo.
Para minimizar este problema, formalismos estruturados como SAN podem ser utilizados.
Redes de Autômatos Estocásticos (SAN) é um formalismo baseado em Cadeias de Markov, proposto por Plateau em 1985.
Consiste numa forma modular de descrever sistemas complexos com grandes espaços de estados a serem modelados.
Para isso necessitam da resolução de modelos matemáticos robustos, que utilizam à Álgebra tensorial ou de Kronecker como forma de armazenamento compacto.
Desta forma, este formalismo consegue manter o mesmo poder de solução obtido com a utilização de Cadeias de Markov, propondo para tal um novo formato de armazenamento ao do espaço de estados.
Este formalismo é composto por no mínimo dois ou mais autômatos, sendo que cada um de eles é definido na forma de um grafo constituído de um conjunto finito de estados e transições entre estes estados, conforme pode ser observado na Figura 2.2, sendo os estados do sistema representados por os nodos.
O estado em que um autômato qualquer se encontra é chamado de estado local, enquanto que o conjunto de estados que todos os autômatos se encontram é denominado estado global, o qual corresponde ao estado na Cadeia de Markov equivalente ao modelo em estudo.
Isto é possível, pois toda SAN pode ser representada por uma Cadeia de Markov, a qual consiste num único autômato estocástico.
A Cadeia de Markov subjacente ao modelo da Figura 2.2 é a da Figura 2.1, apresentada anteriormente.
Essa equivalência é feita por as combinações de estados possíveis entre os autômato A e A da SAN.
Para entender- la considere o estado local do autômato A igual a 0 e o estado local do autômato A igual a 1, neste caso, o estado global na Cadeia de Markov da Figura 2.1 é o estado 01.
Sendo assim, o espaço de estados é calculado por o produto cartesiano da dimensão de todos os autômatos de uma SAN, definido como espaço de estados produto (product state space -- PSS).
No caso de o modelo da Figura 2.2 o PSS é 2 × 3 $= 6.
Além de o PSS e do RSS, o formalismo SAN, ao contrário de Cadeias de Markov, apresenta como uma de suas principais características a possibilidade de dividir um sistema complexo em subsistemas que interagem ocasionalmente.
Esta interação ocorre através de eventos sincronizantes ou até mesmo através de taxas funcionais.
Estes eventos são os responsáveis por disparar as transições entre os estados de um autômato e são descritos com mais detalhes a seguir.
Para que possam ocorrer transições entre os estados de um autômato é necessário que existam eventos associados a estas transições.
Eventos locais caracterizam- se por alterar o estado de apenas um autômato do modelo, possibilitando com que os autômatos tenham comportamentos paralelos.
Sendo assim, um evento local não interfere no estado dos demais.
Os eventos locais podem ser observados no autômato da Figura 2.2 sendo estes representados por e2, e3, e4 e e5.
Um evento sincronizante é caracterizado por alterar o estado de dois ou mais autômatos de forma simultânea.
Sua ocorrência se dá em todos os autômatos envolvidos estabelecendo assim um sincronismo entre eles.
Em a Figura 2.2 este evento é representado por e1, onde pode ser notada sua presença em ambos os autômatos.
A seguir serão apresentados alguns conceitos referentes à s taxas funcionais que também estabelecem relação entre os diferentes autômatos de um modelo.
A utilização de taxas funcionais e/ ou probabilidades funcionais é outra forma de representar interações entre os autômatos de uma SAN sem alterar o estado de todos os autômatos envolvidos.
Taxas funcionais podem ser definidas por funções que refletem a avaliação dos estados atuais do mo delo.
Em a Figura 2.2 o evento e2 do autômato A apresenta uma taxa funcional sobre o autômato A, definida por a função f, descrita abaixo de a figura.
Em este caso, de acordo com a função f, a taxa do evento e2 depende do estado que o autômato A encontra- se, ou seja, a taxa do evento e2 serígual a µ1 caso o estado do autômato A estiver em 0 ou igual a µ2 caso estiver no estado 1.
Em o autômato A, pode- se também observar probabilidades para diferentes transições do evento e3.
Essas probabilidades representadas por 1 e 2 definem a probabilidade de escolha, ou seja, A nomenclatura da função de atingibilidade pode ser encontrada na documentação da ferramenta PEPS.
Em resumo, a utilização de funções para definir taxas ou probabilidades permite associar a um mesmo evento diferentes valores.
Assim sendo, as mesmas são expressas por funções que levam em consideração os estados atuais dos autômatos de um modelo variando, desta forma, seu valor conforme os estados em que se encontram os autômatos envolvidos na função.
Em esta seção são exibidos alguns sistemas modelados com o formalismo SAN, os quais são utilizados para a realização dos testes deste trabalho.
Alternate Service Pattern -- ASP Padrão de Serviço Alternado (Alternate Service Pattern -- ASP) é um sistema modelado com o formalismo de Redes de Filas de Espera e consiste numa rede aberta em que alguns servidores apresentam mais de um padrão de atendimento.
A Figura 2.3 mostra um sistema ASP com quatro filas:
F1, F2, F3 e F4;
Cada qual com capacidade finita K1, K2, K3 e K4 respectivamente.
A taxa de chegada nas filas F1 e F2 são 1 e 2, respectivamente.
A fila F1 atende a uma taxa µ1 e faz roteamento de seus clientes para a fila F3, caso ela possa receber- los.
Esse é o chamado comportamento bloqueante, visto que o cliente não deixa F1 se F3 não puder receber- lo.
O atendimento na fila F2 acontece a uma taxa µ2, devendo o cliente ser roteado para a fila F3, caso esta tenha capacidade para receber- lo.
Caso F3 não possa receber o cliente vindo de F2, este deixa o sistema, sendo isso chamado comportamento de perda, representado na Figura 2.3 por a flecha indicada por loss.
A fila F3 atende cada cliente segundo um dos P padrões de serviço que comporta, cujas taxas são esse mais um caso de roteamento com comportamento bloqueante.
Por fim, os clientes de F4 são atendidos segundo a taxa µ4 e deixam o sistema.
Esse mesmo sistema pode ser modelado usando o formalismo SAN.
Para exemplificar, suponha a SAN da Figura 2.4, com quatro autômatos de K estados cada, mais um autômato de P estados.
O PSS para este modelo é igual a × P, sendo o mesmo igual ao RSS.
Agora analisamos mais atentamente a Figura 2.4 para entender como a representação desse sistema ASP modelado em SAN realmente funciona.
As filas F1, F2, F3 e F4 passam a ser representadas por os autômatos A, A e A, respectivamente, cada um de eles com capacidade K. Um quinto autômato, identificado por A, é usado para representar o padrão de serviço de F3 que, segundo a figura, possui P estados, ou seja, P padrões de serviço com que a fila, representada por o autômato A3, atende seus clientes.
Os eventos de chegada nos autômatos A e A são representados por os eventos e1 e e2, respectivamente, enquanto que o evento e4 representa a saída do autômato A para o exterior, sendo estes eventos locais.
Os eventos indicados com índice duplo são os eventos sincronizantes, como é o caso do evento e13, que corresponde à saída do autômato A para o autômato A.
O evento e23 pode corresponder a duas situações:
A) o estado do autômato A (3) é diferente daquele correspondente à sua capacidade máxima, acontecendo então o roteamento do cliente do autômato A para o autômato A;
B) o estado do autômato A é aquele de capacidade máxima, indicando que o cliente sai de A direto para o exterior, caracterizando comportamento de perda, representado por o loop no último estado do autômato A.
Esse loop significa que o cliente saiu de A mas não foi roteado para A, por isso A mantém seu estado.
First Available Server -- FAS O modelo First Available Server (FAS) analisa a disponibilidade de N servidores, conforme pode ser notado no modelo SAN da Figura 2.5.
Cada servidor (de índice i, onde i) é representado por um autômato A, composto por dois estados:
I (idle/ disponível) e B (i) (busy/ ocupado).
Em este exemplo os pacotes chegam nos servidores desde que pelo menos um de eles não esteja ocupado.
Este modelo pode ser visto como um quadro de análise de diferentes filas onde cada pacote na fila pode avançar para o primeiro servidor disponível.
Sendo assim o pacote chega primeiro no servidor 1, se este não está disponível o pacote tenta o servidor 2, se este também não estiver disponível o pacote tenta o servidor 3 e assim sucessivamente até o servidor N. Resource Sharing -- RS O modelo da Figura 2.6 apresenta um exemplo clássico, que representa um sistema de compartilhamento de recursos, sendo R o número de recursos e P o número de processos.
Cada processo é representado por um autômato A composto por dois estados:
S (em repouso) e U (em uso).
Os recurso são representados por o autômato A (P+ 1) e tem R+ 1 estados indicando o número de recursos em uso.
Este modelo apresenta apenas eventos sincronizantes, visto que os eventos eai representam a aquisição de um recurso com taxa constante i e os eventos eri representam a liberação de um recurso com taxa constante µi.
O PSS deste modelo é formado por 2P × estados globais, porém ao contrário de os demais modelos nem todos os seus estados são atingíveis.
Simulação consiste num processo de elaboração de um modelo de Segundo Shannon a simulac um sistema real (ou hipotético) e a condução de experimentos com a finalidade de entender o comportamento de um sistema ou avaliar sua operação.
Para Law e Kelton, simulação corresponde a uma gama variada de métodos e aplicações que reproduzem o comportamento de sistemas reais, usualmente utilizando- se de ferramentas computacionais.
Através da simulação é possível manipular os componentes que constituem modelos Markovianos, para então encontrar o vetor de probabilidades, resultante da resolução dos mesmos.
Como este vetor consiste em aproximações da solução iterativa, o tempo de simulação deve ser grande o suficiente para que os resultados possam convergir numa solução próxima da estacionária, após uma série de execuções ou coletas de amostras.
Isto é necessário, pois um dos grandes problemas da simulação e á precisão dos resultados, uma vez que a mesma consiste na tiragem de números pseudo-aleatórios para definição de variáveis, regidas por distribuições de probabilidades, conforme descrito a seguir.
Modelos estocásticos apresentam variáveis aleatórias para definir as taxas de transição entre os estados que os compõem.
Praticamente todos os sistemas apresentam determinada fonte de aleatoriedade, como o tempo entre chegadas, tempos de serviço, tempo entre a ocorrência de uma falha, etc..
Pode- se definir variável aleatória como uma função ou regra que associa um número real a cada ponto do espaço amostral.
Espaço amostral, por sua vez, é o conjunto de todos os resultados possíveis para um experimento.
A teoria dos grandes números, definida por Jacob Bernoulli em 1692, estabelece que, numa série imensa de experimentos, a freqüência relativa de um evento se aproxima cada vez mais da sua probabilidade.
Assim, dada uma longa série de experimentos, pode- se, com erro desprezível, calcular a probabilidade de um evento, ou então, dada a probabilidade de um evento, pode- se calcular o número de vezes que ele pode ocorrer numa longa série de tentativas.
A distribuição de probabilidade associa uma probabilidade a cada resultado numérico de um experimento, ou seja, dá a probabilidade de cada valor de uma variável aleatória.
Por exemplo, no lançamento de um dado cada face tem a mesma probabilidade de ocorrência que é dada por 1 Como os valores das distribuições de probabilidades são também probabilidades, e como as variáveis aleatórias devem assumir um de seus valores, têm- se duas regras a seguir que se aplicam a qualquer distribuição de probabilidade:
Em o exemplo do lançamento de um dado, como todas as faces têm a mesma probabilidade de ocorrência, que é1, ao somar- las obtemos o valor 1, que corresponde à primeira regra citada anterior6´ maior do que zero e menor do que 1, assim satisfaz também a segunda regra.
Para gerar a variável aleatória no processo de simulação utiliza- se a distribuição de probabilidade uniforme, descrita a seguir.
A distribuição uniforme é uma distribuição cuja faixa de valores aleatórios estão entre duas variáveis a e b.
Sua função densidade de probabilidade é constante dentro de um intervalo de valores da variável aleatória x.
Cada um dos possíveis valores que x pode assumir seguindo esta distribuição tem a mesma probabilidade de ocorrer.
Sua função de densidade é:
Os parâmetros a e b são números reais, sendo que a b e sua faixa de valores vão da à b.
Discutida a geração de variáveis aleatórias, necessárias no processo de simulação empregado neste trabalho, é preciso compreender como o número aleatório gerado irá definir a transição na Cadeia de Markov.
Esta transição é dada então através de uma função de transição, que estabelecerá o próximo estado a ser visitado, baseado nas taxas discretizadas (probabilidades) da matriz correspondente ao modelo que deseja- se simular.
Esta função de transição é definida a seguir.
Em este trabalho, o vetor de probabilidades será obtido simulando o espaço de estados global de uma SAN, o qual corresponde ao estado na Cadeia de Markov que ela representa.
Além disso, vale ressaltar que apenas o espaço de estados atingível (RSS) dos modelos são considerados no processo de simulação, o que reduz consideravelmente o número de estados em determinados modelos, como por exemplo o modelo RS, descrito na Seção 2.2.3.
Assume- se então, um conjunto composta por as probabilidades de transição do mesmo e um gerador de números aleatórios U.
Este gerador é necessário, pois, a transição entre os estados na simulação, ocorre através de uma função de transição (si, U), a qual se baseia na tiragem (sorteio) de um valor uniformemente distribuído entre 0 e 1.
O retorno da função consiste no intervalo que se apresenta a variável resultante do gerador de números aleatórios.
Esta variável irá indicar a transição para o próximo estado da Cadeia de Markov durante a simulação, sendo si o estado corrente.
A função de transição é definida por a expressão 3.1 a seguir e mostra como são realizadas estas transições entre os estados.
Pil, l $= 0 para U Pil l $= 0 r-1 Pir, 1 l $= 0 Para exemplificar a aplicação desta expressão considere a matriz de probabilidades da Tabela 3.1.
A função de transição irá definir para qual estado si, para i $ { 0, 1, 2, ocorrerá a transição de acordo com o intervalo indicado nas expressões 3.2, 3.3 e 3.4.
S0 para U[ 0, 10s2, U) $= s1 para U[ 0, 30,) s2 para U Tendo como base a expressão 3.4 e assumindo a variável aleatória retornada da função U igual a 0, 40, a transição ocorrerá do estado s2 para o estado s1.
Isto acontece pois a mesmo encontra- se entre o intervalo 0, 30 e 0, 55 indicado na expressão.
Definida a função de transição, três técnicas de simulação são empregadas neste trabalho para a resolução de modelos.
De entre elas estão a Forward Simulation, a Backward Coupling Simulation e a Permanence Time Simulation.
Estas técnicas são descritas a seguir.
Em esta técnica de simulação é considerado um estado inicial arbitrário e em seguida são disparadas transições entre os estados do modelo por uma quantidade de vezes pré-estabelecida, de forma a realizar uma trajetória na Cadeia de Markov.
O tamanho ideal desta trajetória ainda é desconhecido, pois pode variar dependendo da cardinalidade do espaço de estados, que corresponde a dimensão das matrizes de transição.
Entretanto esta trajetória deve ser grande o suficiente para que todos os estados da Cadeia de Markov sejam atingidos.
Após várias transições, o estado final da trajetória é coletado como uma amostra da simulação.
A quantidade ideal de amostras a serem coletadas é um valor ainda desconhecido, no entanto quanto maior esse número melhor tende a ser a precisão dos resultados.
O vetor de probabilidades é calculado contabilizando a proporção das amostras coletadas no final da simulação.
A Figura 3.1 ilustra uma trajetória, disparada para a coleta de uma amostra (representada por o estado circulado com traço contínuo) dada uma função de transição estabelecida (si, U),´ importante notar sendo o estado si, da Cadeia de Markov, representado no eixo vertical da figura.
E que nesta técnica a trajetória ocorre a partir de o tempo zero, parando num tempo t pré-definido.
O tempo, no processo de simulação, corresponde ao número de transições (passos) realizadas na Cadeia de Markov, uma vez que, a matriz de taxas de transição é discretizada.
Desta forma, cada unidade de tempo t corresponde a um passo/ transição na Cadeia de Markov.
Vale destacar que a trajetória realizada na Figura 3.1 baseia- se na matriz de probabilidades da Tabela 3.1, respeitando os intervalos das equações 3.2, 3.3 e 3.4 exemplificadas anteriormente.
De Simulação Em as simulações realizadas neste trabalho, o estado inicial foi definido arbitrariamente como 0, conforme pode ser observado na linha 3 do Algoritmo 3.1 e o tamanho da trajetória foi fixado, também arbitrariamente, em 10.000 transições.
A o final da trajetória a amostra é então coletada e contabilizada, conforme visto na linha 8 do algoritmo.
Esta técnica é baseada no algoritmo Coupling from the Past (CFTP) proposto por Propp e Wilson em 1996.
A técnica foi revolucionária na é poca dentro de o campo de aplicação na física para resolver alguns dos problemas de simulação.
Em esta técnica não é necessário escolher um estado inicial uma vez que várias trajetórias são disparadas em paralelo partindo uma de cada estado do modelo.
Outra vantagem é que não é necessário definir o tamanho da trajetória, pois a amostra é coletada quando todas as trajetórias (disparadas em paralelo) se encontram num mesmo estado no tempo zero de simulação.
Isso ocorre, pois as mesmas são disparadas &quot;de o passado», observando os estados predecessores, ou seja, os estados que trouxeram ao estado atual.
Esta técnica destaca- se também, por permitir a coleta de amostras perfeitamente distribuídas de acordo com a distribuição estacionária do processo Markoviano, não produzindo amostras tendenciosas.
A Figura 3.2 ilustra este processo, apresentando as trajetórias disparadas a partir de cada estado, no passado, parando no tempo 5 de simulação.
O eixo horizontal, de cada sub-figura, representa o tempo que corresponde ao número de transições necessárias até que todos os estados (eixo vertical) se encontrem no tempo zero.
As linhas contínuas apresentam as trajetórias que levam ao mesmo estado, enquanto as linhas pontilhadas correspondem à s trajetórias que não se encontram no mesmo estado.
O Algoritmo 3.2, ajuda a compreender este proavel por armazenar o estado referente a transição de cada cesso, sendo o vetor da linha 9 o respons´ trajetória.
Quando este vetor armazenar transições para o mesmo estado em todas as suas posições, o mesmo será então a amostra a ser coletada, conforme a linha 13 do algoritmo.
Algorithm 3.2: Representação do algoritmo para a técnica Backward Coupling Simulation Esta técnica, ao contrário de as anteriores, consiste apenas na execução de uma única trajetória.
Em ela, as amostras de estados são coletadas após cada transição, sendo assim é contabilizada a quantidade de vezes que um estado é visitado na Cadeia de Markov.
Observando a Figura 3.1, o número de transições para o estado 1 ocorre duas vezes, enquanto que para o estado 2 ocorre apenas uma vez, por exemplo.
De a mesma forma que nas soluções anteriores, para que uma solução estacionária seja aproximada, o número de amostras deve ser significativamente grande para que todos os estados sejam visitados uma grande quantidade de vezes para que as proporções possam ser calculadas.
O Algoritmo 3.3 facilita essa análise, sendo que na linha 6 do algoritmo, o estado visitado é incrementado na posição correspondente no vetor, após cada transição, da linha 5.
Cada estado visitado, corresponde então a uma amostra da simulação.
Algorithm 3.3: Representação do algoritmo para a técnica Permanence Time Simulation Testes Realizados e Resultados Obtidos Para avaliar a precisão da simulação, desenvolveu- se um simulador para as três técnicas mencionadas na Seção 3.3.
O simulador recebe como entrada uma matriz de transição e armazena num arquivo de saída o vetor de probabilidades resultante.
Esta matriz de transição corresponde à Cadeia de Markov equivalente aos modelos SAN apresentados na Seção 2.2.3.
Em este trabalho, para o modelo ASP foi utilizado K $= 2 e P $= 2, ou seja, fila com capacidade de dois clientes e dois padrões de serviço, resultando num PSS e RSS de 4 × 2 $= 162 estados.
Para o modelo FAS foi utilizado o número de servidores N igual a 9, o que resulta num PSS e RSS de 29 $= 512 estados.
Para o modelo RS utilizou- se 10 processos P e 5 recursos R, sendo seu PSS igual a 210 × $= 6.144 e o RSS igual a 638.
Assim como a obtenção do RSS, as conversões dos modelos SAN em Cadeias de Markov foram realizadas utilizando a ferramenta PEPS, cujos arquivos de entrada de cada modelo podem ser observados no Anexo A. Esta ferramenta disponibiliza a matriz equivalente num formato HBF (HarwellBoeing Format), o qual armazena apenas os valores não nulos com seus respectivos índices na matriz.
Este formato é utilizado para reduzir o tamanho do arquivo gerado, já que normalmente a matriz de transição é bastante esparsa, sendo desnecessário armazenar valores nulos.
Além de a matriz de transição obtida a partir de os modelos descritos anteriormente é passado ao umero de amostras desejadas e a técnica de simulação a ser utilizada.
A análise dos resultados é feita então através da comparação do vetor resultante das simulações com o vetor da solução analítica obtido com o Método da Potência, implementado na ferramenta PEPS.
Os testes foram realizados resolvendo o mesmo modelo em ambas as soluções para que as comparações pudessem ser realizadas.
Estes testes e seus respectivos resultados são apresentados a seguir.
Este teste varia a quantidade de amostras coletadas ao aplicar cada técnica de simulação, e posteriormente compara seus resultados com os resultados obtidos com o Método da Potência, para os modelos ASP, FAS e RS, já descritos.
De posse desses resultados foi calculado o erro relativo de cada uma das probabilidades contidas (s) (a) no vetor i resultante da simulação, com a respectiva probabilidade do vetor i do método iterativo (Método da Potência), as quais correspondem a probabilidade de permanência em cada estado do modelo.
O erro relativo i foi então calculado de acordo com a equação 4.1, sendo i cada um desses estados.
Feito isso, foi efetuada a média aritmética x~ entre todos estes erros relativos (i) conforme a expressão 4.2 (sendo x~ × 100 o erro percentual) e destacado também o erro relativo máximo max entre eles.
Esta operação foi feita com o resultado de todas as técnicas de simulação implementadas e foi repetida com os três modelos SAN mencionados (ASP, FAS e RS).
Os resultados são apresentados nas Figuras 4.1, 4.2 e 4.3, sendo que o eixo x de cada gráfico corresponde a quantidade de amostras coletadas em cada execução e o eixo y o erro relativo.
A linha pontilhada representa o erro relativo máximo de entre os estados e as barras correspondem ao erro relativo médio entre eles.
Note que conforme aumenta o número de amostras (eixo x) coletadas na resolução dos modelos, diminui o erro relativo médio e máximo dos resultados da simulação, praticamente em todas as técnicas apresentadas.
Quanto a a precisão das mesmas, pode- se dizer que apresentam melhor eficiência a partir de os resultados com 10.000.000 de amostras.
Isso pode ser notado uma vez que o erro relativo percentual do modelo ASP, considerando todas as técnicas, varia entre 0, 79% (técnica Forward Simulationtécnica Permanence Time Simulation), enquanto que o erro relativo máximo varia entre 6, 4% (técnica Forward Simulation) e 10, 3% (técnica Permanence Time Simulation).
Um comportamento importante a ser destacado neste modelo é que as técnicas de simulação Backward, Forward e Permanence Time apresentam pouca redução do erro para resultados com mais de 10.000.000 de amostras, ou seja, a redução da média do erro relativo é praticamente nula quando a ordem de grandeza das amostras coletadas aumenta mais do que isso.
Já para o segundo modelo analisado (FAS) pode ser observado um comportamento equilibrado em relação a todas as técnicas aplicadas.
No entanto os erros relativos médios e máximos do modelo FAS são maiores que no modelo ASP, isso pode ser justificado, pois o RSS do modelo FAS é de 512 estados, contra 162 do modelo ASP.
Acredita- se que devido a esta diferença no número de estados seria necessário uma quantidade maior de amostras do modelo FAS para atingir a mesma precisão obtida com o modelo ASP.
Com relação a o modelo RS, mesmo este apresentando um RSS ligeiramente maior que o do modelo FAS, de 636 estados, o mesmo apresenta erros relativos médios e máximos bem menores.
Um dos possíveis motivos que podem ter acarretado neste resultado é a influência das taxas de transição que os compõe.
Em este sentido, a ocorrência de eventos raros, ou com probabilidades muito baixas de ocorrência, pode ser a causa de discrepâncias mais acentuadas nos resultados de alguns modelos.
A simulação de modelos com eventos de ocorrência rara é um tema de pesquisa em aberto, com poucos trabalhos relacionados.
No entanto, mesmo esta característica tendo relevância nos resultados, a análise da mesma será proposta como um estudo futuro, sendo que não foram realizados experimentos que avaliem esta propriedade no momento.
Uma característica bastante importante que não pode deixar de ser mencionada é a precisão da técnica Backward Coupling Simulation.
Esta técnica de simulação é considerada uma das mais precisas, por coletar amostras mais perfeitamente distribuídas.
Porém, os resultados mostraram que a mesma apresenta ganhos pouco significativos comparada com as demais, onde em praticamente todos os casos apresenta erro relativo médio maior que a técnica Forward Simulation, com exceção do primeiro ponto do gráfico do modelo FAS (Figura 4.2) e do quinto ponto do gráfico do modelo RS (Figura 4.3).
Vale lembrar que esta última necessita a escolha de um estado inicial e a definição do tamanho da trajetória, ao contrário de a Backward Coupling Simulation.
Além disso, se compararmos o consumo de memória, a técnica Backward Coupling Simulation necessita armazenar trajetórias partidas de todos os estados do modelo, fato que acarreta um consumo elevado, quando comparada com as outras duas.
Alguns estudos recentes vêm sendo realizados para reduzir esse consumo de memória e acelerar a coleta das amostras através em, essa propriedade sóé de propriedades de monotonicidade, conforme visto em Vincent.
Poráplicada para uma determinada classe de modelos, através de técnicas não triviais.
Destaca- se também que os erros apresentados nos gráficos foram calculados a partir de uma única execução, sendo que na simulação este resultado pode apresentar determinada variância quando executado outras vezes.
Isso ocorre devido a influência na escolha da semente utilizada no gerador de números aleatórios.
Para verificar esta variação, foi realizado o intervalo de confiança com a aplicação da técnica Permanence Time Simulation (escolhida por apresentar menor tempo de processamento), para várias execuções, conforme será apresentado a seguir.
O Intervalo de confiança é utilizado para verificar a variação de cada execução da simulação, de forma a observar o quanto o resultado de uma execução difere de outro.
Em outras palavras, o intervalo de confiança dá a margem de erro da simulação, necessitando para isso a realização de várias execuções.
A variação entre os resultados ocorre, pois na simulação as variáveis que disparam as transições entre um estado e outro são variáveis aleatórias definidas por distribuições de probabilidades, conforme visto na Seção 3.1.
Computacionalmente, estas variáveis são definidas por funções que recebem como parâmetro uma semente, que corresponde a um determinado valor real passado como parâmetro para a função.
O uso de diferentes sementes faz com que haja variações entre um resultado e outro da simulação, pois serve como ponto inicial para a geração das variáveis aleatórias.
Por este motivo, estas variáveis são na verdade chamadas de pseudoaleatórias, visto que não são totalmente aleatórias, justamente por serem geradas através de uma função, por o computador.
Para mostrar este comportamento, foram realizadas 50 execuções dos modelos ASP, FAS e RS, utilizando a técnica Permanence Time Simulation.
Feito isso, foi calculado o intervalo de confiança de 95% para cada estado em todos os modelos.
Em cada execução foi passado como parâmetro uma semente diferente ao gerador de números aleatórios e a média aritmética dos intervalos obtidos são apresentados nos gráfico da Figura 4.4.
O eixo x dos gráficos apresenta a quantidade de amostras coletadas em cada uma das 50 execuções da simulação, ou seja, 50 execuções com 10.000 amostras, 50 execuções com 100.000 amostras e assim por diante, para cada modelo.
O eixo y mostra a variação do erro médio relativo entre as execuções, onde nota- se a significativa redução do mesmo à medida que a quantidade de amostras aumenta no eixo x, principalmente para o modelo FAS.
Constata- se com isso que a média dos intervalos de confiança realizada entre os estados de cada modelo, torna- se mais precisa, conforme essas amostras aumentam, visto que um intervalo de confiança menor corresponde a pouca variação entre os resultados de diferentes execuções.
Este fato aumenta a credibilidade das simulações conforme aumenta o número de amostras coletadas uma vez que a semente utilizada no gerador de números aleatórios passa a influenciar cada vez menos no resultado.
Como a mesma quantidade de amostras foi coletada para as demais técnicas, acredita- se que a variação apresentada nos gráficos anteriores seja a mesma para todas elas, não sendo necessário repetir o teste para cada uma novamente.
Além disso, isso seria inviável neste momento, visto que as técnicas Backward Coupling Simulation e Forward Simulation apresentam um tempo de processamento significativamente superior à técnica Permanence Time Simulation, conforme será apresentado na Seção 4.5.
Uma das grandes desvantagens da técnica Forward Simulation é ter que definir o tamanho da trajetória, conforme visto na Seção 3.3.1 e também ter que definir o estado inicial para o disparo da mesma.
Assim sendo, a escolha de uma trajetória muito grande aumenta a carga computacional acarretando num tempo de processamento elevado, enquanto que uma trajetória muito pequena, pode comprometer a precisão dos resultados.
Para avaliar este impacto, foram realizados experimentos variando o tamanho da mesma e também seu estado inicial, na coleta de 1.000.000 de amostras do modelo ASP com K $= 4 e P $= 4, totalizando um RSS de 2.500 estados.
Os gráficos da Figura 4.5 mostram o erro relativo médio (eixo y), variando o tamanho das trajetórias de acordo com o eixo x, sendo que o gráfico (a) da figura mostra os resultados com as trajetórias iniciando todas no estado 0.
Em este gráfico pode- se dizer que o estado inicial fixo apresenta influência apenas para as trajetórias de tamanho 10 e 100, visto que o erro estabiliza conforme ela cresce, caracterizando certa estacionariedade.
Este comportamento permite deduzir que o estado inicial torna- se irrelevante conforme o tamanho da trajetória aumenta, assim como o ganho em relação a precisão mantêm- se praticamente nulo após aumentar- la mais do que 1.000.
Pensando numa forma de reduzir este erro foram realizados experimentos com diferentes estados iniciais.
Para definir- los foi disparada a primeira trajetória partindo do estado 0 e a trajetória seguinte utilizou a amostra coletada por a primeira (que corresponde ao último estado visitado) como sendo seu estado inicial e assim sucessivamente.
O resultado é mostrado no gráfico (b) da Figura 4.5 e apresenta um erro relativo médio significativamente menor que o apresentado no gráfico (a) para trajetórias menores que 1.000.
Embora este erro reduza rapidamente conforme a trajetória aumenta, esta primitiva pode minimizar o erro quando a mesma for fixada num tamanho muito pequeno, erroneamente.
Sugere- se, no entanto, que seu tamanho seja no mínimo maior que o RSS do modelo, caso contrário se todas as trajetórias partirem do estado zero, como no caso de o gráfico (a) da figura, alguns estados podem nunca ser visitados (atingidos).
Outra questão que permanece em aberto tanto para a técnica Forward Simulation, quanto para as demais, é a quantidade de amostras necessárias para se obter uma determinada precisão.
Esta questão é difícil de ser respondida, porém, algumas primitivas podem ser utilizadas para auxiliar nesta inferência, mesmo que superficialmente.
Uma de elas é analisando o intervalo de confiança apresentado na Seção 4.2, sendo que um número de amostras que apresente um intervalo de confiança pequeno pode ser um bom palpite para a obtenção de um resultado preciso, definido de acordo com a necessidade do usuário.
Outra primitiva que permite auxiliar na definição da quantidade de amostras éa distribuição do erro, conforme será apresentado a seguir.
Os testes realizados nesta seção também demonstram a influência na quantidade de amostras coletadas na simulação, apresentado a distribuição do erro relativo com os estado do modelo simulado.
Os gráficos da Figura 4.6, mostram essa distribuição para o modelo FAS com N $= 9, aplicando a técnica Backward Coupling Simulation escolhido por apresentar erros relativos médios maiores que os demais modelos, conforme visto na Seção 4.1.
Cada gráfico da figura apresenta a distribuição do erro relativo com a coleta de uma quantidade diferente de amostras, sendo o eixo x os estados do modelo e o eixo y o erro relativo em escala logarítmica ordenados do menor para o maior para melhorar a visualização.
Os resultados permitem observar uma característica bastante interessante aproximadamente entre os estados 100 e 470 no gráfico (a) e entre os estados 240 a 460 do gráfico (b) à respeito do erro que é exatamente igual a 1, ou seja, 100% enquanto que no gráfico (c), (d), (e) e (f), isso não acontece.
Esse comportamento se explica, pois a pequena quantidade de amostras coletadas nos gráficos (a) e (b) de 10.000 e 100.000, respectivamente, não é suficiente para que amostras em todos os estados do modelo sejam coletadas.
Este fato faz com que os mesmos sejam considerados, erroneamente, como inatingíveis de forma que a probabilidade contida no vetor resultante seja igual a zero acarretando no erro de 100% caracterizado por a linha constante observada nos gráficos (a) e (b) da Figura 4.6.
Esta análise permite concluir que próxima da solução analítica.
Um fator importante que merece ser analisado também é o tempo gasto para se conseguir um resultado preciso.
Conforme observamos até o momento, o aumento do número de amostras coletadas reduz cada vez mais o erro da simulação, até que se atinja uma solução próxima da estacionária.
Esta redução pode ser notada analisando o erro relativo médio do gráfico (a), representado por a linha contínua horizontal e analisando o erro relativo médio do gráfico (f), sendo ele reduzido na ordem de grandeza.
Contudo, o custo computacional referente a o tempo de processamento pode ser bastante elevado e à s vezes dependendo da necessidade do usuário, o mesmo pode não compensar, quando o erro apresenta pouca redução mesmo aumentando em ordens de grandeza a quantidade de amostras coletadas.
Isso pode ser observado analisando o erro médio relativo do gráfico (g) com o do gráfico (f), onde o mesmo diminuiu 0, 0494 -- 0, 0410 $= 0, 0084, ou seja 0, 84%.
Este erro deve, entretanto, ser avaliado por o usuário para que este julgue a relevância dessa redução ao tirar conclusões do sistema que está sendo modelado.
Coupling Simulation neste trabalho aplicadas aos modelos ASP, FAS e RS.
Antes de analisar- los é importante mencionar que foi realizado um estudo na paralelização das técnicas de simulação, utilizando a biblioteca Message Passing Interface (MPI) e o paradigma mestreescravo, bastante comum em sistemas distribuídos.
Em este paradigma, existe um nodo (processo) que divide as tarefas entre todos os nodos escravos e por fim recebe e processa os resultados retornados por os mesmos, através de troca de mensagens.
Em a simulação, este nodo mestre divide as amostras a serem coletadas entre cada nodo escravo, sendo que cada um de eles processa por um tempo proporcionalmente distribuído, juntamente com o nodo mestre.
Por fim, cada nodo escravo envia um vetor contendo os resultados obtidos novamente ao nodo mestre, onde este, por sua vez, acumula o resultado num único vetor e calcula as probabilidades do modelo.
Esta característica permite com que a paralelização se torne bastante vantajosa pois o fato de haver pouca comunicação entre os nodos (escravos com o mestre e vice-versa) torna, consequentemente, o tempo de comunicação bastante pequeno, uma vez que são transmitidos na rede apenas os vetores com as amostras de estados por cada nodo escravo.
Este fato também garante um speedup muito próximo de o ideal, conforme será apresentado mais adiante.
Estes testes foram realizados para avaliar o comportamento paralelo em cada uma das técnicas propostas a fim de acelerar a coleta dos resultados utilizados nas análises deste trabalho.
O gráfico (a) da Figura 4.7 mostra o speedup, definido por o quociente da divisão entre o tempo sequencial e o tempo paralelo, de acordo com a equação 4.3, sendo p o número de processadores, T1 o tempo de execução do algoritmo sequencial e Tp o tempo de execução do algoritmo paralelo com p processadores.
Sp $= Tp afico (a) corresponde ao número de processadores e o eixo y correAssim sendo, o eixo x do gr´ sponde ao speedup resultante.
O gráfico (b) da figura, por sua vez, mostra o tempo de processamento de cada técnica, com os respectivos p processadores no eixo x.
Vale destacar que estes tempos foram adquiridos com a coleta de 1.000.000 de amostras com cada técnica e as simulações foram realizadas no cluster Cerrado da PUCRS.
Este cluster é composto por cinco nodos bi-processados, com tecnologia Intel Itanium com frequência de 1.5 GHz e 2 GB de memória.
Conforme pode ser observado nos gráficos, o speedup apresentou resultados ideais, resultando num o´ timo desempenho com a paralelização de todas as técnicas apresentadas.
A redução do speedup, no entanto, para as técnicas Permanence Time Simulation conforme pode ser observado com a utilização de mais de 6 processadores, ocorre pois a mesma é muito rápida, para a coleta de apenas 1.000.000 de amostras.
Número este, que para esta técnica é de baixa carga computacional, visto que o tempo de processamento foi praticamente desprezível, ainda mais quando comparados com as técnicas Forward Simulation e Backward Coupling Simulation, as quais apresentam um tempo de várias minutos.
Isso pode ser constatado observando o tempo despendido com dois processadores, visto no gráfico (b) para a técnica Forward Simulation o qual foi de 1.020 segundos, ou seja, aproximadamente 17 minutos de processamento.
Os gráficos seguintes, referentes à s Figuras 4.8, 4.9 e 4.10 apresentam um comparativo entre as três técnicas de simulação em relação a o tempo de processamento, utilizando 10 processadores durante a execução dos modelos ASP, FAS e RS respectivamente.
Time este tempo foi cerca de 21 segundos.
O comportamento dos gráficos permite prever também o tempo necessário para a coleta de amostras com uma ordem de grandeza maior para qualquer modelo apresentado.
Isso é fácil de ser inferido, pois o mesmo também aumenta numa ordem de grandeza, que no caso de o modelo ASP aplicando à técnica Backward seria de aproximadamente 1.169.120 segundos para coletar 10.000.000.000 de amostras.
O tempo de processamento das técnicas mencionadas ainda é uma ã rea que pode ser amplamente explorada uma vez que, além de a paralelização, outros métodos podem ser acrescentados a cada uma de elas para agilizar a coleta das amostras.
De entre eles pode ser mencionado o método Aliasing, caracterizado por definir o próximo estado de forma mais eficiente.
Técnica Proposta Conforme observado anteriormente, mesmo aumentando em ordens de grandeza o número de amostras coletadas durante o processo de simulação, obtêm- se ganhos pouco significativo em termos de precisão, após determinada quantidade das mesmas.
Em virtude de isso e na busca de resultados mais precisos, foi realizado um estudo no Bootstrap Method, proposto por Efron.
Este método consiste numa técnica utilizada para resolver uma grande variedade de problemas de estimativa em diversas ã reas, sendo aplicada no campo de estatística para derivar estimativas de erro padrão e intervalo de confiança para estimadores de parâmetros complexos da distribuição.
A essência dessã de técnica é que na ausência de qualquer conhecimento prévio sobre uma determinada população tamanho infinito, a distribuição dos valores encontrados numa amostra aleatória de tamanho n~ e (extraída dessa população, i.
e,)´ a melhor abordagem para obter a distribuição da mesma.
Em outras palavras, a população infinita observada em apenas n valores da amostra, cada um, é usada para modelar a população desconhecida real.
O conjunto de valores de uma nova amostra K, também de tamanho n(| K| $ |), é obtido ex~ conforme clusivamente a partir de a primeira amostra, sem que hajam novos valores da população,´ a amostragem pode ser observado na Figura 5.1.
A principal característica da técnica Bootstrap e com reposição, i.
e, os valores que compõem a amostra podem repetir- se durante tiragens de valores.
Para exemplificar o uso dessa técnica considere que se deseja encontrar a média de altura dã população mundial.
Como seria inviável obter esses valores para toda a população (conjunto), apenas uma amostra desta população é considerada (subconjunto).
De posse de são realizadas z reamostragens, que correspondem então ao número de bootstraps, conforme visto na Figura 5.1, onde de, através de tiragens pseudo-aleatórias podendo haver repetições de valores.
A média x¯ K da altura da população é calculada por as médias x¯ K 1, x¯ Kz de cada bootstrap.
A técnica Bootstrap objetiva uma melhor precisão para a média x¯ K do que a média x Além disso, o aumento do número de bootstraps pode reduzir os efeitos causados por erros (ruídos) dos geradores de números pseudo-aleatórios que os compõem.
A proposta de utilização desta técnica na simulação de modelos Markovianos foi adaptada de forma que bootstraps fossem integrados à função de transição.
Para isso, gera- se um valor real, uniformemente distribuído entre 0 e 1, e a função de transição (si, U) determina o próximo estado do modelo a ser visitado.
A sequência de estados visitados compõe a trajetória da simulação, a qual corresponde à amostra definida anteriormente.
A Figura 5.2 ilustra essa trajetória, sendo que a cada estado visitado são realizadas tiragens de valores pseudo-aleatórios que irão definir os estados que serão coletados em cada bootstrap Ki.
O número de tiragens é igual a quantidade de valores da amostra, ou seja, n valores, que por sua vez é também igual ao tamanho da trajetória da simulação.
As tiragens são realizadas de acordo com a geração de valores inteiros uniformemente distribuídos entre 0 e n -- 1 através do gerador U para cada bootstrap, ou seja, z vezes.
Em a Figura 5.2, Ti (onde i) representa as n tiragens para cada bootstrap Ki.
Logo, as n × z tiragens ocorrem a cada passo/ transição da trajetória (no modelo) e os valores pseudo-aleatórios, retornado por o gerador U, são comparados com uma constante qualquer escolhida arbitrariamente também entre 0 e n -- coletado no bootstrap correspondente (Kb).
Este procedimento se repete para todos os bootstraps até que o tamanho (n) pré-estabelecido da trajetória seja atingido.
Em a Figura 5.2,|¯ b| representa este procedimento a cada transição realizada na simulação.
A o final da simulação (i.
e, quando toda a trajetória for percorrida), o vetor de probabilidades, o qual corresponde à solução do modelo com suas respectivas probabilidades de permanência em cada estado, é calculado através das médias das probabilidades de permanência encontrada para os estados em cada bootstrap.
Para avaliar a influência na redução de n para n¯ tiragens, foi calculada a probabilidade de um mesmo valor ser coletado n¯+ 1 vezes durante n tiragens.
Se essa probabilidade for significativa, significa que o número de tiragens n¯ não é suficiente, visto que uma chance alta de um mesmo valor repetir- se mais de n¯ vezes, por exemplo, irá afetar a precisão dos resultados.
Este cálculo é realizado baseado ¸ão -- o qual consiste em no problema clássico conhecido como The Birthday Problem -- Equac descobrir as chances de num conjunto de pessoas, escolhidas aleatoriamente, existirem pares que façam aniversário na mesma data.
Algorithm 5.1: Representação do algoritmo para a técnica Bootstrap Simulation for t $= 1 to n do for b $= 1 to z do for c $= 1 to n¯ do for b $= 1 to z do for i $= 1 te o| RSS| do b¯ b K for i $= 1 te o| RSS| do for b $= 1 to z de o+ x¯ b calcula Em o Algoritmo 5.1, note que entre as linhas 1 e 4 são realizadas as inicializações das variáveis e vetores utilizados no emprego da técnica no contexto de simulação.
As tiragens armazenadas nos bootstraps percorrendo toda a trajetória de tamanho n ocorrem da linha 6 até a linha 12.
Entre as linhas 13 e 18, são calculadas as probabilidades dos estados do modelo em cada bootstrap.
E, finalmente, entre as linhas 19 e 22, são calculadas as médias das probabilidades de cada estado no vetor a partir de os bootstraps.
Após a adaptação do método bootstrap como uma nova técnica de simulação, foram repetidos alguns dos experimentos apresentados no Capítulo 4 para avaliar o comportamento da mesma em relação a a precisão e ao tempo de processamento.
Os resultados são apresentados a seguir.
Para verificar o comportamento da técnica Bootstrap Simulation aplicada à resolução de modelos Markovianos, a mesma foi utilizada na simulação dos modelos ASP, FAS e RS.
Uma questão ainda em aberto para a utilização desta técnica é definir um número adequado de bootstraps, no entanto para os gráficos apresentados nesta seção, este número foi fixado em 10 de forma arbitrária.
Os resultados com a utilização desta nova técnica podem ser analisados observando as Figuras e as comparações com a nova técnica proposta.
Observando os gráficos da primeira figura, referente a o modelo ASP, nota- se que os resultados demostraram maior precisão com a redução do erro médio a partir de a coleta de 10.000.000 de amostras.
Paralelo a ela as demais técnicas apresentaram reduções muito pequenas no erro relativo médio e máximo, apresentando certa estacionariedade, enquanto que a técnica proposta continua obtendo ganhos significativos.
Se observarmos o último ponto do gráfico (c), referente a técnica permanence time e o gráfico (d), referente a técnica bootstrap, o erro que era de 0, 61% cai para 0, 09%, sendo que a última apresenta uma linha decrescente constante em relação a redução tanto do erro relativo médio como do erro relativo máximo, ao contrário de as demais.
Comparando os resultados obtidos para o modelo FAS (Figura 5.4), obteve- se erros relativos menores com a técnica Bootstrap Simulation, principalmente para os dois últimos pontos do gráfico (d) (1 e+ 08 e 1e+ 09 amostras).
Já para o primeiro ponto (10.000) os resultados apresentaram um erro máximo maior, no entanto conforme visto na Seção 4.2, este valor pode apresentar variações significativas, devido a influência na semente utilizada no gerador de números aleatórios, uma vez que a quantidade de amostras é bastante pequena.
Com relação a o modelo RS, a técnica Bootstrap Simulation apresentou resultados bastante parecidos com as demais, no entanto, mesmo não obtendo ganhos a mesma demonstrou equivalência em relação a o erro obtido entre todas elas.
Desta forma pode- se dizer que a técnica Bootstrap Simulation apresenta em geral boa precisão, uma vez que os resultados obtidos com o maior número de amostras utilizadas foram mais precisos que as demais técnicas, sendo que no pior caso a mesma obteve precisão praticamente equivalente, como no caso de o modelo RS.
Outro fator que merece destaque ao utilizar- la é em relação a escolha do número de bootstraps e ao tempo de processamento, conforme será apresentado a seguir.
Se observarmos os resultados dos demais modelos, não nota- se nenhuma diferença expressiva, uma vez que há pouca variação entre os mesmos.
Acredita- se que caso o gerador de números aleatórios não esteja bem calibrado a variação dos bootstraps seja mais significativa.
Vale destacar que quanto maior a quantidade de bootstraps utilizado na simulação desta técnica, maior é o tempo de processamento da mesma.
Em este sentido vale a pena avaliar a influência deste valor uma vez que um número muito de grande de bootstraps pode acarretar apenas em desperdício de tempo, sem trazer ganhos nos resultados.
Estes tempos serão abordados com mais detalhes na próxima seção.
Para analisar os tempos de processamento com diferentes bootstraps, conforme visto anteriormente, são apresentados os gráficos da Figura 5.9, sendo o eixo x o tamanho das amostras e o eixo y o tempo, em segundos.
Note que o tempo de processamento com 50 bootstraps é significativamente maior, aumentando numa ordem de grandeza em relação a 4 bootstraps.
Este resultado reforça a importância dessa escolha uma vez que um número excedente de bootstraps pode acarretar num custo computacional muitas vezes maior.
Isso foi feito para que a técnica Bootstrap pudesse ser comparada com as demais, sendo os mesmos apresentados na Tabela 5.2, com o tempo sequencial real da técnica Bootstrap entre parênteses.
Um estudo aprofundado na paralelização da mesma será proposto num trabalho futuro.
Este trabalho apresentou o desempenho, em termos de precisão, de diferentes técnicas de simulação conhecidas aplicadas à resolução de modelos Markovianos e propôs a utilização do método Bootstrap adaptado a uma nova técnica de simulação, com o objetivo de melhorar a precisão dos resultados.
Durante a realização dos testes, foi observado que a simulação é uma solução bastante onerosa em termos de tempo de processamento, sendo a mesma alvo constante de aperfeiçoamentos para que resultados com grandes modelos possam ser obtidos num tempo hábil.
Utilizar- se de paradigmas de paralelização ao implementar as técnicas de simulação é uma alternativa claramente viável, pois reduz significativamente este tempo conforme os gráficos de speedup apresentados neste trabalho, uma vez que as amostras podem ser coletadas de forma independente.
Mesmo assim, outras primitivas podem ser empregadas, como a utilização do método Aliasing para acelerar a escolha das transições no modelo.
Além disso, o estudo de propriedades de monotonicidade podem auxiliar na redução das trajetórias disparadas durante a coleta das amostras, para a técnica Backward Coupling Simulation.
Um fator que merece destaque é o tempo de processamento das técnicas Permanence Time Simulation e Bootstrap Simulation, por propiciarem melhor desempenho em relação a a s demais.
Foi mostrado também que de posse do tempo despendido com a coleta de um determinado número de amostras é possível inferir o tempo necessário para a coleta de quantidades maiores das mesmas, uma vez que este tempo aumenta nas mesmas proporções.
Quanto a precisão dos resultados obtidos com a aplicação das diferentes técnicas de simulação, que corresponde ao foco principal deste trabalho, conclui- se que os mesmos apresentam melhor precisão com a técnica Bootstrap Simulation, levando em consideração principalmente os índices com a coleta de amostras maiores do que 1.000.000 para os modelos analisados.
Com relação a técnica Backward, acredita- se que ela demonstre resultados mais precisos para a coleta de uma quantidade pequena de amostras, por ser caracterizada como não tendenciosa, embora para amostras maiores não tenha superado as demais, nos experimentos deste trabalho.
Com relação a técnica Forward, a necessidade de ter que definir o tamanho de suas trajetórias, torna difícil sua calibragem.
Além disso, ter que escolher um estado inicial é outro problema apresentado por ela, porém, constatou- se que utilizando estados iniciais variáveis, este problema é amenizado tornando este fator pouco impactante na precisão.
Quanto a técnica Permanence Time, para grandes quantidades de amostras observou- se bom equilíbrio entre tempo de processamento e precisão dos resultados.
Este trabalho evidenciou particularidades de técnicas de simulação conhecidas apontando as principais vantagens e desvantagens das mesmas.
Com isso, os resultados aqui apresentados servirão como base para o emprego de novos métodos e técnicas de aperfeiçoamento da simulação que possam melhorar ainda mais a precisão dos resultados e reduzir seu tempo de processamento.
A principal contribuição deste trabalho foi propor a adaptação do método Bootstrap como uma nova técnica de simulação aplicada à resolução de modelos Markovianos.
Esta técnica, por sua vez, apresentou resultados satisfatórios reduzindo, em alguns casos, o erro relativo médio e máximo comparado com a solução iterativa.
Além disso uma abordagem paralela para as técnicas conhecidas, demonstrou sua viabilidade na implementação das mesmas.
Alguns trabalhos futuros podem ser motivados com a realização deste trabalho para auxiliar na resolução de modelos Markovianos através de soluções por simulação.
Alguns de eles são descritos a seguir:·
Paralelização da Técnica Bootstrap Simulation -- o fato das técnicas de simulação conhecidas terem demonstrado speedup lineares permite concluir que esta primitiva é uma boa prática para a implementação das mesmas.
Por este motivo, o estudo na paralelização da técnica Bootstrap Simulation é um dos trabalhos futuro à esta dissertação;·
Modelos com Eventos Raros -- outro trabalho futuro é com relação a uma análise aprofundada na simulação de modelos que apresentam eventos de ocorrência rara, determinados por frequências muito pequenas do disparo das transições.
Além de a variação do RSS dos modelos, acredita- se que as taxas também devem acarretar variações na precisão dos seus resultados.
Desta forma, a simulação destes modelos pode demonstrar comportamentos ainda não observados até o momento;·
Categorização dos Modelos -- como mencionado, o estudo de modelos com taxas que determinam transições muito raras podem apresentar comportamentos diferentes e influenciar na precisão dos resultados.
Além disso, o modelo RS, ao contrário de os demais utilizados neste trabalho, não demonstrou diferenças significativas na precisão, mesmo com a utilização de diferentes técnicas de simulação.
Dito isso, identificar possíveis classes de modelos para que possa ser realizada uma categorização dos mesmos ao aplicar diferentes técnicas de simulação, ou até mesmo propor uma nova técnica que apresente melhor eficiência em termos de precisão para elas é um trabalho futuro a ser realizado;·
Simulação do Formalismo SAN -- este trabalho propôs um estudo na precisão dos resultados orias nas Cadeias de Markov de modelos SAN.
Outro trabalho futuro é propor simulando trajetúma solução por simulação, específico para este formalismo, para avaliar o desempenho em relação a o consumo de memória aplicado a resolução do mesmo.
Além disso, poderiam ser aplicadas todas as técnicas de simulação aqui apresentadas e também empregados métodos conhecidos para agilizar o tempo de processamento e reduzir ainda mais o consumo de memória, através de outros formatos de armazenamento.
