Em este trabalho, é apresentado um modelo para a geração de animações faciais personalizadas em avatares.
O modelo inclui duas abordagens de controle paramétrico da face do avatar:
Baseado em scripts de ações faciais em alto-nível ou usando a técnica de Animação Dirigida por Performance (PDA).
A abordagem de controle por script permite a geração de animações com expressões faciais, fala sincronizada e comportamento dos olhos interativamente por a descrição em alto-nível dessas ações faciais.
A abordagem de controle por Animação Dirigida por Performance permite refletir as expressões faciais do usuário em seu avatar, considerando como entrada um pequeno conjunto de pontos característicos providos por algoritmos de Visão Computacional.
O modelo é baseado no padrão MPEG-4 de Animação Facial, estruturado usando uma hierarquia dos parâmetros de animação, proposta para a abordagem de PDA.
Essa estratégia permite a geração de animações nas regiões da face onde não houver dados supridos por Visão Computacional.
Para deformar a face, usa- se Funções de Base Radial e Funções de Cosseno.
Essa abordagem é computacionalmente barata e permite a animação dos avatares em tempo-real.
Foi desenvolvido um framework de animação facial extensível capaz de animar facilmente faces parametrizadas de acordo com o padrão MPEG-4 em aplicações interativas.
Também foram desenvolvidas algumas aplicações interativas como &quot;estudo de caso «das potencialidades do modelo e realizada uma avaliação preliminar com usuários, de maneira a avaliar qualitativamente as abordagens de controle.
Os resultados mostram que o modelo proposto pode gerar animações variadas e com resultados visuais satisfatórios.
Palavras-chave: Animação Facial, Controle de Avatares, Conrole por Script, Animação Dirigida por Performance A. Conjunto de Parâmetros de Animação B. Mapeamento Fonema--Visema Hoje em dia, cresce o número de aplicações de Computação Gráfica (CG) e Realidade Virtual (RV) que utilizam humanos virtuais, cuja exigência é de modelos cada vez mais sofisticados e realistas.
Entre elas, pode- se citar aplicações de mundos virtuais (como o Second Life1 e o Playstation Home2) e jogos de simulação do cotidiano (como o The Sims3), que são povoados por personagens virtuais, autônomos (agentes) ou guiados (avatares).
Para o sucesso dessas aplicações, esses personagens virtuais requerem uma identidade visual, o que envolve o desenvolvimento de modelos geométricos e animações diferenciadas, que lhes conferem características próprias.
Em um mundo virtual 3 D, o usuário interage através de um avatar, que é um personagem virtual que o representa.
Através de seus avatares, os usuários podem agir dentro de o mundo virtual, interagindo com outras pessoas (avatares e/ ou agentes) e compartilhando o mesmo espaço e objetos do mundo, mesmo que todos os usuários estejam em locais físicos muito distantes.
Essa proposta de interação pode servir para vários tipos de aplicação, como jogos, treinamentos, educação à distância e até reuniões de negócio numa teleconferência.
No entanto, para que uma aplicação que usa avatares seja atraente, é necessário prover a eles, além de a customização da sua aparência (o que é possível ver nas aplicações de hoje em dia), a personalização de sua movimentação, de maneira que o usuário se identifique com o seu personagem e possa expressar- se da forma mais natural possível, incluindo a sua maneira de executar gestos e expressões faciais.
Em esse sentido, vê- se que os avatares ainda são representações limitadas de pessoas, refletindo de forma muito abstrata e generalizada as ações dos usuários.
Filmes recentes como o AVATAR4 apresentam excelentes resultados visuais em termos de reflexão da ação de pessoas reais, usando modelos com grande número de polígonos e sofisticadas técnicas de motion capture.
Entretanto, esses resultados não são obtidos em tempo real, sendo inviável sua geração em aplicações interativas.
Em esse contexto, o estudo de métodos para a geração de animações variadas e personalizadas em avatares pode ser considerado um relevante tópico de pesquisa.
Focando- se na animação facial, encontra- se uma variedade de trabalhos que buscam por modelos realistas e exploram a expressividade e comunicação de forma interativa.
Muitas dessas pesquisas são focadas em maneiras para prover uma boa parametrização e descrição das www.
Thesims. Ea.
Com ações faciais, desde o trabalho pioneiro de Parke.
Além disso, existe uma variedade de trabalhos que busca a correspondência dos movimentos e o estado interno das pessoas (como o estado emocional, o humor e a personalidade).
Um exemplo é o trabalho dos pesquisadores canadenses Arya, DiPaola e Zamitto, que procura responder a seguinte pergunta:
&quot;Que pistas visuais os humanos podem usar para passar suas características de personalidade aos personagens animados?».
Em são apresentadas algumas dessas &quot;pistas», relativas à movimentação da cabeça em conjunto com expressões faciais.
Em, é fornecida uma visão mais abrangente da metodologia por eles usada para a associação entre ações faciais e traços de personalidade, no contexto de aplicações de jogos.
Ainda nesse contexto comportamental, pode- se citar outros trabalhos que geram animações (faciais e/ ou corporais) a partir de modelos de emoções e personalidade baseados em estudos de Psicologia, como (42;
34; 76;
62; 68;
3; 33).
No entanto, as animações propriamente ditas são em sua maioria pré-definidas ou geradas a partir de templates, feitos manualmente por artistas, o que é um trabalho dispendioso.
De acordo com Tang, existem três diferentes maneiras de se guiar um avatar:
Dirigida por texto, dirigida por fala e dirigida por performance.
Respectivamente, essas abordagens estão relacionadas com técnicas de TTS (text- to speech), que convertem texto em falas animadas através da geração de suas unidades de representação do som (fonemas) e suas de representação visual (visemas);
Que processam o som da fala e transformam em animação;
E que processam vídeos de pessoas reais e geram a animação conforme o movimento de elas.
Focando- se no controle do avatar usando a técnica de Animação Dirigida por Performance (Performance Driven Animation, PDA) (85), o grande desafio de pesquisa é mapear os movimentos de atores para modelos 3 D, com o mínimo de intervenção por parte de o usuário.
A captura dos movimentos pode ser feita através de equipamentos especializados ou através de seqüências de imagens utilizando- se Visão Computacional (Computer Vision, CV).
É importante ressaltar que esse tipo de trabalho não invalida ou substitui os estudos baseados na Psicologia, e pode ser visto como complementar ao desses trabalhos, uma vez que tem o potencial de fornecer dados reais para a definição de comportamentos corporais e faciais.
De a mesma forma, os estudos baseados na Psicologia podem fornecer informações que auxiliem no mapeamento dos dados capturados para dados de animação (por exemplo, os componentes faciais que atuam juntos em determinada expressão), validando, corrigindo e até mesmo suprindo informações que estejam faltando para gerar uma animação mais coerente.
Objetivos Em este contexto, o objetivo deste trabalho é prover um modelo para a geração automática de animações faciais personalizadas para avatares, em tempo real.
O modelo permite duas opções de controle para a geração das animações:
Por scripts de ações faciais descritas em alto-nível ou por Animação Dirigida por Performance, utilizando uma web cam comum e algoritmos de Visão Computacional, buscando refletir no avatar a expressão do usuário.
Objetivos Específicos Mais especificamente, são objetivos desse trabalho:·
Fazer um levantamento do Estado-da-arte sobre modelos de animação facial, parametrização e controle das animações, sendo este último com maior enfoque em modelos de PDA;·
Fazer um levantamento dos frameworks e/ ou engines disponíveis (de preferência, de código aberto) para o desenvolvimento de aplicações com animação facial, e verificar a viabilidade de usar algum para o desenvolvimento do protótipo.
Se não houver, implementar uma solução conforme o Estado-da-arte.·
Escolher ou definir um conjunto de parâmetros faciais adequado para o modelo de animação;·
Definir uma abordagem de controle em alto-nível de ações faciais básicas, como expressões faciais e visemas via scripts de animação;·
Para o controle por PDA, definir uma estratégia de mapeamento dos dados de Visão Computacional e integrar com o modelo de animação;·
Implementar um protótipo para validação do modelo;·
Realizar testes com usuários e avaliar qualitativamente os resultados obtidos.
A dissertação está organizada da seguinte maneira:
Os capítulos 2 e 3 apresentam, respectivamente, conceitos e trabalhos científicos relacionados com o tema deste trabalho, os quais em grande parte serviram para embasar o desenvolvimento do modelo apresentado no Capítulo 4.
Em o Capítulo 5 são apresentadas algumas aplicações usando o modelo desenvolvido, mostrando o controle dos avatares por a linguagem de script proposta.
Logo após, são apresentados alguns resultados visuais do modelo de PDA e uma avaliação qualitativa feita com usuários, que apresenta um estudo preliminar sobre o impacto da reflexão dos movimentos por os avatares.
Por fim, as contribuições deste trabalho são apresentadas e discutidas no Capítulo 6.
A geração de animações (faciais e corporais) personalizadas é um tema de pesquisa que está no topo do estado da arte de animação de humanos.
De maneira geral, em softwares de animação, os animadores precisam projetar manualmente o movimento de seus personagens em cada quadro-chave (keyframe).
A animação automática de humanos virtuais é, portanto, facilmente justificável.
Técnicas como a cinemática inversa (que faz o cálculo da posição das juntas de um objeto articulado) e modelos de animação baseados em física (dinâmica) podem produzir resultados realistas e simplificar o trabalho do animador.
No caso de as faces, em que a maioria dos movimentos não depende de articulações, é necessário fazer a manipulação da malha tridimensional através de técnicas como a aplicação de funções que deformam um ponto específico e sua vizinhança, ou ainda a modelagem de pseudo-músculos, que conduzem a deformação de regiões da face.
Mais recentemente, surgiram técnicas que fazem a captura dos movimentos de pessoas reais e tentam reproduzir- los e/ ou adaptar- los nos humanos virtuais.
Esta seção apresenta uma visão geral sobre técnicas de animação facial, de maneira a contextualizar as decisões tomadas para o modelo e os termos utilizados.
Faces 3D Animáveis O trabalho considerado pioneiro em modelagem e animação de faces no computador é o de Frederick Parke.
Parke fez um estudo de faces e expressões faciais em voluntários, que tiveram seu rosto pintado com uma malha poligonal, a fim de determinar um layout &quot;ótimo «dos vértices, como mostrado na Figura 2.1.
Satisfeito com a topologia encontrada no experimento, ele utilizou um método fotogramétrico para recriar a malha pintada no rosto numa malha 3D.
Com este estudo, Parke descobriu que a configuração ótima dos vértices e arestas é a que segue as linhas dos músculos faciais.
Esta pode ser considerada uma heurística básica que continua sendo utilizada até hoje por os artistas e softwares de modelagem 3D para produzir faces animáveis realistas.
O estudo de técnicas de parametrização facial, que produzam resultados realistas em aplicações de animação computacional, é uma área ativa de pesquisa desde o trabalho pioneiro de Parke.
A Figura 2.2 mostra a taxonomia apresentada por Pandzic· baixo-nível, que incluem os modelos computacionais que fazem a mudança da geometria e/ ou aparência da face no tempo, baseada no conjunto de parâmetros;
E· alto-nível, que incluem os modelos e métodos que fazem a codificação das ações faciais desejadas para o conjunto de parâmetros, tentando capturar a essência dos movimentos em termos de intensidades, direções e velocidade.
As próximas seções apresentam alguns modelos de parametrização, com destaque ao padrão MPEG-4, que foi o escolhido para o desenvolvimento deste trabalho.
Logo após, é apresentada uma visão geral de modelos para deformação das faces 3D (modelos &quot;baixonível&quot;) e das técnicas &quot;alto-nível», com enfoque na investigada neste trabalho, que é a Animação Dirigida por Performance.
Como fontes mais completas sobre técnicas de animação facial, recomenda- se a leitura de surveys como.
Parametrização Um bom conjunto de parâmetros faciais (que também é um tópico de pesquisas) é essencial para produzir a variabilidade e controle das animações.
Uma parametrização ideal deve especificar qualquer expressão facial possível através da combinação de valores independentes.
Em a literatura, encontram- se algumas abordagens de parametrização da face, para a descrição dos movimentos faciais, descritas a seguir.
O sistema FACS (Facial Action Coding System), proposto por Ekman (29), codifica as expressões faciais humanas através da determinação dos conjuntos de músculos responsáveis por as mudanças faciais.
Em esse sistema, são definidas Unidades de Ação (Action Units, AUs), representadas por a contração e relaxamento dos músculos faciais (ou conjunto de músculos).
É um sistema inicialmente desenvolvido para estudos psicológicos, para avaliar as expressões faciais humanas.
Porém, por a sua abordagem sistemática, fundamentada nas atividades musculares e descrição detalhada dos efeitos visuais de cada AU, o FACS torna- se atrativo para os pesquisadores de animação facial, sendo usado como base da parametrização em vários trabalhos.
Outro tipo de parametrização que tem uma grande influência nos trabalhos de animação facial é o desenvolvido por Parke.
Parke desenvolveu seus parâmetros faciais baseados na observação e análise da face em movimento, e dividiu os parâmetros em parâmetros de expressão (para animação) e de conformação (para modelagem).
Apesar de o seu modesto conjunto de parâmetros, seu modelo consegue gerar uma variedade de faces e expressões.
O modelo de Parke é usado por pesquisadores em trabalhos como.
Magnet-Thalmann Propõe uma parametrização baseada em pseudo-músculos, chamada Abstract Muscle Actions (Ama), inspirada no sistema FACS.
Os Amas simulam a ação de um ou mais músculos, não independentes, cuja ordem de execução é importante.
O esquema compreende dois controles de alto-nível (para visemas e expressões), compostos por Amas de baixo-nível.
Kalra Introduz uma abordagem para animação facial baseada em camadas de abstração.
Os elementos principais de sua parametrização são os chamados Minimal Perceptive Actions (MPAs), que descrevem os efeitos visuais de uma ou mais ações musculares, que podem ser combinados em diferentes níveis de abstração.
Essas &quot;ações mínimas «incluem ações como, por exemplo, &quot;levantar da sobrancelha direita «e &quot;boca fazendo bico», que recebem valores normalizados de 0 a 1 para MPAs unidirecionais e 1 a 1 para bidirecionais.
Esses valores máximos e mínimos são dependentes da implementação e do modelo 3D em particular, portanto não é um esquema portável de uma face para outra.
O Padrão MPEG-4 de Animação Facial O padrão MPEG-4 de Animação Facial especifica um conjunto de 84 pontos característicos (Feature Points ­ FPs) localizados na malha da face.
A Figura 2.3 mostra esses pontos característicos.
Um subconjunto desses pontos atua como pontos de controle para os 68 parâmetros de animação (Facial Animation Parameters, FAPs), também definidos por o padrão.
Os dois primeiros FAPs descrevem ações em alto-nível e os restantes lidam com regiões específicas da face, descrevendo ações mais baixonível como &quot;levantar o canto direito dos lábios «e &quot;fechar a pálpebra superior esquerda».
Os FAPs são codificados como valores numéricos, que são normalizados por um conjunto de unidades baseadas nas distâncias entre alguns pontos característicos principais da face, chamadas FAPU (Facial Animation Parameter Units).
A Figura 2.4 e a Tabela 2.1 apresentam essas distâncias e unidades.
Com essa normalização, é possível animar faces com diferentes tamanhos, proporções e número de polígonos.
Uma descrição mais completa dos FAPs e suas respectivas FAPUs pode ser vista no Anexo A desta dissertação.
Para a determinação da FAPU, o modelo da face deve estar em estado &quot;neutro», ou seja:·
olhar na direção do eixo z;·
todos os músculos da face devem estar relaxados;·
as pálpebras devem estar tangentes à iris;·
a pupila deve medir um terço do diâmetro da iris;·
os lábios devem estar em contato e os cantos dos lábios na mesma altura;·
a boca deve estar fechada, de maneira que os dentes superiores e inferiores estejam em contato;·
a língua deve estar plana na horizontal, e com a ponta tocando o limite entre os dentes superiores e iinferiores.
Gerar uma animação baseada em FAPs consiste em prover, para cada frame de animação, a variação dos valores dos FAPs.
Para cada frame, tem- se então uma stream de valores dos FAPs, que é processada por a aplicação para gerar as animações na face, e pode ser enviada por a rede ou salva em arquivos para porterior animação.
É importante ressaltar que o padrão MPEG-4 apenas sugere os parâmetros envolvidos na animação de faces, e não os métodos para deformar- las.
Tendo um conjunto de valores de parâmetros, é necessário deformar os vértices da face para produzir a animação.
Por exemplo, no padrão MPEG-4, cada FAP atua sobre um FP, que por sua vez influência os vértices de sua vizinhança (que dependem da topologia da face e não são especificados por o padrão), produzindo uma deformação na malha poligonal.
Isso significa que cada valor de FAP é escalado por a sua FAPU para se obter o deslocamento do seu ponto de controle FP, e os vértices de sua zona de infuência podem ser deformados por a aplicação de diferentes técnicas (técnicas &quot;baixo-nível «na Figura 2.2).
A Figura 2.5 (à direita) mostra a atuação do FAP close_ top_ left_ eyelid, utilizando- se uma função gaussiana sobre o FP e sua vizinhança.
A informação sobre a FAPU, FPs e suas zonas de influência de um modelo 3D são os chamados Parâmetros de Deformação Facial (Face Definition Parameters ­ FDP), noemalmente descritos em arquivos de configuração da face.
O FDP pode incluir, opcionalmente, informações sobre a deformação de cada FAP.
O padrão MPEG-4 chama essa informação de FAT (Facial Animation Table).
Cada modelo pode ter sua própria FAT, que pode ser modelada por artistas, como a proposta de Garchery e Thalmann.
Outra proposta é indicar no FDP do modelo a região de influência (índices dos vértices) dos FPs e a indicação de qual função de deformação deve ser aplicada para cada FAP que atua sobre aquele FP Segundo, para um modelo de animação facial ser conforme o padrão, ele deve ser capaz de interpretar todos os FAPs como na sua especificação.
Portanto, o modelo de face deve ter um número de vértices pelo menos correspondente aos pontos característicos necessários para sua animação.
O padrão especifica que o modelo de face deve ter no mínimo 50 vértices.
No entanto, recomenda- se que, para alcançar resultados visuais satisfatórios, o modelo tenha pelo menos 500 vértices.
Assim, para construir uma face animada conforme o padrão, é necessário seguir 4 passos:
De os FPs, o padrão define somente a movimentação numa dimensão.
É deixado a critério do desenvolvedor associar a esses movimentos outras dimensões.
Modelos para a Deformação da Face Em os modelos paramétricos de animação facial, os vértices da superfície (&quot;pele&quot;) do modelo 3D são governados por um conjunto de parâmetros de controle (como os FPs no MPEG-4).
As técnicas &quot;alto-nível «fornecem esses parâmetros, indicando, por exemplo, o quanto (intensidade) e em que direção eles modificam a face em determinados instantes de tempo.
Tendo esses parâmetros, as técnicas &quot;baixo-nível «especificam como a superfície poligonal da face vai deformar:
Como esses parâmetros de controle vão mover os vértices da face.
Esse controle pode ser feito através de diferentes métodos de interpolação, que atuam diretamente sobre a superfície da face, baseada em observações, ou ainda em estruturas auxiliares que simulam a ação de músculos na superfície da pele, podendo ser baseados em Física, simulando a elasticidade do tecido da pele e interações multi-camadas, simulando ossos, músculos e pele.
Outro aspecto importante na geração de animações de face convincentes são as rugas de expressão.
Segundo Oat, as rugas de expressão ajudam a desambiguar certas expressões.
Encontram- se dois tipos de abordagem para a geração das rugas:
Baseada em geometria, em que as deformações são feitas através da movimentação dos vértices da malha poligonal;
E baseada em textura, em que as deformações são geradas através da perturbação das normais dos vértices (utilizando- se técnicas de bump-mapping e normal mapping), sendo aplicadas somente na textura do modelo (58;
50; 74;
27). Essa abordagem possui como vantagem a geração de resultados visuais adequados mesmo quando o modelo possui poucos vértices.
Modelos para Controle dos Parâmetros De acordo com a taxonomia de Pandzic (Figura 2.2), as técnicas &quot;alto-nível «são os modelos que fazem a obtenção e controle dos parâmetros de animação.
Essas abordagens vão desde a edição manual numa ferramenta gráfica de edição, passando por modelos que recebem scripts de ações em alto-nível e as transformam em parâmetros de animação (discutidos no próximo capítulo);
Animações dirigidas por fala, onde há o processamento do som e extração dos parâmetros de animação;
Animações dirigidas por dispositivos de hardware que atuam como &quot;fantoches «mecânicos, como o Waldo;
Em a Animação Facial Dirigida por Performance, procura- se mapear os movimentos capturados de pontos característicos da face de pessoas reais para modelos 3D.
É notável o quanto esse tipo de técnica tem sido visada na indústria de entretenimento, sendo oferecido atualmente por empresas, como a Image Metrics2, que grava a performance dos atores, sem qualquer marcação no rosto, e produz animações faciais de alta qualidade visual (veja Figura 2.7, primeira figura à esquerda), sendo este serviço utilizado para produção de filmes e animações pré-definidas em jogos.
Filmes recentes, como o Avatar, também fazem a captura do movimento da face dos atores para gerar as animações, normalmente em ambientes controlados, dispositivos de captura de alta tecnologia e marcações no rosto (como mostra a Figura 2.7, figura do meio3 e à direita4).
No entanto, esses processos utilizados na indústria do cinema geralmente não produzem as animações em tempo real (não há a exigência), e tão pouco com processos automatizados.
No caso de o filme Avatar, a tecnologia por eles usada permite uma prévisualização dos movimentos dos atores em tempo real, de maneira simplificada, mas os dados faciais são gravados para edição posterior por artistas.
Outras abordagens utilizam apenas uma câmera e marcadores no rosto, sendo essa a configuração do estado-da-arte das pesquisas em trabalhos mais voltados para animação em tempo-real para avatares.
Avanços em algoritmos de CV têm permitido o rastreamento de alguns pontos sem marcadores, como no trabalhos de.
Os trabalhos que utilizam uma câmera e CV para animação em tempo-real são descritos na Seção 3.2.
Vê- se que a geração de animações fidedignas em tempo real, sem o uso de marcadores ou outros métodos intrusivos de captura e ambientes controlados, é um desafio que está no topo do estado- da arte das pesquisas.
Este capítulo apresenta o levantamento dos trabalhos relacionados com os objetivos do modelo proposto.
Primeiro, são apresentados trabalhos de descrição de frameworks de animação facial, que procuram prover uma plataforma desejável para o desenvolvimento de pesquisas e aplicações.
Logo após, são apresentados trabalhos que usam, como fonte de controle dos parâmetros de animação, modelos de Animação Facial Dirigida por Performance, fazendo a geração das animações em tempo real.
Conjuntos de parâmetros como o FACS e o MPEG-4 provêem uma base um tanto abstrata e &quot;baixo-nível «para os animadores.
Por isso, existem vários esforços para produzir uma abordagem paramétrica um pouco mais &quot;alto-nível», de maneira efetiva.
Esse tipo de trabalho envolve diferentes níveis de abstração, e são propostas interfaces para o controle da geração das animações, que vão de linguagens de descrição a ferramentas visuais de edição.
Através de uma linguagem de descrição, o usuário pode definir um cenário completo de animação, não manipulando diretamente quadros-chave, mas por descrição textual, onde expressões e ações são os seus blocos de construção básicos.
Em a literatura, encontram- se várias linguagens de descrição, propostas por pesquisadores para descrever ações do corpo e face dos personagens.
A maioria de elas é baseada em XML (por exemplo, AML, APML, CML, VHML, SMIL e SMIL-Agent) e proveêm diferentes canais de controle e níveis de abstração, como fala, expressões faciais, ou especificação direta de alguns parâmetros de animação.
Outros trabalhos descrevem metodologias e modelos para o desenvolvimento de seus frameworks ou sistemas para a geração de animações faciais, procurando prover uma plataforma desejável para o desenvolvimento de pesquisas na área e aplicações.
Perlin, em seu sistema Responsive Face 1, apresenta uma abordagem multi-camadas para a representação de expressões faciais.
Primeiro, foram definidos parâmetros &quot;baixonível «para a descrição de movimentos, contendo um número discreto de graus de liberdade.
A idéia é que, através da combinação desses parâmetros podem ser construídos movimentos mais &quot;alto-nível», que por sua vez também podem ser combinados, obtendo camadas mais altas de abstração.
O modelo em si é bastante simples, porém expressivo, construído com base em observações.
Além disso, por ser um sistema disponível, torna- se uma plataforma de animação que pode ser estendida e usada em outros trabalhos, como Cassel Propõe um sistema chamado BEAT (Behavioral Expression Animation Toolkit), que permite aos animadores entrarem com um texto a ser dito por o personagem virtual, e obter comportamentos não-verbais sincronizados com a fala (descrição textual) enviada, através de um modelo que faz a análise pragmática do discurso e um conjunto de regras que definem os comportamentos de acordo com essa análise.
Esses parâmetros comportamentais e de síntese da fala são fornecidos em forma de uma linguagem de descrição, que pode ser interpretada por diferentes sistemas de animação.
O trabalho de Wang é mais focado à síntese das animações, descrevendo uma metodologia para a construção de um sistema de animação facial com expressões faciais e sincronização labial usando componentes acessíveis no mercado, como o software FaceGen Modeller 2 para a geração das malhas-chave de face (para cada visema) e a Microsoft Speech SDK 3 para a API de fala.
Cosi propõe um toolkit de animação facial implementado em MATLAB4, criado principalmente para acelerar o processo de criação do agente conversacional Lucia.
Para a determinação das trajetórias dos pontos de controle da animação, é utilizado um equipamento especializado de motion capture.
A tradução para os parâmetros de animação do padrão MPEG-4 se dá por ajuste manual na interface do programa de rastreamento, e para a síntese das animações de boca, é utilizado um modelo de coarticulação labial descrito por os autores em.
Apesar de essa abordagem parecer promissora, ela é construída sobre um equipamento de hardware específico e software proprietário.
DiPaola e Arya propõem um framework de animação facial, também compatível com o padrão MPEG-4, chamado iFace, cujos binários estão disponíveis na web5.
O iFace permite a criação de expressões não-verbais através de uma linguagem de scripting chamada FML (Face Modeling Language) (5), que permite a descrição de ações faciais sequenciais e paralelas.
As ações faciais incluem fala, expressões faciais, movimentos de cabeça e os parâmetros MPEG-4 diretamente.
Atualmente, o iFace está sendo utilizado para estudos de animação comportamental com o objetivo de alcançar a associação entre as ações faciais www.
Facegen. Com/ modeller.
Htm e traços de personalidade, criando um conjunto de parâmetros mais alto-nível.
Balci Projetaram o Xface6, um conjunto de ferramentas de código aberto para a criação de talking heads usando MPEG-4 ou animação por keyframes.
O controle da animação por key-- frames é feito por a linguagem de scripting SMIL-Agent.
Isso significa que, para usar este modo de animação, é necessário preparar diversas malhas com poses diferentes para cada modelo de face que pretende-se usar, e a biblioteca faz a interpolação dessas malhas-chave.
O suporte dado para o modo de animação que segue o padrão MPEG-4 inclui:·
uma biblioteca principal, que inclui as funções principais de animação;·
um editor gráfico, que permite carregar modelos de face neutra e configurar seu FDP (setar os FPs e vizinhança, FAPU, funções de deformação para cada FAP), salvo como um arquivo XML;·
uma aplicação simples que interpreta arquivos com streams de FAPs e gera animações em tempo-real (player)· uma aplicação de programa cliente, que permite o controle do player por a rede.
O Xface é usado em outros trabalhos de pesquisa, como, e, sendo este último estende o editor gráfico, criando uma ferramenta de autoria que permite a geração de expressões faciais baseadas num modelo emocional e fala sincronizada.
Animação Facial Dirigida por Performance em Tempo Real Em modelos de Animação Dirigida por Performance, o mapeamento a ser feito entre os dados obtidos e os parâmetros de animação é um problema complexo, em o qual os pesquisadores têm proposto diferentes métodos.
Uma questão que se impõe é a transformação de movimentos contínuos, na realidade, em dados discretos, nos personagens virtuais.
Em o trabalho de Chai, um pequeno conjunto de parâmetros é extraído de vídeos e combinado com dados de arquivos de motion capture.
A principal contribuição desse modelo é o método que faz a associação do movimento, com poucos pontos de controle, associados com as movimentações de alta qualidade extraídas de um banco de dados de arquivos de motion capture.
Buenaposada Estimam parâmetros de animação em alto-nível (contrações de músculos, movimentos de olhos, abertura das pálpebras, movimentação da mandíbula e contrações dos lábios) de uma sequência de imagens de face, sem marcadores, através de um tracker que utiliza o modelo estatístico de aprendizagem baseado em aparência (appearance based).
Para as animações, os autores utilizam uma versão modificada do modelo de Parke e Waters que é feita em tempo real, de acordo com os parâmetros identificados.
Eles utilizam animações da face 3D também para o treino do modelo estatístico.
Entretanto, a adaptação para um usuário novo é parcialmente manual, por o fato de que não foi estudado como fazer a correspondência entre as imagens do usuário e as expressões faciais do modelo 3D.
A Figura 3.1 mostra alguns resultados desse trabalho.
Khanam et.
Al descrevem um modelo que procura reproduzir expressões faciais detectadas de videos automaticamente.
O modelo propõe uma técnica para melhorar a síntese das expressões faciais detectadas, através da identificação do seu contexto.
As expressões faciais são modeladas como streams de parâmetros de animação MPEG-4.
O contexto do vídeo é processado por um sistema baseado em lógica difusa, que encontra qual a expressão que precisa ser adicionada aos parâmetros da expressão facial detectada, para melhorar sua &quot;naturalidade».
Em o trabalho de Tang, os parâmetros de animação são estimados de um vídeo usando um algoritmo de rastreamento.
Os parâmetros estimados são traduzidos para os parâmetros de animação do padrão MPEG-4 (valores de FAPs) através da resolução de um problema de otimização linear.
O mapeamento é feito através de uma tabela de animação (FAT) para determinar para cada FAP um conjunto de deslocamentos dos FPs.
Figura 3.2 mostra algumas etapas do modelo e um exemplo de resultado aplicado a uma face 3D.
O trabalho de Dutreve (Figura 3.5) apresenta uma metodologia que considera, como entrada para a geração de animações, apenas um pequeno conjunto de pontos característicos.
Estes pontos característicos são selecionados manualmente na inicialização, e uma rede de funções de base radial (Radial Basis Functions ­ RBFs) é treinada para fazer a correspondência desses pontos no modelo-fonte (vídeo do usuário) e no modelo-destino (face 3 D).
As zonas de influência são determinadas considerando as distâncias dos vértices aos pontos de controle, o que nem sempre produz bons resultados e pode requerer ajustes manuais.
Contextualização do Trabalho no Estado-da-arte Essa seção apresenta a contextualização desse trabalho com o estado- da arte levantado nesse capítulo e no anterior.
Modelo de Parametrização Em relação a a parametrização, o modelo proposto neste trabalho optou por o padrão MPEG-4 de Animação Facial.
Como será visto no próximo capítulo, o modelo faz a inclusão de alguns parâmetros de animação, seguindo o modelo de definição dos FAPs.
Isso faz com que o modelo não seja completamente compatível com o padrão MPEG-4 por definição.
No entanto, tecnicamente, esse acréscimo não alterou os parâmetros originais.
Portanto, os parâmetros gerados compatíveis como padrão podem ser enviados para outros sistemas compatíveis com o padrão.
Modelo de Deformação de Faces Considerando as técnicas de &quot;baixo-nível «para a geração das deformações na face do avatar, optou- se em usar interpolações locais baseadas nos trabalhos de, sem a necessidade de estruturas auxiliares, como músculos.
Técnicas de Controle Considerando técnicas de animação facial &quot;alto-nível «para o controle dos avatares, este trabalho focou- se Controle por Script e Animação Dirigida por Performance.
Para o controle de animações faciais em alto-nível, foi desenvolvido um modelo que faz a geração dos parâmetros de animação através de uma linguagem de script.
Esse modelo foi inspirado principalmente nos trabalhos.
Sobre o estado- da arte em PDA, pode- se notar que os pesquisadores atualmente estão concentrando seus esforços na complexa tarefa do mapeamento entre os dados capturados de pessoas reais e os parâmetros de animação.
Alguns trabalhos (por exemplo, o de Tang) estão mais focados nas abordagens de rastreamento dos componentes faciais, tentando imitar diretamente a performance do ator.
Outros trabalhos lidam com a &quot;tradução «de linguagens de representação, como o de Quax, considerando que um avatar pode ser um personagem não realista (cartum).
O modelo proposto neste trabalho utiliza uma abordagem semelhante à de (animação por um conjunto de pontos de controle), porém utilizando a parametrização baseada no padrão MPEG-4 de animação facial no mapeamento e considerando que, dos pontos característicos rastreados podem ser extraídas informações que, utilizando- se um modelo de relacionamento entre os FAPs, seja possível suprir alguns parâmetros que não são fornecidos por CV e deformar de maneira coerente todas as regiões da face.
Com relação a o uso de um framework já existente (cuja investigação constitui um dos objetivos específicos do trabalho, com viés tecnológico, porém indispensável para a condução das pesquisas), com o levantamento na Seção 3.1 e verificação de sua disponibilidade para a comunidade científica, verificou- se que poucos de eles (e alguns de eles, em parte) encontram- se disponíveis (Responsive Face, Lucia, iFace e Xface).
Como a maioria dos trabalhos apresentados tratam- se de trabalhos de pesquisa com diferentes focos, optou- se, inicialmente, em utilizar o Xface apenas como uma engine para a geração de animação facial compatível com o padrão MPEG-4.
O modelo começou a ser desenvolvido utilizando- se as bibliotecas principais de animação do Xface, estendendo a sua aplicação player.
No entanto, apesar de o Xface fornecer subsídios para os fins da pesquisa (código aberto e abstração de classes), ele apresenta algumas limitações técnicas, a saber, problemas com suas dependências e estruturação de classes complexa.
Essas limitações foram perceptíveis ao longo de o desenvolvimento do modelo de geração de animações por script, porém foi no desenvolvimento do modelo de controle por PDA que surgiu a necessidade de algumas alterações no modelo de animação facial, o que exigiria reestruturações no core do Xface.
Em essa etapa, surgiu a necessidade do acréscimo dos parâmetros de animação e alterações/ melhorias no modelo de deformação do modelo 3 D, uma vez que o XFace utiliza a abordagem de apenas um ponto de controle por região, e pesos pré-definidos manualmente para sua função de deformação de cada modelo 3D.
Consequentemente, com as limitações técnicas apresentadas, os acréscimos ao modelo de animação demandariam um muito tempo, o que tornaria inviável a continuação da pesquisa dentro de o cronograma previsto.
Optou- se, então, em construir um protótipo com um modelo de animação facial próprio, que recebe os parâmetros de animação e realiza as deformações na face, conforme apresenta o Capítulo 4.
É importante ressaltar que essa decisão de implementação não teve impacto no modelo de geração dos parâmetros de animação, uma vez que o Xface era usado apenas para as deformações do modelo 3D e visualização das animações.
Parte deste trabalho, que já havia sido publicado como um framework em, utilizava o Xface, mas o modelo do controle para geração dos parâmetros de animação (no caso, por script) é independente do Xface.
Portanto, se arquivos gerados com os parâmetros de animação forem &quot;rodados «no Xface ou na plataforma de animação posteriormente desenvolvida, haverá diferenças apenas na maneira como os parâmetros são interpretados para a geração das deformações.
Avaliação Com relação a a avaliação, Computação Gráfica é certamente uma das áreas da computação mais difíceis de avaliar seus modelos.
O problema normalmente está em como responder a seguinte questão:
&quot;o quão realista é o resultado que o modelo produz?»,
sendo que, muitas vezes, não existe maneiras efetivas de fazer comparações utilizando dados quantitativos do mundo real.
Thalmann e Thalmann sugerem que a avaliação do realismo seja feito de acordo com o seu campo de aplicação:
Animação ou simulação.
Em o contexto deste trabalho, o foco está em produzir animações convincentes, compreensíveis aos usuários.
Por esse motivo, foi realizada uma avaliação qualitativa com usuários, apresentada no Capítulo 5.
Este capítulo apresenta o modelo desenvolvido, cujo objetivo é prover animações de face personalizadas e expressivas para avatares.
O modelo permite duas opções de controle das animações:
A arquitetura geral do modelo é apresentada na Figura 4.1.
Ele está dividido em três etapas principais:
A etapa de Geração/ Visualização da Animação Facial a partir de os parâmetros de animação gerados, e duas correspondentes aos modelos das opções de controle do avatar abordados, Controle por Script e Controle por Animação Dirigida por Performance, que são os geradores dos parâmetros de animação.
A descrição dessas etapas principais e seus modelos, como mostrados na Figura 4.1, são descritos nas próximas seções.
Geração e Visualização da Animação Facial O modelo de Geração das Deformações na Face 3D recebe os parâmetros de animação de um dos modelos de controle e gera as deformações na geometria de uma face 3D parametrizada, que é composta por a malha poligonal do avatar e por os parâmetros de descrição do avatar.
A Figura 4.2 apresenta a arquitetura geral do modelo, que inclui:·
Uma etapa de inicialização, em que é feito o cálculo das unidades de medida utilizadas para escalar os parâmetros de animação;·
A Visualização da animação, que inclui as técnicas de rendering da cena.
Em o modelo, utiliza- se o padrão do OpenGL.
Parâmetros de Descrição O modelo gera as deformações a partir de os dados referentes à geometria do avatar (malha poligonal) e dos seus seguintes parâmetros de descrição, via arquivo de configuração:·
Pontos de Controle:
Devem ser informados os índices dos vértices do modelo 3D correspondentes aos FPs do padrão MPEG-4.
O ideal é que sejam fornecidos todos os FPs que possuam um parâmetro de animação associado, caso contrário, não serão geradas deformações nos pontos/ regiões da face não especificados.
O conjunto mínimo de pontos a serem informados são os que são usados para se fazer o cálculo da FAPU do avatar.
O processo de captura dos índices dos FPs é manual.
Figura 4.3 apresenta exemplos de faces e a localização dos seus FPs, baseada nas indicações do padrão MPEG-4.·
Regiões de Influência: Correspondem às áreas (conjunto de vértices) que são influenciadas por os pontos de controle.
Devem ser informados os índices dos vértices da região e o conjunto de um ou mais FPs que a influencia.
Em este trabalho, essas regiões também foram definidas manualmente, como mostra a Figura 4.4.
MPEG-4 (verdes e azuis).
Performance, pode se obter os dados sobre esse movimento, não necessitando de regras de associação.
Por essa razão, o conjunto de parâmetros de animação deste trabalho propõe a inclusão de novos parâmetros, completando os movimentos nos eixos que não são contemplados por o padrão MPEG-4.
O Anexo A apresenta os parâmetros acrescentados em função de o modelo de PDA, sendo que esse conjunto ainda pode ser estendido.
É importante observar que a inclusão desses parâmetros requer a escolha de uma FAPU que os quantifique.
Para isso, seguiu- se a lógica empregada nos outros FAPs do padrão MPEG-4.
Por exemplo, para a boca, movimentos no eixo x utilizam o FAPU M W (Mouth Width), e em y e z, o FAPU M N S (Mouth-Nose Separation).
A Figura 4.5 mostra um exemplo da inclusão desses parâmetros (considerando apenas os eixos x e y) nos FPs da parte interna dos lábios.
Cálculo da FAPU Dada como entrada do modelo a face 3D parametrizada, a primeira etapa é a determinação das unidades de medida dos parâmetros de animação, a FAPU, como descrita no são normalizadas como sugere o padrão MPEG-4 Tabela 2.1.
IRISD0 $ | F P3.
1y -- F P3.
3y| ES0 $ | F P3.
5x -- F P3.
6x| EN S0 $ | F P3.
5y -- F P9.
15y| M N S0 $ | F P9.
15y -- F P2.
2y| M W0 $ | F P8.
3x -- F P8.
4x| Deslocamentos dos FPs e Deformação das Regiões O deslocamento dos pontos de controle (FPs ativos) se dá por o valor do parâmetro de animação j do conjunto de FAPs (F APj) escalado por a FAPU do avatar (correspondente ao F APj):
F Pj $= F APj· F AP Uavatar (F APj) Após deslocar os pontos de controle, cada vértice da região de influência do FP são deformados de acordo com uma função de deformação.
A estratégia adotada para fazer a deformação das regiões de influência foi a seguinte:·
para regiões influenciadas por apenas um FP, aplica- se uma função de cosseno (explicada a seguir);·
para regiões contornadas por FPs (como os lábios superior e inferior) e a linha das sobrancelhas, optou- se em usar a função de base radial (explicada a seguir);·
para regiões influenciadas por mais de um FP (mas não contornadas por eles), optouse em fazer a média da função de cosseno aplicada a cada FP.
É interessante ressaltar que essa estratégia é a que está sendo adotada no momento, no protótipo implementado.
A justificativa é que essa configuração produz resultados visuais adequados.
No entanto, pode- se adotar estratégias diferentes (por exemplo, utilizando funções de base radial em outras zonas, por exemplo), apenas modificando o arquivo de Parâmetros de Descrição do Avatar.
Função de Cosseno Funções de cosseno (Raised Cosine Functions, ou RCFs) são encontradas em alguns trabalhos da literatura como uma maneira suave e rápida de deformar regiões influenciadas por um ponto de controle.
F (vp) $= 1+ cos· dp dmax· w· F Pj onde dp é a distância entre o vértice vp e o F Pj e dmax é a distância do vértice mais distante do F Pj na região de influência.
Figura 4.6 mostra a aplicação de uma RCF numa zona de influência circular, nos eixos x e y.
Em este trabalho, w é uma constante, que garante que, no FP, o valor de F (vp) retorne 1, fazendo uma normalização do deslocamento dos outros pontos em relação a o F Pj (Equação 4.6).
É importante ressaltar que, em outros trabalhos, como o de Balci, este w constitui um valor que é atribuído manualmente para cada FAP, durante a configuração dos parâmetros de definição de cada face, de maneira a tornar mais suaves as deformações.
Porém, com esses valores, não se garante que no FP haverá o deslocamento desejado conforme o modelo de mapeamento apresentado na seção anterior (além de tornarem o processo manual), e consequentemente, isso impacta no comportamento dos outros pontos da região.
Função de Base Radial Funções de Base Radial (Radial Basis Functions, ou RBFs) são bastante usadas em Computação Gráfica para aproximação e interpolação de superfícies.
Em o presente trabalho, é utilizada a abordagem de Noh (que propõe a animação de faces usando RBFs), baseando- se também na explicação do trabalho de Dutreve (que utiliza as RBFs para o mapeamento dos espaços de parâmetros real e virtual).
Em ambos os trabalhos, utiliza- se a função RBF multi-quadrática.
Para fazer a deformação de uma região da face usando uma RBF, é necessário definir regiões da face compostas por um conjunto de pontos característicos e os outros vértices da região de influência.
Essas regiões são chamadas de GDEs (Geometry Deformation Elements) como mostra a Figura 4.7 (89).
O conjunto de pontos característicos pode ter dois tipos de pontos, conforme Noh:·
pontos de controle:
Um ou mais pontos que sofrerão os deslocamentos;·
pontos-âncora: São das bordas da região, que permanecem estáticos, e são necessários no treino da RBF, para delimitar a região;
A definição dos limites do GDE pode ser feita automaticamente, através da definição de algum critério de distância, como em ou manualmente (pontos das bordas da região manualmente escolhida).
Em este trabalho, as GDEs foram definidas com os pontos de controle (os FPs da região), uma vez que utilizou- se as RBFs na região dos lábios, onde os FPs delimitam a sua região (como mostrado anteriormente na Figura 4.4).
Seja S $= o conjunto de pontos característicos inicial ou &quot;fonte «(source, correspondente à face em repouso) de um GDE.
O primeiro passo é o treino da RBF a partir são calculados o conjunto de pesos W $= os quais, aplicados à função RBF multi-quadrática F (sj) para os outros pontos j da região de inlfuência, determinam sua nova posição (tj $= F (sj)).
É usada uma RBF para cada eixo.
A seguir são apresentados os cálculos de cada uma das etapas:·
Treino Para o treino da RBF, é necessário determinar a matriz H, que contém a função multiquadrática aplicada a todos os pares elementos do conjunto S, conforme a Equação 4.8 Hij $= h (sj -- si) $= (sj -- si) 2+ sc2 Onde sc2 j $= minj $= i (sj -- si) e trata- se de um coeficiente de &quot;rigidez «(stiffness) sugerido por Eck (28) para suavizar deformações onde os pontos característicos estão muito esparsos e reforçar onde eles estão muito próximos.·
Determinação do vetor de pesos Tendo a matriz H e o conjunto Tx $= (tx característicos (tx j é a coordenada x de tj), calcula- se o vetor de pesos do sistema no eixo x, como mostrado na Equação 4.9: Tx $= H· Wx Wx $= H 1 Tx· Determinação das novas posições Uma vez treinada a RBF para cada eixo, a nova posição para cada ponto tj de cada um dos outros pontos sj da zona de influência é obtido por a função F (sj) da Equação 4.10, que representa a RBF propriamente dita:
Controle por Script Esta seção apresenta a descrição detalhada o modelo que faz o Controle por Script dos avatares.
A principal entrada para essse modo de geração das animações é um (ou mais) arquivos de script contendo a descrição de uma ou mais ações faciais.
A linguagem de descrição desenvolvida é chamada FDL (Face Description Language), portanto, seus arquivos serão chamados no decorrer de o texto como arquivos FDL.
O modelo interpreta as ações faciais descritas nos arquivos FDL e gera animações de acordo com sua especificação.
A linguagem compreende seqüências de três tipos de ações faciais:
Cada um desses tipos de ações faciais são descritos de forma independente (a coerência entre o timeline dessas ações pode ser feita manualmente ou através de um modelo emocional que os correlacione, por exemplo), e são processados nas seguintes etapas:
Sincronização da Fala, Geração de Expressões Faciais e Geração de Comportamento dos Olhos.
Essas etapas provêem os valores de cada parâmetro de animação (FAP) correspondente à ação facial desejada, ao longo de o tempo.
Depois disso, é feita a Sincronização dos Parâmetros de Animação, etapa que recebe os valores dos FAPs e resolve possíveis conflitos entre eles (como a combinação entre expressões faciais e visemas).
Essa etapa retorna então as streams de parâmetros de animação final, que são processadas por o modelo de Geração/ Visualização da Animação Facial, podendo ser executadas com diferentes modelos de face 3D.
Geração de Comportamento dos Olhos O modelo de Geração de Comportamento dos Olhos, cuja arquitetura está ilustrada na Figura 4.8, contém a implementação do modelo de Queiroz Para geração automática de animações dos olhos.
O modelo provê um conjunto de comportamentos dos olhos que podem ser usados com diferentes estados afetivos.
O modelo utiliza um modelo estatístico) como o motor de geração dos movimentos sacádicos e cria diferentes olhares (que compõem a Base de Dados Comportamental) através de modificação nos parâmetros do olhar considerados por o modelo estatístico, como direção, magnitude e intervalo entre os movimentos.
Movimentos sacádicos são movimentos rápidos de ambos os olhos de uma posição para outra.
Em os scripts FDL, os comportamentos dos olhos são descritos como ações em alto-nível.
Quando combinados com expressões faciais, eles contribuem no aumento da expressividade e engajamento na comunicação.
O Gerador de Olhares Expressivos recebe as ações faciais correspondentes aos olhos e retorna os parâmetros de animação para cada olhar gerado de acordo com o comportamento de olhos especificado, além de prover movimentos de cabeça e pálpebrasde acordo com as regras do modelo.
Geração das Expressões Faciais A tarefa do modelo de Geração de Expressões Faciais é mais simples:
Ele associa as ações de expressões faciais com um conjunto de expressões cujos parâmetros de animação foram previamente definidos (na Base de Dados Emocional).
O Gerador de Expressões Faciais recebe as ações faciais de expressões, busca os parâmetros na base de dados e gera as streams de parâmetros ao longo de o tempo, de acordo com a duração definida no script, por interpolação linear.
Sincronização da Fala O modelo de Sincronização da Fala segue a metodologia descrita em Rodrigues.
Basicamente, o modelo recebe como entrada um arquivo de áudio, contendo a fala do personagem e um arquivo auxiliar contento sua descrição textual.
Para a Extração dos Fonemas, é usada a ferramenta CSLU Toolkit1, que alinha o arquivo de áudio e a descrição textual e provê um arquivo com os fonemas ditos ao longo de o tempo.
Este arquivo é a entrada para a etapa de Mapeamento Fonema--Visema, que associa os fonemas às suas representações visuais (visemas).
O CSLU Toolkit gera 41 fonemas da língua inglesa, que são mapeados para os 14 visemas especificados por o padrão MPEG-4, de acordo com o mapeamento proposto em (ver Anexo B).
Com essa etapa, obtém- se o timeline completo dos visemas, cujo mapeamento para os parâmetros de animação é feito por a etapa de Mapeamento Visema-Parâmetros de Animação.
Os parâmetros de animação dos visemas estão previamente definidos, assim como as expressões faciais.
Em essa etapa é gerada a animação conforme as durações de cada visema e a transição entre eles, por interpolação linear.
Sincronização dos Parâmetros de Animação Após as três etapas de processamento das ações faciais descritas no arquivo FDL, a etapa de Sincronização dos Parâmetros de Animação recebe os valores dos FAPs retornados e resolve possíveis conflitos entre eles.
O modelo faz o tratamento de três tipos de conflitos:
Esse conflito ocorre principalmente na região da boca, quando as expressõe faciais incluem movimentos na boca.
Para resolver esse conflito, é feita a combinação dos valores dos FAPs gerados por os modelos de expressão facial e sincronização da fala através de uma soma ponderada.
Pyun Propõe que os pesos para diferentes visemas e expressões variem de acordo com sua &quot;importância».
Esses pesos poderiam ser incluidos como parâmetros adicionais no script de ações faciais ou numa &quot;tabela de importâncias», definidas por um artista.
Em o modelo, são usados os mesmos pesos para todas as combinações de expressões e visemas, como mostrado na Equação 4.11.
Cada FAP (F APj) é gerado por a soma ponderada dos parâmetros de expressão (Ej) e visema (Vj).
F APj $= Ej+ 3 Vj O modelo de Geração de Comportamento dos Olhos gera movimentos de pálpebras e cabeça relacionados com a direção e deslocamento do olhar.
Mas algumas expressões faciais definem seus valores para as pálpebras e cabeça.
Em esses casos, também é proposta uma soma ponderada para resolver o conflito (25% para o parâmetro de animação do comportamento dos olhos e 75% os da expressão).
Durante o processamento das ações faciais, alguns comportamentos dos olhos geram movimentos de cabeça, como descrito anteriormente.
Se isso acontece, o movimento é mantido.
Se não há movimentos da cabeça, então é utilizada uma pequena perturbação nos parâmetros de animação da cabeça usando o algoritmo de Perlin noise para gerar suaves movimentos, de maneira a produzir animações mais realistas.
Facial Description Language (FDL) A FDL é uma extensão de um trabalho anterior, em que as ações faciais (no caso, ações dos olhos) foram codificadas como um conjunto de comandos em scripts Lua, chamada GDL (Gaze Description Language).
A FDL, portanto, encapsula a GDL e provê a inclusão de ações de expressões faciais e fala.
Optou- se em usar Lua ao invés de uma sintaxe baseada em XML por duas razões:
A sintaxe da Lua é mais &quot;limpa «(o que torna mais clara a leitura por pessoas) e sua estrutura ajuda a entender intuitivamente a seqüência de ações;
E é uma poderosa maneira de se incorporar funcionalidades no modelo/ framework, uma vez que a linguagem comunica- se de maneira bi-direcional com a linguagem C/ C+ (pensando- se em futuras extensões).
A FDL, assim como a GDL, possui dois propósitos principais:
Geração de comportamento dos olhos e expressões faciais;
Um script de storyboard de ações contém informação sobre a fala (nomes dos arquivos de som e texto) e dos arquivos que contém a seqüência de expressões faciais e de olhar, como mostra o exemplo abaixo.
Caso não exista um desses componentes, deve ser atribuido o valor &quot;none «às variáveis.
Se a saída desejada é um arquivo contendo os parâmetros de animação ao longo de o tempo, deve- se especificar o nome do arquivo de saída.
Uma seqüência de expressões faciais pode ser descrita como no exemplo abaixo (arquivo «expressions.
Sb&quot;), que informa as expressões da Base de Dados Emocional desejadas ao longo de o tempo e sua duração.
De forma similar, é descrita a seqüência de comportamentos dos olhos, como visto no exemplo abaixo (arquivo «eyes.
Sb&quot;) que inclui os comportamentos da GDL (65) e o movimento de &quot;perseguição «dos olhos (pursuit) adicionado num dos estudos de caso desenvolvido (Seção 5.2).
Como mostrado no primeiro exemplo de script FDL, para a geração da fala são especificados um arquivo de som e sua descrição textual (transcrição da fala em texto).
Após o preocesso de mapeamento fonema-visema, o modelo pode gerar um arquivo FDL como o exemplo abaixo, que é processado por a etapa de Mapeamento Visema-Parâmetros de Animação.
Alternativamente, esse tipo de script pode ser armazenado e usado em aplicações com falas-padrão (otimizando a aplicação, não necessitando o processamento dos arquivos de fala repetidamente), ou mesmo refinado manualmente por algum artista.
Seguindo a mesma sintaxe dos outros arquivos, é descrita a seqüência de visemas ao longo de o tempo.
A Figura 4.11 apresenta exemplos de animações geradas a partir de scripts FDL, que podem ser aplicadas a diferentes personagens.
Outros resultados podem ser vistos na Seção 5.2.
Vê- se que é possivel controlar os avatares através de um variado conjunto de comandos de ações faciais, de maneira explicita.
Com a forma de controle apresentada, pode- se criar animações personalizadas no sentido de criar qualquer fala (em língua inglesa), combinada ou não com expressões faciais e comportamentos dos olhos.
No entanto, nessa abordagem não é possível gerar automaticamente a personalização das ações faciais em si.
Por exemplo todos os modelos irão sorrir da mesma maneira, a menos que, com um trabalho manual, especifique- se diferentes tipos de sorriso através de FAPs num editor gráfico como o do trabalho de Rodrigues ou a ferramenta de edição do Xface.
Controle por Animação Dirigida por Performance A arquitetura do modelo de Controle por Animação Dirigida por Performance está ilustrada na Figura 4.12, correspondente às etapas Detecção/ Rastreamento dos Dados Faciais e Mapeamento Real--Virtual da Figura 4.1 apresentada no início do capítulo.
O modelo leva em consideração dois tipos de informação, que são providas por a etapa de Rastreamento dos Pontos:·
Informações &quot;baixo-nível «São as informações numéricas, como por exemplo a nova posição dos pontos característicos rastreados num frame.
Esses dados são mapeados para os pontos característicos do avatar na etapa de Mapeamento por a FAPU, como mostra a Seção 4.3.2;·
Informações &quot;alto-nível «São as informações semânticas sobre expressões faciais (estado afetivo) ou ações faciais mais específicas sobre alguns componentes, como por exemplo, estados da boca:
&quot;fechada», &quot;aberta», &quot;sorrindo», &quot;fazendo bico», etc..
Essas informações servem de informações complementares para o Relacionamento entre FAPs (Seção 4.3.3).
Rastreamento dos Pontos O modelo parte do pressuposto que os algoritmos de Visão Computacional vão detectar e/ ou rastrear um conjunto de pontos característicos da face do usuário.
Esses pontos característicos serão diretamente associados aos FPs do padrão MPEG-4.
Assim, o mapeamento se dá através da determinação dos deslocamentos dos FPs em unidades de FAPU (da pessoa real e do modelo 3 D), associando- se aos seus devidos parâmetros de animação (Seção 4.3.2).
A etapa de Rastreamento de Pontos faz o rastreamento de marcadores localizados na face do usuário, através da técnica de fluxo ótico.
Essa técnica calcula uma aproximação do campo de vetores velocidade a partir de o movimento da intensidade dos pixels da imagem numa sequência de vídeo.
Em o modelo, utiliza- se a proposta de Lucas- KanadeTomasi (LKT) (46).
Para dar início ao rastreamento, é necessária uma etapa de Inicialização do algoritmo, clicando- se no centro dos marcadores.
FAPU, e os pontos azuis são determindos por o Relacionamento entre FAPs estabelecido no modelo.
Mapeamento por a FAPU pontos, calcula- se a FAPU da pessoa no primeiro frame da captura (F AP Uusuario), como no exemplo.
Esses pontos devem corresponder aos pontos apresentados na Seção 4.1.3 (para o cálculo do FAPU do avatar).
Como alguns pontos não possuem marcadores correspondentes, é necessário fazer a aproximação dos seguintes FPs, conforme as equações 4.12 a 4.15: Um dos pontos característicos, o correspondente à ponta do nariz r em cada passo tempo i (ri), é considerado o ponto de referência para o cálculo dos deslocamentos de todos os outros pontos, por não possuir significativos deslocamentos em relação a o restante do conjunto de pontos rastreados.
A partir de rini (ponto r no primeiro frame) e os pontos dj $ | tj -- rini|.
Para cada passo de tempo, obtém- se o conjunto dos pontos rastreados Ti por o algoritmo LKT, cujas distâncias em relação a ri são calculadas, obtendo- se o conjunto dos deslocamentos Di.
O modelo assume que a cabeça do usuário não vai se mover.
Até o momento, nos experimentos, procurou- se trabalhar com a face numa distância constante da câmera, de forma a não alterar a escala da face projetada.
Caso a pessoa se aproxime ou distancie da câmera, seria necessário reescalar a FAPU e o conjunto de referenciais Dini.
Uma alternativa simples para isso seria, utilizando um detector de face robusto, reescalar a FAPU e Dini de acordo com o raio da face detectada.
O modelo também não considera rotações na cabeça.
Uma vez que podem ocorrer pequenas variações na movimentação da cabeça durante a captura, em cada passo de tempo é aplicada uma correção, baseada na diferença entre os pontos rini e ri, na determinação de Di.
Para a determinação dos valores do conjunto de parâmetros de animação (FAPs) num frame, é feita então a diferença entre Di e Dini, normalizada por a FAPU do usuário.
F AP si $= Di -- Dini F AP Uusuario O sinal dos FAPs devem ser dados de acordo com as direções especificadas no conjunto de parâmetros.
Então, para a geração das animações no avatar, esses valores são escalados por a FAPU do avatar em seus FPs e são feitas as deformações do restante dos vértices da face, conforme a equação 4.6 e a função de deformação correspondente.
Relacionamento entre FAPs Optou- se em utilizar um conjunto não muito numeroso de pontos para o rastreamento, visando uma futura substituição do algoritmo com marcadores por algum algoritmo de detecção/ rastreamento mais robusto, que não necessite marcadores e inicialização (detecção) manual.
Por isso, um critério para a escolha dos pontos foi aqueles em que já existem alguns trabalhos de detecção automática na bibliografia.
Além disso, em algumas áreas (como a parte interna dos lábios) é inviável/ incômoda a colocação de marcadores.
Outro critério para a escolha dos pontos rastreado foi a &quot;independência muscular «de eles, baseada em observações.
Por esse motivo, julgou- se necessário também o rastreamento dos dois pontos laterais da pirâmide nasal.
A etapa de Relacionamento entre FAPs estima os valores de FAPs que não são obtidos via o mapeamento por a FAPU apresentado anteriormente.
A idéia básica desse relacio-namento é estabelecer uma hierarquia de FPs e seus FAPs, como se fossem &quot;pseudomúsculos», baseados em observações.
Sejam dois FPs relacionados, um rastreado por CV (F PR) e outro que não é rastreado (F PN).
É feita uma associação entre os FAPs desses FPs, normalmente entre os que possuem o movimento na mesma dimensão (as regras de associação são definidas por observação).
O valor de um F APN é determinado em função de o seu relacionamento com o F APR, de acordo com a Equação 4.17: F APN $= cRN· F APR Onde cRN é o intensificador que relaciona o F APR e o F APN, definido por observações.
De essa maneira, o Relacionamento entre FAPs possui duas tarefas principais:·
Estimar valores de deslocamento de FPs que não são rastreados, como os FPs das bochechas e da parte interna dos lábios.
Em essas situações, propôs- se um relacionamento hierárquico que pode ser visto por as setas pontilhadas pretas na Figura 4.15.
Para os FPs das bochechas, optou- se em associar- los com os FPs dos cantos da boca.
E para cada FP da parte interna dos lábios, associou- se o FP correspondente da parte externa.
O relacionamento entre os FAPs desses FPs é dado por a dimensão da direção do movimento.
O valor dos cRN para os FAPs das bochechas com movimento na dimensão x é 0.5 e na dimensão y é 0.25.
Para os FAPs relacionados dos lábios, definiu- se cRN $= 1.·
Estimar valores de alguns FAPs com movimentos no eixo z, não considerado por o mapeamento, que só lida com a informação bi-dimensional das imagens e gera valores para os FAPs com direções do movimento em x e y.
Em esse caso, não é necessário relacionar dois FPs diferentes, mas o valor de um FAP com direção no eixo x ou y com do eixo z.
Em o momento, o modelo faz o mapeamento do FAP T HRU St_ JAW (deslocamento em profundidade da mandíbula) com o FAP OP EN_ JAW (abertura da mandíbula), com cRN $= 0.5.
Relacionamentos gerados por informações &quot;alto-nível «Outro tipo de relacionamento considerado nessa etapa é através da definição de algumas regras, a partir de informações &quot;alto-nível «obtidas durante o rastreamento.
Em o momento, como o modelo não provê esse tipo de informação, foram feitos alguns estudos preliminares considerando alguns estados de boca que devem ser identificados manualmente no protótipo do modelo (como &quot;fazendo bico «e &quot;sorriso&quot;), uma vez que existe a variação na expessura dos lábios nessa situação, e as informações vindas de CV apenas os deslocamentos dos pontos externos da boca.
Em esse caso, o valor do intensificador cRN é baseado no estado de boca setado.
A Figura 4.16 mostra o impacto visual desse tipo de informação:
A Figura 4.17 mostra um exemplo de animação gerada seguindo a metodologia apresentada.
Outros resultados podem ser vistos na Seção 5.3, assim como uma avaliação qualitativa do modelo.
Algumas Considerações A criação do modelo contou com três etapas principais de pesquisa e desenvolvimento, correspondentes aos modos de controle e síntese da animação facial:
Inicialmente, foram escolhidos o modelo de parametrização das ações faciais e as técnicas &quot;baixo-nível «para implementar- las (geração das deformações).
Optou- se em seguir a parametrização baseada no padrão MPEG-4 de Animação Facial (houve, no entanto, algumas extensões, apresentadas na Seção 4.1.2).
Para a geração das deformações, optou- se em usar, inicialmente, funções de cosseno, como em.
Para zonas com pontos de controle muito próximos e correlacionados (como os dos lábios), optou- se em utilizar uma única zona de influência e fazer a interpolação dos pontos através de uma função de base radial.
A fim de oferecer uma opção de parâmetros mais &quot;alto-nível «que os FAPs do padrão MPEG-4, foi desenvolvido um modelo de controle que inclui uma linguagem de scripting de ações faciais, também facilmente extensível.
Foram incorporados alguns modelos da literatura para a geração de fala sincronizada e comportamentos dos olhos, combinados com um pequeno conjunto de expressões faciais definido em termos de FAPs, e acessíveis via linguagem de scripting.
Em a Seção 4.3 é proposto um modelo que, a partir de a captura de um conjunto de pontos-chave da face do usuário, gera animações verdadeiramente diferenciadas no avatar, procurando refletir os sinais expressivos da face do usuário.
O modelo gera os movimentos com base no deslocamento dos pontos capturados, medidos em termos de FAPU e traduzidos para o conjunto de parâmetros de animação (FAPs estendidos).
De essa maneira, o mapeamento para quaisquer modelos 3D devidamente parametrizados conforme o esquema apresentado na Seção 4.1 se torna imediato.
O modelo proposto pode ser visto como um framework para o desenvolvimento de aplicações e pesquisas em Animação Facial, que permite a inclusão de técnicas para detecção de eventos e a geração de animações faciais interativamente.
Isso pode se dar através da inclusão de diferentes técnicas de RV e CV, que diretamente gerem streams de FAPs ou arquivos FDL (por exemplo, por a inclusão de um modelo emocional que responde a eventos do usuário ou mesmo modelos de processamento de linguagem natural que gerem fala por texto e/ ou áudio).
Ao mesmo tempo, vê- se que é possível a extensão dos componentes ilustrados nos diagramas de arquitetura dos modelos, de maneira a incluir métodos para melhoria das animações, como outras técnicas de deformação ou manipulação das texturas, para a geração de movimentos específicos e rugas de expressão.
Focando no modelo que faz a geração de animações dirigidas por performance, vê- se que ele pode ser estendido através da inclusão de algoritmos de Visão Computacional para detecção e/ ou tracking mais robusto (por exemplo, sem marcadores ou incluir novas features, como a movimentação da cabeça) e melhorias no mapeamento usuário-avatar.
O modelo de animação facial descrito no capítulo anterior permite a geração de animação de faces em tempo-real por duas técnicas de controle:
Por script ou dirigida por performance.
As próximas seções apresentam algumas informações técnicas do protótipo desenvolvido, resultados visuais das animações geradas, algumas aplicações desenvolvidas como estudos-de-caso para a exploração do modelo de controle por script e uma avaliação feita com usuários para avaliar qualitativamente o modelo de controle por PDA.
O Protótipo Para validação do modelo, foi construído um protótipo, que pode ser considerado um framework para o desenvolvimento de pesquisas e aplicações.
O protótipo foi desenvolvido em C/ C+, usando a API gráfica OpenGL.
Durante a primeira etapa da pesquisa, utilizou- se os recursos do Xface para a geração das animações faciais, visualização e interface.
Depois, como explicado anteriormente na Seção 3.3.4, optou- se em implementar uma biblioteca própria para geração das animações.
As animações geradas por o modelo podem ser salvas em arquivos contendo as streams dos FAPs a cada frame, como mostra a Figura 5.1.
Esse arquivo segue o formato encontrado nos trabalhos de (Lucia) e (7) (Xface).
Portanto, os parâmetros de animação gerados podem ser processados e as animações, visualizadas nos sistemas citados.
Porém, a geração das animações será conforme o modelo de deformação de cada aplicação, o que pode gerar resultados visuais diferentes.
Taxa de atualização Frame 0 Número do frame Frame 1 Número de frames Streams de FAPs:
Em esse contexto, uma consideração importante a se fazer é que os modelos de controle dos parâmetros de animação fornecem como saída streams de FAPs.
O processamento da fala se dá através da comunicação via scripts com os aplicativos do CSLU Toolkit.
Para a FDL, foi utilizada a linguagem Lua e uma biblioteca que faz a comunicação entre C+ e Lua, chamada Diluculum1.
Foram utilizados os algoritmos de visão computacional da biblioteca OpenCV2.
Em o estágio atual, o protótipo permite 3 tipos de entrada:
Scripts FDL, arquivos com animações FAPs já geradas e vídeo.
Os algoritmos usados para o rastreamento dos pontos característicos exigem o uso de marcadores e inicialização manual (clicar nos pontos), para uma captura estável.
Foram feitas também algumas integrações com ferramentas do grupo de pesquisa, usando algoritmos de detecção sem marcadores (para um conjunto menor de pontos característicos).
Essa integração permite a geração usando diretamente a web cam.
Porém, como esses algoritmos ainda estão em fase de estudo, o resultado da detecção é ainda um pouco instável.
A Figura 5.2 mostra exemplos da integração do modelo num protótipo de ambiente colaborativo desenvolvido por o grupo de pesquisa.
Controle por Script:
Estudos de Caso A fim de se investigar a capacidade de geração de animações variadas por o modelo de controle por scripts, foram desenvolvidos alguns &quot;estudos de caso», ou seja, aplicações e produções pequenas com alguns objetivos diferentes:·
A exploração da robustez da FDL em termos de:
&quot;o quão usável ela é para produzir diálogos expressivos «e &quot;se as animações produzidas são compreensíveis ao público geral», via a elaboração das cenas de animação de um filme técnico para uma conferência3, sem qualquer retoque por artistas.
Todos os diálogos da personagem virtual foram gerados automaticamente por scripts FDL.
A edição dos scripts, no entanto, foi feita manualmente.
A Figura 5.3 mostra alguns snapshots do filme;·
Alguns testes preliminares de geração de animações interativamente, usando algoritmos simples de CV (detector de face, detector de sorriso, por exemplo) para detectar eventos e mapear para ações de alto-nível, disparando animações pré-gravadas ou gerando scripts de ações em tempo real (Figura 5.4 a), c) e d) ilustra as aplicações interativas desenvolvidas), a saber:
CRS) saúda e apresenta informações sobre a faculdade de acordo com a detecção de dois tipos de evento:
Detecção (para as saudações) e permanência da face do usuário por algum tempo (informações).
Para a detecção da face, também foi utilizado o detector do OpenCV.
FAPs foi estabelecido com base em valores predefinidos (ações que integram as bases de dados dos modelos de Geração de Comportamentos dos Olhos e Geração de Expressões Faciais), sendo as animações geradas em tempo-real.·
Desenvolvimento de uma interface para a geração de ground-truth sintético, a partir de o modelo de geração das deformações e scripts FDL (Figura 5.4 b).
Para maiores detalhes técnicos desses estudos de caso, recomenda- se a leitura de 4.
Em, o modelo de controle por script proposto é apresentado como um framework extensível para o desenvolvimento de aplicações com Animação Facial.
O desenvolvimento desses estudos de caso mostraram que o modelo é capaz de produzir uma variedade de animações e ser integrado em aplicações interativas usando esse abordagem de controle de ações faciais alto-nível apresentada no capítulo anterior.
Controle por PDA:
Avaliação com Sujeitos O protótipo desenvolvido permite o controle por Animação Dirigida por Performance, que recebe como entrada um vídeo de pessoa com marcadores na face e necessita de uma etapa de inicialização manual, onde a pessoa clica no centro dos marcadores.
Depois dessa etapa, o algoritmo de rastreamento obtém os deslocamentos dos pontos, que são mapeados para o avatar conforme o explicado no capítulo anterior.
O protótipo ainda permite, se o usuário desejar, a geração dessas animações interativamente, de maneira a corrigir possíveis erros de rastreamento.
De essa maneira, é possível gerar animações mais acuradas, para situações em que não é exigido tempo-real, como o caso da avaliação apresentada a seguir.
A Figura 5.6 mostra um exemplo de quadros de animação geradas por o modelo, executadas por três personagens diferentes5.
Esta seção apresenta uma avaliação feita com usuários, com o objetivo de fazer uma análise preliminar para verificar se o modelo desenvolvido consegue refletir a expressão da pessoa real, de maneira perceptível ao usuário.
A avaliação foi elaborada de maneira a investigar três situações principais:
Metodologia Para fazer a avaliação alcançando um número satisfatório de voluntários e um público variado, optou- se em fazer um questionário na internet.
Para a elaboração das animações, contou- se com a participação de 5 voluntários, que gravaram vídeos seguindo uma seqüência de ações pré-definida.
Esses voluntários serão chamados de atores no decorrer desse capítulo.
Atores e Avatares Utilizados A Figura 5.7 mostra os atores e seus respectivos avatares, obtidos através do software Facegen6, que gera modelos de face 3D baseados em fotografias.
Para os fins da avalia6 ção, também foram utilizados três avatares diferentes dos atores, sendo dois de eles também gerados por o Facegen e um de eles um personagem cartum disponível na internet7.
Gravação da Performance e Geração das Animações Para a obtenção dos dados para a geração das animações, foi gravada a performance dos 5 atores, seguindo uma seqüência de expressões faciais pré-definida.
As expressões faciais solicitadas foram as 6 expressões básicas de Ekman, a saber:
Alegria, tristeza, surpresa, nojo, medo e raiva.
A Figura 5.8 mostra quadros da atuação de dois atores, correspondentes a cada uma das expressões e o mapeamento para o seu avatar correspondente.
Foram selecionados, manualmente dos vídeos, os frames considerados mais significativos (visualmente expressivos) dessas expressões faciais, e gerou- se a expressão de cada ator em todos os avatares 8.
Foram geradas as animações completas da expressão de alegria de cada ator, também com todos os avatares9.
O processo de geração dos parâmetros de animação para cada vídeo dos atores foi feito apenas uma vez (gerando um arquivo FAP), junto com a animação de um avatar, e a geração das animações dos outros avatares foi feita através do arquivo FAP gerado.
Questionário O questionário10 foi dividido em 3 partes, com 20 questões objetivas no total (obrigatórias), e 1 parte com uma questão dissertativa (opcional).
As três partes do questionário foram elaboradas da seguinte maneira:·
Parte 1 Composta por 8 questões, o objetivo é verificar se os voluntários conseguem identificar a expressão do personagem virtual correspondente ao personagem real, em imagens.
Exemplos das questões podem ser vistos na Figura 5.9.
Em as primeiras 4 questões, utilizou- se o avatar correspondente ao ator e nas 4 seguintes, um avatar diferente.
Em cada questão, existe uma das opções em que o avatar está executando a expressão do ator em questão, e nas outras opções, o avatar está executando a mesma expressão capturada, de atores diferentes.·
Parte 2 Composta por 7 questões, o objetivo é verificar se os voluntários conseguem associar corretamente cada uma das imagens de personagens virtuais com uma das pessoas reais, de acordo com a expressão facial de eles.
Um exemplo de questão pode ser visto na Figura 5.10.
Em essa etapa, foi utlizado um avatar diferente de todos os atores.·
Parte 3 Composta por 5 questões, o objetivo é o mesmo da Parte 1, só que usando vídeos.
Exemplos das questões podem ser vistos na Figura 5.11.
Assim como na Parte 1, em cada questão, existe uma das opções em que o avatar está executando a animação do ator em questão, e nas outras opções, o avatar está executando a mesma animação capturada, de atores diferentes.·
Grupo de pessoas que fazem parte da comunidade acadêmica do curso de Computação (Grupo COMP, com 44 pessoas).
De esse grupo, fêz- se ainda uma subdivisão, considerando:
Relacionando- se as médias de alguns grupos de voluntários, pode- se fazer algumas observações:·
Os usuários da área da Computação acertaram, em média, mais questões que os que não são da área;·
Ainda que com pouca diferença, os usuários da computação que não trabalham dom CG e PI, acertaram mais do que os que trabalham, exceto na Parte 3.·
Aparentemente, o fato de alguns usuários conhecerem os sujeitos avaliados no teste, não fez diferença relevante nos resultados obtidos.
A diferença das médias foi maior também na Parte 3, porém, observa- se por a Tabela 5.1 que os desvios-padrão nessa etapa são maiores também, o que não permite maiores considerações sem testes estatísticos de comparação de médias.
A Figura 5.14 apresenta as médias dos grupos por as partes do questionário separadamente.
Algumas observações podem ser feitas:·
A Parte 1, nas sub-questões 1 a 4 (avatar correspondente ao ator) obtiveram maior acerto que as sub-questões 5-7 (avatar diferente), sendo os valores de 80% e 63%, na média.·
Em a Parte 3 do formulário, o grupo que mais acertou em média foi o grupo de pessoas que trabalham com CG ou PI.
Como conclusões principais, baseadas nessa análise preliminar, pode- se citar:·
Em geral, a média dos acertos foi maior nas questões em que os atores foram representados por o seu avatar de forma correspondente;·
A parte 3 do questionário (avaliação com vídeos) foi a que obteve as menores taxas na média de acertos, como já comentado anteriormte· Conhecer ou não os atores aparentemente não apresentou impacto na média de acertos.
É importante ressaltar que essa análise é bastante preliminar.
Para maiores conlusões, devem ser realizados testes de significância a fim de estabelecer comparações estatísticamente válidas entre os dados obtidos.
Este trabalho apresentou um modelo cujo objetivo é prover animações de face personalizadas e expressivas para avatares, através da escolha de duas formas de controle:
Por script ou por Animação Dirigida por Performance.
O modelo de animação por script permite a geração de animações variadas, incluindo comportamentos verbais e não-verbais (fala, expressões faciais e comportamentos de olhos).
Os comportamentos são descritos em alto-nível como ações faciais numa linguagem de descrição chamada FDL.
Falando em termos de &quot;animações personalizadas», a estratégia de controle por script permite a geração de animações personalizadas no sentido de que, através de um script FDL, editado manualmente ou suprido por um modelo que o gere baseado na detecção de eventos da aplicação, elas serão únicas, geradas em tempo real.
A personalização nesse caso, está da descrição das ações, definindo o comportamento do personagem/ avatar.
No entanto, como comentado na Seção 4.4, no momento em que se indicar para o personagem &quot;sorrir», ele sempre sorrirá conforme os parâmetros de sorriso definidos no modelo de Geração de Expressões Faciais.
O estudo de caso do modelo de controle por script chamado &quot;Espelho Virtual», apresentado no capítulo anterior, mostra que, usando essa abordagem é possível se obter uma Animação Dirigida por Performance baseada em ações de alto-nível, porém sem a personalização dos movimentos, como na proposta do modelo de controle por Animação Dirigida por Performance.
Em essa aplicação, a identificação de um pequeno conjunto de estados alto-nível (olhos e boca abertos ou fechados, sorriso, direção horizontal do olhar) gera animações que, apesar de &quot;imitar «o que o usuário está fazendo, não reflete as características de sua movimentação.
Como trabalhos futuros para melhorias do modelo, pode- se citar o acréscimo de um modelo de emoções que permita a geração de expressões combinadas, como o de e de uma modelo que faça a geração dos parâmetros de animação especificados nas ações faciais baseado em dicas de personalidade (baseado nas idéias do trabalho de (6)), como a geração de um &quot;sorriso tímido», por exemplo.
Tecnicamente, é uma contribuição desse trabalho a descrição de uma metodologia de construção de uma plataforma de animação facial usando ferramentas livres ou de código aberto, integrando alguns modelos da bibliografia capazes de produzir uma combinação substancial de ações faciais em animações de boa qualidade visual, comparada com o estado da arte das pesquisas.
O modelo de Animação Dirigida por Performance proposto permite a geração de animações, em tempo-real, que refletem as ações faciais do usuário, capturadas por uma webcam.
Novamente falando em termos de &quot;animação personalizada», esse modelo gera animações únicas, uma vez que é baseada diretamente nos dados de pessoas reais, que se movimentam de forma diferente umas das outras.
Comparando com a aplicação &quot;Espelho Virtual «citada acima e relacionando com os trabalhos apresentados na Seção 3.2, vê- se os modelos de Animação Dirigida por Performance podem ter diferentes objetivos:
&quot;imitar «a performance do usuário de forma geral, em alto-nível (o usuário sorri e o avatar sorri com o mesmo sorriso que o modelo produz para todos), traduzir a performance do ator em outra linguagem de representação (por exemplo, realista em cartum, como na proposta de (64) ­ o usuário sorri e o seu avatar cartum sorri como um cartum) ou tentar refletir exatamente a performance do usuário (o usuário sorri e o avatar sorri do mesmo jeito que o usuário).
Em o estágio atual do modelo de PDA, os algoritmos usados para o rastreamento dos pontos característicos exigem o uso de marcadores, para uma captura estável.
É importante frisar que o foco da pesquisa é o mapeamento dos dados obtidos por CV para a da face virtual, portanto, o algoritmo de rastreamento usado pode ser substituído futuramente por outro mais robusto, que não necessite marcadores.
De a mesma forma, como mostra o estudo preliminar de relacionamento entre FAPs baseado em informações &quot;altonível «(Seção 4.3.3), é de interesse que algoritmos que forneçam essas informações sejam acrescentados, para que essa etapa do modelo seja viabilizada no processo de reflexão do usuário em tempo-real.
Ainda como trabalhos futuros para melhorias desse modelo de controle, pode- se citar uma integração com os modelos de Geração de Expressões Faciais, Sincronização da Fala e Geração de Comportamentos dos Olhos do modelo controle por script para fornecer dados que auxiliem no relacionamento entre os FAPs, quando fornecida uma informação de alto-nível por os algoritmos de detecção/ rastreamento.
Haveria, portanto, a necessidade de um mapeamento das informações alto-nível identificadas e fornecidas por CV para as ações faciais do modelo de controle por script e um modelo de integração/ combinação dos parâmetros fornecidos por esses modelos.
A respeito de o modelo de Geração/ Visualização da Animação Facial, pode- se dizer que é adequado para a geração em tempo-real das animações.
Porém trata- se de um modelo bastante simples, que poderia ser melhorado com a inclusão de outras técnicas de deformação existentes na literatura, como as apresentadas na Seção 2.4, tanto para automatizar o processo de especificação dos pontos de controle e zonas de influência como para a geração de deformações mais realistas específicas de cada região do rosto, como rugas de expressão.
Sobre a avaliação feita com usuários apresentada no capítulo anterior, percebe- se que, no geral, os usuários tiveram uma taxa de acertos satisfatória no questionário (média geral 73%), o que indica que é possível perceber a reflexão dos movimento dos atores nos avatares.
É necessario, no entanto, realizar alguns testes estatísticos nos dados obtidos, a fim de se ter conclusões estatisticamente válidas sobre os pontos em que se mostrou a intenção de investigar.
Falando de um modo geral, o modelo é promissor e gera resultados variados, &quot;personalizados «(dentro de a proposta dos modos de controle investigados) e visualmente atraentes em tempo real, alcançando satisfatoriamente os objetivos apresentados nessa dissertação.
