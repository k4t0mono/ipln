A tarefa de Descoberta de Conhecimento em Base de Dados (KDD) caracteriza- se por inúmeras atividades distribuídas em etapas altamente iterativas e interativas, o que a torna complexa, e portanto, bastante dependente da pessoa que a está realizando.
Em este trabalho é apresentado um modelo de apoio ao usuário na documentação de aplicações do processo de KDD, e o protótipo de um ambiente de apoio a sua utilização.
O principal objetivo deste modelo é disponibilizar mecanismos que possibilitem estruturar informações que facilitem a gerência de uma aplicação de KDD, independente dos recursos utilizados.
O ambiente irá possibilitar a captura, armazenamento e recuperação das informações e estruturas definidas no modelo.
Os requisitos foram levantados a partir de um estudo aprofundado sobre as propriedades do processo de KDD e dois estudos de caso.
Vii O interesse por o empreendimento de descoberta de conhecimento em bases de dados está crescendo cada vez mais dentro de as organizações, impulsionado por a consolidação da tecnologia de Banco de Dados que propicia, de forma confiável, o armazenamento e recuperação de grandes volumes de dados.
Tradicionalmente, estes dados são manipulados para suprir as necessidades operacionais das organizações.
Contudo, eles guardam sua história, o que fomentou uma mudança na visão do tipo de informação que pode ser extraída a partir de os mesmos.
O que passou a ser importante é o conhecimento que pode ser inferido a partir de os dados, e a capacidade de se tomar decisões baseada neste conhecimento, encontrando informações desconhecidas anteriormente, eventos e tendências que são potencialmente úteis.
Como a área de descoberta de conhecimento é relativamente nova, seus conceitos ainda não estão totalmente consolidados e apesar de estarem sendo desenvolvidas pesquisas há vários anos, existem ainda algumas questões em aberto.
Inicialmente, as pesquisas foram centradas na construção, implementação e otimização de técnicas com capacidade de extração de padrões a partir de uma grande massa de dados, denominados algoritmos de mineração.
Porém, quando estes algoritmos eram aplicados numa tarefa de descoberta de conhecimento num ambiente real, produziam resultados que não eram suficientemente confiáveis, principalmente devido a as possíveis inconsistências existentes nos dados, como &quot;valores faltando «ou incoerentes e a necessidade de adequação destes dados à técnica de mineração implementada por o algoritmo.
A partir de estas experiências, foram então detectados uma série de fatores importantes que influenciam diretamente na confiabilidade dos resultados obtidos através do uso de algoritmos de mineração.
Estes fatores devem ser considerados em atividades acomodadas em diferentes etapas da tarefa de descoberta de conhecimento, caracterizando um processo.
Fayyad Propuseram o termo KDD (Knowledge Discovery in Databases ou Descoberta de Conhecimento em Bases de Dados) para referenciar este processo, que basicamente engloba as etapas de definição do escopo da aplicação, preparação de um conjunto de dados, mineração (quando algoritmos serão aplicados sob esses dados, com o intuito de extrair os padrões), avaliação destes padrões e consolidação dos padrões na forma de conhecimento.
Este conhecimento, por sua vez, deve ser válido, novo, potencialmente útil e compreensível, para usuários e tarefas.
O processo de KDD é altamente iterativo, pois a manipulação de grandes volumes de dados, junção de diversas tabelas e tratamento de ruídos implica na necessidade de um constante ajuste nos parâmetros e reexecução dos procedimentos de preparação dos dados, até que o conjunto de dados seja considerado confiável e apropriado à técnica de mineração adotada.
O mesmo se observa durante a aplicação destas técnicas, onde um grande número de iterações são necessárias para ajuste dos parâmetros.
A intercalação entre atividades de etapas diferentes também é grande, como por exemplo, a avaliação dos padrões pode mostrar inconsistências no domínio dos dados e determinar a volta à etapa de preparação dos dados.
Outra característica inerente ao processo é a sua interatividade, pois a maioria das atividades envolve avaliação e tomada de decisão por o usuário que está conduzindo o processo.
No que diz respeito às necessidades de apoio à tarefa de KDD, observa- se outro fator relevante:
Em a prática são utilizadas várias ferramentas e ambientes de KDD, na maioria das vezes não integrados.
Isto se deve à multi-disciplinaridade da área de KDD, que é uma interseção de diversas outras, incluindo banco de dados, estatística, reconhecimento de padrões (Ia), otimização e visualização.
Além de todas estas peculiaridades, não existe uma metodologia disponível genérica o bastante para descrever e apoiar todos os estágios de uma aplicação de KDD.
Embora exista um grande número de técnicas e aplicações disponíveis, ainda não existem respostas aceitáveis para questões referentes a quais técnicas são mais interessantes para o problema a ser tratado e como a aplicação destas se comportam.
Por todas estas características, a aplicação do processo de KDD é considerada nãotrivial e, por conseqüência, a tarefa do usuário, que deve ser apoiado na sua definição, execução e documentação.
Existem hoje diversas ferramentas disponíveis que oferecem as funcionalidades necessárias para executar descoberta de conhecimento adequadamente.
Entretanto, pouco ou nenhum suporte é dado para a definição da aplicação, a seleção das técnicas e parâmetros apropriados para o problema e o domínio dos dados em questão, e para a documentação das tarefas executadas e dos resultados obtidos.
São apresentadas algumas propostas neste sentido.
Apesar de sua contribuição, elas falham em endereçar algumas questões importantes no que diz respeito à integração de ferramentas, ao registro das iterações e redirecionamentos ocorridos no processo e à coleta de experiências, principalmente na forma de manutenção das informações sobre o raciocínio que levou a ações e a tomada de decisão.
Baseado nisto, o objetivo deste trabalho é desenvolver um modelo de documentação do processo de KDD, integrado a um ambiente de apoio, visando auxiliar o usuário a definir e conduzir uma aplicação de KDD, através de mecanismos que o possibilitem realizar uma documentação detalhada de todas as informações que envolvem as atividades realizadas e suas iterações, bem como dos recursos utilizados e os resultados obtidos.
Esta abordagem também pode ser útil para definir e conduzir aplicações futuras, a coletar experiências de projeto, auxiliar na tomada de decisão, e ajudar a explicitar e entender melhor o processo de KDD, e assim poder apoiar- lo de forma mais efetiva.
O modelo proposto foi baseado num estudo aprofundado do processo de KDD, em alguns trabalhos relacionados com o apoio ao usuário na tarefa de descoberta de conhecimento, bem como dois estudos de caso.
A abordagem proposta é baseada nos trabalhos de gestão de memória organizacional orientados a processo, que visam capturar principalmente o conhecimento informal utilizado ou gerado durante processos organizacionais.
O restante do de este documento está estruturado como segue:
O Capítulo 2 apresenta duas abordagens para o processo de KDD, suas características e dificuldades em sua aplicação.
Em o Capítulo 3, estão apresentados trabalhos relacionados com o apoio à tarefa do analista na construção de aplicações de KDD e com a área de Gerência de Memória Organizacional, bem como alguns tópicos de Inteligência Artificial.
Em o Capítulo 4, são apresentados os dois estudos de caso considerados para levantamento validação dos requisitos do modelo de documentação proposto.
A descrição do modelo encontra- se no Capítulo 5, e um protótipo do ambiente de apoio a este modelo é apresentado no Capítulo 6.
O Capítulo 7 apresenta as conclusões e trabalhos futuros.
A tarefa de descoberta de conhecimento é abordada sob diferentes pontos de vista na literatura.
Alguns autores a visualizam apenas como a aplicação de algoritmos sob uma massa de dados com o objetivo de extrair padrões.
Já outros a visualizam como um conjunto de atividades que envolvem a definição dos objetivos da aplicação, a preparação dos dados e a geração dos resultados, entre outras, como parte integrante desta tarefa.
Estas atividades são tratadas em diferentes etapas, caracterizando um processo.
É esta segunda visão que é adotada no escopo deste trabalho.
Em este capítulo são apresentadas duas abordagens para o processo de KDD com o intuito de contextualizar- lo de forma genérica, apresentando suas principais características e as dificuldades em sua aplicação.
A Seção 2.1 apresenta a abordagem de Fayyad Que está mais voltada para a definição e características das etapas do processo de KDD.
A Seção 2.2 descreve a abordagem de Brachmann que além de a estrutura do processo, enfoca a tarefa do analista na construção de aplicações.
As características do processo e da tarefa do analista, suas dificuldades e problemas são mais detalhados na Seção 2.3.
KDD é definido em como o processo não trivial de identificação de padrões válidos, novos, potencialmente utilizáveis finalmente compreensíveis a partir de os dados.
Esta definição está relacionada ao pressuposto de que este processo deve envolver pesquisa de estruturas, padrões, modelos ou parâmetros com algum grau de autonomia, caracterizando- o como não trivial;
A necessidade dos padrões descobertos serem válidos para novos dados com algum grau de certeza, novos e potencialmente utilizáveis para usuários e tarefas;
E, compreensíveis, se não imediatamente, após algum processamento.
A utilização do termo &quot;processo «implica que há vários passos iterativos e interativos envolvendo a preparação dos dados, a busca por padrões, a avaliação do conhecimento obtido e seu refinamento.
Ele é Interativo porque a maioria dos passos envolve avaliação e tomada de decisão por os usuários e, iterativo porque de qualquer etapa pode- se decidir por retomar etapas anteriores do processo, até que o resultado esperado seja alcançado.
A maioria das propostas existentes, apesar de pequenas diferenças, compartilham da mesma visão global do processo de KDD, entre elas, e.
As etapas definidas estão representadas de forma sintetizada na Figura 2.1 e descritas em maiores detalhes abaixo:·
Compreensão do domínio da aplicação:
Em esta etapa é analisada a viabilidade da aplicação através de um estudo inicial dos dados e do domínio da aplicação.
O objetivo da aplicação é delimitado e o que é conhecimento relevante, do ponto de vista do usuário final, é formalizado.·
Seleção de um conjunto de dados alvo:
Seleção de um conjunto de dados, um subconjunto de variáveis ou uma amostragem dos dados sobre os quais o processo será executado.·
Pré-processamento e Limpeza dos Dados:
Em esta etapa são realizadas operações básicas, tais como coletar informação necessária para modelar ou tratar dos ruídos, decidir estratégias para tratar campos sem valor, remover ruídos e valores desconhecidos, além de questões ligadas ao SGBD (Sistema Gerenciador de Banco de Dados), tais como tipos de dados, esquemas e mapeamento de valores inexistentes e desconhecidos.·
Transformação: Envolve o ajuste dos dados a serem utilizados.
Pode- se utilizar redução das dimensões, métodos de transformação para reduzir o número de variáveis a serem consideradas ou encontrar representações constantes para os dados.·
Mineração: Esta etapa representada na Figura 2.1 é detalhada por, dando lugar, na prática a três etapas, discutidas a seguir:·
definição da tarefa de mineração de dados:
Definir o objetivo do modelo a ser gerado por uma técnica de mineração de dados, considerando o objetivo geral do processo de KDD, ou seja, decidir a classe do algoritmo que será aplicado, como classificação, segmentação, associação, entre outras;·
definição do algoritmo de mineração de dados:
Cada tarefa de mineração pode consistir da aplicação de vários algoritmos.
Em esta etapa o (s) método (s) a ser (em) utilizado (s) para pesquisa dos padrões, bem como os modelos e parâmetros que podem ser apropriados, são selecionados;·
mineração: Aplicar o algoritmo sobre os dados selecionados, ou seja, buscar os padrões de interesse.
Esta etapa envolve o constante ajuste dos parâmetros para refinamento do modelo.·
Interpretação/ Avaliação:
Visualização, interpretação e avaliação dos padrões extraídos;
Remoção de valores redundantes ou padrões irrelevantes e a tradução destes para uma linguagem que possa ser entendida por o usuário.
Como destacado por, os resultados da etapa de mineração não têm efeito algum até que sejam validados e, somente após verificado seu grau de interesse (e.
g novidade, utilidade, validade) é que podem ser classificados como &quot;conhecimento».·
Consolidação do conhecimento descoberto:
Incorporar este conhecimento dentro de a execução do sistema, seja como documentação e relatórios, ou execução de ações baseadas nesse conhecimento.
A abordagem de Fayyad, apesar de conseguir adequar as principais atividades dentro de etapas no contexto de um processo, se detém ao fluxo de procedimentos a serem realizados entre estas etapas.
A principal crítica a esta abordagem é que ela define o processo como interativo e iterativo, mas não explora questões importantes que determinam estas características, tais como:·
a importância do conhecimento prévio do domínio dos dados e da aplicação como um fator determinante na condução do processo;·
as atividades de definição das tarefas a serem realizadas e do objetivo global do processo, que estão sempre sendo refinados à medida que os dados vão sendo explorados com maior profundidade e são responsáveis por os redirecionamentos no processo;·
a importância do papel do usuário dentro de o processo;·
a utilização de um conjunto heterogêneo de ferramentas, geralmente não integradas.
O processo de KDD proposto em (Figura 2.2) parte da descoberta da tarefa e dos dados centrados no objetivo geral do processo e as informações da base de dados e, passa por diversas etapas que são apoiadas por um conjunto de ferramentas de consulta, estatística visualização, apresentação transformação, que interagem com o usuário.
Brachman Consideram que o empreendimento é extremamente complicado e, apesar de existir uma ordem em suas etapas, o processo é iterativo e o analista pode se mover de uma etapa para outra a qualquer momento, sem seguir uma ordem determinada.
As etapas definidas estão descritas abaixo:·
Descoberta da tarefa:
Os objetivos globais do processo devem estar bem definidos e claros, por isso deve- se despender um tempo considerável realizando esta tarefa em conjunto com o usuário.
Domínio do Modelo Ação Relatórios Descoberta da tarefa MoLimpeza dos dados Objetivos Desenvolvimento do modelo Análise dos Dados Geração das saídas Descoberta dos dados Monitor Base de Consulta dados -- Ferramentas Fluxos: --
saídas Processo Estatística e I_ A.· Descoberta dos dados:
É necessário conhecer e entender a estrutura, o conteúdo e a qualidade dos dados.
Outra questão importante é avaliar se os dados podem satisfazer o objetivo definido.
É importante ressaltar que esta etapa e a etapa de descoberta da tarefa, citada acima, são executadas quase que simultaneamente, pois a definição dos objetivos da aplicação e as tarefas que devem ser realizadas para alcançar este objetivo estão muito ligadas ao dados existentes e ao seu domínio.
Consequentemente, o aprimoramento da visão sobre os dados pode refinar os objetivos e tarefas definidas anteriormente.
Estas duas etapas são intercaladas a todo momento:
Em um primeiro estágio, quando de a definição da aplicação;
E em várias outras vezes no decorrer de o processo, pois muitas vezes este aprimoramento é conseguido nas etapas posteriores de limpeza e análise dos dados, o que pode acarretar na redefinição dos objetivos e, naturalmente, das tarefas a serem realizadas para alcançar- los.·
Limpeza dos dados:
Dados podem conter ruídos, como valores inexistentes e/ ou inconsistentes e, um trabalho de limpeza nestes dados é necessário.
Esta tarefa é geralmente inevitável, mas deve- se fazer uma análise cuidadosa, porque muitas vezes o que pode parecer uma anomalia pode indicar um fenômeno crucial do domínio.·
Desenvolvimento do modelo:
Primeiramente um dos grandes trabalhos do analista nesta etapa é encontrar um subconjunto na população da base de dados que se comporte de uma forma tal, que seja significativa o bastante para ser o foco de análise.
Os dados devem ser visualizados e ajustados, se necessário.
Em esta etapa são realizadas as tarefas de:
Segmentação dos dados geralmente utilizando- se de técnicas não supervisionadas como agrupamento;
Seleção do modelo a ser gerado, tais como regressão, árvores de decisão, redes neurais;
E seleção dos parâmetros.
O principal objetivo desta etapa é explorar os possíveis modelos de mineração disponíveis e ajustar os parâmetros de acordo com os dados e o objetivo do processo de KDD.·
Análise dos dados:
Em este momento, o analista tem uma hipótese sobre os dados e vários tipos de ferramentas para desenvolver o modelo.
Em esta etapa é realizada uma especificação formal do modelo, sua avaliação e um possível refinamento.
Quando necessário, os parâmetros específicos para o modelo são ajustados.
Geralmente é necessário utilizar- se iterativamente de ferramentas de análise baseadas em visualização e em algoritmos.·
Geração das Saídas: As saídas podem ser geradas de diversas formas, como relatórios, descrição textual, gráficos, etc..
Pode- se também desenvolver gatilhos no banco de dados que disparam ações ou alertas quando determinadas condições forem detectadas.
O conhecimento prévio do domínio dos dados e do ambiente da aplicação é muito importante e, crucial, para guiar o processo.
Algumas técnicas podem ajudar a formalizar o conhecimento adquirido durante todas as etapas do processo, o que pode ser útil em aplicações futuras.
O dicionário de dados, regras de restrições e várias formas de descrição dos dados no SGBD, também podem contribuir.
Brachman Distinguem dois contextos diferentes de descoberta de conhecimento.
O primeiro é caracterizado por uma aplicação de descoberta de conhecimento que está adaptada (integrada) às necessidades de um ambiente de negócios para fornecer análise e recomendações de ações numa corporação.
Esta aplicação está voltada para um usuário que deve ser uma pessoa de negócios que monitora eventos importantes em dados de negócios.
O segundo contexto é o KDSE (Knowledge Discovery Support Environment ou ambiente de apoio à descoberta de conhecimento), que deve apoiar a construção de tais aplicações as quais, por sua vez, possuem características diferentes e requerem suporte tecnológico distinto.
Este processo tem uma natureza mais exploratória e requer apoio de um usuário que conheça várias técnicas de análise de dados, o domínio dos dados e a tarefa para qual o conhecimento é buscado.
É neste segundo contexto que são ressaltas as características e os requisitos de apoio ao trabalho do analista, doravante denominado &quot;usuário».
Este usuário tem como tarefas básicas:
Seleção e evolução do modelo;
Análise dos dados;
E geração dos resultados.
A abordagem Brachman Tem o mérito de detalhar o processo de KDD de uma forma bem mais abrangente e realista, preocupando- se com fatores que têm grande influência nos resultados de uma aplicação, principalmente com as deficiências apontadas na abordagem de Fayyad entre eles:·
a inclusão das etapas de descoberta dos dados e descoberta da tarefa, que são fundamentais para a definição do escopo da aplicação.
Estas etapas e suas atividades são altamente iterativas e interativas, e o seu resultado é a especificação dos objetivos da aplicação e a definição das atividades que deverão ser executadas para alcançar este objetivo.
Durante qualquer outra etapa do processo, o aprimoramento do conhecimento sobre o domínio dos dados pode gerar uma redefinição das atividades a serem executadas, reestruturando o processo, bem como o ajuste dos objetivos;·
a natureza da tarefa do analista, que na prática é extremamente interativa, prolongada no tempo, envolvendo numerosos arquivos, tabelas, consultas, sistemas não integrados e se utiliza de diversas ferramentas, que na maioria das vezes, não estão projetadas para trabalhar em conjunto.
Existem ainda inúmeros sub-problemas que devem ser tratados antes do problema principal ser atacado, tais como a necessidade de analisar minuciosamente os dados, realizar conversões de dados, trabalhar constantemente com ruídos, dados confusos e complexas consultas SQL.
Além de todas estas dificuldades, a condução de uma aplicação é muito dependente de seu conhecimento prévio sobre o domínio dos dados;·
a falta de apoio à tarefa do analista, por parte de as ferramentas ou técnicas de descoberta de conhecimento, ficando clara a necessidade de enfatizar mais o processo centrado no usuário (analista) e nas suas tarefas, pois o processo de descoberta de conhecimento é muito mais complexo que simplesmente a descoberta de padrões interessantes.
Por estes motivos, esta abordagem é adotada como referência ao processo de KDD nos capítulos subsequentes.
Algumas etapas serão referenciadas de forma mais genérica do que as definidas em.
A etapa de Limpeza de Dados será referenciada como Preparação dos Dados, devido a sua característica mais abrangente de seleção, projeção e limpeza dos dados.
A etapa de Desenvolvimento do Modelo e Análise dos dados serão tratadas em conjunto como Análise dos Dados, no entanto, quando for fundamental para o contexto, estas serão citadas separadamente.
Outros itens da nomenclatura utilizados no restante do trabalho e que devem ficar claros são da utilização de alguns sinônimos como:·
atividades e tarefas:
São referências às diversas tarefas que são executadas em cada etapa, ou seja, uma etapa é um conjunto de atividades ou tarefas;·
analista de dados, analista e usuário:
Pessoa que conhece o processo de KDD e técnicas para preparação e análise dos dados e está construindo uma aplicação de KDD.
Esta seção complementa as abordagens de Brachman E Fayyad descritas anteriormente, e tem como objetivo detalhar algumas características inerentes ao processo de KDD e as diversas dificuldades que são enfrentadas ao longo de suas etapas e, que podem, até certo ponto, ser vistas sob três aspectos diferentes.
O primeiro de eles diz respeito à preparação de um subconjunto de dados, a partir de os quais os padrões serão extraídos, descritas na Seção 2.3.1.
O segundo, a escolha e definição das técnicas apropriadas para os objetivos fixados, a validação dos resultados e a geração de saídas, apresentado na Seção 2.3.2.
O terceiro aspecto, descrito na Seção 2.3.3, corresponde a uma visão do processo de KDD como um empreendimento, que precisa ser delimitado, gerenciado e registrado.
Preparação dos Dados Durante as atividades que compreendem a preparação dos dados e, que segundo Brachman Consome cerca de 75% a 80% dos esforços despendidos na construção de uma aplicação de KDD, as principais dificuldades enfrentadas são oriundas, principalmente do grande volume de dados a ser tratado, da falta de documentação sobre o domínio dos dados e à grande dependência do conhecimento prévio do analista, além de o fato da maioria das bases de dados não terem sido projetadas com o intuito de descoberta de conhecimento.
Em são sintetizados alguns destes problemas:
&quot;A qualidade (ou falta de) e a imensidade de dados em base de dados do mundo real representam o coração dos problemas para KDD.
Superar os problemas de qualidade requer conhecimento do domínio externo para limpar, refinar ou preencher os dados.
A imensidade de dados força a utilização de técnicas que focam porções específicas de dados, que requer conhecimento adicional do domínio, se isto é feito inteligentemente.»
O analista tem que enfrentar vários desafios nesta etapa.
Entre os principais podemos citar:·
Volume de Dados:
As bases de dados possuem quantidades enormes de informação, com centenas de atributos e tabelas e milhares de registros.
A busca exaustiva por padrões em todo conjunto de dados é impossível, além de o que, geralmente existem atributos redundantes e irrelevantes para o objetivo da aplicação.
Em esta situação deve ser possível focalizar a análise numa amostragem de dados através da seleção de atributos e/ ou registros da base de dados, utilizando ferramentas que interajam com o sistema, com o usuário e com a base de dados.·
Ruídos: Atributos armazenados em base de dados podem conter problemas em seu domínio, tais como valores incertos, nulos ou inexistentes e, cujo significado muitas vezes é desconhecido.
Esta situação pode ser um problema significativo na tarefa de descoberta de padrões, pois gera incerteza sobre os resultados, desencadeia a necessidade de uma série de procedimentos que possam tratar os ruídos e, exige uma maior amostragem dos dados e um maior conhecimento do domínio.
Este é um tipo de problema um pouco delicado para se tratar, pois o analista deve ter um cuidado especial em avaliar se o ruído é realmente uma anomalia ou um foco interessante a ser investigado mais a fundo.·
Informação incompleta:
Os dados podem estar incompletos por a ausência de valores num atributo, ou por a ausência de atributos necessários para a tarefa, impossibilitando assim, construir as regras buscadas.·
Dados esparsos:
A descoberta de padrões, na sua maioria, é realizada sobre uma amostragem de dados que representa somente um pequeno subconjunto de todas as possibilidades de comportamento inferidos a partir de uma base de dados num determinado instante.
Esta situação pode ocasionar a descoberta de padrões incorretos, pois objetos significativos podem estar fora de o conjunto de dados selecionado ou estar presente num percentual que não seja significativo.
Portanto, os dados selecionados devem representar uma grande variedade de comportamento e em proporções coerentes, o que não é uma tarefa fácil, devido a a grande quantidade de informações, muitas vezes espalhadas em diversos atributos e tabelas.·
Informação redundante:
Em muitos locais da base de dados existem informações redundantes, o que pode acarretar que estas redundâncias sejam, por engano, descobertas e interpretadas como conhecimento.·
Dados dinâmicos:
Base de dados são atualizadas freqüentemente através de operações de inserção, atualização ou remoção, podendo tornar o conhecimento extraído anteriormente inconsistente.
Precauções podem ser tomadas para assegurar que estas alterações não levem a descobertas errôneas, tornando as regras inconsistentes.
O tratamento dos problemas descritos acima não seria tão complicado se pudessem ser detectados e definidos no momento da definição das tarefas de preparação dos dados, mas a maioria de eles vai surgindo à medida que o analista vai conhecendo melhor o conteúdo e comportamento dos dados, o que geralmente acontece na etapa de análise dos dados ou após algumas atividades de preparação dos dados.
Isto faz com que, muitas vezes, a maioria das atividades executadas anteriormente tenham que ser reexecutadas, gerando novos resultados que, por sua vez podem revelar novas surpresas e, o ciclo se repete até que se dê por finalizada uma aplicação.
As atividades das etapas de descoberta dos dados, descoberta da tarefa e preparação dos dados geralmente são apoiadas por um conjunto de ferramentas não integradas, que vão desde recursos do SGBD que dão suporte à manutenção dos dados, até ferramentas de consulta, estatística e visualização que auxiliam na análise dos dados.
A alta iteratividade faz com que sejam geradas diversas versões tanto de arquivos com resultados quanto de arquivos com comandos (e.
g SQL e PL/ SQL) e com os aplicativos gerados por as ferramentas.
Relacionar todos estes tipos de arquivos, a evolução de cada um, a sua seqüência e o seu papel dentro de o contexto do processo é uma tarefa que exige um nível muito alto de organização e documentação por parte de o analista, o qual muitas vezes precisa reexecutar procedimentos por não saber ao certo como determinado resultado foi alcançado, ou qual a última versão de um arquivo com aplicativo, comandos, ou resultados.
Construção do Modelo O segundo aspecto, visualiza as tarefas concernentes à análise dos dados, que têm como resultado um conjunto de dados ajustados ao modelo considerado apropriado, a partir de o qual as regras serão extraídas, analisadas e interpretadas.
Ele engloba também a geração das saídas que traduz as regras geradas para um formato que possa ser útil e inteligível ao usuário.
A grande dificuldade na concepção do modelo é a escolha entre as diversas técnicas existentes e o grande número de parâmetros que precisam ser definidos e ajustados de acordo com o objetivo global da aplicação e as características dos dados.
A definição dos modelos e parâmetros a serem utilizados não é uma tarefa fácil, pois pequenas alterações destes geram diferenças significativas nos resultados.
Em este momento, a iteração é muito grande, diversas técnicas e parâmetros são testados e avaliados, até que o modelo apropriado seja alcançado e outros descartados, além de a possibilidade de se decidir retornar a quaisquer das etapas anteriores, seja por distorções detectadas no domínio dos dados e que precisam ser solucionadas, por a mudança no foco de análise, ou até mesmo por a necessidade de um refinamento do objetivo global.
Em este tipo de tarefa também podem ser utilizados diferentes algoritmos, que podem, ou não, estar disponibilizados no mesmo ambiente.
Quando os algoritmos não estão no mesmo ambiente, a integração dos resultados pode se tornar um problema, pois geralmente são utilizados um conjunto de algoritmos, onde a saída da aplicação de um algoritmo pode ser a entrada para a execução de outro e, conversões e adequações de formatos podem ser necessárias.
Como existe uma alta iteratividade para ajuste dos parâmetros, e uma intercalação entre os algoritmos utilizados, a mesma dificuldade, no que diz respeito a ferramentas se repete, como gerenciamento de arquivos, formatos e versões.
Outro desafio é avaliar o grau de interesse (e.
g validade, usabilidade, corretude) das regras geradas, de acordo com as expectativas do usuário final e, após, tornar- las compreensíveis.
Esta tarefa é complexa já que o número de regras geradas é muito grande.
Alguns fatores de avaliação devem ser definidos de acordo com a característica da aplicação e levando em consideração aspectos do usuário e dos dados.
A sua importância está em evitar que regras irrelevantes, triviais ou falsas sejam consideradas corretas e/ ou interessantes.
Empreendimento de KDD As dificuldades e problemas descritos nas seções anteriores ilustram a complexidade do processo de descoberta de conhecimento e o quanto as características de iteratividade e interatividade estão presentes em todo o processo.
A iteratividade dentro de o processo não ocorre apenas entre atividades da mesma etapa, mas também entre atividades de etapas diferentes, como por exemplo, após a realização de uma atividade da etapa de análise de dados pode ser necessário voltar à alguma atividade ou acrescentar uma nova tarefa da etapa de preparação de dados.
Portanto, as atividades interagem e se complementam em qualquer ponto, o número de informações e variáveis manipuladas em cada uma de elas é muito grande e, o analista está sempre avaliando os resultados e tomando as decisões, ou seja, conduzindo o processo, baseado na sua visão do processo de KDD, na sua experiência e no seu conhecimento prévio sobre o domínio dos dados.
Outra questão é a variedade de ferramentas que podem ser manipuladas.
Como existem técnicas muito específicas que podem ser utilizadas, provavelmente quanto mais especializada uma ferramenta é, mais específica ela será.
Em este caso, provavelmente o benefício em relação a os recursos disponíveis, flexibilidade e otimização, será maior, porém a dificuldade de gerenciar as idas e vindas no processo torna- se quase impraticável sem um suporte adequado, quando de a utilização de várias ferramentas.
Estas características dificultam o gerenciamento do processo, facilitam a perda do controle dos procedimentos realizados anteriormente, dificultando a manutenção e gerenciamento de todos os conjuntos de dados, os procedimentos e parâmetros utilizados para gerar- los, quais características foram consideradas e quais resultados e/ ou experiências foram adquiridos durante cada etapa do processo.
Estes aspectos são importantes não só para documentação e gerenciamento da aplicação, mas principalmente para evitar reexecução dos procedimentos.
Wirth enfatizam que atualmente a maior parte das pesquisas estão centradas em algoritmos e técnicas de modelagem e, de um ponto de vista prático, o principal &quot;gargalo «para aplicações de KDD não é a falta de técnicas, mas sim explorar e combinar eficientemente os algoritmos existentes e apoiar o usuário durante todas as etapas e atividades do processo.
Segundo ele, a execução das atividades que envolvem KDD requer substancial esforço e habilidade humana, onde os resultados são bastante dependes das pessoas que estão realizando o trabalho, geralmente guiados por o seu conhecimento prévio.
Conduzir a construção de uma aplicação de KDD não é uma tarefa fácil.
As aplicações geralmente são construídas de maneira não estruturada e, a falta de ferramentas de apoio adequadas, podem conduzir a aplicação para problemas de maior ou menor importância, os quais podem consumir desnecessariamente tempo e recursos de desenvolvimento.
Existe também uma dificuldade em documentar as tarefas realizadas e seus resultados devido a a falta de uma metodologia adequada.
A maioria dos autores da comunidade científica são unânimes quanto a as dificuldades em gerenciar uma aplicação de KDD e à necessidade de um apoio mais efetivo ao usuário.
Contudo, devido a a complexidade e multi-disciplinaridade desta área, este apoio pode ser disponibilizado de diversas outras maneiras, como por exemplo, ambientes que auxiliem o usuário a avaliar as regras geradas, a registrar e manter o histórico de sua aplicação na forma de documentação do processo, a definir e executar um processo de KDD, entre outros.
Alguns trabalhos que vão ao encontro de propostas neste sentido, estão descritos no Capítulo 3.
A área de descoberta de conhecimento é relativamente nova e existem poucas referências a trabalhos relacionados diretamente com mecanismos que tratam dos problemas do analista na realização de suas tarefas durante o processo de KDD.
Algumas propostas estão descritas na Seção 3.1.
O seu estudo remeteu a alguns tópicos de Inteligência Artificial (Ia) que poderiam ser úteis na condução deste trabalho, mais especificamente para tratar a complexidade das tarefas, descritos na Seção 3.2.
A tecnologia de Memória Organizacional (OM), apresentada na Seção 3.3, é interessante no contexto deste trabalho, pois sua principal contribuição está em possibilitar que o conjunto de informações registradas em documentos ou detidas por pessoas numa organização, possam ser organizadas, registradas e utilizadas para auxiliar na tomada de decisões atuais e futuras, o que reduz incertezas e indeterminismo.
Descoberta de conhecimento é uma tarefa que abrange tópicos de diversas áreas, a saber:
Métodos para acesso aos dados, métodos estatísticos e de ML (Machine Learning ou Aprendizagem Automática) para análise dos dados, métodos de Ia para tratar da complexidade das tarefas, entre outros.
Esta característica faz com que o apoio a esta tarefa possa ser visualizado e disponibilizado de diversas formas, com diferentes objetivos (e.
g preparação e/ ou análise dos dados) e direcionada a um tipo específico de usuário (e.
g especialista ou não em KDD).
Devido a estas particularidades, dificilmente um ambiente de apoio será abrangente o suficiente para a atender todas as necessidades do usuário.
Se o fizer, provavelmente deixará a desejar em algum ponto.
Existem poucos trabalhos que se preocupam com o apoio ao usuário de forma genérica, ou seja, sem estar ligado a uma aplicação ou ambiente específico.
Para o contexto deste trabalho são interessantes propostas de apoio diferenciadas, que não se preocupem especificamente com algoritmos para mineração e/ ou recursos para acesso e preparação dos dados, mas sim facilitem a sua utilização.
Dentro de estas propostas, podemos citar o sistema IMACS, que apóia o usuário na etapa de preparação dos dados, apresentado na Seção propõe uma arquitetura de apoio ao usuário à construção de aplicações de KDD dentro de um ambiente específico, descrito na Seção 3.1.2.
Em a Seção 3.1.3, o UGM, que corresponde a um módulo do Projeto CITRUS é apresentado com maiores detalhes.
Um dos primeiros trabalhos na área de KDD a se preocupar com a tarefa do usuário de forma realista, e a visualizar os principais problemas que ele enfrenta está descrito em e.
Em é utilizado o termo &quot;arqueologia de dados «para definir KDD, pois esta é vista como uma tarefa que não pode ser realizada por métodos automáticos, e é completamente dependente da habilidade, conhecimento e experiência do usuário.
Outro fator que justifica este termo, é que o conhecimento buscado não pode ser especificado antecipadamente, mas emerge através de um processo iterativo e interativo de análise e segmentação dos dados, ou seja, os objetivos mudam em resposta ao que é descoberto e, é impossível prever antecipadamente o tipo de conhecimento que será encontrado.
O IMACS (Interactive Market Analysis and Classification System ou Sistema de Classificação e Análise de Mercado Interativo), foi projetado para auxiliar alguém que precisa olhar para uma grande massa de dados e, iterativamente, explorar hipóteses e derivar conclusões sobre um domínio.
O IMACS apóia o usuário a criar uma definição semântica para os dados de uma base de dados qualquer, através de uma linguagem de consulta própria e de um sistema de representação formal do conhecimento chamado CLASSIC.
O CLASSIC fornece:
Uma linguagem para descrever o domínio dos dados;
Mecanismos para gerenciar este modelo de domínio como uma descrição hierárquica de classes e manter uma restrição entre estas descrições;
E a base para uma linguagem de consulta.
A idéia por trás do CLASSIC é representar os dados de uma base de dados convencional na forma de objetos, suas propriedades, relacionamentos entre estas propriedades e herança.
Os dados são transportados de sua base de dados original para dentro de o CLASSIC com uma semântica definida por o usuário.
Isto permite classificar conceitos e adicionar- los a uma base de conhecimento de acordo com o seu significado.
O usuário pode visualizar e analisar estes dados, incluindo os conceitos e suas definições, através de uma linguagem de consulta própria, funções estatísticas e vários tipos de gráficos.
Os dados de interesse podem ser segmentados, a partir de o resultado de consultas ou gráficos, dentro de um subconjunto de interesses na forma de abstrações (e.
g clientes com limite de compra alto).
Estas abstrações podem ser transportadas para o CLASSIC na forma de conceitos e de segmentos.
Existe também a possibilidade de criar monitoramentos de mudanças nos conceitos existentes no momento em que os dados são atualizados, através de rotinas específicas.
O IMACS foi concebido numa época onde as ferramentas de KDD ofereciam menos recursos que as atuais, quase todos voltados exclusivamente à etapa de análise (e.
g algoritmos de mineração).
Esta restrição já foi vencida hoje por ambientes mais modernos, como a Clementine e o Intelligent Miner.
Assim, a principal contribuição do IMACS aos problemas envolvidos no processo de KDD destacados no Capítulo 2, é o controle sobre o significado dos dados, através das abstrações e segmentos, sua forma de geração e monitoramento.
Outros problemas como iteratividade, integração entre ambientes heterogêneos e gerenciamento de aplicações não são abordados.
Um ponto interessante na abordagem apresentada em, é a importância com que é tratada a questão da definição dos objetivos e redirecionamentos no processo e sua relação com as etapas de Descobertas da Tarefa e Descoberta do Dados, o que ele define genericamente como uma tarefa de arqueologia.
Projeto CITRUS CITRUS é um projeto em parceria com indústrias e universidades, que tem como objetivo principal desenvolver um sistema de KDD baseado em processo.
Este sistema acrescenta recursos e funcionalidades a uma ferramenta de descoberta de conhecimento comercial, a saber, Clementine, com intuito de apoiar o usuário a construir e manter aplicações.
Clementine foi desenvolvida por a Integral Solution Ltd (ISL) e possui recursos para acesso, importação e exportação dos dados;
Bem como técnicas e métodos para análise dos dados.
O usuário seleciona ícones representando técnicas para resolver tarefas do processo de KDD.
Estes ícones são conectados para definir o fluxo de dados (streams, na terminologia da Clementine).
Existe um menu ligado a cada ícone, que disponibiliza questões relacionadas a cada técnica que devem ser detalhadas por o usuário, tais como opções, parâmetros e formatos de saída.
A arquitetura do CITRUS é apresentada na Figura 3.1.
Alguns de seus compontentes estão descritos a seguir:·
Gerenciador de Informações:
Este módulo tem dois objetivos principais:
Para preparação dos dados é complexa e consome a maior parte dos esforços do processo.
A Clementine oferece recursos para esta tarefa, o que implica que grande parte do tempo consumido para a construção de uma stream é na seleção e conexão dos operados necessários (e.
g Operadores de mais alto nível são oferecidos ao usuário, que tem a possibilidade de trabalhar com uma representação mais abstrata.
As streams são mapeadas, em tempo de execução, para a sua versão relacional equivalente.
Em este caso o esquema OO é usado como repositório central de informações, o qual serve como âncora para ligar dados, aplicações e resultados.
A cada execução de uma stream, um trace é criado no esquema, e assim é possível consultar sobre todas as operações aplicadas aos dados para se chegar a determinado resultado.
Cabe lembrar que o conteúdo das operações da Clementine são de muito baixo nível (e.
g seleção, projeção, conversão de tipos, derivação de atributos), podendo o analista se perder em meio a tantos detalhes operacionais.
Por exemplo, o que da parte de o analista pode corresponder a uma tarefa, em Clementine pode corresponder a uma ou mais streams.·
Servidor de Execução: De o ponto de vista de KDD uma aplicação pode ser vista como uma consulta (complexa) sob a base de dados.
Muitos sistemas de KDD implementam a estratégia onde os dados são carregados para dentro de o sistema para execução, mas na prática isto pode envolver uma massa muito grande de dados, implicando problemas de gerência de memória.
CITRUS implementa uma estratégia mais eficiente para endereçar este problema.
Adicionalmente, devido a a natureza iterativa do processo, os conjuntos de dados intermediários gerados são armazenados para posterior reuso.
O esquema irá gerenciar se o conjunto de dados pode ser utilizado posteriormente, através do histórico do processo.
O sistema também tenta otimizar as consultas geradas para melhorar o desempenho.·
Interface de Apoio ao Processo e Orientação ao Usuário:
O objetivo principal deste módulo é guiar o usuário na definição e execução de um plano para conduzir o processo de KDD.
O módulo orienta o usuário na seleção das técnicas disponíveis, a interpretar e avaliar os resultados e, a coletar e documentar os resultados finais.
O foco é o suporte na definição de um plano e não na criação automática de aplicações (e.
g não é objetivo a derivação automática de uma stream ou seleção das técnicas mais apropriadas, a partir de uma descrição do problema).
O usuário pilota o sistema, e este módulo dá heurísticas e soluções genéricas sempre que possível.
Segundo Wirth os planos gerados em conjunto com o gerenciador de informações podem monitorar as ações do usuário para fins de relatórios e histórico da aplicação, o que pode fornecer um contexto para interpretação dos resultados, evitar reexecução dos procedimentos e coletar resultados interessantes.
No entanto, não foi detalhado como isso seria realizado, com base em que este histórico seria gerado, muito menos como ele poderia auxiliar na interpretação dos resultados de uma forma sistemática, pois esta é uma tarefa bastante dependente do usuário e do domínio da aplicação.
Em os requisitos para este auxílio são estabelecidos.
Posteriormente, em alguns destes requisitos são concretizados através do UGM (User Guidance Module ou Módulo de Orientação ao Usuário), descrito com maiores detalhes na Seção 3.1.3.
Apesar de o escopo do projeto e da arquitetura estarem definidos, não foram encontradas maiores informações sobre o funcionamento de cada módulo, à exceção de parte dos requisitos especificadas para o módulo de apoio ao usuário.
É inegável que os objetivos do projeto CITRUS vão ao encontro de a grande parte dos problemas destacados no Capítulo 2.
Contudo, não foram encontradas informações adicionais junto à página do projeto na internet, nem artigos científicos mais recentes sobre o CITRUS.
Em relação a os problemas destacados no Capítulo 2, ficam em aberto as seguintes questões:·
a integração com outras ferramentas não foi citada, ficando o usuário limitado aos recursos da Clementine para beneficiar- se das vantagens oferecidos por o CITRUS.
Um exemplo é a manutenção do histórico e documentação da aplicação, caso algumas tarefas sejam realizadas em outro ambiente;·
não foram oferecidos maiores detalhes sobre como o esquema OO realmente auxilia o usuário a ter uma visão mais orientada ao domínio de sua base de dados, nem como o usuário pode acrescentar sua própria semântica aos objetos, já que este é gerado e mantido &quot;quase «que automaticamente por o sistema;·
em a prática uma aplicação de KDD pode ser concretizada através da construção de várias streams, no entanto, a relação entre estas streams não foi abordada;·
qual é o apoio efetivo ao usuário em resolver questões não genéricas, com nível de complexidade alto, que envolvam uma análise mais aprofundada nos dados, experiência e conhecimento do domínio por parte de o usuário;·
como são registradas e mantidas as experiências e tentativas realizadas durante o processo que foram descartadas, juntamente com o raciocínio que levou a elas, o que pode ser útil na tomada de decisão futuras.
Como já citado, o UGM implementa alguns dos objetivos atribuídos ao módulo de Interface de Apoio ao Processo e Orientação ao Usuário do projeto CITRUS, apresentado na seção anterior, motivado por os diversos problemas enfrentados por o usuário na construção de aplicações de KDD.
Engels complementa com algumas considerações que reforçam a idéia de um apoio ao usuário:·
em a etapa de mineração, modelos são facilmente gerados, mas são raramente avaliados corretamente, pois é difícil ter certeza que as técnicas escolhidas e a parametrização adotada são as mais adequadas considerando as características dos dados e os objetivos da aplicação;·
a seleção de a (s) técnica (s) apropriada (s) para o problema, é portanto um das maiores questões a serem endereçadas;·
a avaliação e definição de formas de iteração, estão relacionadas com a definição de tarefas apropriadas para o pré-processamento de dados e a inicialização das técnicas e parâmetros para indução;·
levando em consideração que o processo de KDD e seus componentes consomem tempo e recursos, é importante algum tipo de reuso do processo;·
a documentação de aplicações do processo, juntamente com seus objetivos e resultados, é um importante tópico para uma metodologia de aplicação, pois além de facilitar o reuso, contém informações referentes ao projeto e as experiências corporativas.
Segundo Engels, o usuário deve ser apoiado na descrição do problema, no tratamento de sua complexidade, na definição de uma solução, na seleção e inicialização de técnicas apropriadas, bem como na documentação e no armazenamento das aplicações que obtiveram êxito.
A partir de esta visão do processo de KDD, sua abordagem foi definida, que basicamente engloba as mesmas atividades definidas em, e, no entanto com duas diferenças importantes.
A primeira é que existem dois momentos em que as mesmas etapas estão presentes:
Em a definição do processo e na aplicação do processo (este sim, altamente iterativo), implicando que primeiramente as tarefas são planejadas e definidas e, posteriormente, executadas.
A segunda, é o acréscimo da tarefa de documentação das atividades presente em todo o processo.
Segundo ele, a única abordagem em direção a a documentação de aplicações de ML conhecida é discutida em e, apesar de o processo de KDD utilizar- se de técnicas de ML, o seu contexto é bem mais abrangente.
Ele ressalta que todas as propostas de modelos de processo para KDD falham por não endereçarem o problema do armazenamento de experiências, quer na forma de documentação sobre o processo executado, quer na forma de operacionalização, manutenção e rastreamento do processo.
O UGM, também integrado à Clementine, é composto por quatro componentes principais (Figura 3.2):·
Descrição do problema:
Tem como responsabilidade conduzir o diálogo com o usuário para a especificação do problema.
Em este momento é iniciado um processo de pesquisa nos repositórios existentes com objetivo de reusar partes de processos já definidos.·
Decomposição e refinamento das tarefas:
As tarefas são refinadas por decomposição até se chegar a tarefas simples, as quais, quando executadas em ordem, resolvem o problema definido.
As tarefas simples servem como ponto de referência para guiar a pesquisa por técnicas apropriadas.
Essa decomposição está bastante ligada às unidades reusáveis do processo.
Em a verdade, a decomposição termina quando é encontrada uma unidade na base de conhecimento (Cps ou RPS, descritas a seguir) que possa ser reutilizada, ou até se chegar a tarefas simples que possam ser mapeadas para técnicas apropriadas.·
Repositórios: Existem quatro tipos de repositórios que auxiliam na decomposição de tarefas e no reuso dos compontentes em diferentes níveis de abstração:·
Cps (Complete Process Solution ou Soluções Completas de Processo) e RPS (Reusable Process Units ou Unidades Reusáveis de Processo):
Estes dois repositórios contêm descrições de soluções parciais e completas extraídas de aplicações anteriores que obtiveram sucesso.·
Tarefas simples:
Armazena as tarefas simples, que são aquelas em último nível de abstração, as quais podem ser mapeadas para um conjunto de técnicas, mais especificamente operadores elementares na Clementine, tais como seleção, projeção, algoritmo de mineração.
Descrição de técnicas:
Contém a descrição das técnicas disponíveis, tais como restrições de entrada, efeitos do resultado, inicialização e adaptação de regras.·
Características dos dados:
Contém informações sobre os dados em termos de dimensões, ruídos, formatos (tipos de dados, domínio, etc).
Correlação entre atributos, informações implicitamente presente nos dados, etc.· Inteface:
Integra a arquitetura do UGM.
A construção de uma aplicação através do UGM é conduzida por diálogos com o usuário, onde o seu problema é definido e delimitado.
As características dos dados são analisadas através de métodos estatísticos e do conhecimento do domínio fornecido por o usuário.
O ambiente é pilotado por o usuário, que a partir de as informações por ele definidas, fornece &quot;dicas», tais como entradas com valores faltando;
Saídas criadas, mas nunca utilizadas;
Verifica se o usuário esqueceu alguma tarefa importante que precisa ser realizada;
E recomenda técnicas para as tarefas.
Como é baseada em reuso, quando é iniciado o diálogo com o usuário, é realizada uma pesquisa por projetos similares armazenados nos repositórios.
Três situações podem então acontecer e, o usuário poderá optar por uma de elas:
A documentação é gerada automaticamente durante o processo e está ligada às CPSs, às RPSs e às streams da Clementine.
Ligadas a cada uma das CPSs e RPSs estão as informações, tais como o autor, data de criação, tipo de tarefa, problema que endereça, restrições, fonte de dados, formas de acesso aos dados, comentários do usuário, necessidades, entre outros.
Agregadas às streams são geradas informações sobre a função, o tipo, a necessidade, e a etapa de KDD de cada ícone que a compõe, além de um campo de texto para o usuário adicionar informações.
O usuário tem o recurso de editar e acrescentar à documentação gerada ao longo de o processo, informações que, a seu ver, também são importantes.
Caso a aplicação tenha sido construída no ambiente da Clementine, mas não através do UGM, o usuário tem a possibilidade de criar CPSs e RPSs e documentálas.
No entanto, é necessário fornecer todas as informações definidas por o UGM como necessárias.
O ambiente apresentado em é um grande passo em direção a um apoio mais efetivo e completo ao usuário na execução e reuso de um processo de KDD, pois apóia o usuário desde a definição e delimitação do problema até a documentação, preocupando- se de maneira mais ampla com o empreendimento de descoberta de conhecimento, apesar de estar mais voltado para um usuário com pouca experiência em KDD.
Contudo, alguns problemas destacados no Capítulo 2 não foram abordados nesta proposta, entre eles:
A) Integração de ferramentas:
A utilização das técnicas, recursos e funcionalidades com determinado nível de ajuda e documentação, estão restritas às disponibilizadas por a Clementine, ou seja, o problema de integração de ambientes heterogêneos não é tratado.
B) Apoio à definição do plano a ser executado está mais a nível operacional:
Como a grande maioria dos problemas de KDD são vagamente definidos, falta clareza e conhecimento sobre os dados, o domínio, e o que se busca, interpretação das entradas dos usuários na definição de seu problema não é tão simples de ser tratada, e alguns problemas devem ser enfrentados neste sentido, principalmente por não existir um vocabulário próprio definido para realizar a comunicação entre o usuário e o UGM.
Isto dificulta a busca por soluções, se é que elas existem, de forma semi-automática, já que o usuário tem sua parcela de participação na condução do processo.
O apoio ao usuário tanto na definição de um problema e construção de um plano, quanto no reuso de aplicações completas (Cps) ou parciais (RPS) está mais voltada para situações genéricas do uso de técnicas do que para problemas enfrentados na construção de uma aplicação de KDD.
Em outras palavras, o ambiente não apóia:
C) Falta de apoio às iterações e redirecionamentos do processo:
O ambiente é baseado em aplicações de sucesso e não explora questões relativas ao contexto (e.
g domínio, objetivos, técnicas) de tentativas que tiveram que ser abandonadas, o que levou à sua adoção e ao posterior descarte, o que também é de extrema importância para evitar reexecuções e para que recursos não sejam gastos sem necessidade.
O aspecto de manutenção de informações sobre as várias técnicas exploradas e sua parametrização, também não foram abordados.
D) Não permite ao usuário explorar a sua própria experiência:
A condução de um processo é muito dependente da experiência, do conhecimento prévio do domínio dos dados e da aplicação e, do domínio de técnicas para explorar e analisar os dados, por parte de o usuário.
Isto nos leva a acreditar que um apoio importante no seu trabalho é a possibilidade de ele poder analisar e estudar aplicações anteriores, semelhantes à sua em qualquer aspecto, seja domínio ou objetivos e, com base nisto, coletar algumas experiências, como hipóteses trabalhadas, técnicas abordadas, tomadas de decisão, descobertas sobre o domínio (considerando o mesmo domínio da sua aplicação, esta possibilidade é mais produtiva, senão fundamental), que o ajudem a definir e conduzir sua aplicação.
Em contrapartida ele também deve ter recursos para registrar as suas experiências.
A construção de uma aplicação de KDD consiste basicamente no planejamento e execução de um conjunto de tarefas relacionadas entre si, que juntas almejam um objetivo maior.
Este conjunto de tarefas é derivado de uma tarefa maior que, devido a sua complexidade, é decomposta em várias sub-tarefas, até que a execução de uma sub-tarefa se torne apropriada, ou seja, o nível de complexidade seja tratável.
A idéia inicial no estudo de Métodos de Resolução de Problemas e Planejamento era buscar conceitos que pudessem ser úteis na construção de mecanismos para representar e refinar um processo de KDD.
A utilização de PSM (Problem Solving Method ou Método de Resolução de Problemas) no contexto de KDD foi abordado em,, onde foram dados os primeiros passos em direção a o UGM.
O ponto de convergência entre KDD e PSM está:·
em a decomposição de tarefas numa estrutura de sub-tarefas;·
a funcionalidade de certas tarefas são definidas em termos de suas pré e pós-condições;·
a tarefa de decomposição pode causar um refinamento da funcionalidade das tarefas, desde que uma subtarefa introduza pré e pós-condições adicionais.
Um PSM tipicamente descreve como resolver uma tarefa por decomposição e imposição de uma ordem nas sub-tarefas decompostas através de um fluxo de controle, ou seja, quais os passos necessários para se executar determinada funcionalidade.
A maior parte dos PSMs consistem de três partes relacionadas, descrevendo o que um PSM pode resolver, como ele resolve e o que ele precisa para resolver, respectivamente chamados de competência, especificação operacional e requisitos/ suposições:·
Competência: Descrição declarativa do comportamento das entradas e saídas e do que pode ser realizado por o PSM.·
Requisitos/ Suposições:
Descrição do conhecimento do domínio necessário para que o PSM realize sua competência.·
Especificação Operacional: Consiste dos passos de inferência, do conhecimento e do fluxo de controle entre eles.
Os passos de inferência especificam os passos de raciocínio, que juntos realizam a competência de um método (que pode ou não ser decomposto hierarquicamente) e, são descritos por sua relação de entrada e saída.
O fluxo de conhecimento é ativado através dos papéis dinâmicos, os quais são repositórios que atuam como entrada e saída das inferências.
O fluxo de controle descreve a ordem na execução dos passos de inferência.
Embora PSMs possam ser utilizados com diferentes objetivos, sua principal aplicação é para descrever o processo de raciocínio de um SBC (Sistema Baseado em Conhecimento), de uma maneira independente de domínio e implementação, o que possibilita seu reuso em diferentes cenários.
Em este caso, o SBC é composto por a descrição da tarefa a ser realizada, por o conhecimento do domínio a ser aplicado, por o PSM e por um adaptador, que tem o papel de aplicar o processo de raciocínio genérico do PSM ao domínio e à tarefa em questão.
Experiências anteriores da aplicação do PSM para apoio ao processo de KDD foram abandonadas por duas grandes dificuldades:
Em o primeiro caso, a dificuldade reside numa decomposição de tarefas independente dos aspectos de domínio.
Em o segundo, observa- se o pouco conhecimento e experiências coletados sobre processos de KDD, considerando objetivos e domínios específicos.
Planejamento, descrito a seguir, resolve algumas limitações do primeiro aspecto.
Quanto a o segundo, o conhecimento que pode ser coletado é de pouco impacto real, como demonstrado na Seção 3.1.3.
A utilização dos conceitos de Planejamento é interessante quando um problema complexo precisa ser tratado em partes menores que devem ser resolvidas em separado.
Planejamento enfoca diversas maneiras de decompor o problema original em sub-partes apropriadas e mecanismos para registrar e tratar interações entre essas sub-partes durante o processo de solução de um problema Um plano é formalmente definido como uma estrutura de dados que consiste de:·
um conjunto de passos, onde cada passo é um dos operadores para o problema;·
um conjunto de restrições ordenadas, onde é definida a ordem entre os passos;·
um conjunto de restrições na ligação das variáveis entre os passos;·
um conjunto de ligações casuais, que registra o objetivo de um passo, que geralmente é chegar à pré-condição do passo seguinte.
Existem três idéias principais atrás de Planejamento:·
Liberdade para representar estados, objetivos e ações.
Algoritmos de planejamento utilizam descrições em linguagem formal, geralmente em lógica de primeira ordem.
Os estados e objetivos são representados por um conjunto de sentenças e ações por uma descrição lógica de précondições e efeitos.
Isto possibilita ao planejador fazer conexões diretas entre estados e ações.·
O planejador é livre para adicionar ações ao plano quando necessário, o que é melhor que numa seqüência incremental partindo de um estado inicial, que é o caso de PSM.·
A grande maioria das partes é independente uma da outra, o que facilita a utilização de sub-planos em outros planos.
De uma maneira geral, os conceitos que envolvem planejamento são interessantes do ponto de vista de KDD, pois possibilitam construir um plano de tarefas que são executadas buscando resolver um problema;
Decompor um plano hierarquicamente em vários sub-planos para tratar de sua complexidade;
Replanejar, caso não se chegue ao resultado que se busca;
E, partir de um plano incompleto considerando caminhos para expandir- lo para um plano completo.
Por estes motivos em é adotado um planejador parcial para criação dos planos de processo de KDD.
No entanto, planejamento é utilizado em problemas bem comportados, onde as ações estão previamente definidas, devem ser executadas independentemente umas da outras, sempre com uma restrição em sua ordem.
Estas características não são encontradas na construção e execução de um plano em KDD para um domínio bastante específico e onde o usuário tenha diversas opções de técnicas e ferramentas que não estejam restritas a um ambiente específico.
Outra questão é que a técnica de decomposição hierárquica utilizada em planejamento tem o objetivo de auxiliar na pesquisa que leva a soluções de um plano, não de representar as tarefas, o que poderia contribuir para a abordagem deste trabalho.
A necessidade de representar, coletar e recuperar dados, informação e conhecimento dentro de o contexto de um processo, vai ao encontro de alguns trabalhos na área de Memória Organizacional.
A tecnologia de Memória Organizacional pode contribuir, de entre outros, para:·
uma melhor exploração dos documentos disponíveis, mas que não são utilizados efetivamente;·
a formalização de regras de negócios em sistemas de workflow;·
a melhor utilização das habilidades e conhecimento humano;·
registro das experiências e know-how das melhores práticas para tratar base de dados;·
armazenamento dos processos de tomada de decisão que possibilite subsequente rastreamento de decisões em situações de alterações.
A maioria destas atividades são apoiadas total ou parcialmente por a tecnologia da informação, mas ainda falta uma visão compreensiva que caracterize as propriedades específicas de um Omis (Organizational Memory Information System ou Sistema de Informação de Memória Organizacional).
Segundo Abecker um Omis integra basicamente técnicas dentro de um sistema de computador, o qual coleta, atualiza e estrutura conhecimento e informação, disponibilizando- os em diferentes tarefas operacionais de forma sensível ao contexto, resoluta e ativa, como o objetivo de melhorar processos cooperativos e que envolvam intenso conhecimento.
Segundo eles, não existe um objetivo ou abordagem específica para utilizar Omis, mas sim um conjunto de motivações e aspectos técnicos, que podem ser avaliados sob o ponto de vista dos requisitos de captura, armazenamento e recuperação, o que ajuda a estabelecer a necessidade de sua construção.
Salientam também que a aplicação de sistemas baseados em conhecimento na prática apresenta sérias deficiências e a análise destes problemas tem levado ao desenvolvimento de abordagens de Omis, onde a execução das tarefas deixa de ser completamente automatizada para ter objetivos mais focados em apoiar o usuário em executar processos explorando o conhecimento existente em memórias organizacionais, tais como o UGM e o IMACS, apresentados na Seção 3.1.
Abecker Distingue duas categorias básicas da contribuição da tecnologia da informação de acordo com seu foco principal:·
Visão centrada em processo:
Enxerga o gerenciamento do conhecimento como um processo de comunicação social, onde a principal fonte de conhecimento é o usuário, como nos sistemas de groupware.·
Visão centrada em produto:
Focado na criação, armazenamento e recuperação do conhecimento sobre os documentos em memórias corporativas baseadas em computador.
Isto é baseado na explicação, documentação e formalização do conhecimento sobre a criação dos documentos para posterior reutilização.
O desenvolvimento e utilização do conhecimento individual do usuário deve ser apoiado por fontes de conhecimento confiáveis e no tempo apropriado.
Técnicas básicas são oriundas da área de Gerenciamento de Documentos e SBC.
Estas duas visões podem ser ilustradas por diferentes abordagens de ferramentas Case (Computer-Aided Software Engineering ou Projeto de Software Apoiado por Computador).
Em a visão centrada em produto são classificadas as abordagens tradicionais (e.
g System Architect, Rational Rose), onde a ferramenta Case propicia um repositório de dados, documentos e artefatos.
Já na visão baseada em processo, busca- se complementar este repositório com informações sobre o processo de desenvolvimento dos documentos e artefatos, como por exemplo, o registro de decisões tomadas em cada uma das etapas do seu desenvolvimento.
Em é salientado que a maioria das organizações utiliza- se do paradigma orientado ao produto.
Porém, neste paradigma está faltando o contexto (e.
g sentido, raciocínio) que está por trás da criação destes elementos, ou seja, as organizações falham em capturar qualquer registro do processo, através de o qual os artefatos são criados.
A visão da natureza do processo contempla as suposições, valores, experiências, conversas e discussões que levaram e constituem o contexto e raciocínio atrás de os artefatos, ou seja, o conhecimento que levou a determinado resultado.
De o ponto de vista da organização do conhecimento, Conklin chama a atenção para as informações valiosas que as organizações possuem relativas ao seu cotidiano de maneira informal, ou seja, agregadas apenas à memória de seus funcionários e, portanto, difíceis de serem preservadas, gerenciadas e difundidas.
Estas informações podem ser vistas como conhecimento e, são importantes pois podem auxiliar a responder questões tais como:
&quot;O que aconteceria se nós parássemos de fazer desta maneira?»,
&quot;Por que fizemos isto desta maneira?», &quot;Este problema não foi resolvido antes?»,
&quot;Alguém já considerou esta outra abordagem? «e, &quot;O que nós aprendemos da última vez que isso aconteceu?».
Geralmente são estas questões que levam as equipes de projeto a reinventar a roda e repetir discussões que já tenham sido fechadas.
Segundo, as necessidades atuais de um OMSI orientado a processo integram basicamente três tecnologias:
Hipertexto, groupware e um método retórico.
Hipertexto fornece a habilidade para organizar e mostrar a estrutura da informação no repositório.
Groupware permite a coordenação e comunicação entre processos e pessoas.
Um método retórico ou modelo de argumentação fornece a estrutura para a ocorrência de discussões complexas.
Um dos métodos mais conhecidos, que fornece este tipo de estrutura complexa e melhoramento do processo, é o IBIS (Issue-Based Information System ou Sistema de Informação Baseado em Tópicos) (Figura 3.3).
O modelo compreende três elementos básicos:
Experiências relatadas em mostram que o modelo IBIS também pode ser utilizado para registrar (apoiar) &quot;monólogos «(i.
e documentação de linhas de raciocínio), onde um decisor pode explicitar os porquês de suas ações e decisões.
Exemplos de Oims baseados em IBIS são gIBIS, SISCO e GRADD.
DRL (Decision Representation Language ou Linguagem para Representação de Decisões) é outro exemplo de modelo de argumentação, mais complexo que IBIS, o qual foi incorporado no Oims SYBIL.
Usos mais conhecidos de modelos de argumentação são sistemas de Groupware ou CSCW (Computer Supported Coopertive Work ou Trabalho Cooperativo Apoiado por Computador), Sistemas de Apoio à Decisão em grupo e Case orientado a processos.
Conklin ressalta que a questão fundamental de OMSI não é a tecnologia.
Em a verdade, é necessária uma mudança na cultura da organização e um comprometimento com a tarefa, tornando a captura e manutenção do conhecimento e, o uso de OM uma prática natural e importante.
Preferencialmente ela deve ser adaptável e organizável, coletando dados úteis e informações para posterior utilização, da maneira operacional usual, sem perturbar o fluxo normal e o trabalho dos funcionários.
Como em muitos casos isto não é possível, a execução de aquisição manual de conhecimento por um engenheiro é necessária.
Em muitos projetos existe o papel do engenheiro de conhecimento, que é responsável por limpar e organizar o conhecimento para preparar- lo para reutilização, deixando- o num forma útil e adicionando propriedades apropriadas para facilitar sua posterior reutilização.
OM, em particular centrada em processo, é interessante pois permite representar um modelo de documentação de forma flexível e adaptável ao problema em questão.
Esta flexibilidade permite tratar alguns pontos do processo de KDD difíceis de serem endereçados por as abordagens anteriores, entre eles o registro e a manutenção do histórico do processo de KDD, contemplando o apoio às iterações e redirecionamentos do processo;
E à liberdade de representar um método retórico que se adapte ao problema proposto, o que facilita tratar a independência de ferramentas e a disponibilizar mecanismos que possibilitem ao usuário explorar a sua experiência.
Uma das dificuldades nesta abordagem é a falta de critérios para definição e utilização de um OMSI, o que demanda em custo, principalmente na recuperação da informação, geralmente em forma de consultas.
O objetivo deste capítulo é descrever dois estudos de caso utilizados neste trabalho para levantar os requisitos, definir e validar o modelo de documentação proposto.
O primeiro estudo de caso, descrito na Seção 4.1, é a aplicação do processo de KDD para compreensão de revisões curriculares numa universidade brasileira, iniciado em.
Esta aplicação possui características importantes no contexto de KDD, por o fato da base de dados utilizada ser legada e temporal, o que exigiu grandes esforços nas etapas de Descoberta da Tarefa, Descoberta dos Dados e Preparação dos Dados, com um grande número de iterações e descobertas de algumas características sobre o domínio dos dados que não estavam documentadas no sistema fonte.
Com base em foi realizado um trabalho de estruturação, por assunto, de todas as tarefas realizadas, as informações relevantes ligadas a elas, os seus relacionamentos e os eventos que as geravam, com especial ênfase em iteração e redirecionamentos no processo.
O segundo estudo de caso, apresentado na Seção 4.2, é o desenvolvimento de uma nova aplicação de KDD para um problema médico definido para o Mining Knowledge Discovery in Databases), descrito em.
O objetivo desta aplicação foi caracterizar trombose a partir de resultados de exames laboratoriais de pacientes.
Foram necessários alguns esforços para a preparação dos dados, pois existiam atributos com domínio multivalorados com conjunto de valores.
Domínio O trabalho descrito em relata uma experiência de descoberta de conhecimento em base de dados realizada num sistema acadêmico.
Ela foi posteriormente continuada e, seus resultados publicados em e.
A base de dados utilizada faz parte do Sistema Discente, que controla todas as atividades de graduação da UFRGS -- Universidade Federal do Rio Grande do Sul.
Este sistema é considerado um SIL (Sistema de Informação Legado), pois está em funcionamento há 25 anos e é baseado em tecnologia de banco de dados hierárquico e programação em linguagem COBOL.
Muitas das informações estão associadas a um tempo de validade, introduzindo aspectos temporais à base de dados.
O objetivo inicial da aplicação do processo de KDD em era investigar o possível impacto das revisões curriculares na vida acadêmica dos alunos de graduação da UFRGS, uma vez que nesta universidade os alunos se formam em função de o currículo vigente por ocasião de a formatura e não por ocasião de o ingresso no curso e, por o fato dos cursos de graduação da UFRGS passarem freqüentemente por revisões curriculares.
Uma revisão curricular representa cortes de antigos requisitos e inclusão de novos requisitos num currículo.
Para que os alunos não sejam prejudicados desnecessariamente numa transição de currículo, regras de aproveitamento de crédito já cursados, chamadas de &quot;liberações», são definidas.
Mesmo assim existe a possibilidade de alguém ser afetado, pois requisitos cortados podem ter sido alcançados e novos requisitos terão que ser satisfeitos.
Alguns alunos podem ser beneficiados por as revisões, o que não é tão provável, pois geralmente uma revisão tem por objetivo a atualização do curso.
Por motivos administrativos, a UFRGS tenta evitar que existam currículos simultâneos para um mesmo curso e, consequentemente, existe apenas um currículo vigente por curso num dado instante.
Os alunos devem cumprir todos os requisitos curriculares de seu curso vigentes na época da formatura, os quais necessariamente não são os mesmos vigentes na época do seu ingresso.
O impacto de uma revisão curricular pode ter outras conseqüências além de aquelas relacionadas aos requisitos de formatura.
A prioridade de matricula de um aluno é avaliada por o seu desempenho no semestre anterior (e.
g número de aprovações, reprovações, cancelamentos) e, em função de o seu avanço no currículo.
Portanto, alunos podem ter sua prioridade de matrícula prejudicada por uma alteração curricular, apesar de seu desempenho ter sido bom.
O questionamento nesta situação é inevitável:
&quot;Como tal aluno pode estar na minha frente, se meu desempenho no semestre passado foi melhor?».
A análise de alguns casos revelou a revisão curricular como principal causa e houve interesse em buscar generalizações.
Este problema foi trazido por o então responsável por o Sistema Discente há quase uma década e, devido a o número de cursos oferecidos por a UFRGS ser muito grande e seus estudantes poderem ser afetados por as revisões curriculares de diferentes maneiras, considerando seus respectivos históricos acadêmicos, KDD foi visto como uma alternativa bastante interessante para ajudar a determinar se uma revisão curricular traz, ou não, prejuízo aos alunos que passam por ela, ou ainda, explicar o que é uma boa e uma má revisão curricular.&amp;&amp;&amp;
A base de dados do Sistema Discente foi utilizada como a fonte de dados para avaliar o impacto das revisões curriculares.
Este sistema é composto por três subsistemas:
Alunos, Currículos e Turmas.
O Subsistema Alunos armazena o histórico completo dos alunos e ex-alunos (e.
g admissão, matriculas, trancamentos);
Processo O processo de KDD adotado inicialmente foi baseado na seqüência de etapas proposta em.
Segundo Terra, as características de iteratividade e interatividade do processo de KDD destacadas em foram confirmadas por esta experiência e levaram a aplicação a resultados inesperados, desviando o trabalho dos seus objetivos iniciais.
Em a verdade, o que ficou explícito foi a característica ressaltada em de que as tarefas da etapa de Descoberta da Tarefa e dos Dados são intercaladas a todo momento e que influenciam diretamente o objetivo global da aplicação.
Estas características ficaram ainda mais evidentes nesta aplicação por o fato da base de dados fazer parte de um SIL e, fizeram com que fossem resgatadas informações perdidas ao longo de anos de manutenções não documentadas.
Muitas das descobertas foram uma grande surpresa até para o responsável por o sistema.
Uma tentativa de sintetizar diagramaticamante o processo de KDD descrito no restante desta seção é apresentada na Figura 4.1.
A compreensão do domínio foi baseada na documentação do sistema, que inclui um diagrama Er (Entidade-Relacionamento) da base de dados, gerado por o responsável por o sistema através de engenharia reversa;
As descrições dos arquivos Cobol;
Os catálogos de cursos da UFRGS;
E através da constante interação com o responsável por o sistema.
Terra destaca que sem uma intensiva interação, não teria sido possível chegar a um nível razoável de compreensão, pelo menos não em tempo viável.
Muitas informações que foram levantadas na compreensão do sistema foram alteradas à medida em que os dados foram analisados, pois apesar de um estudo do domínio ter sido realizado, os conceitos encontrados não eram definitivos nem mesmo para as pessoas envolvidas com o sistema.
Segundo Terra, a informação inicial de um Sistema Legado e Temporal não pode ser assumida como verdade no seu contexto e, somente com sua validação, é possível utilizar- la.
Após a compreensão do domínio da aplicação e dos dados, foi constatado não haver um arcabouço conceitual para analisar o prejuízo de uma revisão curricular e, hipóteses tiveram quer ser definidas com este intuito.
Foram definidas duas hipóteses, que diferenciam- se por o conjunto de dados a ser utilizado e por o conhecimento ao qual podem levar:
A primeira hipótese foi adotada como ponto de partida por a sua simplicidade, além de poder ser útil para a condução da segunda hipótese.
A tarefa inicial para validar esta hipótese era requisitar exemplos de revisões boas e ruins ao especialista, com o objetivo de criar um conjunto de revisões boas e um de revisões ruins.
Como não existia este conceito, foram fornecidos exemplos de cursos rotulados como &quot;estável «e &quot;não-estável».
Em este caso, o conceito de estabilidade estava relacionado à freqüência de revisões curriculares de um curso 1.
Foram fornecidos apenas dois cursos:
Um considerado estável e outro não-estável, o que não caracterizam exemplos suficientes para aprendizado.
Algumas opções foram avaliadas para contornar este problema, como identificar as possíveis formas de alteração de um currículo e a possibilidade de usar as revisões dos cursos estáveis com boas e cursos não-estáveis como más, mas decidiu- se por descartar esta hipótese ainda na etapa de preparação dos dados, pois a divisão entre boas e más revisões não foi realizada com base na qualidade da transição curricular, mas sim no conceito de estabilidade do curso, definido por o especialista.
Este conceito de estabilidade foi alterado ao longo de o processo, como será visto mais adiante.
A partir deste momento a segunda hipótese, a qual tinha o objetivo de analisar o impacto das revisões curriculares nos alunos, foi adotada.
Inicialmente foram definidas as tarefas que eram necessárias para validação desta hipótese.
Porém, para possibilitar as análises foi necessário definir formas para avaliar o impacto, já que não existia uma medida para tal.
Como o conceito de impacto mede o desempenho de um aluno em relação a o currículo, antes e após uma revisão, foram detectados dois critérios que poderiam ser utilizados para avaliar o impacto, um ligado ao posicionamento do aluno em relação a a conclusão do curso, ou seja, o cumprimento dos requisitos para formatura;
Em este ponto houve a necessidade de retomar a compreensão do domínio, para analisar quais as possíveis formas de avaliar impacto.
Duas formas foram inicialmente detectadas:·
aumento do tempo faz com que o aluno passe por mais revisões;·
número de revisões aumenta seu tempo de permanência no curso.
Após o estudo das possibilidades existentes para medir e avaliar o impacto, foram adotados dois cursos que seriam foco de análise.
Estes cursos foram indicados por o especialista.
Vale a pena lembrar, um foi considerado estável e outro não estável.
Para cada curso, foi criada uma tabela Turma.
Foi considerado para turma o ano de ingresso no curso.
Esta tabela contém valores que representam data de ingresso e de formatura, cujos valores permitem calcular NSem (número de semestres que o aluno levou para se formar) de todos alunos que foram analisados e também auxilia na contagem de NRev (número de revisões curriculares por as quais o aluno passou).
Por sugestão do especialista foram considerados para as datas de ingresso e formatura respectivamente, o ano/ semestre da primeira matrícula e ano/ semestre da última matrícula do aluno no curso, já que outros registros destas datas eram inexistentes no sistema, ou não confiáveis.
Assim, com as informações de Turma, seria possível calcular a correlação entre NRev e NSem, ou seja, a primeira forma de análise levantada.
Para encontrar NSem, é necessário calcular a diferença entre ingresso e formatura.
Os valores de ano/ semestre foram transformados numa representação própria para que seu cálculo fosse facilitado.
Em a primeira análise das tabelas Turma geradas para os dois cursos definidos, um considerado estável e outro não-estável, percebeu- se que não havia diferença significativa entre a freqüência de revisões curriculares destes dois cursos.
Conforme conhecimento prévio fornecido por o especialista, esperava- se que a freqüência de revisões do curso considerado estável fosse menor, o que não aconteceu.
Decidiu- se então realizar o levantamento do número de revisões em cada curso, selecionado somente cursos não extintos.
Os resultados obtidos, a partir de o levantamento do número de revisões em cursos não extintos, apresentaram um número significativo de casos onde o número de revisões curriculares é, numericamente, semelhante ao tempo de existência do curso.
Isto significa que a maioria dos cursos apresentaram a média de revisões curriculares próximas a 1 (um), o que contradiz conceito de estabilidade inicialmente fornecido por o especialista.
Com base nesta informação, concluiu- se não ser possível afirmar que um curso é mais estável que outro, se o critério utilizado for a freqüência de revisões.
O especialista refinou o conceito de estabilidade ressaltando que existem duas formas possíveis de registro de uma revisão curricular:
Uma é a manutenção do código do curso e a criação de uma nova versão de currículo e, outra é a criação de um novo código para o curso e da primeira versão de currículo vigente para ele.
Em este caso, no novo curso existe uma referência para o seu curso antecessor.
Esta também é uma forma de manter dois currículos simultâneos quando a alteração é de grande porte e, portanto, ser menos prejudicial aos alunos.
Quando num curso ocorrem revisões consideradas de grande porte e o código do curso não é alterado, este é considerado por o especialista como &quot;não estável».
Em este momento foi necessário trocar os cursos adotados inicialmente, então, uma nova preparação de dados foi realizada para os mesmos.
O especialista confirmou os cursos inicialmente fornecidos como seguindo a nova interpretação de &quot;estabilidade».
Em a análise das novas tabelas Turma, foi observado que em muitos casos NSem era menor que o número de semestres definidos nos respectivos currículos como necessários para sua conclusão.
Novamente recorreu- se ao especialista, que informou que, quando um aluno escolhe uma ênfase ou curso, este passa a estar vinculado a outro código de curso.
O conceito de curso-ênfase existe em alguns cursos onde o aluno deve optar por uma ênfase especifica no decorrer de o curso, os quais determinam requisitos específicos.
Inicialmente o aluno é admitido no curso com ênfase 0.
Posteriormente, quando fizer a opção, ele passa a cumprir os requisitos da ênfase escolhida.
Por conseguinte, alunos que passaram por cursos com ênfase acabam tendo pouco tempo dentro de um código de curso 2, ficando o resultado distorcido.
Vale a pena lembrar que foi utilizado como ingresso a primeira matrícula de um aluno no curso (código do curso).
Várias soluções foram avaliadas e, uma de elas foi a de unificar o histórico dos alunos.
Para verificar se essa unificação seria possível, diversas alternativas foram analisadas e, em cada uma de elas, novas descobertas sobre o domínio da aplicação e significado dos dados foram realizadas.
Algumas dizem respeito à utilização dos dois últimos dígitos do código do curso não para distinção de ênfases, mas sim para representar o turno em o qual o curso é ofertado;
Ou, em alguns casos para representar o título com o qual os alunos se formam.
Outra descoberta é que, apesar de ser impossível um aluno se formar na ênfase 0 de um curso com ênfases, foram encontrados na base de dados, vários alunos formados em ênfase 0.
Após Em os cursos com ênfase os três primeiros dígitos que compõem o código dizem respeito ao código do curso, e os dois últimos à ênfase específica.
Com todas estas novas informações foi concluído não ser possível saber com certeza, quando um aluno ingressou ou se formou em algum curso.
Novamente a amostra foi trocada, desta vez para alunos de cursos sem ênfase.
O conceito de estabilidade foi abandonado, pois o especialista não soube indicar cursos estáveis/ não-estáveis alternativos, que não tivessem ênfase.
Novamente foram geradas novas amostras e, novas descobertas sobre o domínio surgiram.
Uma análise mais detalhada na amostra revelou tempos de permanência no curso (NSem) muito próximos ao tempo sugerido para formatura.
Uma análise no histórico apontou um alto número de migrações entre diferentes cursos e um alto índice em cursos afins com os cursos analisados (e.
g cursos de engenharia), fazendo com que o aproveitamento de disciplinas reduzisse o tempo necessário para a conclusão do curso.
Em este caso, a amostra foi refeita e foram excluídos os alunos que migraram, ou seja, trocaram de curso.
Quando a amostra finalmente foi considerada estável e confiável, foi calculada a correlação NSem e NRev e descoberto que existe uma relação forte entre elas, porém não se pode afirmar qual variável determina o valor da outra.
Posteriormente, esta análise foi aprofundada em com o uso da ferramenta BKD (Bayesian Knowledge Discovery) e, estes resultados foram confirmados.
O próximo passo foi adotar a segunda forma definida para avaliar o impacto, baseado na transição curricular.
Para isso, foi definida uma tabela Impacto que permite a análise da diferença no número de disciplinas cursadas por um aluno válidas num currículo e no seu subseqüente.
Foram criadas duas tabelas de impacto, uma contendo as disciplinas obrigatórias e outras as opcionais.
As tabelas contêm uma entrada para cada transição de currículo que cada aluno passou, o número de disciplinas que ele havia atendido e o número de liberações que ele possuía no currículo anterior, significando o que ele tinha, em termos de créditos, antes da revisão.
Depois foi verificada a situação em que o aluno ficou, ou seja, número de disciplinas que ele aproveitou diretamente e as liberações que lhe foram dadas.
A diferença entre o que ele tinha e o que ele passou a ter, no novo currículo é o impacto.
A partir de esta tabela várias opções de análise foram possíveis, algumas realizadas em, outras posteriormente desenvolvidas em, entre elas:·
verificar quais revisões tiveram o maior número de alunos afetados;·
calcular o impacto causado em cada revisão para cada aluno.
Um valor positivo significa que o aluno teve prejuízo, um valor negativo representa benefício;·
identificar quais revisões foram neutras, benéficas ou prejudiciais;·
verificar se o impacto total das revisões (soma de todos os impactos de um aluno) tem relação com o seu tempo de permanência no curso;·
analisar o impacto total das revisões na vida escolar do aluno.
Posteriormente ao trabalho, descrito em, o mesmo processo foi aplicado a novos cursos.
Juntamente com os novos tipos de análise definidos, os resultados da continuação destas pesquisas estão descritas em.
No entanto, como existem várias particularidades concernentes a cada curso, algumas considerações estão sendo feitas a nível de preparação dos dados e novas descobertas sobre o domínio estão surgindo.
Novas análises também estão sendo realizadas a partir de as tabelas Turma e Impacto geradas, como por exemplo, tratar o aspecto temporal das revisões.
No que diz respeito às ferramentas utilizadas para a realização de todas estas tarefas, várias foram utilizadas, de acordo com a necessidade da tarefa e domínio no seu manuseio.
Mais especificamente para as tarefas de preparação dos dados foram utilizados comandos em SQL (Structured Query Language ou Linguagem de Consulta Estruturada) e PL/ SQL, linguagem proprietária do SGBD Oracle, que é o gerenciador de banco de dados para o qual a base de dados do Sistema Discente foi exportado.
Algumas operações referentes ao processamento e visualização dos dados também foram realizados utilizando a ferramenta Microsof ACCESS pois, em alguns casos, facilitou a integração com as ferramentas em uso.
A utilização das diversas ferramentas foram intercaladas a todo momento e sua utilização está muito ligada não só ao objetivo da tarefa, mas também ao conhecimento prévio e à facilidade em trabalhar com determinada ferramenta, bem como seu desempenho.
Os recursos utilizados para análise foram bastante dependentes da tarefa a ser realizada.
Algumas tarefas foram realizadas em planilha eletrônica, como sumarização, geração de gráficos e tarefas estatísticas mais simples.
A ferramenta BKD também foi utilizada para análise de dependência e correlação.
As tarefas de classificação e associação, num primeiro momento foram apoiadas por a ferramenta DBMiner.
Posteriormente, com a aquisição da ferramenta Clementine, esta passou a ser mais freqüentemente utilizada, tanto para as tarefas de análise, como preparação dos dados em alguns casos simples, como seleção e projeção de conjuntos.
A documentação do processo está mantida em diversos arquivos de diferentes formatos, geralmente os formatos disponibilizados por a ferramenta que apoiou determinada tarefa e que foi ao encontro de as necessidades de visualização.
Por exemplo, os comandos SQL e PL/ SQL e as regras de associação e classificação estão na forma de arquivo texto;
Resultados de distribuição, correlação em forma de gráficos ou tabelas;
Resultados da execução de comandos SQL em tabelas do SGBD Oracle, etc..
As aplicações construídas nas ferramentas específicas de mineração estão em seu formato próprio, bem como alguns modelos específicos que podem ser gerados para posterior reuso dentro de o seu ambiente.
Conclusões O estudo desta aplicação foi extremamente rico, porque ela consegue ilustrar muito bem as principais características e dificuldades na condução de um processo de KDD.
como se pode perceber o número de retomadas às etapas/ atividades foi muito grande, as interações com o responsável por o Sistema Discente foram constantes e, sem elas, provavelmente o processo não se conduziria da mesma forma.
As descobertas sobre o domínio e a aplicação aconteceram a todo o momento e, muitas vezes, as estratégias e objetivos definidos anteriormente tiveram que ser redefinidos.
Em cada etapa/ atividade, o analista estava tomando decisões e justificando- as.
Outra questão interessante é que a descrição desta aplicação foi focada no processo como um todo, ou seja, nas estratégias definidas, nos redirecionamentos no processo e nos fatores que levaram a eles.
No entanto, existem muitas informações que são manipuladas e geradas ao longo de o processo, tais como os comandos, procedimentos e tabelas utilizadas para gerar as amostras, as ferramentas utilizadas, os resultados obtidos, as reexecuções que foram necessárias a cada retomada de etapa/ atividade, entre outras e, que apesar de não estarem explicitamente citadas, estiveram presentes em todos os momentos do processo.
Especificamente no caso de ferramentas foram gerados diversos arquivos com aplicações e resultados.
Vale a pena salientar que esta aplicação foi desenvolvida ao longo de dois anos e que não foi fácil chegar a todas as hipóteses trabalhadas, às descobertas sobre o domínio e às decisões tomadas.
Devido a o grande número de iterações e hipóteses trabalhadas, muitas informações referentes às etapas foram sendo perdidas ao longo de o processo, desde procedimentos executados até as justificativas das decisões tomadas e o significado do conteúdo das tabelas.
Muitos procedimentos tiveram que ser refeitos para garantir a confiabilidade do processo, o que consumiu tempo e recursos.
A recuperação, quase que na totalidade das atividades e informações que envolveram a construção desta aplicação, se deve muito ao fato de ela ser objeto de um trabalho acadêmico, onde o objetivo era descrever uma aplicação do processo de KDD na sua totalidade.
No entanto, num ambiente de negócios, caso não exista algum mecanismo que possibilite organizar e manter estas informações, de forma prática, provavelmente a manutenção e organização destas informações se tornariam muito mais difíceis, quase que impossível.
Como o mesmo processo aplicado em está sendo estendido, no contexto de um projeto de pesquisa 3, a outros cursos e novas análises estão sendo realizadas, porém, para possibilitar o reuso do trabalho anterior, foi necessária a busca, recuperação e documentação dos procedimentos, comandos e informações sobre os dados e o domínio da aplicação, tomadas de decisões que levaram a ações, à seqüência da execução das tarefas, etc..
Esta não foi uma tarefa fácil e levou praticamente um ano, apesar de grande parte já estar documentada em.
O que dificultou o trabalho foi o grande número de arquivos gerados e utilizados por as tarefas, principalmente de versões de comandos SQL e PL/ SQL e sua seqüência de aplicação, pois estes foram sendo modificados à medida que problemas eram encontrados e, não se sabia ao certo qual a última versão aplicada e porque as outras tinham sido descartadas.
Muitas vezes, os arquivos contendo resultados e os aplicativos gerados nas ferramentas também tinham várias versões.
Portanto, as maiores dificuldades encontradas foram identificar o papel de cada um dos diversos arquivos existentes, quais as versões válidas, sua relação com as tarefas e a seqüência destas dentro de o contexto do processo.
Esta aplicação ilustra tudo o que foi descrito no Capítulo 2, principalmente relacionado à dimensão de uma aplicação de KDD, à dificuldade de manter- la e gerenciar- la, à utilização de fontes de dados heterogêneas e ferramentas não integradas, o papel do analista como tomador de decisão e do especialista no domínio como fonte importantíssima de informação.
Outro conceito ilustrado na prática foi o constante ajuste dos objetivos e os redirecionamentos necessários durante o processo, causados principalmente por o aprimoramento do conhecimento sobre o domínio dos dados e do sistema fonte.
Por estes motivos, Terra dedica um capítulo do seu trabalho para salientar a necessidade de um ambiente que apóie o usuário a organizar e registrar Projeto SLAT (Reengenharia de Sistemas Legados com Características Temporais) apoiado por a FAPERGS (Fundação de Amparo à Pesquisa do Rio Grande do Sul).
Domínio A aplicação descrita nesta seção foi desenvolvida por a autora e faz parte de um desafio em descoberta de conhecimento lançado por a PKDD´ 99.
Esta aplicação possui um escopo bem menor que a aplicação descrita na Seção 4.1 e não pôde ser apoiada por um especialista.
Porém, a documentação do processo pôde ser realizada num nível maior de detalhe, pois o seu objetivo principal era concentrarse nas tarefas, sua estrutura e seu detalhamento e, não nos resultados, no contexto de uma aplicação.
Os dados para esta aplicação foram coletados junto à base de dados do Hospital Universitário de Chiba e, contêm informações referentes a pacientes com doenças do colágeno.
Doenças do colágeno são chamadas doenças auto-imunes, onde os pacientes geram anticorpos que atacam seus próprios antígenos.
Alguns pacientes podem gerar vários tipos de anticorpos e, suas manifestações, podem incluir todas as características de doenças do colágeno.
Dentro de as doenças do colágeno, uma das mais importantes, severas e, também, uma das maiores causadoras de morte é a trombose.
A trombose pode surgir a partir de diferentes doenças do colágeno e ocasiona o aumento da coagulação do sangue, o que pode obstruir as veias.
Foi atestado que sua complicação está relacionada com alguns tipos de anticorpos e, por isso, é importante detectar e prever a possibilidade de sua ocorrência num paciente, já que esta doença é tratada como uma emergência.
A base de dados fornecida é composta por três tabelas:
TSUM_ A_ CSV, TSUM_ B_ CSV e TSUM_ C_ CSV.
Estas três tabelas são conectadas por a identificação do paciente.
A tabela TSUM_ A_ CSV contém informações básicas sobre todos os pacientes, tais como sexo, data de nascimento, diagnóstico, etc;
TSUM_ B_ CSV contém as informações dos exames laboratoriais especiais daqueles pacientes que precisaram de testes especiais relacionados ao diagnóstico de doenças do colágeno.
Os médicos são responsáveis por a inclusão dos dados nestas duas tabelas.
A tabela TSUM_ C_ CSV, contém todos os exames laboratoriais, não necessariamente ligados à ocorrência de trombose, armazenados no sistema de informação do hospital desde 1980.
Baseado nestes dados, os seguintes objetivos foram previamente delimitados para esta aplicação, no contexto da competição de descoberta de conhecimento:
Processo Para efeitos do estudo do caso em questão, foi adotado inicialmente o primeiro objetivo, acima destacado, que é tentar prever e detectar trombose.
Inicialmente decidiu- se trabalhar com as tabelas TSUM_ A_ CSV e TSUM_ B_ CSV para tentar classificar trombose e identificar os valores de atributos relacionados a ela e, após, estender as análises para a tabela TSUM_ C_ CSV, complementando a caracterização com um número maior de variáveis.
Apesar de o enfoque na descrição desta aplicação ser mais detalhada, devido a o seu escopo ser menor, uma tentativa de representação sintetizada, seguindo os mesmos moldes da apresentada para a aplicação das revisões curriculares, é apresentada na Figura 4.2.
Para definir melhor as atividades que deveriam ser realizadas, foi feita uma avaliação sobre a estrutura, domínio e conteúdo dos dados em relação a os recursos disponíveis, a saber, o banco de dados Oracle 7.0.3 e a ferramenta de mineração Clementine.
Todas as tarefas de análise dos dados, sejam nas etapas de preparação ou análise dos dados foram executadas, utilizando a ferramenta Clementine e, as manutenções das tabelas, através de comandos SQL e PL/ SQL.
Em algumas situações, os dados da base de dados foram exportados para o Microsoft Access, pois facilitou a execução de alguns procedimentos.
Este primeiro estudo possibilitou a definição das tarefas que, a principio, deveriam ser realizadas, as quais foram divididas entre as etapas de preparação e análise dos dados:
Após o planejamento, as tarefas começaram a ser realizadas e, a princípio, o que pareciam ser tarefas fáceis, envolveram análise mais específica e tomada de decisão, devido a problemas revelados em sua execução.
A tarefa de conversão das tabelas deu- se sem problemas, como esperado.
O resultado da execução da tarefa para criar chaves e relacionamentos detectou os primeiros problemas no domínio dos dados, devido a valores nulos encontrados no domínio do atributo chave.
Os casos com valores nulos ou brancos foram analisados e, os procedimentos que deveriam ser tomados foram definidos.
Em este caso, foram excluídos alguns registros que não iriam trazer maiores conseqüências para esta aplicação o que, em outras aplicações, poderia se tornar um problema mais difícil de ser contornado.
Esta tarefa foi executada três vezes, mudando apenas a fonte de dados, ou seja, a tabela origem e as informações ligadas aos resultados.
Em a execução da tarefa para a tabela TSUM_ B foram necessárias várias iterações, pois existem dois atributos que compõem a chave.
A tarefa de tratar atributos multivalorados foi decomposta em várias subtarefas, pois foi detectado que no conteúdo dos atributos multivalorados Symptons, Diagnosis e AnaPattern não existia um padrão para os separadores de valores (e.
g &quot;Abort «e &quot;Abo», os quais, apesar de possuírem grafias diferentes, têm o mesmo significado).
Portanto, era preciso analisar e padronizar os valores destes atributos.
No caso de os atributos Symptons e Diagnosis existia a necessidade de tornar- los atômicos, para que as tarefas de mineração definidas pudessem ser executadas.
Para isso, foi definido que, além de a padronização, seria criada uma nova tabela para cada um dos atributos, tornando- os atômicos.
No caso de o atributo AnaPattern foi necessário analisar, definir as políticas de padronização e alterar o conteúdo do atributo, mas este deve permanecer como um único atributo na tabela de origem, sendo que seu valor reflete as possíveis combinações de valores.
As tarefas foram decompostas em:
As tarefas 1, 2 e 4 não possuem tarefas precedentes e podem ser executadas em qualquer ordem.
No entanto, a tarefa número 3 tem que ser executada antes da número 2 e, a tarefa número 4 antes da número 5, determinando ordem em algumas das tarefas.
A execução das tarefas 1, 2 e 4 envolveram primeiramente a identificação de todos os valores encontrados para o atributo e aqueles valores com o mesmo significado, porém com grafias diferentes e, a decisão sobre qual de eles seria adotado como padrão.
Após definidas as políticas, os valores foram alterados.
Para se chegar a todos os valores padronizados, foram intercaladas, várias vezes, as atividades de análise e de alteração.
A partir de isso, as tabelas com os diagnósticos e sintomas foram criadas no Oracle, passando estes, a serem valores atômicos.
A tarefa para criar a tabela de mineração envolveu definir alguns valores da tabela de sintomas e diagnósticos (os que ocorriam com maior freqüência), que deveriam se tornar atributos, onde os valores 0 e 1 indicariam, ou não, sua existência.
Entretanto, para fazer isto houve a necessidade de uma análise mais aprofundada para decidir quais valores se tornariam atributos e analisar a freqüência destes valores no conjunto, para que fosse possível cobrir o maior número possível de casos.
Para isso, foram utilizados algoritmos de associação.
Quando os atributos a serem criados foram definidos, a tarefa de criar tabela para mineração foi executada.
Em este momento, percebeu- se novamente a necessidade de adicionar uma nova tarefa para verificar a confiabilidade dos dados gerados na nova tabela.
Foram identificados alguns atributos com problemas e realizadas algumas alterações na tabela gerada.
A partir de a tabela definitiva para a tarefa de mineração, foram selecionados os atributos para a aplicação do algoritmo de kohonen e os parâmetros definidos e ajustados.
Após gerado o modelo, alguns segmentos foram selecionados através do seu mapeamento em forma gráfica e, o atributo que determinava, ou não, a existência da trombose foi classificado.
Para isso, também houve uma seleção prévia de atributos e ajuste dos parâmetros.
Com base nos resultados da classificação, os atributos mais significativos foram selecionados para a tarefa de associação, com intuito de verificar a relação entre um ou mais sintomas e o valor de alguns exames com a ocorrência de trombose.
Apesar de os poucos atributos e dos valores de parâmetros para confiança e suporte serem baixos, foi gerado um número muito grande de regras.
O número de regras e a falta de um especialista impossibilitou uma análise manual das regras, bem como uma limpeza prévia e, a tarefa de associação foi abandonada.
Isto exemplifica as dificuldades para a avaliação das regras geradas, descritas na Seção Em cada uma das tarefas de análise dos dados foram gerados modelos, a partir de parâmetros específicos e, cada um de eles, serviu como entrada para a construção do próximo modelo.
Como o ajuste dos parâmetros ocasiona várias iterações foi difícil gerenciar quais parâmetros geraram quais modelos e, em quais situações, o que ocasionou a reexecução de algumas tarefas quando era necessário tomar conhecimento dos parâmetros e variáveis utilizadas.
Os resultados específicos de cada modelo foram sendo armazenados em vários momentos e, em diferentes formatos, o que dificultou relacionar os resultados com o modelo que o gerou, em que circunstâncias, e com quais parâmetros.
Conclusões As características desta aplicação são um pouco diferentes da descrita em.
O domínio dos dados é bem menor e menos complexo.
Os objetivos estão pré-estabelecidos e foram previamente avaliados por as pessoas que propuseram este desafio.
Apesar de não existir a presença do especialista do domínio, a análise e tomada de decisão por parte de o analista esteve presente nas diversas atividades.
No entanto, as características e a estrutura do processo se repetiram em ambas as situações.
Avaliando a execução do processo, podemos concluir que, inicialmente foi feito um planejamento das tarefas a serem realizadas, juntamente com seus objetivos e, as restrições de ordem foram definidas.
A partir de a execução destas tarefas, novas tarefas foram acrescentadas, ou por falta de planejamento, ou por decorrência de características, ou problemas detectados nos resultados da execução de uma tarefa.
Existiram também situações em que uma tarefa não foi definida da forma mais apropriada e, então, uma nova definição para a tarefa foi criada.
Houve ainda a necessidade de decompor tarefas que a princípio pareciam mais simples.
Resumindo, pode- se dizer que nesta aplicação, tal como na aplicação das revisões curriculares, as tarefas foram geradas a partir de alguns eventos:·
planejamento: Quando foram definidas as tarefas que inicialmente deveriam ser executadas para se alcançar o objetivo proposto.·
problemas/ inconsistências:
A análise do resultado de uma tarefa acusou algum problema ou inconsistência que devem ser tratados antes de se prosseguir o processo.·
decomposição: Quando uma tarefa que, a princípio parecia simples teve a necessidade de ser decomposta para tratar sua complexidade e facilitar sua execução.·
necessidade de criação de uma nova tarefa que não foi prevista no planejamento.
Em alguns casos, como as tarefas de converter os arquivos de texto para Oracle e criar chaves e relacionamentos, a mesma tarefa foi executada tantas vezes quanto era o número de tabelas existentes na base de dados, mudando apenas a fonte de dados, ou seja, o arquivo de entrada e, obviamente, as informações ligadas ao seu resultado.
Isto quer dizer que a reexecução de uma mesma tarefa pode não estar ligada apenas a problemas/ inconsistências detectadas na sua execução ou na execução de atividades/ etapas posteriores, mas também à execução de uma tarefa com os mesmos objetivos, descrição e procedimentos, porém a partir de fontes de dados diferentes, o que foi o caso.
Analisando do ponto de vista das informações manipuladas durante a aplicação, podemos enumerar diversos tipos e formatos de arquivos que foram gerados, tais como arquivos com as instruções SQL e PL/ SQL para criação, povoamento e alteração dos dados na base de dados;
Arquivos com os valores dos atributos e sua distribuição num momento específico, a partir de os quais foram definidas as políticas de padronização;
Arquivos com gráficos que foram utilizados para visualizar, selecionar e gerar os segmentos;
Resultados da classificação de cada um dos segmentos selecionados;
Arquivos com as aplicações geradas na Clementine, entre outros.
Por outro lado foram utilizados arquivos de entrada de dados, neste caso especificamente, as tabelas da base de dados e arquivos com a descrição do domínio dos dados e da aplicação.
Como em muitas tarefas, o resultado da sua execução é um recurso para a execução da próxima tarefa podemos enxergar que, de um ponto de vista, alguns arquivos são entradas para algumas atividades e, de outro, são saídas.
Por exemplo, um gráfico com os diversos segmentos é resultado do mapeamento dos segmentos gerados por o algoritmo de Kohonen de forma gráfica e, do ponto de vista de classificação, ele foi utilizado para selecionar os segmentos que foram classificados.
Portanto, podemos concluir que foram manipulados arquivos de diversos formatos que podem ter o papel de entrada ou saída de dados, dependendo da tarefa;
E que o número de arquivos utilizados e/ ou gerados por determinada tarefa é variável.
A grande dificuldade é determinar por qual atividade estes arquivos foram gerados, qual a parametrização utilizada, quais as entradas e/ ou variáveis consideradas e, sob que circunstâncias e políticas eles foram alcançados.
Isto tudo é agravado, quando o número de iterações é grande e várias ferramentas são utilizadas em conjunto, o que é uma característica eminente do processo de KDD.
Novamente, as características e problemas descritos no Capítulo 2, foram ilustrados com base em uma aplicação com dados e objetivos de um problema real, complementando o que foi demonstrado na Seção 4.1, pois a descrição deste estudo de caso fixou- se menos no empreendimento em KDD como um todo, e mais nas tarefas e seu detalhamento, e novos aspectos, em relação a o processo de KDD, puderam ser visualizados.
Em este Capítulo é apresentada uma proposta de apoio à tarefa do analista na construção de aplicações de KDD, a qual é baseada na importância que políticas de estruturação e documentação de suas atividades e informações, têm como um dos pontos facilitadores na sua condução.
Como detalhado no Capítulo 2, as características particulares do processo de KDD, somadas às principais dificuldades na sua aplicação, tornam complexa a construção e gerenciamento de uma aplicação, em particular de natureza exploratória e incremental.
Esta complexidade foi ilustrada no Capítulo 4 através de dois estudos de casos e, reforçou a necessidade de um apoio mais efetivo ao usuário na sua execução, tal como ressaltado em,,, e.
Como visto no Capítulo 3, existem poucos trabalhos que endereçam este tópico.
Entre os trabalhos encontrados, a maioria adota uma abordagem de apoio ativo, onde um sistema dá recomendações de como executar uma tarefa, ou seja, escolher um algoritmo de mineração.
Apesar de sua importância, há algumas limitações, entre elas:·
estão mais voltadas para utilização de técnicas de preparação e análise dos dados, em situações genéricas;·
não lidam com a iteratividade do processo;·
não tratam da integração entre ambientes heterogêneos;·
não se preocupam com a gerência da aplicação.
Ainda, apesar de as recomendações dadas por este tipo de sistema serem úteis, a sua capacidade em justificar- las é limitada, bem como é restrita a diversidade de alternativas que podem ser formuladas.
Adicionalmente, perde- se o contexto que gerou uma recomendação e os resultados obtidos com o seu emprego, pois este não é objetivo do sistema.
Outra abordagem possível tem sua inspiração baseada em OMSI orientada a processos, onde mecanismos não ativos podem ser disponibilizados para que o usuário possa organizar e documentar as tarefas realizadas e informações manipuladas na construção de uma aplicação, bem como as informações globais sobre o processo de KDD, sob ponto de vista gerencial.
Este tipo de abordagem é interessante, pois apesar de não dar &quot;dicas «ao usuário, permite- lhe criar sua própria política de documentação baseada nas características reais de suas aplicações, do seu ambiente e de sua metodologia de trabalho.
Outra vantagem, é que informações sobre o raciocínio que levou a objetivos, tarefas e decisões podem ser utilizadas como fonte de consultas sobre as experiências de aplicações de forma mais produtiva.
Em este caso, o usuário pode gerenciar adequadamente todas as tarefas já realizadas e seus resultados, registrar as suas experiências (que tiveram ou não êxito) e, posteriormente, aprender com eles.
A proposta deste trabalho é centrada na importância deste tipo de contribuição ao trabalho do analista na construção de aplicações de KDD, pois vai mais ao encontro de as características do processo e de suas necessidades de apoio e, tem como objetivo principal propor:·
um modelo de documentação para aplicações de KDD, com intuito de disponibilizar mecanismos que possibilitem estruturar informações que facilitam a gerência de uma aplicação, independente dos recursos utilizados.
Estas informações englobam desde o objetivo da aplicação;
As tarefas, sua várias iterações, seus relacionamentos e execuções;
Até as tomadas de decisões e justificativas envolvendo cada retomada ou mudança de etapa.·
a utilização deste modelo através de um ambiente de apoio à documentação que possibilita a captura, armazenamento e recuperação das informações e estruturas definidas no modelo.
Esse ambiente deve ser o mais adaptável possível às necessidades e características do analista.
Este modelo e seu uso através de um ambiente de apoio são importantes pois podem facilitar:·
uma melhor compreensão e consolidação dos conceitos envolvidos com o processo de KDD;·
o registro das várias iterações e redirecionamentos ocorridos durante o processo;·
o registro de tudo o que foi realizado e alcançado durante a construção de uma aplicação;·
o registro de todas as informações e recursos manipulados durante o processo, inclusive referentes ao uso de diferentes ferramentas;·
a gerência da aplicação;·
a tomada de decisão, principalmente nas situações em que é necessário retomar algum procedimento;·
a coleta de experiências e conhecimento que podem guiar analistas em aplicações futuras.
Em resumo, o modelo proposto não tem como objetivo gerar informações, conduzir ou validar uma aplicação de forma automatizada, pois para isso precisaria estar integrado a um ambiente de descoberta de conhecimento específico.
Seu propósito é disponibilizar meios para que o usuário consiga documentar de forma estruturada a sua aplicação e fazer uso disto, principalmente para validar e conduzir a sua tarefa.
Contudo, esta é uma proposta complementar àquelas baseadas em conhecimento, que se preocupariam com o aspecto recomendação.
Mais ainda, é importante salientar que, no futuro, a documentação gerada como resultado de um processo pode vir a constituir uma base de conhecimento para conduzir outros processos, mas esta perspectiva não é abordada neste trabalho.
Para o levantamento dos requisitos do modelo e do ambiente de apoio proposto, foi realizado um estudo aprofundado das diversas abordagens para o processo de KDD,, e em particular, além de os dois estudos de caso descritos no Capítulo 4.
Estas atividades ajudaram a melhor compreender a estrutura do processo, suas necessidades operacionais e organizacionais, bem como os problemas que envolvem sua aplicação em ambientes reais.
Requisitos do Modelo Baseado no conceito de que uma aplicação de KDD é composta por diversas tarefas distribuídas nas etapas que compõem o processo, o ponto de partida para o levantamento dos requisitos foi identificar como as tarefas se comportam dentro de uma aplicação, ou seja, como, quando e por que são criadas, executadas e se relacionam.
Posteriormente, foram identificados os resultados produzidos por as tarefas e os recursos e informações manipuladas.
A partir de a documentação das atividades realizadas nos dois estudos de casos percebeu- se que:
A) o ponto de partida de uma aplicação é um planejamento inicial das tarefas a serem executadas;
B) este planejamento pode ou não impor uma ordem na execução das tarefas, dependendo do contexto;
C) o processo é incremental, novas tarefas vão sendo criadas para solucionar problemas encontrados, tratar questões não definidas anteriormente ou decompor um problema que inicialmente não parecia tão complexo.
Esses refinamentos no processo podem incidir, entre outros, em a:·
reexecução de tarefas;·
alteração da restrição na ordem de execução das tarefas;·
alteração dos parâmetros definidos anteriormente para uma tarefa.
D) em algumas situações, tarefas genéricas são definidas, onde a descrição sobre os objetivos e procedimentos é a mesma.
O que é alterado são os recursos ou parâmetros da atividade usados em sua execução.
Um exemplo que ilustra esta situação são as tarefas para criar a tabela Turma e para cálculo da correlação, executadas para diferentes cursos na aplicação discente, descrita na Seção 4.1, em a qual diferentes amostras são empregadas a cada repetição da tarefa;
E) existem diversas informações que especificam cada tarefa, como seu objetivo, descrição dos procedimentos a serem adotados na sua execução, sua data de criação e dados referentes a cada execução (caso existam), tais como data de execução, descrição e avaliação dos resultados.
Portanto, inicialmente é necessário apoiar o usuário na definição e refinamento das tarefas, possibilitando- o:
A) criar tarefas e especificar as informações que a detalham;
B) relacionar as tarefas através de opções que impõem uma restrição na sua ordem de execução e definem uma hierarquia, no caso de decomposição;
C) redefinir a especificação de uma tarefa, com a possibilidade de manter as informações relativas às versões anteriores, criando um histórico com suas várias versões;
D) manter informações sobre todas as execuções de uma tarefa, sem que seja necessário redefinir- la;
E) possibilitar a definição de tarefas genéricas, para casos onde mudam somente as fontes de dados ou parâmetros para a execução da tarefa;
F) tratar as tarefas da mesma forma nos vários níveis de decomposição.
Informações complementares à estrutura do processo, às tarefas, suas execuções e seus relacionamentos dizem respeito:·
Ferramentas: Como já reforçado várias vezes, o analista pode dispor de um conjunto heterogêneo de ferramentas, cada qual mais apropriado para um aspecto de seu processo.
Em o contexto de uma aplicação, várias ferramentas podem ser utilizadas em conjunto, não só a nível de aplicação, mas também a nível de tarefas, as quais podem intercalar o uso de mais de uma ferramenta, como por exemplo, uma para análise dos dados e outra para alteração dos dados na base de dados.·
Fontes de dados:
Existe um conjunto de informações que são necessárias para que uma tarefa seja executada ou que auxiliam na sua definição, como arquivos com informações sobre o domínio da aplicação e dos dados, base de dados, arquivos texto ou tabelas específicas, arquivos fonte de algoritmos ou instruções, entre outros.
Estas informações podem ser vistas como entrada para a execução de uma tarefa.
Existem diversos tipos de resultados que são gerados por a execução de uma tarefa, tais como, arquivos com regras, gráficos de distribuição, tabelas, base de dados, arquivos gerados por os aplicativos, resultados de operações SQL, etc..
Apesar de esta diversidade de tipos e formatos, as informações necessárias para especificar- los e gerenciar- los são comuns, como nome, descrição e localização.
É interessante notar que, muitas vezes, o resultado de uma tarefa gera subsídios para a execução de outra.
Isto quer dizer que um arquivo com resultados gerado por uma tarefa pode ser fonte de dados em outra (não necessariamente a primeira tarefa imediatamente seguinte), podendo assumir diferentes papéis em diferentes tarefas.
Finalmente, atenção especial deve ser dada ao caso específico das tabelas e/ ou bases de dados projetadas com o objetivo de apoiar inúmeras outras tarefas ou até mesmo aplicações.
Este tipo de resultado necessita de uma definição mais formal e completa sobre o seu objetivo, conteúdo, estrutura, domínio e restrições, do que os outros tipos de resultados.·
Justificativas: As ações que levaram às reestruturações e ao refinamento das tarefas geralmente são baseadas em vários critérios e informações de as quais o analista dispõe para analisar determinada situação e tomar decisões e, justificam os procedimentos adotados.·
Conhecimento: Muitos dos eventos que ocasionaram alterações no processo são gerados por descobertas do domínio adquiridas na execução das tarefas, ou seja, durante a realização de um processo de &quot;arqueologia «nos dados, ou adquirido junto aos especialistas.
O estudo de caso relatado na Seção 4.1 ilustra bem este aspecto.
A documentação deste conhecimento, bem como das justificativas, é fundamental para compreensão das iterações e reestruturações realizadas no processo.
O tópicos descritos acima complementam as informações sobre as tarefas e, o usuário deve então dispor de mecanismos para:
A) relacionar a cada tarefa ou sua execução:·
os resultados gerados;·
as fontes de dados utilizadas como recurso para sua definição ou execução;·
as ferramentas necessárias ou utilizadas na sua execução, bem como os seus recursos específicos e objetivos, caso o usuário queira detalhar o seu uso;·
conhecimento adquirido junto ao especialista ou ao domínio através da análise dos dados;·
justificativas acerca de as decisões tomadas.
B) facilitar a especificação:·
detalhada das tabelas e/ ou base de dados que foram geradas por a aplicação.
Este recurso também pode auxiliar o usuário a definir uma base de dados existente, caso não exista uma documentação mais detalhada sobre os dados no sistema de origem, apesar de não ser este o propósito deste modelo;·
de papéis para arquivos, onde um mesmo arquivo possa ser identificado por o seu papel na tarefa, ou seja, se ele é recurso necessário a sua execução, ou se é um resultado gerado por a sua execução.
Estes recursos podem ser estendidos à aplicação como um todo, já que em termos globais da aplicação podem haver arquivos com explicação sobre o domínio da aplicação, justificativas acerca de a decisão sobre a definição dos objetivos, bem como o conhecimento que pode ser global, não estando relacionado com uma tarefa em particular ou sua execução.
As ferramentas também podem ser definidas por aplicação, caso o analista não ache necessário especificar- las junto as tarefas individuais.
Aplicações de KDD podem ser vistas como um projeto ou parte de um projeto maior.
Como todo projeto, ele possui objetivos globais, uma equipe envolvida, recursos disponíveis, prazos preestabelecidos, critérios para o formato das saídas, entre outros.
O registro destas informações é de extrema importância para ajudar o usuário a delimitar e a conduzir o processo e, posteriormente, na sua análise e avaliação.
Portanto, é interessante disponibilizar meios para que o usuário possa descrever sua análise crítica sobre a aplicação.
Este tipo de informação é útil para aplicações futuras, já que um resumo que espelha, de certa forma, a validade da aplicação.
Sem isso, possivelmente para uma posterior avaliação da aplicação seria necessária uma análise aprofundada da mesma, ou recorrer ao conhecimento informal junto às pessoas que participaram da sua construção.
Os requisitos de modelo levantados nesta seção estão apresentados de forma sumarizada na Figura 5.1.
Especificações Avaliação Conjunto de Tarefas Especificações Execuções ­várias execuções Ordem de Execução Versões -- histórico Decomposições Reestruturação dinâmica Tarefas Genéricas Tratamento homogêneo ­ independência do nível hierárquico Variedade de resultados gerados Fontes de dados utilizadas:
Arquivos, tabelas, modelos, etc..
Papéis diferenciados:
Fonte de dados/ resultados Incorporação de informações à bases/ tabelas geradas no processo Ferramentas manipuladas, seus recursos e objetivos Conhecimento gerado (especialista ou análise) Justificativa acerca de as decisões tomadas.
Requisitos de um Ambiente de Apoio O ambiente que dará suporte ao modelo proposto deve disponibilizar mecanismos que possibilitem a captura e o armazenamento das informações representadas no modelo, bem como várias formas de visualização e consulta das mesmas.
No que diz respeito às informações que devem ser coletadas e sua estrutura, cabe ao ambiente implementar o modelo da forma mais completa e flexível possível.
Quanto a as formas de visualização e consultas, é interessante definir alguns requisitos que são fundamentais para que o uso do modelo atinja seus propósitos:
A) Visualização:
Devido a o grande número de informações ligadas a cada tarefa, é interessante que o usuário possa ter uma visão global da estrutura do processo e suas tarefas e, dispor de mecanismos que deixem explícitas quais informações estão relacionadas a cada uma de elas.
A partir de esta visão, ele pode então navegar entre as tarefas, podendo escolher quais informações quer visualizar em detalhes.
Alguns tópicos que devem ficar explícitos:
B) Consultas:
O ambiente deve apoiar o usuário na construção de consultas complexas, tais como qual tarefa criou determinada tabela, qual foi a parametrização utilizada para gerar determinadas regras, em que situações foram utilizados um algoritmo ou uma técnica de análise de dados específica, entre outras.
O apoio eficiente a este tipo de consulta é uma das limitações no uso de OMSI, como ressaltado na Seção 3.2.
Considerando o uso da tecnologia de hipertexto para implementação do modelo de representação da informação, os aspectos de consulta e visualização acarretam dificuldades adicionais.
No caso de visualização, são necessários mecanismos para localização do usuário.
No caso de consulta, a não-estruturação do modelo dificulta a formulação e a representação das respostas.
O ambiente também deve ser o mais adaptável possível à política de organização do analista que desenvolver a aplicação, onde o nível de detalhe em que as tarefas serão documentadas depende muito do seu estilo, pois é ele o responsável em definir as tarefas, decompor- las e determinar as restrições de ordem na sua execução.
A documentação dos eventos que ocorrem durante a construção de uma aplicação, como as descobertas sobre o domínio dos dados, os problemas que surgem no decorrer de a execução de uma tarefa, a reexecução das tarefas, as justificativas acerca de as decisões de criação/ decomposição de tarefas, o conhecimento adquirido junto aos especialistas, etc., também é influenciado por o analista que está conduzindo o processo, pois provavelmente é ele quem irá definir o que é importante ficar registrado e, de que maneira (nível de granularidade das tarefas), baseado na sua visão pessoal do processo de KDD.
Além de a questão do nível de detalhe, podemos citar dois perfis diferentes de analista, no que diz respeito ao momento em o qual a documentação será realizada.
O primeiro é aquele que irá documentar o planejamento acerca de as tarefas a serem realizadas e, à medida em que elas são executadas, as informações relativas a cada execução, ou seja, o processo está sendo documentado na medida que vai sendo construído.
O segundo é aquele que irá documentar após a aplicação ter sido finalizada ou no momento em que sentir necessidade de organizar suas informações, porque já não se lembra mais de como executou dada tarefa, quais os parâmetros que utilizou, qual o significado de determinada amostra, etc..
O primeiro estilo de documentação provavelmente será mais rico em informações e o modelo irá auxiliar- lo na condução do processo de forma mais efetiva.
O segundo aprenderá de forma incremental a importância deste tipo de ferramenta e, espera- se que ao longo de o tempo, adote uma postura mais sistemática de documentação.
Portanto, deve ser levado em consideração que a construção de aplicações de KDD é bastante dependente da pessoa que a está realizando e um requisito essencial é que a política de trabalho e organização do usuário possa ser representada no ambiente de apoio da forma mais transparente possível, ou seja, ilustrar da maneira mais verdadeira possível, a estrutura de sua aplicação.
Como o processo de KDD geralmente é apoiado por um conjunto heterogêneo de ferramentas, a independência de ambiente é um requisito fundamental dentro de o contexto deste trabalho.
Um ambiente de apoio ao modelo proposto atrelado a um ambiente de KDD específico, além de limitar a implementação de parte dos requisitos levantados na Seção 5.2.1, faria com que algumas das limitações das propostas discutidas na Seção 3.1 persistissem.
Finalmente, pode- se considerar que a aplicação de KDD está sendo elaborada por uma equipe, devendo ser contempladas funcionalidades de apoio ao trabalho cooperativo, tais como a compartilhamento de recursos e a sincronização de tarefas.
Com base nos requisitos expostos na Seção 5.2.1 propõe- se o modelo de documentação para apoio ao processo de KDD, ilustrado na Figura 5.2 através de um diagrama de classes UML (Unified Modeling Language ou Linguagem de Modelagem Unificada).
O modelo pode ser dividido em três grandes blocos de informações que estão mais intimamente relacionadas.
O primeiro, chamado Aplicação contém informações referentes a uma aplicação de KDD, o seu domínio e dos dados, seus objetivos e a sua avaliação em relação a esses.
O segundo, referese às atividades desenvolvidas nas diferentes etapas que compreendem o processo de KDD, suas informações e seu comportamento;
E o terceiro aos complementos, ou seja, às informações adicionais à descrição das tarefas, suas execuções e à aplicação em geral.
Cada um destes blocos é descrito no restante desta seção.
Em o Anexo A é apresentado o dicionário de dados referente a o Diagrama da Figura 5.2.
Bloco Aplicação Por um lado existem informações que são fundamentais para definir e delimitar o escopo de uma aplicação, justificar o planejamento inicial das tarefas e, ajudar a obter uma melhor compreensão da aplicação, como a descrição do seu domínio e dos dados a serem utilizados, os seus objetivos, as saídas esperadas e os recursos disponíveis.
Por outro lado, existem informações que dizem respeito à avaliação de uma aplicação.
Estes dois tipos de informações foram representadas em duas classes diferentes, respectivamente Aplicação e Avaliação_ Aplicação, relacionadas por a associação Avaliação.
Estas duas classes contemplam os requisitos referentes às informações globais sobre a aplicação.
O raciocínio utilizado para o desenvolvimento do modelo foi pensar genericamente numa aplicação como um conjunto de tarefas que detalham as atividades realizadas durante sua construção.
Esta estrutura foi representada no modelo como a agregação Composição, que define o tipo de relação entre uma aplicação e as suas atividades (Classe Tarefa).
Esta agregação é detalhada na Seção 5.3.2.
Bloco Tarefa Existem basicamente três tópicos envolvidos com as tarefas, tal como descrito na Seção 5.2.1:·
Especificação: A classe Tarefa contempla as informações genéricas de uma tarefa, tais como objetivo, descrição dos procedimentos a serem adotados, data de criação, etapa de KDD que representa, etc..
Analisando os vários tipos de tarefas envolvidos num processo de KDD, as únicas que possuem características e informações um pouco diferenciadas são as tarefas da etapa de análise de dados.
Esta diferença está nas informações sobre as técnicas utilizadas, tipos de dados adotados e algoritmos.
Por este motivo, estas informações adicionais foram isoladas numa subclasse chamada Tarefa_ Análise.
Planejamento e comportamento das tarefas:
O comportamento de uma tarefa foi representado basicamente através de relacionamentos, cada qual com um papel que atenda os requisitos necessários, levando em consideração que as informações e o tratamento das tarefas devem ser os mesmos em todos níveis de decomposição.
Estes relacionamentos estão descritos a seguir:·
é pré-condição de/ tem como pré-condição:
Possibilita definir as restrições na ordem de execução das tarefas, onde uma tarefa pode saber quais são as tarefas que devem ser executadas antes, ou após, a sua execução;·
é decomposição de/ é decomposta em:
Relaciona tarefas que estão em níveis de detalhes diferentes, onde uma é a definição genérica de uma tarefa com um certo nível de complexidade e, as outras relacionadas a ela, correspondem ao detalhamento das várias atividades que a compõe, onde o nível de complexidade/ descrição de cada uma é considerado adequado;·
é versão de/ tem como versão:
Uma tarefa pode estar relacionada com sua especificação anterior, caso o analista queira guardar o histórico das tarefas que, de alguma forma, tiveram suas propriedades alteradas.
Seção 5.2.1, ou por ser uma especificação genérica para tarefas onde apenas a fonte de dados é alterada, ou por a necessidade de reexecução.
Portanto, as informações referentes a cada execução de uma tarefa devem estar separadas de sua especificação.
Para isso, foi criada a classe Execução_ Tarefa, que gerência as informações referentes às execuções.
A associação execução liga a especificação da tarefa (Tarefa) com as informações sobre suas execuções (Execução_ Tarefa).
Por as mesmas razões que os dados das tarefas de análise de dados foram separados dos outros tipos de tarefas, isolou- se também as propriedades relativas a sua execução, como os vários algoritmos e parâmetros testados.
A classe Execução_ Análise, subclasse de Execução_ Tarefa, cumpre este papel.
Está frisado no modelo, a necessidade da existência de uma Tarefa_ Análise como pré-condição para a existência de uma Execução_ Análise.
Tarefa e Execução_ Tarefa são subclasses.
Complementos Como descrito na Seção 5.2.
Existem várias informações que são complementares a todas as outras descritas acima, mais especificamente, às classes Aplicação, Tarefa e Execução_ Tarefa.
A classe abstrata Item de Documentação captura o que há de comum entre estas classes e as relaciona com as classes que foram construídas para atender os requisitos necessários para apoiar os complementos, a saber:
Recursos (fonte de dados e ferramentas), resultados e informações sobre os redirecionamentos e refinamentos no processo (justificativas e conhecimento).
As propriedades desta classe estão especificadas através de associações com outras classes, descritas a seguir.
No tocante a os requisitos recursos (ferramentas e fonte de dados) e resultados, destacados na Seção 5.2.1, as seguintes abstrações foram incluídas no modelo:·
Ferramenta: Esta classe representa a especificação das ferramentas utilizadas, como nome, descrição e versão.
Muitos analistas podem estar utilizando várias ferramentas numa mesma tarefa e talvez queiram frisar qual o recurso específico utilizado ou a ser utilizado, junto com o seu objetivo (e.
g Ferramenta:
Clementine, Recurso:
C 4.5, Objetivo:
Analisar atributos relevantes).
Eles podem fazer- lo através da associação Recurso.·
Arquivo: Apesar de estarem definidos separadamente, foram detectados dois tipos de arquivos, um chamado de fonte de dados, e outro que são os arquivos gerados por a execução das tarefas, ou seja, os resultados.
Estes dois itens foram genericamente denominados arquivos porque sempre que um resultado ou fonte de dados é manipulado no contexto de uma aplicação, é na forma de arquivo.
Adicionalmente, as informações sobre estes dois tipos de arquivos são iguais e, muitas vezes, um arquivo que é gerado por a execução de uma tarefa é utilizado como recurso para a execução de outra (s), alterando assim apenas seu papel.
Por isso, foi construída a classe Arquivo para manter as informações referentes ao nome, descrição, localização, data de criação e tipo dos arquivos (e.
g Saída Entrada, representando respectivamente resultados e fonte de dados.·
Definição: Esta classe visa a permitir que as base de dados e as tabelas geradas por as execuções de tarefas com o intuito de apoiar outras atividades dentro de o processo atual ou mesmo posterior, possam ser descritas detalhadamente.
Pode- se também relacionar uma tabela com a base de dados de a qual ela participa (Detalha) e, a base de dados com suas tabelas (É detalhada por).
Estes recursos também podem ser utilizados para a documentação de base de dados e/ ou tabelas utilizadas por uma aplicação que não têm uma documentação no seu sistema de origem, ou necessitam de complementação;
No tocante a o requisito de redirecionamentos e refinamentos no processo, as seguintes classes foram incorporadas no modelo:·
Conhecimento: Esta classe gerência a descrição de informações que podemos considerar conhecimento e que foram descobertas ao longo de o processo.
É importante não só registrar qual o conhecimento adquirido, mas também sua fonte, a data da descoberta e qual a sua implicação, ou seja, quais os efeitos desta descoberta na condução do processo;·
Justificativa: Descrição que tem por objetivo justificar qualquer ação adotada dentro de a aplicação, como por exemplo, a criação de uma nova tarefa ou uma versão de tarefa que, ou não estava planejada, ou que foi criada para resolver problemas encontrados.
A justificativa está representada no modelo como um auto-relacionamento.
O objetivo desta seção é exemplificar o uso do modelo apresentado na Seção 5.3, de forma a ressaltar sua expressividade, bem como sua flexibilidade e adaptabilidade ao perfil do usuário.
Para isso, serão considerados alguns aspectos simplificados dos estudos de casos apresentados no Capítulo 4 que são interesses para esta exposição.
Os exemplos serão demonstrados de forma gráfica por razões puramente ilustrativas.
A proposta de uma representação gráfica adequada ao modelo de documentação transcende o escopo deste trabalho.
A representação gráfica escolhida apresenta semelhanças com os diagramas de classe UML.
Não foram usados diagramas de objetos desta notação por a sua baixa expressividade e legibilidade.
Para representar as instâncias da classe do modelo serão utilizados retângulos, divididos em dois compartimentos:
Em o compartimento superior é identificado o nome da classe e, no inferior, são colocados os atributos, com seus respectivos valores, ficando implícita a existência dos valores para os outros atributos necessários a sua definição.
Os nomes de classes são representados em negrito, as associações em itálico e, a identificação dos atributos em sublinhado.
O primeiro exemplo (Figura 5.3) apresenta algumas situações da aplicação médica descritas na Seção 4.2, mapeadas para o modelo proposto.
A primeira situação que pode ser demonstrada é o uso da classe Aplicação para caracterizar a aplicação em questão e sua relação (composição) com as várias tarefas inicialmente definidas.
Uma restrição de ordem foi imposta entre estas tarefas através da associação é pré-condição de.
Como a tarefa &quot;Tratar atributos multivalorados «foi decomposta em várias outras, tarefas foram criadas para acomodar- las e, uma relação de hierarquia foi definida por a associação é decomposta em.
A fonte de dados a ser utilizada em todas as tarefas de granularidade menor é a mesma (TSUM_ B) e, portanto, esta foi relacionada à tarefa de granularidade maior.
Tarefa Id: Criar Tabela saída Definição especificação execução Arquivo Nome:
A Figura 5.5 ilustra o detalhamento de uma tarefa da aplicação de análise das revisões curriculares.
É possível verificar, que foi feita a opção por relacionar as ferramentas com sua tarefa específica.
Elas também poderiam estar relacionadas a cada uma de suas decomposições, caso o usuário queira um nível de detalhe maior.
Em este exemplo, novamente podem ser vistas situações de decomposição de tarefas e definição das restrições de ordem em suas execuções.
No entanto, é interessante ressaltar a representação de algumas situações, tais como:
Avaliação. Considerando curso não estável, o resultado está dentro de o esperado.
Resultado: Número de Revisões.
Avaliação. Considerando curso estável o número de revisões está muito alta.
A) as várias execuções da mesma tarefa.
Isto acontece com a tarefa «Criar tabela turma, que é executada duas vezes, produzindo dois arquivos distintos.
Também é o caso de &quot;Validar tabelas TURMA», que é executada uma vez para cada arquivo acima mencionado;
B) o mesmo arquivo desempenhando papéis distintos em atividades/ execução de atividades diferentes.
Por exemplo, o arquivo Turma_ 103 é saída de Execução_ Tarefa, ligada à &quot;Criar tabela turma «e, também entrada para a Execução_ Tarefa ligada à &quot;Validar tabelas turma&quot;;
C) a representação de uma restrição de ordem entre tarefas de níveis distintos, como é o caso da tarefa &quot;Criar Tabela IMPACTO», precisa que as tabelas Turmas geradas estejam validadas para sua execução.
Tomando- se por base este mesmo exemplo, podemos representar através do uso do modelo, o conhecimento adquirido e as tarefas acrescentadas ao longo de o processo e justificar sua criação, tal como demonstrado na figura 5.6 e, detalhado abaixo:
A) registrar o conhecimento sobre o domínio adquirido através da execução da tarefa &quot;Validar Tabelas TURMA», para o arquivo Turma_ 103;
b) justificar a necessidade de criação de uma nova tarefa (&quot;Verificar NRev X Curso&quot;) e, a sua relação com o evento que a gerou;
C) registrar o conhecimento adquirido na execução desta nova tarefa e, que implicou a troca dos cursos definidos anteriormente.
Isto por sua vez vai gerar uma reexecução em todas as outras tarefas realizadas até o momento, apesar de não estar demostrado graficamente;
Outro exemplo importante de ser mostrado é que o usuário não precisa, necessariamente, decompor uma tarefa para fins de documentação;
No entanto, sua especificação deve agregar o máximo possível das informações distribuídas em cada uma das atividades.
Um exemplo para a especificação da tarefa &quot;Tratar Atributos Multivalorados «em ambos os casos é demonstrada na Figura 5.7.
Um tópico ainda não endereçado é a questão das versões.
O uso desse tipo de recurso é completamente dependente da necessidade do analista em registrar as especificações anteriores para uma tarefa.
Novamente a tarefa &quot;Tratar Atributos Multivalorados «pode ser utilizada para exemplificar esta situação, pois após a execução das tarefas previamente definidas na decomposição verificou- se que eram necessários alguns procedimentos para validar a tabela gerada.
Essa nova percepção da tarefa &quot;Tratar atributos Multivalorados», faz com que os requisitos para tratar os atributos multivalorados sejam alterados.
Em este caso, se o analista achar interessante guardar a versão anterior da definição da tarefa, ele deverá criar uma versão para esta tarefa, descrever todos os requisitos necessários para sua execução e adicionar as tarefas que são pertinentes à nova versão, caso elas existam (neste caso existem tarefas adicionais que precisam ser executadas) e/ ou, referenciar as tarefas definidas anteriormente e que continuam a fazer parte da nova versão.
Caso o analista não ache necessário documentar esta mudança de requisitos, ele poderá simplesmente adicionar uma nova tarefa àquelas definidas anteriormente, justificando com os comentários sobre os motivos da sua criação.
Em a Figura 5.8 estão apresentadas estas duas visões.
Este caso não só exemplifica a questão do registro do histórico das tarefas como, juntamente com os exemplos anteriores, algumas das várias alternativas de representação que o analista dispõe para a mesma situação, o que demonstra a adaptabilidade do modelo à situação em questão e ao estilo de documentação do analista.
Documentação na Seção 5.2.2 foram apresentados os requisitos de um ambiente de apoio ao modelo de documentação proposto neste trabalho.
Entretanto, o desenvolvimento de um ambiente completo está fora de o escopo desta dissertação, por o grau de complexidade envolvido.
Assim, neste capítulo, são apresentadas as principais funcionalidades de um protótipo, para apoiar o uso do modelo.
Através do protótipo, deseja- se ilustrar aspectos importantes como:
Captura e representação dos dados;
Não foram abordadas questões relativas a consultas complexas, recursos avançados de navegação e localização do usuário, nem trabalho em equipe.
No tocante a o modelo de documentação, foram desprezadas as funcionalidades relativas ao versionamento de tarefas, bem como procedimentos de manutenção e consistência avançados (e.
g a tentativa de criar uma execução para uma tarefa que tenha outra, não executada, como pré-condição).
O protótipo, atualmente em fase de implementação, está sendo construído em ambiente Www (World Wide Web), através de recursos de Html (HyperText Markup Language) e Javascript e, da plataforma Top (Ten Objects Plataform) utilizada para apoio à gestão de Memória Organizacional.
A utilização destas tecnologias permitiu contemplar o requisito de independência de ferramentas e, além disso, facilitar a posterior agregação de recursos para trabalho em grupo.
O sistema está organizado em duas áreas de trabalho.
A primeira, representada através de um frame à esquerda, permite criar e acessar aplicações, ferramentas e arquivos.
A segunda, disponível no frame à direita, permite ações relativas a esses três tipos de objetos, tais como criação e visualização.
Através das operações oferecidas no frame à esquerda, o usuário inicia sua interação.
As descrições de ferramentas e arquivos, por a possibilidade de serem compartilhados em várias aplicações, podem ser criadas visualizadas independentemente de uma aplicação.
Já a criação e visualização de instâncias para as demais classes estão condicionadas à criação de uma aplicação de KDD, ou a seleção de uma aplicação preexistente.
Quando o usuário estiver no contexto de uma aplicação, a área de trabalho do objeto selecionado oferece as funcionalidades pertinentes a ele, estando estas, sempre disponíveis na parte inferior do frame.
Para demonstrar as diversas funcionalidades do ambiente, será adotado como cenário algumas situações ilustradas na Figura 5.2.
Em algumas situações, o grande número de informações impossibilitaria a visualização dos recursos disponíveis na área de trabalho em questão, o que dificultaria o entendimento da estrutura e funcionamento do ambiente, por este motivo, quando necessário, alguns atributos, descritos no diagrama de classes, foram suprimidos dos frames, ficando implícita a sua existência.
Criação de Ferramentas e Arquivos Como já mencionado, descrições de ferramentas e arquivos podem ser compartilhados nas diversas aplicações.
Por isso, distingue- se entre a criação destes recursos e a sua associação a uma ferramenta específica.
Tendo o estudo de caso como exemplo, a Figura 6.2 apresenta a instanciação para arquivo e, a Figura serem feitas:
A importância da definição do seu tipo e a possibilidade de adicionar uma descrição mais detalhada (Definição) sobre o arquivo.
Criação de uma Aplicação A instanciação de todos os demais objetos é realizada no contexto de uma aplicação.
Desta forma, é necessário criar uma nova aplicação ou selecionar entre as já existentes.
Quando de a criação de uma aplicação, a área de trabalho oferece todas as operações disponíveis neste contexto.
Isto pode ser observado na Figura adicionar à Aplicação:
Avaliação, Tarefa, Conhecimento ou Justificativa;
A Figura 6.5 mostra como uma ferramenta é relacionada a uma aplicação.
Em este caso, a ferramenta já deve ter sido criada e, neste momento, são apenas informados (opcionalmente) os recursos e objetivos ligados ao seu uso no objeto corrente.
O mesmo exemplo é aplicável para o contexto de tarefas e de execuções de tarefas, o diferencial está no caminho ao qual se chegou até este frame.
Criação de Tarefas A criação de tarefas pode se dar a partir de a área de trabalho de uma aplicação, da área de visualização, ou mesmo por a ação de limpar uma tarefa recém criada.
Quando o usuário optar por adicionar uma tarefa, primeiramente deverá informar qual a etapa de KDD que a tarefa se encontra, isto para fins de gerência das informações adicionais necessárias às tarefas da etapa de análise de dados.
Algumas particularidades, genéricas a todos os tipos de tarefa, são interessantes de serem frisadas (Figura 6.6):
Para estes três casos, uma lista de tarefas existentes é apresentada, devendo portanto, estarem previamente instanciadas.
Aqui se repetem as opções disponíveis na aplicação, no entanto, a avaliação é substituída por a possibilidade de acrescentar execuções à tarefa.
A Figura 6.7 ilustra como é feita a relação de arquivos com uma tarefa e, tal como para ferramentas, esta mesma situação pode se repetir para aplicações e execuções.
Até este ponto, foi exemplificado como arquivos e ferramentas podem ser relacionados a aplicação, tarefa e execução.
No que diz respeito à adição de conhecimento e justificativa, o processo é exatamente o mesmo, no entanto, como são particulares a cada objeto, estes são adicionados diretamente, não precisando estarem previamente instanciados.
Criação de Execução para uma Tarefa A partir de a criação ou visualização de uma tarefa, pode- se criar suas execuções.
Em esta situação, o usuário também tem a possibilidade de adicionar conhecimento e justificativas, bem como relacionar arquivos e tarefas, tal como definido no modelo.
Sempre que uma execução é adicionada, o nome da sua tarefa correspondente aparece na parte superior do frame, facilitando a localização do usuário.
O mesmo pode ser percebido nas outras ilustrações deste capítulo.
Visualização e Manutenção de Objetos Motivados por o grande número de informações relacionadas à aplicação, tarefas e execuções, alguns requisitos foram levantados na Seção 5.2.2.
Baseado nisto, optou- se por criar uma área de visualização e navegação, em a qual ícones representam a existência de determinados tipos de informações agregadas ao item (aplicação ou tarefa), bem como a situação de cada tarefa (e.
g A Figura 6.9 apresenta um exemplo de área de trabalho para navegação na aplicação, descrita na Seção 4.2, que reflete sua situação como um todo e, das suas tarefas individualmente.
O usuário pode chegar a esta área por dois meios, no momento de abrir uma aplicação, ou através da opção &quot;Área de Visualização», disponível em algumas áreas de trabalho.
Como muitas das tarefas são decompostas, adotou- se uma estrutura hierárquica para visualização, facilitando, desta forma, a visualização da sua estrutura e do processo como um todo.
O papel da identificação da Aplicação e de suas tarefas, bem como das informações agregadas a elas, representadas através dos ícones, não são puramente ilustrativos, mas são os meios por os quais a navegação por a aplicação é possível.
Existem duas funcionalidades principais disponíveis neste contexto:
A) visualizar individualmente as propriedades relativas ao significado do ícone, com relação a a aplicação ou à tarefa em a qual ele se encontra agregado;
B) visualizar as propriedades da aplicação e de cada tarefa através de sua identificação.
Quando de a visualização das propriedades da tarefa ou da aplicação, os ícones agregados a elas são trazidos junto, mantendo- se o mesmo recurso de visualização individual.
A Figura 6.10 apresenta a visualização das propriedades de uma tarefa, sendo que, o mesmo acontece para a aplicação.
Portanto, a visualização de qualquer objeto dentro de o ambiente dará- se- ou através da identificação (aplicação e tarefa), ou do ícone correspondente.
A única situação um pouco diferente é a execução de uma tarefa, que apesar de estar representada como um ícone, também possui alguns relacionamentos (ferramentas, arquivos, conhecimento e justificativa).
Em este caso, quando de a visualização de suas propriedades, os ícones referentes aos objetos relacionados a ela estarão disponíveis e, então, poderão ser visualizados individualmente.
Este tipo de abordagem facilita o gerenciamento do grande número de informações manipuladas no contexto de uma aplicação, sem que a estrutura e semântica do processo seja perdida, pois o usuário tem uma visão global do estado de sua aplicação, com a possibilidade de visualizar os detalhes quando necessário.
Além disso, a utilização de ícones é bem mais intuitiva que a descrição textual dos relacionamentos, inviável de ser adotada neste caso.
Outro fator importante, é que o transporte dos ícones para o contexto de visualização detalhada da aplicação, tarefa e execução, faz com que o usuário não perca a direção dentro de o ambiente e possa identificar os objetos relacionados a ele sem necessidade de voltar à área de visualização.
Outro fator importante, é que o usuário sempre sabe a quem o objeto visualizado está relacionado.
O tratamento individual de cada objeto para fins de visualização facilita também a sua alteração e exclusão e, portanto, estas funcionalidades sempre estão disponíveis nesta situação, ou seja, no momento que o usuário optar por visualizar um objeto ele pode alterar- lo ou excluir- lo.
A Figura 6.11 apresenta um exemplo de visualização do arquivo TSUM_ B, onde a sua relação com a aplicação pode ser, além de visualizada, excluída.
Arquivo apresenta uma situação especial, pois não apresenta informações adicionais a sua identificação agregada ao relacionamento, como é o caso dos outros objeto.
Sendo assim, a única possibilidade nesta situação é excluir a relação.
A Figura 6.12 ilustra outra situação, tomando como exemplo a visualização de conhecimento relacionado à execução da tarefa &quot;Validar Tabela TURMA «que, para enriquecer a exemplificação, foi trazido da aplicação das revisões curriculares (Figura 5.5).
Em estes casos, o usuário pode:
As funcionalidades apresentadas neste exemplo podem ser estendidas para os outros tipos de objetos presentes no ambiente.
Uma única particularidade é o caso específico de tarefas, onde a questão relativa às versões pode ser tratada também neste momento.
Para isso, o usuário pode optar por alterar as informações ou criar uma nova tarefa, guardando assim, as suas especificações anteriores, tal como pode ser visto na Figura 6.10.
Uma última consideração a ser feita é que no momento da visualização de aplicação, tarefa e execução, os mesmos recursos para adição de novos itens estão disponíveis, tal como no momento de sua criação.
A realização deste trabalho envolveu o estudo de algumas das principais abordagens para o processo de KDD, enfatizando principalmente as suas características, bem como os fatores e dificuldades envolvidos em sua aplicação.
Estes conceitos foram consolidados e validados através do estudo aprofundado de uma aplicação de KDD para Análise de Revisões Curriculares realizada por outrem e, a construção de uma nova aplicação por a autora.
As características diferenciadas destes dois estudos de caso possibilitaram uma visualização mais completa dos problemas ressaltados na literatura.
Em linhas gerais, pôde- se perceber:
Portanto, este usuário precisa gerenciar um número muito grande de variáveis, o que é praticamente impossível de realizar, de maneira adequada, sem o uso de uma metodologia.
Isto deixa claro a necessidade de apoio a sua tarefa.
Algumas de suas necessidades de apoio estão disponíveis nas ferramentas atuais de KDD, onde ele pode contar com ambientes que atendem diversas etapas do processo.
Contudo, um levantamento das funcionalidades de algumas ferramentas mostrou que a grande maioria disponibiliza técnicas para preparação, análise e visualização dos dados.
Entretanto, pouca ou nenhuma atenção é dada aos recursos que auxiliam o usuário a gerenciar a complexidade de sua tarefa.
A análise de alguns trabalhos nesta direção demonstrou que:
Um dos tópicos em aberto diz respeito aos recursos orientados à documentação do processo de forma independente de ambiente e, que não registrem apenas as informações e recursos utilizados, mas também as iterações, experiências e o conhecimento informal manipulados ao longo de o processo.
O modelo de documentação proposto foi desenvolvido motivado não só por a inexistência de um ambiente com este propósito, mas também por as contribuições que esta abordagem pode trazer ao trabalho do analista, disponibilizando mecanismos que o auxiliem a:
A tecnologia de OM, devido a sua liberdade de representação, contribuiu bastante para que as exigências de um modelo que atendesse a todos estes requisitos pudesse ser construído.
Outra contribuição importante é a definição de um ambiente de apoio à utilização deste modelo, que por utilizar hipertexto, pode ser visto como agente integrador para os vários ambientes de KDD existentes.
Em o momento, o protótipo está sendo desenvolvido e, posteriormente, algumas questões deixadas em aberto podem ser incorporadas, entre elas versões, consultas complexas e trabalho em grupo.
Como trabalhos futuros, destaca- se a utilização do protótipo por aplicações distintas às citadas neste trabalho, bem como por outros perfis de usuários, por sua importância na consolidação do modelo.
Outra perspectiva é a definição de uma sistemática ou metodologia de documentação, a qual talvez exija a revisão de alguns aspectos do modelo, em particular devido a o grau de liberdade que hoje o mesmo oferece ao analista.
Outro tópico que pode ser abordado no futuro é a inclusão no modelo de aspectos desconsiderados neste trabalho, tais como o conceito de ator, no caso de aplicações desenvolvidas por um grupo e, a agregação de uma dimensão de agendamento de tarefas.
No entanto, este é um aspecto complexo devido a natureza incremental e, muitas vezes, exploratória do processo.
Finalmente, as experiências coletadas e o uso efetivo do modelo podem servir de subsídios para a integração de mecanismos de recomendação (e.
g Sistemas Baseados em Conhecimento) e a estruturação das experiências coletadas na forma de conhecimento, o que pode ser útil para a construção de aplicações que não estão voltadas ao uso de técnicas e aplicações específicas, mas são bastante dependentes de experiência e conhecimento.
