O maior problema na manipulação de grandes cadeias de Markov é a explosão do espaço de estados.
O formalismo SAN (Stochastic Automata Networks) propõe uma altemativa para descrever modelos Markovianos num formato mais compacto e eficiente, utilizando uma estrutura de armazenamento Baseada em álgebra tensorial chamada de Descritor Markoviano ou simplesmente Descritor.
Um dos avanços na area de métodos numéricos foi o algoritmo Shuffle para execução eficiente da multiplicação vetor-descritor.
Um novo método para realizar a multiplicação vetor descritor foi proposto, e em muitos casos reduz o custo computacional.
As vantagens, limitações e o impacto na area são discutidos através de reais modelos SAN.
A multiplicação de um vetor de probabilidades (pi) por o gerador infinitesimal Q (denominada Multiplicação Vetor--Descritor), é uma das operações fundamentais realizadas por os métodos iterativos.
O objetivo principal deste trabalho e investigar alguns fatores relevantes para a otimização da multiplicação vetor-descritor, no intuito de melhorar o desempenho deste procedimento, tirando proveito da forma compacta do descritor SAN armazenado.
É importante salientar que a busca por otimizações nas multiplicações mantera o tempo computacional gasto na geração do descritor, bem como os custos de memória até hoje constatados.
Logo, a proposta é estudar o método de solução baseado na multiplicação vetor-descritor, buscando formas de acelerar esta multiplicação por um vetor de probabilidades, ou seja reduzir o seu custo computacional (número de multiplicações realizadas).
O estudo de um novo método, chamado de Método do Fatiamento (ou Slice), sob o aspecto de eficiência computacional, e a experimentação com problemas reais, torna- se essencial para atestar sua aplicabilidade.
Além de isto, esta pesquisa tem como objetivo fornecer subsídios teóricos para futuros trabalhos, tanto na area de avaliação de desempenho, como em aplicações de outras areas.
Este trabalho foi dividido em oito capítulos incluindo introdução e conclusão.
Os quatro primeiros exceto esta introdução, descrevem as bases teóricas anteriores a esta dissertação.
Em o Capítulo 2 e feita uma conceituação do formalismo SAN, explicitando suas caracteristicas para modelagem e solução de problemas da realidade, ressaltando as formas de representação disponíveis utilizando noções de sincronismo e paralelismo.
Em o Capítulo 3, uma revisão de álgebra tensorial clássica e generalizada é feita abordando algumas de suas principais propriedades.
Em o SAN (baseado na soma de produtos tensoriais), formalmente chamado de Descritor Markoviano.
Em o Capítulo 5 e apresentado o procedimento de multiplicação vetor-descritor, destacando seus algoritmos e etapas envolvidas nesta multiplicação, bem como o custo computacional verificado na execução do mesmo.
Nos demais capítulos estão as principais contribuições originais obtidas ao longo de esta dissertação.
Em o Capítulo 6 apresenta- se um estudo relativo a possibilidades não efetivas de otimização do procedimento de multiplicação vetor-descritor destacando suas vantagens e os fatores lin1itantes que levaram cada abordagem estudada a ser descartada.
Em o Capítulo 7, uma nova técnica de decomposição em fatores normais é introduzida no âmbito da multiplicação vetor-descritor, destacando as vantagens e particularidades desta abordagem, bem como sera mostrada uma comparação entre os custos computacionais verificados.
Finalmente, na conclusão são tecidas as considerações finais a respeito de a altemativa de otimização proposta bem como os resultados obtidos e trabalhos futuros relacionados.
O formalismo de Redes de Automatos Estocásticos (SAN) é baseado em Cadeias de Markov e tem propiciado, desde o seu surgimento nos anos 80 com Plateau, uma forma mais compacta e modular para a descrição de sistemas Complexos e com grandes espaços de estados a serem modelados.
Logo, este formalismo é capaz de manter o poder de modelagem que se tinha com a Irtilização de Cadeias de Markov, porém propõe um novo formato para a mesma, cujo principal objetivo e eliminar problemas como o da explosão do espaço de estados.
E interessante ressaltar que toda SAN pode ser representada por um único autômato estocastico que contém todos os estados possíveis do sistema.
Esse único autômato corresponde a Cadeia de Markov equivalente ao modelo SAN.
O princípio do formalismo SAN é a possibilidade de descrever um sistema Complexo dividindoo em subsistemas quase independentes, pois estes podem interagir ocasionalmente.
Em este sistema, cada subsistema é modelado por um autômato, ou seja, por um conjunto finito de estados e por um conjunto finito de transições entre estes estados, A denominação de estocasticos atribuída aos autômatos neste formalismo, deve- se ao fato de que o tempo é tratado como uma variável aleatória, que na escala de tempo contínua obedece uma distribuição exponencial.
Em as próximas seções serão expostos alguns conceitos relacionados as SAN onde poderemos observar corno esta abordagem, dita modular, permite também descrever primitivas de paralelismo e sincronismo, possibilitando a descrição de sistemas atualmente ditos complexos.
O formalismo SAN irtiliza- se das noções de estado e de transições entre estados para representação dos eventos, assim como as Cadeias de Markov.
Porém estes eventos podem estar relacionados a um único autômato, ou a varios ao mesmo tempo, ocorrendo de acordo com taxas específicas.
O estado individual de cada autômato e chamado de estado local.
Ja o estado global de uma SAN é definido como a combinação de todos os estados locais de cada autômato componente da SAN Existem duas possibilidades de representar eventos que são disparados em mais de um autômato, ou que dependem do estado de outros autômatos para realizar uma determinada transição:
Taxas funcionais e eventos sincronizantes como veremos a seguir.
As ligações existentes entre os estados são chamadas de transições, e estas podem ser de diferentes tipos.
Vejamos um exemplo de rede de autômatos estocasticos que modela um sistema através de dois autômatos:
O autômato A (m) com três estados (D, A e S) e 0 autômato (a) com dois estados (I e O).
Observando 0 modelo, pode- se notar diferentes tipos de transições entre os estados.
Estas transições existentes em cada autômato podem ser divididas em transições locais e transições sincronizadas.
As transições apresentam eventos associados para que posSAN ocorrer.
Logo eventos locais e sincronizantes estão associados a estas.
Em o exemplo, os eventos locais recebem a denominação Z¡ e os sincronizantes são chamados ei.
As transições locais alteram o estado global por a mudança de um estado em apenas um autômato, enquanto as transições sincronizadas alteram o estado global da SAN por a mudança de estado em dois ou mais autômatos simultaneamente.
A vantagem das transições locais e permitir que os autômatos tenham comportamentos paralelos através de eventos locais, ou seja, elas mudam somente o estado local do autômato em que ocorreram e não têm interferências por parte de os estados dos outros autômatos.
Apesar de os eventos serem ditos independentes, eles são disparados um de cada vez, pois numa escala de tempo contínua não ocorrem dois ou mais eventos ao mesmo tempo.
As transições sincronizadas permitem que seja representado um certo sincronismo no disparo de transições entre os autômatos, constituindo eventos sincronizantes entre os mesmos.
Sua ocorrência se da simultaneamente em todos os autômatos envolvidos, pressupondo a existência de um autômato mestre, que coordenara a sincronização.
Tem- se então um ou mais autômatos escravos, estes disparados por o mestre correspondente.
Para cada evento sincronizante têmse a definição de um único autômato mestre, sendo as transições, nos demais autômatos, do tipo escravo.
Cabe salientar, que um autômato pode ser mestre de um determinado evento sincronizante e, ao mesmo tempo, escravo em relação a outro.
O conceito de mestre/ escravo facilita a compreensão dos modelos quanto a a sincronização entre os autômatos, mas não ha a necessidade de definir- los na modelagem.
O autômato representado na Figura 2.2, tem cada um dos seus estados como uma dupla de estados locais de cada autômato da Figura 2.1.
A configuração deste sistema a cada instante definira seu estado global.
De a mesma forma, levando em consideração a SAN da Figura 2.1, o estado global é dado por a combinação de todos os estados locais de cada um dos dois autômatos constituintes da mesma.
Os eventos sincronizantes representam as possíveis interações existentes entre os autômatos, cujas taxas de ocorrência são, muitas vezes, constantes.
Porém, existe outra forma de representar estas interações:
Utilizando taxas e probabilidades funcionais.
Um outro tipo de denominação para as transições locais e sincronizadas é que ambas podem ser chamadas de transições funcionais.
Isto oçorrera quando suas taxas não forem constantes, ou seja, tem- se uma função do estado local de outros autômatos da SAN, avaliada conforme os estados atuais do modelo.
Logo, as taxas funcionais podem ser colocadas tanto em transições locais como em transições sincronizadas (no autômato mestre), e estas podem ser definidas por funções que refletem a avaliação dos estados atuais da rede de autômatos estocásticos.
Por exemplo, se no autômato A (a) da Figura 2.1 quisessemos representar uma transição local funcional (dependente do estado intemo do autômato A (m), teríamos, por exemplo, a definição de uma função f como segue abaixo:
A1 se A (m) está no estado A, 0 se A (m) não esta no estado A;
Outra possibilidade dentro de o formalismo e que para cada modelo pode- se definir uma função de atingibilidade, que em poucas palavras, é uma função booleana que determina os estados atingiveis do modelo dentro de o espaço total de estados.
Esta função definirá, então, o espaço de estados atingível do modelo SAN.
Quando esta for igual a 1, significa que todos os estados do modelo são atingíveis.
Normalmente, isto não é o que acontece na pratica, pois em exemplos da realidade facilmente identificamos condições para que determinados eventos ocorram ou não, de entre estas o estado de outros autômatos.
Isto quer dizer que provavelmente quando estas condições não forem cumpridas alguns estados globais terão probabilidade nula ou quase nula de ocorrerem.
Para o exemplo mostrado na Figura 2.1 podemos supor que quando o A0) estiver no estado A, o autômato A (m) deve estar no estado I, ou se o A (m) estiver no estado S, o autômato A (m) deve estar no estado O, por exemplo.
Para isto é definida a seguinte função de atingibilidade:
Da mesma maneira que se define uma função de atingibilidade ou mesmo probabilidades funcionais, as funções de integração podem ser definidas.
Estas funções são definidas para obtenção de resultados numéricos sobre o modelo SAN, avaliando qual a probabilidade do modelo SAN encontrar- se em determinado estado.
As funções de integração podem ser definidas de forma a avaliarem a probabilidade do modelo estar num conjunto de estados considerando um vetor de probabilidades que contem as probabilidades do modelo encontrar- se em cada estado pertencente a ele, obtendo- se assim índices de desempenho e confiabilidade do modelo.
Por exemplo, na função de integração abaixo descobre- se a probabilidade do autômato Au) estar no estado A:
Como pode- se notar, todas as funções são modeladas da mesma forma no formalismo SAN, apesar de serem empregadas diferentemente.
De entre os métodos iterativos aplicados ã resolução das SAN pode- se destacar:
O método de Arnoldi, GMRES e o método da Potência implementados na ferramenta PEPS Em estes métodos, a operação básica e a multiplicação de um vetor de probabilidades por uma matriz, ou seja, a cada iteração é gerada uma seqüência um de valores aproximados do vetor de probabilidades estacionárias que devem convergir para a solução (pi).
Logo o número de iterações torna- se um fator relevante na verificação do custo total de aplicação destes métodos.
Em este trabalho o foco de estudo sera a busca de altemativas de otimização para o Metodo da Potência em especial, visto que eventualmente as melhorias poderão ser também aplicadas aos demais métodos iterativos citados.
A otimização deste método, no caso específico das SAN, pode em teoria contribuir com a redução das etapas realizadas na multiplicação vetor-descritor.
Cabe salientar que os conceitos vistos nesta seção podem ser encontrados em.
O princípio básico deste método é, através da multiplicação de um vetor de probabilidades por uma matriz de probabilidades P, obter o vetor solução ou auto-vetor (pi), isto se considerarmos que a matriz P estará elevada no infinito, ou seja, (pi) $= (pi0) vezes (P ao mfinito).
Normalmente, as soluções dos métodos iterativos, como é o caso do método da Potência, são obtidas através de aproximações constantes, onde uma solução é considerada satisfatória somente se obedece a algum critério de tolerância estabelecido.
Este critério pode ser observado através da diferença entre as iterações anteriores e a atual iteração.
Desta forma, a solução esperada e (pi na N), e para sabermos seu valor e necessário conhecer o valor de, e assim sucessivamente, como a seguir:
Portanto, para o cálculo do vetor de probabilidades estacionárias N (m), considera- se o esquema iterativo descrito genericamente por (pi na N):
Vezes P. No caso de as cadeias de Markov o elemento j de (pi na K) é igual à probabilidade do processo estar no estado j no k--ésimo passo de iterações.
Em o âmbito da multiplicação vetor-descritor necessária para solução de modelos SAN, considerase P uma dada matriz de transição Q, que deve estar normalizada (aqui representada como Q/ delta, somada a uma matriz identidade I. Além de isto, pode ser um vetor de probabilidades inicialmente equiprovavel, ou um vetor inicializado com probabilidades pré-definidas segundo algum criterio.
A solução que buscamos neste caso é um vetor de probabilidades normalizado, onde a soma de seus elementos é igual a 1.
Logo, o sistema linear a resolver é:
Considerando que um descritor SAN é um gerador infinitesimal dado por (Q/ delta+ I), a solução estacionária de uma SAN é simplesmente a resolução deste sistema linear.
Em este caso, com matrizes de transição, realiza- se o seguinte esquema de iterações:
Em este esquema de iterações demonstrado na Equação 2.1, tem- se o termo (Ê+ I) que representa uma matriz de probabilidade (denominada matriz P anteriormente).
Logo, o esquema iterativo pode ser representado por:
A única operação em que as matrizes estão envolvidas é na multiplicação por um ou mais vetores.
Estas operações não alteram a forma da matriz e do seu armazenamento compacto, o que minimiza os requisitos de memória para armazenamento da matriz, sendo assim ideal para realizar a multiplicação.
Considerando que no caso de a resolução das SAN as matrizes envolvidas são usualmente grandes e esparsas, o espaço requerido em memória normalmente e bastante considerável.
Em o proximo capítulo é feita uma revisão das propriedades da álgebra tensorial, tendo em vista que formalmente, as SAN são descritas através de álgebra tensorial clássica e generalizada utilizando- se da definição de dois operadores matriciais básicos:
Soma e produto tensoriais.
Estes operadores são a base da descrição formal das redes de autômatos estocasticos.
São apresentados a seguir os conceitos de Álgebra Tensorial Clássica e de Álgebra Tensorial Generalizada, necessários para o entendimento das SAN, A primeira seção introduz os conceitos de Álgebra Tensorial Clássica (ATC) e cita suas principais propriedades.
Em a segunda seção introduz- se a Álgebra Tensorial Generalizada (ATC) citando as propriedades relevantes para o formalismo SAN.
Todos os conceitos introduzidos neste capítulo baseiam- se no estudo contido em.
A Álgebra Tensorial Clássica e definida por dois operadores matriciais:
Produto tensorial x (também chamado de Produto de Kronecker);
Soma tensorial+.
Sejam: N conjunto dos números naturais;
JR conjunto dos números reais;
Subconjunto de N que contem todos os valores da ate b (a e b incluídos);
Subconjunto de R que contem todos os valores da até b (a e b incluídos);
Subconjunto de JR que contém todos os valores da até b (a excluído, b incluído).
A notação utilizada na definição dos operadores da ATC e de suas propriedades é introduzida a medida em que se tornar necessária.
O produto tensorial de duas matrizes A e B, de dimensões e, respectivamente, é uma matriz de dimensões.
Essa matriz pode ser vista como uma matriz constituída de a1 x a2 blocos, cada um de dimensào B1 x B2.
A definição de cada um dos elementos da matriz resultante e feita levando- se em conta a qual bloco o referido elemento pertence e a sua posição intema dentro desse bloco.
Sejam: A matriz A;
A x B produto tensorial das matrizes A e B;
A x B produto (convencional) das matrizes A e B;
Sejam, por exemplo, as duas matrizes A e B:
An an 1711 1712 1713 1714 O produto tensorial definido por C:
A x B é igual a:
Em este exemplo, o elemento C53 encontra- se dentro de o bloco e sua posição intema neste bloco é (2,3).
O produto tensorial C $= A x B é definido algébricamente por a atribuição do valor (AijBkl) ao elemento de posição (16,1) do bloco d, j) 7 ie:
Essa representação dos elementos da matriz correspondente ao produto tensorial induz uma relação de ordem sobre os elementos chkmn, que é a ordem lexicogrãfica das duplas de índice Fator Normal O produto tensorial de urna matriz quadrada por uma matriz identidade e um caso particular do produto tensorial, denominado fator normal.
Com uma matriz quadrada A e uma matriz identidade m (de dimensão n), dois fatores normais são possiveis: (
A x m) e (m x Á) sendo n igual ao número de linhas e colunas da matriz.
Sejam a matriz A do exemplo anterior e uma matriz identidade de dimensão 3: Os fatores normais A x[ 3 e Ig x A são:
O produto tensorial de duas matrizes identidade é um outro caso particular do produto tensorial.
O resultado e urna matriz identidade cuja dimensão e igual ao produto das dimensões das duas matrizes, i.
e: A sonia tensorial e definida somente para matrizes quadradas, ao contrário de o produto tensoria, que e definido para quaisquer matrizes.
Sejam: A~ B soma (convencional) das matrizes A e B;
A soma tensorial de duas matrizes quadradas A e B e definida como a soma (convencional) dos fatores normais das duas matrizes segundo a fórmula:
Sejam por exemplo as matrizes A e B dadas por:
O operador produto tensorial tem prioridade sobre o operador soma tensorial(+) e os dois operadores tensoriais tem prioridade sobre os operadores tradicionais de multiplicação e adição de matrizes (x e+).
As propriedades da ATC de interesse para as SAN são listadas a seguir.
Suas demonstrações podem ser vistas em Associatividade da soma e do produto tensorial:
Distributividade com relação a soma donvencional:
Compatibilidade com a multiplicação convencional:
Compatibilidade com a transposição de matrizes:
Compatilbilitlade com a inversão de matrizes (se A e B são matrizes inversíveis):
Decomposição em fatores normais:
Distributividade com relação a multiplicação por a matriz identidade:
Comutatividade dos fatores normais:
A álgebra tensorial generalizada é uma extensão da álgebra tensorial clássica, e tem como principal objetivo permitir a utilização de objetos que são funções discretas sobre linhas de uma matriz, trabalhando- se com um ou mais elementos passíveis de avaliações diferentes, ou seja, tem- se uma matriz que pode ter diferentes instâncias.
A diferença fundamental da ATG com relação a a ATC e a introdução do conceito de elementos funcionais.
Entretanto, uma matriz pode ser composta de elementos constantes (pertencentes a R) ou de elementos funcionais.
Um elemento funcional e uma função real dos índices de linha de uma ou mais matrizes3.
Um elemento funcional b e dito dependente da matriz A se algum índice de linha da matriz A pertencer ao conjunto de parâmetros desse elemento funcional.
Por abuso de linguagem, denomina- se parâmetro de um elemento funcional toda matriz de a qual o elemento funcional é dependente.
Uma matriz que contem ao menos um elemento funcional dependente da matriz A e dita dependente da matriz A. Os parâmetros de uma matriz são a união dos parâmetros de todos seus elementos funcionais.
Assim como a ATQ a ATG é definida por dois operadores matriciais:
Produto tensorial generalizado x.
Soma tensorial generalizada+.
A notação definida na seção 3.1 continua sendo válida para as matrizes constantes de, matrizes sem elementos funcionais).
As matrizes com elementos funcionais, denominadas matrizes funcionais, são descritas com 0 uso da notação seguinte:
Sejam: Ak índice de linha k da matriz A;
A (B, C) matriz funcional A que possui como parâmetros as matrizes B e C;
AU- (Bf) elemento funcional (Lj) da matriz A (B, C);
A (bk, C) matriz funcional A (B, C) em a qual 0 índice de linha da matriz B já e conhecido e igual a k (essa matriz pode ser considerada dependente da matriz C somente);
A (bk, c¡) matriz funcional A (B, C) em a qual os índices de linha das matrizes B e C já são conhecidos e iguais a k e Z respectivamente (uma vez que todos os parametros da matriz são conhecidos, pode ser considerada uma matriz constante);
A (B) x BÇÁ) produto tensorial generalizado entre as matrizes A (B) e B (A);
A (B)+ BÇÁ) soma tensorial generalizada entre as matrizes A (B) e B (A).
Sejam por exemplo duas matrizes A (B) e B (A) dadas por:
O produto tensorial definido por C $= A (B) x B (A) é igual a:
Os elementos da matriz A variam em função de os elementos da matriz B por isso a denominação A (B), ocorrendo o mesmo para a matriz B, onde seus elementos variam em função de a matriz A, ou seja, B (A).
O produto tensorial generalizado C $= A (B) x B (A) é definido algébricamente.
A soma tensorial generalizada e definida utilizando- se o conceito de matriz identidade com o produto tensorial generalizado da Equação 3.2: Sejam as matrizes A (B) e B (A) utilizadas para descrever 0 produto tensorial generalizado.
A soma tensorial definida por C:
A (B)+ B (A) e igual a:
A soma tensorial generalizada C $= A (u)+ B (A) é definida algébricamente por a atribuição do valor.
Distributividade do produto tensorial generalizado com relação a soma convencional de matrizes:
Associatividade do produto tensorial generalizado e da soma tensorial generalizada:
Distributividade com relação a a multiplicação por a matriz identidade:
Decomposição em fatores normais I:
Decomposição em fatores normais II:
Decomposição em produto tensorial classico:
As propriedades da ATG podem ser vistas em maiores detalhes em.
Em a proxima seção veremos que a vantagem do formalismo SAN em relação a os outros7 concentra- se principalmente na sua capacidade de fornecer uma matriz de transição (gerador infinitesimal da Cadeia de Markov correspondente ao modelo global) através de uma descrição amplamente compacta e eficaz na busca de soluções numéricas.
Essa descrição compacta é chamada de Descritor Markomlano, cujo detalhamento é feito no Capítulo 4 deste trabalho.
Uma das vantagens do formalismo SAN em comparação a outros formalismos, como citamos anteriormente7 e a capacidade de fornecer uma descrição compacta da matriz de transição (gerador infinitesimal) correspondente a Cadeia de Markov associada ao modelo completo.
Esta descrição compacta é chamada de Descritor Marisol/ iam).
O descritor Markoviano é uma fórmula algébrica que por o intermédio de uma fórmula matematica descreve, a partir de as matrizes de transição de cada autômato, o gerador infinitesimal da cadeia de lvíarkov associada a SAN.
Primeiramente e necessário que se entenda como pode- se obter a matriz de transição correspondente a um determinado autômato estocastico.
Considerando a cadeia de lvíarkov descrita na Figura 4.1, a matriz de transição Q e uma matriz quadrada de ordem nQ igual ao número de estados da cadeia de Markov, neste caso, nQ 4 (estados A, B, C e D respectivamente).
Cada linha e cada coluna de Q e associada a um estado segundo a ordem lexicografica dos mesmos.
Logo, no exemplo dado, a primeira linha e a primeira coluna correspondem ao estado A, a segunda linha e a segunda coluna correspondem ao estado B, a terceira linha e a terceira coluna correspondem ao estado C, e a quarta linha e quarta coluna ao estado D. Portanto a matriz de transição correspondente à cadeia de Markov representada na Figura 4.1 é:
Os elementos de uma matriz Q (qm) são as taxas de disparo correspondentes as transições do autômato estocastico que se esta representando, transições do estado associado a linha i para 0 estado associado à coluna j.
Obtendo- se os elementos não-diagonais de Q, os elementos da diagonal principal devem ser definidos de forma a ser nula a soma dos elementos em cada uma das linhas da matriz.
A diagonal expressara o ajuste necessário para que a soma de todos os elementos de cada linha seja igual a zero.
Portanto, os elementos da diagonal principal serão necessáriamente negativos ou nulos, obedecendo as seguintes equações:
Em a fórmula, Q e a matriz de transição ou gerador infinitesiinal da cadeia de Markov correspondente a uma SAN.
A partir de o vetor (pi), pode- se obter mais informações sobre o sistema modelado, pois trata- se do vetor solução, considerando- se uma escala contínua de tempo.
Este vetor é um vetor de probabilidades que associa uma probabilidade m E a cada um dos n estados da cadeia de Markov.
Por intermédio deste vetor e possível obter- se os resultados estacionários do sistema modelado.
Para expressar o descritor Markoviano de uma rede de autômatos estocasticos, a todo autômato Ad) da SAN são associadas:
Uma matriz agrupando todas as taxas de transições locais, chamada Q, e 2 E matrizes agrupando todas as taxas de transições sincronizadas para os eventos e do conjunto E, chamada Qd) e (E e o número de eventos sincronizantes, e o conjunto de identificadores destes mesmos eventos).
Existem basicamente duas maneiras dos autômatos estocasticos interagirem:
Ou uma transição que ocorre em determinado autômato não afeta os estados de outros autômatos, ou uma transição dispara outras em autômatos diferentes.
As transições cujas taxas dependem somente do estado do próprio autômato e não de outros autômatos, são transições locais.
As taxas das transições sincronizadas, no entanto, que modificam e dependem do estado de outros autômatos, podem ser tanto funcionais quanto constantes.
Devido a estas características, as SAN podem ser tratadas separando- se as transições locais e tratando- as através da soma tensorial, que representa a independência entre os vários módulos que compõem a SAN, incorporando também a soma de dois produtos tensoriais adicionais, relativos aos eventos sincronizantes modelados na SAN.
Estes últimos representam a dependência (sincronização) dos módulos.
O descritor Markoviano é descrito através de operações tensoriais entre matrizes, as quais são denominadas tensores neste caso.
Os tensores representam as transições locais ou sincronizadas entre os estados de cada autômato.
Um descritor Markoviano é expresso então em duas partes:
As ãlgebras tensoriais clássica e a generalizada são utilizadas para a representação destas transições locais e sincronizadas, como também para a representação das taxas funcionais e/ ou constantes de uma SAN, como veremos nas seções seguintes.
A parte local é definida por uma soma tensorial das matrizes locais de cada autômato:
Sabendo disto, e considerando a SAN descrita na Figura 2.1, vejamos como são descritos os seus eventos locais.
Uma matriz de transição local (QP) sera associada a cada um dos autômatos (A (m)).
A matriz de transição Q do autômato equivalente ã rede de autômatos estocásticos (Figura 2.2) é dada, em parte, por a soma tensorial das matrizes de transições locais:
Portanto, uma matriz de transição local agrupa todas as taxas de eventos locais do autômato.
Se esta SAN da Figura 2.1 apresentasse apenas eventos locais, o gerador do autômato equivalente a este modelo seria simplesmente equivalente ã soma tensoriall das matrizes de transição locais.
É relevante salientar que a soma tensorial correspondente a parte local do descritor e uma soma de produtos tensoriais diferenciada?
Uma possível interpretação dos eventos locais e consideras- los um caso particular de eventos sincronizantes, onde somente um autômato é afetado.
Isto significa dizer que os demais autômatos serão representados por matrizes identidade, É necessário descrever também os eventos sincronizantes deste modelo, visto que os eventos locais foram descritos anteriormente.
Para cada evento sincronizante descreve- se um par de matrizes (para cada autômato da SAN):
Uma de elas descreve a ocorrência do evento sincronizante (positiva), e a outra (negativa) descreve o ajuste diagonal correspondente a cada taxa descrita na matriz de ocorrência.
Os eventos sincronizantes normalmente são definidos através de um autômato mestre e de um ou mais autômatos escravos, podendo também existir autômatos que uão sofrem influência de determinados eventos sincronizantes.
As matrizes positivas e negativas correspondentes aos autômatos que não são influenciados por um determinado evento sincronizante serão matrizes identidade, pois não ocorrerá nenhuma Mudança de estado nestes autômatos em decorrência de tal evento.
A matriz positiva correspondente ao autômato mestre contém a taxa de disparo de um dado evento sincronizante e.
A existência de probabilidade associada a uma transição tem por efeito a multiplicação da taxa correspondente por a probabilidade.
As matrizes positivas dos autômatos escravos contêm uma taxa de disparo igual a um.
Vejamos a parte sincronizante positiva da SAN da Figura 2.1 considerando o autômato AU) mestre para os eventos sincronizantes 61 e;
As matrizes de ajuste diagonal, ou matrizes negativas, possuem elementos nulos e nao-nulos, sendo que os nào-nulos somente aparecem na diagonal principal se existirem.
Somente a matriz negativa correspondente a um autômato mestre pode apresentar taxas negativas, As matrizes negativas dos autômatos escravos contêm uma taxa de disparo igual a um.
Estas taxas aparecerão sempre nas diagonais principais das matrizes de ajuste diagonal, sejam no autômato mestre ou no escravo.
A cada evento sincronizante, correspondem dois produtos tensoriais, um produto das matrizes positivas e outro das matrizes negativas:
Sendo assim, para o exemplo da Figura 2.1 teriamos as seguintes matrizes negativas, considerando o autômato A0) o mestre das sincronizações e a relação:
Tendo estas matrizes positivas e negativas, o descritor Markoviano será composto por as seguintes operações:
Como podemos observar até o momento, o descritor é, em resumo, o somatório da soma da parte local com a parte siucronizaute para cada autômato.
Observemos a equação a seguir:
Considerando o exemplo da figura 2.1, teremos o seguinte descritor Markoviano associado:
As transições funcionais não trarão modificações estruturais no descritor, apenas a utilização do produtos sensoriais generalizados3 será necessária.
Portanto, o descritor correspondente à Figura 2.1 com as taxas descritas na Tabela 2.2 sera formado da seguinte maneira:
Parte local:
Como podemos notar, e possível construir uma matriz global que representa completamente um sistema, porem na abordagem SAN esta matriz nunca é gerada, Isto porque, nas SAN, são geradas matrizes individuais para cada Componente combinadas com as informações referentes às interações entre estes num descritor Markoviano, através dã somã de produtos tensoriais.
Conclui- se que esta abordagem mantem os requisitos de memória em limites gerenciaveis e evita a explosão do espaço de estados, devido a o seu formato compacto Em o próximo capítulo veremos como e realizado o procedimento de multiplicação de um vetor por um descritor Markoviano no cálculo das soluções numéricas dos modelos SAN, no âmbito de métodos numéricos iterativos.
Multiplicação Vetor--Descritor Tradicional Os principais alvos das SAN são os problemas com grandes espaços de estados, sendo os métodos iterativos os mais adequados para resolves- los pois aproveitam as características das técnicas de armazenamento esparsas e não requerem tanta memória quanto os métodos diretos.
A multiplicação de um vetor de probabilidades 71, que nesta seção chamaremos de v, por o gerador infinitesimal Q e uma das operações fundamentais realizadas por os métodos iterativos.
Diferentes formas de armazenar o gerador e o vetor de probabilidades podem ser utilizadas no intuito de executar este produto eficientemente como veremos a seguir, Esta Multiplicação é dada por a seguinte expressão, sendo N o número de autômatos na rede, e E o número de eventos sincronizantes:
Em este capítulo trataremos das particularidades envolvidas na execução desta específica operação de multiplicação:
O algoritmo da multiplicação de um Vetor por um produto tensorial (Algoritmo de Deslocamento, ou simplesmente Shuffle), sera apresentado conforme as referências, sendo previamente necessário estabelecer algumas definições sobre seqüências finitas de matrizes:
Sejam N dimensão da i-esima matriz de uma seqüência;
Para calcular a multiplicação de um vetor 1) por o termo Qlil é necessário e suficiente saber como multiplicar um vetor por um fator normal.
Sabe- se que um fator normal é um caso específico de produto tensorial entre uma matriz Q e uma matriz identidade I. Logo, dois fatores normais são possíveis:
A partir de estas definições o formato do descritor Markoviano pode ser reescrito na forma descrita na Figura 5.1, que representa as matrizes de transição necessárias para descrever uma SAN, mostrando a representação de seu descritor Markoviano.
O vetor o deve ser multiplicado por o primeiro fator normal, o resultado é multiplicado por o segundo fator normal, e assim por diante, até o último dos fatores normais.
Isto é possíxvel graças à propriedade de associatividade da multiplicação (convencional) de matrizes.
Além de isto, a propriedade da comutatividade entre fatores normais permite a multiplicação de fatores normais em qualquer ordem.
O caso mais simples de Multiplicação de um vetor por um produto tensorial e quando as matrizes não apresentam elementos funcionais, apenas constantes.
Em este caso, é necessário calcular:
De acordo com a propriedade de decomposição de produtos tensoriais abaixo, todo produto tensorial de N matrizes é equivalente ao produto de N fatores normais:
Considerando o termo, e utilizando a propriedade citada, tem- se:
Como para calcular a multiplicação de um vetor por o descritor Markoviano de uma SAN é necessário saber multiplicar um vetor por um fator normal, temos por exemplo três matrizes A (m), A (a) e A (m) e o produto tensorial entre estas:
Aplicando a propriedade da multiplicação por fatores normais temos as seguintes operações a realizar:
Quando estas fórmulas tem que ser transformadas num algoritmo é necessário entender a melhor maneira de tratas- las.
Agora, cada um dos termos da fórmula pode ser tratado de forma independente por o algoritmo de multiplicação, da seguinte maneira:
Dentro de um loop, em cada iteraçào considera- se apenas uma parte do vetor para multiplicar.
O algoritmo para esta Multiplicação, apresentado na página 29, realiza a secção das partes do Vetor v, com tamanho mv cada uma7 multiplicando- as por Q (w) como descrito na Figura 5.2.
O resultado da realização da multiplicação de cada vetor zm por a matriz é armazenado em urn outro vetor auxiliar denominado zw, de mesmo tamanho de zm, que é acumulado em 1;
após o término das multiplicações.
Em a seção anterior observamos o procedimento de divisão do Vetor 17 em partes a serem multiplicadas independentemente.
O vetor zm é preenchido com blocos sucessivos de tamanho nN.
Para o primeiro fator normal, ao contrário, a permutação necessária em 1) para formar os vetores 21-», equivale a percorrer o Vetor buscando um elemento a cada nrighn elementos.
A Figura 5.3 representa este processo.
A razão desta permutação pode ser compreendida observando o formato da matriz.
O algoritmo 5.2 divide o vetor 1;
em partes de tamanho nright1 e forma seus vetores 2m conforme nright1 (Figura 5.3).
As posições que comporão cada vetor auxiliar zm são retiradas do vetor U de forma nao-contígua, pois a matriz QO) apresenta elementos espalhados na diagonal e em outras posições.
O armazenamento dos resultados no vetor v e realizado de forma análoga a extração dos elementos, apenas utilizando um vetor auxiliar 20m, como anteriormente, para armazenamento das multiplicações.
Algoritmo 5.2 demonstra este procedimento.
Os outros fatores normais (do segundo fator normal ao último fator normal) são tratados com a combinação das duas etapas precedentes.
A técnica basica consiste em aplicar a propriedade da pseuddcomutatividade do fator normal:
Isto nos leva a sempre multiplicar os fatores normais da seguinte maneira:
As multiplicações são feitas de acordo com a posição da matriz Qü).
O Algoritmo 5.3 resume a multiplicação de um vetor v por um produto tensorial@ LÇWÍ Em este algoritmo os fatores normais são tratados do primeiro ao último.
Entretanto, de acordo com a propriedade da comutatividade dos fatores normais7 outra ordem pode também ser aplicada.
O Algoritmo 5.3 divide o vetor o em partes de tamanho fi, e forma seus vetores zm de tamanho m (Figura 5.3).
As posições que comporão cada vetor auxiliar zm são retiradas do vetor v de forma não-contígua também.
O armazenamento dos resultados no vetor v é realizado de forma analoga a extração dos elementos, apenas utilizando um vetor auxiliar zm;
Como anteriormente.
Em esta seção veremos as etapas constituintes do procedimento de multiplicação vetor-descritor passo a passo, para um modelo SAN definido, por exemplo, através de três autôrnatos:
A (m), A (m) e A (q).
Basicamente a Multiplicação do vetor U por a parte local do descritor ocorrerá com os seguintes termos, sendo QÉ&quot;) o descritor correspondente a descrição das transições locais de cada autômato i:
Cada termo será multiplicado por o vetor v, que e inicializado previamente.
O resultado de cada multiplicação sera acumulado num vetor auxiliar w.
Após realizar as três multiplicações referentes a estes termos o valor resultante acumulado no Vetor w e atribuído novamente ãs posições de v..
O que ocorre, na realidade, resume- se nas seguintes operações:
Para cada evento sincronizante ei, ocorre a multiplicação do vetor v por o seu descritor correspondente.
Supondo a existência de três eventos sincronizantes, a multiplicação por o vetor v tera as operações que seguem:
Note que a multiplicação por a parte sincronizante se da tanto para a parte positiva quanto para a parte negativa, portanto o procedimento acima é realizado duas vezes.
A o final destas multiplicações o vetor auxiliar w conterá os resultados numéricos referentes a multiplicação do vetor v por o descritor Markoviano do modelo, mostrado de forma geral na Tabela 5.1, ou seja:
Esta seção descreve alguns fatores que devem ser observados durante a ocorrência de elementos funcionais num termo tensorial, Devido a ocorrência deste tipo de elementos, este e chamado de termo tensorial generalizado.
A multiplicação de um produto tensorial generalizado pode ter maneiras distintas de ser realizada devido a o tipo de dependências_ funcionais existentes entre as matrizes.
De acordo com a propriedade de decomposição de matrizes em fatores normais de produtos tensoriais generalizados, sempre e possível obter uma ordem a para multiplicar os fatores normais de um termo se e somente se não ha ciclos no grafo de dependências funcionais?
A existência de tais ciclos não permite a utilização direta desta propriedade.
A multiplicação de um vetor o por um produto tensorial (sem ciclos) e realizado de maneira similar ao caso sem elementos funcionais.
Duas modificações são feitas na multiplicação implementada por o Algoritmo 5.3: Para calcular a ordem 0 onde um dos fatores normais deve ser multiplicado;
Para avaliar os elementos funcionais das matrizes antes de suas multiplicações.
Relembrando que de acordo com a propriedade de decomposição em fatores normais dos produtos tensoriais generalizados (Equação 3.22 na Seção 3.2.3), o fator normal da matriz QW normais das matrizes que dependem do autômato Au).
Lsto define a ordem parcial entre os fatores normais.
Para certos produtos tensoriais a ordem de decomposição não necessáriamente e única.
Por exemplo, se duas matrizes da seqüência são constantes, uma das matrizes será certamente tratada antes da outra não importando qual..
Isto se da devido a o fato de que o produto de fatores normais de duas matrizes constantes é comutativo.
A regra geral e que duas (ou muitas) matrizes não tendo dependências funcionais diretas ou indiretas estão livres para mudar de posição para serem multiplicadas.
Esta regra generaliza a ausência de uma ordem precisa para multiplicação de fatores normais de um produto tensorial clássico (casos sem funções).
Quando, por outro lado, existem dependencias funcionais diretas ou indiretas, a ordem das matrizes deve ser corretamente definida, Por exemplo, considerando três matrizes A, B e C, e dado o produto tensorial:
A ordem de multiplicação destas matrizes sera regida por as dependências funcionais existentes entre estas.
Em este primeiro exemplo, o produto de fatores normais gerado e:
Dado este segundo exemplo de produto tensorial:
A ordem de multiplicação destas matrizes também sera regida por as dependências funcionais existentes entre estas, e como podemos notar a ordem de dependência já está devidamente estabelecida.
Logo, o produto de fatores normais gerado é:
Um dos casos n1ais evidentes onde ocorrem ciclos3 e produzido por A (B) xB (A).
Para este caso não existe uma propriedade que permite a direta decomposição em fatores normais.
Entretanto é possível aplicar a propriedade de decomposição em produtos tensoriais classicos.
Com a propriedade abaixo os casos com ciclos de dependências podem ser transformados, de forma a não mais apresentarem esta característica.
Segundo Fernandes, Plateau e Stewart, o custo computacional envolvido no produto de um vetor por um termo tensorial pode ser obtido observando- se o número de multiplicações vetor-matriz executadas dinha 10 do Algoritmo 5.3).
Para cada iteração i do algoritmo são executadas mcft, x nright¡ multiplicações Vetor-matriz com matrizes de tamanho m..
Supondo que as matrizes Q (u) estão cheias, o número de multiplicações para cada produto vetor-matriz e igual a 2 mefti x nright¡ x n, 2 m, o custo computacional do Algoritmo 5.3: Comparando este numero necessário a multiplicação que consiste em primeiro calcular, para então multiplicar o vetor v, temos um custo Computacional na ordem de.
Evidentemente,: A comparação apresentada supõe matrizes plenas, no entanto este não é o caso comum encontrado.
Estes casos estão detalhados em Em esta seção apresentamá- se- apenas a propriedade que transforma este tipo de termo em produtos tensoriais clássicos.
Porém se as matrizes Q (i) são armazenadas num formato esparso, 0 número de multiplicações para cada produto vetor-matriz é, freqüentemente, inferior a, o custo computacional apresentado neste algoritmo é então:
De a mesma forma, o custo de Multiplicar uma matriz unica calculada por a formula x Q j não é m) 2 se esta matriz for armazenada em formato esparso.
Em este caso teremos um custo computacional equivalente à ordem do número de elementos não nulos da matriz resultante.
Em a prática, considerando que para obter as soluções estacionárias dos modelos SAN é necessário Multiplicai um vetor de probabilidades por o Descritor Markoviano completo do modelo, tem- se um custo computacional dado por:
Em este capítulo apresenta- se três altemativas estudadas para a otimização da multiplicação vetor-descritor que não foram efetivas para o propósito estabelecido.
Apesar de isto serviram como base para a elaboração de uma nova altemativa (Capítulo 7), pois permitiram um estudo mais direcionado e prospectivo a respeito de o tema.
A primeira altemativa baseia- se especificamente na avaliação do resultado obtido ao utilizar a potenciação direta dos termos a serem multiplicados por o vetor (pi) de probabilidades.
A segunda altemativa explora algumas propriedades da álgebra tensorial clássica em busca da redução dos produtos tensoriais.
Em esta altemativa questiona- se a criação de matrizes de permutação funcionais no intuito de adequar os termos a aplicação de uma das propriedades selecionadas, A última altemativa não efetiva trata da verificação do custo computacional de realizar- se multiplicações ordinárias entre as matrizes de cada produto tensorial.
Em esta opção experimenta- se variações do posicionamento das matrizes em (tada termo.
Como uma primeira altemativa de otimização da multiplicação xretor-descritor pensou- se em explorar as características do método da Potência tentando reduzir o passo das iterações que são realizadas até a convergência do método, utilizando- se da potenciação direta dos termos envolvidos.
Poderia- se aumentar o passo das iterações, reduzindo assim o tempo global gasto, por exemplo substituindo duas iterações por uma única iteração.
Sejam as seguintes iterações:
Estas poderiam ser substituídas por a iteração única:
Para isto se faz necessária a¡ Jotenciação do termo (Ê+ I), ou seja, elevando- se este termo ao quadrado previamente as iterações, e possível que se reduza por a metade o número de passos a serem executados, acelerando o processo de multiplicação.
No entanto, o custo de cada passo também pode ser aumentado, visto que no caso de as SAN e necessário manter a estrutura tensorial da matriz Q, Adiçionalmente, e necessário levar em consideração que a potenciação de uma sonia como resulta num produto notável dado por, que tera mais termos que o termo original.
Considerando a estrutura do descritor Markoviano e os passos realizados na multilpicação vetor-descritor, pode- se fazer duas analises sobre os termos envolvidos:
Uma sobre a parte local do descritor Markoviano, onde temos somas de fatores normais;
E outra sobre a parte sincronizante, formada por produtos tensoriais;
Supondo que representa a parte local de um descritor Markovianm e que por a definição da soma tensorial generalizada Q,: (
M xIN)+ (IM xN), elevando- se Q¡ ao quadrado Temos:
A utilização deste recurso, de elevar- se ao quadrado a soma tensorial, em teoria denota o aumento de operações a serem realizadas, pois ao invés de uma única soma de dois fatores normais, passa- se a ter uma soma de três termos, sendo dois de eles fatores normais e um outro formado por a Multiplicação convencional de dois fatores normais por um escalar.
Entretanto, na pratica, existem formas de reduzir o número de operações realizadas visto que basicamente todos os termos apresentam operações que são repetidas nos demais termos.
Vejamos termo a termo os efeitos da utilização desta altemativa:
Primeiro termo do produto notável Segundo a propriedade citada na Equação 3.11, teremos o produto convencional de termos do tipo (Mgain), ou seja:
Um fator importante é que a operação (M@ l M) pode ser realizada uma única vez e seu resultado armazenado para que seja reaproveitado por os outros cálculos.
Ou ainda, pode- se calcular M 2 uma única vez e I M2, realizando o produto tensorial M2 x In.
De a mesma forma, segundo a propriedade citada na Equação 3.11, teremos o produto convencional de termos do tipo (IM xN), ou seja:
Em este caso, da mesma forma que com o primeiro termo, pode ser realizada a operação (IM xN) uma única vez e seu resultado armazenado.
Analogamente, como sugerido no primeiro termo, calcula- se N 2 uma unica vez e IMZ, realizando o produto tensorial IM x N?
Segundo termo do produto notável:
Para o segundo termo o cálculo se resume em multiplicar os resultados previamente calculados para os demais termos, o que não é um procedimento muito custoso, além de também realizar sobre este resultado 0 produto convencional por um escalar.
Supondo que QE representa a parte sincronizante de um descritor Markoviano, e tendo Qi:
M xN, ao elevarmos este termo ao quadrado teremos M2 x N2, onde:
A aplicação da potenciação nesta parte do descritor nrarkoviano pode ser considerada uma altemativa valida, visto que a única diferença em relação a execução tradicional da multiplicação vetor-descritor e o riso das matrizes M 2 e N 2, cuja potenciação acrescenta apenas operações de multiplicação clássica entre matrizes, uma para cada matriz envolvida.
Em a tentativa de reduzir as operações realizadas por o método da Potência utilizando 0 recurso da potenciação, pode- se em teoria aumentar o tempo e o Custo de procesSANento, principalmente devido a modificação do termo original que e acrescido de operações de multiplicação convencional.
Para matrizes grandes, isto pode ser muito custoso.
Esta altemativa foi descartada, pois apesar de demonstrar potencial em termos de não comprometer os requisitos de memória (possibilita o reaproveitamento de cálculos), ao propor a potenciação de um produto tensorial, em teoria, denota o aumento de operações a serem realizadas, tornando necessária a execução extra de multiplicações clássicas entre as matrizes envolvidas.
Em a próxima seção, uma segunda altemativa trata de verificar propriedades da álgebra tensorial no intuito de reduzir os produtos tensoriais.
A pesquisa realizada para a primeira altemativa de otimização permitiu- nos levantar algumas questões referentes a propriedades da álgebra tensorial que poderiam ser aplicadas na multiplicação vetor-descritor como uma nova tentativa de otimização, por exemplo:
Decomposição em Fatores Normais:
Distributividade na Multiplicação por uma matriz identidade:
A propriedade que sera 0 foco de pesquisa nesta altemativa de otimização é 0 caso da distributividade na multiplicação por uma matriz identidade destacada na Equação 3.11, e reescrita logo abaixo com a substituição de m por m x m:
Em este caso, verifica- se a possibilidade de executar apenas multiplicações clássicas entre as matrizes envolvidas7 posteriormente aplicando no resultado um produto tensorial com uma matriz identidade.
Isto reduziria as operações realizadas no procedimento de multiplicação vetordescritor.
Supondo três matrizes A, B e C e a respectiva Multiplicação por um vetor de probabilidades (pi) (O), tem- se a multiplicação (pi0) (A x B x C) equivalente a:
Em este caso (pi)(&quot;) refere- se ao vetor final da iteração.
Logo, para efetuar estas multiplicações tem- se os seguintes passos intermediários a realizar:
O resultado obtido ein 71- é o vetor final da iteração, e pode ser acumulado num vetor auxiliar denominado nesta seção de H. Estes passos repetem- se ate a última iteração (referente apenas à parte sincronizante positiva da Multiplicação Vetor--descritor, que a princípio servirá como base para futuras otiinizações nas demais partes componentes do descritor Markoviano.
Utilizando a propriedade citada na Equação 6.3 pode- se teoricamente reduzir as operações a uma única multiplicação clássica de matriz por um vetor 77 a), e a realização de um produto tensorial com uma matriz identidade (resultante da multiplicação clássica de outras matrizes identidades).
Mas esta possibilidade encontra alguns obstáculos para sua implementação.
Como sabemos, o produto tensorial quando escrito na forma de fatores norinais7 faz com que cada tem1o tenha permutações diferenciadas, guiadas por a posição do descritor no termo.
Vejamos um exemplo:
Como podemos ver no exemplo acima, e recapitulando os conceitos vistos na Seção 5.1.2, a matriz resultante apresenta elementos espalhados na diagonal e em outras posições, isto quando o fator normal for do tipo.
Quando o fator normal é do tipo a matriz resultante é uma matriz com blocos diagonais, onde em cada bloco tem- se elementos espalhados na diagonal e em outras posições como demonstrado abaixo:
Por último, quando o fator normal é do tipo tem- se como resultado uma matriz de blocos diagonais onde cada bloco é formado por pequenos blocos e estes são simplesmente a matriz Q (w).
Vejamos o exemplo:
Logo, conclui- se que a ordem de execução dos produtos tensoriais em cada termo e estritamente relevante para a obtenção da solução.
Em este âmbito, a propriedade destacada na Equação 6.3 apresenta uma particularidade que não permite sua utilização direta neste caso.
Enquanto na Multiplicação vetor descritor temos, por exemplo, termos do tipo 103 Q, para a correta apli cação da propriedade os termos só poderiam ser do tipo Q, x I, que como vimos anteriormente, o resultado não e igual a 100 Q,.
O exemplo de Multiplicação vetor-descritor a seguir mostra como esta propriedade poderia ser aplicada com algumas adaptações:
Note que o segundo e o terceiro fatores normais devem sofrer alterações para contemplar o formato ideal a aplicação da propriedade citada na Equação 3.12, pois seria necessário que a multiplicação vetor-descritor pudesse ser totalmente realizada com termos do tipo como mostrado na Equação 6.5, Entretanto sabemos que não obteremos os mesmos resultgados nas duas execuções, pois os termos envolvidos nas operações serão tratados diferentemente.
Diante de isto, duas maneiras foram pesquisadas para contornar este problema.
A primeira de elas sugere alterar o algoritmo existente para cálculo da multiplicação do vetor por fatores normais, de forma que a utilização das variaveis meft e nright, presentes nos algoritmos originais, garanta a execução das permutações necessárias sobre as matrizes.
Esta altemativa foi descartada devido a o interesse em explorar as propriedades da álgebra tensorial generalizada na busca de outra altemativa:
Utilizar matrizes de permutação para modificar cada termo que não for do tipo para este formato, Estas matrizes de permutação chamaremos de PM- e devem permutar os termos da maneira apropriada, ou seja, cada termo da multiplicação vetor-descritor tera o formato após a aplicação das permutações.
Uma matriz de permutação é uma matriz que possui somente um elemento (não nulo e igual a um) por linha e por coluna.
Segundo a propriedade da pseudo-comutatividade do produto tensorial, estas matrizes de permutação quando aplicadas a esquerda de uma matriz qualquer, representam um reordenamento das inhas desta matriz.
De a mesma forma, quando aplicadas à direita, representam um reordenamento das colunas.
Vejamos um exemplo:
Em este caso, tem- se a seguinte equivalência, considerando P, BT a matriz transposta da matriz P, B:
Para obedecer a equivalência estabelecida na Equação 6.6 são necessárias as seguintes matrizes de permutação:
A utilização destas matrizes de perrnutação (PW) permite que os termos sejam alterados para que seus elementos fiquem no formato Q, Ê mngm.
Em este formato, experimentações foram realizadas tentando- se aplicar a propriedade (Equação 6.3) para obtenção de ganho em relação a o tempo de procesSANento.
O próximo passo foi buscar na Equação 6.7 algumas variações, considerando a associãtividãde da multiplicação clássica.
Mais uma vez os testes realizados demonstraram que a utilização de matrizes de perrnutação influência nesta característica não mais sendo possível realizar as multiplicações em ordem distinta.
Supondo- se o vetor rwjá que o mesmo apresentará, neste exemplo, oito posições), após a realização da multiplicação, conforme a Equação 6.7, obtem- se o seguinte vetor solução:
A o tentar- se modificar a ordem de multiplicação, os resultados não mais continuaram corretos, pois as matrizes de permutação em teoria devem somente serem multiplicadas por as matrizes que modifiçam.
Vejamos mais exemplos:
Uma outra altemativa de associatividade destas matrizes, teve como resultado o vetor esperado, mas com os elementos permutados:
Portanto7 através destes exemplos Verifica- se a não compatibilidade com a propriedade da associatividade da multiplicação clássica quando trata- se da multiplicação por matrizes de permutação.
Apesar de isto, o uso destas matrizes para tentar modificar 0 formato dos termos presentes na multiplicação vetor-descritor clássica, torna- se essencial para aplicar a propriedade da Equação 6.3 vista anteriormente.
Um fator relevante a ser considerado e como poderiam ser armazenadas as matrizes de permutação7 visto que em problemas onde a ordem das matrizes é grande, as matrizes de permutação tendem a serem maiores ainda, pois sua ordem é dada por a ordem da matriz resultante de sucessivos produtos tensoriais7 como vimos em exemplos anteriores.
Uma altemativa foi modelar as matrizes de permutação também como produtos tensoriais.
Dadas as matrizes A e B (ambas de ordem n2 por exemplo), e o procedimento de multiplicação vetor-descritor com utilização de matrizes de permutação, a idéia e substituir uma grande matriz PM por um produto tensorial entre N matrizesl.
Portanto, tem- se:
O produto tensorial de duas matrizes constantes C e D não torna possível a representação de uma matriz de permutação, pois como sabe- se, estas matrizes têm a particularidade de que In é o número de matrizes que compõem o produto tensorial a permutar, logo num produto tensorial A x B teremos cada matriz de permutação representada por o produto tensorial entre outras duas matrizes.
Tendo a matriz acima, resultante do produto tensorial entre C e D, e a matriz de permutação Pub, pode- se gerar funções que definirão cada elemento das matrizes C e D de forma que seu produto tensorial resulte na matriz de permutaçào desejada.
Para o exemplo, tem- se:
O uso de produto tensorial generalizado, da- se devido a inevitável utilização de funções para definir os elementos das matrizes C e D. A palavra st e utilizada nas funções para denotar a avaliação do estado de uma dada matriz, este significando a linha ou coluna dos blocos (ou dentro de os blocos) em que a função deve ser avaliada.
Logo, com a ocorrência de dependências funcionais entre estas matrizes, tem- se Pub dada por.
Note que PUBT e a matriz transposta de Pub, entretanto são as mesmas matrizes, pois ambas contem os mesmos elementos, nas mesmas posições.
Utilizando- se matrizes de permutação funcionais não se pode afirmar que o mesmo acontece, pois ao transpormos uma matriz funcional os elementos nas duas matrizes pasSAN a retornar valores diferentes, obviamente porque as avaliações ocorrerão em posições trocadas.
Sabendo que as matrizes ditas PUiT quando descritas no formato de produto tensorial generalizado têm as mesmas funções que a matriz PU¡ no mesmo formato, não sera necessário demonstrar como estas são formadas, tendo em vista que não há necessidade de transpor os elementos funcionais.
Para o caso PUBT:
DQ não ha necessidade de indicarmos diferenciada mente as matrizes componentes deste produto, pois tem- se que PUBT:
Entretanto, estas definições dadas a Pub são Validas apenas quando se tem um termo como, ou seja, quando temos o produto tensorial entre duas matrizes (neste caso, A e B).
Logo, uma forma de generalizar a geração das matrizes de permutação em formato tensorial também deve ser estudada, para que estas possam ser geradas para o caso de um produto tensorial entre N matrizes.
No caso de o produto tensorial de duas matrizes, ambas de ordem n2, gerou- se a seguinte multiplicação, como vista anteriormente na Seção 6.2.2, juntamente com as matrizes de perinutação funcionais C e D:
Em um exemplo com três matrizes de ordem n2 (A, B e C), a matriz de permutação gerada apresentará então o produto tensorial de tres matrizes (D, E e F), cujos elementos também serão funcionais.
Seguindo o raciocínio do exemplo anterior, as etapas da multiplicação e as respectivas matrizes envolvidas seriam:
As funções geradas para representação das matrizes de permutação deste exemplo foram as seguintes:
No caso de três autômatos de ordem 11:3 (A, B e C), tem- se dois fatores normais a permutar, portanto duas matrizes de permutação distintas:
Através dos exemplos apresentados pode- se perceber uma certa similaridade na formação das matrizes de perniutação, 0 que nos leva a considerar que existe uma maneira de generalizalas para que, independente do número de matrizes envolvidas, possa- se obter tais matrizes de perniutação com facilidade.
Porém, mais testes serão necessários visto que na medida em que o número de matrizes envolvidas aumenta, 0 número de diferentes termos tensoriais também.
Em o intuito de otimizar a multiplicação vetor-descritor no contexto de resolução das SAN, o objetivo primordial ao estudar esta altemativa era a obtenção de um único fator normal para cada termo do tipo produto tensorial, ou seja, um fator constituído de um produto tensorial entre uma matriz qualquer, resultante de multiplicações clássicas, e matrizes identidade.
Este único fator normal seria então Multiplicado por o vetor de probabilidades.
Considerando a seguinte estrutura de multiplicação vetor-descritor:
Com a utilização da propriedade da Equação 6.3, tem- se em teoria:
O problema encontrado nesta abordagem e que a propriedade da Equação 6.3 não se Verifica completamente para o caso de matrizes de permutação funcionais, ou seja, não ha equivalência da propriedade da ATC (Equação 6.3) quando se utiliza ATG na execução dos produtos tensoriais.
Como as matrizes de permutação estão representadas por produtos tensoriais de matrizes funcionalmente dependentes, não é possível decompor os termos da maneira proposta por a propriedade.
De acordo com ha uma incompatibilidade do produto tensorial generalizado com a multiplicação ordinaria de matrizes para o caso de existência de ciclo de dependência.
Logo, um novo passo nesta altemativa de otimização seria tentar descobrir uma nova propriedade dentro de a ATG que contemple a utilização de multiplicãções ordinárias no produto tensorial.
A partir de testes realizados com matrizes exemplo, verificou- se a incompatibilidade anteriorrnente provada por, mesmo tratando- se de matrizes de permutação.
Dado o produto, resultante na matriz I «Ê B mostrada abaixo, quando aplica- se a para o priedade 6.3 obtendo a matriz equivalente a (Cx B x C) x (D x 1 x D) percebese a ausência de varios elementos, estes eliminados na execução do produto tensorial em conseqüência de avaliações erradas sobre os elementos funcionais das matrizes.
Por outro lado verificando o produto (Cx D) x (B (591) ao aplicarmos a propriedade tendo (CxB) x (DxI) ha efetivamente a troca das linhas da matriz resultante como esperado ao ãplicarmos a permutação a esquerda.
Contudo, no produto x (CxD) ao aplicarmos a mesma propriedade, tendo (B x C) a (I x D), a troca das colunas não acontece como esperado.
Alguns elementos da matriz resultante são eliminados e outros não ficam posicionados corretamente.
Conclui- se que a propriedade da Equação 6.3 somente pode ser utilizada da seguinte maneira:
Outro fator a considerar e como este formato dado para as matrizes de permutação (através de funções) podera colaborar para a utilização da propriedade descrita por a Equação 6.3, pois a idéia principal seria conseguir obter um único fator do tipo produto tensorial que seria multiplicado por o vetor de probabilidades (pi).
Com os resultados obtidos ate o momento, supõe- se que cada termo deste produto tensorial serã formado por multiplicãções clãssicas entre matrizes.
Portanto, apesar de favorecer a otimização da Multiplicação vetor-descritor, a propriedade estudada (Equação 6.3) só sera válida se as matrizes envolvidas forem de mesma ordem, pois para o produto classico entre matrizes isto é obrigatório.
No caso de modelos SAN, trabalhar com matrizes de mesma ordem torna- se uma desvantagem, visto que uma parcela reduzida de modelos apresenta esta característica.
De entre as altemativas descritas até o momento há em comum o foco na algebra tensorial, que veio não somente como uma nova forma de representação, mas também para otimizar o modo de armazenamento das matrizes.
Em esta seção7 destacaremos a possibilidade de gerar os mesn1os resultados obtidos nos procedimentos tradicionais de realização do produto tensorial, porem a partir de uma nova forma de distribuir as operações realizadas para tal.
Em esta altemativa, apesar de ainda utilizarmos propriedades do produto tensorial, as operações basicas serão multiplicações ordinarias entre termos das matrizes envolvidas.
Algumas idéias testadas preliminarmente a respeito de a utilização de multiplicações ordinarias entre matrizes especificamente serão abordadas nesta seção, começando por 11m teste basico de multiplicação entre as matrizes A e B (ambas de ordem 2), constituintes de um produto tensorial A x B:
A proposta é tentar descobrir quais multiplicações podem ser realizadas utilizando as matrizes A e B na busca dos elementos resultantes do produto tensorial desejado.
Basicamente pode- se pensar em multiplicar AB para verificação do resultado.
O que temos neste caso e uma matriz também de ordem n2 com elementos que são somas de dois termos.
Vejamos: Apesar de termos somas em cada elemento da matriz resultante e evidente pensar que existe uma maneira de obter Cada um dos termos somados isoladamente, multiplicando por exemplo novamente por uma matriz 4 p, que poderia ser chamada de Matriz de Desmembvamento.
Esta matriz seria possivelmente definida através de elementos funcionais capazes de desmembrar as somas presentes nas matrizes resultantes das multiplicações ordinárias.
Assim sendo com a multiplicação (AB) ga e possível obter pelo menos oito termos presentes na matriz resultante R, matriz referente a o produto tensorial em questão.
Para fins ilustrativos, a matriz R representada abaixo apresenta apenas os elementos obtidos nesta primeira multiplicação:
Um fator relevante neste momento e pensar em como obter os demais elementos supondo que é possível efetuar uma operação de desmembramento da soma resultante em cada elemento da matriz AB.
A primeira altemativa e realizar novas multiplicações da matriz A com Variações da matriz B. Por exemplo7 podemos inverter linhas e colunas da matriz B e verificar se são gerados outros elementos necessários ao resultado desejado.
Como foi possível observar, com as multiplicações clássicas (AB) 1,9 e (ABÚga, obtivemos todos os elementos presentes na matriz resultante do produto te11sorial A x B (sendo A e B de ordem n 2), ou seja, através da soma de R e R* obteríamos o mesmo resultado obtido ao resolvermos o produto tensorial A x B da forma habitual.
Entretanto, se estas apresentarem ordem 11:3 não mais este raciocínio pode ser utilizado, pois a matriz B* gerará na multiplicação elementos ja observados na multiplicação.
Vejamos. A multiplicação (AB) gera os seguintes elementos em R:
Pode- se verificar que alguns elementos repetem- se nas duas matrizes resultantes R e R* sugeridas (a segunda coluna de ambas as matrizes possuem os mesmos elementos), o que torna- se um problema para utilização desta altemativa, pois seriam necessárias algumas funções específicas para anular os termos repetidos, e além de isto, outras multiplicações entre A e Variações de B que gerassem os elementos faltantes.
Uma forma de contornar o problema foi realizar operações de inversão parcial da matriz B, ao invés de invertermos as linhas e as colunas simultaneamente.
A inversão parcial é comandada por deslocamentos apenas nas linhas da matriz B, técnica esta que chamaremos de Técnica do Deslocamento (ou Shift).
Note que para isto teremos mais termos de multiplicação alem de (AB) e (AB*) ga para um caso de maior ordem4.
Evidentemente com matrizes de ordem n2 ambas as tecnicas funcionam, tendo apenas os dois termos citados acima a resolver.
Dado um produto tensorial AxB, a técnica do Deslocamento propõe a realização de operações de multiplicação ordinária entre as matrizes envolvidas (A e B) e uma matriz de desmembramento go.
Note que as matrizes de desmembramento são caracterizadas preliminarmente na Seção 6.3 e sua Composição sera abstraída na descrição desta alternativa de representação do produto tensorial.
Primeiramente realiza- se (AB) e após operações de inversão parcial ou deslocamentos da matriz B, para cada deslocamento i ocorrerá uma multiplicação da matriz Bi* por a matriz A, ou seja tem- se multiplicações do tipo (ABÚM, onde nA representa a ordem da matriz A. Estas multiplicações gerarão todos os elementos constituintes de A x B. Exemplificando a utilização desta tecnica, tenhamos por exemplo duas matrizes A e B primeiramente de ordem n2 e vejamos os passos envolvidos na obtenção dos elementos resultantes do produto A x B:
Multiplicação (AB) Uma matriz cp de desmembramento quando aplicada sobre o produto AB geraria os seguintes elementos do produto tensorial A x B (estes representados numa matriz R para fins de ilustração):
Multiplicação (ABV Me a:
Multiplicação, onde a matriz B será invertida parcialmente uma (P), duas (T) ou i vezes conforme a ordem das matrizes.
Cada deslocamento é comandado por deslocamentos apenas nas linhas da matriz B. Note que para contemplar todos os elementos que compõem a matriz resultante do produto tensorial A x B, serão necessárias 71,4 r 1 multiplicações entre A e B».
Em este exemplo de ordem 11:2 será necessário apenas um deslocamento na matriz B, originando o termo:
Como pode- se notar, com as multiplicações (AB) e (AB1), conseguimos obter todos os elementos encontrados em AxB, estes ilustrados em R e RP.
Considerando agora, duas matrizes (A e B) de ordem n 3, teríamos os seguintes passos, considerando os seguintes elementos que compõem o produto tensorial A x B, quando as ordens m¡ e m3 são iguais a 3: Multiplicação Considerações sobre a Técnica do Deslocamento.
A Técnica do Deslocamento permitiu- nos** explorar um pouco mais as características do produto tensorial em termos de posicionamento dos elementos dentro de a matriz resultante e a sua relação com os termos gerados com 11111 ltiplicações ordinarias entre as matrizes.
Porém, como podemos observar, ha um fator limitante na utilização desta técnica:
A necessidade de elaborar matrizes de desmembramento, a princípio matrizes funcionais, para que estas realizem o desmembramento das inevitáveis somas que são geradas na execução de produtos classicos.
Outro fator relevante para estudo e como esta tecnica poderia ser estendida para produtos tensoriais entre mais de duas matrizes.
A idéia de utilização de matrizes de desmembramento não se torna tão atrativa quando existe a possibilidade de se obter termos onde as somas são eliminadas na própria execução do produto ordinário entre as matrizes.
Isto se deve ao fato de que poderia- se realizar uma multiplicação de uma matriz A por uma matriz I bi] de ordem 713, composta do elemento bi] da matriz B replicado em sua diagonal, e de elementos nulos nas demais posições5.
Considerando um produto tensorial A x B a ideia básica é obter, através da Multiplicação entre A e uma matriz 117 v, os elementos constituintes de um determinado bloco dentro de a matriz resultante do produto tensorial em questão.
Algumas tentativas realizadas, como veremos logo abaixo, resultaram na obtenção de elementos em diferentes blocos da matriz resultante, e em posições não aleatórias, pelo contrário, resultaram na disposição dos elementos com um padrão de localização dentro de os blocos da matriz.
Vejamos: Em a primeira tentativa multiplicamos a matriz A por a matriz I b «que é resultado da multiplicação do elemento 1211 por uma matriz identidade I, gerando quatro elementos de A x B:
Para obtenção dos demais elementos seguiriam- se as Multiplicações entre a matriz A e a matriz I b», oude I b «teria como elementos na diagonal outro elemento da matriz B ainda não executado, por exemplo 1112, b21 ou b22:
Assim obteríamos todos os elementos presentes em AxB através de Multiplicações sucessivas entre A e I b».
Caso estivéssemos trabalhando com matrizes de ordem n:
3, a matriz I b «seria de ordem 11:3 cou1 cada elemento 1),¡ de B na sua diagonal (e o restante dos elementos em zero).
Uma matriz 1° «nesta altemativa refere- se a uma matriz esparsa.
Podase dizer que a matriz 1h11 é a multiplicação de um escalar b «por uma matriz identidade I, ou seja, lb:
B&quot; I. Além de isto, como pode- se perceber, preliminarmente, temos 013)?
Variações para a matriz lb a ser multiplicada por a matriz A. Matrizes de Posicionamento Sobretudo, tendo os elementos desejados, um outro fator a considerar e seu posicionamento.
Percebe- se que cada produto A I &quot;b «poderia receber a aplicação de um produto tensorial com uma matriz composta de um único elemento não-nulo, exatamente na posição desejada para os elementos no resultado.
Esta matriz chamaremos de Matriz de Posicionamento, e esta será representada por JU:
Logo, dado um produto tensoríal A x B, poderia- se decompor este produto em termos independentes (estes geram blocos de elementos presentes matriz resultante ao produto tensoríal), como segue na equação abaixo:
Em este exemplo, todos os termos são somados a fim de obter a matriz resultante do produto tensorial entre as matrizes A e B. Os índices i e j indicam qual elemento da matriz B esta sendo utilizado na formação da matriz I b», e tan1 ben1 guiando a formação da matriz de posicionamento Observando o exemplo, podemos definir então uma fórmula generalizada para obtenção destes termos independentes dado um produto tensoríal A (m) (Equação 6.8):
Variação na formação dos Termos mdependentes.
Porém como sabemos, conforme a Seção 5.1.2, na multiplicação vetondescritor, especificamente no algoritmo que realiza o produto de fatores normais com matrizes identidade à direita, o vetor não é dividido em partes, elementos do vetor são utilizados altemadamente.
Para evitar a utilização deste procedimento e necessária a realização de um produto tensoríal entre matrizes identidade à esquerda e uma dada matriz, conforme a Seção 5.1.1.
Em o caso específico de cada termo independente, este problema e resolvido através da Equação 6.9: Em o exemplo abaixo, os termos independentes são somados a fim de obter a matriz resultante do produto tensorial entre as matrizes A e B assim como anteriormente:
Os índices i e j indicam qual elemento da matriz A esta sendo utilizado na formação da matriz[ alí, e também guiando a formação da matriz de posicionamento j».
A diferença é mesmo na maneira em que o produto tensorial podera ser executado de forma facilitada através desta abordagem com termos independentes.
Como visto na Seção 5.4, a complexidade do produto de um Vetor por um termo tensorial pode ser obtida observando- se o número de multiplicações vetor-matriz executadas.
Em esta abordagem, deseja- se demonstrar qual o número de multiplicações xetomnatriz com matrizes de ordem n¡ são necessáriamente executadas.
Supondo que as matrizes QM estão cheias, ou seja, são matrizes plenas, o custo computacional envolvido na realização de um produto tensorial Q2211) x entre N matrizes de ordem N e:
Porém se as matrizes Q (i) são armazenadas num formato esparso, o número de multiplicações para cada produto vetor-matriz, considerando que n 21- é o número de elementos não nulos de uma dada matriz Q (i), o custo computacional para realização das multiplicações existentes nesta abordagem para o produto tensorial QL?
Em a pratica7 considerando que para obter as soluções estacionarias dos modelos SAN é necessário multiplicar um vetor de probabilidades por o descritor Markoviano completo do modelo, nesta altemativa tem- se um custo computacional dado por:
Comparando- se o custo computacional verificado na abordagem atual de multiplicação vetordescritor e do custo necessário ã execução desta altemativa estudada, compreende- se que da maneira sequencial em que as multiplicações entre as matrizes são realizadas, não ha um ganho em termos de número total de multiplicações executadas.
É fato que seqüencialmente esta altemativa não e considerada válida aos propósitos determinados, pois aumentaria consideravelmente o total de operações a serem realizadas.
Apesar de isto, foi possível tirar proveito da ideia de buscar multiplicações clássicas que posSAN gerar os elementos de A x B, elaborando uma nova altemativa de execução do produto tensorial entre matrizes.
Esta nova idéia traz a noção de Fatores Normais Parciais que Veremos no próximo capítulo.
Técnica de Fatiamento ou Slice. O procedimento de multiplicação vetor-descritor é a operação basica realizada por os métodos iterativos utilizados na resolução de modelos SAN.
Em o intuito de otimizar este método em termos de o número de multiplicações realizadas, pesquisas foram direcionadas ao estudo de propriedades da álgebra tensorial e sua aplicabilidade na Decomposição em Fatores Normais clássica, bem como na reestruturação deste tipo de decomposição, originando o que chamaremos de Decomposição Aditivo em Fatores Normais Parciais.
Um novo método para a multiplicação vetor-descritor e proposto a seguir, e denominado Técnica de Fatirimento, ou simplesmente Slice.
Partindo deste princípio, percebeu- se ser possível realizar uma decomposição diferenciada dos termos tipo produto tensorial, investindo na possibilidade de multiplicar- se os elementos das matrizes uns com os outros, ao invés de multiplicarmos matrizes inteiras, como na abordagem apresentada na Seção 6.3.
Considerando, por exemplo, o produto tensorial AxB xC, este pode ser decomposto tradicionalmente em três fatores normais que são multiplicadosg:
Contudo, no intuito de reduzir as operações na execução da multiplicação de um dado vetor (piM) por estes produtos tensoriais, a decomposição em fatores normais foi reestruturada.
Em este capítulo veremos uma nova maneira de decompor o produto tensorial, utilizando o que chamaremos de Fatores Normais Parciais.
Os fatores normais parciais levam em consideração as primeiras matrizes componentes do produto tensorial, onde N e o número total de matrizes neste termo, realizando a multiplicação ordinária entre os elementos, de acordo corn as propriedades inerentes a esta operação, ou seja, o resultado do produto tensorial é uma matriz constituída de blocos, cada um de dimensão 61 x[ M4, e para cada elemento desta matriz resultante será constituído um fator normal parcial pi.
A definição de cada um dos elementos da matriz resultante e feita levando- se em conta a qual bloco o referido elemento pertence, e qual a sua posição intema dentro desse bloco.
Com isto, os fatores normais parciais podem ser definidos como um conjunto de termos em que realiza- se uma a uma das multiplicações necessárias a execução do produto tensorial, das matrizes de cada termo, efetuando 0 produto tensorial das mesmas com a matriz N, neste caso a matriz mais à direita.
Considerando o exemplo A x B x C, onde as matrizes tem ordem n 2, Vejamos a formação dos fatores normais parciais:
Esta decomposição sera chamada Decomposição Aditiua em Fatores Normais Parciais, e para diversos modelos temos um ganho significativo em termos de o número de multiplicações realizadas como veremos na próxima seção.
Constiturse numa decomposição dita aditiva, pois os termos não são multiplicados como na abordagem tradicional 7 mas sim somados.
A o multiplicarmos cada fator normal parcial pi por o vetor de probabilidades (pi), cada resultado e somado num vetor acumulador H, que ao final das multiplicações conterá a solução do modelo.
Em a proxima seção, serão especificadas as etapas que constituirão o procedimento de multiplicação vetovdescritor utilizando esta nova técnica.
Posteriormente, serão explanadas as vantagens e limitações da mesma, abordando questões como custo computacional (em termos de número de multiplicações a realizar) en1 comparação ao método tradicional de multiplicação vetor-descritor.
Diferentes formas de tratar o produto tensorial em termos de operações realizadas, podem ser estudadas no intuito de executar o produto 7m 1 Qyl] eficientemente.
Sabe- se que no procedimento de multiplicação vetor-descritor atualmente implementado é suficiente saber como multiplicar um vetor por um fator normal.
Considerando o termo QM, tem- se as seguintes etapas (como visto na Seção 5.2) na tecnica tradicional:
Em as etapas acima, pode- se ver resumidamente a decomposição do produto tensorial em fatores normais, e a multiplicação de cada fator normal por um vetor de probablidades will.
Utilizando a nova decomposição em fatores normais parciais, as etapas da multiplicação vetor-descritor serão descritas diferentemente, propondo um procedimento altemativo e realmente eficiente em muitos casos:
A Técnica de Fatiamento ou Slice.
Dadas tres matrizes de ordem n2, um vetor de probabilidades, tem- se para este exemplo a definição de fatores normais parciais, um para cada elemento do produto tensorial AxB.
Vejamos então as etapas de multiplicação vetor-descritor para o termo A (29 B x C supondo as seguintes Multiplicação de por o 1° Fator Normal Parcial (pl) matrizes:
Este fator normal parcial sera decomposto em dois fatores normais tradicionais:
O vetor 7 d «conterá dois elementos nãornulos (ordem mv da última matriz).
O vetor em a) conterá também nN elementos não-nulos.
Porém serão necessárias 11102 multiplicações para obter- los.
O resultado em (número) será acumulado num vetor acumulador H. Multiplicação de por o 2° Fator Normal Parcial O vetor (número) conterá também mv elementos não-nulos.
Porém serão necessárias nN2 multíplicações para obter- los.
O resultado em na será acumulado num vetor acumulador H. Seguindo as mesmas etapas demonstradas acima.
A o realizar a última Multiplicação do vetor por o 16° fator normal parcial, o vetor de probabilidades H conterá a solução do produto tensorial.
Observando o exemplo desta seção, conclui- se que para um produto tensorial entre N matrizes plenas de qualquer ordem, tem- se fatores normais parciais.
Se considerarmos as matrizes esparsas, o número de fatores normais parciais e dado por nzi, sendo número de elementos não nulos da matriz z.
Outro fator a considerar e que devido a o número constante de elementos não-nulos em cada vetor resultante (piI), pode- se em cada fator normal parcial, fatiar o vetor (pi na do em mv partes a serem multiplicadas por o elemento alvo da decomposição, sendo devido a isto o nome dado à técnica:
Fatiamento ou Slice.
Pode- se com isto reduzir também o custo em memória ja que a matriz a esquerda não necessita ser armazenada, bem como os vetores auxiliares um podem apresentar tamanho igual a mv.
Sendo N o número de matrizes de um produto tensorial, n, a ordem de cada uma destas matrizes, e mv a ordem da última matriz neste termo, nesta nova abordagem para a multiplicação vetor-descritor, tem- se (para o caso de matrizes plenas) o custo computacional de multiplicar um vetor de probabilidades (pi) por cada fator normal parcial dado por:
Em a Equação acima temos (N -- 2) multiplicações ordinarias entre elementos de as (N -- 1) matrizes envolvidas.
Além de isto, como para cada fator normal parcial são gerados dois fatores normais tradicionais, para o primeiro fator normal temos mv multiplicações por o vetor (pi), e para o segundo, mv?
Multiplicações. O número de fatores normais parciais a tratar é dado por todas as combinações de elementos não-nulos das N~ 1 matrizes iniciais, ou seja na, Logo o custo computacional de tratamento de um produto tensorial nesta altemativa é dado por:
Primeiramente, analisaremos casos diversificados de produto tensorial entre matrizes, ou seja, serão combinadas matrizes de mesma ordem, ou de ordens distintas, com variações no total de seus elementos não nulos, desde matrizes plenas ate matrizes com alta esparsidadei, As tabelas com os comparativos apresentam as seguintes informações a respeito de os produtos tensoriais:
Ordem: Indica a ordem4 de cada uma das matrizes do produto tensorial;
Não-nulos: Representa o total de elementos não-nulos em cada matriz do produto tensorial.
E apresentado no mesmo formato de representação das ordens das matrizes;
Shuffle: Apresenta o custo computacional relacionado ao método tradicional;
Slice: Mostra o custo computacional estimado para o métotlo altemativo;
Matrizes com alta esparsidade serão consideradas as matrizes onde o número de elementos não-nulos é igual ou inferior ã ordem da maior matriz do produto tensorial cm que estas matrizes estão (rnvelvidas.
Dadas duas matrizes de ordem n2, nas tabelas as ordens das mesmas são representadas por 2 x 2.
Logo para N matrizes teremos 2 x 2, N vezes, por exemplo.
A Tabela 7.1 mostra um comparativo entre produtos tensoriais com i matrizes de mesma ordem e mesmo total de elementos não-nulos (nzi).
Nota- se que tanto nos produtos tensoriais com matrizes de ordem n 3 quanto de ordem n 4, 0 número de elementos Irão- nulos determina o custo computacional em ambas as abordagens.
Quando o total de elementos nao-nulos e maior do que a ordem das matrizes, observa- se que a técnica tradicional é mais vantajosa em termos de custo computacional, entretanto, na medida em que aumenta a esparsidade das matrizes envolvidas, ou seja, dÍIIIÍHHÍIIdO o total de elementos Irão- nulos das matrizes (passando a ser igual ou inferior à ordem das matrizes), a tecnica altemativa apresenta um ganho significativo em desempenho.
A Tabela 7.2 mostra um comparativo entre produtos tensoriais com i matrizes de mesma ordem e com variados totais de elementos Irão- nulos para cada matriz z (rm), Observa- se que na abordagem tradicional o custo computacional e independente da ordenação das matrizes por o número de elementos Irão- nulos Lue apresentam, pois o custo computacional mantevese constante.
A abordagem altemativa entretanto, depende estritamente da ordenação das matrizes no produto tensorial.
Esta ordenação deve ser realizada colocando- se as matrizes em ordem crescente de seus nzi.
Em a verdade, a última matriz da seqüência devera ser a de maior nzi, Quando esta ficar por último no termo tensorial, verifica- se uma redução considerável no total de operações a realizar, não importando, a partir de esta ordenação, em que posição estão as demais matrizes.
Em a Tabela 7.3, através de matrizes com ordens N diferentes es era- se demonstrar em que condições cada uma das tecnicas torna- se favorável para otimização do custo computacional.
Para isto foram mantidos os mesmos produtos tensoriais focando no tota de elementos uão-nulos de cada uma das matrizes (mantendo o mesmo nz, em cada produto tensorial).
Observa- se que o custo computacional apresentado por a técnica altemativa passa a ser vantajoso em relação a o tradicional no momento em que os produtos tensoriais apresentam matrizes onde o número de elementos não-nulos e inferior ã ordem na matriz de maior m, considerando que as matrizes estão ordenadas do menor para o maior m..
Em a Tabela 7.4, através de matrizes com ordens n, diferentes espera- se demonstrar a infiuência da ordenação destas matrizes, em ordem crescente de seus diferentes elementos uão-nulos (WLZÍ), para um melhor aproveitamento das características da nova abordagem.
A técnica tradicional não apresenta alterações no custo computacional simplesmente variando o posicionamento das matrizes.
Corn estes dados é possível afirmar que 0 melhor desempenho da tecnica altemativa esta atrelado ao fato de que a última matriz do produto tensorial deve sempre ser a de maior nzi, garantindo- se assim o menor custo computacional possível para esta técnica.
Contudo, existem casos em que o produto tensorial entre N matrizes e dado por uma matriz Q qualquer e N -- 1 matrizes identidade.
Este tipo específico de produto tensorial chamamos de fatores normais, como vimos anteriormente.
Se considerarmos as duas etapas de multiplicação vetor-descritor, uma da parte local e a outra da sincronizante, observa- se que na primeira contamos somente com termos que são fatores normais.
Este tipo de termo é tratado de forma eficiente por o método tradicional, pois ha apenas uma matriz neste produto tensorial que não é identidade.
Utilizar a tecnica de decomposição em fatores normais parciais para este tipo de termo não acrescenta melhoria em termos de número de multiplicações a realizar.
Portanto, a decomposição em fatores normais parciais tem como foco o tratamento otimizado de termos referentes a parte sincronizante do descritor Markoviano, ja que na técnica tradicional os mesmos pasSAN primeiramente por um processo de decomposição em fatores normais para serem tratados eficientemente.
Em esta seção serão mostrados alguns exemplos modelados com SAN para demonstrar as Vantagens de se utilizar a nova abordagem de otimização da multiplicação vetor-descritor, tendo em vista de que as matrizes que compõem cada termo a ser multiplicado por o vetor 71 são, na maioria das Vezes, grandes porém bastante esparsas.
O custo computacional apresentado na Seção 7.3 refere- se a multiplicação de um vetor (pi) por produtos tensoriais, que no descritor Markoviano são encontrados em duas partes distintas:
Em a parte local e na parte sincronizante.
Em o momento, apenas a parte sincronizante e interessante a nova tecnica, lembrando que a parte local do descritor Markoviano é representada por uma soma tensorial de matrizes, sendo possível expressas- la como uma soma de fatores normais, um tipo específico de produto tensorial.
Os algoritmos da tecnica tradicional são otimizados para tratar deste tipo de termo, logo o custo na abordagem tradicional nesta análise comparativa e o melhor para esta parte do descritor, o que não descarta a hipótese de serem realizadas novas otimizações sobre o método altemativo, buscando novas soluções também para este tipo de termoõ.
Para os exemplos a seguir, levaremos em consideração apenas o custo computacional para realizar a multiplicação do vetor (pi) por a parte sincronizante de cada um dos modelos, tendo em vista que neste trabalho assumese que o custo computacional da parte local é dado por a aplicação da tecnica tradicional.
A multiplicação de um vetor de probabilidades por o descritor Markoviano apresentado no exemplo dado na Seção 4.1.2 será analisado agora em termos de custo computacional, onde traçaremos novas analises comparativas entre as duas tecnicas abordadas neste trabalho.
SOtirnizar a multiplicação vetor-descritor altemativa para a parte local do descritor pode ser trivial se pensarmos que nada mais se tem do que uma matriz qualquer multiplicada por matrizes que são identidade, ou seja, a multiplicação ordinaria entre os elementos de cada matriz não necessitará ser realizada pois matrizes identidade são matrizes de zeros e uns.
De a mesma forma, pode- se calcular o custo computacional da parte sincronizante negativa, para cada evento ei em cada um dos N autômatos do modelo, considerando que cada termo é dado por, Tem- se:
Eventos sincronizantes a1 e ef:
Note que o custo envolvido em resolver um produto tensorial referente a um determinado evento c, na parte negativa é o mesmo custo computacional necessário em a¡ Jarte positiva.
Logo, para cada evento e, pode- se afirmar que o mesmo deve ser Multiplicado por dois.
Para a parte sincronizante do descritor Markoviano do exemplo, temos o seguinte custo computacional total:
Observa- se que mesmo para um exemplo simples e com poucos eventos sincronizantes, ha uma melhoria no número total de operações a realizar neste exemplo.
Cabe salientar, que na solução completa, diversas iterações são necessárias e para cada uma destas iterações haverá um ganho de 4 multiplicações.
A Figura 7.1 mostra um exemplo de Rede de Filas Aberta (QN), com apenas uma classe de clientes, composta de três filas com as seguintes características:
Uma das filas tem bloqueio para clientes da fila 1 para a fila 2;
Outra das filas da rede apresenta perda de clien es da fila 1 para a fila 3.
A Figura 7.2 e um modelo SAN equivalente a esta Rede de Filas de Espera.
Cada autômato A (m) representa uma fila.
Os estados acl «de um autômato A (m) indicam o número de clientes na fila i.
A chegada de clientes na fila 1, e a sua partida para a fila 2 e fila 3 sào representadas por eventos locais (11, mg and respectivamente.
Os eventos sincronizantes eu e 613 representam a passagem de clientes da fila 1 para a 2, e da fila 1 para a fila 3.
A ocorrência do evento el;
Determina a mudança de estados nos autômatos All) and A (m) simultaneamente.
O evento sincronizante 61g não pode ocorrer quando o autômato A (m) está no estado local 3 a) (fila 2 esta cheia).
Este representa o bloqueio da partida de clientes da fila 1 para a fila 2.
A transição extra que ocorre no último estado do autômato A (m) (evento 613) permite a saída de clientes da fila 1 sem que estes necessáriamente passem para a fila 3, representando assim uma perda de clientes (fila 3 esta cheia).
Parte Sincronizante do Modelo.
Para cada evento e¡ da parte sincronizante de cada um dos N autômatos do modelo, cálculouse os custos computacionais envolvidos para multilpicação destes termos por o vetor (pi), obtendo- se os seguintes resultados:
Observa- se uma redução do custo computacional ao utilizarmos a nova abordagem de multiplicaçãovetor descritor.
Salienta- se que na solução completa cada iteração terá um custo computacional dado por a soma do custo para a parte local mais a sincronizante.
Como dito anteriormente, a parte local a princípio sera calculada utilizando o método tradicional.
Em este exemplo o custo computacional para a parte local é 208 multiplicações, logo os custos computacionais totais são 712 e 544 multiplicações, respectixramente.
Em este exemplo, tem- se N processos compartilhando R recursos.
Cada processo é representado por um autômato A (m), que tem dois estados:
Si «(Sleeping) e Um (using).
Os recursos são representados por um autômato AW+ 1) que tem R+ 1 estados, indicando o número de recursos em utilização.
Os eventos ea¡ e em são eventos sincronizantes entre este ãutômato (Auwdl) e os autômatos que representam os processos.
Note que este modelo apresenta apenas eventos sincronizantes, logo não ha custo computacional relacionado a parte local do descritor Markoviano deste modelo.
Cabe salientar que o custo computacional que sera apresentado neste exemplo referese exclusivamente a parte sincronizante positiva.
A Figura 7.3 mostra a modelagem genérica deste problema, mas para fins de cálculo do custo computacional nas duas abordagens da multiplicação vetor-descritor assumirernos que n20 e R:
Para (tada evento sincronizante a7;
de a parte sincronizante de cada um dos N autômatos do modelo, em- se os custos computacionais associados.
Observe que neste modelo, cada evento sincronizante aparece num dos autôrnatos A (m) e no autômato A (m), ou seja em apenas dois autômatos, num total de 21 autômatos.
Logo as matrizes referentes a parte sincronizante deste modelo apresentam alto grau de esparsidade, permitindo comprovar a eficiência do novo método nestas condições.
conclui-seg através dos exemplos mostrados nesta seção, que devido a o fato de existirem matrizes muito esparsas, o custo computacional para a nova abordagem sofreu redução (em termos de número de multiplicações) em comparação ao método tradicional, justificando a utilização desta nova abordagem em casos onde as matrizes apresentam alto grau de esparsidade.
Considerações Finais. O processo basico do formalismo SAN é a multiplicação vetor-descritor, e dentro deste contexto, buscou- se nesta dissertação, diferentes maneiras de tratar numericamente modelos SAN, de forma a fornecer subsídios para a elaboração de novos métodos que executem este produto eficientemente.
O tratamento numérico otimizado da multiplicação vetor-descritor e de fundamental importancia no estagio atual de desenvolvimento das técnicas e ferramentas que manipulam modelos Markovianos em formalismos estruturados (por exemplo, SAN), Com o aumento gradativo da complexidade dos sistemas, torna- se essencial o aprimoramento das tecnicas tradicionais ou mesmo o estudo de novas abordagens na busca da solução estacionaria e transiente de tais modelos.
A multiplicação vetor-descritor foi reestruturada numa nova tecnica que chamamos de Decomposição Aditioa em Fatores Normais Parciais, de forma que os produtos tensoriais que compõem o descritor Markoviano pudessem ser decompostos utilizando multiplicações ordinárias entre os elementos das matrizes, reduzindo o custo computacional observado na abordagem tradicional.
Verificando o custo computacional ao utilizarmos a decomposição em fatores normais tradicional (descrita na Seção 5.4), percebe- se que para os casos onde a esparsidade das matrizes é alta, na nova técnica há uma considerável diminuição no número de operações a realizar.
Um fator importante e que cada termo pode ser tratado individualmente, ou por o Shame, ou por o Slice, já que o cálculo do custo computacional é simples, podendo este ser avaliado antes do tratamento de cada termo.
Novas otimizações ainda podem ser realizadas tendo em vista que em termos que ja são naturalmente fatores normais (como ocorre na parte local do descritor Markoviano), a multiplicação vetor-descritor podera descartar as multiplicações dos elementos da matriz por a matriz identidade, pois é óbvia a obtenção de sua matriz resultante em termos algorítmicos.
Vimos também que para a nova abordagem e interessante ordenar as matrizes do produto tensorial por os seus número de elementos não-nulos, de forma que a última matriz do termo seja a de maior nzi.
Isto faz com que hajam ainda maiores reduções no total de operações a serem realizadas, Em pesquisas recentes destaca- se a possibilidade de resolver modelos (esta tecnica inclusive ja é implementada) utilizando um métorlo de agregação algélbrica de autômatos, que consiste em transformar um modelo SAN com N autômatos num modelo SAN com G autômatos, sendo G N. Em a agregação algébrica, os estados do autômato resultante da agregação representa todas as combinações de estados possíveis e atingíveis dos autômatos que foram agregados.
O espaço de estados resultante e o produto do espaço de estados de cada autômato envolvido, bem como o numero de estados atingíveis, é o produto dos estados atingíveis de cada autômato agregado.
Esta técnica muitas vezes pode significar o aumento da ordem de cada uma das matrizes, e consequentemente o número de elementos não-nulos.
Para a nova abordagem esta aplicação pode não ser vantajosa, porem cabe aqui salientar a necessidade de novos estudos direcionados ao aproveitamento simultâneo de tais otimizações já conhecidas.
Desta forma, em trabalhos futuros sera interessante fazer um estudo comparativo do uso da nova tecnica e da tradicional em modelos agregados.
Ainda é possível ressaltar como trabalhos futuros a paralelização da execução destes métodos, ou a elaboração de novas abordagens que contemplem esta característica.
Tendo em vista que a tendência hoje é a distribuição do procesSANento, o custo computacional seria proporcionalmente divido entre os nós de procesSANento estabelecidos.
A única preocupação seria definir como as operações da multiplicação vetor-descritor poderiam ser paralelizadas sabendo que nos métodos iterativos existe uma dependência intrínseca entre as operações.
Em este sentido, a nova decomposição, por ser aditiva, e mais adequada ã parale ização.
Além de isto a manipulação de cada termo parcial atua sobre um conjunto reduzido de elementos do vetor de probabilidades, podendo facilitar a sua manipulação num ambiente para elo em que seja necessária a comunicação de dados.
Finalmente, é importante salientar que o trabalho desenvolvido nesta dissertação representa um avanço consistente na otimização do tratamento numérico de modelos markovianos estruturados.
Este avanço se da de forma organizada, pois parte da definição de uma nova propriedade da álgebra tensorial (Decomposição Aditiva em Fatores Normais Parciais).
A sua aplicação:
Natica implica na definição futura de um algoritmo (o algoritmo Slices) que pode trazer uma redução de custo computacional se associado ao tradicional algoritmo Shuffle.
Alem dos ganhos numéricos, este novo método abre a possibilidade de um tratamento ainda mais eficiente devido as facilidades de paralelização que decorrem da decomposição em fatores que podem ser adicionados.
