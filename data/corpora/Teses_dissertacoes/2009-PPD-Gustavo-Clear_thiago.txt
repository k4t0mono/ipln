A impressão digital de documentos vem se tornando cada vez mais eficiente ao passar dos anos, o que provocou a criação de uma nova tendência:
A personalização de documentos.
Com a finalidade de suprir esta necessidade foram criadas linguagens para a descrição de conjuntos de documentos personalizados (jobs) e processos para permitir a impressão correta de tais jobs.
Um dos processos que se destaca em termos de custo computacional é a rasterização de documentos, realizado sobre uma fila de jobs.
Em o ambiente de impressão tradicional, algumas estratégias foram introduzidas para aumentar o desempenho desta fase através do emprego do uso de técnicas relacionadas ao processamento paralelo e distribuído.
Entretanto, tais estratégias apresentam diversos problemas, de os quais o mais grave é relativo à impossibilidade da garantia de um balanceamento de carga justo para quaisquer seqüências de jobs.
Assim, este trabalho vem a propor novas estratégias para aumentar o desempenho da fase de rasterização, através da análise do perfil dos jobs, para que então seja possível utilizar os recursos disponíveis de uma maneira mais eficiente.
Para tanto, são propostas métricas que avaliam o custo computacional de cada job e ferramentas para permitir o escalonamento destes, de forma a superar o ganho de desempenho das estratégias existentes no âmbito da fila como um todo.
Palavras-chave: RIP, PDF, rasterização, métricas, escalonamento, balanceamento de cargas.
Estrutura do trabalho O Processo de Impressão Ambiente de Impressão Tradicional.
Estratégias de Rasterização Existentes Composição de um Documento PDF Escalonamento Não Determinístico Abordagem Proposta Custo Computacional Total Resultados Obtidos Análise de Desempenho dos Escalonadores Conclusão e Trabalhos Futuros.
1 Introdução O surgimento de impressoras digitais permitiu a evolução da área de publicação a novos patamares.
Com estes dispositivos, a impressão de documentos de alta qualidade deixou de ser um empecilho para os usuários, podendo, então, ser realizada de forma eficiente.
Este foi o primeiro passo para a consolidação de uma tendência emergente:
A personalização de documentos.
Em o passado, uma única instância de documento era produzida para um grande número de recipientes, ou seja, a mesma mensagem era passada para todos.
Com a introdução da personalização, o desejado era adaptar os documentos de forma que diferentes mensagens fossem transmitidas para os correspondentes recipientes.
Em este sentido, procedimentos automatizados para a criação e transformação de documentos se tornaram necessários, com a finalidade de suprir a demanda existente.
Uma nova disciplina -- Variable Data Printing (VDP) -- foi introduzida, provendo diversas técnicas, tecnologias, conceitos e padrões para permitir a criação de documentos com conteúdo dinâmico.
Diversas ferramentas foram desenvolvidas para auxiliar o designer a criar um template, de o qual serão geradas diferentes instâncias de documentos.
Desta forma, o mesmo layout é aplicado sobre diversas informações, gerando um job constituído por um conjunto de documentos personalizados (variáveis).
Em este âmbito, linguagens capazes de apresentar o grau de flexibilidade necessário foram desenvolvidas, permitindo a definição de áreas estáticas e dinâmicas de um documento, assim como a formatação de seus respectivos conteúdos.
Em o cenário atual, a maioria das impressoras não é capaz de interpretar estas linguagens, portanto se tornam necessários diversos processos para possibilitar a impressão de forma correta dos documentos.
Dois destes processos são a renderização e rasterização, que tem como objetivo final fornecer ao dispositivo de impressão o documento no formato necessário para que se consiga imprimir- lo de forma correta.
Com a introdução do VDP, empresas especializadas sobre cada página dos documentos personalizados.
Tal fato, acaba provocando um alto custo computacional para o processamento destes documentos.
De uma forma geral uma PSP gerência uma fila de n jobs iniciais a serem processados e novos jobs podem ser inseridos na fila a qualquer momento.
Assim, o que deseja- se atingir é o melhor desempenho possível no âmbito da fila como um todo e não de apenas um job.
Para tal propósito, é comum a adoção de uma impressora com uma alta capacidade de processamento, em alguns casos sendo capaz de imprimir até 1 página por segundo.
Em este cenário, todas as atividades relacionadas à preparação dos jobs devem ser finalizadas num espaço de tempo limitado, para que não ocorra a subutilização da impressora disponível.
Além disso, PSPs geralmente utilizam impressoras em paralelo para aumentar o consumo dos documentos pertinentes a um dado job.
De esta maneira, o desempenho das fases de pré-processamento deve aumentar proporcionalmente para manter- las continuamente trabalhando.
Com o intuito de amenizar a alta demanda por desempenho trazida por o VDP, passou- se a adaptar as linguagens de descrição de documento, como o Portable Document Format (PDF), objetivando oferecer conteúdos re-utilizáveis.
Até mesmo novos formatos ad-hoc foram A utilização destes formatos reduz a quantidade de conteúdo a ser rasterizada.
Isto se deve ao fato de que as Raster Image Processing (RIP) engines (ou simplesmente RIPs), aplicações responsáveis por realizar a rasterização dos documentos, são capazes de capturar a reusabilidade oferecida e, desta forma, processar os objetos uma única vez, aplicando o resultado obtido à medida de o necessário.
Entretanto, como mencionado anteriormente, a variabilidade dos documentos personalizáveis pode ser muito alta, portanto o recurso da re-usabilidade não consegue suprir por completo a demanda de desempenho existente, pois todas as porções variáveis devem ser rasterizadas uma a uma.
Em este sentido, está claro que o tempo e vazão da fase de rasterização tem repercussões no processo de impressão como um todo, podendo prejudicar a competitividade das PSPs no seu mercado.
Em trabalhos passados foram apresentadas estratégias para elevar a vazão do processo de renderização realizado por a engine, através da aplicação de técnicas de alto desempenho.
Similar ao conceito de utilizar impressoras em paralelo para consumir o job de entrada rapidamente, se propôs o uso de engines de renderização em paralelo.
Em o contexto do processo de rasterização, estratégias existentes obtêm um aumento de desempenho através do emprego de sistemas paralelos e distribuídos.
Contudo, as soluções aplicadas no cenário de rasterização, na maioria das vezes, não proporcionam um desempenho satisfatório, devido a diversas limitações e problemas que podem ser explorados.
Este fato motiva um estudo mais aprofundando de como melhor aproveitar os recursos existentes para maximizar o desempenho obtido em tal fase de processamento.
Em o cenário de rasterização de documentos nenhuma informação é conhecida sobre os jobs da fila e, portanto, estes são distribuídos sem uma maior preocupação quanto a o balanceamento de carga.
Em este sentido, as estratégias existentes para a rasterização de documentos aplicam um algoritmo simples para o escalonamento das tarefas entre os RIPs disponíveis, de maneira que à medida que um de eles esteja livre, este receberá a primeira tarefa da fila.
Esta distribuição simples não oferece nenhuma garantia de uma distribuição de cargas igualitária, podendo provocar a sobrecarga e sub-carga dos rasterizadores.
Frente a isso, o tempo de rasterização de todos os jobs da fila torna- se insatisfatório e acaba por apresentar comportamentos imprevisíveis.
De esta maneira, o ambiente de rasterização das PSPs, torna- se não confiável, causando a necessidade de estimar o término do processamento dos jobs da fila sempre como o pior caso possível.
O objetivo deste trabalho trata- se, então, da criação e aplicação de estratégias de escalonamento que garantam uma distribuição de cargas melhor.
Com isso, o intuito é o de melhorar o desempenho da fase de rasterização sobre a fila de jobs inteira, provocando um uso mais inteligente e justo dos recursos disponíveis.
Para atingir tal objetivo, diversos passos intermediários tornam- se necessários.
Entre eles a criação de métricas capazes de estimar o custo computacional associado a cada job.
Em este sentido, uma ferramenta capaz de prover as informações necessárias sobre cada job para a aplicação das métricas definidas deve ser desenvolvida.
Com o uso desta ferramenta e das métricas definidas, será possível realizar a análise do perfil dos jobs, fornecendo assim detalhes suficientes para a aplicação de algoritmos de escalonamento com eficiências melhores do que o já utilizado.
Em a análise realizada neste trabalho, se está concentrando no formato PDF, por tratar- se de um formato difundido na descrição de documentos personalizados.
Em este sentido, os resultados apresentados poderão ser aplicados sobre um conjunto de job totalmente descritos por o formato PDF, ou sobre formatos de abstração de níveis mais altos, que utilizam tal formato para descrever o conteúdo de seus documentos, como o Este volume organiza- se como segue:
O Capítulo 2 descreve alguns aspectos ligados à construção de documentos personalizáveis;
O Capítulo 3 discorre sobre algumas características principais do formato PDF, explicando a composição de um documento descrito por tal formato;
O o desenvolvimento das estratégias propostas;
O Capítulo 5 demonstra as abordagens propostas, explicitando as métricas propostas e as novas estratégias de escalonamento utilizadas, assim como as ferramentas auxiliares empregadas para a construção destas;
O Capítulo 6 trata sobre o ambiente de teste utilizado para efetuar a validação das métricas propostas sobre jobs reais retirados do cenário de mercado, em conjunto com uma análise de desempenho sobre a ferramenta auxiliar utilizada para a análise do perfil dos jobs e das novas estratégias de escalonamento, comparando- as com as já existentes;
Por fim, o Capítulo 7 conclui o estudo e apresenta possíveis pesquisas futuras a serem realizadas a partir de os resultados obtidos neste trabalho.
O processo de impressão reúne diversas técnicas, tecnologias, padrões e fases de processamento que permitem a obtenção de mais flexibilidade, eficiência e eficácia na criação de documentos.
De esta maneira, possibilita- se às PSPs a atenderem mais facilmente os seus clientes e a conseguirem lidar com suas necessidades.
Este capítulo expõe alguns dos aspectos envolvidos no processo de impressão, explicitando os passos e fases existentes desde a criação de um documento personalizável até a sua impressão.
Além disso, são apresentadas as estratégias existentes no âmbito de rasterização de documentos, visando explicitar suas principais vantagens e desvantagens.
A impressão de dados variáveis ou Variable Data Printing (VDP) se trata de uma tendência para possibilitar a criação de documentos com conteúdo dinâmico.
Também conhecido como Variable Information Printing (Vip), Personalized Printing, Database Publishing, entre outros, VDP traz diversas facilidades para a construção de documentos flexíveis.
Com o uso de VDP, o designer pode definir áreas personalizáveis de um documento que correspondem às porções variáveis deste.
Contrastando com a simples impressão digital, ao invés de se criar um documento contendo uma única mensagem para diversos clientes, com VDP é possível utilizarse um documento para passar tantas mensagens quanto necessárias, personalizadas para cada cliente.
Portanto, a grande vantagem do uso de VDP é o que se pode chamar de personalização em massa.
A Figura 1 ilustra a dinamicidade que pode ser encontrada num documento com conteúdo personalizável, tal como variações de formatação, texto, imagens ou até mesmo de sua própria estrutura interna.
VDP foi adotado por diversas empresas, por facilitar o processo de criação dos documentos.
Tratando- se de situações reais de mercado pode- se citar o caso das Olimpíadas de Sydney, em que os resultados da competição foram publicados através do uso de VDP.
Diversas outras podem ser observadas no cotidiano, como, por exemplo, a fatura de um banco.
Esta possui sempre o mesmo layout, mas difere no conteúdo, mais especificamente nos dados pessoais e financeiros de cada cliente.
De esta maneira, o banco pode definir um único documento com áreas personalizáveis, onde estas correspondem as informações que diferenciam- se de um cliente para o outro.
Outro exemplo muito comum que se pode citar é o caso dos encartes de supermercados.
Faculdade Y Texto Variável 1 Universidade X Job Profiling Variable Data Printing Texto Variável 2 Fulano de Tal Thiago Dissertação Dissertação Dissertação Porto Alegre Alegre Alegre Menores Preços!
Menores Preços! Caderno Imagem Variável 1 Não Perca Esta Chance!
Texto Variável 1 Texto Variável 2 Lápis de Cor A partir de Imagem Variável 2 Menores Preços!
Não Perca Esta Chance!
Espumante X Água 5 Litros Não Perca Esta Chance!
Este tipo de documento possui sempre a mesma estrutura:
Figuras de produtos espalhados na página com seus respectivos preços.
Assim, o designer pode fixar as posições das imagens e de seus preços, mas considerar que tanto um quanto outro são porções variáveis num documento.
Com isso, o template criado pode ser reutilizado para a construção de diversos encartes de produtos diferentes.
Contudo, para que VDP seja realmente aplicável à construção de documentos mais flexíveis são necessárias linguagens para descrever as partes variáveis e estáticas destes.
Entretanto, a maioria das impressoras não são capazes de lidar diretamente com estas linguagens, gerando a necessidade da existência de fases de pré-processamento.
Dois processos pertinentes ao contexto do pré-processamento na impressão de dados variáveis são a renderização e a rasterização de documentos.
Apesar de normalmente serem conhecidos como sinônimos, o propósito de cada um é distinto, mas complementar, no sentido de que a saída gerada por um é utilizada como entrada por outro.
A renderização refere- se ao processo de interpretação de uma determinada linguagem de formatação não usual (ou óbvia) a um leitor humano, objetivando apresentar o conteúdo por ela encapsulado de forma inteligível e usual.
Em este escopo, são utilizadas técnicas de Prettyprinting, que preocupam- se em ilustrar um determinado conteúdo da melhor forma possível para os usuários finais (Figura 2).
Através da aplicação da renderização a um documento personalizável cru, ou seja, descrito por linguagens de formatação com uma apresentação não óbvia a um leitor humano, é produzido um documento com parágrafos, fontes, cores, figuras, margens, páginas, numeração, etc..
Já definidos, de acordo com o especificado por a linguagem de forma-tação utilizada.
A aplicação responsável por realizar este processo é a engine de renderização.
O resultado do processo de renderização é um documento descrito por uma linguagem de alto nível de abstração, pois acaba provendo portabilidade ao documento, tornando possível sua visualização ou impressão em diferentes dispositivos.
Em este documento resultante, boa parte do conteúdo é representado por gráficos vetoriais que independem de resolução, significando que caso se deseje visualizar mais ou menos detalhes de tal conteúdo, não haverá perda de qualidade.
Entretanto, estes gráficos vetoriais não fornecem a informação de como o dispositivo deve apresentar o conteúdo, mas dispõem de uma abordagem genérica para representar- lo.
O processo de rasterização, também conhecido como RIPping (Raster Image Processing), se trata da interpretação de um documento descrito através de gráficos vetoriais para um formato bitmap, ou seja, para uma matriz de pontos (com suas correspondentes cores), que indicam corretamente ao dispositivo de impressão em questão, como apresentar o conteúdo descrito no documento (Figura 3).
Em este contexto, as chamadas RIP engines (Raster Image Processor engines), ou simplesmente RIPs, são os responsáveis por realizarem a rasterização dos documentos.
Esta seção apresenta o ambiente de impressão empregado numa PSP tradicional que utiliza a técnica de VDP.
Em este sentido, são explicitados dois componentes principais:
O fluxo de trabalho, através da técnica de VDT (Variable Data Templates) e arquitetura de hardware, especificando como são dispostos os componentes para realizar o processamento necessário para a impressão dos jobs da fila.
Variable Data Templates Uma técnica amplamente utilizada no cenário de VDP é a VDT.
Esta propõe um fluxo de trabalho (Figura 4), com o objetivo de padronizar o processo de criação e impressão dos documentos, tornando- o confiável, previsível e aumentando a vazão de documentos impressos.
A implementação deste fluxo não estabelece o uso de uma tecnologia específica, podendo se adaptar a qualquer uma que realize a função necessária.
Existem três fases básicas:
Criação, combinação e processamento final.
A fase de criação gera como saída um template de documento que descreve o layout a ser empregado, além de elementos fixos e estáticos em ele contidos.
Em a segunda fase, deste template são instanciados diversos documentos contendo dados específicos provenientes de uma base de dados.
Cada instância é repassada para um processo de formatação, renderização e rasterização (que em conjunto compõem a terceira fase), gerando documentos prontos para serem impressos.
É importante ressaltar que os três processos pertencentes à última fase são complementares:
Primeiramente o documento é formatado por uma determinada linguagem, logo após é aplicado o processo de renderização e, por fim, para que este seja impresso, faz- se o processo de rasterização.
Diversas vantagens são obtidas através da aplicação deste fluxo de trabalho segmentado:
As PSPs são capazes de lidar mais facilmente com impressões de uma grande quantidade de dados, devido a a existência de fases bem definidas e corretamente especificadas;
Separou- se o design do conteúdo;
Permite- se a composição do layout independentemente da disponibilidade dos dados num primeiro momento;
Liberdade para aplicar a tecnologia que melhor se adapta à necessidade de cada empresa, além de o fluxo ser facilmente extensível.
Arquitetura Empregada Levando-se em consideração o uso da técnica de VDT, pode- se apresentar uma visão geral da arquitetura de hardware empregada neste cenário.
A Figura 5 ilustra os diferentes passos sobre os quais os documentos são submetidos para que finalmente possam ser impressos, demonstrando uma visão geral do hardware que pode ser utilizado.
Instâncias de Documentos Servidor de Banco de Dados Template Doc..
Doc. 11 Designer Documentos com Alto Nível de Abstração Renderizadores Documentos Formatados Doc..
Doc. 11 Doc..
Como mencionado anteriormente, o designer será o responsável por estebelecer um template de documento contendo as diretivas de layout e apresentação.
Este template, então será combinado com registros do banco de dados gerando as diferentes instâncias dos documentos personalizados.
Em este sentido, as PSPs dispõem de um servidor de banco de dados contendo as informações personalizadas que serão acopladas a cada documento.
Com isso, os documentos são repassados a uma fase de formatação.
Esta formatação normalmente é realizada de forma automatizada numa estação, onde para cada instância existente, diversos estilos de texto são aplicados.
Partindo dos documentos formatados em mãos, passa- se à fase de renderização, onde geralmente realiza- se a renderização de forma centralizada.
Entretanto, pode- se aplicar neste passo estratégias de renderização paralelas, como apresentado em trabalhos passados.
O resultado da renderização, então, será o conteúdo dos documentos descritos através de uma linguagem de alto nível de abstração.
Considerando- se que é comum a existência de jobs com milhares de instâncias de documentos, aqueles resultantes da renderização serão arquivos muito grandes, em os quais um único job poderá ocupar Gigabytes de disco.
Sendo assim, estes serão transmitidos via redes de alta vazão ou através de mídias portáteis, como o DVD (Digital Versatile Disc) para uma estação contendo diversos RIPs.
Em este contexto, os jobs são armazenados num diretório de uma aplicação escalonadora, que é visível por todas unidades de processamento.
O escalonador dirá, então, qual job ou porção deste deverá ser processado por cada RIP.
Cabe ressaltar que antigamente os RIPs estavam presentes dentro de as impressoras, mas estes foram trazidos para fora devido a motivos de escalabilidade.
Por fim, o resultado da rasterização será repassado para as impressoras disponíveis por os próprios RIPs.
PDL (Page Description Language), também conhecido como PCL (Printer Control Language), se trata de uma linguagem de alto nível de abstração, utilizada para descrever o conteúdo de uma ou mais páginas.
Através do emprego deste tipo de linguagem, o conteúdo de um documento pode ser descrito através de comandos específicos, ao invés de um mero conjunto de pontos.
De esta maneira, obtém- se um nível de abstração mais alto, não prendendo o documento a formatos dispositivos específicos.
Entre os formatos PDL mais conhecidos pode- se citar o PS (Postscript), que é considerado por muitos como uma linguagem de programação completa, o PDF, que nada mais é do que uma linguagem baseada no PS, e o Hewlett Packard Printer Control Language (também conhecido simplesmente como PCL).
Cabe ressaltar que devido a o fato do PDL prover uma linguagem de um alto nível abstração, os dispositivos de impressão e de apresentação não são capazes de reconhecer estas linguagens.
Assim, uma conversão deve ser realizada para apresentar o documento no dispositivo desejado.
Este processamento, então, será realizado por as engines de rasterização.
As estratégias existentes num ambiente de rasterização tradicional baseiam- se em sistemas paralelos e distribuídos para aumentar a vazão e desempenho de tal fase.
Em este sentido, diversos RIPs são aplicados em conjunto para rasterizar uma dada fila de jobs de forma paralela.
Desta forma, através da análise das estratégias é possível verificar as vantagens e desvantagens existentes.
Em este contexto, são aplicadas duas maneiras para rasterizar os jobs da fila de entrada:
Alocar um RIP por job;
Alocar todos os RIPs para um único job;
Alocar um número fixo de RIPs por job.
Cabe ressaltar que no cenário das estratégias existentes os jobs são disponibilizados num diretório visível por todos os RIPs, que realizam a leitura necessária para a execução do processo de rasterização.
Por outro lado, o resultado do processamento de cada RIP é escrito no disco local, para então ser transmitido para sua impressora associada.
Através do emprego da primeira estratégia, cada RIP existente irá processar um job inteiro (Figura 6).
Em este contexto, uma aplicação escalonadora irá repassar um job para cada RIP livre e à medida que cada RIP terminar o seu trabalho, este irá requisitar mais tarefas para o escalonador.
Este cenário funciona bem para jobs pequenos, com alta re-usabilidade, permitindo assim que cada RIP consiga tirar vantagem do recurso da re-usabilidade e possa finalizar a computação de sua tarefa em tempo hábil para continuamente alimentar as impressoras.
Entretanto, se houver jobs grandes onde o tempo de processamento seja muito alto, é provável que não seja possível realizar o processamento com o desempenho desejado, prejudicando o processo de impressão como um todo.
Além disso, caso o tamanho dos jobs variar muito (o que é comum), ocorrerá a sub-carga e sobrecarga das unidades das engines de rasterização, o que por sua vez também afetará o ganho de desempenho obtido.
Por outro lado, outra desvantagem presente nesta estratégia específica, é o fato de que diversos RIPs podem não ser utilizados à medida que a quantidade de jobs na fila for menor que o número de RIPs disponíveis.
Estes RIPs, então, quebrarão os jobs gerando suas porções e processando- as.
Portanto, a quebra propriamente dita dos jobs em fragmentos não será feita por o escalonador, que apenas definirá quais são as páginas de cada fragmento, mas sim por cada RIP.
Uma desvantagem encontrada nesta estratégia é a quebra do recurso da re-usabilidade entre porções, pois a divisão dos jobs em arquivos distintos acaba impossibilitando o uso deste recurso entre eles.
Além disso, há a possibilidade de ocorrer um desbalanceamento de carga entre as unidades de processamento prejudicando o ganho de desempenho que poderia ser obtido, pois não existe garantia de que o esforço gasto para rasterizar diferentes porções seja o mesmo.
Finalmente, a terceira estratégia estipula o uso de um número fixo r de RIPs para cada job da fila.
A Figura 8 exemplifica esta estratégia considerando a situação em que cada grupo comporta três RIPs (r $= 3).
Esta configuração é baseada no fato de que os jobs &quot;padrões «das PSPs necessitam do número especificado de recursos para manterem as impressoras continuamente trabalhando.
Assim, diversos grupos de r RIPs estarão disponíveis para processar os jobs e cada um será direcionado para um grupo distinto.
Em esta configuração, cada job será quebrado em r fragmentos e novamente, a quebra propriamente dita será realizada por os RIPs, não por o escalonador, que apenas definirá as páginas de cada porção.
Como qualquer configuração estática, esta só irá funcionar da maneira esperada caso as PSPs imprimam jobs que se encaixem no perfil de jobs utilizado.
Caso contrário, o número de RIPs alocados para os jobs pode não ser o suficiente para manter as impressoras continuamente alimentadas.
Além disso, uma grande desvantagem apresentada nesta estratégia é o fato de que um conjunto de RIPs somente será alocado para um novo job, assim que todos os RIPs estejam livres, ou seja, no momento em que o job atual for processado por completo por o grupo.
Portanto, diversos RIPs de um grupo podem ficar parados, enquanto outros do mesmo grupo não finalizaram a computação de sua porção, sub-utilizando os recursos disponíveis.
O PDF (Portable Document Format) é um formato altamente difundido de entre as PSPs para a descrição de documentos, com uma riqueza maior em termos de recursos visuais, do que formatos antigos como o PS.
Além disso, tal formato está numa constante evolução para atender a novas necessidades dos usuários, o que torna este formato atual e aplicável a situações que os demais formatos não conseguiriam enfrentar.
Este capítulo vem a apresentar as principais características do formato PDF, objetivando fornecer uma visão geral sobre os elementos que o compõem.
PDF é uma PDL utilizada para descrever gráficos de um documento desenvolvida por a empresa Adobe R.
Este formato provê uma representação que independe de software, hardware ou sistema operacional utilizado para criar- lo, ou do dispositivo que será empregado para apresentar o conteúdo por ele descrito.
Abaixo são explicitadas algumas características e vantagens fornecidas por o formato PDF:·
Portabilidade: O PDF foi desenvolvido para ser portável para quaisquer plataformas.
Um documento PDF é armazenado como um arquivo binário, ao invés de ser representado como texto puro, evitando a tradução de caracteres nativos a determinados sistemas operacionais, como caracteres de fim de linha, acentos, etc.· Compressão:
Documentos PDF suportam padrões difundidos de compressão, como JPEG 13], entre outros, permitindo a redução do tamanho em disco do PDF.·
Gerenciamento de Fontes: Com o uso de PDF é possível adicionar ao documento diversas fontes de distintos formatos e acoplar aquelas que não possuem restrições de copyright diretamente no documento, possibilitando a apresentação do documento, tal como foi criado, em quaisquer outras plataformas.
Além disso, num documento PDF estão contidos descritores de fontes para cada fonte empregada no documento.
Estes descritores provêem métricas e estilos das fontes empregadas no documento, possibilitando a seleção de um conjunto de fontes que se assemelham as primeiras, caso estas fontes originais não existam no sistema operacional utilizado.·
Acesso Randômico: Devido a a maneira de representação de dados num documento PDF, a ordem da descrição das páginas ou objetos no arquivo não é importante, pois se utiliza um sistema de referências.
Isto pode ser vantajoso para qualquer documento interativo ou para uma aplicação que necessite ler o documento numa ordem arbitrária.
Em este contexto, uma tabela (chamada de cross-reference table), que contém a localização de páginas e outros objetos importantes no PDF, está sempre presente no final do arquivo, possibilitando o acesso direto a tais elementos.·
Segurança: Um documento PDF pode ser criptografado (ou assinado digitalmente) e decriptografado por diversos meios, seja com uma chave pública e privada ou através de um meio biométrico, como uma impressão digital.·
Extensibilidade: O formato PDF foi desenvolvido com o intuito de ser inteiramente extensível, possibilitando a criação de novas características que venham a se tornar interessantes ou necessárias para os usuários.
Além disso, o PDF proporciona para aplicações baseadas em versões mais antigas do PDF, a capacidade de lidar elegantemente com recursos que elas não compreendem.
Cabe ressaltar que algumas destas vantagens já existiam num formato, o qual o PDF é baseado:
O PS (PostScript).
Em este contexto, o PDF herdou alguns outros atributos de tal formato, como o fato de que um documento será descrito através de uma série de objetos.
Estes objetos em conjunto irão ser os responsáveis por a descrição da aparência de uma ou mais páginas.
Desta forma, para uma melhor compreensão do formato PDF, a seguir são apresentados aspectos relacionados à estrutura de um documento PDF e os objetos que o compõem, assim como suas características principais.
Em esta seção serão abordados aspectos relacionados a um documento PDF, mais especificamente quanto a sua estrutura interna.
Em este sentido, apresentam- se dois níveis distintos de especificação:
Quanto a estrutura do arquivo e quanto a estrutura do documento.
O primeiro diz respeito a um aspecto mais físico, descrevendo como os objetos são armazenados no arquivo PDF, assim como estes são acessados.
Já o segundo trata sobre um nível de abstração mais alto, relatando como os objetos são utilizados para representar os elementos de um documento PDF, como por exemplo, páginas, fontes, etc..
Estrutura do Arquivo Um arquivo PDF comum é composto por quatro elementos básicos, que possuem funções distintas para a representação de conteúdo.
Estas partes constituíntes estão dispostas fisicamente como apresentado na Figura 9.
Em o elemento cabeçalho é descrita apenas uma linha, que indica a versão da especificação do PDF, a qual o arquivo utiliza.
Esta linha emprega a seguinte sintaxe:
O símbolo%, a palavra PDF seguida de um hífen e a versão utilizada.
Em este contexto, um arquivo que foi descrito através da especificação 1.7 do PDF, por exemplo, terá o seguinte cabeçalho:
Cabe ressaltar que a partir de a quinta versão do PDF, o cabeçalho pode ser substituído por a entrada Version numa estrutura chamada de document catalog, que tratase de um elemento chave num documento PDF.
De esta maneira, a especificação torna- se atualizável, adicionando mais flexibilidade ao documento.
Evidentemente, uma aplicação capaz de interpretar uma dada versão do PDF, também conseguirá compreender as respectivas versões mais antigas.
O elemento seguinte ao cabeçalho trata- se do corpo.
Esta é a porção principal do documento, onde será descrito o conteúdo propriamente dito do arquivo PDF.
Em tal elemento estarão os objetos representando os componentes do documento, como fontes, páginas, imagens, textos, etc..
Assim sendo, o corpo irá englobar uma série de objetos, que podem estar descritos numa ordem arbitrária qualquer.
Como mencionado anteriormente, a cross-reference table é uma tabela que contém a localização dos objetos dentro de o arquivo.
Esta tabela possui uma entrada para cada objeto indireto no documento.
De esta maneira, cada linha sua apontará para a localização de um objeto em questão no corpo do arquivo, evitando a leitura do documento inteiro para a obtenção deste.
Por fim, o elemento rodapé possui a localização da cross-reference table e de alguns objetos especiais.
Frente a isto, a aplicação que está interpretando o documento deve ler o documento a partir de o final, podendo obter informações sobre componentes de interesse rapidamente.
Evitase, então, a realização de buscas custosas para encontrar os elementos desejados.
Em este contexto, é importante mencionar que um dos objetos que podem ser encontrados através do rodapé, trata- se da estrutura document catalog.
Estrutura do Documento Como mencionado anteriormente, um documento PDF trata- se de uma série de objetos que compõem a aparência de sua (s) página (s).
Em este contexto, tais objetos estão organizados de forma hierárquica, definindo desta maneira, como deverá ser interpretado o documento.
Para uma melhor visualização de tal hierarquia, a Figura 10 ilustra os objetos com suas respectivas posições em tal estrutura, assim como as relações existentes entre estes elementos.
Em o topo da hierarquia está o document catalog.
Este elemento trata- se de um dicionário de dados, que contém referências para as páginas do documento, através da estrutura page tree.
Assim sendo, é a partir deste elemento que poderá ser obtido, posteriormente, o conteúdo de cada página do documento.
Além disso, neste catálogo estão presentes diretivas de formatação do documento, como:
O layout das páginas (uma ou duas colunas), a configuração de contorno de cada uma de estas (via o elemento outline hierarchy), se devem ser mostradas fotos miniaturas de cada uma das páginas, se o documento deve ser visualizado em tela cheia, etc..
Como mencionado anteriormente, a partir de a versão 1.4 do PDF, no document catalog também pode estar presente a versão da especificação PDF empregada, inutilizando a informação fornecida no cabeçalho do documento.
As páginas do documento PDF são acessadas através de uma estrutura conhecida como page tree, que define a ordem das páginas e contém referências aos objetos que possuem o conteúdo de cada página.
Através desta estrutura, aplicações são capazes de apresentar as páginas do documento rapidamente, utilizando uma quantidade limitada de memória.
Em este contexto, a estrutura page tree possui dois tipos de nodos:
Nodos intermediários (page tree nodes) e nodos folhas (page nodes).
Os page tree nodes são utilizados para especificar atributos em comum entre as páginas.
Estes atributos são relacionados a apresentação de cada página, como por exemplo, o seu tamanho, se a página deve ser rotacionada, etc..
Em este sentido, as características especificadas para cada page tree node são atribuídas a todos os seus filhos.
É importante ressaltar, que os page tree nodes não estão necessariamente relacionados a estrutura lógica do documento, ou seja, estes elementos podem não representar capítulos, seções e assim por diante.
Os nodos folha, page nodes, são as estruturas que representam as páginas propriamente ditas.
Estes elementos são dicionários, que especificam os atributos de uma página.
Entre os atributos existentes, devem ser ressaltados os seguintes:
Uma referência para o page tree node pai, o tamanho da página (suas dimensões de altura e largura), uma referência para o conteúdo da mesma e outra para os recursos utilizados na página.
Por fim, o conteúdo de uma página trata- se de uma unidade auto-contida, sendo definida através de um elemento chamado de content stream, que utilizará se necessário objetos externos ao seu escopo, com o emprego de uma estrutura conhecida como resources.
Por unidade auto-contida se quer dizer que, todos os objetos necessários para a definição do conteúdo da página em questão deverão estar propriamente definidos no objeto content- stream, ou devidamente referenciados via o correspondente elemento resources.
O content stream, então, trata- se de uma sequência de instruções descrevendo os elementos a serem pintados na página.
Estas instruções são descritas através de um operador e de um operando.
O primeiro trata- se de uma palavra reservada do PDF especificando uma ação a ser tomada.
O segundo nada mais é do que um objeto PDF ou uma referência interna ao mesmo.
Portanto, o operando deve ser um objeto direto, significando que não podem ser utilizadas referências a objetos externos ao par content stream e resources.
Por outro lado, pode ser necessário a utilização de tais objetos e para este fim é empregada a estrutura resources.
Esta estrutura conterá o mapeamento nome/ objeto externo, que poderá então ser referenciado através do atributo nome dentro de o content stream.
Deve ser ressaltado que enquanto o documento PDF trata- se de um sistema baseado em referências, no contexto de um content stream, as instruções serão lidas e executadas seqüencialmente.
O formato PDF utiliza uma sintaxe própria (disponível em), que é embasada num sistema de referências através do uso de diversos objetos.
Em este sentido, a especificação do PDF provê objetos para a representação de dados básicos, que podem ser utilizados em conjunto para a composição de objetos mais complexos ou apenas para auxiliar a descrição do conteúdo de forma mais flexível.
Além disso, o formato PDF dispõe de maneiras para repre- sentar e referenciar elementos, os quais serão aqueles desenhados nas páginas do documento:
Objetos Básicos O PDF provê suporte para a definição de tipos de objetos básicos, que em conjunto formam todas as demais estruturas aplicadas no documento.
Tais objetos podem ser utilizados de forma direta (objetos diretos), onde juntamente com a definição do objeto, já se está utilizando- o, ou de forma indireta (objetos indiretos), através da definição e utilização separadamente do mesmo.
Em o segundo caso, para que um objeto possa ser utilizado, ele deve ser referenciado e, portanto, a ele é atribuído um identificador único.
Este identificador é composto por um número inteiro positivo, chamado de número do objeto e um número de geração não negativo.
O número de geração refere- se a versão do objeto que se está utilizando, onde o valor inicial é sempre 0.
Caso este objeto seja atualizado por alguma aplicação, pode- se simplesmente atualizar tal número indicando a operação realizada.
Desta forma, a definição de um objeto indireto consistirá em seu identificador único em conjunto com os delimitadores obj e endobj, que irão indicar o início e o fim do conteúdo propriamente dito de tal objeto.
Por outro lado, a referência a um objeto indireto deve ser realizada através de seu identificador único e a palavra-chave R. A Figura 11 exemplifica os dois tipos de objeto mencionados, para uma melhor compreensão de suas composições.
Em o contexto da linguagem PDF, então, existem oito tipos básicos de objetos, cada um enquadrando- se na classificação de diretos e/ ou indiretos.
Estes são:·
Valores numéricos (Objetos numéricos);·
Nomes (Objetos nominais);·
Vetores;· Dicionários de dados;·
Objetos stream;·
Objeto nulo.
Os valores numéricos podem assumir valores inteiros ou reais.
Em a especificação do PDF, valores reais são representados através de um radical decimal e não podem ser descritos com o uso de radicais não decimais (como 5&amp; F F F E) ou em formato exponencial Para a representação de texto são empregadas as strings, que são objetos constituídos de uma série caracteres, representados como bytes.
Estes objetos podem ser descritos como strings literais ou strings hexadecimais.
As primeiras devem ser delimitadas por parêntesis, formando a e o caractere contra-barra(\).
Por outro lado, as strings hexadecimais devem ser delimitadas por os caracteres e\&gt;, podendo conter caracteres hexadecimais.
Em este contexto, cada par de caracteres hexadecimais representará um byte na string.
Os nomes são símbolos atômicos (sem nenhuma estrutura interna) unicamente definidos através de uma seqüência de caracteres.
Isto significa que, quaisquer dois nomes compostos por o mesmo conjunto de caracteres, representam o mesmo objeto.
Em este contexto, um nome é definido através do caractere inicial barra(/), mas este não é parte do nome, apenas indica que os caracteres seguintes representam tal tipo objeto.
Assim, um nome não é considerado como um texto que deverá ser apresentado para o usuário final, mas sim como um objeto interno ao documento.
Em o formato PDF, os vetores são elementos heterogêneos, ou seja, constituídos de qualquer combinação dos oito objetos explicitados.
Em este sentido, o PDF suporta diretamente a construção de vetores unidimensionais, delimitados por os caracteres.
Entretanto, devido a heterogeneidade provida por os vetores, pode- se construir- los de maneira multi-dimensional, fazendo com que elementos de um vetor sejam outros vetores, podendo estar aninhados a qualquer profundidade.
Um dicionário de dados representa um mapa contendo um par de objetos:
Uma chave e um valor.
Este par de elementos constituem uma entrada do dicionário.
Em este sentido, uma chave deve sempre ser um nome, enquanto o valor pode ser qualquer outro tipo de objeto (incluindo outro dicionário).
Para a definição deste tipo de objetos, deve- se utilizar os caracteres &quot;e «como delimitadores de um dicionário.
Cabe ressaltar que num dicionário, por convenção, existe sempre uma entrada Type que identifica o tipo do objeto que o dicionário representa.
Além disso, outra entrada Subtype pode ser utilizada para especificar uma categoria especializada do objeto especificado por o tipo em questão.
Um objeto stream trata- se de uma seqüência de bytes, que pode ser lida de forma incremental.
Este objeto não possui limitações quanto a o tamanho e é utilizado para representar conteúdos muito grandes, como imagens.
Assim, todas as streams devem ser objetos indiretos, sendo constituídas por um dicionário, através de o qual se especifica o número exato de bytes da stream em questão com a entrada Length, e do conteúdo do objeto propriamente dito, delimitado por as palavras-chave stream e endstream.
Por fim, o objeto nulo é um objeto especial para indicar a inexistência de algum valor.
Este objeto é único no documento PDF e pode ser utilizado através da palavra chave null.
Gráficos Gráficos são elementos utilizados por o formato PDF para representar o conteúdo de uma ou mais páginas.
Os objetos gráficos possuem diversas características que os tornam flexíveis e capazes de suprir diversas necessidades dos usuários para a definição de documentos de alta resolução.
Além disso, para estes elementos é disponibilizada uma estrutura com o intuito de definir propriedades em comum para um conjunto de gráficos, chamada de estado dos gráficos (graphics state).
Em esta seção são especificados os tipos de gráficos presentes no cenário do PDF, juntamente com suas funcionalidades e a seguir são descritos os recursos oferecidos por o estado dos gráficos, assim como as características gerais pertinentes a todos os objetos gráficos.
Tipos de Gráficos Como mencionado anteriormente, o conteúdo que define a aparência de uma página trata- se de uma seqüência de operadores e operandos utilizados em conjunto com os recursos da página em questão.
Em este contexto, operadores e operandos para a construção e o uso de objetos gráficos são utilizados no content stream de cada página.
Assim, existem 5 objetos gráficos distintos que podem ser aplicados:
Path objects, external objects (XObjects), inline image objects, shading patterns objects e text objects.
A seguir, serão descritas as características e funcionalidades destes objetos, ressaltando alguns operadores utilizados no contexto de cada um de eles.
Em este sentido, cabe lembrar que estes operadores são utilizados unicamente no contexto de um content stream de uma página e referem- se a palavras reservadas da linguagem PDF.
Os path objects representam caminhos que compõem formas arbitrárias, trajetórias e regiões, através do uso de retas e curvas cúbicas de Bézier.
Sendo assim, um caminho é formado por segmentos de linha retos ou curvos, podendo estar conectados ou ser disconexos.
De esta maneira, no formato PDF são fornecidas operações para os definir e preencher- los, assim como estabelecer regiões de corte com o seu uso.
Através de operações de definição de caminhos é possível descrever a geometria dos caminhos propriamente ditos, sem quaisquer restrições.
Permite- se, então, que um único caminho contenha regiões convexas, côncavas, áreas disjuntas ou regiões que se interseccionam.
Entre os principais operadores para a construção de um caminho estão:·
m: Dá início a construção de um caminho nas coordenadas x e y recebidas como parâmetro;·
l: Adiciona uma reta ao caminho em questão, partindo do ponto atual para outro ponto x e y recebido como parâmetro.
Nota- se que para a realização desta operação, deve- se ter realizado a operação m previamente;·
re: Adiciona um retângulo ao caminho atual com o ponto inferior esquerdo (coordenadas x e y), sua largura e sua altura definidos respectivamente por os operandos x, y, width e height passados por parâmetro.
As operações de preenchimento de um path object possibilitam desenhar de fato um caminho na página, seja através de funções para preencher as regiões explicitadas (fill) ou para pintar um contorno no caminho em questão (stroke).
Os principais operadores para tal função são:·
f: Preenche o caminho atual;·
s: Pinta um contorno no caminho em questão;·
b: Preenche e pinta um contorno no caminho;·
n: Finaliza um caminho sem preenches- lo ou contornar- lo (utilizado para regiões de corte).
Finalmente, as operações para estabelecer regiões de corte permitem que os caminhos sejam utilizados como regiões limites de corte para gráficos subseqüentes.
O operador principal para este tipo de operação trata- se do W, que modifica a região de corte atual, através de operações com o caminho definido.
Outro tipo de objeto existente no formato PDF trata- se do external object, que é uma referência a um objeto externo ao conteúdo (content stream) da página em questão, sendo então referenciado através dos recursos de tal página.
Estes objetos são streams, exclusivamente indiretas, e são pintadas na página através do comando De o.
O PDF conta com três tipos distintos de external objects:
Image XObjects, group (form) XObjects e postscript XObjects.
Para indicar qual o tipo do XObject que está definido, existirá sempre uma entrada Subtype no dicionário de suas respectivas streams indicando tal informação.
Em este contexto, os external objects são os objetos que podem ser re-utilizados no cenário de rasterização de um documento PDF.
A primeira referência a um dado external object será aquela realmente rasterizada por os RIPs, que armazenarão o resultado de tal processamento em sua cache.
As referências subseqüentes, então, não precisarão ser rasterizadas novamente, pois o RIP poderá apenas aplicar o resultado já armazenado.
Entre os tipos existentes de XObjects, as image XObjects dizem respeito a imagens que são inclusas no documento num formato bitmap qualquer, como JPEG, PNG (Portable Network Width e ColorSpace respectivamente, enquanto na sua stream estará o conteúdo da imagem propriamente dito.
Por outro lado, os group XObjects são objetos utilizados para definir propriedades em comum dos objetos em ele contidos.
Assim, na stream deste grupo estarão as definições dos objetos pertinentes a ele.
A especificação do PDF 1.
7, define a existência de apenas um tipo de group XObjects, os transparency group XObjects, que podem estabelecer a transparência dos objetos do grupo.
Finalmente, os postscript XObjects são utilizados para expressar comandos através da PDL PostScript, mas estes já não possuem uso efetivo no formato PDF e apenas são mantidos por questões de compatibilidade.
Os text objects tratam do texto de um documento.
Por possuírem maior importância e mais ampla utilização na descrição de conteúdo, são oferecidas algumas funcionalidades específicas para lidar com estes.
Em o âmbito de um objeto gráfico de texto, cabe ressaltar que o PDF diferência os termos caractere e caractere tipográfico.
O primeiro diz respeito ao elemento que representa uma letra, número ou qualquer outro símbolo presente quando se está escrevendo um texto.
O segundo refere- se ao desenho do caractere, que será pintado no documento, dependendo da fonte que será escolhida para um determinado conteúdo.
A distinção destes termos deve ser destacada, pois é de grande importância para a compreensão da interpretação feita por o PDF sobre objetos gráficos de texto.
Assim que encontrada uma porção de texto no documento, o PDF realiza um mapeamento de cada caractere para o caractere tipográfico correspondente.
Em termos de desempenho, este mapeamento pode ser de muito custoso, portanto o formato PDF provê operações de caching e reuso para contornar tal empecilho.
Tratando- se de funcionalidades específicas para textos, o formato PDF provê diversas operações, entre elas:
Ajuste do espaçamento entre caracteres, palavras e linhas;
Ajuste do tamanho horizontal e vertical ocupado por as palavras, e o posicionamento das palavras na frase, possibilitando a aplicação de palavras sobrescritas e subscritas.
Para tanto, são definidas diversas operações para os textos, de entre as quais devem ser destacadas as seguintes:·
BT: Operação que indica o início de um text object;·
Td: Move o objeto de texto atual para o ponto definido por os operandos x e y recebidos por parâmetro;·
Tf: Seleciona a fonte para o objeto de texto atual;·
Tj: Operação para realmente pintar o texto na página;·
ET: Operação que indica o final de um text object.
Os inline image objects são objetos auxiliares num documento PDF e não são muito comuns.
Estes são utilizados para definir uma imagem dentro de o próprio content stream da página.
Esta imagem possuirá diversas limitações, de as quais a mais marcante trata- se do tamanho da imagem.
Em este sentido, diz- se que a imagem em questão deverá possuir 4 KB ou menos de conteúdo, caso contrário não poderá ser utilizado este tipo de objeto para a definição de tal imagem.
Para a construção de uma inline image deve ser utilizado o operador Bi, dando início ao objeto, seguido das propriedades da imagem com o uso do par nome/ valor.
Logo após, os operadores Id e Ei serão empregados para delimitar o conteúdo da imagem em questão.
Os shading patterns objects também tratam- se de objetos não muito comuns em documentos PDF.
Estes provêem uma transição suave entre cores numa determinada área a ser pintada, independentemente de resolução de qualquer dispositivo de saída.
Em este sentido, sua cor é definida por uma função arbitrária.
Estes podem ser utilizados para que sejam obtidos efeitos em outras formas gráficas, como por exemplo, sombreamento e degradês.
Eles são objetos stream ou apenas dicionários, onde poderá ser encontrada a função necessária para pintar as cores especificadas.
Estes objetos são diretos e podem ser pintados através do operador sh seguidos de um dicionário/ stream.
Características Gerais e Estado dos Gráficos Uma estrutura muito importante no formato PDF trata- se do estado dos gráficos.
Em suma, este elemento mantém os parâmetros de controle para pintar os gráficos que se encontram no seu escopo.
Estes parâmetros definem uma série de informações sobre as quais os operadores dos gráficos são executados.
Desta forma, para os gráficos do conteúdo de uma página sempre existirá um estado dos gráficos corrente, que deverá ser considerado para apresentar os gráficos de tal página.
Em o content stream de cada página este estado dos gráficos atual é sempre reinicializado, passando a conter, então, valores padrões que serão explicitados no decorrer de esta seção.
Um documento PDF bem estruturado normalmente possui diversos elementos gráficos que são independentes um do outro.
Assim, diversas modificações no estado dos gráficos podem ser necessárias, sendo que pode ser desejado utilizar um mesmo estado dos gráficos várias vezes em diferentes pontos do documento.
Para tal propósito, o formato PDF provê uma maneira de salvar o estado dos gráficos atual, para que este possa ser restaurado assim que for preciso.
Previne- se, assim, a necessidade de estabelecer todos os parâmetros do estado dos gráficos toda vez que se deseje re-utilizar algum previamente definido.
Os operadores para salvar e restaurar o estado dos gráficos são respectivamente:
Q e Q. De esta maneira, para realizar estas operações sobre os estados dos gráficos é utilizada uma pilha LIFO (Last In First Out) (chamada de graphics state stack), onde sempre no topo da pilha estará o último estado dos gráficos salvo.
Em este contexto, à medida que se salvar o estado dos gráficos atual, este será colocado no topo da pilha.
Por outro lado, quando se restaurar um estado dos gráficos, aquele do topo da pilha será removido e atribuído ao estado dos gráficos corrente.
Através de tais operadores, então, será possível existir diversos estados dos gráficos no documento, mas vale a pena ressaltar que apenas um estará em uso.
Como dito anteriormente, este estado dos gráficos corrente possuirá diversos recursos que influenciarão no comportamento dos operadores dos gráficos, que serão descritos a seguir.
Um dos recursos mais importantes oferecidos por o estado dos gráficos trata- se da disponibilidade de um sistema de coordenadas especial.
Com o objetivo de garantir a portabilidade de um documento PDF, este formato utiliza um sistema de coordenadas próprio, chamado de user space.
Este sistema de coordenadas possui sempre a mesma relação com a página que está sendo considerada, independentemente do dispositivo de saída o qual se está tentando imprimir.
PDF, que irá utilizar a CTM correspondente para cada gráfico em questão.
Cabe ressaltar que o valor padrão de uma CTM de um estado dos gráficos é sempre uma sem quaisquer transformações.
Outra característica existente para a obtenção de documentos com mais recursos visuais trata- se da utilização de transparência.
Elementos gráficos num documento PDF foram definidos até a sua quarta versão através da utilização de um modo de imagem totalmente opaco, onde cada gráfico seria desenhado em seqüência, sobrescrevendo qualquer porção de outro gráfico a qual viesse a interseccionar.
A seguinte versão do PDF introduziu um modo de imagem transparente em que os elementos gráficos não necessariamente estariam totalmente opacos, permitindo que estes transparecessem.
De esta maneira, através da utilização de tal funcionalidade, tornou- se possível estabelecer uma quantidade arbitrária de opacidade para cada gráfico existente no documento.
A opacidade dos gráficos pode ser estabelecida através do estado dos gráficos, por meio de uma entrada do dicionário:
Alpha constant. Esta entrada nada mais é do que um objeto númerico (real), cujo valor pode variar de 0.0 à 1.0.
Em este contexto, o valor 1.0 representará objetos totalmente opacos, enquanto o valor 0.0, objetos totalmente transparentes.
Para a mudança do valor de opacidade do estado dos gráficos corrente, utiliza- se o operador gs.
Em este contexto, um estado dos gráficos inicial de uma página será definido como totalmente opaco, ou seja, com o alpha constant $= 1.0.
Em termos de definição de cores para cada elemento gráfico, a versão atual do PDF provê diversos modos distintos a serem utilizados.
Em este sentido, a grande vantagem oferecida por o PDF é que estes modos são totalmente independentes do dispositivo.
Para a apresentação do conteúdo descrito no documento, tais modos de cores são convertidos para o modo de cor presente no dispositivo em questão.
Desta forma, o usuário poderá aplicar os modos de cores que o convém, sem ter a preocupação de qual deve ser utilizado num ou outro dispositivo específico.
Entre alguns dos modos disponíveis no PDF, estão o RGB (Red Green Blue) e o CMYK (Cyan Magenta Yellow Key).
Por outro lado, o formato PDF não restringe a utilização somente de modos de cor pré-definidos, permitindo ao usuário a criação de seus próprios, para então aplicar- los aos gráficos desejados.
Os modos de cores devem ser especificados no estado dos gráficos, na entrada color space, que, então, será aplicado para os gráficos subseqüentes.
Vale a pena mencionar que o PDF permite o uso de diversos modos de cores distintos num mesmo documento.
Além disso, é no estado dos gráficos que estará a cor que deve ser aplicada, quando pintando text objects e path objects.
Em o contexto de cores para o estado dos gráficos, o modo de cor padrão para um estado dos gráficos trata- se do RGB e a cor padrão é o preto.
Durante o desenvolvimento de uma versão paralela de um programa diversas características devem ser levadas em consideração para maximizar o desempenho que se deseja adquirir.
Em este contexto, uma das primeiras preocupações é a escolha do melhor tamanho de tarefa (grão) a ser transmitido e/ ou processado por cada unidade ativa de processamento.
Em termos de unidades de processamento que possuam memória distribuída, a escolha do grão é um aspecto crítico para o bom funcionamento da aplicação.
Caso se defina um grão ruim, o overhead existente para a transmissão ou quebra das tarefas pode não compensar o ganho de desempenho obtido através da divisão do trabalho.
Em este sentido, pode- se afirmar que esta escolha deve ser baseada nas características específicas da aplicação e da arquitetura, buscando por a relação ótima entre a quebra das tarefas, em conjunto com o custo de comunicação, e o ganho de processamento obtido através da paralelização.
Assim que o grão estiver definido é necessário estabelecer a melhor estratégia de distribuição das tarefas disponíveis entre os processos, de maneira que nenhum destes fique sobrecarregado ou sub-carregado.
Caso a estratégia escolhida não represente uma boa escolha, a eficiência da paralelização pode ficar diretamente comprometida.
A Figura 14 demonstra uma situação em como a distribuição das tarefas pode influenciar diretamente no ganho ou perda de desempenho potencial que poderia ser encontrado.
Em esta figura, pode- se notar que com uma distribuição seqüencial), em a qual associam- se as tarefas uma a uma para cada máquina de forma circular (a primeira tarefa para a máquina 1, a segunda para a máquina 2, a terceira para a máquina 3 e assim por diante), tem- se uma carga total na máquina 3 de 35.
Considerando que o processamento de toda a fila de tarefas será dito como finalizado, assim que todas as máquinas terminem suas tarefas, este valor (sendo o maior existente) representa tal momento.
Por outro lado, buscando uma distribuição igualitária), onde a carga de cada tarefa é levada em consideração a priori a sua distribuição, tem- se uma carga total de 23, diminuindo então o tempo de finalização de todas as tarefas.
De esta maneira, a procura por a divisão ideal de tarefas carateriza o problema de escalonamento.
Em este contexto, o objetivo final é uma configuração de distribuição de tarefas de forma que os recursos envolvidos sejam aproveitados da melhor maneira possível, buscando por a otimização de uma medida de desempenho arbitrária.
Em este capítulo serão abordados aspectos relacionados ao problema de escalonamento, fornecendo uma base teórica para a implementação de uma aplicação paralela capaz de superar os desempenhos obtidos com as estratégias de rasterização existentes.
Em este contexto, são apresentadas:
Uma notação para o discernimento de situações em que se deve aplicar distintas estratégias de escalonamento, uma classificação para os problemas de escalonamento e alguns algoritmos de escalonamento existentes, assim como aspectos relacionados diretamente a suas respectivas eficiências.
Graham et al.
Introduziram a notação| com a finalidade de agrupar problemas de escalonamento com características em comum, permitindo, assim, uma análise mais objetiva de aspectos que influenciam cada grupo.
Em esta notação, cada campo representa uma característica diferente quanto a o problema de escalonamento que deve ser considerada.
O campo refere- se ao ambiente de execução da estratégia de escalonamento, relacionado diretamente ao hardware disponível.
Alguns dos valores possíveis são apresentados a seguir:·
Máquina única:
Existe apenas uma máquina no sistema;·
Máquinas paralelas e idênticas (P m):
Existem m máquinas idênticas em paralelo no sistema.
A o omitir- se o valor m se estabelece que o número de máquinas é arbitrário, ou seja, o grupo comporta qualquer número de máquinas idênticas (sendo este valor maior do que 1);·
Máquinas não relacionadas (Rm):
Existem m máquinas em paralelo no sistema, mas cada uma de elas pode processar as tarefas em tempos distintos.
Isto significa que as máquinas podem possuir hardware e velocidades distintas.
Novamente, pode- se omitir o valor m para se referenciar um número arbitrário qualquer de máquinas.
O campo diz respeito às peculiaridades de cada tarefa e restrições da estratégia de escalonamento que devem ser respeitadas.
Entre os valores possíveis para este campo estão:·
Preempção (pmtn):
Tarefas podem ser preemptadas e resumidas mais tarde, possivelmente numa máquina diferente.
Se preempções são permitidas, este valor estará incluso no campo, caso contrário tal operação não é permitida;·
Restrições de precedência (prec):
Este campo especifica que existem tarefas da fila que devem ser completadas antes que determinadas tarefas iniciem seu processamento.
Caso prec não esteja especificado no campo, as tarefas não estão sujeitas a restrições de precedência;·
Prazos (dj):
Se este símbolo estiver presente, cada tarefa j deve ser completada até o seu prazo final dj.
Caso, contrário as tarefas não necessitam ser completadas até um determinado prazo.
Por fim, o campo se trata da medida de desempenho que se deseja otimizar com a aplicação da estratégia de escalonamento.
Tais medidas estão sempre relacionadas ao tempo para completar as tarefas.
Em este contexto, algumas definições são utilizadas para formalizar as subseqüentes medidas de desempenho:·
Cj: Denota o tempo para completar a tarefa j, sendo que este tempo refere- se ao momento em que a tarefa j foi inserida na fila, até o seu processamento por completo;·
Tj: Trata- se do atraso encontrado para a tarefa j, em o qual Tj $= max.
Com o uso destas definições, então, algumas medidas de desempenho que podem ser descritas no campo em questão são:·
Makespan (Cmax):
Se refere ao tempo para finalizar todas as tarefas de uma determinada fila.
Em este sentido, ele pode ser definido como o max;·
Maximum lateness (Lmax):
Trata- se do maior atraso para as tarefas da fila.
Assim, esta medida será calculada como max;·
Total tardiness (Tj):
É o atraso encontrado nas em todas as tarefas da fila, sendo computado através do somatório dos atrasos de todas as tarefas.
O escalonamento pode ser classificado quanto a duas situações distintas, dependendo da disponibilidade de informações sobre as tarefas:
Determinístico e não determinístico.
A seguir são apresentadas particularidades sobre cada uma destas categorias, juntamente com alguns tipos de algoritmo a elas associados.
Escalonamento Determinístico Técnicas de escalonamento pertinentes a esta categoria são aquelas em que todas as características das tarefas e as relações entre cada uma de elas são conhecidas previamente à execução da aplicação.
Em este contexto, destacam- se dois tipos de algoritmos:
Ótimos e sub-ótimos.
Algoritmos ótimos preocupam- se em encontrar a melhor solução para o problema de escalonamento, baseando- se num conjunto de informações sobre as tarefas da aplicação.
Entretanto, na maioria dos casos para que tal solução seja de fato descoberta, necessita- se de uma quantidade muito grande de informações.
Além disso, sabe- se que o caso geral e diversos particulares dos problemas de escalonamento são Np-completos.
Contudo, para alguns poucos casos existem algoritmos com a capacidade de resolver o problema de escalonamento em tempo polinomial, os quais conseguem apresentar soluções ótimas.
O caso ideal seria o emprego de algoritmos ótimos para todas as situações, desta maneira se teria a garantia que sempre a melhor configuração, em termos de divisão de tarefas, seria utilizada.
No entanto, este tipo de algoritmos pode ser muito custoso em termos de desempenho e quantidade de informações necessárias, além de apenas existir para um pequeno conjunto das situações.
Devido a este fato, os algoritmos sub-ótimos buscam uma solução através do uso de técnicas específicas, como a heurística, sem nenhuma garantia que é a melhor para o dado problema, mas sabe- se que a solução encontrada é ao menos próxima da ótima.
Apesar disso, com o uso destes podem- se obter respostas num tempo polinomial, o que não seria possível em diversos casos com o emprego de um algoritmo ótimo.
Em este sentido, algoritmos sub-ótimos tem a capacidade de englobar uma gama muito maior de resoluções para problemas do que os ótimos, além de conseguirem solucionar os problemas num tempo significativamente menor.
Escalonamento Não Determinístico O escalonamento é dito não determinístico quando não se possui todas as informações necessárias sobre as tarefas antes da execução do programa paralelo.
De esta maneira, este tipo de escalonamento provê uma decisão sobre como distribuir as tarefas durante a execução do pro-grama e por isso acarreta um overhead na própria aplicação que deve ser minimizado.
Em este contexto, dois tipos de algoritmos devem ser destacados:
Dinâmicos e estáticos.
Algoritmos estáticos são aqueles realizam uma análise sobre todas as tarefas antes de distribuir qualquer uma, possibilitando assim, a obtenção da melhor maneira de realizar o escalonamento para o todo o conjunto em questão.
É importante ressaltar que esta análise é realizada durante a execução do programa e por isso se enquadra na categoria de escalonamento não determinístico.
Esta abordagem torna- se vantajosa, caso o processamento desta análise consiga ser realizado num espaço de tempo que seja compensado por o ganho de desempenho obtido através da estratégia de escalonamento estabelecida.
Contudo, existem casos em que não é possível realizar a análise de todas as tarefas em conjunto, por o fato de que todas elas não estão disponíveis ou porque o custo desta análise é muito grande.
De esta maneira, algoritmos dinâmicos propõem a intercalação do processo de análise com a distribuição das tarefas.
Em outras palavras, se realiza a análise apenas de um conjunto de tarefas, estas são distribuídas para as unidades ativas de processamento, que iniciam a computação, para então se partir para a análise de outro conjunto.
Desta forma, podese afirmar que a partir de um certo ponto, o processo de escalonamento passa a ser concorrente com o cálculo do custo computacional de cada tarefa.
Considerando a notação|, de entre os problemas existentes na área de escalonamento, um dos fundamentais é denominado P m| Cmax, que diz respeito ao escalonamento de um makespan, ou seja, do tempo para completar a última tarefa (Cmax).
Este problema mostrouse como Np-difícil, provocando a impossibilidade de criação de algoritmos ótimos para resolver este problema.
Esta seção apresenta características relacionadas a este problema de escalonamento, descrevendo alguns algoritmos empregados nesta situação para a resolução de tal problema.
Análise de Competitividade Como mencionado anteriormente, o problema de escalonamento P m| Cmax é Np-difícil e portanto diversos algoritmos foram desenvolvidos a partir de heurísticas para a obtenção de um resultado o mais perto de o ótimo possível.
Em este sentido, uma abordagem comumente aplicada para a avaliação de algoritmos sub-ótimos no contexto de problemas Np-difíceis é empregada para analisar suas respectivas eficiências.
Tal método trata da distância do resultado do algoritmo em questão, considerando o seu pior caso, em relação a o ótimo.
Em o cenário de escalona-mento esta abordagem é chamada de análise de competitividade.
Para estabelecer a competitividade de um algoritmo de escalonamento A qualquer, algumas formalizações são necessárias.
Denota- se que f (A, I) é o resultado do escalonamento produzido por o algoritmo A sobre a fila de tarefas de entrada I. Com isso, f representa a medida de desempenho que se deseja otimizar.
Em este trabalho considera- se que f $= Cmax.
Além disso, estabelece- se que a solução ótima para o escalonamento é obtida quando A $= OP T.
Então, partindo destas constatações, pode- se dizer que o algoritmo A é c-competitive caso f (A, I) c f (OP T, I), para qualquer I. Em outras palavras, um algoritmo é c-competitive se no seu pior caso, este é capaz de gerar um resultado que se afasta até c vezes do ótimo.
Por outro lado, é importante ressaltar que um algoritmo com uma competitividade melhor do que outro não necessariamente gerará um melhor resultado, mas apenas será garantido que o resultado, no seu pior caso, não ultrapassará a competitividade especificada.
Algoritmos de Escalonamento Atualmente, diversos algoritmos podem ser encontrados para resolver o problema P m| Cmax.
Entre estes, os algoritmos mais difundidos são:
List Scheduling (Ls), Largest Processing Time first (LPT) e Multifit.
Esta seção discorre sobre o funcionamento de cada um destes algoritmos, com o objetivo de apresentar suas vantagens e desvantagens, ressaltando suas competitividades.
List Scheduling O algoritmo Ls foi introduzido por Graham na década de 60 e tornou- se a base para o desenvolvimento de diversos outros algoritmos de escalonamento.
Além disso, a partir deste algoritmo, descobriram- se outros problemas na área de escalonamento, como a consideração de restrições de precedência para as tarefas da fila.
Em este algoritmo, determina- se que para uma fila de tarefas organizadas em qualquer ordem, deve- se sempre transmitir a primeira tarefa para uma máquina ociosa qualquer.
A Figura 15 apresenta o funcionamento desta abordagem sobre uma fila de tarefas arbitrária.
Em o exemplo apresentado na Figura 15, deve- se ressaltar que uma tarefa é associada a uma máquina somente quando tal máquina estiver ociosa, ou seja, assim que esta terminar por completo sua tarefa atual.
Devido a tal fato, pode- se notar, por exemplo, que a máquina 1, recebe uma nova tarefa somente após terminar sua primeira, que ocorre no tempo 9.
Sendo assim, a nova tarefa recebida por a máquina 1 será a 7, pois as anteriores foram distribuídas para as outras máquinas à medida que estas terminaram suas respectivas tarefas.
Considerando a análise competitiva para m máquinas, este algoritmo é (2 -- m) competitive, significando que sua competitividade piora à medida que mais máquinas são utilizadas.
Largest Processing Time First Com o intuito de melhorar a competitividade do algoritmo Ls, Graham propôs a construção de um novo algoritmo conhecido como LPT.
Para tanto, Graham focou na tentativa de prevenção do pior caso do algoritmo Ls, que ocorre quando a última tarefa é aquela com a maior tempo de processamento de.
Em este sentido, o algoritmo LPT introduz a ordenação das tarefas de forma decrescente, fazendo com que aquelas com maiores tempos de processamento estejam entre as primeiras posições de.
Feito isso, aplica- se o algoritmo Ls sem nenhuma outra alteração.
De esta maneira, evita- se o processamento de tarefas grandes ao final do escalonamento, amenizando o efeito de perda de desempenho provocada por o não balanceamento de cargas grandes entre os processos.
Com esta simples mudança, o algoritmo LPT representou um grande avanço em relação a o Ls, apresentando uma eficiência (4 -- 3m) competitive.
como se pode notar, a competitivi3 dade deste algoritmo fica pior à medida que mais máquinas são introduzidas no escalonamento.
Multifit A partir de o estabelecimento do LPT, um dos primeiros algoritmos a apresentar um ganho realmente significativo foi o Multifit.
Este algoritmo foi baseado em técnicas de bin-- packing, que tratam do empacotamento de n tarefas, com cargas quaisquer, num número finito de bins (pacotes) de forma que o número de bins utilizado seja o menor possível.
O problema de bin-- packing é Np-completo e portanto diversas heurísticas foram estabelecidas para resolver este problema.
Em este sentido, o algoritmo Multifit utiliza a heurística First-Fit Decreasing (FFD) para agrupar as tarefas da fila em até m bins, considerando que a carga de um bin trata- se do tempo de processamento das tarefas em ele contidas.
A estratégia FFD funciona da seguinte maneira:
Organiza- se as tarefas conforme seus tempos de processamento de forma decrescente numa sequência;
Cada tarefa da sequência é adicionada no bin com a menor carga de entre todos, de forma que a capacidade do bin não ultrapasse um limite pré-definido C;
Os passos 1 e 2 são executados até que nenhuma todas as tarefas estejam num bin.
Em este contexto, para encontrar o bin-- packing ótimo, o limite C teria que ser o makespan ótimo para melhor configuração de bins possível, ou seja, C $= max (l (Bi)), onde l (Bi) denota a carga do bin Bi, para 1 i m..
Entretanto, para a situação mencionada descobrir max (l (Bi)) é tão difícil quanto descobrir a menor divisão das tarefas em até m bins.
Para que um bom (sub-ótimo) limite C seja encontrado em tempo polinomial, métodos de busca iterativa são empregados, estipulando um C inicial e refinando este valor ao longo de a computação.
De esta maneira, a cada iteração da busca será executada a técnica FFD, considerando o C atual.
A busca será encerrada caso k iterações sejam atingidas.
De entre os métodos de busca existente, utiliza- se a busca binária no algoritmo Multifit.
O limite C é obtido através da média de um limite inferior, Cl, e outro superior Cup, que respectivamente referem- se ao melhor caso e pior caso possíveis para o empacotamento das tarefas da fila.
O limite inferior é inicializado com o makespan ótimo para, calculado através do máximo entre carga total da fila de tarefas dividido por m e a maior carga das tarefas Tk.
Por outro lado, o limite superior é obtido através do máximo entre o dobro da carga total da fila de tarefas dividido por m e a maior carga das tarefas em.
Definido este valor inicial de C, passa- se para a busca binária do menor C possível até que k iterações sejam atingidas.
Em este sentido, uma iteração corresponde a execução do algoritmo FFD considerando o C atual.
Assim, no contexto de uma iteração, caso o resultado do algoritmo FFD, sejam mais de m bins, Cl receberá o C atual e Cup será mantido.
Caso o resultado do FFD sejam até m bins, refina- se o valor do limite superior, onde Cup receberá o C atual e se manterá o limite inferior.
Com isso, passa- se para a próxima iteração, onde C receberá novamente uma média entre Cl e Cup.
Em o momento, em que k iterações sejam atingidas, Cup conterá o menor valor para C encontrado nesta busca binária.
Caso na busca binária não seja encontrado nenhum C, tal que o FFD correspondente gere até m bins diz- se que C será o limite superior inicial.
Em ambos os caso através da aplicação do FFD mais uma vez, com a capacidade C prédeterminada, poderão ser gerados até m bins.
Um exemplo da aplicação do algoritmo Multifit é ilustrado na Figura 18.
Como resultado final do algoritmo Multifit, então, serão obtidos até m bins com cargas similares, que poderão ser escalonados em até m máquinas.
A eficiência deste algoritmo mostrou- se(+ 2-k) competitive, para 1.176 1.22.
As estratégias existentes no âmbito da rasterização de documentos enquadram- se na classificação de problemas de escalonamento do cenário P m| Cmax, aplicando diferentes formas de quebrar os jobs para distribuir- los entre os RIPs disponíveis.
Em este contexto, pode- se perceber que todas estas estratégias utilizam o algoritmo Ls para a distribuição de tarefas.
Como descrito anteriormente, sabe- se que tal algoritmo foi um dos primeiros estabelecidos na área de escalonamento, não possuindo a melhor competitividade de entre os algoritmos existentes.
Além disso, pode- se afirmar que a natureza dos jobs das PSPs é diversificada, provocando a existência de um ambiente com uma ampla variação de cargas.
Assim, devido a nenhuma consideração quanto a o custo computacional das tarefas por parte de o algoritmo Ls, é provável a ocorrência de anomalias no desempenho de tais estratégias, observando- se comportamentos estranhos, como picos e quedas bruscas em curvas de tempo de execução.
Este fato, compromete as estratégias existentes, que não apresentam um comportamento previsível e confiável.
Com o intuito de melhorar a situação existente, pode- se aplicar algoritmos de escalonamento que utilizam heurísticas baseadas no custo computacional das tarefas, na tentativa de obter um balanceamento de cargas mais justo.
Em este cenário, pode- se dizer que se está lidando com o escalonamento não-determinístico dinâmico, pois nenhuma informação sobre as tarefas é conhecida previamente à execução do escalonamento e que tais tarefas podem ser inseridas na fila a qualquer momento.
Assim, propõe- se o emprego dos algoritmos LPT e Multifit, que são indicados para lidar com problemas P m| Cmax, por superarem a eficiência do algoritmo Ls em termos de competitividade, além de poderem ser empregados no problema de escalonamento não-determinístico e dinâmico.
Para satisfazer este objetivo, torna- se necessária uma maneira de estimar os custos computacionais de cada tarefa.
Portanto, é preciso definir métricas para analisar o perfil dos jobs, que poderão ser aplicadas através do uso de uma ferramenta capaz de extrair as informações necessárias.
Além de isto, para a aplicação dos algoritmos é interessante a distribuição de cargas não muito grandes, com a finalidade de evitar que alguns RIPs fiquem com jobs muito grandes enquanto outros processem jobs pequenos.
Assim, outra ferramenta auxiliar deve ser utilizada para quebrar os jobs em tarefas, de forma a diminuir o grão de trabalho.
Em este capítulo, então, é apresentado o trabalho desenvolvido para melhorar o desempenho das estratégias existentes.
De esta maneira, num primeiro momento são demonstradas as métricas obtidas para estimar o esforço que será realizado a fim de rasterizar um determinado job descreve- se a aplicação PDF Splitter que possibilita a quebra dos jobs em porções menores, com a finalidade de diminuir o seu grão.
Por fim, explica- se os escalonadores implementados, que utilizam as ferramentas e as métricas desenvolvidas.
Estes escalonadores aplicam os algoritmos LPT e Multifit na tentativa de melhorar o desempenho da fase de rasterização, onde para o LPT propõem- se uma otimização relativa a análise das tarefas realizada sobre os jobs, na tentativa de possibilitar ao escalonador uma distribuição mais imediata das tarefas aos RIPs ociosos.
Diversas métricas foram analisadas com o intuito de estimar o custo computacional de um job.
O primeiro passo para a definição destas foi a decisão de que tipos de objetos e características analisar.
Para tanto, é necessário conhecer um pouco mais sobre o funcionamento dos RIPs e sobre as características principais dos jobs das PSPs.
Como explicitado anteriormente (Seção 2.2), os RIPs são os responsáveis por a rasterização de um job PDF.
Em suma, a função do RIP é a de gerar uma imagem bitmap correspondente ao PDF de entrada.
O RIP executa esta função com base em páginas, o que significa que uma imagem é criada para cada página do PDF de entrada.
A o final de sua execução, um conjunto de imagens será obtido como resultado.
Para formar as imagens de saída, o RIP deve interpretar cada um dos objetos no PDF para pintar- los na imagem correspondente.
De entre estes objetos, aqueles sobre os quais será realizado o processo de conversão são os gráficos.
Os gráficos mais comuns nos jobs são dois:
Os textos e as imagens.
Em função de estas características, estes objetos gráficos, juntamente com as páginas, foram os utilizados para compor as métricas para avaliar o custo de um PDF.
A seguir, são apresentados os experimentos que indicam a validade das métricas propostas.
Estes resultados foram obtidos através da média de 20 execuções com o uso de um RIP open-, o ImageMagick converter, e normalizados, obtendo assim um fator de relevância para cada um destes.
Em estes experimentos, os PDFs foram rasterizados utilizando- se 300 Dpi (Dots Per Inch) de resolução.
Através dos experimentos realizados será possível estabelecer uma maneira para calcular o custo computacional associado a cada uma das métricas avaliadas.
É importante ressaltar que as métricas apresentam uma aproximação destes custos e não servem para prever valores exatos.
Além disso, através da aplicação de tais métricas será possível ter uma idéia do custo de uma página/ objeto.
Páginas O número de páginas de um documento PDF está diretamente relacionado ao número de imagens geradas no final da rasterização.
Desta forma, quanto maior for o número de páginas existentes, maior será a quantidade de operações de E/ S (Entrada e Saída) que deverão ser realizadas por os RIPs.
Em esta seção é apresentado o custo de páginas em branco (sem quaisquer objetos gráficos) com o intuito de verificar o seu impacto no processo de rasterização como um todo.
Para a realização dos experimentos a seguir, tomou- se como base diferentes tamanhos de páginas (com seus respectivos tamanhos denotados como &quot;largura x altura «em pixels):
Com a finalidade de estabelecer o custo computacional para a rasterização de uma página, observou- se o incremento do fator de relevância existente à medida que mais páginas foram adicionadas.
Em este sentido, diz- se que o custo de uma página será estabelecido como o incremento observado.
Está claro que quanto maior as dimensões ou área da página em questão, maior é o incremento no fator de relevância.
Esta situação está diretamente relacionada ao fato de que para páginas com áreas maiores, mais operações de E/ S (Entrada e Saída) deverão ser realizadas, resultando assim numa imagem final maior.
Assim, definiu- se que o custo de uma página será denotado por cP agtP ag, onde tP ag representa as dimensões da página correspondente.
Os custos aproximados obtidos para os experimentos em questão foram:
CP ag283 x416 $= estabelecer o custo cP agtP ag para um tP ag qualquer, tomou- se como base o menor custo obtido nos experimentos:
CP ag283 x416. Em este sentido, diz- se que areatP ag denota a área de um retângulo com as dimensões definidas por tP ag..
Com isso, se estabeleceu que o custo de uma area tP ag página de tamanho tP ag será obtido através da proporção de area283 x416, que resultará num fator que será multiplicado por o custo base.
Esta estratégia é apresentada na Equação 5.1.
CP ag283 x416 Devido a a relação direta da área da página em questão com a grandeza do incremento, a equação apresentada é capaz de aproximar o fator de relevância.
Assim, o custo de todas as páginas de um dado documento pode ser calculado através da soma do custo de cada página.
Considerando os custos individuais de duas páginas, é possível perceber que o custo de ambas pode ser obtido através do custo da soma de seus tamanhos.
Tal constatação pode ser generalizada, portanto, como demonstrado na Equação 5.2, diz- se que o custo de todas as páginas de um documento PDF é obtido através do custo da soma dos tamanhos das páginas (denotado como tP agT ot).
CP ag283 x416 Imagens Imagens são objetos fundamentais na criação de jobs para clientes de uma PSP, sendo utilizadas para inúmeros fins, entre eles a apresentação de logotipos, propagandas, produtos, idéias, projetos, etc..
Devido a o fato de que estas imagens são definidas através de uma matriz de pontos, uma grande preocupação por parte de as PSPs é a de evitar a perda de qualidade, durante a impressão dos documentos.
Assim, é comum a utilização de imagens com dimensões muito maiores do que o tamanho da página a qual estão inseridas, sendo redimensionadas para o tamanho desejado.
Desta forma, mais pontos por polegada (Dots Per Inch -- Dpi) poderão ser aplicados para formar a imagem em questão.
Este aspecto impossibilita o uso de inline image objects (Seção 3.2.2), devido a as limitações apresentadas, que impedem a definição de imagens com um Dpi desejável.
Em este sentido, para definir imagens são aplicados somente os image XObjects (Seção 3.2.2).
Os experimentos apresentados a seguir levam em consideração um documento PDF contendo apenas uma página de tamanho A4 e imagens de tamanhos distintos, definidas através da utilização dos image XObjects.
Os tamanhos de imagem selecionados foram quatro (descritos da seguinte forma &quot;largura x altura «em pixels):
1190x1684, 1785x2526, 2380x3368 e 2975x4210.
O primeiro tamanho de imagem escolhido se refere ao dobro das dimensões de uma página A4, o segundo ao triplo e assim consecutivamente.
Além disso, para compor estes experimentos, imagens distintas foram consideradas, como fotos (de alta resolução) e imagens artísticas (com degradês e efeitos gráficos), assim como o emprego da reusabilidade ou não para os objetos de imagens que as descrevem.
Diz- se que uma imagem não re-utilizável é uma primeira instância (referência) a um dado image XObject, enquanto imagens re-utilizáveis são aquelas instâncias subseqüentes as suas respectivas primeiras referências.
Os resultados apresentados a seguir se referem a uma média dos resultados individuais de cada uma das imagens consideradas (fotos e imagens artísticas).
Sem Re-usabilidade Para os experimentos apresentados nesta seção, cada objeto de imagem presente nos casos de teste é utilizado uma única vez, de forma a evitar a re-usabilidade.
O primeiro aspecto considerado na avaliação do custo computacional de uma imagem foi a sua cobertura (área de ocupação) na página.
Isto se deve ao fato de que a mesma imagem deverá ser pintada no arquivo bitmap de saída, portanto este experimento avalia se quanto maior a imagem desenhada na página do documento PDF, maior será o esforço computacional aplicado para gerar o bitmap de saída.
Para tanto, foram criados casos de teste sobre imagens, variando seu percentual de cobertura de 1% a 100%.
A Figura 20 ilustra os resultados obtidos.
Analisando o gráfico apresentado, pode- se perceber que a cobertura da imagem não é um fator relevante para a rasterização do PDF e por outro lado, pode- se constatar que as dimensões das imagens são relevantes para tal processo.
Isto ocorre devido a o fato de que o RIP deve interpretar o conteúdo da imagem, converter- lo para o formato de saída selecionado e, então, redimensionar- lo para que então a imagem seja pintada.
Com isso, quanto maior as dimensões da imagem em questão, mais processamento deverá ser realizado por o RIP, impactando no tempo de rasterização.
Em este sentido, para um mesmo tamanho de imagem, considerando- se o redimensionamento para 1% ou 100% da área da página, o RIP deverá interpretar e converter a mesma quantidade de conteúdo.
O primeiro aspecto que pode ser analisado trata- se do fato de que quanto mais objetos de imagem, maior se torna o fator de relevância.
Seguindo as constatações prévias, esta situação deveria ocorrer, pois isto representa mais conteúdo a ser processado por o RIP.
Outro fator que comprova esta afirmação está diretamente relacionado a que, quanto maior é a área do objeto inserido, maior é o incremento do fator de relevância.
A partir de as constatações obtidas, deve- se quantificar o custo de um ou mais objetos de imagem.
Em este sentido, diz- se que o custo de um objeto de imagem de tamanho tIm é representado como cImtIm.
Para calcular o custo cImtIm dos objetos imagens, tomou- se como base cIm1190 x1684, que se trata do custo da menor imagem considerada nos experimentos.
Analisando o gráfico ilustrado na Figura 21, pode- se notar que cIm1190 x1684 $= 0.019.
Observando o comportamento dos demais tamanhos de imagens, pode- se dizer que o incremento a cada objeto adicionado aumenta de acordo com o tamanho deste objeto, ou seja, quanto maior a área do objeto adicionado, maior será o incremento.
Em este contexto, o custo de uma imagem com um tamanho tIm qualquer pode ser calculado da mesma forma que o custo de uma página, ou seja, através de uma proporção da área base com a área representada por tIm resultando num fator que multiplicará o custo base.
Seguindo este raciocínio, os seguintes custos são obtidos:
CIm1785 x2526 $= 0.042, cIm2380 x3368 $= 0.076 e cIm2975 x4210 $= 0.119.
Tais custos se aproximam da realidade observada nos experimentos e portanto o custo de uma imagem é definido como apresentado na Equação 5.3.
CIm1190 x1684 Definido o custo de um objeto de imagem, o custo de todos os objetos de imagem de um documento PDF pode ser calculado a partir de a soma dos custos individuais de cada objeto.
Por outro lado, está claro que o tamanho de dois objetos é igual a soma dos respectivos tamanhos e, portanto, o custo destes objetos pode ser obtido através do custo da soma de seus tamanhos.
A partir de esta constatação o custo total poderá ser obtido, então, através do custo da soma dos tamanhos tIm dos objetos (representado aqui como tImT ot).
Esta equação é apresentada a seguir (Equação 5.4).
CIm1190 x1684 Partindo dos custos de objetos sem re-usabilidade então, foram realizados experimentos com a aplicação da transparência sobre estes, para verificar a importância desta característica.
A Figura 22 apresenta os resultados obtidos, considerando a variação de transparência sobre tais objetos.
Como pode- se notar, devido a a aplicação de transparência existe um grande crescimento no fator de relevância.
Isto ocorre devido a a necessidade do RIP de compor as cores dos objetos transparentes baseado na sobreposição das cores destes com as cores das páginas e objetos sobre os quais estão posicionados.
Além disso, no gráfico apresentado pode- se notar que quanto maior a área dos objetos transparentes em questão, maior será é o incremento no fator de relevância associado.
Tal fato decorre de que quanto maior a área do objeto transparente em questão, mais cores deverão ser computadas.
Então, para descobrir a grandeza do custo de um objeto transparente, definido como cImTtImT, pode- se adotar a estratégia de análise utilizada anteriormente:
Observar o incremento no fator de relevância.
Analisando o custo médio para os objetos com o menor tamanho dos experimentos podese concluir que cImT1190 x1684 $= 0.026.
Sabendo que o custo de um objeto transparente está relacionado diretamente a sua área, pode- se facilmente verificar que cImT1785 x2526 pode ser calculado através da multiplicação de um fator por o custo do menor objeto.
Assim, da mesma areatImT.
De forma que para objetos sem transparência, este fator pode ser calculado como area 1190x1684 forma análoga ao custo de todos os objetos não re-utilizáveis opacos, o custo destas imagens transparentes podem ser calculadas através da aplicação do tamanho de todos estes objetos (tImT T ot) na Equação 5.5.
CImT1190 x1684 usabilidade Os casos de teste apresentados nesta seção, possuem a definição de uma única imagem no contexto do documento PDF e esta é referenciada quantas vezes necessário.
Assim, desejou- se avaliar o impacto da re-usabilidade no custo computacional presente na rasterização de imagens.
Para tanto, casos de teste similares aos apresentados na Seção 5.1.2 foram criados, porém as imagens presentes nos documentos representam referências a uma instância de image XObject definida uma única vez.
Por exemplo, na situação em que o caso de teste contenha 8 objetos, 7 destes serão apenas referências ao primeiro.
Com isso, não são apresentados experimentos com um único objeto de imagem, pois nesta situação não há o emprego da re-usabilidade.
Os resultados obtidos são apresentados na Figura 23.
como se pode perceber, os resultados apresentados seguem o mesmo comportamento daqueles sem a re-usabilidade, entretanto o custo adicionado à medida que novos objetos são inseridos é menor.
Cabe ressaltar que existe um custo ao adicionar mais objetos, mesmo que re-utilizáveis, pois os RIPs têm de buscar o resultado da rasterização dos mesmos em sua cache, aplicando- o na imagem de saída.
Tal computação provoca a adição de um custo ao processamento total.
Em este contexto, diz- se que o custo de um objeto reutilizável será denotado através de cRetRe.
Tomando como base o custo do menor tamanho de imagem considerado, tem- se que cRe1190 x1684 $= 0.011.
Os demais custos de objetos para os tamanhos testados são aproximadamente:
CRe1785 x2526 $= 0.030, cRe2380 x3368 $= 0.042 e cRe2975 x4210 $= 0.065.
Em este sentido, pode- se perceber que novamente existe uma relação entre a área do objeto adicionado com o seu custo associado.
Mais uma vez, esta relação pode ser aproximada através da divisão entre a área base, area1190 x1684, por a área do objeto de tamanho tRe que se está analisando (areatRe).
Entretanto, pode- se afirmar que este crescimento pode ser aplicado no contexto da existência de re-usabilidade, portanto se está considerando que tRe diz respeito ao tamanho de um objeto re-utilizado, excluindo, assim, a primeira instância do mesmo.
Como visto anteriormente, o custo total dos objetos pode ser calculado na forma do custo da soma dos tamanhos.
Em este sentido, pode ser estabelecido que tReT ot refere- se ao tamanho total dos objetos re-utilizados.
A Equação 5.6 demonstra o custo para estes objetos.
CRetReT ot $= areatReT ot cRe1190 x1684 area1190 x1684 De a mesma forma que para os objetos sem re-usabilidade, alguns experimentos foram realizados considerando a aplicação de transparência para os objetos re-utilizáveis.
A Figura 24 ilustra os resultados obtidos.
O gráfico apresentado demonstra que para objetos re-utilizáveis transparentes, o mesmo aumento no fator de relevância pode ser observado.
Considerando- se os tamanhos de imagem apresentados, os seguintes custos aproximados foram obtidos:
CReT1190 x1684 $= 0.019 e cReT1785 x2526 $= 0.042.
Tomando como base o menor custo, mais uma vez o outro custo pode ser aproximado através da proporção entre suas respectivas área, multiplicando- se o resultado por o custo base.
Portanto, o custo total de objetos de imagens re-utilizáveis transparentes pode ser computado por o custo da soma de seus tamanhos (tReT T ot), como apresentado na Equação cReTtReT T ot $= areatReT T ot cReT1190 x1684 area1190 x1684 Textos Os textos são normalmente aqueles sobre os quais a personalização dos documentos é aplicada.
Com isso, nestes elementos estarão contidas as mensagens direcionadas para cada recipiente das PSPs.
Em o formato PDF, estes elementos são representados por os text objects, que foram instanciados para a criação dos experimentos desta seção.
A variabilidade considerada para estes objetos é representada por a quantidade de texto, tamanho de fonte e aplicação de transparência.
Além disso, todos os testes apresentados foram criados com tamanho (s) de página (s) A4 e com 70 fontes distintas, com a aplicação de variações de cada uma de elas, como negrito, itálico ou ambos.
Em este sentido, foram gerados 188 casos de teste para cada característica analisada.
Para fins de ilustração e visualização, 10 fontes foram selecionadas, que representam o comportamento observado em todos os casos de teste, mas uma listagem de todas as fontes empregadas estão disponíveis no Apêndice A. O primeiro aspecto considerado foi impacto no custo de rasterização ao aumentar a quantidade de texto num documento PDF.
Para tanto, define- se que um objeto de texto corresponde a uma frase no documento.
A Figura 25 apresenta os resultados obtidos.
Pode- se notar que existe uma flutuação nos valores obtidos à medida que mais objetos são adicionados.
Entretanto, estes resultados não apresentam nenhuma tendência de crescimento ou decréscimo.
Os mesmo valores foram obtidos quando variou- se o tamanho das fontes empregadas.
Portanto, pode- se dizer que a quantidade de texto e o tamanho da fonte não é impactante no custo da rasterização do texto.
Por outro lado, a presença de texto por si só influência no tempo de rasterização, pois se comparado ao testes de páginas em branco (Seção fonte aplicada, nota- se diferentes influências.
Analisando o gráfico apresentado, pode ser visto que destacam- se três grupos distintos em termos de custo computacional.
Em este sentido, define- se que estes grupos serão referenciados, da sua maior influência para a menor respectivamente, como:
Pesado, Médio e Leve.
Em o primeiro grupo está presente a maior influência no fator de relevância (como a fonte Hira Min Para o W3), representando 5% das 188 variações de fontes consideradas.
O segundo grupo está relacionado às fontes que possuem uma influência média no fator de relevância (como a fonte Koz Go Para o Bold) e denotam 9% das variações testadas.
Finalmente, o terceiro grupo representa as fontes que apresentam a menor influência no fator de relevância.
Considerando- se as variações testadas, este grupo representa o comportamento de 86% de elas.
A diferença na influência entre o tipo de fontes com o fator de relevância ocorre devido a peculiaridades de cada fonte, como o seu próprio design e a quantidade de detalhes, que afetam o custo de processamento por parte de os RIPs.
Entretanto, não é o objetivo deste trabalho a realização de uma análise da complexidade das fontes, que pode ser muito custoso em termos de esforço de processamento e poderia comprometer ou invalidar a aplicação da métrica em PDFs que contenham diversas variações de fontes.
Portanto, será assumido que a presença de texto representa um custo constante para cada página, já que o número de objetos de texto não influenciam nesta questão.
Este custo será calculado como uma média ponderada dos custos individuais de cada um dos três grupos apresentados.
Assim, diz- se que um custo cT xt $= txt deverá ser computado caso exista a presença de na página do documento PDF.
Com o intuito de encontrar o valor aproximado de txt no contexto dos diferentes grupos, experimentos usando um número fixo de text objects e um número variado de páginas foram gerados.
De esta maneira, tornou- se possível avaliar os custos obtidos à medida que mais páginas com texto foram adicionadas.
Estes resultados são ilustrados na Figura 26.
Descartando o custo das páginas para os casos apresentados e analisando o custo restante, pode- se notar que este aumenta à medida que mais páginas com texto estão envolvidas.
Assim, é possível dizer que o texto realmente influência no processo de rasterização de um PDF.
Por- tanto, através da divisão do custo restante (dos textos) por o número de páginas envolvidas no experimento, os seguintes valores foram obtidos:·
Grupo leve:
0.042;· Grupo médio:
0.061;· Grupo pesado:
0.088. Utilizando estas constatações, é possível aplicar uma média ponderada sobre tais resultados, obtendo um único custo constante para a presença de texto:
CT xt $= 0.046.
Com isso, diz- se que para np páginas com texto, o custo total das porções de texto num documento PDF (denotado como cT xtT ot) é calculado como apresentado na Equação 5.8.
De a mesma maneira do que para os objetos de imagem, alguma flutuação foi encontrada, mas nada que indicasse uma relação entre o fator de relevância e o valor de opacidade empregado.
Por outro lado, como pode ser visto no gráfico apresentado, o fator de relevância aumentou devido a o fato dos objetos de texto serem transparentes.
Para verificar o valor dos textos transparentes, desta vez variou- se o número de páginas com texto transparente, analisando o custo à medida que mais páginas foram adicionadas.
Estes resultados são demonstrados na Figura 28.
Para os três grupos de texto estabelecidos, os seguintes custos computacionais foram obtidos:·
Grupo leve:
0.092;· Grupo médio:
0.127;· Grupo pesado:
0.154. Comparando estes resultados com aqueles dos textos sem transparência, pode- se notar que o efeito de transparência provoca um aumento de aproximadamente incT xt $= 2 vezes no fator de relevância.
Desta forma, a fórmula para calcular o custo dos textos com e sem transparência pode ser unificada numa única, como apresentado na Equação 5.9.
Em a equação apresentada, a variável tri representa a existência ou não de transparência nos textos na página i.
Assim, pode- se afirmar que tri pode assumir os valores 0 ou 1, provocando ou não um aumento no custo dos textos.
Em este cenário, o valor 0 é assumido caso não exista transparência na página i e, caso contrário, o valor 1 é assumido.
Custo Computacional Total Até o momento foram definidos os custos analisando os objetos e características individualmente.
Entretanto, deseja- se saber o custo computacional de um documento PDF, fornecendo assim uma estimativa de qual o esforço que será realizado para rasterizar- lo.
Para tanto, dizse que o custo computacional de um documento trata- se da soma dos custos individuais das características analisadas.
De esta maneira, a Equação 5.10 apresenta a fórmula que deverá ser utilizada para obter o custo de um documento, denotado como cDoc.
Esta funcionalidade será interessante para as estratégias de escalonamento que serão aplicadas Páginas:
Área total das mesmas;
Textos: Presença destes elementos em quantas páginas e presença de transparência para estes em quantas páginas.
Imagens: Área total ocupada com e sem re-usabilidade, assim como área total de imagens com transparência, com e sem re-usabilidade.
Escopo de image XObjects:
Para cada image external object, quais são as páginas em que eles são aplicados.
Transparência de páginas:
Quais páginas contêm elementos transparentes e quais contêm apenas objetos opacos.
Considerando- se as informações especificadas acima, pode- se afirmar que o processamento objetos de texto, imagem e o estado dos gráficos.
Pode- se perceber que a principal função da ferramenta é a busca e interpretação dos objetos do PDF.
Assim, com o intuito de descrever tal processamento é necessário expor algumas estruturas que são utilizadas para a obtenção das informações de interesse:·
estadoGraficosAtual: O estado dos gráficos corrente.
Em esta estrutura estará apenas a informação se deve- se utilizar transparência ou não;·
pilhaEstadoGraficos: A pilha para o controle dos estados dos gráficos;·
mapaImagens: Mapa para o controle das imagens, possibilitando a verificação se estas estão sendo re-utilizadas ou não.
Este mapa é acessado através do identificador único da imagem retornando um objeto que conterá informações sobre tal objeto.
Assim, nestas informações estarão os escopos de cada imagem.
Com isso, uma estrutura hierárquica é obtida, onde no topo desta hierarquia está o document catalog do PDF.
Esta estrutura fornecerá acesso a todas as páginas e atributos das mesmas, assim como a seus respectivos content streams e resources.
Realizada a interpretação do documento por completo, com o uso da estrutura obtida como resultado, são recuperadas todas as páginas do documento, que serão analisadas uma a uma num laço.
Desta forma, uma iteração do laço em questão corresponderá a análise de uma página e de seus objetos correspondentes.
A o início de uma iteração, o estado dos gráficos atual é inicializado, estabelecendo- se a ausência de transparência.
A seguir, a altura e largura da página são recuperadas e multiplicadas para a obtenção da área desta, acumulando- se o resultado.
Com isso, parte- se para a recuperação dos atributos de interesse para os objetos de texto e imagem da página.
Para tanto, se torna necessário interpretar cada operação realizada no conteúdo da página que influencie sobre os objetos de interesse.
Em este sentido, apenas os operadores que dizem respeito à pintura de texto, imagens e grupos nas páginas, assim como ao estabelecimento Para o estado dos gráficos os operadores gs, q e Q são analisados.
Em o momento em que se encontre o primeiro destes operadores, uma análise sobre os seus respectivos operandos é realizada, para verificar se está se aplicando transparência.
Em caso afirmativo, a variável estadoGraficosAtual é marcada com o valor de utilização de transparência, senão a ela é atribuído o valor de ausência de transparência.
Nota- se, como exposto anteriormente, que esta variável apenas armazenará informação quanto a transparência, já que é o único parâmetro do estado dos gráficos o qual deseja- se conhecer.
Os demais operadores do estado dos gráficos, provocarão alterações na estrutura pilhaEstadoGraficos.
Assim, à medida que o operador q for encontrado, o valor de transparência do estadoGraficosAtual será colocado no topo da pilha simulando uma operação de salvamento.
Quando o operador Q estiver presente, a variável estadoGraficosAtual receberá o valor daquele elemento no topo da estrutura pilhaEstadoGraficos, sendo que este elemento será removido da mesma.
Considerando os objetos de texto avalia- se apenas o operador Tj.
Assim que tal operador for encontrado, sabe- se que deseja- se pintar um texto na página.
Desta forma, verifica- se o estado dos gráficos atual avaliando a existência de transparência.
Caso esteja sendo empregado a transparência, marca- se que a página corrente contém texto transparente e diz- se que não há a necessidade de analisar mais a presença de textos para tal página.
Isto decorre do fato de que a quantidade de texto não é interessante para as métricas, portanto apenas é desejado saber se a página possui texto e se este texto é transparente ou não.
Em o cenário em que não se esteja aplicando transparência, verifica- se se a página ainda deve ser analisada.
Em caso afirmativo, marca- se a página atual com a presença de texto opaco.
Caso contrário, nenhuma ação é tomada.
A o final de cada iteração, então, será verificado se a página foi marcada com presença de texto ou texto transparente, somando um ao respectivo contador (de páginas com texto ou com texto transparente).
O operador De o se refere a pintura de XObjects na página.
Em este sentido, deseja- se apenas considerar os objetos deste tipo que sejam imagens ou grupos (para verificar os objetos em ele contidos).
Para tanto, assim que um operador De o for encontrado, através do operando que tratase nome do objeto, procura- se a referência para o mesmo no dicionário resources da página.
De esta maneira, obtém- se o identificador único do XObject, possibilitando a procura da definição de tal objeto no documento PDF, através de uma consulta a cross-reference table.
Com a definição do XObject em mãos, verifica- se seu tipo (através da entrada do dicionário Subtype), para estabelecer se é uma imagem ou um grupo.
Caso o objeto seja um grupo, recupera- se o seu conteúdo, tratando- o como se fosse o conteúdo de uma página.
Isto quer dizer que analisa- se todos operadores para o conteúdo do grupo, tomando as mesmas ações definidas para a avali-ação do conteúdo de uma página.
Tal atitude é tomada, pois como mencionado anteriormente um group XObject trata- se de uma unidade auto-contida, onde os objetos pertinentes a ele estão definidos no seu próprio stream.
Por outro lado, caso o objeto seja uma imagem, procede- se para o processamento de seus atributos.
O processamento de uma imagem consiste em três passos:
Em a verificação se está imagem é uma primeira instância, ou trata- se da re-utilização da mesma, atualizando o mapa de imagem;
Através da análise descrita acima para os operadores de interesse, ao final da análise de todas as páginas obterão- se- as informações requisitadas.
Cabe ressaltar que no arquivo XML de configuração define- se quais são as informações de saída desejadas, evitando- se, assim o quantidade de informações requisitadas no arquivo de configuração terá um impacto direto no tempo processamento da ferramenta.
A ferramenta PDF Splitter foi desenvolvida com o intuito de quebrar um documento PDF em diversos fragmentos.
Como mencionado anteriormente, o PDF se trata de um formato baseado em referências, o que significa que um objeto qualquer (não somente objetos gráficos) utilizado numa página, pode ser definido no contexto de outra página.
Portanto, o PDF Splitter preocupa- se em resolver referências para cada fragmento do PDF em questão.
Com esta finalidade, para cada página do fragmento gerado, a ferramenta irá procurar por a definição de objetos no documento PDF original que não se encontrem no contexto da própria página e das demais páginas do fragmento.
Para tanto, utiliza- se uma funcionalidade fornecida por a biblio-teca PDF
Box, que é capaz de resolver todas as referências de uma página que estão faltando para um documento em questão.
Em suma, a funcionalidade do PDF Splitter é a criação de um novo documento PDF, para cada fragmento especificado, com suas páginas correspondentes.
Em este sentido, a ferramenta gera um novo arquivo para cada um dos fragmentos e insere as páginas correspondentes em cada um destes.
Esta inserção, consiste na cópia dos objetos de cada página para o seu arquivo, caso estes objetos já não existiram no documento em questão.
Assim, alguns objetos terão de ser re-definidos para cada documento novo, podendo- se perder parte da vantagem fornecida por o recurso da re-usabilidade.
A Figura 30 exemplifica esta situação.
Os modos de quebra suportados por a ferramenta tratam- se da quebra por intervalos e da quebra através de páginas específicas.
Através da utilização do primeiro, é possível obter fragmentos que representem uma quebra do documento original a cada p páginas, onde p representa o intervalo desejado.
O segundo trata- se de um modo de quebra mais flexível, onde é possível especificar exatamente as páginas desejadas do documento original que estejam contidas em cada um dos fragmentos.
Esta segunda situação é interessante para aproveitar melhor a estrutura do documento, como por exemplo, a re-usabilidade.
Em esta situação, poderia- se agrupar as páginas que contêm os mesmos objetos re-utilizáveis, mantendo assim o recurso de re-usabilidade entre os diversos fragmentos.
Além disso, através dos modos de quebra é possível ignorar certas páginas do documento PDF, ou seja, pode- se obter fragmentos do documento PDF original que não necessariamente contemplem o documento inteiro.
Esta característica otimiza o desempenho do processo de quebra, pois não será necessário analisar todas as páginas do documento original para obter- se os fragmentos desejados.
Como mencionado anteriormente, as estratégias existentes aplicam apenas o algoritmo Ls para ganhar desempenho no âmbito da rasterização dos jobs da fila.
Em este sentido, dois algoritmos de escalonamento foram aplicados, objetivando a obtenção de um desempenho melhor:
LPT e Multifit.
Além destes, propôs- se um algoritmo, chamado aqui de LPT Otimizado, onde realizou- se uma pequena modificação sobre o algoritmo LPT para contornar um problema que poderia ocorrer no escalonamento dos jobs.
Em este contexto, estes três algoritmos foram aplicados tomando como base a estratégia existente que possui o menor número de desvantagens:
Em um primeiro momento, será apresentada uma abordagem genérica adotada para o funcionamento dos escalonadores desenvolvidos.
A seguir serão descritas as peculiaridades de cada algoritmo de escalonamento empregado em tal abordagem.
Em este contexto, as ferramentas PDF processamento disponíveis, o algoritmo de escalonamento A e a fila de jobs, contendo n jobs Para cada jobi da fila, onde 1 i n, é executado o passo 2.
Considerando- se um jobj com um número total de páginas tpj, são estabelecidos m limites de fragmentos (página inicial e páginal final) caso tpj seja maior que m, denotados tp por limj, l.
Em esta situação, cada um destes conterá mj páginas.
Por outro lado, caso tpj seja menor que m, são gerados apenas tpj limites, cada um com uma página.
A métrica é aplicada sobre as informações obtidas, resultando no custo cDocl associado a cada tarefa Tl analisada.
São aplicadas as diretivas de escalonamento definidas em A para organizar as tarefas da fila.
As tarefas são transmitidas para cada máquina ociosa de acordo com o definido em A. A cada novo job obtido por o escalonador uma regra é aplicada conforme o algoritmo A empregado.
Toda vez que uma máquina ficar ociosa é executado novamente o passo 7.
De entre os passos estabelecidos, é importante ressaltar que cada tarefa corresponde ao limite de páginas de cada fragmento, ou seja, a partição dos jobs não é realizada por o escalonador.
Estes limites são considerados como tarefas, pois serão transmitidos a cada máquina que, então, fará a partição do jobj associada ao limite limj, l para cada tarefa Tl recebida.
De esta maneira, o escalonador não ficará sobrecarregado com a computação de cada fragmento, pois tal processamento será delegado para os RIPs disponíveis que realizarão tal tarefa em paralelo.
Assim, para a aplicação desta estratégia, se está considerando um ambiente onde todas as máquinas tenham acesso aos jobs da fila.
Além disso, vale a pena destacar que as informações de cada tarefa (fragmentos) associados a este de uma única vez.
Considerando a abordagem genérica apresentada acima, cada algoritmo irá realizar diferentes ações, consistentes com o funcionamento do próprio algoritmo.
A seguir, serão estabelecidas as ações realizadas sobre a abordagem genérica para o cenário dos três algoritmos de escalonamento descritos.
A base do algoritmo LPT está centrada na execução de tarefas mais pesadas antes daquelas mais leves.
Assim, com a utilização deste algoritmo, uma ordenação é realizada sobre o conjunto de tarefas (passo 6), tal que estas tarefas estejam organizadas de forma decrescente, conforme cDocl, para cada tarefa Tl da fila.
Desta forma, as tarefas com o custo computacional maior, conforme as métricas estipuladas, estarão por primeiro na fila.
Feito isso, no passo 7, serão transmitidas as primeiras tarefas da fila, uma para cada máquina disponível, de forma que todas as máquinas recebam uma tarefa ou que não hajam mais tarefas na fila para as máquinas que não receberam nenhuma tarefa.
À medida que uma máquina ficar ociosa ela receberá a primeira tarefa da fila, caso esta existir.
Considerando o passo 8, a cada novo job obtido, os passos 2, 3 (este com uma pequena modificação), 4 e 5 serão realizados.
Em outras palavras, este novo job será quebrado em tarefas e cada tarefa será inserida na fila.
Entretanto, esta inserção será realizada de forma a manter a ordenação da decrescente da fila, conforme o custo de cada tarefa.
Com isso, será evitada a necessidade de re-ordenar a fila cada vez que novas tarefas tenham que ser inseridas em.
LPT Otimizado A maior desvantagem do algoritmo LPT é a necessidade da avaliação do custo computacional das tarefas para, então, inserir- las na fila, disponibilizando- as para os RIPs.
Esta avaliação, no momento em que os RIPs estejam livres para receber mais tarefas.
Tal situação irá comprometer o possível desempenho adquirido com o uso do LPT, já que os RIPs ociosos estarão esperando por tarefas à medida que o escalonador está realizando a análise do custo computacional das tarefas.
Assim, propôs- se uma pequena modificação quanto a esta análise, para amenizar o problema descrito.
Todos os passos além de a alteração descrita a seguir, serão os mesmos para o LPT e o LPT Otimizado.
Para melhorar o tempo de resposta do escalonador, estabeleceu- se que a tarefa de aplicação da métrica sobre as tarefas será realizada de forma concorrente com o escalonamento das tarefas para os RIPs livres.
Para tanto, assim que o escalonador precisar aplicar a métrica sobre um determinado job, será lançada uma nova thread responsável por tal função.
Entretanto, anteriormente à criação desta thread, as tarefas deste job serão inseridas no final da fila estando, desta maneira, disponíveis para serem transmitidas.
À medida que as threads terminem sua função, o custo computacional das tarefas correspondentes será atualizado, removendo- as da fila e inserido- as novamente de forma ordenada.
Através desta estratégia, caso um RIP fique livre e não existam mais tarefas na fila a não ser aquelas para as quais o custo computacional não tenha sido estabelecido, uma destas tarefas será enviada assim mesmo.
Caso uma thread finalize sua computação e as suas tarefas já tenham sido enviadas, nenhuma ação será tomada.
Multifit No caso de a utilização do algoritmo Multifit, as tarefas da fila são empacotadas em bins (durante o passo 6), através do emprego do algoritmo FFD, resultando na geração de b bins, sendo 1 b m..
Com isso, estes b bins são transmitidos para b máquinas (passo 7), onde cada máquina ficará com até 1 bin.
À medida que novos jobs são inseridos na fila, são executados os passos 2, 3, 4 e 5.
Como descrito anteriormente, através da aplicação destes passos são obtidas as tarefas com os seus respectivos custos computacionais.
Assim que estes sejam executados, caso não existam bins no momento, aplica- se a técnica FFD, gerando assim os novos bins que são mantidos numa fila de bins.
Para esta fila será associado o C que foi utilizado para gerar os bins.
Por outro lado, no caso em que novos jobs sejam obtidos e já existam bins na fila, a estratégia adotada para o empacotamento das novas tarefas geradas será a tentativa da inserção de cada tarefa no bin com a menor carga existente, considerando o C atual da fila.
Se todas as tarefas forem acomodadas nos bins atuais, nenhum outro processamento será necessário.
Com o uso desta estratégia, ameniza- se o esforço computacional para obter os bins, disponibilizando- os mais rapidamente para os RIPs.
Entretanto, se não for possível acomodar- los considerando o C atual, os bins existentes serão quebrados e o FFD será re-executado considerando as tarefas antigas e atuais.
Com o intuito de avaliar aspectos relativo à abordagem proposta neste trabalho, utilizaramse diversos jobs que representam casos reais, juntamente com três distintas configurações de filas.
Entre os componentes analisados, estão a validação das métricas nos jobs reais, além de a verificação de desempenho para ferramenta de análise do perfil dos jobs e para as estratégias de escalonamento definidas.
Em este sentido, este capítulo discorre sobre o ambiente de teste, em termos de hardware e software, utilizado para executar os componentes da abordagem proposta, apresentando os jobs e configurações de filas utilizadas.
Logo a seguir, demonstra- se uma avaliação de desempenho Finalmente, ilustra- se os resultados obtidos com as estratégias de escalonamento desenvolvidas, comparando- as com aquelas já existentes.
Em esta seção será descrito o ambiente de hardware e software empregado para implementar e executar os escalonadores novos e já existentes, como também as ferramentas auxiliares desenvolvidas.
Além disso serão apresentados os jobs utilizados no contexto dos testes realizados, para a avaliação do comportamento e desempenho dos novos escalonadores em comparação com os já existentes.
Ambiente de Hardware e Software Com o intuito de se aproximar da realidade existente nas PSPs, o ambiente de hardware utilizado nos testes assemelha- se ao empregado no ambiente de produção de tais empresas.
Este trata- se de um agregado de blades, que é uma arquitetura multicomputador, podendo ser classificado como um COW (Cluster Of Workstations).
Em este sentido, utilizou- se um agregado com 5 blades, onde cada uma de elas contém um processador Intel Xeon 3.0 GHz (de 64 bits) quad- core, com 8 GB de memória RAM e 400 GB de disco.
Em este sentido, considerou- se cada core como sendo uma unidade de processamento ativa da arquitetura, suportando assim até 20 processos simultaneamente (quatro por blade).
Além disso, estas blades estão conectadas através de uma rede de alta velocidade, que é a Gigabit Ethernet.
O sistema operacional aplicado nas máquinas é o Windows Server 2003 R2 Standard x64 Edition, com o Service Pack 2, um sistema comumente aplicado em ambientes de impressão, devido a a compatibilidade com diversas versões de RIPs industriais.
Em termos de implementação, utilizou- se a linguagem Java para implementar as estratégias de escalonamento previamente descritas.
Apesar de a utilização de Java não ser usual na área de alto desempenho, tal linguagem vem oferecendo ao longo de o tempo melhorias que permitem sua utilização, fazendo com que pesquisadores da área adotem- na como escolha para a implementação.
Exemplos desta situação podem ser vistos em.
Além disso, esta linguagem oferece vantagens quanto a a portabilidade, extensibilidade e alto nível de abstração para as soluções propostas, além de oferecer compatibilidade com a biblioteca Java PDF
Box, utilizada no JDK 1.
5.16 amd64.
Assim, devido a a arquitetura mencionada acima ser um multicomputador, a comunicação entre os processos deve ser realizada através da troca de mensagens.
Para tanto, utilizou- se a biblioteca MPJ-Express, que é uma implementação do padrão MPI (Message Passing Interface) puramente com o uso da linguagem Java.
Devido a a utilização da linguagem Java, esta biblioteca fornece conceitos de alto nível de abstração, além de a portabilidade oferecida por o linguagem, que é uma característica muito importante para as PSPs.
Em termos de desempenho, a MPJ-Express consegue obter resultados similares quando comparada a bibliotecas como o MPICH (MPI CHameleon) e o LAM/ MPI (Local Area Multicomputer/MPI), mostrando- se como uma boa escolha para a troca de mensagens entre processos que utilizam Java (para mais detalhes ver).
Casos de Teste Com o objetivo de avaliar a validade das métricas propostas sobre exemplos reais de jobs e para verificar o desempenho das ferramentas e estratégias implementadas, distintos jobs foram selecionados.
Em este sentido, estabeleceu- se um conjunto fixo de 20 jobs com diferentes características, juntamente com três distintas configurações de fila através do uso destes.
Estes componentes são apresentados nesta seção.
Jobs Os jobs selecionados estão distribuídos entre 8 tipos comumente presentes no âmbito das PSPs.
De entre estes tipos, cabe ressaltar que diversos se assemelham em termos de a presença de certos elementos para descrever- los, mas podem se distinguir devido a o tipo físico de papel sobre o qual serão impressos ou ao layout utilizado em cada um de eles.
Um exemplo de cada documento é ilustrado na Figura 31.
Além disso, a seguir algumas características de cada um dos 8 tipos, em conjunto com uma breve descrição sobre eles, serão apresentados, ressaltando suas peculiaridades quanto a presença de texto, imagens e ao número de páginas:
Carta: Uma carta contém uma ou mais páginas com uma grande quantidade de texto e poucas imagens.
Estas imagens normalmente são logotipos, carimbos ou assinaturas.
Carta de notícias:
Uma carta de notícias difere de uma carta normal, contendo diversas imagens para a apresentação de propagandas, além de notícias sobre produtos e serviços.
O número de páginas deste tipo de documento pode variar, conforme a idéia que se deseja passar ao cliente.
Cartão: Cartões de visitas, contendo imagens de fundo e pouco texto.
Em uma página deste tipo de documento podem existir diversos cartões, assim como uma única instância dos mesmos.
Cartão postal:
Cartões postais são utilizados para a apresentação de um lugar, produto ou serviço para o cliente.
Em este sentido, são empregados textos e imagens para compor cada página.
De um modo geral, cartões postais contêm duas páginas (frente e verso), podendo ser colocados diversos numa única página.
Flyer: Estes documentos são utilizados para a divulgação de produtos, idéias, eventos, entre outros.
Geralmente cada documento contêm duas páginas, que são normalmente impressas no modo frente e verso, onde cada documento estará contido numa folha de papel.
Em termos de conteúdo tanto imagens quanto textos são utilizados.
Folheto: Documento composto geralmente por um grande número de páginas, sendo rico em imagens e texto.
A o final da impressão, as páginas de um documento em folheto serão dispostas de maneira a formar uma espécie de livreto.
Jornal: Documento informativo contendo manchetes e notícias.
Em este tipo de documento estão presentes uma ou mais páginas, cada uma com a presença de textos e imagens.
Pôster: Documento com normalmente apenas uma página de tamanho grande.
Em este sentido, um pôster pode conter apenas imagens, textos ou uma mistura de imagens com texto.
Levando em consideração os tipos explicitados acima, os 20 jobs escolhidos distribuem- se entre eles, representando exemplos reais de documentos utilizados nas PSPs.
Assim sendo, estes jobs não possuem apenas textos e imagens, mas também contêm a presença de objetos gráficos não analisados para as métricas estipuladas, como paths e shading patterns.
Estes jobs são apresentados na Tabela 1, que explicita suas características, juntamente com seu custo computacional estimado através da aplicação das métricas.
Vale a pena ressaltar que o custo computacional estimado é obtido através da soma dos fatores de relevância obtidos por as fórmulas.
Banner texto, texto texto, texto, texto texto, texto texto, texto, texto texto, texto texto, texto, texto texto, texto texto, texto, texto Assinatura Notícias Cartão Carta de Notícias Fig..
como se pode notar, os documentos selecionados para a realização dos testes possuem uma grande variabilidade de quantidade de páginas, textos e imagens, possuindo assim uma grande abrangência de custos.
Esta alta variabilidade retrata a realidade da maioria das PSPs, em as quais os jobs provêem de diversos cenários, acabando por acarretar tal diversificação de custos computacionais.
De esta maneira, com o uso dos jobs estipulados será possível, então, avaliar o comportamento das estratégias novas e já existentes quanto a o balanceamento de carga entre as unidades de processamento disponíveis.
Filas de Jobs Para realizar a rasterização de determinados jobs de uma fila é importante notar que a ordem dos jobs da fila poderá afetar o ganho de desempenho quando empregando uma determinada estratégia de escalonamento.
Em este sentido, torna- se interessante a execução das estratégias sobre diferentes configurações de fila, avaliando assim como será realizado o escalonamento para as unidades de processamento, assim como o impacto de tal escalonamento sobre o possível ganho de desempenho.
Sabendo que os jobs da fila podem ser disponibilizados a qualquer momento, nas configurações de fila testadas estabeleceu- se que alguns jobs estão disponíveis inicialmente na fila (chamados de jobs iniciais) e outros serão inseridos nesta depois de x segundos (jobs tardios).
Definiu- se, então, que 10 x 60.
Com o uso deste intervalo pequeno, novas tarefas serão disponibilizadas rapidamente, tornando possível visualizar como cada estratégia trata da análise do perfil, organização e distribuição dos jobs de forma concorrente.
Em este contexto, para a simulação do ambiente das PSPs, os jobs tardios são inseridos seqüencialmente na fila e o tempo para inserir um destes apenas começará a ser contabilizado à medida que seu predecessor já tenha sido inserido na fila.
Com isso, definiu- se três distintas configurações de fila, através da utilização dos 20 tipos de jobs descritos na Seção 6.1.2, que estão apresentadas na Tabela 2.
As configurações de fila estipuladas foram direcionadas para tratar de três casos distintos no cenário dos algoritmos escolhidos.
Estes cenários dizem respeito à distribuição de carga dos jobs no contexto da fila, provocando assim diferentes conseqüências para as estratégias empregadas.
Estes cenários são descritos a seguir:
Fila de jobs com uma variação de cargas do primeiro job ao último;
Fila de jobs ordenados respectivamente da maior carga a menor (Fila 2);
Fila de jobs ordenados da menor carga a maior (Fila 3).
O primeiro caso foi definido para a avaliação do comportamento das novas estratégias quanto a variações de cargas dos jobs, verificando assim se as estratégias são capazes de lidar com este tipo de irregularidade, balanceando as cargas de entre os RIPs disponíveis.
Além disso, tornou- se possível avaliar o impacto desta variação nas estratégias de escalonamento já existentes.
O segundo caso representa a melhor situação para o algoritmo Ls, pois as tarefas serão inseridas na fila de forma decrescente em termos de custo computacional, prevenindo o pior caso do algoritmo.
Portanto, através desta configuração tornou- se possível avaliar qual a influência das computações adicionais necessárias para a aplicação dos algoritmos utilizados em comparação com o melhor caso para o algoritmo das estratégias de escalonamento existentes.
Por fim, em contraste com o segundo caso, o terceiro representa o pior caso para o algoritmo Ls.
Assim, avaliou- se qual o possível ganho de desempenho com a utilização dos novos algoritmos empregados no cenário da rasterização em comparação com o algoritmo Ls em tal situação.
Até o momento definiram- se diversas métricas para medir o custo computacional de um job através de casos de teste experimentais, totalmente direcionados para seus respectivos propósitos.
Entretanto, tais métricas devem corresponder ao custo computacional de jobs reais, para que sua aplicação se justifique.
Para tanto, os jobs selecionados para a realização dos testes, que tratam- se de exemplos reais de mercado, foram executados (rasterizados) seqüencialmente obtendo assim um tempo de execução para cada um destes.
De esta maneira, tornou- se possível comparar o custo computacional estimado com este tempo de execução, verificando assim se a métrica condiz com os custos reais.
Estes resultados são demonstrados na Tabela 3, em a qual os jobs estão ordenados de forma decrescente através do custo computacional, apresentando o valor da média de 20 execuções para cada job:
É possível notar que a métrica utilizada provê uma boa estimativa para os custos computacionais dos jobs apresentados.
Em este contexto, percebe- se que através do custo estimado pode- se estabelecer faixas de tempo de execução que são retratados por os diferentes valores de métrica.
De esta maneira, com a métrica pode- se facilmente diferenciar jobs custosos daqueles mais leves, fornecendo assim uma boa base para o propósito de balanceamento de carga que deseja- se atingir neste trabalho.
Por outro lado, à medida que os custos ficam próximos, verifica- se que existe um erro envolvido.
Tal situação pode ser visualizada, analisando o custos dos exemplos Flyer 1, Carta de notícias 2 e Carta 3, onde o tempo de execução é maior do que alguns exemplos com custos computacionais estimados menores.
Acredita- se que este erro é resultado da existência de objetos PDF nos jobs, os quais não foram considerados para a formulação das métricas, como, por exemplo, paths e shading pattern objects.
Assim, estes objetos não considerados podem adicionar um determinado tempo de rasterização à computação, o qual não está sendo contabilizado para a estimativa proposta.
Entretanto, a métrica ainda é capaz de dar uma boa estimativa dos custos dos jobs, mesmo com este erro associado, podendo assim ser aplicada para o objetivo de balanceamento de carga.
Como visto anteriormente, as estratégias utilizadas para melhorar o desempenho da fase de rasterização enquadram- se no cenário de escalonamento não-determinístico.
Devido a tal fato, nenhuma informação sobre as tarefas é conhecida previamente, provocando assim a necessidade da análise de cada uma de elas para a aplicação dos algortimos empregados.
Entretanto, está claro que se esta análise se mostrar muito custosa, o desempenho das estratégias não será satisfatório, pois, assim, tal fase irá impedir a distribuição de tarefas num tempo hábil para manter os RIPs trabalhando continuamente.
De esta maneira, nesta seção apresenta- se uma avaliação sobre a viabilidade deste pré-processamento.
A Tabela 4 demonstra os tempos obtidos (em segundos) para a análise do perfil dos diferentes jobs utilizados no trabalho, juntamente com o tempo de rasterização correspondente a cada um de eles e uma proporção percentual, indicando qual a grandeza de tal análise quando comparada ao respectivo tempo total de rasterização.
Tempo de rasterização Tempo para a análise do perfil Percentual da análise Carta 1 Carta 2 Carta 3 Cartão 1 Cartão 2 Carta de notícias 1 Carta de notícias 2 Cartão postal 1 Cartão postal 2 Flyer 1 Flyer 2 Flyer 3 Flyer 4 Folheto 1 Folheto 2 Folheto 3 Jornal 1 Jornal 2 Pôster 1 Pôster 2 Através da tabela de resultados pode- se notar que em alguns jobs o tempo de análise se mostra muito maior do que em outros.
Isto ocorre devido a a quantidade de objetos do próprio PDF (não apenas os gráficos), que influenciam o tempo de interpretação do arquivo para que este seja convertido para a representação de objetos Java empregada na biblioteca PDF
Box. Além disso, outros fatores impactantes em tal análise são o tamanho em disco do arquivo PDF, pois este fator está diretamente relacionado ao número de operações de leitura do disco que devem ser realizadas, em conjunto com a combinação de quantidade de imagens, textos e páginas que cada job possui, os quais são os objetos de interesse e demandam processamento por parte de a ferramenta.
Apesar de existir estas variações, os tempos de análises para os arquivos não ultrapassam os 20 segundos, com uma média de aproximadamente 5 segundos de análise.
Por outro lado, levando- se em consideração o percentual da análise do perfil dos jobs em relação a o tempo total de rasterização, pode- se notar que a grande maioria é ínfima, permanecendo em torno de 0.20% a 2.00%.
Assim, o uso da ferramenta pode ser realizado facilmente, proporcionando um desempenho satisfatório em relação a o esforço total que será aplicado sobre os jobs posteriormente.
Entretanto, percebe- se um percentual maior de aproximadamente 3%, 6%, 7% e 10% para os jobs Pôster 2, Cartão Postal 2, Cartão 2 e Folheto 1 respectivamente.
Esta situação decorre do fato de que estes jobs são pequenos em termos de tempo de rasterização, sofrendo assim uma maior influência na sua análise de perfil por parte de o tempo de operações de E/ S e para a conversão de seus objetos para a representação da biblioteca PDF
Box. Em este sentido, acredita- se que quanto menor o custo computacional para a rasterização do job e quanto maior o número de objetos em ele contidos, maior será a influência destas duas fases iniciais.
Vale a pena ressaltar que o número de objetos diz respeito a todos os objetos do job, não apenas os gráficos, pois todos estes devem ser interpretados e convertidos para a representação de dados barreira que não pode ser quebrada.
Todavia, a aplicação da análise se justifica, pois mesmo com tal barreira ela representa, no pior caso, um valor em segundos baixo, 10 vezes menor do que o tempo de rasterização do correspondente job.
Esta seção discorre sobre medidas de desempenho para as estratégias propostas, comparandoas com as anteriores já empregadas e com o tempo sequencial de rasterização da fila de jobs como um todo.
Em este sentido, 10 execuções foram realizadas sobre as 6 estratégias consideradas neste trabalho, para cada uma das três configurações de fila descritas.
De estes resultados, então, foram descartados aqueles de maior e menor tempo para cada bateria de teste, obtendo uma média dos outros 8 valores restantes.
Vale a pena ressaltar que para executar qualquer uma das estratégias descritas, 1 processo sempre será o escalonador, que apenas realizará a distribuição de tarefas sem rasterizar nenhum dos jobs.
Assim, para a existência de RIPs em paralelo o número mínimo de processos relacionados deve ser 3, contando com 2 RIPs e um escalonador.
De esta maneira, para a execução dos testes variou- se o número de processos de 3 a 20, com ressalva da estratégia existente de N RIPs por job.
Em esta estratégia, foram considerados 4 tamanhos de grupos distintos:
2 RIPs por job, 3 RIPs por job, 4 RIPs por job e 5 RIPs por job.
Para cada tamanho de grupo, o número de processos foi direcionado para comportar sempre 1 ou mais grupos inteiros, até que este número chegasse o mais perto de 20 (que corresponde ao total de unidades de processamento disponíveis).
Como é possível verificar nos resultados apresentados a estratégia 1 RIP por job manteve seu fator de aceleração para a variação de processos utilizada por volta de 3, sem qualquer tendência de crescimento.
Isto ocorre, pois no cenário em que são aplicados um número baixo de RIPs, os jobs iniciais são distribuídos imediatamente para eles, fazendo- os trabalhar.
Entretanto, considerando que o número de jobs iniciais é pequeno e é escalonado imediatamente para os primeiros RIPs ociosos, à medida que o número de RIPs disponíveis aumenta, diversos ficarão sem tarefas para computar, devido a o fato de que os jobs tardios são inseridos incrementalmente na fila e possuem um atraso para o mesmo.
De esta maneira, a adição de mais processos nesta estratégia não irá representar um ganho efetivo, pois várias unidades de processamento não serão empregadas para a rasterização.
Os speed-ups obtidos para a estratégia N RIPs por job, mostraram- se insatisfatórios também.
Como pode- se visualizar na Tabela 32 (b), existe uma flutuação dos fatores obtidos, com aumentos e diminuições nas medidas.
Além disso, nota- se que o melhor resultado atinge um máximo de 5.2 com o uso de 16 processos (casualmente, 3 RIPs por job).
Estas situações podem ser explicadas devido a o fato de que tal estratégia acaba provocando o mau uso dos RIPs, além de um balanceamento de carga pobre, pois RIPs de um mesmo grupo poderão receber tarefas de tamanhos muito diferentes, fazendo com que aqueles que não terminaram o seu processamento acabem por impedir os demais do grupo de receberem novas tarefas.
Assim, a paralelização fica comprometida, atrasando a rasterização dos jobs remanescentes na fila.
De entre as estratégias existentes, a Todos RIPs por job foi em a qual pode- se visualizar os melhores resultados.
Em este contexto, o melhor speed-up obtido foi de 17.51 com o uso de 20 processos.
Entretanto, verifica- se que o comportamento de sua curva de desempenho possui flutuações, gerando situações imprevisíveis e fatores de aceleração insatisfatórios para determinados números de processos.
Para compreender estas anomalias, deve- se analisar o funcionamento desta estratégia mais a fundo.
Em este contexto, sabe- se que tal estratégia quebra os jobs em fragmentos, diminuindo o grão a ser computado por cada RIP.
Cada tarefa, então, é transmitida para os RIPs ociosos, assim que elas estiverem disponíveis.
Porém, nenhuma consideração quanto a carga da tarefa que está sendo transmitida é considerada.
De esta maneira, esta distribuição torna suscetível a ocorrência da sobrecarga e sub-carga de RIPs, gerando o comportamento visualizado.
Partindo, então, para a análise de desempenho das estratégias desenvolvidas, pode- se verificar que as grandes flutuações nos resultados foram eliminadas.
Isto decorre do fato de que estas estratégias consideram a carga das tarefas para distribuir- las, a fim de balancear o esforço computacional entre as unidades de processamento disponíveis.
Levando- se em consideração as estratégias LPT e Multifit percebe- se que o speed-up obtido, apesar de constante, mostra- se em diversos pontos abaixo de aquele visto com a Todos RIPs por job.
Acredita- se que esta situação acontece, devido a a necessidade por parte de estas estratégias de sempre realizar a análise do perfil dos jobs previamente a disponibilização destes na fila de tarefas.
Assim, caso existam RIPs ociosos durante a análise de um ou mais jobs, estes terão de esperar até que este processamento seja realizado por completo para receberem uma nova tarefa.
Isto acaba por acarretar um overhead que se reflete diretamente na curva de speed-up.
Por outro lado, verifica- se que o speed-up da estratégia Multifit está, na maioria das vezes, abaixo de a LPT, pois com o uso do Multifit adiciona- se ainda mais overhead para o empacotamento das tarefas em bins.
Por fim, os melhores resultados foram apresentados por a estratégia LPT Otimizado, que além de amenizar as flutuações atingiu speed-ups que superam todas as demais estratégias.
A grande vantagem oferecida por esta estratégia é o fato de ela conseguir transmitir imediatamente as tarefas para os RIPs ociosos, concorrentemente a análise do perfil dos jobs.
Com isso, ameniza- se o overhead do LPT para sempre analisar as tarefas previamente ao seu envio.
Os resultados apresentados até então refletem- se diretamente no percentual de utilização de cada unidade de processamento.
Para verificar esta medida, então, apresenta- se a eficiência obtida para cada estratégia na Tabela 6.
RIP por job verifica- se claramente que à medida que mais processos são inseridos a eficiência cai abruptamente, devido a existência de diversos RIPs ociosos durante a execução.
O mesmo percebe- se para os grupos considerados (N RIPs por job), onde a adição de mais processos não representa um ganho efetivo de desempenho, fazendo com que a eficiência permaneça sempre baixa.
No caso de o uso de Todos os RIPs por job pode- se visualizar eficiências altas em torno de 90% para os casos em que o algoritmo Ls obtém um desempenho satisfatório, mas também notam- se eficiências menores de 70% a 85% para aqueles cenários em que existe a queda de desempenho, provocando assim a existência de flutuações nesta medida também.
As estratégias desenvolvidas mantiveram eficiências altas, mesmo com a adição de mais processos.
Considerando- se uma média dos valores de eficiência para estas estrátegias observa- se um valor de 84% para a estratégia Multifit, de 88% para o LPT e de 92% para o LPT Otimizado.
Partindo, então, para a análise de desempenho das aplicações consideradas com a segunda configuração de filas (Fila 2), em a qual as cargas dos jobs estão ordenadas da maior para a menor, os resultados de speed-up são apresentados na Figura 33 e na Tabela 7.
Cabe ressaltar que esta configuração visava a organização das tarefas de maneira a tentar prevenir o pior caso do algoritmo Ls, enquanto as maiores tarefas serão executadas por primeiro.
Para a estratégia de N RIPs por job não é possível notar quaisquer mudanças significativas no ganho de desempenho.
O problema da existência de RIPs ociosos permanece como um fator agravante para o ganho de desempenho, resultando em flutuações nos valores obtidos e em fatores de aceleração baixos.
Este comportamento pode ser observado para todas as configurações de grupos analisadas.
Observando o speed-up da estratégia Todos os RIPs por job pode- se visualizar que houve uma linearização dos resultados, sem a presença de variações significativas, como picos e vales.
Com a configuração de filas considerada, foi possível distribuir primeiramente as tarefas mais pesadas, amenizando o fator da sobrecarga de RIPs no final do processamento, que representam o pior caso para o algoritmo Ls.
Com isso, pode- se dizer que esta configuração de fila simula o funcionamento do algoritmo LPT para a estratégia em questão, sem a necessidade de analisar o custo computacional das tarefas e ordenar a fila.
De esta maneira, esta estratégia consegue apresentar os melhores resultados de entre todas as demais.
Considerando as estratégias desenvolvidas, pode- se notar que estas mantiveram resultados sem comportamentos estranhos, mas com fatores de aceleração menores do que a estratégia de Todos os RIPs por job.
Isto pode ser devidamente explicado por causa de a realização da análise das tarefas que acaba gerando um overhead, onde para a dada configuração de filas não traz nenhum benefício, já que os jobs já estão ordenados.
Por outro lado, analisando a estratégia LPT Otimizado pode- se dizer que os resultados apresentados não afastam- se muito daqueles de a estratégia Todos os RIPs por job, devido a a capacidade de transmitir as tarefas imediatamente aos RIPs ociosos.
Entretanto, existe ainda um pouco do overhead gerado por as threads que analisam o perfil dos jobs, provocando uma perda de desempenho.
Em este sentido, nota- se que os dois primeiros valores afastam- se bastante do obtido com o Todos os RIPs por job, mas o último consegue apresentar uma eficiência bastante similar, pois este não sofre tanto com o impacto da sobrecarga desnecessária gerada com a análise dos jobs.
Por fim, executou- se as estratégias com o uso da terceira fila proposta, onde os jobs foram ordenados de maneira que os menores fossem disponibilizados primeiramente aqueles com cargas mais pesadas.
Os resultados obtidos são demonstrados na Figura 34 e na Tabela 9.
Os speed-ups para as estratégias desenvolvidas mostraram- se novamente com um comportamento previsível.
Estes mantiveram- se perto de o ideal, em os quais deve- se destacar aquele obtido por a estratégia LPT Otimizado, que superou os fatores de aceleração de todas as demais estratégias.
As estratégias LPT e Multifit obtiveram seus correspondentes desempenhos influenciados por o overhead da análise do perfil dos jobs (e empacotamento no caso de o Multifit), que atrasa a distribuição das tarefas.
Apesar de isto, estas estratégias conseguiram superar ou igualar, na maioria dos casos, os desempenhos obtidos por as estratégias existentes.
Considerando as estratégias 1 RIP por job e N RIPs por job nenhuma alteração do comportamento previamente descrito foi observado.
Em este sentido, estas estratégias apresentaram uma baixa escalabilidade, sem nenhum ganho significativo à medida que mais processos foram adicionados.
Por outro lado, a estratégia Todos os RIPs por job apresentou um comportamento particular com aumentos e quedas bruscas de desempenho.
Em este contexto, acredita- se que o desbalanceamento de carga provocado por a execução das maiores tarefas por último, acaba gerando sobrecargas e sub-cargas de RIPs que são diretamente impactantes nesta estratégia.
Isto se deve ao fato de que no âmbito desta estratégia em alguns momentos os RIPs sobrecarregados no final estarão com as maiores cargas da fila, provocando quedas abruptas de desempenho.
Por outro lado, em alguns casos esta situação não ocorrerá, pois diversos RIPs estarão processando cargas pesadas ao final do processamento, terminando sua computação em tempos não muito distantes uns dos outros.
Como já se sabe, o comportamento observado no speed-up está diretamente relacionado às eficiências obtidas.
Desta forma, as estratégias 1 RIP por job e N RIPs por job apresentam eficiências menores com a inserção de mais processos rasterizadores.
Devido a as flutuações bruscas no desempenho da estratégia Todos RIPs por job, existe uma grande variação nas eficiências observadas, partindo de 66% até 92% (com 3 processos).
Devido a a tal situação, a eficiência média desta estratégia ficou em torno de 82%.
Para as estratégias desenvolvidas, novamente pode- se observar eficiências médias altas de 87% para o LPT e Multifit, e de 92% para o LPT Otimizado, que supera todas as demais estratégias.
O trabalho apresentado mostrou- se como um estudo válido para melhores resultados na rasterização de jobs a partir de a computação de alto desempenho.
Através da definição de métricas, que se mostraram consistentes para o estabelecimento de um perfil de jobs, pôde- se aplicar estratégias de escalonamento com competitividades melhores do que as existentes, obtendo assim resultados mais satisfatórios.
Tornou- se possível, então, a resolução de problemas de balanceamento de carga das estratégias existentes, fornecendo aplicações capazes de apresentar um comportamento previsível e com um desempenho que se mostrou satisfatório, atingindo speed-ups perto de o ideal para a variação de unidades de processamento considerada.
Com isso, os recursos disponíveis foram melhor utilizados, como demonstrado através das tabelas de eficiência descritas nos resultados obtidos.
Em este contexto, o trabalho realizado englobou a aplicação de diversos estudos para o desenvolvimento das estratégias apresentadas.
Entre estes destacam- se os problemas existentes na área de rasterização, a composição de um documento PDF, o enquadramento do problema em classificações de escalonamento existentes, modelos teóricos de algoritmos para o problema de escalonamento, granularidade, a seleção de possíveis elementos PDF que influenciam no custo do processamento de um documento, além de o desenvolvimento de ferramentas auxiliares que permitiram o desenvolvimento das novas estratégias de escalonamento.
Cabe ressaltar que métricas similares às apresentadas neste documento, mas no cenário de RIPs industriais, também foram definidas durante o período do mestrado, dando início a geração de uma patente para a empresa Hewlett Packard (Hp).
Em o contexto das métricas discutidas neste trabalho, deseja- se realizar a submissão de um artigo para o journal MTAP (Multimedia Tools and Applications) no mês de Janeiro de 2009, demonstrando os resultados obtidos com tais métricas, que foram definidas a partir de a utilização do RIP open- ImageMagick.
Por outro lado, através dos resultados apresentados pode- se estabelecer um ponto de partida para trabalhos futuros, buscando uma maior otimização da fase de rasterização de documentos.
Em este contexto, pretende-se focar na utilização de RIPs industriais, os quais possuem diversas maneiras de otimizar o processamento de um job, caso lhes seja passada uma devida configuração que condiz com o perfil do job em questão.
Mais especificamente, pode- se citar dois tipos de otimizações disponibilizadas por tais RIPs industriais:
As de re-usabilidade e as de transparência.
As primeiras dizem respeito a um controle maior quanto a o gerenciamento da cache de cada RIP.
Sabe- se que objetos re-utilizáveis no formato PDF são definidos para prevenir o reprocessamento de dados que podem ser armazenados na memória de um RIP, à medida que foram computados pela primeira vez.
Entretanto, diversos objetos re-utilizáveis podem ser aplicados num documento PDF, fazendo com que o resultado da rasterização daqueles mais antigos possa ser apagado da memória do RIP, devido a a necessidade de tal fatia de memória para outro objeto re-utilizável recém encontrado no documento.
Esta situação pode ser boa, no sentido que o objeto re-utilizável descartado não apareça mais no documento, ou ruim se ele for re-aplicado diversas outras vezes.
Em o primeiro caso, nenhuma perda efetiva será obtida, pois como tal objeto não será mais re-utilizado, este pode ser apagado.
Em o último caso, porém, será necessário o re-processamento deste objeto, perdendo a vantagem do recurso da re-usabilidade na sua íntegra.
De esta maneira, alguns RIPs industriais possuem uma opção de indicar que existem muitos objetos sendo re-utilizados no contexto de um documento PDF.
Assim, cada RIP irá priorizar o armazenamento do resultado destes objetos em sua cache e em algum meio de armazenamento auxiliar (como um disco ou uma memória maior), garantindo a re-utilização dos resultados.
Entretanto, se o PDF de entrada apresentar muitos objetos re-utilizáveis que são utilizados uma única vez, as operações de armazenamento tornam- se custosas comprometendo o desempenho do processo de rasterização.
Entre os jobs existentes nas PSPs, não é incomum a existência de PDFs com uma mistura de objetos re-utilizáveis que são aplicados diversas vezes no documento e que são utilizados uma única vez.
Em este contexto, torna- se difícil saber quando ativar a opção de otimização ou não.
Devido a a tal situação, deseja- se realizar uma quebra inteligente dos jobs para processar- los em paralelo, no sentido de que as páginas com objetos re-utilizáveis em comum sejam processadas por o mesmo RIP com o uso otimização de re-usabilidade, enquanto aquelas sem objetos que estão sendo re-utilizados sejam processadas sem a otimização em questão.
De esta maneira, acredita- se será possível superar o desempenho existente no cenário de rasterização.
Para a realização desta quebra, a ferramenta de análise de perfil dos jobs terá de prover uma lista contendo quais objetos são aplicados em quais páginas, para que um processamento adicional consiga ditar uma quebra do job que agrupe os objetos re-utilizáveis de uma maneira inteligente.
Assim, quanto a o escopo de cada objeto re-utilizável (Seção 5.2), além disso a quebra dos jobs já é suportada por meio de uma lista de páginas por parte de a aplicação PDF Splitter (Seção 5.3).
A outra otimização existente no contexto dos RIPs industriais é relativa à transparência.
Como explicitado anteriormente, para que um RIP seja capaz de compor as cores de um objeto transparente é necessário um processamento sobre todas as cores que o objeto em questão está sobreposto, como as de páginas e outros objetos.
Desta forma, alguns RIPs industriais possuem a possibilidade de indicar que um documento PDF contém diversos objetos transparente, aplicando, então, um processo otimizado para a rasterização destes objetos.
Novamente, caso o documento PDF possua diversos objetos opacos juntamente àqueles transparentes, o desempenho da rasterização será comprometido.
Isto se deve ao fato de que o processamento de objetos opacos com esta otimização ativada acarreta um custo adicional ao processamento ao uma lista de quais são as páginas que contêm objetos transparentes, assim como aquelas que não possuem nenhum.
Frente a isso, poderia- se quebrar os jobs de uma maneira inteligente, agrupando as páginas que possuem transparência num ou mais conjuntos para transmitir- los a RIPs com a otimização de transparência ligada.
Por outro lado, as páginas sem transparência seriam enviadas aos RIPs sem a otimização de transparência.
Por fim, um outro estudo que deverá ser realizado diz respeito ao estabelecimento de métricas para aqueles tipos de objetos gráficos não considerados no trabalho atual.
Estes são:
Shading pattern, inline image objects e path objects.
Apesar destes objetos não apresentarem- se comumente entre os jobs, com a análise destes teria- se um conjunto de dados completo para a análise de perfil de quaisquer tipos de jobs.
