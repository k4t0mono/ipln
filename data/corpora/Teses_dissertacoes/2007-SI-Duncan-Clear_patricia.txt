A busca por a qualidade é uma constante nos ambientes corporativos.
Para tanto, as operações de desenvolvimento de software utilizam métricas para mensurar a qualidade dos seus produtos, processos e serviços.
As mesmas devem ser coletadas, consolidadas e armazenadas num repositório central único, tipicamente implementado na forma de Data Warehouse (DW).
A definição do processo de extração, transformação e carga (Etc) das métricas a serem armazenadas nesse repositório, considerando as características do ambiente de desenvolvimento de software (heterogeneidade de fontes, de modelos de processos, de tipos de projetos e de níveis de isolamento) não é uma tarefa trivial.
Este trabalho apresenta um ambiente de data warehousing denominado SPDW+, como solução para a automatização do processo de Etc das métricas.
Esta solução contém um modelo analítico abrangente e elegante, para análise e monitoração de métricas, e é baseada numa abordagem orientada a serviços, aliada à tecnologia de Web Services (Ws).
Além disso, o SPDW+ trata a carga incremental com baixo nível de intrusão, e alta freqüência e baixa latência na coleta das métricas.
Os principais componentes da solução são especificados, implementados e testados.
Os benefícios desta solução são:
I) ser flexível e adaptável para atender às constantes modificações do ambiente do negócio;
Ii) oferecer suporte à monitoração, permitindo a realização de cargas freqüentes e incrementais;
Iii) ser capaz de desonerar os projetos da tarefa, laboriosa e complexa, de captura das métricas;
Iv) manter a liberdade de escolha dos projetos, quanto a os modelos de gestão e às ferramentas de apoio empregadas;
E v) possibilitar que as informações contidas no repositório de métricas estejam coesas e consistentes, para que os dados de diferentes projetos sejam comparáveis entre si.
Palavras Chave: Métricas de Software, Data Warehousing, Etc, Web Services e A os.
A busca por a qualidade é uma constante nos ambientes cooporativos.
Para tanto, as operações de desenvolvimento de software utilizam métricas para mensurar a qualidade dos seus produtos, processos e serviços.
As mesmas devem ser coletadas, consolidadas e armazenadas de maneira consistente, pouco onerosa e estruturada num repositório central único, a partir de o qual seja possível oferecer suporte à tomada de decisão e, dessa forma, permitir a análise, monitoração e previsão das métricas de qualidade.
Segundo, Data Warehouse (DW) é o cerne para a construção de sistemas de apoio à decisão.
A definição do processo de extração, transformação e carga (Etc) dos dados a serem armazenados num DW representa uma das tarefas mais dispendiosas durante o estabelecimento de um ambiente de data warehousing, podendo consumir pelo menos um terço do esforço e do orçamento inicialmente estimado para a construção do mesmo.
A heterogoneidade e as particularidades do ambiente de desenvolvimento de software, tornam a definição de um processo de Etc de métricas uma tarefa não trival.
Poucas propostas de repositório de dados que ofereçam suporte à Etc de métricas, de maneira automatizada, foram encontradas na literatura.
De entre elas destacam- se:
MMR (Multidimensional Measurement Repository), BPI (Busines Process Intelligence), iBOM (Intelligence Operations Manager) e SPDW (SDP Performance Data Warehousing).
No entanto, nenhuma de elas contempla de maneira adequada o suporte à análise, monitoração e previsão da mensuração da qualidade de software, a partir de métricas, de maneira pouco intrusiva e considerando um ambiente dinâmico e heterogêneo de desenvolvimento de software.
Esta pesquisa visa facilitar a gestão de um ambiente de desenvolvimento de sofware ao propor a automatização do processo de Etc de métricas, a serem armazenadas num repositório estruturado na forma de um DW, e utilizar uma abordagem orientada a serviços aliada à tecnologia de Web Services (Ws).
A solução desenvolvida, implementada e testada, denomina- se SPDW+ 1 e é uma extensão substancial em relação a a SPDW.
Esta solução apresenta um modelo analítico abrangente e elegante, que oferece suporte à análise e monitoração segundo a técnica de Earned Value Analysis (EVA) para monitorar custos e prazos de projetos.
Acrônimo pronunciado como &quot;speedway plus».
A solução proposta apresenta como benefícios:
I) ser flexível e adaptável para atender às constantes modificações do ambiente do negócio;
Ii) oferecer suporte à monitoração, permitindo a realização de cargas freqüentes e incrementais;
Iii) ser capaz de desonerar os projetos da tarefa, laboriosa e complexa, de captura das métricas;
Iv) manter a liberdade de escolha dos projetos, quanto a os modelos de gestão e às ferramentas de apoio;
E v) possibilitar que as informações contidas no repositório de métricas sejam coesas e consistentes, para que os dados dos diferentes projetos sejam comparáveis entre si.
Os capítulos deste trabalho estão organizados da seguinte forma.
O Capítulo 2 apresenta o referencial teórico essencial para o entendimento dos conceitos, técnicas e métodos adotados por esta pesquisa.
O Capítulo 3 descreve o cenário de desenvolvimento, sob a perspectiva de três dimensões ortogonais distintas, que envolvem os aspectos relacionados com a automação da mensuração da qualidade de software, tabulados num instrumento de refência.
O Capítulo 4 relata um estudo de caso de uma operação de software, de grande porte, certificada CMM3.
O Capítulo 5 descreve a solução proposta, denominada SPDW+, e a contribuição deste trabalho de pesquisa.
O Capítulo 6 apresenta os testes realizados para avaliar se o processo automatizado de Etc do SPDW+ consegue tratar adequadamente, e em tempo compatível, os aspectos que envolvem a mensuração da qualidade de software, segundo o instrumento de referência.
O Capítulo 7 discorre sobre as considerações finais e os trabalhos futuros.
Este capítulo tem o intuito de permitir um melhor entendimento de alguns conceitos e tópicos relacionados com o tema pesquisa.
Para tanto, o mesmo apresenta uma breve descrição sobre:
PDS e modelos de desenvolvimento de software;
Mensuração de PDS por intermédio de métricas de software, bem como a documentação das mesmas por intermédio de um Programa de Métricas organizacional;
Utilização da técnica de EVA para monitorar prazos e custos durante a execução de um projeto;
Ambientes de data warehousing, incluindo as técnicas e os passos utilizados para a sua construção;
E padrões de Ws envolvidos para desenvolver uma A os (Arquitetura Orientada a Serviços).
A disseminação da utilização de soluções computacionais nos mais variados cenários faz com que as organizações de Tecnologia de Informação (Ti) busquem uma forma de produzir software, com o máximo de lucro, qualidade e satisfação do cliente.
Dentro desse contexto, o desenvolvimento de software deixou de ser uma arte.
O mesmo tornou- se um processo estruturado, organizado e bem definido, seguindo determinados padrões e conceitos, sendo muitas vezes uma tarefa complexa, envolvendo diversas pessoas e resultando em altos custos.
Para endereçar de maneira adequada as questões relacionadas com o desenvolvimento de software, as organizações de Ti têm adotado estratégias de desenvolvimento, baseadas em processos, métodos e ferramentas.
Essas estratégias são conhecidas como modelos de processo de software, também chamados de paradigmas de engenharia de software.
Esses modelos devem ser escolhidos conforme a base e a natureza do projeto e da aplicação, bem como os métodos e as ferramentas utilizadas durante a concepção dos produtos.
Por essas razões, a Engenharia de Software propõe diversos modelos de desenvolvimento, estabelecidos com base nas diferentes perspectivas e necessidades do PDS.
É importante ressaltar que essa variedade de modelos de desenvolvimento de software é conseqüente da realidade distinta dos clientes e dos usuários dos sistemas.
A seguir é apresentada uma breve descrição de alguns modelos de processo de software.
Maiores detalhes sobre esse assunto podem ser encontrados em.
Modelo Cascata ou Seqüencial Linear:
Divide a missão complexa de desenvolvimento de software em vários passos lógicos (projeto, codificação, teste, entre outros), com entregáveis intermediários, que conduzem ao produto final.
Para assegurar a qualidade desses entregáveis cada passo possui um critério de validação, parâmetros de entrada e de saída.
Modelo Espiral: Cada porção do produto e nível de elaboração deve envolver a mesma seqüência de passos (ciclos), começando no centro da espiral, sendo que, cada fase de desenvolvimento compreende apenas um único ciclo da espiral.
É tipicamente representado por duas dimensões, onde a dimensão radial representa o custo cumulativo para completar um determinado passo, enquanto que a dimensão angular caracteriza o progresso realizado obtido a partir de a completude de cada ciclo do espiral.
Modelo Evolutivo ou Iterativo: Inicia com um subconjunto de requisitos para desenvolver um subconjunto de produtos, que satisfaz as necessidades essenciais de um determinado cliente.
Esse modelo tem por objetivo proporcionar um veículo para análises e treinamento para o cliente, além de oferecer experiência de aprendizagem ao desenvolvedor.
Com base nas análises recorrentes de cada produto intermediário, o projeto e os seus requisitos são modificados numa série de iterações.
Esse modelo combina a prototipação com a rigidez clássica do modelo cascata.
Além disso, o mesmo tem muito em comum com o modelo espiral, principalmente com relação a a gerência de riscos e a prototipação.
Processo Unificado: Consiste na repetição de uma série de ciclos durante o tempo de vida de desenvolvimento de um sistema.
Cada ciclo concluído é considerado uma versão do produto pronta para distribuição.
As versões devem apresentar um conjunto relativamente completo e consistente de artefatos, possivelmente incluindo manuais e um módulo executável do sistema, podendo ser distribuídas aos usuários internos e externos.
Esses ciclos são formados por quatro fases distintas, denominadas:
Início, elaboração, construção e transição, sendo que cada uma de elas pode ser subdivida em iterações.
Seja qual for o modelo de desenvolvimento de software adotado para um projeto, a realização do mesmo sempre pode ser planificada em função de o tempo e estruturada segundo uma hierarquia de tarefas.
Assim, em linhas gerais, pode- se dizer que um determinado projeto apresenta versões, compostas de iterações que, por sua vez, são constituídas por fases e atividades.
De essa forma, quando todas as atividades que compõem um determinado nível hierárquico forem concluídas, o mesmo é considerado finalizado e assim, sucessivamente, até que o projeto seja finalizado por completo.
Segundo, a atividade de mensuração é crucial para o avanço de qualquer ciência.
O progresso científico é realizado a partir de observações e generalizações baseadas em dados e medições, e na derivação de teorias como resultados, bem como na confirmação ou refutação dessas teorias, por intermédio de testes de hipóteses, baseados em dados empíricos avançados.
Ainda, conforme o mesmo autor, a qualidade deve ser definida operacionalmente, mensurada, monitorada, gerenciada e melhorada.
Em o contexto de desenvolvimento de software a mensuração da qualidade, dos processos, produtos e projetos, é determinada por métricas.
Segundo os modelos de qualidade, é necessário realizar a mensuração das características e dos parâmetros de qualidade do PDS nos seus diferentes estágios, bem como estabelecer métricas e modelos para assegurar que o processo de desenvolvimento esteja sob controle e no caminho do cumprimento dos objetivos de qualidade do produto.
O desafio de capturar métricas de software está em ter a certeza que a coleta proporciona informação útil à gestão de qualidade e que essa captura é realizada de uma forma pouco intrusiva para o time de desenvolvimento.
Intrusão neste contexto significa deslocar recursos do projeto (colaboradores), de suas atividades normais, para realizar a coleta de métricas.
Coletar dados de Engenharia de Software pode ser dispendioso.
No entanto, coletar dados e analisar- los produz inteligência e conhecimento sobre os projetos e o seu PDS, além de ser vital para o sucesso do negócio.
Uma métrica é uma função mensurável, cujas entradas são dados de software e seu resultado corresponde a um único valor numérico, que pode ser interpretado como o grau de qualidade do software, sendo esse afetado por um determinado atributo.
Esse último pode ser definido como &quot;uma propriedade física ou abstrata mensurável de uma entidade».
Um fator de qualidade é um tipo de atributo orientado à gestão de software, que contribui para sua qualidade.
Ainda conforme a IEEE, as métricas de software são classificadas como diretas e derivadas.
As diretas não dependem de nenhum outro atributo, nem necessitam de validação, enquanto que as derivadas são definidas através de outros atributos e precisam ser validadas.
Ambas têm por objetivo proporcionar uma informação quantitativa sobre um processo ou um produto, sendo importantes às atividades de gerenciamento de software e ao controle de desempenho de processos, tanto no âmbito da organização, como no ambiente de cada projeto.
Segundo, as métricas de software também podem ser classificadas em três categorias:
Produto, processo e projeto.
A primeira descreve as características do produto (e.
g Ainda, conforme o mesmo autor, as métricas podem apresentar domínios de valores numa hierarquia de quatro escalas, descritas abaixo na forma bottom-up:
Escala nominal:
A classificação é considerada a operação mais simples da ciência e representa o nível mais baixo de mensuração.
Classificar consiste na tarefa de separar elementos de acordo com um determinado atributo.
Por exemplo, com base nos seus ciclos de vida pode- se classificar os modelos de desenvolvimento de software em:
Escala ordinal:
Refere- se a operações de mensuração em que as medidas podem ser comparadas de maneira ordenada.
Por exemplo, os processos de um determinado projeto podem ser mensurados conforme seu grau de aderência aos modelos de maturidade em:
Totalmente aderente, parcialmente ou não aderente.
Segundo a hierarquia de mensuração, a escala ordinal representa um nível maior que a escala nominal.
A partir de a primeira é possível agrupar as métricas em categorias e estabelecer uma ordem de comparação.
A escala ordinal é considerada antissimétrica e transitiva, logo (A\&gt; B) é verdadeiro, porém (B\&gt; A) é falso.
Além disso, se (A\&gt; B) e (B\&gt; C) então (A\&gt; C).
Assim, com base nessas propriedades, é possível oferecer informações sobre a magnitude da diferença entre elementos distintos.
Escala intervalar e de razão:
Escala intervalar indica a diferença exata entre dois pontos de medição, aceita operações matemáticas de adição e subtração e deve apresentar uma unidade de medida bem definida.
Já a escala de razão, é obtida quando um zero absoluto pode ser alocado na escala intervalar.
Assim, essa última representa o nível mais alto de mensuração, e permite a aplicação de todas as operações matemáticas, incluindo divisões e multiplicações.
Em ambas, os valores podem ser expressos, tanto em inteiros, como em reais.
Essas escalas são consideradas hierárquicas, pois os níveis mais altos apresentam as propriedades dos níveis inferiores.
O nível mais simples e limitado é definido por a escala nominal, permitindo somente a detecção de categorias.
Em seguida, tem- se a escala ordinal, que possibilita diferenciar patamares.
Posteriormente, tem- se a escala intervalar, que proporciona o posicionamento de valores em relação a um ponto arbitrário.
Finalmente, a mais importante de todas é a escala de razão, que admite a comparação de valores em termos absolutos.
Assim, quanto mais alto o nível da mensuração, melhores são as análises que podem ser aplicadas.
Para permitir a institucionalização e fomentar o uso das métricas de software, as mesmas devem ser documentadas e organizadas por intermédio de um Programa de Métricas.
Esse deve conter a definição das métricas adotadas por a organização, as suas respectivas unidades de medida, bem como as equações utilizadas para o seu cálculo, no caso de as métricas derivadas.
Tanto a norma 1061, como os modelos de qualidade (CMM (Capability Maturity Model) e CMMI (Capability Maturity Model Integration)), não definem um conjunto mínimo de métricas.
Ambos apenas afirmam que devem ser estabelecidas métricas úteis e significativas, conforme as necessidades dos projetos e da organização.
Em esse sentido, o principal desafio durante a definição de um Programa de Métricas está intrinsecamente ligado com a identificação da relação entre os objetivos da organização e os objetivos das métricas (apud), buscando proporcionar informações adequadas aos diferentes níveis organizacionais.
Segundo a norma 1061, cada métrica que pertencente ao Programa de Métricas organizacional deve apresentar:
O seu respectivo nome;
O custo, os benefícios e os impactos associado à sua utilização;
As faixas de valores esperados que correspondam ao desempenho das métricas;
Iv) as ferramentas utilizadas para armazenar, computar e avaliar os resultados;
A aplicabilidade da métrica, descrevendo como a mesma pode ser utilizada e quais as suas áreas;
Os valores de entrada necessários para o cálculo da métrica;
E um exemplo de utilização para cada uma das métricas.
As métricas podem ser logicamente agrupadas em áreas de qualidade, segundo as expectativas e/ ou necessidades de análise de uma determinada organização.
A definição de um Programa de Métricas que contemple todas as áreas de qualidade desejadas por uma organização é de suma importância, já que a análise de uma métrica de forma isolada apresenta pouco valor.
Avaliar- las dentro de o contexto de um Programa de Métricas propicia acompanhar a história de um processo ou organização.
O conceito de valor agregado teve sua origem no Departamento de Defesa Americano custos de projetos e programas.
A análise do mesmo surgiu da necessidade de realizar previsões confiáveis relacionadas com custos e prazos, tendo como foco a relação entre o custo real e o produto físico produzido no projeto, por intermédio de uma quantidade fixa de trabalho e dentro de um prazo determinado.
O conceito de valor agregado exige que as medidas de despesa-produto sejam estabelecidas dentro de um cronograma físico de projeto.
Assim, a partir de a relação entre o valor agregado e o trabalho estimado no tempo, é possível realizar um controle do andamento das atividades de maneira mais precisa do que com método tradicional, que analisa esses fatores de forma isolada.
A EVA permite tomar ações corretivas e preventivas com antecedência, pois oferece, aos gestores do projeto, uma forma de acompanhar o andamento das atividades para verificar se as mesmas estão sendo realizadas dentro de o custo estimado, ou não, e se há algum tipo de atraso ou aceleração da execução das atividades.
Para tanto, são definidos alguns termos Valor Estimado (VE):
Indica a parcela de orçamento que deveria ser gasta, segundo as estimativas realizadas no início do projeto (linha base), considerando as atividades, as atribuições ou os recursos envolvidos.
O VE é definido como:
O valor total do orçamento planejado, dividido por o tempo total e acumulado até a data atual do projeto ou de status.
Valor Agregado (VA):
Indica a parcela do orçamento que deveria ser gasta, considerando o trabalho realizado até o momento e o custo da linha base das atividades, das atribuições ou dos recursos.
Esse valor é obtido por intermédio do produto entre o percentual de trabalho completado até a data de status e o valor total do orçamento estimado para determinada atividade.
Custo Real (Cr):
É o custo total decorrido do trabalho já realizado, por um recurso ou atividade, até a data de status ou data atual do projeto.
O valor do Cr deve ser obtido levando em conta os mesmos dados usados no cálculo de VE e VA.
Por exemplo: A EVA é realizada a partir de a relação entre esses termos.
A Figura 1 contém uma representação gráfica de um possível conjunto de valores de VE, VA e Cr..
O gráfico apresenta como ordenada os valores de tempo e como abscissa o custo (valor monetário).
As três curvas do gráfico representam os valores dos termos ao longo de o tempo, até uma determinada data de status.
As posições relativas entre as mesmas variam conforme os valores do projeto que está sendo analisado.
Observa- se no gráfico que a referência para análise é a data de status.
Cabe salientar que somente VE ultrapassa essa data, já que representa o valor estimado para toda a atividade ou projeto.
Para analisar a relação entre os valores de VE, VA e Cr foram definidas algumas variações:
Variação de Custo (VC):
É a diferença entre o valor agregado (VA) e o custo real (Cr).
Se a mesma for positiva, o custo está abaixo de o valor estimado;
Caso contrário encontra- se acima de o orçamento estipulado.
VC=0  dentro de o orçamento VC 0 fora do orçamento Variação de Prazo (Vp):
É a diferença entre o valor agregado (VA) e o valor estimado (VE).
Se a mesma for positiva, o projeto está adiantado;
Caso o contrário é considerado atrasado.
A Figura 1 ilustra como essas variações são identificadas no gráfico da EVA.
A partir de a mesma pode- se concluir que a variação de custo (VC) e de prazo (Vp) pode ser considerada como a diferença entre as curvas VA e Cr e as curvas VA e VE, respectivamente.
Quanto mais distante a curva Cr estiver das demais maior será a variação do prazo e no custo planejado para aquela data de status.
Os valores VC e Vp ainda podem ser convertidos em indicadores de eficiência para refletir o desempenho de custos e de prazos de qualquer projeto.
Índice de Desempenho de Custo (IDC):
Relaciona o custo do valor agregado com o executado.
Um valor de IDC menor que 1 indica que orçamento consumido foi menor que o planejado.
Já um valor maior que 1 representa que os custos planejados não foram atingidos.
O IDC é igual à razão entre VA e Cr e mostra qual a conversão entre os custos reais consumidos por o projeto e os valores agregados num determinado período.
Por exemplo, um IDC $= 0,83 indica que, para cada R$ 1,00 de capital realmente consumido, apenas R$ 0,83 estão sendo convertidos fisicamente em produto e que existe uma perda de 0,17.
IDC=1  menor que o planejado IDC 1 maior que o planejado Índice de Desempenho de Prazo (IDP):
O IDP é usado, em adição ao andamento do cronograma, para prever a data de término, e representa a taxa de conversão do valor estimado (VE) em valor agregado (VA).
O mesmo também pode ser utilizado em conjunto com o IDC para prever as estimativas de término do projeto.
Por exemplo, um IDP $= 0,85 indica que 85% do tempo estimado no orçamento (VE) foi convertido em trabalho e que houve uma perda de 15% no tempo disponível.
IDP=1  dentro de o prazo IDP $= 0 fora de o prazo Um data mart consiste numa base de dados para armazenar um único processo de negócio ou um grupo de processos de negócio voltados para alcançar determinado objetivo.
Em esse sentido, um DW é definido como a união de todos os data marts de uma organização.
Especificamente, o DW pode ser considerado uma fonte de consulta de dados, que visa disponibilizar- los de maneira compreensível e otimizada, a partir de campos précalculados e de um modelo dimensional de dados.
O DW serve como uma implementação física do modelo de dados de suporte à decisão organizacional, armazenando informações cruciais à tomada de decisão estratégica.
Já um ambiente de data warehousing é caracterizado por contemplar o processo de Etc e a manutenção dos dados do DW.
O estabelecimento desse ambiente é útil por o ponto de vista da integração de fontes de dados heterogêneas.
Diversas organizações tipicamente armazenam e mantêm variados tipos de dados provenientes de grandes bases de informações heterogêneas, autônomas e distribuídas.
Assim, a consolidação desses dados num ambiente único, de maneira estruturada e concisa, permite a aplicação de técnicas de consultas com o intuito de auxiliar a tomada de decisão, melhorar o entendimento do negócio e permitir a manutenção do histórico da organização.
Tipicamente, os dados num DW estão estruturados segundo um modelo dimensional.
Seus dados estão empacotados num formato projetado para possibilitar mudanças e aumentar o entendimento e o desempenho das consultas.
O modelo dimensional apresenta, como componentes básicos, tabelas dimensão e fato.
As tabelas dimensão representam as diferentes perspectivas de análise de um determinado fato.
A maioria das dimensões contém atributos textuais, essenciais para a construção das consultas e agrupamento dos fatos.
Logo, entende- se por dimensões as perspectivas de uma base de dados que possam gerar registros.
Tais dimensões são basicamente organizadas em torno de um termo central, onde os registros são inseridos, representado por a tabela fato.
As tabelas fato são consideradas as estruturas básicas e primordiais do modelo dimensional.
As mesmas devem conter mensurações do negócio definidas como métricas.
Em a maioria das vezes os fatos apresentam métricas com valores aditivos e numéricos.
Além disso, cada tabela fato apresenta relacionamentos do tipo muitos para muitos e seus atributos não precisam ser necessariamente métricas de software;
Pois podem representar qualquer atributo do negócio.
O que determina a escolha do modelo analítico dimensional a ser desenhado para o DW são as necessidades do negócio e as características de suas bases de dados.
Em o que tange a essa definição, é possível representar diferentes tipos de modelos analíticos:
Modelo Estrela: Caracteriza- se essencialmente por conter uma única tabela fato e várias dimensões, cujas chaves compõem essa tabela.
Modelo floco de neve: Difere- se do modelo estrela por admitir que as tabelas dimensões possuam outros relacionamentos.
Modelo Constelação de Fatos: De uma forma mais abrangente, permite dispor mais de uma tabela fato, as quais possuem suas dimensões, e tais dimensões, também podem admitir outros relacionamentos.
A idéia fundamental do modelo analítico dimensional está relacionada com a possibilidade de representar dados de negócio na forma de cubos, onde as células contêm os valores das métricas e as arestas significam as dimensões dos dados.
Tipicamente, em modelos dimensionais de negócio reais, é comum encontrar cubos com dimensionalidade maior que três, podendo variar seu tamanho entre quatro e quinze dimensões.
Os recursos OLAP (On-Line Analytical Process) permitem a realização de consultas em base de dados dimensionais e a apresentação dos seus resultados, textuais ou numéricos, de maneira clara e simplificada, considerando as diferentes perspectivas de análise e níveis de sumarização.
Esses recursos admitem que os usuários do DW possam explorar os dados através de tabelas pivotantes, possibilitando uma apresentação tabular, e a interação por operações OLAP de drill-down e drill-up.
Essas operações, quando implementadas de maneira correta, mesclam atributos hierárquicos ou não de todas as dimensões disponíveis.
Um aspecto relacionado com as operações OLAP que merece destaque é a possibilidade de executar consultas à base de dados sem a necessidade de editar código e sem ter conhecimento técnico aprofundado do negócio ou da interface OLAP utilizada.
Por essas razões, os recursos OLAP apresentam as características ideais para oferecer informações diferenciadas, conforme distintos papéis organizacionais e níveis de gestão.
Para que os dados possam ser devidamente disponibilizados no ambiente de data warehousing, é necessário estabelecer um processo de Etc..
Este tem por objetivo principal proporcionar, aos usuários do DW, dados estruturados, consistentes e de qualidade para oferecer suporte à tarefa de tomada de decisão de negócio.
Em o contexto de PDS, a Etc de métricas de software é responsável por capturar, transformar e carregar os dados dos processos, finalizados e em execução, e dos artefatos produzidos e utilizados por eles.
As seções subseqüentes abordam cada uma das etapas desse processo, detalhando as suas particularidades e os seus aspectos relevantes.
A extração representa o primeiro passo para inserir dados no ambiente de data warehousing.
Extrair significa ler e entender as fontes de dados originais, copiando apenas as partes necessárias para uma área de armazenamento temporária.
A partir de esta última, os dados podem ser transformados conforme o modelo dimensional do DW alvo.
As fontes originais podem apresentar localizações físicas distintas, estruturas de dados diferentes e ambientes de execução variados, caracterizando um ambiente heterogêneo.
Além disso, essas fontes também podem conter um volume de dados bem maior do que se deseja extrair.
Por essa razão, nem todas as informações mantidas nas bases de dados das ferramentas originais agregam valor ao negócio e precisam ser consolidadas e analisadas.
Após a extração, os dados devem ser temporariamente mantidos numa área de armazenamento especial, denominada DSA (Data Staging Area).
No entanto, para que os mesmos possam ser consolidados e armazenados no DW, segundo o modelo dimensional, uma série de transformações precisa tipicamente ser realizada.
A mesma inclui atividades para:
Limpar ruídos;
Resolver domínios;
Solucionar domínios conflitantes, dados faltantes e formatar valores conforme o padrão pré-estabelecido;
Eliminar campos selecionados dos sistemas legados que não são úteis ao DW;
Combinar fontes de dados, verificando a integridade entre as chaves primárias ou realizando combinação entre atributos não chave, incluindo a procura por textos equivalentes em códigos fonte de sistemas legados;
Segundo, a preparação dos dados, efetuada numa área de armazenamento temporária denominada DSA, é considerada uma das etapas mais importantes e críticas de um projeto de DW.
Em essa área é executado um conjunto de processos para limpar, transformar, combinar, duplicar, arquivar e preparar os dados para serem utilizados por o DW.
Esses processos são determinados por atividades simples de ordenação e processamento seqüencial, e em alguns casos, não precisam ser baseadas no modelo relacional.
Para tanto, propõe um plano de dez passos para a criação de uma DSA:
Criar um plano de alto nível, por intermédio de um esquema, ilustrando o fluxo de dados da fonte para o destino, sendo o mesmo representado numa única página.
Testar, escolher e implementar uma ferramenta de data staging.
Diminuir a granularidade da tabela alvo, esboçando graficamente qualquer transformação e reestruturação de dados complexa, e ilustrando, também, o processo de geração de chaves surrogates.
Construir e testar a carga de uma dimensão estática.
O objetivo principal deste passo é desenvolver uma infra-estrutura que suporte conectividade, transferência de arquivos e resolva problemas de segurança.
Construir e testar minuciosamente processos de mudanças para uma dimensão.
Construir e testar cargas nas dimensões restantes.
Construir e testar cargas nas tabelas fato históricas, incluindo substituição de chaves surrogates.
Construir e testar o processo de carga incremental.
Construir e testar a carga em tabelas agregadas e/ ou carga em MOLAP (Multidimensional Online Analytical Processing).
Projetar, construir e testar a automação da aplicação de staging.
Em o final do processo de transformação os dados devem apresentar a forma adequada para que sejam inseridos no DW, conforme o modelo dimensional.
Assim, os mesmos podem ser devidamente carregados da DSA para o DW.
Com relação a a carga, a maioria das implementações de processos de Etc, em ferramentas líderes de mercado, realiza a mesma de maneira incremental, mesmo que os dados envolvidos sejam snapshots extraídos somente uma vez por mês.
Em esse sentido, é muito mais eficiente carregar incrementalmente apenas os registros que apresentaram modificações ou que foram adicionados após a última carga.
Usualmente, a carga incremental é baseada em uma data de transição ou algum outro tipo de indicador.
A data de última carga também é considerada importante para realizar a manutenção do processo de carga.
DW é tipicamente construído para conter dados que representam fatos encerrados.
Por essa razão, não precisam se preocupar com acontecimentos em andamento, caracterizados por apresentar alterações constantes nos valores armazenados.
No entanto, para manter um histórico adequado dos fatos em andamento, é necessário prover informações pertinentes no DW.
Por essa razão, o processo de carga deve estabelecer regras para lidar com atributos que sofrem alguma alteração em relação a os valores anteriormente armazenados no DW.
A A os pode ser definida como uma coleção de serviços, que apresentam baixo acoplamento entre si, sendo que os mesmos encapsulam componentes de software reutilizáveis e se comunicam com outros serviços.
Essas interações são realizadas através de tecnologias de comunicação, que podem envolver uma troca de dados entre dois ou mais serviços que estejam coordenando alguma atividade.
Ws são considerados a tendência de tecnologia para desenvolver serviços que implementam as funções de negócio, segundo uma A os.
Isso está relacionado com a padronização do protocolo e da linguagem de comunicação utilizada por esses serviços.
Estudos, como, declaram que a utilização de XML como a linguagem padrão de comunicação entre aplicações, permite que a troca de informação por intermédio de mensagens SOAP (Simple Object Access Protocol) possa ocorrer em ambientes heterogêneos e de maneira simples.
A ubiqüidade do protocolo de comunicação e de transporte, utilizado por essas mensagens, também contribui significativamente para disseminar sua utilização.
No entanto, cabe salientar que os Ws não foram desenvolvidos para apresentar uma visão lógica orientada a negócios, sendo essa carência suprida por o uso de uma A os.
A utilização de Ws, definida conforme uma A os, permite que as aplicações encapsuladas por os serviços, e eventualmente escritas em linguagens distintas, possam ser executadas em servidores diferentes, sem a necessidade de considerar como cada um desses serviços foi desenvolvido.
Por intermédio da tecnologia de Ws é possível mapear os processos de negócio em serviços e, assim, publicar- los e descobrir- los dinamicamente, tanto dentro de os limites de uma corporação como fora de eles.
Para isso, os Ws podem serguir o modelo teórico definido por a A os, o qual é baseado em três entidades e três operações básicas, sendo a combinação entre essas entidades a responsável por esse paradigma de comunicação entre aplicações.
A Figura 2 ilustra as principais entidades de uma A os (Service Provider, Registry e Service Consumer) e as operações básicas entre elas (Register, Find e Bind and Invoke):
Service Provider (Provedor de Serviço):
Entidade que cria os Ws, descrevendo os serviços num formato padrão, compreensível a qualquer cliente que queira utilizálos.
Além disso, publica através da operação Register, os detalhes desses serviços num registro público (Registry), disponível a todos os interessados.
Registry (Registro de Serviço):
Local onde os provedores registram seus serviços, sendo considerado uma central de serviços, onde os clientes podem pesquisar- los e encontrar- los conforme suas necessidades.
Service Consumer (Cliente do Serviço):
É a entidade que solicita o serviço, por intermédio da operação Find, ao Registro de Serviço que, por sua vez, encontra um provedor de serviços disponível e repassa o pedido para esse último, utilizando a operação Bind and Invoke.
Os Ws, baseados nessas entidades que compõem a A os, são publicados para os clientes na web quando são registrados no diretório UDDI (Universal Description, Discover and Integration).
A estrutura e o modo de funcionamento dos componentes XML, SOAP, WSDL (Web Services Description Language) e UDDI, responsáveis por os Ws, são descritos nas próximas seções.
A utilização de XML como linguagem padrão para a comunicação dos Ws foi de extrema importância para a sua disseminação e aceitação.
A adoção desse padrão garante simplicidade e heterogeneidade de linguagens de programação e plataformas, devido a o fato de ser muito próximo a a linguagem natural, extensível e não requerer conhecimentos aprofundados para sua utilização.
Além disso, garante flexibilidade à troca de mensagens entre os serviços, pois é considerada uma linguagem descritiva baseada em etiquetas (tags), que podem ser interpretadas conforme a necessidade do cliente, permitindo que a mesma informação possa ter significado distinto para duas ou mais aplicações.
Cabe salientar que, para isso, elas devem respeitar um mesmo esquema (XML--schema) conhecido por as partes envolvidas, o qual é determinado durante o contrato do serviço.
Esse contrato é definido como a ligação entre o cliente e o Ws, e sua validade é expirada quando todas as iterações, realizadas por as trocas de mensagens, são executadas.
Já os XML--schemas exprimem vocabulários e definem as regras para escrever os documentos XML, bem como sua estrutura, seu conteúdo e a sua semântica.
A comunicação entre os Ws é feita através do protocolo SOAP, que tem por objetivo organizar as informações escritas em XML de maneira estruturada, permitindo a troca de mensagens entre as aplicações.
No entanto, SOAP apenas define o formato dessas mensagens, deixando a semântica por conta das aplicações.
O protocolo SOAP baseia- se na linguagem XML, diferentemente do formato binário utilizado por outros protocolos de comunicação de sistemas distribuídos, como:
RMI, DCOM e CORBA, os quais usam RPC (Remote Procedure Call) para a comunicação entre seus objetos.
As chamadas RPC não são facilmente adaptáveis ao padrão web, visto que o protocolo Http não foi desenvolvido para dar suporte às aplicações distribuídas.
Porém, SOAP permite empacotar chamadas RPC dentro de o conteúdo das mensagens trocadas entre as aplicações.
De essa forma, possibilita, além de a transmissão de dados, chamadas e respostas de métodos de RPC entre o cliente e o servidor SOAP.
O conceito de SOAP é simples:
Se uma única conexão é utilizada para chamar uma função ou um objeto de um método, disponibilizado por um servidor, esse servidor não deve se preocupar com o tipo do cliente que realizou o chamado, desde que os dados que estão sendo transferidos por a conexão atendam os padrões de formatação definidos por o protocolo SOAP foi desenvolvido para utilizar o padrão Http, considerado o padrão de comunicação mais popular para a utilização de Ws.
Logo, apresenta tráfego livre entre firewalls e servidores proxy, sendo suportado por todos os servidores e navegadores web.
Por esse motivo, SOAP caracteriza- se como um protocolo confortável para a comunicação entre aplicações remotas.
Além de o protocolo Http, as mensagens SOAP também podem ser transmitidas por FTP e SMTP.
O cabeçalho das mensagens SOAP especifica detalhes para o protocolo de transporte, enquanto que o corpo, tipicamente, contém os parâmetros utilizados para chamar os métodos dos serviços e os seus resultados.
Segundo, um servidor SOAP em Java pode ser considerado um listener (um ouvinte permanente), executado dentro de um servidor web onde estão hospedados os Ws, que ficam aguardando chamadas de clientes.
A Figura 3 ilustra esse servidor (Web Server), que apresenta três componentes principais:
Um Gerenciador de Serviços, um Tradutor XML e uma Lista de Serviços.
O Gerenciador de serviços recebe a chamada do cliente (SOAP Request) e verifica se o serviço requisitado por ela encontra- se na lista de serviços disponíveis no servidor web.
Caso a busca tenha sucesso, o Gerenciador de Serviços encaminha a chamada ao Tradutor XML, que é o responsável por processar a mensagem SOAP, para que a chamada do cliente possa ser compreendida por a aplicação que implementa as funcionalidades do Ws.
Finalizada a execução da aplicação, o seu resultado é repassado ao tradutor para que seja incorporado na mensagem de resposta (SOAP Response), a qual é encaminhada ao Gerenciador do Serviço, que a transmite via Http ao cliente.
A especificação WSDL, definida em XML, permite que os provedores de Ws possam disponibilizar suas interfaces de acesso de forma padronizada.
O conteúdo dessa especificação consiste em todas as informações necessárias para que o cliente possa acessar os Ws, tais como:
Localização, métodos disponíveis e tipos de dados envolvidos.
Uma WSDL pode ser considerada um contrato entre o cliente e o provedor do serviço, definindo o que pode ser feito, onde o serviço está hospedado e como pode ser acessado.
De essa forma, o cliente não precisa ter conhecimento prévio da localização dos serviços, da maneira como foram implementados, da linguagem e da plataforma utilizada.
Portanto, a WSDL garante a interoperabilidade entre linguagens e plataformas, e sua padronização é definida por a W3C Web Services Description Working Group, composta por organizações como Sun, IBM, Hp, entre outras.
As informações que compõem a WSDL estão dispostas na forma de metadados padronizados, os quais são organizados na forma de etiquetas (tags).
Essa estrutura permite que um parser XML possa percorrer o arquivo WSDL e reconhecer as informações que devem ser extraídas para a especificação do conjunto de mensagens XML SOAP do cliente.
Os valores dessas informações são armazenados em variáveis locais, que podem ser utilizadas para gerar outras aplicações.
Concluída essa etapa de extração e geração da aplicação o cliente está apto para se conectar ao Ws.
O documento WSDL é constituído por duas partes denominadas:
Abstrata e concreta, também chamadas de não-funcionais e funcionais, respectivamente (Figura 4).
A descrição concreta é composta por elementos que são orientados para vincular fisicamente o cliente ao serviço, enquanto que a abstrata contém elementos orientados para descrever as competências do Ws.
A parte abstrata tem a responsabilidade de definir o que um Ws pode fornecer em termos de funcionalidade e é formada por os seguintes elementos:
Types apresentam a estrutura dos tipos de dados que compõem o serviço;
Messages apresentam a comunicação efetiva entre o cliente e o serviço, definindo quais os serviços que devem ser executados;
Operations são chamadas de serviços, onde as operações possíveis são:
Requisição/ resposta (cliente faz uma requisição e o Ws responde), solicitação/ resposta (Ws envia uma mensagem ao cliente), sentido único (o cliente envia uma mensagem ao Ws, mas não espera resposta) e notificação (Ws envia uma mensagem ao cliente, sem esperar uma resposta).
Interações síncronas são definidas através de operações requisição/ resposta ou solicitação/ reposta, enquanto que interações assíncronas são estabelecidas utilizando sentido único ou notificação;
O restante do documento WSDL é composto por a parte concreta, cuja finalidade é estabelecer ligações físicas entre os Ws, subdividida em bindings e services and ports.·
Bindings realizam o vínculo entre os atributos concretos e os abstratos no documento WSDL, fornecendo informações, tais como:
O protocolo e o endereço do Ws.
Services e Ports apresentam a porta real e o IP do Ws que estão sendo representados por o documento WSDL.
Os Ws podem ser disponibilizados por meio de registros de Internet, estruturados conforme alguns padrões, chamados UDDIs.
A estrutura desses padrões possibilita que o cliente de serviços possa ir ao diretório e acessar o serviço que melhor satisfaz a sua necessidade em tempo de execução.
As regras e padrões que definem o UDDI são estabelecidos por a Organization for Advancement Structured Information Standards (OASIS) O UDDI é um meio de publicar e encontrar serviços.
Sua utilização tem um papel fundamental na disponibilização dos Ws, pois proporciona, de forma centralizada, a descoberta de serviços disponíveis, suas localizações e a forma de interação com os mesmos.
A utilização desse centralizador permite que novas versões de um determinado serviço possam ser disponibilizadas, sem que o sistema distribuído seja interrompido.
Além disso, pode- se permanecer com os serviços de versões anteriores que satisfaçam determinados clientes e com serviços novos que atendam especificações recentes de diferentes clientes.
A possibilidade de reutilização de código também representa uma parcela importante no que se refere à agilidade frente a as modificações exigidas por o negócio.
Como os serviços são considerados aplicações bem definidas e com baixo acoplamento, podem ser compostos ou alterados para dar suporte às novas necessidades, de forma mais otimizada, por intermédio da simples publicação e edição de novos serviços.
O diagrama, ilustrado por a Figura 2, apresenta a interação entre os componentes que constituem uma A os, especificados nas seções anteriores deste capítulo, e os padrões utilizados por os Ws.
O ciclo básico de funcionamento dos Ws (Figura 5) inicia com sua disponibilização num provedor de serviços (Service Provider), na forma de um servidor web.
O primeiro passo para oferecer esses serviços aos clientes ocorre com sua publicação (Publish) num repositório centralizado (Service Repository), implementado por meio de um repositório UDDI.
Após a publicação, todas as informações necessárias para encontrar e estabelecer contratos entre serviços e clientes estão disponíveis.
Independentemente da publicação dos serviços, os clientes podem pesquisar (Find) (2) por eles através de buscas no repositório UDDI que, por analogia, pode ser comparado às páginas amarelas de um guia telefônico, onde se encontram as informações dos serviços disponíveis, dos seus fornecedores e da sua localização.
Quando o cliente encontra o serviço que atende às suas necessidades, recebe os dados necessários para estabelecer um contrato com o fornecedor do serviço.
O cliente, de posse desses dados, estabelece o contato com o Ws que apresenta as funcionalidades desejadas (Bind) (4) através de mensagens SOAP, que contêm a localização do serviço, bem como seus métodos, seus tipos de dados e os seus parâmetros.
O processador SOAP (listener), presente no servidor web, processa essa mensagem enviada por o cliente, através dos dados presentes no seu cabeçalho e procura o serviço na lista de serviços.
Quando o encontra, estabelece a ligação e repassa os dados da sua chamada.
Assim, a aplicação que constitui o serviço é executada e os resultados são repassados ao processador SOAP, que é responsável por compor a mensagem de resposta e enviar- la ao cliente.
Essas interações entre os clientes e os serviços são feitas diretamente, conforme a quantidade de iterações necessárias para que o serviço consiga concluir o objetivo.
Esses serviços também podem ser chamados por outros serviços, não sendo essa ação exclusiva dos clientes.
A maneira como os Ws são disponibilizados permite que os contratos entre clientes e fornecedores de serviços possam ser feitos dinamicamente, conforme as necessidades dos clientes e as facilidades oferecidas por os fornecedores de serviços.
Além disso, alterações e novas versões podem ser disponibilizadas em paralelo com os sistemas existentes, proporcionando flexibilidade e adaptabilidade do negócio.
Outro ponto que merece destaque é a padronização de comunicação.
Com ela, os clientes não precisam ter conhecimento da linguagem e do código envolvido na aplicação que oferece o serviço, fator que garante a heterogeneidade de ambiente e a interoperabilidade de aplicações.
Este capítulo descreve o cenário onde esta pesquisa está inserida, de maneira sistemática, por intermédio de três dimensões ortogonais:
1) nível de mensuração, 2) atividades de supervisão, e 3) plataforma computacional.
Para tanto, são relatados os aspectos específicos de cada uma dessas dimensões e, posteriormente, é realizada uma leitura das suas possíveis interações, buscando caracterizar àqueles aspectos essenciais para coletar métricas, e oferecer suporte à mensuração da qualidade de software.
Faz- se uso dos termos e descrições referentes a métricas de software, feitas no Capítulo 2.
Em o desenvolvimento de software, a qualidade do produto está diretamente relacionada à qualidade do seu processo de desenvolvimento.
A norma ISO 9000 define qualidade como o grau em que um conjunto de características inerentes a um produto, processo ou sistema cumpre os seus requisitos inicialmente estipulados.
Em esse sentido, foram criados modelos de qualidade para software:
CMMI, SPICE (Software Process Improvement and Capability Determination), MPS.
Br (Melhoria de Processos do Software Brasileiro).
Os mesmos apresentam processos bem definidos, práticas e objetivos, gerais e específicos, organizados em níveis de maturidade e competência.
Em a tentativa de minimizar riscos e adquirir produtos de qualidade, com preços justos e dentro de os prazos previstos, muitos clientes buscam empresas de desenvolvimento de software certificadas, segundo esses modelos.
Por sua vez, essas empresas, para atender as exigências dos clientes e continuar competitivas no mercado, também acabam optando, freqüentemente, por obter certificações de qualidade.
Tanto esses modelos de qualidade (), como a norma 1061 e Stephen Kan, afirmam que a mensuração da qualidade do software pode ser efetuada por intermédio de métricas, definidas segundo um programa de métricas.
Para sistematizar a discussão, sobre a mensuração da qualidade de software, optou- se por esquematizar os aspectos envolvidos na sua realização em três dimensões ortogonais:
1) nível de mensuração, 2) atividades de supervisão, e 3) plataforma computacional.
A primeira dimensão apresenta os níveis de mensuração propostos por os modelos de qualidade.
A segunda contempla as atividades básicas de gestão adotadas por diferentes segmentos de desenvolvimento de projetos (e.
g construção civil, construção naval) e sugeridas por institutos reconhecidos, como:
Software Engineering Institute (Sei) e Project Management Institute (PMI).
A terceira trata de aspectos das empresas de desenvolvimento de software que influenciam na realização da solução computacional de apoio à mensuração da qualidade.
A esquematização dessa última dimensão é baseada em estudos anteriores e no conhecimento de um ambiente de desenvolvimento de software de grande porte.
A mensuração da qualidade de software pode ser realizada em diferentes níveis:
Equipe (e.
g qualidade, teste, desenvolvimento);
projeto; organizacional;
interorganizacional (e.
g operações de software globalizadas).
Porém, os modelos de qualidade de software () tratam apenas os níveis de projeto e organizacional, simplificação convenientemente adotada nesta pesquisa.
A realização da mensuração da qualidade de software depende da coleta de métricas do seu ambiente de desenvolvimento, tendo auxílio computacional ou não para a sua gestão.
Assim, as métricas diretas podem estar originalmente armazenadas na forma:
Adhoc, em planilhas ou documentos texto;
Integrada, a partir de ferramentas de automação de escritório (e.
g pacote Office da Microsoft);
Ou automatizada, através de ferramentas, dedicadas ou personalizadas, orientadas à gestão, que apresentam tratamento especial para as métricas (e.
g MS Project, Project and Portfolio Management, IBM Rational ClearQuest).
Em o primeiro caso, as métricas encontram- se apenas documentadas em arquivos, sem nenhuma estrutura ou organização pré-definidas.
Já no segundo, as mesmas também estão documentadas em arquivos de forma estruturada e organizada, tipicamente seguindo uma padronização estabelecida (document template).
Por fim, no terceiro caso, as métricas estão localizadas em bases de dados das respectivas ferramentas.
Os modelos de qualidade de software, CMMI3 e MPS.
Br Nível E, exigem que além de a coleta das métricas, a partir de as diferentes formas de armazenamento apresentadas, seja realizada a sua consolidação num repositório central único, tanto no nível do projeto como no da organização.
Com base nesse último é possível obter uma visão comparável e unificada dos diferentes projetos, objetivando auxiliar os diversos níveis de gestão (e.
g organizacional, projeto) e papéis organizacionais (e.
g gerente de projeto, gerente de software, gerente de qualidade).
A existência de um repositório central permite a manutenção da história dos dois níveis.
De essa forma, as métricas de software podem ser utilizadas como fonte de dados para a realização de estimativas e para o planejamento inicial de novos projetos.
Além disso, o repositório também permite compartilhar boas práticas de desenvolvimento de software entre os projetos da organização e apresentar aos clientes métricas sobre o desempenho do seu projeto ou de experiências anteriores.
Em alguns casos, os repositórios de métricas baseiam- se em ambientes de data warehousing (Seção 2.4) (), estruturados segundo um modelo dimensional.
Vale ressaltar que a definição desse ambiente deve incluir um processo de Etc (Seção 2.4.3), o qual faz parte da etapa de preparação dos dados, considerada crítica por.
Também é importante salientar que a forma de armazenamento das métricas está intimamente relacionada com a definição de processo.
As atividades de análise consistem no estudo sobre o histórico de métricas através de técnicas estatísticas, mineração de dados, geração de gráficos e verificação de indicadores de desempenho, tendo em vista a utilização dos seus resultados para um melhor entendimento do próprio projeto, formular planos de melhorias e ações corretivas no mesmo, ou em projetos que apresentem características semelhantes.
Por exemplo, pode- se analisar o percentual de retrabalho para corrigir defeitos e buscar descobrir a causa raiz dos mesmos e, com isso, criar um plano de melhoria para reduzir os principais fatores responsáveis por a geração de defeitos.
Já a monitoração tem por objetivo oferecer informações do andamento dos projetos em relação a o que foi inicialmente planejado, a partir de um acompanhamento regular do planejamento e buscando detectar desvios significativos.
Esse acompanhamento pode ser efetuado por intermédio de técnicas específicas de monitoração (e.
g EVA, Seção 2.3) e deve ser realizado de modo a oferecer informações, úteis e atualizadas, que auxiliem na tomada de decisão, para que ações corretivas possam ser executadas, no momento certo.
Por exemplo, quando uma determinada tarefa encontra- se em atraso e a sua finalização é prérequisito para o início de outras tarefas, o gestor pode optar por alocar mais recursos nesta tarefa atrasada, ou transferir- la para outro recurso mais qualificado e/ ou disponível, para que a mesma não cause impacto nas demais e não ocasione um atraso do prazo de entrega e aumento do orçamento, ou, pelo menos minimizar esse impacto.
Por sua vez, a previsão também está relacionada com o estudo de dados passados.
Porém, ela objetiva a realização de estimativas confiáveis e próximas de valores reais, bem como na detecção de desalinhos de objetivos, além de a descoberta da causa raiz de falhas dos produtos.
Os resultados da previsão podem ser oferecidos através de modelos, ditos preditivos, produzidos ou por técnicas probabilísticas ou por técnicas de mineração sobre as métricas de software.
De essa forma, os gestores podem se beneficiar do histórico de mensurações para melhorar suas estimativas, buscando atingir uma maior precisão dos prazos e custos.
Por exemplo, com base no número de defeitos de versões anteriores, no esforço associado para a sua correção e no tamanho do seu código, técnicas preditivas podem ser aplicadas para estimar o número de defeitos da versão corrente, podendo dessa forma alocar horas de projeto em atividades de correção com uma maior precisão e, conseqüentemente, estimar custos e prazos mais próximos dos reais.
A definição de uma arquitetura computacional para a automação, parcial ou total, da mensuração de qualidade de software, precisa tratar aspectos relacionados com:
A latência e a freqüência de coleta das métricas, o ambiente heterogêneo de desenvolvimento de software, e, principalmente, a intrusão do processo de captura das métricas.
A seguir são discutidos cada um desses aspectos.
A latência na coleta de uma métrica consiste na diferença entre o instante de tempo em que ocorre o seu evento gerador e a disponibilização da mesma aos gestores.
De essa forma a latência pode ser vista como o tempo total consumido por as seguintes ações:
Lançamento do evento ocorrido por o recurso (integrante do projeto), execução da coleta das métricas, consolidação das mesmas no repositório e disponibilização dos seus resultados.
Já a freqüência da coleta representa a periodicidade com que essa coleta das métricas é realizada no ambiente de desenvolvimento de software.
É importante ressaltar que a latência e a freqüência, definidas por a solução de automação, devem considerar algumas particularidades desse ambiente de desenvolvimento.
Os valores de algumas métricas diretas, como esforço real e número de defeitos encontrados, tipicamente podem ser informados por os recursos, ao término de um determinado evento.
No caso de o esforço, por exemplo, para cada tarefa planejada são atribuídos um ou mais recursos que devem lançar, num registro de banco de horas (numa base de dados ou não), o tempo consumido para a realização dessa tarefa.
Esse lançamento pode ser realizado logo ao término da tarefa, no final do turno ou do dia de trabalho ou, até, ao término da semana, conforme a política da organização.
Conseqüentemente, a definição da freqüência de coleta deve considerar essas particularidades da latência para que os dados capturados representem uma situação correta das métricas, no instante em que foram coletadas.
Em esse sentido, considerouse que:
A latência pode ser nula, é medida em unidade de tempo e pode apresentar grandezas na ordem de segundos, minutos, horas, dias, semanas ou meses.
Por sua vez, a freqüência é medida em número de vezes por unidade de tempo, onde esta última pode apresentar as mesmas ordens de grandeza da latência.
Também vale ressaltar que o tempo de execução do processo de coleta das métricas é crucial para a determinação de ambas.
A realização de tarefas laboriosas e manuais para coletar, consolidar e disponibilizar as métricas pode sobrecarregar e onerar o projeto, interferindo diretamente na definição da sua freqüência.
Com base no que foi exposto, conclui- se que a latência e a freqüência estão intimamente relacionadas.
Contudo, para um melhor entendimento da contribuição desta pesquisa, optou- se por discutir as questões referentes à latência e à freqüência de maneira individual, adotando para a latência, as seguintes ordens de grandeza:
Horas, dias e meses.
Horas, significando que ao término da tarefa ou, no máximo, ao final do turno/ dia de trabalho, o recurso lança valores para as métricas no ambiente computacional e que essas passam a estar disponíveis.
Dias, significando que ou o lançamento das métricas por parte de os recursos se dá com um espaçamento maior ou que a coleta é feita mais espaçadamente (uma vez por semana, por exemplo).
Meses, significando que somente ao final de cada mês é que as métricas são consolidadas no repositório, mesmo que o lançamento das mesmas seja feito em intervalos menores.
Com relação a a freqüência foi adotada a mesma ordem de grandeza para a unidade de tempo.
Com relação a o ambiente heterogêneo de desenvolvimento de software, o mesmo é conseqüente da diversidade de:
Modelos de processo de software, ferramentas de apoio, tipos de projetos, e níveis de isolamento. --
Modelos de Processo de Software: A Engenharia de Software propõe diversos modelos de processo, como:
Cascata, iterativo, espiral, unificado, entre outros, sendo que cada um de eles pode apresentar ciclos de vida, fases e iterações distintas.
Esses modelos servem de base para a realização do planejamento do projeto.
Tipicamente é feita uma planificação dos mesmos em função de o tempo, representada por um cronograma, onde as linhas definem as tarefas que devem ser realizadas e as colunas as métricas diretas (e.
g esforço, custo, data inicial e final) ou demais informações necessárias para a condução do projeto.
Os modelos de processo interferem diretamente na ordem de execução (e.
g data inicial e final) e na disposição das tarefas no cronograma, originando uma estrutura hierárquica.
No caso de a organização estudada (Capítulo 5), diversas tarefas compõem uma fase, que por sua vez constituem uma iteração, que podem ser agrupadas numa versão pertencente a um determinado projeto.
Vale ressaltar que essa estrutura também pode ser modificada, conforme as necessidades dos projetos e as exigências dos clientes.
Por essa razão, esses modelos e suas adaptações interferem diretamente na estrutura dimensional de armazenamento das métricas e no seu processo de coleta.
Assim, quando essa heterogeneidade é constatada, é necessário considerar as diferentes estruturas de dados para extrair as métricas, bem como definir uma série de transformações para consolidar- las, segundo as diferentes perspectivas de análise permitidas por os modelos de processo (e.
g fase, iteração, versão).
Logo, quanto maior a diversidade de modelos de processo e estruturas de cronograma, mais heterogêneo se torna o ambiente de desenvolvimento.
Entretanto, essa diversidade não precisa necessariamente existir, e as questões mencionadas passam a não ser relevantes.
Para o desenvolvimento desta pesquisa, assume- se que os modelos de processo podem apresentar o seguinte domínio:
Homogêneo ou heterogêneo, sendo que o segundo é caracterizado por a presença de no mínimo dois modelos de processo e de estruturas de cronograma distintos. --
Ferramentas de apoio: Atualmente, o mercado dispõe de uma diversidade de ferramentas para apoio à gestão, que permitem o armazenamento das métricas, como:
I) MS Project e Project and Portfolio Management (PPM) (cronograma);
Ii) IBM Rational Clear Quest, Bugzilla, Mantis e Clarity (acompanhamento e controle de defeitos);
Iii) Borland CaliberRM e IBM Rational RequisitePro (requisitos) entre outras.
Além de essas ferramentas, também podem ser utilizados editores de documentos texto e planilhas eletrônicas.
As organizações podem adotar um conjunto padronizado de ferramentas, onde uma mesma métrica é armazenada de forma idêntica em todos os projetos, e o seu ambiente de desenvolvimento é considerado homogêneo por este aspecto.
Porém, no cenário mais típico, os projetos podem ter autonomia para escolher um conjunto de ferramentas que melhor atenda as suas necessidades e as do seu cliente, ou ainda desenvolver as suas próprias ferramentas de apoio.
Assim, a mesma métrica pode ser armazenada em diferentes bases de dados ou arquivos, conforme as suas respectivas estruturas.
Além disso, uma mesma ferramenta pode apresentar variações na sua estrutura de dados segundo a sua versão e, ainda, oferecer, aos seus usuários, a opção de personalizar campos, aumentando ainda mais a heterogeneidade das formas de armazenamento.
Por essas razões, as ferramentas e os arquivos utilizados por os projetos interferem diretamente na definição da coleta das métricas, exigindo, no pior caso, um método específico de captura e transformação, para cada ferramenta, para que as métricas provenientes de diferentes fontes possam ser comparáveis.
De essa maneira, a heterogeneidade, conseqüente da diversidade de ferramentas, contribui significativamente para aumentar a complexidade de uma eventual automação da mensuração ou o esforço despendido no caso de coleta manual.
Para o desenvolvimento desta pesquisa adota- se que o ambiente de desenvolvimento pode ser homogêneo, ou heterogêneo no que se refere às ferramentas de apoio à gestão.
A existência de pelo menos duas ferramentas para armazenar a mesma métrica, já caracteriza o ambiente como heterogêneo. --
Tipos de Projetos: Os projetos podem ser classificados segundo seu porte (pequeno, médio e grande), em função de o seu tempo de duração e do seu número de recursos.
Em esse sentido, é possível encontrar casos reais de projetos, considerados de pequeno porte (e.
g dez recursos em tempo integral, num ano de duração), onde o seu acompanhamento é realizado de maneira pessoal e direta, a partir de reuniões diárias entre os gestores e a equipe de desenvolvimento.
Já em projetos de maior porte esse modo de acompanhamento se torna impraticável, devido a o número de recursos e dependência entre as suas tarefas.
Por essa razão, nesse último caso é primordial a utilização de algum suporte computacional para conseguir acompanhar o andamento do projeto.
Percebe- se, portanto, que as necessidades de informação dos gestores são distintas, e as métricas utilizadas também, bem como, o tipo de suporte computacional e, conseqüentemente, a forma de armazenamento das métricas no ambiente do projeto.
Todos esses aspectos interferem diretamente no processo de coleta das métricas.
Em esta pesquisa, por motivos de conveniência foram adotados dois portes de projeto:
Pequeno e grande.
O primeiro deve tipicamente apresentar equipes de no máximo vinte recursos, duração na ordem de meses, sendo suficiente pouco suporte computacional, e o segundo pode ter acima de cinqüenta pessoas, anos de duração e ferramentas dedicadas de apoio à gestão são necessárias. --
Níveis de Isolamento: Em muitos casos, clientes podem determinar, para seus projetos, políticas de segurança e privacidade, ferramentas de apoio à gestão, padrão de documentos de requisitos, classificação de defeitos, entre outros, que devem ser obedecidos por as organizações de Ti.
Particularmente, as políticas de segurança e privacidade podem interferir diretamente no nível de acesso às métricas, por conseqüência das configurações de rede nos projetos (e.
g exigir que a rede de comunicação do ambiente de desenvolvimento de seu projeto seja fisicamente isolada tanto do mundo externo (Internet), como dos outros projetos da mesma organização (Intranet organizacional)).
De essa forma, esta pesquisa caracteriza a heterogeneidade do nível de isolamento como:
Isolado ou compartilhado.
Em o primeiro os projetos apresentam redes internas, fechadas, onde somente os recursos do projeto têm permissão para realizar autenticação, impedindo a comunicação entre projetos.
Já no segundo, os projetos compartilham a rede da organização e seus dados podem ser acessados por recursos externos.
Por essas razões, essas políticas de segurança e privacidade precisam ser consideradas durante o processo de coleta, consolidação e disponibilização das métricas, sendo crucial que a solução de automação consiga tratar as dificuldades derivadas do nível isolado.
Intrusão consiste no grau de envolvimento dos recursos do projeto durante a execução do processo de coleta das métricas, que tipicamente deve apresentar os seguintes passos:
Capturar as métricas da sua fonte de origem, editar- las e transformar- las convenientemente, e armazenar- las num repositório central.
Para tanto, primeiramente, é preciso consultar as diversas fontes de métricas, considerando suas respectivas formas de armazenamento (adhoc, integradas, ou estruturadas).
Após, deve ser efetuada, quando necessária, uma série de edições manuais ou não, (e.
g padronização da classificação da severidade dos defeitos, tratamento das chaves de identificação dos registros que contêm as métricas, entre outras) ou inserções numa interface específica, para que as métricas possam ser consolidadas de maneira consistente e comparável.
E, por fim, é necessário armazenar- las num repositório central.
Em esse sentido, quanto mais manual e laborioso for esse processo, maior o envolvimento e, conseqüentemente, o nível de intrusão.
Por motivos de simplificação, nesta pesquisa, são adotados apenas três níveis de intrusão:
Isento, baixo e alto.
Em o primeiro, considerado ideal, não existem atividades manuais, o processo é completamente automatizado.
Em o segundo, o processo deve ter um bom nível de automação, demandando apenas que pequenas atividades, dos passos citados anteriormente, sejam manuais (e.
g homologação dos resultados ou ativação de uma rotina de extração).
Em o terceiro, a maioria das tarefas é realizada manualmente exigindo um grande envolvimento dos recursos.
A partir de uma leitura das três dimensões ortogonais é possível examinar as interações entre elas e destacar os aspectos básicos que devem ser atendidos por uma solução computacional de automação da mensuração da qualidade de software.
Essas interações estão sintetizadas na forma de uma tabela, que pode ser vista como uma planificação do cubo formado por as dimensões anteriormente discutidas, onde:
As colunas centrais representam as atividades de supervisão (análise, monitoração e previsão);
a coluna lateral da esquerda refere- se ao nível de mensuração enquanto que, a da direita, aos aspectos envolvidos por a plataforma computacional;
As linhas contemplam os aspectos computacionais e os níveis de mensuração;
E as células contêm os domínios que devem ser considerados por a solução.
Além disso, cada domínio pode ser classificado por uma das inexistente (Ñ).
A segunda escala diz respeito somente aos aspectos de heterogeneidade, considerando os dois níveis de mensuração.
Já a primeira envolve os restantes.
Com base nas interações entre as três dimensões e nas classificações dos domínios, apresentadas na Tabela 1, é possível destacar (sombreado em azul) os domínios dos aspectos mais críticos, para efetuar a mensuração da qualidade de software nos diferentes níveis, oferecendo suporte às atividades de análise, monitoração e previsão, considerando as heterogeneidades do ambiente de desenvolvimento, e realizar as seguintes leituras:
A partir de essa interação, primeira linha da Tabela 1, percebe- se que para realizar análise e previsão são aceitáveis todos os domínios definidos para latência.
No entanto, o mesmo comportamento não pode ser aplicado à monitoração.
Essa aceita, no máximo, latência na ordem de dias, pois requer valores atualizados para as métricas, que reflitam a situação atual do ambiente de desenvolvimento.
Por essa razão, é possível constatar que a baixa latência, associada à monitoração, é obrigatória para oferecer suporte a todas as atividades, nos dois níveis de mensuração.
2º interação:
Freqüência × (Análise, Monitoração e Previsão) × (De Projeto, Organizacional) Através da análise dessa interação, segunda linha da Tabela 1, constata- se que as atividades de análise e previsão aceitam freqüências de coleta na ordem de dias.
Por sua vez para monitoração esse domínio é pouco aceitável, sendo ideal apresentar uma freqüência de horas, para que seja possível efetuar um acompanhamento regular do andamento do planejamento inicial.
Portanto, é possível concluir que, entre as atividades, a monitoração é determinante na definição da freqüência de execução do processo de coleta das métricas.
A partir de essa interação, terceira linha da Tabela 1, conclui- se que não existe, no nível de projeto, heterogeneidade de:
Modelos de processo, projetos e isolamento.
Portanto, o cenário mais crítico dessa interação é caracterizado por a existência de heterogeneidade de ferramentas, no ambiente, para a coleta das métricas, podendo ser mais simples mensurar a qualidade num cenário totalmente homogêneo.
Com base nessa interação, quarta linha da Tabela 1, verifica- se no cenário mais crítico a existência de todas as heterogeneidades, de todos os tipos de projetos e do nível isolado.
Estudos anteriores afirmam que a definição de um processo de coleta de métricas, que contemple todos esses aspectos e mantenha a liberdade, dos projetos e dos clientes, não é considerada uma tarefa trivial.
A heterogeneidade contribui significativamente para aumentar a complexidade dos aspectos da automação computacional.
Por intermédio dos valores dos domínios, localizados na última linha da Tabela 1, conclui- se que, para oferecer suporte à análise e à previsão, é aceitável um processo de coleta de métricas com um nível alto de intrusão, já que a freqüência de captura pode variar na ordem de meses.
No entanto, para suportar monitoração, que apresenta uma freqüência no mínimo diária, torna- se inaceitável realizar- lo com um nível alto de intrusão, tanto no âmbito do projeto, como no da organização.
A freqüência diária reforça a necessidade de que a mensuração seja realizada de modo pouco intrusivo, exigindo o mínimo de esforço para o time de desenvolvimento.
Além disso, não é visto com bons olhos afastar recursos do projeto, das suas atribuições normais para efetuar a captura das métricas, por o custo envolvido.
Essa tarefa não pode ser realizada por qualquer recurso, pois o mesmo deve ter conhecimento tanto das particularidades do ambiente do projeto como da organização.
Portanto, como as métricas são úteis não apenas ao projeto, não é justo que somente o mesmo seja onerado.
Logo, conclui- se que o pior cenário envolve a mensuração da qualidade do software no nível organizacional, com atividades de análise, monitoração e previsão, num ambiente heterogêneo de desenvolvimento, de maneira pouco intrusiva, considerando a latência e a freqüência na ordem de horas.
De esse modo, a definição de um processo de mensuração da qualidade de software pouco intrusivo é fundamental para a sua aceitação no ambiente do projeto.
Este capítulo apresentou a mensuração da qualidade de software a partir de três dimensões ortogonais e as suas possíveis interações.
Para tanto, foi utilizada uma tabela que pode ser vista como uma planificação dessas dimensões e que serve de base para:
Discutir os trabalhos relacionados apresentados no Capítulo 4;
levantar os aspectos envolvidos na mensuração da qualidade de software, que compõem a tabela, num ambiente real de desenvolvimento, relatado no Capítulo 5;
mencionar quais desses aspectos são tratados por a solução descrita no Capítulo 6;
e verificar se a mesma os atende de maneira adequada, a partir de os testes documentados no Capítulo 7.
Este capítulo apresenta os trabalhos, encontrados na literatura, que são relacionados com o tema de pesquisa e que buscam soluções para mensurar a qualidade de software, a partir de métricas e de um repositório central.
Entre eles encontram­se MMR, BPI/ iBOM e SPDW.
A o final de cada uma das próximas seções é apresentada uma tabela contendo os aspectos da solução de automação, para mensuração da qualidade de software, a partir de as três dimensões ortogonais definidas na Seção 3.1.
Essa tabela tem por base a estrutura da Tabela 1, porém, as suas células estão preenchidas, somente, com os domínios atendidos por a solução relatada na Seção 3.2, onde() indica o não atendimento.
A ferramenta MMR foi desenvolvida para atender às necessidades de gerência de projeto da Enterprise Performance Unit da Ericsson Research, Canadá.
A mesma propõe um repositório genérico, flexível e integrado, para coletar, armazenar, analisar e reportar dados, baseados nos requisitos de mensuração sugeridos por o modelo CMMI.
A apresenta visões compartilhadas relacionadas análise de tendências acompanhamento do PDS, por intermédio de um repositório.
Este último tem suas informações disponibilizadas por consultas multidimensionais sobre os dados, através de cubos OLAP.
A flexibilidade desse repositório é oferecida através do desenvolvimento de metadados, que provêm definições de medidas e as relações entre elas.
A MMR busca apoiar a maturidade de organizações nos diferentes níveis do CMMI, com o respectivo suporte ao seu programa de métricas.
Em essa ferramenta a disponibilização de informações aos usuários finais é realizada por um portal web, na forma de relatórios pré-definidos e contendo alguns indicadores, sendo que novos relatórios podem ser desenvolvidos a partir de linguagem SQL ou através de recursos OLAP.
Com relação a a coleta das métricas, a MMR apresenta um método manual, onde os dados são inseridos individualmente, a partir de uma interface web.
A Figura 6 ilustra a arquitetura de software da MMR responsável por o oferecimento das suas funcionalidades.
Indicadores e Tendências têm por objetivo mensurar as questões de negócio definidas por a organização, respondendo como está o seu desempenho.
Os mesmos podem ser divididos segundo quatro áreas distintas e providos por o componente Management Indicators &amp; Trending.
De essa forma, pretende-se satisfazer as necessidades da gerência, apresentando informações baseadas em relatórios e gráficos pré-definidos disponibilizados na web, para facilitar a tarefa de medição e análise da PA (Process Area) de MA (Measurement and Analysis) do CMMI.
A capacidade analítica e multidimensional, oferecida por o desenvolvida para dar suporte aos gerentes intermediários e aos desenvolvedores de operações, sendo personalizada através de relatórios dinâmicos, exportações para planilhas MS Excel e funcionalidades de drill-down/ drill-up similares aos recursos OLAP.
O componente de Gestão e Controle de Qualidade (Administration and Quality Control) permite ao responsável por essas atividades definir novas medidas e permissões de acesso, e auditar a qualidade dos dados suportados por a ferramenta.
A máquina analítica (Analytical Engine -- OLAP Technology) oferece a funcionalidade de computar medições derivadas e agregadas, a partir de as múltiplas dimensões do modelo analítico suportado por o repositório.
O repositório de medidas (Measurement Repository) é considerado o núcleo da ferramenta MMR.
A sua base de dados é composta de entidades dos metadados, definidas por um metamodelo especificado por um diagrama de classe e associações, detalhado em apud.
Com base na Tabela 2 percebe- se que a ferramenta MMR apresenta a freqüência e a latência do processo de coleta das métricas variando na ordem de dias, podendo chegar a meses.
Para monitoração esses valores de tempo são considerados, respectivamente, pouco aceitáveis e inaceitáveis.
Para as demais atividades, podem ser aceitáveis.
É importante salientar que esses valores de tempo são conseqüentes do processo manual e pouco freqüente de coleta das métricas.
Em o que tange à heterogeneidade, pode- se destacar na MMR, apenas, a presença de um ambiente homogêneo ou heterogêneo de ferramentas, porém sem maiores detalhes de como os mesmos são suportados por a ferramenta.
Por fim, com relação a a intrusão conclui- se que o processo de coleta da MMR é bastante laborioso e manual, sendo considerado inaceitável para monitoração e pouco aceitável para análise e previsão.
Além disso, essa ferramenta não esclarece como a flexibilidade provida por o seu modelo analítico garante a consolidação das métricas, dos diferentes projetos, e a manutenção da sua consistência ao longo de a sua evolução das métricas.
BPI[ CAS02a] consiste numa arquitetura geral, composta por um pacote de ferramentas, desenvolvida por a Hewlett--Packard (Hp), para oferecer suporte, automatizado ou semi-automatizado, à gestão de qualidade do processo de negócio, do Business Process Management System (BPMS).
Esse pacote permite realizar atividades de:
Análise: Possibilita a realização de análises completas das execuções dos processos, tanto da perspectiva do negócio (e.
g alto nível, número de processos terminados com &quot;sucesso&quot;), como dos analistas de Ti (e.
g baixo, média de tempo de execução por nó), apresentando funcionalidades para reportar dados, que auxiliam a identificação de prováveis causas de determinados comportamentos dos processos.
Previsão: Possibilita derivar modelos de previsão e aplicar- los nos processos em execução, para identificar exceções ou comportamentos indesejados.
Monitoração: Possibilita monitorar instâncias de processos em execução, informando sobre situações indesejadas ou incomuns.
De essa forma, os usuários podem verificar o status do sistema, dos processos, dos serviços e dos recursos.
Fora isso, também, podem ser definidas situações críticas e alertas, quando as mesmas forem detectadas.
De essa forma a solução BPI pretende:
Identificar a arquitetura e as tecnologias que podem proporcionar as atividades mencionadas;
Permitir a definição de conceitos e métricas que permitem estabelecer níveis de negócio e análises quantitativas dos processos;
Desenvolver técnicas para facilitar a sua utilização, objetivando que a extração do conhecimento desejada, seja obtida sem nenhuma escrita de código;
Entender como realizar a interação com o BPMS e com os usuários, objetivando reportar e corrigir situações críticas num intervalo de tempo adequado, para que ações corretivas possam ser efetuadas.
O BPI assume a execução de processos de negócio num sistema de gerência de workflow específico denominado Hp Process Manager (HPPM), gerando e armazenando dados em logs.
A arquitetura do BPI (Figura 7) é responsável por estabelecer um ambiente para monitoramento em tempo real, análise, gerenciamento e otimização de processos de negócio.
Essa arquitetura apresenta três componentes principais:
PDW Loader, Process Mining Engine e Cockpit.
Audit Logs), verificando a sua consistência, calculando as métricas do negócio e inserindo os dados no PDW.
Esse consiste num DW e está estruturado segundo um modelo analítico que possibilita a definição e a rápida execução de relatórios agregados e detalhados, além de um vasto conjunto de funcionalidades analíticas.
O modelo analítico do PDW apresenta estrutura multidimensional, conforme um modelo estrela, ilustrado por a Figura 8, onde o Process State Changes e o Service State Changes são as tabelas fato e as demais constituem as dimensões.
Com base nessa estrutura, os usuários podem, por exemplo, examinar quantas instâncias de processos são inicializadas por um determinado usuário dentro de um intervalo de tempo específico.
O PDW Loader captura os dados dos logs e insere- os no PDW, podendo ser ativado periodicamente (agendado) ou sob requisição.
Durante a carga, o loader verifica a consistência dos dados dos logs e corrige informações errôneas, que poderiam tornar as análises mais complexas ou tendendo a erros.
Após o armazenamento dos dados no DW, o Process Mining Engine aplica técnicas de mineração de dados, buscando produzir modelos úteis para predizer a ocorrência de determinados comportamentos dos processos em execução, bem como identificar as causas desses comportamentos.
Por fim, as informações são apresentadas por a interface Cockpit, projetada para produzir relatórios aos usuários do negócio.
A mesma apresenta os dados do DW de maneira simplificada, permitindo que os analistas definam uma série de consultas de maneira intuitiva, sem exigir edição de código.
Além disso, essa interface apresenta análises de processos e monitoração dos mesmos através de métricas e indicadores, apresentados na forma de gráficos e dashboards.
O iBOM é considerado uma evolução do BPI e inclui a funcionalidade de lidar com fontes de dados heterogêneas, por intermédio de um componente especial, chamado Abstract Process Monitor (APM), e permite a utilização de métricas definidas por os usuários.
O APM é basicamente um componente para coletar e interpretar métricas, no contexto de processos abstratos.
Assim, é possível realizar previsões, simulações e otimizações dos processos a partir de o momento em que os dados dos processos estão disponíveis, e os valores das métricas estão computados e analisados.
Os componentes responsáveis por essas funcionalidades obtêm esses dados e os valores das métricas do APM.
A partir de os domínios apresentados na Tabela 3, referentes ao BPI e ao iBOM, é possível perceber que ambos oferecem suporte às atividades de análise, previsão e otimização, apresentando tanto para a latência, quanto a a freqüência, um domínio de tempo na ordem de horas, podendo em alguns casos aproximar- se do processamento em tempo-real.
Além disso, apenas, o iBOM permite que métricas armazenadas em diferentes fontes de dados originais possam ser consolidadas, por intermédio do componente APM, oferecendo suporte a um ambiente heterogêneo de ferramentas.
Essas soluções não discutem explicitamente o nível de isolamento.
Porém, como o APM lida com diferentes ferramentas, é possível inferir que não existem dificuldades de acesso aos dados e as suas respectivas fontes originais.
Por fim, devido a as funcionalidades oferecidas por o PDW Loader é possível concluir que as soluções são isentas de intrusão, caracterizando um processo de Etc completamente automatizado.
Vale ressaltar que as rotinas de extração e os métodos de transformação não são relatados na descrição da solução.
Contudo, BPI/ iBOM endereça processos de negócio apoiados por uma ferramenta de automação de workflow.
Com efeito, é considerado que os valores das métricas estão sempre computacionalmente disponíveis e, portanto, acessíveis por alguma ferramenta, o que implica que os recursos não têm de apropriar nenhum valor de métrica.
Essa situação inexiste num ambiente de desenvolvimento de software.
O SPDW[ BEC06] é um ambiente de Data Warehousing para apoiar o Programa de Métricas da Hp EAS Brasil, desenvolvido por o projeto de parceria entre o Programa de PósGraduação de Ciência da Computação da PUCRS (PPGCC-PUCRS) e a Hp EAS Brasil, durante o seu processo de certificação CMM3.
A Figura 9 ilustra a arquitetura do SPDW, organizada em camadas distintas:
Camada de Integração das Aplicações (Application Integration Component), Camada de Integração dos Dados (Data Integration Component) e Camada dos Componentes de Apresentação (Presentation Components).
A Camada de Integração de Aplicações é responsável por a extração automática dos dados diretamente das fontes dos projetos, advindos de diferentes ferramentas (Tools) de apoio ao PDS.
Os dados são carregados numa DSA, que pertence à Camada de Integração de Dados, onde tais dados são devidamente transformados conforme o padrão organizacional e, posteriormente, armazenados no DW.
Essa extração é feita com o uso de Ws, representados por semicírculos pretos, baseados nos metadados dos projetos (Project Metadata).
Por último, a Camada de Componentes de Apresentação permite a exibição das métricas para análise armazenadas no repositório (DW), de acordo com os diferentes perfis de usuário (organizacional, de projeto, de gerência e técnico) e objetivos de análise.
Essa camada é composta por dois mecanismos de apresentação:
Recursos OLAP e dashboards.
A arquitetura do SPDW apresenta apenas um fluxo unidirecional de dados, tendo a Extraction Routine como coordenadora do processo de extração das métricas.
De essa forma, somente essa última consegue realizar a comunicação entre as camadas de Integração de Aplicações e de Dados, impedindo que os wrappers ou outros serviços possam iniciar o processo de Etc das métricas, de maneira pró-ativa quando novos lançamentos forem detectados ou quando o gestor do projeto achar oportuno.
Esse ambiente foi positivamente apreciado durante o processo de avaliação de qualidade e a sua adoção trouxe resultados benéficos e significativos à organização, associados com a padronização, regularidade e consistência de captura das métricas.
Além disso, o SPDW melhorou os recursos analíticos e de comunicação, devido a a utilização de indicadores de qualidade (dashboards), relatórios e análises pré-definidos, e recursos OLAP.
Esses últimos permitem que o grupo de qualidade e os responsáveis por os projetos possam explorar o DW para encontrar as razões das deficiências ou as melhorias detectadas, conseqüentes das ações tomadas.
A parceira também utiliza o SPDW para solidificar sua credibilidade, apresentando os indicadores de qualidade dos processos, dos projetos, dos produtos e da satisfação dos clientes, durante as apresentações de pré-vendas.
As seções a seguir detalham, do SPDW, o programa de métricas adotado, o modelo analítico desenvolvido e o processo de Etc especificado.
Este detalhamento se faz necessário, pois a pesquisa sendo relatada nesta dissertação tem como ponto de partida as contribuições oferecidas por o SPDW.
Para atender as exigências de PA de MA, do CMM2, foi estabelecido um Programa de Métricas Organizacional (Tabela 4), onde as métricas encontram- se agrupadas em áreas de qualidade (Quality Areas) distintas:
Schedule (Tempo, Cronograma), Effort (Esforço), Size (Tamanho), Cost (Custo), Requirements (Requisitos) e Quality (Qualidade, Defeitos).
Cabe salientar que essas métricas permitem apenas a execução de análises e predições do PDS, não sendo adequadas à monitoração, pois não seguem nenhuma das técnicas usadas para tanto (e.
g EVA detalhada na Seção 2.3) Esse conjunto de métricas não permite acompanhar regularmente o andamento do planejamento do projeto, pois detecta desvios significativos somente quando o tempo estimado para executar uma determinada fase, iteração ou versão está encerrado.
De essa maneira, não é possível antecipar decisões e tomar ações corretivas para evitar que esses desvios interfiram no prazo ou no custo do produto final.
Por exemplo, a métrica Variação de Baseline Original (SVOB ­ Schedule Variance Original Baseline) quando avaliada, segundo índices de desempenho (desejados, pouco desejados ou indesejados) definidos por valores de mercado (e.
g GDIC IQMS e PMBOK -- Risk Management (Moderate Impact) apresenta valor indesejado somente no momento em que foi concluída ou após ultrapassar o limite de prazo definido para a sua execução.
Por essa razão, a mesma não reflete a situação real do andamento do projeto, não sendo úteis para monitoração.
O repositório de dados, estruturado na forma de um DW, é o núcleo da arquitetura do SPDW.
Em ele, os dados dos projetos são armazenados para dar suporte à análise do PDS, de acordo com as métricas definidas por a organização.
A Figura 10 mostra a estrutura estendida dos projetos suportados por o DW, por intermédio de um diagrama de classes UML.
A mesma permite a adoção de projetos com ciclo de vida cascata e iterativo.
De essa forma, cada projeto (Project) pode apresentar uma ou mais versões (Release), sendo que as mesmas podem conter um conjunto de iterações (Iteration) ou fases (Phase).
Porém, uma fase pode ou não pertencer a uma iteração.
Essa condição somente é possível devido a a presença da superclasse estágio (Stage).
Os atributos das classes do diagrama correspondem às métricas da Tabela 4, representando os diferentes níveis de detalhe em que as mesmas podem ser capturadas, ou produzidas, para cada especialização do OSSP (Organization´ s Standard Software Process).
Estimativas de Esforço e Tempo são estabelecidas em termos de iteração e fase, enquanto custo e tamanho são estimados no nível de versão.
Os valores reais dessas métricas são capturados no grão de atividade.
Já os defeitos são mensurados a partir de os estágios.
Os requisitos e a satisfação do cliente são medidos no nível de versão.
Não existem estimativas para as áreas de requisitos e qualidade (defeitos e satisfação do cliente).
Durante a concepção do SPDW foi imposto por a organização que os projetos deveriam classificar as suas atividades em:
Trabalho, retrabalho, revisão e qualidade.
Além disso, também foi constatado que as estimativas de esforço não eram realizadas até o grão de atividades e, por essa razão esse nível apresenta apenas métricas com valores reais (efetivamente realizados).
Essas decisões foram cruciais na definição da granularidade das tabelas fato que compõem o modelo analítico.
A Figura 11 Apresenta o modelo analítico do SPDW.
Este é constituído por uma estrutura dimensional organizada na forma de uma constelação de fatos, onde as tabelas dimensão representam as perspectivas de análise dos projetos e as tabelas fato armazenam as métricas nos seus diferentes níveis, considerando as diferentes granularidades de informação.
É importante salientar que esse modelo é totalmente dependente do programa de métricas e dos processos organizacionais (OSSP e PDP).
Segundo, o modelo analítico considera um conjunto restrito, porém significante de métricas.
Além disso, sua estrutura multidimensional favorece a inclusão de novas métricas, por intermédio da simples adição de dimensões, desde que sua granularidade esteja de acordo com o modelo.
Em esse sentido, métricas com granularidade diferente devem ser incluídas a partir de a criação de novas tabelas fato.
Essa facilidade agrega flexibilidade ao modelo, conseqüentemente também favorece a extensão do programa de métricas e a evolução dos processos adotados por a organização.
O modelo analítico do SPDW foi desenvolvido para análise e apresenta limitações quanto a a granularidade das informações e às perspectivas de análise.
A partir de o mesmo não é possível acompanhar regularmente o desempenho das atividades de maneira individual, ou sumarizar- las por intervalo de data, devido a a consolidação estabelecida por tipo atividade.
Além disso, as informações de custo são apresentadas, apenas, no nível de versões impedindo, por exemplo, a verificação do valor consumido por determinada fase ou iteração, bem como a monitoração dos mesmos numa granularidade menor.
Outra limitação do modelo original está relacionada com a existência de métricas não aditivas nas tabelas fato.
As mesmas impedem que sejam realizadas sumarizações das métricas nos níveis mais altos (versão, iteração ou fase) a partir de o Activity_ Fact (Fato_ Atividade).
Por essa razão, o original apresenta uma tabela fato para cada nível hierárquico da estrutura do cronograma:
Atividade, Fase e Iteração.
Em a perspectiva de captura dos dados, as diferentes particularidades existentes nos projetos implicam em lidar com os aspectos envolvidos por as três dimensões ortogonais.
Por essa razão, o processo de Etc de métricas de software pode ser considerado complexo.
Além disso, no contexto do SPDW, esse processo também envolve a interação entre componentes de camadas diferentes, Integração das Aplicações e Integração dos Dados, para carregar os dados de maneira concisa e consolidada no DW.
O Etc de métricas de software, proposto por o SPDW, trata a liberdade dos projetos de escolher diferentes ferramentas, que variam desde simples planilhas MS Excel a ferramentas dedicadas, como MS Project e IBM Rational ClearQuest, entre outras.
O mesmo também possibilita que cada um dos projetos adote processos especializados do OSSP (PDP), que introduzem variações quanto a o ciclo de vida (cascata, iterativo) e modelos de gerenciamento (orientado a fases, orientado a entregáveis) dos projetos.
Para tratar essas questões de heterogeneidade, o processo de Etc segue um padrão arquitetural orientado a serviços, responsável por a comunicação entre os diferentes componentes da arquitetura.
Em a Figura 9, onde a arquitetura do SPDW está ilustrada, os serviços (Ws) estão representados por semicírculos pretos, que simbolizam os wrappers das ferramentas, os quais utilizam mensagens SOAP para realizar a comunicação entre os diferentes componentes da arquitetura, encapsulando as especificidades das ferramentas (Tools) utilizadas por os projetos.
No entanto, para que a A os possa ser considerada completa, essa solução deveria apresentar a definição dos serviços que compõem o processo automatizado de Etc das métricas.
Além disso, vale ressaltar que o SPDW não apresenta uma especificação detalhada ou uma prototipação das Rotinas de Extração e das de Transformação, nem dos wrappers.
Em esse trabalho apenas são realizadas descrições superficiais desses componentes, mencionando o papel de cada um de eles na arquitetura.
Ainda, cabe mencionar que o SPDW não discute a freqüência e a latência de extração das métricas, muito menos as questões relacionadas com a carga incremental.
Com relação a a Rotina de Extração (Extraction Routine), a mesma tem por objetivo localizar os wrappers das ferramentas e repassar as informações, contidas nos metadados dos projetos, para que eles possam localizar o dado requerido na base de dados das ferramentas.
Em essa arquitetura, são mencionados dois tipos de metadados.
O primeiro, Metadado do Projeto (Project Metadata), contém informações para o mapeamento entre o dado requerido e o armazenado na fonte, enquanto que o segundo, Metadado da Organização (Organizational Metadata), contém informações necessárias às rotinas de limpeza e transformação da Camada de Integração dos dados.
Ambos são definidos por esquemas XML e contribuem para a flexibilidade do processo de Etc das métricas de software.
A automatização do Etc das métricas, por intermédio da utilização de Ws e metadados, possibilita que as mesmas possam ser capturadas, transformadas e consolidadas no repositório, com baixa intrusão e alta flexibilidade, num ambiente heterogêneo.
Assim, a automatização facilita a evolução do programa de métricas organizacional e a utilização de novas ferramentas, caracterizando a arquitetura proposta, como sendo uma solução com baixo acoplamento entre as ferramentas e alta coesão dos dados.
A partir de a Tabela 5 percebe- se que o SPDW não oferece suporte à monitoração e à previsão, tratando apenas os aspectos relacionados com as atividades de análise.
Em esse sentido, esse ambiente apresenta uma baixa freqüência e alta latência, conseqüentes de um processo semi-automatizado de Etc, variando na ordem de dias e meses.
Ambas são consideradas, respectivamente, aceitáveis e pouco aceitáveis para analisar a qualidade de software.
Com relação a a heterogeneidade, nota- se que o SPDW descreve como lidar com todos os aspectos mencionados.
Por fim, também é possível concluir que o seu processo de Etc apresenta um baixo nível de intrusão.
É importante mencionar que o SPDW apresenta uma arquitetura em alto nível, sem detalhar as especificidades dos seus componentes, a latência, a freqüência e a carga incremental, considerados aspectos essenciais para armazenar as métricas no Repositório de Dados.
Além disso, o seu modelo analítico contém problemas de construção, devido a a existência de métricas não aditivas nos seus fatos, impedindo a sua sumarização nos níveis superiores e exigindo a utilização de um número maior de tabelas fato.
Com relação a os testes feitos à época, foram implementados apenas protótipos de alguns wrappers de extração.
Essa solução também não menciona como a Rotina de Extração inicia o processo de Etc nem demonstra como a baixa intrusão é garantida.
Atualmente, no ambiente da Hp EAS Brasil, encontra- se em funcionamento um processo semi-automatizado de Etc, com alto nível de intrusão, baseado no proposto por o SPDW, desenvolvido por uma outra empresa de Ti.
Logo, conclui- se que o SPDW propõe uma solução em alto nível, sem ter especificado e testado suficientemente as soluções computacionais adotadas para resolver os aspectos críticos destacados na Tabela 1.
Esse capítulo apresentou os seguintes trabalhos:
1) MMR, 2) BPI/ iBOM e 3) SPDW, vistos a partir de as três dimensões, definidas no capítulo anterior.
A Tabela 6 contém uma consolidação apenas dos domínios dos aspectos críticos e referências aos trabalhos estudados, conforme a legenda abaixo de a tabela.
Por questões de legibilidade, quando o aspecto crítico não foi atendido por nenhum dos trabalhos, o mesmo foi marcado com &quot;X».
A partir de a leitura da Tabela 6 percebe- se que a proposta do conjunto de ferramentas BPI/ iBOM consegue solucionar uma grande parte dos aspectos relacionados com à latência, à freqüência, à heterogeneidade de ferramentas, apresentando um nível isento de intrusão e considerando as três dimensões.
É importante salientar que essas soluções são aplicadas num ambiente de execução de processos de negócio, sem considerar as particularidades do ambiente de desenvolvimento de software (e.
g latência inerente).
Por essa razão, os aspectos mencionados são parcialmente atendidos.
Por sua vez a MMR atende quase os mesmos aspectos que a solução BPI/ iBOM, porém considerando um ambiente de desenvolvimento de software.
Contudo, essa ferramenta apresenta um processo de coleta de métricas eminentemente manual, exigindo um lançamento individual dos valores por intermédio de uma interface específica.
De essa forma, essa solução não atende o aspecto referente a a baixa intrusão, considerado fundamental para mensurar a qualidade de software, freqüentemente e com uma latência na ordem de horas.
Já o SPDW, atende somente a dimensão de atividade de análise, tanto para o nível de projeto quanto organizacional, considerando intrusão e todos os aspectos críticos relacionados com heterogeneidade, com exceção do nível de isolamento.
Diferentemente dos outros trabalhos é o único que trata modelos de processo e projetos.
Porém, não oferece suporte à monitoração, essencial para permitir um acompanhamento regular do planejamento inicial e crucial para atingir os níveis mais altos dos modelos de qualidade.
É importante destacar que, no cenário das três dimensões, a monitoração no nível organizacional, considerando todos os aspectos da plataforma computacional, apresenta os domínios mais críticos a serem atendidos por uma solução ideal (Seção 3.2).
Em esse sentido, a partir de a Tabela 6, é possível constatar que os trabalhos relacionados não tratam adequadamente todos os aspectos relevantes desse cenário.
De essa forma, esse capítulo buscou destacar os requisitos da solução que não foram atendidos.
Legenda: 1 ­ MMR, 2 ­ BPI/ iBOM, 3 -- SPDW Ferramentas Projetos Nível de Isolamento Modelos de Processo Ferramentas Projetos Nível de Isolamento Intrusão Heterogeneidade De Projeto Em este capítulo é apresentado o cenário real da operação de software parceira, certificada CMM3, e considerada de grande porte na América Latina.
Para tanto, primeiramente é feita uma descrição desta operação, relatando a solução de automação computacional adotada, denominada Base Organizacional (BO) e seus principais componentes.
Posteriormente, são apontadas algumas dificuldades encontradas neste cenário.
Por fim, é apresentada uma estimativa do esforço necessário para tornar a solução atual mais automatizada e menos intrusiva.
A operação de software escolhida apresenta como solução, para mensurar a sua qualidade, uma adaptação do ambiente proposto por o SPDW, denominada Base Organizacional (BO).
Essa adoção parcial foi conseqüente da opção, por parte de a parceira, em implantar rapidamente a BO, o que foi feito licenciando e adaptando uma Ferramenta de Bi (Business Intelligence) de uma empresa de Ti.
A concepção da BO teve como meta principal prover uma visão unificada dos diferentes projetos da operação, de forma a viabilizar a comparação e análise dos mesmos, de maneira sistemática e confiável.
Em esse sentido, foi desenvolvida uma arquitetura composta por três grandes componentes:
A) Processo de Carga;
B) Repositório;
E c) Interface de Bi, descritos sucintamente nas próximas seções.
O processo de Carga da BO é responsável por coletar métricas das seis áreas de qualidade (Esforço, Qualidade, Custo, Cronograma, Tamanho e Requisitos), relatadas na Seção 4.3.1, e consolidar- las no repositório central, a partir de um ambiente heterogêneo de projetos e ferramentas.
Em esse ambiente as métricas de Cronograma encontram- se armazenadas no MS Project.
Por sua vez, as de Qualidade podem ser capturadas do Bugzilla, ClearQuest ou Mantis, dependendo do projeto.
Já as métricas referentes ao esforço realizado podem estar armazenadas na base de dados do MS Project, localizada no ambiente do projeto, ou no Banco de Horas, desenvolvido por um dos projetos da operação.
E, por fim, os requisitos e os tamanhos podem estar armazenados em documentos distintos não estruturados, variando conforme o projeto.
A definição do processo de carga, adotado por a BO, apresenta uma série de etapas manuais e semi-automatizadas, executadas por pessoas específicas.
A Figura 12 ilustra a ordem seqüencial de execução dessas etapas.
1) Preparação dos dados:
Consiste na execução de uma série de atividades manuais, realizadas no ambiente de projeto, as quais exigem que um dos seus recursos seja deslocado das suas atividades regulares.
Essas atividades têm por objetivo capturar as métricas de fontes distintas e organizar- las em arquivos estruturados, conforme um padrão determinado por a organização.
Para tanto é necessário preparar um pacote contendo os seguintes arquivos:
O arquivo de cronograma possui valores referentes às estimativas das diferentes etapas do projeto, caracterizando iterações, fases e atividades.
Esses dados estão dispostos na ferramenta MS Project na forma de itens, de acordo com uma ordem de classificação, que pode ser percebida visualmente conforme a disposição dos mesmos.
Essa ordem é conseqüente do ciclo de vida (e.
g cascata, iterativo) ou do modelo de gestão (e.
g orientado a fases ou a entregáveis), adotado por o projeto.
Assim, a preparação desse arquivo requer a inclusão de três campos adicionais:
Nome da fase, tipo da atividade e número da iteração, informados de forma manual, explicitando a classificação de cada dos seus itens.
Além disso, é necessário realizar a exportação desses dados para um arquivo de extensão».
Mpp». Os dados de esforço realizado devem ser exportados do Banco de Horas, conforme o padrão estabelecido.
Cabe salientar que, num dos projetos, o esforço realizado encontra- se armazenado no cronograma.
Por essa razão, não é necessário realizar consultas específicas para coletar- los, pois os mesmos são lançados por os recursos diretamente no cronograma.
A preparação do arquivo de defeitos exige a execução de consultas nas bases de dados das ferramentas de bugtracking (IBM Rational ClearQuest e Bugzilla) e padronização dos seus resultados segundo a guia de registro de defeitos.
Com relação a os dados de requisitos, os mesmos não se encontram armazenados de forma estruturada e, por essa razão, não é possível realizar consultas para capturar- los.
Portanto, os mesmos são coletados de forma manual e inseridos numa planilha MS Excel, conforme o padrão documentado na guia de captura de requisitos.
O arquivo de informações adicionais apresenta dados de custo, tamanho da versão, unidade de tamanho (e.
g KLOC, PF, PMHP), tamanho da equipe, status (e.
g Fechando, Em andamento), entre outros.
Os dados utilizados para alimentar esse arquivo são provenientes de diferentes fontes e o seu procedimento de construção varia conforme o projeto.
No entanto, tanto a organização como os projetos não possuem uma documentação apropriada relatando os passos necessários para a obtenção dos valores mencionados.
2) Upload do Pacote de Carga:
Concluída a fase de preparação dos dados, cada projeto gera um conjunto de arquivos, e esse é enviado por upload para o site da BO.
3) Homologação dos Dados Coletados:
O responsável por a carga recebe um aviso que os arquivos se encontram à disposição.
Assim, eles passam por um processo manual de conferência, onde ocorre a verificação de coerência geral do preenchimento e da formatação, o qual denomina- se homologação das fontes de informação, realizada segundo o documento checklist de carga.
Além disso, durante essa etapa também os dados que compõem o arquivo de informações adicionais são inseridos nos arquivos de cronograma, e esses últimos são publicados na base de dados organizacional do EPM (Enterprise Project Manager).
4) Realização da Carga:
Depois que os dados são homologados, eles encontram- se no formato adequado para que possa ser realizado o processo de Etc das métricas.
Esse é automatizado por intermédio de pacotes DTS (Data Transformation Services) do SQL Server 2000, tendo como pré-condição que todos os dados dos projetos estejam coletados, transformados e disponibilizados, segundo o padrão organizacional especificado nos documentos de carga.
Concluída essa etapa, os dados estão devidamente armazenados no repositório de dados da BO.
5) Homologação dos Dados apresentados por a Interface de Bi:
Quando a etapa anterior é concluída, o gerente de Software do projeto é notificado.
Então, um novo processo de homologação de dados deve ser realizado, onde ocorre a conferência dos dados apresentados por a Interface de Bi.
Após a finalização dessa parte do processo de carga, uma notificação de homologação do conteúdo é inserida na interface.
O repositório de métricas está estruturado na forma de um DW e contém um conjunto de tabelas, organizadas segundo um esquema dimensional do tipo constelação de fatos.
As tabelas fato contêm medidas básicas e calculadas sobre versões, fases, iterações, tipos de atividades e defeitos.
As tabelas dimensão contêm atributos que qualificam estes fatos, tais como:
Diferentes propriedades dos projetos, versões, iterações, fases, etc..
Cabe salientar que o modelo analítico do repositório baseia- se na solução proposta por o SPDW (Seção 4.3).
Porém, foram feitas algumas adaptações devido a as limitações da ferramenta que suporta a interface de Bi.
Maiores detalhes desse modelo e do conteúdo das suas tabelas podem ser encontrados em.
A interface de Bi da ferramenta licenciada consiste num portal web, formado por:
Recursos OLAP de acesso aos cubos de dados;
Um painel de indicadores;
Recursos para visualização das consultas realizadas por os usuários;
Calendário e integração com correio eletrônico;
Etc.. A interface OLAP permite apresentação dos dados através de tabelas dinâmicas, manipuladas com o apoio de um browser, facilidades de tabelas pivotantes e recursos tradicionais OLAP.
A partir de os dados das tabelas pivotantes, gráficos podem ser gerados e visões sobre os cubos podem ser pré-definidas.
Esta seção resume as principais dificuldades encontradas no ambiente da BO.
Para que essa percepção fosse possível, diversos estudos foram realizados, bem como entrevistas com os potenciais usuários da BO, as quais seguiram um instrumento de pesquisa.
Ambos encontram- se especificados no documento M1 -- Diagnóstico da Base Organizacional.
O consenso nas opiniões coletadas, dos diferentes perfis de usuários entrevistados, deixou claro que o funcionamento e a utilização efetiva da BO dependem da modificação do atual processo de carga.
Esse foi caracterizado como crítico devido a uma série de limitações, as quais estão descritas abaixo. --
Processo de preparação dos dados laborioso e sujeito a erros:
O processo de obtenção dos dados necessários não é satisfatório, sendo caracterizado como laborioso e sujeito a erros.
Isso se deve ao fato de ser altamente manual e, em alguns momentos, considerado como sobrecarga de atividades.
Essa limitação foi apontada como crucial por os usuários, tanto por os participantes do processo de preparação como por os que têm conhecimento da forma por a qual o mesmo é desempenhado. --
Confiabilidade dos dados implica em grande esforço e trabalho manual:
Devido a o fato da coleta das medidas ser realizada de forma manual, a confiabilidade dos dados coletados e apresentados por a interface de Bi requer a execução de duas tarefas manuais de homologação, também consideradas sobrecarga de atividades e passível de inserção de erros. --
Latência dos dados não é a desejada:
A dificuldade de preparação e consolidação dos dados faz com que essas informações sejam repassadas à BO mensal ou quinzenalmente.
Devido a esse intervalo de tempo, os dados disponibilizados na interface de Bi não refletem a situação atual do projeto, impedindo qualquer ação de monitoração. --
Nível de Isolamento dificulta o processo:
A BO está localizada na Intranet da organização, enquanto que alguns projetos devem, por razões de segurança, trabalhar em subredes isoladas, sem acesso mesmo à Intranet.
Sendo assim, para a coleta de qualquer métrica desse tipo de projeto, é inevitável alternar, fisicamente, as conexões com as redes ou usar algum tipo de mídia para transporte de arquivos.
Isso dificulta o acesso e cria dificuldades adicionais no processo de preparação e envio dos dados. --
Granularidade da informação disponibilizada nem sempre é a desejada:
A grande maioria dos entrevistados não está satisfeita com o nível de detalhamento das informações disponibilizadas.
Muitos precisam de informações mais detalhadas e não conseguem ter acesso às mesmas.
Alguns sugerem informações em granularidade maior (e.
g por projeto), enquanto outros, em granularidade menor (e.
g por atividade, e não por tipo de atividade (ver Seção -- Não é oferecido suporte à Monitoração:
Entre os Gerentes de Projetos e Gerentes de Software, foi unânime a sugestão de se utilizar a BO para o monitoramento dos projetos em desenvolvimento.
As atuais métricas baseadas em variações só produzem resultados significativos quando aplicadas sobre projetos concluídos, em situações de análise retrospectiva.
Atualmente, esses usuários já realizam esforço para coletar as medidas, preparar e homologar os dados do processo de carga.
Porém a BO não lhes oferece informações úteis para monitoração.
Sendo assim, o benefício da BO não é claramente percebido, o que reforça a caracterização das atividades de preparação e inserção de dados na BO como sobrecarga.
Para que a BO possa dar suporte à monitoração de projetos, os dados coletados, as métricas calculadas e os indicadores devem ser revistos. --
Processo de Carga não é Incremental:
O atual processo de carga não é incremental.
Sempre que um novo conjunto de dados precisa ser inserido ou alterado, todos os dados anteriores são recarregados.
Sendo assim, o responsável por o processo de carga deve manter um histórico contendo todos os arquivos de carga utilizados anteriormente.
Essa prática oferece vários riscos à organização, além de ocorrerem inserções desnecessárias de registros já existentes dentro de o banco de dados, ocupando espaço e dificultando o processo de extração de informação através de outras interfaces que não sejam a da Ferramenta de Bi.
Outro problema está relacionado às rotinas de backup para manter, com segurança, todos os arquivos utilizados em cargas anteriores.
Com base nas dificuldades encontradas no ambiente da BO constatou- se a necessidade de automatizar o processo de Etc das métricas, objetivando minimizar as tarefas manuais, executadas durante o processo de carga.
Para tanto, foram realizados estudos para estimar o esforço necessário e os possíveis impactos causados por a automatização.
As próximas seções descrevem brevemente:
O tempo total consumido por o atual processo de carga;
A solução computacional vigente desse processo;
Uma proposta de automatização para o mesmo;
E as estimativas e o impacto dessa última.
Para mensurar o tempo total consumido por o processo de carga foi realizado o acompanhamento da sua execução no ambiente de um dos projetos da organização.
O projeto escolhido utiliza como ferramentas de apoio à gestão:
O MS Project (Cronograma), o ClearQuest (Qualidade), um software personalizado para coletar esforço realizado, algumas planilhas MS Excel e documentos MS Word (demais áreas de qualidade).
A Tabela 7 apresenta os tempos despendidos durante a preparação dos dados, a serem armazenados em arquivos padronizados, de uma determinada versão (podem haver várias em aberto), com suas respectivas fontes de armazenamento.
Já a Tabela 8 mostra os tempos despendidos durante a realização das atividades de homologação (conferência) e realização da carga (execução dos DTS de carga).
O somatório de tempos da primeira tabela é de 90 minutos por versão.
Como, durante o acompanhamento da carga, existiam seis versões em andamento, o tempo total para a realização dessa etapa foi de 540 minutos.
Assim, a partir de as duas tabelas, constata- se que foram despendidos 758 minutos para execução de todo o processo de carga, ou seja, 12h38 min (três turnos de trabalho de um recurso).
Ressalta- se que esses tempos são proporcionais ao número de versões em andamento, mas mesmo assim são considerados elevados e mal vistos por os projetos.
No entanto, salienta- se que esse processo de carga ainda é considerado conveniente à organização, já que o repositório de métricas fornece, somente, suporte à análise dos projetos, e que se admita uma maior latência dos dados.
Em esse sentido, a organização entende que é economicamente aceitável deslocar recursos, tanto de projeto quanto do grupo de qualidade, das suas atividades normais para realizar o processo de coleta uma vez por mês.
Tabela 8 -- Tempos para a homologação e execução dos pacotes de carga.
Atividades Aceitar Pacote de Carga Conferir e Publicar Cronogramas Conferir Arquivo de Defeito Conferir Arquivo de Esforço Conferir Arquivo de Requisito Mover Arquivos Executar DTS Conferir Dados Interface de Bi Ambiente/ Ferramenta Site BO MS Project MS Excel MS Excel Windows Explorer SQL Server 2000 Site BO Tempo (min) O atual processo de Etc consiste na execução seqüencial de pacotes DTS, tendo como pré-condição que todos os dados brutos dos projetos sejam coletados, transformados e disponibilizados segundo o padrão organizacional especificado nos documentos de carga.&amp;&amp;&amp;
A Figura 15 representa o fluxo seqüencial de execução dos principais DTS, os quais contêm subfluxos onde são especificadas as ações necessárias para realizar:
A extração dos dados dos arquivos das diferentes áreas de qualidade;
A transformação desses dados a partir de uma área de armazenando temporária (DSA, Seção 2.4);
E (iii) a sua carga e a geração dos cubos, conforme a estrutura dimensional do repositório de métricas.
Cada um desses DTSs, apresentados na Figura 15, possui um escopo bem definido:
Carga Requisitos:
Responsável por extrair, transformar e limpar as informações relacionadas com requisitos, disponibilizadas por os projetos através de planilhas MS Excel, formatadas conforme o padrão organizacional especificado, e armazenar- las no Carga Defeitos:
Realiza a extração, transformação, limpeza e cálculo das métricas referentes aos defeitos localizadas em planilhas MS Excel.
Carga Esforço: Trata os dados e as métricas relacionadas com o esforço realizado.
No caso de um determinado projeto, esses dados de esforço são extraídos de planilhas MS Excel.
Já em outro projeto, essas informações são retiradas diretamente da base de dados do EPM.
Temporária: Responsável por coletar os dados relacionados com os projetos (tamanho do time, custo, versão, tamanho, entre outras) e com o cronograma (atividades, baselines, fases, iterações, entre outros).
Durante a execução desse pacote, esses dados são extraídos diretamente do EPM e colocados primeiramente em tabelas temporárias (DSA), onde passam por um processo de transformação e limpeza para depois serem armazenados no DW.
Em esse pacote também é realizado o cálculo de algumas métricas derivadas da organização.
Dimensões: Responsável por atualizar as dimensões com base nos dados carregados nas tabelas temporárias, alimentadas por os DTSs anteriores.
Fato: Responsável por construir as tabelas fato com base nas tabelas temporárias, alimentadas por os DTSs anteriores.
A proposta de automatização consiste na migração dos atuais DTS para SQL Server Integration Services (SSIS), utilizando Ws para extrair os dados diretamente da fonte original, localizada no ambiente dos projetos.
Essa proposta é conseqüente do interesse da organização de continuar utilizando a interface de Bi, a inteligência dos DTS, bem como a ferramenta de gestão de banco de dados oferecida por a Microsoft.
Os SSIS acompanham a versão 2005 do SQL Server e seu uso é considerado a evolução para os DTS da versão 2000.
Além disso, esses novos serviços possuem componentes especiais que facilitam a inclusão de Ws, permitindo que os mesmos possam ser chamados durante a sua execução.
Segundo a Microsoft, os DTS estão sendo descontinuados e por essa razão não serão disponibilizados nas versões posteriores do SQL Server.
Logo, os mesmos devem ser migrados.
Dentro desse contexto foram realizados estudos para migrar os DTS para SSIS e incorporar Ws de extração.
Os mesmos apresentam:
Uma breve descrição da configuração do ambiente computacional necessário para editar, executar e testar os SSIS;
Os passos executados para realizar a migração dos DTS para SSIS;
E a especificação dos DTS de Defeitos e Temporária e dos seus SSIS correspondentes, incluindo a funcionalidade de Ws;
E o esforço estimado e o impacto causado para realizar a migração completa do processo de Etc atual.
A Figura 16 tem por objetivo ilustrar os componentes envolvidos na construção de um DTS e destacar os pontos onde os Ws devem ser inseridos.
A mesma contém os componentes que constituem o DTS Temporária.
As setas determinam a ordem do fluxo de execução dos mesmos.
Os demais componentes do fluxo, representados por cilindros, são responsáveis por executar tarefas SQL, e os mini-diagramas caracterizam as atividades programadas em scripts ActiveX.
As setas horizontais de cor preta, com rótulos Produto e Análise x Fase, realizam a importação e exportação de dados entre os componentes Base Project e Base Analítica, que representam respectivamente a base de dados do EPM e o repositório da BO.
Portanto, os procedimentos dessas setas são os que devem ser substituídos por Ws, para que a transferência dos dados possa ser realizada diretamente da fonte original para a área de armazenamento temporária (DSA).
A solução de automação proposta por os estudos citados, baseada em Ws e SSIS, exige que os DTS sejam migrados e adaptados.
Para tanto, foi feito um estudo para avaliar o impacto dessa migração, envolvendo serviços de extração, dos pacotes de carga DTS Temporária e de Defeitos para o formato SSIS.
Esses serviços foram codificados no Visual Studio, utilizando J&amp; como linguagem de programação.
Essa solução também contemplou a avaliação do impacto e do esforço associados com a evolução do Programa de Métricas e do modelo analítico do repositório central.
Ambas são conseqüentes da incorporação de métricas de monitoração.
Cabe ressaltar que essas evoluções não são adotadas nesta pesquisa.
As soluções citadas sugerem meramente o armazenamento das métricas derivadas, segundo os valores definidos por a EVA, e a inclusão de um novo fato, chamado Fato Demanda.
O modelo analítico proposto por o trabalho citado, mantém as mesmas limitações relacionadas com granularidade mencionadas na Seção 5.2, deste capítulo.
Já esta pesquisa realiza apenas a extração das métricas diretas dessa técnica, calculando os seus valores durante o processo de sumarização dos cubos.
Além disso, esta solução apresenta um modelo analítico mais elegante, com um número menor de tabelas fato, porém permitindo uma maior abrangência de perspectivas de análise e níveis de sumarização, o que a torna mais abrangente.
Os serviços de extração substituem as tarefas de transformação de dados Produto e Análise x Fase.
Como resultado do estudo citado foi possível contabilizar o número de componentes que devem ser modificados, bem como classificar seus níveis de complexidade e dificuldade (e.
g Intermediário, Difícil).
Essa classificação foi determinada com base na experiência de migração relatada em, para auxiliar a estimativa do esforço de migração de todos os pacotes de carga.
A Tabela 9 mostra essa classificação, o número total de componentes que constituem o SSIS, a quantidade de componentes que precisa ser alterada ou incluída, e o esforço total de migração estimado em horas.
Assim, contabilizando os esforços apresentados nessa tabela obtêm- se uma estimativa de aproximadamente 900 horas de desenvolvimento.
Com base no que foi apresentado neste capítulo, percebe- se que o atual ambiente da BO não satisfaz a empresa parceira, principalmente no que se refere ao processo, laborioso e manual, de captura das métricas e a ausência do suporte à monitoração.
Ambos são, respectivamente, conseqüentes de um processo semi-automatizado de captura das métricas e da ausência de métricas adequadas para monitoração, bem como de um modelo analítico com uma granularidade maior que a desejada.
Além disso, cabe salientar que essa operação de software pretende atingir os níveis mais altos do CMMI.
Por essa razão, é essencial que essas dificuldades sejam minimizadas, para que o repositório central possa oferecer suporte à monitoração, e conseqüentemente a uma gestão quantitativa, conforme o Nível 4.
Para tanto, é primordial realizar cargas freqüentes e com um baixo nível de intrusão.
A proposta de migração dos DTS para SSIS, incluindo Ws e incorporando serviços de extração, mostra- se adequada para minimizar o esforço de carga e permitir a monitoração.
No entanto, essa solução não trata todos os aspectos de heterogeneidade, freqüência e latência mencionados na Seção 3.1.
Além disso, a adoção dessa proposta foi suspensa, devido a recente aquisição, por a Hp Company, da Mercury Interactive, que licência um conjunto de ferramentas de apoio à gestão do desenvolvimento de software.
Por essa razão, a gerência da operação brasileira ainda não se pronunciou sobre qual encaminhamento vai ser dada à BO como instrumento de gestão de projetos.
Portanto, conclui- se que as características e as dificuldades da operação de software, apresentadas neste capítulo, serviram para corroborar as dificuldades levantadas no SPDW, reportadas na Seção 4.3, e enfatizar a necessidade de procurar uma solução que enderece os aspectos computacionais mencionados no Capítulo 2.
Este capítulo descreve a solução computacional para automatizar a mensuração da qualidade de software denominada SPDW+.
É uma evolução substancial do SPDW, proposto por e por, principalmente, dar suporte à monitoração.
Em esse sentido, as próximas seções contemplam:
O método de pesquisa utilizado;
A definição do programa de métricas suportado por a solução;
A determinação do modelo analítico;
Uma visão geral da arquitetura do SPDW+;
A especificação do processo automatizado de Etc das métricas, juntamente com as camadas que compõem a arquitetura e as soluções adotadas para os seus componentes principais;
E a verificação da qualidade da solução, segundo o instrumento de referência (Seção 3.2).
Segundo &quot;todo trabalho científico deve ser apreciado em procedimentos metodológicos».
Quanto a os objetivos, classifica- se esta pesquisa como exploratória, pois foi realizada a identificação de um problema e tem- se como finalidade a descoberta de teorias e práticas a serem aplicadas no ambiente real, pretendendo modificar as ações existentes e buscando oferecer as inovações tecnológicas necessárias para solucionar as dificuldades identificadas.
Quanto a o procedimento, inicialmente foi realizado um estudo de um ambiente real numa operação de software certificada CMM3, onde foram constatadas limitações importantes no monitoramento dos seus projetos em andamento, devido a a dificuldade de obter métricas atualizadas, de maneira automatizada, considerando o ambiente heterogêneo e dinâmico do PDS.
Ainda, foi construído um ambiente controlado, em laboratório (laboratório GPIN), com a intenção de simular o ambiente real, onde foram realizados testes para avaliar a qualidade da solução proposta.
Segundo a técnica de EVA, só se pode monitorar ao longo de o tempo algo que apresente uma linha base, e que permita uma comparação entre o inicialmente planejado e o efetivamente realizado, verificando se o seu desempenho apresenta variações dentro de uma faixa de valores considerada aceitável.
Assim, com base nessas exigências foi feita uma análise das áreas de qualidade originais, buscando detectar quais poderiam ser monitoradas.
Com base nos resultados dessa análise, concluiu- se que: --
Tempo: As métricas desta área são fundamentais à análise do valor agregado, pois são responsáveis por determinar o intervalo cronológico em que determinada tarefa deve ser realizada.
Por esse motivo, as métricas de tempo estão diretamente relacionadas com as demais áreas que podem ser monitoradas. --
Esforço e Custo:
As métricas da área de esforço apresentam as características desejáveis à realização da análise do valor agregado:
Esforço planejado e real, dentro de um intervalo de tempo definido por um cronograma.
Logo, é conveniente verificar se o esforço realizado está conseguindo agregar o valor desejado à determinada tarefa, produto ou projeto.
Assim como as de esforço, as métricas de custo detêm as características desejáveis para que se possa aplicar a EVA. --
Tamanho: Esta área contém métricas de tamanho estimado e real, dentro de um intervalo de tempo pré-estabelecido.
Contudo, durante o desenvolvimento do código de um componente de software, é difícil ter- se a informação do percentual de tamanho completado, pois a mesma depende muito da técnica de escrita de código que o desenvolvedor usa bem como do tipo de ambiente de programação adotado.
Logo, a mesma não apresenta os pré-requisitos necessários suportar a monitoração. --
Requisitos: As métricas desta área controlam a variação do número de requisitos inicialmente acordados com os clientes.
Durante a construção de um determinado produto, o cliente pode adicionar ou excluir funcionalidades conforme as suas necessidades e prioridades.
Esse comportamento interfere no desempenho da execução do cronograma, pois o planejamento inicial precisa ser refeito e, em alguns casos, o trabalho realizado acaba sendo computado, mesmo sem agregar valor ao produto, podendo gerar atraso na data de entrega.
No entanto, tipicamente não é realizado um controle ou avaliação do percentual de completude de um requisito, fundamental à determinação do valor agregado.
VBO ­ Variação do Baseline Original Dir ­ Data Inicial Real VBR ­ Variação do Baseline DFR ­ Data Final Real DIBO ­ Data Inicial do Baseline Original Revisado DFBO ­ Data Final do Baseline Original DIBR ­ Data Inicial do Baseline Revisado DFBR ­ Data Final do Baseline Revisado VEBO ­ Variação de Esforço do Er ­ Esforço Real Baseline Original EBO ­ Esforço do Baseline Original VEBR ­ Variação de Esforço do EBR ­ Esforço do Baseline Revisado Baseline Revisado TR ­ Tamanho Real PR ­ Produtividade VTBO ­ Variação de Tamanho do TBO ­ Tamanho do Baseline Original Baseline Original TBR ­ Tamanho do Baseline Revisado VTBR ­ Variação de Tamanho do TR ­ Tamanho Real Baseline Revisado Cr ­ Custo Real VC ­ Variação de Custo VCA ­ Variação de Custo Agregada CBO ­ Custo do Baseline Original VPA ­ Variação de Prazo Agregada CBR ­ Custo do Baseline Revisado IDC ­ Índice de Desempenho de CRAR ­ Custo Real da Atividade de Revisão CRFT ­ Custo Real da Fase de Teste Custo CRAQ ­ Custo Real das Atividades de IDP ­ Índice de Desempenho de Prazo Qualidade CRART ­ Custo Real das Atividades de Retrabalho% Tc ­% Trabalho Completado Ds ­ Data de Status VR ­ Volatilidade de Requisitos NMA ­ Nro.&amp;&amp;&amp;
De Modificações Aprovadas NMR ­ Nro.
De Modificações Requeridas NRE ­ Nro.
De Requisitos Excluídos NRM ­ Nro.
De Requisitos Modificados ERD ­ EfiCiência de Remoção de NDI ­ Nro.
De Defeitos Internos NDE ­ Nro.
De Defeitos Externos Defeitos DDE ­ Densidade de Defeitos TR ­ Tamanho Real ISC ­ Índice de Satisfação do Cliente Entregues DDI ­ Densidade de Defeitos Internos ERV ­ EfiCiência de Revisão SC ­ Satisfação do Cliente -- Qualidade:
As métricas desta área não apresentam valores estimados durante um espaço de tempo pré-definido.
Além disso, a métrica NDE (Número de Defeitos Externos) considera valores detectados quando o produto encontra- se em funcionamento no ambiente do cliente.
Logo, não existe um intervalo de tempo definido para a captura dessas informações.
Por essa razão, o método de análise do valor agregado não pode ser aplicado.
A partir de essas conclusões optou- se por incluir métricas para monitorar Custo e Tempo.
Considerou- se Esforço um caso particular da primeira (quando os recursos alocados para realizar uma determinada tarefa apresentam custos equivalentes) e, ao considerar Custo, entende- se que Esforço também está sendo coberto.
A área de qualidade Tamanho não foi incluída devido a a dificuldade em coletar e tratar adequadamente o percentual de tamanho completado.
Já as áreas de qualidade Requisitos e Qualidade (Defeitos e Satisfação do Cliente) também não foram incluídas porque dificilmente apresentam valores estimados e parciais.
Logo, para oferecer suporte à monitoração, de prazo (Tempo) e custo, utilizando a técnica de EVA foram definidas as seguintes métricas:
Diretas: -- Percentual de Trabalho Completado(% Tc) -- Data de Status (Ds) Derivadas: --
Índice de Desempenho de Prazo (IDP) -- Índice de Desempenho de Custo (IDC) -- Variação de Prazo Agregada (VPA) -- Variação de Custo Agregada (VCA) A Tabela 11 apresenta a definição dessas métricas derivadas, juntamente com o seu objetivo, as métricas diretas utilizadas para o seu cálculo, bem como a sua equação e a sua unidade de tamanho.
Esta seção apresenta um exemplo da monitoração de custo e prazo, segundo a técnica de EVA.
Inicialmente, é apresentado o cronograma de um projeto fictício, juntamente com os dados estimados e as métricas de monitoração.
Por fim, é relatada a interpretação desses valores.
A Tabela 12 ilustra o planejamento inicial desse projeto fictício, com duração de um ano e contendo uma única versão, composta por diversas fases, formadas por um conjunto de tarefas.
As colunas dessa tabela apresentam os valores estimados (VE e VE Ds), o valor agregado (VA), as métricas diretas(% Tc e Cr) e derivadas (VPA, VCA, IDP e IDC).
Para cada uma das Tarefas é realizado um planejamento inicial de custo (VE).
Com relação a o prazo, supondo- se uma distribuição linear do orçamento, espera- se que no término do primeiro trimestre as atividades estejam 25% concluídas;
Em o segundo, 50%;
Em o terceiro, 75%;
As células em amarelo, da Tabela 12, correspondem às tarefas rigorosamente no prazo e no custo inicialmente planejado.
Por sua vez, as em verde significam que os níveis encontram- se adiantados ou abaixo de o orçamento previsto.
Já as em vermelho representam níveis em atraso ou acima de o custo estimado.
Com base nos valores referentes à Tarefa 1 é possível construir o gráfico da Figura 17.
Por intermédio do mesmo, percebe- se que essa tarefa está sendo realizada conforme o prazo planejado, pois as curvas VE e VA se sobrepõem no ponto determinado por a data de status.
Logo, pode- se concluir que essa tarefa foi executada exatamente no prazo planejado (25% concluída).
Porém a mesma está sendo efetuada com um orçamento maior do que o estimado.
Isso pode ser constatado através da análise das curvas Cr e VE, onde a primeira apresenta uma distância positiva em relação a a segunda, tendo como referencial o eixo das ordenadas.
Por essas razões, a variação de prazo (VPA) apresenta valor zero, enquanto que a variação de custo (VCA) é negativa.
Maiores detalhes sobre a obtenção dos valores, que compõem a Tabela 12, e a interpretação dos mesmos podem ser encontrados na Seção 2.3.
O modelo analítico descrito nesta seção é conseqüente do Programa de Métricas e dos modelos de processo de desenvolvimento de software, representados por a estrutura de projetos, relatada na Seção 6.3.1.
É relatado em detalhes o modelo dimensional proposto, e é discutida a necessidade de adoção do período de validade para os fatos, baseada em técnicas de banco de dados temporais, e mostrada a solução adotada.
A Figura 18 mostra a estrutura de projetos adotada, representada num diagrama de classe UML.
A definição dessa estrutura considera o conteúdo do Programa de Métricas (Seção 6.2) e os pontos dos modelos de processo de desenvolvimento de software onde as métricas devem ser coletadas.
Tal estrutura permite representar modelos de processos com ciclo de vida cascata e iterativo, considerados clássicos e amplamente utilizados.
Além disso, a mesma também admite variações desses ciclos que contenham pequenas diferenças relacionadas com:
O nome, a ordem e a relação entre as fases ou os artefatos adotados.
Por exemplo, cascata com sobreposição, espiral e evolutiva, citadas na Seção 2.1.1.
De essa forma, esta solução busca tratar de maneira adequada às particularidades relacionadas com a heterogeneidade dos modelos de processo de software, relatadas na Seção 3.1.3.2.
A estrutura adotada permite que cada projeto contenha uma ou mais versões, sendo que as mesmas podem apresentar um conjunto de iterações ou fases.
Porém, uma fase pode ou não pertencer a uma iteração.
Essa condição somente é possível devido a a presença da classe estágio.
As fases contêm atividades classificadas segundo seu tipo:
Trabalho, retrabalho, revisão e qualidade.
Os defeitos são mensurados a partir de as fases e devem ser classificados conforme o seu grau de severidade (e.
g alto, médio ou baixo).
Os atributos das classes do diagrama correspondem às métricas diretas e derivadas da Tabela 10, representando os níveis de detalhe em que as mesmas podem ser capturadas, ou produzidas.
Essa estrutura também admite mais de uma hierarquia de cronograma:
Orientado a fases e orientado a entregáveis.
Maiores detalhes sobre essas hierarquias e os ciclos de vida, com seus respectivos impactos, podem ser encontrados em.
Os aspectos ressaltados nessa estrutura representam:
As métricas para oferecer suporte à monitoração, destacadas em sombreado;
As métricas coletadas no grão de atividade, delimitadas por retângulos;
E as classes Severidade e Tipo, enfatizadas por círculos.
O primeiro aspecto permite que os projetos e a organização tenham seus custos e prazos monitorados a partir de o nível de atividade.
Já o segundo possibilita uma supervisão mais detalhada dos projetos e oferece subsídios para que a primeira seja realizada.
Por sua vez, o terceiro admite, por um lado, classificar os defeitos conforme ao seu grau de severidade, permitindo que novas categorias possam ser adicionadas quando necessário, e, por outro lado, torna possível, por a classe Tipo, a classificação de novas atividades, permitindo que outras categorias sejam criadas, sem que isso comprometa a estrutura de projeto.
Uma situação real ilustrativa foi a nova classificação de retrabalho, adotada por a operação de software relatada no Capítulo 5, em que foi necessário definir novos valores para as atividades de retrabalho:
Pré-Release (Ret-Pré-Rel) (realizado durante o desenvolvimento do produto) e Pós-release (Ret-Pós-rel) (efetuado quando o produto encontra- se em produção).
É importante mencionar que os aspectos ressaltados contribuem significativamente para tornar a solução mais flexível e abrangente, pois possibilitam que novas categorias possam ser inseridas e a monitoração de custo e prazo possa ser feita a partir de o nível de atividade.
O modelo analítico, substancialmente aprimorado em relação a o proposto por o SPDW, mantém a estrutura dimensional do tipo constelação de fatos e oferece suporte à análise e monitoração.
Para ser legível, o modelo ilustrado nesta seção contém, apenas, as áreas de qualidade que correspondem às métricas armazenadas nas tabelas fato, brevemente, descritas na Tabela 13.
Já as tabelas dimensão são representadas apenas por o seu nome e encontram- se descritas na Tabela 14.
Maiores detalhes sobre o embasamento para a definição desse tipo de modelo de dados dimensional podem ser encontrados na Seção Com relação a a estrutura das tabelas fato, esta proposta apresenta somente métricas aditivas (e.
g esforço (Er) e custo (CBO)) e semi-aditivas (e.
g percentual de trabalho completado(% Tc) nos seus atributos, fundamentais para possibilitar sumarizações entre as diferentes dimensões.
De essa forma, com somente três tabelas fato (Fato_ Atividade, Fato_ Versão e Fato_ Defeito), tem- se diferentes perspectivas de análise e níveis de sumarização (iteração, fase, tipo do fato), de uma maneira elegante.
Os fatos também são compostos por chaves estrangeiras, responsáveis por manter a integridade referencial das dimensões e possibilitar diferentes perspectivas de análise e níveis de sumarização.
É importante salientar que as métricas derivadas não fazem parte da estrutura das tabelas fato e são calculadas no momento da criação dos cubos (Seção 7.4).
Maiores detalhes sobre a estrutura das tabelas fato e dimensão e dos seus relacionamentos podem ser encontrados no Apêndice A. Armazena métricas de Defeitos (internos e externos) de uma versão em determinada fase.
Armazena métricas (originais, revisadas e reais) de Esforço, Cronograma e Custo das atividades.
Corresponde a distintas dimensões com propriedades de projetos, que por razões de legibilidade, foram agrupadas no desenho.
As dimensões são:
Projeto, Indústria, Cliente, Porte, Tipo (manutenção ou desenvolvimento) e Tecnologia.
Dados que caracterizam as versões do projeto.
Dados que caracterizam iterações de versões de projeto.
Dados que caracterizam fases de versões de projeto.
Dados que caracterizam atividades das fases do projeto, classificadas de acordo com seu tipo (trabalho, retrabalho (Ret-Pré-Rel e Ret-Pós-rel), revisão, qualidade).
Dados que caracterizam defeitos de fases de versão, baseados em categoria (interno, externo) e severidade (baixa, média e alta).
Dados que caracterizam status do projeto, versão, iteração ou fase (em andamento ou finalizado).
Dados que caracterizam a data inicial e final do fato.
Define se o fato é uma estimativa (baseline original, baseline revisado) ou registro de uma realização (real).
Com relação a as tabelas dimensão, vale ressaltar Dim_ Tipo_ Fato, composta por uma chave primária e um atributo categórico, contendo o tipo do fato:
Original (planejado), revisado (replanejado) e real (efetivamente realizado).
Assim, a partir de essa dimensão, é possível oferecer essas três perspectivas de análise e, além disso, sumarizar- las em diferentes níveis, de maneira intuitiva e transparente.
A realização da monitoração requer um acompanhamento regular dos custos e prazos associados para cada atividade, conforme as particularidades exigidas por a supervisão do ambiente de desenvolvimento de software, descritas na Seção 3.1.2.
Para tanto, o modelo proposto inclui métricas das áreas de qualidade:
Esforço, Tempo e Custo, no nível de atividade.
Para oferecer um suporte adequado à monitoração da mensuração da qualidade de software, é aceitável que a coleta das métricas apresente uma freqüência e uma latência na ordem de horas.
Além disso, ainda é necessário disponibilizar- las de maneira consolidada, consistente e atualizada a partir de um repositório central de métricas.
Por outro lado, esse repositório também deve manter um histórico dessas métricas.
Para tanto, sugere que apenas sejam mantidos dados novos ou atualizados no DW.
Em esse sentido, para tratar essa última exigência, esta pesquisa propõe a adoção de período de validade para os fatos, baseado no proposto em.
Conforme a definição de banco de dados temporais e do modelo relacional temporal, para estabelecer validade aos dados é obrigatório definir dois atributos temporais (timestamps):
Data de validade inicial (Di) e data de validade final (DF).
Ambos correspondem, respectivamente, aos limites inferior e superior de um intervalo:
Um fato é considerado válido em todo o período compreendido entre Di e DF.
Além disso, cada fato pode ter valor para Di, porém valor desconhecido para DF, no instante da sua criação.
O uso de um valor infinito para representar essa situação pode resultar na errônea interpretação de que o fato é válido em qualquer instante de tempo no futuro.
Por essa razão, é sugerido que quando um evento ocorre no presente e, o seu término não é conhecido, sua DF não tenha valor válido (seja null).
Com base nessas definições e conceitos, esta solução determina que a Di receba o valor da data da coleta e DF seja nulo, até que o dado deixe de ser válido no presente e receba uma data de validade final.
De essa forma, o repositório não armazena dados repetidos, mantendo apenas os fatos novos ou atualizados, permitindo a realização da carga de maneira incremental, de acordo com as regras criadas para controlar inclusões, atualizações e término da validade dos fatos, detalhadas na Seção 6.5.4.
Tanto a estrutura das tabelas fato, quanto os atributos de validade, estão no Apêndice A. O modelo analítico proposto nesta seção amplia significativamente as vantagens já mencionadas para o SPDW (Seção 4.3), pois oferece: (
i) suporte à monitoração;
Perspectivas de análises a partir de o nível de atividade, para as áreas de qualidade:
Tempo, Esforço e Custo;
Um número reduzido de tabelas fato;
Sumarizações nos diversos níveis, a partir de métricas aditivas e semi-aditivas;
E tratamento temporal para a validade dos fatos.
O SPDW+ é um ambiente de data warehousing que oferece suporte para análise e monitoração da mensuração da qualidade de software, a partir de um Repositório de Dados, estruturado conforme o modelo analítico da Seção 6.3, e de um processo automatizado de Etc das métricas, orientado a serviço.
Para tanto, esta solução trata os seguintes aspectos de automação computacional, definidos na Seção 3.1.3: Latência dos dados;
Freqüência de realização do processo de Etc das métricas de software;
Heterogeneidades de ferramentas, modelos de processo e tipos de projetos;
E baixo nível de intrusão.
A arquitetura do SPDW+, ilustrada por a Figura 20, está organizada em camadas:
Integração de Aplicações, Integração dos Dados e de Apresentação.
A próxima seção apresenta o processo automatizado de Etc das métricas, realizado ao longo de essas camadas, especificando a solução adotada para resolver os seus principais componentes.
Cabe salientar que a Camada de Apresentação não é detalhada, visto que os seus principais componentes não fazem parte deste tema de pesquisa.
A solução adotada para definir o processo automatização de Etc de métricas, orientado a serviços, baseia- se em Ws e nos conceitos de A os, apresentados na Seção 2.5, pois ambos são considerados a tendência de mercado para atender às necessidades relacionadas à flexibilidade e adaptabilidade de processos de negócio.
Esta seção descreve a solução, a partir de as camadas da arquitetura do SPDW+, destacando os seus principais componentes:
A Camada de Integração de Aplicações, juntamente com a definição dos Wrappers, Metadados de Projeto e Rotina de Extração;
A Camada de Integração dos Dados, contemplando a DSA e as Rotinas de Limpeza e Transformação;
A carga incremental e as regras utilizadas;
E o Repositório de Dados, implementado na forma de um DW e visualizado por intermédio de um cubo OLAP.
Os Metadados de Projeto mantêm a mesma estrutura e as funcionalidades, originalmente propostas por e documentadas em, de armazenar as informações que permitam:
O mapeamento entre o dado bruto e o dado requerido, e a localização e chamada dos wrappers.
Esses metadados possibilitam que variações na gestão de desenvolvimento de software que impliquem na extração de novas métricas ou variação das suas características (e.
g nome atributo, tabela de origem, domínio), possam ser efetivadas a partir de a simples inclusão das mesmas, sem a necessidade de alterar a solução proposta para a Camada de Integração de Aplicações e, principalmente, com mínimo esforço de programação.
Para o desenvolvimento desta solução, os metadados foram estendidos para incorporar as métricas de monitoração, destacadas na Tabela 10, e informações de acesso às diferentes fontes de dados.
Essas últimas são compostas por os seguintes atributos:
URL da base de dados da ferramenta ou endereço físico do arquivo (documentos texto ou planilhas eletrônicas), (ii) username e (iii) password (Figura 21).
Tanto a estrutura desses metadados, como o seu conteúdo utilizado nos testes (Capítulo 7), estão documentados no Apêndice B. Os Wrappers são pacotes que encapsulam uma ou mais aplicações provendo uma única interface.
Esta solução determina que cada uma das ferramentas de apoio à gestão do ambiente de desenvolvimento de software tenha um wrapper correspondente, responsável por esconder as suas especificidades e extrair as métricas armazenadas na sua base de dados.
Além disso, os mesmos devem ser publicados e chamados conforme o padrão WSDL, descrito na Seção 2.5.3 e estão ilustrados na Figura 20 por semicírculos pretos.
A utilização de wrappers, segundo os padrões de Ws, apresenta as seguintes vantagens:
Possibilita a adoção de novas ferramentas e oferece suporte a novos projetos, unicamente com a definição e publicação de novos serviços e a criação de Metadados de Projeto;
Permite compartilhar código de serviços entre projetos que usem a mesma ferramenta, desde que essa apresenta estrutura de dados semelhante;
Permite a extração de dados entre diferentes ferramentas, executando em sistemas operacionais distintos;
Oferece liberdade de escolha de linguagem de programação para codificar os serviços.
Durante a execução desta pesquisa, foram definidos, implementados, publicados e testados wrappers de extração para as seguintes fontes:
MS Project, IBM Rational ClearQuest e planilhas Excel.
Como a estrutura do documento WSDL e do protocolo SOAP são semelhantes para todos esses serviços, nesta seção é apresentada, somente, a solução adotada para o MS Project.
A Figura 21 mostra o documento WSDL gerado a partir de o wrapper da ferramenta MSProject.
Para facilitar o entendimento, o mesmo encontra- se dividido em parte abstrata e concreta, conforme a estrutura apresentada por a Figura 4, na Seção 2.5.3.
Em a primeira linha desse documento está definida a versão XML utilizada.
Em seguida, os xmlns especificam a localização dos padrões de formatação empregados para criar o Ws e o trecho servidor onde os serviços estão publicados.
A parte do documento WSDL delimitada por o primeiro retângulo apresenta os elementos Types\&gt; que definem o tipo dos dados trocados entre as mensagens SOAP, incluindo o número mínimo (minOccurs) e máximo (maxOccurs) de ocorrência dos mesmos.
Com base nessa parte, percebe- se que o método MSProject apresenta tipo de dados complexo de entrada e de saída, sendo que ambos são strings, e a sua entrada é composta por os atributos url_ EPM (endereço da base de dados da ferramenta), nomeprojeto (projeto que deve ter suas métricas coletadas), versão do projeto, username e password (dados de acesso da base de dados da ferramenta).
Por sua vez a saída apresenta um único parâmetro, denominado MSProjectResult, que retorna o resultado obtido após a execução do Ws, contendo as métricas capturadas, concatenadas e separadas por um caractere especial.
A parte abstrata (assinalada na figura) é composta por mensagens, operações e port types.
Assim, as tags Message name $= &quot;MSProjectSOAPIn&quot;\&gt; e &quot;Message name «$= &quot;MSProjectSOAPOut&quot;\&gt; informam, respectivamente, o nome das mensagens de chamada e de retorno do serviço.
O elemento PortType\&gt; apresenta o conjunto das operações de um determinado Ws, e é considerado um elemento abstrato, pois não oferece informações sobre como se conectar diretamente a um Ws.
Cada uma dessas operações é definida por o elemento operation, equivalente a uma chamada de método Java ou uma subrotina no Visual Basic.
No caso de o wrapper do MSProject, o mesmo apresenta somente a operação &quot;MSProject», tendo como entrada e saída, respectivamente, as mensagens:
Parte Abstrata Mensagens Operações e Port Types Parte Concreta Binding Portas e Serviços A parte concreta é composta por:
Binding, portas e serviços.
O elemento binding, tomado como exemplo, é do tipo SOAP (Seção 2.5.2) e utiliza o protocolo de transporte Http.
Já o elemento Service\&gt; contém a definição, a localização física e o nome do serviço descrito no Binding\&gt;.
As Figura 22 e 23 ilustram, respectivamente, os protocolos SOAP de solicitação de um serviço &quot;Request «e resposta &quot;Response», responsáveis por a troca de mensagens entre os serviços.
A presença de um fluxo bidirecional de dados, e de camadas de serviços entre os wrappers e a Rotina de Extração, permite que o processo de Etc seja realizado de duas maneiras:
Sob demanda ou por iniciativa.
Em a primeira, a solução determina que a Rotina de Extração seja responsável por o início da execução do processo, por intermédio ou de eventos pré-agendados ou da ação de um determinado recurso.
Já na segunda, o processo pode ser iniciado por wrappers pró-ativos, que realizam a chamada dessa Rotina a partir de o ambiente do projeto, tanto quando valores atualizados estiverem disponíveis quando no momento em que os gestores acharem conveniente.
Em esse sentido, as camadas de serviços permitem que o processo de Etc fique sempre à disposição para eventuais chamadas, estabelecendo a troca de conteúdo entre os projetos e o ambiente de data warehousing.
Em as duas maneiras citadas, a Rotina de Extração efetua a chamada dos wrappers, com base nas informações contidas nos seus respectivos documentos WSDL e no Metadado de Projeto, tendo autonomia para se comunicar com a Camada de Integração de Dados, no momento em que achar adequado.
Assim, com base nos wrappers, nos Metadados dos Projetos e na Rotina de Extração, a coleta das métricas pode ser executada de modo automatizado, com o mínimo de intrusão e considerando um ambiente de desenvolvimento heterogêneo de ferramentas, modelos de processos e projetos.
Essa automatização do processo de extração é fundamental para diminuir a latência e possibilitar uma alta freqüência de captura, essenciais à monitoração.
Durante a execução desta pesquisa, foi implementado um protótipo de Rotina de Extração, acionado através de uma interface web e contendo três passos distintos:
1) Localização e Leitura dos Metadados do Projeto;
2) Chamada dos Wrappers especificados no metadados, passando como parâmetros as informações necessárias para capturar as métricas;
E 3) Tratamento do Retorno dos Wrappers e Inserção das Métricas na DSA.
Os itens a seguir correspondem aos trechos de códigos desenvolvidos para efetuar os três passos citados, tomando como exemplo o wrapper do MS Project.
É importante mencionar que esses trechos representam uma forma simplificada de implementar a Rotina de Extração, mas que a mesma pode apresentar variações, conforme as características do ambiente para o qual a solução está sendo aplicada. --
Passo 1: Localização e Leitura dos Metadados de Projeto A leitura dos Metadados de Projetos (Figura 24) baseia- se na biblioteca System.
Xml, que oferece métodos para acessar arquivos XML.
A variável &quot;doc», do tipo XMLDocumento, faz referência ao conteúdo do arquivo contendo o metadado.
O método &quot;load «estabelece a relação entre essa variável e o documento XML, a partir de a sua localização física, determinada por o parâmetro&quot;@ caminho».
Tal variável deve conter o endereço do arquivo».
Xml», que representa o metadado, juntamente com o seu nome.
A solução recomenda que os arquivos de Metadados de Projeto recebam o nome dos seus respectivos projetos, por exemplo:
&quot;Projeto1. Xml».
Doc.. Load(@ caminho);
Os conteúdos dos metadados podem ser vistos como uma árvore, onde os diferentes nós contêm as informações desejadas.
Assim, para acessar- los, utiliza- se o método &quot;SelectSingleNode «que permite a seleção de um único nó com base nos valores passados como parâmetro.
Por exemplo, linha de comando busca o nó denominado MSProject e armazena as suas informações na variável &quot;node».
Essa última pode ser vista, de maneira simplificada, como um vetor a partir de o qual é possível acessar, por intermédio de índices, conteúdo das suas posições através do método como parâmetros de entrada durante a chamada dos wrappers. --
Passo 2: Chamada dos Wrappers A chamada dos wrappers é realizada através de métodos da biblioteca System.
Web. As primeiras linhas do trecho ilustrado (Figura 25) mostram a criação do serviço &quot;servicoMSP», onde está declarado o wrapper MSProject, recebendo como namespace &quot;localhostMSProject».
A última linha constitui a chamada do método denominado MSProject, oferecido por o &quot;servicoMSP», com seus respectivos parâmetros de entrada (url, nomeprojeto, username, passwd).
O resultado dessa execução fica armazenado na variável retorno. --
Passo 3: Tratamento do Retorno do Wrappers e Inserção das Métricas na DSA O conteúdo do retorno dos serviços deve ser interpretado, conforme a estrutura e as informações armazenadas no Metadado de Projeto, e enviado à DSA.
Em a solução codificada durante a execução desta pesquisa, adotou- se que os wrappers retornam as métricas por retorno do wrapper, e a inserção das métricas numa tabela temporária (DSA_ Produto), presente na DSA.
A Camada de Integração de Dados é responsável por a transformação e limpeza dos dados armazenados temporariamente na DSA.
Esta seção descreve os principais aspectos relacionados com:
A determinação das Rotinas de Limpeza e Transformação, e a estrutura e construção da DSA.
Com relação a o Metadado Organizacional, adotou- se nesta pesquisa a mesma solução proposta e documentada em.
Visando minimizar a intrusão do processo de Etc das métricas, utiliza- se uma área de temporária de dados, denominada DSA, composta por um conjunto de tabelas onde os dados brutos extraídos das fontes de origem devem ser armazenados e pré-processados por intermédio das Rotinas de Limpeza e Transformação (Seção 6.5.3.2), considerando o modelo analítico alvo descrito na Seção 6.3.
A solução proposta tem como fonte de dados as ferramentas MS Project, ClearQuest e planilhas MS Excel.
Porém entende- se que esses passos podem ser seguidos independentemente das fontes adotadas.
Estrutura da DSA A solução proposta e implementada para estruturar a DSA segue os passos sugeridos por e listados na Seção 2.4.3.3.
Os itens a seguir relatam as ações e os resultados obtidos a partir de a execução desses.
Passo 1: Determina a criação de um plano de alto nível, contendo as fontes de origem dos dados, as transformações requeridas e a carga nas tabelas alvo.
Como resultado obteve- se o plano ilustrado por a Figura 27, a partir de o qual foi possível obter uma visão geral do processo de Etc..
Passo2: Aconselha implementar uma solução dedicada quando a natureza do ambiente de desenvolvimento apresenta sistemas de dados personalizados, baseados em banco de dados proprietários.
Com base nas características do ambiente heterogêneo, optou- se por desenvolver uma DSA dedicada.
Para tanto, foi definida a estrutura das tabelas temporárias, incluindo seus atributos e domínios, considerando o modelo analítico proposto (Seção 6.3).
A mesma encontra- se no Apêndice C. Passo 3: Determina a elaboração de planos detalhados para cada uma das fontes de dados, contidas no plano de alto nível.
Como resultados da sua execução foram criados os planos ilustrados por as Figuras 28 e 29 que correspondem, respectivamente, à base de dados do MS Project e do ClearQuest.
O plano referente a a Planilha MS Excel também foi efetuado.
Em ambos os planos são apresentadas as tabelas que contêm os dados brutos, juntamente com a especificação, em alto nível, da consulta que deve ser realizada para extrair os dados que compõem primeiramente as tabelas temporárias (e.
g DSA_ Atividade e DSA_ Defeito) e, posteriormente, as fatos e as dimensões.
Esses planos também ilustram as limpezas e transformações inicialmente realizadas (e.
g ntextvarchar) para que os dados brutos possam ser devidamente armazenados no DW, conforme a estrutura proposta por o modelo analítico (Fato_ Atividade, Fato_ Versão e Fato_ Defeito e as dimensões).
De essa forma, foi possível obter uma visão inicial completa de todas as etapas que envolvem o processo de Etc dos dados.
MSP_ OUTLINE_ CODES:
9 colunas MSP_ WEB_ PROJECTS:
74 colunas MSP_ VIEW_ PROJ_ PROJECTS_ ENT:
334 colunas MSP_ VIEW_ PROJ_ PROJECTS_ STD:
12 colunas MSP_ VIEW_ PROJ_ TASKS_ CF:
263 colunas MSP_ VIEW_ PROJ_ TASKS_ STD:
182 colunas MS Project Server SQL Server Processamento das Dimensões Para informações das Dim_ Cliente, Dim_ Industria, Dim_ Porte_ Projeto, Dim_ Status, Dim_ Tecnologia, Dim_ Unidade_ Tamanho, Dim_ Versão e do DSA_ Produto extrair de MSP_ OUTLINE_ CODES &amp;.
OC , MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
ProjectEnterpriseCost&amp; MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
ProjectEnterpriseText&amp; MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
ProjectEnterpriseNumber&amp; quando (MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
ProjectEnterpriseOutlineC ode&amp; ID $= MSP_ OUTLINE_ CODES &amp;.
Code_ UID) e (MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
ProjectEnterpriseText3 $= 'nomeprojeto') e MSP_ VIEW_ PROJ_ PROJECTS_ ENT.
Após o Processamento das Dimensões atualizar os ids das tabelas temporárias DSA_ Produto e DSA_ Atividade.
Passo 4: Determina, por questões de facilidade, que as tabelas dimensão sejam populadas primeiro, e que suas chaves sejam atualizadas a cada nova linha incluída, sendo definida uma única chave de acesso por registro.
Para tanto, esta solução determina que sempre que um novo conjunto de métricas for extraído a rotina responsável por atualizar as dimensões deve ser executada (Rotina 2 -- Atualizar Dimensões, Seção 6.5.3.2).
IBM Clear Quest SQL Server Processamento das Dimensões É necessária a conversão de tipo de dado de Fase e Severidade Para informações do DSA_ Defeito de Teste Extrair Versão, Tipo de defeito, Categoria de defeito, Severidade e Fase de origem de Defect.
Dbid, Defect.
Versão, Defect.
Identificador, Defect.
Tipo, Defect.
Faseorigem, Defect.
Severidade e statedef.
Name quando Defect.
Versão $= 'versao' e Defect.
Dbid 0 e ou Defect.
State $= 0))) Para informações do DSA_ Defeito de Peer Review Extrair Versão, Tipo de defeito, Categoria de defeito, Severidade e Fase de origem de Defect.
Dbid, Defect.
Versão, Defect.
Identificador, Defect.
Tipo, Defect.
Faseorigem, Defect.
Severidade e statedef.
Name quando Desvios.
Status is NULL) and Desvios.
Id is not NULL) and Após o processamento das dimensões atualizar os Ids da tabela DSA_ Defeito.
Passo 5: Determina a realização dos testes para validar o conteúdo das dimensões e as atualizações das suas chaves.
Os mesmos foram realizados durante o desenvolvimento da solução e sua execução pode ser considerada pré-requisito para a realização dos testes relatados no Capítulo 7.
Passo 6: Determina métodos para atualizar valores das dimensões existentes no DW.
Por questões de conveniência e limitação de tempo esse passo não foi seguido, esta solução define que os atributos das dimensões não podem ter seu conteúdo atualizado.
Por essa razão, quando novos valores de dimensões forem necessários, os mesmos devem ser inseridos como novos registros.
Passos 7, 8 e 9: Estão relacionados com a alimentação, atualização e teste do conteúdo das tabelas fato.
Esses passos foram seguidos, a partir de a Rotina 3 ­ Atualiza Fatos, definida na Seção 6.5.3.2.
Passo 10: Propõe a automação da aplicação de staging.
Para tanto, esta solução definiu e implementou serviços SSIS contendo componentes especiais que efetuam as rotinas de Limpeza e Transformação apontadas por os passos 4 e 8, escritas em SQL e especificadas na Seção 6.5.3.2.
Neste passo, também foi definida e implementada a rotina de carga incremental, descrita na Seção 6.5.4 e um ambiente de cubo OLAP.
Esse último foi desenvolvido a partir de a ferramenta de Bi do MS Visual Studio, para testar a carga incremental e o conteúdo das tabelas que compõem o DW, com base nas informações temporariamente armazenadas na DSA.
A definição das Rotinas de Limpeza e Transformação adotadas por esta solução segue algumas das atividades de transformação propostas por, descritas na Seção 2.4.3.2, sendo elas:
Organizacional (e.
g padronização da severidade de defeitos);
Combinação das fontes de dados, verificando a integridade entre as chaves primárias (e.
g mapeando as chaves utilizadas na base de dados de origem das ferramentas para as chaves das dimensões);
As demais não foram consideradas, pois não existem ruídos e dados inúteis, já que somente dados requeridos são extraídos da origem.
A solução implementada, em SQL e disponibilizada por SSIS, apresenta as seguintes rotinas:
1) Inicializar;
2) Atualizar Dimensões;
E 3) Atualizar Fatos.
A primeira alimenta a Dim_ Tipo_ Fato com os seguintes valores:
Original, revisado e real.
A segunda realiza a leitura das tabelas temporárias e verifica se os seus valores existem nas tabelas dimensões, caso essa condição não seja satisfeita, efetua a sua inclusão.
A terceira realiza a leitura de todas as tabelas temporárias e, a partir de agrupamentos e condições, gera as tabelas fato, juntamente com as chaves estrangeiras das dimensões que caracterizam o fato.
Para oferecer suporte adequado à monitoração e manter um Repositório de Dados atualizado e consistente, comportando cargas freqüentes, esta solução propõe um procedimento de carga incremental, conforme os conceitos e métodos definidos por, descritos na Seção 2.4.3.3.
A partir de o mesmo, é possível manter o histórico e o acompanhamento regular do andamento do projeto.
Para tanto, a carga incremental considera a validade dos fatos, segundo a definição apresentada na Seção 6.3.3.
Com base nessas definições e conceitos relacionados com a validade dos fatos, foram criadas três regras que regem a carga incremental que têm, como objetivo, controlar inclusões, atualizações e término da validade dos dados.
Regra 1: Quando um novo fato apresentar chave primária diferente da previamente armazenada no DW, o mesmo deve ser inserido recebendo como Di a data da carga e DF o valor nulo.
Regra 2: Quando um fato atualizado, caracterizado por apresentar chave primária igual à previamente armazenada no DW, com exceção do valor que corresponde à data de carga, e pelo menos uma das suas métricas com valor diferente, o mesmo deve ser inserido no DW, recebendo como Di a data da carga e DF o valor nulo.
Regra 3: Os fatos semelhantes (mesma chave primária, com exceção do identificador de data de validade, e contendo pelo menos uma métrica diferente e com data de validade final (DF) nula) aos inseridos por a regra 2 devem ter seu período de validade atualizado.
Para tanto, o seu valor de DF recebe a data corrente e o seu Di permanece inalterado.
Durante a execução desta pesquisa, foi implementado um procedimento de carga, contendo as três regras citadas anteriormente, escritas em SQL¸ e disponibilizado na forma de um Ws.
Por questões de conveniência, essa carga é chamada por um serviço SSIS do MS SQL Server.
Assim, depois que os dados encontram- se devidamente estruturados e consolidados na DSA esse procedimento é executado tendo como resultado os dados corretamente armazenados no DW.
Esta solução determina que a carga no DW seja realizada por intermédio de serviços, ilustrados na segunda camada da arquitetura do SPDW+.
De entre os benefícios dessa forma de realização, pode- se facilmente citar:
Substituir o ambiente da base de dados que armazena o Repositório de Dados;
Utilizar os mesmos serviços de carga em ambientes semelhantes;
E inserir novas métricas no procedimento, a partir de a publicação de serviços.
A solução descrita neste capítulo, denominada SPDW+, propõe um processo automatizado de Etc de métricas de software a serem armazenadas num Repositório de Dados (DW), na forma de um ambiente de data warehousing.
As células da Tabela 15 contêm uma tabulação dos domínios dos aspectos envolvidos por a automação computacional, levantados na Seção 3.1, considerados necessários para mensurar a qualidade de software.
A solução proposta trata tanto o nível organizacional, como o de projeto, considerando atividades de análise e monitoração.
Para facilitar o entendimento e sistematizar a explicação, os próximos itens descrevem como a solução proposta para tratar os aspectos da plataforma computacional, resolve o problema.
Por questões de simplificação esses itens endereçam o nível organizacional que, quando atendido, também resolve as questões relacionadas ao nível de projeto.
Latência e Freqüência:
Para tratar esses dois aspectos a solução apresenta um processo automatizado de Etc das métricas orientado a serviços.
O mesmo permite que a coleta das métricas seja realizada por demanda ou por iniciativa, tão logo novos valores forem lançados, por os recursos, nas ferramentas usadas no projeto para acompanhamento, ou conforme as políticas de coleta estabelecidas por a organização.
Assim, é possível ter uma freqüência e uma latência com ordem de grandeza de horas, considerada essencial à monitoração.
Heterogeneidade de Modelos de Processos: Para endereçar de maneira adequada esse aspecto, a solução propõe um modelo analítico abrangente e flexível, que permite que os diferentes projetos da organização adotem modelos de processos distintos (e.
g cascata, cascata com sobreposição, iterativo, evolutivo e unificado).
A estrutura das tabelas fatos e dimensões apresentam métricas aditivas e semi-aditivas que possibilitam diferentes perspectivas de análise e níveis de sumarização, conforme as etapas dos ciclos de vida adotados por os modelos de processo.
Heterogeneidade de Ferramentas:
Para resolver esse aspecto a solução propõe a utilização de wrappers de extração, que escondem as especificidades das ferramentas e Metadados de Projeto, que são responsáveis por o mapeamento entre cada dado requerido e o correspondente dado bruto.
De essa forma, as métricas podem ser extraídas de diferentes fontes, necessitando apenas da publicação dos respectivos serviços de extração.
Além disso, a solução permite que novas ferramentas ou versões das mesmas possam ser inseridas no ambiente, somente com a publicação de novos serviços.
Novos projetos também podem se beneficiar desses wrappers, pois os wrappers podem ser adotados por projetos distintos, desde que esses adotem as mesmas ferramentas, para as mesmas métricas.
Heterogeneidade de Projetos:
A solução proposta trata adequadamente heterogeneidade de projetos, permitindo que os mesmos sejam de pequeno ou grande porte.
Para tanto, são utilizados wrappers, que permitem tanto a extração de ferramentas de apoio à gestão do ambiente de desenvolvimento de software, como de planilhas MS Excel.
Heterogeneidade de Nível de Isolamento: A solução resolve apenas para o nível de isolamento compartilhado, não tratando redes isoladas que, necessariamente, dependem da intervenção humana para serem suportadas.
Intrusão: A solução propõe um processo automatizado de Etc das métricas, que pode ser iniciado sob demanda ou por iniciativa, baseado em serviços.
De essa forma, é possível coletar as métricas e disponibilizar seus resultados sem que seja necessário haver envolvimento dos recursos do projeto, caracterizando o processo como isento de intrusão.
Com base nos aspectos tratados por a solução, pode- se afirmar que o SPDW+ oferece suporte:
A análise e monitoração da mensuração da qualidade de software, nos dois níveis, de projeto e organizacional, considerando todas as heterogeneidades, com exceção do nível de isolamento, e apresenta um nível isento de intrusão.
Além disso, pode- se afirmar que, devido a a flexibilidade e abrangência do modelo analítico, a aplicação do mesmo é possível em organizações de desenvolvimento de software, que apresentem estrutura de projeto semelhante, exigindo, apenas, a realização de pequenas modificações.
A partir de a comparação entre esta solução e a original (SPDW), é possível perceber que a primeira propõe um modelo analítico mais abrangente e mais elegante, com suporte à análise e monitoração, conseqüente de um Programa de Métricas estendido, segundo a técnica EVA, e de uma estrutura de projetos estendida.
Além disso, esta solução possibilita a captura de métricas no grão de atividade, aumentando as perspectivas de análise, garantindo um acompanhamento regular deste nível.
Isso tudo, com um número menor de tabelas, contendo apenas métricas aditivas e semi-aditivas, suportando todos os níveis de sumarização.
Esta solução também apresenta definições mais detalhadas dos wrappers e das rotinas (extração e limpeza e transformação).
Ainda, cabe salientar que esta pesquisa implementou todos os componentes relatados neste capítulo, apresentando uma solução completa, desde a obtenção das métricas até a disponibilização dos seus resultados, a partir de uma interface de cubo Este capítulo relata o conjunto de testes realizados sobre a solução proposta no computacionais, configurados para criar o ambiente de teste em laboratório;
As fontes de dados utilizadas;
A estrutura do cubo OLAP construído, utilizado como interface de teste;
Os testes têm como objetivo principal avaliar se o processo automatizado de Etc proposto por o SPDW+ consegue tratar adequadamente, e em tempo compatível, os aspectos que envolvem a mensuração da qualidade de software, segundo a Tabela 15.
Tratar adequadamente significa propiciar pelo menos as funcionalidades oferecidas para análise propostas por, e mais as facilidades para monitoração.
Em tempo compatível significa que a latência dos dados e a freqüência de coleta devem ser, no máximo, da ordem de horas.
Em esse sentido, foi criado um ambiente de testes com a implementação do modelo analítico proposto (Seção 6.3) e dos componentes das camadas de Integração de Dados e de Aplicações:
Wrappers para MS Project, ClearQuest e planilhas MS Excel, Rotina de Extração, Rotinas de Limpeza e Transformação e DSA.
O ambiente de teste é de baixa intrusão em sua concepção, por necessitar de intervenção humana apenas para ativar alguns passos do processo.
Esse ambiente de testes apresenta, como única característica heterogênea, o uso de distintas ferramentas de armazenamento de métricas.
O mesmo não trata heterogeneidade de modelos de processo de software, por já ter sido tratada em, de tipos de projetos e de níveis de isolamento.
Os testes realizados utilizam cronogramas consistentes com projetos reais de grande porte, como forma de testar tipos de projetos, e foram efetivados num único computador (portanto, sem isolamento).
Dois tipos de teste foram efetuados, com:
Versões encerradas (análise) e (2) versões em andamento (monitoração).
Em o primeiro tipo, foram criados cinco cronogramas com versões encerradas.
Em o segundo, para emular um acompanhamento regular das atividades de uma única versão, ao longo de três dias, foram construídos três cronogramas dessa versão, com pequenas alterações diárias.
Os testes consistem, basicamente, na execução do processo de Etc das métricas, a atualização do modelo analítico, a geração do cubo de dados, e a verificação dos valores das métricas com o auxílio de tabelas pivotantes.
A execução desse processo de Etc é composta por os seguintes passos, a partir de a publicação de um cronograma:
Execução da Rotina de Extração (Seção 6.5.2), escolhendo o projeto e a versão desejada;
Execução do SSIS que contém as Rotinas de Limpeza e Transformação (Seção 6.5.3.2);
chamada do Ws responsável por a carga incremental no modelo analítico (Seção 6.5.4);
E (iv) atualização dos valores do cubo OLAP.
Após a execução dos passos que compõem o processo de Etc, foi realizada uma série de consultas no cubo, como auxílio de tabelas pivotantes, para verificar se o SPDW+ atende os seguintes objetivos específicos:
1) oferece suporte à análise e à monitoração da mensuração da qualidade de software, considerando:
Perspectivas de análise e níveis de sumarização distintos;
2) permite a monitoração segundo as métricas de EVA incorporadas no programa de métricas;
3) apresenta um processo automatizado Etc, a partir:
De as rotinas de Extração e Limpeza e Transformação, dos wrappers e da carga incremental.
Para a realização dos testes foi construído um ambiente de laboratório, nas dependências do GPIN, onde foram instalados e, em alguns casos configurados, os seguintes recursos computacionais:
Iis 6.0: Necessário para publicar os serviços (wrappers e a carga incremental) e permitir que os mesmos possam ser acessados por o browser ou chamados por os componentes especiais para Ws existentes nos serviços SSIS, do SQL Server 2005.
MS Project 2003: Utilizado para construir cronogramas contendo as métricas diretas das áreas de qualidade:
Tempo, Esforço, Custo, Tamanho e Qualidade (Satisfação do Cliente), que compõem o Programa de Métricas proposto por esta solução MS Project Server:
Instalado e configurado para permitir que as informações dos cronogramas, possam ser armazenadas numa base de dados estruturada, denominada EPM, localizada no SQL Server.
Esse aplicativo apresenta uma interface web para gerenciar projetos, a partir de a qual é possível publicar os cronogramas e salvar seu conteúdo no EPM.
O Project Server também permite a configuração de campos especiais (Enterprise Custom Fields), responsáveis por habilitar a personalização do cronograma conforme as necessidades da organização.
Em esta solução foram adicionados campos desse tipo para manter informações das áreas de qualidade Tamanho e Custo, além de dados referentes ao modelo de processo adotado por o projeto (e.
gversão, fase, iteração).
Durante a instalação desse aplicativo foi necessário realizar uma série de configurações e executar alguns scripts adicionais para que o Project Server conseguisse acessar corretamente a nova versão do SQL Server 2005.
MS Visual Studio 2005: Instalado e empregado para codificar os wrappers em J&amp; e a Rotina de Extração em C&amp;, que apresenta bibliotecas especiais para:
Tratar documentos XML(` System.
Xml'), realizar conexões com bases de dados(` System.
Data. OleDb') e construir Ws(` System.
Web'). MS SQL Server 2005 Professional:
Instalado e utilizado como sistema gerenciador de banco de dados, responsável por armazenar as soluções propostas para o Repositório de Dados (DW), a DSA, bem como as bases de dados do EPM e do ClearQuest.
MS Server Business Intelligence Development Studio:
Aplicado na construção do cubo OLAP e dos serviços SSIS que contêm as Rotinas de Limpeza e Transformação.
Este aplicativo também é responsável por o cálculo das métricas derivadas, a partir de scripts MDX (Multidimensional Expressions), e por a visualização do conteúdo do cubo por intermédio de tabelas pivotantes.
Os dados utilizados nos testes são fictícios, mas representam situações bem próximas das reais, e têm por base a estrutura e as ferramentas adotadas no cenário real, relatado no Planilhas MS Excel;
Base de dados do ClearQuest;
E base de dados do EPM.
As planilhas MS Excel são constituídas por métricas diretas da área de qualidade Requisitos (NMA, NMR, NRM e NRE), nome do projeto e nome da versão, conforme a estrutura ilustrada por a Figura 30.
A base de dados do ClearQuest armazena nas suas tabelas, entre outras informações, os seguintes dados:
Nome projeto, nome da versão, fase de origem do defeito, grau de severidade (A ­ alto, M ­ médio e B -- baixo), tipo do defeito (interno e externo) e quantidade.
A base de dados do EPM mantém métricas das seguintes áreas de qualidade:
Tempo, Esforço, Custo, Tamanho e Qualidade (somente o ISC), obtidas a partir de a publicação de cronogramas.
Esses últimos são estruturados conforme a hierarquia da Figura 32, proposta originalmente por.
De essa forma, é possível planificar os modelos de processo suportado por a solução (cascata, iterativo, evolutivo e unificado), descritos na Seção 2.1.1, a partir de seis níveis distintos.
Para testar a solução foram criados e publicados oito cronogramas.
Os cinco primeiros correspondem a versões encerradas, contendo todas as suas atividades concluídas, e que, por essa razão, não podem ser monitoradas.
Os restantes simulam uma única versão em andamento, durante três datas distintas, com atividades:
Não iniciadas, em andamento e encerradas.
A Figura 33 mostra, numa forma reduzida, o cronograma da versão 06.00.00, onde são listadas as colunas do baseline original.
Vale ressaltar que as mesmas também existem para dados reais e do baseline revisado, mas foram suprimidas na figura.
As linhas do cronograma representam os níveis de versão, fase, tipo de atividade e atividade.
Os dados mostrados são relativos ao terceiro dia de acompanhamento do cronograma, das atividades do tipo Trabalho, pertencentes a fase de Design, da versão Esforço e Custo, que compõem o Programa de Métricas.
Cabe salientar que a utilização de cronograma, e de uma ferramenta dedicada para a sua construção, espelha o ambiente de um projeto de grande porte.
Além de a estrutura de cronograma padrão usada, foram criados, no Project, dados personalizados do projeto (Enterprise Custom Fields), a partir de uma tela especial (Figura 34).
Foram definidas as métricas da área de qualidade Tamanho e Qualidade (ISC), além de informações adicionais relacionadas com as Dimensão de Projeto e com o nível de iteração.
Para a realização dos testes foi criado um modelo OLAP, com o auxílio da ferramenta de Bi do SQL Server.
O mesmo é composto por as tabelas fatos e as dimensões propostas no modelo analítico (Seção 6.3), juntamente com a inclusão da Dim_ Validade, que determina se os valores do fato são válidos no presente ou não (Seção 6.3.3).
Por padrão, essa ferramenta determina que as dimensões Dim_ Tempo_ Final e Dim_ Tempo_ Inicial, que armazenam estampas de tempo, também devam ser consideradas tabelas fato.
A Figura 35 ilustra:
A) as tabelas fato e as suas métricas diretas aditivas e semiaditivas, b) as dimensões do cubo, e c) as métricas derivadas.
A Dim_ Atividade foi expandida para mostrar seus atributos e exemplificar o modo de visualização proporcionado por a ferramenta.
As métricas derivadas são calculadas por intermédio de quinze scripts MDX, executados durante a geração do cubo.
A Figura 36 ilustra o script desenvolvido para realizar o cálculo da métrica derivada VCB, onde primeiramente é testada uma condição para verificar se o tipo do fato é Original.
Caso a condição seja verdadeira, o valor é determinado conforme as equações presentes no script.
Caso contrário, o script testa mais uma condição, onde verifica se o tipo do fato é Revisado, seguindo o mesmo procedimento anterior.
Se o fato não atender nenhuma das duas condições, a métricas VCB recebe valor nulo (null).
Para testar a mensuração da qualidade de software das versões encerradas (análise), foram realizadas consultas nas três tabelas fato principais:
Fato_ Atividade, Fato_ Versão e Fato_ Defeito.
Os próximos itens apresentam os resultados obtidos em cada uma de elas. --
Fato_ Atividade:
A Figura 37 apresenta os dados da solução segundo as seguintes perspectivas de análise:
Versão, tipo do fato, fase, tipo da atividade e nome da atividade.
Por motivos de legibilidade a dimensão iteração foi suprimida da consulta.
Para cada uma dessas perspectivas também são apresentados totais que mostram os diferentes níveis de sumarização.
Em o exemplo ilustrado, são disponibilizadas métricas das áreas de qualidade:
Tempo, Esforço e Custo.
Cabe salientar que as equações das métricas derivadas VCB e VB consideram o horário das datas (Data Inicial e Final), e que essas últimas apresentam o formato mês /dia/ano.
É possível concluir que a solução apresenta as métricas conforme a hierarquia de cronograma apresentada na Figura 32, considerando as diferentes perspectivas de análise e os níveis de sumarização.
A Figura 38 apresenta métricas de Esforço e Custo para todas as versões encerradas, considerando os três tipos de fatos, reforçando o suporte a sumarizações em diferentes níveis.
As métricas de VCB e VB são calculadas para fatos originais e revisados, em função de os valores reais.
De essa forma, os fatos reais não possuem valores para essas métricas.
Percebe- se que a solução totaliza Esforço e Custo, para cada uma das versões.
Além disso, é importante salientar que o% Tc, considerado primordial à monitoração, apresenta um somatório correto.
Por ser tratar de um valor percentual, o% Tc não pode ser obtido a partir de o simples somatório entre os níveis, exigindo que o seu valor seja calculado em função de o seu nível e do número de níveis inferiores.
Por exemplo, uma fase formada por duas atividades A e B, onde cada uma represente 50% do esforço, se a atividade A apresentar% Tc igual a 100 e B igual a 50, o valor correto é de% Tc é 75. --
Fato_ Versão:
A Figura 39 ilustra os resultados obtidos a partir de a consulta da tabela Fato_ Versão e Fato_ Atividade, onde são apresentados valores das áreas de qualidade Tamanho e Requisito.
Esta solução relaciona métricas entre diferentes tabelas fato.
Isso pode ser comprovado por a presença da métrica de custo, pertencente à tabela Fato_ Atividade, na consulta da Fato_ Versão.
Além disso, também é possível notar que a solução apresenta valores consistentes para esse cruzamento, já que os custos possuem os mesmos resultados ilustrados na Figura 39.
Os valores mostrados na Figura 39 apresentam tanto métricas diretas (NRA, NRE, NRM e NRO), como derivadas (VR). --
Fato_ Defeito:
A Figura 40 contém valores das métricas de defeitos (Defeitos e ERD), filtradas para todas as versões, da fase de Design, de um determinado projeto classificado como de grande porte.
Assim, é possível concluir que a solução permite realizar consultas de defeitos segundos condições pré-definidas.
Para testar os aspectos de latência de dados e de freqüência da coleta dos mesmos, foram feitas as cargas dos cronogramas em andamento, de maneira seqüencial, minimizando o intervalo de tempo entre as mesmas.
A Tabela 16 mostra o registro das datas e horas iniciais e finais, de cada passo realizado para a carga dos cronogramas, nos testes feitos em laboratório.
Em ela pode ser constatado que uma carga completa de um cronograma, com todo o processo de Etc automatizado, leva em torno de 4,6 minutos.
Logo, pode- se concluir que o processo de Etc proposto atende a freqüência de coleta na ordem de horas e, por conseqüência, a latência de mesma ordem de grandeza temporal.
Para testar a mensuração da qualidade de software das versões em andamento (monitoração), segundo a técnica de EVA, foram realizadas consultas na tabela Fato_ Atividade, em diferentes datas de carga.
Os próximos itens apresentam os resultados obtidos em cada uma de elas. --
Primeira Carga -- 04/06/2007 A Figura 41 apresenta os valores da primeira carga com os dados do cronograma da versão 06.00.00, onde são mostrados somente valores originais.
Em o momento da carga, a versão ainda não tinha entrado em execução, tendo valor zero em todos os valores de% Tc e de esforço das suas atividades.
Por motivos de legibilidade, as dimensões Validade e Versão foram suprimidas dessa figura, bem como os valores totais da versão em andamento. --
Segunda Carga -- 05/06/2007 A Figura 42 lista os resultados da segunda carga, em dois blocos:
O superior refere- se a fatos do baseline original;
O inferior, a fatos reais.
As atividades da fase de Design do tipo Trabalho (bloco superior) foram iniciadas, apresentando 44,30% para% Tc.
A origem desses valores está na Figura 33.
Para cada uma dessas atividades a solução calcula as métricas de EVA.
Como as atividades do Tipo Revisão ainda não foram iniciadas, seus valores para monitoração não foram calculados.
Por sua vez os fatos do tipo Real (bloco inferior) também não apresentam valores, mas são utilizados para o cálculo das métricas EVA.
Além de computar os valores de EVA para as tarefas de maneira individualmente, a solução também propaga esse comportamento para os demais níveis de hierarquia.
Isso pode ser percebido por o Total dessas métricas, apresentados para Tipo de Atividade (Trabalho), Fase (Design) e Tipo do Fato (Original).
Os indicadores de qualidade, para as métricas VB e VCB, utilizados por a operação de software (Capítulo 5), são interpretados conforme a Tabela 17.
Esses indicadores, apesar de destinarem- se apenas à análise, têm sido usados para monitoração.
A partir de a avaliação dos valores dessas métricas mostrados na Figura 42, segundo esses indicadores, constata- se que as atividades estão sendo realizadas dentro de o prazo mas acima de o custo esperado.
Por exemplo, para a atividade Des-OS2, VCB $= 42,86% e o IDC $= 0,88.
Logo, por a técnica de EVA a atividade está acima de o custo, apesar de o indicador de qualidade acusar Aceitável para VCB.
A Figura 43 ilustra os valores coletados no terceiro dia de carga.
A partir de os mesmos é possível perceber que as atividades de Trabalho da fase de Design, juntamente com a atividade Des-01 do tipo Revisão, já foram concluídas, e por essa razão não possuem métricas de monitoração.
Porém, a atividade de Revisão Des-02 encontra- se em andamento e a Des-03 ainda não começou.
A solução apresenta valores de monitoração totais para a fase de Design, para valores do baseline original e real, e para a versão 06.00.00 (última linha da figura).
Com base nos valores apresentados por as Figuras 41, 42 e 43 é possível verificar os resultados obtidos com a execução da carga incremental, segundo as três regras propostas na Seção 6.5.4.
A condição da Regra 1 foi verificada como verdadeira, somente, durante a carga do dia 03/06/2007, pois todos os valores do cronograma estavam sendo inseridos pela primeira vez no DW.
Já na carga do dia 04/06/2007, apenas as condições das Regras 2 e 3 foram consideradas verdadeiras, devido a a presença de novos valores para todas as atividades de trabalho, da fase de Design, já inseridas previamente no DW.
De essa forma, o período de validade dos fatos anteriormente armazenados tiveram que ser encerrados, tomando com data final a data de carga, como pode ser visto na consulta feita na tabela Fato_ Atividade, selecionando apenas a Atividade Des-OS1, do tipo Trabalho e da fase de Design, mostrada na Figura 44.
A partir de os resultados obtidos, por intermédio da exploração do cubo OLAP gerado para testar a solução, foi possível concluir que:
1) o modelo analítico proposto oferece suporte adequado à análise e à monitoração da mensuração da qualidade de software, permitindo diferentes perspectivas de análise e níveis de sumarização;
2) a solução apresenta de forma consistente as métricas de EVA, incorporadas no Programa de Métricas;
3) o processo de Etc das métricas a serem armazenadas no Repositório de Dados é automatizado, possibilitando cargas freqüentes, com latência na ordem de horas e baixa intrusão, desde que seja considerada a sua automação, conforme os componentes apresentados na solução.
É importante mencionar, que a construção do cubo OLAP, descrito na Seção 7.4, foi fundamental para a geração dos resultados e exploração das métricas.
A utilização do mesmo permitiu concluir que a proposta do SPDW+ atende os pré-requisitos da solução original, de uma maneira mais elegante e abrangente.
Este trabalho propõe um processo automatizado de Etc de métricas da qualidade de software, a serem armazenados num Repositório de Dados.
Para tanto, define um instrumento de referência, ilustrado por a Tabela 1, que apresenta a mensuração da qualidade a partir de três dimensões ortogonais:
1) nível de mensuração, 2) atividades de supervisão, e 3) plataforma computacional.
A partir de essas dimensões, é realizada uma discussão sobre o cenário de desenvolvimento de software, caracterizando os principais aspectos que envolvem a definição de uma arquitetura computacional para automação, parcial ou total, desse cenário.
Com base nessas dimensões e nos aspectos definidos por o instrumento, foi efetuada uma pesquisa bibliográfica para encontrar trabalhos relacionados com este tema de pesquisa.
O resultado da mesma é apresentado no Capítulo 4, a partir de o qual pôde- se perceber que os trabalhos relatados atendem parcialmente os aspectos definidos no instrumento de referência.
Para caracterizar um cenário real de aplicação desta pesquisa, foi realizado um estudo de caso, relatado no Capítulo 5, onde é apresentado o ambiente de desenvolvimento de software de uma organização de Ti, considerada de grande porte na América Latina e certificada CMM3.
Em esse capítulo foi descrito o seu processo de Etc, destacando as suas facilidades e dificuldades.
A solução proposta denomina- se SPDW+ e consiste num ambiente de data warehousing, que oferece suporte à análise e à monitoração da mensuração da qualidade de software.
A mesma contempla um modelo analítico elegante e abrangente, que permite a monitoração segundo a técnica EVA, a partir no nível de atividade.
Além disso, o SPDW+ define um processo automatizado de Etc das métricas que trata a carga incremental, a alta freqüência e a baixa latência e intrusão da coleta.
Para isso, a solução é consistente com a arquitetura orientada a serviços.
Foram especificados os seus principais componentes:
Rotina de Extração, wrappers, Rotinas de Limpeza e Transformação, Metadados de Projeto, DSA, serviço de carga incremental e Repositório de Dados (DW).
Para avaliar esta solução foi efetuado um conjunto de testes, descrito no Capítulo 7, seguindo um número pré-definido de passos e a partir de a exploração do conteúdo das métricas por intermédio de tabelas pivotantes.
De essa maneira, foi possível verificar:
1) a adequação do modelo analítico para análise e monitoração, desde o nível de atividades, 2) a efetividade das métricas EVA na monitoração de métricas, e 3) a eficácia do processo automatizado de Etc..
Essa solução é um importante avanço em relação a trabalhos anteriores por apresentar um modelo analítico elegante e abrangente, e especificar um ambiente com suporte à monitoração de métricas de software.
A continuidade da pesquisa vai, justamente, em propor uma solução estritamente consistente com o padrão arquitetural orientado a serviços para o ambiente de data warehousing proposto em.
Em especial, pretende-se:
Adicionar um módulo para previsão de métricas, baseado em técnicas de mineração, na arquitetura do SPDW+.
Permitir a evolução do Programa de Métricas e dos processos organizacionais (OSSP e PDP), a partir de esquemas, conforme o trabalho proposto por.
Investigar outros modelos de desenvolvimento de software para verificar a adequação desta solução.
Pesquisar quais os aspectos que merecem ser melhorados para que a arquitetura do SPDW+ possa considerada de fato uma A os.
