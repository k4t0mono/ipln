Organizações atuantes nos mais diferentes mercados, têm utilizado os benefícios oferecidos por a utilização de técnicas de Data Mining ­ DM como atividades complementares a seus sistemas de apoio a decisão estratégica.
Porém, para a grande maioria das organizações, a implantação de um projeto de DM acaba sendo inviabilizada em função de diferentes fatores como:
Duração do projeto, custos elevados e principalmente por a incerteza quanto a a obtenção de resultados que possam auxiliar de fato a organização a melhorar seus processos de negócio.
Em este contexto, este trabalho apresenta um processo, baseado no processo de Knowledge Discovery in Database ­ KDD, que visa identificar oportunidades para aplicação de técnicas de DM através da indução e ranqueamento de árvores de decisão geradas por a exploração semiautomática de modelos On-Line Analytical Processing -- OLAP.
O processo construído utiliza informações armazenadas num modelo OLAP preparado com base nas informações utilizadas por sistemas de Customer Relationship Management -- CRM e Business Intelligence ­ Bi, tipicamente utilizados por organizações no apoio a tomada de decisão estratégica.
Em este trabalho é apresentada uma série de experimentos, gerados de forma semiautomática, utilizando técnicas de DM, cujos resultados são coletados e armazenados para posterior avaliação e ranqueamento.
O processo foi construído e testado com um conjunto significativo de experimentos e posteriormente avaliado por especialistas de negócio numa instituição financeira de grande porte onde esta pesquisa foi desenvolvida.
Palavras chave:
Descoberta de Conhecimento em Banco de Dados, Mineração de Dados, OLAP, Gestão de Relacionamento com Cliente, Inteligência de Negócios.
A utilização de Sistemas de Apoio a Decisão ­ SAD é uma prática comum em organizações de grande porte.
De entre estes sistemas destacamos o uso dos Sistemas de Customer Relationship Management -- CRM e Business Intelligence ­ Bi por parte de instituições financeiras para obtenção de vantagens num mercado cada vez mais competitivo.
Sistemas de CRM, em especial, são utilizados por as organizações na gestão de relacionamento com seus clientes permitindo uma maior assertividade na oferta de produtos e serviços.
Utilizados também como fonte de informação para sistemas de CRM e Bi, os modelos On-Line Analytical Processing -- OLAP são amplamente utilizados no apoio a tomada de decisão estratégica nas organizações.
Em a busca por informações que possam gerar diferenciais competitivos, organizações atuantes nos mais diferentes mercados têm utilizado técnicas de Data Mining ­ DM, que é parte integrante de um processo mais abrangente como complementares aos tradicionais SADs.
Entretanto, investimentos em projetos de KDD ainda não são amplamente realizados principalmente em função de fatores como:
Custos elevados, longa duração do projeto e principalmente por a incerteza quanto a a obtenção de resultados.
Em este contexto, este trabalho apresenta um processo, baseado no processo de KDD, que visa identificar oportunidades para aplicação de técnicas de DM através da indução e ranqueamento de árvores de decisão geradas por a exploração semiautomática de modelos On-Line Analytical Processing ­ OLAP.
O processo construído é executado com o apoio de um ferramental construído para automatizar a execução dos experimentos com algoritmos de mineração de dados sobre uma base de dados modelada e populada com informações utilizadas por sistemas de CRM e Bi da organização.
O processo foi construído, testado com um conjunto significativo de experimentos e avaliado por especialistas de negócio numa instituição financeira de grande porte onde esta pesquisa foi desenvolvida.
Para uma melhor compreensão, este trabalho está organizado da seguinte forma:
Em o Capítulo 2, apresentamos uma revisão sobre o referencial teórico onde abordamos questões relacionadas aos dados, suas formas de apresentação e tratamento.
Abordamos ainda o armazenamento de dados em grandes repositórios bem como o processo de KDD.
Em o Capítulo 3, detalhamos o cenário onde esta pesquisa foi desenvolvida, seus objetivos e características, bem como o processo de exploração semiautomática proposto.
Em o Capítulo 4, apresentamos o processo de KDD realizado, o ferramental desenvolvido, os experimentos executados e as avaliações dos especialistas de negócio.
Em o Por fim, no Capítulo 6, apresentamos as conclusões e os trabalhos futuros e, em seguida, as referências bibliográficas.
Em este capítulo é realizada uma revisão bibliográfica dividida em duas partes.
Em a primeira, apresentamos questões referentes aos dados, formas de representação, tratamento e armazenamento.
Abordamos uma técnica de armazenamento de dados conhecida como Data Warehouse ­ DW, Em a segunda parte o processo de KDD é apresentado, são mostradas as principais etapas deste processo e destacadas quatro das principais técnicas de mineração de dados.
Dados Os conjuntos de dados diferem de diversas formas.
Por exemplo, os atributos usados para descrever objetos podem ser de diferentes tipos ­ quantitativos ou qualitativos ­ e os conjuntos de dados podem ter características especiais como séries temporais ou objetos com relacionamentos explícitos entre si.
É comum que o tipo de dado determine quais ferramentas e técnicas devam ser usadas no processo de análise de dados.
No tocante a qualidade de dados, cabe observar que a maioria das técnicas de mineração de dados tolera algum nível de imperfeição nos dados.
Entretanto, operações que visam melhorar a qualidade dos dados colaboram para análises mais qualificadas.
Em as próximas seções são descritos alguns dos principais tipos de dados bem como destacadas questões relacionadas à qualidade destes.
Um conjunto de dados pode ser visto como uma coleção de objetos de dados, registros, entidades, etc..
Objetos de dados são representados por um determinado número de atributos que descrevem as características básicas como tamanho, altura, idade, peso, etc..
Um atributo é uma propriedade ou característica de um objeto que pode variar de um objeto para outro ou em função de o tempo.
Dois exemplos clássicos de atributos são a cor dos olhos que varia de pessoa para pessoa e o peso que, além de variar de pessoa para pessoa, pode variar em função de o tempo para uma mesma pessoa.
Estes atributos podem representar informações de diferentes tipos e podem ser classificados como ordinal, nominal, intervalo e proporção.
As operações que podem ser utilizadas sobre estes atributos são:
A Tabela 1 descreve estes quatro tipos de atributos bem como as operações que podem ser realizadas com estes.
Uma forma independente de distinguir atributos é por o número de valores que eles podem receber tais como:
Discretos -- um atributo discreto possui um conjunto de valores finito.
São exemplos de atributos discretos o CEP, matrícula, Id de um produto, etc..
Contínuos -- um atributo contínuo é representado por um número real.
São exemplos de atributos contínuos a temperatura, altura, peso, etc..
Existem muitos tipos de conjuntos de dados que são utilizados no processo de mineração nas mais variadas áreas.
Porém, no escopo desta pesquisa, destacamos apenas o tipo de dados de registros e suas características gerais de dimensão, dispersão e resolução.
Grande parte do trabalho de mineração de dados supõe que o conjunto de dados seja uma coleção de registros, cada registro formado por um conjunto fixo de atributos, como ilustrado na Figura 1 (a).
Em a forma mais básica de um dado em registro, não há relacionamento explícito entre registros ou atributos.
Dados em registro são geralmente armazenados em arquivos ou bancos de dados relacionais.
A Figura 1 ilustra diferentes tipos de dados em registros que são detalhados na sequência.
Fonte: Dados de transação ­ dados de transação são um tipo especial de dados em registros, onde cada registro (transação) envolve um conjunto de itens.
Um exemplo adequado para representar este tipo de dado é um cupom fiscal de compras realizadas num estabelecimento comercial.
Este tipo de dado está representado na Figura 1 (b).
Matriz de dados ­ se os objetos de dados numa coleção possuem mesmo conjunto de atributos numéricos, então estes dados podem ser representados por uma matriz m por n, onde existem m linhas, uma para cada objeto, e n colunas, uma para cada atributo.
Esta matriz é chamada de matriz de dados que, por permitir operações de matrizes aplicadas nas transformações de dados, são utilizadas como formato padrão de dados para a maioria dos dados estatísticos.
Este tipo de dado está representado na Figura 1 (c).
Matriz de dados dispersos ­ uma matriz de dados dispersos é um caso especial de matriz de dados em o qual os atributos possuem o mesmo tipo e são assimétricos, onde somente os valores diferentes de zero são significativos.
Este tipo de matriz é muito utilizado em mineração de textos onde as linhas da matriz representam os documentos e as colunas representam as palavras com número de ocorrências das mesmas em cada documento.
Este tipo de dado está representado na Figura 1 (d).
Algoritmos de mineração de dados são muitas vezes aplicados a dados que foram coletados para outros propósitos.
Por este motivo, a mineração de dados envolve etapas de detecção e correção de problemas relacionados à qualidade dos dados além de permitir a utilização de algoritmos que toleram dados com baixa qualidade.
A etapa de detecção e correção é conhecida como limpeza dos dados.
Em este tópico abordamos questões referentes à qualidade de dados que podem ser identificadas já na coleta das informações.
Também destacamos problemas que podem ser encontrados em dados previamente armazenados.
O termo erro de medição refere- se a qualquer problema resultante do processo de medição, como no caso onde um valor registrado diferente do valor real.
Para atributos contínuos, a diferença numérica entre o valor medido e o valor real é chamada de erro.
O termo erro de coleção de dados refere- se a erros com a omissão de objetos de dados ou valores de atributos, ou a inclusão inapropriada de um objeto de dados.
Existem certos tipos de erros de dados que são encontrados com frequência e, para estes, existem técnicas desenvolvidas para detecção e correção.
A precisão é muitas vezes medida por o desvio padrão de um conjunto de valores, enquanto que o foco é medido utilizando a diferença da média do conjunto de valores e o valor conhecido da quantidade que está sendo medida.
O foco só pode ser determinado para objetos cuja quantidade medida é conhecida através de meios externos a situação corrente.
A exatidão depende da precisão e do foco e, para esta, deve- se utilizar dígitos significativos.
O objetivo é usar apenas tantos dígitos para representar o resultado de uma medição ou cálculo quanto for justificado por a precisão de dados.
Cabe ainda observar que questões como dígitos significativos, precisão, foco e exatidão são às vezes negligenciados, mas muito importantes para a mineração de dados.
Muitas vezes conjuntos de dados não apresentam informações sobre a precisão dos mesmos.
Porém, sem uma compreensão sobre a exatidão dos dados e dos resultados, analistas correm o risco de cometer grandes erros na análise de dados.
Segundo, diferentes problemas com a qualidade dos dados podem ser identificados em dados previamente armazenados para posterior mineração.
Para tratar estas questões, destacamos as características destes e algumas técnicas que podem ser úteis no tratamento e correção destas informações.
Outliers são objetos que, de alguma forma, apresentam características diferentes da maioria ou ainda atributos que apresentam valores incomuns com relação a os valores típicos para este.
Diferentes definições para outliers são apresentadas por as comunidades de estatística e mineração de dados.
No entanto, ambas afirmam que estes objetos não devem ser confundidos com erros e que, dependendo do foco da mineração, estas informações são muito importantes como no caso de detecção de anomalias.
Diversas estratégias podem ser utilizadas para lidar com valores ausentes num conjunto de dados sendo que cada uma pode ser melhor indicada em determinadas circunstâncias.
Em a sequência, destacamos algumas destas estratégias juntamente com suas vantagens e desvantagens.
Eliminar objetos ou atributos -- uma estratégia simples e eficaz é a eliminação de objetos com valores ausentes.
Entretanto, deve- se levar em conta que estes objetos, mesmo com valores ausentes, possuem informações que podem ser relevantes.
Em este caso, se muitos objetos apresentam estas características, uma análise confiável pode ser comprometida.
No caso de conjunto de dados que apresentam apenas alguns objetos com valores ausentes uma boa estratégia seria a remoção destes objetos observando que esta deve ser adotada com cautela em função de que os objetos eliminados podem ser significativos para a análise.
Eliminar valores ausentes -- em muitos casos dados ausentes podem ser estimados confiavelmente.
Como exemplo, podemos supor um conjunto de dados com muitos valores semelhantes.
Em esta situação, valores dos atributos mais próximos do ponto com valores ausentes são utilizados para estimar o valor ausente.
No caso de atributos contínuos, utiliza- se a média dos valores dos atributos mais próximos e, para o caso de atributos categorizados, utiliza- se o valor com maior frequência.
Valores inconsistentes -- dados com valores inconsistentes são normalmente inseridos no conjunto de dados durante a aquisição, principalmente quando digitados.
Estes, por sua vez, são mais fáceis de identificar como no caso onde, no cadastro de um cliente, o CEP não pertencer à cidade informada por o mesmo ou no caso de pessoas cuja altura, peso ou idade apresentam valores negativos.
Estas inconsistências, quando identificadas, devem ser corrigidas mesmo que, para isso, seja necessário a utilização de informações adicionais.
Um conjunto de dados pode incluir objetos duplos e, para detectar e eliminar tais objetos, duas questões dever ser observadas.
Primeiro, se houver dois objetos que realmente representam um único, então os valores dos atributos correspondentes podem diferir e estes, se diferentes, devem ser ajustados.
Segundo, deve- se tomar cuidado para não combinar acidentalmente objetos semelhantes, como no caso de pessoas distintas com o mesmo nome.
Data Warehouse -- DW é uma base de dados projetada e modelada para dar suporte à tomada de decisão estratégica.
Em esta, são armazenadas características importantes diferenciam um DW de uma base de dados transacional:
Utiliza modelagem &quot;cubo», construído históricas.
Projetado para armazenar um grande volume de informações.
Agrega informações de diversas origens, de maneira uniforme e consistente.
Tende a ser não-volátil, ou seja, seus dados não serão perdidos e/ ou atualizados ao longo de o tempo.
Não se destina ao armazenamento de informações transacionais diárias.
Oferece formas extremamente flexíveis de visualização de suas informações.
Em a modelagem multidimensional, os dados são armazenados em tabelas identificadas como fato e dimensão.
As tabelas do tipo fato armazenam valores e medidas que representam um fato da organização.
Em as tabelas do tipo dimensão, são armazenados os pontos de observação utilizados por a organização nas análises realizadas sobre os fatos armazenados.
Cabe observar que nas dimensões é comum utilizar informações que, quando associadas ao fato, respondem questões como:
&quot;O quê?»,
&quot;Quem?», &quot;Quando?»
e &quot;Onde?».
Ainda sobre a modelagem multidimensional, esta pode ser construída de três formas:
Modelo Estrela:
Apresentado na Figura 2, este modelo caracteriza- se por conter uma única tabela fato ligada a várias tabelas dimensão.
Modelo Floco de Neve: Difere- se do modelo estrela por permitir a existência de relacionamentos entre as tabelas dimensão.
Modelo Constelação de Fatos: Em este modelo, podem- se encontrar diversas tabelas fato relacionadas com suas próprias dimensões e, assim como no modelo floco de neve, entre estas dimensões podem existir relacionamentos.
Fonte: Adaptado de Mineração de Dados Mineração de Dados é parte integrante de um processo mais descobrir informações relevantes, até então armazenadas, porém desconhecidas, em grandes bases de dados que podem auxiliar seus processos de tomada de decisão.
O processo de KDD, segundo, está dividido em sete etapas distintas identificadas como:
Limpeza: Esta etapa é responsável por a correção de informações e eliminação de dados inconsistentes.
Integração: Em esta fase é realizada a unificação de informações disponíveis em diferentes fontes de dados.
É comum a execução destas duas primeiras etapas numa fase de pré-processamento cujo resultado armazenamento dos dados num Data Warehouse -- DW.
Seleção: É a fase em a qual os dados relevantes para o processo de análise são selecionados na base de dados.
Transformação: Etapa onde os dados são transformados e/ ou consolidados de acordo com o modelo adequado para o processo de mineração.
No caso de a utilização de Data Warehouse, esta etapa constitui um processamento posterior ao armazenamento dos dados de diferentes fontes.
Mineração de Dados:
Basicamente é o processo onde métodos inteligentes de processo de software são aplicados sobre os dados no intuito de identificar determinados padrões de comportamento.
Validação: Em esta etapa são analisados os resultados obtidos com o processo de mineração buscando identificar informações realmente relevantes.
Representação: Etapa responsável por disponibilizar os resultados obtidos de forma adequada para os usuários.
Os resultados obtidos com o processo de KDD dependem, de entre outros fatores, da qualidade dos dados utilizados na etapa de mineração.
Segundo, estas etapas, que compõem basicamente uma fase de preparação de dados, consomem em torno de 85% de todo o processo de KDD.
A Figura 3 representa as etapas do processo bem como a sequência de execução destas dentro de o processo de KDD.
Tanto as técnicas utilizadas especificamente no processo de mineração quanto a a fonte de dados adequada para tal estão detalhadas nas próximas seções.
Fonte: As técnicas de Mineração de Dados podem ser classificadas em dois grandes grupos:
Técnicas preditivas e técnicas descritivas.
As técnicas preditivas possuem como objetivo prever o valor de uma informação em especial em função de o conjunto das demais informações disponíveis para análise.
Os termos comumente utilizados para identificar estas variáveis são respectivamente &quot;variável dependente «e &quot;variáveis independentes».
Já as técnicas descritivas possuem objetivo de encontrar padrões de comportamento ou situações em que se observa fuga de um determinado padrão.
Estas são frequentemente utilizadas e associadas a processos de validação e interpretação de seus resultados.
A Figura 4 exibe quatro das principais técnicas de mineração detalhadas a seguir.
Fonte: Adaptado de Esta técnica busca realizar agrupamentos de objetos em função de a proximidade dos valores de seus atributos.
Esta proximidade é avaliada em função de a comparação de atributos de um objeto &quot;A «com os mesmos atributos do objeto &quot;B».
Supondo que um atributo de &quot;A «descreva a qualidade do produto numa escala de 0 a 5, podemos afirmar que o produto &quot;A», qualificado como 4 está mais próximo de &quot;B «qualificado como 5 do que de &quot;C «qualificado como A análise de agrupamentos trabalha sobre dados em que os valores das classes não estão definidos.
A tarefa consiste em identificar agrupamentos de objetos, agrupamentos estes que identificam uma classe.
Esta técnica é utilizada para descobrir condições que ocorrem com frequência num determinado conjunto de dados.
Os padrões encontrados são tipicamente representados sob a forma de regras de implicação.
Regras de Associação são amplamente utilizadas na análise de comportamento de clientes que realizam compras via internet utilizando cestas de compras.
Em uma definição mais formal, utiliza associações do tipo&quot;», no caso de o comportamento de clientes em compras, quando identificado um determinado padrão, a regra de associação pode ser destacada como &quot;se comprou X, então comprou Y».
Atributos que indicam anomalias são aqueles que, dentro de um determinado grupo de objetos, fogem totalmente ao padrão de comportamento.
A maioria das técnicas de mineração classifica estes objetos como outliers e desconsidera os mesmos.
Entretanto, quando se está buscando detectar anomalias, os outliers são justamente os objetos que se deseja identificar.
Outliers são identificados através da utilização de testes estatísticos que determinam a distribuição ou a probabilidade do modelo de dados.
Modelagem preditiva compreende a construção de um modelo para uma determinada variável dependente em função de os valores encontrados nas variáveis exploratórias.
A modelagem preditiva é dividida em dois tipos de tarefas:
Classificação: Usada quando se está trabalhando com variáveis discretas.
Regressão: Utilizada quando se está trabalhando com variáveis contínuas.
Segundo, algoritmos utilizados em tarefas de classificação buscam a predição de valores desconhecidos em função de um atributo de interesse, denominado atributo classe, levando a construção de um modelo preditivo.
Para que este modelo seja construído, um classificador passa uma etapa conhecida como treinamento onde, para este, é fornecido um conjunto de dados onde o atributo classe é conhecido.
Após o treinamento, o classificador é submetido ao conjunto de testes que é constituído por atributos semelhantes aos do conjunto de treino, porém, neste conjunto, o atributo classe não é conhecido.
Como resultado destas etapas, temos a construção de um modelo preditivo.
A partir de o treinamento, um classificador produz um modelo preditivo que pode ser representado de forma adequada por uma árvore de decisão.
Uma árvore de decisão é um grafo cujos nodos internos são compostos por os atributos explanatórios e os nodos folha representam os valores do atributo classe.
O algoritmo C4.
5 é um popular algoritmo que implementa árvores de decisão.
O software Weka, que será utilizado para realização dos experimentos propostos nesta pesquisa, disponibiliza uma implementação do C4.
5 identificada como J48.
A Figura 5 e a Figura 6 detalham o funcionamento de um classificador baseado em árvore de decisão e a representação dos resultados.
Com base nas informações de uma matriz de confusão, algumas métricas de desempenho podem ser utilizadas para a avaliação de resultados.
Em este contexto, apresentamos na Tabela 6, sete métricas que podem ser calculadas com base nas informações da matriz.
Uma variada gama de ferramentas de mineração de dados podem ser encontradas com facilidade no mercado.
De entre estas, destacamos soluções oferecidas por reconhecidos fornecedores como IBM SPSS Modeler, SAS Enterprise Miner, Oracle Data Mining, entre outros.
Como soluções open source destacamos softwares como o RapidMiner e, em especial, software Weka utilizado nos experimentos realizados nesta pesquisa.
Embora o software Weka seja capaz de buscar informações diretamente do banco de dados, o formato mais utilizado é um arquivo com a extensão arff.
A Figura 7 apresenta um exemplo de um arquivo no formato arff.
Esse arquivo é formado por atributos e instâncias destes onde, a primeira entrada deste arquivo define seu nome (A).
Em seguida, são declarados os atributos (B) que compõe o arquivo que podem ser de dois tipos:
Contínuos ou categóricos.
Quando contínuo, deve- se informar o tipo de valor aceito:
Em o exemplo, real.
Quando categóricos, é necessário informar todas as entradas declaração, são inseridas as instâncias (C) do arquivo, obedecendo à ordem declarada dos atributos e separando os mesmos por vírgula.
Todas as entradas precedidas do sinal&quot;@ «são utilizadas para a correta formatação do arquivo.
Considerações do Capítulo Este capítulo apresentou um estudo sobre os assuntos tratados nesta pesquisa.
Em a seção 2.1, tratamos da caracterização dos dados discorrendo sobre os tipos de dados existentes, as operações possíveis sobre estes e, no tocante a qualidade dos dados, abordamos os problemas que a baixa qualidade dos dados pode trazer para o processo bem como algumas técnicas de tratamento.
Ainda nesta seção abordamos o armazenamento de dados num modelo dimensional, destacamos algumas características das diferentes formas de modelagem multidimensional.
Em a seção 2.2, abordamos o processo de mineração de dados onde detalhamos as etapas envolvidas neste processo e discorremos sobre as principais técnicas utilizadas.
Ainda nesta seção, apresentamos o formato de entrada para um algoritmo de mineração utilizado por o software com o qual foram realizados os experimentos detalhados na seção 4.
Cabe observar que os assuntos abordados até então são utilizados como fundamentação para as questões abordadas nos próximos capítulos onde, apresentamos o detalhamento do cenário onde esta pesquisa foi realizada, a caracterização do problema, a contribuição científica e o desenvolvimento desta pesquisa.
A amostra de dados, selecionada e extraída para a realização desta pesquisa, contemplou informações de 10 cooperativas distribuídas nos estados do RS, SC, PR, MT e SP onde, em cada estado, selecionamos as duas cooperativas com o maior número de clientes.
Para armazenar estas informações, foi construído um DW com a mesma estrutura do utilizado por a organização e, para este modelo, foram importadas as informações das tabelas relacionadas na Tabela 4 que compreendem o período de jan/ 2009 à dez/ 2009.
A o analisar as informações da amostra de dados, observamos algumas inconsistências geradas por falhas no processo de Extract Transform Load ­ ETL da organização.
Estas inconsistências, identificadas nas dimensões Pessoa e Conta, destacavam a falta de informações nos atributos data de nascimento, estado civil e tipo de conta para alguns registros destas tabelas.
Estas inconsistências foram ajustadas com base em informações coletadas de uma Staging Area utilizada por os sistemas de Bi da organização.
Para armazenar as informações utilizadas nos experimentos realizados nesta pesquisa, foi construído um modelo estrela composto por cinco dimensões e uma tabela fato.
Este modelo, apresentado em detalhes no amostra de dados utilizadas nos experimentos realizados nesta pesquisa.
As consultas preparadas e utilizadas nos experimentos foram construídas com base nas informações armazenadas nos diferentes níveis das dimensões.
Procurou- se construir um número de consultas suficientes para responder um variado conjunto de questões de negócio utilizando, em cada consulta, diferentes conjuntos de atributos.
A execução dos experimentos foi realizada com o apoio de uma ferramenta construída para automatizar o máximo de operações possíveis.
Esta ferramenta, apresentada em detalhes no Capítulo 4, utiliza as consultas construídas com os diferentes conjuntos de dados para coletar as informações armazenadas no DW construído, prepara os arquivos de acordo com o formato utilizado por a ferramenta de mineração de dados e executa, utilizando um conjunto de parâmetros definidos e bibliotecas auxiliares, os experimentos de mineração.
A avaliação dos resultados pode ser realizada utilizando as diferentes informações coletadas dos logs de execução dos experimentos.
A ferramenta de apoio, construída para execução dos experimentos, coleta e armazena na base de dados as diferentes informações apresentadas nos logs após a execução de cada experimento.
Considerações do Capítulo Em este capítulo foi apresentado o cenário de desenvolvimento da pesquisa, destacadas questões que podem levar uma organização a não investir em projetos de KDD e, para tratar destas, foi apresentado um processo que discorre sobre a exploração de modelos OLAP e a execução assistida de experimentos de mineração de dados com o objetivo de identificar previamente informações relevantes que possam ser utilizadas para viabilizar a implantação de um projeto de KDD numa organização.&amp;&amp;&amp;
Em o capítulo seguinte, são apresentados maiores detalhes do desenvolvimento deste trabalho e os resultados obtidos.
Este capítulo apresenta o processo de KDD desenvolvido detalhando as atividades realizadas no que diz respeito a integração de dados, o modelo OLAP construído, o ferramental desenvolvido utilizado na execução dos experimentos e, por fim, apresenta uma avaliação dos resultados obtidos.
Processo de integração de Dados Para a definição do modelo analítico utilizado nos experimentos realizados nesta pesquisa, tomamos como ponto de partida a construção de um modelo estrela capaz de consolidar as informações originais da organização possibilitando uma visão abrangente e centralizada dos diferentes produtos comercializados.
Em este contexto, definimos que os objetos fundamentais seriam as dimensões Pessoa, Conta, Tempo, Entidade e Produto associadas a uma tabela Fato.
A Figura 9 e a Figura 10 apresentam a estrutura e hierarquias das dimensões bem como o modelo analítico por completo.
Em o intuito de detalhar o modelo construído apresentamos os diferentes objetivos e características pertinentes a cada tabela:
DIM_ PESSOA:
Esta dimensão armazena as informações de Tipo, Sexo, Estado Civil (Solteiro, Casado, Divorciado, Viúvo, União Estável, Não Informado) e, para o atributo Idade, foram criadas as classes A, B, C, D e E que contemplam os seguintes agrupamentos:
A: Até 20 anos;
B: De 20 a 30 anos;
C: De 30 a 40 anos;
D: De 40 a 50 anos;
E: Acima de 50 anos;
DIM_ ENTIDADE:
Esta dimensão armazena as informações de UF, Cooperativa e as Agências que pertencem a cada cooperativa.
DIM_ CONTA:
Esta dimensão de Tipo de Conta (Individual, Conjunta solidária, conjunta não Solidária, Menor) e Tempo de Conta que foi dividido nas classes A, B, C, D, E e F com os seguintes agrupamentos:
A: Até 2 anos;
B: De 2 a 4 anos;
C: De 4 a 6 anos;
D: De 6 a 8 anos;
E: De 8 a 10 anos;
F: Acima de 10 anos;
DIM_ TEMPO:
Esta dimensão apresenta as informações de Ano, Semestre, Trimestre e Mês.
DIM_ PRODUTO:
Apresenta as informações dos Produtos considerados no modelo, suas Famílias e respectivas Áreas de Negócio.
FTO_ PRODUTOS:
Esta é a tabela Fato do modelo analítico em ela, são armazenadas as chaves para cada uma das dimensões e a informação de Valor do produto.
Com relação a o volume de dados destacamos que, mesmo que tenhamos selecionado apenas 10 cooperativas e os registros de 12 meses, o volume da amostra mostrou- se satisfatório para a realização dos experimentos desta pesquisa.
A Tabela 5 apresenta o volume de cada tabela do modelo gerado.
Em o arquivo de configurações, cujo conteúdo é apresentado por a Figura 11, são definidas informações vitais para a execução da ferramenta como:
O caminho de instalação dos softwares Weka e Java, a configuração de memória que pode ser alocada para a execução da ferramenta, o algoritmo de mineração que será utilizado, a pasta onde são disponibilizadas as consultas que serão executadas por a ferramenta, a identificação do experimento, o atributo classe e um parâmetro de balanceamento.
A classe PrepareFile pode ser considerada a principal classe da ferramenta.
De entre suas funcionalidades destacamos a leitura dos arquivos SQL, a conexão com o banco de dados, a execução das consultas e a exportação para CSV do resultado de cada consulta executada.
Como última atividade, esta classe prepara um script que deve ser executado para conversão dos arquivos CSV em arquivos ARFF.
A Figura 12 apresenta um diagrama de atividades UML que detalha o funcionamento da classe.
As outras duas classes utilizadas por a ferramenta desempenham um papel menos complexo, mas de igual importância, para execução dos experimentos.
A classe BalanceFilter realiza o balanceamento de classes em cada arquivo ARFF gerado obedecendo o parâmetro «classe.
Alvo «definido no arquivo de configuração.
Após a realização do balanceamento, que se resume em manter no arquivo ARFF o mesmo número de instâncias de cada classe definida no parâmetro, esta classe prepara um script para execução dos experimentos utilizando as bibliotecas disponibilizadas com o Weka.
Uma pequena amostra deste script pode ser observada na Figura 13.
A classe GetLog é utilizada após a realização dos experimentos.
Esta classe realiza a leitura do log de execução de cada experimento coletando as informações necessárias para a avaliação dos resultados.
O armazenamento destas informações é realizado no banco de dados numa tabela identificada como experimentos_ log.
A Figura 14 apresenta um trecho do log de execução de um experimento com um algoritmo de classificação onde são destacadas as informações armazenadas na tabela experimentos_ log.
Execução dos Experimentos Com o objetivo de atender os propósitos desta pesquisa e ainda, em função de as características das informações disponíveis no OLAP construído, optamos por limitar o escopo a utilização do algoritmo classificador J48 disponibilizado por o Weka.
Isso posto, a execução dos experimentos é realizada em quatro etapas.
Inicialmente, deve- se preparar as consultas, que possam ser utilizadas para responder determinadas questões de negócio, utilizando diferentes conjuntos de atributos do modelo de dados.
Em esta etapa, pode- se relacionar informações das diferentes dimensões em seus diferentes níveis de hierarquia.
O conjunto de consultas gerado será utilizado por a etapa seguinte do processo.
A Figura 15 apresenta o exemplo de uma consulta construída utilizando atributos de diferentes dimensões.
A Tabela 6 apresenta o conjunto de consultas preparadas para os experimentos realizados onde podemos observar os atributos utilizados.
Após a construção das consultas, os arquivos sql precisam ser disponibilizados numa pasta, indicada juntamente com os demais parâmetros do arquivo Config.
Properties para que as classes, detalhadas no item 4.2, possam ser executadas.
A o final do processo de execução, as informações coletadas diretamente dos logs de execução dos experimentos poderão ser observadas e avaliadas diretamente na base de dados onde os resultados são armazenados.
Considerando que foram utilizados 16 conjuntos de classes e, para cada conjunto, foram preparados 32 arquivos ARFF resultantes das consultas apresentadas na Tabela 6, foram realizados 512 experimentos distintos.
Os resultados destes experimentos foram armazenados na tabela de logs cuja estrutura é apresentada por a Figura 16.
Para possibilitar a avaliação por parte de os especialistas de negócio da organização, selecionamos os 25 experimentos utilizando dois critérios de seleção.
O primeiro critério utilizado diz respeito ao número de nodos da árvore de decisão gerada.
Em este, selecionamos experimentos em que o número de nodos estivesse no intervalo entre 10 e 40.
Estes limites foram estabelecidos após termos verificado na tabela de resultados experimentos com árvores extremamente pequenas, que não possuem um significado relevante, e árvores com centenas de níveis que dificultariam bastante a avaliação dos modelos gerados.
O segundo critério utilizado foi a acurácia, selecionamos de entre os experimentos classificados por o primeiro critério, aqueles que apresentaram a maior acurácia.
Um resumo dos logs de execução dos experimentos selecionados para avaliação é apresentado na Tabela 8.
Uma métrica que também pode ser utilizada para avaliar o resultado de um experimento é a confiabilidade de uma determinada classe.
Esta métrica leva em consideração o percentual de acertos realizados por o algoritmo para cada classe.
Em este contexto, destacamos a ocorrência de 6 experimentos onde podemos observar uma confiabilidade acima de 96% para uma das classes utilizadas.
A o analisar as árvores de decisão geradas por os experimentos observamos que, na grande maioria, foram apresentados resultados num nível de detalhe adequados para interpretação e análise dos modelos.
Porém, em função de termos utilizado as configurações padrão do algoritmo classificador, processos de poda foram realizados eliminando em praticamente todos os experimentos as ramificações das árvores que apresentariam informações de registros dos estados de MT, SC e SP.
Observamos que as cooperativas destes estados apresentam um número muito inferior de associados quando comparadas com as selecionadas nos estados do RS e PR.
Esta diferença significante quanto a o número de registros é refletida nos arquivos preparados e utilizados em todos os experimentos realizados.
Avaliação dos Especialistas de Negócio Para possibilitar a avaliação dos experimentos selecionados por parte de os especialistas de negócio da organização foi realizada uma reunião com cinco analistas de negócios onde foram apresentadas as características da pesquisa realizada, seus objetivos, detalhamento da amostra de dados e experimentos realizados.
Em o intuito de simplificar o processo de avaliação foram disponibilizados arquivos com a representação gráfica de cada árvore de decisão, coletadas do software weka, que facilitam a interpretação das árvores detalhadas nos logs dos experimentos realizados apresentados no Apêndice A. Após a apresentação da pesquisa e do material para avaliação, foi entregue aos especialistas um questionário, apresentado em detalhes no Apêndice B. Em este, os especialistas poderiam sugerir, após terem avaliado o material entregue, um ranqueamento dos experimentos realizados de acordo com suas percepções sobre o problema tratado, definir um grau de relevância do processo apresentado nesta pesquisa no contexto da organização e, para complementar suas observações, foi disponibilizado um espaço para que pudessem descrever seus comentários gerais.
Observamos que o perfil técnico, a formação e a experiência na área de atuação foram fatores relevantes para a seleção dos especialistas que participaram da avaliação sendo que o grupo selecionado é composto de:
Dois estatísticos, dois analistas de business intelligence e um analista de planejamento comercial, ambos desempenham suas atividade na área de inteligência de negócios da organização.
Um resumo das avaliações realizadas é apresentado na sequência sendo que todos os questionários respondidos são apresentados em detalhes no Apêndice B. Primeira questão:
Analisando as árvores geradas, como você ranquearia os experimentos realizados?
Todos os especialistas indicaram que a acurácia, utilizada no ranqueamento realizado por o processo apresentado nesta pesquisa, adequado para ranquear os experimentos realizados sem que, para isso, fosse necessário avaliar cada árvore de decisão.
Segunda questão:
Observando que o propósito do processo apresentado nesta pesquisa identificar oportunidades para aplicação de projetos de mineração de dados, como você classificaria o grau de relevância deste no contexto da organização onde você trabalha?
Indique um valor de 0 a 4, sendo 0 o menor grau de relevância e 4 o maior grau.
A Figura 17 apresenta o gráfico gerado com as respostas dos especialistas para esta questão.
Terceira questão:
Qual a sua percepção geral quanto a os propósitos e a aplicabilidade do trabalho realizado?
As percepções indicadas por os especialistas foram as seguintes:
De uma forma bem interessante foi proposto uma escolha do melhor modelo através da acurácia.
Trata- se de uma proposta bem interessante e que vale a pena validála o quanto antes.
Inicialmente deve- se propor um corte e testar os grupos formados.
Recomendo a, leitura dos grupos, a fim de verificar se faz sentido o agrupamento de variáveis que se, fundiram.
Também, a partir de esta proposta de modelagem será interessante aplicar a metodologia em outras áreas, como por exemplo estudos em geomarketing.
A escolha da melhor localização para a abertura de pontos comerciais é o grande desafio das redes de varejo&quot;;
Mesmo não avaliando árvore de decisão.
Atualmente, o processo de Data Mining além de preparar e integrar dados estruturados, pode também:
Construir e validar modelos, utilizando- se das mais avançadas técnicas de estatística;
Disponibilizar eficientemente o conhecimento e aplicar os modelos preditivos, para os tomadores de decisão de sua empresa e os sistemas que os apoiam.
Este processo é visto com custo as organizações, que através de uma lente míope, esperam que tal modelo complexo gere resultados no curtíssimo prazo.
Apoiando- se sobre esta visão curta, as organizações não compreendem o quão ótimo é o retorno sobre o investimento em mineração de dados, mas, no longo prazo.
Apesar de, globalizado o mercado dificilmente dará retorno de imediato.
Sendo assim, o modelo proposto neste trabalho evidência sua total aplicabilidade no mercado atual.
Desta forma, as organizações poderão antecipar as necessidades dos seus mercados consumidores».
Considerações do Capítulo Este capítulo apresentou o processo de KDD desenvolvido nesta pesquisa detalhando desde a preparação do modelo, o processo de integração e tratamento dos dados até o ferramental desenvolvido para automatização dos experimentos.
Foi apresentado também o processo de definição, preparação e execução dos experimentos, o processo de armazenamento de resultados, a avaliação dos resultados obtidos realizada por o autor, bem como um resumo das avaliações realizadas por especialistas de negócio da organização.
No que diz respeito às avaliações realizadas por os especialistas de negócio podemos afirmar que o objetivo foi parcialmente atingido.
Nenhum dos especialistas avaliou o conjunto de árvores geradas limitando seus pareceres a uma avaliação dos benefícios que o processo apresentado pode trazer para a organização.
Este capítulo apresenta alguns trabalhos que estão relacionados com o tema desta pesquisa.
Estes trabalhos são relatados a seguir e, após a descrição destes, discorremos sobre a comparação com o processo desenvolvido nesta pesquisa.
A identificação de trabalhos relacionados com o tema pesquisado foi uma das maiores dificuldades enfrentadas no desenvolvimento desta pesquisa.
Foram realizadas inúmeras buscas utilizando diferentes strings relacionadas com o tema e, de entre os trabalhos que de alguma forma apresentam semelhanças com o processo apresentado nesta pesquisa, destacamos o CRISP-DM e o processo de KDD apresentado por e.
O CRISP-DM apresenta um processo sobre o ciclo de vida de um projeto de mineração de dados cujo objetivo é definir e controlar as diferentes fases do projeto visando à qualidade do mesmo.
A abordagem utilizada divide o projeto de mineração de dados em seis fases distintas que são utilizadas como um guia para a execução e controle das etapas do projeto.
O CRISP-DM diferenciasse do processo apresentado nesta pesquisa na medida em que este, mesmo utilizando determinadas etapas do processo de KDD busca a identificação de cenários promissores que motivem investimentos nestes projetos.
Entretanto, destacamos que o processo apresentado por o CRISP-DM pode ser utilizado com complementar ao proposto nesta pesquisa.
A Figura 18 apresenta as fases do processo e a relação entre estas.
Fonte: Adaptado de No que diz respeito a o processo de KDD apresentado por e adaptado por com a inclusão de uma etapa para armazenamento de dados num Data Warehouse, apresentado em detalhes no Capítulo 2, observamos que o processo proposto por esta pesquisa está diretamente relacionado em função de que utiliza as mesmas etapas do processo de KDD.
Porém, em função de características apresentadas no cenário onde esta pesquisa foi realizada, foi necessário realizar uma etapa de tratamento e integração de informações coletadas de um DW formado por um conjunto de modelos OLAP previamente modelados e utilizados por a organização.
Destacamos ainda que, em função de a dificuldade em identificar trabalhos semelhantes ao processo proposto nesta pesquisa, realizamos um contato com o Prof. Dr. Jiawei Han, autor de inúmeras publicações desta área bem como de livros amplamente utilizados em universidades que abordam mineração de dados em disciplinas de cursos de graduação e pós-graduação.
Este, por sua vez, retornou nosso contato indicando dois grupos de pesquisa onde poderíamos, pesquisando em seus trabalhos publicados, encontrar alguma abordagem que se aproximasse ao tema que estávamos desenvolvendo.
Foi realizada então uma busca nos artigos publicados por os grupos sugeridos onde novamente não encontramos materiais especificamente relacionados.
A grande maioria dos trabalhos publicados por os grupos indicados abordava processos de armazenamento ou coleta de informações de modelos OLAP focados no atendimento de requisitos de determinados algoritmos de mineração.
Este trabalho apresenta um processo de indução e ranqueamento de árvores de decisão baseado em informações extraídas de modelos OLAP com o objetivo de identificar, de forma semiautomatizada, modelos que apresentem oportunidades para aplicação de técnicas de mineração de dados.
Um dos principais objetivos do processo desenvolvido é fazer com que mais organizações venham a investir em projetos de mineração de dados no momento em que estas, utilizando o processo apresentado, possam visualizar indícios de retorno sobre os investimentos realizados nestes projetos.
Questões que envolvem tipo e qualidade dos dados, limpeza, integração, seleção, transformação e armazenamento, bem como a execução de uma série de experimentos, executados com o apoio de um ferramental desenvolvido, foram exploradas e detalhadas nos Capítulos 2, 3 e 4 desta pesquisa.
O processo desenvolvido foi executado com o apoio de um conjunto de programas de código aberto, possibilitando que estes sejam melhorados, otimizados e adaptados para diferentes necessidades das organizações.
Para disponibilizar informações utilizadas nos experimentos realizados foi preparada uma base de dados e construído um modelo estrela que foi populado com informações coletadas de diferentes modelos utilizados por a organização.
O volume de informações coletadas possibilitou a realização de um considerável número de experimentos.
Este fato destacou a capacidade e flexibilidade do processo desenvolvido em trabalhar com grandes volumes de dados, gerar armazenar um diferente conjunto de resultados e, principalmente, a simplificação do processo de avaliação dos resultados na medida em que estes são coletados e armazenados numa tabela do banco de dados.
Por se tratar de um tema consideravelmente abrangente, identificamos que algumas melhorias podem ser realizadas no processo apresentado, de entre estas destacamos:
A adaptação do processo para coleta e armazenamento de experimentos realizados com outras técnicas de mineração de dados;
A automatização da construção de consultas geradas com diferentes atributos do modelo de dados;
A otimização do código para que se possam realizar experimentos de classificação com um volume maior de informações em cada experimento;
A utilização deste processo em diferentes organizações e segmentos de negócios;
Por fim, considerando as avaliações realizadas por os especialistas de negócio, detalhadas no Capítulo 4, o trabalho desenvolvido possibilita uma maior segurança para as organizações investirem em projetos de mineração de dados na medida em que se consegue, através da identificação de modelos professores, minimizar os riscos de insucesso em projetos realizados nesta área.
