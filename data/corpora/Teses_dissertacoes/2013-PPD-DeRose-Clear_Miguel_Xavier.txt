Em a última década, cresce o número de usuários de HPC em grandes centros de computação.
Isso dáse em virtude de o aumento na diversidade de aplicações paralelas escaláveis, ao reflexo do aumento na capacidade dos processadores, bem como do surgimento das arquiteturas multicore.
A o reflexo desse crescente número de usuários, surgem desafios para os grandes centros de computação, na tentativa de prover ambientes compartilhados compatíveis de forma eficiente, que atendam as necessidades dos usuários em termos seus requisitos de software.
Impulsionados por os avanços nas tecnologias de virtualização, esforços vêm sendo realizados na comunidade científica, na tentativa de aumentar a compatibilidade utilizando técnicas de customização e facilitar o gerenciamento desses ambientes por meio de tecnologias utilizando máquinas virtuais.
Embora que virtualização possa fornecer subsídios que auxiliem no gerenciamento, as soluções existentes não são eficientes em relação a o tempo de provisionamento, além de introduzir penalidades de desempenho.
Desta forma, este trabalho tem por objetivo aumentar a eficiência e amenizar essas penalidades por meio de uma arquitetura.
A solução proposta busca resolver os problemas de incompatibilidade entre ambientes e aplicações utilizando o conceito de workspaces customizados, refletindo as necessidades dos usuários.
Contudo, para sustentar- los, é proposto o uso de uma tecnologia de virtualização baseada em contêiner, com o intuito de provisionar ambientes de forma eficiente, no entanto, com uma ínfima sobrecarga de desempenho.
Os resultados até o momento se mostraram satisfatórios em razão de o tempo de carga dos workspaces.
Além disso, os resultados de desempenho das aplicações HPC revelaram que a AP não introduziu sobrecargas de desempenho, satisfazendo o objetivo geral do trabalho.
Palavras-chave: Incompatibilidade, customização;
Virtualização; Em as últimas décadas, com o surgimento dos clusters de computadores em resposta ao alto custo dos supercomputadores, surgiram oportunidades para uma variedade de usuários que, anteriormente, limitavam- se apenas a recursos de alto custo presentes nos grandes centros de dados.
Como motivação, uma diversidade de aplicações escaláveis emergem em centros de pesquisa em prol de solucionar problemas que demandam alto poder computacional, geralmente relacionadas com áreas climáticas, energéticas, biológicas, entre outras.
Essa grande diversidade de aplicações tornam eminente uma demanda por ambientes de Computação de Alto Desempenho (HPC) flexíveis e compatíveis, que atendam as necessidades dos usuários em termos de seus requisitos de software, como bibliotecas, aplicações e ferramentas.
Como resposta, alguns centros de computação optaram por construir ambientes altamente heterogêneos, no sentido de possuírem uma alta pilha de software instalada.
Por conseguinte, tais ambientes incorrem em dificuldades de gerenciamento, em virtude de a crescente varidade de aplicações e dependências entre as diferentes versões de requisitos de software instalados.
Atualmente, com o avanço na capacidade de processamento dos computadores, como o surgimento das tecnologias de processadores multicore e da computação em unidades de processamento gráficos, cresce o número de usuários HPC em razão de novas oportunidades.
Grade Computacional, por exemplo, permite que usuários em diferentes localidades realizem computação sob ambientes colaborativos distribuídos, aumentando assim, a capacidade de processamento.
Similarmente e com o mesmo potencial, surgem ambientes HPC em Nuvens Computacionais, viabilizando computação de pequenas e de médias aplicações por usuários distribuídos em diferentes localidades, por um baixo custo operacional.
A o reflexo dessa crescente número de usuários, surgem fatores ainda mais desafiadores para os grandes centros de HPC, na tentativa de prover ambientes compartilhados ainda mais compatíveis que atendam as necessidades dos usuários em termos de seus requisitos de software.
Tais desafios estão fortemente relacionados com o gerenciamento.
Uma vez que ambientes altamente heterogêneos incorrem em dificuldades de manutenção, em razão de as incompatibilidades e dependências entre diferentes versões de bibliotecas e aplicações.
Em um cenário onde aplicações e requisitos de software crescem rapidamente, fatores como:
Dificuldade de gerenciamento e usuários descontentes, tornam- se cada vez mais uma realidade.
Em esse ínterim, os avanços nas tecnologias de virtualização trouxeram subsídios que viabilizaram a construção de ambientes de HPC com maior flexibilidade e maior compatibilidade.
O uso de virtualização em computação é uma ideia estabelecida a mais de 30 anos.
Tradicionalmente, seu uso teve significava aceitação por a redução considerável de desempenho, em troca da conveniência do uso de máquinas virtuais (VM).
Atualmente, entretanto, as penalidades de desempenho foram reduzidas.
Processadores mais rápidos, bem como tecnologias mais eficientes de virtualização permitem agora que computadores mais modestos, como computadores pessoais (PC), possam hospedar máquinas virtuais.
Como resultado, o interesse da virtualização em HPC vem crescendo rapidamente.
O baixo custo de implementação, assim como as facilidades de gerenciamento, são as principais motivações que tornaram a virtualização a base de diversos estudos na área de HPC.
Motivação Impulsionados por os avanços nas tecnologias de virtualização, esforços vêm sendo realizados, como no trabalho de Foster, na tentativa de amenizar as restrições e aumentar a compatibilidade entre ambientes e aplicações através de técnicas e de tecnologias utilizando máquinas virtuais.
Embora que virtualização possa fornecer subsídios que facilitam o gerenciamento, as soluções existentes que utilizam virtualização não demonstram ser eficientes, incorrendo em sobrecargas de desempenho.
Uma vez que o volume de dados que as imagens das máquinas virtuais representam é grande, percebe- se a dificuldade inerente a forma como e onde as imagens são organizadas e armazenadas, resultando no tempo de carga destas imagens para o provisionamento dos ambientes.
Em um cenário onde o numero de usuários cresce em centros de computação, o tempo de espera para alocação dos recursos é impactado, em consequência do tempo de turnaround dos jobs (JTT) nos gerenciadores de recursos, em outras palavras, o tempo desde a submissão de um job, até seu término.
Com essa motivação, este trabalho tem por objetivo consolidar técnicas, ferramentas e tecnologias organizadas por meio de uma Arquitetura para Provisionamento de Ambientes Customizados (a partir de o presente momento, Arquitetura para Provisionamento de Ambiente, será denotado apenas por AP, com o objetivo de evitar confusão ao se expor arquitetura de diversas naturezas no decorrer de o texto).
A AP proposta busca resolver os problemas de incompatibilidade entre ambientes e aplicações por meio de a customização desses ambientes, adequando- os as necessidades dos usuários em termos de seus requisitos de software.
Para o provisionamento é proposto o uso de uma tecnologia de virtualização baseada em contêiner, em prol de amenizar as penalidades de desempenho e facilitar o gerenciamento, tornando- a mais eficiente.
Objetivos O Objetivo Geral deste trabalho é a proposta e a validação de uma arquitetura para provisionamento de ambientes customizados em termos de requisitos de software, de forma eficiente.
Além disso, tem- se como objetivo prover uma biblioteca de gerenciamento de tais ambientes, de forma a integrar- la em gerenciadores de recursos em clusters, como o Torque.
Motivado por o objetivo geral deste trabalho, foram definidos os seguintes objetivos específicos:
Estudar os ambientes de HPC, alimentando a pesquisa em sua Fundamentação Teórica;
Estudar as arquiteturas e tecnologias de virtualização, alimentando a pesquisa em sua Fundamentação Teórica;
Estudar as técnicas de virtualização existentes no Estado da Arte, no intuito de auxiliar no alcance do Objetivo Geral;
Propor uma AP para ambientes customizados;
Propor uma biblioteca que permita gerenciar a AP proposta por meio de gerenciadores de recursos;
Avaliar a AP proposta por meio de experimentos, observando o tempo de provisionamento dos ambientes e o desempenho das aplicações HPC;
O presente trabalho busca resolver as duas seguintes questões de pesquisa:
Qual o impacto da AP proposta em relação as APs existentes, levando em consideração as técnicas utilizadas para prover ambientes que atendam a necessidade dos usuários em termos de seus requisitos de software?
Qual o impacto da AP proposta em relação as APs existentes, levando em consideração o desempenho das aplicações HPC e o tempo de provisionamento Hipóteses de Pesquisas Formulou-se duas hipóteses neste trabalho.
A tem como objetivo contribuir na resolução da primeira questão de pesquisa e a (H2) está associada com a segunda questão de pesquisa:
Para enfrentar os problemas de gerenciamento relacionados com incompatibilidade entre ambientes e aplicações, sugere- se o uso de técnicas, de ferramentas e de tecnologias que viabilizem a customização desses ambientes. (
H2) Para provisionar ambientes customizados em termos de requisitos de software de forma eficiente, sugere- se o uso da arquitetura de virtualização baseada em contêineres.
Etapas da Pesquisa Uma boa pesquisa é sempre acompanhada de um bom planejamento.
Assim, foi elaborado, de forma resumida, um Plano de Pesquisa subdivido em etapas de pesquisa correlacionadas com entregáveis.
Para este plano foram identificadas quatro etapas (Estudo, Planejamento, Construção, Validação), como ilustrado na Figura 1.1.
A etapa de Estudo tem como objetivo contextualizar, por meio de uma revisão da literatura, os principais conceitos que estão intimamente relacionados com o contexto dessa dissertação, entrega- se nesta etapa, a Fundamentação Teórica referente a os conceitos de HPC e de Virtualização (Capítulo 2), bem como o Estado da Arte dessa dissertação (Capítulo 3).
Fazem parte da etapa de Planejamento os entregáveis Avaliação das Tecnologias de Virtualização e a Arquitetura de Provisionamento de Ambientes Customizados propriamente dita.
Em o primeiro, objetiva- se uma avaliação sobre as atuais tecnologias de virtualização, no segundo, a Arquitetura representada por meio de diagramas de blocos e entidades é entregada (Capítulo 4).
A etapa de Construção tem como finalidade entregar a modelagem do protótipo utilizado na validação da arquitetura (Capítulo 5) e os fontes do protótipo propriamente dito.
Por fim, na etapa Validação serão entregados os experimentos seguido da avaliação dos resultados (Capítulo 5).
Organização do Trabalho O presente capítulo apresentou a introdução deste trabalho, contemplando a motivação e os objetivos da pesquisa inserida no contexto desta dissertação.
Os próximos capítulos estão organizados como segue:
O Capítulo 2 apresenta o embasamento teórico deste trabalho, concentrando- se nas áreas de HPC e de Virtualização.
Além disso, foi realizado um estudo exploratório sobre os benefícios e desafios da Virtualização para ambientes de HPC.
O Capítulo 3 apresenta o Estado da Arte, o qual foi a principal motivação para a condução desta pesquisa.
Foi possível entender a necessidade e a falta de arquiteturas no âmbito de prover ambientes customizados com uma baixa sobrecarga de desempenho.
A arquitetura proposta neste trabalho é apresentada no Capítulo 4, contemplando seus componentes e suas técnicas, assim como as tecnologias e as ferramentas empregadas.
Em seguida, o avaliação propriamente dita.
As considerações finais e os trabalhos futuros são apresentados no Capítulo 6.
Este capítulo apresenta o embasamento teórico desta dissertação.
A seção 2.1 apresenta os conceitos relacionados com Computação de Alto Desempenho e com Processamento Paralelo e Distribuído.
Em a seção 2.2 apresenta- se os conceitos sobre virtualização, abordando suas arquiteturas, suas tecnologias e seus benefícios no contexto de HPC.
Computação de Alto Desempenho Os requisitos de HPC num sistema operacional diferem, significativamente, da carga de processamento de um servidor típico ou de estações de trabalho.
As aplicações HPC vêm, historicamente, consumindo cada vez mais os recursos de CPU e memória, com problemas cada vez mais complexos, tornando- os cada vez mais desafiadores.
Algumas técnicas, modelos e frameworks de programação, como Processamento Paralelo e Distribuído, MapReduce e Hadoop, surgiram para auxiliar na resolução desses problemas de forma eficiênte, todavia, máquinas especiais surgiram para suportar essas técnicas e modelos, fornecendo poderosos ambientes colaborativos onde essas aplicações podem operar.
A seguir, será apresentado os conceitos relacionados com Processamento Paralelo e Distribuído;
Os ambientes concebidos para Processamento Paralela e Distribuída;
Os tipos de aplicações paralelas que operam sobre esses ambientes.
Segundo De Rose, Processamento Paralelo é definido como:
Várias unidade ativas colaborando na resolução de um mesmo problema, atacando cada uma de elas uma parte do trabalho e se comunicando para a troca de resultados intermediários ou para a divisão inicial do trabalho e, por fim, para a consolidação final dos resultados.
Entende- se também por Processamento Paralelo e Distribuído quando várias unidades de processamento dispostas em mais de um computador trabalham para a resolução de um único problema.
Hwang classifica paralelismo em três níveis:
Paralelismo de instruções, paralelismo de dados e paralelismo de tarefas.
Em o nível de instruções o paralelismo é explorado por arquiteturas de processadores capazes de executar instruções ou operar dados em paralelo para resolução do mesmo problema.
Em o paralelismo de dados, as operações sobre os dados são executadas em diferentes unidades de processamento, onde cada unidade realiza processamento sobre parte deste dado.
Por fim, paralelismo de tarefas refere- se a uma ou mais tarefas sendo executadas independentemente sobre diferentes unidades de processamento.
Como supra explanado, O Processamento Paralelo surge como uma forma de auxiliar na resolução de problemas que demandam alto poder computacional e de finalizar o trabalho como um todo de forma eficiente.
Este conceito desencadeou o surgimento de inúmeros tipos de ambientes de computação (por meio de máquinas especiais) os quais viabilizaram este tipo de processamento.
Esta seção apresenta os conceitos fundamentais sobre ambientes especiais em os quais as aplicações paralelas podem operar, e as principais características que diferem essas aplicações uma das outras.
Máquinas SMP Segundo De Rose, máquinas SMP (Symmetric Multiprocessors) são sistemas constituídos de processadores comerciais, também intitulados como &quot;de prateleira «conectados a uma memória compartilhada, na maioria dos casos por meio de um barramento de baixa latência.
Em este ambiente, os processadores compartilham a mesma memória principal por meio de um único barramento, e todos os processadores têm acesso igual ao barramento e a memória.
Agregado de Computadores consiste de um conjunto de computadores interligados por uma rede de comunicação, trabalhando juntos para resolver o mesmo problema de forma mais eficiente.
Um computador pode ser um simples sistema multiprocessado com memória, dispositivos de E/ S e sistema operacional.
O alto custo de supercomputadores e máquinas SMP, bem como a baixa escalabilidade de máquinas SMP foram motivações que levaram a criação deste ambiente colaborativo.
Os computadores neste ambiente não possuem memória compartilhada, as memórias possuem espaços de endereçamento distintos.
Portanto, como não é possível o uso de variáveis compartilhadas nesse ambiente, a troca de informações entre os processos é feita por envio de mensagens por a rede de comunicação.
Por esta razão, estas máquinas também são chamadas de sistemas de troca de mensagens (Message Passing Systems) De acordo com a classificação comercial de máquinas paralelas, os clusters são divididos em três subclasses:
Máquinas Maciçamente Paralelas (MPP -- Massively Parallel Processors), Redes de of Workstations).
De a mesma forma que máquinas especiais foram concebidas para rodar aplicações paralelas, alguns conceitos sobre as características que diferem essas aplicações foram estabelecidos, tais como:
Aplicações Parameter Sweep (PS), que consistem em execuções de várias iterações da mesma aplicações paralelas cujas tarefas são executadas independentes umas das outras, ou seja, o resultado de uma aplicação não interfere no resultado de uma outra aplicação, apesar de a sua simplicidade, aplicações BoT são utilizados numa variedade de cenários, incluindo simulação de imagens e banco de dados;
Por fim, aplicações do tipo Troca de mensagens (Message Passing), são aplicações que utilizam mecanismos de troca de mensagens para trocar dados durante a sua execução, esse mecanismo geralmente é utilizado para aplicações que apresentam dependência de dados entre as tarefas em tempo de execução e normalmente são implementadas através de uma biblioteca MPI, como OpenMPI.
Grade Computacional Uma Grade Computacional, ou simplesmente grid, é um tipo de ambiente HPC distribuído, proposto originalmente para permitir o compartilhamento de recursos entre diferentes organizações científicas espalhadas geograficamente, fornecendo ambientes HPC virtuais, conhecidos como Virtual Organization (VO).
As grids fornecem mecanismos que auxiliam no compartilhamento e no gerenciamento de diversos recursos, viabilizando a criação, a partir de componentes geograficamente distribuídos, de ambientes HPC virtualizados, que são suficientemente integrados para providenciar uma satisfatória qualidade de serviço.
Estes mecanismos incluem:
Soluções de segurança que suportam o gerenciamento de permissões de usuários e políticas de acesso, uma vez que a computação percorre diversas instituições;
Protocolo para gerência de recursos e de serviços que viabilizam acessos remotos seguros e confiáveis aos recursos, além de a alocação cooperativa de múltiplos recursos;
Protocolos que fornecem informações referentes aos recursos, organizações e serviços;
E serviço de gerenciamento de dados que localizam e transferem dados entre as diferentes aplicações e sistemas de armazenamento.
Nuvem Computacional Computacional, em inglês, Cloud Computing, é um modelo que permite o acesso à rede sob demanda para um conjunto compartilhado de recursos de computação configurável, como redes, servidores, armazenamento, aplicativos e serviços, que podem ser rapidamente fornecidos com um esforço mínimo de gerência ou de interação por o provedor de serviços.
Em o modelo de nuvem, um consumidor pode obter, conforme necessário, recursos de computação automaticamente sem a necessidade de interação humana com os provedores de serviços.
Estes recursos geralmente estão disponíveis por meio de a rede, e podem ser acessados por meio de mecanismos e de padrões utilizados por plataformas heterogêneas de cliente, como telefones celulares, tablets, notebooks e estações de trabalho.
Atualmente, os provedores de recursos em nuvem vêm fornecendo capacidades especiais para execução de aplicações HPC.
A Amazon, por exemplo, por meio de os serviços do AWS, permite agora que cientistas resolvam problemas complexos de diferentes naturezas, utilizando aplicações que exigem baixa latência de rede e capacidades muito altas de computação.
Normalmente, os cientistas aguardam em longas filas para acessar clusters compartilhados ou então, adquirirem sistemas especiais de hardware pagando altos preços, como máquinas SMP.
Usando o serviço de HPC na nuvem, os usuários podem acelerar suas cargas de trabalho HPC em recursos elásticos de acordo com suas necessidades, poupando dinheiro ao escolher a partir de modelos de baixo custo, os recursos que melhor atendam suas necessidades.
Embora a Nuvem Computacional não tenha sido concebida como um ambiente HPC em seu propósito, o modelo possibilitou que diversos cientistas e pesquisadores pudessem criar clusters sob demanda e utilizar- los para processamento de pequenas e médias aplicações.
Mesmo não fornecendo um ótimo desempenho, eles provêem um desempenho relativamente razoável que acaba sendo compensado por o baixo custo e por as facilidades de uso e de gerenciamento.
Assim, provendo uma alternativa atrativa ao cluster convencional.
Virtualização Segundo Jim, a chave para a gestão da complexidade em computadores é a sua divisão em níveis de abstração separados por interfaces bem definidas.
Esses níveis de abstração permitem que os detalhes de implementação em níveis mais baixos possam ser ignorados ou simplificados, simplificando o design de componentes em níveis mais elevados.
Os níveis de abstração são organizados numa hierarquia, com níveis mais baixos implementados em hardware e os níveis mais elevados implementados em software.
Em o primeiro, todos os componentes são físicos, têm propriedades reais e suas interfaces são definidas de modo que as diversas partes possam ser conectadas fisicamente.
O segundo;
No entanto, os componentes são lógicos, com menos restrições com base em características físicas.
Em este contexto, a virtualização atua nos níveis de abstração que se encontram no limite entre o software e o hardware, em o qual o software é separado da máquina em que ele é executado e operado.
Subsistemas e componentes projetados para uma arquitetura específica não irão trabalhar com aqueles projetados para outra.
Existem processadores com conjuntos de instruções diferentes, e existem diferentes sistemas operacionais.
As aplicações vinculadas a um conjunto de instruções específicas e a um sistema operacional somente serão executadas na arquitetura a qual estas foram construídas.
Muitos sistemas operacionais são desenvolvidos para uma arquitetura específica, por exemplo, para um único processador ou um multiprocessador de memória compartilhada, e são projetados para gerenciar os recursos de hardware diretamente.
A suposição implícita é que os recursos de hardware de um sistema são gerenciados por um único sistema operacional.
Isso vincula todos os recursos de hardware a uma única entidade em regime de gestão única.
E esse, por sua vez, limita a flexibilidade do sistema, não só em termos de softwares, mas também em termos de isolamento de segurança e de falha, especialmente quando o sistema é compartilhado por vários usuários ou grupos de usuários.
A virtualização oferece uma maneira de amenizar estas restrições e aumentar a flexibilidade.
Quando um sistema, por exemplo, um processador, uma memória ou um dispositivo de E/ S é virtualizado, sua interface e todos os recursos visíveis através dessa interface são mapeados como um recurso de um sistema real.
O conceito de virtualização pode ser aplicado não só para os subsistemas como discos, mas também para uma máquina inteira.
Uma máquina virtual (VM) é implementada por a adição de uma camada de software a uma máquina real para suportar à arquitetura da VM desejada, esse conceito é conhecido como virtualização de hardware, Soluções de virtualização de hardware são hoje baseada no conceito de Virtual Machine Monitor (VMM), também chamado de hypervisor.
O hypervisor é responsável por a virtualização do hardware e execução de VMs em cima de o hardware virtualizado.
O hypervisor é tipicamente um sistema operacional de pequeno porte que não inclui drivers de acesso ao hardware.
Para acessar os recursos físicos, o hypervisor é combinado com um sistema operacional padrão, como o Linux, que fornece dispositivos de acesso ao hardware.
Uma vez que a virtualização do tipo I tem acesso direto aos recursos do hardware, o desempenho é comparável a uma execução sem virtualização.
Em contraste, o tipo II de virtualização incorre em sobrecarga adicional devido a a camada do hypervisor em cima de o sistema operacional.
O Xen é um exemplo de tecnologia de virtualização do tipo I, o QEMU e o VMWare Workstation são exemplos de tecnologias do tipo II.
Atualmente, uma grande variedade de máquinas virtuais existem para fornecer uma variedade igualmente grande de benefícios.
Múltiplas máquinas virtuais replicadas podem ser implementadas numa plataforma única de hardware para fornecer indivíduos ou grupos de usuários com seus ambientes próprios.
Esses ambientes também oferecem isolamento e segurança reforçada.
Ou então, um servidor pode ser dividido em pequenos servidores virtuais, mantendo a capacidade de equilibrar o uso dos recursos de hardware no sistema como um todo.
Em a próxima seção serão explanadas as arquiteturas dessas máquinas, incluindo as vantagens e as desvantagens entre essas arquiteturas.
Esta seção apresenta uma visão geral sobre as arquiteturas de virtualização existentes atualmente.
Virtualização Total A Virtualização Total ou, em inglês, Full Virtualization, algumas vezes é interpretada por emulação de hardware.
Em este caso, um sistema operacional não modificado é executado utilizando um hypervisor para receber e traduzir as instruções em tempo real para as VMs (Figura 2.2).
Esse processo pode levar a perdas significativas de desempenho.
No entanto, novas estratégias estão sendo utilizadas para agregar múltiplas instruções e traduzir- las juntas.
Embora a Virtualização Total ainda possua penalidades de desempenho que inviabilizam sua utilização em ambientes de alto desempenho, essa arquitetura permite a execução de ambientes operacionais não modificados, o que é ideal, principalmente quando os códigos fonte do sistema não estão disponíveis, como os sistemas operacionais proprietários.
Para-Virtualização Como na Virtualização Total, a Para-Virtualização ou, em inglês, Para-Virtualization, também utiliza um hypervisor, e usa o termo máquina virtual para se referir aos seus sistemas operacionais virtualizados.
Entretanto, ao contrário de a Virtualização Total, a Para-Virtualização exige mudanças no sistema operacional.
Isso permite que a VM possa se coordenar diretamente com o hypervisor, reduzindo o uso de instruções privilegiadas que são tipicamente responsáveis por as penalidades de desempenho na arquitetura Virtualzação Total (Figura 2.3).
A vantagem dessa arquitetura é que as máquinas virtuais geralmente superam em nível de desempenho as máquinas da arquitetura Virtualização Total.
A desvantagem, no entanto, é a necessidade de modificar o sistema operacional.
Isso acarreta implicações para os sistemas operacionais sem código fonte disponível.
Esta arquitetura de virtualização é bastante utilizado em ambientes de alto desempenho, devido a o desempenho razoável, em algumas vezes compensado por as facilidades de gerenciamento.
Virtualização Nativa A Virtualização Nativa, em inglês, Native Virtualization, utiliza suporte de hardware para virtualização internamene no processador para auxiliar no esforço de virtualização.
Esta arquitetura permite que múltiplos sistemas operacionais não modificados possam rodar em paralelo.
Assim, todos os sistemas operacionais são capazes de rodar sobre o mesmo processador, realizando acessos diretamente ao hardware.
Ou seja, a Virtualização Nativa não emula um processador.
Isso é diferente da técnica de Virtualização Total, em a qual é possível rodar um sistema operacional num processador emulado, embora, geralmente, com um desempenho ruim.
Em os processadores x86 da série 64, tanto Intel, como AMD, suportam esse tipo de arquitetura por meio de as tecnologias Intel-VT e AMD-V.
Virtualização no Nível de Sistema Operacional Virtualização em Nível de Sistema Operacional, comumente chamada de Virtualização baseada em Contêineres, é a forma mais intrusiva de virtualização.
Ao contrário de ambas Para-Virtualização e Virtualização Total, essa forma não depende de um hypervisor.
Em vez de isso, o sistema operacional é modificado de forma a isolar múltiplas áreas de usuários (User-Spaces), em mais alto nível, intituladas contêineres, de um sistema operacional dentro de uma única máquina.
A arquitetura viabiliza esse cenário por meio de duas entidades:
Namespaces e Gerência de Recursos.
Um namespace é um ambiente abstrato criado para conter um agrupamento lógico de identificadores únicos ou de símbolos.
Um identificador definido num namespace está associado ao próprio namespace.
O mesmo identificador pode ser definido independentemente em diversos namespaces.
Um exemplo disso são os diretórios num sistema de arquivos, cujos diretórios possuem diferentes arquivos, no entanto, na mesma unidade de disco.
As tecnologias mais recentes baseadas em contêineres utilizam namespaces para introduzir maior isolamento, geralmente providos por o próprio sistema operacional.
De este modo, assegura- se que diferentes usuários utilizem diferentes espaços no mesmo sistema operacional, ou seja, o processo de um usuário torna- se incapaz de se comunicar com os processos de outros usuários, garantindo o isolamento do sistema como um todo.
Uma fundamentação sobre os principais namespaces providos por o kernel do Linux é apresentado a seguir:
Namespace de Sistema de Arquivos:
Limita o escopo do sistema de arquivos para um processo ou um grupo de processos.
Desta forma, o processo fica restrito à uma limitada subárvore de diretórios e arquivos.
Em o Linux, este namespace é implementado por a ferramenta chroot.
Em tecnologias que implementam a arquitetura baseada em contêiner, é implementado de forma a restringir os contêineres a determinados diretórios na máquina hospedeira.
Por conseguinte, não há necessidade de um dispositivo de bloco separado ou uma partição de disco;
O administrador da máquina hospedeira possui acesso a todos os arquivos em todos os contêineres;
Manutenção, como backup/ restore, torna- se uma tarefa trivial;
Permite o provisionamento de contêineres em larga escala.
Namespace PID: Assegura que todos os processos num contêiner possuam seus identificadores únicos, e o primeiro processo dentro de um contêiner possua o PI D1.
Para cada processo no contêiner, o seu PID é diferente daquele na máquina hospedeira.
Logo, os contêineres só podem enxergar seus próprios processos, e eles não podem enxergar (ou acesso de qualquer forma, como o envio de sinais) processos em outros contêineres.
Namespace IPC: Assegura que cada contêiner possua o seu próprio Sistema V IPC (InterProcess Communication), ou seja, seus próprios segmentos de memória compartilhada, semáforos e mensagens.
Por exemplo, a saída de IPC difere- se em cada contêiner.
Namespace de Rede: Garante que cada contêiner possua seus próprios dispositivos de rede, endereços IP, roteamento, regras de firewall, caches de rede, entre outros.
Namespace/ proc e/ sys:
Assegura que cada contêiner possua sua própria representação do proc e do sys (sistemas de arquivos especiais usados para exportar algumas informações do kernel para aplicações.
Em poucas palavras, são os subconjuntos de um sistema hospedeiro Linux.
Namespace de Usuário: Consiste de tabelas de identificadores de usuários (uid), de forma que, o mesmo uid em diferentes contêineres possa ser referir a diferentes usuários.
Namespace UTS: Garante que cada contêiner possua seu próprio hostname.
Em suma, um contêiner é a soma de todos os namespaces.
Portanto, existe apenas um único kernel do sistema operacional em execução, sob o tal diversos contêineres isolados compartilhando esse único kernel.
Essa abordagem possui uma ínfima sobrecarga de desempenho em analogia as arquiteturas Virtualização Total e Para-Virtualização.
No entanto, incorre em tais consequências:
O mesmo sistema operacional é executado em todos os contêineres e na máquina hospedeira.
Apesar de esta limitação, diferentes distribuições Linux podem coexistir em diferentes contêineres.
Dispensa a necessidade de múltiplos kernels em execução, resultando no tempo de inicialização, como também no tamanho dos contêineres (comparando com as VMs).
A pilha de software que encontra- se entre o hardware e as aplicações do usuários é similar a pilha de software de uma arquitetura sem virtualização.
Em outras palavras, os contêineres possuem um desempenho similar à uma máquina nativa, com um custo mínimo da virtualização (em comparação com as VMs).
Essa pilha é observada na Figura 2.4.
Devido a um modelo único de kernel, existe uma única entidade que controla todos os recursos do sistema operacional:
O próprio kernel.
Todos os contêineres compartilham o mesmo conjunto de recursos:
CPU, memória, disco e rede.
Todos os recursos não são pré-alocados, eles são apenas limitados.
Isso significa que:
Todos os recursos podem ser alterados dinamicamente (em tempo de execução);
Se um recurso não é usado, ele fica disponível para outros contêineres, tornando trivial uma tarefa de overcommitting 1.
A vantagem deste tipo de virtualização reside, principalmente, no desempenho.
Não é necessário o uso de um hypervisor para tradução das instruções, o que geralmente resulta em ganhos de desempenho do sistema.
Além disso, essa arquitetura é capaz de realizar migração de contêineres em tempo real (Live Migration), que pode ser utilizado para balanceamento de carga dinâmico entre nodos num cluster.
A principal desvantagem dessa arquitetura é que todos os contêineres compartilham o mesmo núcleo.
Assim, se um núcleo do sistema operacional falha ou é comprometido, todos os contêineres são comprometidos.
Todavia, a vantagem de possuir uma instância única do núcleo do sistema operacional é que menos recursos são consumidos.
Geralmente mais do que uma arquitetura de virtualização pode coexistir no mesmo ambiente, como Virtualização Nativa e Para-Virtualização.
Ambas utilizadas juntas formam um poderoso ambiente virtualizado, normalmente utilizado em ambientes de alto desempenho.
Em o mesmo contexto, estudos demonstram que a arquitetura de virtualização baseada em contêineres pode ser uma alternativa atrativa às arquiteturas Virtualização Total e Para Virtualização em ambientes HPC virtualizados, devido a a sua baixa sobrecarga de desempenho.
Assim como, provendo facilidades de uso e de gerenciamento que podem vir a se úteis em ambientes customizados.
Os avanços nas arquiteturas de virtualização contribuíram para a surgimento de diversas tecnologias que visam suportar as arquiteturas anteriormente contextualizadas.
Essas tecnologias proporcionam poderosos ambientes gerenciáveis por meio de ferramentas e softwares de controle, constituídas de acordo com as características de cada arquitetura.
Cada tecnologia está intimamente relacionada 1 overcommitting -- é uma função do kernel que permite alocar mais memória do que realmente é disponível.
A ideia por de trás dessa funcionalidade é de que alguns aplicativos alocam mais espaço que precisam, mas nunca usam com uma das classificações de Goldberg previamente explanadas.
Esta seção apresenta as tecnologias de virtualização que estão em evidência atualmente.
Enfatizase as tecnologias que implementam a arquitetura baseada em contêineres.
A baixa sobrecarga de desempenho dessa arquitetura motivou para a realização de um estudo em profundidade nas tecnologias que à implementam.
Foram estudados aspectos como isolamento de CPU, de rede e de E/ S. Além disso, procurou- se identificar características relacionadas com a gerência e a limitação de recursos que diferem uma das outras.
Xen Xen é um hypervisor, o que torna possível executar várias instâncias de um sistema operacional ou mesmo diferentes sistemas operacionais em paralelo numa única máquina.
O Xen encontra- se no tipo I, na classificação de Goldberg.
Uma vez que o Xen é uma aplicação de código aberto, ele é amplamente utilizado como plataforma para uma série de diferentes aplicações comerciais e de código fonte aberto, tais como:
Virtualização de servidores, Infraestrutura como Serviço (IaaS), virtualização de desktop, aplicativos de segurança, embarcados e appliances.
O Xen permite aos usuários aumentar a utilização de servidores por meio de a consolidação de grupos de servidores numa única máquina, reduzindo a complexidade e reduzindo o custo final.
O Xen é hoje largamente utilizado em ambientes de Nuvem Computacional, como AWS.
Linux- VServer Linux-VServer é um dos mais antigos sistemas baseado em contêineres no Linux.
Ao invés de utilizar namespaces para garantir o isolamento, Linux- VServer introduz, por meio de um patch, suas próprias capacidades no kernel do Linux, como isolamento de processo, como isolamento de rede e como isolamento da CPU.
Linux- VServer utiliza a chamada de sistema chroot.
Chroot é utilizado para alterar o diretório raiz do processo que o chamou, para um outro sistema de arquivos especificado.
Em este caso, chroot é utilizado para limitar o escopo de um sistema de arquivos para os processos contidos num contêiner.
O isolamento dos processos entre os contêineres é realizado por meio de um espaço global, que contém todos os identificadores de processos (PIDs).
Este espaço assegura que comunicações indesejadas entre os processos de diferentes contêineres sejam realizadas.
A principal vantagem dessa abordagem é a sua escalabilidade.
No entanto, a desvantagem é que o sistema é incapaz devido a impossibilidade de re-instanciar processos com o mesmo PID.
Estas técnicas poderiam contribuir por uma melhor segurança em ambientes de clusters.
Linux- VServer não virtualiza subsistemas de rede.
Em vez de isso, os subsistemas de rede, como tabelas de roteamento IP e de firewall, são compartilhados entre todos os contêineres A fim de evitar que um contêiner intercepte tráfego destinado a outros contêineres, esta abordagem utiliza uma tag identificadora, introduzindo- a nos pacotes e nos filtros apropriados na pilha de rede.
Desta forma, garante- se que apenas o contêiner ao qual o pacote foi originalmente encaminhado, receba o pacote.
As desvantagens, no entanto, reside na limitação de números de soquetes por contêiner para um subconjunto de IPs, como também de alterar sua própria tabela de roteamento e de firewall.
Estas ações devem ser realizadas por o administrador da máquina hospedeira.
O isolamento de CPU é realizado por meio de a mesma técnica utilizada no escalonamento padrão do Linux.
Em este caso é intitulado Token Bucket Filter (TBF).
O TBF é associado a cada contêiner, acumulando símbolos ao alcançar uma determinada taxa.
De essa forma, todos os processos em execução num contêiner são relacionados com a criação de um símbolo.
Os processos de um determinado contêiner são removidos da fila de execução até que seja acumulado um número mínimo de símbolos.
Esta técnica pode ser utilizada para fornecer uma partilha justa de trabalho e conservação da CPU.
A técnica Token Bucket Filter é muito semelhante ao Xen Credit Scheduler.
A limitação dos recursos, como quantidade de memória e como número de processos, é realizado por meio de a ferramenta (rlimit) 2, fornecida por o kernel do Linux.
Além disso, o kernel do LinuxVServer inclui ainda mais funcionalidades para limitar outros tipos de recursos, como número de soquetes e número de arquivos abertos.
No entanto, as versões recentes do Linux- VServer incluem suporte para cgroups, que também pode ser usado para restringir o uso da CPU e da memória.
O Linux- VServer gerência seus contêineres por o pacote de ferramentas util-vserver, fornecido por os desenvolvedores do próprio Linux- VServer.
OpenVZ O OpenVZ oferece funcionalidades similares ao Linux- VServer.
No entanto, ele é construído em cima de namespaces do kernel, certificando- se de que cada contêiner possui seu próprio subconjunto isolado de recursos.
O sistema utiliza um namespace de PID, que garante o isolamento de processos entre diferentes contêineres.
Desta forma, é possível que haja processos em diferentes contêineres com o mesmo PID, facilitando o gerenciamento por parte de o sistema.
Além disso, ao contrário de o Linux- VServer, o namespace PID possibilita o uso de técnicas de virtualização, como Live Migration, Em esse sistema, cada contêiner possui seus próprios segmentos de memória compartilhada, semáforos e mensagens, devido a o namespace Inter-Process Comunication (IPC).
Além disso, o OpenVZ também utiliza namespace de rede.
De este modo, cada contêiner tem a sua própria pilha de rede, que inclui dispositivos, tabela de roteamento, de firewall, de entre outros.
Ele também fornece alguns modos de operação de rede, tais como:
Route, Bridge e Real Network.
As principais diferenças entre eles residem na camada de operação.
Enquanto Route opera na camada 3 (camada de rede), Bridge opera na camada 2 (camada de enlace) e Real Network na camada 1 (camada física).
Em o modo Real Network, o administrador do sistema hospedeiro pode atribuir um dispositivo de rede real (como 2 rlimit -- define limites para a quantidade de memória consumida por qualquer programa eth1) num contêiner, semelhante ao Linux- VServer, proporcionando um melhor desempenho de rede.
O OpenVZ disponibiliza quatro componentes que podem ser utilizados para gerência de recursos, tais como:
User Beancounters (UBS), Fair CPU scheduling, Cota de Disco e I/ O scheduling.
A seguir é apresentado cada um de eles.
User Beancounters fornece um conjunto de limites e garantias controladas por cada contêiner realizado por meio de parâmetros de controle.
Desta forma, pode- se restringir o uso de memória e de objetos do kernel, como segmentos de memória compartilhada e buffers de rede.
As informações sobre o controle dos recursos pode ser observado no arquivo /proc/user_ beancounters· Fair CPU scheduling é implementado em dois níveis na tentativa de realizar uma programação justa entre os contêineres.
O primeiro nível decide qual o contêiner vai ganhar o processador, em algum instante de tempo.
O segundo nível realiza o agendamento de processos internos dos contêineres com base em políticas prioritárias de agendamento, da mesma forma que no escalonador do Linux.
Além disso, existe uma outra abordagem, chamada VCPU Affinity, que informa o kernel o número máximo de CPUs que um contêiner pode alocar.
Cota de Disco é um recurso que permite a criação de cotas de disco em contêineres.
A cota por ser realizada tanto por usuários quanto por grupos de usuários.
I/ O Scheduling é uma abordagem semelhante ao escalonamento de CPU utilizado para E/ S. Em este caso, o segundo nível utiliza o escalonamento Completely Fair Queuing (CFQ).
Para cada contêiner é dado uma prioridade de E/ S, o escalonador, então, distribui a largura de banda de E/ S para os contêineres de acordo com suas prioridades.
De este modo, assegura- se que o canal não seja saturado por um único contêiner, impactando no desempenho.
Os contêineres são controlados por a ferramenta vzctl, fornecida por os desenvolvedores do próprio OpenVZ.
Linux contêineres (LXC) Da mesma maneira que no OpenVZ, LXC utiliza namespaces do kernel para fornecer isolamento de recursos entre todos os contêineres.
Durante a inicialização de um contêiner, por padrão, PIDs, IPCs e pontos de montagem são virtualizados e isolados por meio de namespace de PID, de IPC e de sistema de arquivos, respectivamente.
A fim de comunicar- se com o mundo exterior e de permitir o isolamento de rede, LXC utiliza namespace de rede.
O LXC oferece dois modos de operação de rede, tais como:
Route e Bridge.
Ao contrário de o Linux- VServer e do OpenVZ, o gerenciamento dos recursos é apenas possível por meio de cgroups.
O Cgroups tem a função de restringir o número de CPUs por contêiner, bem como, isolar os processos de diferentes contêineres.
As operações de E/ S são controladas por o escalonador CFQ, da mesma forma que no OpenVZ.
Os contêineres no LXC são gerenciados por as ferramentas lxc-tools, fornecida por os desenvolvedores do próprio LXC.
Aumentar a flexibilidade e facilitar o desenvolvimento de aplicações que necessitam trocar mensagens entre diferentes tipos de tecnologias de virtualização sempre foi uma atividade desafiadora.
A Libvirt surge como uma forma de enfrentar esses desafios, fornecendo uma interface de programação para gerência de diversas tecnologias de virtualização, de entre outras, destacamos o Xen e o LXC.
A Libvirt fornece uma camada comum, genérica e estável para controlar eficientemente Domínios numa máquina.
Entende- se por Domínios, com sendo uma instância de um sistema operacional (ou subsistema, no caso de a virtualização baseada em contêiner) rodando numa máquina virtual fornecida por o hipervisor.
Como máquinas podem estar remotamente localizadas, como num cluster ou numa grid, a libvirt fornece todas as Apis necessárias para provisionar, criar, modificar, monitorar, controlar, migrar e parar os domínios, dentro de as limitações do hypervisor, que deve dar apoio a estas operações.
Embora que várias máquinas possam ser acessadas com a libvirt simultaneamente, as Apis são limitadas à operações de apenas uma máquina por vez.
A Libvirt foi projetada para funcionar em múltiplos ambientes de virtualização, o que significa que as capacidades mais comuns são fornecidas como Apis.
Devido a isso, certas capacidades específicas não são fornecidas.
Por exemplo, a libvirt não fornece políticas de alto nível ou características de gerência de múltiplos nodos, tais como balanceamento de carga.
No entanto, a estabilidade da API garante que esses recursos possam ser implementados numa camada acima de a libvirt.
Para manter este nível de estabilidade, a libvirt tenta isolar as aplicações das frequentes mudanças esperadas no nível inferior na camada de virtualização.
Em suma, a Libvirt foi concebida como um bloco de construção de ferramentas de alto nível e aplicações de gerência com foco em virtualização de um único nodo, com a única exceção sendo a migração de domínio entre diversas máquina.
Ela fornece Apis para monitorar e utilizar os recursos disponíveis na máquina gerenciada, incluindo CPUs, memória, armazenamento e redes.
Conexão com os hypervisors A conexão com os hypervisors é dada por meio de um objeto de nível primário ou superior na API libvirt.
Uma instância deste objeto é necessária antes de utilizar quase todas as Apis.
A conexão está associada a um determinado hypervisor, que pode ser executada localmente na mesma máquina em que a libvirt está rodando, ou num computador remoto por meio de a rede.
Em todos os casos, a conexão é representada com o objeto virConnectPtr e identificada por um Uniform Resource Identifier (URI).
O esquema de URI define o hypervisor em o qual a aplicação irá se conectar, bem como sua localização na rede.
Uma aplicação é capaz de abrir múltiplas conexões ao mesmo tempo, mesmo quando se utiliza mais do que um tipo de hypervisor numa única máquina.
Por exemplo, uma máquina pode fornecer tanto virtualização de VMs utilizando Xen, quanto uma virtualização baseada em contêineres LXC.
Modelo de drivers A biblioteca libvirt expõe uma API estável que é dissociada a qualquer tecnologia de virtualização, onde cada tecnologia é vinculada a uma implementação de um driver.
A o obter uma instância do objeto virConnectPtr, o desenvolvedor da aplicação pode fornecer um URI para determinar qual driver é ativado.
Para um melhor entendimento da arquitetura de drivers, observa- se a Figura 2.6.
Se um driver não implementar uma API, então ele irá retornar um código de erro permitindo que este seja detectado.
A seguir, é brevemente alguns os drivers providos por a libvirt:
Xen: O hypervisor do Xen prove máquinas para-virtualizadas e totalmente virtualizadas.
Um driver é executado na máquina hospedeira se comunicando diretamente com o· OpenVZ:
A tecnologia OpenVZ utiliza um kernel Linux modificado na máquina hospedeira.
O servidor do serviço RPC é fornecido por o processo libvirtd, que deve ser executado na máquina hospedeira a ser gerenciada.
Em uma implementação padrão, este processo epenas escuta conexão num soquete de domínio local UNIX.
Isso permite apenas conexões de aplicações utilizando transporte dados por meio de um túnel SSH.
Com a configuração adequada de certificados x509, ou credenciais SASL, o processo libvirtd pode ser configurado para escutar num soquete TCP para conexões diretas e não em túnel.
Benefícios e Desafios da Virtualização em Ambientes HPC Para Mergen, a virtualização se tornou bastante atraente para sistemas comerciais, implementada por um hypervisor e abstraindo a camada de hardware.
De a mesma forma e com o mesmo potencial, a virtualização trouxe benefícios para aplicações HPC em todas as suas dimensões, como produtividade, confiabilidade, disponibilidade, segurança e simplicidade.
Essa seção apresenta soluções e benefícios encontrados na revisão da literatura que se enquadram nessas quatro dimensões.
Embora que o presente Trabalho não tenha como objetivo usufruir da maioria dos benefícios na AP proposta, é importante destacar- los uma vez que a AP possuirá uma tecnologia de virtualização em sua concepção.
Estes benefícios podem ser futuramente estudados e introduzidos na AP.
Mergen afirma que a &quot;Reinicialização Virtual «das máquinas evita latência na inicialização do hardware, em virtude de o desacoplamento do hardware e do sistema operacional, reduzindo significativamente o tempo de reinicialização do sistema.
Inerente a isto, uma imagem de VM pré-inicializada e congelada pode ser enviada a todos os nós de um cluster, reduzindo consideravelmente o tempo de provisionamento de um cluster virtualizado.
Devido a o isolamento da VM, falhas de hardware ou software, como num núcleo de processador, numa memória, num sistema operacional ou num aplicativo, afetam diretamente apenas uma VM, a menos que o hypervisor sofra uma falha.
Se a VM afetada não pode se recuperar e se ela realmente falhar, seus recursos de hardware são reivindicados por o hypervisor e usados para reiniciar outras VMs.
Outras VMs pertencentes ao cluster virtual não estão cientes dessa falha, a menos que tenham uma dependência de comunicação sobre a VM afetada.
Esse incidente pode causar lentidões caso as máquinas estejam compartilhando o mesmo núcleo afetado.
Entretanto, este isolamento aumenta a confiabilidade do sistema e aumenta a probabilidade de finalização de aplicações HPC de longa duração, sem nenhum esforço especial por parte de os sistemas operacionais que rodam nas VMs.
Em sistemas HPC, o custo de uma falha na aplicação ou da reinicialização inesperada desta aplicação, é muito significativo.
Com o aumento no número de aplicações que demandam de muito desempenho, esses custos tendem a subir ainda mais, exigindo maior esforço dos desenvolvedores para buscarem melhores soluções de confiabilidade.
A virtualização é um fator essencial para a implementação dos métodos necessários para tolerância a falhas, uma vez que cada VM encapsula todo o aplicativo, toda a biblioteca, e todo o estado do sistema operacional, tornando o sistema mais confiável e fácil de ser migrado, caso ocorra alguma falha.
Soluções para tolerância a falhas exigem monitoração contínua do sistema, que incluem:
Os componentes de hardware, o comportamento dos aplicativos e a integridade dos dados.
Assim, a virtualização trouxe a possibilidade de isolar a carga dessas aplicações HPC das funcionalidades de controle de gerenciamento necessários para implementar essas soluções de tolerância a falhas, sem a necessidade de hardware ou software de gerenciamento especializado.
O isolamento da VM fornece uma plataforma para a construção de sistemas seguros.
No entanto, uma VM isolada não pode ter interação com outro software em execução numa máquina real.
Se um programa confiável é carregado numa VM isolada, então a sua comunicação estabelecida com outros softwares autorizados a se comunicar com ele pode ser confiável.
Outra VM, se autorizada, pode usar a introspecção para auditar o estado de uma VM, por exemplo, para procurar vírus.
Introspecção também pode ser utilizado para monitorar continuamente as comunicações e o estado de uma VM, a fim de verificar de forma independente seu correto funcionamento.
Sistemas seguros são muito importantes para aplicações de alto desempenho que, geralmente, executam dados governamentais.
Este capítulo apresenta o Estado da Arte dessa dissertação.
Buscou- se identificar arquiteturas que contemplem ferramentas, técnicas e tecnologias no âmbito de prover ambientes de Alto Desempenho customizados baseados em requisitos de software de usuários.
O presente capítulo foi subdividido em categorias correlacionadas com o contexto dessa dissertação, intituladas:
Agregado de Computadores e 2) Grade Computacional.
Foi utilizado essa abordagem de forma a agrupar os conteúdos inerentes a cada categoria e, com isso, manter a organização do volume.
As próximas seções descrevem com maior detalhe cada categoria.
Agregado de Computadores Em esta seção serão apresentados os trabalhos relacionados com ambientes de clusters.
Kang Ressaltam que em clusters, a pilha de software, como sistema operacional, como bibliotecas e como aplicativos, são geralmente instalados uma única vez e mantidos inalterados durante um longo período de tempo.
Em ambientes desta natureza, torna- se viável apenas pequenas atualizações, como correções de segurança ou como pequenos ajustes no desempenho, em virtude de as dependências entre as diferentes aplicações e as diferentes bibliotecas instaladas no ambiente.
Em esse sentido, os autores justificam a criação de uma arquitetura de provisionamento flexível e ao mesmo tempo eficiente, que permita criar ambientes customizados que satisfaçam os requisitos de qualquer aplicação, independente de sua natureza.
Os autores intitulam a nova arquitetura como VirtualCluster, introduzindo o conceito de templates.
Os templates viabilizam a criação de uma VM a partir de uma imagem pré-configurada com pacotes básicos instalados, tais como:
Sistema operacional, bibliotecas e aplicativos.
Uma vez criada a VM, o usuário é capaz de customizar seu próprio ambiente com sua respectiva pilha de software e então, provisionar- lo num cluster.
Como resultado, emergem novos desafios quanto a organização das imagens na arquitetura, como gerenciamento, como espaço de armazenamento e como tamanho das imagens.
Estes fatores refletem diretamente no tempo de provisionamento, isto é, o tempo de inicialização das VMs em todos os nodos de um cluster, tornando- o pronto para uso.
Para enfrentar esses desafios, os autores apresentam três subsistemas que compõe o VirtualCluster, tais como:
A Interface de Controle com o Usuário, o Sistema de Armazenamento e o Sistema de Comunicação de Rede.
O primeiro, fornece uma interface de iteração com o usuário e, além de permitir o acesso ao ambiente customizado, a interface permite o gerenciamento, a configuração e a customização das VMs.
O segundo é implementado combinando uma estratégia utilizando caches de disco e cálculos de hashes, evitando assim, a duplicidade de imagens armazenadas em disco.
Essa estratégia reduz o espaço em disco necessário para o armazenamento das imagens.
Por fim, o terceiro, é o sistema responsável por a comunicação.
O Sistema de Comunicação de Rede está intimamente relacionado com a velocidade de inicialização do cluster.
Com isso, os autores buscam por uma estratégia cooperativa de transferência de dados, onde cada nodo está envolvido, tanto em esforços de envio, quanto de recepção de dados.
Ainda na mesma obra, foram elaborados experimentos para avaliar o tempo de criação e de inicialização das imagens num cluster, incluindo implicitamente uma análise de desempenho do subsistema de Comunicação de Rede.
Apesar de os resultados demonstrarem um resultado aceitável e cumprir com os objetivos do trabalho, percebe- se que o trabalho carece de uma avaliação de desempenho utilizando aplicações HPC, que nos permita analisar as penalidades de desempenho oriundas da tecnologia de virtualização incluída na arquitetura.
Kate Artificiam uma Prova de Conceito sobre uma arquitetura utilizando workspaces, Entende- se por workspace, um ambiente inteiramente customizável e gerenciável de forma a atender os requisitos de um usuário.
A arquitetura busca provisionar, dinamicamente, workspaces sobre máquinas remotas.
Para tornar esse serviço viável, os autores optaram por utilizar máquinas virtuais, em virtude de a sua portabilidade para uma ampla variedade de plataformas.
Os autores argumentam que, uma máquina virtual pode ser configurada de forma a incluir uma pilha de software completa e, uma vez configurada, iniciada em máquinas remotas em questão de milissegundos, tornando o provisionamento de workspaces por meio de máquinas virtuais muito atraente.
Com base no conceito acima descrito, um pesquisador pode desenvolver sua aplicação num workspace local e depois migrar este workspace para um cluster remoto, para então, realizar a computação.
A arquitetura proposta utiliza um arquivo metadata (XML Schema) para descrever os requisitos de software e de hardware e, por meio de uma interface de gerência, o usuário é capaz de realizar ações de gerenciamento, tais como:
Instalar, desligar, pausar e reativar os workspaces.
Como resultado, a Prova de Conceito demostrou que, na prática, o tamanho das imagens pode ser grande, resultando no tempo de provisionamento do workspace nos clusters remotos.
Isso reforça novamente a dificuldade em provisionar um ambiente customizado de forma eficiente.
Além disso, a prova de conceito foi conduzida sobre nodos individuais, invalidando o conceito sobre um cenário de produção utilizando um clusters.
Nishimura et al apresenta uma arquitetura de provisionamento de clusters inteiramente customizados.
A arquitetura permite que máquinas virtuais com configurações customizadas sejam criadas de acordo com as necessidades dos usuários.
A partir de uma requisição de um usuário, o sistema determina os recursos físicos necessários para provisionar o ambiente, cria um conjunto de novas máquinas virtuais e, por fim, instala o sistema operacional e o conjunto de aplicações.
Os autores afirmam que o processo de instalação dos softwares acarreta em perdas significativas de desempenho, em razão de a descompactação dos pacotes.
Como resposta à este problema, os autores apresentam uma técnica na tentativa de otimizar o processo de instalação, intitulada Disco Virtual de Cache (VD Cache).
Essa técnica baseia- se na suposição de que os pacotes utilizados recentemente provavelmente serão utilizados mais tarde.
Desta forma, a VD Cache é uma imagem de máquina virtual, onde uma combinação de pacotes frequentemente requisitados já estão préinstalados, no entanto, ainda não configurados.
Desta forma, para cada requisição do usuário, o processo de instalação primeiramente procura na VD Cache, utilizando uma cópia da cache como uma imagem de disco inicial.
O sistema então resolve as diferenças entre a cache e os requisitos do usuário, instalando os pacotes restantes.
Ainda na obra supracitada, os autores demonstram a eficácia da arquitetura por meio de um experimento.
Em esse experimento foi avaliado o desempenho ao criar um cluster customizado e a escalabilidade conforme o aumento no número de nodos.
Para avaliar a escalabilidade, foram realizadas instalações variando o número de VMs, de 21 até 190 máquinas.
Como resultado, o provisionamento de um cluster de 190 nodos utilizando a arquitetura proposta foi realizada em 40 segundos.
Por fim, os autores concluem afirmando que, o provisionamento de um cluster de 1000 nodos poderia ser realizado em menos de dois minutos.
Em suma, mesmo sabendo que a arquitetura é escalável, o trabalho carece de uma avaliação de desempenho utilizando aplicações HPC.
Não ficou claro a partir de o experimento realizado, quais as penalidades de desempenho oriundas da camada de virtualização introduzida na arquitetura.
Isso deixa a entender implicitamente que a arquitetura não busca resolver os problemas de desempenho.
No entanto, a arquitetura resolve os problemas em torno de os requisitos das aplicações sobre o ambiente criado.
Grade Computacional Em esta seção serão apresentados os trabalhos relacionados com Grade Computacional.
Keahey Introduz o conceito Virtual Workspace (VW).
Este conceito permite que ambientes customizados em termos de requisitos de software e de hardware sejam provisionados.
Além disso, permite o gerenciamento e o provisionamento desses ambiente em grids.
A premissa deste conceito é obter os requisitos de software e de hardware e, assim, utilizar ferramentas automatizadas para procurar, configurar e provisionar o ambiente mais adequado inerente aos requisitos obtidos.
Os requisitos correspondem a um conjunto de configurações de softwares e de hardwares descritas num arquivo metadata.
As configurações de hardware correspondem de entre outras, especificações de memória, de disco e de rede.
As configurações de software, entretanto, correspondem às especificações relacionadas com versão de sistema operacional (kernel, distribuição) e de aplicações.
Os autores enfatizam a necessidade de uso de máquinas virtuais para sustentar o conceito de workspaces virtuais, apontando que a virtualização de hardware, por sua natureza, fornece isolamento, serialização e segurança, diminuindo os esforços de gerenciamento.
Desta forma, as imagens das VMs refletem aos requisitos de softwares, enquanto o hypervisor assegura que os requisitos de hardware sejam fornecidos de forma transparente.
Os autores propõem uma arquitetura baseada em dois serviços, um de eles permite a configuração e o gerenciamento dos workspaces por os usuários, o outro, todavia, é responsável por o processo de alocação dos recursos físicos.
As atividades que envolvem configuração e gerenciamento dos workspaces incluem ações como instalar/ desinstalar, iniciar/ parar e pausar/ continuar.
Após concluído o provisionamento dos workspaces nos recursos físicos, a computação no ambiente torna- se disponível.
Uma vez que o workspace esteja pronto para execução, as aplicações podem ser iniciadas utilizando qualquer mecanismo de alocação de recursos em grid.
Ainda na mesma obra, os autores, a partir de experimentos realizados, afirmam que o provisionamento dos workspaces numa grid é bastante eficaz em virtude de o tempo de provisionamento, comparada a um arquitetura convencional.
Embora que a virtualização seja uma alternativa atraente para a arquitetura com workspaces, seu uso pode não ser favorável quando se necessita 100% dos recursos da máquina hospedeira, não havendo necessidade de ambientes customizados, ou ainda, quando o tempo de execução das aplicações for relativamente baixo, neste caso, o esforço gasto nas etapas de configuração e de provisionamento dos workspaces pode não ser compensável.
Mais do que isso, os autores salientam que existe uma certa dificuldade na transferência dos workspaces para as máquinas hospedeiras, devido a o tamanho das imagens das máquinas virtuais.
Zhang Alertam que o compartilhamento de clusters entre diferentes Vos numa grid tornou- se uma tarefa não trivial, em razão de a incompatibilidade entre os diferentes requisitos de software e de hardware existentes entre os diferentes clusters numa VO.
Embora que essa incompatibilidade possa ser resolvida por meio de o particionamento dos clusters e da instalação das bibliotecas e ferramentas necessárias, outros problemas de compartilhamento, como o isolamento e como a necessidade de aplicações específicas, ainda persistem.
Impulsionados por esta dificuldade entanto, os autores realizam uma extensão do conceito, abrangendo agora, a noção de clusters.
Alterações na definição do VW, bem como, uma extensão na arquitetura, de modo a atender a customização de ambientes de clusters, foi realizada.
De a mesma forma que descrito anteriormente, o VW é composto por um conjunto de configurações descritas num arquivo metadata, que consolida os requisitos em termos de recursos e de especificações de software.
A ideia, neste caso, foi expandir o arquivo metadata, incluindo clusters como sendo um novo tipo de workspace suportado.
Os autores intitulam esse novo tipo de workspace como Virtual Cluster Workspace (VCW).
Chase Apontam que, a integração de clusters em grids é uma realidade, conduzida no âmbito de tornar- los de uso misto e compartilhado, servindo diversos grupos de usuários com diferentes requisitos e necessidades.
Alertam, ainda, que o uso desses ambientes de forma compartilhada resulta na consolidação de clusters, na tentativa de prover acesso sob demanda e maior economia em termos de implantação e de administração.
Com essa motivação, os autores apresentam uma de gerenciadores de recursos em clusters, como instalação de pacotes de softwares pré definidos, customização de softwares e atualizações remotas.
De entre outros fatores chaves que a arquitetura proposta por Chase Busca resolver, os autores destacam Ambientes de Software Customizados.
Alertam que cada ambiente é capaz de executar softwares de usuários, independente de seus requisitos e de suas necessidades.
Apontam que o tempo médio de instalação de uma imagem Linux a partir de uma pequena imagem num servidor de rede é de cerca de dois minutos.
A arquitetura permite que usuários selecionem imagens numa biblioteca de templates, como também, que novas imagens de templates sejam enviadas para um servidor de rede.
Os clusters no COD são provisionados utilizando um conjunto de técnicas, como PXE, como DHCP e como NIS/ DNS, além de um espaço de armazenamento compartilhado.
Quando um nodo inicializa, o servidor DHCP consulta o seu status num banco de dados.
Se o nodo precisa uma nova configuração, o servidor DHCP carrega um sistema operacional mínimo para instalar o ambiente com os softwares especificados por os usuário.
O sistema operacional mínimo consiste de um kernel genérico do Linux e um sistema de arquivos pequeno baseado em RAM.
Um Processo identifica e envia um resumo do hardware instalado para um Processo de configuração.
O Processo de configuração, em seguida, altera as partições para as unidades locais de disco e inicia a etapa de instalação dos pacotes de softwares.
Os autores realizam experimentos no intuito de avaliar a escalabilidade da arquitetura proposta, conforme o aumento no número de nodos.
Foram realizados experimentos variando a quantidade de nodos, de forma a avaliar a dinamicidade da arquitetura.
O número máximo de nodos avaliados foi 1000 e percebeu- se que a arquitetura é escalável para grandes clusters.
No entanto, a arquitetura carece de uma avaliação de desempenho que inclua benchmarks específicos de HPC.
Síntese Para uma melhor interpretação sobre os trabalhos estudados, a tabela 3.1 aponta as principais diferenças entre os trabalhos e a AP proposta nesta dissertação.
Foram elencadas características necessárias para a criação de ambientes customizados.
Desta forma, diferencia- se os trabalhos relacionado- os com estas características.
Com base no Estado da Arte, foi possível identificar necessidades e desafios para a concepção de uma AP que provisione ambientes customizados.
Logo, os seguintes itens foram destacados:
A maioria dos trabalhos utilizam virtualização baseada em hypervisor na concepção de APs.
Como consequência, nenhuma das APs encontradas possuem um desempenho de aplicações HPC equivalente a uma AP sem virtualização;
Conceber templetes pré-configurados utilizando virtualização baseada em hypervisor torna- se um desafio, em razão de o tamanho que as imagens das máquinas virtuais representam;
Percebe- se a necessidade de uma entidade de armazenamento de ambientes customizados para reuso.
A virtualização baseada em hypervisor não demonstra ser eficiente neste caso, em razão de o volume de dados que uma imagem de VM representa, dificultando seu transporte na rede de comunicação;
Nenhuma AP provê uma interface que permita a sua integração com gerenciadores de recursos, como Torque.
Uma vez que foi possível encontrar boas APs na literatura, percebe- se, como relatado na Seção contrário, as APs buscam enfrentar desafios e resolver problemas que estão comumente relacionados com alta disponibilidade, com segurança e com melhor utilização dos recursos.
Além disso, as APs encontradas utilizam tecnologias de virtualização baseadas em hypervisors, como visto, geralmente incorrem em penalidades de desempenho.
Este capítulo descreve com acurácia as técnicas que concebem a AP proposta.
Tais quais foram especificadas levando em consideração o estudo exploratório realizado no Estado da Arte.
No entanto, as técnicas encontradas unem- se às hipóteses definidas, na tentativa de atingir o objetivo geral do trabalho, além de suprir as necessidades e enfrentar os desafios identificados nas APs existentes.
Para atingir tal objetivo, foi preciso especificar requisitos aos quais a AP precisa atender por meio de as técnicas propostas.
Assim, ao avaliar a aderência de tais requisitos durante a etapa de validação, sabe- se se o objetivo geral do trabalho foi atingido.
Portanto, os requisitos especificados foram:
Customização: A AP deve prover ambientes customizados que atendam as necessidades dos usuários em termos de seus requisitos de software.
Estes ambientes devem interferir minimamente no desempenho das aplicações HPC.
Em outras palavras, os meios utilizados para prover estes ambientes customizados não devem ser intrusivo nas aplicações paralelas dos usuários;
Comunicação: Como descrito na Seção 2.1, as aplicações do tipo message passing possuem seu desempenho bastante influenciado por a latência da rede de comunicação.
Logo, os meios utilizados para prover os ambientes customizados não devem interferir na latência da rede de comunicação;
Armazenamento: A AP deve possuir um dispositivo para armazenar templates com diferentes características de softwares.
O dispositivo deve estar estrategicamente localizado na AP, de tal forma que o tempo de provisionamento de um ambiente a partir de um template seja minimizado· Gerenciamento:
A AP deve estar provida de uma interface para o gerenciamento dos ambientes customizados, de forma que seja possível criar/ destruir ambientes a partir de os templates contidos no dispositivo de armazenamento.
Essa interface deve ser capaz de estabelecer uma relação de confiança com os gerenciadores de recursos em clusters.
Os requisitos da AP foram concebidos na ambição de alcançar o objetivo geral do trabalho.
Com estes especificados, as técnicas propostas para atendes- los foram definidas, quais sejam:
Workspaces, Sincronização Incremental de Dados, Customização dos Workspaces e API de gerenciamento.
A Figura 4.1 destaca onde cada uma das técnicas estão inseridas na AP proposta.
As próximas seções apresentam, enfim, em maiores detalhes, cada uma das técnicas definidas para compor a AP.
Além disso, apresenta- se as ferramentas e as tecnologias sugeridas para viabilizar tais técnicas.
Por fim, apresenta- se uma avaliação de desempenho das tecnologias de virtualização baseadas em contêineres, com o objetivo de identificar a tecnologia mais apropriada para sustentar as técnicas propriamente ditas.
Técnicas Propostas Está seção apresenta as técnicas definidas para compor a AP proposta.
As técnicas apresentadas estão fortemente relacionadas com o requisitos especificados anteriormente, quai sejam:
Customização, Comunicação, Armazenamento e Gerenciamento.
Defini- se workspace como sendo um sistema de arquivos customizado de acordo com os requisitos de software de um usuário ou grupos de usuários, sustentado por meio de uma tecnologia de virtualização.
Estes workspaces localizam- se em todos os nodos num cluster.
Como citado no Estado da Arte (Seção 2), o conceito de workspaces foi anteriormente introduzido por Keahey Em a AP VCW.
Em a obra citada, estes workspaces consistem de um conjunto isolado de softwares com características distintas.
Tais workspaces são viabilizados por meio de uma tecnologia baseada em hypervisor.
É possível construir workspaces para uma variedade de cenários com características distintas de softwares.
Em grandes centros de pesquisas, há diferentes grupos de usuários associados a diferentes projetos de pesquisas, estes por sua vez, podem possuir seus workspaces próprios, com seus requisitos de software previamente instalados e configurados.
De a mesma forma, aplica- se à áreas da ciência como biologia, física, entre outras.
A biologia, por exemplo, possui uma ampla variedade de aplicações paralelas escaláveis, cada qual acompanhadas por requisitos específicos de software.
Como exemplo, citamos a ferramenta BLAST.
O BLAST é uma ferramenta globalmente utilizada na área científica biológica para alinhamento de sequencias de forma paralela.
Destaca- se o BLAST por este necessitar de ferramentas de apoio para realizar trabalhos de pré e pós processamento.
Para casos como este, um workspace customizado, contendo o BLAST instalado, bem como suas ferramentas de apoio, seria muito atrativo.
Novas instalações, atualizações, entre outras manutenções, podem ser facilitadas sem que haja impacto nos workspaces de outros usuários.
O conceito de workspaces vai em direção a o requisito Customização, provendo sistemas de arquivos customizados de acordo com os requisitos de usuários.
A transferência das imagens de um dispositivo de armazenamento compartilhado para os nodos num cluster ainda é um dos maiores desafios identificados nas APs citadas no Estado da Arte.
Isso dá- se em virtude de o tamanho que as imagens das máquinas virtuais representam.
A técnica VD Cache introduzida por Nishimura ameniza essas restrições provisionando uma imagem padrão simplificada contendo pacotes de softwares frequentemente utilizados, para então, instalar o restante dos pacotes por meio de um gerenciador de pacotes.
No entanto, a necessidade de instalar pacotes durante o processo de provisionamento impacta fortemente no tempo de turnaround.
Assim, o uso dessa técnica é descartado, levando em consideração a hipótese (H2).
De a mesma forma que nas APs existentes, propõem- se que os workspaces estejam armazenados num dispositivo de armazenamento compartilhado entre todos os nodos que concebem um ambiente.
Este dispositivo é conectado à uma switch que, por usa vez, conectada diretamente aos nodos por uma rede dedicada.
O conceito de redes publicas e privadas é utilizado para este propósito, de forma a assegurar que os dados oriundos do dispositivo de armazenamento possuam exclusividade na mídia de transporte destinado aos nodos.
Portanto, é proposto uma abordagem utilizando uma técnica de sincronização incremental de dados.
A técnica considera que exista uma cópia dos workspaces num repositório local em todos os nodos do cluster.
Uma vez que os workspaces sejam transferidas para os nodos, esses serão apenas sincronizados com o dispositivo de armazenamento em situações onde houverem alterações, como novas instalações ou atualizações.
De esta maneira, a maior quantidade de dados transferidos será apenas na primeira sincronização.
O processo é composto por as seguintes etapas:
Sincronizar workspace:
Um requisição parte do gerenciador de recursos para sincronizar um workspace pré-configurado.
Essa requisição é destinada a todos os nodos do cluster;
Sincronizar Repositórios com o Dispositivo de Armazenamento:
Os nodos que compõe o cluster, simultaneamente sincronizam seus repositórios locais com o dispositivo de armazenamento.
Em este caso, apenas o workspace requisitado é sincronizado evitando congestionamento de dados na mídia de transporte.
Workspace Sincronizado: Com o workspace já sincronizado, os nodos enviam uma mensagem para o gerenciador de recursos, informando que o processo de sincronização foi concluído.
Esta técnica torna- se viável quando utilizada sobre workspaces, pois neste caso, um workspace representa apenas uma estrutura de diretórios em árvore, constituindo um sistema de arquivos, diferentemente de uma AP utilizando imagens de máquinas virtuais.
Uma vez que as imagens de máquinas virtuais representam além de o sistema de arquivos, o kernel e seus módulos.
O tamanho das imagens cresce, tornando a sincronização uma técnica pouco eficiente.
Além disso, as imagens de máquinas virtuais geralmente são representados por arquivos binários, tornando o uso da técnica de sincronização incremental uma alternativa ainda mais distante.
Uma vez que sempre que houver uma atualização no dispositivo de armazenamento, todo o binário precisa ser sincronizado.
Em direção a o requisito Armazenamento, a forma onde e como os workspaces são mantidos, bem como questões relativas à transferência de dados entre as entidades na arquitetura, foram definidas nesta Seção.
Ainda no requisito Customização, a instalação de bibliotecas, de ferramentas, de entre outras, torna- se uma tarefa trivial na AP proposta.
A AP não contribuí com uma interface automatizada para customização, do contrário, a customização deve ser realizada manualmente, isto é, toda a pilha de software é inserida nos workspaces no dispositivo de armazenamento, para posteriormente, ser sincronizada com os nodos no próximo processo de Sincronização Incremental de Dados.
Como citado no Estado da Arte (Seção 3), Kate Destacam a necessidade de uma interface para provisionamento remoto de workspaces, ou seja, o usuário ser capaz de customizar seu próprio workspace aderente às suas necessidades, para a posteriori utilizar- lo no provisionamento de um cluster num infraestrutura remota.
Com essa motivação, os autores artificiam uma prova de conceito de uma AP que ambiciona suprir tal necessidade, como resultado, o conceito demonstrou ser ineficiente em razão de o tamanho das imagens.
Esse conceito vem a tona no âmbito da AP proposta.
Uma vez que os workspaces constituem apenas de um sistema de arquivos customizados, seu tamanho é relativamente menor do que uma imagem de máquinas virtual.
Em suma, acreditamos ser viável o provisionamento remoto de clusters por meio de a AP proposta.
No entanto, o desenvolvimento desta funcionalidade não encontra- se no escopo do trabalho e deverá ser melhor estudada e viabilizada mais tarde num trabalho futuro.
Como foi possível observar na síntese do Estado da Arte (Seção 3.3), as APs existentes não atentam e carecem de uma interface de gerenciamento que permita sua integração com gerenciadores de recursos existentes.
Uma interface desta natureza aumenta a interoperabilidade, no sentido de tornar os workspaces operáveis por uma diversidade de gerenciadores de recursos, como Torque.
Além disso, inibe a AP de se preocupar com gerências usuais em gerenciadores de recursos, como filas, como interface de escalonamento, como recursos disponíveis, de entre outros.
Por meio de essas abstrações, as ações de gerência são facilitadas.
Em direção a o requisito Gerenciamento, propõem- se uma API que viabilize o gerenciamento dos workspace na AP proposta.
Esta dispõe de uma biblioteca, que pode ser integrada em gerenciadores de recursos, inserindo ações de gerenciamento como criar/ destruir clusters de workspaces durante um processo de provisionamento.
Para isso, sugere- se que tais ações sejam realizadas numa atividade de pré e pós alocação de recursos num gerenciador de recursos.
Desta forma, assegura- se que o ambiente seja provisionado, para depois ser alocado.
A API busca atender o requisito Interface de Gerência, para tanto as seguintes funções foram definidas:
CreateCluster, destroyCluster, syncWorkspace, syncAllWorkspaces.
Uma descrição detalhada de cada uma de elas é apresentado a seguir.
Descrição e Parâmetros da Função createCluster O gerenciador de recursos poderá criar um cluster de workspaces por meio de a função createCluster.
Criar um cluster significa iniciar um contêiner em todos os nodos especificados.
Segue a estrutura da função na Tabela 4.1.
Descrição dos Parâmetros da Função createCluster Descrição Tipo Exemplo Nome do cluster que será criado Nome do workspace associado ao cluster Parâmetro clusterName workspaceName configurationData Arquivo de configuração no formato XML.
Em a Tabela 4.2 segue a tabela com as descrições de todos os parâmetros do arquivo de configuração.
Em a Figura 4.1 segue o exemplo da estrutura do arquivo.
A função createCluster retornará um valor do tipo string contendo os dados separados por pipe&quot;|».
Segue a estrutura do retorno de dados:
O gerenciador de recursos poderá destruir um cluster de workspaces por meio de a função destroyCluster.
Destruir um cluster significa parar todos os contêineres em todos os nodos associados ao cluster.
A função destroyCluster retornará um valor do tipo string contendo os dados separados por pipe&quot;|».
Segue a estrutura do retorno de dados:
O gerenciador de recursos poderá forçar o sincronismo de um workspace por meio de a função syncWorkspace, utilizando a técnica de sincronização incremental.
O sincronismo é realizado em todos os nodos.
Segue a estrutura da função na Tabela 4.1.
A função syncWorkspace retornará um valor do tipo string contendo os dados separados por pipe&quot;|».
Segue a estrutura do retorno de dados:
O gerenciador de recursos poderá forçar o sincronismo de todos os workspaces por meio de a função syncAllWorkspaces, utilizando a técnica de sincronização incremental.
A função não possui parâmetros.
Retorno da Função syncAllWorkspaces pipe&quot;|».
Segue a estrutura do retorno de dados:
As seções que antecedem conceitualizam workspaces e suas técnicas de gerenciamento, consolidando a AP proposta neste trabalho.
O conceito de workspace foi concebido no intuito de suprir a necessidade de um ambiente de HPC customizado.
A técnica de sincronização incremental, no entanto, define a localização estratégica de armazenamento destes workspaces, bem como assegura o sincronismo entre o dispositivo de armazenamento e os repositórios locais.
Para a customização dos workspaces, não foi especificado nenhuma técnica em especial.
Para tanto, sugere que a pilha de software dos usuários seja inserida manualmente nos workspaces.
Por fim, a API foi concebida para integrar a gerência dos workspaces aos gerenciadores de recursos em clusters, tornando a AP interoperável.
Considerando que a AP foi conceitualmente definida por meio de o conceito de workspaces e suas técnicas de gerenciamento, é preciso que haja ferramentas e tecnologias que viabilizem tal conceito e tais técnicas.
Assim sendo, um estudo exploratório foi conduzido no propósito de encontrar ferramentas e técnicas que fomentem a AP proposta.
Tecnologias e Ferramentas As próximas seções revelam os resultados do estudo, cada tecnologia e ferramenta apresentada vai ao encontro de sustentar o conceito de workspaces e suas técnicas de gerenciamento definidas.
Para sustentar o conceito de workspaces, propõe- se o uso de virtualização.
A diferença notória em relação a o conceito de workspaces da AP VCW reside na arquitetura de virtualização utilizada para sustentar o conceito.
Enquanto na VCW utiliza- se uma tecnologia de virtualização baseada em hypervisor, aqui propõe- se a utilização de uma tecnologia baseada em contêineres, ambicionando sobrecargas mínimas de desempenho nas aplicações HPC, indo ao encontro de satisfazer o requisito Customização, bem como aceitar a hipótese (H2).
Os autores da VCW salientam que existe uma certa dificuldade na transferência dos workspaces para os nodos, devido a o volume de dados que representa uma imagem de máquina virtuai.
Como o conceito de workspace numa arquitetura baseada em contêiner representa apenas o sistema de arquivos customizado de um contêiner, sem kernel e seus módulos.
Logo, acredita- se que essa arquitetura de virtualização possa trazer subsídios para enfrentar tais dificuldades, em razão de o menor volume de dados que um contêiner representa.
Para uma melhor interpretação, observa- se na Figura 4.3 a pilha de software de um nodo num cluster utilizando workspace, provido por meio de uma tecnologia de virtualização baseada em contêiner.
A desvantagem aqui reside no compartilhamento do sistema operacional.
Como explicado na Seção 2.4, a arquitetura de virtualização baseada em contêineres, por sua natureza, compartilha o mesmo kernel entre o sistema operacional e os workspaces, inviabilizando a customização do sistema operacional, como versão, como drivers, entre outras.
Como enfatizado anteriormente, em arquiteturas de virtualização baseada em contêiner, um workspace representa o sistema de arquivos de um contêiner.
Para tanto, é preciso entender como os workspace estão organizados nos nodos.
A Figura 4.4 ilustra tal organização.
É fácil perceber que não existe complexidade na forma como os workspaces são organizados nos nodos, os workspaces consistem, basicamente, de um diretório sob o diretório raiz do sistema operacional.
Mesmo que esta seção tenha proposto qual arquitetura de virtualização insere- se na AP.
É preciso ainda identificar que tecnologia de virtualização é a mais adequada para suportar o conceito de workspaces e, finalmente na etapa de validação, verificar se o requisito Customização foi satisfeito.
Assim, uma avaliação comparando as tecnologias de virtualização baseadas em contêineres existentes é apresenta mais adiante, na Seção 4.3.
Buscando atingir ao máximo a capacidade de rede, propõem- se o uso do protocolo de comunicação entre dispositivos de armazenamento em rede iSCSI.
SCSI é uma família popular de protocolos que permitem que sistemas comuniquem- se com dispositivos de E/ S, especialmente dispositivos de armazenamento, como disco, fitas, entre outros.
O protocolo iSCSI, no entanto, descreve um meio de transporte de pacotes SCSI sobre redes TCP/ IP, que prevê uma solução interoperável que pode tirar proveito de uma infraestrutura de rede já existente.
Estudos assimilam ISCSI com uma rede com fibra (fiber channel) por sua alta taxa de transmissão.
Por está razão, o protocolo foi introduzido na AP.
A Figura 4.5 apresenta o fluxo de informações que ocorre durante um processo de alocação de workspaces num cluster utilizando a AP proposta.
A escolha da ferramenta deu- se em razão de sua capacidade de realizar cópia incremental de dados, ou seja, apenas arquivos modificados serão copiados, diminuindo, desta forma, a quantidade de dados trafegados.
Para auxiliar na técnica de Customização dos workspaces, sugere- se o uso da ferramenta chroot.
O chroot é um diretório que possui uma cópia completa de um sistema de arquivos.
A o utilizar- lo, tem- se um ambiente independentemente onde pode- se testar modificações no sistema ou fazer testes de comportamento num sistema de arquivos diferente.
Em o contexto da AP, o diretório chroot equivale um workspace.
Para casos onde não exista pacote de softwares pré-compilado e inviabilize o uso de um gerenciador de pacotes, as bibliotecas ou ferramentas podem ser simplesmente inseridas no workspace, como aconselhado anteriormente.
Essas novas instalações serão sincronizadas com os nodos na primeira requisição em a qual solicite o provisionamento do respectivo workspace.
Para viabilizar a implementação das funções definidas na API de gerenciamento, sugere- se o uso da biblioteca de gerenciamento de tecnologias de virtualização libvirt.
A biblioteca foi apresentada em maiores detalhes na Seção 2.2.3.
De forma sucinta, a libvirt fornece uma API para o gerenciamento de hypervisors.
Em tecnologias de virtualização baseadas em contêineres, como OpenVZ e LXC, a libvirt atua na gerência dos contêineres, utilizando para isso, drivers que se comunicam diretamente com o sistema operacional.
A libvirt é uma alternativa atrativa para a implementação das funções createCluster e destroyCluster, definidas na API de gerenciamento.
Uma vez que a libvirt provê capacidades para gerenciamento remoto através de SSH, como criar e destruir contêineres.
A biblioteca provida por a API pode ser lincada numa aplicação de controle no gerenciador de recursos, a fim de prover os subsídios necessários para realizar as ações de gerenciamento na AP proposta.
A Figura 4.7 ilustra onde a libvirt insere- se na AP proposta.
Para que a gerência remota funcione por meio de a libvirt, é preciso que haja uma implementação de um driver de controle específico para a tecnologia de virtualização a qual se queira gerenciar, esse driver, por sua vez, é chamado por um processo RPC em execução no nodo (libvirtd), o qual encontra- se em todos os nodos de um cluster.
De entre as tecnologias de virtualização baseadas em contêineres, a libvirt possui drivers apenas para OpenVZ e para LXC.
O Linux- VServer, por ser uma implementação bastante legada, com suas capacidades próprias implementadas no kernel, ainda não possui tal suporte.
Ao encontro de satisfazer o requisito Gerenciamento, o uso da libvirt foi proposto como uma ferramenta para sustentar a técnica de gerenciamento da AP.
Experimentos para Avaliação das Tecnologias de Virtualização Considerando que foi definido a arquitetura de virtualização utilizada para sustentar os workspaces.
A definição da tecnologia que será utilizada, bem como a justificativa do uso deste tipo de virtualização ainda carece de uma avaliação.
Contudo, esta seção apresenta uma avaliação de desempenho das tecnologias de virtualização baseadas em contêineres existentes.
Foram realizados experimentos buscando identificar qual tecnologia provê maiores vantagens nas condições da AP proposta.
Aspectos como desempenho e isolamento foram investigados.
Além disso, para enriquecer a análise, foi inserido nos experimentos uma tecnologia de virtualização baseada em hypervisor a fim de comparar- la, e tornar mais evidente a aceitação da hipótese (H2), em a qual sugere- se o uso de uma tecnologia baseada em contêiner em razão de as suas vantagens de desempenho.
Diversos trabalhos têm estudado as sobrecargas de desempenho impostas por as tecnologias de virtualização, alguns de eles com foco em ambientes HPC.
Walters Avaliaram o desempenho do VMware Server, do Xen e do OpenVZ para ambientes HPC utilizando o benchmark NPB (OpenMP e MPI) e micro-- benchmarks para Rede e Disco.
Em seus experimentos, embora que ambos, Xen e OpenVZ tenham alcançado um desempenho relativamente razoável para os benchmarks de CPU intensivo, o OpenVZ obteve um desempenho superior enquanto os testes de rede foram realizados.
O VMware, no entanto, teve o pior desempenho em todos os casos.
Seus experimentos foram apenas focados em desempenho.
Regola e Ducom avaliaram o KVM, o Xen e o OpenVZ para HPC.
Eles também utilizaram o benchmark NPB tanto em sua versão OpenMP quanto MPI, e micro-- benchmarks para Rede e Disco.
Seus experimentos incluíram instâncias EC2 da Amazon AWS, que supostamente utilizam o hypervisor Xen e uma rede de 10 Gbit/ s para comunicação entre os nodos.
De a mesma forma que no trabalho do Walters et al, todas as tecnologias de virtualização obtiveram desempenho quase nativo para o testes de CPU.
No entanto. O OpenVZ obteve o melhor desempenho de E/ S. Seus experimentos usando instâncias EC2 mostraram que o serviço da Amazon não foi capaz de fornecer a capacidade total da rede, resultando em penalidades de desempenho no benchmark de Rede.
Soltesz Apresentou o projeto de implementação do Linux- VServer, comparando- o com o hypervisor Xen.
Os autores não utilizaram quaisquer cargas de trabalho HPC nesta avaliação.
Em vez de isso, seus experimentos utilizaram benchmarks de banco de dados e micro-- benchmarks para avaliar a CPU, o Disco e a Rede.
Os resultados mostraram que o Linux- VServer fornece suporte comparável para isolamento, no entanto, que o desempenho ainda é superior ao Xen.
O desempenho da rede no Linux- VServer foi significativamente melhor do que no hypervisor Xen.
Atualmente, parece evidente que tecnologias baseadas em hypervisors oferecem um bom desempenho para aplicações que necessitam de alto poder computacional, no entanto, introduzem uma grande penalidade de desempenho para aplicações que necessitam de intensa gerência de E/ S, especialmente de rede.
No entanto, existem algumas alternativas para evitar a sobrecarga elevada em operações de rede em ambientes virtualizados.
Por exemplo, Liu Apresentaram a idéia de VMM--bypass, que se estende a idéia Os--bypass para as VMs.
Basicamente, esta técnica permite que operações críticas de E/ S sejam realizadas diretamente por as VMs, sem qualquer envolvimento do hypervisor.
Quando aplicada a redes, VMM--Bypass permite, por exemplo, o acesso direto aos dispositivos de rede de alto desempenho, tais como InfiniBand, resultando num desempenho da rede quase nativo.
O isolamento é uma preocupação importante em virtualização para ambientes HPC, especialmente para o caso de partilha de uma máquinas multi núcleo entre múltiplos usuários.
No entanto, poucos trabalhos foram encontrados no intuito de avaliar- lo.
Deshane Propuseram o Isolation Benchmark Suite (IBS), que quantifica o grau em que uma tecnologia de virtualização limita o impacto de uma VM e outra em execução na mesma máquina física.
Os autores também avaliaram o impacto para VMware, para Xen e para OpenVZ.
Embora que isolamento ainda não seja uma questão em a qual a AP proposta busca resolver em razão de não haver requisitos que se enquadrem em consolidação de workspaces.
Mesmo assim, decidiu- se estender os estudos existentes e enriquecer o trabalho, no entanto, inserindo ambos Linux- VServer e LXC nos experimentos.
Matthews Também utilizaram o IBS para comparar o desempenho de isolamento do Xen e do KVM.
Os autores relataram que o KVM obteve grandes penalidades para os testes de isolamento de Disco e de Rede.
Com o objetivo de complementar os trabalhos supracitados, foi conduzido uma avaliação de desempenho e de isolamento das atuais tecnologias de virtualização baseadas em contêiner para ambientes HPC.
Para tanto, os experimentos cobrem todos os requisitos importantes de um ambiente HPC (CPU, memória, disco e rede), bem como, as principais ferramentas utilizadas no desenvolvimento de aplicações HPC (OpenMP e MPI).
Com isso, espera- se que o requisito Customização da AP proposta seja satisfeito por uma destas tecnologias.
A tecnologia LXC foi incluída nos experimentos, uma vez que nenhum trabalho ainda avaliou a sobrecarga de desempenho e de isolamento que incorrem dessa tecnologia.
Ainda que nenhum trabalho tenha realizado uma avaliação incluindo uma comparação entre as três atuais implementações em Linux baseadas em contêineres (Linux- VServer, OpenVZ e LXC), este estudo torna- se útil na ótica de identificar qual tecnologia é mais apropriada no contexto de provisionamento de ambientes HPC customizados.
Para tanto, as tecnologias baseadas em contêineres incluídas nos experimentos foram:
LinuxVServer, OpenVZ e LXC.
Além de essas, foi incluído na avaliação o Xen como o representante da arquitetura baseada em hipervisor, por ser considerado uma das mais maduras e eficiente tecnologias deste tipo de virtualização.
O ambiente experimental foi construído em prol de refletir os componentes da AP proposta.
Assim foram criados clusters de workspaces para todas as tecnologias de virtualização avaliadas.
Estes clusters foram criados manualmente, sem nenhum tipo de ação automatizada de gerenciamento.
O ambiente consiste de quatro máquinas idênticas Dell PowerEdge R610 com dois processadores Intel Xeon E5520 2.27 GHz, 8M de cache L3 por núcleo, 16 GB de memória RAM e um adaptador de rede NetXtreme II BCM5709 Gigabit.
Todos os nodos estão interligados por uma switch Dell PowerConnect 5548.
A distribuição Linux Ubuntu 10.04 LTS (Lucid Lynx) foi instalada em todas as máquinas e suas respectivas configurações padrões foram mantidas, exceto para o kernel e para pacotes que foram compilados de forma a satisfazer os requisitos das tecnologias de virtualização.
Sabe- se que diferentes versões do kernel podem introduzir ganhos ou perdas de desempenho, que poderiam influenciar nos resultados dos experimentos.
Contudo, foi tomado o cuidado de compilar a mesma versão do kernel para todas as tecnologias de virtualização avaliadas.
A versão 2.6.32-28 do kernel foi utilizada nos experimentos.
A escolha do kernel foi realizada baseado no suporte aos patches e as configurações das tecnologias avaliadas.
Portanto, para o OpenVZ, foi instalado um patch no kernel (2.6.32-Feoktistov), além de o pacote de gerenciamento vzctl, necessário para gerenciar os contêineres do OpenVZ.
A compilação do kernel do OpenVZ foi realizada utilizando o arquivo de configuração(.
Config) oficial sugerido por a equipe de desenvolvedores do próprio OpenVZ, a fim de assegurar que todas as opções de kernel foram ativadas.
Para o Linux- VServer, também foi instalado um patches no kernel, além de o pacote de gerenciamento util-vserver, necessário para controlar os contêineres do Linux- VServer.
O LXC já possui implementação nativa nos fontes oficiais do kernel.
De essa forma, foi necessário instalar apenas as ferramentas de gerenciamento lxc-tool e garantir que todos os requisitos necessários relatados por o utilitário lxc-- checkconfig foram atendidos.
Por fim, para o Xen, o kernel xen-4.
1.2 foi compilado e instalado, juntamente com as ferramentas fornecidas por o pacote do Xen.
As próximas seções apresentam os resultados obtidos durante a condução dos experimentos.
Para avaliar o desempenho de computação num único nodo, foi utilizado o benchmark LINPACK.
O LINPACK é constituído por um conjunto de sub rotinas Fortran que analisam e resolvem equações lineares por o método dos mínimos quadrados.
O LINPACK é executado sobre um único processador e seus resultados podem ser utilizados para estimar o desempenho de um computador para executar aplicações que demandam alto poder computacional.
O benchmark foi executado em todas as tecnologias de virtualização, para matrizes de ordem 3000.
Como observado na Figura 4.9, todos as tecnologias baseadas em contêiner obtiveram resultados semelhantes ao nativo, ou seja, não houve diferença estatisticamente significativa entre os resultados obtidos.
Acredita- se que isso foi devido a o fato de que não há influência dos diferentes escalonadores de CPU utilizados por as diferentes tecnologias, quando um processo de CPU intensivo é executado num único processador.
Os resultados também mostram que o Xen não foi capaz de atingir o mesmo desempenho, apresentando uma média de sobrecarga de 4,3%.
O desempenho da memória num único nodo foi avaliado por meio de o benchmark STREAM.
O STREAM é um benchmark sintético simples que mensura a largura de banda máxima alcançada num ambiente de computação.
O benchmark realiza quatro tipos de operações com vetores (Add, Copy, Scale and Triad), utilizando conjuntos de dados muito maiores do que a quantidade de memória cache disponível no ambiente de computação, o que reduz o tempo de espera por erros de cache (cache misses) e evita a reutilização da memória.
Esta sobrecarga é causada por o hipervisor que faz a tradução dos acessos à memória para os níveis mais baixos, o que resulta em perdas de desempenho.
Além disso, observa- se no Xen o problema de cache dupla (double-cache), isto é, os mesmos blocos são utilizados por o hospedeiro e por a máquina virtual.
Uma análise detalhada nas tecnologias com contêineres mostradas na Figura 4.11 revela que ambas, LXC e Linux- VServer obtiveram um resultado semelhante para as operações Read e ReRead.
Caso contrário para as operações de gravação, onde o Linux- VServer excede um pouco o desempenho do nativo, e o LXC atinge o desempenho perto de o nativo.
O mesmo comportamento obtido por o Linux- VServer nas operações de gravação foi observado no trabalho do Soltesz et.
Observa- se um ganho de desempenho ao comparar estes resultados com o OpenVZ.
Acredita- se que esse comportamento é devido a os diferentes escalonadores de E/ S utilizados por as diferentes tecnologias.
Enquanto o LXC e o Linux- VServer utilizam por padrão o escalonamento &quot;deadline «do Linux, o OpenVZ utiliza o escalonamento CFQ, a fim de prover a capacidade de priorização de acesso ao disco para um grupo de contêineres, como descrito na seção 2.2.2.
O escalonamento &quot;deadline «impõe um &quot;tempo de espera «para todas as operações de E/ S para garantir que nenhuma requisição fique trancada, aguardando por tempo indeterminado para ser executada, e agressivamente reordena as requisições para garantir melhoria no desempenho de E/ S. O pior resultado foi observado no Xen para todas as operações de E/ S devido a os drivers para-virtualizados.
Esses drivers ainda não são capazes de atingir um alto desempenho.
O desempenho da rede foi avaliada por meio de o benchmark NetPipe (Network Protocol Independent Performance Evaluator).
NetPipe é uma ferramenta que mede o desempenho da rede sob uma variedade de condições.
O benchmark executa testes simples como ping-pong, enviando mensagens de tamanho crescente entre dois processos, por meio de uma rede ou de uma arquitetura multiprocessada.
Os tamanhos das mensagens são definidos e enviados em intervalos regulares para simular perturbações, e fornecer um teste completo do sistema de comunicação como um todo.
Cada ponto de dados envolve muitos testes de ping-pong para fornecer medições precisas de tempo, permitindo o cálculo de latências.
A Figura 4.12 demonstra a comparação da largura de banda de rede em cada tecnologia de virtualização.
O comportamento do Linux- VServer equivale ao obtido na máquina nativa, seguido por LXC e OpenVZ.
O pior resultado foi observado no Xen.
A sua largura de banda média foi 41% menor do que o nativo, com uma degradação de 63% para pacotes pequenos.
De a mesma forma, a latência de rede apresentada na Figura 4.13 mostra que o Linux- VServer atinge uma latência equivalente à nativa.
O LXC novamente atinge um bom resultado, com uma diferença muito pequena quando comparada com Linux- VServer e com a máquina nativa, seguido por OpenVZ.
A pior latência foi observada no Xen.
Estes resultados podem ser explicados devido as diferentes implementações de isolamento de rede das diferentes arquiteturas de virtualização.
Enquanto o Linux- VServer não implementa modos de operação utilizando dispositivos virtualizados, tanto OpenVZ, quanto LXC implementam namespaces de rede, que fornecem um subsistema de rede inteiro.
No caso de o OpenVZ, não foi utilizado o modo de operação Real Network, como descrito na Seção 2.2.2, pois reduziria a escalabilidade devido a o número limitado de adaptadores de rede que normalmente existem em ambientes de alto desempenho reais.
A degradação de desempenho da rede no Xen é justificada por a complexidade adicional de transmitir e receber os pacotes.
O caminho de transferência de dados entre as VMs e hipervisor afeta negativamente o rendimento da rede em arquiteturas para-virtualizadas.
Estes resultados demonstram que todas tecnologias baseadas em contêineres atingem um uma latência equivalente à uma máquina nativa sem virtualização, impactando minimante em aplicações HPC to tipo &quot;message passing».
Assim, independentemente da tecnologia que será escolhida para suportar a AP, neste instante, o requisito Comunicação é satisfeito.
Esta seção apresenta uma análise de desempenho entre as tecnologias de virtualização para aplicações HPC.
Para isso, foram realizados experimentos utilizando o benchmark NPB.
NPB é derivado de aplicações da Dinâmica de Fluidos Computacional (CFD) e consiste de cinco núcleos (Is, PE, AC, MG, FT) e de três pseudo-aplicações (BT, SP, Lu).
As características e o padrão de utilização de cada benchmark são descritos a seguir:
BT (Block Tridiagonal):
É um benchmark que resolve sistemas de equações, resultantes de uma diferença finita aproximada do método de discretização das equações de Navier--Stokes.
Devido a a grande quantidade de multiplicações entre matrizes, o BT faz uso intensivo da memória cache.
CG (Conjugate Gradient):
É um benchmark que usa o método do gradiente conjugado para calcular uma aproximação para o menor valor de uma grande matriz simétrica positiva.
Representa cálculos típicos sobre grades não estruturadas, que são conhecidos por ter uma comunicação irregular ao longo de o tempo.
EP (Embarrassingly Parallel):
É um benchmark que fornece uma estimativa de desempenho máximo para análise de cálculo de ponto flutuante.
É normalmente usado para avaliar o desempenho sem o custo significativo de comunicação entre processos, portanto, um benchmark ideal para realizar uso intensivo de CPU.
FT (Fast Fourier Transform):
É um benchmark que consiste de uma equação diferencial parcial que utiliza a transformação tridimensional Fast Fourier.
O FT fornece um teste rigoroso de desempenho de todos- para- todos processos numa comunicação, tornando- o muito sensível à latência de rede.
Is (Integer Sort):
É um benchmark que executa operações de ordenação sobre um grande número de inteiros.
O Is realiza acesso aleatório à memória e testa o desempenho de ambos, cálculo de valores inteiros e comunicação entre processos.
Lu (Lower--Upper Gauss-Seidel):
É um benchmark que realiza fatoração de matrizes, sendo um produto de uma matriz triangular inferior e uma matriz triangular superior.
O Lu realiza comunicações complexas entre processos.
É um benchmark muito completo para HPC e é semelhante ao utilizado para construir a lista Top500.
MG (MultiGrid):
É um benchmark que realiza uma solução aproximada para a equação tridimensional de Poisson usando o método V--Cycle.
O MG é um benchmark que realiza uso intensivo da memória.
O primeiro experimento realizado teve como objetivo avaliar a sobrecarga de desempenho das tecnologias de virtualização num único nodo.
A Figura 4.14 mostra os resultados para cada NPB benchmark utilizando a sua implementação OpenMP.
Em todos os casos, as tecnologias baseadas em contêiner obtiveram tempos de execução muito próximas da máquina nativa.
No entanto, entre as tecnologias baseadas em contêiner avaliadas, o OpenVZ obteve o pior desempenho, especialmente para os benchmarks BT e Lu.
Este resultado é devido a os muitos acessos à memória que estes benchmarks realizam, resultando em muitos cache misses 2 e algumas tecnologias baseadas em contêineres, como OpenVZ, possuem blocos de memória cache limitados.
Além disso, o Xen mostra novamente o problema de double-cache, como descrito na Secção 4.10.
Os resultados do Xen mostram que este só foi capaz de atingir o desempenho perto de o nativo para os benchmarks que realizam uso intenso da CPU, como EP, FT e Is.
Em um único nodo, as diferenças não são tão expressivas, com algumas diferenças apenas nos benchmarks que usaram intensamente a memória cache.
Quando se avalia um ambiente com diversos nodos, a influência da rede pode ser notado como a principal métrica a ser discutida, já que a rede tem um impacto direto em isolamento.
Para avaliar os melhores resultados de isolamento entre as diferentes tecnologias, foi utilizado o conjunto de benchmarks IBS, que demonstra o quanto uma tecnologia de virtualização pode limitar o impacto de uma instância com outra instância em execução na mesma máquina.
Alertase aqui novamente que isolamento não consta nos objetivos do trabalho, no entanto, estes testes foram incluídos no intuito de enriquecer a contribuição da avaliação das tecnologias e num trabalho futuro, viabilizar o uso do conceito de workspace em ambientes de clusters com nodos simultaneamente compartilhados.
Primeiramente, foi coletado o tempo de execução de uma aplicação Modelo numa das instâncias.
A aplicação Modelo utilizada foi a pseudo-aplicação Lu do conjunto de benchmarks NPB.
Depois de coletar a medida de tempo de execução, a aplicação Modelo foi novamente executada, agora, no entanto, em ambas as instâncias.
Com a aplicação em execução, aplicou- se os testes de estresse numa de elas.
Coletou- se novamente o tempo de execução da aplicação Modelo da instância a qual nenhum teste de estresse está sendo executado.
A métrica do IBS foi obtida comparando o tempo de execução da aplicação Modelo coletado enquanto nenhum teste de estresse estava sendo executado contra o tempo de execução da aplicação obtida enquanto os testes de estresse foram realizados.
Por fim, foi possível quantificar a degradação de desempenho da instância em a qual não houve estresse.
Como mostrado na Tabela 4.9, todas as tecnologias não obtiveram nenhum impacto nos testes de CPU.
Isso demonstra que as afinidade de CPU configurada por o cgroup, e a técnica Affinity VCPU usado na tecnologia OpenVZ estão funcionando corretamente.
No entanto, todos os outros recursos quando estressados tiveram algum impacto.
Conforme descrito na Seção 2.4, os contêineres presentes nas tecnologias baseadas em contêineres compartilham o mesmo sistema operacional.
Desta forma, supõe- se que, enquanto o kernel precisa lidar com as chamadas de instrução do contêiner estressado, ele é incapaz de lidar com as chamadas de instrução do outro contêiner.
VM/ Contêiner.
DNR significa que a aplicação foi incapaz de ser executada.
CPU Stress Memory Disk Stress Fork Bomb Network Receiver Network Sender OpenVZ VServer Xen Este comportamento pode ter influenciado o desempenho de todos as tecnologias de virtualização baseadas em contêineres enquanto os testes de memória, de disco e de rede foram realizados.
O teste fork bomb é um teste clássico, que consiste da criação de novos processos num laço até que não haja mais recursos disponíveis.
O teste demonstrou que existem falhas de segurança na tecnologia LXC, devido a a impossibilidade de limitar o número de processos utilizando cgroups.
Tanto o Linux- VServer, quanto o OpenVZ utilizam suas próprias implementações a fim de limitar o número de processos.
Como resultado, os contêineres não são afetados.
Os testes demonstram também que o Xen possui o melhor isolamento, devido a o sistema operacional não compartilhamento.
Esta seção apresentou uma avaliação entre as tecnologias de virtualização baseadas em contêineres como uma alternativa aos hypervisors no contexto de HPC.
O objetivo dessa avaliação foi identificar que tecnologia de virtualização é mais adequada para a AP proposta, que satisfaça seus requisitos.
Enfatizou- se as tecnologias baseadas em contêineres devido a os históricos das baixas sobrecargas de desempenho que estas tecnologias incorrem, direcionado a pesquisa em busca de aceitar a hipótese (H2).
Em esse sentido, verificou- se que todos as tecnologias baseadas em contêineres possuem um desempenho quase nativo de CPU, memória, disco e rede.
As principais diferenças entre elas reside na gerência dos recursos, resultando no isolamento e na segurança.
Enquanto o LXC controla seus recursos apenas por cgroups, tanto o Linux- VServer, quanto o OpenVZ implementam suas próprias capacidades para limitação de recursos, introduzindo ainda mais opções, como limitação no número de processos por contêiner, o que demonstrou ser uma contribuição importante para introduzir maior segurança.
Acredita- se que esta capacidade será introduzida no cgroups futuramente.
Uma avaliação acurada nos resultados de isolamento em ambientes compartilhados, revelam que todos as tecnologias baseadas em contêineres não estão maduras ainda.
A CPU demonstrou ser o único recurso totalmente isolado em ambientes desta natureza.
No entanto, todos as tecnologias avaliadas mostraram um fraco isolamento para memória, para disco e para rede.
Ainda que para ambientes HPC que normalmente não requerem alocação compartilhada de uma partição de um cluster para múltiplos usuários, como é o caso da AP proposta, este tipo de virtualização torna- se muito atraente, devido a os resultados de desempenho e das facilidades de gerenciamento.
Até ao momento, LXC demonstra ser a tenologia mais adequada para a AP proposta.
Apesar de não mostrar o melhor desempenho no NPB na avaliação de múltiplos nodos, seus problemas de performance são compensadas por as facilidades de uso e gerenciamento.
Ainda, o fato do LXC estar contido de forma mainline nos fontes do kernel do Linux, assegura que a tecnologia evolua linearmente com o desenvolvimento do kernel, evitando sua descontinuidade.
Fatores Considerados Este Capítulo apresentou a Arquitetura para Provisionamento de Ambientes de Alto Desempenho Customizados.
Foram destacados conceitos e técnicas, para os quais, tecnologias e ferramentas foram propostos para compor a AP propriamente dita.
Ainda, foi realizado uma avaliação em busca de identificar qual tecnologia de virtualização baseada em contêiner mais se enquadra no objetivo geral do trabalho, inserindo facilidades de gerenciamento com uma baixa sobrecarga de desempenho.
A tecnologia LXC foi escolhida para resolver os problemas de incompatibilidade, provendo workspaces customizados e inferindo minimamente no desempenho das aplicações.
Assim sendo, para um melhor entendimento de onde o workspace está inserido, bem como a tecnologia de virtualização e as ferramentas de gerenciamento, a Figura 4.16 ilustra a AP proposta nesse trabalho.
Uma boa proposta sempre vem acompanhada de uma boa avaliação.
Para isso, a realização de uma avaliação é essencial para identificar a verdadeira eficácia da AP proposta neste trabalho e verificar se a solução alcançou o objetivo proposto.
Este capítulo apresenta esta avaliação por meio de prototipação.
Prototipação é uma abordagem largamente utilizada na área de Engenharia de Software, baseada numa visão evolutiva do desenvolvimento de software, afetando o processo como um todo.
Esta abordagem envolve a produção de versões iniciais de um sistema futuro com o qual pode- se realizar verificações e experimentos, com intuito de avaliar algumas de suas características antes que o sistema venha realmente a ser construído, de forma definitiva.
Com o mesmo intuito, no entanto, num contexto diferente, utilizou- se a abordagem de prototipação para avaliar a AP proposta.
A diferença de contexto reside na entidade a qual se deseja avaliar.
Enquanto na Engenharia de Software o protótipo é construído para verificar se o software está aderente aos seus requisitos.
Em o âmbito desse trabalho, o protótipo será construído com o objetivo de verificar se a AP adere aos seus requisitos e se os objetivos propostos foram alcançados.
A Seção 5.1 apresenta a modelagem do protótipo.
A avaliação que ambiciona demonstrar a efiCiência da AP proposta por meio de as baixas sobrecargas de desempenho, e por a redução no tempo de provisionamento de um ambiente utilizando workspaces, portanto, é apresentada na Seção 5.2.
Prototipação Esta seção contribui com a modelagem e a descrição do desenvolvimento do protótipo que será utilizado para avaliar a solução proposta.
O protótipo foi modelado levando em consideração as funções definidas na API de Gerenciamento, quais sejam:
CreateCluster, destroyCluster, syncWorkspace e syncAllWorkspaces.
Para tanto, a modelagem do protótipo foi feita utilizado a Linguagem Unificada de Modelagem (UML), com a intensão de especificar por meio de diagramas todos os componentes que irão compor o protótipo, bem como a iteração entre eles.
A UML é uma especificação largamente utilizada, concebida para modelar, não só a estrutura, o comportamento e a arquitetura de uma aplicação, mas também os processos de negócios e as estruturas de dados.
Para uma boa organização, além de ambicionar novas soluções de aplicações que visam se apoiar na AP proposta neste trabalho, optou- se por contribuir com duas modelagens utilizando dois tipos de diagramas, quais sejam:
Diagrama de componentes e diagrama de sequencia.
Em UML, um diagrama de componentes descreve como os componentes de um sistema são ligados entre si para formar componentes maiores e/ ou sistemas de software.
Eles são utilizados para ilustrar a estrutura dos sistemas num alto nível.
Por outro lado, um diagrama de sequência em UML é um tipo de diagrama de interação que mostra como os componentes num sistema se comunicam entre si.
Basicamente, é um gráfico de sequência de mensagens, em o qual mostra interações entre diferentes componentes dispostos em sequências de tempo.
Acredita- se que com essas modelagens seja possível entender o funcionamento do protótipo propriamente dito, auxiliando no seu desenvolvimento.
As próximas seções apresentam as modelagens.
A estrutura do protótipo é composta de cinco componentes.
Tais componentes são ilustrados na Figura 5.1.
Como é possível observar, os componentes estão dispostos em diferentes entidades na AP.
Isso significa que haverá uma relação de confiança entre um e outro por meio de conexões remotas.
Os componentes worma-cd e worma-sync, foram especificados no intuito de auxilar nas tarefas de gerenciamento por parte de os gerenciadores de recursos, como também de viabilizar a técnica de sincronização incremental de dados.
O nome worma surgiu da união das palavras workspace com management.
O sufixo lib, cd e sync assimilam os componentes ao seu propósito:
Uma explanação sobre cada componente é apresentado a seguir:
Worma-lib: Este componente é uma biblioteca que fornece as funções da API.
Este componente tem por finalidade conectar os gerenciadores de recursos com a AP.
Assim, realizando as ações de gerenciamento, como criar/ destruir clusters e sincronizar workspaces nos nodos.
Worma-cd: Esse componente é o controlador da AP proposta, onde estão implementadas todas as regras de negócio da API.
O componente é um processo daemon em execução na máquina a qual partirá as ações de gerência, oriundas do componente worma-lib;
Worma--syncd: Esse componente está disposto nos nodos que constituem o cluster.
Uma vez que uma requisição oriunda do componente worma-cd chegue, este componente assegura que os workspaces locais estejam sincronizados com os workspaces no dispositivo de armazenamento, ao executar uma ação de gerência.
Para isso, o componente utiliza a ferramenta rsync, como descrito na Seção 4.2.
Libvirtd: Este componente é uma interface para o gerenciamento de domínios em hypervisors.
Em o contexto da solução proposta, esse componente é utilizado para realizar a gerência dos contêineres LXC, tornando possível criar/ destruir um contêiner;
Driver LXC: O Driver LXC é utilizado por o componente libvirt para se comunicar diretamente com o kernel do Linux;
A modelagem comportamental de um sistema ilustra a iteração entre seus componentes.
Desta forma é possível entender e observar todas as etapas de comunicação realizadas deste o início de uma ação de gerenciamento, até ela ser efetivamente executada.
Para tanto, foram modelados 4 diagramas de sequência refletindo as funções definidas na API.
Modelagem Comportamental da Função createCluster (Figura 5.2) A sequência de mensagens trocadas durante uma ação de criar um cluster de workspaces segue:
O Gerenciador de Recursos chama a função createCluster submetendo por parâmetro os dados de configuração do cluster de workspaces;
A biblioteca envia a mensagem &quot;Create Cluster «via socket TCP para o processo controlador dos workspaces.
Esta mensagem segue acompanhada dos dados de configuração do cluster (Tabela 4.2), submetido anteriormente por o Gerenciador de Recursos;
O processo controlador envia uma mensagem de sincronização de dados para o componente worma-sync.
A mensagem é enviada acompanhada do nome do workspace que deverá ser sincronizado com o dispositivo de armazenamento;
O componente worma-sync returna sucesso, caso o processo de sincronização incremental senha sido concluída com êxito;
O processo controlador, a partir de os dados de configuração recebidos, constrói os arquivos XML necessários para iniciar os contêineres em todos os nodos que compreendem o cluster que será criado.
Por meio de a função virConnectCreateXML, provida por a API libvirt, o controlador inicia um novo contêiner em todos os nodos, utilizando tais arquivos construídos.
Um exemplo de um arquivo construído neste instante de tempo pode ser observado no Trecho 5.1: O componente libvirtd returna sucesso, caso o contêiner tenha sido iniciado com sucesso:
O componente worma-cd returna sucesso, caso o cluster tenha sido criado com sucesso:
Modelagem Comportamental da Função destroyCluster (Figura 5.3) A sequência de mensagem trocadas durante uma ação de destruição de um cluster de workspaces segue:
O Gerenciador de Recursos chama a função destroyCluster submetendo por parâmetro o nome do cluster que será destruído;
A biblioteca envia a mensagem &quot;Destroy Cluster «via socket TCP para o processo controlador dos workspaces.
Esta mensagem segue acompanhada do nome do cluster que será destruído;
O processo controlador, por meio de a função virDomainDestroy, provida por a libvirt, destrói o contêiner em todos os nodos do cluster;
O componente worma-cd retorna sucesso, caso o cluster tenha sido destruído com sucesso;
Modelagem Comportamental da Função syncWorkspace A sequência de mensagem trocadas durante uma ação de sincronização de um workspace segue:
O Gerenciador de Recursos chama a função syncWorkspace submetendo por parâmetro o nome do cluster associado ao workspace que será sincronizado;
A biblioteca envia a mensagem &quot;Sincronizar Workspace «via socket TCP para o processo controlador dos workspaces.
Esta mensagem segue acompanhada do nome do workspace que será sincronizado;
O processo controlador envia uma mensagem de sincronização de dados para o componente worma-sync.
A mensagem é enviada acompanhada do nome do workspace que deverá ser sincronizado com o dispositivo de armazenamento;
O componente worma-sync retorna sucesso, caso o processo de sincronização incremental senha sido concluída com êxito;
A sequência de mensagem trocadas durante uma ação de sincronização de todos os workspaces segue:
O Gerenciador de Recursos chama a função syncAllWorkspaces;
A biblioteca envia a mensagem &quot;Sincronizar Todos Workspace «via socket TCP para o processo controlador dos workspaces;
O processo controlador envia uma mensagem de sincronização de dados para o componente worma-sync.
A mensagem é enviada acompanhada do nome de todos os workspaces que serão sincronizados com o dispositivo de armazenamento;
O componente worma-sync returna sucesso, caso o processo de sincronização incremental senha sido concluída com êxito;
O componente worma-cd retorna sucesso, caso o cluster tenha sido destruído com sucesso;
Para um melhor entendimento de como o protótipo se insere na topologia da AP proposta, ilustra- se na Figura 5.6 todos os componentes que concebem o protótipo.
O protótipo foi desenvolvimento utilizando a linguagem de programação sequencial C. A escolha da linguagem foi dada em razão de sua baixa sobrecarga de desempenho.
Ainda, o desenvolvimento de uma biblioteca utilizando a linguagem C, abre um leque de opções para os gerenciadores de recursos em clusters que, em sua maioria, são implementados em C, facilitando a integração.
O ambiente de programação utilizado para construir o protótipo propriamente dito foi um conjunto de ferramentas de desenvolvimento para C/ C+, fornecido por um ambiente de desenvolvimento integrado provido por a plataforma Eclipse.
Experimentos para Avaliação da Arquitetura Experimentos foram conduzidos no intuito de verificar se a AP proposta atingiu o seu objetivo geral.
Para atingir tal objetivo, especificou- se requisitos essenciais aos quais a AP se propôs satisfazer.
Os requisitos especificados na Seção 4 são novamente elencados aqui, todavia, na circunstância de como estes serão avaliados.
Customização: &quot;Os meios utilizados para prover ambientes customizados não devem impactar no desempenho das aplicações HPC dos usuários, de forma que o desempenho não seja afetado&quot;;
Para avaliar se o requisito Customização está aderente na AP proposta, foram realizados experimentos analisando o impacto de desempenho de aplicações HPC sobre clusters utilizando workspaces.
Comunicação: &quot;Os meios utilizados para prover ambientes customizados não devem interferir na latência da rede de comunicação&quot;;
Para avaliar se o requisito Comunicação está aderente na AP proposta, foram realizados experimentos analisando a taxa de rede, assim como a latência de rede em clusters utilizando workspaces.
Tais experimentos já foram conduzindo enquanto as tecnologias de virtualização foram avaliadas.
Os resultados foram apresentados na Seção 4.3.6.
Portanto, o requisito já foi considerado satisfeito.
Armazenamento: &quot;O tempo de provisionamento de um ambiente a partir de um workspace armazenado num dispositivo de armazenamento deve ser minimizado em comparação com as APs existentes&quot;;
Para avaliar se o requisito Armazenamento está satisfeito na AP proposta, foram realizados experimentos analisando o tempo de carga de um workspace, assim como a eficiência da técnica de Sincronização Incremental de Dados para um único nodo e para múltiplos nodos.
Por fim, foi analisado o tempo de provisionamento de um cluster utilizando workspace e suas técnicas e ferramentas de gerenciamento.
Interface de Gerência:
&quot;A interface de gerência deve ser capaz de estabelecer uma relação de confiança com os gerenciadores de recursos em clusters, sendo capaz de criar/ destruir ambientes a partir de um workspaces&quot;;
Para avaliar se o requisito Interface de Gerência está aderente na AP proposta, foi realizado uma Prova de Conceito utilizando uma pseudo-aplicação, simulando um gerenciador de recursos em clusters;
Avaliar o tempo de carga de um workspace torna- se essencial neste instante, pois esse tempo está fortemente relacionado com a efiCiência da AP como um todo.
Por exemplo, se o tempo de carga de um workspace for relativamente alto, o tempo de provisionamento de um cluster possivelmente vai será impactado, em conseguinte, o turnaround dos gerenciadores de recursos.
Desta forma a motivação para realizar este experimento vai em direção a justificativa necessária para os resultados obtidos no experimento relativo ao tempo de provisionamento de um cluster, que será apresentado mais adiante.
Achou- se necessário comparar o tempo de carga de um workspace, o qual utiliza uma tecnologia de virtualização baseada em contêineres, com uma imagem de máquina virtual utilizando uma tecnologia baseada em hypervisor.
A razão foi tentar identificar a real vantagem do uso deste tipo de virtualização, comparada as outras.
Além de demonstrar o impacto da AP proposta em relação as AP existentes.
Para isso, escolheu- se a tecnologia Xen, em virtude de esta estar inserida na maioria das AP existentes que foram estudadas, e permitir assim, realizar uma análise mais justa.
Para conduzir os testes, foi artificiado uma técnica que possibilitou medir o tempo de inicialização de uma instância.
A técnica baseia- se no seguinte pressuposto:
A o término do processo de inicialização do sistema de arquivos, a instância é declarada iniciada.
O processo de inicialização de sistemas operacionais Linux, resumidamente, é dado por as seguinte etapas:
O kernel é carregado em memória após o término da inicialização do hardware;
O kernel detecta e configura todos os dispositivos de hardware conectados a máquina Após detectar todos os dispositivos de hardware, o kernel monta o sistema de arquivos e inicia o processo pai (/sbin/init);
O processo init troca o estado do kernel para multitarefa e lê o arquivo /etc/inittab.
Esse arquivo define em que nível de execução (runlevel) o Linux inicializará a sequência de scripts que serão executados para inicializar todos os serviços O processo init executa o script /etc/rc.
D/ rc que recebe o nível de execução do init como parâmetro;
Local instrui o Linux sobre o que deve ser executado logo após iniciar todos os serviços, ou seja, é o último script que será executado no processo de inicialização do sistema.
Em este ponto, todos os dispositivos de hardware já estão iniciados e configurados;
O processo init executa processos mingetty para configurar os terminais e liberar o login;
Não será explanado os detalhes sobre a organização, nem sobre o conteúdo desses scripts, pois vai além de o contexto deste trabalho.
Workspace Xen Tempo de Inicialização (segundos) Portanto, a técnica consiste de duas etapas:
Obter a hora local da máquina hospedeira no instante de tempo que a instância começou a ser inicializada;
Obter a hora local na instância no instante de tempo que instância terminou de ser inicializada.
A etapa é realizada gravando o timestamp do relógio no momento que o comando de inicialização da instância é executado.
Para a etapa, no entanto, foi preciso construir um script com a finalidade de gravar a hora atual do sistema num arquivo.
Esse script, por sua vez, é chamado por o rc.
Local na instância.
Como pode- se perceber, o script é executado na etapa 7 do processo de inicialização do Linux, num instante de tempo em o qual os dispositivos de hardware, como rede, já estão iniciados, assim como, o sistema de arquivos já está montado.
Em outras palavras, a instância é declarada iniciada nesta etapa.
A métrica utilizada consiste do cálculo da diferença entre a hora local obtida na etapa e a hora local obtida na etapa.
Como ambas, a máquina hospedeira e a instância estão sobre o mesmo hardware físico, pressupõe- se que também estão utilizando o mesmo relógio.
Assim, está técnica torna- se confiável.
Diante de os resultados obtidos, o tempo de inicialização de um workspace foi de uma média de Xen foi de uma média de 11.85 segundos.
Infere- se neste instante que a tecnologia de virtualização baseada em hypervisor incorre de um tempo de inicialização de uma máquina virtual mais do que 6 vezes o tempo de inicialização de um workspace, o qual é sustentado por uma tecnologia baseada em contêiner.
A o observar as diferentes características entre uma arquitetura e outra, os resultados tornamse mais evidentes.
Enquanto a tecnologia baseada em hypervisor utiliza máquinas virtuais, que consolidam além de o sistema de arquivos, o kernel e os módulos.
A tecnologia baseada em contêiner, ao invés de isso, consiste apenas do sistema de arquivos.
O kernel, assim como os módulos, passam a ser compartilhados entre a máquina hospedeira e os contêineres.
Diante de essas observações, é notório ao atentar as etapas de inicialização do sistema operacional Linux, que as máquinas virtuais precisam percorrer etapas que vão deste a primeira até a oitava, em razão de a necessidade de inicializar o kernel.
Enquanto nos workspaces, por compartilhar o mesmo kernel com a máquina hospedeira, este já está inicializado.
Assim, o processo de inicialização de um workspace parte da etapa cinco, diminuindo a sobrecarga de inicialização.
Para verificar se a técnica de Sincronização Incremental de Dados é realmente eficiente e satisfaz o requisito Armazenamento, foram realizados testes de sincronização de workspaces entre o repositório local num nodo e o dispositivo de armazenamento.
Para realizar tal teste, não foi utilizada nenhuma das funções contidas na API, pois a API não possui suporte para este tipo de operação.
A operação foi realizada manualmente utilizando a ferramenta rsync, copiando dados entre um repositório e outro.
A métrica utilizada neste experimento foi o tempo de sincronização.
Para se obter um resultado comparativo, foi incluído no experimento imagens de máquinas virtuais Xen.
Assim, comparou- se o tempo de sincronização de um workspace contra o tempo de sincronização de uma imagem de máquina virtual.
Desta forma, foi possível perceber as reais vantagens de uso da técnica proposta.
Contudo, foi criado um workspace de tamanho 2.5 GB, bem como foi criado uma imagem de máquina virtual do mesmo tamanho, a fim de conduzir uma avaliação justa.
A Figura 5.9 apresenta os resultados do experimento.
Como foi possível perceber, o tempo de sincronismo de um workspace num único nodo foi relativamente baixo, comparando- o com o sincronismo de uma imagem Xen.
Enquanto o workspace foi sincronizado numa média de 2 segundos, a imagen Xen levou uma média de 28 segundos.
A razão desses resultados são explicados por a forma como ambos, workspace e imagem Xen, foram sincronizados.
Para sincronizar um workspace, o rsync apenas identificou os arquivos que foram alterados, para então sincronizar- los com os nodos.
Para sincronizar uma imagem Xen, no entanto, foi preciso realizar o sincronismo do volume inteiro da imagem.
Uma vez que uma imagem representa um arquivo binário, este será sempre inteiramente sincronizado com os nodos, mesmo quando houverem pequenas atualizações ou instalações.
Para avaliar a eficiência da técnica de Sincronização Incremental de Dados para múltiplos nodos, foi construído uma pseudo-aplicação com o objetivo de simular um gerenciador de recursos em clusters.
Tal aplicação foi lincada (l) na biblioteca provida por a API, fazendo uma chamada para a função syncWorkspace, implementada na API de gerenciamento.
Sua implementação é figurada por o Trecho de código 5.2 Uma vez que a função syncWorkspace sincroniza apenas workspaces, foi preciso adaptar a implementação do componente worma-sync, de tal forma que este fosse capaz de sincronizar imagens Xen.
Isso foi feito apenas no âmbito da avaliação.
O cenário de teste utilizado nesse experimento reflete o mesmo cenário utilizado no experimento para um único nodo, ou seja, um workspace e uma imagem Xen de 2.5 GB de volume de dados.
A diferença neste experimento reside na quantidade de nodos e na automatização da operação.
Enquanto anteriormente foi avaliado o sincronismo de um único nodo de forma manual, aqui será apresentado uma avaliação de sincronismo para múltiplos nodos (4 nodos, conforme a descrição do ambiente experimental) utilizando a função syncWorkspace provida por a API.
A figura 5.10 apresenta os resultados obtidos.
É notório que o tempo de sincronização de um workspace em múltiplos nodos foi maior do que o tempo de sincronização das imagens Xen em múltiplos nodos.
Isso dá- se em razão de o volume de dados trafegados, obtendo um resultado semelhante ao experimento num único nodo, no entanto, numa escala de sobrecarga de tempo maior.
O tempo de sincronismo de um workspace em 4 nodos simultaneamente foi de uma média de 9.9 segundos, de uma imagem Xen, no entanto, foi de uma média de 92 segundos Percebe- se que o tempo de sincronismo aumenta em razão de a quantidade de dados trafegados na rede de comunicação.
Em este caso, o gargalo na transferência de dados será na interface do dispositivo de armazenamento.
Quanto melhor for essa interface, espera- se que melhor será a eficiência da técnica de sincronização.
Em este contexto, percebe- se a escalabilidade conforme o aumento no número de nodos ao se utilizar a técnica de sincronismo incremental.
Uma vez que o volume de dados trafegados é menor enquanto um workspace é sincronizado, a técnica quando utilizada sobre workspaces, permite uma escalabilidade muito maior.
Em este contexto e com esses resultados, afirma- se que a técnica proposta é eficiente.
De a mesma forma que no experimento anterior, foi construído uma pseudo-aplicação, a qual assumiu o papel de um gerenciador de recursos em clusters.
Esta aplicação foi desenvolvida a fim de executar operações de gerenciamento.
Para tanto, foi preciso lincar (l) à biblioteca provida por a API à pseudo-aplicação.
Sua implementação é figurada por o Trecho de código 5.3 Trecho 5.3: O arquivo de configuração XML necessário para a criação do cluster, foi construído remetendo as configurações do ambiente experimental.
Em este experimento também foi incluído uma avaliação utilizando a tecnologia Xen.
Para viabilizar tal avaliação, foi preciso alterar o driver utilizado por a libvirt.
Essa alteração foi feita trocando o driver de acesso de LXC para Xen.
Ainda, a fim de comparar os resultados obtidos durante os experimentos com uma solução existente e enriquecer a análise, foi incluído o tempo de provisionamento relatado por os autores da AP CoD.
Os resultados obtidos foram comparados e podem ser observados na Figura 5.11.
Percebe- se, ao avaliarmos os diferentes resultados obtidos, que a AP proposta obteve o melhor tempo de provisionamento, uma média de 12 segundos, enquanto que a AP utilizando o Xen obteve uma média de 101.
O motivo torna- se mais expressivo ao observarmos os resultados obtidos nos experimentos anteriores.
Uma vez que a AP proposta obteve o melhor desempenho ao se utilizar as técnicas propostas, tal desempenho refletiu no resultado do tempo de provisionamento.
Diante deste resultado, é possível afirmar que o tempo de turnaround em gerenciadores de recursos utilizando a AP proposta pode sofrer um impacto de 12 segundos, se utilizado sobre as mesmas circunstâncias.
No entanto, esse tempo pode ser reduzindo se a mídia de transporte entre o dispositivo de armazenamento for mais eficiente, ou ainda, se o protocolo utilizado na comunicação entre o dispositivo os nodos também for mais eficiente.
Em o intuito de analisar as penalidades de desempenho das aplicações HPC ao utilizar workspaces.
Além de verificar se a AP atende o requisito Customização de fato, foram realizados testes utilizando o benchmark NPB em sua versão MPI.
Este trabalho teve como objetivo propor uma Arquitetura para Provisionamento de Ambientes de Alto Desempenho Customizados, que atenda a necessidade de usuários em termos de seus requisitos de software de forma eficiente.
Além disso, teve- se como objetivo prover uma biblioteca de gerenciamento de tais ambientes, de forma a integrar- la em gerenciadores de recursos em clusters.
Primeiramente, procurou- se na literatura por conceitos e técnicas que auxiliassem na sua concepção, além de identificar desafios e problemas ainda não resolvidos nas APs existentes.
Em seguida, foram elencados requisitos essenciais que a AP precisava satisfazer, em busca de enfrentar tais desafios e alcançar o objetivo geral do trabalho.
Tais requisitos foram endereçados em 4 pilares, quais sejam:
Customização, Comunicação, Armazenamento e Gerenciamento.
Para alcançar um resultado que satisfaça tais requisitos, utilizou- se o conceito de workspaces customizados e suas técnicas de gerenciamento propostas, tais quais sustentados por uma tecnologia de virtualização e por ferramentas de gerência, respectivamente.
De esta maneira, as seguintes Questões de Pesquisa foram respondidas:
Qual o impacto da AP proposta em relação as APs existentes, levando em consideração as técnicas utilizadas para prover ambientes que atendam a necessidade de usuários tem termos de seus requisitos de software?
Considerando as dificuldades encontradas para prover ambientes desta natureza.
A AP proposta considera o conceito de workspaces customizados.
Tal customização foi auxiliada por o uso da ferramenta chroot.
Além disso, foi utilizado uma técnica para sincronização incremental de dados.
Contudo, os resultados obtidos se mostraram favoráveis em razão de o baixo volume de dados que os workspaces representam como também da viabilidade da sincronização incremental de dados em workspaces.
Qual o impacto da AP proposta em relação as APs existentes, levando em consideração o desempenho das aplicações e o tempo de provisionamento?
Considerando que as APs existentes incorrem em penalidades de desempenho devido a o uso de uma tecnologia de virtualização baseada em hypervisor para sustentar ambientes customizados.
A AP proposta sustenta o conceito de workspace por meio de uma tecnologia de virtualização baseada em contêiner, na ambição de inferir minimamente no desempenho das aplicações HPC, bem como diminuir o volume de dados trafegados na rede.
Os resultados obtidos revelaram que a AP incorre numa ínfima sobrecarga de desempenho em aplicações HPC, bem como um baixo tempo de provisionamento de ambientes customizados, em comparação com as APs existentes.
Contribuições As principais contribuições deste trabalho são:
Avaliação das Tecnologias de Virtualização baseada em Conteineres:
Essa Dissertação avaliou todas as tecnologias de virtualização baseadas em contêineres para Linux no contexto de HPC.
Até o presente momento, nenhum outro trabalho tinha realizado tal avaliação.
Arquitetura para Provisionamento de Ambientes Customizados de Alto Desempenho:
Essa Dissertação propôs uma alternativa para os problemas e desafios enfrentados nas APs existentes, relacionados com penalidades de desempenho em aplicações HPC, bem como o tempo de provisionamento.
API de Gerenciamento:
Foi modelado e construído um protótipo para a integração da AP em gerenciadores de recursos em clusters existentes, por meio de uma API de gerenciamento;
Avaliação da AP proposta:
A AP foi avaliada, levando em consideração o desempenho das aplicações HPC, bem como o tempo de provisionamento de um cluster customizado.
A hipótese afirma que para enfrentar os problemas de gerenciamento relacionados com incompatibilidade entre ambientes e as aplicações, sugere- se o uso de técnicas e de tecnologias que viabilizem a customização desses ambientes.
A hipótese foi comprovada.
A hipótese (H2) afirma que para provisionar ambientes customizados em termos de requisitos de software de forma eficiente, sugere- se o uso da arquitetura de virtualização baseada em contêineres.
A hipótese foi comprovada.
Trabalhos Futuros Considerando os benefícios proporcionados neste trabalho, os seguintes tópicos carecem de um estudo em maior profundidade, apresentados em trabalhos futuros:
Realizar um estudo aprofundado em busca de amenizar ou contornar os problemas identificados nos testes de isolamento da tecnologia de virtualização LXC.
Assim viabilizando o uso da AP proposta em ambientes com nodos compartilhados, ou seja, mais do que um workspace em execução na mesma fatia de tempo;
Propor a funcionalidade de provisionamento cluster de workspaces remoto.
Percebeu- se durante a realização da Pesquisa, que o uso de workspaces pode viabilizar tal funcionalidade, dado seu baixo volume de dados.
Esta funcionalidade torna- se atrativa em cenários onde o número de usuários é extremamente elevado, dificultando o gerenciamento dos workspaces por parte de os provedores.
De esta maneira, o usuário é capaz de construir seu próprio workspace e utilizar- lo para provisionar clusters numa infraestrutura remota;
Considerando que a AP proposta possui um baixo tempo de provisionamento, que teoricamente, reflete num baixo impacto no turnaround dos gerenciadores de recursos.
Propõe- se num trabalho futuro a integração da AP num gerenciador de recursos por meio de a API;
