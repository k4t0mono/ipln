As técnicas de Business Intelligence (Bi) firmaram- se como grandes aliadas das organizações nas tarefas de transformar dados em conhecimento, apoiando a média e alta gestão na tomada de decisões.
As ferramentas de Bi em sua composição são fundadas em técnicas de gestão do conhecimento, tais como Data Warehouse (DW), OLAP (Online Analytical Processing), Mineração de Dados (Md), entre outras.
Em este contexto, observa- se que em muitos casos, projetos de Md acabam sendo inviabilizados por alguns fatores, tais como custo do projeto, duração e principalmente, a incerteza na obtenção de resultados que retornem o investimento despendido no projeto.
O presente trabalho busca minimizar os fatores acima por meio um diagnóstico sobre os dados, através de um algoritmo loaseado em Rough Sets Theory (Teoria dos Conjuntos Aproximados (TCA)).
O algoritmo desenvolvido, nomeado Rough Set App (RSAPP) objetiva criar um diagnóstico sobre os dados persistidos no DW, a fim de mapear quais atributos possuem maior potencial de gerar modelos de mineração mais precisos e resultados mais interessantes.
Desta forma, entende- se que o diagnóstico gerado por RSAPP pode complementar o processo de KDD (Knowledge Discovery in Database), reduzindo o tempo gasto nas atividades de entendimento e redução da dimensionalidade dos dados.
Em o trabalho se faz uma descrição detalhada acerca de o algoritmo implementado, bem como o relato dos testes que foram executados.
A o final faz- se uma análise empírica sobre os resultados a fim de estimar a eficácia.
De o algoritmo quanto a sua proposta.
Palavras Chave: Mineração de dados, Teoria dos Conjuntos Aproximados, Business Intelligence, Data Warehouse, Knowledge Discovery in Database, KDD.
O crescimento do Volume de dados aportados por sistemas de informação estabelece um cenário crítico para a captura e análise de informações estratégicas.
Uma Vez que o ambiente corporativo mostra- se cada vez mais competitivo, a gestão tática e estratégica necessita de subsídios de informação que os dê suporte as decisões executivas.
Em muitos casos, os dados estão persistidos em estruturas de armazenamento heterogêneas, tais como bancos de dados relacionais, arquivos, documentos, etc..
Este contexto levou a concepção de um tipo de banco de dados diferenciado, chamado Data Warehouse Esse banco de dados propõe a integração das diversas estruturas de dados da corporação num ambiente de persistência único, orientado à recuperação da informação.
Por deter essas características, possui alto performance no apoio a análise de grandes volumes de informações, de forma consolidada e integrada.
Sob o mesmo argumento da concepção dos DW, as aplicações OLAP (Online Analytical Processing) são ferramentas preparadas para a análise dos dados.
Geralmente, esse tipo de aplicação conecta- se ao DW e suporta diversas funções de análise.
Desta forma, o software OLAP fornece uma interface amigável, onde o usuário especialista do domínio pode manipular os seus recursos, e assistindo a informação consolidada sob diversas perspectivas.
Gráficos, tabelas pivô e relatórios analíticos são interfaces comuns desse tipo de aplicação.
As tecnologias de DW e OLAP são amplamente difundidas, através de soluções de software e serviços.
Normalmente, projetos de DW e OLAP são complementares, pois aplicações OLAP preferencialmente usam DW como origem dos dados, aproveitando- se de sua organização orientada a recuperação de informação.
Essas tecnologias fazem parte do processo conhecido como Business Intelligence (Bi), que essencialmente refere- se à prática de capturar, organizar e analisar de informações para suporte a decisão De fato, aplicações OLAP sobre estruturas de DW têm apoiado as organizações, subsidiando informação estratégica.
No entanto, em aplicações OLAP a percepção do especialista de domínio prende- se ás informações resultantes da manipulação dos dados, nos seus diferentes níveis de agregação.
Dados (Data Mining).
A Mineração de Dados (Md) é um processo exploratório interdisciplinar, que usa técnicas de aprendizagem de máquina, banco de dados e modelos estatísticos.
Basicamente, Md propõe descobrir conhecimento que está implícito nos dados, evidenciando padrões e associações.
Organizações de diferentes ramos de atividade têm buscado em KDD informações que agreguem Valor competitivo ao negócio.
No entanto, a longa duração dos projetos, alto investimento e principalmente a incerteza na obtenção dos resultados são fatores que em muitas Vezes inviabilizam projetos desse tipo.
Conforme, os sistemas de mineração de dados tem potencial de gerar milhares ou até milhões de padrões ou regras.
No entanto, nem todos os padrões são considerados interessantes.
O autor define algumas características para que os padrões e regras sejam considerados interessantes.
São elas:
Facilmente compreensível por humanos.
Válido para novos dados com algum grau de certeza potencialmente útil e novo.
Em suma, um padrão interessante representa conhecimento novo para os especialistas do domínio estudado.
Inserido neste contexto, este trabalho concentra- se na elaboração de um algoritmo para auxilio à Md sobre dados de DW, dando suporte no momento inicial de pré-processamento dos dados, quando o conhecimento de dominio ainda é superficial para seleção eficaz dos dados.
O algoritmo aqui proposto, chamado Rough Set App (RSAPP) faz uso da chamada Teoria dos Conjuntos Aproximados TCA (Rough, Sets Theory) para criar um mecanismo semiautomáico de diatgnóstico dos dados.
Com base nesse diagnóstico, tem- se a intenção de focar as tarefas de mineração sobre os dados com maiores chances de produzir padrões mais precisos e interessantes para a area de domínio, logo nas etapas iniciais do projeto de KDD.
Tal técnica pode ser utilizada numa prova de conceito ou anteprojeto, na tentativa de reduzir a dúvida de obtenção dos resultados significativos, podendo- se assim inferir com mais propriedade o retorno sobre o investimento do projeto de KDD.
Esta pesquisa da continuidade ao trabalho desenvolvido por, que por sua vez propôs uma técnica de indução e ranqueamento de árvores de decisão sobre modelos OLAP.
O trabalho aqui desenvolvido e a pesquisa de são frutos dos estudos do GPlN-PPGCC (Grupo de Pesquisa em Inteligência de Nepócios do Programa de Pós-graduação em Ciencia da Computação da PUCRS) e convergem na intenção de criar mecanismos de avaliação prévia dos dados, que possibilitem a indicação de tarefas de mineração mais assertivas em projetos de KDD sobre DW.
O presente capítulo faz uma breve revisão da bibliografia acerca de os principais temas relacionados a pesquisa.
Primeiramente discorre- se sobre os bancos de dados Data Warehouse e sua modelagem dimensional.
Em a segunda seção faz- se um apanhado sobre os conceitos que envolvem KDD, detalhando cada etapa deste processo.
Por fim, examina- se a Teoria dos Conjuntos Aproximados (Rough Sets Theory) relacionando as suas definições e expondo um exemplo de seu calculo sobre um conjunto de dados sucinto, encontrado na bibliografia.
Em meados da década de oitenta surgem os primeiros DW para subsidiar a tomada de decisão sobre grandes volumes de dados.
Subsídios esses, que os bancos de dados existentes na época não atendiam com eficiência.
Desde sua concepção, o modelo usado em DW, chamado de Modelo Dimensional prioriza a recuperação de informação, diferentemente do modelo de padrão relacional (normalizado), que é orientado ao processamento de transações (Online Transaction Processing, OLTP).
Em este cenário, os DW consolidaram- se como técnica de armazenamento de informações analíticas e atualmente integra o processo de Bitsíness Intelligence (Bi).
Ralph Kimball e William lnrnon são importantes pesquisadores sobre o tema, suas pesquisas e métodos são usados como base da arquitetura de DW.
A abordagem de baseia- se essencialmente na criação de um banco de dados corporativo, usando um modelo dimensional integrado e singular.
Desta forma, montando uma fonte de informações apropriada com granularidade satisfatória, para suporte as decisões estratégicas.
Em linhas gerais, caracteriza DW como &quot;uma coleção de dados orientada por assunto, integrada, não volátil, Variante em relação a o tempo, que da apoio as decisões gerenciais».
E importante fragmentar este conceito e aprofundas- lo para compreender melhor o seu significado:
Orientado por assuntos:
Estruturado conforme a perspectiva que se quer analisar, ou seja, é organizado em assuntos de negócio.
Por exemplo, assuntos como estoque e vendas são comuns na concepção de DW.
O Integrado:
Com frequência os dados estão distribuídos em diversas plataformas, tais como arquivos, bancos de dados relacionais, entre outros.
Segundo o autor, faz parte das atribuições do DW organizar e integrar esses dados, independentemente de sua origem, visando obter uma visão única das informações.
Para tanto, faz- se necessário o uso do processo chamado ÉTL (Eztmct Transform Load).
Em a Subseção 2.1.1 faz- se uma breve descrição sobre o processo de ÉTL.
Adptado de· Variante no tempo:
Os dados persistidos em DW devem possuir um horizonte temporal significativamente maior que os dados armazenados em bancos relacionais (OLTP).
Assim, por padrão o DW armazena o histórico temporal dos dados.
Essa característica torna possível que as informações sejam analisadas sob o prisma temporal, por exemplo, hora, dia, mês, ano, etc..
Define uma abordagem ligeiramente diferente de.
Entretanto, ambos convergem na maioria das suas definições.
Para, o DW deve ser concebido de forma fragmentada sobre estrututuras (tonceituais chamadas Data Maris.
Os Data Maris essencialmente remetem ao conceito de assuntos, sendo cada Data Mart a transcrição de um assunto de negócio específico.
Em suma, na abordagem do DW é formado por a união de projetos de Data Marts.
Em este contexto o autor argumenta que projetos de DW são evolutivos, ou seja, novos assuntos podem ser acrescidos ao DW ao longo de o tempo, desde que este esteja preparado para recebes- los.
Em a modelagem dimensional proposta por, o banco de dados é composto por dois tipos de entidades (tabelas), são elas:
Tabelas Fato e tabelas Dimensões.
As tabelas dimensionais comportam as informações qualitativas dos dados, já as tabelas de fatos armazenam as informações quantitativas.
Por exemplo, no caso de Venda de produtos, a quantidade e Valor Vendidos permanecem computados na tabela fato e as informações descritivas do protudo estão ilustradas típicas tabelas Fato (esquerda) e Dimensão (direita).
Adaptado de Para a modelagem dimensional permite duas abordagens distintas, são elas:
Star Schema (modelo estrela) e Snowflake (modelo floco de neve).
Tecnicamente essas abordagens definem como as tabelas se relacionam, ou seja, fornecem um desenho bem definido para as entidades de cada Data Mart.
O Modelo Estrela, conforme pode ser Visto na Figura 3 é simplificado, e ao contrário de a abordagem relacional (OLTP), não exige normalização.
Adaptado de A abordagem do Modelo Floco de Neve é um pouco mais complexa, permitindo alguma normalização das dimensões e relacionamentos entre elas, conforme pode ser visualizado na Figura 4.
Adaptado de nas Subseções a seguir faz- se um apanhado de conteúdos relacionados ao tema dessa seção.
Discorre- se sobre o processo de ETL e suas motivações, sobre granularidade dos dados e por fim, uma sucinta descrição das aplicações OLAP e a relção sua estreita com DW.
Segundo o processo de ETL basicamente consiste em extração deitura das informações dos bancos de dados de origem.
Em sua obra, destatrca que as ferramentas de ETL são fundamentais no projeto de DW.
Para esse autor, tais ferramentas são encarregadas de transformar os dados do formato operacional (OLTP) em dados &quot;informacionais», ou seja, organizatdos dimensionalmente.
A granularidade dos dados é o atributo que caracteriza o nível de detalhe ou resumo das informações de DW.
Em esse contexto, quanto mais detalhzxda estiver persistida a informação diz- se que a granulação é menor, do contrario, quando mais resumida, maior a granulação.
Segundo a granuraridade é aspecto muito importante na concepção de projetos de DW, pois tem um reflexo direto no custo do processamento das consultas, bem como o volume de dados armazenados.
Sendo assim, a escolha da granularidade deve levar em conta o custo e também a necessidade do negócio envolvido, ou seja, qual o nível de detalhe que se deseja observar a informação.
Desta forma, evitando desperdícios de recursos computacionais e o descontentamento do cliente.
Adaptado de Em o exemplo acima se tem a comparação de registros de ligações telefônicas de um cliente em diferentes níveis e granulãção.
À esquerda da figura os dados estão detalhados por registros individuais de ligações no período de um mês, somando 40.000 bytes, representando baixo nível de granularidade.
A direita da figura tem- se os mesmos registros, porém de forma sumarizada, abstraindo os detalhas individuais de cada ligação.
As habituais operações de sistemas de informação, tais como, criar, modificar e recuperar informações no detalhe elementar são baseadas em OLTP.
Por exemplo, uma operação de saque em caixa eletrônico, ou ainda, uma transação individual de venda são típicas funções deste tipo de aplicação.
Tais operações são eficientes no contexto operacional, porém deixam a desejar quando a necessidade é recuperar e apresentar informações de apoio a decisão estratégica sobre grandes volumes de dados.
Em este cenário que as aplicações OLAP protagonizam.
Uma consulta de OLAP pode acessar um banco de dados com gigabytes ou terabytes de informações sobre vários anos de vendas a fim de encontrar todas as vendas de produtos em cada região para cada tipo de produto.
Após revisar os resultados, um analista pode refinar ainda mais a pesquisa com o objetivo de encontrar o volume para cada Canal de vendas dentro de uma região, ou dentro de determinadas classificações do produto.
Urna última etapa pode envolver o analista executando comparações entre anos ou trimestres para cada canal de vendas.
O processo todo pode ser executado online, com um tempo rápido de resposta para que o processo de análise ocorra sem interferências.[
TURDSL página 109.
Para OLAP caracteriza- se por o processo dinâmico de criar, gerenciar e produzir relatórios analíticos sobre os dados da organização.
Dados estes que podem ser oriundos de DW ou cubos multidimensionais.
Segundo este autor, uma vez que tais ferramentas permitem que o próprio usuario possa customizar de forma ágil as suas consultas, o subsidio de informação se torna dirigido.
Desta forma é comum a busca por respostas de questões de negócio, padrões e tendências indicadas por medidas de desempenho.
As ferramentas OLAP são baseadas na arquitetura de dados multidimensional.
Comumente essas aplicações são nomeadas como &quot;Cubos de Dados «(uma referência a dimensionalidade de um cubo).
Os cubos multidimensionais são assim chamados por permitir a análise de fatos nas suas múltiplas perspectivas (dimensões).
A análise das informações sobre cubos são conduzidas por as funções chamadas roll-up, drill-down, slícing e dicing.
As funções roll-up e drill-doum, remetem ao conceito de &quot;hierarquias «dimensionais.
Essas hierarquias são mapeadas no cubo para que a informação possa ser explorada dinamicamente, navegando entre os diferentes níveis de detalhe.
Teste caso, a função drill-- dowvz aprofunda a análise de uma determirrada medida (fato) para um nível mais detalhado e a função roll-up retorna ao nível resumido da dimensão analisada.
Em ambos os casos, quando essas funções são aplicadas é papel do cubo agregar ou detalhar a informação nas categorias da dimensão.
Por exemplo, é possível usar uma hierarquia da dimensão de tempo para observar a informação de Vendas por trimestre e dinamicamente, através da função drill-down chegar ao detalhe de mês.
Em este caso, obviamente a aplicação distribui o valor de venda de cada trimentre nos seus respectivos meses de venda.
De forma análoga, um novo estímulo de detalhamento sobre o mês poderia detalhar as vendas por dia.
Técnicamente essas funções quando aplicadas sobre o DW são representadas através de consultas que utilizam a cláusula GROUP By.
As operações slicing e dícíng são usadas para selecionar uma parte especifica do cubo, filtrando assim os dados de uma ou mais dimensões em algum nível hierárquico específico, neste caso, obtem- se um subcubo de dados parciais.
A Figura 6 abaixa baseada em ilustra a multidimensionalidade conceitual de OLAP bem como a sua representação hierárquica.
Adaptado de Em a reprentação acima o cubo OLAP é composto por quatro dimensões, sendo tempo.
Localidade. Cliente e produto.
Desta forma, o fato (Vendas) pode ser Visualizado através do prisma dessas quatro dimensões e detalhado em seus níveis de hierarquia, usando as funções roll-up/ dTíll-dorxm.
Organizações dos mais diversos nichos procuram manter suas informações armazenadas em DW a fim de posteriormente utilizar tais recursos como subsídio para decisões estratégicas.
Em este sentido, o processo de KDD é utilizado na descoberta automática de conhecimento sobre grandes volumes de dados, uma vez que capacidade de análise manual se torna inviável.
Por exemplo, utilizando um banco de dados de uma empresa de bens de consumo, através de KDD é possível traçar correlações de vendas de certos grupos de clientes e então usar esse conhecimento para introduzir propagandas e novas ofertas direcionadas.
O valor do volume de armazenamento de dados depende da nossa capacidade de extrair relatórios úteis, detectar eventos interessantes e tendências, decisões estratégicas baseadas na análise estatística e inferência, e explorar os dados como diferencial Competitivo de negócios, operacionais ou objetivos Científicos.
Basicamente KDD é composto por uma sequência bem definida de etapas que ao final buscam encontrar, automaticamente, informações úteis em grandes bases de dados, estabelecendo padrões, associações e previsões, que por sua vez passariam despercebidas sob a análise manual.
As etapas que compõem 0 processo do KDD são:
Seleção dos dados. (
2) pré-processamento, (3) transformação.
Mineração de dados e avaliação e interpretação dos resultados.
A Figura 7 de viazualizada logo abaixo demonstra a visão geral do processo de KDD.
Fonte Cabe estudar cada etapa do processo separadamente, em especial referências de pré-processamento e mineração.
Portanto, ao longo de as próximas Subseções faz- se um apanhado acerca de os estágios do processo de KDD.
Em esta etapa faz- se o levantamento de quais dados serão usados no processo de KDD.
Para tamto é necessário estudar as necessidades de negóico e quais dados podem estar relacionados com essas questões.
Em esse contexto, para é necessário que o domínio seja estudado, a fim de otimizar a seleção das informações.
Segundo as informações podem ser oriundas de plataformas diferentes, tais como bancos de dados relacionais, arquivos de dados, entre outras fomtes.
Para este autor os DW estão intimamente relacionados com o processo de KDD.
Segundo ele o uso de DW é conveniente, pois se faz uso da sua prévia preparação dos dados, bem como a sua orientação para consultas e modelagem dimensional.
Em este contexto, os dados de KDD são mantidos no DW, selecionados através do processo de ÉTL.
Segundo bancos de dados do &quot;mundo real «são altamente sucetíveis a ruídos, valores ausentes e inconsistências de dados.
Para o autor, a baixa qualidade dos dados implica diretamente nos resultados de mineração.
Embora extraídos por o processo de ETL e persistidos em DW (abordagem de HANm]), comumente os dados precisam ser pré-processados, eventualmente transformados e reduzidos para a execução das tarefas de mineração.
Esta etapa responde por grande parte do esforço e tempo gasto em KDD.
Mesmo que o investimento na preparação dos dados seja significativo, esse trabalho tem reflexo direto sobre a confiabilidade dos resultados obtidos.
Sugere algumas etapas de preparação para a mineração, que podem ser empregadas ou irão, dependendo do proBIema.
A seguir são detalhadas essas etapas.
Frequentemente os dados estão disponíveis num nível granular muito baixo e dependendo de o (raso é conveniente aumentar esta granularidade, processo semelhante ao empregado em ferramentas OLAP.
Quando as informações estão dispostas no nível elementar, tem- se uma Visão mais operacional desses dados.
Portanto, a fim de se obter resultados de menor nível de detalhe é possível usar a agregação.
Ainda, em condições normais o nível granular é proporcional ao volume de objetos a ser processado.
Portanto, em casos quando a tarefa de mineração demanda alto custo de processamento, o emprego de agregação dos dados pode reduzir esse custo computacional.
Tecnicamente atributos quantitativos são agregados aplicando- se funções de soma ou média.
Os atruibutos qualitativos podem ser simplesmente suprimidos ou podem ser substituídos por informações de maior nível granular na hierarquia física.
A agregação deve ser aplicada com cautela, visto que podem ser perdidos detalhes interessantes dos dados.
Por exemplo, no caso de uma agregação de vendas por mês abstaindo os datalhes de dia pode- se ignorar algum padrão relacionado às sazonalidades dos dias.
A amostragem é uma técnica estatística para seleção de um subconjunto dos dados a serem analisados.
No caso de a disciplina de estatística, frequentemente não é possível trabalhar com a totalidade dos dados por não ser possível coletar as informações de toda a população.
Em este caso então, busca- se um conjunto parcial dos dados chamado de &quot;amostra representativa «tanto para os experimentos preliminares quanto para as análises finais.
A amostragem também se mostra útil na mineração, no entanto, diferente da motivação estatística, em mineração de dados o estímulo é reduzir à complexidade computacional, visto que, em alguns casos o processamento de determinado algoritmo de mineração sobre um grande conjunto de dados pode ser impraticável.
A dimensionalidade representa as diferentes informações (características) que se tem sobre um mesmo objeto.
Conforme a dimensionalidade do conjunto cresce, normalmente os dados ficam mais dispersos e consequentemente é mais difícil de se obter padrões precisos.
A redução de dimensionalidade busca eliminar as características irrelevantes ou combinar duas ou mais dimensões para execução da tarefa de mineração, visto que tal redução pode levar a resultados mais compreensíveis.
Além disso, quanto maior a quantidade de dimensões do conjunto de dados, maior o esforço computacional do algoritmo minerador.
Desta forma, reduzem- se as dimensões com pouca ou nenhuma significância para a mineração.
Diferenciar quais dimensões tem maior potencial de gerar padrões classificaveis com maior precisão é uma tarefa que exige análise sobre os dados.
Em muitos casos, o técnico que esta executando as tarefas de mineração não ossui pleno conhecimento sobre o assunto dos dados.
Por esse motivo, com requência utilizam- se técnicas de redução.
Algumas técnicas conhecidas utilizam álgebra limear como base para suprimir dimensões, como por exemplo, a análise de Componentes Principais (PCA).
Quando ha conhecimento de domínio torna- se mais fácil suprimir redundancias e irrelevancias.
No entanto, em alguns casos o conhecimento de domínio não é suficiente para realizar esse trabalho, assim uma abordagem sistemática pode ser usada.
Uma abordagem poderia ser executar experimentos de mineração sobre todas as possibilidades de subconjuntos, e então selecionar os subconjuntos que ofereçam os melhores resultados.
Obviamente essa abordagem não é viável, pois sendo n atributos, haverá 2 «possibilidades.
Em este caso, existem três técnicas para seleção de subconjuntos, são elas:
Interna. Filtro e envoltório.
Interna: Em essa abordagem atribui- se a seleção de subconjunto ao algoritmo de mineração.
Em este caso não há pré-processamento, pois se deixa que o próprio algoritmo decida quais os atributos deve ou não usar para geração do modelo.
Filtro: Em este caso faz- se uma seleção antes do processamento do algoritmo, baseada num critério que o técnico ou especialista de domínio podem definir, como por exemplo, uma seleção baseada em correlação.
Envoltório: A seleção feita nessa abordagem baseia- se na aplicação de um algoritmo de mineração exclusivamente para seleção dos atributos a serem trabalhados.
Em este (taso a execução do algoritmo que irã indicar os melhores atributos para execução dos próximos experimentos de mineração.
Em algumas condições é possível construir novos atributos baseados em combinações.
Essa tecnica busca extrair as informações mais importantes através da junção, além de o ganho provido por a redução de dimensionalidade.
A criação de recursos baseia- se em três métodos:
Extração de características.
Mapeamento dos dados para novo espaço e construção de recursos.
Determinados algoritmos de mineração exigem que as informações sejam discretizadas 0m binarizadas.
Especialmente em tarefas de classificação o atributo classificador deve estar no formato categórico.
De forma similar, para execução de tarefas de padrões de associação as informações devem possuir formato binário.
Frequentemente a categorização não é uma tarefa triviatl, pois os atributos classificadores podem estar contidos numa grande escala de valores.
Assim, Valores que possuem pouca frequência teriam sua própria descrição.
Em este caso, convenientemente a escala de valor pode ser fragmentada em categorias que englobem um intervalo da escala e não todas as variações.
Por exemplo, valores contínuos de O a 100 podem ser enquadrados em categorias de pequeno, médio e grande.
Em a binarização a tarefa é transformar os atributos contínuos em atributos booleanos.
Em este contexto, verifica- se a escala do atributo que se quer binarizar e depois se cria novos atributos baseado nessa escala, transpondo cada valor no formato de coluna, neste caso, se o objeto se enquadra naquele atributo, então é verdadeiro, senão falso.
A tabela abaixo, de exemplifica o processo de binarização.
Em o exemplo acima, do atribuo de valor contínuo foi convertido em cinco novas propriedades, sendo, onde cada objeto foi identificado por O ou 1 em cada nova propriedade.
Existem diversas técnicas disponiveis para auxilio de discretização, pois uma vez que os atributos contínuos são aleatórios as categorizações devem manter a semãntica dos dados.
Em a bibliografia da disciplina de análise multivariada são propostas diversas formas de discretizaçá) de dados.
A transformação de variáveis (atributos) baseia- se na aplicação de um calculo sobre o valor elementar a fim de obter outro valor de referência.
Por exemplo, a conversão de uma unidade de medida, ou ainda, uma função de transformação de número absoluto para um número de referência.
Em este caso é possivel também combinar diferentes variáveis de um objeto para inferir outra variavel.
Por exemplo, tendo as informações das coordenadas geográficas de um objeto, pode- se transpor essas informações em categorias de localidade, como cidade, CEP, etc..
Embora muitas vezes tratada individualmente, a mineração de dados compõe o processo de KDD.
Esse processo combina técnicas de banco de dados, estatistica e aprendizagem de maquina.
Em o autor define mineração de dados como o tema essencial de KDD, onde algoritmos inteligentes são aplicados sobre os dados a fim de extrair novos padrões.
Segundo mineração de dados é um processo de descoberta automatica de conhecimento útil sobre grandes conjuntos de dados.
Para o autor, os algoritmos de mineração são construídos com a intenção de encontrar padrões irão evidentes nos dados e prever informações futuras, conhecimento esse que poderia não ser detectado sob análise convencional.
Segundo[ TUROSL mineração de dados é um domínio de análise de informações, fundamentado em banco de dados que podem prever comportamentos futuros.
Por exemplo, organizações de varejo podem usar os algoritmos de mineração para mapear classes de clientes com interesses comuns.
Para o autor, a mineração pode viabilizar a descoberta de novas e importantes correlações, padrões e tendências através de métodos estatísticos e de identificação de padrões.
Algoritmos de mimeração podem ser classificados em dois tipos:
Descritivos e preditivos.
Em este aspecto, as tarefas de mineração com abordagem descritiva propõem explorar propriedades de características que estão implícitas nos dados, para tanto, o modelo é orientado a um atributo alvo e explicado através dos demais atributos (condicionais).
Os algoritmos preditivos concentram- se em prever informações de um dado atributo a partir de os valores dos demais atributos disponíveis, sendo chamada de atributo alvo a informação a ser prevista e variáveis independentes ou explicativas aquelas usadas para operar a previsão.
As abordagens de mineração podem ser divididas em subcategorias, sendo classificação, regressão e detecção de anomalias as preditivas e associação e agrupamento as descritivas.
A Figura 8 mostra as diferentes técnicas de mineração.
Em as próximas subseções faz- se uma pequena explanação de cada técnica segundo.
Fonte Basicamente a tanto a classificação como a regressão buscam criar um modelo a partir de o atributo alvo em relação a os demais atributos do conjunto.
Em esse caso tem- se um conjunto de dados de amostra ou população e o objetivo é prever valores de alvo (classe) futuros ou simplesmente mapear as características que classificam esse atributo alvo.
A classificação e usada para execuções sobre atributos alvo categóricos e a regressão para atributos alvo do tipo contínuo.
Comumente os modelos baseados em classificação são representados por grafos chamados árvores de decisão, que mapeiam condições para enquadrar os objetos em urna categoria específica do atributo alvo.
Por exemplo, prever se um usuário de uma livraria online fara ou não uma compra.
Em este exemplo o atributo classe é a compra, que neste caso possui dois domínios (sim ou não), as demais informações como assunto pesquisado, tempo de pesquisa, cidade, entre outros, são informações que irão classificas- lo no modelo, identificando condições para enquadras- lo num potencial comprador ou não.
Em a classificação e regressão, para gerar o modelo o algoritmo primeiramente executa a etapa conhecida como indução sobre um conjunto de treino, onde os valores do atributo classe são conhecidos.
Logo após, esse modelo passa por o processo de dedução, onde é submetido a uma fração do conjunto de dados onde os Valores do atributo classe não são conhecidos, conjunto esse chamado teste.
A etapa de teste verifica o quanto o modelo conseguiu classificar os dados segundo a propriedade alvo.
A imagem abaixo exemplifica o processo de um classificador típico.
Adaptado de A detecção de anomalias busca objetos que apresentam características de exceção perante o conjunto dos demais objetos.
Em este contexto, aponta as informações que representam alguma anormalidade nos dados, ou seja, que estejam inconsistentes com os padrões dos demais objetos do conjunto.
Uma aplicação comum deste tipo de mineração é a detecção de fraudes, por exemplo, a fraude de compra com cartão de crédito.
Em esse exemplo, um modelo é criado a partir de o padrão de consumo do portador do cartão, assim cada compra é comparada com esse modelo e eventualmente, caso identificar um comportamento fora de o padrão tal transação pode não ser autorizada.
Esse tipo de algoritmo examina associações entre objetos, associações essas que não seriam detectadas empiricamente.
Os relacionamentos entre esses objetos são representados na forma de regras de associação.
Em este contexto o algoritmo examina os Valores de cada atributo e identifica os padrões mais frequentes.
Em muitos (tasos esses padrões podem ser óbvios, porém o objetivo das regras de associação é detectar de entre esses padrões, os mais interessantes e que intuitivarnente não seriam identificados.
Um exemplo comum de aplicação desse tipo de algoritmo é a associação de itens de cesta de supermercado.
Em esse caso, items que não possuem relação direta podem estar associados por a frequência que estão presentes nas cestas.
Em a análise de agrupamento os dados são distribuídos em grupos que possuem alguma similaridade.
Em este caso o algoritmo faz um enquadramento do objeto de entre os grupos disponíveis e posiciona- o no grupo que melhor o descreve.
Normalmente os agrupamentos são observados em graficos de dispersão.
Aplicações comuns para esse tipo de algoritmo são desde a descoberta de relações de padrões genéticos até identificação de comportamento de índices financeiros.
Cada técnica de mineração possui uma saída de formato específico.
Como é o caso do modelo de agrupamento, que pode ser Visualizado através de um grafico de dispersão, sendo sua precisão medida através do erro quadrático.
No caso de as técnicas de associação a saída é apresentada por as categorias mais frequentemente associadas dentro de o conjunto, acompanhada de medidas de aderência ao modelo gerado, como suporte e confiança.
As técnicas preditivas possuem ampla opção de algoritmos, sendo que alguns desses são capazes de gerar modelos através das arvores de decisão.
Em esse contexto a arvore classifica o conjunto de dados de forma intuitiva, sendo os nodos folha o resultado da classificação e os nodos acima representam os pontos condicionais de classificação da classe.
A Figura 10 demonstra um exemplo da árvore que classifica declarações de imposto de renda, sendo o atributo alvo a informação sonega (SIM/ Não) e os demais atributos que induzem o resultado.
Adaptado de Os algoritmos preditivos também possuem medidas que verificam a aderência do modelo em relação a o conjunto de teste, tais como:
Acuracia, taxa de erro, confiabilidade positiva, confiabilidade negativa, sensibilidade e especificidade.
Tais medidas são calculadas a partir de a matriz de confusão.
Essa matriz verifica os objetos que foram enquadrados (classificados) de forma correta e de forma errada através da dedução do modelo sobre o conjunto de teste.
A Tabela 2, baseada em representa uma matriz de confusão.
Em o quadro subsequente (Tabela 3) tem- se a formalização das medidas de aderência obtidas através da matriz de confusão.
A teoria dos conjuntos aproximados TCA (Rough Sets Theory) foi proposta por como uma alternativa as já existentes teorias que trabalham com conhecimento imperfeito, como Conjuntos Fuzzy, Redes Bayesianas, entre outras.
Segundo[ PAWSQL TCA pode ser entendida como uma ferramenta de alto nível para pré-processamento de dados a serem utilizados por algoritmos de aprendizagem de maquina, sendo empregada em diversos domínios onde ha dificuldade de análise dos dados.
A TCA loaseia- se na suposição de que o (tonherzimento esta associado a algumas informações do conjunto universo de dados.
Em esse contexto, os atributos caracterizados por a mesma informação são considerados indiscerníveis.
Essa relação indiscernibilidade que diferência os atributos é a base do cálculo matemático da TCA.
O conjunto de todos os objetos indiscerníveis é chamado de conjunto elementar.
A relação de indiscernibilidade de atributos constrói os conjuntos parciais chamados redutos.
Por sua vez os redutos são formados por atributos que mantêm as mesmas propriedades do conjunto universo.
Cada reduto é medido quanto a sua discernibilidade através da aproximação inferior e superior.
Em esse contexto a aproximação inferior consiste em todos os objetos que certamente pertencem ao reduto e a aproximação superior contém todos os objetos que possivelmente pertencem ao reduto.
Usando um exemplo transcrito de nas subseções a seguir parte do formalismo da TCA é explicado.
Exemplo de Aplicação TCA O exemplo de ilustra a aplicação de TCA sobre un1 sistema de informação de rotatividade de clientes telecomunicações.
A análise baseada em TCA inicia- se a partir de uma tabela de dados chamada &quot;tabela de decisão», onde as (tolunas são os atributos as linhas os objetos.
Os atributos da tabela de decisão são divididos em dois grupos.
São eles:
Atributos de condição e atributo de decisãol.
Em a Tabela 4 demonstrada abaixo se tem os elementos relativos a seis segmentos de cliente.
Acima tem- se os atributos:
&quot;In «que corresponde às chamadas recebidas.
&quot;Out «que representa as chamadas efetuadas para a mesma operadora.
&quot;Change «que corresponde às chamadas efetuadas para outras operadoras de telefonia móvel.
&quot;Chum «que descreve se o cliente foi mantido ou não (se Chum $= S então cliente não foi, se Chum $= N então cliente foi mantido), &quot;N «identifica a frequência de casos semelhantes.
Em suma, esse exemplo tenta idenfificar quais são os fatores mais relevantes para explicar a rotatividade de.
Em a TCA o atributo de decisão é a informação que se quer Classificar.
Fazendo uma associação à mineração de dados esta informação é representada por o atributo alvo.
Observamos a análise abaixo:
Segmentos de 2 e 3 (4 e 6) podem ser Classificados como· segmentos 1, 2, 3 e 5 podem ser (tlassificados corno conjuntos de clientes que possivelmente Chum 2 não;
Segmentos 1 e 5 são conjuntos indecidíveis de (zlientes.
Dada a análise acima temos a seguintes noção:
O conjunto 9,3] (f4, 6]) é a aproximação inferior do conjunto m2731 minute;
O o conjunto.()
é a aproximação superior do conjunto();
O conjunto.
É a região de fronteira do conjunto Em a próxima Subseção a aproximação superior/ inferior e região de fronteira são vistos em detalhe.
A fim de identificar as aproximações primeiramente defini- se o conjunto de dados, nesse contexto titulado como &quot;sistema de informação».
Um sistema de informação é definido como S 2 (UA), onde U e A são finitos e não vazio, sendo o U o conjunto de objetos de S e A o conjunto de atributos de S. Para todo atributo a E A associa- se um conjunto V», de seus valores, chamados de domínio da..
Para qualquer subconjunto de B de A aplia- se a relação de indiscernibilidade KB) em U, definido da seguinte forma:
Dy) E KB) se e somente se $= (Kg) para todo a E A, onde representa o valor do atributo a para o objeto m..
Portanto KB) é uma relação de equivalência.
O grupo de todas as classes de equivalência de KB) é denotado por U/ KB), ou simplesmente por U/ B. Uma Classe de equivalência de KB) é denotado por B Se (z, y) pertence a KB) diz- se que:
R e y são índíscerníveis em relação a B. Classes de equivalência da relação KB) são referidos conIO o conjunto B-elementares.
Para um sistema de informação S $= (U, A), X E U, e B E A defini- se duas operações atribuídas a Cada X E U. São os conjuntos B, e B* denomidados de aproximação B-infeíor e B-superíor, respectivamente.
Abaixo a formalização de ambos as operações:
Portanto a aproximação inferior de um dado conjunto é a união de todos de B-elementares deste conjunto.
Por outro lado a aproximação superior é a Linião de todos B-elementares que possuem intersecção não vazia com o conjunto.
A Figura u, adaptada de ilustra a noção de aproximação inferior e superior de S: (
UA) numa representação bidimensional de quadrados elementares.
Adaptado de Uma. Terceira região do conjunto, nomeada conjunto B-fronteira inclui os objetos que não podem ser enquadrados em X. Para tanto, aplica- se a diferença entre as aproximações (superior e infeioo, conforme formalizaçat) a seguir.
Cada subconjunto pode ser medido quanto a sua acurácia.
Em esse contexto tem- se as medidas de precisão de aproximação e qualidade de aproximação.
Em são demonstradas ambas as medidas.
Abaixo a representação das medidas adaptadas para a ilistração de.
Para X, por meio de os atributos de B define- se a precisão de aproximação:
A qualidade de aproximação de X para por meio de os atributos de B é definida conforme abaixo:
A qualidade de aproximação representa a frequência relativa dos objetos corretamente classificados por meio de os atributos de B. Em este capítulo apresentou- se a fundamentação teórica dos principais temas relacionados a este trabalho.
Buscou- se referências biBnográficas essenciais, bem como exemplos e ilustrações a fim de evidenciar o conteúdo da pesquisa.
Primeiramente, na seção 2.1 foram discorridos os conceitos de DW bem como as características de sua arquitetura e aplicação.
Tal fundamentação ancora- se principalmente nas obras de e.
Em a segunda seção (2.2) as referências de KDD foram esclarecidas detalhando cada uma das etapas do processo.
Em esse contexto, utilizou- se principalmente as obras de[ FAYQGL e.
Em a seção 2.3 fez- se urna descrição da Teoria dos Conjuntos Aproxitnados, usando como referência, e.
A pesquisa concentra- se em (zri-ar um algoritmo de préprocessamento baseado em TCA onde o resultado da execução forneça um diagnóstico sobre os dados.
O diagnóstico resultante provê uma base de conhecimento ao tecnico minerador para auxilias- lo na seleção dos dados, principalmente quando há carência de entendimento do domínio.
Os dados utilizados nesse estudo provêm de um DW de operação real, onde se focou num de seus data marts.
Ao longo de as próximas seções são detalhadas questões como a caracterização do proBIema, contribuição e o contexto de implementação da pesquisa.
Para a alta dimensionalidade e o entendimento dos dados são fatores críticos em projetos de KDD.
A grande quantidade de atributos pode levar a indução de modelos sobre informações que não são relevantes ao negócio.
Em este Cenário, o autor destaca a importância do conhecimento do domínio em todas as etapas de KDD.
Mesmo dispondo de padrões de projeto bem definidos, como por exemplo, CdSP-DM, segundo projetos de mineração até então estão sendo desenvolvidos mais como uma arte do que como uma ciência.
Para esse autor isso se deve ao fato de que se depende muito da experiência e do conhecimento que o técnico minerador tem sobre o domínio em questão, ou seja, o conhecimento que tem sobre os dados de forma que possa trabalhas- los e a capacidade técnica para interpretar os modelos gerados.
Segundo o conhecimento de domínio é crucial para o sucesso do projeto, principalmente na fase inicial (seleção e pré-processamento) e na fase final (análise dos resultados).
Portanto devem ser desenvolvidas ferramentas que auxiliem o técnico minerador a compreender o domínio estudado.
Os fatores acima tendem a aumentar o tempo e consequentemente o custo de projetos de mineração, independente do método utilizado.
Soma- se também o fato de que irão há como garantir que o valor' do conhecimento obtido supere o investimento despendido, uma vez que os dados podem ou não demonstrar novo conhecimento.
De forma empírica tem- se observado que a incerteza do retormo sobre o investimento em KDD por muitas vezes inviabiliza projetos desse tipo.
Encontra- se na literatura diversos estudos sobre o uso da TCA no pré-processamento dos dados, mais especificamente, para auxilio na redução de dimensiomalidade.
O trabalho de titulado «Dímensionality Reduction Based On Rough Ret Theory:
A Review «faz uma revisão das contribuições até então puBncadas sobre redução de dimensionzxlidade usando TCA, descrevendo resumidamente a proposta de cada trabalho.
Em esta revisão e possível perceber que grande parte das puBncações são propostas de variações do aparato matemático de TCA com vistas a resolução de algum tipo de proBIema, como por exemplo, o trabalho de, que faz uma análise sobre aquisição de conhecimento sobre conjuntos de dados incompletos.
Dada argumentação da seção 3.1 entende- se que quanto maior a compreensão do dominio, mais eficaz se torna o processo de KDD.
D0 ponto de Vista pratico, ao dominar o assunto sabe- se discernir os dados e, portanto a melhor forma de selecionar, pré-processar, reduzir dimensionalidade, analisar resultados, etc..
Também é sabido que o estudo do domínio tem un1a curva de aprendizagem.
Logo, a criação de ferramentas que auxiliem o técnico minerador a compreender os dados tende a tornar o processo mais efetivo, bem como minimizar o tempo/ custo de projeto.
Em essa direção, o presente trabalho apresenta um algoritmo baseado em TCA, nomeado Rough Set App (RSAPP), para geração de um diagnóstico sobre os dados.
Tal diagnóstico infere medidas de qualidade de aproximação de diferentes redutos.
Após a execução de RSAPP tem- se uma base de conhecimento, onde o técnico pode consultar quais combinações são mais representativas para uma determinado atributo alvo, ou ainda, quais atributos condicionais podem ser suprirnidos (redução de dimensionalidade) sem prejuízos.
O algoritmo aqui proposto pretende criar um mecanismo que providencie melhores resultados de mineração, logo nas etapas inicias do projeto, uma vez que, desta forma o técnico minerador dispõe de uma ferramenta que o induz a redução do universo de dados para uma dimensionalidade de maior relevância.
Cabe salientar que tal algoritmo não isenta o essencial estudo de domínio e a interação com o especialista, mas propõe servir como um artifício nesse processo de aprendizado.
O algoritmo implementado foi executado sobre um banco de dados OLAP a fim de identificar de entre as dimensões e suas hierarquias quais são os dados mais relevantes para mineração, ou seja, os dados com maior potencial de produzir modelos interessantes com precisão satisfatória.
Em suma, o diagnóstirto gerado por RSAPP pretende reduzir o tempo de projeto de KDD nas tarefas de entendimento dos dados e redução de dimensionalidade.
Com base na revisão de é possível verificar diversas frentes onde TCA é empregada para redução de dimensionalidade.
Porem se entende como diferencial dessa pesquisa as seguintes características:
Uso de um banco de dados OLAP com dados de operação real.
Apresentação dos resultados de RSAPP de forma intuitiva (diagnóstico) e(3) 2 desenvolvimento com uso de linguagem nativa de banco de dados (SQL).
Este estudo é fruto do GPIN (PPGCC-PUCRS).
Uma das frentes de pesquisa que o grupo tem trabalhado e a criação mecanismos de avaliação prévia dos dados, que possibilite a indicação de tarefas de mineração mais assertivãs em projetos de KDD.
Por exemplo, no trabalho de foi proposta uma técnica de indução e ranqueamento de arvores de decisão sobre modelos OLAP.
O presente trabalho se mantém nessa direção, ou seja, concentra- se em criar subsídios a fim elevar a eficacia do projeto de KDD.
A pesquisa foi desenvolvida sobre o DW de uma organização de grande porte que opera no mercado de transações eletrônicas multisserviços.
Este DW foi implementado em 20m e atualmente é utilizado por aplicações OLAP, auxiliando na tomada de decisão de diversas frentes, como por exemplo, O diagxióstico perznanece no lormatto de tnetadado no banco de dados onde RSAPP é executado, ou seja, não ó necessário outro tipo dc aplicativo além de o próprio SGDB.
OLAP na busca por informações que subsidiem a tomada de decisões estratégicas.
Em esse contexto o algoritmo RSAPP foi aplicado e segue o Íluxo de implementação ilustrado na Figura 12, abaixo:
Em a primeira etapa os dados são processados por o algoritmo, logo após o resultado é gravado em tabelas de metadados, feito isso o técnico minerador tem em mãos o diagnóstico e pode analisar quais as potencias combinações que possivelmente classífícanl melhor os dados, segundo o argumento da TCA.
A fim de testar a efetividade da ferramenta foram propostas algumas heurísticas, onde as combinações de atributos condicionais (redutos) processadas por RSAPP e suas respectivas medidas de qualidade de aproximação foram comparadas com a precisão dos modelos de classificação imdmzidos por a ferramenta de mineração.
Mais detalhes acerca de os testes realizados estão documentados no capítulo 5.
Em este capítulo foi apresentado o panorama da pesquisa.
Para tanto, na seção 3.4.1 foram referenciados alguns desafios de projetos de KDD.
Em esta seção procurou- se evidenciar os proBIemas enfrentados, onde foi percebida a essencial relação entre compreensão do domínio e eficacia do processo de KDD.
Em a seção seguinte (3.42) argumentou- se sobre a solução proposta a fim de justificas- la em relação a sua (íOlltdBIllçá) para a area.
Por último, na seção 3.4.3 fez- se uma breve descrição do cenário onde esta pesquisa esta inserida bem como resumo do seu desenvolvimento.
A pesquisa fez uso de um DW de uma empresa de grande porte.
Embora o processo de ETL não tenha sido implementado nessa pesquisa, faz- se necessario descrever os dados usados, quanto a sua modelagem e características, portanto na seção 4.1.1 essas informações são detalhadas.
Os passos&amp; 2 e&amp; 3 da Figura 13 representam o desenvolvimento de RSAPP propriamente dito, portanto na seção 4.1.2 são discorridas as etapas desse desenvolvimento.
O diagnóstico gerado por RSAPP é explicado pormenor na seção 4.1.3.
Entende- se que o diagnóstico gerado por RSAPP pode integrar o processo de KDD independente da metodologia utilizada, Visto que tal ferramenta posiciona- se como artifício de pré-processamento, por isso na Figura 13 3 uso de RSAPP é visto como uma etapa geradora de conhecimento para a sequência de processos de KDD.
Abaixo uma descrição breve descrição das propriedades de cada uma das entidades.
DIl\/ Í_ TEl\/ IPO_ CREDENC:
Dimensão de informações de tempo de quando o cliente foi credenciado, ou seja, quando determinado cliente passou a compor a carteira de clientes.
Atributos como ano e mes persistem nessa entidade.
DIM_ TEMPO_ FUNDACAO:
Dimensão de informações de tempo de fundação da empresa cliente.
Atributos como ano e mês persistem nessa entidade.
DIM_ LOCALIDADE:
Dimensão de informações de localidade do cliente.
Em esta estão contidas informações como região e UF.
DIM_ CLIENTE:
Dimensão de informações de cadastro de cliente.
Em esse contexto o cliente é um ponto de Venda.
Tal dimensão armazena informações como, natureza jurídica, classificação, entre outras informações cadastrais.
DIM_ PRODUTO:
Os clientes estão aptos a executar diversos tipos de transações.
Esses diferentes tipos de transações eletrônicas são identificadas como produtos.
Por exemplo, transações de pagamento com cartão de Vale refeição, ou ainda, recarga de celular pré-pago são típicos produtos.
Essa dimensão idenfica quais os produtos que o cliente esta apto a operar.
DIM_ REDE:
Essa dimensão distingue se aquele registro de fato, provém de um cliente que pertence ou não a uma rede de clientes.
No caso de pertencer, esta dimensão vai identificar qual a rede.
O DIM_ RAMO_ ATIVIDADE:
Dimensão de informação do ran1o de atividade do cliente.
Também nesta consta o subramo de atividade.
Um ran1o de atividade pode ser, por exemplo, alimentação, vestuário, etc..
No caso de o ramo de alimentação, pode haver subramos, como por exemplo, restaurante, cafeteria, etc..
Fato_ lNADll\ lPLENClA tabela fato onde estão computados os registros de inadimplência.
Tais registros identificam se o cliente encontra- se adimplente ou inadimplente em determinado titulo.
RSAPP foi aplicado a fim de identificar de entre as informações disponíveis, quais subconjuntos (redutos) podem produzir melhores resultados de mineração.
Em esse contexto, as dimensões possuem diferentes tamanhos (quantidade de atributos).
Abaixo (Tabela 5) são mostradas as quantidades de atributos de cada uma das dimensões e da tabela fato.'
A quantidade de atributos inlormada não inclui chaves primárias e estrangeiras.
Cabe salientar essas chaves foram desconsideradas no processamento de RSAPP.
QUALITl/_ APR A primeira etapa dOWER_ APP) é executada para extrair a aproximação inferior de cada um dos redutos.
O Cálculo da aproximação inferior é explicado em detalhes na Subseção 2.3.2.
Com base do resultado gerado por LOWER_ APP o procedimento QUALITl/_ APP é executado e então extrai a qualidade da aproximação de cada reduto.
O calculo da aproximação inferior é explicado em detalhes na Subseção 2.3.2.
Em a Figura 15 é representado o diagrama dos componentes e as suas interações.
Em as próximas Subseções faz- se a descrição detalhada do funcionamento de LOWER_ APP e QUALITl/_ APR Esse procedimento aplica diversas consultas sobre os dados.
Esses dados podem estar contidos numa míew ou numa consulta padrão (Seleci) inserida diretamente no seu fonte.
O procedimento LOWER_ APP recebe como parametros a consulta (ou nome da view) e a identificação do atributo alvo.
Com base nessas informações o procedimento cria diferentes subconjuntos (redutos) e calcula a aproximação inferior de cada combinação (conforme fórmula detalhada na Subseção 2.3.1.3).
A o final o procedimento grava os resultados numa tabela de metadado intermediária, chamada STEP_ A. Em o quadro abaixo o algoritmo do procedimento LOWER_ APP é descrito e comentado:
&quot;limpa tabela parcial do diagnóstico (Step/ a) delete from step_ a where 1 e 1;
commít: «loop para selecão de cada um dos domínios do atributo alvo trom/* nome da tabela ou viem/ l) loop from dual unlon all select column_ id trom sys.
All_ tah_ cols c where tahle_ name $ '/* nome da view ou consulta*/` and Columníname 'CLASS'* tdsscarta O atributo Classe dos redutos order by column_ id i loop escreve na variavel vwnere o valor do dominio do atributo alvo que sera usado neste loop vwhere:
&quot;testa se o reduto e vazio, senão incrementa ate haver os diferentes redutos formados it vnome is null then vnome col.
Column_ name;
&quot;escreve na variavel vquery a classe testada o conjunto escrito em vnome e a quantidade de objetos contidos na aproximação interior vquery:
E insert into step_ a select&quot;' Hrec.
O procedimento LOWER_ APP grava os metadados parciais na tabela de S TEILA.
O layou de STEP_ A e ilustrada na Tabela 7 conforme abaixo.
A tabela de metadados STEP_ A (ilustrada acima) serve de origem de dados para execução do procedimento seguinte (QUALITl/_ APPL onde sera calculada a qualidade da aproximação do subconjunto.
A qualidade da aproximação de TCA identifica quanto os redutos se aproximam do conjunto total.
O procedimento QUALlTl/_ APP implementa tal calculo (conforme fórmula detalhada na Subseção 2.3.2) e após grava os resultados na tabela de metadados STEP_ B, que por sua vez representa o diagnóstico proprialnente dito.
A Tabela 8 de dados figllrados demonstra a disposição do reslllado final de RSAPP, no formato de diagnóstico.
Oracle Database: Sistema de gerenciamento de banco de dados, versão ug, release 2.
Provido por Oracle Corporation;
Oracle SQL Developer:
Ferramenta de administração de SGDB, versão 2.0.04.
Provida por Oracle Corporation;
WEKA: Ferramenta de mineração de dados, versão 3.6.4.
Provida por Waikato University[ WEKIQL Em este capítulo os dados utilizados na pesquisa foram contextualizados.
Também neste fez- se a descrição do algotitrno RSAPP, detalhando a sua arquitetura bem como o seu código fonte e o layout do seu resultado.
Por fim foram referenciadas as ferramentas utilizadas na construção de RSAPP.
O diagnóstico gerado por RSAPP sobre os dados usados nessa pesquisa pode ser visualizado integralmente no Anexo A. Para aferir os resultados gerados por o diagnóstico RSAPP em relação a sua proposta, aplicou- se uma série de tarefas de mineração sobre o mesmo conjunto de dados onde RSAPP foi executado.
Foram usadas conIO medidas comparativas a acurácia dos modelos de mineração e a qualidade de aproximação do diagnóstico.
Para as execuções de mineração utilizou- se 4 algoritmos, ou seja, os 190 redutos foram submetidos aos quatro algoritmos de mineração a fim de apurar a acuracia de cada um.
Adicionalmente foi criada a métrica de média dos quatro algoritmos.
O Anexo B demonstra os resultados das tarefas de mineração.
Em as próximas seções são descritos os testes e as respectivas referências aos anexos, onde estão documentados por completo.
O conjunto de dados onde RSAPP foi aplicado possuía vinte atributos, sendo um alvo e dezenove atributos condicionais.
RSAPP gerou 190 redutos, com tamanhos que variam de 1 até 19 atributos condicionais.
Os resultados dos testes de mineração podem ser observados por completo no Anexo B. Para comparar a qualidade de aproximação de um reduto com a acurácia deste reduto, os resultados foram plotados aqui graficamente.
Em esse contexto a qualidade de aproximação gerada por RSAPP foi ordenada deçrescentemente.
Logo, a análise deve voltar- se a verificar se a curva dos resultados de açuracia acompanha ou não os resultados de qualidade de aproximação inferidos por o diagnóstico.
O eixo à esquerda representa a métrica de qualidade de aproximação apurada por o diagnóstico.
A direita tem- se média de acurácia das execuções de mineração dos quatro diferentes algoritmos.
Em o çaso acima pode ser visto que a acurácia da mineração teve uma curva alinhada com o resultado da métrica de qualidade de aproximação do diagnóstico.
Em o reduto 14 observa- se uma pequena oscilação da acurácia, que não esta refletida na qualidade de aproximação.
Porém essa oscilação é bastante pequena.
Em suma, no gráfico fica evidente que, para redutos dessa cardinalidade, o algoritmo conseguiu identificar quais atributos gerariam modelos de mineração mais precisos.
Ao longo de as próximas subseçõ é demonstrado cada um dos gráficos comparativos, sendo Cada um desses a..
Referência de uma cardinalidade.
De modo geral os graficos mostram o diagnóstico provê uma referência sobre a acuracia.
Embora as duas medidas estejam configuradas em unidades diferentes, verifica- se que a curva de acuracia decresce em todos os casos.
Observando em detalhe o Anexo B é possivel ver que há muitos casos em que um reduto de menor cardinalidade alcança maior acuracia que omtro de maior cardinalidade.
Embora esses casos sejam contraintuitivos, são reais.
De n1odo geral entende- se que quanto maior o subconjunto de dados, maior sera a sua acurácia.
Porém, uen1 sempre isso acontece e é justamente nesse cenário que RSAPP ajuda o minerador, na escolha de um subconjunto (reduto) ótimo, ou seja, aqueles redutos com maior acuracia e menor custo computacional.
Vejamos um exemplo do Anexo B. Um dos redutos de cardinalidade 9 pontuou 0,21 de qualidade de aproximação e um dos redutos de cardinalidade 14 pontuou 0,17.
O reduto de tamanho 9 obteve aouracia média de 76% e o de tamanho 14 de 63%.
Em esse caso, o reduto menor possui custo computacional inferior e acuracia superior ao reduto de cardinalidade 14.
Com o diagnóstico em mãos o técnico tem subsídios de balancear a cardinalidade versus a qualidade de aproximação.
Em essa seção fez- se uma heurística diferente da seção anterior, a fim de verificar o quanto o diagnóstico RSAPP pode (aontribuir na indic ção de um subconjunto de dados mais promissor para tarefas de mineração.
Para tanto, foram identificados os &quot;melhores «e &quot;piores «redutos segundo a qualidade de aproximação do diagnóstico RSAPP, para cada cardinalidade.
Feito isso, aplicom- se tarefas de mineração a fi1n de obter a acurácia desses redutos.
A o final, calculou- se a variação de acurácia entre o melhor e o pior reduto.
Vejamos o um dos casos da Tabela 9, omde as variações estão dispostas.
De os redutos de cardinalidade 10, o reduto que tem a melhor qualidade de aproximação pontuou 0,39 e o reduto com pior qualidade de aproximação pontuou 0,08 (como pode ser Visto no Anexo A).
Em esse caso obteve- se uma Variação 23,5663%.
Ou seja, para o melhor reduto de cardinalidade 10 se obteve acurácia média de 75,4104% e para o pior reduto de mesma cardinalidade se obteve 61,0283% de acurácia média.
Portanto, um ganho de 23,5663% de variação entre os dois redutos.
Em este caso, e nos demais, o diagnóstico mostrou- se eficaz, pois houve uma variação considerável entre o melhor e o pior reduto.
Em o Anexo C essa heurística pode ser vista por completa, para (zada um dos quatro algoritmos de mineração utilizados.
Em este capítulo foram discorridos os resultados dos testes de mineração, bem como a relação da acurácia obtida nesses testes com a qualidade de aproximação apurada por RSAPP.
Duas heurísticas foram usadas para avaliar a relação entre a qualidade de aproximação e acuracia de mineração.
Os resultados apontados nas seções 5.1 e 5.2 sugerem uma relação entre a qualidade de aproximação, gerada por RSAPP e a acuracia dos modelos de Inineração.
Embora essa relação não tenha se mantido homogênea para todos os redutos, ainda assim observa- se que quando a qualidade de aproximação foi ordenada, a curva de acuracia acompanhou o decréscimo.
Portanto, nesse contexto de pesquisa, a TCA através de RSAPP efetivamente diferenciou os &quot;melhores «e &quot;piores «redutos para tarefas de Inineração.
A intensão deste trabalho foi desenvolver um algoritmo capaz de produzir um diagnóstico de auxílio ao técnico Ininerador.
Tal diatgnóstico informa algumas métricas para cada um dos redutos do conjunto de dados (neste trabalho, um DW).
Visto a proposta deste trabalho e os testes executados, entende- se que RSAPP pode ser usado para produzir uma base de conhecimento sobre os dados, mesmo que o conhecimento de domínio seja superficial.
RSAPP Vem a contribuir como um artifício sob o contexto de KDD, permindo que, irão Somente o conhecimento de domínio possa indicar quais dados tem maior potencial de produzir resultados mais precisos.
O diagnóstico de RSAPP usou apenas métricas da TCA, porén1 outras métricas, usando outras abordagens poderiam complementar o diagnóstico.
Ou seja, o diagnóstico pode ser enriquecido através de outras técnicas disponíveis.
Por exemplo, na literatuda diversos trabalhos indicam a técnica de análise de Componentes Principais como uma proposta semelhante a TCA.
Em esse contexto, em trabalhos futuros essa possibilidade pode ser explorada.
Ainda, corno uma possível evolução deste trabalho, RSAPP pode ser melhorado, quanto a sua complexidade e sua interface.
Em esse cenário, outras abordagens de algoritmos/ linguagem podem ser desenvolvidas, bem como urna interface GUI para seleção dos atributos dos redutos.
