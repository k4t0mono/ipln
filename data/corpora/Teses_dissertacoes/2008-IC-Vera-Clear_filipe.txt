Este trabalho realiza um estudo da tipologia e disponibilidade de corpora.
São discutidas questões referentes ao projeto de um corpus no que se refere a sua compilação.
São apresentadas funcionalidades para exploração de corpora e analisadas ferramentas e recursos disponíveis para trabalhar com corpus.
A seleção de ferramentas adequadas para compilação e exploração de corpora de textos em língua natural representa hoje um desafio aos pesquisadores da área.
Muitas das ferramentas disponíveis dependem de licenças e plataformas específicas para serem executadas, limitam o uso de vários formatos de documento ou criam padrões próprios de codificação de corpus e de anotações, dificultando a criação, a interoperabilidade e o compartilhamento de recursos lingüísticos entre grupos de pesquisas.
Em esse sentido é apresentada e descrita uma ferramenta para a lingüística de corpus que construímos e oferecemos à comunidade de pesquisadores em língua portuguesa ­ a ferramenta Entrelinhas.
Esta ferramenta facilita a compilação e agrega funcionalidades essenciais para exploração de corpora.
A ferramenta adere a um formato de codificação compatível com o Portal de Córpus do NILC/USP contribuindo com o intercâmbio de recursos para o processamento da língua portuguesa.
Uma análise quanto a o uso dessa ferramenta também é apresentada.
A linguagem, elemento fundamental da comunicação, está constantemente evoluindo e se modificando.
A cada dia, novas palavras são incorporadas ao vocabulário, enquanto outras caem em desuso ou ganham novos significados.
O ser humano é capaz de compreender e se adaptar rapidamente a essas evoluções e a toda a variedade lingüística que nos rodeia.
Porém, esse dinamismo da língua torna- se um enorme desafio ao processamento da linguagem natural, passo fundamental para aproximar a comunicação entre humanos e computadores.
A lingüística, ciência que estuda a linguagem humana, é uma ciência empírica, em a qual todo o conhecimento é resultado de nossas observações e experiências sobre o uso da linguagem.
A lingüística de corpus estuda a linguagem através de amostras de &quot;textos reais».
Corpus (plural corpora) é uma grande coleção de textos com milhares de palavras escritas por humanos e é também a base do processamento estatístico da linguagem e da lingüística de corpus.
A abordagem empírica nos estudos sobre a linguagem, através da lingüística de corpus, era muito comum entre os anos de 1920 e 1960.
Porém, por volta de 1960, a abordagem racionalista passou a dominar grande parte das pesquisas, especialmente devido a os trabalhos de Noam Chomsky.
Em os últimos 20 anos, a abordagem empírica vem experimentando uma espécie de renascimento, impulsionada principalmente por a crescente disponibilidade de corpora eletrônicos.
Este recente crescimento no número de corpora eletrônicos fez com que as pesquisas em processamento estatístico da linguagem e lingüística de corpus voltassem a se intensificar e contribuir com a descrição da linguagem.
Apesar de, na língua portuguesa, o termo linguagem ter um significado distinto de língua, neste trabalho, os dois termos serão utilizados com o mesmo significado, como empregado na língua inglesa.
A lingüística de corpus serve a diversas aplicações como, por exemplo, o estudo de como ensinar e aprender uma língua.
A análise de um corpus contribui para o processamento computacional da língua natural fornecendo evidências que melhoram a descrição da estrutura e do uso das línguas.
Santos agrupa os pesquisadores da área de lingüística de corpus basicamente em dois grupos:
Compiladores de corpora e usuários de corpora.
Os compiladores de corpora preocupam- se especialmente com questões tais como criar, estruturar e anotar corpora.
Já os usuários de corpora, preocupam- se em extrair informações a partir de os corpora.
Além destes, emerge nos grupos de pesquisa e vem ganhando espaço os desenvolvedores de ferramentas para corpora.
Motivações A lingüística de corpus pode contribuir significativamente para a descrição da linguagem natural e, apesar de o crescimento no número de recursos e ferramentas disponíveis nessa área, ainda existem muitos problemas em aberto.
A seleção de ferramentas adequadas às necessidades de cada projeto representa hoje um desafio aos pesquisadores da área.
Muitas ferramentas para compilação e exploração de corpora utilizam padrões de codificação de texto e anotações incompatíveis entre si, dificultando a interoperabilidade e o compartilhamento de recursos lingüísticos entre grupos de pesquisas.
Outras ferramentas, algumas comerciais, necessitam plataformas específicas para serem executadas e não suportam adequadamente esquemas de codificação de caracteres para a língua portuguesa.
Objetivos O objetivo geral deste trabalho é fazer um estudo amplo sobre corpora e ferramentas para corpora, propondo e desenvolvendo uma ferramenta que possa ser distribuída sem custos, voltada a usuários da língua portuguesa.
Tem- se como objetivos específicos:
Contribuir com o projeto PLN-BR2 disponibilizando uma ferramenta para compilação e exploração de corpora aderente ao formato de codificação XCES, adequado para a língua portuguesa e compatível com o Portal de Córpus3;
facilitar a compilação de corpora permitindo que formatos populares de arquivos sejam utilizados;
Organização desta dissertação Em o Capítulo 2 apresentaremos o modo como alguns autores definem o que é um corpus e introduzimos as noções de tamanho e representatividade de um corpus.
Apresentaremos também o modo como corpora costumam ser classificados.
Descrevemos alguns dos maiores corpora em língua inglesa e língua portuguesa hoje existentes e abordaremos aspectos sobre uso da web como um corpus.
Em o Capítulo 3 abordaremos as principais etapas envolvidas na compilação de um corpus.
Discutiremos questões e critérios relacionados ao projeto de um corpus e a coleta de documentos.
Descreveremos tarefas relacionadas à preparação, à segmentação e à anotação dos textos.
Apresentaremos também projetos e iniciativas em vista de estabelecer um padrão de codificação para corpora e anotações.
Em o Capítulo 4 apresentaremos funcionalidades básicas de exploração de corpora e algumas aplicações baseadas em corpus.
Em o Capítulo 5 apresentaremos algumas ferramentas para compilação e exploração de corpus, abordando seus principais recursos e funcionalidades.
Em o Capítulo 6 apresentaremos características e funcionalidades da ferramenta Entrelinhas, que desenvolvemos no âmbito da compilação e da exploração de corpora em língua portuguesa.
PLN-BR (Recursos e Ferramentas para a Recuperação de Informação em Bases Textuais em Português do Brasil) financiado por o CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico), projeto Em o Capítulo 7 apresentaremos experiências realizadas com a Ferramenta e mostraremos o relato de um usuário especializado quanto a as suas funcionalidades, em decorrência de suas percepções no uso da Entrelinhas.
Em o Capítulo 8 apresentaremos as considerações finais, retomando os assuntos abordados nos capítulos anteriores e lições aprendidas, e indicaremos trabalhos futuros sobre a ferramenta Entrelinhas e sobre o tema estudado.
Corpora O termo corpus vem do latim e significa corpo, conjunto.
Corpora (plural de corpus) lingüísticos são enormes coleções de textos, criteriosamente selecionados, apresentando exemplos escritos ou falados numa língua.
Em a literatura, há várias definições sobre o que exatamente constitui um corpus.
De acordo com Manning e Schütze:
&quot;Um corpo de textos é chamado corpus, corpus é simplesmente o termo em latim para` corpo' e, quando você tem muitas coleções de textos, você tem corpora.»
Kennedy escreve:
&quot;Em a Lingüística, um corpus é um corpo de texto escrito ou de falas transcritas que pode servir de base para análise e descrição lingüística.»
Sardinha analisou a definição de corpus apresentada por vários autores.
Por mencionar vários aspectos importantes ­ tais como a origem e a formatação dos dados, o propósito, a composição, a representatividade a extensão do corpus ­ o autor selecionou a definição corpus dada por Sanchez, Cantos e Cumbre como a mais completa:
&quot;Um conjunto de dados linguísticos (pertencentes ao uso oral ou escrito da língua, ou a ambos), sistematizados segundo determinados critérios, suficientemente extensos em amplitude e profundidade, de maneira que sejam representativos da totalidade do uso linguístico ou de algum de seus âmbitos, dispostos de tal modo que possam ser processados por computador, com a finalidade de propiciar resultados vários e úteis para a descrição e análise.»
Atualmente, a maioria dos corpora está armazenada em formato eletrônico, porém nem sempre corpora foram armazenados eletronicamente para serem lidos por computadores.
Corpora eletrônicos passaram a ser usados mais intensamente a partir de os anos 60.
Teoricamente poderíamos considerar um corpus qualquer coleção de textos, em formato eletrônico ou não (KENNEDY, Ainda que não sejam exatamente textos, existem também corpora de falas gravadas, que não serão estudados neste trabalho.
Em este documento, consideraremos corpus como uma coleção de textos computacionalmente armazenada e organizada para o estudo de fenômenos lingüísticos ou a criação de ferramentas computacionais, como etiquetadores e corretores ortográficos.
Corpora são geralmente extensos e, quanto maior um corpus, mais representativo ele é.
Geralmente, o tamanho de um corpus é medido por a quantidade de palavras que ele contém.
Corpora devem conter o maior número possível de estruturas existentes na linguagem pertencentes ao domínio de interesse.
A representatividade de um corpus permite que de ele sejam extraídas observações confiáveis sobre o uso da língua.
Diz- se que um corpus é representativo quando ele é uma amostra representativa da população de interesse, ou seja, os resultados obtidos na amostra também valem para a população pesquisada (MEGERDOOMIAN, Tipologia de corpora Corpora podem ser compilados para diferentes finalidades, a partir de diferentes tipos e quantidades de texto, em diferentes línguas.
Estes corpora podem diferenciar- se significativamente uns dos outros em diversos aspectos como, por exemplo:
Origem dos textos, gênero dos textos, áreas de domínio dos textos, idioma dos textos e disposição interna.
Estes diferentes tipos de corpora buscam atender a diferentes propósitos.
O objetivo desta seção é apresentar os tipos mais comuns de corpora citados na literatura.
Corpora podem ser do tipo escrito ou falado.
A maior parte dos corpora são do tipo escrito, pois são mais fáceis de obter e processar.
Um corpus escrito é formado simplesmente por a produção escrita da língua em estudo, como:
Textos de livros, revistas, jornais, artigos, páginas na Internet, etc..
Enquanto que corpora falados, também chamados de corpora de falas, são compostos por falas transcritas, que podem ser originadas, por exemplo, de diálogos, monólogos ou conversas telefônicas.
Em um corpus falado, a transcrição das falas implica o tratamento de aspectos fonéticos e prosódicos bastante complexos.
Esse tratamento da transcrição torna a compilação de um corpus falado muito mais lenta e cara.
Transcrições de um corpus falado, normalmente, possuem muitas contrações, vários tipos de representações fonéticas, variações de pronúncia, interjeições e muitos fragmentos de frases e de palavras.
O tratamento de todas estas ocorrências ainda representa um enorme desafio (MANNING;
Existem corpora formados de textos escritos e também de falas transcritas.
Considerado um dos mais importantes corpora pré-eletrônicos e mais tarde convertido para o formato eletrônico, possui 100 textos de origem escrita e 100 textos de origem falada em inglês britânico.
Corpora podem ser classificados quanto a o gênero dos textos em dois tipos principais:
Balanceados ou especializados.
Um corpus balanceado normalmente é compilado quando se deseja construir um corpus para uso geral, sem um objetivo específico de pesquisa.
Corpora balanceados são formados de textos de diferentes gêneros e domínios, estes incluídos em iguais quantidades por gênero, ou em quantidades proporcionais à relevância que cada gênero de texto tem na língua.
É bastante comum encontrar- se corpora balanceados também chamados de corpora genéricos, corpora equilibrados ou core corpora.
Corpora especializados, também chamados de corpora oportunistas ou especiais, são compilados para objetivos específicos de pesquisa.
Um corpus especializado pode objetivar o estudo, por exemplo, do desenvolvimento da linguagem falada ou escrita por crianças, de uma língua falada ou escrita através de falantes não nativos dessa língua ou, ainda, do uso de uma língua numa área específica do conhecimento, como medicina ou informática.
A maior parte dos corpora especializados foi compilada tendo como objetivo o estudo de aspectos sociais do uso da língua e variações regionais da linguagem.
Portanto, corpora específicos de um dialeto ou de uma região também são classificados como corpora especializados.
Algumas vezes a noção de corpus balanceado tem sido usada de maneira relativa.
Um corpus especializado pode ser balanceado, dentro de o escopo de um dado domínio, se forem incluídos vários gêneros de texto dentro de o domínio em estudo.
Por exemplo, se o objetivo é analisar fenômenos lingüísticos em notícias econômicas, então o corpus pode ser balanceado por a inclusão de notícias de vários jornais, revistas ou outras fontes, com o objetivo de capturar diferentes estilos, vocabulário ou padrões.
Um corpus estático é um corpus compilado e planejado para ser uma amostra finita da linguagem, e nenhum aumento ou diminuição do corpus ocorre dinamicamente.
Corpora dinâmicos, também chamados de corpora orgânicos, são corpora que podem crescer ou diminuir dinamicamente, opondo- se aos corpora estáticos.
Um corpus monitor é um tipo de corpus dinâmico cujo conteúdo é constantemente substituído.
Através das substituições se inserem novos textos e, em quantidade equivalente, se removem os textos mais antigos.
O objetivo das substituições é compor um corpus capaz de refletir as mudanças da língua.
Corpora monitores são normalmente usados para fins lexicográficos pois, por serem constantemente reciclados, representam o estado atual da língua Corpora podem ser classificados quanto a sua finalidade.
Denomina- se corpus de estudo o corpus principal de uma pesquisa, que contém os textos que representarão a língua que se pretende descrever.
Muitas pesquisas contam também com corpora de referência e corpora de treinamento.
Corpora de referência são utilizados para a comparação de diferenças em relação a outros corpora, podendo, por exemplo, guiar a compilação de corpora de estudos.
Corpora de treinamento, por sua vez, são utilizados para o teste de ferramentas de PLN em desenvolvimento Corpora também podem ser classificados como monolíngües ou multilíngües.
Corpora monolíngües contêm textos numa única língua, enquanto corpora multilíngües contêm textos em diferentes línguas.
Corpora multilíngües que contêm o mesmo texto em mais de uma língua são chamados corpora paralelos ou comparáveis.
Entretanto, nem todo corpus multilíngüe é um corpus paralelo.
O corpus jurídico Aarthus, por exemplo, contém textos em inglês, francês e dinamarquês, porém não tem qualquer compromisso com a equivalência e a tradução entre seus textos (MCENERY;
WILSON, 1996;
Corpora paralelos são formatados para que seus textos, em diferentes línguas, possam ser facilmente comparados.
Este tipo de corpus é utilizado em sistemas mais recentes de tradução automática.
Porém, para tornar um corpus paralelo realmente valioso, é preciso identificar quais sentenças e palavras são traduções umas das outras.
Corpora que trazem estas anotações são chamados de corpora alinhados.
Corpora bilíngües têm sido muito usados em projetos de tradução automática baseados em métodos estatísticos de desambiguação contextual do significado.
O mais conhecido corpus bilíngüe de textos paralelos e alinhados é o Canadian Hansards, formado por a transcrição dos debates do parlamento canadense.
O corpus consiste de textos em francês e inglês, equivalentes de tradução um do outro.
O Canadian Hansards5 está disponível no Linguistic Data Consortium (LDC) (MEGERDOOMIAN, 2003).
Corpora podem ser classificados como sincrônicos ou diacrônicos.
Corpora sincrônicos são formados por textos que refletem o estado de uma língua num momento específico no tempo.
Geralmente corpora sincrônicos são formados por textos contemporâneos à época em que foram criados.
Sardinha classifica corpora contemporâneos como um tipo específico de corpus.
A maioria dos corpora são sincrônicos:
Um exemplo é o Brown Corpus6, que contém textos em inglês americano publicados em 1961.
Corpora diacrônicos são formados por textos escritos em diferentes períodos de tempo, ou seja, são formados por textos históricos.
Alguns autores, como Sardinha, classificam corpora históricos, com textos escritos no passado, como um tipo específico de corpus.
Corpora diacrônicos ajudam os lingüistas a estudar e entender as mudanças de uma língua.
O primeiro corpus que vão do século XIII ao século XVIII, como mostrado na Tabela 1.
Período Old English Middle English Early Modern English (British) Sub-períodos Total Total Total Palavras Um dos principais corpus diacrônicos em língua portuguesa é o Corpus Anotado do Português Histórico Tycho--Brahe8.
O Tycho--Brahe contém textos em prosa, escritos em português europeu entre os séculos XVI e XIX, anotados morfológica e sintaticamente.
Corpora disponíveis Há pouco mais de 20 anos eram pouquíssimos os corpora em formato eletrônico disponíveis, a maioria sem fins lucrativos.
Porém, o rápido desenvolvimento da informática que ocorreu nas últimas décadas permitiu que mais e mais pesquisadores trabalhassem com corpora.
Em os anos 90 muitos projetos de compilação de corpora surgiram em todo o mundo.
Muitos dos maiores projetos foram concebidos para fins comerciais, principalmente por editores de dicionário.
Em Manning e Schütze é apresentada uma lista com as maiores organizações que distribuem corpora.
A lista com os endereços atualizados das organizações pode ser vista na Tabela 2.
A maioria de elas cobra por os corpora e o custo depende da finalidade do uso.
Obviamente, licenças para uso comercial tendem a ser mais caras que licenças educacionais ou privadas.
2006) é criar um grande corpus de textos de vários gêneros e falas transcritas da língua inglesa americana produzidos a partir de 1990.
O projeto pretende que o ANC chegue a 100 milhões de palavras.
A edição mais recente do corpus ANC tem 22 milhões de palavras e é disponibilizada por o LDC.
British National Corpus10 (BNC):
O BNC foi completado em 1994 e é um corpus com aproximadamente 100 milhões de palavras.
É composto tanto por textos em língua escrita como falada.
Os textos que formam BNC provêm de uma enorme variedade de fontes e buscam representar o inglês britânico utilizado no final do século XX.
A parte escrita do BNC representa cerca de 90% do corpus e é composta por amostras de jornais, revistas, livros acadêmicos, romances, cartas e trabalhos escolares, entre outros.
A parte falada do BNC representa cerca de 10% do corpus e é composta por uma grande quantidade de conversações informais registradas por voluntários selecionados, de diferentes idades, regiões e classes sociais.
As conversas estão demograficamente balanceadas e foram coletadas em diferentes contextos como, por exemplo:
Reuniões governamentais, programas de rádio e chamadas telefônicas.
O corpus foi codificado de acordo com as regras do Text Encoding Initiative11 (TEI) usando Standard Generalized Markup Language (SGML).
Brown Corpus: Criado em 1964 na Brown University12, o Standard Corpus, como é mais conhecido, é historicamente importante por ter sido o primeiro corpus eletrônico a ser criado.
O Brown Corpus é genérico e foi criado para ser uma amostra representativa do inglês americano usado em 1961.
Apesar de hoje ser considerado pequeno e pouco atualizado, o Brown Corpus ainda é muito utilizado, pois serviu como modelo para muitos outros corpora.
A estrutura do corpus, que conta com 500 amostras de texto, pode ser vista na Tabela 3.
O Brown Corpus é disponibilizado por o LCD e por o ICAME.
Penn Treebank13: O Penn Treebank é um grande corpus com cerca de 4,5 milhões de palavras.
Todas as palavras do corpus foram anotadas com suas classes gramaticais e rótulos marcando a análise sintática.
Este corpus foi coletado do jornal The Wall Street Journal14.
Apesar de ser bastante usado, o Penn Treebank não está disponível gratuitamente A principal fonte de corpora em língua portuguesa é o projeto AC/ DC15 (Acesso a corpora/ Disponibilização de corpora) da Linguateca16.
Alguns dos corpora existentes em língua portuguesa são:
Corpus NILC/ São Carlos:
O corpus NILC/ São Carlos do Núcleo Interinstitucional de Lingüística Computacional17 (NILC) contém aproximadamente 35 milhões de palavras em português brasileiro contemporâneo.
O corpus é composto por diversos tipos de textos, como:
Didáticos, jornalísticos, legais, literários, entre outros.
A Figura 1 mostra um trecho de uma notícia de turismo contida no corpus.
O Corpus NILC/ São Carlos está disponível gratuitamente para fins de pesquisa na página da Linguateca.
Greenwich Village, garantem lazer 24 horas para o visitante
De a enviada especial a Nova York
São 11 mil quilômetros de ruas em Manhattan e, portanto, há passeios por toda a parte.
Fonte:
CETEMPúblico: O Corpus de Extractos de Textos Electrónicos MCT/ Público (CETEMPúblico) é um corpus com aproximadamente 180 milhões de palavras da área jornalística em português europeu.
O corpus foi criado por o projeto Processamento Computacional do Português (projeto que deu origem à Linguateca) após a assinatura de um protocolo entre o Ministério da Ciência e da Tecnologia (MCT) português e o jornal Público em Abril de 2000.
O Público, fundado em 1990, é o primeiro jornal diário português de grande circulação a disponibilizar uma edição eletrônica na rede.
A Figura 2 mostra um trecho de uma notícia esportiva contida no corpus.
O CETEMPúblico está disponível gratuitamente para fins de pesquisa na página da Linguateca através do projeto AC/ DC da língua portuguesa criado por o projeto Processamento Computacional do Português, com financiamento do Ministério da Ciência e Tecnologia de Portugal.
Lingüística Computacional e Processamento da Linguagem Natural.
O grupo reúne pesquisadores da Universidade de São Paulo (USP) em São Carlos, Universidade Federal de São Carlos (UFSCar) e Universidade Estadual Paulista (UNESP) de Araraquara.
O Farense aceitou o natural domínio dos primeiros 20'.
Depois, controlou o jogo, na sequência do seu dinâmico contra-ataque, onde Pitico, por a direita, e Djukic, por o outro lado, criavam perigo para as redes de Zivanovic.
Só de bola parada e num livre de Baía, é que os madeirenses se aproximaram da baliza de Lemajic.
Mas, no minuto seguinte, o protagonista seria o Farense, ao ver o árbitro Carlos Valente anular um golo, por irregularidade, num remate de Ricardo, a cruzamento de Pitico.
O União, após várias tentativas de aproximação da área do adversário, acabou por perder uma grande oportunidade de golo quando o jugoslavo Lepi falha o cabeceamento à entrada da baliza, na sequência da marcação de um canto.
CETENFolha: O Corpus de Extratos de Textos Eletrônicos NILC/ Folha de São Paulo (CETENFolha) é um corpus de cerca de 24 milhões de palavras em português brasileiro.
O corpus foi criado no âmbito do projeto Processamento Computacional do Português (mesmo projeto que criou o CETEMPúblico) com base nos textos do jornal Folha de São Paulo que fazem parte do corpus NILC/ São Carlos, compilado por o NILC.
O CETENFolha está disponível gratuitamente para fins de pesquisa na página da Linguateca através do projeto AC/ DC.
CRPC: O Corpus de Referência do Português Contemporâneo (CRPC) teve sua criação iniciada em 1988 no Centro de Lingüística da Universidade de Lisboa18.
O corpus contém atualmente 201 milhões de palavras.
O CRPC é constituído por amostras de diversos tipos de texto escrito (literário, jornalístico, técnico, científico, didático, econômico, jurídico, parlamentar, etc) e de falas transcritas (elocuções informais e formais).
São amostras que guardam variedades nacionais e regionais do português de textos que vão desde a segunda metade do séc..
XIX até 2002, sendo, na sua maior parte, posteriores a 1970.
Estão incluídas, no corpus, amostras do português europeu, português do Brasil, português dos cinco países africanos de língua oficial portuguesa (Angola, Verde, Guiné- Bissau, Moçambique, Príncipe), português de Macau19, português do Timor-Leste e do português de Antiga colônia portuguesa que hoje é administrada por a República Popular da China.
Goa20. A distribuição entre as regiões se dá conforme a Figura 3.
Note que o corpus é formado quase que totalmente por o português europeu.
O projeto Lácio-Web21 tem o objetivo de disponibilizar, principalmente para lingüistas e cientistas da computação, corpora e ferramentas lingüístico-computacionais.
O projeto disponibiliza quatro corpora corretamente compilados de português brasileiro escrito contemporâneo.
Os corpora estão catalogados e codificados de forma que possam ser facilmente intercambiados, analisados e utilizados.
Os quatro corpora disponibilizados no Lácio-Web são:
Antiga colônia portuguesa que hoje é administrada por a Índia.
Matemática e Estatística da Universidade de São Paulo) e a FFLCH/ USP (Faculdade de Filosofia, Letras e Ciências Humanas da USP).
Lácio-Ref: É o corpus de referência do projeto Lácio-Web, composto de textos provenientes de diversos jornais, revistas, teses, dissertações, livros e informativos.
As únicas anotações presentes neste corpus referem- se à existência de elementos gráficos, e cabeçalhos que contém informações bibliográficas e de catalogação.
A grande maioria dos textos está integralmente disponibilizada.
Par-C: A revista divulga projetos científicos e tecnológicos financiados por a instituição.
Comp-C: Corpus de textos jurídicos em inglês de conteúdo comparável a textos jurídicos em português, armazenados no Lácio-Ref. Mac-Morpho:
Corpus fechado, formado por artigos publicados no jornal Folha de São Paulo em 1994.
O Mac-Morpho contém mais de 1 milhão de palavras e foi anotado por o etiquetador Palavras (BICK, Mecanismos de busca na Internet servem como interfaces ou portas de entrada para a exploração da web como um poderoso recurso lingüístico.
A web pode ser usada, por exemplo, para a extração do contexto de uso das palavras.
A simples busca de uma palavra num dos serviços mais populares de busca na Internet, como Google23 ou Yahoo24, é capaz de retornar milhares de trechos de textos (snippets) em que o termo pesquisado aparece destacado no contexto, com algumas palavras que o antecedem e que o sucedem.
Diversas publicações demonstram o potencial de uso da web para tarefas tais como tradução automática, extração de relações semânticas, identificação de colocações ou desambiguação de palavras (SANTAMARÍA;
Gonzalo, 2003).
Mais adiante neste trabalho, na Seção 5.6, é apresentada a WebCorp25, um ferramenta com a finalidade de usar a web como um corpus.
No entanto, a maior parte das ferramentas de busca na web existentes está preparada para a recuperação da informação, e não para a extração de dados lingüísticos.
Rehbein destaca algumas desvantagens do uso da web como corpus:
Metadados não confiáveis, ausência de anotações, pouco controle sobre as ferramentas de busca e dificuldade para replicar os resultados devido as modificações que ocorrem nas páginas.
A mesma autora apresenta algumas estratégias utilizadas para contornar o problema, tais como:
A formatação dos dados retornados por as ferramentas de busca e a criação do corpus a partir de documentos obtidos na web.
Considerações sobre este capítulo Em este capítulo, apresentamos o modo como alguns autores definem o que é um corpus e introduzimos as noções de tamanho e representatividade de um corpus.
Apresentamos também o modo como corpora costumam ser classificados quanto a o propósito para o qual foram criados, o gênero, as áreas de domínio, os idiomas, e a época em que foram produzidos os textos em eles contidos.
Em seguida apresentamos alguns dos maiores corpora em língua inglesa e língua portuguesa hoje existentes e abordamos aspectos sobre uso da web como um corpus.
Em o próximo capítulo apresentaremos as principais questões e atividades envolvidas no processo de compilação de um corpus, tais como:
O projeto, a coleta, a preparação, a anotação e a codificação dos textos.
Compilação de corpora Compilar ­ ou criar ­ um corpus é projetar e codificar uma coleção de documentos coletados dentro de determinados padrões ou exigências, para a realização de estudos lingüísticos ou computacionais de aprendizagem de máquina.
Apesar de o número de corpora disponíveis ter crescido significativamente nos últimos anos, alguns pesquisadores necessitam que novos corpora sejam criados.
Isso ocorre quando os corpora existentes não atendem aos requisitos da pesquisa, possuem um custo de acesso muito elevado, ou, simplesmente, porque uma coleção específica de documentos faz parte da descrição do problema.
Vários aspectos envolvem a compilação de um corpus.
Características como tamanho, balanceamento, representatividade, direitos autorais, conversão de formatos, limpeza de textos, meta-dados, inserção de anotações e padrões de codificação devem ser discutidas, estudadas e planejadas previamente, de acordo com o propósito do corpus.
Em esta seção apresentaremos as principais tarefas envolvidas na compilação de um corpus:
O projeto, a coleta dos documentos, a preparação dos textos e a codificação do corpus.
Projeto de um corpus As características que um corpus deve ter não costumam ser objeto de consenso entre os pesquisadores.
O tamanho, o balanceamento e a representatividade são características fortemente relacionadas aos objetivos da pesquisa em a qual o corpus está inserido:
Alguns tipos de corpora tendem a ser mais adequados para tipos específicos de análise e alguns corpora simplesmente não são adequados para certos tipos de pesquisas.
Mesmo quando concordam sobre os requisitos que determinado corpus deve ter, muitos pesquisadores discordam quanto a a forma prática de conseguir- los.
O tamanho de um corpus, normalmente medido por a quantidade de palavras, depende da quantidade de textos e do tamanho dos mesmos.
Em geral, quanto maior um corpus, melhor.
No entanto, o tamanho de um corpus normalmente é limitado por o tempo e por o volume de recursos disponíveis para o projeto, que por sua vez influenciam na quantidade de textos que podem ser obtidos e suas respectivas permissões de uso.
Outra característica importante refere- se ao fato do corpus ser uma amostra balanceada da população em geral, ou ser um corpus especializado.
Um corpus balanceado, como já foi discutido na Seção 2.1.2, é uma coleção que tenta cobrir o máximo possível de domínios, gêneros, tipos e estilos textuais.
O projeto de um corpus deve especificar a quantidade e o tipo dos textos que serão incluídos.
O corpus será composto de textos escritos ou de falas transcritas?
Textos formais ou informais?
Textos didáticos, jornalísticos, publicitários ou literários?
Textos jurídicos, religiosos ou esportivos?
Textos em prosa ou poesia?
Narrativas ou dissertações?
Textos históricos ou contemporâneos?
Textos produzidos por adultos, crianças ou imigrantes?
Em que proporção cada tipo deve ser incluído?
Textos de diferentes gêneros e domínios devem ser incluídos em iguais quantidades ou em quantidades proporcionais à relevância que cada gênero ou domínio de texto tem na língua?
O que verdadeiramente constitui um corpus balanceado ainda é uma questão em aberto (ARNOLD;
BUCKLEY, 2006;
É preciso especificar também se o corpus será ou não composto de textos completos.
Um corpus pode ser formado por textos completos ou apenas por amostras dos textos originais.
As amostras de textos são partes dos textos originais completos.
Essas amostras podem ser de tamanho específico para o corpus inteiro.
Para o estudo do estilo e do discurso, por exemplo, um corpus formado por amostras de textos de duas mil palavras, extraídas de vários textos, não é capaz de capturar confiavelmente características da estrutura interna de textos completos, onde se espera que as características lingüísticas das seções introdutórias e finais sejam diferentes.
Estudos como estes requerem corpora de textos completos.
Um corpus deve ser representativo da língua de uma população;
A população não abrange, necessariamente, a linguagem como um todo.
Os textos podem ser amostrados a partir de sub-populações de acordo com a região, o gênero ou grupos específicos.
Um corpus pode ser considerado representativo quando as descobertas feitas a partir de ele puderem ser generalizadas para a língua como um todo.
Coleta dos textos A coleta de textos visa obter documentos textuais em formato eletrônico que atendam os requisitos estabelecidos para a composição do corpus.
Em muitos casos, os textos de interesse podem não estar disponíveis em formato eletrônico, o que pode ocorrer com livros, documentos antigos, impressos ou manuscritos.
A coleta destes textos implica digitação, digitalização de documentos impressos ou a transcrição de áudios, tarefas que podem levar muitos dias, pois necessitam ser realizadas de forma quase totalmente manual.
Mesmo documentos impressos, quando digitalizados e processados por software de reconhecimento óptico de caracteres26, necessitam ser minuciosamente analisados e corrigidos por um humano.
Este trabalho manual é imprescindível pois, mesmo com o alto nível de precisão alcançado por este tipo de software, os textos gerados são suscetíveis a erros e podem apresentar diversos problemas, tais como trechos não reconhecidos ou incorretos (EVANS, Normalmente, os documentos que atendem aos requisitos da pesquisa podem já estar em formato eletrônico, facilitando significativamente a coleta dos textos.
Estes documentos podem estar acessíveis por a Internet, armazenados em discos rígidos, CD-ROMs, DVDs, gravados num banco de dados, ou diretamente num sistema de arquivos.
Existem ferramentas especilizadas na captura de documentos da Internet, tais como HTTrack27, WebZIP28 ou WebCloner29.
Essas ferramentas, conhecidas como navegadores offline ou web crawlers, podem acelerar o processo de coleta de textos.
A maioria dos documentos, mesmo quando disponíveis na Internet, está protegida por direitos autorais.
Convém sempre verificar se os textos coletados Reconhecedores óticos de caracteres, também conhecidos como OCR (optical character recognition), convertem documentos em formato de imagem para o formato texto.
Durante a coleta dos documentos é recomendado registrar o máximo possível de informações referentes à autoria e à origem dos mesmos, tais como:
Preparação dos textos Existe uma grande variedade de formatos de arquivo publicados na Internet;
No entanto, a maioria ferramentas de análise de corpora aceita apenas alguns poucos formatos de documento como entrada.
Mesmo que estejam em formato eletrônico, muitos dos documentos coletados não estarão prontos para ser lidos por as ferramentas de PLN disponíveis.
Preparar manualmente um corpus com algumas centenas de documentos, copiando e colando textos da área de transferência do computador, sem auxílio de um software conversor, pode levar meses.
O trabalho de preparação dos textos, através da conversão desses arquivos para um formato adequado, depende muito da forma como foram produzidos e da disponibilidade de software conversores.
Alguns editores de texto mais sofisticados podem gerar arquivos em formatos bastante complexos.
Muitos conversores podem não conseguir converter adequadamente alguns elementos presentes nos documentos originais ou, simplesmente, esses elementos não podem ser representados no formato aceito por as ferramentas de análise de corpora.
Em geral, ferramentas de exploração de corpora suportam como entrada apenas documentos em texto puro (txt) ou algum formato específico de codificação de corpora.
Mesmo quando os textos de um corpus são codificados em formato diferente de txt, é comum que cópias em texto puro sirvam como formato intermediário na conversão do arquivo e sejam mantidas internamente na estrutura do corpus, com o objetivo de otimizar o processamento.
Formatos populares de documento como HyperText Markup Language, Microsoft Word, Rich Text Format, Portable Document Format ou OpenDocument Format são extremamente ricos e podem conter elementos não-textuais que podem ser perdidos durante conversão.
É importante verificar com cuidado se layout de páginas, imagens, fórmulas, tabelas, legendas, estilos (negrito, itálico, sublinhado, etc), números de linha, cabeçalhos e rodapés foram convertidos de modo satisfatório.
Não há um modo definido de converter esses elementos, que podem ser descartados ou, quando interessantes para o projeto, convertidos em anotações.
Caracteres especiais, acentos, hifenizações, quebras de linha e de parágrafo, também requerem atenção.
Problemas na representação de letras de alfabetos diferentes do inglês normalmente estão relacionados ao esquema de codificação de caracteres (charset) utilizado.
A versão do documento em texto puro, mesmo quando acompanhada de anotações, é apenas uma representação do documento original.
A conservação dos documentos originais permite que falhas ou informações perdidas durante a preparação dos textos sejam recuperadas ou corrigidas manualmente, mesmo quando detectadas mais tarde.
Segmentação e anotação dos textos Anotações, também conhecidas como marcações ou etiquetas, são informações explicitamente adicionadas a um corpus.
Apesar de um corpus desprovido de anotações representar um recurso bastante útil, um corpus anotado representa para a lingüística de corpus um recurso ainda mais útil e valioso.
Anotações agregam valor a um corpus através da expansão das possibilidades de exploração, permitindo que buscas mais refinadas sejam realizadas.
É possível acrescentar a um corpus anotações lingüísticas, ou referentes à autoria, à origem e à estrutura dos textos originais.
Informações referentes a autoria e origem dos textos, geralmente são obtidas durante a coleta dos textos.
Acrescentar anotações com informações como título, ano de publicação ou até mesmo a origem, o sexo e a idade dos autores pode permitir o estudo de fenômenos sociais.
Esse tipo de anotação, por estar vinculada a textos inteiros, normalmente é registrada em seções na forma de cabeçalhos ou headers, independentemente da forma como o corpus está codificado.
Os textos também podem receber anotações lingüísticas de vários tipos.
A segmentação do texto é pré-requisito para os demais tipos de anotação.
A segmentação, ou tokenização, delimita elementos lingüísticos contínuos, determinando, por exemplo, o início e o final de palavras, sentenças e parágrafos.
As menores unidades de um texto são também conhecidas como tokens.
Em algumas línguas as palavras não são separadas por espaços em branco;
No entanto, na língua portuguesa e em diversas outras línguas, a simples existência de um espaço em branco indica o fim de uma palavra e o início de outra.
Há casos especiais, como &quot;Porto Alegre «ou &quot;guarda- chuva», em que pode ser interessante considerar dois segmentos como um único segmento.
Há segmentadores que podem fazer essa segmentação de forma automática, como por exemplo, o QToken30.
Há vários tipos e níveis de anotação lingüística, que podem enriquecer o texto com informações morfológicas, sintáticas e semânticas.
Os tipos mais comuns de anotação lingüística em corpus são a etiquetagem das classes gramaticais das palavras, também conhecido como Part-Of-Speech (POS tagging), e a lematização, onde cada palavra recebe uma etiqueta com seu lema.
O processo de anotar, ou etiquetar, um texto pode ser feito de modo manual ou automático, com auxílio de um software etiquetador.
Atualmente, software como o Palavras, que fazem a etiquetagem de classes gramaticais, são capazes de marcar com um alto nível de precisão enormes quantidades de texto.
A escolha do software etiquetador implica a escolha de um conjunto de etiquetas específico.
Muitos outros tipos de anotação podem ser adicionados.
Em algumas situações, pode ser interessante anotar o texto com informações sobre a estrutura e a formatação utilizada do documento original, indicando, por exemplo, a presença de cabeçalhos, rodapés, quebras de página, trechos em negrito, itálico, sublinhado, etc..
Codificação dos textos e das anotações O crescimento na disponibilidade de recursos lingüísticos nas últimas décadas, fez com que diversos formatos de codificação de textos, anotações surgissem.
A maioria dos projetos de corpus, com objetivo de atender requisitos de ferramentas de anotação e exploração de corpus específicas, veio a criar seus próprios formatos para codificação de textos e anotações.
A diversidade de formatos aumentou a importância da busca por padrões que facilitassem o compartilhamento, a combinação e o intercâmbio desses recursos.
Entre os principais projetos e iniciativas em vista de estabelercer um padrão de codificação para textos e anotações, podemos destacar:
MuchMore31, TigerXML32, Text Encoding Initiative33 (TEI), Corpus Encoding Standard (CES), Corpus Encoding Standard for XML (XCES) e padrão Iso TC37/ SC434.
Em as próximas seções apresentaremos o padrão da Iso TC37/ SC4, o formato de codificação Corpus Encoding Standard for XML (XCES) e o formato de codificação do projeto MuchMore.
A escolha por estes padrões ou formatos se deve à observação do crescente destaque com que eles vêm sendo adotados em sistemas e plataformas.
O Technical Comitee 37 (TC37, Terminology and Other Languages Resources), da International Organization for Standardization (Iso), criou um sub-comitê (SC4) para preparar padrões internacionais e recomendações para a modelagem de dados, anotação, intercâmbio de dados e avaliação de recursos lingüísticos.
Dentro de o Iso TC37/ SC4 um grupo de trabalho foi criado para estabelecer um padrão internacional e prover um framework para criação, anotação e manipulação de recursos lingüísticos que sirva de referência para diferentes esquemas de anotação e software de processamento.
O framework deve permitir e estimular o intercâmbio e o reuso de anotações lingüísticas e, ao mesmo tempo, prover codificação e anotação flexíveis.
O grupo, formado por especialistas de diversas universidades, Declerck.
O projeto desenvolve tecnologias para a construção de um sistema de recuperação de informações interlíngua, cross-language information retrieval, para o domínio médico.
O framework deve permitir a representação de qualquer variedade de informação lingüística, seja ela geral ou específica, e não deve impor ou restringir nenhuma teoria lingüística.
As estruturas de representação devem possuir uma semântica definida e, descritores e categorias de informação devem ser compartilhados de maneira centralizada e consistente, evitando que mecanismos distintos sejam usados para descrever o mesmo tipo de informação.
Os dados devem ser representados através de uma nomenclatura padrão, de forma legível para humanos.
Além disso, os dados devem também poder ser inseridos a qualquer momento, de forma incremental e, informações específicas devem poder ser facilmente extraídas ou separadas.
De essa forma, deve ser possível isolar camadas específicas da anotação de outras camadas, permitindo que novas informações de anotação sejam adicionadas e somente apontem para os dados originais (stand-off annotation) ao invés de estas estarem em meio aos dados.
Para isso, o modelo de dados deve separar claramente estrutura e conteúdo, porém mantendo- os mapeáveis entre si.
O documento anotado deve poder ser facilmente manipulado por o usuário, e seu mapeamento deve ser documentado num XML Schema35 (ou equivalente) associado ao modelo de dados.
O framework deve poder ser estendido e ser independente de mídia, capaz de lidar com imagens e vídeos.
O mapeamento entre documento anotado e formato final deve poder ser feito através folhas de estilo (schema-- derived stylesheet).
O formato final deve ser permitir a serialização36 e a desserialização de dados.
O XCES37, Corpus Encoding Standard for XML38, é um conjunto bastante aceito de padrões de codificação A serialização permite que dados, quando gravados ou transmitidos através de uma rede, sejam lidos e carregados corretamente.
O objetivo inicial do XCES foi prover um framework de acesso e representação para o American National Corpus.
O framework beneficiou a comunidade de engenheiros da linguagem com a especificação de um formato que permite grande interoperabilidade entre diferentes tipos de anotações e anotações diferentes de um mesmo fenômeno, servindo de interface entre diferentes tipos de anotações lingüísticas.
Em o XCES as anotações devem poder ser facilmente definidas e validadas, não impondo ao desenvolvedor valores ou elementos específicos.
O XCES é a versão XML do CES, Corpus Encoding Standard.
O CES integra o EAGLES Guidelines39.
O EAGLES Guidelines provê um conjunto de padrões de codificação de aplicações para processamento da língua natural baseado em corpus.
O CES é uma aplicação Standard Generalized Markup Language40 (SGML) compatível com as especificações do TEI Guidelines for Electronic Text Encoding and Interchange, da Text Encoding Initiative.
O CES foi projetado para servir de maneira adequada às pesquisas em engenharia da linguagem e aplicações.
XML, eXtensible Markup Language, é uma linguagem de marcação derivada do SGML.
XML tornou- se um padrão amplamente aceito para representação e intercâmbio de dados na rede mundial de computadores.
A evolução do CES para o XCES permitiu o uso de diversos mecanismos disponíveis para o XML como, por exemplo:
XSTL41 (eXtensible Stylesheet 39 O EAGLES Guidelines foi desenvolvido por o grupo de pesquisa europeu Expert Advisory Group on Language Engineering Standards Language Transformations), XML Schemas, XSL42 (eXtensible Stylesheet Language), XPointer43 e XLink44 (XML Linking Language).
O XCES especifica uma arquitetura de dados para corpora e provê DTDs (Document Type Definitions) para codificar a estrutura básica dos documentos e suas anotações lingüísticas.
Há DTDs XML e XML Schemas do XCES para a codificação de dados segmentados, dados com anotações gramaticais, dados alinhados, entre outros.
Em a Tabela 4 estão listados e brevemente descritos os oito XML Schemas para a codificação de documentos e anotações lingüísticas providos por o XCES.
O projeto MuchMore, Multilingual Concept Hierarchies for Medical Information Organization and Retrieval, provê um framework que permite que as tecnologias correlatas existentes possam ser integradas e aperfeiçoadas, ao mesmo tempo que novas tecnologias possam ser desenvolvidas.
De entre as várias contribuições do projeto destaca- se a descrição de um formato de anotação lingüística e semântica baseado em XML.
O formato de anotação do MuchMore é capaz de integrar múltiplos níveis de análise lingüística.
Os níveis de informação, que incluem anotações morfológicas, sintáticas e semânticas, podem ser organizados separadamente e arquivo XML.
Em a Figura 4 é apresentado um exemplo de anotação lingüística XML no MuchMore sobre uma passagem de texto.
Em o MuchMore, o texto é representado por o elemento que, por sua vez, é composto de um ou mais elementos que identificam as palavras do texto e carregam com si informações morfossintáticas, além de a forma canônica de cada palavra.
Em o formato utilizado por o MuchMore, o texto, com suas informações morfossintáticas e sintagmáticas, são armazenados num único arquivo.
Desta forma, é necessário repetir o mesmo texto em vários arquivos, de maneira redundante, quando se deseja distribuir diferentes informações lingüísticas em diferentes arquivos.
Essa redundância pode representar um significativo desperdício de espaço para armazenamento.
Há outros aspectos desfavoráveis, tais como a escassez de informações morfossintáticas, a ausência de informações de gênero e número das palavras, além de a ausência de informações semânticas Considerações sobre este capítulo Em este capítulo apresentamos as principais tarefas envolvidas nas etapas de compilação de um corpus:
O projeto, a coleta dos documentos, a preparação dos textos e a codificação do corpus.
Apresentamos também o padrão Iso, o formato de codificação Corpus Encoding Standard for XML (XCES) e o formato de codificação do projeto MuchMore.
Em o próximo capítulo apresentaremos as principais questões e atividades relacionadas à exploração de corpora e suas aplicações.
Exploração e uso de corpora Até recentemente, na lingüística computacional as abordagens tradicionais concentravam- se no desenvolvimento de mecanismos formais e na melhora das fontes de conhecimento para análise da linguagem.
Porém tem se confirmado uma tendência de empregar a abordagem quantitativa com foco nas técnicas estatísticas de aquisição de conhecimento, usando corpora (MEGERDOOMIAN, O processamento estatístico da linguagem viabiliza a análise e a compreensão da linguagem natural através de uma abordagem baseada em corpus, com o uso de estatística e aprendizado de máquina.
Geralmente essa abordagem conduz a modelos probabilísticos da linguagem, que podem ser aprendidos a partir de dados, de maneira mais simples que gramáticas formais e, assim, colocados em evidência com a crescente disponibilização de corpora Modelos probabilísticos apresentam vantagens, pois podem ser treinados a partir de dados:
O aprendizado do modelo ocorre, em última análise, a partir de a contagem de ocorrências.
A tolerância a erros e a capacidade de lidar com qualquer palavra, tornam os modelos probabilísticos bastante robustos.
Além disso, esses modelos são capazes de representar o fato de que as pessoas nem sempre concordam sobre todos os aspectos no uso da língua e, nos casos de ambigüidade, a estatística pode ser usada para interpretar os dados da maneira mais provável.
Segundo Manning e Schütze, os três principais requisitos para o processamento estatístico da linguagem natural são computadores, corpora e software.
Computadores, porque corpora são geralmente grandes e demandam recursos computacionais compatíveis com grandes volumes de texto (diferentemente da época em que os primeiros corpora foram criados, atualmente os computadores não representam um custo significativo).
Corpora, porque são base do processamento estatístico da linguagem natural (felizmente, hoje, muitas organizações distribuem corpora gratuitamente).
E software, como ferramentas de busca, de marcação e outras, para analisar os dados de um corpus.
Exploração de corpora Através da exploração de listas de palavras, concordâncias ou colocações feitas sobre um corpus pode- se obter informações que contribuem, por exemplo, para a melhora no ensino e no aprendizado de uma língua.
Em esta seção apresentaremos três funcionalidades, muito comuns em ferramentas de exploração de corpora:
Contadores de ocorrências (às vezes chamados de listas de palavras ou contadores de freqüência), concordanciadores e buscadores de colocações.
Contadores de ocorrências realizam contagens e calculam a freqüência de ocorrências de itens lexicais ou palavras num corpus.
O cálculo das freqüências permite a identificação de palavras-chave e stopwords.
A contagem de palavras pode contribuir também para a identificação de documentos similares, sendo usada em abordagens voltadas à recuperação da informação, classificação automática de documentos e outras.
As contagens mais comuns, que podem ser aplicadas a corpora inteiros ou textos específicos, são:·
o número total de palavras;·
o número de palavras distintas;·
o número de ocorrências de cada palavra e sua freqüência em relação a o total de palavras;·
o número de textos em que a palavra ocorre e sua freqüência em relação a o total de textos.
Alguns contadores mais sofisticados permitem diferenciar maiúsculas de minúsculas, detectar palavras com mesmo lexema ou desprezar deliberadamente a acentuação.
Algumas ferramentas que dispõem de contadores de palavras são:
WordSmith (Seção 5.3), BNCWeb46, TACT47, CLAN48 e Corsis49.
Em a Figura 5 é possível ver a lista de palavras do Corsis.
Concordâncias são listas de palavras ou seqüências de palavras dentro de um contexto.
As concordâncias são muito utilizadas na lingüística de corpus por permitirem que importantes padrões de uso da língua sejam descobertos ou compreendidos.
Concordanciadores são ferramentas muito utilizadas na lingüística de corpus, pois constroem concordâncias de forma automática:
O usuário entra com o termo a ser pesquisado e dispara a busca, como ocorre com qualquer outra ferramenta de busca.
Porém, diferentemente de uma busca normal, em que os resultados são os arquivos ou páginas que contêm o termo pesquisado, concordanciadores apresentam trechos dos textos em que o termo pesquisado aparece.
Em os resultados, o termo pesquisado é sempre apresentado de forma destacada e alinhada no centro dos resultados, dentro de uma janela de contexto.
Geralmente é possível configurar o tamanho da janela de contexto, ou seja, especificar o número de palavras que devem ser apresentadas no resultado, antes e depois do termo pesquisado.
Essa forma de visualização facilita o entendimento de padrões de uso, pois permite que um lingüista detecte diferentes usos de uma mesma palavra.
É possível descobrir, por exemplo, se o termo pesquisado costuma ser seguido ou precedido de palavras ou classes de palavras específicas ou, ainda, se o termo costuma ser usado no início ou no final de frases.
Alguns concordanciadores mais sofisticados permitem que a busca seja feita com expressões regulares, possibilitando, por exemplo, encontrar palavras com prefixos ou sufixos específicos.
Quando utilizados em corpora anotados, algum concordanciadores permitem que as busca sejam aplicadas, por exemplo, sobre classes de palavras específicas.
Algumas ferramentas que dispõem de concordanciadores são citadas a seguir:
Corpógrafo, WordSmith (Seção 5.3), Unitex (Seção 5.5), WebCorp (Seção 5.6), AntConc50, WebCONC51, Corsis, MonoConc52, GlossaNet53 e CorpusEye54.
Em a Figura 6 é possível ver o concordanciador do Corsis.
Colocações são expressões formadas por duas ou mais palavras que tendem a aparecer juntas ­ de forma consecutiva ­ ou próximas umas das outras, indicando combinações preferenciais ou usuais de palavras.
Em essas combinações recorrentes, aparentemente livres, as colocações revelam maneiras comuns de organizar e posicionar as palavras em determinados contextos.
Em colocações mais significativas, as palavras ocorrem com maior freqüência quando combinadas do que com outras palavras.
A identificação de colocações é importante, por exemplo, para a escrita de dicionários e o ensino de línguas e se dá através da observação das coocorrências.
Gasperin e Strube de Lima descrevem algumas abordagens para encontrar colocações presentes num texto, tais como:
Seleção das colocações por freqüência, seleção baseada em média e variância da distância entre a palavra foco e uma palavra vizinha, teste de hipótese e informação mútua.
O significado de uma colocação difere da simples combinação do significado das palavras que a compõem, pois apresenta um componente semântico que não poderia ser identificado se suas partes fossem observadas isoladamente.
Aplicações baseadas em corpus Corpora são amplamente utilizados em diversas aplicações do processamento da linguagem natural.
São exemplos de aplicações que podem ser baseadas em corpus:
Recuperação de informação, extração de informação, sistemas pergunta-resposta e sistemas de tradução automática (RUSSELL;
Recuperação de informação é a tarefa de encontrar documentos relevantes para as necessidades do usuário.
Ferramentas de busca na Internet estão entre os melhores exemplos de sistemas de recuperação de informação.
Em a Internet o usuário pode pesquisar por uma palavra qualquer numa ferramenta de busca e, em poucos segundos, ter uma lista de páginas relevantes.
Um corpus pode servir como uma base de treino para este tipo de sistema.
Extração de informação é o processo de pesquisa em textos por ocorrências de uma classe particular de objeto ou evento e por relacionamentos entre estes objetos e eventos.
Essas ocorrências oferecem aos lingüistas descrições da língua que antes não eram percebidas ou não podiam ser facilmente comprovadas.
Outra aplicação são sistemas pergunta-resposta, ou QA (Question Answering), um tipo de sistema de recuperação de informações inteligente que requer técnicas de PLN mais complexas.
O principal objetivo deste tipo de sistema é recuperar, a partir de uma coleção de documentos, pequenas passagens de texto capazes de responder às perguntas formuladas em linguagem natural por os usuários.
Perguntas do tipo &quot;Porquê?»
ou &quot;Como?»,
são mais complexas que perguntas do tipo &quot;Quando?»
ou &quot;Onde?».
Se o usuário, por exemplo, formular a pergunta &quot;O que é um motor?»,
a resposta a ser procurada é do tipo &quot;Um motor é X».
Já perguntas do tipo &quot;Quem?»
e &quot;Quando?»,
por exemplo, fazem com que, respectivamente, documentos do corpus com entidades nomeadas e datas, tornem- se candidatos a conter a resposta.
Outra importante aplicação em PLN com larga utilização de corpora é a tradução automática, ou machine translation, cujo objetivo é traduzir automaticamente textos de uma língua para outra.
As principais abordagens para a tradução automática vão desde sistemas baseados em conhecimento, com formalismos para representação de conhecimento, até abordagens puramente estatísticas.
Atualmente os sistemas são uma combinação de módulos estatísticos e não estatísticos.
Especificamente para tradução automática são necessários corpora paralelos e alinhados.
O objetivo do alinhamento é combinar sentenças, frases e palavras nos dois textos, fonte e alvo.
Um corpus alinhado pode ser usado para criar dicionários bilíngües ou gramáticas paralelas.
O alinhamento do texto não é uma tarefa simples, pois as traduções não refletem a estrutura original nem as mesmas palavras do texto traduzido.
Os tradutores normalmente reorganizam o texto para obter um sentido melhor na língua alvo ou para converter expressões idiomáticas.
Considerações sobre este capítulo Em este capítulo apresentamos três funcionalidades de exploração de corpora:
Contador de ocorrência, concordanciador e buscador de colocações.
Apresentamos também, brevemente, algumas aplicações baseadas em corpus:
Recuperação da informação, extração da informação, sistemas pergunta-resposta e sistemas de tradução automática.
Em o próximo capítulo traremos informação sobre algumas ferramentas que realizam compilação e exploração de corpus, abordando seus principais recursos e funcionalidades.
Ferramentas para corpora Em este capítulo apresentaremos ferramentas para corpora consideradas importantes, ou por serem muito citadas, ou por serem largamente utilizadas nas pesquisas em língua portuguesa.
Observaremos os principais recursos e funcionalidades, assim como os principais aspectos positivos e negativos de algumas de elas.
São estudadas as seguintes ferramentas:
Corpógrafo, Lácio-Web, Oxford WordSmith Tools, GATE, WebCorp, Unitex e Philologic.
Destacam- se especialmente o Corpógrafo e o WordSmith, por serem entendidos como ferramentas similares à proposta no contexto desta dissertação.
O Corpógrafo é um ambiente integrado, em língua portuguesa, projetado para a lingüística de corpus e engenharia do conhecimento.
Entende- se que o Corpógrafo se volta à engenharia do conhecimento por trabalhar com vocabulários e oferecer mecanismos para identificação de termos relevantes à compreensão dos domínios analisados.
Nosso trabalho poderá, igualmente, ser entendido como uma ferramenta intermediária para aplicações de engenharia do conhecimento.
O sistema Corpógrafo provê, gratuitamente, diversas funcionalidades que permitem aos usuários compilar e explorar seus próprios corpora, mesmo sem muitos conhecimentos técnicos.
O Corpógrafo é uma ferramenta web, portanto Gestor de Corpora.
O Corpógrafo facilita a compilação de corpora permitindo ao usuário a submissão e a extração de textos em diversos formatos.
Os formatos de arquivo suportados são:
PDF, Html, DOC, PS, RTF e texto puro.
Uma interface de edição permite que os textos submetidos possam ser limpos e segmentados em sentenças.
Em a Figura 7 é possível observar a tela para submissão de arquivos a partir de o computador do usuário.
O Corpógrafo permite também a submissão de textos através do download direto de documentos da Internet.
Para submeter um texto, o usuário deve apenas indicar a URL do texto a ser adicionado.
O usuário pode solicitar também que o sistema inspecione links dentro de a página indicada, e filtre os tipos de documentos a serem adicionados.
Em a Figura 8 é possível ver a tela para submissão de arquivos a partir de URLs.
O usuário pode armazenar seus textos num espaço privado no servidor, tendo também a opção de compartilhar seu trabalho com outros usuários cadastrados.
A quota de espaço de armazenamento no servidor, para um usuário padrão, é de 10 megabytes.
É possível preencher e armazenar meta-dados específicos para cada um dos textos submetidos.
Entre as informações que podem ser reunidas estão:
Título do documento, idioma, ano de publicação, autores, organização, área de domínio e gênero, entre outras.
O preenchimento destas informações pode ajudar na organização e nas buscas sobre os corpora.
Os corpora compilados no sistema recebem o nome de selection e podem ser montados a partir de os textos submetidos e armazenados em seu espaço privado no servidor.
Um corpus pode ser formado por vários textos, que por sua vez podem integrar diferentes corpora.
No entanto, o envio de grandes volumes de texto pode ser prejudicado por a velocidade de transmissão da Internet e limitações de espaço e de capacidade de processamento do servidor.
O Corpógrafo disponibiliza também funcionalidades para exploração de corpora:
Concordanciador com uso de expressões regulares, extrator de colocações, contagem de palavras e buscador de n-gramas.
O sistema ainda permite a extração de informações terminológicas, relações semânticas e mapas conceituais.
Ferramentas do projeto Lácio-Web O projeto Lácio-Web, descrito na Seção 2.2.3, disponibiliza ferramentas lingüístico-computacionais aos usuários cadastrados.
De entre as ferramentas encontram- se concordanciadores, contadores de freqüência e etiquetadores morfossintáticos.
Essas ferramentas, disponíveis após cadastro no site do projeto, são apresentadas a seguir:
Contador de freqüência padrão:
Calcula a freqüência de ocorrência das palavras de um corpus, retornando os resultados numa tabela de freqüência e informando o total de arquivos do corpus e a variação do vocabulário.
Contador de freqüência por palavra:
A partir de o resultado ordenado de uma contagem de ocorrências de palavras num corpus, a ferramenta busca a freqüência de uma palavra específica e um determinado número de palavras com freqüência maior e menor que a palavra escolhida.
Concordanciador para corpus sem anotação:
Gera uma listagem de todas as ocorrências de uma palavra ou expressão.
Concordanciador para corpus anotado morfossintaticamente:
Gera uma lista de ocorrências de uma determinada palavra, com determinada etiqueta.
Etiquetadores morfossintáticos:
A anotação morfossintática do corpus Lácio-Ref e seus subcorpus foi revisada manualmente e serviu de treinamento para três etiquetadores:
MXPOST, TreeTagger e Brill.
O Oxford WordSmith Tools é um conjunto integrado de ferramentas, bastante útil na preparação, manipulação, análise e descrição lingüística de um corpus.
Entre os recursos disponíveis no WordSmith estão as ferramentas WordList, KeyWords e Concord.
A ferramenta WordList permite, através da contagem de palavras, a criação de listas de palavras.
As listas mostram a freqüência com que cada palavra foi encontrada nos textos e em quantos textos foi encontrada.
Além 59 O Oxford WordSmith Tools foi desenvolvido por Mike Scott e disponibilizado pela primeira vez por a Oxford University Press em 1996.
Atualmente, encontra- se na versão 4.0.
A licença da versão completa para um usuário custa aproximadamente 75.
Uma versão de demonstração está disponível para download na disso, é possível fazer a contagem usando lemas e organizar as listas por ordem alfabética ou por ordem de freqüência.
Com a KeyWords, é possível obter listas de palavras-chave de textos através da comparação da lista de freqüências de ocorrências de palavras dos textos em estudo com a lista de freqüências de ocorrências de palavras num conjunto de textos de referência.
A ferramenta Concord produz concordâncias em que todas as ocorrências de uma palavra ou de um conjunto de palavras são listadas.
Em a listagem, o contexto da palavra-chave é exibido, mostrando as palavras adjacentes à mesma.
É possível produzir concordâncias diretamente a partir de as ferramentas WordList e KeyWords, selecionando palavras das listagens geradas por as duas ferramentas.
O Oxford WordSmith Tools conta ainda com outros pequenos programas bastante úteis na manipulação e visualização de arquivos de textos de um corpus.
Entre eles, podemos destacar os utilitários Text Converter, Spliter e Viewer.
O utilitário Text Converter faz a busca e substituição de texto em arquivos.
Ele é bastante útil quando um grande número de textos e arquivos necessita ser modificado.
O Text Converter suporta conversões complexas, auxiliando quando muitos arquivos precisam ser reformatados.
O Splitter parte arquivos muito grandes em arquivos menores.
Em este utilitário é possível escolher um símbolo ou termo para indicar os pontos onde devem ser feitas as divisões, como por exemplo «Cada termo ou símbolo de divisão encontrado no arquivo original irá gerar um novo arquivo texto.
O Viewer é um utilitário de visualização de arquivos texto que funciona de maneira integrada às demais ferramentas.
Este utilitário permite que o usuário visualize o conteúdo de arquivos em vários formatos.
O Viewer também produz saídas com sentenças ou parágrafos numerados, facilitando o alinhamento de duas versões de um texto.
GATE ­ General Architecture for Text Engineering O GATE60, General Architecture for Text Engineering, provê uma infra-estrutura para o desenvolvimento de software de PLN.
Desde seu lançamento o GATE tem sido usado por muitas organizações em diversos projetos de pesquisa e desenvolvimento.
O sistema oferece uma arquitetura (estrutura organizacional), um framework (biblioteca de classes) e um ambiente gráfico de teste e desenvolvimento.
O ambiente gráfico pode ser visto na Figura 11 e permite que o GATE seja usado de forma completamente independente, mas não impede que o sistema seja integrado a outras aplicações.
A arquitetura do GATE define três tipos fundamentais de recursos ou componentes para a construção de sistemas de PLN:
Language Resources (LRs), Processing Resources (PRs) e Visual Resources (VRs).
Esses tipos assumem papéis completamente distintos dentro de a arquitetura:
Language Resources (LRs) são componentes que representam recursos lingüísticos como documentos, corpora, esquemas de anotação e ontologias.
Processing Resources (PRs) são componentes que representam recursos de processamento fundamentalmente algorítmicos, como parsers etiquetadores.
Visual Resources (VRs) são componentes que representam recursos que oferecem funcionalidades de visualização e edição de dados.
Além de sua arquitetura robusta, o GATE destaca- se também por possuir código fonte aberto e ter sido desenvolvido totalmente em Java, o que o torna independente de plataforma.
Embora a persistência de seus recursos lingüísticos atenda ao padrão Iso TC37/ SC4, o formato de codificação dos dados é próprio, o que faz com que o intercâmbio de recursos com outros sistemas exija a implementação de conversores.
Existe um grande número de componentes disponíveis para o GATE, a maioria de eles para o tratamento de uma língua específica.
Embora existam componentes disponíveis para diversas línguas, principalmente para a língua inglesa, poucos são os recursos existentes específicos para o processamento da língua portuguesa.
Unitex Unitex61 é um conjunto de programas que possibilitam o tratamento de corpora utilizando recursos lingüísticos como gramáticas e dicionários eletrônicos De ela (Dictionnaires Électroniques du LADL).
O Unitex surgiu como uma alternativa gratuita a outro sistema de processamento de corpus, o Intex62.
Entre as funcionalidades disponíveis no sistema Unitex encontram- se:
Um gerador de concordâncias, um contador de freqüências, um gerenciador de dicionários De ela e um gerenciador de gramáticas.
O Unitex permite também que o usuário faça buscas complexas utilizando expressões regulares.
Uma das características favoráveis do Unitex, é que ele utiliza a codificação de caracteres Unicode, suportando praticamente todos os caracteres de todos os idiomas.
Outra característica positiva é o fato do Unitex poder ser livremente modificado e distribuído nos termos de a licença LGPL (Lesser General Public License).
O Unitex, desenvolvido nas linguagens C e Java (J2SE 6), infelizmente não suporta o padrão XCES.
Além disso, exige que os textos de um corpus estejam agrupados em único arquivo para que possam ser analisados, algo não usual.
Philologic Philologic63 é conjunto de ferramentas para processamento de corpus.
A ferramenta suporta anotações TEI Lite (Text Encoding Initiative), usadas em buscas por critérios bibliográficos, tais como:
Título, autor e data de publicação.
Além disso o Philologic pode ser configurado para processar outras versões do TEI ou outros padrões, como o XCES.
O Philologic suporta a criação de subcorpora e dispõe de uma interface web que facilita sua utilização.
Chicago. Para poder ser utilizado através de uma interface web, o Philologic requer a instalação de um servidor web e software adicionais num ambiente Linux.
Esses requisitos podem tornar a instalação complexa e de difícil execução para muitos usuários.
WebCorp A WebCorp é uma ferramenta de busca projetada para buscar e apresentar, de maneira adequada à análise lingüística, exemplos de uso de palavras em textos disponíveis na Web.
A ferramenta refina as buscas na Web ao permitir o uso de caracteres curinga e a busca por padrões (realizando pattern matching), além de acrescentar outros recursos importantes para estudos lingüísticos.
WebCorp foi projetado para, internamente, buscar páginas relacionadas aos termos pesquisados por o usuário fazendo uso de ferramentas de busca já existentes na Web.
Feita essa busca, cada uma das páginas é acessada e analisada, e cada ocorrência do termo com seu respectivo contexto é extraída e apresentada ao usuário.
O formato de apresentação dos resultados é configurável.
Entres as opções disponíveis ao usuário estão o formato de saída e o tamanho da janela de contexto do termo buscado.
O tamanho da janela de contexto pode ser configurado entre uma e 50 palavras.
Em a Figura 12 é mostrado um exemplo de saída do WebCorp para o termo &quot;machine», gerado com uma janela de 8 palavras para a esquerda e para a direita.
As concordâncias foram geradas a partir de páginas publicadas na Internet.
Portal de Córpus O Portal de Córpus é um portal para compilação, manutenção e disponibilização de corpora desenvolvido com recursos do projeto PLN-BR e FAROL67.
O principal objetivo do projeto PLN-BR foi a construção e o compartilhamento de recursos e ferramentas lingüístico-computacionais entre sete importantes grupos de pesquisas brasileiros.
O Portal de Córpus abriga atualmente três corpora de textos jornalísticos extraídos do jornal Folha de São Paulo:
O PLN-BR FULL, o PLN-BR CATEG e o PLN-BR GOLD.
O corpus PLN-BR FULL contém 103.080 textos e conta com quase 30 milhões de tokens.
Os outros dois corpora foram compilados a partir de o primeiro.
O corpus PLN-BR CATEG, compilado para a pesquisa e classificação de textos, contém 30 mil textos e quase 10 milhões de tokens, enquanto o PLN-BR GOLD contém 1.024 textos, 338.441 tokens e suas anotações lingüísticas adicionais.
O Portal de Córpus é inteiramente compatível com o padrão de codificação XCES, visto que as ferramentas do portal permitem o armazenamento e a recuperação de textos em conformidade com o formato.
A Farol (Fortalecimento e Integração das Competências do Processamento da Língua), financiado por a estrutura de arquivos dos documentos do Portal é semelhante à estrutura adotada por o American National Corpus (ANC), que também adere ao XCES.
A estrutura de arquivos para cada documento lógico existente num corpus do Portal é apresentada na Tabela 5.
Essa estrutura faz com que as anotações sejam stand-off, separadas dos dados primários.
Em a estrutura, o arquivo de conteúdo, com os dados primários, é codificado em UTF-16, enquanto os demais arquivos são codificados em UTF-8.
O Portal de Córpus tem código-fonte aberto, bem como são abertas todas as tecnologias que ele utiliza.
Além disso, a estrutura da base de dados e a documentação do Portal estão disponíveis gratuitamente, fazendo com que possa ser facilmente portado para outros servidores.
O Portal de Córpus é baseado numa arquitetura cliente-servidor.
Em o lado do cliente, a interface do Portal pode ser acessada através de um navegador web qualquer, com suporte a applets68.
O Portal foi desenvolvido utilizando as tecnologias Java 2 Platform Enterprise Edition (J2EE):
Java Server Pages (JSP), Java Servlets e Java Standard Tag Library (JSTL).
O lado servidor é composto por dois servidores:
Um servidor de aplicação J2EE, Apache Tomcat, que também funciona como servidor web;
E um servidor de banco de dados, MySQL Server.
Ambos são sistemas gratuitos, robustos, estáveis e amplamente usados por a comunidade de desenvolvedores, além de possuírem código-fonte aberto.
Para adicionar textos aos corpora do Portal, existe uma applet chamada Header Editor.
O Header Editor, além de permitir a inserção de vários textos ao 68 Applets são pequenos aplicativos escritos em Java, embutidos em páginas Html, e executados na Java Virtual Machine (JVM) do browser da máquina cliente.
O Portal suporta o armazenamento de múltiplos corpora.
Cada corpus é armazenado numa base de dados diferente.
Através de suas colunas e tabelas, essas bases de dados do portal mapeiam completamente o formato XCES e a estrutura de arquivos definida para os corpora armazenados no portal.
O Portal provê aos usuários poucas ferramentas para análise e exploração de um corpus.
Entre as funcionalidades disponíveis podemos citar:
A geração de subcorpus a partir de um corpus e a busca de textos baseada em informações armazenadas no cabeçalho dos documentos.
O retorno das buscas e da geração de subcorpus é um arquivo compactado contendo os textos resultantes.
Considerações sobre este capítulo Atualmente ­ apesar de a maior disponibilidade de ferramentas para compilação, processamento e análise de corpora ­ muitos problemas ainda persistem.
Muitas das ferramentas disponíveis são comerciais, dependem de plataformas específicas para serem executadas ou criam padrões próprios de codificação de corpus e de anotações, dificultando a interoperabilidade e o compartilhamento de recursos lingüísticos entre aplicações.
A seleção de ferramentas adequadas às necessidades de cada projeto representa hoje um desafio aos pesquisadores da área.
Outros problemas afetam especialmente os pesquisadores interessados no estudo da língua portuguesa.
Além de a falta de suporte a esquemas de codificação de caracteres adequados, enfrenta- se a escassez de recursos e de ferramentas específicos para nossa língua.
Ferramentas web como o Corpógrafo têm seu desempenho prejudicado por a velocidade de transmissão de grande volumes de texto, e limitações de espaço no servidor.
Some- se a isso o fato de que a maioria das ferramentas exige grandes esforços para limpeza e conversão de dados, principalmente em etapas de pré-processamento.
Esses esforços, muitas vezes realizados de forma totalmente manual, além de dificultar o uso das ferramentas por leigos, tornam mais onerosas as pesquisas com corpus e as deixam mais suscetíveis a erros.
Sendo assim, observamos a oportunidade de preencher algumas das lacunas deixadas por essas ferramentas e optamos por desenvolver uma nova, ferramenta apresentada no próximo capítulo.
A ferramenta Entrelinhas Dadas as considerações apresentadas no capítulo anterior e considerando também a possibilidade de contribuir com o projeto PLN-BR através do Portal de Córpus, optamos por desenvolver e disponibilizar uma ferramenta para compilação e exploração de corpora.
A ferramenta idealizada, que recebeu o nome de Entrelinhas, visa facilitar as atividades relacionadas à compilação de corpora e oferecer funcionalidades de exploração compatíveis com o padrão de codificação dos corpora disponibilizados por o Portal de Córpus.
Parte das funcionalidades é suportada através da integração de bibliotecas e ferramentas como GATE, Apache Lucene69, Apache XMLBeans70, ICU4 J71, JChardet72 e Yahoo!
Search Web Services73.
A Figura 14 apresenta as bibliotecas internas e as relações de dependência com as bibliotecas integradas.
Entrelinhas foi implementada em Java, o que a torna independente de plataforma.
Pode ser utilizada de duas formas:
Como uma aplicação independente, ou como uma biblioteca integrável a outras aplicações Java.
Em as próximas seções descrevemos como um corpus é codificado e como as funcionalidades de compilação foram desenvolvidas e disponibilizadas dentro de a ferramenta.
Codificação de um corpus De maneira simplificada, poderíamos dizer que a ferramenta Entrelinhas compreende um corpus como um conjunto de arquivos XML gravados num mesmo diretório.
Esses arquivos devem poder ser validados conforme o tipo xcesDocType do schema xcesDoc do XCES.
Qualquer outro formato de arquivo não é considerado parte corpus, exceto se referenciado por um dos demais documentos.
A opção por utilizar um diretório como referência para o corpus em detrimento de um XML schema, também do esquema xcesDoc, é em razão de o tipo não suportar links para xcesDocType.
Essa característica obrigaria que todos os textos fossem armazenados num único arquivo, fato que não consideramos desejável.
O tipo xcesDocType é utilizado para armazenar os dados primários com anotações de segmentação.
O tipo suporta também anotações de cabeçalho (tipo xcesHeader do schema xcesHeader).
Em o XCES, o header pode ser embutido dentro de o xcesDocType ou num arquivo separado, desde que seja referenciado no xcesDocType.
Por praticidade, optou- se por manter o header dentro de o documento.
A Entrelinhas é capaz de ler e escrever três informações do header:
Cada texto adicionado ao corpus recebe um código gerado a partir de o do texto original combinado com o hash da data e hora de inclusão.
Essa combinação evita que dois documentos recebam o mesmo código quando textos idênticos forem deliberadamente adicionados.
Muitos compiladores de corpora duplicam textos do corpus com a intenção de obter o balanceamento desejado.
Cada texto adicionado ao corpus gera um arquivo do tipo XML e outros dois arquivos:
Um no formato original, outro em texto puro.
O código do texto é utilizado para a composição do nome destes arquivos conforme a Tabela 6.
As diferenças em relação a os textos do Portal de Córpus não são percebidas por o usuário, pois os textos são &quot;convertidos «automaticamente na primeira leitura.
Sendo assim, supondo um corpus contendo dois textos, um em PDF e outro em RTF, com códigos 0A588 D4BDA9EBC respectivamente, conteria em seu diretório seis arquivos:
O arquivo xcesDocType e a versão em texto puro são codificados em UTF16.
A adoção de esquema de codificação, que suporta adequadamente caracteres acentuados, se deve à preocupação em oferecer funcionalidades adequadas para estudos voltados para língua portuguesa.
Os XMLs gerados por a Entrelinhas atendem ao requisito do nível 175 de conformidade do xcesDocType, que exige a segmentação dos parágrafos do texto.
Um exemplo de documento do tipo xcesDocType gerado por o XCES é apresentado no Apêndice A. Funcionalidades para a compilação de corpora Entrelinhas oferece duas funcionalidades principais para a compilação de corpora:
A compilação de corpora a partir de vários formatos de documentos e a compilação de corpora a partir de documentos disponíveis na Internet.
As funcionalidades podem ser acessadas após a seleção da opção &quot;Compilar corpus «ou &quot;Editar corpus», na janela principal e a seleção de um diretório adequado.
Em a janela de compilação de corpus, são oferecidas as opções para adicionar textos locais, adicionar documentos da Internet (detalhado na Seção 6.2.2), editar ou remover um texto.
A mesma janela permite visualizar numa tabela o título, o código e a data e hora da inclusão ou última edição dos textos.
Os textos da tabela podem ser ordenados em ordem crescente ou decrescente por qualquer um destes dados.
A janela de edição de textos permite ao usuário, quando necessário, fazer a limpeza manual de um texto do corpus sem a necessidade de uma ferramenta externa.
A partir de o editor também é possível abrir a versão original do documento.
Essa funcionalidade visa facilitar a identificação de problemas durante a limpeza dos textos, pois em muitos casos o usuário precisa recorrer ao documento original para verificar alguma parte do texto.
A abertura do documento original é condicionada a associação correta entre a extensão e o software adequado no sistema operacional do usuário.
Atualmente, a compilação de corpora na maioria das ferramentas impõe restrições ao usuário, exigindo, por exemplo, que os arquivos contenham apenas texto puro.
Essa restrição impede que o usuário utilize documentos nos formatos mais populares, sem que antes os arquivos sejam convertidos ou tenham seus textos extraídos manualmente.
A ferramenta Entrelinhas, através de componentes Language Resources do GATE, permite a compilação de um corpus a partir de vários tipos de arquivo.
Estes arquivos são convertidos em texto puro e em seguida convertidos para o formato XML, compatível com o schema xcesDocType, do XCES.
Os formatos suportados são:
TXT (arquivos de texto puro, plain text) DOC (documentos do Microsoft Word) PDF (Portable Document Format) Html (HyperText Markup Language) XML (Extensible Markup Language) RTF (Rich Text Format) EML (Electronic Mail) O componente do GATE responsável por a conversão dos documentos exige como parâmetro de entrada o esquema de codificação do documento (charset).
Para tal tarefa, a Entrelinhas utilizou duas bibliotecas open source, em Java, para detecção automática da codificação dos textos:
Mozilla Chardet (utilizado por o navegador web Mozilla FireFox76) e o ICU (International Components for Unicode, utilizado por diversos produtos da IBM77).
Alguns documentos do tipo DOC e do tipo PDF, dependendo da forma como foram criados e da aplicação que os gerou, podem não ser convertidos corretamente.
Esses documentos podem posteriormente dentro de a Entrelinhas.
A conversão da versão em texto puro para o formato XCES é feita com auxílio da biblioteca Apache XMLBeans, que facilita a manipulação de arquivos XML.
O uso dessa biblioteca foi fundamental devido a a complexidade do formato XCES:
São nove XML Schemas interligados, em que muitas marcações podem aparecer em tags com nomes diferentes.
Essa característica se deve a tentativa do XCES de manter a compatibilidade com as especificações anteriores e algumas marcações do TEI.
Outra funcionalidade, disponível na biblioteca ICU e disponibilizada na ferramenta Entrelinhas, é a identificação automática do idioma dos textos.
Essa funcionalidade permite que o idioma identificado seja informado no cabeçalho do documento.
O grande número de documentos publicados na Internet torna esta rede uma fonte quase inesgotável de textos que podem ser utilizados como recursos lingüísticos.
Através da biblioteca Yahoo!
Search Web Services, a ferramenta Entrelinhas oferece ao usuário a opção de popular um corpus a partir de documentos na Internet.
Essa compilação de corpora a partir de documentos na Internet é feita através da janela mostrada na Figura 19.
O usuário deve especificar os termos da pesquisa, o formato de documento e o idioma desejado.
É possível especificar também o número máximo de resultados que devem ser retornados.
O usuário dispõe de quatro formatos de documentos na busca:
DOC, Html, PDF ou TXT.
Além de o português, é possível optar por oito outros idiomas:
Alemão, árabe, espanhol, francês, inglês, italiano, japonês ou russo.
Em muitas situações, a Internet não é adequada por não ser uma fonte confiável de documentos.
Uma alternativa é tentar refinar os resultados dentro de a Entrelinhas através de expressões que o Yahoo chama de Meta Words78, presentes em várias ferramentas de busca na Internet.
É possível, por exemplo, especificar um domínio para o qual a busca deve ser realizada, digitando «projeto de lei site:
Camara. Gov. Br &quot;para pesquisar por «projeto de lei «no site da Câmara dos Deputados79.
Após disparar a busca, a Entrelinhas envia os parâmetros informados para o serviço de busca do Yahoo através de sua biblioteca e, alguns segundos depois os resultados encontrados são apresentados.
É possível clicar sobre um resultado e verificar a URL, o título da página, e um snippet com aproximadamente 20 palavras.
O usuário pode optar por abrir para visualização e verificar qualquer um dos links retornados no botão &quot;Abrir link».
A URL será aberta no browser padrão.
Os resultados que não interessarem podem ser descartados através do botão &quot;Descartar texto», que removerá o texto da lista.
A o finalizar a escolha dos textos o usuário deve clicar em &quot;Adicionar textos «para fazer o download e adicionar os documentos presentes na lista ao corpus.
Quando nenhum resultado interessar o usuário pode disparar uma nova busca com outros parâmetros.
Funcionalidades para exploração de corpora Em as ferramentas estudadas, existem diversas funcionalidades que permitem ao usuário explorar um corpus.
Em esse aspecto o Portal de Córpus ainda carece de ferramentas que proporcionem aos usuários a possibilidade de explorar e tirar melhor proveito dos corpora disponíveis no Portal.
Em esse sentido, duas funcionalidades de exploração de corpora foram disponibilizadas:
Um gerador de lista de palavras com contagem de ocorrências e um concordanceador (Seção 6.3.2) com tamanho da janela de contexto configurável.
Essas funcionalidades são acessíveis a partir de a janela mostrada na Figura 20.
As funcionalidades citadas foram implementadas com auxílio da biblioteca Apache Lucene, um mecanismo de busca.
Essa biblioteca foi escrita Java e é largamente utilizada na comunidade de desenvolvedores por possuir excelente desempenho.
A idéia inicial de utilizar a biblioteca Ngram Statistics Package80 (NSP), escrita em Perl, na implementação das funcionalidades de exploração de corpora, foi descartada devido a dificuldades encontradas na integração das duas linguagens.
Além disso, programas em Perl requerem a instalação adicional deum software interpretador denomiado ActivePerl81 para serem executados, fato que poderia tornar mais complexa a instalação e distribuição da Entrelinhas.
Existe uma iniciativa em andamento para criação de um plugin82 do NSP para o GATE, porém os resultados disponibilizados ainda estão em versão beta.
O índice gerado por o Lucene é gravado numa pasta denominada &quot;index «dentro de o diretório do corpus e é atualizado quando uma das ferramentas é acessada.
Apesar de o Lucene, por padrão, remover as stopwords, optamos por configurar a indexação de modo que estas sejam também indexadas.
A indexação utilizada diferência palavras acentuadas de não acentuadas, mas não diferência maiúsculas e minúsculas e segmenta palavras a cada ocorrência de caractere diferente de letra ou número.
Por exemplo, a frase84:
O governo dos Estados Unidos confirmou nesta quarta-feira a realização no sábado de uma reunião do G20, grupo atualmente presidido por o Brasil.
A lista de palavras apresenta o número de ocorrências de cada item lexical armazenado no índice e sua freqüência em relação a o número total de itens lexicias.
Além disso, é apresentado o número de textos em que cada item ocorre e sua freqüência em relação a o total de textos do corpus.
Em o topo da janela é exibido o total de textos do corpus, o total de itens lexicais e o total de itens lexicais distintos (que não se repetem).
A lista pode ser ordenada em ordem alfabética, crescente ou decrescente por qualquer uma das colunas exibidas.
O concordanciador do Entrelinhas (Figura 22) apresenta as concordâncias existentes para um termo específico dentro de um corpus.
O usuário pode escolher o tamanho do contexto que deseja visualizar para cada concordância, em caracteres.
O termo pesquisado sempre aparece alinhado e em destaque nos resultados.
O Entrelinhas busca, através do Lucene, os textos em que os termos pesquisados ocorrem.
Depois, utilizando funções nativas do Java, percorre cada um dos textos retornados, para extrair as concordâncias.
Considerações sobre este capítulo Em este capítulo apresentamos a Entrelinhas, uma ferramenta desenvolvida a partir de percepção de necessidades no âmbito da compilação e da exploração de corpus em língua portuguesa.
Esta ferramenta a facilita a compilação e disponibiliza funcionalidades essenciais para exploração de corpora.
A ferramenta adere a um formato de codificação XCES, compatível com o Portal de Corpus, contribuindo com o intercâmbio de recursos no Projeto PLN-BR.
Em o próximo capítulo, apresentaremos algumas experiências realizadas e mostraremos o relato de um usuário especializado quanto a as funcionalidades do programa, em decorrência de suas percepções no uso da Entrelinhas.
Experiências de uso Visando obter indicação da confiabilidade e da usabilidade do software desenvolvido, realizamos iniciativas de uso e acompanhamos os usuários através de relatos de uso.
O objetivo dessas experiências foi conhecer a opinião do usuário especializado quanto a as funcionalidades do programa.
Para tanto identificamos um usuário com experiência em lingüística de corpus e habituado a utilizar ferramentas para compilação e exploração de corpora.
Este usuário foi convidado a analisar se as funcionalidades apresentam os resultados esperados, verificar se o desempenho é aceitável e tentar revelar falhas ainda não descobertas.
Por falta de tempo hábil, optamos neste momento por não incluir a integração com o Portal de Córpus nos experimentos.
O usuário especializado ficou livre para seguir seu próprio padrão de utilização de ferramentas dessa natureza, e definir a estratégia de como avaliar, os tipos de testes a serem aplicados, as técnicas e os critérios adotados.
Para não influenciar no resultado, optou- se por não oferecer e não especificar uma coleção de textos a ser utilizada.
Os requisitos mínimos para o pleno funcionamento da Entrelinhas foram informados:
Java Plataform, Standard Edition 6 instalado;
Gate 4.0 (build 2752) instalado;
Não disponibilizamos ao usuário especializado ferramentas de teste automatizadas ou qualquer outro software além de os listados acima.
Primeira experiência de uso Entrelinhas foi utilizada por a Profa..
Susana de Azeredo85.
Ela recebeu um breve treinamento e solicitou- se que avaliasse as funcionalidades de compilação e exploração de corpus da Entrelinhas com textos de sua preferência.
Solicitamos que a avaliação fosse redigida na forma de um parecer relatando sua experiência, impressões, sugestões, dificuldades, problemas encontrados, pontos positivos e aspectos a serem melhorados.
Ao longo de essa seção apresentaremos trechos do parecer, seguidos imediatamente por os comentários do autor referentes ao trecho exposto.
Trechos do parecer que se referiam a falhas na realização do experimento ou erros que foram imediatamente corrigidos antes da realização do segundo experimento, foram suprimidos desta seção.
O parecer completo pode ser lido no Anexo A. Susana de Azeredo O objetivo aqui é fazer um relato de um teste com o software O teste foi dividido em dois momentos.
Em um primeiro momento, foi utilizado um corpus previamente montado de cerca de 100 mil palavras (todos os arquivos desse corpus eram de formato txt).
O objetivo neste primeiro momento foi testar a ferramenta de contagem de palavras e o concordanciador.
Em um segundo momento, foi feito a montagem de um corpus através do Entrelinhas.
O objetivo no segundo momento, foi testar a efiCiência do Entrelinhas com relação a a montagem do corpus.
A seguir, detalhamos os dois momentos e as observações feitas.
Em este primeiro momento de teste, utilizamos um corpus previamente montado, o qual chamaremos aqui de corpus QUIM.
Esse Susana de Azeredo possui graduação em Letras (Bacharelado) Habilitação Português-Inglês por a Universidade Federal do Rio Grande do Sul.
Possui mestrado em Teorias do Texto por a mesma universidade.
Tem experiência na área de Lingüística, com ênfase em Lingüística de Corpus e Terminologia, atuando principalmente nos seguintes temas:
Textos de química, corpus, lingüística de corpus, coesão e expressões anunciadoras de paráfrase.
Está habituada a utilizar a ferramenta Oxford WordSmith Tools na compilação e exploração de corpora.
Atualmente é professora substituta na Faculdade de Letras da UFRGS.
Química Geral. O corpus QUIM é composto de cerca de 100 mil palavras e os arquivos estavam todos em formato txt.
O objetivo aqui foi testar as ferramentas de contagem de palavras e o concordanciador.
Com relação a a contagem de palavras:
C) A ferramenta de contagem de palavras revela o número total de palavras do corpus (tanto os itens lexicais quanto os itens lexicais distintos).
Esse informação é muito relevante para um lingüista.
No entanto, seria útil revelar também o número de palavras de cada texto.
Essa é uma informação importante para o lingüista, pois é possível fazer uma comparação entre os textos do corpus, traçando diferentes perfis de textos.
Além disso, não tem como selecionar apenas um dos textos do corpus para fazer uma análise específica ou uma comparação desse texto específico com o restante do corpus.
Em a versão atual, esse tipo de comparação pode ser feito se os textos a comparar forem compilados em corpora distintos;
O mesmo procedimento vale para a análise da contagem de palavras num texto específico.
A apresentação da contagem de palavras, de forma individualizada para cada texto ou conjunto de textos, foi registrada como sugestão de melhoria futura.
D) Uma informação interessante para um lingüista e que não consta no Entrelinhas é a relação entre o número total de palavras do corpus e o número de palavras diferentes do corpus.
Essa relação permite ver a variedade vocabular de um texto.
A variedade vocabular pode ser calculada a partir de as contagens apresentadas no topo da janela de contagem de palavras.
A apresentação mais clara dessa informação foi registrada como sugestão de melhoria futura.
E) Com relação a o tempo que leva para finalizar a operação, pode- se dizer que foi bem eficiente.
Há uma barra que indica que o Entrelinhas está fazendo o processamento estatístico, o que é bem útil.
Uma única observação é que essa barra, às vezes, tranca e ficamos sem saber se o processo já terminou ou não.
O problema foi identificado e corrigido.
F) As informações que aparecem nas colunas da janela de contagem de palavras são bem relevantes e úteis, principalmente, a coluna &quot;textos».
Essa coluna indica em quantos dos textos do corpus aparece a palavra.
Clicando no cabeçalho das colunas, é possível ordenar os dados em ordem crescente ou decrescente.
Com relação a o concordanciador:
A) Em o concordanciador, não aparece o total de vezes que aparece, no corpus, a palavra buscada.
Para isso, é necessário voltar para a janela de contagem de palavras.
Seria interessante aparecer junto com as concordâncias da palavra &quot;tabela», por exemplo, quantas vezes ela aparece.
B) Não aparece de que texto é cada concordância.
Seria interessante colocar ao lado de a concordância, o texto onde ela aparece.
A identificação dos textos junto às concordâncias foi registrada como sugestão para melhoria futura.
C) Quando se abre o concordanciador, há uma formatação de 30 caracteres à direita e à esquerda da palavra.
Quando se amplia esse horizonte para 50 ou mais, o Entrelinhas se perde, não mostrando mais a palavra buscada.
Para algumas pesquisas com corpus, o horizonte mostrado ali é muito reduzido.
Seria necessário que o usuário pudesse aumentar o horizonte para, no mínimo, ver a frase inteira.
Quando se tenta buscar o horizonte da frase inteira, no Entrelinhas, ele se perde.
A concordância pode não ser exibida corretamente quando o número de caracteres não cabe numa única linha da tabela, por exceder o tamanho da janela.
O usuário pode redimensionar a janela para aumentar o espaço disponível.
Para facilitar a visualização do contexto, estuda- se a possibilidade de levar ao texto a partir de a concordância, registrada como sugestão para melhoria futura.
D) Algo bem útil seria poder acessar o texto original a partir de a concordância mostrada.
Por exemplo, clicando na concordância, aparece o texto original com a palavra buscada onde ela se encontra no texto.
Assim, é possível visualizar a frase inteira e até mesmo o parágrafo em que a palavra buscada aparece.
A proposta foi identificada no item anterior e registrada como sugestão para melhoria futura.
O segundo momento envolveu a montagem de um corpus, utilizando os recursos do Entrelinhas.
Esse corpus, que será chamado aqui de Teste, contou com 23 arquivos.
Entre esses arquivos, há textos de formato Doc, Rtf, Pdf, Txt e Html.
Com relação a a compilação do corpus, adicionando textos locais:
A) Os arquivos Doc, rtf e txt foram incorporados ao corpus sem problemas maiores.
A única coisa é que os gráficos, as tabelas e as figuras que constavam no arquivo foram transformados em símbolos.
Isso exigiu uma limpeza dos textos antes que fossem processados no contador de palavras e no concordanciador.
Entrelinhas utiliza os conversores embutidos no GATE.
A limpeza dos textos é uma atividade muito comum na compilação de corpus, e o usuário pode realizar- la de dentro de o Entrelinhas.
Estuda- se a utilização de outros conversores em trabalho futuro, ou a aplicação de algoritmos que realizem parte da limpeza de maneira automática.
B) Os arquivos.
Pdf também foram incorporados sem maiores problemas.
Eles tiveram uma incidência maior de símbolos do que os outros tipos de arquivo, exigindo uma tempo maior na limpeza dos textos.
É ótimo que há a possibilidade de limpeza do texto dentro de o Entrelinhas.
Arquivos. PDF são bastante complexos porque podem ser gerados de diferentes formas por diferentes aplicações.
Essa dificuldade pode ser observada até mesmo com um simples copiar e colar de parte de um texto em PDF, principalmente quando o texto inclui imagens ou fórmulas.
Além disso, alguns documentos em PDF estão protegidos contra cópia e a conversão pode apresentar resultados inesperados.
C) Depois que os textos foram adicionados, todos os tipos de arquivo exigiram uma verificação da formatação do texto e da existência (e sua remoção) de símbolos.
Conforme dito anteriormente, a limpeza dos textos faz parte da preparação dos textos e é uma atividade muito comum na compilação de corpus (ver Seção 3.3), e o usuário pode realizar- la dentro de o Entrelinhas.
Com relação a a compilação do corpus, adicionando textos da Internet:
A) A primeira coisa que aparece na janela &quot;Adicionar textos da Internet «é o campo &quot;Termo».
Essa palavra não seria a mais adequada para uso.
A definição de &quot;Termo «em Lingüística é muito controversa.
Sugiro a utilização de &quot;PALAVRA DE BUSCA «ou &quot;PALAVRACHAVE».
A sugestão resultou na alteração de &quot;Termo «para Palavra-chave.
B) A partir de a colocação do &quot;Termo», aparecem, no mínimo, 10 sites.
O resumo que aparece no campo &quot;resumo «é muito reduzido, tornando- se pouco confiável para que um texto possa ser selecionado e adicionado ao corpus.
Seria muito bom que o botão &quot;abrir link «estivesse ativo.
Assim, seria possível verificar se o conteúdo da página é realmente útil para ser adicionado ao corpus.
O número máximo de resultados apresentados pode ser escolhido por o usuário na mesma tela.
É possível inclusive retornar menos resultados através da entrada de mais palavras no campo de busca, restringindo a procura e obtendo resultados mais relevantes, como ocorre com qualquer outro mecanismo de busca na Internet.
O botão &quot;Abrir link «pode não funcionar adequadamente quando o endereço retornado for muito extenso e apresentar muito parâmetros.
Mesmo assim, nestes casos os documentos poderão ser adicionados ao corpus e o usuário poderá decidir mais tarde por manter- los ou não.
O tamanho do resumo, cerca de 20 palavras, é uma limitação da biblioteca utilizada:
Yahoo Search API. Essa limitação é idêntica ao sistema de busca do Yahoo disponível na web e semelhante à do Google.
C) Muitas vezes, o lingüista já tem um site de onde ele quer retirar textos para incorporar ao seu corpus ou até mesmo para montar seu próprio corpus.
Em o Entrelinhas não é possível buscarmos um determinado site.
Quanto a a busca em site específico, ela pode ser feita através da utilização da expressão &quot;site:»
em o campo de procura.
Por exemplo, para procurar páginas relacionadas a &quot;trabalho «em sites do governo, poderíamos digitar:
&quot;trabalho site:
Gov. Br».
Essas expressões são chamadas de meta words de busca e estão disponíveis na página de dicas do Yahoo86.
D) Se os 10 primeiros sites retornados não são úteis, se faz uma nova busca.
No entanto, os sites que aparecem continuam sendo os mesmos.
Inclusive, se aumentarmos os números dos sites para serem mostrados, o Entrelinhas retorna os mesmos primeiros 10 e mais outros 10 diferentes.
Se aumentarmos para 30, aparecem sempre os mesmos primeiros 20 e mais 10 diferentes.
Assim, ficamos sempre com os mesmos sites aparecendo.
É esperado que buscas idênticas retornem resultados e posições idênticas.
Estuda- se a possibilidade de paginar e navegar nos resultados para evitar esse comportamento inconveniente.
E) Não consigo selecionar apenas um dos textos da busca na Internet para adicionar ao corpus.
Foi preciso adicionar todos ao corpus e só depois selecionar os necessários.
Os arquivos.
Pdf e.
Doc que continham figuras e gráficos ou tabelas também precisaram de uma limpeza antes que fossem processados.
Em o mais, o texto estava perfeito.
Os textos podem ser removidos antes de serem adicionados ao corpus através do botão &quot;Descartar texto».
Registramos, para trabalhos futuros, o estudo de um maneira de melhorar a interface, deixando- a mais intuitiva.
F) Aplicam- se aqui as mesmas considerações feitas acima no primeiro momento da pesquisa com relação a a contagem das palavras e ao concordanciador.
Os mesmos comentários feitos anteriormente, relacionados ao uso de uma versão mais antiga, são válidos aqui.
A) Não é possível selecionar um diretório vazio para montar meu corpus.
O diretório sempre precisa conter algum texto.
Assim, foi necessário colocar um texto dentro de o diretório para que o trabalho pudesse começar (mesmo que esse texto não fosse ser utilizado no meu corpus).
Fica a pergunta:
Esse primeiro texto que foi necessário colocar é contado como parte do meu corpus?
Se sim, o resultado do número de palavras do corpus pode ser irreal, pois o Entrelinhas está contando um texto que eu não queria que estivesse ali.
O problema foi identificado e solucionado.
O texto que necessitou ser adicionado antes da compilação não interferiu na contagem nem no concordanciador.
Para ser &quot;visto «por a ferramenta ele deveria estar no formato b) A barra &quot;procurando concordâncias «tranca seguidamente.
A janela de status fecha automaticamente após finalizada a operação.
Em algumas situações, a ferramenta não conseguia fechar janela ao terminar e era preciso clicar no botão de fechar para prosseguir o uso normal, sem nenhuma concordância perdida.
O problema no encerramento foi identificado e solucionado.
C) Em a janela de compilação do corpus, os textos aparecem numa determinada seqüência.
No entanto, eles saem dessa ordem se selecionamos algum para editar- lo.
Em que ficar arrumando sempre que se volta para essa janela.
Fica confuso.
Assim como na lista de palavras, nessa janela de compilação e edição de corpus também é possível ordenar os textos clicando no cabeçalho das colunas.
É possível ordenar os textos por o código, por o título, ou por a data da última modificação, no entanto após uma edição a ordenação por o código é sempre restaurada porque as demais colunas podem ter sido alteradas na edição.
D) Uma sugestão com relação a o concordanciador.
Algumas pesquisas lingüísticas focalizam uma palavra e suas derivações.
Por exemplo, uma pesquisa que focaliza o verbo Poder, talvez queira observar as derivações Poderá, Poderia, Poderemos, Pode, etc..
Para isso, se coloca o radical POD e um asterisco ao lado (POD*).
Essa forma retorna o verbo Poder e seus derivados.
O Entrelinhas não retorna essa informação.
Tentei fazer isso com o verbo Poder (POD*) e com advérbios terminados em ­mente(* Mente), mas não obtive resposta.
O uso de expressões regulares ainda não é suportado.
Registramos a sugestão para melhoria futura.
E) O Entrelinhas faz uma separação entre palavras no singular e plural.
Se eu coloco a palavra &quot;Tabela «no singular, aparece apenas a palavra no singular.
O mesmo ocorre com o plural.
Isso é muito bom.
O concordanciador e a contagem de palavras diferenciam palavras acentuadas e não acentuadas.
As palavras &quot;para «(preposição), &quot;pára «(verbo) ou &quot;Pará «(o Estado), por exemplo, apresentarão resultados diferentes.
Segunda experiência de uso Para verificar se os problemas encontrados anteriormente na contagem de palavras repetiam- se na versão mais recente da Entrelinhas, solicitou- se à Susana Azeredo uma nova utilização.
A avaliadora complementou o parecer anterior com o relato a seguir, acompanhado dos comentários do autor:
Apareciam Em a Lista De Contagem:
Eu selecionei novamente o corpus QUIM e, realmente, agora as duas palavras apareceram.
Talvez tenha dado algum problema na minha máquina quando fiz a exploração do corpus anteriormente.
2) Interface:
A Interface do Entrelinhas é muito elegante, agradável e de muito bom gosto.
Também, não há uma poluição visual que confunde o usuário.
3) USABILIDADE:
A usabilidade do Entrelinhas é boa.
Em o início, o Filipe deu algumas dicas para o primeiro acesso.
Mas, acredito ser possível o acesso sem preparação antecipada sobre o programa.
Penso ser interessante que o &quot;botão «&quot;explorar corpus «possa aparecer na janela de &quot;compilação do corpus», o que tornaria a atividade que está sendo realizada mais dinâmica.
A escolha do diretório é bastante recorrente e, em alguns casos, parece desnecessária.
Registramos a observação e estudaremos uma maneira de tornar a interface mais fácil de ser utilizada.
4) Instalação:
A instalação do Entrelinhas foi fácil e rápida.
Também, não foi necessário instalar outros programas para que o Entrelinhas pudesse ser utilizado.
Os programas necessários (Java 6 e Gate) já haviam sido instalados no experimento anterior.
Terceira experiência de uso Documentos jurídicos são em geral muito extensos e pouco estruturados.
A área jurídica produz um grande volume de textos e carece de ferramentas específicas para processar- los.
Esses documentos ­ redigidos por Juízes, Desembargadores, advogados e seus assessores ­ são freqüentemente consultados por os mesmos, na busca de jurisprudência que fundamente suas decisões ou sustente seus apelos.
Em esse sentido, obtivemos junto ao Egrégio Tribunal Regional do Trabalho da 4ª Região decisões judiciais publicadas entre junho de 1993 e abril de Internet, são acórdãos e representam decisões colegiadas na segunda instância da Justiça do Trabalho.
Os acórdãos, em formato RTF, totalizam aproximadamente 500 mil documentos e 13 gigabytes de dados e foram gravados em 5 DVDs.
Tabela 7 ­ Resumo da coleção de documentos para o terceiro estudo de caso Número de textos:
Formato dos arquivos:
Tamanho total:
Tamanho médio dos arquivos:
Os documentos foram compilados num corpus na ferramenta Entrelinhas.
O volume de dados gerado e os tempos de compilação e indexação foram registrados.
Coletamos os tempos de processamento da lista de palavras e duas buscas no concordanciador.
A lista de palavras foi gerada em quatros segundos, e contabilizou o total de 1.438.908 itens lexicais, e 25.127 itens lexicais distintos.
Além de a utilização por Susana Azeredo, as mestrandas Lilian Figueiró Teixeira87 e Josiane Fountora Brandolt88, em estágio mais incipiente do trabalho, contribuiram efetuando os primeiros relatos de uso que permitiram corrigir falhas importantes e identificar os desejos de potenciais usuários da ferramenta.
As sugestões coletadas proporcionaram uma visão do que ainda precisa ser feito e poderão guiar trabalhos futuros.
Em as considerações finais, retomaremos os assuntos que foram abordados ao longo de o trabalho e lições aprendidas, e indicaremos trabalhos fututos sobre a ferramenta Entrelinhas e sobre o tema estudado.
Mestranda em Lingüística Aplicada na Universidade do Vale do Rio dos Sinos (UNISINOS) e graduada em Letras Licenciatura -- Português e Inglês por a mesma universidade.
Tem experiência na área de Lingüística, com ênfase em Lingüística Computacional, atuando principalmente nos seguintes temas:
Web Semântica e Informática na Educação.
Considerações finais Em este trabalho apresentamos um estudo sobre a compilação e a exploração de corpora.
Iniciamos apresentando o conceito de corpus e em seguida introduzimos as noções de tamanho e representatividade de um corpus.
Vimos que essas características são bastante subjetivas e dependem muito do contexto em que se inserem.
Em seguida, apresentamos o modo como corpora costumam ser classificados quanto a o propósito para o qual foram criados, o gênero, as áreas de domínio, o idioma, e a época em que os textos foram produzidos.
Percebemos que a taxionomia de um corpus não é um assunto consolidado e que a mesma classificação pode aparecer sob diferentes nomes.
Fizemos referência a alguns dos maiores corpora em língua inglesa e língua portuguesa hoje existentes e abordamos aspectos sobre uso da web como um corpus.
Notamos o crescimento no número de instituições que disponibilizam corpora cada vez maiores e mais ricos em anotações.
É notável o potencial de uso da web como um corpus, revelado no crescente número de publicações e projetos sobre o tema.
Abordamos as principais etapas envolvidas na compilação de um corpus:
O projeto, a coleta dos documentos, a preparação dos textos e a codificação do corpus.
Discutimos questões importantes sobre critérios relacionados ao tamanho, ao balanceamento e à representatividade dos textos no projeto de um corpus.
Descrevemos o modo como tarefas tais como conversão de formatos, limpeza de textos, uso de meta-dados, inserção de anotações e codificação dos textos impactam nos projetos e são afetadas por a disponibilidade de ferramentas adequadas.
Apresentamos brevemente algumas aplicações do uso de um corpus e funcionalidades utilizadas por os lingüistas para a exploração de corpora.
Descrevemos ferramentas para corpora que dispõem de parte dessas funcionalidades, apresentando suas principais características.
Observamos alguns problemas em aberto nessas ferramentas, como a dificuldade em criar um corpus com documentos em formatos que estamos habituados a utilizar, e o intercâmbio de recursos.
Assim, enxergamos a possibilidade de contribuir com o Portal de Córpus através de uma nova ferramenta, a Entrelinhas.
Apresentamos as funcionalidades da Entrelinhas, que utiliza uma codificação compatível com textos do Portal.
Relatamos experiências de uso da Entrelinhas e tivemos a oportunidade de receber o retorno de uma usuária qualificada e experiente no uso desse tipo de ferramenta.
Os relatos confirmaram, a nosso ver, a contribuição esperada com a ferramenta e nos permitiram que coletássemos diferentes sugestões de melhorias.
Entre estas melhorias, destacamos:
A paginação dos resultados da busca de textos na Internet, seleção de textos para a lista de palavras, comparação de listas de palavras, suporte ao uso de expressões regulares no concordanciador, acesso aos textos completos a partir de o concordanciador e alterções na interface no intuito de deixar- la mais intuitiva e mais fácil de usar.
Sabemos que, além de as melhorias apontadas, há muitos outros pontos em que a Entrelinhas precisa evoluir, avançando rumo a uma maior integração com o Portal de Córpus e oferecendo mais funcionalidades.
Lembramos que este trabalho resultou, além de a própria ferramenta Entrelinhas89, na apresentação de um pôster no VI Encontro de Linguítica de Corpus90, ocorrido em setembro de 2007, em São Paulo, e a publicação de um artigo nos anais91 do mesmo evento e em livro.
Consideramos importante também a possibilidade de explorar e melhor aproveitar em trabalhos futuros os textos disponibilizados por o Tribunal Regional do Trabalho 4ª Região, permitindo novas publicações na área.
