O projeto de MPSoCs é uma clara tendência na indústria de semicondutores.
Os MPSoCs são capazes de executar várias aplicações em paralelo, suportando carga dinâmica de trabalho, ou seja, aplicações podem iniciar a qualquer momento.
Outra característica importante em MPSoCs é QoS (qualidade de serviço), pois aplicações multimídia e de telecomunicações possuem requisitos estritos de desempenho, os quais devem ser respeitados por o sistema.
O crescimento constante do número de núcleos em MPSoCs implica numa questão importante:
Escalabilidade. Apesar de a escalabilidade oferecida por NoCs, e o processamento distribuído permitindo a execução de carga dinâmica de trabalho, os recursos dos MPSoCs devem ser gerenciados para proporcionar o desempenho esperado.
Tarefas de gerenciamento incluem acesso de entrada/ saída a dispositivos externos ao MPSoC, mapeamento de tarefas, migração de tarefas, monitoramento, DVFS, de entre outras.
Um único elemento de processamento (PE), responsável por a gerência dos recursos pode se tornar um gargalo no desempenho do sistema, já que este PE vai servir a todos os PEs do sistema, aumentando sua carga de trabalho e criando regiões com congestionamento de tráfego (hot-spots).
Uma alternativa para garantir escalabilidade é descentralizar ou distribuir as funções de gerenciamento do sistema.
Duas abordagens principais de gerência são discutidas na literatura:
Um gerente por aplicação, ou um gerente por região do MPSoC.
A segunda abordagem é preferível, já que o número de recursos utilizados no gerenciamento permanece constante, independentemente do número de aplicações em execução na MPSoC.
As regiões são definidas como clusters.
Todas as tarefas das aplicações são executadas num cluster, se possível.
Em relação a o tamanho do cluster, ele pode ter seu tamanho modificável em tempo de execução para permitir o mapeamento de aplicações com um número de tarefas maior do que seus recursos disponíveis.
Este trabalho propõe uma gerência distribuída de recursos em MPSoCs, utilizando um método de clusterização, permitindo que o tamanho do cluster seja modificado dinamicamente.
Esse sistema inicializa cada cluster com um tamanho fixo, e durante a execução das aplicações, os clusters podem requerer recursos a seus clusters vizinhos para mapear tarefas.
Os testes foram executados utilizando a plataforma HeMPS, e foram comparados o desempenho do método de gerência centralizado contra o método de gerência distribuído.
Os resultados mostram uma importante redução no tempo total de execução das aplicações e no número de hops entre as tarefas (menor energia de comunicação) utilizando o método de gerência distribuída.
Os resultados também avaliam o método de reclusterização, utilizando monitoração de desempenho e migração de tarefas.
Palavras-chave: MPSoC, NoC, Gerência Distribuída, Mapeamento, Migração de Tarefas.
Distributed Resource Management in MPSoCs ­ Task Mapping and Task Migration Com o avanço da tecnologia de fabricação de circuitos integrados, é possível criar transistores cada vez menores, possibilitando o desenvolvimento de sistemas completos num único chip, denominados Systems-on-Chip (SoCs).
Um SoC é um circuito integrado que implementa a maioria ou todas as funções de um sistema eletrônico completo.
Muitas aplicações requerem SoCs com vários processadores para poder suprir seus requisitos de desempenho.
Um SoC que contém diversos elementos de processamento (PEs, em inglês, processing element) é chamado de Multiprocessor System-on-Chip (MPSoC).
A utilização de MPSoCs é uma tendência no projeto de sistemas embarcados.
Um MPSoC consiste de uma arquitetura composta por recursos heterogêneos, que podem incluir múltiplos processadores, módulos de hardware dedicados, memórias e um meio de interconexão.
Os MPSoCs estão presentes em várias aplicações comercias, sendo amplamente utilizados na área de redes, telecomunicação, processamento de sinais, multimídia, entre outras.
O desempenho destas aplicações pode ser otimizado se estas forem particionadas em tarefas, as quais são executadas em paralelo nos diversos recursos do MPSoC.
Define- se tarefa como um conjunto de instruções e dados, com informações necessárias à sua correta execução num dado Os MPSoCs podem receber uma carga dinâmica de trabalho, ou seja, aplicações podem ser carregadas em tempo de execução.
Por exemplo, um dado MPSoC pode estar executando o tratamento de um fluxo de dados para processamento de telefonia 4G (LTE), e durante o processamento desta aplicação o usuário inicia o processamento de vídeo em altaresolução.
Apesar de a escalabilidade oferecida por NoCs e o processamento distribuído, permitindo a execução de carga dinâmica de trabalho, os recursos dos MPSoCs devem ser gerenciados para proporcionar o desempenho esperado.
Tarefas de gerência incluem o acesso a dispositivos de entrada/ saída, mapeamento tarefas, migração de tarefas, monitoramento de desempenho, DVFS, de entre outras.
Um único elemento de processamento (PE), responsável por a gerência dos recursos pode se tornar um gargalo no desempenho do sistema, já que este PE vai servir todos PEs do sistema, aumentando sua carga de trabalho e criando regiões com congestionamento de tráfego (hot-spot).
Uma alternativa para garantir à escalabilidade é descentralizar ou distribuir as funções de gerenciamento do sistema.
As atuais tendências apontam para projetos de MPSoCs com dezenas de PEs.
O Intel SCC e a família de processadores Tilera Tile-Gx são exemplos oriundos da indústria.
Ambas as arquiteturas possuem um grande conjunto de núcleos que estão ligados através de uma rede intrachip (NoC).
O Intel SCC possui 48 núcleos e o Tilera atualmente possui até 100 núcleos por chip.
Mil núcleos por chip é uma perspectiva tecnológica que deverá torna- se realidade dentro de uma década.
Assim, o problema de escalabilidade é real e de significativa relevância.
Se novos paradigmas de projeto não forem desenvolvidos, sistemas com múltiplos núcleos irão sofrer de baixa eficiência, uma vez que esses sistemas tendem a usar grande parte de sua comunicação e capacidade de computação com o gerenciamento de seus próprios recursos, ao invés que usar essa capacidade de computação para executar mais eficientemente as aplicações.
Objetivos Os objetivos deste trabalho compreendem objetivos estratégicos e específicos, e são definidos a seguir.
Objetivos estratégicos:
Domínio da tecnologia de projeto de sistemas multiprocessados em chip (MPSoC);
Domínio dos mecanismos de comunicação e controle em sistemas operacionais (microkernel) em MPSoCs;
Domínio das técnicas de gerência distribuída de recursos em MPSoC;
Domínio de técnicas de mapeamento distribuído de tarefas e de migração de tarefas em MPSoCs.
Objetivos específicos:
Definir qual das técnicas de gerência distribuída de recursos é mais eficiente e quais as vantagens e limitações de cada uma;
Alterar o MPSoC HeMPS para que ele passe de uma gerência centralizada de recursos para uma gerência distribuída;
Definir uma política de desfragmentação das aplicações no MPSoC;
Avaliação de desempenho do MPSoC desenvolvido.
Comparação de desempenho entre MPSoCs com gerência centralizado (HeMPS) e gerência distribuída.
Contribuições O presente trabalho de Dissertação tem por contribuição principal a definição e implementação de um MPSoC com gerência distribuída de recursos.
Esta arquitetura permite estudar quando a opção por gerenciamento centralizado ou distribuído é recomendada e, no caso de gerenciamento distribuído, qual a melhor estratégia a ser adotada.
Outra importante contribuição é a definição do processo de teste das arquiteturas através de testes de regressão.
Estes testes permitem avaliar de forma automatizada dezenas de casos de testes, permitindo a verificação da corretude das implementações assim como a obtenção automatizada de resultados.
Estrutura do Documento migração de tarefas e mapeamento distribuído, apresentando em seu final uma tabela comparativa entre os trabalhos revisados e o trabalhado proposto por esta Dissertação.
O Capítulo 3 descreve a plataforma HeMPS, utilizada como MPSoC de referência.
O Capítulo 4 apresenta o desenvolvimento do mecanismo de gerência distribuída de recursos.
O Capítulo 5 apresenta o mecanismo de reclusterização proposto por este trabalho.
O Capítulo 6 apresenta resultados experimentais, fazendo uma comparação entre gerência centralizada e distribuída.
O Capítulo 7 apresenta as conclusões desta Dissertação, publicações e trabalhos futuros.
Em este Capítulo é feita uma revisão no estado-da-arte em gerência distribuída em MPSoCs, que será o foco principal do trabalho proposto, seguido por uma revisão sobre mapeamento distribuído e migração de tarefas, que também são técnicas utilizadas na presente Dissertação.
Controle Distribuído Em foi apresentado um esquema de gerente de recursos distribuído em tempo de execução para MPSoCs, chamado DistRM.
Ele foi projetado para ser completamente distribuído, sem qualquer sincronização ou comunicação global, tornando o sistema escalável.
Foi empregado o princípio de multiagentes para gerenciar os recursos, em o qual cada agente gerência uma aplicação.
Cada agente tem por objetivo aumentar a aceleração de suas aplicações procurando outros PEs do sistema que possam ser usados.
Com isso, ele usa a ideia de aplicações maleáveis, para que as aplicações se adaptem aos PEs adicionais.
Essas aplicações maleáveis são capazes de adaptar seu grau de paralelismo ao número de núcleos atribuídos dinamicamente.
Quando uma nova aplicação está prestes a entrar no MPSoC, seu agente é iniciado num PE qualquer do sistema.
No entanto, o agente acabará migrando posteriormente para PEs próximos à aplicação.
O PE inicial que é escolhido aleatoriamente, atua como uma semente para a busca de recursos.
Como o agente não conhece o estado global do sistema, ele tenta alocar de forma aleatória a aplicação num recurso disponível do sistema.
Para reduzir a distância média de comunicação, escolhem- se regiões próximas do agente para se alocar aleatoriamente.
Caso não exista tal região, o agente vai aumentando o seu espaço de busca.
Depois que um agente descobriu o conjunto inicial de PEs para a sua aplicação, ele não para de tentar aumentar o desempenho de sua aplicação através de uma busca constante de novos PEs para alocar a aplicação.
Para ser capaz de avaliar como o sistema multiagente executa, foi criado um ambiente de simulação completo em nível de sistema, capaz de simular configurações arbitrárias de um sistema multiprocessado.
A métrica usada para comparar o método proposto a um método centralizado foi a carga total de trabalho de todas as aplicações, dividido por a soma dos tempos de todas as aplicações em cada simulação, denominada speedup.
Os resultados da Figura 1 mostram que o método distribuído tem um desempenho inferior comparado ao método centralizado usando o speedup como métrica.
De acordo com os Autores, à medida que o número de PEs aumenta, a diferença entre o mapeamento distribuído usando a técnica aleatória aproxima- se ao mapeamento centralizado utilizando uma heurística com visão completa do sistema.
Esta desvantagem deve- se à escolha aleatória dos PEs a receberem tarefas e ao elevado número de migração de tarefas.
Embora os resultados favoreçam o método centralizado, a proposta do método DistRM é capaz de gerenciar recursos num sistema com dezenas de PEs, com speedup próximo a o do método centralizado.
A diferença tende a reduzir- se, segundo os Autores, com heurísticas melhores de mapeamento e restrições no número de migrações de tarefas.
Em o trabalho de foram propostas duas versões de gerenciadores de recursos distribuídos num MPSoC, os quais são escaláveis tanto em número de aplicações quanto em número de processadores e comparados com um gerenciador de recursos centralizado.
O primeiro gerenciador de recursos proposto chama- se Credit Based.
Ele pode ser usado para aplicações que tenham restrições rígidas de desempenho, ou seja, seu desempenho não pode ser superior que um determinado valor, mesmo se os recursos estão disponíveis para ter um melhor desempenho.
O segundo gerenciador de recursos proposto chama- se Rate Based.
Ele é adequado para aplicações que podem ter mais desempenho do que um nível mínimo se os recursos de computação estiverem disponíveis.
Por exemplo, codificadores de streaming são uma aplicação para esse tipo de gerenciador, de modo que se houver recursos disponíveis no MPSoC, eles podem codificar a uma taxa maior e terminar o seu trabalho mais rapidamente.
Em este trabalho, os gerenciadores de recursos distribuídos foram avaliados com base em sua capacidade de satisfazer as restrições das aplicações.
Foram realizados também experimentos, adicionando aplicações em tempo de execução e analisando seu comportamento.
As métricas de avaliação utilizadas neste trabalho incluem perdas de deadlines, necessidade de buffers e jitter máximo.
As aplicações utilizadas foram especificadas como Synchronous Data Flow (SDFs), onde os vértices correspondem a tarefas (também chamados de atores) de uma aplicação e as arestas são as dependências entre as mesmas.
SDF é amplamente utilizado e é muito adequado para expressar a simultaneidade em aplicações, portanto, útil para analisar sistemas multiprocessados.
O arquitetura do modelo é composta por processadores interconectados por uma NoC.
Os gerentes de recursos consistem num controlador de admissão.
O papel do controlador de admissão é avaliar as restrições de tempo de uma nova aplicação contra os recursos disponíveis no MPSoC.
O controlador de admissão é uma interface com o usuário que calcula os créditos que serão distribuídos para os processadores.
Os árbitros desses processadores aplicam localmente esses créditos, tal que as restrições de vazão da aplicação sejam satisfeitas.
As seguintes informações são requeridas por o controlador de admissão:
Modelo SDF de cada aplicação;
A Figura 2 mostra o diagrama funcional do gerente de recursos centralizado e do gerente de recursos distribuído proposto.
O gerente centralizado monitora a vazão de cada aplicação e compara com sua vazão desejada.
Já o gerente distribuído procura minimizar o seu envolvimento no processo de monitoramento e investe em mais inteligência nos árbitros dos processadores locais, como mostra a Figura 2 (b).
As restrições de cada aplicação são calculadas centralmente, mas eles são aplicados em todos os processadores localmente.
O gerente distribuído não monitora cada aplicação, eliminando o problema de escalabilidade devido a o monitoramento centralizado.
Os modelos dos gerentes de recursos distribuídos usados para a simulação foram desenvolvidos usando a linguagem POOSL.
Foram usados dois modelos de aplicações para as simulações:
Decodificador JPEG (6 atores) e decodificador H. 263 (4 atores), mostrados na Figura 3.
A plataforma de computação é composta por 6 processadores e a Figura 3 também mostra o mapeamento dos atores nos processadores.
Como cenário de teste, foi inserido no MPSoC uma aplicação H. 263 com restrição de vazão de 20 frames/ seg.,
em o tempo de 700 milhões de ciclos de relógio.
Em a plataforma já estão executando previamente uma aplicação decodificador JPEG e outra aplicação decodificador H. 263.
A Figura 4 (a) mostra o comportamento do gerenciador de recursos centralizado, em o qual as aplicações que já estavam sendo executadas não sofrem qualquer impacto em seus desempenhos com a inserção da nova aplicação.
O alto jitter na execução da aplicação ocorre porque durante o período de monitoramento as aplicações podem estar inativas.
A Figura 4 (b) mostra o comportamento do gerenciador de recursos Credit Based.
Em esse cenário, as aplicações que já estão em execução na plataforma não sofrem qualquer efeito no seu desempenho.
Além disso, o jitter na execução da aplicação é muito pequeno em comparação com o gerenciador centralizado.
A Figura 4 (c) mostra a resposta do gerenciador Rate Based, o desempenho dos decodificadores JPEG e H. 263 diminuem imediatamente, logo que a segunda instância do decodificador H. 263 inicia sua execução.
Isto é devido a o fato de que quando a segunda instância do H. 263 inicia, ele tem o mais baixo nível de desempenho a ser alcançado, o que faz ganhar a preferência e rapidamente consegue o nível de desempenho exigido.
Com o tempo, as outras aplicações também obtêm os recursos de computação e o sistema converge rapidamente para o novo estado estacionário.
Os experimentos mostram que o Credit Based é muito eficaz para fazer cumprir as restrições de vazão e o Rate Based é eficaz para a obtenção de uma alta utilização de recursos no contexto de aplicações que podem se beneficiar com a disponibilidade de recursos.
Ambos os gerenciadores distribuídos podem lidar com um número maior de processadores, bem como um grande número de aplicações executando simultaneamente em comparação com o gerenciamento centralizado.
Os problemas dessa proposta é que ela limitou- se a realizar experimentos com um número reduzido de processadores (6 processadores), implementação em alto nível da plataforma, além de não suportar mapeamento dinâmico e migração de tarefas.
Em é apresentada uma abordagem para combinar métodos centralizados e distribuídos de gerência de um MPSoC e construir um sistema com gerenciamento hierárquico, a fim de beneficiar os esquemas distribuídos e centralizados, como mostrado na Figura 5.
A gerência do sistema é de granularidade fina em alguns aspectos, embora alguns dados monitorados e relatórios de desempenho sejam enviados para um gerente de nível superior que envia de volta comandos a serem executados.
O roteador e os componentes locais ligados a ele são chamados de célula, como é mostrado na Figura 6, sendo a célula o menor componente de um sistema hierárquico.
De acordo com a demanda do sistema, cada célula pode conter diferentes tipos de monitores, por exemplo, sensores de temperatura, monitores de energia, detectores de falhas, juntamente com atuadores, tais como reguladores de tensão e frequência.
Cada célula tem seu próprio gerente, em o qual reporta sua condição e executa comandos recebidos de um gerenciador de mais alto nível hierárquico.
Um grupo de células é agrupado para formar um cluster e gerenciado por um gerente de nível médio.
Diferentes políticas de agrupamento podem ser empregadas, mas a utilizada no trabalho foi que cada cluster executa tarefas de uma determinada aplicação.
Esse gerente é chamado de gerente de aplicação.
Cada gerente de aplicação é responsável por as demandas de sua aplicação, isto é, ele gerência os gerentes das células a fim de atender aos requisitos da aplicação.
O gerente de alto nível, denominado de gerente global, é responsável por o controle geral do sistema, e coordena as ações dos gerentes de níveis inferiores.
O gerente global é executado em nível de sistema operacional e está encarregado de criar novos gerentes no caso de novas alocações de aplicações.
As vantagens de sistemas de gestão centralizada e distribuída são obtidas através da exploração de gestores inteligentes de células, através de um gerente de nível médio para minimizar os dados monitorados que chegam aos gestores de nível superior.
Segundo os Autores, tal sistema tem como penalidade a ineficiência dos agentes de software para sistemas com menos de 100 núcleos.
Não foram apresentados resultados de implementação do sistema de gerenciamento hierárquico, apenas uma possível implementação como trabalhos futuros tendo como base a plataforma HeMPS, desenvolvida no grupo de pesquisa GAPH e utilizada na presente Dissertação.
Mapeamento Distribuído Communication propõe um método para um mapeamento de aplicações em tempo de execução de forma distribuída usando agentes, para MPSoCs heterogêneos baseados em NoC.
O método criado denomina- se Adam (Run-time Agent-based Distributed Application Mapping for on- chip Communication), e uma visão geral deste método é apresentada na Figura 7.
O mapeamento em tempo de execução é conseguido através de uma política de negociação entre agentes do Cluster (Ca) e agentes Globais (Ga).
Em a Figura 7, uma solicitação de mapeamento de uma aplicação é enviada para o Ca do cluster origem que recebe todos os pedidos de mapeamento e negocia com o Ga.
Podem haver várias instâncias de Ga que são sincronizadas ao longo de o tempo.
O Ga tem informações globais sobre todos os clusters do MPSoC, a fim de tomar decisões sobre para qual cluster a aplicação deve ser mapeada.
Possíveis respostas a este pedido de mapeamento:
Quando existe um cluster adequado para a aplicação, o Ga informa ao Ca que requisitou o mapeamento, a existência desse cluster, que pede ao Ca do cluster destino, o mapeamento real da aplicação.
Quando não há clusters adequados, o Ga avisa o cluster mais promissor, onde é possível mapear a aplicação após a migração de tarefas.
Quando não há nenhum cluster adequado e nenhum candidato para migração de tarefas, então o conceito de reagrupamento é usado.
Ele tenta encontrar PEs livres de vizinhos para aumentar o seu cluster.
Se os requisitos forem satisfeitos após o reagrupamento, a aplicação pode ser mapeada no cluster.
Em os experimentos realizados, foi comparado o método Adam com outros mapeamentos centralizados.
Em um cenário com apenas um cluster, mostrado na Figura 8, o método Adam só se torna mais eficiente em redes com mais de 4096 PEs, com menos processadores a heurística NN se faz mais eficiente.
Já num cenário com diversos clusters, Figura 9, o método Adam foi mais eficiente em redes com mais de 144 processadores, superando os demais métodos de mapeamento centralizado.
O que torna esse método relevante para a pesquisa realizada é a divisão das funções de mapeamento entre o agente global e um agente do cluster, não deixando só um agente a cargo de essa função.
Porém, o ponto negativo, é que seu desempenho em redes com menos de algumas centenas de processadores é muito semelhante a alguns mapeamentos centralizados, até inferior em alguns casos, o que o tornaria útil somente para redes com centenas ou milhares de processadores.
É importante também destacar que o trabalho não detalha como um sistema com 4096 PEs foi modelado.
O trabalho apresentado por é uma evolução do método proposto por.
É apresentado um método de mapeamento de tarefas descentralizado com reclusterização.
O fluxo da proposta de mapeamento é mostrado na Figura 10.
O mapeamento de tarefa é realizado por o agente global e por o agente local.
Entretanto, diferentemente do método proposto por, todas as informações dos recursos são armazenados nos clusters.
O método de reclusterização é controlado localmente por os clusters.
O agente global não precisa armazenar as informações sobre recursos disponíveis como em.
Após o agente global selecionar o cluster para uma nova aplicação, o grafo de comunicação das tarefas da aplicação é enviado para o agente local do cluster para realizar o mapeamento das tarefas.
Se o agente local detectar que o número de recursos livres dentro de o cluster não é o suficiente para alocar a aplicação, o agente local executa automaticamente o processo de reclusterização.
Os três principais passos da reclusterização são mostrados na Figura 11 e detalhados a seguir:
Primeiro passo:
Requisição de recursos livres ­ o cluster que requer mais recursos para executar o mapeamento de tarefas, envia mensagens de requisição para os agentes locais dos clusters vizinhos.
Segundo passo:
Mapear tarefas ­ as tarefas são mapeadas tanto nos recursos originais do cluster, quanto nos recursos obtidos na reclusterização.
Terceiro passo:
Retornar ao tamanho original ­ depois que a aplicação terminar sua execução, os recursos emprestados voltam ao cluster original.
Para demonstrar as vantagens da proposta apresentada por, foram executados aplicações cujos grafos de tarefas foram gerados randomicamente (TGFF).
Os resultados de tráfego de monitoramento, volume de comunicação e consumo de energia são comparados com mapeamento centralizado e o método proposto por.
Em a Figura 12 foi comparado o volume de comunicação do mapeamento das aplicações em diferentes tamanhos de NoC.
O volume de comunicação foi reduzido em 20,9% para uma NoC de tamanho 64x64.
O trabalho, assim como, não detalha como o sistema foi modelado e nem o seu nível de abstração, além de não demonstrar os resultados de tempo de execução das aplicações em comparação aos outros métodos.
Em, foi proposto um método de dividir para conquistar para a execução de mapeamento distribuído em tempo de execução.
O método proposto por os Autores é mostrado na Figura 13.
Quando uma nova aplicação é requerida para ser executada no sistema, o método proposto divide a rede em clusters, usando como critério o tamanho da aplicação.
A região que a aplicação melhor se adaptar é escolhida.
Depois que a região do cluster é definida, um PE dentro de o cluster é transformado em gerente do cluster para executar o algoritmo de mapeamento de tarefas.
Foram executados diversos cenários de aplicações no método proposto por, no método e num sistema com mapeamento em tempo de projeto, sendo assim comparados os seus resultados de tempo de mapeamento em ciclos de relógio, tal comparação é mostrada na Figura 14.
O método proposto por teve um ganho significativo no tempo de mapeamento em relação a o mapeamento em tempo de projeto, mas teve um tempo maior em quase todos os cenários comparando com o método proposto por.
Segundo os Autores, extensões futuras do trabalho incluem heurística de mapeamento multi-objetivo e controle de mapeamento hierárquico.
Os resultados mostrados em, se limitaram a comparações de variações do método proposto, não fazendo comparações com outros métodos, como por exemplo, o mapeamento centralizado de tarefas.
Migração de Tarefas O trabalho de propõe um método para migração de tarefas baseado em canais virtuais ponto-a-ponto (Vip) para criar conexões que proporcionem baixa latência e baixo consumo de energia para o fluxo de comunicação criado por o mecanismo de migração de tarefas.
Em trabalhos anteriores sobre migração de tarefas, normalmente se considera cada par de PEs, origem-destino, e avalia- se o custo de inicialização e retomada do processo de migração.
Em o trabalho do, usa- se o conceito de submalhas, que é definido como um subconjunto da rede que contenha mais de um PE.
Por a migração de tarefas impor um atraso às mensagens de dados de outros PEs (a migração gera muito tráfego na rede), foi tentado reduzir o número de caminhos envolvidos na migração.
Para isso foi usado o algoritmo Gathering-Routing-Scattering, onde as tarefas que estão migrando são coletadas num número limitado de PEs (nesse trabalho esses PEs são diagonais adjacentes) e transmitidas para a diagonal adjacente correspondente no destino.
Como mensagens que transportam tarefas são maiores que mensagens normais, foram configuradas rotas para formar conexões dedicadas, responsáveis por a comunicação das mensagens de migração.
Quando as mensagens de migração são recebidas nos núcleos da diagonal adjacente da submalha de destino, as tarefas são disseminadas para os destinos finais.
A Figura 17 mostra o processo de migração proposto por o trabalho.
A fase de coleta das tarefas é representada em vermelho, os Vip (caminhos) estão em azul e a fase de disseminação das tarefas é representada em verde.
Durante a migração, os PEs da submalha de origem param de se comunicar com outros PEs, limitando- se ao envio das mensagens de migração para a submalha de destino.
Os demais PEs são informados sobre o processo de migração para evitar o envio de mensagens para os PEs da submalha de origem durante a migração.
Entretanto, os PEs da submalha de destino podem continuar suas comunicações com outros PEs.
Experimentos foram realizados a fim de comparar o processo de migração proposto (com Vip) com outros dois processos:
Gathering-Routing-Scattering e Diagonal.
Todas as estratégias consideradas de migração de tarefas foram implementadas numa arquitetura NoC simulada por o Xmulator.
Os experimentos foram realizados para uma plataforma de 128 bits, com uma NoC de tamanho 16x16, 32 flits de comprimento de mensagem, com frequência de 250 MHz.
A Figura 18 mostra os resultados da migração de tarefas de uma submalha 5x5 para outra submalha 5x5, sendo respectivamente( (2,2), (6,6)) e( (9,9), (13,13)) suas coordenadas na rede, numa NoC 16x16.
O eixo horizontal da figura representa a taxa de geração de tráfego, e o eixo vertical mostra a latência média de mensagem (em ciclos), a latência média da migração (em ciclos) e o total de energia consumida por a rede (em nJ/ ciclos).
Os resultados mostram que a proposta de migração de tarefas baseada em Vip reduziu a latência média de mensagem em 13%, a latência média da migração em 14% e o total de energia consumida por a rede em 10% comparada ao método Gathering-Routing-Scattering.
Este trabalho apresenta uma plataforma MPSoC capaz de migrar tarefas em tempo de execução.
O sistema é distribuído, no sentido de que cada processador é capaz de tomar decisões locais, sem depender de uma unidade central.
Toda a gestão é assegurada por um RTOS em cada processador, que é responsável principalmente por a execução e distribuição de tarefas entre os PEs.
O objetivo dessa estratégia é melhorar o desempenho do sistema, assegurando a escalabilidade do projeto.
A arquitetura da plataforma é composta de uma matriz homogênea de elementos de processamento, interconectados por a NoC Hermes.
Esta plataforma é muito semelhante à plataforma HeMPS, por utilizar a NoC Hermes e o processador Plasma.
A Figura 19 mostra uma visão estrutural desta plataforma.
O protocolo de migração de tarefas, ilustrado na Figura 20, foi implementado como segue.
Considerando uma aplicação dividida em três tarefas, executando numa arquitetura que possui uma NoC 2x2.
Em o início do processamento, as tarefas estão executando em diferentes NPU.
Em um dado momento, NPU1 decide migrar T2 para NPU0.
O NUP1 envia um pacote de controle para o mestre da rede pedindo autorização para realizar a migração da tarefa (passo 2).
O mestre verifica em sua tabela de roteamento se há uma ou mais tarefas enviando dados para a tarefa que deve ser migrada.
Em esse caso T1 e T3.
Em seguida, o mestre envia um pacote de controle para estas tarefas pedindo que elas não enviem mais pacotes para T2, para que a migração possa ser iniciada (passo 3).
Vale ressaltar que o mestre apenas contém uma tabela global de roteamento, mas cada PE toma suas próprias decisões.
Imediatamente após a recepção do pacote de controle do Mestre, T1 e T3 param de enviar dados para T2 (passo 4).
Então T2 é migrado para NUP0 (passo 5).
Durante o processo de migração, somente o código objeto da tarefa é enviada através da NoC.
A migração do contexto da tarefa não é suportada nesse sistema, portanto, tarefas que precisam manter seus parâmetros (como filtros adaptativos) não podem ser migradas.
O próximo passo é registrar a tarefa na memória e inserir- la na lista de escalonamento.
Esse processo é feito logo que a tarefa chega ao destino.
Finalmente, o mestre envia um pacote de controle para T1 e T3, informando que T2 foi migrado e a comunicação pode ser retomada.
Este pacote de controle carrega a mensagem para a retomada da comunicação e a nova localização de T2.
Para avaliar o desempenho, a arquitetura do sistema foi descrita nas linguagens SystemC e VHDL, e foi usado como aplicação um decodificador MJPEG.
Os resultados mostraram que a sobrecarga provocada por o mecanismo de migração de tarefas é amortizada por o ganho em termos de desempenho, que é cerca de 25% maior em comparação ao sistema sem migração.
Foram apresentados trabalhos com objetivos distintos, sendo três com foco em desenvolver um controle distribuído a fim de tornar o MPSoC escalável, e comparar- lo com arquiteturas centralizas, outros quatro trabalhos exploram mapeamento distribuído de tarefas em tempo de execução, visando evitar que um único agente tenha a função de mapear tarefas, e os dois últimos trabalhos são de migração de tarefas, com foco em proporcionar uma menor latência para o fluxo de comunicação criada por o mecanismo de migração de tarefa e apresentar uma estratégia parcialmente distribuída de migração de tarefas.
As lacunas identificadas nos trabalhos revisados em gerência e mapeamento distribuído incluem:
Ausência de informações relacionadas à arquitetura utilizada;
Modelagem abstrata do sistema não permitindo avaliação detalhada do desempenho, ou mesmo ausência de informações relacionadas à modelagem.
Já nos trabalhos revisados de migração de tarefas, nenhum detalhou o processo de migração de tarefas, assumindo uma modelagem abstrata para o mesmo, além de nenhum método possuir uma migração de tarefa que faça a migração de contexto.
A Tabela 1 posiciona o trabalho da presente Dissertação em relação a o estado da arte.
A gerência de recursos será distribuída, havendo um gerente por cluster (terceira coluna).
O capítulo 4 discute em mais detalhes as opções de gerência:
Por aplicação, por cluster, por PE.
O sistema proposto é subdivido em regiões (clusters) onde seus tamanhos são dinamicamente adaptados em função de as aplicações (capítulo 5).
Há também um monitoramento de tarefas visando a desfragmentação do sistema, utilizando como mecanismo a migração de tarefas (seção 5.2).
O sistema foi implementado usando uma modelagem precisa no nível de ciclo de relógio.
Em este Capítulo é apresentada a arquitetura e a estrutura de comunicação entre tarefas do MPSoC homogêneo HeMPS, utilizado como plataforma de referência deste trabalho.
Esta plataforma foi modificada a fim de passar de uma plataforma com controle centralizado, executado por um único PE (mestre global), para uma plataforma com controle distribuído, controlado por mestres locais, posicionadas em regiões do MPSoC, regiões estas denominadas clusters.
A Seção 3.2 apresenta o mecanismo de comunicação entre as tarefas.
As características relevantes do MPSoC HeMPS incluem:
Processamento homogêneo.
Todos os PEs possuem a mesma arquitetura.
Justifica- se o emprego de um MPSoC homogêneo para simplificar a geração dos códigos objetos e a migração de tarefas.
Um MPSoC heterogêneo implica em diferentes códigos objetos para a mesma aplicação, ou tradução dinâmica de código.
Dados os objetivos apresentados na Seção 1.1, o emprego de MPSoCs heterogêneos não pertence ao escopo da presente Dissertação.
Estudo relativo a MPSoCs heterogêneos foi o tema abordado de a «Dissertação Integração de Novos Processadores em Arquiteturas MPSoC:
Um Estudo de Caso&quot;* WAC11].
Utilização de rede intrachip (NoC).
Justifica- se o emprego desta arquitetura de comunicação dada a escalabilidade da mesma e a possibilidade de múltiplas comunicações entre PEs ocorrerem simultaneamente.
Memória distribuída.
Cada processador possui uma memória privada, responsável por armazenar instruções e dados.
Optou- se por esta arquitetura, pois a mesma reduz o tráfego de dados no meio de comunicação, e não requer memórias cache externas.
Comunicação por troca de mensagens.
Justifica- se o emprego de troca de mensagens, pois este é o mecanismo natural de comunicação num sistema com memórias distribuídas.
A utilização de memória compartilhada é mais adequada a sistemas que utilizam barramento, com poucos PEs.
Organização de memória paginada.
Este mecanismo possui as seguintes desvantagens, comparado à organização de memória segmentada:
Restrição do tamanho máximo das tarefas ao tamanho da página, desperdício de memória caso as tarefas possuam tamanho menor que a página (fragmentação interna).
Porém esta escolha simplifica tanto o processo de mapeamento quanto o de migração de tarefas, pois uma página de memória é vista como um recurso de processamento de tarefas, podendo a tarefa ser mapeada em qualquer página livre.
Não são necessários mecanismos complexos de gerência de memória.
Arquitetura HeMPS (Hermes Multiprocessor System) Os principais componentes da arquitetura HeMPS são os elementos de processamento, denominados Plasma-IP, que são interconectados por a NoC Hermes, utilizada como infraestrutura de comunicação.
Além disso, tem- se uma memória externa, denominada de repositório de tarefas.
Em a Figura 21 é ilustrada uma instância da arquitetura HeMPS, utilizando uma NoC Hermes de dimensão 2x3 interconectando os Plasmas-IP.
O Plasma-IP, elemento de processamento do MPSoC HeMPS, possui duas instâncias distintas:
Mestre, denominado de Plasma-IP MP, responsável por a gerência dos recursos do sistema;
E escravo, denominado de Plasma-IP SL.
O MPSoC HeMPS contém apenas um Plasma-IP MP, pois possui uma gerência de recursos centralizada.
O Plasma-IP contém os seguintes componentes:
Memory Access) tem como principal função transferir o código-objeto de tarefas que chegam à interface de rede para a memória do processador e enviar para o processador mestre mensagens de depuração.
A interconexão dos elementos de processamento do MPSoC HeMPS é realizada através da NoC Hermes.
Esta NoC é parametrizável e possui topologia malha 2D.
O mecanismo de comunicação é realizado por chaveamento de pacotes, utilizando o modo wormhole, em o qual um pacote é transmitido entre os roteadores em flits.
Os roteadores da NoC possuem buffers de entrada, uma lógica de controle compartilhada por todas as portas do roteador, um crossbar interno e até cinco portas bidirecionais.
Estas portas são:
East, West, North, South e Local.
A porta Local estabelece a comunicação entre o roteador e seu núcleo local, sendo as demais portas utilizadas para conectar o roteador aos roteadores vizinhos.
A arbitragem round-robin é utilizada por o roteador da NoC Hermes.
Essa política utiliza um esquema de prioridades dinâmicas, proporcionando um serviço mais justo que a prioridade estática.
O algoritmo de roteamento utilizado é o XY, que envia os pacotes na rede primeiramente horizontalmente até chegar à coordenada X do roteador destino, e depois percorre verticalmente, até encontrar o roteador destino.
O MPSoC HeMPS assume que todas as aplicações são modeladas através de um grafo de tarefas, onde os vértices representam tarefas e as arestas representam a comunicações entre as tarefas da aplicação.
As tarefas sem dependências de recepção de dados são denominadas iniciais (tarefa A na Figura 22).
Em o momento de inicialização de uma dada aplicação, a heurística de mapeamento carrega no MPSoC somente as tarefas iniciais.
As demais tarefas são inseridas dinamicamente no sistema em função de as requisições de comunicação e dos recursos disponíveis.
A Figura 22 mostra um exemplo de uma aplicação modelada de acordo com esta abordagem.
O repositório de tarefas é uma memória externa ao MPSoC que contém o código-objeto de todas as tarefas que executarão no sistema.
As primeiras posições do repositório de tarefas contêm os descritores de cada tarefa, com informações como identificador único, posição inicial no repositório de tarefas, tamanho da tarefa, de entre outras informações.
Comunicação entre Tarefas A comunicação entre tarefas no sistema é realizada através de trocas de mensagens que ocorre através de pipes.
Um pipe é uma área de memória reservada para a troca de mensagens, alocado no microkernel.
Em esta área são armazenadas todas as mensagens das tarefas que executam num determinado PE.
As mensagens são armazenadas de forma ordenada, e consumidas na mesma ordem.
A Figura 23 ilustra a comunicação entre duas tarefas, A e B, mapeadas em diferentes PEs.
Tarefa A continua.
Isso caracteriza uma escrita assíncrona não-bloqueante.
Quando a tarefa B mesmo processador, a tarefa executa uma leitura no pipe local.
Se a tarefa está mapeada em outro PE, como nesse exemplo, o microkernel envia uma requisição de mensagem através da NoC e a tarefa entra em estado de espera (estado waiting) (5), caracterizando uma leitura síncrona bloqueante.
A tarefa A recebe a requisição de mensagem e envia a mensagem através da NoC, liberando espaço no pipe.
Quando a mensagem chega no PE da tarefa B, o microkernel armazena a mensagem no espaço de memória da tarefa B, habilitando a execução da tarefa B e mudando o seu estado (estado ready) (8).
Os serviços que o microkernel pode tratar incluem:
Message_ request:
É enviado para realizar a requisição de uma mensagem a uma tarefa que está alocada em outro PE do sistema (direção Escravo Escravo).
Message_ delivery:
Contém a mensagem a ser entregue que foi requisitada por um pacote de MESSAGE_ REQUEST (direção Escravo Escravo).
TASK_ ALLOCATION:
É utilizado para a alocação de uma tarefa solicitada em determinado PE da rede (direção Mestre Escravo).
TASK_ ALLOCATED:
É utilizado para informar que uma determinada tarefa foi alocada no sistema (direção Mestre Escravo).
TASK_ REQUEST:
É utilizado para solicitar o mapeamento de uma tarefa.
Caso a tarefa solicitada já esteja mapeada, um pacote de resposta de LOCATION_ REQUEST é enviado à tarefa solicitante contendo a localização da tarefa solicitada (direção Escravo Mestre).
TASK_ TERMINATED:
É utilizado para avisar que uma tarefa terminou sua execução (direções Escravo Mestre e Mestre Escravos).
TASK_ DEALLOCATED:
É utilizado para avisar que a tarefa terminou sua execução e pode ser liberada a página do PE em que a mesma estava executando (direção Escravo Mestre).
Location_ request:
É utilizado para requisitar e responder a localização de uma determinada tarefa (direções Escravo Mestre e Mestre Escravo).
Para exemplificar a funcionamento de trocas de mensagens e os respectivos serviços, foi utilizada uma rede de tamanho 2x3, com uma aplicação produtor-consumidor (Figura 24 (a)).
Em o diagrama de sequência da Figura 25, pode- se ver a troca de mensagens entre os processadores.
Em este diagrama, no primeiro momento, o processador Mestre, envia para os processadores 02 e 12 o código objeto das tarefas A e B para serem alocadas.
O envio dessas tarefas é realizado através da mensagem contendo o serviço &quot;TASK&quot;_ ALOCATION».
Após as tarefas houve comunicação entre as tarefas, o microkernel do processador executando a Tarefa A ainda não contém o endereço da Tarefa B. Com isso, a Tarefa A envia um &quot;LOCATION&quot;_ REQUEST &quot;para o processador Mestre, requisitando a localização da Tarefa B. Então o processador Mestre envia para o processador 02 a localização da Tarefa B, através de outro «LOCATION_ REQUEST».
&quot;LOCATION_ REQUEST», se envia um &quot;TASK&quot;_ REQUEST «para o processador mestre.
Como resposta, o Mestre envia para o processador 02 e para o processador 12 uma mensagem de &quot;TASK_ ALLOCATED «com a localização de ambas as tarefas.
Notar que neste caso a mensagem &quot;TASK_ ALLOCATED «para o processador 02 é redundante, pois o endereço da tarefa B já foi recebido no &quot;LOCATION REQUEST».
Isso ocorre por a ordem de execução adotada no exemplo, onde a Tarefa A iniciou primeiro.
Caso fosse a Tarefa B a que iniciasse primeiro, a mensagem &quot;TASK_ ALLOCATED «eliminaria a necessidade do envio e do recebimento do serviço de &quot;LOCATION_ REQUEST», pois quando a Tarefa A fosse requisitar a localização da Tarefa B, ela já saberia a sua localização.
Em a sequência, a Tarefa A envia um &quot;MESSAGE&quot;_ REQUEST «para a Tarefa B, requisitando dados.
Enquanto a Tarefa A não receber a resposta da mensagem, esta tarefa fica bloqueada no através da mensagem de &quot;MESSAGE_ DELIVERY».
Quando as tarefas terminam, estas enviam para o Mestre uma mensagem &quot;TASK_ TERMINATED», como resposta o Mestre envia para todos os processadores uma mensagem de &quot;TASK_ DEALLOCATED», informando que a tarefa terminou.
Este Capítulo apresenta a primeira contribuição da Dissertação:
O mecanismo de gerência distribuída de recursos, publicada em.
A revisão do estado-da-arte (Capítulo 2) identificou 4 abordagens distintas para realizar a gerência distribuída de recursos:
Sistema dividido em clusters, onde cada cluster é controlado por um gerente.
Os clusters podem ser responsáveis por mais que uma aplicação e eles também podem ser reorganizados, modificando seus tamanhos em tempo de execução através de um método de reclusterização.
O sistema é divido em clusters baseados no tamanho das aplicações.
Um cluster contém somente uma aplicação.
Um gerente de aplicação é usado como na segunda abordagem, mas esse método não usa clusters com tamanho predefinidos, permitindo assim o espalhamento de aplicações em PEs não contínuos.
Uma função global do sistema é implementada em cada PE do sistema, tal como a heurística de mapeamento de tarefas.
O presente Capítulo detalha o processo de mapeamento distribuído, utilizando clusters de tamanho fixo.
O foco de Capítulo é o detalhamento do processo no nível de transações para tornar o mapeamento distribuído.
A heurística de mapeamento utilizada, LEC-DN, não será detalhada, pois a mesma foi tema da Dissertação &quot;Mapeamento Dinâmico de Aplicações para MPSoCs Homogêneos».
Arquitetura com Controle Distribuído de Recursos A arquitetura com controle distribuído proposta dividiu o MPSoC em n clusters de tamanhos iguais, definidos em tempo de projeto.
Em tempo de execução, se uma dada aplicação não couber no cluster por falta de recursos disponíveis, o cluster pode pedir recursos emprestados aos seus clusters adjacentes, sendo o processo denominado reclusterização.
Tal abordagem é preferível às demais abordagens pois:
O número de PEs dedicados à função de gerência é limitado ao número de clusters.
Uma abordagem com um gerente por aplicação pode implicar numa maior sobrecarga, já que o número de aplicações que irão executar no sistema é desconhecido em tempo de execução.
A abordagem clusterizada reduz o número de hops entre tarefas pertencentes a uma mesma aplicação, reduzindo o tráfego global da NoC.
Não é necessário criar/ destruir agentes toda a vez que uma nova aplicação entra/ deixa o sistema, aumentando desse modo o desempenho global do sistema.
A Figura 26 mostra um MPSoC 9x9 contendo nove clusters de tamanho 3x3.
O MPSoC contém três tipos de PEs:
O Mestre Global (GMP), Mestre Local (LMP) e Escravo (SP).
O LMP é responsável por o controle do cluster, executando funções como o mapeamento de tarefas, migração de tarefas, monitoramento, verificação de deadlines e comunicação entre outros LMPs e GMP.
O GMP contém todas as funções do LMP, e as funções relacionadas com a gerência global do sistema.
Um exemplo dessas funções inclui a escolha de qual cluster uma dada aplicação irá ser mapeada, o controle de recursos disponíveis em cada cluster, o recebimento de mensagens de controle e de debug dos LMPS, acesso ao repositório de aplicações (memória externa contendo os códigos objetos de todas as tarefas) e recebimento de requisições de novas aplicações de uma interface externa.
Os SPs são responsáveis por a execução das tarefas.
Observar que a memória externa, anteriormente denominada &quot;repositório de tarefas», denomina- se na arquitetura com controle distribuído de recursos de &quot;repositório de aplicações».
Esta alteração deve- se ao fato que a granularidade do mapeamento mudou de tarefa para aplicação.
Anteriormente, no controle centralizado, o processador mestre recebia requisições de mapeamento de tarefas.
Em a arquitetura com controle distribuído de recursos, aplicações são inseridas no sistema, e o GMP envia o descritor das novas aplicações a um determinado LMP, o qual inicia o mapeamento das tarefas.
Quando o sistema inicia, o GMP é também responsável por a inicialização dos clusters, informando aos LMPs quais regiões eles iram gerenciar.
Após o LMP conhecer a região que irá controlar, ele informa a todos os SPs dessa região que ele será seu gerente.
Este mecanismo de inicialização de cluster e SPs torna o sistema flexível, permitindo que o sistema mude o tamanho do cluster em tempo de execução.
O GMP é o único PE com acesso a dispositivos externos, como o repositório de aplicações.
Em a Figura 26, nove PEs são reservados para as funções de gerência, representando 11,1% dos PEs sem executar aplicações de usuário.
Usando clusters de tamanho 4x4 num MPSoC com tamanho de 16x16, esse número cai para 6,25%, o qual é um custo aceitável, considerando os benefícios obtidos, como será demonstrado na seção de resultados.
Dado que o mapeamento de aplicações é a primeira ação a ser executada por o gerente do MPSoC quando uma dada aplicação entra no sistema, a próxima seção detalha a abordagem do mapeamento distribuído.
Mapeamento Distribuído Essa seção apresentação o processo de mapeamento distribuído de tarefas.
A seção 4.2.1 detalha o protocolo relacionado à seleção de clusters quando uma nova aplicação entra no sistema, detalhando a interação entre GMP LMP.
A seção 4.2.2 detalha o processo de mapeamento intraclusters, detalhando a interação entre LMP SP.
A seção 4.2.3 compara o processo de mapeamento distribuído contra o centralizado.
Como já mencionado, as aplicações são modeladas como grafos de tarefas.
Isso supõe que pelo menos uma tarefa não possui dependência com outras tarefas, sendo essas chamadas de tarefas iniciais.
As aplicações são armazenadas no repositório de aplicações.
De acordo com as requisições do usuário, uma nova aplicação pode ser requisitada para ser executada no sistema.
Em o exemplo apresentado na Figura 27, essa ação é representada como a seta &quot;Nova Aplicação», entrando no GMP.
O GMP lê do repositório de aplicações a descrição da aplicação e executa a heurística de &quot;Seleção de Cluster «para escolher qual cluster irá receber a nova aplicação.
A heurística escolhe o cluster em que a aplicação melhor se encaixa em termos de recursos disponíveis.
O número de recursos de um dado cluster é definido de acordo com a equação 1.
O cluster escolhido é aquele com o número de recursos livres igual ou superior ao número de tarefas da aplicação.
Não havendo um cluster que atenda a esta condição, é escolhido o cluster com o maior número de recursos livres, desde que haja um número igual ou superior de recursos livres no MPSoC correspondente ao número de tarefas da aplicação.
Essa condição garante que todas as aplicações que entrarem no sistema, tenham recursos livres para que suas tarefas sejam mapeadas.
Caso uma aplicação tenha um número de tarefas superior ao número total de recursos livres do MPSoC, essa aplicação só poderá ser inserida no sistema quando o número total de recursos livres do MPSoC for igual ou superior ao seu número de tarefas.
Após a execução da heurística de &quot;Seleção de Cluster», o GMP transmite a descrição da aplicação para o LMP do cluster selecionado.
A descrição da aplicação contém:
Identificador de aplicação, IDapp, juntamente com sua lista de tarefas;
Identificação de cada tarefa que pertence a aplicação (equação 2).
O LMP selecionado armazena a descrição da aplicação, que inicia o processo de mapeamento das tarefas (Figura 27 seta &quot;requisição de alocação de tarefa&quot;).
Duas situações de mapeamento podem surgir:
Mapeamento das tarefas iniciais e mapeamento das tarefas remanescentes.
O mapeamento das tarefas iniciais busca mapear as tarefas no SP com o maior número de recursos disponíveis ao seu redor.
Isso aumenta a probabilidade de que as tarefas restantes da aplicação sejam mapeadas perto umas das outras, reduzindo a distância de comunicação entre tarefas, e por consequência, a energia de comunicação.
O mapeamento das tarefas restantes é explicado a na seção 4.2.2.
Depois da seleção do SP que irá receber a tarefa inicial, o LMP envia um pacote para o GMP programa o módulo DMA para ler um dado número de palavras (SIZEi) do repositório de aplicações do endereço ADDi, transmitindo essas palavras para o PEposição.
O SP irá escalonar a nova tarefa no final da recepção do pacote de alocação de tarefa.
Além disso, o LMP mantém uma tabela com todos os endereços de rede das tarefas mapeadas.
Considerando a inserção da terceira aplicação na Figura 27, essa situação ilustra um cenário em que o cluster selecionado é aquele com o GMP.
Em este caso, o GMP também executa o procedimento de mapeamento de tarefas.
Cada aplicação é iniciada após o mapeamento de a (s) tarefa (s) inicial (is).
Quando uma tarefa inicial t1 é iniciada, ela executa algumas funções de processamento, e em seguida, se comunica com uma determinada tarefa.
O SP de origem da t1 verifica se a tarefa alvo (t2 no exemplo) está na tabela local de endereços de rede das tarefas.
Se estiver, a mensagem é transmitida para a tarefa alvo (de fato é armazenada no pipe).
Se não estiver, um pacote com o serviço de &quot;requisição de tarefa «é transmitido para o LMP responsável por a t1.
Message msg1; send;
Com o pacote de &quot;requisição de tarefa «recebido, o LMP executa a heurística de mapeamento PREMAP-DN para selecionar o SP que deverá receber t2, esse processo pode falhar caso o cluster não possua recursos disponíveis, o que acarretará na execução do protocolo de reclusterização, explicado na próximo Capítulo.
Esta heurística de mapeamento minimiza a energia consumida na NoC, através da aproximação das tarefas com maior volume de comunicação.
Distribuindo a heurística de mapeamento entre vários LMPs o desempenho geral do sistema melhora, uma vez que o cálculo de mapeamento é um processo computacionalmente intensivo.
Supondo que a heurística de mapeamento selecionou o SP 2 para receber a t2, assim o LMP envia um pacote para o GMP com o serviço de &quot;requisição de alocação de tarefa», tal como explicado no mapeamento inicial de tarefas.
O LMP também envia para o SP 1 a localização da t2 e para o SP 2 a localização da t1.
Essas localizações são armazenadas nas tabelas locais de tarefas.
O último evento no processo de mapeamento de tarefa é a transmissão do código da tarefa por o GMP para o SP 2.
Em a abordagem de mapeamento centralizado, existe apenas um GMP, o qual é responsável por o mapeamento de todas as tarefas.
Todos os pedidos de mapeamento de tarefas são serializados), reduzindo o desempenho do sistema e aumentando o tráfego na NoC na região do GMP.
Usando o método de mapeamento distribuído de tarefas), o processamento do mapeamento é distribuído em vários LMPs, reduzindo a carga de computação gerada por os pedidos de mapeamento.
O mapeamento centralizado causará um maior atraso para o início da execução das tarefas, dada a serialização do processo de mapeamento.
Por consequência, as aplicações terão seu tempo total de execução penalizado e durante o processo de mapeamento as restrições das aplicações, como vazão, dificilmente serão atendidas.
Por outro lado, em aplicações periódicas, que executam por um longo período de tempo, após o sistema estabilizar, o mapeamento tanto centralizado quanto distribuído tendem a ter o mesmo desempenho.
Esta é uma avaliação informal, sendo a avaliação quantitativa apresentada na Seção 6.2.
Este Capítulo apresenta a segunda contribuição da Dissertação:
O mecanismo de reclusterização.
O processo de reclusterização implica no ajuste em tempo de execução do tamanho de um determinado cluster.
Em um primeiro momento o processo de reclusterização implica no aumento do tamanho do cluster, por indisponibilidade de recursos.
Este processo de aumento dinâmico no tamanho dos clusters é detalhado na seção 5.1.
Em um segundo momento o processo de reclusterização pode migrar tarefas que foram mapeadas em clusters vizinhos (clusters diferentes de o qual a aplicação é gerenciada), com o objetivo de melhorar o desempenho das aplicações.
Este segundo processo é condicionado não apenas à existência de recursos livres, mas também ao ganho de desempenho por a redução do número de hops entre as tarefas comunicantes.
O processo de migração de tarefas é detalhado na seção 5.2.
Aumento Dinâmico do Tamanho dos Clusters Considerar a Figura 31 como exemplo para ilustrar o processo de reclusterização.
Em esta figura, o cluster posicionado na parte superior esquerda não possui recursos livres, enquanto os demais clusters possuem 1 SP disponível.
O cluster sem recursos disponíveis recebe num dado momento uma solicitação de mapeamento de tarefa oriundo de um SP do seu cluster.
Dada a ausência de recursos livres, o LMP desse cluster envia a mensagem &quot;Requisição de Empréstimo «requisitando recursos a todos os seus clusters vizinhos.
A mensagem de pedido de recursos, &quot;Requisição de Empréstimo», contém também o endereço do SP que requisitou o mapeamento de uma nova tarefa.
Este processo de requisição é enviado num primeiro momento para até 8 clusters vizinhos (quadrado envolvente do cluster).
Caso não haja sucesso na primeira exploração, o quadrado envolvente cresce e a exploração pode continuar até todos os clusters serem requisitados a responderem se possuem recursos livres.
Como já mencionado anteriormente, sempre haverá recurso disponível, pois a aplicação só é inserida no MPSoC se esta condição for verdadeira.
Os LMPs vizinhos ao receberam uma mensagem &quot;Requisição de Empréstimo «buscam por recursos disponíveis em seus clusters.
Se houver apenas um recurso disponível, este recurso é reservado para ser emprestado, caso contrário, se existir mais do que um recurso disponível, o LMP irá reservar o recurso mais próximo possível, em número de hops, entre a tarefa a ser mapeada e a tarefa que solicitou o mapeamento.
Após a reserva, todos os LMPs vizinhos, notificam a posição do recurso, caso esse existir.
O LMP que requisitou recurso externo ao cluster escolhe o SP mais próximo de quem requisitou a tarefa, enviando uma mensagem de &quot;Liberar Recursos «para liberar os recursos dos LMPs que não foram selecionados.
Em seguida, o LMP envia uma mensagem de &quot;Requisição de alocação de tarefa «para o GMP requisitando o mapeamento da tarefa no recurso emprestado.
Portando o tamanho do cluster aumenta em tempo de execução, pois o recurso emprestado faz parte agora desse cluster.
Esse processo otimiza a gerência do sistema, uma vez que as aplicações podem ser mapeadas, mesmo se o cluster não possuir recursos disponíveis.
A Figura 32 mostra um exemplo de troca de mensagens durante o processo de reclusterização.
Em esse exemplo o LMP1 não possui recursos disponíveis para mapear uma tarefa, o que o faz iniciar o processo de empréstimo de recursos.
Ele faz uma requisição de empréstimo para os seus clusters vizinhos.
O LMP de cada cluster vizinho busca por um PE livre que minimize a distância Manhattan até a tarefa requisitante (ação &quot;mapeamento do empréstimo&quot;).
Os clusters vizinhos ao terminarem o &quot;mapeamento do empréstimo «respondem com a localização do recurso disponível, se houver.
Em esse exemplo, o LMP1 escolheu o recurso do cluster gerenciado por o LMP4, liberando os recursos dos demais clusters.
Após essa escolha, o LMP1 requisita a alocação da tarefa para o GMP que faz a alocação no SP escolhido.
Lembrando que esta alocação de tarefa corresponde apenas à transmissão do código objeto, através do DMA do GMP.
Todas as tarefas alocadas em clusters vizinhos são gerenciadas por os LMPs que gerenciam suas aplicações e não o LMP do cluster que a tarefa está mapeada.
Quando uma tarefa termina sua execução, ela informa ao LMP que a gerência o seu témino e também informa ao LMP do cluster de o qual está mapeada para que ele possa liberar esse recurso e redimensionar o seu cluster.
Apenas os LMPs têm o controle das localizações das tarefas de suas aplicações e do número de tarefas que já terminaram de uma determinada aplicação.
O GMP somente tem a visão do total de recursos utilizados nos clusters e no sistema, sendo que somente atualizam essas informações quando alguma aplicação finaliza e não alguma tarefa.
Quando todas as tarefas de uma aplicação terminam, o LMP que a gerência informa ao GMP que essa aplicação terminou sua execução e o GMP atualiza sua tabela de recursos disponíveis no sistema.
Migração Migração de tarefas pode ser usada para balanceamento de carga, tolerância a falhas e para restaurar o desempenho de uma determinada aplicação devido, por exemplo, a inserção de uma nova aplicação no sistema.
No caso de o presente trabalho, a migração de tarefas foi utilizada para desfragmentar o sistema, migrando as tarefas mapeadas em clusters vizinhos, com o objetivo de melhorar o desempenho das aplicações.
A migração de tarefas faz parte do processo de reclusterização e é condicionado não apenas à existência de recursos livres, mas também ao ganho de desempenho por a redução do número de hops entre as tarefas comunicantes.
A Figura 33 mostra um exemplo do uso da migração de tarefas no protocolo de reclusterização.
A Figura 33 (a) apresenta o mapeamento de uma determinada aplicação composta de 6 tarefas (A-F).
Em um cenário onde as aplicações são inseridas/ removidas em tempo de execução.
O MPSoC pode ter a maioria de seus recursos ocupados no momento da inserção da aplicação (PEs com a cor laranja), restringindo a procura por recursos livres por a heurística de mapeamento, o que ocasionou no mapeamento de algumas tarefas em clusters vizinhos ao cluster que gerência a aplicação (cluster 2).
Em um dado momento, uma tarefa dentro de o cluster termina a sua execução, informando ao LMP o seu término e liberando um recurso do cluster (Figura 33 (b)).
Após o recebimento da mensagem de término de tarefa, caso haja alguma tarefa que o LMP gerência fora de o seu cluster, ele executa uma função que calcula se é viável a migração.
Essa função compara o número de hops da localização atual da tarefa até suas comunicantes com o número de hops da localização, caso ela seja migrada até suas comunicantes.
Se o número de hops se tornar menor após a realização da migração, essa migração é viável.
Com isso, o LMP envia uma mensagem para a tarefa que será migrada, solicitando sua migração para o PE com o recurso livre.
A tarefa então migra para o PE destino e informa ao LMP do cluster que está mapeada, que está migrando e que desocupará o recuso do cluster (Figura 33 (c)).
O detalhamento do protocolo de migração de tarefas é mostrado na próxima seção.
O protocolo de migração de tarefas é ilustrado na Figura 34 com um exemplo e pode ser resumido da seguinte forma:
Uma tarefa alocada no SP1 termina sua execução e avisa o LMP do seu cluster o seu témino.
O LMP verifica se há alguma tarefa que ele gerência fora de o cluster.
Caso existe, ele verifica se a migração dessa tarefa é viável, ou seja, há uma redução no número de hops entre as tarefas comunicantes.
Caso a migração seja viável, o LMP envia uma mensagem de requisição de migração para essa tarefa, nesse exemplo, localizada no SP2.
A tarefa só pode ser migrada se está no estado de execução.
O escalonador do microkernel verifica essa condição.
Se a tarefa pode ser migrada, o microkernel envia para o PE destino, um pacote com o conteúdo completo da página da tarefa e seus descritores de tarefas (TCB ­ Task Control Block).
O detalhamento do processo de migração de tarefas é detalhado em.
A tarefa migrada é escalonada uma vez que o código e o TCB forem completamente recebidos.
Em o exemplo, uma tarefa comunicante envia uma requisição de leitura para a tarefa que foi migrada, mas para o SP que ela não está mais alocada.
Quando essa requisição de leitura chega, ela é repassada para o SP onde se encontra a tarefa.
O SP1 então recebe a requisição de leitura e responde a requisição, já com a sua nova posição.
O passo` 4' do processo de migração pressupõe que a tarefa esteja no estado de execução, ser migrada não está esperando dados de outra tarefa.
Caso estivesse aguardando dados de outra tarefa, o PE da tarefa poderia receber dados durante a migração, levando à perda de informações.
Ainda neste contexto, há o controle de entrega de mensagens.
O processo que garante que as mensagens serão consumidas na ordem correta é o armazenamento local das tarefas armazenadas no pipe do PE original.
Portanto, as tarefas que se comunicarem com a tarefa migrada ainda utilizam o endereço da tarefa antes da mesma migrar.
Uma vez que todas as mensagens sejam consumidas, as requisições de comunicação são encaminhadas para a nova posição da tarefa, que responde essas requisições com o novo endereço.
A partir deste momento, as tarefas que se comunicam com a tarefa migrada utilizam o novo endereço.
Foi utilizado para os experimentos desse trabalho o MPSoC HeMPS, descrito num nível RTL preciso no nível de ciclo de relógio (SystemC).
O MPSoC HeMPS é modelado em outros dois níveis de abstração:
VHDL RTL, onde todos os módulos são descritos em VHDL sintetizável;
ISS+ VHDL, processadores são modelados usando um simulador de conjuntos de instruções preciso em nível de ciclo (ISSs), o restante dos módulos são descritos em VHDL.
A Tabela 2 mostra a comparação dos tempos de simulação entre os três diferentes níveis de abstração, onde cada linha da tabela é identificada por o tamanho do MPSoC e um sufixo.
O detalhamento destes experimentos foi publicado em.
Quando esse sufixo é_ 1, isso significa que somente uma instância da aplicação foi inserida no MPSoC durante a simulação.
Quando sufixo é_ 25,_ 50,_ 70 ou_ 100, isso corresponde a um número de instâncias de aplicações que de tal modo chegue perto de 25%, 50%, 70% e 100% de páginas ocupadas do MPSoC.
Foi escolhido o modelo do MPSoC descrito em SystemC RTL, pois como mostra a Tabela 2, ele chega a ser 175 vezes mais rápido do que o modelo descrito em VHDL e 8 vezes mais rápido em comparação ao modelo ISS+ VHDL.
Essa seção apresenta os resultados usando três benchmarks:
MPEG, executa uma parte de um decodificador MPEG;
Multispec image analysis, avalia a semelhança entre duas imagens usando frequências diferentes;
Synthetic, aplicação sintética.
Os dados obtidos na Tabela 2, e no presente Capítulo, foram obtidos com uma ferramenta desenvolvida por o Autor, que permite a execução de dezenas de simulações simultaneamente, sobre diferentes configurações de benchmarks.
Esta ferramenta permite a execução testes de regressão.
Estes testes de regressão permitem validar cada nova técnica agregada ao MPSoC, e obter de forma automatizada os resultados.
Características relevantes do MPSoC incluem:
Palavra do PE de 32-bit e 16-bit de flit;
Tamanho da página com 16 Kbytes;
Time-slice: 16,384 ciclos de relógio (quantidade de tempo que uma tarefa é escalonada), 2 páginas por PE para execução de tarefas;
Tempo Total de Execução O primeiro conjunto de resultados avalia o tempo total de execução das aplicações, considerando:
Tamanho do MPSoC;
Tamanho do cluster;
Carga do MPSoC, ou seja, percentual de PEs escravos executando tarefas da aplicação.
A Tabela 3 apresenta os tempos de execução normalizados em relação a um MPSoC 12x12 com gerência centralizada e uma carga igual a 75%.
Como pode ser observado, a gerência distribuída conduz a uma redução total no tempo de execução.
A menor redução observada no benchmark MPEG é devido a a sua característica de periodicidade.
A redução no tempo total de execução deve- se a:
Isso reduz a carga do gerente e o tráfego na NoC.
Uma questão relevante a avaliar é o tamanho do cluster.
Existe um tamanho ótimo de cluster?
Mesmo com poucos clusters, como dois, o tempo total de execução é reduzido, devido o paralelismo das tarefas de gerenciamento.
Um grande número de clusters requer um grande número de recursos do MPSoC, reduzindo o número de aplicações que se pode executar simultaneamente.
A partir de a Tabela 3, um tamanho de cluster com 18 (6x3) ou 16 (4x4) PEs representa um bom custo-benefício entre a redução no tempo de execução e os recursos reservados ao gerenciamento.
A Tabela 4 apresenta o tempo total de execução numa instância 6x6 do MPSoC.
Em esse MPSoC menor, o número de tarefas simultâneas é menor comparada a um MPSoC de grande dimensão.
Portanto, o gerenciamento de carga é menor.
Em esse caso, o particionamento de um MPSoC 6x6 é menos eficiente do que num MPSoC maior.
Uma segunda questão se impõe:
Quando o gerenciamento distribuído deve começar a ser utilizado?
A partir de os resultados apresentados Tabela 5, MPSoCs começando em 64 PEs, com 4 clusters de tamanho 4x4, apresentam significativas reduções no tempo de execução.
Este tamanho de cluster também prove redução no número médio de hops entre tarefas (discussão aprofundada na Seção 6.3), e um baixo comprometimento de recursos para gerência (6,25%).
Logo, clusters cujo tamanho implique na utilização de 6% a 8% dos PEs para atividades de gerência correspondem a um bom custo-benefício entre ganho de desempenho e perda de recursos.
A Tabela 6 avalia o impacto do tamanho do cluster contra a carga do sistema.
Isso é importante para diferenciar o tempo de execução de uma aplicação e o tempo total de execução.
O tempo de execução de uma aplicação é praticamente constante, uma vez que a aplicação inicia sua execução.
O tempo total de execução considera a quantidade total de tempo necessário para executar todas as aplicações inseridas no MPSoC.
Dois cenários devem ser considerados.
O primeiro está relacionado a aplicações periódicas, com tempo de execução longo.
Em tais casos, as ações do gerenciamento não devem influenciar fortemente com o aumento da carga.
Como pode ser observado no benchmark MPEG, o tempo total de execução no método centralizado aumentou 48% comparando um sistema com uma aplicação com outro sistema com 75% de carga.
O gerenciamento distribuído, com tamanho de cluster 6x3, apresentou um aumento menor no tempo de execução, 28%.
Apesar de o fato da aplicação ser periódica, esse resultado mostra os benefícios de executar em paralelo o gerenciamento do sistema.
O segundo cenário diz respeito a aplicações com tempo de execução delimitado.
Em esse cenário, o gerente desempenha um papel importante.
Em o gerenciamento centralizado, todos os mapeamentos de tarefas são serializados, com aplicações sendo iniciadas sequencialmente.
Em o gerenciamento distribuído, os mapeamentos são realizados em paralelo, com várias aplicações iniciando ao mesmo tempo.
Isso explica as diferenças observadas na Tabela 6: De 264%/ 230% no gerenciamento centralizado para 117%/ 77% com tamanho de cluster 6x3 no gerenciamento distribuído.
Resumindo a avaliação do tempo total de execução:
Gerência distribuída é eficaz para MPSoCs grandes, ou seja, que contenham mais do que 64 PEs;
Um tamanho de cluster cujo tamanho implique na utilização de 6% a 8% dos PEs para atividades de gerência correspondem a um bom custo-benefício entre ganho de desempenho e perda de recursos;
A gerência distribuída reduz o aumento do tempo total de execução quando se aumenta a carga do sistema.
Benchmark MPEG para dois tamanhos de MPSoC.
Observa- se na Figura 36 um comportamento similar à Figura 35: Serialização do mapeamento na gerência centralizada e tarefas com mesmo tempo de execução.
Estes gráficos ilustram o porquê da redução do tempo de execução observado nas tabelas apresentadas na Seção anterior.
À medida que o tamanho do MPSoC aumenta, o tempo de execução para o mapeamento centralizado cresce dramaticamente, impactando fortemente no tempo total de execução das aplicações (Tabela 3, apresentada anteriormente).
Número de Hops A avaliação do número médio de hops é um parâmetro importante para avaliar a qualidade do mapeamento de tarefas.
Um número pequeno de hops entre as tarefas reduz a energia de comunicação, uma vez que as tarefas comunicantes são mapeadas perto umas das outras.
Foi energia consumida na comunicação para transmitir um bit por uma distância de nhops, utilizando a Equação 3.
Ehops $= nhops* ESbit+* ELbit onde:
ESbit, ELbit, e nhops representam o consumo de energia num roteador, nos fios de interconexão e o número de roteadores por onde um bit passou.
Vale ressaltar que é considerada uma NoC malha homogênea, sendo que a energia consumida para a transferência de um bit por cada roteador da rede é constante.
Por outro lado, um número alto de hops entre as tarefas além de aumentar a energia consumida na comunicação prejudica o desempenho das aplicações, uma vez que o tráfego das demais aplicações pode interferir na comunicação (disputa por caminhos comuns na NoC).
O cálculo do número de hops de uma aplicação foi feito somando- se o número de hops que cada tarefa da aplicação tem de distância para suas tarefas comunicantes.
Um exemplo desse cálculo é mostrado na Figura 37.
Em a Figura 37 (a) é mostrado o grafo de uma aplicação, onde o número nas setas corresponde ao número de hops que a tarefa tem de distância entre sua tarefa comunicante.
Esses números de hops foram obtidos usando o mapeamento da Figura 37 (b).
Uma distância em hops igual a zero (como entre as tarefas` C'e` D') signifgica que ambas foram mapeadas no mesmo PE.
A Tabela 7 apresenta os dados estatísticos relativos ao número de hops entre tarefas, num MPSoC 12x12, com 75% de carga, para dois benchmarks e três tamanhos de cluster.
Em ambos os benchmarks, a gerência centralizada apresentou um número médio de hops maior do que no gerenciamento distribuído.
As colunas &quot;min «da Tabela 7, representam o melhor mapeamento, em número de hops, das aplicações executadas.
Tabela 8 ­ Número médio de hops entre tarefas em MPSoCs com carga igual a 25%.
Tamanho do MPSoC Tamanho do cluster Em o de Clusters 6x6 6x6 Benchmark Synthetic Em os cenários com alta carga (Tabela 7), o número de regiões contínuas usando mapeamento centralizado reduz, aumentando desde modo o número de hops.
Por outro lado, o uso de clusters favorece a utilização dos recursos do MPSoC, mantendo regiões continuas mesmo sob cargas maiores aplicadas no MPSoC.
O resultado apresentando na Tabela 7 está também relacionado ao tempo total de execução (seção anterior).
Quanto maior o número médio de hops, maior o tempo total de execução.
O desvio padrão observado no MPSoC com gerência centralizada é alto, significando que algumas aplicações têm suas tarefas mapeadas longe umas das outras.
O MPSoC com gerência distribuída apresentou um desvio padrão menor, de 0,15 até 0,66.
Isso significa que a maioria das aplicações foram mapeadas em regiões continuas, dentro de o cluster, favorecendo QoS.
Quando o cluster está quase cheio, a gerência distribuída &quot;toma emprestado «recursos dos seus clusters vizinhos.
Esse fato pode ser observado nas colunas &quot;max «da terceira e quarta linha da Tabela 7, onde se pode observar um número máximo de hops bem superior à média obtida, devido a o fato de algumas tarefas terem sido mapeadas em outros clusters devido a falta de recursos.
Para reduzir o número de hops em tempo de execução, é adotado a migração de tarefas, como explicado anteriormente.
Reclusterização Essa seção avalia o processo de reclusterização usando migração de tarefas.
Os resultados foram obtidos num MPSoC com tamanho 6x6 com ambas as técnicas de gerenciamento.
Em a gerência distribuída, foi utilizado tamanho de cluster igual a 3x3.
Dois cenários foram avaliados:
Main application (aplicação principal) -- MApp (aplicação avaliada) com aplicações &quot;perturbadoras «(aplicações simples, que geram tráfego para competir com recursos da NoC utilizados por MApp), sem migração de tarefas;
MApp com aplicações &quot;perturbadoras «e duas migrações de tarefas (tarefas E e D).
Estes cenários são apresentados na Figura 39.
A Figura 39 (a) mostra o mapeamento inicial das tarefas.
As tarefas D e E da MApp foram mapeadas em clusters vizinhos, e não no cluster que contém o PE que gerência a MApp (cluster no topo à esquerda).
Essas duas tarefas foram mapeadas em clusters vizinhos devido a a ausência de recursos cluster que gerência a aplicação MApp.
Quando as tarefas pertencentes às aplicações &quot;perturbadoras «terminam suas execuções, o gerente do cluster verifica se existem tarefas que ele gerência, executando em outros clusters.
Em esse caso, o LMP envia a requisição de migração para os SPs executando essas tarefas.
Em a Figura 39 (b), as tarefas D e E foram migradas para o cluster que gerência a aplicação MApp.
Isso resultou num menor número de hops entre as tarefas da aplicação MApp, com uma melhora no desempenho de execução.
A aplicação MApp é periódica, com um comportamento pipeline, repetindo cada tarefa por um número parametrizável de iterações.
As Figura 40 e Figura 41 mostram o tempo de execução de cada iteração da tarefa F. Os resultados obtidos no começo da simulação correspondem ao mapeamento das tarefas, sendo considerado como estado transitório.
De acordo com a Figura 40, o tempo por iteração é estabilizado em aproximadamente 10.000 ciclos de relógio (o tempo por iteração varia em função de o tempo de chegada dos pacotes).
A Figura 41 avalia o cenário com migração de tarefas.
Tal como esperado, durante a migração de tarefas o tempo da iteração aumenta, porque a aplicação é suspensa durante a migração, pois os dados não estão sendo consumidos por as tarefas D e E, que estão sendo migradas.
Depois da migração, o tempo por iteração da tarefa F estabiliza em 8.600 ciclos de relógio, reduzindo também a oscilação observada na Figura 40.
Tal melhoria é decorrente da redução do número de hops entre as tarefas da aplicação MApp.
O tempo total de execução para ambos os cenários foi de 2.772.692 (sem migração) e tempo total de execução, considerando duas migrações de tarefas.
Portanto, mesmo a migração de tarefa aumentando momentaneamente o tempo de execução, no resultado final há uma melhora no desempenho global.
A presente Dissertação contribuiu com o estudo da gerência distribuída de recursos em MPSoCs.
A opção escolhida foi a gerência em clusters, com tamanho fixo em tempo de projeto, e dimensionamento modificável dinamicamente em tempo de execução.
As avaliações das técnicas desenvolvidas buscaram responder a questões como:
Para qual tamanho de MPSoC a gerência distribuída é vantajosa em relação a a gerência centralizada?
Qual o melhor compromisso entre o comprometimento de recursos versus ganho de desempenho, i.
e, tamanho do cluster?
O processo de reclusterização impacta no desempenho glogal, i.
e, custo da migração de tarefas?
Para responder a estas questões, além de a implementação das técnicas de gerência, foi desenvolvida uma ferramenta que permite a execução de dezenas de simulações simultaneamente sobre diferentes configurações de benchmarks.
Este processo é denominado de &quot;teste de regressão».
Este teste de regressão permite validar cada nova técnica agregada ao MPSoC, e obter de forma automatizada os resultados apresentados no Capítulo 6.
De posse dos resultados, pode- se responder às questões acima.
MPSoCs a partir de 64 PEs, com um compromentimento entre 6-8% dos PEs para gerência respondem as questões e.
A migração de tarefas impacta apenas momentaneamento no desempenho das aplicações, mas no final o desempenho é melhorado devido a a aproximação das tarefas comunicantes.
Finalmente, é importante destacar que este estudo foi implementado sobre um MPSoC particular, HeMPS.
Entretanto, o Autor acredita que as características arquiteturais adotadas e justificadas no início do Capítulo 3 são genéricas os suficiente para que os resultados obtidos possam ser considerados genéricos para MPSoCs semelhantes, como os que estão sendo hoje adotados por a indústria e academia.
Publicações Relacionadas ao Tema da Dissertação As contribuições apresentadas neste trabalho foram enviadas para conferências pelo meio de três artigos aprovados e um submetido.
Estes artigos compreendem:
Como trabalhos futuros enumeram- se as seguintes atividades:
Incluir métricas de desempenho, como latência e energia de comunicação.
Mesmo eles sendo proporcionais ao número de hops, sua avaliação é importante para corroborar a abordagem de gerenciamento distribuído.
Adicionar parâmetro de QoS no monitoramento do sistema, como vazão, para adaptar dinamicamente as aplicações de acordo com a carga do sistema.
Adicionar num MPSoC com gerência distribuída uma rede dedicada para o fluxo de dados do gerenciamento.
