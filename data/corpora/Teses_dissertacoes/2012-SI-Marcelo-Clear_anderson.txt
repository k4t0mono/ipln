A engenharia de software é um domínio altamente orientado ao conhecimento, em o qual os fatores de sucesso estão relacionados com a experiência das pessoas envolvidas nas diversas fases e atividades do processo.
O conhecimento na engenharia de software é disperso, de proporção imensa e de crescimento contínuo, e a gestão deste conhecimento nas organizações é uma área ampla com muitas disciplinas que podem influenciar nos seus resultados.
De entre as questões que envolvem a gestão do conhecimento no desenvolvimento de software está a preferência por o uso do conhecimento em seu modo tácito ou explícito.
Essa preferência poderá definir, entre outros aspectos, a opção por a redução da explicitação do conhecimento que circula nos projetos em detrimento de um uso maior do conhecimento em seu formato tácito.
Em função de estas definições, podemos reduzir o uso de artefatos como diagramas e outros tipos de documentos e incrementar a comunicação direta através de conversações entre os envolvidos nos projetos (comum em metodologias ágeis).
Desta forma, o fato de priorizar o uso do conhecimento no seu formato tácito, reduzindo o processo de externalização do conhecimento, faz com que as metodologias, técnicas e ferramentas para gestão do conhecimento no processo de desenvolvimento de software sejam repensadas.
Diante deste contexto, o objetivo desta pesquisa é apresentar uma metodologia para aquisição do conhecimento presente nas conversações realizadas nas reuniões de projeto, de forma a classificar este conhecimento e indexar- lo através do uso de ontologias.
Palavras chave:
Gestão do conhecimento, aquisição do conhecimento, desenvolvimento de software, conhecimento tácito.
As organizações já perceberam que a Gestão do Conhecimento (GC) ou Knowledge Management (Km) apresenta- se como um grande diferencial competitivo, mas gerenciar o conhecimento tem se tornado um grande desafio, visto a complexidade dos processos envolvidos para se obter sucesso na gestão deste recurso.
A gestão do conhecimento não é um tema novo no contexto organizacional, mas nos últimos anos tem recebido uma maior atenção com o surgimento de novas tecnologias e metodologias que contribuem para sistematizar e aperfeiçoar algumas de suas fases como a aquisição, a validação, o armazenamento e o compartilhamento do conhecimento.
O sucesso de uma organização está relacionado em grande parte ao conhecimento que esta possui.
O conhecimento de uma organização é formado a partir de o conhecimento dos seus membros e pode aumentar à medida que este conhecimento é armazenado e compartilhado.
Este é um conceito associado às organizações que aprendem.
Em este contexto de valorização do conhecimento como um recurso corporativo, surgiu a gestão do conhecimento, que tem o seu foco voltado para questões relacionadas à como as organizações podem tirar maior proveito do conhecimento existente dentro de elas.
A gestão do conhecimento facilita a distribuição deste conhecimento entre seus membros, encorajando o registro das soluções adotadas para resolução de problemas, evitando a perda do conhecimento de seus especialistas quando estes, por exemplo, deixam a organização.
Em o desenvolvimento de software o panorama é semelhante, pois para as empresas deste seguimento o seu principal recurso é o capital intelectual e não os prédios e as máquinas, sendo que a principal sugestão para melhor explorar este recurso é focar no uso da gestão do conhecimento.
A gestão do conhecimento em organizações de desenvolvimento de software é uma área ampla com muitas disciplinas que podem influenciar nos seus resultados.
Em Bjørnson e Dingsøyr é apresentada uma revisão sistemática sobre gestão do conhecimento em engenharia de software, onde são identificados estudos empíricos, discutidos os principais conceitos e as tendências desta área.
Em esta revisão sistemática é observado o interesse crescente nos últimos anos sobre o tema e os investimentos realizados por as empresas e institutos de pesquisa na busca de soluções que envolvam a gestão do conhecimento no contexto de desenvolvimento de software.
A engenharia de software é um domínio altamente orientado ao conhecimento, em o qual os fatores de sucesso estão relacionados com a experiência das pessoas envolvidas nas diversas fases e atividades do processo.
O conhecimento na engenharia de software é disperso, de proporção imensa e de crescimento contínuo.
De entre as questões que envolvem o uso da gestão do conhecimento na engenharia de software está a questão do uso do conhecimento em seu modo tácito ou explícito.
Podemos exemplificar esta situação com o surgimento das metodologias ágeis, que propõem alterações nos processos de desenvolvimento de software, de entre as quais, está a opção por a redução da externalização1 do conhecimento que circula nos projetos em detrimento de um uso maior do conhecimento em seu formato tácito.
Portanto, é reduzido o tempo despendido no processo de conversão do conhecimento tácito para explícito, que é um dos principais desafios na gestão do conhecimento.
Ao invés de um uso mais intenso de artefatos como diagramas e outros tipos de documentos, o conhecimento que circula por o projeto estará preferencialmente na sua forma tácita e isso acabará incrementando, por exemplo, a comunicação direta entre os envolvidos nos projetos.
Essa mudança na forma preferencial em que o conhecimento circula nos projetos de desenvolvimento de software, no caso de as metodologias ágeis quando comparadas às metodologias chamadas tradicionais ou &quot;tayloristics», altera a forma como deve ser concebida a comunicação e a gestão do conhecimento no ciclo de desenvolvimento de software.
O fato de priorizar o uso do conhecimento no seu formato tácito, reduzindo o processo de externalização do conhecimento, faz com que as metodologias, técnicas e ferramentas para gestão do conhecimento devam ser repensadas.
Por exemplo, o uso da comunicação direta, através das conversações, traz uma série de benefícios à comunicação num projeto, mas também vários aspectos relacionados às questões humanas e sociais emergem e devem ser considerados para uma maior efetividade nesta forma de compartilhamento e transmissão de conhecimento.
Neologismo utilizado em que define o processo de articulação do conhecimento tácito em conceitos explícitos.
Uma das fases mais complexas na gestão do conhecimento é a aquisição, pois ela incorpora aspectos organizacionais e individuais onde são considerados os modelos do processo de aprendizado individual, o processo criativo individual, o conhecimento tácito e a intuição.
Uma forma de tornar mais efetivo o processo de Aquisição do Conhecimento (AC) é diminuir o esforço dispensado para realização desta tarefa.
Quando se trabalha com gestão do conhecimento nas organizações, uma abordagem adequada para a aquisição do conhecimento é tentar ao máximo realizar esta etapa sem um esforço extra além de a própria realização da tarefa por o usuário.
De este modo, evita- se despender um esforço por o funcionário/ colaborador que seja específico para a aquisição e representação do conhecimento, além de a realização da própria tarefa, pois isso traz implicações nos processos organizacionais, que podem comprometer a estratégia adotada para a aquisição de conhecimento.
Diante deste contexto, caracterizado em projetos de desenvolvimento de software onde o conhecimento apresenta- se tanto no formato explícito, como no formato tácito, emerge a questão de pesquisa inicial:
&quot;como realizar a aquisição do conhecimento presente nas conversações realizadas nas reuniões de projetos de desenvolvimento de software, de forma a classificar este conhecimento e indexar- lo através do uso de ontologias?».
Convém destacar que a questão de como gerenciar o conhecimento que circula nas conversações dos projetos, não é exclusiva de quando se utiliza metodologias ágeis, mas sim, que essa questão é intensificada quando de o uso deste tipo de abordagem para o desenvolvimento de software.
Desta forma, o objetivo desta pesquisa é apresentar uma metodologia para aquisição do conhecimento presente nas conversações realizadas nas reuniões de projeto, de forma a classificar este conhecimento e indexar- lo através do uso de ontologias.
Não estão no escopo desta pesquisa e da metodologia apresentada aspectos de construção e criação de conhecimento.
Convém destacar que embora muitos ensaios e estudos de caso apresentados nesta pesquisa estão no contexto de projetos que seguem metodologias ágeis, a metodologia resultante apresentada não é direcionada para projetos que seguem metodologias ágeis.
Ela pode ser aplicada em qualquer projeto de desenvolvimento de software que cumpra os requisitos apresentados.
Esta pesquisa é relevante devido a o fato de que uma das formas mais representativas do conhecimento num projeto de software é gerada nas conversações durante as reuniões e este conhecimento acaba não sendo devidamente aproveitado.
Durante um projeto, este conhecimento que circula nas reuniões pode ser transformado para ser utilizado em outras formas de representação do conhecimento, como diagramas, formulários, atas, textos, códigos, etc..
Sendo que durante este processo de transformação as perdas e distorções do conhecimento são comuns, além de as dificuldades em manter estes artefatos atualizados com as constantes mudanças nos projetos.
Este conhecimento que circula nas reuniões pode também não ser transformado em outras formas de representação de conhecimento, acabando sendo somente transmitido entre os participantes sem que haja um mecanismo efetivo de retenção e indexação deste conhecimento para consultas posteriores e relacionamento com outras formas de representação de conhecimento do projeto.
Organização da proposta de tese Este documento está dividido da seguinte forma:
O Capítulo 2 apresenta a fundamentação teórica necessária para um bom entendimento das definições e conceitos relacionados à gestão do conhecimento no contexto de desenvolvimento de software;
O Capítulo 3 descreve o método de pesquisa utilizado ao longo de o projeto;
O Capítulo 4 apresenta os resultados preliminares;
O desenvolvimento de software;
O Capítulo 6 apresenta o estudo de caso explanatório;
E por fim, no Referencial Teórico Esta seção apresenta os principais conceitos que são abordados nesta pesquisa, que envolvem a gestão do conhecimento e suas implicações no âmbito organizacional, algumas classificações e metodologias.
Em seguida, será focada a gestão do conhecimento no processo de desenvolvimento de software, ressaltando diferenças entre metodologias tradicionais e ágeis e a evolução do interesse da academia e do mercado nesta área.
A fase de aquisição do conhecimento será tratada destacadamente, pois o foco desta pesquisa se concentra nesta etapa de uma metodologia de gestão do conhecimento.
Também será abordado o reconhecimento automático de fala, que será tratado como uma das formas de automatizar o processo de aquisição do conhecimento dentro de a proposta apresentada.
E por último, será abordado o tema ontologias, pois o processo de organização do conhecimento que será adquirido através das reuniões de projeto terá sua estratégia de indexação baseada no uso de ontologias.
Gestão do Conhecimento Para que as organizações garantam sua permanência no mercado é fundamental que possuam a capacidade de realizar mudanças que aumentem suas vantagens competitivas.
No entanto, a realização dessas mudanças nem sempre trazem os resultados esperados devido a pouca compreensão da organização sobre ela mesma, ou seja, a realização de mudanças que almejam benefícios à organização é dificultada por o escasso conhecimento da organização sobre a forma como os seus processos de negócio são realizados e sobre a sua própria estrutura organizacional.
Sendo assim, uma das maneiras das organizações tornarem- se mais competitivas é aumentar o seu conhecimento organizacional.
O conhecimento organizacional abrange, mas não se restringe, ao conhecimento relacionado aos processos de negócio, conhecimento sobre o relacionamento entre os diversos setores organizacionais, além de o conhecimento sobre o mercado, tecnologias, clientes e competidores.
Este conhecimento constitui o que é definido como capital intelectual e, portanto, deve ser gerenciado de forma eficiente para garantir a sua preservação e permitir a sua constante evolução e isso pode ser garantido através de uma política de gestão do conhecimento.
Vários autores,,, apresentam definições sobre a gestão do conhecimento, seus benefícios e desafios.
Novas abordagens surgem em forma de metodologias e tecnologias para auxiliarem nos processos da gestão do conhecimento, pois ainda são muitos os desafios para as empresas e especialistas da área.
Mesmo com a gestão do conhecimento não sendo uma área de estudos recente e apesar de todo incremento tecnológico dos últimos anos que contribuíram, direta ou indiretamente, para alavancar a gestão do conhecimento nas organizações, tem- se muitas dificuldades para implantação de projetos bem sucedidos nesta área.
Pode- se atribuir a estas dificuldades muitas variáveis, que vão desde aspectos tecnológicos e metodológicos a fatores organizacionais e comportamentais.
&quot;Organizações aprendem somente através de indivíduos que aprendem.
Aprendizagem individual não garante aprendizagem organizacional.
Mas sem ela a aprendizagem organizacional não ocorre».
Buono and Poulfelt apontam que a gestão do conhecimento está saindo da primeira para a segunda geração.
Em a primeira geração, o conhecimento era considerado uma possessão, sendo algo que poderia ser capturado.
A gestão do conhecimento era baseada em questões técnicas de como capturar e distribuir o conhecimento através de ferramentas como sistemas de gerenciamento de informações, repositórios de dados, etc..
A segunda geração é caracterizada por o knowing-in-- action.
O conhecimento é concebido como um fenômeno social e as soluções devem considerar sistemas humanos complexos, comunidades de práticas, zonas de conhecimento e estruturas de suporte.
Uma forma de compreender a abrangência e estruturação da gestão do conhecimento no panorama organizacional atual é apresentada em Earl, que classificou a gestão do conhecimento em &quot;escolas».
As escolas são categorizadas em &quot;Tecnocrática», &quot;Econômica «e &quot;Comportamental».
As escolas Tecnocráticas são:
1) Escola de sistemas:
Foca na tecnologia para compartilhar conhecimento usando repositórios de conhecimento.
2) Escola cartográfica:
Foca em mapas de conhecimento e criação de diretórios de conhecimento.
3) Escola de engenharia:
Foca em processos e fluxo de conhecimento nas organizações.
As escolas Econômicas focam em como os ativos de conhecimento, ou o capital intelectual, relaciona- se com os resultados financeiros da organização.
A escola Comportamental consiste em três sub-escolas:
1) Escola organizacional:
Foca em redes para compartilhar o conhecimento.
2) Escola espacial:
Foca em como o ambiente de trabalho pode ser projetado para promover o conhecimento.
3) Escola estratégica:
Foca em como o conhecimento pode ser a essência da estratégia nas organizações.
Em Dingsoyr et al.
É apresentada uma relação entre estas escolas e aspectos de desenvolvimento de software, onde, por exemplo, é mencionado que as escolas tecnocráticas estão mais relacionadas a projetos de desenvolvimento de software que seguem metodologias tradicionais.
Enquanto que escolas comportamentais estão mais relacionadas às abordagens ágeis.
Quando uma organização pretende aplicar gestão do conhecimento para obter retornos da gerência do seu capital intelectual, isso deve ser feito através da adoção de uma metodologia.
Uma metodologia para gestão do conhecimento pode ser apresentada de diversas formas, com diferentes etapas e componentes.
Em a literatura sobre este tema, encontram- se algumas sugestões que vem sendo aplicadas e servem de referência para trabalhos nesta área.
Para Turban et al.,
uma metodologia para a gestão do conhecimento deve seguir seis passos num ciclo.
Estes passos são:
Para Davenport a gestão do conhecimento compõe- se de, pelo menos, três etapas, não necessariamente consecutivas ou ordenadas:
Os autores Nonaka e Takeuchi também sugerem um modelo de cinco fases, focando o processo de criação do conhecimento:
As metodologias apresentam variações, mas possuem uma estrutura dorsal baseada na geração, aquisição, depuração, aplicação, armazenamento e compartilhamento do conhecimento.
Não estando estas vinculadas a ferramentas ou tecnologias específicas.
Gestão do Conhecimento na Engenharia de Software É aceito por a comunidade que é desejável realizar o esforço possível para que todas as formas de conhecimento de engenharia de software sejam capturadas e armazenadas em repositórios.
Forma- se um consenso entre empresas que atuam no desenvolvimento de software que a gestão do conhecimento, ou do seu capital intelectual, em nível individual, dos times e da organização é extremamente relevante para a melhoria dos processos e, por conseguinte, constitui- se num diferencial.
&quot;Um dos maiores problemas do capital intelectual é que ele tem pernas e volta para casa todos os dias.
Em a mesma proporção que a experiência caminha porta à fora, inexperiência caminha porta à dentro».
Uma organização que atua em desenvolvimento de software tem como seu principal ativo o capital intelectual, que é um reflexo, ou uma síntese, de todas as suas atividades, processos, tarefas e projetos que geram, consomem e transformam conhecimento.
Portanto, uma organização de desenvolvimento de software possui os mesmos desafios que empresas de consultoria, investimento, publicidade, advocacia, entre tantos outros ramos de atividade que possuem como seu principal asset o conhecimento e precisam fazer de ele o seu diferencial no mercado em que atuam.
Gestão do conhecimento nas organizações de desenvolvimento de software é uma área ampla com várias disciplinas que podem influenciar em seus resultados.
Em Bjørnson e Dingsøyr é apresentada uma revisão sistemática sobre gestão do conhecimento em engenharia de software.
Esta revisão sistemática discute as principais implicações que devem ser ponderadas na decisão sobre qual metodologia de gestão do conhecimento é adequada, considerando, por exemplo, se uma empresa pretende ter seus processos de desenvolvimento baseados em metodologias ágeis ou em metodologias tradicionais.
Quando metodologias tradicionais estão em uso, o objetivo da gestão do conhecimento é tentar o máximo possível transformar o conhecimento tácito em conhecimento explícito, vindo a representar este conhecimento através de diversos artefatos previstos nas metodologias de desenvolvimento de software.
Já para as metodologias ágeis, o foco está em trabalhar com o conhecimento tácito.
As iniciativas estão focadas em promover a troca mais direta de conhecimento através da conversação entre os membros do projeto.
Em as metodologias ágeis uma das principais questões avaliadas para estimular a troca do conhecimento no seu formato tácito é o esforço que se despende para converter o conhecimento tácito em explícito, a chamada &quot;explicitação do conhecimento».
Em a engenharia de software temos algumas definições para as chamadas &quot;organizações de software que aprendem».
Feldmann e Althoff, definem uma organização de software que aprende, aquela que cria uma cultura que promove aprendizagem contínua e promove a troca de experiências.
Dyba enfatiza este tipo de empresa como aquela que promove ações de melhoria através de um melhor conhecimento e entendimento dos seus processos.
Temos muitos estudos de casos e iniciativas de gestão do conhecimento na área de engenharia de software.
Como exemplo, podemos apontar uma que está fundamentada no reuso da experiência do ciclo de vida, processos e produtos para o desenvolvimento de software, que é a Experience Factory.
Esta proposta está baseada no fato de que projetos de desenvolvimento de software podem melhorar sua performance (custos, qualidade e cronograma) através da utilização das experiências de projetos anteriores.
A Experience Factory deve analisar e sintetizar todos os tipos de experiência, incluindo lições aprendidas, dados de projetos, relatório de tecnologia, entre outros e, fornecer serviços de repositório para estas experiências.
A Experience Factory emprega alguns métodos para empacotar experiência, incluído aí medições de projeto de vários processos de software e características de produtos e, então, constrói modelos destas características que descrevem seu comportamento em diferentes contextos.
Os dados destes modelos são provenientes de projetos de desenvolvimento obtidos de pessoas, documentos e suporte automatizado.
Podemos observar o interesse e crescimento de iniciativas de gestão do conhecimento em engenharia de software por o incremento de publicações nesta área, conforme destacado na revisão sistemática de.
Em este estudo foram identificados 68 trabalhos como &quot;lessons learned reports «ou &quot;empirical studies».
Os trabalhos foram publicados entre 1992 e 2007, sendo que é perceptível o incremento e a constância de publicações a partir de o ano de 2000.
Entre 1992 e 2000 foram 12 publicações e o restante, 66, a partir de o ano 2000.
Em maio de 2002 a IEEE Software tratou especificamente do tema knowledge conhecimento em empresas que desenvolvem software.
Empresas como Chrysler, Infosys, Nasa, Departamento de Defesa dos EUA (DoD), entre outros, apresentaram relatos de experiências do uso de gestão do conhecimento no processo de desenvolvimento de software.
O livro Managing Software Engineering Knowledge publicado em 2003 focou em tópicos que tentaram identificar o motivo por o qual gestão do conhecimento é importante para engenharia de software.
Em são apresentados overviews de trabalhos em gerenciamento do conhecimento em engenharia de software.
Em este trabalho são tratados aspectos sobre motivações e abordagens para gerenciar o conhecimento e fatores que devem ser considerados quando estratégias para gerenciar o conhecimento em empresas de software são implantadas.
Estes aspectos envolvem questões tecnológicas, organizacionais e de pessoas.
Em este artigo os autores também descrevem os tipos de ferramentas que são relevantes para gerenciar conhecimento, incluindo ferramentas para gerenciar documentos, competências e colaboração.
Em os autores utilizaram um survey para estudar iniciativas de gestão do conhecimento em engenharia de software.
Foram apontados oito relatórios no formato de lições aprendidas, os quais tratam como as companhias realizaram suas ações nesta área, quais os efeitos sofridos, ações realizadas, benefícios reportados e que tipos de estratégias para gerenciar o conhecimento foram empregadas.
Em o ano de 2009 temos o lançamento de dois importantes livros nesta área.
Schneider publicou um livro que trata de aspectos sobre experiência e gestão do conhecimento na engenharia de software, abordando:
Que tipo de conhecimento é importante para engenharia de software;
O reuso de experiências e conhecimento;
As representações formais e estruturas como ontologias e;
Estudos de casos.
Babar et al.
Organizou um livro que fornece uma visão geral dos principais conceitos de arquitetura de software e gestão do conhecimento, abordando tópicos como ferramentas e tecnologias para gestão do conhecimento de arquitetura de software onde também são apresentadas experiências observadas em estudos de casos.
Aquisição de Conhecimento Considerando que a fase de Aquisição do Conhecimento (AC) é uma das mais complexas e importantes num ciclo de gestão de conhecimento, é de suma importância que ao elaborar- se uma estratégia para aquisição do conhecimento, que esta seja bem definida e apropriada à realidade da organização, considerando aspectos tecnológicos, organizacionais e pessoais.
Rubin e Dai destacam a importância desta definição, pois eles consideram a fase de aquisição um &quot;gargalo «dentro de um ciclo de gestão do conhecimento.
O processo de gestão do conhecimento numa empresa despende muitos recursos (pessoas, tempo, investimento financeiro, etc.), tornando- se normalmente uma atividade custosa.
Portanto, é indicado que a fase que contempla o processo de aquisição de conhecimento deva ser proposta e implantada considerando que esta venha a ter um impacto mínimo nos processos organizacionais.
Existem diversas abordagens de aquisição de conhecimento que podem ser definidas, devido a a diversidade dos tipos de conhecimento que uma organização pode considerar como úteis e de acordo com os recursos tecnológicos e humanos que dispõem.
Para a efetivação do processo de aquisição de conhecimento, deve- se considerar qual a infraestrutura disponível que permitirá a aplicação de técnicas variadas de aquisição e a representação do conhecimento em diversos formatos, assim como possibilitará a aquisição de conhecimento apoiada por recursos computacionais.
As abordagens de aquisição podem ser definidas segundo a estrutura do conhecimento.
Estas abordagens podem ser de duas formas:
Manual ou automática.
O conhecimento tácito, complexo e de valor, como melhores práticas, lições aprendidas ou conhecimento do domínio é geralmente adquirido manualmente.
A infraestrutura de aquisição de conhecimento (técnicas, ferramentas, modelos e especialistas) deve, então, prover mecanismos que facilitem a captura manual do conhecimento.
O conhecimento explícito e menos complexo pode ser adquirido através de componentes de aquisição automática de conhecimento,.
Existem muitos conhecimentos de difícil captura no contexto organizacional, e o conhecimento sobre o domínio do negócio é um exemplo.
Técnicas de aquisição manuais como entrevistas estruturadas são bastante adequadas para capturar esse tipo de conhecimento O conhecimento tácito de experiências pessoais de membros da organização também constitui um recurso de conhecimento muito importante e de difícil extração, pois muitas vezes os membros da organização têm dificuldade de expressar suas idéias e seu conhecimento de forma adequada.
Este problema pode ser minimizado através de questionários que permitam adquirir de forma uniforme e estruturada o conhecimento absorvido por um membro da organização ao longo de vários anos de trabalho.
Também podem ser utilizadas técnicas como mapas conceituais para realizar este tipo de aquisição do conhecimento.
Abecker et al.
Sugerem a utilização de componentes de aquisição automática de conhecimento para analisar fontes de conhecimento internas ou externas à organização e alimentar as bases de conhecimento da memória organizacional.
Esta abordagem de aquisição de conhecimento permite que as informações distribuídas em diversas fontes, como as bases da organização, sejam analisadas e relacionadas automaticamente sem interação humana.
É claro que esta abordagem está associada ao uso de conhecimento explícito e estruturado, pois é difícil a tarefa de realizar a aquisição automática de conhecimento tácito, de forma que o mesmo possa ser depurado posteriormente.
Para a fase de aquisição do conhecimento nem sempre é necessário o uso de uma ferramenta específica.
Pode- se, por exemplo, fazer a aquisição do conhecimento numa organização através dos documentos, modelos, formulários e diagramas gerados durante a execução das tarefas.
Entrevistas formais e informais podem ser usadas como forma de captura do conhecimento das pessoas da organização.
Pode- se utilizar tecnologias e ferramentas já implantadas na empresa para outros propósitos, mas que podem acabar sendo aproveitadas para aquisição do conhecimento, como intranets, data warehouse, ferramentas de groupware, FAQs, etc..
Um aspecto importante na adoção de uma ferramenta para aquisição do conhecimento, assim como para qualquer outra fase na gestão do conhecimento, é que esta esteja vinculada a uma política organizacional e contemplada numa metodologia abrangente para todo o processo de gestão do conhecimento.
Reconhecimento Automático de Fala O uso de aplicações para o reconhecimento automático de fala pode ser um aspecto facilitador no processo de aquisição de conhecimento, já que um dos requisitos chaves a ser considerado durante o processo de aquisição é o mínimo de intervenção na atividade realizada de onde se pretende extrair o conhecimento.
Considerando que em muitas situações o conhecimento que circula nos projetos é exposto em reuniões e conversas entre membros das equipes e clientes, facilitaria para o processo de aquisição o uso de um mecanismo que converta o conhecimento gerado nestas reuniões para um formato mais apropriado de ser indexado e relacionado com outras informações do projeto.
Podemos observar o avanço da área de reconhecimento automático de fala (ASR Automatic Speech Recognition) nos últimos anos devido a o incremento de tecnologias e ferramentas relacionadas a esta área.
Em o estudo realizado por Adami é apresentado um panorama da evolução das técnicas, métodos e tecnologias aplicadas ao reconhecimento automático de fala.
Em a Figura 1 é ilustrado um gráfico de evolução de aspectos que compõem o reconhecimento automático de fala.
Em este gráfico os dois eixos apresentam variáveis que influenciam diretamente no desempenho dos sistemas de reconhecimento de fala, que são o estilo da fala (speaking style) e o tamanho do vocabulário (vocabulary size).
Um panorama considerado simples contemplaria palavras isoladas e um vocabulário pequeno e restrito.
Enquanto que um panorama complexo tenderia a utilizar um estilo de fala espontâneo e um vocabulário com um grande número de palavras tendendo a irrestrito.
As três linhas plotadas no gráfico fornecem uma idéia da evolução de tipos de aplicações empregadas no decorrer de as últimas décadas.
Temos como um exemplo que demonstra o investimento que esta área vem sofrendo, o projeto Calo -- Cognitive Assistant that Learns and Organizes, desenvolvido por o SRI (Stanford Research Institute) que possui projetos de speech recognition direcionados para reuniões.
O Calo é um projeto patrocinado por a DARPA (Defense Advanced Research Projects Agency) e desenvolvido em parceria com:
CMU (Carnegie Mellon University), Ga Tech (Georgia of Washington.
Este é um projeto desenvolvido juntamente com outras iniciativas do SRI para reconhecimento automático de fala em reuniões, que objetiva proporcionar não só a transcrição das conversações compostas por diversas pessoas, mas fornecer um ambiente composto por uma série de ferramentas com recursos para dar suporte aos participantes com feedbacks de assuntos relacionados aos tópicos que surgem na reunião.
Em os estudos observados, uma das maiores dificuldades encontradas para a transcrição das conversações das reuniões é a dificuldade em ter como fonte um áudio composto por diversos padrões de fala diferentes, ruídos, interrupções, transposição de falas e diálogos, entre outros Uma série de aplicações para reconhecimento automático de fala já estão disponíveis.
Algumas destas ferramentas são gratuitas como o Sphinx3, que é mantida por a CMU -- Carnegie Mellon University, sendo uma das mais conhecidas no meio acadêmico.
As soluções da empresa Nuance4, como o Dragon Naturally Speaking, estão entre as mais conhecidas no mercado e possuem várias versões, inclusive para áreas específicas como saúde e direito.
Muitos estudos relacionados à área de speech recognition preocupam- se também com questões acústicas e tratamento de áudio.
Outros estudos envolvem muitos aspectos das estruturas linguísticas e comportamentais dos participantes das conversações,.
Outro aspecto que tem recebido atenção da comunidade são as técnicas associadas à Diarization que identificam numa conferência quem falou o quê e quando, distinguindo o que é fala e o que não é fala.
Muitas vezes estas técnicas empregam simultaneamente recursos de áudio e vídeo.
Percebe- se então que o reconhecimento automático de fala, mesmo com restrições de desempenho em determinados contextos, apresenta- se com uma técnica adequada para auxiliar no processo de aquisição de conhecimento oriundo de conversações realizadas em reuniões Ontologias O termo ontologia é derivado da filosofia, onde seu significado está associado à natureza do ser, da realidade, da existência dos entes e das questões metafísicas em geral.
Em a visão de Aristóteles significa &quot;uma explicação sistemática da existência», que pode ser interpretado como a definição de um domínio do conhecimento num nível genérico, utilizada para especificar o que existe ou o que se pode dizer sobre o mundo.
Em este contexto, filósofos tentam responder as questões:
&quot;O que é um ser?
E quais são as características comuns de todos os seres?»
Para as áreas da Ciência da Computação e da Ciência da Informação, uma ontologia é um modelo de dados que representa um conjunto de conceitos dentro de um domínio e os relacionamentos entre estes.
Uma ontologia é utilizada para realizar inferência sobre os objetos do domínio.
Uma ontologia também representa a aquisição do conhecimento a partir de dados semiestruturados utilizando um conjunto de métodos, técnicas ou processos automáticos ou semiautomáticos.
O termo foi recentemente adotado por as comunidades de Inteligência Artificial e gestão de conhecimento para se referir a conceitos e termos usados para descrever alguma área do conhecimento ou construir uma representação deste.
Em Inteligência Artificial pode- se assumir uma interpretação para ontologia como sendo um conjunto de entidades com suas relações, restrições, axiomas e vocabulário.
Uma ontologia define um domínio, ou, mais formalmente, especifica uma &quot;conceituação «acerca de ele.
Ontologias ajudam a formalizar o conhecimento dividido por um grupo de pessoas quando o conhecimento precisa ser modelado, estruturado e relacionado e abrem o caminho para substituir a visão orientada a documentos por uma visão orientada a conteúdo em a qual itens de conhecimento são relacionados, combinados utilizados Desta forma, compartilhamento efetivo de conhecimento pode ser alcançado através de acesso a múltiplas bases de conhecimento utilizando ontologias que permitam usuários definir os recursos que eles necessitam e requerem.&amp;&amp;&amp;
Ontologias descrevem explicitamente modelos conceituais de um domínio e, portanto, são úteis na construção de memórias organizacionais, pois permitem definir as estruturas e os relacionamentos de itens de conhecimento armazenados nas bases de conhecimento, além de definir as características e visões dessas bases e prover modelos que ajudam na definição e acesso a elas.
Ontologias ajudam a indexar a memória organizacional para permitir pesquisas posteriores e recuperação de conhecimento na memória corporativa materializada em documentos ou outros arquivos e facilitam também a identificação de comunidades de prática.
Em uma comunidade de prática um grupo distribuído de pessoas pode compartilhar os mesmos interesses numa tarefa, problema, ou prática.
Desta forma, ontologias possibilitam relacionar membros de uma organização ou várias organizações segundo conhecimento específico que cada um possui ou tem interesse.
A formalização do conhecimento através de ontologias facilita a comunicação entre especialistas de um domínio e evita falhas no processo de aquisição e transferência do conhecimento, pois colocam restrições na estrutura e no conteúdo do conhecimento do domínio.
Além de o mais, a formalização e restrições do conhecimento do domínio impostas por as ontologias facilitam a integração de múltiplas bases de conhecimento e minimizam o risco de ambiguidades entre elas.
Para se definir e manipular ontologias se sugere a utilização de linguagens que suportem estruturas para representação do conhecimento.
Esta representação é realizada através da descrição formal de um conjunto de termos sobre um domínio específico.
A definição de uma linguagem é necessária para a representação e descrição formal da estrutura que especifica uma conceituação.
Apontamos como exemplo de uma linguagem a OWL (Web Ontology Language), que é recomendada por a W3 C5 como linguagem para manipulação de ontologias e seu diferencial é a capacidade de processamento semântico através de inferência.
A linguagem OWL é construída sobre RDF e RDF Schema e baseada na sintaxe XML.
O modelo básico de dados do RDF, e herdado por OWL, é definido através de,:
Identifier); Propriedade:
Representam recursos, características que representam recursos ou relacionamento entre recursos;
Para se estruturar um documento OWL, define- se em alto nível:
Classes: Conjunto de instâncias com características comuns, podendo ser consideradas superclasses, relacionamentos e disjunções;
&quot;parte de&quot;;
Indivíduos: Representam os objetos num domínio, isto é, instâncias específicas.
Verifica- se aqui que dois nomes podem representar o mesmo objeto no mundo real.
As ontologias possuem várias classificações e aplicações, que diferem quanto a a função, ao grau de formalismo, à estrutura e ao conteúdo.
Para o uso no contexto de desenvolvimento de software, também encontramos diversificadas aplicações, onde podemos destacar o estudo de Happel e Seedorf.
Estes autores apontam os diversos usos de ontologias na engenharia de software durante o ciclo de vida de desenvolvimento, onde destacam seu uso para as fases de:
Análise e Projeto:
Ontologias podem beneficiar a área de engenharia de requisitos em termos de representação do conhecimento e suporte ao processo.
Durante o projeto também podem auxiliar no reuso de componentes.
Implementação: Durante o ciclo de desenvolvimento, a transição da análise e projeto para a implementação é uma etapa crítica e depende da maneira como o domínio do problema é mapeado.
Para esta etapa sugere- se a modelagem de software com auxílio de ontologia.
As ontologias também podem fornecer suporte para codificação e documentação do código.
Manutenção: Ontologias podem ser usadas para unificar as diversas fontes de informação sobre os sistemas e evitar trabalho redundante.
Também são sugeridos frameworks específicos para auxiliar na atualização de versões dos sistemas e para a fase de testes.
Para o contexto desta tese serão utilizadas ontologias de domínio, que descrevem os conceitos e relacionamentos de um determinado domínio de conhecimento.
Levando em consideração a classificação de Happel e Seedorf uma ontologia do domínio poderia ser utilizada em todo o ciclo de desenvolvimento de um software.
Considerações sobre o capítulo Considerando a questão de pesquisa deste projeto, que envolve a aquisição do conhecimento presente nas conversações realizadas em reuniões de projeto, as seções anteriores apresentaram uma revisão da literatura, buscando contextualizar dentro deste foco de pesquisa, definições sedimentadas sobre os tópicos que envolvem gestão do conhecimento e engenharia de software, aquisição do conhecimento, reconhecimento automático de fala e ontologias.
A revisão na literatura apresentou a importância da gestão do conhecimento como uma das formas de uma empresa obter um diferencial competitivo, esta visão também foi estendida para o contexto de desenvolvimento de software, onde foi percebida uma divisão entre o uso preferencial do conhecimento tácito (metodologias ágeis) e o uso preferencial do conhecimento explícito (metodologias tradicionais).
Um aspecto chave na gestão do conhecimento, considerando esta dicotomia (tácito/ explícito) é a fase de aquisição do conhecimento.
Durante a revisão sobre a fase de aquisição, percebeu- se a importância de aplicações para reconhecimento automático de fala, como uma opção para auxiliar a automatização da aquisição do conhecimento na forma tácita.
Também foram realizados estudos sobre ontologias, pois se apresentou como uma alternativa para a indexação deste conhecimento a ser adquirido das conversações.
Este capítulo destacou ainda questões em aberto que necessitam de maior investimento para uma maior efetividade das soluções de gestão do conhecimento no contexto de desenvolvimento de software, considerando aspectos como, por exemplo, o uso preferencial do conhecimento em sua forma tácita, que é mais comum em metodologias ágeis.
Método de Pesquisa Este capítulo apresenta o método de pesquisa empregado neste projeto, sendo que este faz uso de estudos de caso, estudos de viabilidade, revisão sistemática e a definição de uma metodologia a ser utilizada no contexto de desenvolvimento de software.
No decorrer de as próximas seções será apresentado o método utilizado e a forma como este foi estruturado.
Em Oates é apresentado um sumário de diferentes tipos de produtos derivados de pesquisas com contribuições para o conhecimento que podem ser:
Evidência, metodologia, análise, conceitos e teorias e, produto.
Esta tese apresenta uma metodologia como contribuição principal, que segundo Oates, &quot;também pode ser chamado de Método e é um guia de orientações sobre os modelos a serem produzidos e etapas do processo a serem seguidas para resolver problemas usando Tecnologias da Informação».
Como usaremos estudos de caso nesta pesquisa, a seguir temos algumas definições para estes tipos de estudos de caso, segundo Oates:
Como também foi utilizada uma revisão sistemática na metodologia, cabe destacar as definições, e um guia para realizações de revisões sistemáticas, que são encontradas em Kitchenham.
Em Dyba e Dingsøyr é apresentado um estudo sobre a realização de revisões sistemáticas em engenharia de software e é posto que uma revisão sistemática «é um resumo conciso das melhores evidências disponíveis, que utiliza métodos explícitos e rigorosos para identificar, avaliar criticamente e sintetizar os estudos relevantes sobre um determinado tema.
Estes métodos são definidos com antecedência e documentados num protocolo para que outros possam apreciar criticamente e replicar a revisão».
Para o desenvolvimento da metodologia proposta foram seguidas seis grandes etapas que constituem esta pesquisa.
Para uma melhor compreensão das atividades que compõem estas seis etapas, a Tabela 1 apresenta uma descrição das principais atividades propostas para cada etapa.
Como pode ser visto na Tabela 1, as etapas da metodologia são sequenciais e serão detalhadas a seguir:
Etapa 1 -- Reconhecimento inicial do problema Essa etapa foi eminentemente exploratória.
Em ela foram estudados conceitos e aspectos relevantes à formulação da metodologia proposta, entre os quais, conceitos de aquisição e gestão do conhecimento.
A área de gestão do conhecimento é ampla e multidisciplinar, sendo que focamos em aspectos mais relacionados à sua influência no desenvolvimento de software.
Também foram realizados estudos mais aprofundados em fases específicas como a etapa de aquisição de conhecimento, que está no foco da nossa pesquisa.
Devido a a percepção inicial do contexto mais adequado para aplicação da metodologia, foram realizados estudos sobre aspectos da gestão do conhecimento em metodologias ágeis para o desenvolvimento de software.
Convém destacar que embora a percepção inicial fosse da aplicação da metodologia proposta em ambientes de desenvolvimento de projetos de software que seguissem metodologias ágeis, ao final da pesquisa, apontamos que a metodologia proposta não se restringe e não está focada somente em projetos ágeis.
Esta etapa foi finalizada com a realização de um estudo de caso exploratório realizado numa grande empresa multinacional fabricante de computadores.
Esta empresa contratou outra empresa para desenvolvimento de uma aplicação, sendo que as equipes (da contratante e da contratada) realizaram o projeto seguindo uma metodologia híbrida (cascata e Scrum).
Foi aplicado um survey e realizadas entrevistas com os membros das equipes de desenvolvimento e o cliente.
Os detalhes do contexto onde foi aplicado o estudo de caso, os instrumentos de pesquisa, a análise dos dados e os resultados foram descritos num artigo apresentado na 11 th International Conference ­ ICEIS 2009.
Entre as lições aprendidas apresentadas neste artigo, podemos destacar de forma genérica os problemas relacionados à comunicação entre os membros da equipe e entre estes e o cliente;
Os problemas nos repositórios de artefatos do projeto;
O uso consciente de documentação desatualizada por os membros da equipe;
O intenso uso de comunicação informal em questões relacionadas ao projeto e;
Uma diferença do ponto de vista da gestão do conhecimento quando é empregada uma metodologia ágil de projeto, pois o conhecimento passa a ser utilizado preferencialmente no formato tácito.
O estudo dos conceitos envolvidos na pesquisa e o estudo de caso confirmaram a necessidade de uma revisão aprofundada sobre o tema.
As conclusões detalhadas do estudo de caso exploratório são apresentadas na seção 4.1 Estudo de caso explanatório.
Etapa 2 ­ Estudo em profundidade sobre gestão do conhecimento em projetos de software ágeis Percebeu- se na Etapa 1 que a gestão do conhecimento quando aplicada a projetos de software que seguem as chamadas metodologias tradicionais priorizam o uso do conhecimento em seu formato explícito, enquanto que, quando segue- se metodologias ágeis, o uso do conhecimento é priorizado em seu formato tácito, com um incremento das conversações através da comunicação direta entre os participantes do projeto.
Com o objetivo de identificar, avaliar e interpretar a literatura relevante sobre estes temas decidiu- se por a realização de uma revisão sistemática, sobre gestão do conhecimento em desenvolvimento de software ágil.
Esta revisão objetivou responder, entre outras questões, que influências são percebidas na gestão do conhecimento quando o conhecimento que circula no projeto é trabalhado preferencialmente em seu formato tácito e quando o processo de externalização do conhecimento é preterido por o incremento da comunicação direta entre os membros da equipe.
A revisão sistemática objetivou fornecer uma visão geral de estudo dentro de a gestão do conhecimento em projetos de software ágeis, que tipos de conceitos têm sido explorados, quais são os principais resultados encontrados e os métodos de pesquisa utilizados.
Mais especificamente nós procuramos responder as seguintes questões de pesquisa:
Quais são os principais conceitos em gestão do conhecimento que têm sido investigados em projetos de software ágeis?
Quais são os principais resultados encontrados em pesquisas sobre gestão do conhecimento em projetos de software ágeis?
A o responder estas duas questões também verificamos a existência de uma área que surge da intersecção da gestão do conhecimento e de projetos de software ágeis e se há contribuição suficiente para caracterizar- la como uma área de pesquisa.
As contribuições da revisão sistemática são apresentadas através de uma síntese dos resultados dos trabalhos relevantes encontrados, das discussões sobre estes e as conclusões do trabalho.
O planejamento, o desenvolvimento e as conclusões detalhadas da revisão sistemática são apresentadas na seção 4.2 Revisão sistemática sobre gestão do conhecimento em projetos de software ágeis.
Diante de os resultados obtidos nas etapas 1 e 2, foi elaborado um estudo de viabilidade para se verificar a possibilidade de utilização do conhecimento gerado e compartilhado nas reuniões de projeto como uma forma de representação do conhecimento, que durante o seu processo de aquisição possa ser classificado e indexado, como será mostrado na etapa a seguir.
Etapa 3 ­ Estudo de Viabilidade Em esta etapa foram realizados vários ensaios para se verificar a viabilidade de aproveitar os diálogos de uma reunião como forma de conhecimento que possa ser indexado e relacionado com a ontologia do domínio de um projeto de desenvolvimento de software.
A proposta para este estudo de viabilidade, de forma genérica, foi realizar reuniões seguindo um roteiro baseado no estudo de caso exploratório e gravar- las num ambiente acusticamente adequado e com equipamentos apropriados.
Para cada reunião foi gerado um arquivo de áudio, que foi submetido a um software de reconhecimento de fala, que gerou a transcrição textual das conversações.
Estes arquivos com as transcrições foram &quot;etiquetados», por um aplicativo &quot;tagger», de forma a extrair alguns tipos de palavras, como os substantivos, que serviram para representar os conceitos associados à reunião.
Estes conceitos foram classificados e comparados à ontologia do domínio.
Por último foi realizado um survey para avaliar a &quot;qualidade «destes conceitos gerados.
Foram realizados ensaios para verificarmos a viabilidade dos seguintes tópicos:
Após a realização dos ensaios acima, foi realizado um survey para confrontar os conceitos que foram extraídos das reuniões automaticamente e que poderiam ser utilizados como indexadores destas reuniões, com os conceitos que humanos apontariam como indexadores para estas mesmas reuniões.
Os resultados e detalhes da execução dos ensaios realizados neste estudo de viabilidade estão descritos na seção 4.3 Estudo de caso explanatório.
Etapa 4 -- Proposta Metodológica O objetivo desta quarta etapa é a proposição de uma metodologia que possibilite a aquisição de conhecimento das reuniões de projetos de desenvolvimento de software, de forma a indexar este conhecimento através da ontologia do domínio, possibilitando, por exemplo, recuperar reuniões que abordaram determinado tema durante o projeto de desenvolvimento de software;
Listar conceitos associados a uma determinada reunião do projeto e;
Associar conceitos das reuniões a conceitos de outros artefatos do projeto.
Os detalhes de como foi concebida e elaborada a proposta metodológica, assim como suas etapas e fases são apresentadas no Capítulo 5 ­ Metodologia Proposta.
Etapa 5 ­ Validação através de um estudo de caso explanatório Um estudo de caso foca numa instância do que está sendo investigado:
Uma organização, um departamento, um sistema de informação, um fórum de discussão, um desenvolvedor de sistemas, um projeto de desenvolvimento, etc..
Esta instância deve ser estudada em profundidade, usando uma variedade de métodos de geração de dados (entrevistas, observação, análise da documentação, questionários, etc.) e o objetivo é obter uma visão rica e detalhada sobre a vida desse caso, suas relações e os seus processos complexos.
Além de a definição anterior, Oates também apresenta alguns pontos que devem ser observados na seleção de um estudo de caso.
Considerando que um estudo de caso é uma instância da &quot;coisa «a ser investigada, é importante a escolha correta da instância e esta pode ser baseada em:
Bons pesquisadores atentam para este tipo de oportunidade.
O nosso estudo de caso explanatório foi classificado como &quot;conveniência «por as definições anteriores, mas um fator decisivo para escolha da instância foi atender os nossos requisitos elencados de forma a aplicar todas as etapas da metodologia proposta.
Os requisitos especificados para a realização do estudo de caso foram:
O principal aspecto avaliado em nossa proposta através deste estudo de caso diz respeito à &quot;qualidade «ou &quot;relevância «dos conceitos extraídos das conversações e apontados como indexadores das reuniões.
O objetivo principal desta avaliação foi verificar os resultados gerados com a aplicação da metodologia proposta, quando comparados com os resultados de um processo realizado de forma manual por as pessoas que participaram das reuniões.
A avaliação utilizou a aplicação de um instrumento de survey.
Os detalhes sobre o planejamento do estudo de caso explanatório, da sua execução e dos resultados obtidos, são apresentados no Capítulo 6 ­ Resultados do estudo de caso explanatório.
Etapa 6 ­ Refinamento da metodologia proposta e criação de um protótipo para auxiliar na avaliação da metodologia De acordo com as etapas propostas para o nosso método de pesquisa, após a realização do estudo de viabilidade (Etapa 3), foram reunidos subsídios para a proposição de uma metodologia (Etapa 4) para aquisição de conhecimento de reuniões de projeto.
A metodologia neste ponto foi proposta com base no resultado das etapas anteriores, mas com a realização do estudo de caso explanatório, alguns pontos foram revistos e modificados.
O estudo de caso explanatório serviu para modificarmos e aperfeiçoarmos pontos da metodologia proposta, em relação a o que havia sido concebido até o estudo de viabilidade, que são os seguintes:
Além de o refinamento destes pontos da metodologia proposta, após o estudo de caso explanatório também foi elaborado um protótipo para realizar consultas à ontologia do domínio, acrescida de informações adquiridas das reuniões.
A proposta para o uso do protótipo é realizar queries na ontologia, através de um mecanismo de inferência, que possam retornar, por exemplo, em quais reuniões determinado conceito, ou conjunto de conceitos, da ontologia foi abordado.
O protótipo, além de apontar quais as reuniões estão associadas a determinado conceito, possibilita a visualização dos outros conceitos relevantes da reunião, a visualização do texto transcrito da reunião e a execução do áudio da reunião.
Detalhes do funcionamento do protótipo são apresentados no Capítulo 6 ­ Resultados do estudo de caso explanatório.
Considerações sobre o capítulo O objetivo deste capítulo foi apresentar as etapas necessárias para a execução do método de pesquisa proposto.
Destacamos no método de pesquisa apresentado a realização de dois estudos de casos, um exploratório e outro explanatório, acompanhados de dois surveys que contribuíram significativamente para os requisitos de qualidade das conclusões apresentadas.
Também destacamos a realização de uma revisão sistemática para apontar de forma mais precisa, dentro de um rigor científico desejado, as questões em aberto na área de pesquisa focada nesta tese.
Resultados preliminares O objetivo deste capítulo é analisar os resultados obtidos com a realização das três etapas iniciais do método de pesquisa para esta tese.
Estas três etapas apresentaram resultados que serviram de subsídios para a quarta etapa, onde é apresentada a proposta metodológica para aquisição de conhecimento em reuniões de projetos de desenvolvimento de software.
Como estas etapas já foram introduzidas e apresentadas no capítulo 3, neste capítulo focaremos no desenvolvimento das mesmas e na análise dos resultados obtidos.
Estudo de caso exploratório Entre julho e novembro de 2008 foi realizado um estudo de caso exploratório numa empresa fabricante de computadores, que possui uma de suas fábricas de software instalada em Porto Alegre.
Em este estudo de caso, esta empresa fabricante de computadores, utilizou uma empresa terceira para desenvolver parte de uma aplicação.
Durante o estudo de caso as empresas foram referenciadas como a Organização Proprietária (A) e a Organização Offshore (B).
Foi analisado o trabalho das equipes que trabalharam em dois projetos diferentes, mas correlacionados nas organizações citadas.
O primeiro projeto consistiu no desenvolvimento de um novo software e o outro (Projeto 2) foi um projeto de manutenção de software (ambos projetos possuem o mesmo escopo de negócio).
Para detalhar o estudo de caso, serão apresentados os seguintes tópicos:
Uma explanação sobre o contexto de cada organização;
Os instrumentos utilizados para coleta de dados e;
As considerações obtidas a partir de análise de dados.
O método de pesquisa aplicado neste estudo de caso utilizou dados qualitativos e quantitativos coletados em questionários, análise documental e entrevistas semi-estruturadas.
A chamada organização A desenvolveu um produto P que consiste num software que controla as suas importações, e tem como intuito diminuir os impostos para este tipo de operação.
Além disso, esse produto é dividido em dois subprodutos:
Subproduto 1, que implementa as regras e o processo de redução de impostos e o subproduto 2, que deve acelerar o processo de alfândega destes bens importados, reduzindo o esforço de controle e vigilância.
Estes subprodutos foram implementados com tecnologias atualmente obsoletas e não automatizaram algumas novas regras do negócio, o que implica numa grande quantidade de trabalho que acaba sendo realizado manualmente.
No entanto, estes subprodutos tem sido mantidos continuamente por o Projeto 2.
Devido a as necessidades da organização, um novo projeto, com tecnologias atualizadas, visa proporcionar uma automação de todas as regras de negócios e ficar conectado com as interfaces dos sistemas de comunicação do Governo.
Devido a o grande número de stakeholders, à interface com vários outros sistemas e muitas restrições de acesso, este novo projeto tem um alto nível de complexidade.
Para o desenvolvimento deste novo projeto foram utilizadas duas equipes distribuídas.
A primeira equipe, chamada S-Team, está localizada na região sul do Brasil.
A outra equipe, chamada N-Team, está localizada na região nordeste.
Ambas as equipes possuem colaboradores com o SCRUM Master Certify.
Para avaliar o perfil das pessoas envolvidas com os projetos, bem como para identificar as responsabilidades de cada um durante a execução dos projetos, definimos uma pesquisa chamada &quot;survey de aspectos gerais da equipe».
Esta pesquisa foi dividida em três partes:
Questões gerais;
Questões sobre a experiência dos membros da equipe em diferentes metodologias de desenvolvimento de software e;
Questões sobre características específicas dos projetos.
As questões incluídas na primeira parte procuravam verificar:
O nível educacional dos membros da equipe, quantos anos cada membro tinha de experiência profissional em Tecnologia da Informação, a relação de trabalho atual dos membros da equipe com as organizações (se empregado, contratado ou estagiário), há quantos meses estão trabalhando para as organizações, fluência dos membros da equipe no idioma Inglês e idade dos membros da equipe.
A segunda parte da pesquisa teve como objetivo avaliar o conhecimento dos membros da equipe em metodologias ágeis, tradicionais e híbridas.
Para este item, cada membro da equipe foi questionado sobre:
Os anos de experiência profissional com cada metodologia, o número de projetos distintos que trabalharam com as metodologias durante a sua experiência profissional e;
Os membros da equipe com experiência em metodologias ágeis responderam mais quatro questões.
As questões adicionais eram destinadas a verificar:
Qual foi a metodologia ágil utilizada em projetos anteriores;
Se a empresa forneceu um treinamento formal em processos de desenvolvimento ágeis para a equipe;
Se os membros da equipe tinham alguma certificação em metodologias ágeis e; Quais
as características de metodologias ágeis estavam presentes nos processos utilizados em projetos anteriores.
A terceira parte do survey destinava- se a verificar os papéis e responsabilidades dos membros da equipe nos projetos.
As questões incluídas nesta parte tiveram como objetivo identificar:
Os papéis dos membros da equipe;
A experiência do membro da equipe no domínio do problema;
O tempo (percentual) de alocação estimado de cada membro da equipe para o projeto;
A percepção de cada membro da equipe sobre o processo utilizado nos projetos (se o processo definido era principalmente uma metodologia tradicional ou ágil) e; (
v) a percepção dos membros da equipe sobre a utilidade da documentação gerada durante os projetos.
O survey foi respondido por 15 dos 17 membros das equipes da Organização Proprietária e da Organização Offshore que participaram do Projeto 1 e do Projeto 2.
O formulário respondido por os membros da equipe encontra- se no Apêndice A deste trabalho.
Além de a aplicação do questionário, os membros das equipes também foram entrevistados para ampliar a coleta de dados.
O objetivo principal das entrevistas foi discutir sobre a documentação produzida e utilizada durante os projetos.
Para tanto, os funcionários foram questionados sobre o tipo de documentação gerada nos projetos, quem gerava e disponibilizava a documentação, e como conhecimento tácito era trocado entre eles (quais canais de comunicação foram utilizados para troca de conhecimento de forma mais direta).
As entrevistas foram realizadas de forma semi-estruturada.
Quinze funcionários foram selecionados para serem entrevistados em reuniões individuais de cerca de uma hora.
As questões abordadas nas entrevistas estão vinculadas aos documentos que abordam os requisitos do projeto que serviu de estudo de caso.
O objetivo desta seção é analisar os dados obtidos através do questionário e entrevistas já descritos.
Vamos apresentar apenas as informações pertinentes obtidas através de uma análise estatística.
O objetivo foi identificar quais as tendências e as correlações entre os dados analisados podem trazer alguma contribuição no âmbito do presente estudo.
O foco desta análise é compreender aspectos relevantes fazendo uma comparação entre as metodologias ágeis e tradicionais, com foco na mudança do uso do conhecimento explícito para um maior uso do conhecimento tácito.
Em esse sentido, o núcleo da nossa análise estava centrado em questões relacionadas com documentação, conversações entre a equipe, forma de representação do conhecimento e como o conhecimento é adquirido e compartilhado.
A Figura 2 apresenta alguns dos dados relevantes obtidos através de análise de dados de questionários e entrevistas.
Question 11: Is the documentation generated during agile software project tasks?
Em o;
33% Yes, sometime s;
13% Question 17: Is the documentation generated during traditional software project tasks?
Yes, always;
A Figura 2 ilustra as respostas às perguntas 11 e 17 do questionário.
A pergunta 11 tentou verificar se os membros da equipe consideram útil a documentação num processo ágil.
Isso pode mostrar o grau de confiança que a equipe tem sobre os documentos gerados durante o processo de desenvolvimento de software.
Houve mais respostas indicando que eles não consideram útil (33%) do que sempre é útil (7%).
A pergunta 17 do questionário tentou verificar com os membros da equipe como estes consideram útil a documentação num processo tradicional.
Não houve resposta indicando que eles não considerem útil, nem que sempre é útil.
A Figura 3 ilustra as respostas às perguntas 5 e 7 da entrevista.
A pergunta 5 examinou se os entrevistados consideram atualizada a documentação gerada durante o processo de desenvolvimento de software.
Quase todos consideram a documentação desatualizada.
A pergunta 7 examinou se os membros das equipes utilizam a documentação ou os demais membros da equipe e colegas como fonte prioritária de conhecimento sobre o projeto.
A maioria (73%) consulta a documentação primeiramente.
Deve ser enfatizado que esta questão define a ordem de prioridade preferida por o entrevistado no momento de tentar conseguir algum conhecimento sobre o projeto.
Question 5: De o you consider the documentation generated during updated?
Question 7: When do you need to obtain some knowledge, what do you consult first?
A Figura 3 mostra os resultados sobre a utilidade da documentação no processo de desenvolvimento de software.
O que chama a atenção nesses dados é que os entrevistados consideram mais importante a documentação em metodologias ágeis.
Durante as entrevistas percebemos que estas respostas estão associadas ao fato de que em metodologias tradicionais eles consideram uma boa parte da documentação não útil.
No entanto, a documentação que é utilizada em projetos ágeis é considerada mais útil.
Foi utilizado o coeficiente de correlação estatística de Pearson para avaliar a correlação entre as respostas das questões 11 e 17 e das questões 5 e 7.
Para a questões 11 e 17 se obteve uma medida de correlação de 0,536.
Esse valor não indica uma forte correlação entre os conjuntos de respostas duas questões.
A Figura 3 mostra que a maior parte dos entrevistados considerou a documentação desatualizada, mas ainda usa a documentação antes de consultar os colegas.
O coeficiente da correlação de Pearson encontrado entre as questões 05 e 07 é de 0,99.
Esse valor indica uma forte correlação entre o conjunto de respostas duas questões.
As principais análises apontadas nas lições aprendidas foram obtidas através da análise qualitativa dos dados dos questionários e, especialmente, a partir de as entrevistas.
De acordo com as observações extraídas do estudo de caso, sugerimos alguns itens a serem discutidos, como resultado das análises dos dados, que são os seguintes tópicos:
A) O uso de metodologias ágeis em desenvolvimento de software distribuído intensifica os problemas de comunicação:
Em este estudo de caso o processo de desenvolvimento é caracterizado como Desenvolvimento Distribuído de Software (DDS), onde as equipes estão localizadas em dois sites.
Foram percebidos por meio de entrevistas e do acompanhamento das reuniões (daily Scrum), alguns problemas de comunicação entre os grupos distribuídos.
A comunicação em DSD é, naturalmente, identificada como uma grande dificuldade.
Em o contexto de projetos ágeis a comunicação se torna ainda mais relevante porque nesse tipo de projeto a comunicação com base em discussões é priorizada em relação a outras formas de intercâmbio de conhecimento.
Korkala e Abrahamsson destacam que o desenvolvimento ágil de software envolve requisitos altamente voláteis, que são geridos através de uma comunicação verbal eficiente.
Como a comunicação é um fator importante a ser observado, no nosso estudo de caso verificou- se que cada grupo tem seu próprio repositório de documentos e artefatos que não funcionam em sincronia.
Não há uma prática estabelecida para o acesso e compartilhamento de artefatos nos repositórios.
Isso levou os membros das equipes a intensificar a comunicação direta entre os integrantes dos dois sites.
Se existissem mecanismos de compartilhamento de conhecimento (repositórios) que funcionassem de modo planejado e coordenado entre os grupos, a necessidade de comunicação através de conversações tenderia a diminuir.
Isso diminuiria o impacto do fator comunicação, como um problema neste tipo de projeto, ágil em DSD.
É importante ressaltar que não estamos sugerindo uma maior utilização dos repositórios e artefatos, em conflito com alguns pressupostos das metodologias ágeis, mas os artefatos que já estão disponíveis para os grupos devem ser devidamente compartilhados, reduzindo a necessidade de conversações sobre a informação que já está disponível num dos sites.
B) O uso intenso de comunicação informal em projetos ágeis cria problemas de aquisição de conhecimento e de armazenamento.
Outro aspecto percebido sobre a comunicação entre os membros da equipe e os clientes é que a maioria das negociações realizadas durante o projeto é informal.
Este formato torna a comunicação mais ágil, mas em compensação o conhecimento gerado durante as conversações só se mantém na forma tácita quando, mesmo em projetos ágeis, muitos detalhes devem ser registrados e armazenados para consultas posteriores ou confirmações.
Um exemplo desta situação percebida no estudo de caso foi em relação a os requisitos levantados junto aos clientes.
Muitos entrevistados (53%) relataram que é importante registrar as conversas com os clientes, de acordo com as constantes mudanças e atualizações nos requisitos.
Muitas vezes as conversas não são gravadas ou registradas criando um conflito de informações e interesses entre as partes envolvidas no processo.
Esta questão suscita um dos principais pontos discutidos neste trabalho, que é a necessidade de uma nova perspectiva para uma metodologia de gestão do conhecimento a ser empregada nos processos de desenvolvimento ágeis.
Esta metodologia deve ser centrada no conhecimento tácito que circula nos projetos.
Como o objetivo não é tornar o processo de comunicação entre as partes interessadas burocrático, uma sugestão é usar mecanismos de reconhecimento automático de voz para organizar e resumir as conversações.
Em outras palavras, haveria um esforço mínimo da equipe, com quase nenhuma interferência na forma como as negociações são realizadas, mas que poderia resultar na captura e registro do conhecimento num formato que pode ser utilizado em conjunto com mecanismos de indexação e classificação, facilitando as consultas e o compartilhamento deste conhecimento.
C) Apesar de a documentação ser reduzida e ultrapassada, a equipe a usa como fonte de conhecimento para extrair o contexto do domínio e reduzir a comunicação direta.
Durante o processo de entrevistas foi observado que os entrevistados fizeram algumas críticas sobre a documentação disponível por ser insuficiente e muitas vezes desatualizada.
Em um primeiro momento, poderíamos imaginar que os respondentes não utilizavam a documentação por considerar- la ultrapassada e inadequada.
No entanto, apesar de esta situação, os entrevistados utilizam a documentação.
Eles explicaram que, pelo menos, os documentos fornecem o contexto do conhecimento que procuram.
Eles também os utilizavam para reduzir o tempo de comunicação direta.
Isto é, quando eles precisavam obter algum conhecimento, se eles já haviam consultado os documentos, eles acreditavam que isso iria diminuir o tempo das conversações com os colegas e clientes.
A maioria dos entrevistados comentou que os documentos no início do projeto são atualizados, mas no decorrer de o projeto vão se tornando desatualizados.
Os documentos de requisitos são os mais citados entre os que estão desatualizados durante o projeto.
D) Problemas de comunicação com o cliente ao utilizar uma metodologia híbrida.
Muitos entrevistados comentaram sobre a dificuldade de contato com o cliente.
Quando eles consultavam os documentos com os requisitos e estes estavam desatualizados, eles precisavam então entrar em contato com o cliente que não estava sempre disponível.
A forma como os requisitos foram tratados neste projeto não se encaixa corretamente, nem com os princípios de metodologias ágeis, nem das metodologias tradicionais.
Se o projeto segue os princípios ágeis, o cliente deve estar mais acessível e disponível para toda a equipe.
Se ele segue os princípios de metodologias tradicionais, então, existe a exigência dos documentos serem atualizados durante todo o projeto.
Sob o enfoque da gestão do conhecimento, quando se adota uma metodologia ágil, a escolha é por o aumento de comunicação entre a equipe em si e entre esta e o cliente.
Quando se adota uma abordagem tradicional, a escolha é por priorizar o conhecimento explícito, dando mais ênfase ao uso de artefatos e documentos, que devem ser atualizadas ao longo de o ciclo de vida do projeto.
Portanto, quando se utiliza uma metodologia híbrida, deve- se tomar cuidado para evitar que estas questões de comunicação e documentação acabem não sendo atendidas do ponto de vista das metodologias ágeis, nem das metodologias tradicionais, como foi o caso dos requisitos neste projeto.
Há uma falta de definição do que é uma metodologia híbrida.
Qumer e Henderson-Sellers apresentam um framework para tentar determinar o grau de agilidade de algumas metodologias.
A fronteira entre ser ágil e não ser híbrido, e ser tradicional e não ser híbrido é muito tênue e merece um melhor debate na comunidade.
Entre alguns pontos elencados na conclusão do artigo que retratou o estudo de caso, destacamos:
&quot;Em este estudo de caso e em trabalhos correlatos percebemos a existência de uma lacuna na gestão do conhecimento para metodologias ágeis.
A gestão do conhecimento em projetos tradicionais é focada na utilização do conhecimento explícito e é uma tarefa complexa.
Mas em projetos ágeis, a gestão do conhecimento agrega ainda outros componentes, pois o conhecimento na forma tácita envolve muitos aspectos subjetivos».
De o ponto de vista da pesquisa aqui proposta, este estudo de caso exploratório serviu adequadamente para o reconhecimento inicial do problema, onde também foram estudados conceitos relevantes à formulação da metodologia proposta, entre os quais, conceitos de gestão do conhecimento associado às metodologias ágeis, por a preferência no uso do conhecimento em seu formato tácito e por o incremento da comunicação entre os stakeholders.
Os resultados detalhados do estudo de caso foram apresentados em artigo na 11 th International Conference ­ Revisão sistemática sobre gestão do conhecimento em projetos de software ágeis Com o objetivo de sumarizar os principais aspectos envolvidos na gestão do conhecimento em projetos de desenvolvimento que seguem metodologias ágeis e identificar possíveis lacunas na área, foi proposta a execução de uma revisão sistemática.
A motivação para a realização da revisão surgiu de estudos preliminares na literatura, onde não foram encontrados trabalhos que sumarizassem, no contexto da engenharia de software, os principais aspectos que emergem da gestão do conhecimento quando o uso do conhecimento explícito é preterido ao uso do conhecimento tácito, situação comum em metodologias ágeis.
De acordo com Kitchenham, uma revisão sistemática permite identificar, avaliar e interpretar toda a literatura relevante para uma determinada questão de pesquisa, tópico ou fenômeno de interesse.
Entre as razões para se realizar uma revisão sistemática, citam- se:
A sumarização dos benefícios ou limitações de determinado tratamento ou tecnologia;
A identificação de lacunas na pesquisa corrente, o que possibilita a sugestão de novas áreas para investigação futura;
E a aquisição de conhecimento sobre determinada área ou assunto, permitindo o correto posicionamento de novas atividades de pesquisa.
Em Dyba e Dingsøyr é apresentado um trabalho sobre revisões sistemáticas em engenharia de software, onde são destacados aspectos que evidenciam a relevância deste tipo de pesquisa para a área de Ti.
Estes autores também apresentam, adaptado do trabalho de Kitchenham, os estágios do processo de uma revisão sistemática, que são:
A seguir apresentaremos as principais etapas do processo da elaboração da revisão proposta, de acordo com Brereton et al.,
Kitchenham e Dyba e Dingsøyr.
Em esta fase da revisão é definido um protocolo que descreve, entre outras coisas, o propósito da revisão e os procedimentos que serão adotados na sua execução.
A definição de um protocolo garante também a replicação da revisão sistemática.
Um fator importante da revisão sistemática proposta é que ela tomou como base outras duas revisões já publicadas que serviram de partida para os nossos estudos.
Uma de elas é uma revisão sobre gestão do conhecimento em engenharia de software.
Em esta revisão o contexto de engenharia de software é genérico, não focando, por exemplo, em metodologias tradicionais ou ágeis.
A outra revisão sistemática é sobre estudos empíricos de desenvolvimento de software ágil.
Em esta revisão não são focados aspectos de gestão do conhecimento aplicados a este tipo de metodologia de desenvolvimento de software.
Diante de os estudos preliminares que já haviam sido realizados, como o estudo de caso exploratório e, diante de o quadro em que as duas revisões sistemáticas mais próximas do nosso trabalho não abordam o foco de nossa pesquisa, emergiu então a necessidade de realizarmos uma revisão sistemática que apontaria justamente na intersecção entre gestão do conhecimento em engenharia de software e projetos de desenvolvimento de software ágeis.
Esta revisão serviu para identificar, avaliar e interpretar em toda a literatura relevante uma base teórica para os próximos passos da nossa metodologia.
A nossa revisão sistemática tinha como objetivo fornecer uma visão geral de estudo dentro de a gestão do conhecimento em projetos de software ágeis, que tipos de conceitos têm sido explorados, quais são os principais resultados encontrados e os métodos de pesquisa utilizados.
Mais especificamente nós procuramos responder as seguintes questões de pesquisa:
Quais são os principais conceitos em gestão do conhecimento que têm sido investigados em projetos de software ágeis?
Quais são os principais resultados encontrados em pesquisas sobre gestão do conhecimento em projetos de software ágeis?
A o responder estas duas questões também estaremos verificando se existe uma área que surge da intersecção da gestão do conhecimento e projetos de software ágeis e se há contribuição suficiente para caracterizar- la como uma área de pesquisa.
O protocolo definido para esta revisão sistemática identificou então:
A questão de pesquisa (planejamento da revisão);
a estratégia de pesquisa e fonte de dados, critérios de inclusão e exclusão, seleção dos estudos primários, critérios de qualidade e métodos de síntese (condução da revisão) e;
os resultados, as discussões e as conclusões (relatório da revisão).
Os tópicos &quot;condução da revisão «e &quot;relatório da revisão «serão detalhados nas seções a seguir.
Em a condução da revisão as etapas definidas no protocolo são executadas, sendo que algumas etapas são realizadas em seqüência e outras puderam ser executadas simultaneamente.
Algumas destas etapas foram realizadas por um dos autores da revisão, sendo que outras, por uma questão de exigência de qualidade do método, tiveram de ter a participação de dois dos autores da revisão sistemática.
OR (&quot;software development&quot;) And (&quot;knowledge management&quot;) OR (&quot;tacit knowledge&quot;) OR (&quot;explicit knowledge&quot;) OR (&quot;knowledge creation&quot;) OR (&quot;knowledge acquisition&quot;) OR (&quot;knowledge sharing&quot;) OR (&quot;knowledge retention&quot;) OR (&quot;knowledge evaluation&quot;) OR (&quot;knowledge use&quot;) OR (&quot;knowledge application&quot;) OR (&quot;Organization knowledge&quot;) OR (&quot;knowledge engineering&quot;) And (&quot;agile software&quot;) OR (&quot;agile methodologies&quot;) OR (&quot;agile methods&quot;) OR (&quot;agile process&quot;) OR (&quot;extreme programming&quot;) OR (xp software&quot;) OR (&quot;scrum software&quot;) OR (&quot;crystal software&quot;) OR (&quot;dsdm software&quot;) OR (&quot;fdd software&quot;) OR (feature driven development software) OR (lean software development) Simultaneamente ao processo de elaboração das strings foram selecionadas as fontes para digitais selecionados foram:&amp;&amp;&amp;
Scopus Science@ Direct Para definirmos um critério de inclusão e exclusão dos trabalhos a serem selecionados, foi busca nos mecanismos de pesquisa definidos.
A revisão sistemática incluiu artigos publicados do ano 2000 em diante, sendo que a pesquisa foi realizada em Setembro de 2009.
Os níveis definidos para inclusão e exclusão dos artigos na revisão foram elaborados para serem aplicados em todos os artigos que foram retornados nos mecanismos de busca selecionados, como apresentado na Tabela 2.
O foco do artigo é gestão do conhecimento em metodologias ágeis.
O artigo trata sobre gestão do conhecimento em metodologias ágeis, mas de forma periférica.
Não é o foco, mas é tratada de forma secundária no artigo.
O artigo é sobre gestão do conhecimento ou sobre métodos ágeis.
O artigo foca sobre um destes tópicos, mas o outro aparece no contexto do artigo.
O artigo é sobre gestão do conhecimento ou sobre métodos ágeis.
O foco do artigo não é gestão do conhecimento, nem métodos ágeis.
Para a seleção dos estudos primários, foi definido um processo que iniciou com a aplicação dos artigos de onde foram retiradas as sínteses e as contribuições para a revisão sistemática.
A Tabela 3 mostra o processo da seleção dos estudos primários e o número de artigos identificados em cada estágio.
Nosso trabalho ancorou os aspectos relativos à qualidade nos seguintes pontos:
Para classificar os artigos encontrados por os mecanismos de busca, os autores utilizaram o níveis de 1 a 5 conforme Tabela 2.
O uso destes níveis de classificação assegurou que as contribuições usadas eram centradas no foco da pesquisa proposta.
Os artigos foram classificados por os autores independentemente e quando as classificações eram diferentes, era feita uma discussão para chegar a um consenso;
Também foi elaborado um instrumento para avaliar os estudos sem resultados empíricos.
Não separamos os trabalhos com evidências de resultados empíricos dos que só apresentavam Lições aprendidas, pois manter na revisão sistemática somente trabalhos com resultados empíricos reduziria muito a quantidade de artigos a serem utilizados para sintetizar as descobertas e conclusões.
Esta síntese é um resumo dos principais conceitos encontrados nos trabalhos selecionados que foram lidos na íntegra.
Estes tópicos foram extraídos por fazerem parte do foco principal dos artigos selecionados, ou mesmo não sendo o tópico principal do artigo, terem sido amplamente abordados no decorrer de o mesmo.
Para a elaboração da síntese dos estudos desta revisão sistemática foi usado o método de meta-- ethnography apresentado por Noblit e Hare.
Também foram consideradas algumas sugestões propostas de técnicas de ethnography e análise qualitativa de dados encontradas em Para realizar os passos do método de meta-- ethnography, os autores fizeram uma leitura completa dos artigos usando a ferramenta Mendeley, onde foram registrados os principais conceitos e definições identificados em cada artigo.
Após, os autores criaram um mapa conceitual para cada artigo lido.
Depois disso, foram criados mapas conceituais para co-relacionar todos os conceitos dos artigos lidos.
Esta síntese foi extraída dos mapas resultantes.
&quot;G. Melnik and F. Maurer, «Direct verbal communication as a catalyst of agile knowledge sharing, «&quot;H. Holz and F. Maurer, Knowledge management for distributed agile software processes, «&quot;H. Holz and J. Schafer, «Collaborative, task-- specific information delivery for agile processes, «Enabling Technologies:
Infrastructure for Collaborative Enterprises, 2003.
WET ICE 2003.
Proceedings. Twelfth IEEE International Workshops on, 2003.
&quot;T. Dingsøyr and G. K. Hanssen, «Extending agile methods:
Postmortem reviews as extended A. R. Yanzer Cabral, M. Blois Ribeiro, A. P. Lemke, M. T. Silva, M. Cristal, and &quot;C. Franco, «A case ICEIS 2009, Milan, Italy:
2009, pp. 627-638.
&quot;G. Canfora , A. Cimitile , F. Garcia , M. Piattini , and C. A. Visaggio , Confirming the influence. B. Ramesh, P. Cao, L. Mohan, and «K. Xu, Can distributed software development be agile?,»
the theoretical base for agile methods, «Euromicro Conference, 2003.
Proceedings. 29 th, 2003.
&quot;J. K. Kokkoniemi, Gathering Experience Knowledge from Iterative Software Development, 2008, pp. 333-333.
Technology , IEEE , 2005 , pp. 75-87. Framework, «Decision Support Systems, vol. 46, 2009, pp. 803-814.
&quot;M. Kuniavsky and S. Raghavan, «Guidelines are a tool:
Building a design knowledge management eXperience, San Francisco, California:
2005, p. 8.
&quot;M. Tosic , V. Milicevic , and M. Stankovic , Collaborative Knowledge Acquisition for Agile Project 2005 , pp. 1081-1084. Bioinformatics), vol. 3075, 2004, pp. 173-183.
Software Engineering, vol. 30, 2005.
Development Conference, 2004, pp. 2-11.
G. Salazar-- Torres, E. Colombo, F. S. Correa Da Silva, C. A. Noriega, and &quot;S. Bandini, Design issues for knowledge artifacts, «Knowledge--Based Systems, vol. 21, 2008, pp. 856-867.
Com isso encerramos a parte da condução da revisão, sintetizando os resultados resultantes das leituras dos estudos primários selecionados de acordo com os critérios estabelecidos no protocolo.
Esta seção irá apresentar as discussões, contribuições e as conclusões extraídas da revisão sistemática.
A seção &quot;Resultados «da revisão será apresentada no Apêndice B, pois ela é bastante extensa e é um detalhamento de cada tópico que já foi sumarizado na Tabela 4.
De esta sumarização que foi detalhada na seção &quot;Resultados», surgiram as discussões sobre os principais tópicos elencados da área, que são apresentadas na próxima seção.
Em esta seção irão ser apresentadas as discussões que emergiram da análise dos resultados observados nas seções anteriores e também iremos apontar as questões em aberto oriundas destas discussões.
Estas discussões irão focar em como identificar e tratar os principais fatores que influenciam e são influenciados por a preferência do uso do conhecimento tácito, ao invés de o conhecimento explícito, em projetos ágeis.
A Figura 4 ilustra os tópicos a serem discutidos nesta seção. --
Meios de comunicação: Podemos observar na Figura 5 que o fator &quot;Meios de comunicação «é derivado de todos os principais temas mencionados na síntese dos resultados (Tabela 4).
No entanto, dos dez artigos que envolvem discussões sobre os meios de comunicação, seis de eles se concentram em &quot;Problemas e aspectos específicos de documentação em métodos ágeis «e &quot;Características que envolvem a transferência e colaboração do conhecimento tácito durante o desenvolvimento de software».
Portanto, problemas envolvendo a documentação em projetos ágeis e a manipulação de conhecimento tácito no desenvolvimento de software, estão fortemente associados com os meios de comunicação.
É também significativo que dos cinco artigos que abordam o uso de ferramentas de gestão do conhecimento em projetos ágeis, quatro de eles envolvem discussões sobre os meios de comunicação.
Consequentemente, as ferramentas que estão sendo desenvolvidas para apoiar a gestão do conhecimento em projetos ágeis estão focadas nos aspectos de comunicação.
A comunicação em projetos tradicionais prioriza o uso de conhecimento explícito.
Sobre projetos ágeis, a prioridade é a comunicação direta dentro de a equipe e entre a equipe e clientes, e essa comunicação deve ser realizada de uma maneira mais direta (face a face), priorizando a conversa ao invés de outras formas de comunicação.
A questão em aberto apresentada para este tópico está focada em como uma metodologia de gestão do conhecimento deve lidar com o volume de informação gerado por a forma prioritária de comunicação em projetos ágeis, que é a conversação.
Um exemplo de como resolver este problema é o uso de técnicas e ferramentas que gerenciam as conversações entre os envolvidos no projeto.
Isto pode ser feito por a adoção de comunidades de prática, que podem melhorar o compartilhamento do conhecimento através da comunicação direta entre os participantes.
Também podemos fazer uso de ferramentas que auxiliam a gestão do conhecimento tácito.
Poderia- se- utilizar ferramentas para auxiliar no armazenamento e indexação das discussões entre a equipe e extrair algum conhecimento a partir de as conversações de uma forma (semi) automática.
Uma proposição para esta questão em aberto é o uso de mecanismos de gestão do conhecimento em projetos ágeis com base em sistemas de reconhecimento automático de voz,.
Um exemplo de um projeto como este, que já mostrou resultados é o Calo. --
Fatores humanos e sociais:
Em artigos que abordaram &quot;A adoção de uma metodologia de gestão do conhecimento em projetos ágeis «e &quot;Artefatos de conhecimento e conhecimento de experiência e como estes são usados no processo de desenvolvimento de software», fatores humanos e sociais não foram discutidos.
Em os documentos que tratam de problemas de documentação, este tópico foi abordado de forma superficial e ocorre quando o artigo discute a questão do aumento da comunicação face a face.
Além de os artigos que focaram na questão da &quot;influência dos fatores humanos e sociais nas equipes e entre a equipe e os clientes», este é também um tema de grande discussão em trabalhos sobre comunidades de prática, onde foi abordado em dois dos três artigos (Figura 6).
É também uma questão muito discutida em artigos sobre &quot;as características que envolvem a transferência de conhecimento tácito e colaboração durante o desenvolvimento do software», onde a maioria (3 de 5) dos artigos apontam as discussões sobre questões relacionadas a fatores humanos e sociais.
Por este motivo, trabalhos que envolvem a manipulação de conhecimento tácito tendem a discutir os fatores humanos e sociais e também envolvem discussões sobre a comunicação face a face.
Fatores humanos e sociais tem sido uma questão muito discutida por um longo tempo na gestão do conhecimento e na engenharia de software.
Podemos ver nestes artigos que este tópico reabriu novas discussões e ganhou novos contornos com o surgimento das metodologias ágeis.
A interação entre equipe, clientes e funcionários é alterada em projetos ágeis, porque eles precisam interagir com mais frequência e mais diretamente.
Portanto, em projetos ágeis, para o conhecimento ser tratado de forma mais eficaz, é necessário que os envolvidos sejam preparados para interagir e colaborar na geração, compartilhamento e aquisição do conhecimento.
A questão em aberto neste tópico discute como os fatores humanos e sociais são tratados quando um projeto de desenvolvimento de software segue as práticas ágeis.
Fatores como a empatia, educação, cultura, nível educacional, entre outros, podem influenciar o nível de integração do grupo e, portanto, interferem no canal de comunicação em que o conhecimento tácito flui.
Quanto melhor esses fatores forem tratados no grupo, melhor o canal de comunicação será.
Se este tipo de comunicação (direta) ocorre num ambiente harmonioso, a comunicação direta acaba sendo mais eficaz do que a comunicação mediada (mais comum em projetos tradicionais).
Por outro lado, se muitos problemas de cunho humano e sociais ocorrem, a comunicação direta é altamente prejudicada.
Devido a a importância dos fatores humanos e sociais em projetos ágeis, propomos o uso, na formação de equipes para trabalhar com projetos ágeis, de uma avaliação do perfil da equipe que deve incluir aspectos que influenciam na gestão do conhecimento.
Esta situação pode ser trabalhos os autores incluíram uma avaliação para verificar se determinadas personalidades e características pessoais emergem em diferentes papéis a serem desempenhados num projeto.
A sugestão é então estender a estas avaliações, um conjunto de características interpessoais que influenciam no desempenho de atividades relacionadas à gestão do conhecimento, principalmente focando na criação, retenção e transferência do conhecimento através da comunicação direta. --
Ciclo de desenvolvimento de software e artefatos:
O tópico &quot;ciclo de desenvolvimento de software e artefatos «foi derivado de todos os tópicos, exceto de &quot;ferramentas para gestão do conhecimento no contexto de projetos ágeis», sendo desta forma o tópico com o maior número de artigos analisados.
Como mencionado anteriormente, os artigos que abordaram ferramentas de gestão do conhecimento oferecem discussões mais focadas sobre as formas de comunicação.
No entanto, todos os trabalhos que discutem a &quot;adoção de uma metodologia de gestão do conhecimento em projetos ágeis «sugerem mudanças e contribuições para o ciclo de desenvolvimento de software e artefatos.
Por exemplo, nos autores sugeriram associar atividades e perspectivas de um modelo de processo de desenvolvimento ágil de software para as cinco etapas da criação do conhecimento organizacional propostas por Nonaka e Takeuchi.
Os autores sugerem um redesenho do ciclo de desenvolvimento de software de acordo com as fases de uma metodologia para gestão do conhecimento.
A o comparar os métodos tradicionais de desenvolvimento de software com métodos ágeis, existem muitas diferenças no que diz respeito ao ciclo de desenvolvimento de software, que são amplamente discutidas na literatura.
Podemos observar que muitas dessas diferenças derivam do fato de que as metodologias ágeis priorizam o uso do conhecimento tácito.
Este paradigma acaba influenciando o ciclo de desenvolvimento de software.
Como exemplo, temos a metodologia Scrum, que promove reuniões diárias com a equipe, de modo que o conhecimento é compartilhado diretamente entre a equipe.
Em métodos ágeis, reuniões diárias e outras atividades de interação entre as partes interessadas são comuns.
Conhecimento é potencialmente armazenado na memória das pessoas e estas reuniões de trabalho servem para dar um refresh na memória.
Como a maioria do conhecimento não é explícita, as reuniões freqüentes entre outras atividades servem para atualizar o conhecimento tácito.
Em abordagens tradicionais há outras formas priorizadas de compartilhar conhecimento entre os membros da equipe usando artefatos (conhecimento explícito).
Portanto, a forma como o conhecimento flui no projeto influência o ciclo de desenvolvimento de software.
Quanto mais conhecimento é exteriorizado durante o projeto, mais artefatos são incluídos no ciclo de vida (e as atividades correspondentes para criar- los e manter- los).
A questão em aberto envolve a influência da gestão do conhecimento no ciclo de desenvolvimento de software.
Atividades e artefatos serão inseridos ou removidos no ciclo de desenvolvimento de acordo com o conhecimento que deve ser explicitado durante o projeto.
Assim, a definição de um ciclo de desenvolvimento de software deve estar diretamente ligada à concepção de gestão do conhecimento para o projeto.
A maneira como o conhecimento é gerado, armazenado, validado, distribuído, apresentado e usado, influência diretamente na definição do ciclo de desenvolvimento de software e de seus artefatos.
Nossa proposta para esta questão é a incorporação de artefatos de conhecimento para o ciclo desenvolvimento de software.
Isto proporciona um tratamento mais direto do conhecimento em sua forma tácita.
Esses artefatos podem ser baseados em formas de representação do conhecimento, tais como áudio, vídeo e arquivos de imagem, transcrições automáticas, etc..
Em o intuito de evitar que este tipo de conhecimento seja perdido, situação comum em empresas que trabalham com grandes volumes de informação, deve haver mecanismos de indexação desse conhecimento.
Esses mecanismos de indexação podem ser baseados numa representação formal do conhecimento que é computacionalmente processável, como por exemplo, as ontologias. --
Ferramentas para a gestão do conhecimento:
O tópico que discute ferramentas para gestão do conhecimento deriva claramente de todos os artigos de &quot;ferramentas utilizadas em alguma fase da gestão do conhecimento no contexto projetos ágeis».
Ele também deriva de alguns trabalhos que abordaram &quot;problemas e aspectos específicos de documentação em métodos ágeis «e &quot;características que envolvem a transferência e colaboração do conhecimento tácito durante o desenvolvimento de software».
Com exceção do artigo de Kuniavsky e Raghavan, todos os outros artigos que tratam de ferramentas de gestão do conhecimento também lidam com os meios de comunicação.
Portanto, isso reforça que as ferramentas discutidas nos artigos que constituem os estudos primários desta revisão estão fortemente associadas com meios de comunicação em projetos ágeis.
Em projetos que seguem metodologias tradicionais, as ferramentas de gestão do conhecimento são mais focadas em situações que lidam com o conhecimento em formato explícito.
Por outro lado, em projetos ágeis as ferramentas devem estar preparadas para oferecer alternativas eficazes para gerir amplamente o conhecimento de forma tácita.
Não podemos ignorar o fato de que as ferramentas que trabalham com o conhecimento explícito fazem uso de uma fase de explicitação do conhecimento que cria um processo intermediário em que a &quot;qualidade «da pessoa que faz este processo vai acabar influenciando na qualidade da aquisição e da representação do conhecimento.
A aquisição de conhecimento tácito é geralmente feita de uma forma mais direta.
No entanto, temos vários outros problemas associados com a gestão do conhecimento na forma tácita, como indexação, recuperação, compartilhamento, etc..
A questão em aberto neste tópico refere- se a como as ferramentas de gestão de conhecimento para projetos ágeis devem manter as características comuns usadas para projetos tradicionais, que manipulam os artefatos baseados em conhecimento explícito, e o que devem fazer a fim de incorporar funcionalidades para o gerenciamento de outras formas de representação do conhecimento.
Para abordar esta questão em aberto, enfatizamos a necessidade de aprofundar estudos para verificar a viabilidade tecnológica de ferramentas para lidar com o conhecimento tácito.
Considerando o avanço em áreas como reconhecimento fala e imagem, processamento da linguagem natural, ontologias, entre outros.
Seria então, viável o uso de ferramentas baseadas nessas tecnologias para a gestão do conhecimento tácito em projetos de desenvolvimento de software?
É inviável o uso destes tipos de ferramentas devido a questões tecnológicas ou devido a sua ineficiência na adaptação aos processos de desenvolvimento de software?
Estas são questões que podem ser aprofundadas no uso de ferramentas para apoio à gestão do conhecimento em projetos ágeis. --
Formas de representação do conhecimento:
Artigos que apresentam implicações para &quot;formas de representação do conhecimento «estão fortemente associados com o uso de artefatos de conhecimento e conhecimento de experiência.
Mas eles também derivam, mais superficialmente, de discussões sobre documentação, colaboração e transferência de conhecimentos na forma tácita.
Como apresentado em Torres- Salazar et al.
Existe uma preocupação em propor formas alternativas de representação do conhecimento em projetos ágeis, devido a o maior uso do conhecimento tácito.
A questão em aberto para discussão neste tópico diz respeito a como o conhecimento deve ser representado nas diversas fases e etapas do ciclo de desenvolvimento de software de projetos ágeis, de forma que possa ser mais bem gerenciado.
Se o conhecimento deve ficar somente concentrado, ou retido, nas pessoas, ou se é interessante tentar capturar o máximo possível deste conhecimento, mesmo que na forma tácita.
Poderíamos dar como um exemplo de proposição para este tópico, a utilização de formas de representação do conhecimento adequadas ao conhecimento tácito, como as gravações em áudio dos diálogos entre as pessoas ou os vídeos das conversações ou da execução das tarefas explanando e demonstrando como realizar- las.
Também podem ser usadas formas de representação do conhecimento onde o mesmo foi explicitado, mas que procuram manter aspectos mais comuns ao conhecimento tácito (difícil de ser representado em artefatos tradicionais) tais como mapas conceituais e mapas mentais.
Novamente surge a questão da importância de conseguir indexar este conhecimento para que possa ser recuperado e distribuído de forma eficiente.
Como resultado das discussões realizadas ao longo de a análise desses mapas conceituais, as nossas principais conclusões são:
Levando em conta estas considerações, destaca- se como oportunidade de pesquisa o uso de ferramentas de gestão do conhecimento com implicações diretas no processo de desenvolvimento de software, e não apenas focadas em ferramentas de comunicação e colaboração.
O foco dessas ferramentas deve ser em minimizar o impacto da aplicação de técnicas de gestão do conhecimento.
Por exemplo, elas podem manipular o conhecimento em formatos alternativos (como áudio, vídeo, experiência, etc) no ciclo de desenvolvimento de software, o que implica no uso de ferramentas que manipulam o conhecimento prioritariamente num formato mais próximo a o tácito.
A motivação e a justificativa para a realização desta revisão emergiram após:
Estudos preliminares na literatura, onde não foram encontrados trabalhos que sumarizassem, no contexto da engenharia de software, os principais aspectos que emergem da gestão do conhecimento quando o uso do conhecimento explícito é preterido ao uso do conhecimento tácito, situação comum em metodologias ágeis e;
Após o estudo de caso exploratório que apontou para a existência de uma lacuna na gestão do conhecimento para metodologias ágeis e traz à tona problemas de comunicação neste tipo de projeto sugerindo, entre outros pontos, um mecanismo (semi) automático para aquisição e compartilhamento de conhecimento, baseado por exemplo, no uso de aplicações de reconhecimento automático de fala, confirmando a necessidade de uma revisão aprofundada sobre o tema.
Percebemos desta forma, que a revisão sistemática alcançou seus objetivos no contexto desta proposta de tese, ajudando a responder através do planejamento formal, da execução metódica e do alto rigor científico, que influências são percebidas na gestão do conhecimento quando o conhecimento que circula no projeto é trabalhado preferencialmente em seu formato tácito e quando o processo de explicitação do conhecimento é preterido por o incremento da comunicação direta entre os membros da equipe.
A revisão sistemática apontou que existe uma grande contribuição desta área de pesquisa para a comunidade, pois aspectos que já vem sendo tratados por a academia e profissionais, como por exemplo:
Uso de artefatos;
A aquisição, retenção e compartilhamento do conhecimento;
As questões em aberto apresentadas na revisão sistemática e, as proposições de como estas podem ser abordadas, subsidiariam a elaboração da metodologia proposta apresentada neste trabalho.
Estudo de viabilidade O intuito deste estudo foi realizar um conjunto de ensaios para verificar a viabilidade da aquisição de conhecimento a partir de os diálogos de reuniões de projetos.
Este conhecimento adquirido servirá de base para criação de um formato que possa ser utilizado como um indexador das reuniões, em conjunto com uma ontologia do domínio.
A aquisição do conhecimento foi feita por a análise do áudio da gravação das reuniões para extração dos conceitos relevantes.
Consequentemente, um dos focos deste estudo de viabilidade foi analisarmos a relevância, ou qualidade, dos conceitos que são extraídos das reuniões de projeto.
Os ensaios utilizados para avaliar a qualidade dos conceitos, onde consideramos a capacidade dos mesmos para indexar o conhecimento capturado das transcrições das reuniões, foram realizados de três formas:
A Figura 10 apresenta a estrutura do estudo de viabilidade de forma a melhorar a compreensão de suas fases, ferramentas e produtos gerados no decorrer de a execução de suas etapas.
Speaking Speech Recognition6, que é o principal produto deste segmento para fazer as transcrições das falas das reuniões7.
Cada um dos participantes teve de gerar um perfil de usuário no software, sendo que essa tarefa serve para criar um padrão de reconhecimento de fala para cada usuário.
Para cada reunião foram gerados dois arquivos texto com as transcrições, conforme as duas formas descritas no passo 2: Cada participante realizava as suas falas no decorrer de a reunião;
Cada participante gravava a mesma reunião fazendo a fala de todos os participantes.
WER com a fala de todos significa que a reunião foi gravada com cada participante realizando a sua própria fala.
Essa reunião foi transcrita e o WER aponta o percentual de palavras que foram transcritas erroneamente, baseado na comparação com o texto que foi lido por os participantes na reunião.
WER com a fala de um participante significa que a reunião foi gravada com um participante realizando a fala de todos os &quot;personagens «da reunião.
Essas duas medições foram feitas para comparar situações que depreciam o desempenho dos softwares de reconhecimento de fala, que é interpretar a fala de uma única pessoa, ou mais de uma pessoa ao mesmo tempo.
Podemos observar então que na média, quando os três participantes da reunião realizaram as suas falas o WER foi de 12,9%, sendo que quando somente um dos participantes realizou a reunião, lendo a fala de todos, o WER caiu para 8,0%.
Evidenciando desta forma, que ao analisar o áudio com a fala de mais de um participante, o desempenho do software de reconhecimento automático de fala é degradado.
Noun singular or mass, NNS Noun plural, NNP Proper noun singular, NNPS Proper noun plural.
Também foram feitos testes com sumarizadores, sendo que a saída destes normalmente são frases.
Alguns até apontam os principais conceitos além de as frases, como o Copernic9.
Mas como o objetivo é extrair os conceitos das falas das reuniões, um etiquetador é mais adequado que um sumarizador, que também apresenta problema em ser aplicado em textos transcritos que não possuem pontuação, dificultando o entendimento de estrutura das frases e parágrafos.
Comparar os conceitos da ontologia do domínio com os da transcrição:
Neste passo o objetivo foi comparar os conceitos extraídos das transcrições das reuniões, com os da ontologia do domínio do projeto que serviu para o nosso estudo de caso exploratório.
Para auxiliar nesta comparação foi utilizada uma aplicação (ONTrace) desenvolvida num trabalho realizado junto ao ISEG na PUCRS.
Esta aplicação possui um de seus módulos (matcher), que resumidamente, recebe uma lista de termos em formato texto e verifica a similaridade destes com os conceitos da ontologia.
É utilizado um algoritmo de stemmer para realizar esta verificação.
A comparação consistiu então, em pegar todos os conceitos que foram extraídos das transcrições das reuniões e verificar se estes conceitos ocorriam na ontologia do domínio.
Os resultados obtidos e os detalhes deste ensaio são apresentados na seção 4.3.2.
8. Survey para avaliar os conceitos gerados automaticamente:
Esta etapa do estudo de viabilidade consistiu em submetermos à avaliação de humanos os conceitos que foram extraídos das reuniões do estudo de caso exploratório e que já foram comparados à ontologia do domínio no passo 7.
Esta avaliação consistiu em comparar os &quot;conceitos relevantes «identificados de forma automática numa reunião, através do uso da nossa metodologia proposta, com os &quot;conceitos relevantes «da reunião que foram apontados através da percepção humana.
A partir de uma lista de termos, que são os conceitos da ontologia do domínio, os voluntários analisaram as reuniões e apontaram os termos relevantes para os tópicos tratados em cada reunião.
O ensaio foi composto por os seguintes passos:
Analisar o glossário fornecido para um melhor entendimento das siglas e termos específicos do domínio que foi tratado nas reuniões;
Analisar cada uma das reuniões realizadas sobre o projeto (três reuniões) através da leitura completa de todas as reuniões e;
Apontar na planilha de avaliação os termos relevantes de cada reunião.
Em o apêndice C é apresentado o instrumento utilizado neste survey, com mais detalhes sobre a aplicação do mesmo.
Participaram do survey oito professores de cursos superiores da área de computação e informática, que preencheram individualmente seus formulários.
O autor desta tese reuniu- se com cada um dos participantes, antes da aplicação do formulário, para a explicação do contexto do estudo de viabilidade.
Os resultados das comparações propostas e do survey apresentados nos passos 6, 7 e 8 serão detalhadas nas próximas seções.
Em esta seção são apresentadas as análises das reuniões realizadas no estudo de viabilidade, no intuito de verificar a margem de acerto da transcrição textual do áudio quando comparado ao texto em o qual foi baseada a gravação da reunião.
Em o Apêndice D são apresentadas as informações de cada uma das reuniões realizadas e os dados coletados durante os ensaios.
Em as próximas seções, estes dados serão comparados e analisados de forma a sintetizar os resultados destes ensaios.
Em esta seção faremos algumas análises em função de os resultados obtidos com os experimentos, comparando os dados das reuniões (Apêndice D) e apontando conclusões e questões em aberto, que foram posteriormente testadas em experimentos da Etapa 5 do método de pesquisa proposto, onde é realizada uma validação através de um estudo de caso explanatório.
Em o intuito de sistematizar as análises derivadas dos resultados das reuniões apresentados na Tabela 6, elencaram- se alguns tópicos relevantes que serão detalhados, a seguir:
WER (Word Error Rate) significa a taxa de erro de palavras transcritas.
Se consultarmos a literatura sobre reconhecimento automático de fala, observaremos que são informados vários índices sobre o desempenho destes softwares, e o principal índice é o WER.
Eles irão variar de acordo como o tamanho do dicionário, condições acústicas, língua e estilo da fala, treinamento realizado com o usuário, entrada de áudio composta por a voz de uma única pessoa, ou não, entre outros.
Alguns produtos atestam índices próximos a 99% de reconhecimento, em condições ditas ideais10.
Em nossos ensaios obtivemos os índices de WER apontados na Tabela 6, que dizem respeito a transcrição de um áudio composto com a voz de mais de um participante e com a voz de somente um participante.
Quando o áudio é composto por somente uma voz este índice melhora substancialmente, pois a taxa cai de 12,9% para 7,9%.
Percebemos que estes índices podem melhorar, pois foram usados muitos substantivos próprios como siglas e termos do domínio do negócio, assim como termos comuns à área de engenharia de software.
O software pode ser treinado para que reconheça palavras que não fazem parte do seu dicionário.
Também pode ser melhorado o treinamento do perfil do condutor/ mediador das reuniões, que normalmente é a pessoa que mais fala e acaba sumarizando os assuntos tratados, de acordo com o protocolo sugerido.
Mas consideramos bastante aceitável o índice encontrado, baseado em contextos similares;
O que chamamos de &quot;Porcentagem de acerto «é a relação entre os conceitos que foram extraídos por o tagger do texto original da fala, comparados aos conceitos extraídos por o tagger na transcrição das falas.
O item frequência diz respeito ao número de vezes em que o conceito aparece no texto original.
Como pode ser observado não consideramos a frequência $= 1, pois sem um melhor refinamento e melhoria do WER e sem a utilização de um processo de exclusão de stop words e utilização de um algoritmo de stemmer11, acabam sendo incluídas muitas palavras que podemos considerar &quot;lixo».
A &quot;Frequência\&gt; $= 3 «nos ensaios realizados apresenta- se como um balizador, pois da &quot;Frequência 2&gt; «para esta, temos um salto significativo e, depois desta, os valores se mantém, ou até reduzem, como pode ser observado na Tabela 6.
Atribuímos este comportamento, ao fato que, com frequências baixas de ocorrência, muitas palavras que mencionamos no parágrafo anterior, categorizadas como &quot;lixo», acabam sendo incluídas.
Mas, na medida em que aumenta a frequência, essa possibilidade diminui.
Este também é outro ponto que mereceu nossa atenção nos ensaios definidos para a Etapa 5 do método de pesquisa (estudo de caso explanatório).
Durante o estudo de viabilidade foi elaborada uma ontologia do domínio do projeto que serviu para o estudo de caso exploratório.
Esta ontologia foi elaborada por uma pesquisadora com experiência neste tipo de atividade e que participou do estudo de caso exploratório da Etapa 1.
Ela utilizou o mesmo documento de onde foram retiradas informações para a elaboração dos roteiros das reuniões do estudo de viabilidade.
Este documento apresenta a visão geral do projeto, com quarenta e oito requisitos funcionais e quatro requisitos não funcionais da aplicação.
Este ensaio consistiu em analisar separadamente os conceitos extraídos das transcrições de cada reunião e verificar a similaridade destes, com os conceitos da ontologia.
O resultado deste ensaio é a indicação se os conceitos que estão na transcrição ocorrem na ontologia.
Para realizar o ensaio utilizamos a aplicação ONTrace, já comentada na seção 4.3.1, etapa 11 temos a definição de como é feita esta análise de similaridade por a ferramenta ONTrace.
Um dos propósitos desta ferramenta é estabelecer correlações entre os artefatos utilizados num processo de desenvolvimento de software através da ontologia do domínio.
Os resultados destes ensaios são apresentados na Tabela 7, onde a &quot;Frequência «é o número de vezes em que um conceito aparece no texto transcrito da conversação;
&quot;Conceitos Transcritos «é a quantidade de conceitos detectados na transcrição para determinada frequência e;
&quot;Identificados na ontologia «é a percentagem destes conceitos que foram encontrados similares na ontologia.
Este ensaio não foi realizado para a primeira reunião do estudo de viabilidade, somente para as três últimas, pois na primeira ainda não se tinha uma definição clara de qual domínio seria utilizado e, portanto, omitiram- se muitos aspectos do domínio do negócio no roteiro na reunião.
Analisando os resultados, podemos observar que na medida em que aumenta a frequência em que um conceito aparece na transcrição, também aumenta a probabilidade de haver um conceito similar na ontologia.
Como em todas as análises fizemos medições com a frequência até o valor 5, para uma análise mais aprofundada, foram realizados ensaios com frequências maiores no estudo de caso explanatório.
Desta forma, poderemos confirmar esta tendência, indicando que conceitos mais freqüentes nas reuniões são mais qualificados, sob o ponto de vista da correspondência com os conceitos da ontologia do domínio.
Após os oitos participantes do survey preencherem seus formulários, todos os dados foram tabulados e então foram realizadas as análises das informações.
A Tabela 8 apresenta um exemplo de como os dados de uma reunião foram tabulados.
A coluna&quot;% «apresenta o percentual de participantes que apontaram aquele termo como relevante para aquela reunião.
Depois de tabulados, estes dados foram comparados com os dados que foram obtidos das etapas anteriores deste estudo de viabilidade, onde os termos foram extraídos dos áudios das reuniões e comparados à ontologia do domínio.
Várias análises foram realizadas no intuito de observar o comportamento das informações derivadas do cruzamento dos dados da geração automática dos termos das reuniões, com o survey onde os participantes apontaram os termos relevantes associados às reuniões.
Podemos apontar como síntese destas observações, as informações apresentadas nas Tabela 9, Tabela 10 e Tabela na Tabela 9 pode- se observar uma das análises realizadas, que foi verificar os termos apontados por os humanos de acordo com o número de participantes que escolheram o termo e se este não foi detectado automaticamente.
Em a coluna &quot;100% (todos) estão os termos que todos os participantes do survey apontaram como um termo relevante para a reunião 1, neste caso somente «commodity&quot;;
Em a coluna «50-99% (maioria) estão os termos que no mínimo a metade dos participantes apontaram como relevantes e;
Em a coluna «1-49% (minoria) estão os termos que menos da metade dos participantes apontaram como relevantes.
Esta análise foi feita para as três reuniões utilizadas no survey.
Convém destacar que os termos gerados por a extração automática nesta análise considerou termos cuja frequência de ocorrência no texto foi igual a três, que foi escolhida por motivos já apontados nos ensaios relatados na seção 4.3.1.
Mas também foram feitos ensaios considerando outras frequências e as diferenças serão apontadas na seção de conclusões.
Termo ocultado por estar associado à identidade das empresas participantes Para esta reunião apontada na Tabela 9 podemos considerar o resultado satisfatório, dentro de a expectativa de que os termos apontados por os humanos tivessem sido gerados como termos relevantes também por o processo automático.
O termo apontado por todos participantes e todos os termos apontados por a &quot;maioria «também foram gerados automaticamente e somente 3 dos 8 termos apontados por a &quot;minoria «não foram gerados automaticamente.
Em o total, dos 18 termos apontados por os humanos, 3 não foram gerados automaticamente.
Em a Tabela 10 temos uma visão sob outra perspectiva da análise das informações sobre a reunião 1, que é o cruzamento das informações a partir de os termos relevantes gerados automaticamente.
Em esta análise temos listados todos os termos gerados automaticamente como relevantes para esta reunião e se também foi detectado por a percepção humana durante o survey.
De os 23 termos gerados automaticamente, 15 foram apontados por humanos, de acordo com a classificação da Tabela 9, e 8 não foram apontados.
Observa- se que os participantes apontaram 18 termos diferentes como relevantes para a reunião 1, e se considerarmos o que a maioria apontou, teremos então 10 termos.
Enquanto que de forma automática foram gerados 23 termos.
Portanto, o método automático gerou mais termos que os apontados por os participantes.
Isso foi um comportamento observado em todas as reuniões, o número de conceitos gerados automaticamente sendo maior que o apontado por os participantes.
Em o estudo de caso explanatório são apresentados experimentos em que variamos a frequência da ocorrência dos termos selecionados automaticamente para reduzir o número de conceitos gerados e atribuir um grau de relevância aos mesmos.
Estes resultados são apresentados no capítulo 6, onde temos os resultados do estudo de caso explanatório.
Em a Tabela 11 temos um resumo das três reuniões abordadas no survey, onde é comparada a quantidade de termos detectados por humanos e quantos destes foram detectados também de forma automática.
A coluna&quot;% Acerto «indica o percentual de termos que foram apontados por humanos e também foram gerados automaticamente.
Observa- se na Tabela 11, que nas três reuniões, quando um termo foi apontado por todos os humanos, ele foi detectado por o método automático.
Quando ele foi apontado por a metade ou maioria dos participantes este índice de acerto já diminui um pouco, ficando na média das três reuniões em 83,8%.
Já quando é apontado por a minoria dos participantes, a média de acerto nas três reuniões ficou em 41,8%.
Com os resultados do survey percebemos que o método que gera os termos automaticamente melhora a taxa de acerto(% Acerto) na medida em que aumenta o &quot;consenso «entre os participantes em apontar um termo relevante.
Também observamos que uma melhoria para o método seria diminuir o número de termos gerados de forma automática focando nos termos em que são apontados por a maioria dos participantes.
Ensaios para ajustar este funcionamento do método foram realizados no estudo de caso explanatório, capítulo 6.
Durante o estudo de viabilidade foi possível avaliar a correlação entre os conceitos que foram extraídos de um texto, que foi usado como roteiro para a fala dos participantes, e os conceitos que foram extraídos da transcrição da fala dos mesmos.
Em estes ensaios percebemos que para os termos com frequência de ocorrência no texto maior ou igual a três, a probabilidade de serem transcritos por o método automático proposto está acima de 90%.
Após este primeiro ensaio, foi verificada a probabilidade destes termos que foram transcritos das reuniões ocorrerem na ontologia do domínio.
Constatou- se que na medida em que aumenta a frequência de ocorrência do termo na reunião, aumenta a probabilidade deste termo ser um conceito da ontologia do domínio.
Um termo que ocorre três vezes na reunião tem menor probabilidade de ser um conceito da ontologia do domínio do que um termo que ocorreu dez vezes na reunião.
E por último, foi realizado um survey, para analisar se os termos que são gerados automaticamente são os mesmos que foram apontados por os humanos, ao indicar os termos relevantes das reuniões.
Os resultados foram satisfatórios, pois no conjunto das três reuniões mais de 70% dos termos apontados por os humanos foram gerados automaticamente.
Contudo, percebeu- se a oportunidade de aprimorar o método para gerar os termos relevantes automaticamente em função de a variação da frequência da ocorrência na reunião.
Considerações sobre o capítulo Este capítulo apresentou resultados dos experimentos das três etapas iniciais que compõem o método de pesquisa para esta tese, que são:
Os resultados dos experimentos realizados nestas etapas apresentaram dados relevantes e evidências, baseados em estudos com resultados empíricos.
Estes resultados serviram de ancoradouro para a proposição da metodologia de aquisição do conhecimento em reuniões de projeto de desenvolvimento de software, apresentada no capítulo 5.
O reconhecimento inicial do problema, através do estudo de caso exploratório, possibilitou- nos identificar uma área de pesquisa específica com questões em aberto onde poderíamos apresentar contribuições para o avanço da área.
O Estudo em profundidade, através da revisão sistemática, permitiu- nos** mapear na literatura o panorama preciso da área de pesquisa, possibilitando certificar- nos** das contribuições já realizadas e das questões em aberto.
O estudo de viabilidade permitiu- nos** a possibilidade de aferir, técnica e instrumentalmente, pontos que sugerimos para a proposta metodológica, que endereçam as questões em aberto e relevantes, observadas no reconhecimento inicial do problema e no estudo em profundidade da área.
Os resultados obtidos nos ensaios apontam a viabilidade, apesar de algumas restrições tecnológicas já comentadas, de que é possível extrair automaticamente conhecimento das conversações das reuniões de projeto de desenvolvimento de software.
Este conhecimento é apresentado na forma de conceitos que podem ser utilizados como indexadores das reuniões.
Metodologia Proposta Como etapa do método de pesquisa adotado, foi proposta uma metodologia para aquisição de conhecimento em reuniões de projetos de desenvolvimento de software, que objetiva indexar os principais conceitos associados às reuniões através da ontologia do domínio.
Esta metodologia está dividida em sete etapas, elencadas a seguir e que serão detalhadas nas próximas seções:
Estas etapas foram planejadas e elaboradas com base nos ensaios realizados no estudo de viabilidade, que apresentaram resultados satisfatórios, e das contribuições do estudo de caso exploratório, assim como da revisão sistemática, conforme apresentado no Capítulo 4.
Além de o estudo de viabilidade, que avaliou as atividades da metodologia proposta, também foi realizado um estudo de caso explanatório, detalhado no Capítulo 6, que apresenta ensaios para todas as etapas da metodologia, aplicado numa empresa, que se prestou como uma instância adequada para a realização dos experimentos, de forma a avaliar todas as atividades propostas para a metodologia.
A Figura 12 apresenta uma visão geral da metodologia, elencando:
As atividades associadas a cada etapa;
Para uma melhor compreensão da metodologia proposta, nas próximas seções são apresentadas as suas etapas de forma detalhada, conforme apresentada na Figura 12.
Etapa 1 -- Realização da reunião seguindo o protocolo Para o estudo de viabilidade, descrito na seção 3.3, foi elaborado um protocolo para as reuniões, com o intuito de facilitar o processo de transcrição das falas visando melhor extrair do arquivo de áudio os conceitos abordados nas reuniões.
Este protocolo passou por definir entre os participantes da reunião o papel de um condutor, ou moderador, da reunião.
Para este estudo de viabilidade, também foram definidas &quot;tags «ou marcadores para criar separadores de partes da reunião de acordo com os assuntos tratados, procurando facilitar a localização de em qual parte da reunião determinado conceito foi abordado.
Isso serviria para evitar que fosse escutada toda uma reunião para recuperar alguma informação.
Também se pode atribuir graus de relevância diferenciados para assuntos tratados em diferentes partes da reunião.
Em o estudo apresentado em, similarmente, são utilizadas algumas palavras chaves comuns em reuniões de times que seguem a metodologia SCRUM para facilitar a atualização de artefatos usados no projeto.
Além destes pontos, algumas questões referentes ao comportamento de participantes durante as reuniões também são importantes, como pode ser observado em e.
É importante que os participantes das reuniões sigam pontos de um protocolo, mas sabe- se que isso é um desafio.
Por exemplo, é difícil evitar que mais de uma pessoa fale ao mesmo tempo durante uma reunião.
Várias pessoas falando ao mesmo tempo podem prejudicar o desempenho do software de reconhecimento de fala.
Por isso, é importante o conhecimento do protocolo por os participantes, mas também que este altere o mínimo possível o andamento natural da reunião.
Convém destacar que existem algumas aplicações de áudio mining ou áudio indexing, que indexam um arquivo de áudio e possibilitam localizar determinada palavra no arquivo de áudio.
Além de indicar se determinada palavra foi encontrada, estas aplicações também podem indicar em que posição do áudio encontra- se tal palavra, facilitando desta forma a localização de determinada informação no arquivo de áudio.
De entre as aplicações encontradas para este propósito destacamos as seguintes soluções:
Nuance ­ Dragon12, Aurix13 e Nexidia14.
Estas aplicações não são gratuitas e não possuem versões para demonstração ou teste.
Destacamos também as pesquisas feitas por a Microsoft nesta área, através do projeto MAVIS 15, onde já podemos encontrar aplicações para este fim em produtos como o OneNote 2007 e Exchange Server 2010.
A empresa Google também possui pesquisas nesta área, onde destacamos o projeto Gaudi, que surgiu no ano de 2008, mas que foi retirado da lista do labs.
Google e no momento concentra suas pesquisas de speech recognition em aplicações focadas para o Youtube16.
Desta forma, quando de a realização do estudo de caso explanatório, para as reuniões foi mantida no protocolo a figura do condutor da reunião e uma organização na condução para evitar que mais de uma pessoa fale ao mesmo tempo.
O uso de &quot;tags «para marcar parte do texto não foi utilizado no estudo de caso, na perspectiva de usar futuramente alguma aplicação de áudio indexing, facilitando a localização das palavras no arquivo de áudio.
Também se percebeu no estudo de viabilidade que o uso de &quot;tags «não apresentou influência na definição da relevância de um conceito por este estar em determinada seção da reunião.
Portanto, o uso de &quot;tags «nas reuniões, que havia sido usado no estudo de viabilidade, foi retirado do protocolo.
Desta forma, o protocolo proposto para a realização das reuniões, em adequação à metodologia proposta, está baseado na definição da figura de um condutor para a reunião, que irá realizar as falas introdutórias, mencionando o tema da reunião, participantes e data/ hora da realização e realizar o encerramento da mesma.
Assim como irá conduzir os participantes para que eles falem em tom alto e claro, de forma a facilitar a captura do áudio por os equipamentos e também evitar, na medida do possível, que mais de um participante fale ao mesmo tempo.
Podemos apontar como um requisito a qualidade da acústica do local da reunião e equipamentos apropriados para captura do áudio das falas para a realização das reuniões, como descrito no estudo de viabilidade, apresentado no Capítulo 4.
Etapa 2 -- Transcrição do áudio da reunião Após a realização da reunião que deve seguir o protocolo e os requisitos já apontados, o arquivo de áudio com o conteúdo da reunião será submetido a um software de reconhecimento automático de fala, para que seja realizada a transcrição da reunião.
Os principais softwares de reconhecimento de fala exigem um treinamento do usuário para um melhor desempenho, portanto, incluí- se como requisito também que o condutor da reunião faça este treinamento no software, para que o reconhecimento da sua fala tenha um desempenho melhor.
Os softwares para reconhecimento automático de fala apresentam desempenho melhor na língua inglesa, do que na língua portuguesa.
Aliás, os principais produtos pesquisados, apresentados na Seção 2.4 não apresentam suporte para a língua portuguesa.
O resultado desta etapa é um arquivo com a transcrição textual da reunião, gerado por o software de reconhecimento automático de fala.
Etapa 3 -- Etiquetar as palavras da reunião Para realizar esta etapa é necessário utilizar o arquivo texto resultante da transcrição da reunião e aplicar um software etiquetador para classificar todas as palavras do texto.
Desta forma teremos como extrair os substantivos do texto, que é o tipo de palavra mais relevante para o processo de indexação dos termos das reuniões.
Em o estudo de viabilidade e no estudo de caso explanatório, consideramos os conceitos extraídos de uma reunião todos os substantivos encontrados na transcrição e mais alguns outros tipos de palavras, como numerais e palavras estrangeiras.
Os tipos de palavras retornados, de acordo com a tabela de classificação do tagger17 (Tabela 5) foram:
NN (substantivo singular ou coletivo), NNS (substantivo plural), NNP (nome próprio singular), NNPS (nome próprio plural), CD (números cardinais) e FW (palavras estrangeiras).
Em a Figura 13, temos um exemplo de como é apresentado um arquivo de texto que foi etiquetado.
A Etapa 3 é então concluída com a geração de um arquivo com o texto etiquetado, conforme a Figura 13.
Etapa 4 -- Ordenar palavras da reunião por frequência de ocorrência no texto Após etiquetar as palavras, que são selecionadas de acordo com a classificação NN, NNS, NNP, NNPS, CD e FW, é realizada uma análise no arquivo de texto etiquetado, de forma a gerar uma matriz com as palavras ordenadas por a frequência com que ocorrem no texto, como exemplificado na Tabela 12.
Esta etapa consiste, portanto, na aplicação de um algoritmo para a geração de uma matriz da frequência de ocorrência das palavras no texto, a partir de o arquivo texto etiquetado.
Etapa 5 -- Tratar similaridade das palavras classificadas Em esta etapa utiliza- se um algoritmo do tipo stemmer, para agrupar palavras com o mesmo núcleo, evitando, por exemplo, que na lista de palavras apareçam separadamente &quot;comanda «e &quot;comandas», &quot;table «e &quot;tables», sendo que nestes casos as palavras são agrupadas na sua forma singular.
Por razões gramaticais, os documentos usam diferentes formas de uma palavra, como podemos observar em &quot;democracy», &quot;democratic «e &quot;democratization».
O objetivo de um algoritmo de stemmer é reduzir as formas flexionadas a um mesmo núcleo chamado stem.
A pesquisa sobre ferramentas automáticas para reduzir uma palavra ao seu stem remonta a 1960, quando derivações iniciais foram propostas e experiências foram relatadas para avaliar seu impacto sobre a eficácia da recuperação do núcleo das palavras.
Nós utilizamos em nosso projeto o algoritmo mais comum para a língua Inglesa, e que tem demonstrado ser muito eficaz empiricamente -- o algoritmo de Porter.
Em o site do NLP Group -- The Stanford Natural Language Processing Group18 pode- se realizar teste com o algoritmo stemmer, são apresentados exemplos de uso e pode- se baixar o software.
Pode- se ainda utilizar uma lista de stop words baseadas em ontologias sobre engenharia de software para refinar a lista dos conceitos válidos extraídos das reuniões,.
Por exemplo, durante uma reunião de um projeto de desenvolvimento de software muitas vezes a palavra &quot;software «será mencionada, mas não é de interesse que seja apontada como um conceito relevante.
Por isso, a retirada de conceitos do domínio engenharia de software, é uma forma interessante de refinar a lista de conceitos relevantes das reuniões.
A o final desta etapa é gerada a matriz de frequência de ocorrência das palavras no texto, refinada por o uso de um algoritmo do tipo stemmer.
A Tabela 13 ilustra como ficaria a matriz de frequência das palavras após ter sido utilizado o algoritmo stemmer.
Etapa 6 -- Classificar palavras da reunião de acordo com frequência no texto Durante o estudo de viabilidade realizou- se ensaios em que os conceitos retirados das reuniões eram comparados com os conceitos presentes na ontologia do domínio.
Podemos destacar de entre os resultados apresentados, que os termos que possuem frequência inferior a três deveriam ser descartados e que, na medida em que aumenta a frequência de ocorrência de um termo no texto, maior é a probabilidade deste termo ser um conceito da ontologia do domínio.
Os resultados detalhados destes ensaios estão na seção 4.3 Estudo de viabilidade.
Também percebemos no estudo de viabilidade, a necessidade de ensaios que apontassem uma heurística que indicasse mais precisamente, em que frequência, os termos apontados tornarse- iam relevantes para serem usados como indexadores das reuniões.
Estes ensaios foram propostos para serem realizados no estudo de caso explanatório, apresentado no Capítulo 6.
Para propor uma forma de calcular a faixa de frequência dos termos a serem selecionados, analisamos a curva de distribuição normal.
Em a Figura 14, percebemos que 68,2% dos valores estão entre a média e o valor de uma vez o desvio padrão (área azul), já 95,4% dos valores estão entre a média e duas vezes o valor do desvio padrão (área verde) e 99,7% dos valores estão entre a média e três vezes o valor do desvio padrão (área cinza).
Considerando que nos ensaios anteriores foi percebido que quanto maior a frequência de um termo na reunião, maior a probabilidade deste estar associado à ontologia do domínio, procurou- se então uma forma de selecionarmos os termos que estivem mais próximos à &quot;cauda «da curva de distribuição normal.
Desta forma, durante o estudo de caso explanatório, os experimentos indicaram que palavras na faixa entre o valor do desvio padrão e duas vezes o valor do desvio padrão da frequência de ocorrência das palavras no texto, é que deveriam ser apontadas como termos relevantes para indexar uma reunião.
Exemplificando, para melhor demonstrar a explanação anterior, tomemos o caso da reunião utilizada como exemplo na Tabela 12, onde foram extraídas 112 palavras classificadas como NN, NNS, NNP, NNPS, CD e FW e a frequência média em que elas ocorreram no texto foi 2,72;
o desvio padrão foi 5,93 e;
O valor de duas vezes o desvio padrão foi 11,86.
Então se definiu duas faixas de termos relevantes:
A primeira faixa começa no valor da soma da média da frequência mais o valor do desvio padrão, neste caso, &quot;2,72+ 5,93», que é igual a 8,65, que arredondando ficou 9;
a segunda faixa começa no valor da soma da média da frequência mais duas vezes o valor do desvio padrão, neste caso, &quot;2,72+ 11,86», que é igual a 14,58, que arredondando ficou 15.
Desta forma, para este caso definimos então duas faixas:
Uma que vai da frequência de 9 a 14, e outra que vai de valores iguais e acima de 15.
Em a Figura 15 temos uma ilustração que exemplifica como são definidas estas duas faixas, que concentram os termos mais relevantes de uma das reuniões do estudo de caso explanatório.
A escolha por o uso do desvio padrão no cálculo para apontarmos a faixa de frequência dos termos relevantes manifestou- se adequada, pois desta forma conseguimos perceber se nas reuniões alguns poucos termos aparecem com uma frequência bem acima de os demais, que é um indicativo para ser um termo indexador da reunião.
Em reuniões que isso acontece o desvio padrão tende a aumentar.
Já em reuniões onde não temos poucos termos em que a frequência destoa dos demais, o desvio padrão tende a diminuir e isso aumentará o número de termos na faixa de frequência dos termos relevantes.
Em a Tabela 14 podemos observar este comportamento através do exemplo entre duas reuniões.
Percebemos nesta análise entre estas duas reuniões, que na reunião 3, onde o desvio padrão foi maior, a quantidade de termos selecionados foi menor, já na reunião 4, onde o desvio padrão foi menor, a quantidade de termos selecionados foi maior.
Em a Figura 16 podemos perceber esta tendência no comparativo entre as reuniões do estudo de caso explanatório.
Essa análise utilizando a quantidade de termos em função de o desvio padrão poderia auxiliar na indicação de que uma reunião é mais focada em determinados tópicos, ou se ela possui um perfil mais abrangente, considerando a discussão de tópicos associados ao projeto em o qual a reunião está inserida.
Quanto menor a quantidade de termos usados como indexadores de uma reunião, mais focada ela é;
Quanto maior a quantidade de termos;
Menos focada ela é.
Portanto, ao final desta etapa da metodologia, que prevê classificar os termos abordados na reunião, o resultado é uma matriz com os conceitos mais &quot;relevantes «e a frequência em que os mesmos ocorreram na reunião.
Em esta matriz são apontados, com base no cálculo do desvio padrão, os termos que aparecem na &quot;Faixa 1 e «Faixa 2, conforme mostrado na Figura 15.
A &quot;Faixa 1 representa os conceitos que possuem uma relação de média relevância com a reunião, pois estão na faixa entre uma vez o desvio padrão e duas vezes o desvio padrão, enquanto que a «Faixa 2 representa os conceitos com uma relação de alta relevância, pois estão na faixa acima de duas vezes o desvio padrão.
Etapa 7 -- Associar conceitos extraídos da reunião com ontologia do domínio Após elencados numa matriz, os termos mais relevantes associados à reunião, torna- se possível então, associar estes com os conceitos da ontologia do domínio do projeto.
Para fazer esta associação é criada uma classe na ontologia do domínio que represente as informações de identificação de uma reunião e os termos relevantes associados a esta.
A metodologia proposta não prevê a elaboração de uma ontologia e sim, que ela já tenha sido previamente concebida.
A Figura 17 apresenta um exemplo de como são estruturadas as classe &quot;Meeting «e &quot;Tag», que representam os conceitos associados às reuniões.
Como o estudo de viabilidade e os estudos de caso tiveram as reuniões gravadas na língua inglesa, a ontologia e os exemplos também serão apresentados nesta língua.
Desta forma, a classe &quot;Meeting «está estruturada da seguinte forma:
&quot;Tag». A classe &quot;Tag «possui os seguintes elementos:
&quot;Restaurant&quot;; Um object property &quot;References_ Weakly», usado para associar um elemento classificado na &quot;Faixa 1 da classe «Tags &quot;com um elemento da classe «Restaurant».
Depois de criada a estrutura de &quot;Meeting «na ontologia do domínio pode- se incluir os indivíduos (instâncias) desta classe, como pode ser observado na Figura 18, onde foram inseridas dez reuniões.
Para cada reunião foi informado data e horário de início e os termos extraídos associados a esta reunião.
Temos no exemplo da Figura 18 as informações da &quot;meeting_ 3», que são a data e hora da reunião, inseridas através do datatype property &quot;Date_ time «e os termos associados a ela são indivíduos da classe &quot;Tag «associados através do object property &quot;Have_ many».
Para associar uma reunião aos seus termos, temos de primeiramente cadastrar estes termos através de instâncias da classe &quot;Tag», como está ilustrado na Figura 19.
Em este exemplo, temos uma instância do termo &quot;Comanda «que ocorreu em &quot;Meeting_ 3», onde foram inseridos os datatype properties &quot;Frequency 58 e «Term comanda».
Depois de inseridos os termos da reunião, como instâncias da classe &quot;Tag», estes podem ser associados a uma instância da classe &quot;Meeting «através do object propery &quot;Have_ many».
Em o exemplo apresentado, que pode ser observado através da Figura 18 e da Figura 19, a &quot;Meeting_ 3 «tem seus termos &quot;Code, Comanda, Order, Product e Table «associados através do object property &quot;Have_ many».
Como último passo para que os conceitos extraídos das reuniões estejam associados com a ontologia do domínio, os conceitos equivalentes, neste caso, de &quot;Restaurant «e de &quot;Tag «devem ser associados.
Contudo, para cada conceito associado, deve ser informado se ele está classificado na Faixa 1 ou 2 (conforme o desvio padrão), de acordo como foi descrito na Etapa 6 desta metodologia.
Tomemos como exemplo novamente a Figura 19, onde o conceito &quot;comanda «da &quot;Meeting_ 3 «foi associado à instância da classe &quot;Comanda «(subclasse de &quot;Restaurant&quot;) através do object property &quot;References_ Strongly».
Esta associação indica que o conceito &quot;comanda «extraído da &quot;Meeting_ 3», por ser um termo classificado na Faixa 2 desta reunião possui uma &quot;forte referência «ao conceito &quot;Comanda «da classe &quot;Restaurant».
Desta forma, para concretizar esta última etapa da metodologia, todos os conceitos das reuniões que possuem conceitos correspondentes na ontologia do domínio, devem ser associados a estes numa das seguintes formas:
Convém destacar que a metodologia proposta não apresenta como usar o relacionamento entre os conceitos para realização de inferências.
Mas este trabalho faz parte de um projeto de pesquisa com um escopo maior, onde outros projetos proporcionam mecanismos para que o conhecimento e os relacionamentos entre os termos da ontologia sejam usados para executar inferências.
Em a Figura 20 é apresentado um diagrama que proporciona uma visão geral do processo que contempla as etapas da metodologia, onde podemos observar os participantes, as tarefas, os fluxos e os artefatos que fazem parte do processo.
Em o diagrama apresentado, temos três participantes que executam as atividades:
A equipe, o condutor e o engenheiro de conhecimento.
O papel da equipe é participar da reunião, em a qual serão gravados os diálogos, e ela pode representar os desenvolvedores, analistas, testadores, clientes, fornecedores, enfim os stakeholders de uma forma geral.
O condutor é um membro da equipe e o seu papel é conduzir a reunião, dentro de um protocolo estabelecido, de forma a otimizar a qualidade do áudio a ser gerado com as conversações.
Isso envolve operar os equipamentos para gravação dos arquivos de áudio e conduzir os participantes de forma a melhorar a qualidade do áudio a ser gerado, evitando que vários participantes falem ao mesmo tempo e que se manifestem de forma adequada para a gravação dos diálogos.
O engenheiro de conhecimento realiza várias atividades dentro de o processo, que compreendem todas as tarefas que vão, desde a gravação do áudio gerado nas reuniões, até a associação dos termos extraídos das reuniões com a ontologia do domínio.
O engenheiro de conhecimento manipula e trata todas as ferramentas e artefatos definidos no processo.
O diagrama apresentado na Figura 20, em relação a o apresentado na Figura 12, proporciona uma perspectiva mais voltada para uma visão das tarefas a serem realizadas por os participantes.
O diagrama da Figura 20, também oferece uma percepção do fluxo das atividades e os artefatos gerados e utilizados durante o processo.
Convém destacar que o uso de uma ontologia do domínio é um requisito para a metodologia apresentada, e que este ainda não é um artefato amplamente usado em projetos de desenvolvimento de software, principalmente em projetos que seguem metodologias ágeis.
Salientamos novamente, que a metodologia aqui apresentada é para projetos de desenvolvimento de software, independente de serem adotadas práticas tradicionais ou ágeis.
Para uma organização adotar a metodologia proposta é necessário, portanto, que a mesma faça de ontologias de domínio dos seus projetos, de um software de reconhecimento automático de fala;
De um software etiquetador (tagger) e que utilize dispositivos de gravação de áudio adequados para as reuniões.
Também é necessário que alguém da equipe desempenhe o papel de condutor nas reuniões e de um engenheiro do conhecimento, para a realização das tarefas apontadas na Figura 20.
Considerações sobre o capítulo Realizando então todas estas etapas da metodologia proposta, temos uma estrutura de indexação dos termos mais relevantes das reuniões associados à ontologia do domínio, que permitirá consultas para verificar, por exemplo, em que reuniões, um ou mais conceitos da ontologia foram tratados, ou quais conceitos da ontologia foram tratados numa reunião e, até associar as reuniões, a outros artefatos através da ontologia do domínio.
Em o estudo de caso explanatório (Capítulo 6) é apresentado um protótipo que permite a realização de consultas à ontologia.
Convém destacar que a metodologia apresentada neste trabalho segue em linhas gerais como pressuposto conceitual e teórico para um modelo de gestão do conhecimento, o modelo de cinco fases para criação do conhecimento proposto por Nonaka e Takeuchi:
Em o próximo capítulo será apresentado um estudo de caso explanatório, onde foi possível aplicar a metodologia proposta e avaliar seus resultados através de um survey.
Resultados do estudo de caso explanatório Entre junho e julho de 2011 foi realizado um estudo de caso explanatório, conforme previsto no método de pesquisa, numa empresa desenvolvedora de software que possui sua sede instalada na cidade de Torres -- RS.
Para este estudo de caso foi realizado o acompanhamento de um projeto de atualização de um módulo do principal produto produzido por a empresa, que é uma aplicação para automação comercial, que atende o comércio varejista em geral.
Para apresentar os tópicos que compõem o estudo de caso, o capítulo será dividido nas seguintes seções:
O contexto da organização;
As atividades que foram analisadas durante o desenvolvimento do projeto;
Os instrumentos utilizados para coleta de dados;
As considerações obtidas a partir de a análise de dados e;
As discussões sobre os resultados obtidos.
Contexto da Organização A empresa onde foi desenvolvido o estudo de caso possui dezesseis anos de atuação no mercado, conta com vinte funcionários na área de desenvolvimento e suporte aos seus produtos e possui clientes espalhados em quase todos os estados brasileiros.
O carro chefe da empresa é uma aplicação para automação comercial que atende os seguintes seguimentos:
Supermercados, comércio em geral, confecção e calçados, materiais de construção, ópticas e relojoarias, livrarias e papelarias, fastfoods, bares e restaurantes, autopeças e serviços, e postos de combustíveis.
A empresa nos últimos dois anos vem implantando gradualmente práticas oriundas de metodologias ágeis ao seu processo de desenvolvimento de software.
A gerente de projetos da empresa define que &quot;o processo atual é híbrido, partindo de um modelo em cascata e adotando gradualmente práticas ágeis, dentro de um processo de amadurecimento da equipe e colaboradores».
O estudo de caso foi focado numa alteração na aplicação para bares e restaurantes, onde alguns relatórios e regras de negócio foram atualizados e foi implantado um módulo para conexão com uma balança para pesagem de produtos.
O sistema não fazia a leitura do peso e valor de um produto diretamente da balança, era necessário anotar as informações numa comanda e depois digitar- los num terminal.
Com a alteração proposta, será possível na aplicação para bares e restaurantes, a leitura dos dados diretamente da balança.
Estas modificações no sistema são o resultado da análise de um conjunto de melhorias apontadas por a base de clientes que utiliza a aplicação, desta forma não foi desenvolvido para um cliente em específico.
O projeto teve a duração de vinte e um dias e quatro funcionários participaram das atividades, sendo um analista e três desenvolvedores.
As reuniões que foram o objeto do estudo de caso, contaram com a participação destes quatro funcionários, sendo que nem sempre todos participaram das reuniões.
Atividades analisadas Foram utilizadas para o nosso estudo de caso as reuniões realizadas por a equipe de análise e desenvolvimento responsável por as alterações no sistema para bares e restaurantes.
As reuniões eram realizadas com o intuito de que a equipe equalizasse o entendimento dos requisitos associados às mudanças propostas e informasse o andamento das tarefas.
Estas reuniões ocorreram dez vezes durante a duração do projeto e foram realizadas em português.
As reuniões foram realizadas nas dependências da sede da empresa, numa sala reservada para reuniões.
O analista de sistemas do projeto conduziu as reuniões que foram gravadas num gravador digital portátil.
Não foi requisito uma excelente qualidade das condições acústicas para a gravação das reuniões, pois estas tiveram de ser transcritas para depois serem gravadas na língua inglesa.
O autor desta tese acompanhou a primeira reunião, orientando e evitando principalmente que mais de uma pessoa falasse ao mesmo tempo e também para verificar se a qualidade do áudio, considerando os equipamentos e as condições acústicas, seria apropriada para a posterior transcrição.
Após a gravação das dez reuniões, cada uma de elas foi, de forma manual, transcrita em arquivo texto contendo todas as falas realizadas durante o andamento da reunião.
Depois, baseado no texto da transcrição, todas as reuniões foram gravadas na língua inglesa, por uma professora de inglês, que também participou de estudo de viabilidade.
Esta tradução realizada para que o áudio fosse submetido ao software de reconhecimento automático de fala pode gerar um viés, sendo uma ameaça ao processo.
Para a gravação do áudio durante esta etapa foi utilizado um microfone direcional sensível com sistema eliminador de ruídos.
De a mesma forma como foi realizada no estudo de viabilidade, a qualidade do áudio gerado foi averiguada por o próprio software de reconhecimento automático de fala utilizado.
A gravação das reuniões em inglês gerou um arquivo de áudio em formato &quot;mp3 «para cada uma das dez reuniões, que foram submetidos ao processo descrito por a metodologia proposta, conforme apresentado no capítulo 5, e melhor visualizado na Figura 12.
O resultado da aplicação da metodologia resultou numa série de informações extraídas das reuniões, que serão analisados na seção 6.4 Análise dos dados.
Antes da realização das reuniões, o autor desta tese juntamente com o analista responsável por o projeto, elaborou uma ontologia do domínio do projeto, apresentado na Figura 21, que serviu para o estudo de caso.
Durante o estudo de caso foi elaborada e testada uma heurística para a classificação dos termos selecionados em duas faixas, ou níveis, de acordo com a frequência em que o termo ocorre na reunião.
Estas duas faixas estão baseadas no cálculo do desvio padrão da frequência dos termos na reunião e indicam um grau de associação destes termos com a reunião através do uso da ontologia do domínio.
Todo o processo para a definição destas duas faixas e como os termos são selecionados foram detalhadamente explicados na seção 5.
6 Etapa 6 -- Classificar palavras da reunião de acordo com frequência no texto.
Em as próximas seções também são apresentadas algumas discussões sobre o método de classificação dos termos das reuniões.
Instrumento de pesquisa Em este estudo de caso, para as dez reuniões realizadas foi aplicada a metodologia proposta neste trabalho, que de forma automática, a partir de o áudio das reuniões gerou os conceitos mais relevantes associados às reuniões.
A realização de um survey, com os participantes das reuniões, foi uma forma encontrada de avaliar se os conceitos mais relevantes gerados através da nossa metodologia estariam de acordo com a percepção humana, de quais seriam os conceitos mais relevantes.
Para a realização do survey foi elaborado um instrumento, que foi submetido à avaliação dos participantes das reuniões de projeto do estudo de caso explanatório.
De uma forma geral, este instrumento apresentou aos respondentes uma lista de termos, que são os conceitos da ontologia do domínio, para que estes analisassem as reuniões e apontassem os termos que seriam relevantes para os tópicos tratados em cada reunião.
O survey foi composto por os seguintes passos:
O autor desta tese fez uma reunião individual com cada respondente do survey explicando o contexto da realização do mesmo;
Cada respondente revisou o conteúdo de cada uma das reuniões realizadas durante o projeto, sendo que foi disponibilizado no instrumento a transcrição completa de todas as reuniões;
Cada respondente apontou na planilha de avaliação os termos relevantes de cada reunião.
Em o Apêndice E é apresentado o instrumento utilizado neste survey, com mais detalhes sobre a aplicação do mesmo.
Análise dos dados A análise dos dados gerados no estudo de caso explanatório é composta de duas etapas:
Análise dos termos gerados de forma automática para refinamento do método proposto e;
Comparação entre os dados que foram gerados de forma automática e os dados obtidos através do survey.
Em um primeiro momento, foram realizadas análises sobre os resultados gerados com a aplicação da metodologia proposta às reuniões que compuseram o estudo de caso, partindo da observação dos termos que foram gerados de forma automática.
De acordo com os resultados do estudo de viabilidade, a &quot;linha de corte «para selecionar os termos relevantes era a frequência de ocorrência na reunião sendo igual ou superior a três (seção 4.3 Estudo de viabilidade).
A o aplicar este método às reuniões do estudo de caso explanatório percebemos que a quantidade de termos selecionada como &quot;relevante «era mais extensa do que nos ensaios do estudo de viabilidade.
Como o objetivo era capturar os termos mais relevantes que poderiam servir de indexadores para uma reunião, o foco era apontar um número pequeno de termos, mas que realmente servissem para identificar e diferenciar os assuntos tratados nas reuniões.
Como apontado nos surveys realizados (estudo de viabilidade e estudo de caso explanatório) estes termos relevantes geralmente são os termos que mais ocorrem na reunião.
Depois de alguns ensaios para tentar selecionar os termos que se diferenciassem dos demais por a frequência em que ocorriam no texto, surgiu uma proposta de usar o cálculo do desvio padrão para separar da lista de termos de uma reunião, aqueles que se destacam dos demais por a maior frequência.
A seção 5.6 Etapa 6 -- Classificar palavras da reunião de acordo com frequência no texto, apresentou como foi elaborada a heurística para o uso do desvio padrão na classificação dos termos relevantes de uma reunião.
Para exemplificar a situação percebida no estudo de caso explanatório, podemos observar a Figura 22 onde temos a relação dos termos de cada uma das dez reuniões.
Em esta figura foi feito um corte para aparecerem os termos com frequência igual ou superior a três, pois a reuniões tinham em média cem termos.
Podemos verificar, por exemplo, que em a «Reunião 1 se mantivéssemos a linha de corte na frequência igual ou maior a três, seriam selecionados como termos relevantes para esta reunião trinta e uma palavras.
A o aplicarmos o método proposto, conforme descrito na metodologia (seção 5.6), usando os valores de desvio padrão das frequências, teremos então seis termos selecionados como relevantes.
O método proposto ainda divide os termos selecionados em duas faixas:
Faixa 1: Começa no valor da soma da média da frequência mais o valor do desvio padrão, que neste caso seria 3,69 (média)+ 7,80 (desvio padrão) $= 11,49, arredondado para 11.
Isto indica que, a Faixa 1 começaria nos termos com frequência igual ou maior a 11 e iria até a Faixa 2, descrita a seguir.
Faixa 2: Começa no valor da soma da média da frequência mais duas vezes o valor do desvio padrão, que neste caso seria 3,69 (média)+ 15,60 (duas vezes o desvio padrão) $= 19,29, arredondado para 19.
Em este caso, a Faixa 2 começaria nos termos com frequência igual ou maior a 19.
Convém destacar que neste exemplo da Reunião 1 (Figura 22), de acordo com a metodologia proposta, ainda faltaria aplicar aos termos selecionados o tratamento de similaridade através do algoritmo stemmer e verificar se o termo é um conceito existente na ontologia do domínio, ou seja, esta lista de termos ainda não é a final e poderá diminuir.
Para resumir a análise feita nas dez reuniões, na Figura 23 é apresentada para cada reunião uma série de informações que auxiliam a compreensão dos principais dados extraídos das análises, que foram realizadas por a aplicação da metodologia proposta.
A seguir detalharemos as informações apresentadas na Figura 23: Faixa 1: Média+ desvio padrão:
Aponta o valor da frequência em que inicia a Faixa Faixa 2: Média+ desvio padrão x2:
Aponta o valor da frequência em que inicia a Faixa 2;
qtde termos Faixa 1: Aponta a quantidade de termos que foram classificados para a Faixa 1 e o percentual que representa esta quantidade em relação a o total de termos selecionados da reunião;
Faixa 2 e o percentual que representa esta quantidade em relação a o total de termos selecionados da reunião;
Faixa 1+ Faixa 2: É a soma da quantidade de termos classificados nas duas faixas e o percentual que representa esta quantidade em relação a o total de termos selecionados da reunião;
Qtde termos Freq\&gt; $= 3: Indica a quantidade de termos que possuem frequência igual ou maior que três e o percentual que representa esta quantidade em relação a o total de termos selecionados da reunião.
Diante de as informações apresentadas na Figura 23 podemos destacar que em média, a quantidade de termos selecionados na Faixa 1+ Faixa 2 foi quase um terço do valor da quantidade de termos selecionados com Freq\&gt; $= 3 (25,6%).
Isso significa que considerando uma reunião em que fosse extraída a quantidade de cem termos, se fosse usado o critério de Freq\&gt; $= 3, vinte e seis termos seriam considerados relevantes, enquanto que usando a heurística baseada nos valores do desvio padrão, teríamos nove termos considerados relevantes, e na média, metade em cada faixa.
Esta análise reforçou a percepção, já apontada no estudo de viabilidade, da necessidade de apresentar uma heurística para aperfeiçoar o processo de seleção dos termos relevantes, que foi alcançada com o cálculo utilizado para definir as duas faixas de termos selecionados, baseado no desvio padrão.
Em a primeira etapa desta seção, apresentamos as análises realizadas sobre a geração automática dos termos relevantes baseado na metodologia proposta, para as dez reuniões do estudo de caso.
A seguir, apresentaremos as análises dos dados focando na comparação dos termos gerados automaticamente, com os termos apontados por os humanos no survey, para as mesmas dez reuniões do estudo de caso.
O principal objetivo desta comparação foi avaliar a precisão da metodologia proposta, para elencar os termos relevantes de uma reunião, comparados à percepção dos participantes das reuniões.
Convém destacar que os termos que foram gerados automaticamente para as dez reuniões do estudo de caso, antes de serem comparados com os termos apontados por os participantes das reuniões, passaram por o tratamento do algoritmo stemmer e também os termos foram comparados e limitados aos conceitos da ontologia do domínio, conforme a metodologia proposta (capítulo 5).
Em a Tabela 15 temos um exemplo de comparação dos termos gerados automaticamente para uma reunião e os termos que foram apontados por os participantes da reunião no survey.
Esta mesma análise foi realizada para todas as dez reuniões e os resultados estão sinteticamente apresentados na Tabela 16.
A Tabela 15 apresenta todos os termos da ontologia divididos em categorias:
Classes, datatype properties e object properties e, na coluna &quot;Automático», quais destes conceitos foram detectados automaticamente e qual a classificação atribuída:
&quot;Faixa 1 «ou «Faixa 2.
Também temos as colunas &quot;H1», &quot;H2», H3 e &quot;H4», que representam os apontamentos feitos por os participantes do survey, indicando os conceitos considerados por eles relevantes.
A coluna &quot;HT* «totaliza as marcações feitas por os participantes, indicando em valores percentuais a proporção de humanos que marcaram aquele conceito.
Quando o valor é negativo, significa que o conceito foi apontado por os participantes, mas não foi detectado automaticamente.
A coluna &quot;HT* «também totaliza as marcações feitas por os participantes, mas indica se a metade ou maioria dos participantes apontaram o termo como relevante.
Se a metade ou maioria apontar como relevante é apontado 100%, caso contrário 0%.
Essa divisão em duas formas de analisar a totalização das marcações dos participantes foi feita por o fato de, ser esta, uma avaliação subjetiva, difícil de ser quantificada.
Desta forma apresentamos duas visões, uma que apresenta uma média aritmética das marcações dos avaliadores para os termos (HT*);
E outra que considera se um termo é relevante (100%) ou não, considerando se a metade ou maioria dos participantes o considerou relevante (HT*).
Em a parte superior da Tabela 15 temos um resumo dos dados apresentados.
O item &quot;Faixa 2 (média + DP x2) «indica quantos conceitos nesta faixa foram apontados automaticamente, sendo informado na coluna &quot;Qtde».
Para a margem de acerto em relação a a avaliação humana, temos as colunas&quot;% Acerto*», baseada nos valores apontados em &quot;HT* «e a coluna&quot;% Acerto*», baseada nos valores apontados em &quot;HT*».
Abordando primeiramente a coluna&quot;% Acerto*», esta margem de acerto significa o percentual de marcações feitas por os participantes considerando &quot;HT*».
Por exemplo, para a Faixa 2 tivemos três termos detectados automaticamente:
&quot;Comanda», onde todos os participantes também o identificaram como um conceito relevante, ou seja 100%;
&quot;Table», todos também apontaram como relevante, ou seja 100% e;
&quot;Code_ Product», onde três dos quatro participantes apontaram como relevante, ou seja, 75%.
Desta forma, para a Faixa 2, houve na média uma margem de acerto de 92%.
De a mesma forma foi feito o cálculo para a Faixa 1, onde foi encontrado o valor de 75%.
Diferentemente, a coluna&quot;% Acerto*», utiliza o percentual de marcações feitas por os participantes considerando o valor de &quot;HT*».
Por exemplo, para a Faixa 2 tivemos três termos detectados automaticamente:
&quot;Comanda», onde todos os participantes também identificaram como um conceito relevante, ou seja 100%;
&quot;Table», todos também apontaram como relevante, ou seja 100% e;
&quot;Code_ Product», onde três dos quatro participantes apontaram como relevante, ou seja, a maioria, portanto também será 100%.
Desta forma, para a Faixa 2, houve uma margem de acerto de 100%.
De a mesma forma foi feito o cálculo para a Faixa 1, onde foi apontado o valor de 100%.
Para o item &quot;Termos não detectados automaticamente», temos na coluna &quot;Qtde «quantos termos foram apontados por humanos no survey e não foram gerados automaticamente através da metodologia proposta.
O cálculo para apontar os valores das colunas&quot;% Acerto* &quot;e&quot;% Acerto* «é diferente do que foi usado para Faixa 2 e Faixa 1.
Em este caso, a coluna&quot;% Acerto* «indica o percentual de quantos termos foram apontados por humanos e também detectados automaticamente.
Para o exemplo da Tabela 15, temos 10 termos apontados por humanos e cinco não foram detectados por o método automático, portanto o valor é 50%.
Em a coluna&quot;% Acerto*», seguimos usando o conceito de &quot;metade ou maioria», portanto, um termo só é considerado relevante se foi apontado por a metade ou maioria dos participantes.
Em este caso então, os termos &quot;Client», &quot;Restaurant «e &quot;Transferred «foram desconsiderados, pois nestas três situações somente um participante considerou o termo como relevante.
Desta forma o valor da coluna&quot;% Acerto* «é 71,4%, ou seja, de sete termos apontados por os humanos, dois não foram detectados, pois três foram descartados.
Estas análises apresentadas para a reunião usada como exemplo na Tabela 15, também foram realizadas para todas as dez reuniões do estudo de caso e os dados estão sinteticamente apresentados na Tabela 16.
Podemos observar na Tabela 16 que para a Faixa 2 a média de &quot;Acerto* «foi de 93%.
Isso significa que dos termos gerados automaticamente, 93% estavam de acordo com a percepção humana dos participantes do survey.
Este cálculo considera a média aritmética para verificar se os respondentes do survey apontam o termo como relevante.
Por exemplo, se um termo é considerado relevante por três dos quatros respondentes, apontamos que 75% consideram o termo relevante.
Já a média &quot;Acerto* «foi de 97%, onde não usamos a média aritmética, e sim, é apontado se a metade ou maioria considera o termo relevante, caso contrário (minoria) ele não é considerado relevante.
Já para os termos não detectados automaticamente, temos o índice de 50% de acerto (Acerto*), indicando que foi detectado na média 50% dos termos apontados por os respondentes do survey.
Em este cálculo foi considerado que se somente um respondente apontou determinado termo como relevante e os demais respondentes não, e este termo não foi gerado automaticamente, ele foi considerado não detectado.
Já o índice de Acerto*, que foi de 80%, significa que 80% dos termos apontados por os respondentes foram gerados automaticamente.
Este cálculo considera que se um termo foi apontado como relevante por somente um dos quatro respondentes, ou seja, não representa a metade nem a maioria, este termo foi descartado, portanto, retirado do índice dos termos não detectados automaticamente.
Outra observação, sobre a análise dos dados das reuniões do estudo de caso, diz respeito à associação da ocorrência dos termos classificados como mais relevantes a um determinado tipo de conceito da ontologia.
Podemos observar este tipo de associação na Tabela 17, onde, por exemplo, é apontado que a maioria dos termos classificados na Faixa 2, onde estão os conceito com maior frequência, estão associados majoritariamente com conceitos de tipo class na ontologia.
Já a maioria dos termos classificados como Faixa 1, que são de menor frequência que os da Faixa 2, ocorreram mais vezes associados a conceitos do tipo datatype property da ontologia.
Os conceitos do tipo object property apresentaram somente um termo como relevante.
Baseado nos dados da Tabela 17, percebemos que embora somente doze conceitos da ontologia sejam do tipo class, 60% dos termos classificados como relevantes estão associados a estes conceitos e se considerarmos somente a Faixa 2 (mais relevante) 71% são do tipo class.
Observamos então, que os conceitos do tipo class são os mais relevantes do domínio e que os datatype properties são conceitos subordinados às classes.
Como o objetivo da metodologia é indexar os conceitos mais relevantes das reuniões de projeto, as classes poderiam representar um nível mais representativo e os datatype e object properties um nível secundário.
Essa diferenciação pode ser usada, por exemplo, nas consultas, pois num primeiro momento pode- se apresentar para uma consulta as classes e não todos os termos da ontologia.
Caso a consulta não atenda às expectativas, apresenta- se um segundo nível, composto por os demais conceitos da ontologia.
Desta forma, criamos para cada reunião uma análise do coeficiente de correlação de Pearson entre a matriz dos conceitos gerados automaticamente e a matriz dos conceitos apontados por os humanos.
Em cada matriz, que é baseada nos termos de a ontologia, um conceito gerado, ou apontado, é marcado com &quot;1», enquanto que o conceito não gerado é marcado com &quot;0», como pode ser observado na Tabela 18.
Essa análise serviu para observarmos o comportamento da correlação entre o conjunto total dos conceitos gerados automaticamente, assim como os não gerados, com o conjunto total dos conceitos apontados, e os não apontados, por os respondentes do survey.
Em a Tabela 18 apresentamos um exemplo do cálculo do coeficiente de correlação de termos gerados automaticamente para a Reunião 3 e os termos relevantes apontados por os quatro respondentes do survey.
Em a primeira coluna, &quot;Termos», é apresentada a listagem dos conceitos da ontologia do domínio.
Em a segunda coluna, &quot;Aut», temos em valor &quot;1 «os termos que foram gerados automaticamente tanto para a Faixa 2, como para a Faixa 1.
As colunas &quot;H1», H2, &quot;H3 e «H4», indicam em valor &quot;1 «o que foi marcado por os respondentes como conceito relevante.
Em a coluna &quot;HT*», temos a média aritmética das marcações feitas por os respondentes para cada conceito e na coluna &quot;HT*», temos a indicação, através do valor &quot;1», se a metade ou maioria dos respondentes considerou aquele conceito relevante.
Em a última linha da tabela temos os coeficientes de correlação entre os valores gerados automaticamente e os valores gerados a partir de a avaliação dos respondentes.
Em a Tabela 19 temos o conjunto de todos os coeficientes de correlação para as dez reuniões do estudo de caso.
Cabe destacar que foram feitas análises de duas formas:
Também podemos observar que o coeficiente de correlação na média sempre ficou acima de 0,7.
Isso indica uma forte correlação entre as duas variáveis.
A análise que considerou todos os tipos de conceitos da ontologia apresentou entre as dez reuniões, três com coeficientes abaixo de 0,7.
No entanto, a análise que considerou somente os conceitos do tipo class, apresentou somente uma reunião com coeficiente abaixo de 0,7.
Estes resultados reforçam os argumentos que apontam a viabilidade da metodologia proposta para a aquisição de conhecimento, por a geração automática de conceitos extraídos das reuniões.
Uso de um protótipo para realizar consultas na ontologia Durante o estudo de caso explanatório foi elaborada uma ontologia do domínio e conforme a metodologia proposta, após serem gerados e classificados os conceitos relevantes das reuniões de projeto, estes devem ser inseridos na ontologia através de uma classe &quot;Meeting «(descrito na seção 5.7 Etapa 7 -- Associar conceitos extraídos da reunião com ontologia do domínio).
Após a ontologia do domínio ser enriquecida com as informações extraídas das reuniões, poderíamos realizar consultas na ontologia para verificar, por exemplo, em quais reuniões determinado conceito foi abordado, ou quais os conceitos estão associados a uma determinada reunião, entre outras consultas que explorem a relação entre os conceitos da ontologia do domínio e a sua associação aos conceitos tratados nas reuniões do projeto.
Cabe destacar, que o que chamamos de ontologia do domínio é referente a a etapa anterior ao enriquecimento da ontologia com as informações das reuniões, sendo depois esta atualização, uma ontologia do projeto, pois não reflete mais somente o domínio.
Para facilitar o processo de elaboração das consultas, foi construído um protótipo que carrega a ontologia do domínio atualizada com as informações das reuniões e através de uma máquina de inferência, que neste caso integrada ao framework Jena19, possibilita que sejam realizadas consultas utilizando os conceitos da ontologia.
A Figura 24 apresenta a tela do protótipo que foi elaborado para a realização das consultas sobre os conceitos da ontologia.
Em este protótipo a primeira tarefa a ser realizada é a carga da ontologia a ser analisada, que é feita através do botão &quot;Load Ontology».
Depois de carregada a ontologia, podemos realizar as consultas utilizando os comboboxes 1 e 3, que listam todos os conceitos da ontologia e o combobox 2 que possibilita usar os operadores lógicos &quot;E «e &quot;Ou», no caso de a consulta utilizar mais de um termo.
Temos duas checkboxes para definir o nível de associação do conceito com a reunião, conforme foi apresentado na seção 5.7.
Caso queira- se incluir mais uma cláusula à consulta, pode- se utilizar o botão &quot;Add as a new».
Depois de escolhido o conceito, ou os conceitos, juntamente com os operadores lógicos que comporão a expressão para a consulta, deve- se clicar no botão &quot;Execute query», para que sejam listadas as reuniões que atendem às especificações da consulta.
Em a Figura 24, temos o exemplo de uma consulta onde foi verificado em quais reuniões os conceitos &quot;Order «e &quot;Comanda «são termos relevantes, tanto para associações do tipo &quot;Strongly referenced», que equivalem a conceitos da Faixa 2, como associações do tipo &quot;Weakly referenced», que equivalem a conceitos da Faixa 1.
O resultado aponta que as reuniões &quot;Meeting_ 1 «e &quot;Meeting_ 3 «satisfazem os critérios da consulta.
A Reunião é apresentada numa estrutura de árvore, que permite exibir ou ocultar os demais conceitos associados à reunião, juntamente com a frequência e qual o tipo de associação, que é diferenciada por a cor da fonte.
A o clicar- se duas vezes sobre o rótulo da reunião é aberta uma caixa de diálogo que permite ao usuário visualizar o texto com a transcrição textual da reunião, ou que execute o áudio da gravação da reunião, conforme mostrado na Figura 25.
Estas consultas poderiam ser realizadas numa aplicação para edição de ontologias, como o Protégé20, mas o uso do protótipo apresentado facilita a elaboração das consultas e da visualização das informações de forma mais adequada aos objetivos desta pesquisa.
Contribuições deste capítulo Este capítulo apresentou uma detalhada descrição de como foi conduzido o estudo de caso explanatório, que representou uma etapa importante no método de pesquisa desta tese, pois se prestou como uma forma de avaliação baseada em resultados empíricos para a metodologia proposta de aquisição de conhecimento em reuniões de projeto de desenvolvimento de software.
Além de a descrição do contexto da organização, das atividades analisadas durante a execução do estudo de caso e do instrumento de pesquisa utilizado, foi apresentada uma detalhada análise dos dados coletados.
Em a análise de dados apresentada, destacamos as seguintes contribuições:
Além de as contribuições provenientes dos resultados da análise de dados, também elencamos como uma contribuição a apresentação de um protótipo que facilita a realização de consultas à ontologia do domínio, atualizada com os conceitos extraídos das reuniões.
Com os resultados apresentados neste estudo de caso exploratório percebe- se que a metodologia proposta é viável, pois leva a resultados consistentes na aquisição de conhecimento de reuniões de projeto de desenvolvimento de software, gerando conceitos capazes de servir como indexadores destas reuniões.
Considerações Finais A maioria das empresas despende horas em reuniões entre funcionários, clientes e colaboradores.
Muitas informações importantes são abordadas e quase não se utilizam mecanismos para fazer a gestão do conhecimento que circulam nestas reuniões.
Avanços tecnológicos têm permitido gravar e armazenar estas conversações.
Meios automáticos para fazer a transcrição e indexação do conhecimento destas reuniões incrementariam em vários aspectos a produtividade dos participantes das reuniões, assim como dos não participantes direta ou indiretamente envolvidos.
Para empresas de desenvolvimento de software temos uma situação alinhada ao contexto descrito anteriormente, onde algumas tendências, como as associadas ao surgimento e avanço das metodologias ágeis, têm intensificado ainda mais este tipo de situação.
A redução da externalização do conhecimento durante o processo de desenvolvimento de software tem intensificado a comunicação direta entre os envolvidos nos projetos.
Algumas iniciativas como o projeto Calo apresentado em, indicam a preocupação e os investimentos que estão sendo realizados em várias áreas, no intuito de melhor aproveitar o conhecimento que circula nas reuniões e outros tipos de conversações realizadas entre os diversos envolvidos nas atividades das organizações.
Diante deste quadro, e ratificando como objetivo geral deste projeto &quot;a proposta de uma metodologia para aquisição de conhecimento em reuniões de projetos de desenvolvimento de software», concluímos que a metodologia proposta nesta tese é adequada para o contexto de desenvolvimento de software e consideramos positivos os resultados apresentados através dos experimentos realizados.
Os resultados iniciais, apresentados no capítulo 4, foram satisfatórios e ancoraram a metodologia proposta, com subsídios baseados em resultados empíricos.
Estes resultados iniciais foram compostos por:
Um estudo de caso exploratório, que possibilitou o reconhecimento inicial do problema, identificando questões em aberto onde poderiam ser apresentadas contribuições para o avanço da área;
Uma revisão sistemática que permitiu realizar um estudo em profundidade, identificando na literatura um quadro preciso da área de pesquisa e;
Um estudo de viabilidade, que apresentou resultados de experimentos que corroboraram com a proposta metodológica apresentada.
Com o subsídio dos resultados do estudo de viabilidade e com todas as etapas da metodologia definidas, realizou- se um estudo de caso explanatório para uma avaliação completa da metodologia, incluindo todas as suas etapas, numa instância adequada ao contexto deste projeto.
O estudo de caso explanatório, como apresentado na análise de dados, apresentou resultados favoráveis às propostas apresentadas para este projeto.
Salientamos que a metodologia apresentada não é direcionada para ser aplicada em projetos que seguem métodos ágeis.
A metodologia proposta pode ser aplicada tanto em projetos que seguem métodos tradicionais, como os que métodos ágeis.
Embora os estudos iniciais tenham sido realizados em projetos que aplicavam métodos ágeis, estando então as questões iniciais de pesquisa neste contexto, a evolução do projeto apontou para questões que levaram à especificação de uma metodologia em a qual, é indiferente se o projeto segue práticas ágeis ou tradicionais.
Com estes resultados, percebeu- se que a metodologia proposta é viável e que leva a resultados consistentes, resultando num método capaz de extrair e indexar conhecimento das reuniões de projeto de desenvolvimento de software e que a metodologia apresentada é passível de automação, desta forma, atendendo plenamente a questão de pesquisa geral desta tese.
Apontamos como as principais contribuições desta tese:
Uma metodologia para aquisição do conhecimento de reuniões de projetos de desenvolvimento de software, elaborada e avaliada a partir de um método de pesquisa ancorado em resultados empíricos e satisfatórios ao contexto proposto;
Este projeto também apresentou algumas limitações e, que devidos a estas, alguns experimentos não puderam ser realizados, sendo que estes poderiam influenciar nas análises apresentadas.
De entre as limitações, estas são apontadas como as mais relevantes:
Em o intuito de dar continuidade aos resultados já alcançados nesta tese, apresentamos como sugestão para trabalhos futuros, as seguintes propostas:
Portanto, seria avaliado qual o impacto desta prática de gestão do conhecimento, proposta por a metodologia nesta tese, num ciclo de desenvolvimento de software já estruturado e definido;
Desta forma, com a implementação dos trabalhos futuros aqui propostos, acreditamos que novos resultados venham a contribuir e somar- se aos resultados já apresentados nesta tese.
