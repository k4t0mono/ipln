As últimas décadas presenciam uma necessidade crescente por sistemas computacionais que garantam o sigilo de informações, seja durante o processamento ou armazenamento destas.
Hoje são comuns atividades como compras, transações bancárias, consulta a informações pessoais e reserva de passagens usando a Internet.
O uso de redes abertas exige a transmissão protegida de dados confidenciais.
O projeto de sistemas integrados num único chip (em inglês, SoCs) que atendam a restrições de segurança requer protocolos especiais de comunicação e o emprego de criptografia, a ciência que se baseia na aritmética para ocultar informações.
Em geral, SoCs que usam criptografia utilizam um texto relativamente curto, denominado chave criptográfica, cujo segredo condiciona a eficiência do processo de esconder informações.
Em todo sistema criptográfico moderno, conhecer a chave criptográfica equivale a ser capaz de efetuar qualquer operação sobre o conjunto de informações de um dado usuário num dado sistema.
Algoritmos de criptografia são desenvolvidos para resistir à criptoanálise, a ciência de violar textos encriptados explorando vulnerabilidades do processo de ocultação de informação.
Embora a maioria dos algoritmos atuais seja robusta a ataques baseados na matemática da criptografia empregada, uma nova classe de técnicas de criptoanálise pode ser usada contra suas implementações.
Estes são os chamados Ataques por Canais Escondidos ou Laterais (do inglês, Side Channel Attacks, ou SCA), que permitem correlacionar informações sigilosas tal como uma chave criptográfica com propriedades físicas tais como tempo de processamento, consumo de potência e radiação eletromagnética de dispositivos eletrônicos.
O fluxo tradicional de projeto que usa o paradigma síncrono e a tecnologia CMOS favorece a fuga de informações por canais escondidos.
Várias propostas para imunizar sistemas criptográficos contra ataques SCA existem na literatura.
De entre as alternativas para a obtenção de sistemas criptográficos seguros, destacam- se paradigmas de projeto específicos tais como o Globalmente Assíncrono e Localmente Síncrono (GALS) e o completamente assíncrono.
Esta tese propõe uma nova arquitetura GALS para melhorar a robustez de algoritmos criptográficos.
Pressupõe- se o emprego de técnicas pipeline e de comunicação assíncrona entre estágios.
A robustez é obtida através da combinação de replicação de hardware em estágios pipeline, comunicação assíncrona entre estes estágios e variação independente da freqüência de operação em cada estágio.
Os resultados obtidos demonstram um aumento da robustez contra análises de consumo de potência e de radiação eletromagnética nas arquiteturas propostas.
Além disso, as arquiteturas apresentam um aumento significativo da vazão de dados, ao custo de um aumento da latência de processamento e da área do circuito, este último provocado por a replicação de hardware.
Comparado com o estado da arte em propostas de lógica assíncrona segura, o custo em área mostra- se inferior ou no pior caso compatível, demonstrando que a proposta é uma alternativa interessante de solução para neutralizar ataques SCA.
Palavras chave:
Criptografia, ataques criptográficos, GALS, pipeline.
Em as últimas décadas a tecnologia CMOS (do inglês, Complementary Metal--OxideSilicon) desenvolveu- se vertiginosamente, impulsionada por a evolução da tecnologia submicrônica, que reduz consideravelmente as dimensões de transistores, componente elementar desta tecnologia.
Esta evolução permite a concepção de circuitos integrados (CIs) com dezenas de bilhões de transistores, com dimensões na ordem de algumas dezenas de nm nas atuais tecnologias e previsão de dimensões de até 7,4 nm para o ano de 2024 segundo o International Technology Roadmap for Semiconductors (ITRS).
A chamada &quot;Lei de Moore «determina a tendência de aumento da capacidade de integração de transistores num Ci.
De acordo com a ITRS, no futuro, a integração da tecnologia CMOS com tecnologias não-CMOS tais como componentes biológicos, sensores e atuadores darão origem aos sistemas integrados em produtos nanométricos.
O avanço na tecnologia de fabricação de circuitos integrados em alta escala de integração (do inglês, Very Large Scale Integration -- VLSI) proporciona o advento de SoCs baseado na técnica de reuso de componentes pré-projetados e pré-validados denominados de núcleos IP (do inglês, Intellectual Property Core -- IP), como forma de reduzir fortemente o tempo de concepção de um produto e garantindo o lançamento no mercado dentro de uma janela adequada de mercado.
É possível encontrar SoCs hoje em produtos das mais diferentes áreas de aplicação tais como, por exemplo, sensoriamento remoto, processamento paralelo e ainda, em aplicações que exijam segurança ao processamento de dados entre outras.
As últimas décadas presenciam a necessidade crescente por sistemas digitais que garantam o sigilo de informações, seja em seu processamento ou no armazenamento de dados.
São comuns as atividades de compra por a Internet, transações bancárias, consultas a informações pessoais, sistemas de reserva de passagens entre outros, que exigem sistemas computacionais operando em redes de acesso como a Internet, o que requer a transmissão protegida de dados confidenciais.
O projeto de SoCs que atendam as restrições de segurança exige protocolos especiais de comunicação e também o uso da criptografia, ciência que se baseia na aritmética para ocultar dados.
Atualmente com a globalização das indústrias de semicondutores e a verticalização do processo de fabricação existe também a preocupação com a inserção de &quot;armadilhas «no projeto de SoCs, principalmente em SoCs para fins militares, segundo Adee em.
Em a Europa, por exemplo, todos os produtos com restrições de segurança, antes de estarem disponíveis ao mercado, são antes avaliados por uma agência governamental responsável por regulamentar a qualidade dos produtos.
Em a França, por exemplo, a Agence Nationale de la Securité des Systèmes d'Information (ANSSI) emprega um plano denominado de Critérios Comuns para Avaliação da Segurança em Tecnologia da Informação (em francês, Critères Communs pour de la Sécurité des Technologies de l'Information) onde constam vários níveis de segurança os quais um produto é submetido para avaliar vulnerabilidades, possibilidade de clonagem e riscos provocados por usuários mal intencionados.
Este plano de avaliação é desenvolvido em comum acordo com as demais agências européias e norte-americanas de segurança, segundo o Padrão Internacional Iso/IEC 15408:2005.
Embora a criptografia tenha sido continuamente desenvolvida para garantir que algoritmos sejam robustos às tentativas de violação de dados confidenciais, novas técnicas demonstram que através de propriedades físicas dos sistemas digitais é possível revelar os dados secretos processados.
Esta classe de técnicas conhecida como ataques a canais laterais ou escondidos (em inglês, Side Channel Attacks -- SCA) explora a fuga de informações sensíveis a grandezas tais como o consumo de potência, a radiação eletromagnética, o tempo de processamento, etc..
Estas fugas permitem descobrir informações secretas de um sistema, sobretudo aquelas protegidas de criptografia.
Estes ataques procuram estabelecer uma relação de dependência entre os dados processados e as grandezas físicas analisadas.
As vulnerabilidades têm origem principalmente nas características de implementação da tecnologia de fabricação de circuitos e também no paradigma síncrono tradicionalmente adotado para a concepção de sistemas digitais.
O consumo de potência em circuitos pode ser modelado formalmente, e seu comportamento previsível pode ser explorado via SCA.
O mesmo ocorre com a radiação eletromagnética (em inglês, electromagnetic radiation EMR) do circuito, pois as variações de corrente ocasionadas durante o processamento geram variações proporcionais no campo eletromagnético radiado por o circuito, deixando o sistema vulnerável também a este canal lateral.
O processo de fabricação de CIs não garante que, por exemplo, os fios de um barramento de dados tenham o mesmo tempo de propagação.
As ferramentas de síntese e as variabilidades do processo de fabricação do Ci causam desalinhamentos destes fios o que os torna um sistema vulnerável, por exemplo, à análises de tempo de processamento.
O projeto de sistemas criptográficos, ou seja, sistemas digitais que utilizem criptografia para ocultar informações confidenciais imunes a ataques por canais laterais (SCAs) é uma área de pesquisa relevante.
Em este trabalho sistemas criptográficos são também referenciados como criptosistemas, termo comumente encontrado na literatura.
O restante deste Capítulo introduz em maiores detalhes os fatores que influenciam na fuga de informações por canais laterais na Seção 1.1.
A Seção 1.2 apresenta as principais técnicas de ataques a canais laterais.
A Seção 1.3 introduz as alternativas de projeto de sistemas criptográficos para resistir a ataques por canais laterais.
Em seguida são apresentados os objetivos do trabalho na Seção 1.4.
A contribuição original é destacada na Seção 1.5 e as demais contribuições são listadas na Seção 1.6.
Um resumo da organização dos capítulos que compõem a presente tese conclui este Capítulo na Seção O desenvolvimento de SoCs em sua quase totalidade parte do pressuposto da discretização de tempo.
Este pressuposto submete todas as entradas do sistema à temporização de um único sinal de controle, gerado externamente e denominado relógio (em inglês, clock).
O pressuposto da discretização de tempo por o uso de um sinal de relógio global no projeto de sistemas digitais é a característica principal do estilo síncrono de projeto.
O tempo transcorrido entre transições do sinal de relógio permite que os valores de entrada amostrados sejam usados na computação de novos resultados.
Este tempo garante a estabilização dos valores nas entradas e saídas de elementos de armazenamento, bem como a ocorrência e o descarte adequado de valores transitórios no interior do circuito.
O uso intensivo deste estilo de projeto na concepção de CIs VLSI bem como o uso da tecnologia de fabricação CMOS não traz apenas benefícios.
A adoção de um único relógio para controlar todo o sistema digital traz alguns problemas, tais como a distribuição global do relógio, o escorregamento e o consumo de potência neste sinal.
Estes problemas eram desprezíveis até meados da década de 80 ou facilmente tratáveis, porém começam a se tornar difíceis de resolver com a acentuada miniaturização da tecnologia Segundo demonstram Ho, à medida que a tecnologia VLSI evolui, o tempo de propagação de sinais em fios globais vai cada vez mais exceder o período de relógio.
Desde a tecnologia 130 nm é necessário em média mais de um período de relógio para um sinal propagando- se num fio global atravessar todo o comprimento de um chip.
Isto contribui para aumentar a complexidade de projeto de sistemas síncronos.
A exigência de altas freqüências de operação para aumentar a velocidade de processamento eleva o consumo de potência, o que não é desejável, principalmente em produtos móveis tais como telefones celulares, games, PDAs, notebooks entre outros, caracterizando uma restrição do projeto de sistemas digitais com o paradigma síncrono.
Além de esta restrição, a sincronização das operações com o uso do relógio global facilita a correlação entre dados e efeitos físicos mensuráveis externamente, como apresentado por Kocher.
A necessidade de métodos alternativos para projetar chips motiva o desenvolvimento de trabalhos tais como proposto por Chapiro.
O Autor propõe um novo método de projeto denominado Globalmente Assíncrono e Localmente Síncrono (do inglês, Globally Asynchronous Locally Synchronous -- GALS) que visa eliminar os relógios globais em sistemas digitais.
Esta solução mantém o paradigma síncrono na concepção de módulos funcionais, denominados de ilhas síncronas e elimina o problema do uso de sinais globais por empregar interfaces de comunicação assíncrona entre módulos.
Trabalhos tais como se apoiam nos métodos GALS de projeto para eliminar problemas causados por relógios globais.
Em geral, um sistema criptográfico utiliza uma palavra relativamente curta chamada de chave criptográfica cujo segredo condiciona sua eficiência.
Em sistemas criptográficos modernos, conhecer a chave equivale a ser capaz de efetuar as operações no criptosistema.
Em 1996, Kocher apresentou o primeiro ataque por canal lateral, denominado de ataque por análise de tempo (em inglês, Timing Attacks -- Te a).
Este ataque analisa o tempo de execução de operações num sistema criptográfico.
Te a explora ligeiras diferenças na quantidade de tempo necessária para efetuar o processamento de diferentes dados.
O ataque mostrou- se capaz de quebrar o sigilo de informações em algoritmos tais como RSA (do inglês, Rivest Shamir Adleman) e Diffie-Hellman Kocher Em 1999 provam ser possível relacionar o consumo de potência de um circuito digital com os dados processados.
Estes Autores mostram que diferentes operações aritméticas possuem diferentes traços de consumo de potência, e denominaram este tipo de avaliação de Análise Simples de Potência (do inglês, Simple Power Analysis -- Spa).
Ainda, eles demonstram que com o uso de métodos estatísticos é possível estabelecer uma correlação entre o consumo de potência e os dados processados num sistema criptográfico, sendo este tipo de avaliação denominada de Análise Diferencial de Potência (do inglês, Differential Power Analysis -- DPA).
Mais tarde, em 2001 Gandolfi Apresentam experimentos comprovando a possibilidade de encontrar uma chave criptográfica através da análise da radiação eletromagnética de sistemas criptográficos usando os algoritmos Des (do inglês, Data Encryption Standard) ou RSA.
Estas análises são denominadas de Análises Simples da Radiação Eletromagnética (do inglês, Simple Electromagnetic Analysis -- SEMA) e Análises Diferenciais da Radiação Eletromagnética (do inglês, Differential Electromagnetic Analysis -- DEMA).
As análises por consumo de potência, principalmente as análises DPA são as mais citadas na literatura, devido a o seu baixo custo de execução e eficiência.
Estas análises exploram basicamente duas características dos circuitos:
O comportamento da tecnologia CMOS quanto a o consumo de energia e o sincronismo das operações controladas por o sinal de relógio.
Com base nestes fatores, atacantes exploram a relação deste canal lateral com os dados processados por o circuito.
Maiores detalhes sobre esta análise são apresentados no Capítulo 2 do presente trabalho.
Os trabalhos citados nesta Seção deram início a uma importante área de pesquisa em segurança de projeto de sistemas digitais.
As vulnerabilidades encontradas em sistemas criptográficos despertaram o interesse da comunidade acadêmica e da indústria de semicondutores, além de levantar questões de segurança, por exemplo, em entidades governamentais.
Outras formas de ataques derivados destas primeiras propostas são encontradas na literatura.
Exemplos são os ataques por correlação de modelos de potência, análises de consumo de potência de segunda ordem, e outras alternativas, revisadas no Capítulo 2 deste trabalho.
O desenvolvimento de soluções que reduzam a fuga de informações a níveis que tornem impraticável a obtenção de dados sigilosos é então um tema pertinente de pesquisa.
O problema da fuga de informações por canais laterais tornou- se uma preocupação no projeto de sistemas digitais que operam com informações confidenciais tais como smart cards.
Um smart card nada mais é que um circuito integrado embutido num cartão plástico que permite o armazenamento, processamento e comunicação de dados de forma segura numa rede de comunicação pública.
Muitos trabalhos têm sido propostos desde então, visando evitar a fuga de informações ou neutralizar a ação de atacantes, pessoas mal intencionadas com alto conhecimento técnico.
Em a literatura encontram- se basicamente relatos de três abordagens para reduzir as vulnerabilidades dos sistemas criptográficos a SCAs, sendo elas:
Injeção de ruído Uniformização do consumo de potência Mascaramento de dados A primeira abordagem visa construir circuitos de modo que o consumo de potência seja aleatório durante a execução do algoritmo de encriptação.
Assim, a fuga de informações é ocultada por ruídos que dificultam a ação dos atacantes.
Isto pode ser feito em diferentes níveis de abstração do projeto, tais como no nível da arquitetura e no nível de portas lógicas.
A maioria das propostas de contramedidas que utilizam esta abordagem adiciona hardware extra e/ ou atrasos pseudo-aleatórios ao processamento para dificultar as análises DPA.
Algumas propostas sugerem métodos específicos para implementar a abordagem.
Os trabalhos e propõe a inserção de atrasos aleatórios em nível de circuito, visando aumentar a robustez de sistemas a análises DPA.
Este método intuitivo pode ser superado por ataques especializados, tal como demonstrado em, pois a encriptação completa de um dado é realizada com a mesma freqüência de relógio.
Kamoun Propuseram a replicação das funções vulneráveis do algoritmo criptográfico AES, de modo a gerar ruído no consumo de potência e assim ocultar a fuga de informações.
Standaert Propuseram o uso de uma arquitetura pipeline para neutralizar ataques DPA.
Os resultados revelam redução significativa na fuga de informações, embora existam relatos de ataques DPA bem sucedidos, mesmo que com uma pequena margem de certeza de hipóteses corretas de chave.
A segunda abordagem visa alterar as características de consumo de potência do circuito de modo a obter um consumo uniforme e independente dos dados processados durante a encriptação.
A maior parte das soluções propostas opera no nível de portas lógicas, havendo a proposição de vários estilos lógicos específicos e fluxos de projetos a fim de reduzir a fuga de informações sigilosas através de canais laterais.
O uso do paradigma assíncrono de projeto é uma alternativa para a concepção de sistemas robustos a DPA conforme, mas existem vulnerabilidades resultantes do processo de síntese de hardware, segundo estudos realizados por Razafindraibe Em.
A terceira abordagem visa mascarar os valores intermediários produzidos durante a execução do algoritmo de encriptação, ou seja, estes valores são alterados de modo que sejam independentes dos valores realmente produzidos.
Embora as características de consumo de potência dos circuitos sejam as mesmas, a abordagem previne a ação dos ataques por a alteração dos valores intermediários apenas.
Diversas propostas de sistemas criptográficos empregando mascaramento de dados são encontradas na literatura tais como,, os quais serão discutidos em mais detalhes no Cabe salientar que em muitos trabalhos o termo mascaramento é utilizado para descrever a tentativa de ocultar a fuga de informações por a inserção de ruído ou atrasos ao processamento, como propõe a primeira abordagem.
Este comportamento difere da terceira abordagem onde realmente existe a aplicação de uma máscara gerada aleatoriamente a fim de modificar os dados antes e/ ou durante a execução do algoritmo de criptografia.
Em este trabalho o termo mascaramento será utilizado apenas para os métodos descritos como esta terceira abordagem de contramedida.
Esta tese propõe contramedidas para a concepção de sistemas criptográficos imunes a análises diferenciais de potência (DPA) e análises diferenciais de radiações eletromagnéticas (DEMA).
Esta Seção lista os objetivos estratégicos e específicos deste trabalho.
Os objetivos estratégicos desta tese são:
Contribuir para a pesquisa em Segurança a Sistemas Embarcados;
Revisar o problema da fuga de informações através de canais escondidos;
Dominar e avaliar os métodos empregados para resolver este problema;
Dominar a metodologia não-síncrona de projeto adotada como base de projeto para a tese;
Dominar o funcionamento do algoritmo criptográfico Des usado como estudo de caso neste trabalho;
Propor soluções de imunização usando duas abordagens diferentes;
Avaliar as soluções propostas.
Para alcançar os objetivos estratégicos são realizadas atividades para atingir os seguintes objetivos específicos:
Definir uma infraestrutura para a prototipação de circuitos não-síncronos em FPGAs:
Definir uma infraestrutura GALS adequada para implementar contramedidas:
Propor uma biblioteca de hard macros (módulos implementados por componentes primitivos do FPGA permitindo restrições temporais ou de posicionamento);
Definir um protocolo de comunicação assíncrono entre as ilhas síncronas;
Propor um subsistema de geração pseudo-aleatória de sinais de relógio;
Propor a replicação dos módulos funcionais do algoritmo Des de modo a obter uma arquitetura pipeline;
Encapsular os módulos funcionais em ilhas síncronas;
Validar os sistemas com e sem contramedidas.
Propor diferentes configurações de pipelines para avaliar a robustez a ataques DPA e DEMA;
Avaliar a infraestrutura proposta quanto a os custos em área, vazão e latência:
Construir um sistema preciso para medição de potência e de radiação eletromagnética.
A originalidade do trabalho consiste na proposta de contramedidas a análises DPA e DEMA, visando explorar a lacuna existente por a combinação dos métodos de inserção de atrasos, uso de arquiteturas pipeline e o método GALS de projeto associado à variação dinâmica de freqüência.
Os métodos de inserção de ruído aplicam- se sobre o processamento completo de um algoritmo, sendo facilmente contornados por técnicas de ressincronização existentes.
As arquiteturas pipelines encontradas na literatura são síncronas, o que facilita a ação de atacantes, devido a o determinismo do relógio.
O paradigma GALS permite a combinação de arquiteturas pipeline com técnicas de variação dinâmica de freqüência.
Desta forma, potencializa os efeitos de aleatoriedade sobre padrões de consumo e radiação eletromagnética.
Conseqüentemente, melhoram a robustez a estes tipos de análises, conforme demonstram os experimentos descritos e realizados aqui.
O presente trabalho tem duas principais contribuições para a concepção de sistemas criptográficos robustos a ataques DPA e DEMA:
O desenvolvimento de um protótipo de uma biblioteca lógica assíncrona para prototipação em FPGAs;
A proposta de arquiteturas GALS pipeline.
Estas contribuições podem ser resumidas do seguinte modo:
Protótipo STTL:
Desenvolvida em standard cells por Razafindraibe, denominada de lógica segura em três trilhas (em inglês, Secure Triple Track Logic -- STTL).
Arquiteturas GALS Pipeline:
Em estágios de hardware de diferentes maneiras, validados em FPGA.
O restante deste documento organiza- se da seguinte forma.
O Capítulo 2 apresenta um estudo sobre o consumo de potência em circuitos com tecnologia CMOS e o fluxo de execução de um ataque DPA, mostrando as vulnerabilidades da tecnologia.
O Capítulo 3 apresenta trabalhos relacionados aos temas abordados nesta tese.
Em o Capítulo 4 discutese um conjunto de experimentos relacionados ao uso de lógica assíncrona orientada a FPGAs como contramedida a SCAs.
O Capítulo 5 detalha uma proposta geral de contramedida usando o paradigma GALS pipeline.
O Capítulo 6 conclui a Tese, destacando os trabalhos desenvolvidos no contexto do doutorado, as contribuições deste e apontando um conjunto de atividades de continuidade para o trabalho.
Este Capítulo explora alguns conceitos básicos relacionados à criptografia e criptoanálise.
Além disso, é apresentada uma classificação para os diversos tipos de criptoanálise, e um detalhamento sobre análises de consumo de potência, sua base teórica e sua dinâmica para revelar o conteúdo de uma mensagem.
Estas análises servirão como ferramenta para avaliar as arquiteturas propostas no presente trabalho quanto a a robustez a análises DPA e DEMA.
Utilizam- se aqui definições básicas de textos clássicos, tais como o livro de Schneier.
Suponha que um emissor deseja enviar uma mensagem a um receptor.
Suponha também que o emissor deseja enviar a mensagem de forma segura, tal que nenhum outro ente exceto o receptor possa ler a mensagem.
O processo de disfarçar uma mensagem para conseguir este intento é denominado de encriptação.
O processo de obter a mensagem original a partir de a mensagem encriptada se denomina decriptação.
Criptografia é a ciência responsável por manter mensagens seguras e ela é praticada por criptógrafos.
Os criptoanalistas são os praticantes da criptoanálise, a arte e a ciência de violar textos encriptados.
Diz- se que um criptoanalista realiza um ataque criptográfico quando este busca violar um texto encriptado.
O ramo da matemática que inclui criptografia e criptoanálise é a criptologia, e seus praticantes são denominados criptologistas.
A criptologia, palavra derivada do grego krypto (oculto) e logos (estudos), tem como objetivo a concepção e análise de mecanismos que permitem assegurar a integridade, autenticidade e a confidencialidade de dados em comunicações.
As Seções seguintes discutem sobre a criptografia e a criptoanálise.
A criptografia é a ciência que se baseia no estudo de princípios e técnicas por as quais mensagens podem ser transformadas de sua forma original para outra ilegível, de forma que possam ser conhecidas apenas por os entes comunicantes, emissor e receptor.
A operação de transformar os símbolos que compõem uma mensagem numa sucessão distinta de símbolos aplicando cálculos de modo que o apenas o receptor das mensagens, de posse de informação privilegiada, possa decifrar- las, é a essência da encriptação.
A criptografia é realizada geralmente com ajuda de uma chave criptográfica.
Uma chave criptográfica é um conjunto de símbolos que permite ao receptor da mensagem aplicar o processo de decriptação a um texto encriptado e assim obter a mensagem original.
A chave é ou está intimamente relacionada ao conceito de senha, muito difundido entre usuários finais de aplicações que empregam criptografia como forma de obter segurança na comunicação de dados sensíveis.
O objetivo principal de ataques criptográficos é descobrir chaves.
De acordo com a forma de disponibilização de chaves, os algoritmos de criptografia podem ser classificados em duas categorias:
Os algoritmos com chave privada e os algoritmos com chave pública.
Os algoritmos com chave privada, também conhecidos como algoritmos simétricos, têm por princípio a utilização de uma mesma chave para as operações de encriptação e decriptação de mensagens.
Esta última implica que as entidades que desejam se comunicar de maneira segura devem imperativamente trocar a chave e este é o principal inconveniente desta classe de algoritmos.
De entre os algoritmos de criptografia com chave privada, pode- se destacar o Des, o triplo Des (3-DES) e o AES.
Este trabalho concentrará esforços nesta categoria de algoritmos, particularmente no algoritmo Des, por ter uma grande aplicação em smart cards.
Embora seu sucessor AES opere com chaves de até 256 bits e também permita o emprego da abordagem proposta neste trabalho, a escolha por o algoritmo Des foi tomada com base na experiência de estudos de caso anteriores realizados por o grupo de pesquisa ao qual os Autores deste trabalho estão inseridos.
Os algoritmos com chave pública, também conhecidos como algoritmos assimétricos, têm por princípio a utilização de duas chaves:
Uma chave publicamente disponível para a encriptação e uma chave privada para a decriptação.
Em um sistema de criptografia deste tipo, usuários escolhem uma chave aleatória que apenas eles devem conhecer (a chave privada).
A partir de esta chave e de um algoritmo eles geram a chave pública.
Em seguida, os usuários disponibilizam a chave pública através de canais possivelmente não-seguros.
Qualquer emissor pode assim criptografar mensagens com a chave pública que somente poderão ser decriptadas por o receptor, de posse de sua chave privada.
A comunicação bidirecional pressupõe então o uso de duas chaves públicas e duas chaves privadas, todas distintas.
Este tipo de algoritmo foi estabelecido inicialmente por Diffie e Hellman, a fim de resolver o problema ligado à transferência de chaves secretas.
De entre os algoritmos de criptografia com chave pública pode- se destacar o RSA, o El Gamal e o DSA (em inglês, Digital Signature Algorithm).
A encriptação de mensagens via algoritmos de criptografia é necessária devido a os inúmeros meios disponíveis hoje para quebrar o sigilo de mensagens.
Uma tentativa de criptoanálise é comumente chamada de ataque criptográfico.
Entre os vários tipos de ataques existentes, é possível agrupar- los em duas grandes famílias, os ataques lógicos e os ataques físicos.
Inicialmente, revisam- se ataques lógicos, que exploram as vulnerabilidades matemáticas dos algoritmos.
Em seguida, revisam- se os ataques físicos, que exploram as vulnerabilidades físicas dos dispositivos eletrônicos que dão suporte à execução de algoritmos de criptografia, também referenciados neste trabalho como sistemas criptográficos.
De entre os ataques que exploram as vulnerabilidades matemáticas é possível dividilos em dois grupos, sendo eles lineares e diferenciais.
Os ataques lineares propostos por Matsui em 1993 estudam as relações existentes entre os bits de uma mensagem, os bits da mensagem encriptada correspondente, ou criptograma correspondente, e da chave utilizada na criptografia.
Estas relações são usadas para obter uma expressão linear capaz de predizer valores dos bits da chave quando muitas mensagens e os respectivos criptogramas são conhecidos.
Aumentado o número de pares mensagemcriptograma disponível, é possível melhorar a precisão da aproximação.
Os algoritmos de criptografia devem apresentar resistência a este tipo de ataque.
Em a literatura são encontradas outras propostas baseadas nos ataques lineares de Matsui, tais como O ataque diferencial proposto por Bihan e Shamir baseia- se em ataque em o qual o atacante dispõe de vários pares mensagem-criptograma escolhidos, cujas diferenças entre os respectivos criptogramas são analisadas.
Entende- se por diferença a operação de ou- exclusivo (XOR) entre dois criptogramas.
A criptoanálise é realizada sobre pares de criptogramas encriptados com a mesma chave e cujas mensagens correspondentes possuem certo valor particular de diferença.
O efeito desta diferença é analisado através de as` n'iterações do algoritmo resultando em parâmetros que permitem inferir possíveis valores da chave utilizada no processo de encriptação.
Estes parâmetros são expressos analiticamente através de probabilidades e são usados como indicadores para tomada de decisão de qual chave foi utilizada para cifrar a mensagem criptoanalisada.
O método fornece como resultado um conjunto de probabilidades associadas respectivamente a um conjunto de chaves.
A decisão por a chave correta é feita escolhendo- se aquela cuja probabilidade é a de maior valor.
Em a literatura são encontradas diversas propostas baseadas nos ataques diferenciais de Bihan, tais como Em os últimos anos vários tipos de ataques a sistemas criptográficos têm surgido com o propósito de revelar a chave criptográfica destes sistemas.
Entretanto, ataques usados para alcançar este objetivo apresentam- se de várias maneiras se diferenciando significativamente em termos de custo, tempo, equipamentos necessários e conhecimento técnico.
Em conseqüência, existem vários modos de classificar- los.
Em a literatura é comumente encontrada a classificação segundo dois critérios ortogonais.
O primeiro destes divide os ataques com relação a o controle sobre o dispositivo, classificando- os como passivos ou ativos.
Ataques Passivos: O sistema criptográfico opera normalmente dentro de suas especificações.
O ataque consiste apenas em observar as propriedades físicas do dispositivo tais como tempo de execução, consumo de potência e radiação eletromagnética.
Ataques Ativos: Estes ataques controlam as entradas e/ ou o ambiente onde o sistema criptográfico está inserido.
De este modo, os ataques adulteram dados de entrada e/ ou o ambiente de modo a explorar anormalidades produzidas nestes casos por o sistema.
O segundo critério classifica os ataques segundo a interface do dispositivo explorada por os atacantes.
Os sistemas criptográficos têm diversas interfaces físicas e lógicas.
Algumas destas podem ser acessadas facilmente enquanto outras exigem equipamentos especiais.
Com base nestas interfaces os ataques físicos podem ser classificados em três grupos:
Ataques invasivos:
Em estes ataques não existem limites para se manipular o dispositivo eletrônico visando revelar a chave secreta do sistema.
Tipicamente, inicia- se com a remoção do encapsulamento do dispositivo.
A seguir, componentes são acessados diretamente por sondas especiais para observar sinais num barramento (ataques passivos), por exemplo, ou provocar a alteração da funcionalidade do dispositivo num ataque ativo.
Estes ataques violam a integridade física do dispositivo, impossibilitando seu uso após o ataque.
Além disso, exigem equipamentos especiais, o que torna os ataques extremamente caros.
Exemplos deste ataques:
Ataque por sondagem consiste em espionar a atividade elétrica do componente extraído do circuito, com a ajuda de uma sonda.
Esta técnica permite recuperar os dados transitando dentro de o circuito, mas permite também impor valores lógicos em certos pontos deste.
Pode- se imaginar que com o controle do ambiente, o atacante pode estar medindo e deduzindo boa parte do segredo do circuito criptográfico.
Ataques por reconstrução de leiaute (engenharia reversa):
Consistem em estudar o componente eletrônico extraído, para determinar de maneira precisa a estrutura interna e assim deduzir seu funcionamento.
Para isto é necessário extrair as informações sobre o local exato de todos os transistores e de todas as conexões, compondo a estrutura do sistema, a fim de reconstruir a totalidade do seu leiaute.
Este ataque é muito raro, pois exige materiais sofisticados, pessoal altamente treinado e tem alto custo.
Ataques semi-invasivos:
Em estes ataques o dispositivo também tem seu encapsulamento removido.
Entretanto, diferentemente dos ataques invasivos, não existe contato elétrico com o circuito do dispositivo, ou seja, o circuito permanece intacto.
Tipicamente estes ataques não exigem equipamentos caros, porém os esforços para executar- los são relativamente altos, pois o processo de localizar a posição correta dos componentes para um ataque na superfície de um chip moderno exige tempo e conhecimentos especializados.
É possível citar como exemplo os seguintes ataques:
Ataques por injeção de falhas, introduzidos por Boneh Em 1997 e Skorobogatov e Anderson em, consistem em gerar intencionalmente falhas no criptosistema a fim de obter comportamentos anormais, pode- se então explorar os mesmos para revelar informações secretas.
No entanto, estes ataques precisam criar modelos de falhas, exigindo competências particulares e um conhecimento detalhado da estrutura interna do circuito.
Sua eficiência representa hoje um forte perigo para a segurança de criptosistemas.
Ataques eletromagnéticos eletromagnética produzida consistem por um em revelar circuito analisar criptográfico.
Ataques não-invasivos:
Em este ataques apenas as interfaces diretamente acessíveis são atacadas.
O dispositivo não é permanentemente alterado, fato que não deixa evidências de que um ataque tenha sido realizado.
A maioria destes ataques é realizada a custos reduzidos, se comparado a ataques invasivos.
Isto os torna uma séria ameaça à segurança de sistemas criptográficos.
Em particular ataques passivos e não-invasivos têm recebido uma grande atenção durante os últimos anos.
Estes ataques são freqüentemente referenciados como ataques a canais laterais (SCAs).
Prover soluções arquiteturais para aumentar a robustez contra estes tipos de ataque é o alvo do presente trabalho.
De fato, tais ataques consistem em explorar os canais laterais (tempo de cálculo, consumo de potência, radiação eletromagnética).
Estes canais laterais correlacionam- se com o estado interno do circuito, sendo possível a partir de eles extrair algumas informações secretas.
Alguns exemplos deste tipo de ataque:
Ataques por análise de tempo, propostos por Kocher em 1996 exploram a correlação entre os dados processados e o tempo de processamento durante as operações criptográficas.
Algoritmos criptográficos têm tempos de cálculo dependentes dos dados e da chave secreta.
Uma análise destes tempos pode permitir revelar o valor da chave secreta.
Ataques por análise do consumo de potência consistem em analisar o consumo de potência de dados manipulados num circuito a fim de extrair a chave criptográfica.
Este trabalho tem ênfase nos ataques por análise do consumo de potência e por análise da radiação eletromagnética como modo de avaliar a robustez das arquiteturas propostas.
Em este caso, as análises de radiação eletromagnéticas são não-invasivas, pois nenhum tipo de alteração do empacotamento do FPGA é realizado.
Os ataques por análise de tempo são ignorados no processo de avaliação deste trabalho.
Esta Seção detalha ataques por análise do consumo de potência.
Em um primeiro instante revisam- se características do consumo de potência em circuitos CMOS.
A seguir, detalham- se os ataques por análise diferenciais de potência.
Para entender o fundamento de ataques por análise de consumo de potência, em particular os ataques DPA, é necessário revisar as características de consumo de potência da tecnologia CMOS.
Como ilustrado na Figura 2.1, um circuito CMOS síncrono é tipicamente composto de estágios, formados por portas lógicas e delimitado por registradores, que por sua vez são compostos de transistores operando como chaves.
A potência consumida em tais circuitos depende da atividade de chaveamento de seus componentes.
Esta atividade é promovida por a variação dos dados de entrada no circuito, bem como por a atividade na rede de distribuição de relógio.
Inicialmente, assumese que o circuito encontra- se num estado de equilíbrio, onde todos os sinais de entrada têm valores constantes e estão estáveis, nenhum sinal interno encontra- se chaveando e os sinais de saída estão também em valores estáveis.
Variações nos sinais de entrada provocam o chaveamento de sinais internos na primeira parte do estágio, o registrador de entrada.
Caso haja então uma transição do sinal de relógio, as mudanças nas entradas propagam- se para a segunda parte do estágio (o circuito combinacional) e até a terceira parte do estágio (o registrador de saída).
Uma transição posterior do sinal de relógio pode então propagar valores para fora de o estágio, alterando os sinais da saída do estágio.
A sequência de atividades de chaveamento descrita se tomada em nível de todo o circuito, com todos seus estágios considerados, é definida como uma transição de estado do circuito.
De este modo, pode- se afirmar que o consumo de potência de um circuito depende das transições de estado e também do estado em que o mesmo se encontra.
As fontes de dissipação de potência num circuito CMOS podem ser divididas em duas classes de contribuições:
Componentes estáticos e componentes dinâmicos.
No caso de uma porta CMOS, componentes estáticos correspondem à potência dissipada quando este se encontra em estado de equilíbrio, ou seja, quando não passa por transições nas suas entradas ou saídas.
Idealmente, o consumo de potência estática de um circuito lógico CMOS deveria ser nulo na ausência de atividade.
Contudo, transistores não são chaves perfeitas.
Mesmo em estado de equilíbrio, eles apresentam uma corrente de fuga responsável por a dissipação estática de potência.
Em tecnologias mais recentes esta componente de dissipação tem se tornado cada vez mais relevante.
A potência dinâmica costumava ser e em alguns casos ainda é a principal fonte de dissipação nos circuitos CMOS.
Esta potência é dissipada durante as transições de estado do circuito.
É possível admitir que a potência estática seja insignificante diante de a potência dinâmica enquanto o circuito está em plena atividade tal como, por exemplo, a execução de uma encriptação de dados.
Por conseqüência, é possível exprimir a potência consumida por uma porta CMOS de acordo com a equação:
P $= CLVdd 2 F Em esta Equação, representa a taxa de atividade da porta, Cl representa a carga capacitiva desta, F é a freqüência de operação e Vdd corresponde à tensão de alimentação.
Em esta Seção, a fim de limitar o escopo das análises, serão consideradas apenas as cargas/ descargas capacitivas como as principais fontes de consumo de potência.
Em tecnologias de até 130 nm é possível afirmar que a potência estática num circuito CMOS é insignificante em relação a a potência dinâmica.
A partir de as tecnologias de 65 nm ou menores o consumo de potência estático pode tornar- se a maior componente no consumo de potência total de um circuito.
Isto leva a crer que os efeitos causados por a encriptação de dados em criptosistemas correspondem à menor parte do consumo total de potência do sistema.
Logo se espera que as fugas de informação por este canal lateral sejam atenuadas em tecnologias com dimensões inferiores a 65 nm.
Os ataques por análise do consumo exploram as variações instantâneas de corrente de um circuito.
Em esta Seção são apresentados alguns parâmetros do ambiente em que o circuito se encontra e parâmetros da tecnologia de concepção que influenciam nas variações do consumo de corrente das portas lógicas CMOS, seguindo estudo O traço de consumo de corrente de uma porta lógica CMOS depende de parâmetros do ambiente em o qual a porta está inserida e da tecnologia adotada, conforme.
Entre os parâmetros do ambiente inclui- se a rampa de entrada, definida como o tempo em o qual um sinal transita do nível lógico` 0 'para` 1' ou de` 1 'para` 0', e a carga ou, em inglês, fan-out, definido como o número máximo de portas de entrada às quais uma porta de saída pode conectar- se.
Entre os parâmetros da tecnologia de concepção estão a topologia do circuito e o dimensionamento dos transistores, entre outros.
A fim de identificar o modo como alguns destes parâmetros influenciam os traços de corrente, Razafindraibe Em desenvolveram simulações elétricas sobre circuitos CMOS com diferentes complexidades e dimensões, usando uma tecnologia 130 nm.
O circuito inversor CMOS mostrado na Figura 2.2 foi usado como referência nestes estudos.
Primeiramente, analisa- se o impacto do parâmetro carga (Cl) sobre os traços de corrente.
Para simular a ação deste parâmetro no consumo de corrente do circuito utilizase um capacitor com valor Cl que reproduz o comportamento elétrico de outras portas de entrada conectadas à saída do circuito.
Em este caso, o parâmetro carga é diretamente proporcional à capacitância Cl..
Simulou-se o circuito inversor CMOS submetido às seguintes condições:
Rampa de entrada constante e a diferentes cargas.
Em um segundo instante, observou- se o efeito do parâmetro rampa de entrada sobre o traço de corrente, com o inversor submetido a uma carga constante e a diferentes valores de rampa de entrada.
Os resultados da simulação aparecem na Figura 2.3.
Vin representa as possíveis transições de entrada, entre os instantes 0 e 1 ns, e entre 3 ns e 4 ns.
As curvas Vout representam a tensão de saída do circuito para as diferentes cargas.
I (Vdd) representa a corrente consumida durante as transições.
A análise destes resultados permite concluir que as transições e produzem circulação de corrente com amplitudes significativas.
A partir de a Figura 2.3, percebe- se que as transições de entrada produzem consumo de corrente significativamente superiores às transições.
O traço de corrente de um inversor depende significativamente da capacitância de carga Cl..
A Figura 2.3 representa a evolução temporal das correntes de carga e descarga para diferentes valores de Cl..
Contudo, a partir de certo valor de Cl, a amplitude máxima de corrente não depende mais do valor de Cl..
Em este instante, o valor máximo de corrente é limitado por as características de condução de corrente dos transistores.
Em Tension conseqüência, os traços de corrente têm tendência a se espalhar no tempo de maneira proporcional ao valor da carga.
Os traços de corrente são muito sensíveis a variações de inclinação da rampa de entrada.
Como mostra a Figura 2.4, a amplitude máxima de corrente é diretamente proporcional à inclinação da rampa de entrada.
Por outro lado, uma diminuição da inclinação da rampa de entrada se traduz num deslocamento temporal da resposta do inversor e por conseqüência, dos traços de corrente.
A fim de avaliar a influência de conexões série/ paralelo de transistores numa porta lógica sobre o traço de corrente, Razafindraibe Utilizaram uma porta lógica NAND de duas entradas (NAND-2) da Figura 2.5 como estudo de caso.
Durante as simulações, a rampa de entrada e a carga são constantes.
Os parâmetros que variam neste caso são apenas a ordem de chegada dos sinais e os valores de entrada.
Como é possível observar na Figura 2.6 o consumo de potência dinâmica é maior quando ambas as entradas chaveiam simultaneamente do nível lógico 1 para 0.
Em este caso, é possível relacionar o consumo de potência dinâmica de circuitos CMOS ao peso Hamming da porta de saída de dados do circuito.
Por definição, peso Hamming (em inglês, Hamming Weight ­ HW) é o número de bits diferentes do nível lógico` 0'.
Logo, é possível modelar o consumo de potência em circuitos CMOS segundo o peso Hamming das saídas do circuito em análise.
A fim de observar o impacto da atividade de um circuito sobre o traço de corrente, Razafindraibe Avaliam o consumo de corrente de um circuito composto por um grupo de portas NAND-2, conforme ilustra a Figura 2.7.
As condições de controle, rampa de entrada e cargas capacitivas são mantidas constantes.
Apenas à cada período de relógio.
Em este estudo de caso, pressupõe- se que as simulações se desenvolvem durante um período de relógio e considera- se que no instante t $= 0s, todas as saídas são` 0'.
Em estas condições, a taxa de atividade é simplesmente o número de portas comutando dividido por o número total de portas.
O número de portas comutando representa seu peso Hamming.
Como é de se esperar, quanto maior for a atividade do circuito maior será a amplitude de corrente, ou seja, a amplitude de corrente num circuito é proporcional ao peso Hamming dos dados processados.
Introduzidos por Kocher Em, os ataques por análise do consumo de potência exploram essencialmente duas dependências:
A dependência de dados e a dependência de operações.
Kocher Observaram que os traços de consumo de potência diferem para diferentes operações e para diferentes dados.
Os ataques que exploram estas dependências realizando uma interpretação direta de traços de potência medidos durante operações criptográficas são referenciados como ataques Spas.
Estes ataques exigem um conhecimento detalhado sobre o modo de implementação do algoritmo criptográfico executado no dispositivo atacado, além de exigirem conhecimento do algoritmo criptográfico.
Spas são úteis quando apenas um traço ou poucos traços de potência estão disponíveis para um conjunto de dados de entrada.
O ataque explora diferenças dentro de um traço causadas por dependências da chave.
Kocher Realizaram experimentos com o algoritmo Des para demonstrar a efetividade desta análise.
Detalhes sobre este algoritmo estão disponíveis no Anexo deste trabalho.
A Figura 2.8 representa o traço de corrente correspondente a um cálculo criptográfico com o Des:
A permutação inicial, as 16 rodadas e a permutação final são claramente identificáveis.
Em este caso, os ataques Spas possuem precisão para identificar as diferentes operações e suas instâncias de ocorrência.
Em a análise de Kocher, uma observação detalhada sobre as rodadas 2 e 3 permitiu identificar os traços de corrente dos blocos de geração de subchaves do algoritmo Des e concluir sobre diferentes operações tais como permutações e deslocamento à esquerda as quais a chave é submetida.
Além disso, a cada operação de deslocamento, um teste sobre o valor de um bit da chave é efetuado.
De acordo com o valor do bit, o traço de corrente de um salto condicional difere ligeiramente.
Por conseqüência, o atacante é capaz de concluir sobre o valor de certo número de bits da chave secreta.
Nota- se que as implementações do Des em software executando sobre um processador síncrono sem contramedidas são vulneráveis a ataques Spa.
Claro, o sucesso destes ataques exige um conhecimento detalhado do algoritmo de criptografia e a maneira através de a qual este é implementado.
Atualmente, com o surgimento de novas técnicas de contramedida, o ataque Spa não representa mais uma ameaça séria a criptosistemas.
Por outro lado, é possível verificar que este ataque serve como uma etapa preliminar em análises diferenciais de potência.
O ataque por análise diferencial de potência (DPA) é o mais popular ataque por consumo de potência.
Isto se deve ao fato de não exigir conhecimento detalhado sobre o dispositivo atacado.
Além disso, ele pode revelar a chave secreta de um criptosistema mesmo na presença de perturbações elétricas causadas durante o processo de medição dos traços de potência.
Ao contrário de Spa, DPA exige um grande número de traços de potência para a análise.
A principal vantagem de DPA em relação a Spa é não precisar de conhecimento detalhado sobre o modo de implementação do algoritmo no dispositivo criptográfico.
O conhecimento do algoritmo executado por o criptosistema é suficiente para realizar a análise.
Como tais algoritmos são tipicamente de domínio público, nota- se como ataques DPA podem ser efetivos na prática.
Além disso, o modo como os traços são analisados difere em cada um dos ataques.
Em Spa, o consumo de potência é analisado ao longo de o eixo do tempo, ou seja, o atacante busca encontrar padrões de consumo num único traço.
Já em DPA, a forma do traço ao longo de o eixo do tempo não é tão importante.
DPA analisa como o consumo de potência em instantes fixos de tempo depende dos dados processados.
Logo, é possível dizer que ataques DPA exploram as dependências de dados do consumo de potência dos criptosistemas.
Por estas razões, estes se tornam mais eficientes e ameaçadores à segurança de criptosistemas que ataques Spa.
Ao contrário de Spa, ataques DPA empregam uma estratégia genérica que é usada em todos os ataques.
Esta estratégia consiste de 5 etapas, descritas a seguir.
A primeira etapa de um ataque DPA é escolher um resultado intermediário do algoritmo criptográfico que será alvo do ataque.
Este resultado precisa ser uma função f (d, k), onde d é um dado conhecido e k é uma parte da chave criptográfica secreta.
Resultados intermediários que satisfazem esta condição podem ser usados para revelar k.
Em a maioria dos ataques, d é uma mensagem de entrada ou um criptograma de saída.
A segunda etapa de um ataque DPA é medir o consumo de potência do criptosistema enquanto este encripta ou decripta um conjunto de dados distintos D usando a mesma chave criptográfica.
Para cada encriptação ou decriptação executada, o atacante precisa conhecer o valor d correspondente ao dado que está envolvido no cálculo do resultado intermediário escolhido no Passo 1.
Estes valores de dados conhecidos são definidos por o vetor d $ ', onde di denota o valor do dado na iésima execução de encriptação ou decriptação.
Durante cada uma destas execuções o atacante armazena o traço de consumo de potência correspondente.
Estes traços são definidos como ti' $ , onde T denota o número de amostras de consumo de potência medido em cada traço.
O atacante mede um traço para cada dado di contido no conjunto D de dados.
Portanto os traços são armazenados numa matriz MT de tamanho D x T. É importante para os ataques DPA que os traços medidos sejam corretamente alinhados.
Isto significa que os valores de consumo de potência de cada coluna tj da matriz MT devem corresponder à execução das mesmas operações realizadas durante a encriptação ou decriptação.
Para obter o consumo de potência alinhado, o osciloscópio usado na medição deve ser disparado de modo que os traços de consumo de potência correspondam exatamente à mesma seqüência de operações durante cada encriptação ou decriptação executada.
O próximo passo do ataque é calcular o valor intermediário hipotético para todas as possibilidades de valores de k, de acordo com a função f (d, k).
Estes valores são definidos por o vetor k $ , onde K denota o número total de possibilidades de k.
Em o contexto de ataques DPA, é comum denominar este vetor como as hipóteses de chave.
Dado o vetor de dados d e as hipóteses de chave k, um atacante pode facilmente calcular todos os valores intermediários hipotéticos possíveis para f (d, k).
Estes cálculos mostrados genericamente por a Equação 2 resultam na matriz Mv de tamanho D x K. A primeira parte da Figura 2.9 ilustra esta etapa de cálculos.
Cada coluna j de Mv contém os resultados intermediários calculados com base na hipótese de chave kj.
É claro que uma coluna de Mv contém os valores intermediários reais calculados por o criptosistema durante as execuções de encriptação ou decriptação realizada no Passo 2.
Lembrando, o vetor k contém todas as escolhas possíveis para k.
Portanto, o valor de chave k da função f (d, k) usado por o criptosistema no Passo 2 é um elemento do vetor k.
Define- se o índice deste elemento como ck.
Portanto, kck é a chave realmente usada por o criptosistema.
O objetivo do ataque DPA é encontrar qual coluna de Mv contém os mesmos valores produzidos por f (d, k) durante a encriptação ou decriptação do vetor D. A próxima etapa do ataque DPA é aplicar um modelo de consumo de potência ao dispositivo atacado visando simular valores de consumo de potência a partir de os resultados hipotéticos obtidos no Passo 3, conforme mostrado na Figura 2.9.
Como discutido anteriormente em nível de portas lógicas, o consumo de potência de um circuito desenvolvido com tecnologia CMOS é proporcional ao peso Hamming dos resultados obtidos em sua porta de saída de dados.
Em esta etapa do ataque DPA, o atacante relaciona o consumo de potência do criptosistema com os pesos Hamming calculados a partir de os resultados hipotéticos intermediários vi, j obtidos no Passo 3.
Entretanto, este modelo de consumo de potência não é muito adequado para descrever o consumo de um circuito CMOS, pois o consumo em circuitos depende também das transições de sinais internos e não apenas do resultado obtido.
Este modelo é geralmente usado quando o atacante não conhece os dados consecutivamente aplicados à função escolhida no Passo 1.
Outro modelo de consumo de potência é a distância Hamming (HD).
Em este caso, a idéia básica é contar o número de transições e que ocorrem num circuito digital durante certo intervalo de tempo.
Este número de transições é usado para descrever o consumo de potência neste intervalo de tempo.
Definindo de outro modo, a distância Hamming entre dois resultados intermediários correspondente a HD $ , onde r0 e r1 representam seus respectivos pesos Hamming.
Em geral, o modelo HD pode ser usado para simular o consumo de potência de uma parte do criptosistema, tal como a função escolhida no Passo 1.
Para isso, o atacante deve conhecer os valores dos dados processados consecutivamente no criptosistema atacado.
Isto não implica que simulações baseadas no modelo HW são inúteis.
O consumo de potência em criptosistemas é apenas aproximado proporcionalmente ao número de transições que ocorrem no dispositivo.
O modelo HD assume que transições e apresentam o mesmo consumo de potência.
Em a prática, estas transições apresentam consumos diferentes como mostrado por os experimentos de Razafindraibe Discutidos na Seção anterior.
Em este caso, é possível assumir que em média, o consumo de potência é maior quando são obtidos resultados com valores de HW maiores em relação a valores HW menores.
Estes modelos são os mais utilizados para modelar o consumo de potência em ataques DPA.
Porém, outros modelos são também propostos para dispositivos com características específicas.
Tais modelos podem ser derivados dos modelos HD.
O modelo HD, por exemplo, assume que todos os n bits de um dispositivo contribuem igualmente para o consumo de potência, assumindo que as cargas dos n bits são iguais.
Mas se um atacante sabe que alguns bits consomem mais que outros, isto pode ser considerado no modelo, de modo a estender o modelo HD e tornar- lo mais específico para um dado dispositivo.
Outra possibilidade de modelar o consumo de potência com base no modelo HD é introduzir pesos diferentes para diferentes transições ocorridas num dispositivo.
Por exemplo, a transição pode ter um peso equivalente ao dobro da transição, já que experimentos como os de Razafindríbe Mostram que estas transições produzem consumos diferentes.
Kocher Em, simulam o consumo de potência do dispositivo atacado usando um modelo binário, ou seja, os resultados intermediários hipotéticos vi, j produzem Analisando apenas um bit de cada resultado intermediário hipotético vi, j, se o bit corresponde ao valor lógico` 1' apresenta um consumo maior, logo hi, j $= 1.
Caso contrário, se o bit corresponde ao valor lógico` 0' apresenta um consumo menor, e assim o coeficiente correspondente é hi, j $= 0.
Portanto, usando um destes modelos, o consumo de potência do criptosistema para cada valor intermediário hipotético vi, j é simulado de forma a obter- se um valor de consumo de potência hipotético hi, j.
A qualidade da simulação depende fortemente do conhecimento do atacante sobre o criptosistema analisado.
A melhor simulação é aquela que mais se aproxima das características do consumo de potência do criptosistema atacado.
Depois de calcular os valores intermediários hipóteses Mv e obter os respectivos valores de consumo de potência MH a partir de um dado modelo de potência, a última etapa do ataque tem como objetivo avaliar as hipóteses de chave.
Em este Passo, cada coluna hj de MH é comparada com cada coluna de tj da matriz MT.
Isto significa que o atacante compara os valores de consumo de potência hipotéticos de cada hipótese de chave com os traços coletados.
O resultado desta comparação é a matriz MR de tamanho K x T, onde cada elemento ri, j contém o resultado da comparação entre as colunas hj e tj.
A comparação é feita com base no método da diferença das médias, conforme proposto por Kocher Em.
Outros métodos para avaliação das chaves são também Os traços de potência correspondem ao consumo de potência do dispositivo enquanto este executa um algoritmo criptográfico usando diferentes dados de entrada.
O resultado intermediário escolhido no Passo 1 é uma parte deste algoritmo.
Portanto, o dispositivo precisa calcular o valor intermediário vck durante as diferentes execuções do algoritmo.
Conseqüentemente, os traços coletados dependem destes valores intermediários num mesmo instante de tempo.
Este instante no traço de potência é referenciado como ct, ou seja, a coluna tct contém os valores de consumo de potência que dependem dos valores intermediários vck.
Os valores de consumo de potência hipotéticos hck são simulados por o atacante com base nos valores vck.
Portanto, as colunas hck e tct são fortemente relacionadas.
Estas duas colunas conduzem a um valor alto em MR, ou seja, o maior valor da matriz MR é o valor rck, ct.
Todos os outros valores são menores porque as colunas de MH e MT não são fortemente relacionadas.
Um atacante pode revelar o índice da chave correta ck e o instante de tempo ct por simplesmente observar o valor mais alto na matriz MR.
Os índices deste valor são então o resultado do ataque DPA.
Kocher Propuseram o método da diferença das médias para avaliar as hipóteses de chaves.
Este método estabelece uma relação entre as colunas das matrizes MH e MT com base na seguinte observação.
Analisam a seqüência de zeros e uns dos coeficientes hi, j calculados no Passo 4.
Para verificar se uma hipótese de chave Ki está correta ou não, o atacante divide a matriz MT em duas matrizes MT0 e MT1 de acordo com os valores de hi.
A primeira matriz MT0 contém as linhas da matriz MT cujos coeficientes hi, j são zeros.
A segunda matriz MT1 contém todas as linhas restantes de MT.
A seguir, a média das linhas deve ser calculada, para da uma das matrizes.
O vetor m '0 i denota a média das linhas da matriz MT0 e m' 1 i denota a média das linhas da matriz MT1.
As hipóteses de chave ki são corretas se ocorrer uma diferença significativa entre m '0 i e m' 1 i no mesmo instante de tempo.
A diferença entre m '0 i e m' 1 i indica que existe uma correlação entre hck e as colunas de MT.
Esta diferença ocorre exatamente no instante em que o valor intermediário que corresponde a hck é processado.
Em todos os outros instantes a diferença entre os vetores é aproximadamente zero.
No caso de uma hipótese não correta, a diferença entre m '0 i e m' 1 i é próxima de zero em todos os instantes de tempo.
O resultado de um ataque DPA baseado no método da diferença é uma matriz MR onde cada linha de MR corresponde às diferenças entre as médias dos vetores m '0 i e m' 1 i de uma hipótese de chave.
É importante destacar a possibilidade de todos os valores contidos em MR serem aproximadamente os mesmos.
Em este caso, o atacante não coletou uma quantidade suficiente de traços para estimar a relação entre as colunas de MH e MT.
Quanto maior a quantidade de traços, mais elementos estão nas colunas, conseqüentemente mais preciso será o ataque.
Isto também implica que quanto mais medições e coletas de traços sejam feitas, as menores relações entre as colunas podem ser determinadas.
Os ataques DPA revisados e discutidos até então tem a propriedade de explorar apenas um valor intermediário do algoritmo alvo para revelar a chave criptográfica, pois apenas os dados de saída da primeira rodada ou os dados de entrada da última rodada são considerados durante as análises.
Estes ataques são também referenciados na literatura como ataques de primeira ordem.
Se vários valores intermediários são considerados para formular as hipóteses de chave, então os ataques são referenciados como ataques de alta ordem (do inglês, High Order DPA -- Ho DPA).
Prouff Em propuseram uma versão especializada do ataque DPA, chamada de DPA de segunda ordem (do inglês, Second Order DPA, ou SO-DPA).
Esta proposta é especializada em atacar criptosistemas protegidos por métodos de mascaramento de dados.
Esta análise combina as fugas de informações causadas por o processamento e por o mascaramento de dados, relacionando- as a fim de encontrar a chave secreta conforme proposto também em e.
Bevan e Knudsen propõem melhorias no ataque proposto por Kocher.
Os Autores apresentam justificativas para a ocorrência de picos em hipóteses incorretas de subchaves e propõem utilizar estas informações de modo a tornar os ataques mais eficientes, ou seja, reduzir o número de traços necessários para revelar a chave secreta.
Esta eficiência é obtida através de ataques multi-bits, onde a função seleção proposta por Kocher não é aplicada apenas a um bit por ataque e sim a combinações de dois ou mais bits.
De este modo, os consumos de potência são somados tornando os ataques mais rápidos e eficientes conforme demonstram seus resultados.
Brier Em propuseram outra especialização do ataque DPA denominada Análise por Correlação de Potência (do inglês, Correlation Power Analysis CPA).
CPA é uma análise DPA que emprega um modelo linear de consumo de potência aplicado sobre os dados manipulados.
Esta abordagem basicamente visa reduzir o problema de picos fantasmas durante os ataques DPA.
Classicamente utiliza- se o modelo peso Hamming para correlacionar dados manipulados e o consumo de potência conforme revisado na Seção 2.2.1.
Em este caso, utiliza- se o modelo de potência distância Hamming, cujo cálculo é realizado segundo a variação do número de bits com valor` 1' entre uma mensagem m e uma dada mensagem de referência mr estimada previamente segundo Brier Em.
Logo, o consumo de potência pode ser resumido por a equação W $= aH (m mr)+ b, onde` a'é um ganho escalar entre a distância HD` H'e a potência consumida` W'.
Já b inclui todas as variações de consumo decorrentes dos elementos que compõem o restante do circuito e que são independentes dos dados manipulados tais como variações de tensão (offsets), atrasos intrínsecos aos componentes e ruído.
Com base nesta Equação são obtidos fatores lineares de correlação que relacionam as variâncias dos termos considerados com a potência consumida medida.
Estes fatores de correlação são capazes de rejeitar falsas hipóteses de subchaves segundo o modelo de potência HD adotado como mostrado em.
Estas análises são utilizadas para avaliar a robustez das arquiteturas propostas neste trabalho.
Fahn e Pearson em propuseram a análise de potência por inferência (do inglês, Inferential Power Analysis -- IPA).
Esta análise se desenvolve basicamente em duas etapas.
Inicialmente realizam- se operações estatísticas tais como diferenciação entre traços de consumo e médias entre outras, aplicadas sobre uma grande quantidade de traços de consumo a fim de aprender detalhes de implementação, o que conduz à localização e identificação dos bits da chave criptográfica.
A etapa seguinte extrai a chave por inferência, a partir de poucos traços de potência, conforme.
Chari Em introduziram os ataques denominados em inglês Template Attacks.
Estes ataques probabilísticos assumem um modelo Gaussiano de ruído para definir e registrar os templates de traços DPAs relativos a um conjunto pré-definido de operações.
As informações são aprendidas e registradas segundo parâmetros tais como a média e uma matriz de covariância as quais são otimizadas para cada operação, de acordo com o princípio da probabilidade máxima.
A seguir, o ataque propriamente dito ocorre quando um traço relativo a uma operação tem suas propriedades estatísticas comparadas ao repositório de templates através de regras Bayesianas.
Estas regras classificam o traço de acordo com as probabilidades e atribuem ao traço a operação correspondente.
Pressupõe- se que o repositório de templates e os traços relativos ao canal lateral analisado sejam obtidos através do mesmo dispositivo.
Goubin em apresenta uma análise de potência refinada (do inglês, Refined Power Analysis -- RPA) que permite revelar a chave criptográfica de algoritmos que se baseiam na estrutura algébrica de curvas elípticas tal como o algoritmo ECC (do inglês, Eliptic Curve Cryptography) mesmo na presença de algumas contramedidas.
Outra técnica proposta por Akishita e Takagi em explora uma característica especial das curvas elípticas onde o consumo é nulo.
Este método é uma extensão de RPA proposto dados onde não exista coordenadas com valor nulo, o método proposto explora registradores laterais que podem obter o valor nulo e sobrepor algumas possíveis contramedidas.
Logo, estes tipos de análises não são uma ameaça à segurança de algoritmos tais como Des e AES.
Nagashima Em propuseram um método para realizar ataques DPA em criptosistemas que usam inserção de atrasos aleatoriamente como contramedida.
O método proposto realiza uma ressincronização de formas de onda de diferentes fases como pré-processamento ao ataque DPA.
Os Autores usam funções matemáticas normalmente usadas em tratamento de sinais, tais como a Transformada Discreta de Fourier para definir parâmetros que identificam a diferença de fase entre curvas de potência.
Embora seja uma interessante ferramenta de criptoanálise, o método limita- se apenas a analisar o deslocamento de tempo que um traço de consumo de potência sofre durante uma encriptação completa.
Uma revisão dos avanços em ataques DPA é realizada e resumida na Tabela 2.1.
Depois que Kocher Propuseram os ataques Te a, Spa e DPA, estes ataques sofreram algumas modificações ou receberam etapa de pré-processamento, porém sua essência permanece a mesma.
As criptoanálises que se baseiam em métodos probabilísticos tais como IPA e ataques por templates apresentam resultados apenas em ensaios com criptosistemas sem contramedidas.
Embora usem ferramentas e modelos estatísticos eficientes, na prática a aleatoriedade inserida por as arquiteturas propostas neste trabalho representa um desafio a este tipo de criptoanálise.
Os métodos RPA, ZPA são criptoanálises propostas para algoritmos que se baseiam em propriedades algébricas de curvas elípticas que por sua vez fogem o escopo do presente trabalho.
Já SO-DPA é proposta como um método de ataque DPA capaz de correlacionar mensagens, mensagens mascaradas e criptogramas para encontrar a chave secreta do sistema.
Como o trabalho proposto não usa este tipo de contramedida, esta abordagem não é adequada para realizar avaliação da robustez, tais como RPA e ZPA.
Os ataques propostos por Nagashima E Clavier Embora sejam especializados em criptoanalisar sistemas protegidos por a abordagem de inserção de aleatoriedade para ocultar a fuga de informações, os estudos de casos apresentados mostram ataques bem sucedidos apenas à inserções de aleatoriedade simples tal como instruções dummies (nenhuma operação realizada) e deslocamento temporal do processo de encriptação integral.
Em o presente trabalho a execução do algoritmo usando a arquitetura GALS com domínios de freqüência aleatórios representa um cenário desafiador para estes tipos de ataques.
Acredita- se que estas abordagens juntamente com métodos de ressincronização de curvas sejam o caminho para quebrar o sigilo de criptosistemas que empreguem aleatoriedade do modo como é proposto aqui.
Outra técnica importante de criptoanálise é o ataque por indução a falhas em criptosistemas.
Um atacante pode induzir falhas durante a computação do criptosistema e explorar o resultado defeituoso para extrair informação sobre a chave secreta.
As falhas podem ser caracterizadas como permanentes, se o dano causado ao dispositivo é definitivo, tal como forçar uma posição de memória para um valor constante, ou podem ser transientes, se o dispositivo sofrer distúrbios durante seu funcionamento em operações específicas.
Como exemplo de modos de inserção de falhas é possível citar o bombardeio radioativo, criar anormalidades na freqüência do relógio ou na tensão de alimentação do dispositivo, a geração de glitchs no barramento de dados, o aumento da temperatura, a emissão de luz sobre o silício e a imersão do circuito em campos eletromagnéticos.
Este método pode ser classificado como semi-invasivo ou não-invasivo dependendo de como a inserção de falhas no dispositivo é realizada.
Biham e Shamir propuseram análises diferenciais de falhas (do inglês, Differential Fault Analysis -- DFAs) destinadas a algoritmos de encriptação simétricos.
Os Autores usam o modelo de falha transiente aplicado aos registradores de uma implementação do algoritmo.
O método proposto é aplicado durante as operações de encriptação/ decriptação do criptosistema, causando a inversão do valor de um bit com uma pequena probabilidade de ocorrência, comportando- se como uma falha transiente.
As análises DFAs apresentam- se como um método eficiente e de baixo custo para atacar criptosistemas.
Estes métodos empregam, por exemplo, a variação da tensão de alimentação ou da freqüência do relógio do circuito.
O estudo e avaliação da robustez segundo este método não faz parte do escopo do presente trabalho.
Em a literatura encontram- se várias propostas para contramedir ataques por análise do consumo de potência a criptosistemas.
Estas propostas podem ser agrupadas em três grupos de acordo com a abordagem usada para imunização.
A primeira abordagem consiste em introduzir ruído nas medidas de consumo de potência.
A idéia básica aqui é reduzir a relação sinal-ruído, de modo a impossibilitar ou inviabilizar na prática tentativas de correlacionar o consumo do criptosistema com os dados processados.
Uma segunda abordagem consiste em mascarar os dados processados, e assim inviabilizar a realização de análises de correlação.
Finalmente, a terceira abordagem concentra esforços para obter um consumo de potência equilibrado ou uniforme para qualquer seqüência de valores de dados de entrada.
Em a literatura encontram- se também trabalhos específicos a fim de imunizar criptosistemas a ataques por SEMA/ DEMA e indução a falhas.
Como prevenção a ataques por indução a falhas não é o alvo deste trabalho, a revisão destes temas é apresentada de forma mais sucinta, apenas visando apresentar aos leitores outra área relevante de pesquisa.
Já os ataques SEMA/ DEMA por serem mais recentes possuem poucos trabalhos propondo métodos de prevenção.
Além disso, como a radiação eletromagnética está diretamente relacionada à variação de corrente no circuito, os métodos propostos para imunizar circuitos a ataques por consumo de potência também imunizam ou no mínimo reduzem as fugas de informações através das radiações eletromagnéticas.
Em o restante deste Capítulo apresenta- se uma revisão de propostas subdivididas de acordo com o tipo de abordagem usada para imunizar criptosistemas contra ataques DPA.
O objetivo de toda contramedida é tornar a fuga de informação por canais secundários de um sistema criptográfico independente dos dados processados.
O mascaramento atinge este objetivo através da aleatorização dos valores intermediários processados por o sistema.
Uma vantagem desta abordagem é poder ser implementada no nível do algoritmo sem mudar as características de consumo do dispositivo.
Ou seja, mesmo o dispositivo apresentando consumo de potência dependente de dados, o método consegue descorrelacionar o consumo dos dados manipulados.
Apesar disso, na literatura são encontradas propostas de mascaramento aplicadas em nível de porta lógicas tal como sugerido em e.
A seguir são revisadas diferentes propostas para mascaramento de dados.
Pramstaller Em propõem um método para mascarar dados no algoritmo AES e o comparam com outras duas propostas.
O principal desafio encontrado em ambas as propostas é evitar a fuga de informações na operação SubBytes realizada maiores detalhes no Anexo.
Cada SBOX realiza operações sobre uma porção de 8 bits de dados de entrada, em cada uma das rodadas do AES.
SubBytes realiza transformações não-lineares em campos de Galois (do inglês, Galois Field -- GF).
As transformações consistem num inverso multiplicativo1 em GF e uma transformação afim2.
O primeiro trabalho revisado, proposto por Akkar e Giraud realiza uma operação de soma de 128 bits entre dado e máscara (um número gerado aleatoriamente), operação denominada máscara aditiva.
A máscara aditiva é removida antes da inversão de bytes e substituída por uma máscara multiplicativa (operação de multiplicação envolvendo o dado e a uma nova máscara) também de 128 bits.
Após a inversão de byte, a máscara multiplicativa é removida e a máscara aditiva é re-introduzida.
Este método introduz custo em área significativo, pois é necessário gerar duas máscaras aleatórias por operação de encriptação/ decriptação.
Este método é vulnerável a ataques conhecidos como Valor Zero.
Tais ataques exploram um caso específico onde os valores parciais do dado e da chave são iguais.
Em a proposta de Trichina, a máscara aditiva é reusada, excluindo o uso da máscara multiplicativa.
Isto reduz consideravelmente o custo de cálculo.
Entretanto, uma nova máscara é exigida para cada execução da rodada para garantir que a máscara não seja zero.
Por outro lado, isto não soluciona o problema do valor zero.
A proposta de Pramstaller É denominada de IAIK por os Autores.
Este método não remove a máscara aditiva antes da inversão de byte.
A saída deste passo é portanto (Dado+ Máscara) 1, o multiplicativo inverso do dado mascarado.
O método IAIK obtém o resultado esperado por calcular termos de correção em paralelo.
Aritmética sobre GFs é usada extensamente nesta abordagem e a inversão de byte em GF é primeiro mapeada para GF e novamente para GF.
Em este campo, a inversão pode ser computada efetivamente.
A abordagem IAIK é a única imune a ataques por Valor Zero.
Um criptosistema foi desenvolvido sob a forma de um ASIC em tecnologia CMOS de 0,25 µm empregando o algoritmo AES com a abordagem IAIK.
O custo para se obter uma maior resistência a ataques DPA foi uma redução entre 40 e 50% no desempenho devido a a inserção dos métodos de segurança.
Mesquita Em propõem o projeto de um criptosistema como uma arquitetura reconfigurável que mascara o consumo de potência.
Os Autores exploram propriedades das funções modulares de cálculo do algoritmo RSA.
Este algoritmo divide seu processamento em operações de multiplicação e exponenciação.
Os Autores propõem o uso da multiplicação modular proposta por Montgomery, que permite o uso do sistema numérico residual (em inglês, Residue Number System -- RNS) baseado no teorema chinês do resto (em inglês, Chinese Remainder Theorem -- CRT).
De este modo, o módulo de multiplicação é capaz de realizar cálculos em diferentes bases numéricas.
Isto produz um mascaramento dos dados processados e conseqüentemente o descorrelacionamento com o consumo de potência.
A arquitetura proposta, denominada de Arquitetura Reconfigurável Resistente a Fuga (do inglês, Leak Resistant Reconfigurable Architecture ­ LR2A) permite que em tempo de execução o módulo de multiplicação tenha sua base alterada aleatoriamente.
A arquitetura sofre penalidades em área, mas apresenta um bom desempenho com relação a criptosistemas existentes.
Em a matemática, o inverso multiplicativo de um número x é denotado por 1/ x ou x-1.
Este número quando multiplicado por x produz a identidade multiplicativa, ou seja, 1.
Em geometria, uma transformação afim entre dois espaços vetoriais consiste de uma transformação linear Ax seguida por uma translação+ b, ou seja, x Ax+ b.
Popp e Mangard, em propõem um novo estilo lógico para a concepção de criptosistemas resistentes a ataques DPA, aplicando mascaramento de dados em nível de portas lógicas.
O estilo, denominado por os Autores de Lógica Pré-Carregada com Trilha Dupla Mascarada (em inglês, Masked Dual Rail Pre--charge Logic -- MDPL), permite ao criptosistema um consumo de corrente aleatório, tendo como principal benefício a não necessidade de balancear o circuito complementado.
A idéia do mascaramento é tornar aleatório cada resultado intermediário de um circuito ou algoritmo aplicando a equação (dm $= d m), onde respectivamente dm é o dado mascarado, d o dado propriamente dito e m uma máscara aleatória.
Os experimentos realizados mostram elevados custos de potência e área comparados a circuitos projetados em lógica CMOS tradicional.
Motivados por estes resultados os Autores adicionam técnicas de redução de potência aos circuitos MDPL.
Durante os períodos em que os circuitos não desempenham operações consideradas críticas a ataques DPA, estas técnicas reduzem em torno de 4 vezes o consumo de potência em relação a um circuito CMOS tradicional equivalente.
Ghosh Em motivam- se por o fato de que circuitos de mascaramento usados para evitar fuga de informações através de SBOXes do AES apresentam vulnerabilidades devido a a ocorrência de glitchs.
Os Autores propõem um método de mascaramento para circuitos multiplicadores de SBOXes AES.
Em este método, as entradas dos multiplicadores são sincronizadas por componentes seqüenciais ou portas lógicas And controladas, visando evitar a fuga de informações.
Este trabalho destaca que a principal desvantagem das arquiteturas de mascaramento anteriormente propostas é a comutação desbalanceada dos circuitos.
A propagação dos sinais de entrada através de caminhos com diferentes comprimentos faz com que as portas lógicas &quot;XOR «realizem operações bit a bit com diferentes temporizações.
Isto provoca glitchs internos, responsáveis por fuga de informações.
A arquitetura proposta possui dois propósitos:
Huiping Propõem outro método para mascaramento de dados no algoritmo Des.
Os Autores introduzem no algoritmo operações lógicas e aritméticas (XOR e soma) realizadas entre dados e números gerados aleatoriamente, para produzir o mascaramento de dados em duas partes do algoritmo.
Estas operações são realizadas na função da rodada, no momento da entrada da subchave e antes da execução das SBOXes, conforme mostra a Figura 3.1.
Para aumentar o desempenho do algoritmo implementa- se um pipeline com a replicação das rodadas do algoritmo.
Os Autores propõem 4 configurações diferentes de pipelines com o Des, de modo a analisar o desempenho do processamento.
A partir de os resultados obtidos concluem que o pipeline com 16 estágios (uma rodada executada em cada estágio) obtém o melhor desempenho, porém o maior custo em área.
O circuito é implementado com tecnologia CMOS 0, 25 µm e possibilita operações em freqüências de até 100 MHz, atendendo a exigências de projetos de smart cards.
Ordu e Örs em são os primeiros a apresentar uma implementação em FPGA do algoritmo AES usando métodos para mascarar dados.
Os Autores propõem dois métodos diferentes, mascaramento aditivo de IAIK e mascaramento multiplicativo de Akkar.
Os autores realizaram três implementações do algoritmo AES:
Sem contramedidas;
AES com método de Akkar e;
AES com método de IAIK.
Comparações de custos em área e desempenho foram realizados no dispositivo FPGA Virtex-E 1000 da Xilinx.
Os resultados indicam que a implementação utilizando o método IAIK apresentou menor custo em área e menor período de relógio em relação a o método proposto por Akkar.
Haijun Em analisam várias contramedidas a ataques DPA e concluem que o método de mascaramento único (em inglês, Unique Masking Method UMM) não é eficiente para proteger criptosistemas contra ataques DPA de alta ordem.
Os Autores apresentam um novo método de proteção baseado em circuitos projetados com lógica diferencial dinâmica (em inglês, Simple Dynamic Differential Logic -- SDDL) proposta em, também conhecida na literatura como lógica de trilha dupla com dois estágios de funcionamento, pré-carga e avaliação (em inglês, Dual Rail Precharge Logic -- DPL).
Maiores detalhes sobre tais lógicas encontram- se na Seção 3.3.
Em um estudo de caso usando o algoritmo Des, o método propõe implementar em lógica SDDL as duas primeiras e duas últimas rodadas do algoritmo e manter o mascaramento UMM nas rodadas restantes, 3 a 14, visando aumentar a resistência a ataques DPA.
Os resultados demonstram que o método proposto aumenta a robustez de UMM a ataques DPA de alta ordem ao custo de um aumento de 29,3% no consumo de potência em relação a a implementação do algoritmo sem contramedidas.
Golic em propôs um novo método para mascaramento de dados em hardware no nível de portas lógicas.
Operações de mascaramento realizadas tanto em software quanto em hardware no nível de palavras não são eficientes para imunizar sistemas a ataques DPA, conforme demonstra o Autor.
Em o nível de bit, as operações podem ser desbalanceadas, causando glitches durante o processamento e sendo dependente dos dados de entrada.
O método proposto é classificado por o Autor como mascaramento lógico, e pode ser aplicado a algoritmos criptográficos em geral e a técnicas de conversão de operações lógicas e aritméticas de mascaramento.
O método proposto é baseado no uso de multiplexadores (MUXs), destinados a balancear em nível de bit operações de mascaramento.
Mostra- se a possibilidade de implementação do método usando apenas portas lógicas NAND.
Como vantagens, o método apresenta uma redução em área e latência em relação a técnicas de mascaramento similares tal como.
Por outro lado, o método mostra- se eficiente apenas a ataques DPA de primeira ordem, sendo vulneráveis a ataques de alta ordem (Ho DPA).
Ghellar e Lubaszewski em propõem uma nova implementação do algoritmo criptográfico AES para resistir a ataques DPA.
O método proposto visa mascarar os dados com base em propriedades da estrutura de campos de Galois, mais precisamente em GF.
Resumidamente, campos de Galois possuem elementos representados por dois algarismos, permitem apenas duas operações binárias (soma e multiplicação) e, para o caso específico GF (28), contém 256 elementos.
Uma propriedade desta estrutura é o fato de que campos GF (n) com mesmo número de elementos são isomórficos entre si.
De este modo, o mascaramento de dados é realizado através da definição de uma função de mapeamento para converter representações existentes em módulos da rodada do AES.
Como vantagem do método, destaca- se um aumento por o fator de 240 na complexidade dos ataques.
Em contrapartida, o hardware adicional aumenta 295% em área e reduz a freqüência de operação em 60% em relação a o algoritmo original.
Goodwin e Wilson em apresentam uma modificação simples na implementação do algoritmo AES sem alterar suas características de funcionamento como forma de mascarar dados e evitar ataques DPA.
Em o AES, a chave criptográfica é expandida inicialmente e a cada nova rodada de execução do algoritmo utiliza- se o valor da chave da rodada anterior.
Em o sistema proposto por os Autores um módulo de controle gera chaves a cada rodada do AES.
O método proposto foi implementado em FPGA e os resultados mostram um aumento da robustez a um custo baixo em área em relação a o algoritmo sem contramedida.
Por outro lado, a complexidade do módulo de geração empregado reduz significativamente a freqüência máxima de operação do relógio e por conseqüência causa uma redução de 30% na vazão do criptosistema.
A o analisar as propostas aqui discutidas cujo resumo encontra- se na Tabela 3.1, é possível concluir que o uso de mascaramento em nível algorítmico causa um aumento considerável na latência de processamento, sofrendo também penalidades quanto a o consumo de potência e área.
O uso desta contramedida em nível de circuito também eleva os custos em área e o tempo de execução.
A título de exceção é possível citar.
Apesar de a lógica proposta impor um aumento considerável no consumo de potência, os Autores conseguem adaptar estratégias que reduzem substancialmente o consumo de potência do circuito.
O método proposto nesta tese usa inserção de aleatoriedade e ruído no processamento para ocultar a fuga de informações, diferenciando- se dos demais trabalhos que visam modificar os dados de entrada durante o processamento de operações do algoritmo vulneráveis a ataques.
Conforme revisado, a presente tese apresenta estudos de caso com o algoritmo Des como os trabalhos de e protótipos em FPGA conforme, e.
Os custos da segurança obtida por os métodos propostos e por a tese aqui proposta se resumem a um aumento de área e latência.
Porém, a tese proposta possui a vantagem do aumento da vazão de dados obtida por a implementação pipeline.
Este método tem como objetivo construir criptosistemas com consumo de potência aleatório.
Isto significa que a cada ciclo de relógio uma quantidade aleatória de potência é consumida por o sistema.
Obter um consumo de potência idealmente aleatório não é possível na prática.
Entretanto, existem várias propostas de como se aproximar do ideal.
Em a literatura são encontradas propostas que afetam o tempo de execução do algoritmo criptográfico e propostas que tornam o consumo de potência aleatório, ou seja, inserem uma atividade de chaveamento aleatória no circuito com consumo de potência dominante sobre o consumo correspondente à execução do algoritmo.
Em engenharia elétrica, a qualidade de um sinal medido pode ser definida como a razão existente entre o sinal pretendido e interferências sobre este sinal.
A esta razão dáse o nome de relação sinal-ruído (em inglês, signal to noise ratio -- SNR).
Esta relação é definida como a razão entre as potências de um sinal contendo algum tipo de informação (Psignal) e do ruído (Pnoise), conforme indicado na Equação 1.
Em termos de segurança, o chaveamento aleatório produz um ruído que reduz a relação sinal-ruído, aumentando a complexidade das análises de correlação.
Em um caso ideal, o SNR deve ser zero, obtido teoricamente caso a potência do ruído inserido tenda a um valor infinito.
Em a prática, a potência de ruído pode ser elevada de modo a atender aos requisitos de segurança, sem elevar excessivamente o consumo de potência do dispositivo.
A seguir apresentam- se algumas propostas que empregam aleatoriedade para contramedir ataques DPA.
Benini Em propuseram uma combinação de técnicas de redução de potência e de controle da ativação do sinal de relógio, visando introduzir aleatoriedade significativa no traço de potência sem aumentar e, em alguns casos até reduzindo, o consumo de potência de sistemas.
De este modo, a técnica introduz ruído ao processamento de operações criptográficas.
A maior contribuição dos Autores é não aumentar o consumo de potência com o uso da técnica.
Esta abordagem reduz o consumo de potência se comparada com a implementação inicial.
A idéia básica é oferecer um conjunto de primitivas de hardware (macros) que podem ser instanciadas por os projetistas em muitos algoritmos.
Adicionalmente, permite- se ao projetista reduzir o consumo de potência de modo controlado em tempo de projeto.
Isto é uma característica desejável, pois encriptação e decriptação são freqüentemente exigidas em ambientes com restrições de potência.
Ciet Em propuseram o desenvolvimento de um criptosistema tomando como base o algoritmo RSA.
Para imunizar o criptosistema contra ataques DPA, os Autores empregam o algoritmo de multiplicação de Montgomery, que permite cálculos em diferentes bases numéricas usando como recurso números RNS.
A arquitetura proposta replica o módulo de multiplicação de 16 bits de largura, de modo a evitar perda de desempenho.
Diferentes estratégias de controle são aplicadas, de modo a se ter operações de 512 bits.
O processamento paralelo e o uso de diferentes bases numéricas para realizar a multiplicação contribuem para descorrelacionar o consumo de potência dos dados processados.
A arquitetura é validada em FPGA e os resultados mostram eficiência promissora, com tempos de processamento inferiores a 150 ms para uma chave de 1024 bits e com área adicional competitiva com outras implementações com contramedidas.
Standaert Em investigaram a vulnerabilidade da implementação em hardware do algoritmo AES usando pipelines.
Os Autores propuseram inicialmente o uso de pipeline no processamento de uma rodada do algoritmo, alterando o tempo de execução de 1 ciclo de relógio para 5 ciclos de relógios.
Após, propuseram um ataque CPA considerando os registradores internos adicionados às rodadas, bem como a atividade de chaveamento dos circuitos e o modelo de potência baseado na distância de Hamming.
O ataque realiza uma predição dos dados no conjunto de registradores a cada ciclo de relógio, estabelecendo uma matriz de predição para uma dada função de seleção escolhida.
Resultados teóricos (via simulação) e práticos demonstram ser possível revelar a chave secreta.
Assim, os Autores propõem uma arquitetura com pipeline interno à rodada do algoritmo e a replicação das rodadas deste para neutralizar os ataques.
Resultados teóricos mostram que a chave correta é revelada, porém com uma probabilidade muito baixa em relação a outras hipóteses de chaves.
O método usado utiliza o paradigma síncrono de projeto, facilitando ao atacante prever através de modelos adequados os instantes onde ocorrem os chaveamentos de dados no circuito.
Além disso, não foram mostrados resultados práticos de ataques.
A proposta sofre ainda um alto custo em área e latência, esta última compensada parcialmente por a utilização de pipeline.
Bucci Em propuseram uma nova contramedida a ataques DPA baseada na inserção de elementos de atraso no caminho de dados de criptosistemas.
Inicialmente, cada dispositivo é composto por um flip-flop tipo D (FFD), uma cadeia com n elementos de atraso em série (sendo n o número de elementos) e um multiplexador (MUX).
Um circuito de controle deve selecionar pseudo-aleatoriamente entre a saída direta do FFD ou a saída do FFD através da cadeia de atraso.
O dispositivo proposto pode ainda ser estendido para 2m possibilidades de cadeias de atraso (sendo m o número de cadeias), de modo a aumentar a aleatoriedade, por outro lado aumentando o dispêndio de hardware.
Este dispositivo é inserido em cada bit que compõe o caminho de dados do criptosistema, visando aleatorizar o consumo de potência e assim reduzir a correlação entre dados processados e potência consumida.
Estudos de caso foram realizados com um SBOX do algoritmo AES, usando tecnologia CMOS 0, 18 µm.
Os resultados foram obtidos por simulação usando a ferramenta Nanosim da Synopsys.
Como vantagens do método destacam- se o uso de um fluxo de projeto padrão e a não interferência na árvore de relógio do circuito.
Como desvantagem destaca- se a excessiva área adicional ao circuito.
Além disso, a inserção de atrasos aumenta a latência de processamento no criptosistema.
Yang Em apresentam uma nova abordagem contra ataques DPA, baseada no chaveamento aleatório de freqüência e tensão de alimentação (em inglês, Dynamic Voltage and Frequency Switching -- DVFS).
Os Autores reutilizam uma estrutura proposta originalmente para redução do consumo de energia em sistemas intrachip para descorrelacionar o consumo de potência dos dados processados.
Propõem- se três estratégias de controle diferentes.
Estas são classificadas por os Autores de acordo com seu nível de imunidade, sendo elas ingênua (naive), melhorada (improved) e avançada (advanced).
As duas primeiras aumentam a robustez, porém ainda apresentam vulnerabilidades.
A terceira revelou- se uma estratégia eficiente para bloquear ataques Spa e DPA.
Os resultados mostram que os traços de potência apresentam níveis de aleatoriedade suficiente para evitar a fuga de informações.
Além disso, o consumo médio de energia é reduzido em 27% na implementação do algoritmo Des.
Por outro lado, o criptosistema sofre um aumento de 16% em seu tempo de execução em relação a a implementação original.
Gürkaynak Em apresentam desafios e experiências de projeto com o paradigma GALS, tomando como estudo de caso o algoritmo criptográfico AES.
Os Autores propuseram o uso do paradigma GALS para aumentar a robustez de um criptosistema a ataques por consumo de potência.
O paradigma GALS oferece a projetistas recursos adicionais para implementar contramedidas a DPA.
Para demonstrar isto, os Autores desenvolveram o criptosistema Acácia.
Acácia, cuja estrutura é mostrada na Figura 3.2, implementa o algoritmo AES o qual executa sucessivamente uma rodada composta por 4 blocos funcionais:
AddroundKey, SubBytes, ShiftRows e MixColumns.
Acácia é subdividida num modulo chamado Goliath, composto por um caminho de dados de 128 bits e um gerador aleatório de chaves criptográficas.
Dois outros módulos menores denominados David completam o criptosistema.
Estes módulos são equipados com caminho de dados de 32 bits e realizam as operações SubBytes e Mixcolumns da rodada do AES.
Acácia é equipada com várias camadas de contramedidas a ataques DPA.
São elas:
Operações com dados falsos;
David contém dois operadores SubBytes de 8 bits.
Para executar a operação MixColumns são necessárias 4 operações SubBytes.
David pode escalonar aleatoriamente estas quatro operações, de forma que num dado ciclo de relógio, todos, apenas um, ou nenhum dos operadores SubBytes podem processar dados reais, enquanto os demais processam dados falsos.
Para cada rodada do AES são necessárias quatro operações MixColumns, sendo que estas podem ser executadas em qualquer ordem nos módulos David.
Todos os três caminhos de dados são implementados em módulos GALS com geradores de relógios próprios.
Os módulos GALS usam geradores de relógio que podem pausar o relógio a cada ciclo.
Com a combinação de todas as contramedidas listadas os Autores esperam oferecer um desafio maior a atacantes que usam técnicas de análise de consumo de potência.
O particionamento proposto aumenta significantemente a latência do sistema, o que se agrava ainda com o processamento de dados falsos para aleatorizar o consumo.
A interface assíncrona empregada requer o pausamento do relógio para troca de dados o que pode tornar o sistema vulnerável aos ataques Spa, visto que o relógio é o principal responsável por o consumo de um circuito.
Baddam e Zwolinski em apresentam uma análise e discussão sobre o uso de variação aleatória de freqüência e tensão (em inglês, Random Dynamic Voltage and Frequency Scaling -- RDVFS) como contramedida a ataques DPA.
Os Autores mostram que a partir de a análise do traçado de corrente é possível medir a freqüência de operação e/ ou o par tensão-freqüência durante o processo de criptografia.
Os Autores propõem um método que emprega apenas chaveamento aleatório de tensão e mantém constante a freqüência de operação do circuito.
Os resultados comprovam a eficiência do método contra ataques DPA e destacam como vantagem do método o fato deste não exigir mudanças na lógica e/ ou fluxo de projeto, bem como não implicar em custos adicionais significativos em área, desempenho ou consumo de potência.
A principal restrição da proposta é não permitir ao atacante ter acesso à conexão entre o gerador de números aleatórios e o controlador de voltagem.
Caso isso aconteça, o sistema perde sua aleatoriedade e se torna vulnerável a ataques DPA.
Outra restrição do método é que a taxa de mudança de voltagem seja menor que o tempo para processar o número de entradas mínimo, de modo a evitar um ataque DPA.
Se a taxa é muito próxima ao número mínimo, o atacante pode implementar um ataque bem sucedido antes da aleatoriedade ser introduzida.
Zafar e Har em propuseram um método para variação aleatória de freqüências em tempo de execução como forma de ocultar a fuga de informações num criptosistema contendo o algoritmo AES.
O módulo gerador do sinal de relógio com freqüência aleatória proposto seleciona uma nova freqüência de relógio a cada nova mensagem de entrada no criptosistema.
Com isso, a aleatoriedade é inserida de modo a descorrelacionar os dados processados e o consumo de potência medido.
O módulo gerador de relógio é composto por o oscilador proposto em sendo facilmente implementado em FPGA.
Lu Em investigam o uso da técnica de inserção de atrasos aleatórios (do inglês, Random Delay Insertion -- RDI) em projetos de criptosistemas destinados a FPGAs.
Os Autores provam teórica e praticamente que a técnica é efetiva contra ataques DPA e propõem parâmetros que podem ser utilizados para otimizar a segurança do projeto em termos de área, desempenho e potência consumida.
RDI foi inicialmente aplicada a criptosistemas microprocessados por Clavier Em para reduzir a correlação entre um modelo de potência previamente definido e o consumo de potência verdadeiro de um dispositivo.
Para tanto, uma cadeia de elementos de atrasos programáveis é adicionada ao caminho de dados, a fim de aleatorizar as curvas de potência na dimensão do tempo.
Este método é vulnerável quando aplicado em sistemas microprocessados, segundo os Autores.
Logo, Lu Propuseram a implementação em FPGA com o ajuste de parâmetros tais como variação do atraso inserido, seu desvio padrão e um fator de multiplicação k.
Os resultados obtidos mostram que RDI em FPGA é mais eficaz que a versão original do algoritmo implementada em FPGA.
A proposta sofre uma penalização em área de cerca de 100%, custo relativamente baixo comparado a outros métodos.
Kamoun Em propuseram um gerador de ruído como forma de descorrelacionar o consumo de potência dos dados processados num projeto orientado a FPGA.
Os Autores usam o algoritmo AES como estudo de caso para validação da técnica.
O gerador proposto nada mais é que a replicação das duas funções mais vulneráveis do AES, AddRoundKey e SubBytes.
Quando um dado é processado por o criptosistema, este é executado concorrentemente por estas operações, porém com chaves diferentes.
Um de eles utiliza a chave secreta e o gerador de ruído usa uma chave aleatória.
O método proposto apresenta baixo custo em área comparado a um método de mascaramento, embora estes módulos replicados representem a maior área do algoritmo.
Por outro lado, o método determina restrições nas etapas de posicionamento e roteamento para garantir que o processamento seja realmente paralelo.
Conforme a revisão das propostas resumidas na Tabela 3.2 é possível afirmar que esta abordagem depende da capacidade de reduzir a taxa SNR referente a o consumo de potência do criptosistema.
Quanto maior for a aleatoriedade inserida no sistema, ou seja, o ruído e as variações no tempo de execução de operações, melhor será a eficiência do método.
A necessidade de inserção de hardware extra para aumentar a aleatoriedade do consumo exige um aumento em área e conseqüentemente um aumento no consumo de potência do sistema.
Observa- se ainda que esta técnica pode ser desenvolvida tanto em hardware como em software.
Em os Autores usam uma estrutura de hardware para efetivamente atuar na variação de freqüência e tensão, porém o gerenciamento é executado em software num processador de propósito geral.
A Tese aqui proposta também implementa um pipeline tal como Standaert Propuseram, mas a diferença neste caso está no fato dos estágios do pipeline comunicarem- se assincronamente.
Em relação a a proposta de Ciet, a Tese aqui proposta diferencia- se por replicar o bloco de encriptação completo do algoritmo ao invés de apenas uma operação.
Em relação a os demais trabalhos, a Tese diferencia- se por inserir aleatoriedade no processamento de grupos de rodadas e com variação da freqüência do sinal do relógio em cada estágio a cada novo dado.
Tal como implementado nos estudos de caso, é possível processar o algoritmo Des em grupos de oito rodadas (pipeline 2 estágios), quatro rodadas (pipeline 4 estágios), duas rodadas (pipeline 8 estágios) e apenas 1 rodada em cada estágio numa arquitetura pipeline com 16 estágios.
Em oposição ao método revisado anteriormente, este método tem como objetivo construir criptosistemas com consumo de potência constante, ou ainda, independente dos dados processados.
Isto significa que a cada ciclo de relógio uma quantidade constante de potência é consumida por o sistema.
Em o caso ideal, apenas contramedidas que tornam o consumo de potência num criptosistema exatamente igual para todas operações e todos valores de dados oferecem perfeita proteção aos ataques DPA.
Em a prática existem duas estratégias para atingir este objetivo, a primeira de elas emprega estilos lógicos específicos no projeto do dispositivo criptográfico.
A segunda abordagem utiliza um filtro para remover as dependências de consumo de potência existentes nas operações e dados manipulados.
Esta segunda abordagem é pouco usada, a julgar por a quantidade de literatura disponível.
Por outro lado, encontra- se uma grande quantidade de propostas que utiliza a primeira abordagem.
A seguir são apresentadas algumas estratégias adotadas no projeto de circuitos com consumo de potência independente de dados.
Tiri Em propuseram um novo estilo de lógica CMOS que opera com consumo de potência independente de valores lógicos e da seqüência de dados.
O estilo denominado em inglês, Sense Amplifier Based Logic ou SABL, apresenta um consumo independente de dados seguindo dois princípios:
Ter apenas um evento de chaveamento por ciclo de relógio, independentemente da seqüência de dados de entrada e;
Ter uma carga capacitiva constante durante este evento de chaveamento.
SABL apresenta uma estrutura dinâmica e diferencial similar à lógica DCVSL (do inglês, Differential Cascode Voltage Switch Logic).
Entende- se por lógica dinâmica circuitos lógicos que alternam sucessivamente entre etapas de pré-carga (saídas forçadas para um valor lógico determinado) e avaliação (saída computada para o valor lógico correto para os valores de entrada).
Já a lógica diferencial é caracterizada por representar um bit de informação com dois fios que representam o valor de um bit quando em polaridades inversas ou a ausência de informação quando ambos os fios estão em` 0'.
Ou seja, esta é uma codificação denominada trilha dupla (do inglês, Dual Rail -- Dr).
Experimentos realizados com uma SBOX do algoritmo Kasumi demonstram uma variação de energia normalizada 116 vezes menor quando comparada a implementações CMOS típicas.
Por outro lado, a solução apresenta um custo de área e potência duas vezes maior.
Tiri e Verbauwhede em apresentam um novo método de projeto de standard cells que visa construir portas lógicas com consumo de potência uniforme para fluxos de projeto ASIC e FPGA.
Embora SABL tenha sido projetado para este fim, o fluxo de projeto de CIs com esta lógica requer a caracterização da nova biblioteca.
O método proposto por os Autores em evita o projeto de uma nova biblioteca completa, permitindo construir portas complexas a partir de bibliotecas já existentes, seguindo o comportamento de SABL.
A lógica diferencial dinâmica simples (em inglês, Simple Dynamic Differential Logic -- SDDL) é complementar à lógica convencional.
Além disso, adiciona um circuito de pré-carga às saídas tradicional e complementar de modo a forçar- las ao nível lógico` 0'.
Um projeto alternativo desta lógica é a implementação do circuito de pré-carga na entrada do circuito e a introdução de registradores na saída.
Esta alternativa é chamada de lógica diferencial dinâmica em onda (em inglês, Wave Dynamic Differential Logic -- WDDL).
Experimentos revelam que esta é eficiente na redução da variação do consumo de potência tanto em ASICs quanto em FPGAs, porém apresenta custos significativos em área, desempenho e consumo de potência.
Vahedi Em propuseram um circuito para uniformizar a corrente consumida em criptosistemas microprocessados.
O circuito proposto controla dinamicamente o consumo de potência de duas maneiras:
Injetando corrente;
E regulando a tensão sobre o criptosistema.
A corrente de alimentação é monitorada por um sensor que repassa as variações de corrente a um circuito responsável por manter a corrente consumida constante durante a encriptação de dados.
Quando as variações de corrente excedem os limites de atuação do circuito de controle de corrente, o regulador de tensão é acionado, de modo a redimensionar a tensão sobre o criptosistema fazendo com que o controle de corrente volte a atuar novamente sobre o criptosistema.
Como desvantagem do método, o circuito regulador de tensão deve limitar- se a operar com variações de tensão definidas por o fabricante do microprocessador.
Além disso, a redução da tensão de alimentação ocasiona atrasos no processamento de operações, e por conseqüência pode impor limites para o sistema atender aplicações que tenham restrições de tempo de processamento.
Razafindraibe Em realizaram uma avaliação detalhada da robustez da lógica em trilha dupla (Dr) a ataques DPA, e mostraram que a faixa de operação da lógica considerada efetivamente robusta é surpreendentemente pequena.
Dr não reduz suficientemente a correlação entre dados e o tempo de computação para caracterizar- se como uma contramedida robusta a ataques DPA.
Motivados por esta análise, os Autores propõem o uso de uma lógica alternativa à Dr, chamada de lógica segura em três trilhas (em inglês, Secure Triple Track Logic -- STTL).
Esta lógica utiliza uma terceira trilha para a validação de dados.
O circuito de validação é projetado com portas de baixa corrente, cujos atrasos de propagação são maiores que os do restante do circuito.
Esta característica garante que a validação seja gerada após a estabilização dos dados na saída do circuito e independentemente dos dados processados.
Outra característica de STTL é que o processamento do dado ocorre somente após a chegada de todos os sinais de validação envolvidos em seu processamento.
Isto garante a STTL uma tolerância ao desequilíbrio do tempo de propagação entre fios complementares na lógica Dr, um efeito naturalmente introduzido durante a etapa de posicionamento e roteamento do circuito.
A lógica de validação redundante garante que os dados sejam processados independentemente de tempo e do consumo de potência.
Esta lógica apresenta custos em área semelhantes aos da lógica Dr. Por outro lado, apresenta custos elevados em latência devido a o circuito de validação empregado adicionalmente.
Guilley Em propuseram uma investigação da contramedida WDDL proposta por Tiri e Verbauwhede.
Estes Autores apresentam uma metodologia de CAD para desenvolver WDDL em FPGA e a seguir realizam uma avaliação de robustez da lógica, usando como estudo de caso o algoritmo Des.
Os Autores discutem um método para redução de área e, além disso, propõem uma avaliação de algumas ferramentas de síntese de hardware.
Em este caso, os Autores propõem uma heurística para obter SBOXs menores em relação a as geradas automaticamente por o CAD de ASICs.
Em outro trabalho, Guilley Em propuseram um estudo do impacto causado por as etapas de posicionamento e roteamento em FPGA usando duas das principais ferramentas de CAD disponíveis no mercado, os ambientes de síntese física da Altera e da Xilinx.
Segundo os Autores, as lógicas DPL propostas preocupam- se em manter a quantidade de chaveamento de transistores constante para qualquer dado processado como forma de manter o consumo de potência independente dos dados processados.
Porém, os Autores destacam que o tempo de propagação dos sinais entre as trilhas duplas também tem importância.
Assim, realizaram experimentos de maneira a estabelecer restrições de posicionamento que atenuem os desequilíbrios de tempo de propagação dos sinais.
As ferramentas da Altera apresentaram melhores resultados que as ferramentas da Xilinx.
Embora o método seja interessante e apresente resultados relevantes, a tarefa de estabelecer restrições de roteamento em fios no fluxo de projeto de FPGAs é complexa, devido a o não-determinismo das ferramentas de síntese.
Esta é a principal desvantagem do método.
Kulikowski Em propuseram o uso de projetos com codificação em trilha dupla com consumo de potência independente de dados, tolerante ao desequilíbrio de interconexões entre portas lógicas e à variabilidade do processo de fabricação de circuitos integrados.
O projeto, denominado por os Autores em inglês Asynchronous Directional Latch Based Logic (ADLBL) permite um consumo independente de dados por a adição de um protocolo de descarga capacitiva direcional em trilha dupla, baseado no projeto de um latch que permite a descarga completa de ambas as trilhas.
Os resultados obtidos por simulação com a implementação de um submódulo do AES demonstram que mesmo na presença de desequilíbrio de capacitâncias no circuito, a lógica proposta resiste a ataques CPA, mostrando- se mais robusta que WDDL.
Apenas experimentos teóricos foram realizados, não sendo mostradas avaliações práticas de robustez da lógica proposta.
Os custos em área e potência são potencialmente elevados, mas os Autores não apresentam avaliação de área.
Além disso, a técnica requer o projeto de uma biblioteca específica.
Muresan e Gregori propuseram no uso de um circuito de proteção contra ataques DPA para criptosistemas tais como smart cards.
O circuito é baseado na técnica de regulação de corrente (em inglês, current flattening technique) inicialmente introduzida em e posteriormente usada em e.
Este circuito pode ser integrado ao mesmo chip ou ao mesmo encapsulamento do criptosistema.
O objetivo desta técnica é manter constante a corrente necessária para suprir o criptosistema, mascarando assim a dependência entre dados processados e a corrente consumida.
Como a corrente Is consumida por o criptosistema varia a cada dado processado, o circuito de proteção monitora Is e varia dinamicamente seu consumo de corrente If de modo que a corrente total consumida por a fonte Idd $= I s+ I f seja constante.
A principal vantagem desta proposta é a simplicidade da integração ao criptosistema existente, não sendo necessário o uso de biblioteca dedicada, reprojeto do criptosistema ou modificações em software.
Por outro lado, a técnica eleva o consumo de potência, visando uniformizar- lo.
Vahedi Em propuseram modificações no trabalho proposto em.
Em este último adiciona- se um método de chaveamento de freqüências para remediar as penalidades com atrasos no processamento.
Durante a operação de encriptação no processador, se a corrente não alcança a faixa de alimentação pré-definida para o processador, então um bloco de chaveamento de freqüência é ativado para encontrar a freqüência de operação mínima que satisfaça as exigências de tempo de processamento.
Este bloco aumenta a faixa de voltagens de alimentação do processador e traz dois benefícios:
Garante a funcionalidade do processador e;
Reduz os custos com consumo de potência.
Este novo método de projeto introduz um circuito de uniformização de corrente mais eficiente quanto a o consumo de potência.
A proposta apresenta um circuito com baixos custos de potência e área, sendo útil para aplicações tais como smart cards e dispositivos de comunicação móvel.
Rammohan Em apresentaram o estilo lógico denominado Lógica Diferencial e Dinâmica Complementar Reduzida (em inglês, Reduced Complementary Dynamic and Differential Logic -- RCDDL) para conceber sistemas imunes a ataques DPA.
Este estilo garante um consumo de potência uniforme para dados de entrada (mensagem e chaves criptográficas).
Em oposição aos estilos DPL existentes, que complementam todas as portas lógicas para gerar a saída diferencial, o estilo RCDDL propõe o reuso de portas lógicas, de modo a reduzir o número de portas na geração da lógica complementar para obter a saída diferencial.
Esta é a primeira proposta de reuso de portas lógicas para conceber uma lógica complementar e conseqüentemente reduzir o custo em área.
Duas são as exigências para garantir segurança a lógicas DPL:
Exatamente uma transição de saída a cada ciclo de relógio;
A capacitância total (de carga e de descarga) deve permanecer constante a cada ciclo de relógio.
Resultados experimentais sobre circuitos tais como o algoritmo criptográfico Des e circuitos sintéticos mostram que o uso de RCDDL produz um aumento significativo de resistência a ataques, apresentando uma redução de 42% na variação de corrente máxima em relação a a lógica WDDL e 11 vezes quando comparada à lógica CMOS ordinária.
Os resultados também mostram uma melhora no consumo médio de potência e área, mas com uma penalidade na latência, quando comparado à lógica WDDL.
Moradi Propuseram um estudo com a lógica de recuperação de energia, destinada à concepção de circuitos de baixo consumo de energia.
Esta lógica apresenta características tais como mecanismo de pipeline, consumo reduzido de energia dependente de dados e reduzida radiação de energia, o que é útil em áreas de aplicações tais como segurança de circuitos criptográficos embarcados.
Os Autores examinam a robustez de portas lógicas utilizando a lógica proposta bem como os custos de implementação associados.
Os resultados demonstram que a lógica proposta requer menos área em relação a lógicas destinadas à segurança tais como SABL e demais DPLs.
Por outro lado, os estudos mostram que a segurança da lógica é limitada por a freqüência de operação, apresentando melhores resultados em baixas freqüências.
A Tabela 3.3 resume as características dos trabalhos revisados na Seção 3.3.
O tipo de contramedida revisado aqui visa eliminar a fuga de informação ao tornar o consumo de potência constante e independente dos dados processados por o sistema criptográfico.
Com este objetivo, os Autores buscam propor novas estruturas lógicas para conceber circuitos com consumo uniforme, evitando tanto quanto possível as variações de cargas capacitivas no circuito.
A vantagem deste método é atuar diretamente na origem do problema, buscando alternativas para contornar os problemas inerentes das tecnologias atualmente disponíveis.
Por outro lado, os custos para se obter uma lógica com tais características são altos em termos de área, consumo e latência, principalmente devido as portas passarem a ter estruturas diferenciais (uso de lógica complementar) e comportamento dinâmico (ter obrigatoriamente o mesmo número de chaveamentos para cada dado processado), a fim de obter cargas capacitivas constantes.
Além disso, alterações no fluxo de projeto podem ser necessárias para permitir a concepção do circuito com uma nova biblioteca lógica.
O protótipo da lógica STTL proposto neste trabalho visa eliminar as fugas de informações através da uniformização do consumo de potência.
Ao contrário de as demais lógicas, STTL utiliza três trilhas para codificar um bit de informação.
A lógica mostra- se robusta a ataques por consumo de potência e por radiação eletromagnética.
Por outro lado, STTL sofre uma penalidade na latência da computação.
Quanto a o custo em área, STTL mostra- se similar às demais lógicas em trilha dupla.
Em esta Seção revisam- se alguns trabalhos que têm por objetivo imunizar criptosistemas contra a indução maliciosa de falhas.
De um modo geral, os trabalhos encontrados na literatura utilizam técnicas de detecção e correção de erros para evitar que criptogramas defeituosos sejam produzidos e conseqüentemente usados em ataques DFA.
Embora este tipo de ataque esteja fora de o escopo deste trabalho, uma pequena revisão da literatura é apresentada em caráter exploratório.
Yen e Wu em propuseram vários esquemas de detecção de erros implementados em hardware com base na verificação de redundância cíclica (do inglês, Cyclic Redundancy Check -- CRC).
O esquema proposto facilmente prevê a paridade do resultado de uma operação.
Os Autores usam o algoritmo AES como estudo de caso devido a sua estrutura orientada a bytes, o que facilita a previsão da paridade de operações internas, a partir de combinações lineares das paridades de entrada.
Como vantagens do método proposto, destaca- se sua escalabilidade, que permite sua aplicação em arquiteturas com caminho de dados de 8, 32 e 128 bits.
Além disso, o cálculo de paridade proposto é simétrico, ou seja, aplicável tanto a operações de encriptação como de decriptação.
O processo de geração de paridade em ambas operações é muito similar, o que traz benefícios na implementação da abordagem em hardware.
Regazzoni Em propuseram um estudo sobre o impacto na robustez de um criptosistema equipado com uma contramedida específica para um dado canal lateral, porém submetido a ataques por outro canal lateral.
Em este trabalho os Autores utilizam um circuito de detecção de erros, mais precisamente um detector de paridade, a fim de imunizar contra ataques por indução a falhas uma SBOX do algoritmo Kasumi, implementada em hardware com tecnologia CMOS 0, 18 µm.
Em simulação, a SBOX é então submetida a ataques DPA visando avaliar sua robustez.
Durante os experimentos, os Autores realizaram ataques em duas versões do circuito, uma com e outra sem contramedida.
Os resultados obtidos revelaram um aumento de 35% no número de chaves criptográficas descobertas em relação a o circuito sem a contramedida a DFA.
Embora a contramedida e o circuito adotados sejam pouco complexos, ou seja, apenas um submódulo do algoritmo Kasumi equipado com um detector de um bit de paridade, a questão levantada por os Autores é relevante.
O estudo revela que os esforços em pesquisa direcionados a encontrar uma solução a um tipo de ataque pode causar vulnerabilidades a outro método de ataque.
Maistri e Leveugle em propuseram o uso da técnica denominada em inglês double data rate (DDR) como base para implementar uma contramedida a ataques DFA.
Trabalhos anteriores revelaram que o uso de redundância temporal como método de detecção de falhas tem um impacto negativo na latência de um criptosistema.
Ou seja, a encriptação/ decriptação de um dado deve ser realizada duas vezes e por conseqüência ocasiona a redução da vazão no criptosistema.
Com o uso de DDR, os Autores contornam este problema, obtendo uma vazão compatível com implementações típicas do algoritmo.
Em relação a a robustez, o método mostra- se compatível com métodos anteriormente propostos.
Por outro lado, a área e a freqüência de operação sofrem penalizações neste método.
Bhasin Em apresentam um estudo provando que os estilos lógicos em trilha dupla dedicados a evitar ataques por consumo de potência são naturalmente imunes à maioria dos ataques por indução a falhas.
Os Autores apresentam um estudo de evaluation ou WDDL EE).
Este estilo lógico remove a fase de avaliação antecipada de modo a evitar vulnerabilidades provocadas por os tempos de propagação diferentes dos sinais diferenciais de entrada.
Estas lógicas garantem que em caso de problemas com os sinais de entrada presos a um valor nulo ou a chegada de um valor não especificado as saídas terão sempre valores nulos (os espaçadores da codificação Dr), o que evita ataques DFA.
O fluxo de projeto apresentado é dedicado a FPGAs Altera, mas pode ser adaptado para a FPGAs Xilinx segundo os Autores.
A proposta dos Autores apresenta penalidades em área e desempenho.
Este método é discutido aqui para complementar a revisão da literatura sobre os métodos de contramedidas para circuitos criptográficos.
O objetivo básico dos métodos é impedir que a ocorrência de falhas no processamento, seja qual for sua origem, propague um erro ao meio exterior do circuito.
Isto pode ser usado como informação privilegiada para a descoberta da chave secreta num circuito criptográfico.
A inclusão de métodos de detecção de erros ocorre geralmente em nível de hardware, a um custo de área e principalmente gerando aumento da latência.
Isto se deve principalmente ao uso de redundância temporal ou previsão de cálculo usado para evitar as falhas induzidas.
A Tabela 3.4 apresenta um resumo dos trabalhos revisados sobre métodos para evitar que análises por indução a falhas revelem os dados secretos de um criptosistema.
A presente Seção apresentou uma revisão de propostas para evitar a fuga de informações através de canais laterais em sistemas criptográficos.
Como revisado no de potência ocorre devido a o uso da tecnologia CMOS de concepção de circuitos integrados.
Após o problema ser definido nos experimentos realizados por Kocher, muitos trabalhos foram propostos visando encontrar uma solução a estes tipos de vulnerabilidades.
Em a literatura nota- se claramente que existem 3 métodos principais para evitar a correlação de dados com o consumo de potência de um dispositivo eletrônico.
São estes:
Mascaramento, inserção de aleatoriedade e uniformização do consumo de potência, conforme revisado neste Capítulo.
Dois destes métodos, mascaramento e inserção de aleatoriedade propõem soluções para impedir a correlação de dados com a fuga de informação no canal lateral analisado.
Em estes casos, as características de consumo da tecnologia CMOS são mantidas, porém os métodos provocam desordenamento do consumo, a fim de dificultar a tarefa dos atacantes.
Já o método de uniformização do consumo é aplicado diretamente na tecnologia de concepção visando obter circuitos com consumo de potência constante para qualquer dado processado.
Logicamente, a segurança proposta por os métodos apresenta custos em área, potência e desempenho.
De acordo com a revisão realizada, nenhum método aplicado isoladamente é capaz de garantir a segurança completa de um criptosistema, todos apresentam algum tipo de restrição.
Acredita- se que a combinação de métodos eleva o nível de segurança nestes sistemas.
Os estudos e avaliações de lógicas em trilha dupla (DPLs) propostas para imunizar sistemas criptográficos a ataques DPA revelaram vulnerabilidades conforme indicado por Razafindraibe Em.
Com base nestas avaliações os Autores propuseram novas diretivas para conceber a lógica STTL visando reforçar a imunidade de DPLs ao introduzir uma lógica de validação associada a um sinal de validade redundante conforme discutido anteriormente.
Uma biblioteca lógica é proposta em tecnologia CMOS e validada com um estudo de caso para prova de conceitos.
Os resultados obtidos por simulação mostram um processamento de dados quase independente do consumo de potência e do tempo de propagação motivando o uso da lógica STTL.
Para melhor avaliar a robustez da lógica proposta torna- se imprescindível submetêla a experimentos práticos de avaliação do consumo de potência.
O tempo de projeto e os custos para desenvolver estudos de caso usando um circuito integrado dedicado são elevados.
De este modo a prototipação usando dispositivos FPGA mostra- se atrativa para esta nova etapa de validação da lógica.
Este foi basicamente o objetivo do estágio sanduíche realizado no LIRMM -- (Université Montpellier II -- França) durante um ano e uma das contribuições originais desta tese.
Este Capítulo apresenta as atividades e os resultados obtidos com o desenvolvimento de protótipos da biblioteca STTL em FPGA bem como a avaliação de sua robustez às análises de consumo de potência e de radiação eletromagnéticas.
Como as propostas de contramedidas desenvolvidas neste trabalho baseiam- se no paradigma não-síncrono de projeto, a Seção 4.1 apresenta uma revisão específica de pressupostos empregados para o desenvolvimento de circuitos não-síncronos.
A Seção uma biblioteca de portas lógicas STTL é mostrada na Seção 4.3.
Um estudo de caso com a implementação do algoritmo Des usando a lógica STTL é apresentado na Seção 4.4.
Em as Seções 4.5 e 4.6 são abordados os sistemas de medição e aquisição de dados usados para as análises.
As avaliações de robustez a ataques por consumo de potência e por radiação eletromagnética estão discutidas nas Seções 4.7 e 4.8.
Com o objetivo de superar as limitações do projeto síncrono, diversos grupos de pesquisa estão retomando o interesse no desenvolvimento de circuitos não-síncronos.
Para facilitar a distinção entre os estilos de projeto, o termo não-síncrono é utilizado neste trabalho para englobar paradigmas assíncronos (clockless), o paradigma GALS e outros paradigmas tal como a dessincronização.
Circuitos não-síncronos são circuitos que assumem sinais binários, mas não assumem o pressuposto de discretização do tempo, isto é, em circuitos assíncronos o tempo é tratado como uma variável contínua.
Este tipo de circuito pode eliminar os problemas de escorregamento e de dissipação de potência do sinal de relógio.
Entretanto, o desenvolvimento de sistemas não-síncronos esbarra na falta de ferramentas adequadas para a automatização do processo de desenvolvimento Motivados por as limitações do estilo síncrono de projeto, por a falta de ferramentas para dar suporte a circuitos totalmente assíncronos, e por a grande popularidade do estilo síncrono de projeto, alguns trabalhos de pesquisa propõem soluções intermediárias entre o projeto síncrono e o projeto assíncrono.
O objetivo principal é manter as ferramentas do projeto síncrono e eliminar ou reduzir o uso de sincronização através do sinal de relógio.
Entre essas propostas pode- se destacar o uso de sistemas GALS e o uso da dessincronização (do inglês, desynchronization).
A dessincronização é uma técnica de geração de circuitos assíncronos a partir de o estilo síncrono.
A única alteração do estilo síncrono é a substituição da etapa de geração da árvore de relógio por uma etapa de inserção de circuitos de handshake, um para o controle de cada etapa do circuito síncrono.
O período do sinal de relógio é substituído por um elemento de atraso que deve possuir atraso maior que o atraso de propagação de pior caso da lógica combinacional a qual está associado.
A segunda proposta que pode potencialmente preencher a lacuna entre sistemas síncronos e assíncronos é a decomposição de um sistema síncrono em diversos módulos que não trabalham globalmente sincronizados, ou seja, onde cada módulo possua um domínio de relógio distinto.
Este estilo de projeto é conhecido como Globalmente Assíncrono e Localmente Síncrono.
Em sistemas GALS, cada módulo trabalha sincronamente, mas a interação entre módulos utiliza uma interface de comunicação assíncrona, responsável por efetuar a transferência de informações entre os módulos síncronos.
Estilos de projeto não-síncronos mantêm o pressuposto de discretização dos níveis de tensão, mas adotam outros pressupostos quanto a o tempo ou nenhum pressuposto neste sentido.
De essa forma, o tempo tem de ser tratado como uma variável contínua, o que torna tais circuitos mais sensíveis a fenômenos temporais que ocorrem em circuitos digitais.
A seguir serão descritos alguns destes fenômenos temporais, protocolos de comunicação, codificação de dados extraídos na sua maioria do trabalho de Sparsø e Furber.
Transitórios (em inglês, hazards) são exemplos de fenômenos temporais que podem afetar o funcionamento de circuitos não-síncronos.
Em sistemas síncronos, valores transitórios podem ocorrer sem problemas, desde que no instante de amostragem todos os valores estejam estáveis.
Em sistemas assíncronos, um valor transitório pode levar o circuito a um estado inválido ou indesejado.
A existência de transitórios acontece devido a a ocorrência de atrasos diferenciados de componentes ativos e fios ao longo de o circuito.
Entretanto, existem algumas técnicas que podem garantir o desenvolvimento de circuitos combinacionais com comportamento livre de transitórios.
Outro fenômeno temporal que pode prejudicar o funcionamento de circuitos digitais é a metaestabilidade.
Este fenômeno é mais difícil de tratar quando técnicas nãosíncronas de projeto são adotadas.
A metaestabilidade é um fenômeno que pode ocorrer em dispositivos de armazenamento.
Quando um dado a ser registrado num elemento de armazenamento altera o seu valor simultaneamente ou muito próximo a a transição do sinal que habilita a amostragem, a saída pode apresentar problemas.
Isto provavelmente ocorre se não forem respeitadas as restrições de tempos de setup e hold dos elementos de memória.
Algumas falhas possíveis são:
A saída apresentar um valor indeterminado entre` 0 'e` 1', ou haver uma demora arbitrariamente longa para que a transição de valor surja na saída.
Quando este fenômeno ocorre, ele pode produzir uma falha de sincronização, isto é, o valor de saída do elemento de armazenamento pode ser interpretado como distinto do valor correto por circuitos subseqüentes.
O protocolo de comunicação assíncrono mais simples e mais comum é conhecido como handshake.
Este protocolo utiliza dois sinais, geralmente denominados de request (Req) e acknowledge (Ack), para controlar um processo de transmissão de dados ou de sincronização.
O canal de dados é opcional, uma vez que o protocolo pode ser utilizado apenas para sincronização entre os módulos, sem troca de dados.
No caso de troca de dados, um canal do tipo push channel, ou seja, um canal onde a fonte de dados é o mestre do protocolo de handshake.
Uma forma alternativa desse protocolo é a utilização de pull channels, onde o destino dos dados atua como mestre.
De acordo com o número de sinalizações envolvidas no protocolo handshake têmse duas versões de este:
Protocolo de quatro fases e protocolo de duas fases.
A codificação de dados num circuito digital pode ser feita de diversas formas.
A codificação mais comumente empregada é conhecida como trilha única (em inglês, single rail -- Sr).
Em circuitos não-síncronos, a utilização do protocolo de handshake associado a dados codificados em trilha única determina o estilo de projeto denominado bundled data.
Em esse estilo, o sinal de requisição de comunicação (Req) é responsável também por sinalizar a validade dos dados transmitidos.
Para o funcionamento do estilo bundled data é necessário que o atraso do sinal de requisição seja maior que o atraso de todos os sinais de dados.
Dito de outra forma, a máquina de estados responsável por produzir o sinal de requisição deve ser projetada de forma que a geração do sinal de requisição só aconteça após os dados estabilizarem na entrada do receptor.
Para eliminar a restrição de temporização imposta à geração do sinal de requisição no estilo bundled data, projetistas de circuitos assíncronos exploram possibilidades de codificação capazes de transportar o dado juntamente com a sua informação de validade.
Tais tipos de codificação permitem o desenvolvimento de protocolos de comunicação onde não é necessária a verificação de atendimento de restrições temporais na troca de dados.
Tais codificações são conhecidas como insensíveis a atrasos (do inglês, delay insensitive Em uma codificação Di o sinal de requisição é embutido como parte inseparável dos dados, mas o sinal de reconhecimento (Ack) ainda é necessário.
Assim, o receptor deve ser capaz de verificar a validade dos dados, bem como sua presença ou ausência no canal de comunicação.
Códigos especiais são reservados por esquemas Di para indicar a ausência de dados, os chamados códigos neutros.
Outro nome para códigos neutros é espaçadores.
Existem várias formas de codificação Di, mas as duas formas mais utilizadas são o código trilha dupla (Dr) e o código m-de-N.
A implementação de dispositivos e circuitos assíncronos pode ser feita tanto sobre informação codificada usando trilha única, através de circuitos combinacionais semelhantes aos utilizados em lógica síncrona, quanto sobre informação expressa via codificações Di.
A seguir exploram- se essas formas de implementação.
Antes, contudo será apresentado o C-element de Muller, um componente básico em diversos estilos de projeto assíncronos, e também empregado em vários pontos neste trabalho.
Trata- se de um componente de importância em muitos estilos de projeto assíncrono.
Ele também é chamado de C-element de Muller.
O Celement funciona como um sincronizador de eventos, produzindo um evento na sua saída quando todas as suas entradas recebem eventos específicos.
A Tabela 4.1 é uma especificação de comportamento de um C-element de 2 entradas, onde a e b são os sinais de entrada e Zi é o sinal de saída.
O C-element gera 0 na sua saída quando ambas as entradas são 0, 1 na saída quando ambas as entradas são 1 e mantém o valor anterior para qualquer outra configuração de entradas.
Logo, trata- se de um circuito seqüencial que pode ser implementado de diversas formas.
Para FPGAs, a maneira mais simples de implementar um C-Element é apresentada na Figura 4.1 (a).
O circuito possui duas entradas e sua saída é realimentada.
Assim, um C-Element de duas entradas pode ser implementado numa LUT (do inglês, Look Up Table) de 4 entradas (o que representa apenas metade das LUTs disponíveis num slice nas famílias Virtex II e Spartan 3, por exemplo).
C-elements são na realidade uma família de componentes.
Outros tipos de C-element podem ser obtidos variando o número de entradas, a polaridade de entradas ou acrescentando sinais de controle (como reset e/ ou set).
O sinal de realimentação de um C-element constitui uma derivação que deve ser isócrona assimétrica, ou seja, o fio de realimentação deve possuir atraso menor que o fio de saída.
Existem diversas técnicas de implementação de blocos funcionais para codificação trilha dupla.
Um exemplo de técnica empregada para construir blocos funcionais é Delay Insensitive Minterm Synthesis (DIMS).
Essa técnica utiliza um conjunto de C-elements para gerar todos os mintermos das variáveis de entrada.
Uma outra para somar os mintermos que levam a saída ao estado de reset.
A Figura 4.2 mostra um exemplo de porta XOR DIMS de duas entradas, que pode ser usada para a construção de blocos funcionais.
Para uma melhor compreensão da influência do deslocamento temporal sobre o traço de corrente de uma célula em trilha dupla, a Figura 4.3 apresenta uma abstração deste efeito.
A topologia de uma célula em trilha dupla pode ter seu comportamento abstraído como mostrado na Figura 4.3 a).
Em esta Figura, ATT e ATF representam respectivamente os tempos de chegada dos fios de entrada de um inversor trilha dupla teórico, representa a duração da transição dos sinais e t o deslocamento temporal entre os inícios das variações nas duas entradas.
Como ilustrado na Figura 4.3 b), o traço diferencial de corrente é a diferença entre os traços de corrente de INV1 e INV2.
Assumese, sem perda de generalidade, tratar- se de inversores.
como se pode constatar, um deslocamento temporal t nas entradas resulta num deslocamento de mesma ordem nos traços de corrente.
Por conseqüência, durante este tempo t, o traço diferencial de corrente é igual ao traço de corrente do INV1, cuja amplitude máxima IMAXINV1 pode ser atingida se t $ .
Em este contexto, o objetivo em propor STTL é garantir que este deslocamento temporal t não seja perceptível no nível de corrente em células trilha dupla.
A Figura célula consumidora B. É possível imaginar as células em trilha dupla usando um sinal de validade W, responsável por habilitar que B use a saída de A ao final de um tempo t i.
Este tempo é ajustável e deve garantir que os sinais de entrada da célula B estejam num estado válido e estável.
Analisando a Figura 4.4 b), nota- se que se o deslocamento temporal entre as entradas SF e St de B não excede ti, isto garante que não existirá nenhum impacto nos traços de corrente do módulo B. Em outras palavras, espera- se com isto que os traços de corrente do módulo B sejam quase superpostos e por conseqüência insensíveis a qualquer deslocamento temporal existente entre os sinais SF e St, desde que este deslocamento não exceda ti.
Com base nesta premissa, os Autores de propuseram a lógica STTL.
De um modo geral, esta lógica tem características bem próximas da lógica em trilha dupla (DPL), porém com uma codificação de dados e um modelo de funcionamento particulares.
Diferentemente das lógicas DPL existentes, STTL utiliza três fios ao invés de dois para codificar um bit de informação.
Dois fios servem para a codificação de dados enquanto que o terceiro fio serve para identificar o estado de validade dos dados.
A Figura trata de uma codificação em três trilhas, a informação contida neste terceiro fio é redundante e corresponde à validade dos dados.
A conseqüência direta é que num ciclo de processamento, este terceiro fio comuta a VDD na fase de avaliação e depois retorna a 0 (GND) na fase de pré-carga.
Tendo uma atividade completamente independente dos dados tratados, este terceiro fio não apresenta nenhuma incidência particular sobre os traços hipóteses DPA.
A Figura 4.6 ilustra o funcionamento da lógica STTL através de um circuito exemplo.
Legenda d:
Tempo de processamento de dados.
Depois de ativados os sinais aV, bV, cV e dV (assumindo- se que ocorram ao mesmo tempo), e0, e1, f0, f1 estabilizam primeiro.
A seguir, a validação ocorre em eV e fV, que habilitam o chaveamento de g0 ou g1, seguido por gV, assumindo- se (como especificado por a lógica) que os sinais de validade tenham atraso de propagação maior.
De este modo, o chaveamento das portas STTL é controlado por a trilha de validade, caracterizada por um chaveamento mais lento em relação a as trilhas de dados.
Em outras palavras, as trilhas de validade representadas por as flechas pontilhadas na Figura 4.6 operam como o principal suporte de temporização dos blocos lógicos, dando seqüência aos eventos independentemente dos dados processados.
Observa- se que durante a seqüência de chaveamento, os tempos de ativação de e0 e de e1 podem variar devido a os possíveis desequilíbrios de carga de saída.
Isto é representado por os hachurados cinza na Figura 4.6.
Entretanto, estes desequilíbrios de tempo não afetam a operação das portas seguintes, pois são ativadas por os sinais de validação.
Esta característica evita os efeitos de acúmulo de desequilíbrios ao longo de o caminho de dados.
Isto garante um consumo de potência e tempo de processamento quase independente de dados no circuito.
Esta Seção apresenta a proposta de uma biblioteca de portas lógicas STTL baseadas nos pressupostos de projeto de circuitos não-síncronos orientados a dispositivos reconfiguráveis.
O protótipo da biblioteca lógica é seguir as premissas definidas por os Autores em, de modo que seja possível avaliar sua robustez através de análises reais de consumo de potência e de radiação eletromagnéticas.
A maior parte dos circuitos implementados em FPGAs baseia- se no uso de ferramentas de síntese automática.
Entretanto, pode ser difícil satisfazer requisitos de temporização rígidos (como os exigidos em módulos assíncronos).
Também, obter um percentual alto de uso de um dispositivo FPGA pode não ser factível via síntese automática apenas.
Para tentar superar estas restrições, a maioria das ferramentas de síntese automática dá suporte à especificação de uma ampla variedade de restrições, indo desde especificações de planta baixa e freqüência de operação mínima até o controle de atrasos em fios específicos.
Ainda assim, restrições são apenas guias para ferramentas de alto nível e um processo de síntese pode chegar ao seu final violando uma ou mais das restrições impostas.
Um dos recursos oferecidos em FPGAs da Xilinx para se ter um controle do processo de síntese são as chamadas hard macros.
Hard macros são células definíveis em nível de leiaute do FPGA, através de uma ferramenta (FPGA Editor) que dá acesso a fios, LUTs e outros elementos específicos de um dispositivo FPGA específico de uma família específica.
A liberdade de usar os recursos do FPGA não é absoluta, mas suficientemente detalhada para permitir criar portas STTL que respeitam restrições de temporização impostas por a lógica.
Como FPGAs são em essência matrizes bidimensionais de componentes idênticos, uma vez projetadas, hard macros podem ser utilizadas como uma célula de biblioteca, instanciadas em descrições de alto nível (como VHDL ou Verilog).
Elas são manipuladas como um módulo de leiaute rígido por ferramentas de síntese lógica e física.
Partes críticas em funcionalidade e/ ou temporização podem assim ser projetadas a mão.
Hard macros provêem controle sobre projetos, obviamente ao custo de complexidade adicional.
Hard macros não são novidade em projeto de FPGAs.
Elas foram usadas, por exemplo, por Martín--Langerwerf Em para reduzir o número de FPGAs e o tempo de execução para aplicações de tratamento de vídeo.
Além de isto, hard macros já foram usadas para habilitar o uso de reconfiguração dinâmica e parcial de FPGAs, como descrito, por exemplo, em e.
Aqui se emprega hard macros para implementar primitivas assíncronas da lógica STTL, viabilizando o uso desta lógica em FPGAs de forma compacta.
Estudos realizados por Pontes Mostram que FPGAs da Xilinx possuem recursos que habilitam a implementação da biblioteca pretendida.
Quatro versões de portas lógicas And são desenvolvidas com base nos pressupostos DIMS de concepção assíncronos de circuitos.
Como exemplo, a Figura 4.7 apresenta quatro diferentes versões implementadas da porta lógica And de duas entradas.
Inicialmente, desenvolveram- se duas versões em trilha dupla (Dr), em (a) uma versão Dr DIMS conforme definida na literatura e em (b) uma versão Dr DIMS balanceada, ou seja, as portas de saída Z1 e Z0 tendo a mesma profundidade lógica.
A seguir, desenvolveram- se duas versões de protótipos da lógica STTL.
A versão (c) mostra uma lógica baseada nos pressupostos DIMS, porém adaptada às premissas definidas em.
A seguir, uma versão empregando menos recursos (d) é definida, apresentando o mesmo comportamento de (c), mas com vantagens em termos de área e tempo de cálculo.
Como a Figura mostra, a lógica é composta por C-element de Muller, responsáveis por evitar que transições espúrias ocorridas nas entradas sejam propagadas através de portas lógicas tradicionais OR, And, NAND.
Para implementar estas portas em lógica STTL, é necessário que os caminhos de dados da lógica-verdadeira e lógicafalsa tenham a mesma profundidade lógica.
Esta característica é importante para se obter consumo de potência e tempo de propagação quase independentes dos dados em cada célula.
Um C-element de três entradas é usado com o objetivo de otimizar o protótipo da lógica STTL apresentado na Figura 4.7 (c).
A função lógica de C'apresenta o mesmo comportamento das portas envolvidas na geração da saída Z0 na Figura 4.7 (c).
Isto permite reduzir o número de LUTs necessárias para implementar a lógica e conseqüentemente reduzir o atraso necessário para o sinal de validade.
A Figura 4.8 apresenta a implementação da porta And STTL da Figura 4.7 (d) sobre um bloco lógico configurável (do inglês, Configurable Block Logic -- CLB) de um FPGA Xilinx da família Spartan3.
A lógica de validade em portas STTL é implementada a partir de as entradas AV e BV conforme discutido anteriormente para circuitos CMOS.
Assim, uma lógica independente com tempo de propagação maior em relação a os sinais de saída Z1 e Z0 é projetada a partir de elementos de atraso como mostrado na Figura 4.7.
Este atraso é obtido por o uso de LUTs em cascata implementando cada uma a função identidade, de modo a obter um atraso constante para todos os dados de entrada.
O projeto de uma porta lógica And STTL de duas entradas tal como da Figura 4.7 (c) pode ser realizado usando 11 LUTs (ou 6 slices).
De estas 11 LUTs, 6 são usadas para a definição da lógica propriamente dita e 5 empregadas na lógica de validação.
Com esta proposta de porta lógica é possível notar que a concepção de uma porta STTL sobre o FPGA apresenta alto custo de área.
Em um momento inicial, o objetivo principal desta proposta foi validar os conceitos de STTL e não necessariamente encontrar um meio ótimo de desenvolver a lógica sobre FPGAs.
Porém, uma otimização se faz necessária de modo a tornar- la competitiva em relação a outras propostas disponíveis na literatura.
Assim, a segunda versão (Figura 4.7 (d)) utiliza 6 LUTs, gerando um ganho de quase 50% em termos de área.
Seguindo este método de concepção, outras portas lógicas STTL foram implementadas, incluindo:
NAND, OR, NOR, XOR e XNOR.
No caso de as portas lógicas XOR e XNOR não é possível usar a otimização dada por C'.
Para isso, seria necessário construir um C-element com mais de 4 sinais de entrada.
Como as LUTs dos dispositivos Spartan3 possuem apenas 4 sinais de entrada, isto exigiria que o C-element fosse implementado em duas ou mais LUTs.
Isto tornaria complexa a tarefa de obedecer às exigências temporais do C-element, e ainda, não reduziria o número total de LUTs para sua implementação.
Portanto estas portas lógicas são implementadas apenas com a versão inicial, onde são necessários 5 elementos de atraso tal como mostrado na Figura simplesmente como a inversão dos fios verdadeiro e falso da codificação trilha dupla, não sendo necessário o uso de componentes ativos para sua implementação.
Antes de avaliar a robustez do processo de prototipação da lógica STTL, é necessário inicialmente validar o funcionamento das portas lógicas desenvolvidas e avaliar através de simulação se os tempos de processamento são independentes dos dados de entrada.
Para isso o submódulo SBOX1 do algoritmo Des associado a uma função XOR de entrada foi escolhido como estudo de caso.
O motivo para tal escolha é o simples fato deste submódulo ter sido usado anteriormente por o grupo de pesquisa, reduzindo assim o tempo de sua validação.
Outros algoritmos mais recentes ou submódulos destes, tal como o AES e sua SBOX, também poderiam ser usados neste estudo de caso.
O submódulo do Des nada mais é que uma função combinacional com seis bits da mensagem de entrada, 6 bits da subchave criptográfica e 4 bits de saída para a mensagem cifrada, tal como mostrado na Figura 4.9.
Maiores detalhes podem ser obtidos no Anexo deste trabalho.
Inicialmente é necessário mapear esta função para portas lógicas tradicionais de duas entradas, já que a biblioteca de protótipos da lógica STTL possui apenas portas de duas entradas.
Descrições de hardware com este tipo de restrição podem ser obtidas a partir de uma descrição do submódulo Des em linguagem de descrição de hardware como VHDL ou Verilog e de um sintetizador de hardware.
O sintetizador deve ser parametrizado, de modo a gerar uma descrição do circuito desejado em portas lógicas de duas entradas, descrição esta denominada de netlist por as ferramentas de síntese de hardware.
Com a geração do netlist do submódulo Des verificou- se que o circuito combinacional é composto por 175 portas lógicas de duas entradas.
Este número significativo de portas lógicas para um estudo de caso exigiu uma ferramenta para automatizar a transcrição da descrição em trilha única para três trilhas como exigido por a lógica STTL.
Descrições de hardware de circuitos mais complexos tais como os algoritmos Des e AES exigem um número significativamente maior de portas lógicas, o que pode se tornar inviável de ser realizado manualmente.
De este modo, foi necessário propor uma nova etapa ao fluxo de projeto para automatizar o desenvolvimento de circuitos com a lógica STTL.
Esta nova etapa pode ser vista no fluxo de síntese de hardware da Xilinx, como mostra a Figura 4.10.
O fluxo de projeto tem como entrada a descrição VHDL ou Verilog do circuito codificado em trilha única.
A etapa inicial de síntese de hardware deve ser executada para gerar o netlist.
O sintetizador XST da Xilinx (do inglês, Xilinx Synthesis Technology -- XST) gera arquivos netlists em formato NGC, um formato específico da Xilinx que contém informações do projeto lógico e também restrições de projeto, sendo complexo para ser usado como entrada para a etapa de tradução.
De este modo é necessário usar um sintetizador tal como o Encounter RTL Compiler da Cadence que gera netlist em formato Verilog, o que simplifica o processo de tradução na etapa seguinte do fluxo.
A etapa de tradução do netlist em trilha única para uma descrição com portas lógicas STTL foi desenvolvida em linguagem C, sendo uma das contribuições deste trabalho.
Esta etapa gera como saída uma descrição do circuito em três trilhas, instanciando as hard macros contidas na biblioteca STTL.
A o final desta etapa se obtém um arquivo com a descrição VHDL do circuito STTL que é usado como entrada para o fluxo de projeto da Xilinx.
Como um grande número de hard macros é instanciado no projeto, é ainda necessário realizar restrições de posicionamento dos componentes envolvidos no projeto, como indicado na Figura 4.10 na etapa &quot;Translate Design».
Isto permite que a ferramenta de síntese física tenha capacidade de realizar o posicionamento e roteamento de fios a fim de interconectar todos componentes do projeto satisfazendo restrições temporais de projeto.
Assim, quanto mais recursos de um dispositivo prototipação forem utilizados, mais complexa é a tarefa de encontrar restrições de posicionamento que satisfaçam as exigências do projeto.
Maiores detalhes sobre estas etapas de síntese de hardware podem ser obtidos em.
O script de tradução e o fluxo de projeto completo foram validados com o circuito da SBOX1.
Simulações explorando todas as possíveis 64 entradas, a análise e validação dos dados de saída comprovaram o funcionamento correto do circuito.
Embora o script seja útil aos propósitos do projeto, este ainda limita- se a traduzir apenas descrições de portas lógicas, não sendo capaz de traduzir estruturas mais complexas tal como flip-flops, latches, multiplexadores, entre outros componentes comuns em projeto de circuitos digitais.
Este fluxo de projeto permite que a etapa de tradução seja alterada para implementar circuitos em trilha dupla, tal como o estilo DIMS apresentado anteriormente.
Assim, é possível realizar outros estudos de caso, visando comparar a versão STTL com a implementação DIMS, por exemplo.
De um modo geral, o fluxo de projeto baseado em hard macros apresenta uma restrição.
As hard macros quando desenvolvidas são dependentes da tecnologia do dispositivo escolhido para sua prototipação.
Por exemplo, uma hard macro construída para o dispositivo XC3 S200 da família Spartan3 não pode ser reusado num projeto sobre o dispositivo da família Spartan3E.
Para contornar isto, a Ferramenta FPGA Editor permite a geração de scripts a fim de automatizar a construção de hard macros para dispositivos diferentes.
Este script registra todos os passos usados para criar a hard macro.
A o final, esta mesma hard macro pode ser construída sobre outros dispositivos simplesmente por executar o script especificando o dispositivo ao qual se deseja projetar.
Em este trabalho scripts foram criados para construir as hard macros usadas nos estudos de caso, sendo esta outra contribuição deste trabalho.
A nova etapa de tradução do fluxo de projeto permite a obtenção de quatro versões diferentes do submódulo tomado como estudo de caso:
Uma implementação para cada versão descrita na Figura 4.7, além de a versão original, projetada em trilha única.
Com a realização da síntese completa de todas as versões é possível avaliar e comparar os tempos de cálculo e os custos em área da prototipação.
Para avaliar a dependência entre dados e o tempo de cálculo foram realizadas simulações pós-síntese com os circuitos, visando medir o tempo de cálculo para cada uma das combinações possíveis de dados de entrada.
Para isso, fixa- se uma das entradas (chave) num valor aleatório e aplica- se um vetor à entrada de dados com as 64 combinações de valores possíveis, registrando o tempo de cálculo para cada dado processado.
O experimento foi realizado nas 5 versões do submódulo, todos prototipados no dispositivo Spartan XC3 S200 da Xilinx e simulados com a ferramenta ModelSim da Mentor.
Os resultados apresentados na Tabela 4.2 demonstram que o tempo de cálculo do submódulo STTL é rigorosamente constante para todos os dados de entrada, como se esperava.
Por outro lado, o tempo de cálculo de STTL é 5 vezes maior que o tempo de cálculo obtido na versão trilha única.
A lógica de validação independente implementada por o atraso segundo a Figura 4.7 explica este resultado.
Reduções de 48% em termos de área e 20% em termos de tempo de cálculo são obtidos com a otimização proposta.
Os resultados também mostram que o tempo de cálculo reduz- se em torno de 44% ao se usar um balanceamento em lógica Dr, como visto em DR2.
Os resultados obtidos pós-síntese revelam a área necessária para implementar cada uma das versões do submódulo, como apresentado na Tabela 4.2.
O custo em termos de área para se obter tempo de cálculo independente de dados é na ordem de 5 vezes em relação a a implementação tradicional.
As plantas-baixas dos submódulos implementados em STTL e trilha simples são mostradas na Figura 4.11.
Após a validação da biblioteca STTL para prototipação em FPGAs, com a implementação do submódulo SBOX1, a etapa seguinte é projetar o algoritmo Des com a biblioteca.
Este estudo de caso permite avaliar a robustez da lógica STTL através da execução de um algoritmo difundido.
O algoritmo Des é composto basicamente por uma função F (Feistel Function), funções de permutação e expansão de dados.
A função F é puramente combinacional, sendo factível sua implementação em lógica STTL.
As funções de permutação e expansão resumem- se a permutações de fios no projeto de hardware, não representando grande complexidade para o projeto.
Uma lógica seqüencial de controle é utilizada na geração de subchaves criptográficas e no controle da realimentação de dados parcialmente cifrados durante as 16 rodadas de execução do algoritmo.
Mais detalhes do algoritmo Des serão apresentados no Capítulo 5.
A implementação parcial do Des em STTL descrita na Seção 4.3 serve como ponto de partida para criar uma versão completa do algoritmo.
Esta implementação utiliza uma abordagem híbrida, do ponto de vista de sincronização, devido a a complexidade de lidar com grandes quantidades de hard macros ao longo de o fluxo de projeto de FPGAs.
Em esta abordagem, registradores e a lógica de controle do laço de execução do Des são módulos síncronos, enquanto que a implementação das SBOXes e outras partes da lógica combinacional (operações XOR) são realizadas com hard macros STTL.
Além de isto, o controle seqüencial de geração de subchaves mantém- se síncrono, porém adaptado a interface em três trilhas de STTL.
A Tabela 4.3 apresenta os resultados de área e latência para implementar esta versão do algoritmo Des usando a biblioteca STTL2 e a comparação com uma versão completamente síncrona do Des.
Dada a natureza híbrida da implementação STTL pode- se esperar que uma implementação completamente assíncrona conduza a áreas maiores, devido a a expansão dos módulos que empregam lógica trilha tripla, e possivelmente uma maior robustez a SCA.
Nota- se claramente o alto custo em área da implementação STTL2.
Embora apresente um aumento de aproximadamente 20 vezes em relação a a implementação síncrona, STTL2 é compatível em termos de área com outras DPLs.
As simulações usadas na validação do algoritmo mostram que o tempo de processamento é constante e independente de dados, diferentemente da implementação regular, onde ocorre uma variação de até 2 ns no tempo de execução quando submetido ao processamento da mesma seqüência de dados.
Com a validação da biblioteca de portas lógicas STTL e a verificação por simulação da premissa tempo de cálculo independente de dados, a etapa seguinte é avaliar a robustez da lógica proposta, através das análises de potência DPA.
Como visto anteriormente, no Capítulo 2, a primeira fase das análises é a medição e armazenamento dos traços de consumo de potência de cada dado processado.
Logo, para realizar estes experimentos é necessário um sistema de medição preciso, capaz de medir consumo de potência em circuitos prototipados numa plataforma FPGA.
O sistema proposto nesta Seção também foi utilizado para análises de radiação eletromagnética produzida por o circuito, usando SEMA, DEMA e CEMA.
Para isto, é necessário apenas substituir a sonda eletromagnética de corrente por uma sonda eletromagnética adequada às características de freqüência das ondas emitidas por o circuito.
Como estas ondas possuem pequena intensidade, utiliza- se também um amplificador de sinal.
Por definição, potência ou potência instantânea (P) é o produto entre os valores instantâneos da tensão elétrica e da corrente elétrica, P $= V (t) I (t).
Em este caso, considerando que a tensão de alimentação de um dispositivo FPGA é constante, a potência consumida será diretamente proporcional às variações de corrente elétrica produzidas por o chaveamento do circuito durante o processamento dos dados de entrada.
Em a literatura encontram- se basicamente dois modos de medição de potência:
Usando um resistor em série com a fonte de alimentação, onde a variação de tensão sobre o resistor permite encontrar a corrente elétrica no dispositivo segundo a Lei de Ohm I $ ;
E usando uma sonda eletromagnética para medição de corrente.
Experimentos com ambos os modos de medição foram realizados.
As medições de potência usando a sonda eletromagnética apresentaram menor ruídos e, portanto foi adotada nestes experimentos.
Para realizar a primeira fase do ataque DPA define- se o sistema de medição de potência como ilustra a Figura 4.12.
Em este caso, um osciloscópio preciso é o equipamento mais indicado para medir o consumo de um circuito.
Uma sonda eletromagnética de medição de corrente conectada a um canal de entrada do osciloscópio serve como sensor de corrente do sistema.
A plataforma de prototipação contém o protótipo do sistema alvo da medição e um dispositivo de comunicação para entrada e saída de dados, neste caso uma porta de comunicação serial.
Um hospedeiro deve gerar os dados necessários a serem medidos e armazenar seus respectivos traços de consumo de potência.
Além destes equipamentos, é necessário um sistema embarcado para servir de interface entre o circuito em avaliação e o restante do sistema.
Este sistema deve ser capaz de indicar ao osciloscópio o momento exato do final do processamento para o circuito em avaliação.
De este modo, o hospedeiro armazena o traço de consumo referente apenas ao período de tempo em que o dado é processado, permitindo medições precisas.
Além disso, a plataforma de prototipação deve sofrer uma modificação no circuito de alimentação do núcleo FPGA, para que seja possível usar a sonda de corrente.
A alteração consiste em substituir o regulador de tensão destinado à alimentação do núcleo FPGA por uma bateria recarregável externa, que passa a alimentar exclusivamente o FPGA na plataforma.
O consumo de potência causado por os componentes presentes na plataforma é desprezado nestes experimentos.
Os FPGAs possuem outros pinos de alimentação destinados aos circuitos de entrada e saídas de dados (E/ S).
Estes são organizados em grupos de pinos e possuem tensões de alimentação ajustáveis.
Os experimentos realizados neste trabalho levam em conta apenas o consumo do núcleo de lógica programável do dispositivo onde se encontra o protótipo do circuito em avaliação.
Entretanto, o chaveamento de tensão nos pinos de E/ S pode gerar ruído na alimentação do núcleo do FPGA.
Portanto, é aconselhável restringir ao máximo o uso de pinos de E/ S de dados no sistema embarcado projetado, ou manter estes inoperantes durante o período de medição do traço de potência.
MHz. A plataforma Digilent Spartan3 Starter Board usada dispõe de um sinal de relógio com freqüência de 50 MHz, logo a divisão da freqüência deste sinal é necessária.
O circuito Decodificador do Mostrador é usado apenas num instante inicial, para fins de validação do sistema.
A versão do sistema sem este recurso é utilizada no processo de medição.
Isto visa reduzir ruídos causados por o chaveamento de pinos de E/ S do FPGA durante o processo de medição de consumo, e assim melhorar a precisão do sistema.
Este mesmo sistema embarcado deve servir para a avaliação das 3 versões do submódulo a ser avaliado, lógica trilha simples (Sr), (ii) trilha dupla (Dr) e (III) STTL.
O sistema embarcado é projetado com lógica tradicional, porém deve dar suporte à avaliação das versões STTL e Dr. Logo, é necessário uma interface para codificar cada bit de informação para uma codificação em três trilhas ou duas trilhas.
Esta interface é composta por portas lógicas And, OR e XOR, responsáveis por gerar as codificações necessárias e um controle para as fases de pré-carga e avaliação das lógicas em análise.
A Figura 4.14 mostra uma simulação usada para validar o sistema embarcado projetado para medir os traços de potência de uma SBOX1 STTL.
O período entre os eventos e representa o tempo de processamento da SBOX1 que deve ser analisado posteriormente por os ataques DPA.
Em o instante 3, quando o sinal synchro_ oscillo é ativado, um ruído intenso é produzido, fato que prejudica a análise de potência.
Em este caso, o experimento tem como objetivo avaliar a eficiência da lógica proposta contra os ataques.
O ruído produzido por a ativação deste sinal interfere nesta avaliação, de modo a ocultar informações do consumo de potência úteis aos ataques e conseqüentemente atrapalhando a avaliação da robustez da lógica.
Portanto é necessário atrasar o início do processamento do circuito para que ocorra a estabilização do consumo do FPGA, obtendo- se assim uma melhor qualidade do sinal medido.
O hospedeiro executa um programa que gera e envia dados ao sistema embarcado por a porta serial de comunicação e armazena os dados de consumo medidos por o osciloscópio.
Este programa, inicialmente desenvolvido em linguagem C, realiza 3 operações:
Lê dados a partir de um arquivo com a seqüência de dados a serem enviados e medidos por o sistema, aguarda o término do processamento e da medição do traço de consumo de potência ou radiação eletromagnética por o osciloscópio e armazena os traços medidos por o osciloscópio.
A sincronização entre hospedeiro, sistema embarcado e osciloscópio ocorre através de uma função disponibilizada por o fabricante do osciloscópio que é ativada por o sinal de gatilho enviado por o sistema embarcado, o sinal synchro_ oscillo.
A o finalizar a medição, a função é desbloqueada para realizar a leitura da memória do osciloscópio através da rede local de comunicação (LAN).
Os dados são armazenados num arquivo em formato ASCII cujo nome deriva do dado, chave e valor de saída do submódulo.
Após a medição e armazenamento dos traços de consumo ou radiação eletromagnética é encerrada a primeira etapa da análise DPA.
O sistema utilizado para medir os traços de consumo de potência e de radiações eletromagnéticas do algoritmo Des STTL passa por algumas adaptações em relação a o usado anteriormente.
Em este novo sistema são enviados dados e mensagens de 64 bits ao invés de dados de apenas 8 bits.
Ainda, realiza- se a verificação do resultado cifrado antes do armazenamento do traço medido.
O sistema embarcado empregado para dar suporte à avaliação do algoritmo Des STTL é capaz de receber e enviar dados por a porta serial.
Usa- se também uma interface para adaptar os dados codificados em trilha única para três trilhas e vice-versa.
Para medir a radiação eletromagnética emitida por a implementação do algoritmo Des STTL utilizam- se os seguintes elementos:
Uma plataforma Digilent Spartan 3E1600 Development Board com um dispositivo Xilinx Spartan-3E XC3 S1600E, uma sonda eletromagnética de 500 µm, um amplificador com baixo nível de ruído para amplificar o sinal obtido por a sonda e aumentar a precisão da medição, um osciloscópio Agilent Infinium DS80000B, e um PC com suporte a scripts MATLAB para controle completo do sistema de medição.
O sistema de controle executado no hospedeiro é totalmente reescrito para o ambiente MATLAB.
Como este ambiente é propício para tratamento de sinais e as análises DPA, DEMA, CEMA, CPA foram todas desenvolvidas no MATLAB, optou- se por unificar o desenvolvimento dos scripts de aquisição de dados para este mesmo ambiente.
Estes scripts foram desenvolvidos originalmente no LIRMM, conforme descrito por Lomné Para desenvolver as análises DPA e DEMA é preciso coletar inicialmente os respectivos traços para todos os possíveis dados de entrada, como se discutiu anteriormente.
Em este caso, como o circuito estudo de caso possui apenas 6 bits de entrada, é possível realizar análises exaustivas, ou seja, analisar os traços de processamento de todos os possíveis dados de entrada e todas as possíveis sequências de envio de dados.
Porém, para o algoritmo Des, que usa efetivamente como entrada 56 bits de dados e 48 bits de chave, não é facial viabilizar a realização de análises exaustivas.
Em este caso, define- se uma seqüencia de dados de entrada de 64 bits, sem repetições e gerados aleatoriamente.
De acordo com a edição do DPA Contest em 2009, análises realizadas com menos de 200 traços são suficientes para revelar a chave secreta de um algoritmo criptográfico sem prevenções a ataques DPA.
Conforme apresentado no fluxo, as análises diferenciais de potência propostas empregam diferentes funções de seleção.
Além de a função de seleção proposta por Kocher, que analisa cada bit de saída da SBOX1 individualmente, utilizam- se também análises multibits.
Em este caso, os traços são selecionados de acordo com o valor de 2 ou mais bits de saída.
No caso de análises de 2 bits, os traços de potência resultantes de processamentos cujos bits analisados apresentam os valores` 11 'e` 00' são recolhidos e divididos respectivamente em dois grupos V1 e V0, e descartam- se todos os demais traços.
Análises considerando os 4 bits de saída da SBOX1 são também executadas da mesma forma.
Uma variante destas análises é considerar o valor HD do resultado da saída da SBOX1.
Por exemplo, no caso de a análise ao bit` 0' de saída da SBOX1, cada traço resultante de uma encriptação é selecionado de acordo com o valor HD do dado produzido.
Este método pode ser aplicado também em análises multibits, sendo mais eficiente que as funções seleção propostas por Kocher, pois se aproxima do consumo de potência real de um circuito CMOS.
Outra variante de função seleção é o uso de HW.
Este método é usado especificamente para análises simultâneas aos 4 bits de saída da SBOX1.
Em este caso, a função seleção define que traços correspondentes a valores de saída com HW maiores que 2 pertencem a um grupo e os demais traços pertencem ao outro grupo.
As análises CPA e CEMA utilizam também HW e HD como função de seleção.
Estas análises utilizam um fator de correlação linear, o qual é definido a partir de o modelo de distância Hamming de consumo de potência.
Em a prática o coeficiente é calculado conforme a Equação 2.
H i, R N Wi H i, R -- Wi H i, R N Wi -- (Wi) i, R -- (H i, R) Em esta equação, N é o número de traços utilizados, Wi os traços com i pontos e $= H (M i R) a distância Hamming a partir de o estado anterior R. Este fator maximiza predições corretas de hipóteses de chaves e minimiza predições erradas, gerando traços hipóteses CPA ou CEMA com maiores probabilidades de acerto.
Como ilustram a Figura 4.16 e a Figura 4.17, todas as análises realizadas apresentam 64 traços hipóteses representados em duas dimensões, uma quantidade versus tempo.
Estes traços representam a probabilidade associada a cada hipótese de chave e são resultantes de uma diferença de valores de corrente, de campo eletromagnético ou uma medida de correlação.
Em geral, a chave secreta corresponde teoricamente ao traço com maior amplitude.
Porém, uma margem deve ser considerada na prática para garantir um maior nível de confiabilidade para se concluir uma análise.
A margem é definida como a diferença mínima entre a amplitude da assinatura diferencial obtida para a chave correta e a amplitude obtida para a segunda maior amplitude.
Uma análise é considerada bem sucedida se o resultado possuir margem maior que 10%.
A Figura 4.18 apresenta uma avaliação da robustez de STTL contra DPA e DEMA partindo da medição do desvio padrão do consumo de corrente durante a computação do SBOX1 implementado nas seguintes versões:
Trilha única (Sr), (ii) trilha dupla (DR2) (Figura 4.7 (b)) e (iii) STTL2 (Figura 4.7 (d)).
Em esta Figura é possível observar que o desvio padrão de ambas lógicas balanceadas é aproximadamente três vezes menor em relação a a lógica em trilha única.
Este fato valida a efetividade de trilha dupla e STTL do ponto de vista da amplitude.
Todas as análises descritas no fluxo da Figura 4.15 são aplicadas na avaliação da SBOX1.
Realizam- se as análises usando uma seqüência de entrada de 4033 dados.
Esta seqüência contém os 64 possíveis dados de entrada com repetições, de modo a garantir que se exercitam todas as possíveis transições de um valor qualquer de dado de entrada para qualquer outro valor.
O valor 4033 garante que entre o primeiro e o último dado ocorrem 4032 transições.
É fácil calcular que para um vetor de n bits realizar todas as possíveis transições de qualquer valor para qualquer outro são necessárias* n transições.
Assim é possível desta forma obter todos os traços de potência e de radiação eletromagnética para a SBOX1 em análise.
Para cada valor de subchave, a maioria das análises DPA e DEMA é bem sucedida.
Entretanto, as margens obtidas para análises DPA variam entre 10% e 30% enquanto para análises DEMA variam entre 16% e 52%.
Isto pode ser notado na Figura 4.16, onde se mostram as assinaturas diferenciais de potência obtidas para a subchave 10, enquanto a Figura 4.17 representa a evolução dos coeficientes de correlação com respeito a o número de traços usados para desenvolver o CPA e o CEMA.
Em esta última, 200 e 50 traços são respectivamente suficientes para revelar a subchave secreta usando CPA e CEMA, mesmo se a convergência estatística não é completamente alcançada.
Em seguida realizam- se os mesmos experimentos de avaliação sobre a SBOX1 implementada com lógica Dr DIMS e STTL.
Porém nestes casos não é possível usar o mesmo vetor de entrada contendo todas as possíveis transições de dados, pois entre cada dado processado é obrigatório o uso do espaçador.
Logo, as análises limitam- se a processar apenas HW.
A Tabela 4.4 relata o percentual de hipóteses de subchaves corretas reveladas após as análises.
Com esses resultados é possível notar que STTL mostra- se mais robusta aos ataques DPA/ CPA em relação a as demais lógicas analisadas.
Entretanto, este aumento em robustez é obtido ao custo de área adicional.
Por outro lado pode ser uma vantagem, pois não é necessário o uso de roteamento específico como aplicado em.
Certamente outras lógicas DPL, tal como WDDL, são mais robustas que a lógica usada como comparação.
Porém, no escopo deste trabalho é inviável avaliar todas as opções de lógica disponíveis, devido a o tempo necessário para projeto e validação das mesmas, sem mencionar o tempo para medição e execução do fluxo de análises.
Logo, apenas a lógica Dr foi avaliada aqui.
Realizou- se também um experimento visando avaliar a robustez de STTL a ataques Em.
Durante este experimento, substituiu- se a sonda de corrente por uma sonda eletromagnética posicionada sobre o FPGA.
Coletou- se traços Em relativos às versões Sr, Dr e STTL e executaram- se as mesmas análises sobre todas as versões.
Os resultados obtidos com as análises são apresentados na Tabela 4.5.
A partir destes resultados é possível concluir que a lógica em trilha dupla e STTL mostram- se mais resistentes a análises Em em relação a a lógica trilha simples.
Além disso, nota- se que STTL é mais resistente que lógica trilha dupla.
Considera- se que o comportamento de tempo de processamento quase independente de dados de STTL explica o aumento de resistência a ataques Em.
De fato, o balanço simultâneo do chaveamento de corrente e do tempo de processamento permite balancear o campo eletromagnético, o qual é proporcional à variação de corrente no tempo (di/ dt), radiado em todo o circuito.
Entretanto, o balanceamento em nível de bloco não garante que todos os pontos do circuito radiam o mesmo campo eletromagnético, uma vez que o posicionamento do circuito e o roteamento das linhas de alimentação deste não possuem restrições.
Isto explica a fuga de informação remanescente em Dr e STTL em relação a os ataques DEMA e CEMA.
Assim, devem- se empreender esforços no posicionamento do circuito.
Isto implica distribuir melhor a atividade do circuito, e rotear as linhas de alimentação e terra, que são as principais fontes de radiação eletromagnética.
Para a avaliação do algoritmo Des em lógica STTL realizou- se a coleta de 400.000 traços diferentes de radiação eletromagnética.
O fluxo de análises empregado é o mesmo realizado anteriormente.
Porém neste caso realizam- se ataques às oito SBOXes do algoritmo, visando revelar a chave criptográfica completa.
Após as análises DEMA, é possível resumir os resultados das análises conforme mostra a Tabela 4.6.
Com aproximadamente 10.000 traços é possível revelar a chave secreta usada por o algoritmo Des implementado em lógica regular.
As análises realizadas na implementação do algoritmo com lógica STTL exigiram um número significativamente maior de traços para que os ataques obtivessem sucesso.
Embora seja necessário um número maior de traços, a lógica STTL ainda mostrou- se vulnerável a ataques DEMA.
Um dos motivos que conduzem a tal vulnerabilidade é a implementação parcialmente síncrona do algoritmo Des.
O controle síncrono do algoritmo revela os instantes em que ocorre o término da execução de cada rodada.
Além disso, a versão STTL usa os mesmo registradores para armazenar temporariamente os dados parcialmente cifrados, fato que contribui para a fuga de informações.
Apenas uma subchave não foi revelada.
Uma justificativa para isso pode ser a posição da sonda, que não favoreceu a captação de ondas eletromagnéticas provenientes desta parte do circuito durante a fase de medições, e por conseqüência não propiciou bons resultados para as análises sobre a SBOX1.
Este Capítulo introduziu uma avaliação experimental da robustez de STTL a ataques DPA e DEMA.
Esta avaliação realizou- se em FPGAs, usando hard macros e síntese de hardware com roteamento e posicionamento padrão, sem restrições de área.
Os resultados permitem as seguintes conclusões: (
a) STTL é definitivamente mais robusta contra DPA/ CPA em relação a a lógica em trilha única e ligeiramente mais robusta que lógica em trilha dupla;
Os últimos resultados sugerem que mais esforços devem ser feitos para balancear espacialmente, em amplitude e no tempo, o fluxo de chaveamento de corrente dentro de o chip.
Este Capítulo apresenta a principal contribuição deste trabalho, a proposta de arquiteturas pipeline GALS como contramedida a análises por consumo de potência e radiação eletromagnética.
As arquiteturas propostas usam também circuitos não-síncronos em sua infraestrutura.
Porém, diferentemente da biblioteca STTL, estas empregam inserção de aleatoriedade no processamento como método de prevenção a ataques.
Como já discutido no Capítulo 3, apenas uma proposta de contramedida usando metodologia GALS de projeto é encontrada na literatura.
Já arquiteturas pipeline são encontradas na literatura com diversos propósitos, de entre estes a prevenção a ataques DPA tal como proposto em.
Este Capítulo propõe pela primeira vez a combinação de arquiteturas pipeline implementadas com metodologia GALS de projeto usando relógios com freqüências de operação aleatoriamente selecionadas como forma de contramedir a ação de análises por consumo de potência e radiação eletromagnética.
Uma vantagem adicional significativa da arquitetura proposta é o aumento da vazão de dados, proporcionado por o estilo pipeline de implementação.
Este trabalho emprega uma técnica de projeto de hardware muito comum em processadores, o pipeline de execução de instruções.
As instruções são executadas por processadores em várias etapas.
O processador apresenta uma infraestrutura com hardware dedicado a cada uma das etapas, de modo que seja possível haver diferentes instruções sendo executadas em etapas distintas simultaneamente, a fim de aumentar o desempenho do processamento global do sistema.
Este mesmo princípio de execução pode ser aplicado a sistemas criptográficos em hardware.
Em este caso, o algoritmo criptográfico deve permitir implementações em modo pipeline, fato que pode restringir a abordagem proposta a um grupo de algoritmos.
De entre os algoritmos passíveis de se beneficiar de implementações pipeline, possível citar Des, AES, SHA-1 e Md-5.
Além disso, Des e AES são os algoritmos mais usados em smart cards e aparecem comumente na literatura como alvo de avaliações de robustez a ataques por consumo de potência e radiação eletromagnética.
A escolha do algoritmo Des justifica- se por ter este sido alvo de estudos de Kocher em e por utilizar menos área do que seu sucessor AES, quando implementado em hardware.
A Figura 5.1 mostra a estrutura geral do Des.
Este algoritmo opera sobre blocos de entrada de 64 bits.
Cada entrada passa através de 16 rodadas de modificação.
Cada rodada usa uma subchave distinta de 48 bits, duas entradas de 32 bits em operações que incluem permutações (P), expansões de dados (E), deslocamentos, operações lógicas ou exclusivo (XORs`') e caixas de substituição (SBOXes).
Depois de executar 16 rodadas, o algoritmo produz um resultado que sofre uma permutação inversa e se torna o dado cifrado de 64 bits.
O algoritmo Des esquematizado na Figura 5.1 pode ser implementado em hardware ou software.
Uma implementação convencional do Des cria um único bloco de encriptação do algoritmo (seja este em software ou hardware) e executa 16 iterações sobre este bloco com parametrização adequada.
Entretanto, o uso de múltiplos blocos de encriptação permite implementar o algoritmo em modo pipeline.
Assim, é possível ter arquiteturas pipeline a partir de 2 até 16 estágios para executar o algoritmo completo.
Uma vantagem deste tipo de implementação é auferir um aumento significativo da vazão de dados cifrados.
Por outro lado, como desvantagem, esta abordagem apresenta claramente um custo adicional em área.
Este trabalho propõe a implementação de um algoritmo Des pipeline GALS como mostrado na Figura 5.2.
A implementação é baseada na replicação do bloco de encriptação do algoritmo.
Cada estágio do pipeline é envolvido por uma interface assíncrona e gerenciado por um módulo de controle.
Este controle é definido como uma máquina de estados finita que gerência a comunicação ponto a ponto com seus estágios vizinhos através de um protocolo de comunicação handshake (Req -- Ack) de 2 fases.
Um subsistema externo ao pipeline é responsável por gerar um sinal de relógio com freqüência pseudo-aleatória, o qual supre os estágios envoltos por a interface assíncrona, também chamados de ilhas síncronas por Chapiro.
Este subsistema gera uma nova freqüência de relógio sempre que um estágio termina o processamento de um dado.
Em este contexto, a estrutura do subsistema de relógio pode ser construída de diversas maneiras.
Osciladores externos a cristal são uma opção e osciladores em anel internos ao circuito são outra possibilidade.
A Figura 5.2 mostra ainda o esquema da interface assíncrona.
O uso de sincronizadores simples do tipo 2-flip- flops permite a comunicação entre módulos com diferentes domínios de relógio.
A principal suposição de tais sincronizadores é que o tempo reservado para a resolução de metaestabilidade permite um tempo médio entre falhas (do inglês, Mean Time Between Failures -- MTBF) satisfatório.
Outras propostas de circuitos para sincronização em sistemas que envolvem diferentes domínios de freqüência estão disponíveis na literatura, tipicamente mais eficientes e mais complexos de desenvolver que o esquema básico 2-flip- flops.
O leitor interessado pode para tanto consultar, por exemplo, Dobkin e Ginosar.
GALS de projeto.
A proposta inclui interfaces assíncronas do tipo 2-flip- flops usando protocolo de comunicação em 2 fases e geradores de relógio independentes que se operam sob comando de cada estágio pipeline.
Os sincronizadores introduzem uma penalidade de tempo que aumenta a latência do sistema.
Com a ativação do sinal de requisição de processamento (Req), dois ciclos de relógio do estágio receptor podem ser necessários para que este sinal se propague até o próximo estágio e dê início ao processamento dos dados naquele.
Um sinal de reconhecimento (Ack) é enviado ao requisitante a fim de informar- lo que os dados foram recebidos com sucesso.
Em esta arquitetura, cada subsistema de relógio modifica sua freqüência de saída a cada nova requisição de processamento.
Isto pressupõe que o tempo de processamento em cada estágio é alterado para cada novo dado de entrada.
A latência inserida por os sincronizadores pode ser vista como uma desvantagem em termos de desempenho.
Porém este atraso é variável devido a os sinais de relógio com freqüências distintas.
Estes fatores aumentam a aleatoriedade do processamento, dificultando a ação das análises de consumo de potência e de radiações eletromagnéticas.
O subsistema de relógio desempenha um papel importante na abordagem proposta, pois uma nova freqüência deve ser gerada a cada novo dado processado.
Isto tende a aumentar a segurança, pois oculta a fuga de informações do criptosistema.
Trabalhos anteriores propõem alcançar este mesmo objetivo.
Porém, até onde o autor pode avaliar, o presente trabalho é o primeiro a combinar uma implementação GALS baseada em freqüências de relógio aleatórias e uma estrutura pipeline.
O modo como o subsistema de relógio é implementado não é um foco fundamental desta arquitetura.
Contudo, uma proposta de implementação é discutida na Seção 5.3.1.
O método proposto requer a replicação de hardware para contribuir para a neutralização de análises DPA e DEMA.
De fato, o processamento paralelo das rodadas produz um ambiente ruidoso que dificulta a tarefa de SCAs.
Por outro lado, a replicação de hardware num pipeline naturalmente aumenta a vazão de dados do criptosistema.
Esta Seção disponibiliza uma comparação do método proposto com alguns trabalhos anteriores em termos de área, vazão e robustez.
Os resultados apresentados sobre robustez mostram evidências de que este método melhora a segurança do criptosistema.
A Figura 5.3 apresenta a arquitetura projetada para conduzir os experimentos de avaliação de robustez e comparações em termos de área e vazão.
Esta arquitetura contém uma ilha síncrona responsável por a comunicação de dados com um hospedeiro através de uma porta serial.
Este mesmo módulo também tem como função manter o pipeline GALS sempre preenchido ao máximo com dados a criptografar, independente da taxa de comunicação com o hospedeiro.
Esta ilha opera com um sinal de relógio único e convencional, com freqüência de 50 MHz, conforme disponibilizado em diversas plataformas de prototipação.
Como mencionado antes, a fim de manter o pipeline da arquitetura repleto de dados, a ilha principal contém um dispositivo de armazenamento FIFO (do inglês, First In First Out).
Este dispositivo permite que a arquitetura proposta tenha sua robustez a ataques DPA/ DEMA avaliada de duas maneiras.
A primeira destas é sem uso da FIFO.
Aqui, cada dado de entrada é processado consecutivamente em todos os estágios do pipeline, onde cada estágio opera com um sinal de relógio diferente escolhido aleatoriamente por o seu respectivo Gral como mostrado na Figura 5.3.
Em este caso, apenas um dado de entrada permanece no pipeline até que este seja completamente encriptado e transmitido ao hospedeiro para que o próximo dado seja processado.
Isto permite avaliar o efeito produzido por os relógios com diferentes freqüências escolhidas aleatoriamente de forma a aumentar a robustez da arquitetura.
Em a segunda maneira, com uso da FIFO, além de as execuções com diferentes freqüências avalia- se a arquitetura processando diferentes dados simultaneamente.
A FIFO permite que dados sejam enviados ao pipeline tão logo quanto o primeiro estágio esteja disponível para atender a uma nova requisição de processamento.
O processamento paralelo dos estágios aumenta o ruído no consumo de potência e conseqüentemente na radiação eletromagnética produzidos por a arquitetura.
A Figura 5.3 também mostra o sinal (saída localizada na parte superior do controlador da comunicação serial) usado para sincronizar o osciloscópio com o processamento dos dados.
A arquitetura genérica apresentada na Figura 5.3 foi prototipada em 4 versões diferentes, com 2, 4, 8 e 16 estágios, respectivamente denominadas PIPE-2, PIPE-4, PIPE-8 e PIPE-16 GALS.
Em a implementação PIPE-2 GALS o bloco de encriptação do algoritmo é replicado duas vezes e cada um destes executa 8 rodadas do algoritmo.
Raciocínio similar define as demais implementações.
Cada estágio de cada uma das implementações emprega um subsistema gerador de relógio local que produz um sinal de relógio com uma freqüência escolhida aleatoriamente entre n possíveis a cada dado processado.
O gerador de relógio pode ser composto de n osciladores em anel compostos por um número ímpar definido de elementos de atraso operando cada um como um inversor.
Um multiplexador livre de transitórios é usado para chavear entre os sinais de relógios.
Esta restrição se faz necessária, pois a ocorrência de um transitório no sinal de relógio pode conduzir o sistema a estados incorretos.
A Figura 5.4 apresenta o circuito do multiplexador empregado, para n $= 4.
Um módulo com a função de gerar números aleatórios usados para a escolha de um sinal de relógio deve compor o subsistema de relógio.
Em este caso emprega- se um registrador LFSR (do inglês, Linear Feedback Shift Register ­ LFSR).
Embora seu comportamento determinístico permita apenas a geração de dados pseudo-aleatórios, este módulo serve à validação do método proposto.
A Figura 5.5 apresenta uma simulação da operação de chaveamento livre de transitórios deste subsistema.
Observando a Tabela 5.1, nota- se que o método proposto exige menos área que implementações assíncronas na maioria dos casos, mas sofre de um custo adicional em área de até 25 vezes no pior caso quando comparada a implementação regular do Des.
Esta avaliação baseou- se no dispositivo FPGA VirtexII-XC2 V4000 da Xilinx.
Assim, a abordagem GALS pipeline permite negociar área a fim de obter robustez.
Além disso, é virtualmente impossível implementar STTL em FPGAs usando um fluxo de projeto síncrono padrão.
Os resultados na Tabela 5.1 originados do uso de um fluxo específico baseado no leiaute de hard macros para FPGAs Xilinx, conforme detalhado no Capítulo 4.
A Tabela 5.2 apresenta os resultados de latência e vazão de dados para as seguintes implementações do algoritmo Des:
Regular, GALS PIPE, STTL e a comparação destas arquiteturas com a abordagem proposta por Gürkaynak* Autores não apresentam detalhes sobre as condições de medição, mas mencionam o uso de três domínios de relógio, um de 190 MHz e dois outros de 250 MHz.
Em estes experimentos, os osciladores em anel dos subsistemas de relógios são implementados a partir de cadeias de LUTs.
Embora limitado, este é um modo simples para implementar uma prova de conceito da operação das arquiteturas propostas.
A freqüência mínima é 7,2 MHz e a máxima 21 MHz.
Apesar de as baixas freqüências usadas nos experimentos, a freqüência máxima estimada por a ferramenta de síntese para as arquiteturas GALS PIPE é em torno de 100 MHz.
Observa- se que na terceira coluna avalia- se o pior caso de vazão das arquiteturas e na quinta coluna é apresentado o melhor caso de vazão atingida.
É importante notar que as freqüências operacionais dos subsistemas de relógio são escolhidas de tal modo que os estágios operem com freqüências distintas.
Por isso, para estimar os limites de vazão, comparações são baseadas na execução de todos os estágios das arquiteturas na mesma freqüência, conforme a Tabela 5.2.
Observa- se que a replicação do hardware não apresenta efeito significativo na freqüência máxima de operação das arquiteturas pipeline.
Em relação a a implementação regular do Des, as versões GALS PIPE aumentam a vazão proporcionalmente ao número de estágios, como esperado.
Já em relação a a STTL, mostram uma vazão muito superior.
O problema com STTL é que a propagação do sinal de validade de informação apresenta alto custo em latência devido a as cadeias de atrasos.
A magnitude da vazão das arquiteturas GALS PIPE são compatíveis com abordagens similares disponíveis na O método proposto aqui se baseia na inserção de aleatoriedade no processamento criptográfico, de forma a ocultar a fuga de informações.
Como revisado na literatura, o simples fato de inserir atrasos no processamento não garante completamente o sucesso da contramedida empregada.
Portanto, a forma como esta contramedida é inserida é importante para garantir a eficiência do método.
O método insere aleatoriedade em partes do algoritmo implementado em pipeline.
Para tanto, cada estágio do pipeline possui seu domínio de freqüência e controle da inserção de aleatoriedade.
De este modo, quanto maior for o número de estágios do pipeline, maior será a aleatoriedade inserida no sistema.
Os ataques DPA relacionam a amplitude da potência consumida por o circuito e o período de tempo em a qual os dados são computados.
Portanto, a aleatoriedade inserida para ocultar a fuga de informações pode ser uma variação na amplitude da potência bem como deslocamentos temporais de cálculos executados por o circuito.
O método proposto insere ruídos através do processamento paralelo dos estágios do pipeline e através do chaveamento de sinais de relógio com diferentes freqüências.
Para estimar de maneira simplificada a aleatoriedade inserida no domínio do tempo, define- se aqui que a aleatoriedade inserida é equivalente a variação do instante de tempo em que o sinal de sincronismo fica ativo para a medição de consumo e radiação eletromagnética do circuito.
Em o modo de operação sem FIFO, onde se analisa apenas a influência do sinal de relógio, o número de ciclos de relógio para se processar um dado é constante.
Por conseqüência, o número de ciclos de relógio em o qual o sinal de sincronismo fica ativo é constante também.
Logo, o período do sinal de sincronismo varia apenas com os períodos dos sinais de relógio dos estágios do pipeline.
Com o objetivo de avaliar de forma aproximada a aleatoriedade inserida nas arquiteturas propostas é realizada uma medição e avaliação estatística sobre o sinal de sincronismo.
A Tabela 5.3 apresenta os resultados.
Inicialmente, observa- se os valores das médias aritméticas de tempo para encriptar um dado.
Nota- se que a média µ aumenta proporcionalmente com o número de estágios usados na implementação da arquitetura.
Este aumento é decorrente do número de sincronizadores usados e do protocolo de comunicação usado entre cada estágio.
Isto é confirmado por os períodos mínimo e máximo para cada uma das arquiteturas e também por os valores de latência medidos e apresentados na Tabela 5.2.
Os valores referentes ao parâmetro Moda indicam os tempos que mais ocorreram durante as avaliações.
Estes valores são resultados dos geradores LFSRs usados para inserir aleatoriedade no processo de escolha de sinais de relógios.
As probabilidades das escolhas de cada um dos 4 sinais disponíveis não são igualmente distribuídas.
Logo, existem sinais de relógio em cada um dos estágios que possuem maiores ocorrências em relação a os demais.
Isto é demonstrado por o parâmetro Moda em cada uma das arquiteturas.
O desvio padrão indica o desvio médio do período de tempo em relação a a média µ.
Quanto maior for o desvio padrão nas arquiteturas propostas, melhor será a aleatoriedade inserida no processamento, conseqüentemente maior será a complexidade das tarefas de ataques para correlacionar o consumo com os dados processados.
Esta Tabela revela que o desvio padrão na arquitetura PIPE-4 é o menor em relação a os demais.
Isto revela que as freqüências usadas nos subsistemas ou as funções pseudo-aleatórias usadas devem ser otimizadas de modo a obter- se um maior desvio padrão.
O sistema utilizado para medir os traços de consumo de potência e de radiações eletromagnéticas é fundamentalmente o mesmo usado na avaliação de STTL.
Porém, neste sistema é necessário enviar dados de 64 bits ao invés de dados de 8 bits, e verificar se o resultado cifrado está correto antes de armazenar o traço medido.
Para avaliar as arquiteturas propostas utilizaram- se os seguintes elementos:
Uma plataforma Digilent Spartan-3 Board com um dispositivo Xilinx Spartan-3 XC3 S1000, uma sonda eletromagnética de 500 µm, um amplificador com baixo nível de ruído para amplificar o sinal obtido por a sonda e aumentar a precisão da medição, uma mesa cartesiana XYZ para posicionamento automático da sonda eletromagnética, um osciloscópio Agilent Infinium DS80000B, uma gaiola de Faraday para eliminar interferências eletromagnéticas e um PC com suporte a scripts MATLAB para controle do sistema de medição.&amp;&amp;&amp;
A Figura 5.6 mostra imagens do sistema de medição empregado.
Esta nova versão do sistema de medição adiciona a mesa cartesiana e a gaiola de Faraday ao sistema descrito no Capítulo 4.
Além disso, o sistema de controle executado no hospedeiro também foi reescrito para uso no ambiente MATLAB.
Estes scripts foram todos desenvolvidos conforme descrevem Lomné E Dehbaoui E usados no DPA Contest 2009.
O uso da sonda eletromagnética permite que sejam explorados vários pontos de medição sobre a superfície do chip a atacar.
Alguns fatores, tais como restrições de planta baixa no projeto da arquitetura e disposição dos pinos de E/ S do chip influenciam na escolha dos pontos de medição.
Os pinos de entrada do sinal de relógio e saída do sinal de sincronização com o osciloscópio produzem ruídos que interferem significativamente na medição de traços.
Havendo a necessidade de automatização da procura por os melhores pontos de medição, desenvolveu- se um script para controlar a mesa cartesiana e realizar uma varredura sobre a superfície do chip, visando selecionar os melhores pontos de medição isto é, os pontos de medição onde se descobre evidências das rodadas executadas.
Os melhores pontos são escolhidos.
A seguir, suas coordenadas cartesianas são registradas para que se realize o processo de aquisição de traços.
Em a avaliação das arquiteturas propostas apenas um ponto de medição foi usado para a coleta de traços referentes às radiações eletromagnéticas geradas por o circuito.
A decisão sobre por este ponto é justificada por o uso de uma simples técnica para posicionamento da sonda eletromagnética sobre o FPGA:
O FPGA é dividido em 64 pequenos quadrados e 5 traços de radiação eletromagnética são adquiridos em cada ponto;
Uma inspeção visual de todos os 320 traços define o melhor ponto como aquele que melhor lembrar a forma de onda da Figura 2.8.
Uma alternativa a este abordagem para definir pontos atacados seria empregar uma técnica tal como Weighted Global Conforme já discutido anteriormente, menos de 200 traços são suficientes para se revelar uma subchave criptográfica de um algoritmo sem métodos de prevenção a ataques SCAs.
Em os experimentos descritos aqui, processa- se uma seqüência de 100 mil dados distintos aleatoriamente gerados com uso de uma única chave criptográfica.
A Figura 5.7 e a Figura 5.8 mostram, respectivamente, os traços de radiação eletromagnética e de consumo de potência obtidos por o sistema de medição para arquiteturas Des PIPE-2.
Em as versões com FIFO (ii e iv), são enviados 3 dados distintos para serem processados, sendo eles dado 1, dado 2 e dado 3.
Porém, os ataques por consumo de potência e/ ou radiação eletromagnética são realizados apenas sobre o dado fim de manter ambos os estágios da arquitetura PIPE-2 processando durante o período em que se realiza a medição e coleta do traço referente a dado 2.
Durante a realização das medições de consumo de potência, nota- se claramente a interferência provocada por uma fonte de ruído não identificada externa ao circuito.
Mesmo com o uso da gaiola de Faraday e uso de uma fonte de alimentação estabilizada não foi possível eliminar- la.
Em a Figura 5.8 e é possível identificar claramente esta interferência.
Como estas arquiteturas utilizam sinais de relógios com diferentes freqüências não é possível realizar a média dos traços adquiridos para reduzir os efeitos do ruído sobre os traços.
Em este caso, o tempo de processamento de cada dado varia de acordo com as freqüências selecionadas e a realização da média ocultaria a fuga de informações contidas nos traços.
FIFO, GALS e GALS com FIFO.
A Figura 5.9 apresenta as medidas de radiação eletromagnética obtidas para as arquiteturas Des PIPE-4.
Em é possível identificar as rodadas sendo processadas nos 4 estágios do pipeline.
Cada estágio executa 4 rodadas do algoritmo.
Estes grupos de 4 rodadas são identificados na Figura 5.9.
Porém nas versões e as rodadas não são claramente visíveis.
Em são enviados 5 dados distintos para manter o pipeline completo no instante da medição.
Em este caso, as medições são realizadas apenas sobre o dado 3.
Estas arquiteturas foram prototipadas sobre um FPGA Spartan-3 XC3 S1000.
Em este dispositivo as medições de consumo de potência obtiveram traços com uma qualidade ruim do sinal, não sendo possível identificar as rodadas do mesmo com a prototipação da arquitetura em modo síncrono.
A hipótese provável para isso não foi identificada, pois a plataforma é a mesma em ambos os casos, apenas o dispositivo é alterado.
Por este motivo, apenas medições de traços de radiação eletromagnética foram realizadas.
A Figura 5.10 apresenta traços de radiação eletromagnética das arquiteturas PIPE8.
A Figura 5.10 contém o traço para uma arquitetura Des PIPE-8 síncrona.
Em mostra- se um traço da arquitetura Des PIPE-8 com FIFO e num traço referente a a radiação eletromagnética da arquitetura GALS Des PIPE-8.
como se pode perceber em e, não é possível identificar as rodadas do algoritmo Des em execução.
Os demais traços de consumo de potência e radiação eletromagnética medidos estão disponíveis no Apêndice A deste volume.
Para avaliar a robustez das arquiteturas propostas são utilizados quatro tipos de ataques:
DPA, DEMA, CPA e CEMA.
Inicialmente, 100 mil traços foram coletados para executar os ataques.
A Tabela 5.4 apresenta os resultados relativos às análises DEMA e DPA.
Estes resultados representam o número mínimo de traços necessários para descobrir as subchaves referentes às SBOXes 1 a 8, as quais foram alvos dos ataques.
A partir destes resultados nota- se que nenhuma das versões GALS propostas permitiram revelar suas subchaves, como esperado.
Todas as versões síncronas tiveram suas subchaves descobertas provando usa vulnerabilidade.
O uso exclusivo do processamento paralelo das rodadas como contramedida não apresenta melhoras significativas na resistência a estes ataques e, em alguns casos, os ataques foram até mais eficazes em versões com processamento paralelo, tal como PIPE-2 e PIPE-8.
As análises DEMA aplicadas sobre a arquitetura PIPE-2 apresentaram um comportamento atípico para a arquitetura síncrona.
Um provável motivo para este comportamento é posicionamento da sonda eletromagnética durante o processo de coleta de traços.
A posição escolhida provavelmente capta radiações eletromagnéticas com baixa intensidade produzidas por o circuito SBOX8, o que reduz a quantidade de informação e dificulta as análises.
Uma solução alternativa ao problema do posicionamento da sonda é o uso de WGMSI.
Já os resultados obtidos por as análises DPA demonstram um comportamento esperado, ou seja, um menor número de traços para se obter a chave criptográfica em relação a a versão síncrona com processamento paralelo.
Observando estes resultados e comparando- os com os obtidos na avaliação do Des STTL na Tabela 4.6 nota- se que com 100 mil traços 3 subchaves são reveladas na arquitetura Des STTL e evidências estatísticas demostram que outras subchaves ficam próximas de serem reveladas.
De este modo um aumento para 400 mil traços permitiu revelar as demais subchaves, à exceção de a subchave 1.
Entretanto, para as arquiteturas GALS PIPE os resultados dos ataques não apresentam indicação, mesmo com 100 mil traços, de convergir para a subchave correta.
Para justificar estes resultados, duas razões explicam o fato de não ocorrer uma convergência durante as análises executadas sobre as arquiteturas GALS PIPE.
A primeira de elas, o aumento da aleatoriedade no processamento, sendo efetivo em aleatorizar o momento da encriptação alvo e o tempo de duração da mesma.
A outra justificativa é dada a execução superposta dos estágios que de certa forma contribuem para dificultar as análises.
A execução destas análises sobre 100 mil traços exigiu até dois dias para serem concluídas.
Em a maioria dos casos, revelar a subchave exigiu apenas uma pequena fração do número total de traços.
De a mesma forma, a execução das análises sobre a arquitetura STTL usando 400 mil traços exigiu um tempo de processamento em torno de uma semana para revelar quase a totalidade das subchaves.
A Tabela 5.5 apresenta os resultados referentes às análises CPA e CEMA.
Nota- se que estas análises também encontram as subchaves secretas do algoritmo, de um modo geral com um menor número de traços.
Apenas num caso, sobre a arquitetura PIPE-2 com FIFO, a análise CEMA exigiu um maior número de traços em relação a a DEMA.
Em dois outros casos, existe um equilíbrio, tal como nas arquiteturas PIPE-4 sem FIFO e PIPE-2 com FIFO.
Com base nesta amostra de resultados é possível afirmar que em geral ataques por correlação são mais eficientes que os demais.
A Figura 5.11 apresenta o número mínimo necessário de traços para revelar cada uma das subchaves do algoritmo Des.
Com estes resultados é possível concluir que, de um modo geral, o ataque DPA exige um maior número de traços em relação a os demais ataques.
A análise por correlação aplicada sobre os traços de consumo de potência (CPA) é ligeiramente mais eficiente que DPA considerando os resultados obtidos para cada uma das arquiteturas.
De um modo geral, as análises sobre traços de radiações eletromagnéticas mostram- se mais eficientes em relação a o consumo de potência.
Em alguns casos o número de traços é praticamente o mesmo, em raras ocasiões exige um número maior de traços.
O mesmo comportamento é encontrado entre as análises DEMA e CEMA, ou seja, as análises por correlação demonstram de um modo geral, um melhor comportamento em relação a análises diferenciais.
Com estes resultados conclui- se que ataques por correlação exigem um número menor de traços para revelar a chave secreta de um algoritmo ao custo de um maior tempo de processamento para executar as análises.
Os gráficos apresentados na Figura 5.12 demonstram a robustez obtida com o ruído inserido por o processamento paralelo dos estágios operando em modo síncrono.
Os resultados não demonstram uma vantagem significativa do processamento paralelo para neutralizar os ataques, confirmando as conclusões obtidas por Standaert Em.
Embora não contribuam significativamente no aumento da robustez quando usados em arquiteturas síncronas, esta contribuição potencializa a robustez a ataques quando acrescida à aleatoriedade inserida por as arquiteturas GALS PIPE.
Os gráficos da Figura 5.13 apresentam uma comparação dos resultados obtidos com as análises DEMA e CEMA realizadas sobre as arquiteturas PIPE propostas.
Notase, a partir destes gráficos, que na maioria dos casos, a arquitetura com maior número de estágios exigiu um número maior de traços para revelar a chave secreta do algoritmo e a arquitetura com menos estágios exigiu menos traços na maior parte das análises.
Com estas informações é possível concluir que apenas a replicação do bloco de encriptação do algoritmo numa arquitetura síncrona já aumenta a robustez a ataques por análise de radiação eletromagnética.
Os resultados até aqui comprovam mais uma vez que circuitos síncronos são vulneráveis a ataques SCA.
A eficiência destas análises depende da precisão do sistema de medição usado.
Ou seja, quanto maior for a razão SNR dos traços de consumo de potência e de radiação eletromagnética medidos, mais eficientes serão as análises.
Considera- se uma análise eficiente aquela que conseguir revelar a chave criptográfica de um criptosistema com o menor número possível de traços.
As análises sobre as arquiteturas GALS propostas não obtiveram êxito.
Nenhuma das oito subchaves referentes às SBOXes atacadas foi revelada, mesmo usando 100 mil traços obtidos de dados distintos.
Isto comprova que as arquiteturas propostas são robustas a este tipo de ataques.
As Figuras a seguir demonstram os resultados finais obtidos para cada uma das arquiteturas propostas e submetidas à avaliação de robustez.
A Figura 5.14 mostra os resultados das análises DPA e DEMA sobre a SBOX3 em todas as arquiteturas Des PIPE GALS implementadas.
Cada gráfico mostra os 64 traços hipóteses correspondentes às subchaves possíveis.
Os traços pretos correspondem à hipótese correta da subchave, os traços vermelhos correspondem à hipótese incorreta da subchave, já os traços azuis correspondem aos demais traços hipóteses.
Quando o traço hipótese de maior pico é um traço preto, o ataque é bem sucedido, ou seja, a subchave correta foi encontrada.
Por outro lado, se o traço com maior pico não corresponde à subchave correta, este traço é representado em vermelho correspondendo a uma hipótese incorreta, ou seja, um ataque mal sucedido.
FIFO. Resultados das análises DEMA para as mesmas arquiteturas são mostrados em e.
As análises DEMA desenvolvidas sobre as arquiteturas Des PIPE-4 e Des PIPE-8 versões com FIFO são apresentadas em e.
Os traços hipóteses pretos correspondem à subchave correta, os traços vermelhos correspondem à subchave incorreta e os demais traços azuis completam as 64 hipóteses possíveis de subchave para a SBOX3 do algoritmo Des.
Em a Figura 5.14 mostra- se em e os resultados das análises DPA sobre as arquiteturas GALS PIPE-2 nas versões sem e com FIFO.
Os resultados das análises DEMA sobre as mesmas arquiteturas aparecem respectivamente em e.
As arquiteturas PIPE-4 e PIPE-8 com FIFO foram submetidas apenas a análises DEMA, devido a o longo tempo de medição e a restrições de espaço em disco necessário (na ordem de Terabytes).
Os resultados aparecem em e.
Em todas as análises, nota- se que não foi possível encontrar a subchave secreta, mesmo usando 100 mil traços nas análises.
Observa- se também certo equilíbrio entre os traços hipóteses.
Em todos os casos não existe um traço hipótese que tenha uma amplitude destacada entre os demais.
Logo, as margens de traços hipóteses são pequenas, o que reduz a eficiência dos ataques.
Este mesmo efeito ocorre nas demais SBOXes do algoritmo submetidas aos ataques.
A aleatoriedade inserida no processamento do algoritmo por a arquitetura proposta demonstra um aumento da robustez a ataques DPA e DEMA como necessário em sistemas criptográficos robustos.
Embora os resultados tenham comprovado o comportamento desejado das arquiteturas, outras análises específicas para este tipo de contramedida devem ser realizadas sobre as arquiteturas propostas, tais como as análises propostas por Nagashima Discutida anteriormente.
Assim, poderia- se- afirmar que mesmo submetido a análises específicas o método proposto mostra- se robusto.
Este Capítulo apresentou uma nova proposta de arquitetura pipeline GALS para o algoritmo criptográfico Des, com o objetivo de neutralizar a ação de análises de consumo de potência e de radiações eletromagnéticas.
Pela primeira vez obtém- se robustez combinando o uso de replicação de blocos de encriptação operando em modo pipeline e comunicação assíncrona entre os blocos.
Cada bloco de encriptação é suprido com sinais de relógio com freqüências distintas sorteadas aleatoriamente antes de processar um dado para aumentar a aleatoriedade do processamento do circuito visando neutralizar ataques DPA e DEMA.
O compromisso entre robustez e área é a principal preocupação do método.
Comparado a uma implementação regular do Des (não-pipeline), a arquitetura proposta realmente apresenta alto custo em área.
Entretanto, comparada ao estado da arte de lógicas assíncronas propostas para conceber sistemas criptográficos seguros tal como STTL, a maioria das arquiteturas GALS pipeline exige menos área.
Além disso, os resultados obtidos com as análises DPA, DEMA, CPA e CEMA demonstram excelente robustez.
Quando as mesmas análises são aplicadas a arquiteturas síncronas nas mesmas condições, praticamente todos os estudos de casos obtiveram sucesso na tarefa de descobrir a chave secreta.
As avaliações de robustez realizadas confirmam também as previsões de Standaert Em, onde os Autores afirmam que apenas o uso da implementação pipeline não é suficiente para resistir a ataques SCA.
Porém, o uso de arquiteturas pipelines onde cada estágio tem seu próprio domínio de freqüência e ainda, o sinal de relógio em cada estágio tem sua freqüência alterada dinamicamente mostra- se robusta a ataques.
A partir destes experimentos nota- se também que as análises por correlação de potência são mais eficientes.
De um modo geral, todos os ataques por correlação revelaram as chaves secretas com um número menor de traços em relação a as análises propostas por Kocher.
Porém, nem mesmo o uso de um modelo de potência correlacionado às medições reais geradas, tal como o usado nas análises CEMA e CPA, foi suficiente para neutralizar a contramedida proposta.
A robustez da abordagem proposta está diretamente associada à aleatoriedade inserida no processamento associado ao método GALS de projeto.
O aumento do número de estágios da arquitetura paralelizando o processamento do algoritmo Des permite que se tenha uma maior aleatoriedade no sistema e conseqüentemente se torne mais difícil a identificação de informações que escapem por os canais colaterais do sistema.
Este Capítulo apresenta uma relação das contribuições do trabalho, na Seção 6.1.
Em seguida, mostra- se um conjunto de conclusões resultantes do trabalho na Seção 6.2.
Finalmente a Seção 6.3 apresenta um conjunto de sugestões de trabalhos futuros.
De entre as contribuições deste trabalho citam- se aqui as mais importantes:
Revisão do problema da fuga de informações por canais laterais:
A primeira contribuição deste trabalho consiste na revisão do problema da fuga de informações através de características físicas dos circuitos, tais como o tempo de processamento, o consumo de potência (análises DPA) e a radiação eletromagnética (análises DEMA), conforme apresenta o Capítulo 2.
Revisão do estado da arte:
A segunda contribuição deste trabalho é a elaboração do estado da arte de propostas de contramedidas às análises DPA.
No que diz respeito ao alvo desta pesquisa, apresenta- se uma revisão de propostas subdivididas em três grupos, cada um destes contendo uma abordagem para neutralizar análises DPA.
O primeiro grupo emprega mascaramento de dados, o segundo inserção de aleatoriedade no sistema, tal como proposto nesta tese e o terceiro grupo emprega a uniformização do consumo de potência durante o processamento de dados.
O Capítulo 3 descreve esta contribuição.
Biblioteca STTL: A terceira contribuição do trabalho consiste na proposta, elaboração e validação de uma biblioteca de células para a prototipação eficiente da lógica STTL em FPGAs.
A lógica STTL foi inicialmente proposta e validada através do uso de uma biblioteca dedicada, usando standard cells CMOS voltada para implementação em ASICs.
Esta biblioteca apresenta as mesmas premissas protótipos são baseados em estilo de projeto assíncrono insensível a atrasos, usando hard macros para obedecer a restrições de tempo.
O Capítulo 4 deste trabalho detalha esta biblioteca de prototipação.
Implementação de script para automatizar fluxo de projeto de circuitos STTL:
A quarta contribuição deste trabalho é a implementação de um script para converter descrições verilog de portas lógicas (Sr) contidas em netlists para instâncias de hard macros STTL descritas em VHDL a fim de automatizar o fluxo de projeto STTL.
Implementações do algoritmo Des em pipeline GALS:
A quinta contribuição deste trabalho é a proposta da primeira implementação do algoritmo Des em modo pipeline usando o método GALS de projeto.
As ilhas síncronas de encriptação usam um sinal de relógio com freqüências que mudar de forma pseudo-aleatória, o que aumenta a robustez do sistema criptográfico a análises de consumo de potência e de radiação eletromagnética.
O Capítulo 5 deste trabalho apresenta e discute esta contribuição.
Infraestrutura de avaliação dos sistemas:
A sexta contribuição do trabalho surgiu da necessidade de se desenvolver uma infraestrutura que permita a prototipação de sistemas criptográficos, tanto usando a lógica STTL quanto usando a abordagem pipeline GALS, oferecendo recursos que permitam a comunicação com um hospedeiro e com um osciloscópio para medição dos traços necessários.
Sistema de medição de traços de consumo:
Um sistema para medição do consumo de potência e a radiação eletromagnética produzidos por um sistema prototipado em FPGA foi proposto neste trabalho.
Publicações: O desenvolvimento deste trabalho resultou até o momento num conjunto de publicações.
Dois trabalhos referem- se ao início do doutorado, envolvendo o uso de hard macros em interfaces assíncronas em FPGAs e implementação de redes intrachip com método GALS de projeto, respectivamente.
Os quatro trabalhos seguintes dizem respeito às atividades desenvolvidas durante o estágio sanduíche, realizado no LIRMM (em Montpellier, França).
O último trabalho é a primeira publicação de resultados iniciais de avaliação da robustez da abordagem GALS pipeline.
A publicação mais extensa de resultados é trabalho em andamento.
&quot;Triple Rail Logic Robustness against «DPA».
In: International Conference on Reconfigurable Computing and FPGA , Dec 2008, pp. 415-420.
In: Design, Automation and Test in Europe Conference and Exposition, Mar 2009, pp. 634-639.
Calazans, N.;
Moraes, F.;
&quot;Secure Triple Track Logic Robustness Against vol. 4-1, Mar 2009, pp. 20-28.
GALS Pipeline Des Architecture to Increase Robustness Against DPA and, Sep 2010, pp. 115-120.
Com a realização deste trabalho foi possível verificar que a tecnologia CMOS usada no projeto de circuitos digitais tem características de consumo de potência dependentes dos dados computados por o circuito.
Além disso, o paradigma síncrono de projeto de circuitos, utilizado por a grande maioria dos equipamentos eletrônicos, usa um sinal global de relógio para sincronizar todas as operações executadas por um sistema.
Estas duas características são as principais responsáveis por a fuga de informações relevantes que tornam sistemas computacionais vulneráveis a análises baseadas no consumo de potência e na radiação eletromagnética.
A revisão da literatura sobre o problema da fuga de informações através de canais laterais revelou, de um modo geral, que existem três possíveis abordagens para conceber sistemas seguros evitando a correlação de dados sigilosos através da fuga de informações.
Duas abordagens, mascaramento de dados e inserção de ruído e aleatoriedade, não alteram nem as características da lógica à qual são projetados os circuitos nem o fluxo de projeto.
Elas criam meios para ocultar a fuga de informações existente nos circuitos.
Já a abordagem por uniformização do consumo de potência visa propor alterações na estrutura lógica e no fluxo de projeto a fim de obter circuitos onde o consumo de potência, o tempo de propagação dos sinais e a radiação eletromagnética emitida por o circuito sejam constantes e independentes dos dados computados.
Teoricamente, a tarefa de projetar circuitos com consumo de potência uniforme parece ser uma forma simples e eficaz de eliminar a fuga de informações.
Porém na prática esta tarefa é complexa e exige um grande esforço de projeto, tal como a construção de bibliotecas lógicas personalizadas e a definição de fluxos de projeto com diversas restrições para obter caminhos balanceados para que o circuito tenha um consumo de potência o mais próximo possível do caso ideal.
Apesar destes esforços, um sistema de medição robusto capaz de medir e armazenar algumas centenas de milhares de traços, adicionado a um longo tempo de processamento possibilita revelar a chave secreta na maioria das situações.
Com a revisão da literatura foi possível verificar que a abordagem por uniformização do consumo de potência implica custos elevados em termos de área, latência e potência dissipada.
Como exemplo desta, o uso de lógica assíncrona com codificação em trilha dupla representa um aumento significativo da área.
Experimentos realizados com a lógica STTL demonstraram um aumento significativo da robustez a custos similares ao de lógicas em trilha dupla.
Portanto, abordagens alternativas acabam sendo soluções de menor custo em termos de área e latência.
Por outro lado, sabe- se que estas abordagens não eliminam a fuga de informações, apenas a ocultam de tal modo que se tenha esforço computacional elevado e algoritmos complexos para revelar os dados secretos, sendo ainda possível obter análises com sucesso.
O trabalho proposto foi desenvolvido visando obter arquiteturas robustas a ataques por análise do consumo de potência e da radiação eletromagnética através da inserção de aleatoriedade no consumo de potência do circuito.
O trabalho combina o uso do modo de implementação pipeline tal como proposto em e, porém com comunicação assíncrona entre os estágios, conforme a metodologia GALS de projeto.
Além disso, os estágios operam em diferentes domínios de freqüência com variação da freqüência do sinal do relógio para cada dado processado, similar aos trabalhos propostos em e.
Implementou- se o algoritmo Des em oito versões usando arquitetura pipeline:
PIPE-2, PIPE-4, PIPE-8 e PIPE-16.
Cada uma destas pode operar em modo síncrono e modo GALS.
Estas arquiteturas, prototipadas em FPGA, foram submetidas a ataques DPA, CPA, DEMA e CEMA, com exceção da arquitetura PIPE-16 que não foi possível prototipar na plataforma usada nos experimentos.
As avaliações de robustez realizadas confirmam os resultados obtidos por Standaert Em, onde os Autores afirmam que apenas o uso de implementação pipeline não é suficiente para resistir a ataques DPA.
O processamento paralelo dos estágios demonstrou um pequeno aumento da robustez, porém todas as análises obtiveram sucesso.
Um dado interessante obtido a partir de a avaliação dos resultados foi que, na maioria dos casos a arquitetura com maior número de estágios exigiu um número maior de traços para revelar a chave secreta do algoritmo, significando nesse caso um aumento da robustez.
Todas as arquiteturas síncronas tiveram as chaves criptográficas reveladas por as análises.
Já as arquiteturas GALS, com diferentes domínios de relógios e variação de freqüência não tiveram nenhuma subchave revelada por as análises.
Com a avaliação dos resultados das análises pode- se concluir que os ataques por correlação (CPA e CEMA) são mais eficientes em relação a os demais.
Uma redução no número de curvas foi constatada nas avaliações efetuadas.
Esta eficiência é obtida ao custo de um aumento no tempo de processamento, em geral cinco vezes maior que ataques DPA ou DEMA.
Em relação a o canal lateral analisado, as análises de radiação eletromagnética se mostraram ligeiramente melhores em relação a ataques por consumo de potência.
Isto se deve também ao ruído existente no sistema de medição usado durante os experimentos, que prejudica as análises por consumo de potência.
A robustez obtida através da tese proposta está diretamente associada à aleatoriedade inserida ao processamento.
O aumento do número de estágios da arquitetura particiona o processamento das rodadas do algoritmo Des em estágios, permitindo que se tenha uma maior aleatoriedade no sistema e conseqüentemente torne mais difícil a tarefa de identificar informações por canais laterais do sistema.
Os resultados das análises comprovaram que o aumento do número de estágios, o processamento paralelo das rodadas e a variação da freqüência do sinal do relógio em cada estágio contribuem para dificultar os ataques, ou até mesmo inviabilizar- los.
Acredita- se que usando novas técnicas que empreguem um pré-processamento de traços a fim de ressincronizar- las haja uma possibilidade de contra-atacar a implementação proposta.
De este modo, pode ser possível obter resultados que comprovem os limites da imunidade obtida com esta tese proposta.
Esta Seção apresenta um conjunto de sugestões para trabalhos futuros.
A primeira sugestão de trabalhos é a avaliação da robustez usando alguma técnica de pré-processamento sobre os traços coletados tais como filtros de sinais como proposto por Nagashima Em.
De este modo, a avaliação da robustez das arquiteturas propostas em relação a as análises por consumo de potência e radiação eletromagnética torna- se mais real.
A segunda sugestão é propor uma técnica que permita a ressincronização dos traços de consumo de potência e radiação eletromagnética, de modo que as análises possam ser executadas com maior precisão.
O processo de validação e prova de conceito das arquiteturas propostas neste trabalho foram realizados sobre dispositivos FPGAs conforme descrito nos diversos experimentos.
Estes dispositivos compostos de várias estruturas programáveis tais como blocos lógicos onde as LUTs são configuradas e canais de interconexão de sinais por onde são roteados os fios de comunicação.
Embora seja um dispositivo confiável para a prova de conceito destes experimentos, estas estruturas programáveis consomem energia adicional e são fontes potenciais de geração de ruído.
Portanto, sugere- se aqui o projeto de um circuito de aplicação específica (ASIC) para as arquiteturas aqui propostas visando avaliar sua robustez num chip dedicado a este propósito.
As arquiteturas propostas apresentam uma vulnerabilidade decorrente do modo pipeline de implementação.
As análises propostas por Kocher realizam ataques na primeira e na última rodada do algoritmo.
Logo, quando apenas um dado é processado no pipeline, o método aqui proposto comporta- se tal como o método proposto por Lu Uma sugestão interessante é o projeto de um bloco de encriptação genérico, ou seja, um bloco que permite a execução de n 16 rodadas do algoritmo.
Assim, um pipeline pode ter a opção de executar rodadas em diferentes estágios.
Isto além de aumentar a aleatoriedade no processamento também dificultaria as análises de radiação eletromagnética, pois a execução daria- se- em diferentes regiões do chip em instantes diferentes.
Outra opção é o projeto de uma rede de interconexão blocos de encriptação genéricos, o que expande o processamento aleatório em duas dimensões do chip.
Obviamente, estes métodos aumentam ainda mais a penalidade em área e potência dissipada, mas a segurança obtida pode justificar tal método.
A automatização do projeto também é importante para o uso de um método de contramedida.
Uma sugestão neste sentido é o desenvolvimento de uma ferramenta de geração automática de pipeline Des.
Esta ferramenta pode ser capaz de gerar arquiteturas pipeline de diversos tamanhos, com 2 a 16 blocos de encriptação do Des.
Cada estágio pode ter seu hardware facilmente replicado.
Porém, um cuidado necessário é levar em conta no projeto do circuito a geração de subchaves, pois como mostrado no Otimizações na arquitetura podem ser realizadas de modo a reduzir os custos em área de sua implementação.
O subsistema de relógio também pode ser otimizado de modo a oferecer um maior número de freqüências de sinais de relógio a menores custos de implementação visando obter uma melhor aleatoriedade e conseqüentemente uma maior robustez a ataques.
