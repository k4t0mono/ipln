Com o aumento da densidade e da freqüência de operação dos circuitos integrados, equipamentos de teste mais sofisticados e caros são necessários para garantir a qualidade dos dispositivos fabricados.
Para reduzir as restrições tecnológicas destes equipamentos de teste e o custo total de teste, tem- se pesquisado formas de executar partes do teste dentro de o próprio dispositivo.
A abordagem mais comum e a adição de módulos de hardware projetados para realizar partes da etapa de teste.
Esta abordagem é conhecida como BIST, built-in selt-test..
Entretanto, a adição destes módulos implica em custos principalmente em área de hardware e desempenho.
Os atuais circuitos integrados complexos (SOC) são projetados, em geral, com um ou mais processadores embarcados.
Baseado na capacidade de programação destes processadores e nas técnicas de teste de hardware, este trabalho busca estudar formas alternativas de inserir funcionalidades de teste no SOC de forma a minimizar os custos de teste.
A técnica estudada é chamada de teste baseado em software, onde um código escrito numa linguagem de programação é responsável por as principais tarefas do teste.
Entretanto, sabe- se que o uso desta técnica também implica em certos custos, como maior requisito de memória de teste e tempo de teste.
Os principais objetivos deste trabalho são avaliar os custos da técnica de teste baseada em software em relação a o teste baseado em hardware (i.
e BIST) e construir um ambiente para teste baseado em software.
Os resultados preliminares desta avaliação mostram um pequeno aumento dos requisitos de memória e um aumento significativo no tempo de teste, quando comparado com a implementação puramente hardware.
Lucro O constante avanço tecnológico dos semicondutores, definido na lei de Moore, e o aumento do mercado de dispositivos eletrônicos tem impulsionado o desenvolvimento de sistemas computacionais num único circuito integrado (Ci).
Tais sistemas são tipicamente compostos de milhões de transistores que englobam hardware digital e analógico.
Como exemplos de módulos que compõem um system-on-a-chip (SOC) podemos citar microprocessadores, memória RAM e ROM, DSP, Conversores D/ A e A/ D, LDU (lógica definida por o usuário) e até mesmo tecnologias mais recentes como MEMS (MicroElectroMechanical Systems) e eletro-ótica.
A necessidade de reuso para aumentar a produtividade se reflete na previsão da Semiconductor Industry Association (SIA) que prediz que em 2005 90% da área dos CIs será composta por núcleos de hardware (em inglês, cores).
Estes núcleos são descrições de hardware complexo pré-projetadas e pré-verificadas que podem ter sido implementadas por o próprio usuário ou por terceiros.
Para o projetista acompanhar a evolução da complexidade de projeto, técnicas mais abstratas devem ser desenvolvidas, baseadas no suporte de ferramentas de CAD.
O projeto conjunto de hardware e software (em inglês, hardware/ software codesign), visa o desenvolvimento de ferramentas de CAD para automatizar o processo de concepção paralela das partes de software e de hardware de um sistema computacional.
O objetivo de codesign é encontrar a melhor partição entre hardware e software a partir de uma especificação do sistema que atenda às restrições desta.
Como dito anteriormente, não são incomuns sistemas compostos por dezenas de módulos que compreendem hardware, software, memória, sub-sistemas analógico e eletro-mecânico.
Estes sistemas são chamados de sistemas heterogêneos.
Para tornar gerenciável a complexidade imposta por a quantidade e heterogeneidade dos módulos são necessárias novas metodologias de concepção e de especificação de sistemas.
A metodologia de concepção de sistemas computacionais heterogêneos utilizando técnicas abstratas de desenvolvimento possui vantagens como:
Uso de especificação de projeto realizada no nível sistêmico, que permite iniciar o desenvolvimento de uma especificação com pequeno nível de detalhes, sendo os mesmos incrementados no decorrer de o projeto;
Curto ciclo de concepção, atendendo restrições de tempo necessárias para a chegada do produto no mercado.
Isto é possível devido a a adoção do fluxo concorrente de concepção de hardware e software, ao invés de o fluxo seqüencial tradicional.
A Figura 3 apresenta um fluxo básico de codesign.
O projeto inicia com uma especificação funcional do sistema numa ou várias linguagens.
O objetivo da co-simulação sistêmica é validar e explorar os algoritmos e a funcionalidade do sistema.
A próxima etapa, executada por uma ferramenta de co-síntese, é responsável por produzir descrições de componentes de hardware e software e também por selecionar o protocolo de comunicação entre estes.
A cosimulação arquitetural visa validar as descrições de hardware e software geradas, incluindo as primitivas abstratas de comunicação.
O nível de ciclo, que utiliza ferramentas de síntese de hardware e compilação de software, mapeia as descrições de hardware e software para a arquitetura e processador alvo, respectivamente.
As interfaces de comunicação são implementadas em hardware dedicado.
A co-simulação no nível de ciclo utiliza um simulador HDL para a parte de hardware e de comunicação e um simulador do processador alvo para a parte de software para a validação do sistema completo.
Uma validação mais precisa pode ser obtida através da prototipação do sistema em, por exemplo, dispositivos FPGA (Field Programmable Gate Array).
Como dito anteriormente, as etapas de concepção e verificação de um projeto compõem a maior parte do tempo de desenvolvimento de CIs.
Porém, conforme a Figura 4, é previsto que próximo de 2015 o custo de teste de um transistor irá exceder o custo de desenvolver- lo, a menos que outras técnicas de teste sejam empregadas.
A Figura 4 apresenta duas curvas, uma que representa o custo de fabricação por transistor ao longo de os anos, e a outra representa o custo de teste de um transistor também ao longo de o tempo.
Conforme estas curvas, é previsto que próximo de 2012 o custo de fabricação esteja próximo de o custo de teste de um sistema.
De entre os motivos que levam ao aumento de custo de teste podemos citar o aumento de número de transistores por pino, o que dificulta o acesso e o controle dos nós internos do Ci por um equipamento de teste (em inglês, Automatic Test Equipment -- Ate).
Por outro lado, o desenvolvimento acelerado da tecnologia e o conseqüente aumento na freqüência de operação dos dispositivos, chegando a freqüências da ordem de GHz, tem tornado cada vez mais difícil realizar teste em tempo de execução, uma vez que os próprios equipamentos de teste devem acompanhar o estado-da-arte.
Isto acarreta alto custo de tais equipamentos.
A SIA prevê que um equipamento de teste estado- da arte custará mais de 20 milhões de dólares em 2014.
Além de isto, o tempo de teste utilizando estes aparelhos levará horas por dispositivo devido a o aumento de nós internos que devem ser testados e a baixa largura de banda dos testadores.
Para resolver este problema, tem- se pesquisado fluxos de projeto que visam facilitar a etapa de teste após a fabricação do Ci.
O nome deste conjunto de técnicas é projeto visando a testabilidade (em inglês, Design for Testability -- DFT).
Diversos exemplos de aplicações desenvolvidas em SOCs podem ser encontrados não somente no ambiente acadêmico mas também na indústria.
Em a maioria destes casos existe algum módulo programável como um processador, processador de sinais digitais (digital signal processor ­ DSP) ou microcontrolador.
A presença deste tipo de módulo num SOC confere ao mesmo características de programabilidade, flexibilidade, reuso de software e hardware.
Por outro lado, existe a necessidade de garantir a qualidade, robustez, confiabilidade e disponibilidade dos dispositivos desenvolvidos.
Técnicas de teste descritas em hardware são utilizadas para garantir estas qualidades aos dispositivos.
Uma vez que os SOCs contêm normalmente ao menos um processador, estas mesmas técnicas de teste podem ser implementadas em software, atingindo os quesitos de flexibilidade e confiabilidade desejados.
Explorar rotinas de software que testem hardware pode trazer benefícios como:
A ausência ou redução da área de silício adicional para executar o teste, diminuindo o custo do produto final e não degradando o desempenho do produto;
A possibilidade de executar múltiplos algoritmos de teste resulta numa maior qualidade de teste.
O número de algoritmos aplicados seria limitado somente por a restrição de tempo de teste da especificação.
Benso Argumentam que o uso de múltiplos algoritmos de teste implementados em hardware aumenta a cobertura de falhas do sistema, portanto sua confiabilidade;
A flexibilidade intrínseca do software que possibilita aplicar diferentes rotinas de teste em situações ou períodos de tempos variados.
Por exemplo, na inicialização do dispositivo um teste mais robusto (com maior cobertura de falhas) poderia ser aplicado.
Já durante o funcionamento, poderia ser escolhida uma rotina de teste com menor cobertura, mantendo um compromisso entre confiabilidade e tempo de teste.
Outra vantagem da flexibilidade é a facilidade de trocar o algoritmo de teste.
Hellebrand Afirmam que a flexibilidade do software favorece a escolha do melhor método de geração de estímulos de teste para cada módulo a ser testado;
O reuso das rotinas de teste é facilitado por serem implementadas em software.
Uma biblioteca de software de teste escrita em linguagem C é facilmente aplicável a um processador;
A ausência ou redução de lógica de controle de teste, pois o próprio processador se encarrega do fluxo de teste.
As desvantagens são:
SOC deve possuir um componente programável como processador, microcontrolador ou DSP;
Tamanho do programa de teste e tamanho da memória para armazenar programa de teste e padrões de teste;
Software de teste deve ser considerado livre de falhas;
O possível aumento do tempo de teste.
Em a revisão bibliográfica apresentada na Seção 4.2 foram encontradas algumas aplicações acadêmicas e industriais de teste baseado em software.
Porém, a automatização e avaliação deste processo ainda são incompletas.
Por exemplo, existem abordagens acadêmicas que automatizam a geração de código de teste para processadores e aplicações de software para testar memória.
Porém, não foram encontrados trabalhos que automatizassem o processo de geração de código de teste para núcleos não programáveis, tampouco um framework que automatizasse o teste de um SOC completo considerando apenas módulos digitais.
Em o que concerne a avaliação de técnicas de teste baseadas em software, não existem trabalhos que demonstrem de forma quantitativa suas vantagens e, principalmente, suas desvantagens.
Esta lacuna impede a definição objetiva da aplicabilidade desta técnica de teste, deixando sem respostas perguntas como:
&quot;Teste baseado em software pode ser utilizado para teste de fabricação?»
ou «Esta técnica pode ser aplicada em teste de campo?
Quais são as penalidades e vantagens?».
Provavelmente não existe uma resposta única para estas perguntas devido a a heterogeneidade e variações de restrições de cada projeto.
O objetivo estratégico deste trabalho é realizar a avaliação de teste de hardware baseado em software, e responder às perguntas acima utilizando estudos de caso.
Desta forma, alguns objetivos específicos moldaram a busca por o objetivo maior, como por exemplo:
O desenvolvimento de técnicas de teste flexíveis para SOCs com arquitetura de interconexão por barramento;
O desenvolvimento de técnicas de teste de hardware digital que possam ser aplicadas tanto em teste de produção quanto em teste de campo.
Os SOCs podem possuir módulos analógicos, eletro-mecânicos e eletro-óticos.
Porém, este trabalho limita- se ao uso de memórias RAM, ROM, núcleos de hardware não programáveis e microprocessadores simples, e.
g 8051;
Explorar o teste de SOCs utilizando somente técnicas de teste baseadas em software ou misto hardware/ software.
Deve- se identificar, e não necessariamente implementar, formas de minimizar as principais desvantagens de teste baseado em software que são o tempo de teste e tamanho do programa de teste;
Avaliar as técnicas de teste implementadas.
Uma proposta é:
Avaliar tempo de teste do SOC usando a mesma estratégia de teste implementada em software e a outra em hardware;
Explorar a flexibilidade do software em relação a o hardware para conseguir uma maior cobertura de falhas para o SOC.
Para dar suporte aos objetivos acima, este trabalho resultou no desenvolvimento das seguintes ferramentas de CAD:
Ambiente de co-simulação funcional e geograficamente distribuída de descrições de hardware em VHDL e módulos de software em linguagem C (Capítulo 5);
Ambiente de desenvolvimento e teste de SOC.
O ambiente de desenvolvimento integra diversos núcleos descritos em VHDL e o ambiente de teste suporta geração de código de teste para núcleos não programáveis (Capítulo 7).
Não é objetivo deste trabalho:
Explorar técnicas de teste mais abstratas que consideram teste no início do projeto como síntese para testabilidade e testabilidade no nível de sistema;
Explorar técnicas de teste de software.
Uma vez que o objetivo é teste de hardware, somente teste de hardware digital é considerado.
Pressupõe- se que a parte de software do projeto foi devidamente verificada;
Explorar problemas de teste de SOC tais como escalonamento de núcleos, consumo de potência durante teste, escolha da largura de barramento de teste, entre outros.
Este documento está organizado como segue:
Hardware baseadas em software.
É necessário o uso de um ambiente de co-simulação para validação conjunta do software de teste e dos núcleos.
Desta forma, o Capítulo 5 visa apresentar a primeira contribuição deste trabalho que é o desenvolvimento e a avaliação de um ambiente de co-simulação funcional e geograficamente distribuído.
O código gerado por o ambiente de teste é avaliado em termos de tempo de teste, requisitos de memória e cobertura de falhas.
A preocupação com teste, tipicamente até meados da década de 80, surgia somente na etapa final de concepção do Ci.
Entretanto, com o aumento da complexidade e o advento dos SOCs tornou- se obrigatório a presença de teste desde o início da fase de concepção.
Este capítulo tem como objetivo apresentar uma revisão geral de técnicas de teste.
Assim, o mesmo é organizado da seguinte forma:
Em a Seção 2.1 é apresentada uma introdução e conceitos relacionados ao teste.
A Seção 2.2 apresenta uma breve revisão de projeto visando a testabilidade.
A Seção 2.3 apresenta uma revisão de conceitos relacionados a auto-teste.
Por fim, a Seção 2.4 apresenta as conclusões deste capítulo.
O requisito principal para obter um sistema confiável é determinar se um sistema eletrônico não possui erro.
A confiabilidade em sistemas era principalmente desejável em aplicações militares e aeroespaciais.
Porém, ultimamente tem crescido o interesse em confiabilidade na maioria das áreas que envolvem computação.
Para atingir o grau de confiabilidade desejado pode- se realizar teste no circuito desenvolvido procurando defeitos de fabricação.
Dependendo do tipo de teste, falhas induzidas por efeitos do ambiente e flutuação na tensão de alimentação também podem ser detectadas.
Devido a a complexidade do processo de teste, pode- se optar por um fluxo de concepção de CIs que vise facilitar a etapa de teste após a fabricação.
Esta abordagem é chamada de projeto visando o teste.
Outra abordagem para atingir o grau de confiabilidade é através de técnicas de tolerância a falhas.
O objetivo da tolerância a falhas é permitir que o sistema continue funcionando na presença de falhas.
Estas técnicas residem, principalmente, na utilização do conceito de redundância ao nível de módulos de hardware, software, informação ou tempo.
Teste de um circuito antes de sua implementação é conhecido como verificação.
Seu objetivo é verificar se o projeto está de acordo com a especificação.
Atualmente, simulação é a ferramenta de verificação mais utilizada, uma vez que as técnicas de verificação formal ainda são imaturas e complexas.
Existem dois tipos de simulação.
A simulação funcional é utilizada para verificar se o projeto executa a função desejada sem incluir modelos temporais.
Em a simulação temporal atrasos são adicionados às portas lógicas, ocorrendo uma simulação mais precisa que a simulação funcional.
Ainda é possível realizar uma simulação com elementos parasitas, obtidos da síntese física, técnica esta chamada de back annotation.
Uma vez que o projeto está implementado em silício, ele pode ser verificado.
Esta verificação, responsável por verificar erros de fabricação do Ci, é chamada teste.
Existem dois tipos principais de teste:
Teste paramétrico e teste funcional.
O primeiro está relacionado ao teste de parâmetros do circuito como tensão e corrente.
O segundo tipo, responsável por testar a funcionalidade do circuito, é o objeto de estudo deste trabalho.
Um defeito é uma imperfeição física ocorrida no circuito.
A Figura 5 (a) ilustra um defeito de fabricação.
Uma área de metal não desejada provoca um curto entre duas áreas de metal, modificando o comportamento do circuito.
Uma falha é uma abstração de um defeito.
O defeito ilustrado em Figura 5 (a) pode ser modelado como um curto em nível de transistor, Figura 5 (b), ou SA (stuck-at) 1 no nível de portas lógicas, Figura 5 (c).
Um erro é uma manifestação de um defeito, isto é, a sua propagação para as saídas primárias do sistema através da geração de uma resposta incorreta.
Por exemplo, na Figura 5 (c) existe um erro quando as portas de entrada A e B forem iguais ao nível lógico 1, pois o valor da saída é 1 quando deveria ser 0.
Por outro lado, quando A e B forem igual a 0, não há erro uma vez que o valor esperado é igual ao valor gerado.
Falhas permanentes existem indefinidamente e se referem, geralmente, a defeitos físicos no circuito.
Falhas temporárias transientes são falhas que aparecem e desaparecem num curto espaço de tempo, geralmente causadas por alguma perturbação externa como flutuações de energia, radiação do tipo Seu (single-event upset) e interferências eletromagnéticas.
Falhas temporárias intermitentes existem somente em alguns intervalos.
Este tipo de falha ocorre devido a a degradação do componente ou também por efeito externo como temperatura.
Por outro lado, de acordo com o modo, as falhas podem ser introduzidas:
Em a etapa de especificação, no nível mais alto de abstração, através de algoritmos e arquiteturas errôneas;
Em a etapa de implementação através de uma interpretação errônea da especificação;
Em a fabricação do circuito;
Por perturbações externas como radiação, interferências eletromagnéticas, umidade, erro de operação e ambientes extremos.
Tendo em vista estas definições, este trabalho se concentra na identificação de componentes com falhas permanentes induzidas por defeitos introduzidos na etapa de fabricação de ASICs e falhas temporárias que afetam a propagação de FPGAs.
Como apresentado anteriormente, modelos de falhas são utilizados para representar defeitos físicos num nível de abstração maior, geralmente no nível de porta lógica.
A principal função desta abstração é reduzir a complexidade da análise do circuito.
Os modelos de falhas tradicionais correspondem a falhas que afetam as conexões no nível de portas lógicas.
As falhas típicas são curtos e conexões abertas.
Quando o curto ocorre entre uma linha l e a alimentação positiva, o modelo de falhas chama- se stuck-at 1.
Este modelo costuma ser representado por l/ 1.
Se a linha l está conectada ao terra, o modelo chama- se stuck-at 0.
O modelo de falhas stuck-at é o modelo de falhas mais básico e mais utilizado tanto na academia quanto na indústria de semicondutores.
No caso de conexão aberta, a tensão na conexão permanece constante e, desta forma, também pode ser modelada como SA0 ou SA1.
Quando um curto ocorre entre duas linhas, l1 e l2, ambas as linhas possuem a mesma tensão.
Este tipo de modelo de falhas é chamado bridging.
Modelos de falhas variam de acordo com a estrutura a ser modelada.
Por exemplo, memórias RAM possuem modelos de falhas específicos para esta estrutura de circuito.
Estes modelos são brevemente apresentados na Seção 2.3.3.1.
As falhas stuck-at na entrada A são indicadas por A/ 0 para stuck-at em nível 0 e A/ 1 para stuck-at 1.
Uma notação similar é utilizada para B e Z. em A/ 0 a saída com falhas difere da saída correta somente quando a entrada AB $= 11 é aplicada na porta lógica.
Esta combinação de valores de entrada é chamada de padrão de teste, pois detecta a falha A/ 0.
Este mesmo padrão de teste também detecta B/ 0 e Z/ 0.
Também há casos onde uma falha pode ser detectada por mais de um padrão, como é o caso de Z/ 1.
Quaisquer dos padrões 10, 01 e 00 podem detectar esta falha.
Por fim, para detectar todas as falhas possíveis numa porta lógica e, utilizando o modelo stuck-at, três padrões de teste são necessários.
Três das principais métricas qualitativas de teste são:
Cobertura de falhas, observabilidade e controlabilidade.
A cobertura de falhas (em inglês, fault coverage) é definida como a porcentagem de falhas detectáveis por os padrões de teste por o total de falhas do modelo.
Uma de suas vantagens é que pode ser obtida por simulação, o que se torna economicamente mais atraente que a métrica de rendimento, apresentada na Seção 2.1.8, que é obtida por amostragem.
É importante salientar que uma cobertura de falhas de 100% não garante que o circuito não possui falhas.
O processo de teste verifica somente as falhas representadas por o modelo de falhas utilizado, como o modelo stuck-at (SA).
A observabilidade é representada por o esforço necessário para observar o valor lógico de um nó interno através de um pino de saída.
A controlabilidade é representada por o esforço necessário para atribuir um determinado valor lógico a um nodo.
O aumento da integração dos CIs, ou seja, o aumento do número de nodos internos por pino, afeta direta e negativamente estas duas últimas métricas de teste.
Esta seção classifica técnicas de teste de acordo com três parâmetros:
Quanto a o método de geração dos padrões de teste, quanto a o momento em que o teste é realizado e quanto a a origem dos padrões de teste.
De acordo com o método de geração dos padrões de teste, existem quatro classificações possíveis para o teste:
Exaustivo, pseudo-exaustivo, pseudo-aleatório e determinístico.
O teste exaustivo utiliza todas as combinações de entradas como padrão de teste.
Tem a vantagem de ser possível atingir 100% de cobertura de falhas, em geral sem grande dificuldade, para circuitos combinacionais.
Para um circuito combinacional com 8 entradas, seriam necessários 256 padrões de teste para testar o circuito.
Porém, para circuitos seqüenciais complexos não é viável a aplicação deste tipo de teste dado o grande número de estados do circuito.
O teste pseudo-exaustivo divide um circuito em sub-circuitos testado- os exaustivamente sem testar exaustivamente o circuito completo.
A Figura 8 ilustra este conceito.
O circuito em questão possui 8 entradas foi dividido em três sub-circuitos:
Alfa, beta e gama.
Estas partições possuem, respectivamente, 2, 3 e 3 entradas, totalizando 4, 8 e 8 padrões de teste para cada sub-circuito.
Desta forma são necessários 20 padrões de teste ao invés de 256 do teste exaustivo.
O teste determinístico ou orientado a falhas gera padrões específicos a uma falha.
Geralmente estes padrões são gerados por ferramentas de geração automática de padrões de teste (Automatic Test Pattern Generation -- ATPG).
Uma vez que o processo de geração de padrões é Np-completo, este consome um longo tempo.
Além de isto, é necessário uma grande quantidade de memória para armazenar os padrões, visto que estes padrões costumam ser aplicados por um Ate.
O tempo para gerar os padrões de teste mais a quantidade de memória necessária ao Ate são fatores que fazem aumentar o custo de teste de circuitos.
Existem duas classificações possíveis para o teste de acordo com o momento em que o teste é realizado:
Off-line e on-line.
O teste off-line é executado quando o circuito não está em uso.
É a principal aplicação do BIST.
Por outro lado, o teste on-line pode executar testes durante o funcionamento normal do circuito.
Este tipo de teste geralmente faz uso de alguma técnica de codificação ou duplicação de dados (e.
g paridade simples, código de Hamming, votador, entre outros).
A maior vantagem do teste on-line é a capacidade de detectar falhas transientes ou intermitentes.
Dois principais fatos fazem com que o tempo de teste aumente:
O primeiro refere- se ao aumento da densidade dos CIs ter tornado o volume dos padrões de teste extremamente grande.
Outro fato é a disparidade da velocidade de operação interna e externa devido a a limitação na quantidade de pinos e entrada e saída.
Estes fatos fazem com que a largura de banda externa seja muito menor que a largura de banda interna.
Portanto, para minimizar a interação com o mundo externo tem- se adicionado módulos de teste embutidos, procurando um compromisso entre tempo de teste e acréscimo de área.
Assim, de acordo com o origem dos padrões de teste, existem duas classificações possíveis para teste:
Embutido (em inglês, on- chip) e externo (em inglês, off-chip).
O teste embutido faz uso de módulos de hardware para gerar padrões de teste, compactar respostas ou até avaliar- las.
A sua grande vantagem é a velocidade de operação, pois realiza o teste na freqüência de operação nominal (em inglês, at- speed).
A desvantagem é que pode induzir acréscimo de área de silício e reduzir a freqüência de operação devido a a adição destes novos módulos de teste.
Porém, existem diversos trabalhos que visam reduzir ou até eliminar este acréscimo de área.
Outra vantagem atribuída a este tipo de teste é a redução dos requisitos dos Ates uma vez que o tráfego de dados entre o Ate e o Ci sendo testado é menor.
Isto reduz a quantidade de memória necessária para armazenar os padrões de teste e as repostas, pois o mesmo pode ser feito por o módulo de hardware embutido.
Também reduz a necessidade de uma alta freqüência de operação do Ate, uma vez que o processo de teste é menos dependente deste equipamento e o tráfego de dados é menor.
O teste externo é altamente dependente do Ate.
Sabe- se que estes equipamentos tendem a aumentar de preço com o aumento de números de pinos e freqüência de operação dos CIs a serem testados e quantidade de memória necessária.
Como dito anteriormente, a SIA prevê que o custo de Ates será de aproximadamente 20 milhões de dólares em 2014.
Este custo, mais o tempo adicional que esta técnica de teste demora em relação a o teste embutido, faz com que o custo do projeto cresça, além de aumentar o time- to-- market.
O rendimento (em inglês, yield) de produção de CIs é definido por a fração de dispositivos que não acusam defeitos de fabricação em relação a o número total de dispositivos produzidos.
É determinado por Y $= G/ (G+ B), onde G e B representam respectivamente os número de circuitos bons e com defeitos.
O rendimento pode ser interpretado como a probabilidade de se produzir um circuito sem defeito.
Embora outros fatores também influenciem o rendimento como área do wafer, maturidade do processo e número de etapas do processo, rendimento também é uma estimativa de métrica de qualidade de dispositivos, expressa em percentual.
Nível de defeito (em inglês, defect level) é a fração de CIs defeituosos que passam nos testes.
Geralmente medido em defeitos por milhão (em inglês, defects per million -- DPM).
Pode ser interpretado como a probabilidade de vender um circuito com algum defeito.
É uma das métricas de avaliação de qualidade de um processo de fabricação.
Y -- rendimento(%) nível de defeito (DPM) cobertura de falhas(%) A área destacada é representada por a Figura 10.
Maxwell Realizaram um estudo para avaliar a qualidade de teste baseado na cobertura de falhas do modelo stuck-at.
Para isto três tipos de teste foram realizados:
Funcional, scan (visto na seção 2.2.2) e misto funcional+ scan.
Conforme a Figura 10, o resultado é que o teste funcional+ scan precisou de uma menor cobertura de falhas que o teste baseado em scan para obter o mesmo nível de defeito.
O motivo disto é que teste funcional detecta falhas de temporização, o que o teste baseado em scan não consegue.
A conclusão é que não basta ter como objetivo alcançar uma dada cobertura de falhas, mas também se deve levar em conta como a cobertura de falhas vai ser alcançada.
Williams scan funcional+ scan cobertura de falhas(%) O teste funcional+ scan com 83% de cobertura possui o mesmo nível de defeito que o teste baseado em scan com 93% de cobertura.
A curva denominada Williams representa a fórmula Dl $= 1-Y.
Dados os problemas relacionados ao teste citados anteriormente, é importante manter os custos relacionados ao teste em valores aceitáveis.
Para que isso seja possível, uma maior quantidade de área de silício e degradação de desempenho pode ser tolerada, desde que sejam mantidos os compromissos em relação a o custo, área e desempenho.
Esta abordagem é conhecida como projeto visando testabilidade (DFT).
O objetivo de DFT é aumentar a controlabilidade e observabilidade, reduzindo os custos relacionados ao teste e aumentando a cobertura de falhas.
Embora vários problemas de testabilidade sejam amenizados com o uso de DFT, utilizar somente esta técnica já não é mais viável para circuitos grandes.
Técnicas tradicionais de DFT estão ligadas ao uso de aparelhos testadores externos que aplicam estímulos e comparam respostas.
Devido a este fato, o tempo para aplicar os vetores de teste se torna inaceitável, sem citar o volume de vetores de teste que testadores externos têm que armazenar.
Outra desvantagem desta abordagem é o custo destes testadores e o fato de não se poder testar o dispositivo já inserido no sistema e executar teste em tempo de execução.
Por causa de estas desvantagens faremos apenas um pequeno apanhado de tais técnicas.
Técnicas ad hoc ou caso a caso são medidas tomadas para melhorar a testabilidade durante a fase de projeto de forma não estruturada e não sistematizada.
Como exemplo podemos citar a inicialização de circuitos seqüenciais, introdução de sinais de reset, particionamento de circuitos e a introdução de pontos de observação.
Algumas técnicas são utilizadas largamente em placas de circuito impresso.
Porém, no universo de circuitos integrados as técnicas estruturadas, apresentadas a seguir, são mais aconselhadas.
As técnicas estruturais de DFT representam um enfoque sistemático para a questão de testabilidade, superando as limitações técnicas da abordagem ad hoc e viabilizando a automatização saídas primárias entradas primárias desta fase através de ferramentas de CAD.
Uma técnica de DFT largamente difundida é o boundary scan.
A técnica mais tradicional é o scan path.
Em esta técnica, os registradores do sistema são modificados de forma a terem dois modos de operação:
Modo normal e de teste.
Em o modo normal os registradores funcionam executando sua função de armazenar dados.
Já no modo de teste, estes se tornam registradores de deslocamento, aumentando o número de nós internos que são controláveis e observáveis.
A Figura 11 ilustra esta estrutura.
O funcionamento desta técnica está ilustrado na Figura 12.
É apresentada uma série de ciclos de relógio necessários para a carga dos FFs e a leitura dos seus conteúdos.
Primeiramente, o sinal T é ativado configurando os FFs em modo de teste (registrador de deslocamento) e o vetor é inserido serialmente nas entradas dos FFs.
A o final de n períodos de relógio, onde n é igual ao tamanho do Scan Path, o vetor está carregado nos FF e as suas saídas estão acionando as entradas do circuito combinacional em teste.
Por um período de relógio, o sinal de controle T sinaliza o modo normal de operação e na próxima borda de relógio as saídas do circuito combinacional, que foram produzidas a partir de as entradas definidas por o vetor de teste carregado previamente nos FFs, são carregados nos FFs.
Estes valores são deslocados para a saída para serem comparados com o valor esperado.
Desta forma, se consegue controlabilidade de todas as entradas dos circuitos combinacionais e observabilidade de todas suas saídas.
Uma alternativa às técnicas de DFT, onde vetores de teste são gerados por um testador externo, é o auto-teste (em inglês, Built-in Self-Test -- BIST).
Em BIST são incorporados módulos de hardware responsáveis por a geração de vetores de teste, comparação da resposta (assinatura) e controle do fluxo de teste.
BIST reduz o custo e o tempo da etapa de teste, diminuindo o ciclo de concepção do dispositivo, minimizando o volume de dados com algoritmos de compactação embutidos no Ci e diminui a necessidade e requisitos de testadores externos.
BIST também tem a vantagem de possibilitar teste em diferentes níveis da hierarquia do sistema, simplificando o projeto de teste do sistema completo.
Além de isto, permite o teste em tempo de execução (em inglês, at- speed), podendo cobrir falhas de características temporais.
Porém, o uso de BIST está associado a certos custos adicionais.
É necessária mais área de silício para os módulos de geração, avaliação e controle de teste, e acarreta uma degradação de desempenho com a presença de multiplexadores adicionais.
O último custo citado, principalmente, pode inviabilizar o uso de BIST em arquiteturas que precisem de alto desempenho.
A Figura 13 apresenta uma arquitetura de BIST genérica.
Em as Seções seguintes apresentaremos exemplos de módulos de geração de vetores de teste e de avaliação e compactação de respostas.
A técnica mais utilizada para a geração de estímulos (vetores de teste) para o auto-teste é a geração de estímulos pseudo-aleatórios.
A estrutura utilizada chama- se Linear Feedback Shift Register (LFSR), cujo formato genérico é apresentado na Figura 14 (b).
Um LFSR está associado a uma função matemática, apresentada na Figura 14 (a).
Outras técnicas utilizadas para geração de padrões são cellular automata, weighted pattern generator e contadores.
Os coeficientes hi, apresentados na Figura 14 (a), são iguais a 1 quando existe ligação de realimentação e iguais a 0 quando não existe.
Este polinômio associado ao LFSR é chamado de função geratriz e quando primitivo faz com que todas as combinações de valores sejam geradas por o LFSR de forma aleatória.
Em, existe uma tabela de polinômios primitivos com o menor número de termos de até o grau 300.
A Figura 15 apresenta um LFSR baseado num polinômio primitivo (f (x) $= x4+ x3+ 1) que produz a seqüência de valores mostrada na Tabela 1.
Observa- se que os valores são gerados de forma pseudo-aleatória e que o LFSR deve ser inicializado com um valor diferente de zero.
Pode- se usar como estímulos a saída do FF 0 do LFSR (modo serial) ou todas as saídas dos FFs (modo paralelo).
A Figura 16 apresenta uma simulação utilizando o mesmo LFSR da figura anterior.
A porta lfsr_ in representa uma entrada onde o usuário atribuiu um valor inicial, semente, para o gerador de números pseudo-aleatórios.
A partir deste ponto o LFSR segue gerando na porta lfsr_ out a mesma seqüência de dados apresentada na tabela anterior.
Por exemplo, na Tabela 1, após o dado A, em representação hexadecimal, é gerado o valor D e assim sucessivamente.
A simulação acaba quando o LFSR gerou todos os valores possíveis exceto zero.
Em as técnicas de teste tradicionais, os valores de teste são comparados com um valor esperado pré-computado.
Em BIST isto é inviável, devido a a grande quantidade de memória necessária para armazenar este volume de dados.
Esta Seção tem como objetivo apresentar alguns métodos de compactação de respostas apresentados na literatura.
O objetivo de compactação de respostas de teste é reduzir este volume de dados a um tamanho gerenciável (geração de assinatura) que ainda assim seja capaz de detectar falhas ao custo da adição de um pequeno circuito.
Um fenômeno chamado de mascaramento de erro (em inglês, aliasing) pode ocorrer durante a compactação das respostas, fazendo com que circuitos com falhas gerem a mesma assinatura que um circuito sem falha.
A probabilidade de ocorrência de mascaramento de erro é definida como a probabilidade média de nenhuma falha ser detectada devido a a compactação depois de uma execução de teste suficientemente longa.
Entretanto, determinar a probabilidade de mascaramento de erro não é uma tarefa simples.
Uma primeira solução seria fazer simulações do algoritmo de compactação injetando falhas e verificar o número de falhas não detectadas, porém, esta abordagem pode ser computacionalmente intensiva para seqüências de testes longas.
Outras abordagens são apresentadas em.
Existem diferentes formas de compactação de respostas de teste.
Algumas são simplistas, como contadores de 1s e contadores de transições.
Porém, as mais utilizadas são baseadas em LFSR) e MISR (&quot;Multiple Input Signature «Register «-- Figura 17 (a)).
O LFSR da Figura 17 (b) possui uma modificação em relação a a implementação original por receber uma entrada externa, agindo como um divisor de polinômios.
Esta modificação é necessária porque a versão de LFSR mostrada anteriormente não garante o valor correto do quociente da divisão.
À medida que a divisão prossegue, o quociente aparece na saída do LFSR e o resto permanece no LFSR.
A o fim da etapa de teste, o conteúdo do LFSR é a assinatura do teste.
Apresentamos um exemplo de compactação com LFSR.
Considere um LFSR de 5 bits com polinômio gerador h $= x5+ x3+ x+ 1, representado na Figura 18.
A resposta sem falhas gerada por o circuito sendo testado é &quot;10001010 «e sua assinatura é &quot;10110».
A seqüência de passos para gerar a assinatura é apresentada na Tabela 2.
Supondo a resposta &quot;11001010 «com falhas, sua assinatura seria &quot;11011».
Uma comparação entre a assinatura gerada e a assinatura correta gerada anteriormente acusaria a falha no teste.
Figura 19 ­ Simulação do LFSR compactador de polinômio x+ x+ 1.
Por outro lado, MISR é uma variação do LFSR que aceita múltiplas entradas simultâneas.
Nota- se por a Figura 17 (a) que, se todas as entradas do MISR menos a entrada y1 estiverem em zero, o MISR se transforma num LFSR.
MISR é recomendado para circuitos com múltiplas saídas, podendo executar teste de forma paralela.
Assim como o LFSR, ao fim da etapa de teste a assinatura estará armazenada nos FFs do MISR.
Idealmente, uma dada técnica de BIST deveria poder ser aplicada a qualquer tipo de circuito.
Porém, devido a a diversidade de arquiteturas e requisitos de projeto, existem variações de BIST que são mais bem aplicadas a uma ou a outra situação.
Dentro deste universo de variações nos limitaremos a apresentar as técnicas de BIST off-line aplicadas a memórias, a processadores e teste baseado em software (Capítulo 4).
Outras classes como teste de corrente, teste térmico e síntese de teste (entre outras) não são exploradas neste texto.
Módulos de memória são as estruturas de hardware onde mais se utilizam técnicas de teste.
O principal motivo está ligado à estrutura regular que facilita o teste.
Porém, um dos principais desafios no teste de memórias é o tempo de teste.
Este tempo pode se tornar excessivo para memórias grandes sendo testadas por um testador externo.
Uma solução atrativa para este problema é a aplicação de BIST.
Modelo de falhas de memória têm por objetivo representar falhas que podem acontecer numa memória.
Abaixo segue uma lista dos principais modelos de falhas de memória:
Stuck-at--Fault Refere- se a uma célula com valor fixo em nível lógico` 0 'ou` 1';
Falhas de Acoplamento ­ Uma dada posição de memória é afetada por outra posição de memória.
Por exemplo, uma escrita no bit 5 da palavra 3 muda o bit 6 da palavra 2;
Falhas de Endereçamento ­ São falhas localizadas nos decodificadores de endereço;
Retenção de Dados ­ São falhas onde uma célula de memória não pode guardar dados por um longo período de tempo;
Falhas de Transição ­ Este tipo de falha ocorre quando células não podem mudar de` 0 'para` 1' ou de` 1 'para` 0'.
A Tabela 3 apresenta uma comparação entre a complexidade de diversos algoritmos de teste de memória.
Esta tabela também informa os modelos de falhas que cada algoritmo suporta.
Algoritmos com complexidade maior tendem a se tornar impraticáveis com o aumento da capacidade de armazenamento das memórias.
Em as leituras realizadas, observou- se a maior utilização dos algoritmos march, em especial march C e C+, por terem uma complexidade linear e possuírem boa cobertura de falhas.
Maiores informações sobre os algoritmos de memória podem ser encontradas em.
Usando as técnicas de teste apresentadas anteriormente, sempre que a memória for testada, o seu conteúdo deve ser armazenado numa memória auxiliar durante o teste para não perder o conteúdo.
Isto é claramente uma grande desvantagem quando se precisa executar teste de memória periodicamente.
Para contornar este problema foi criada uma nova abordagem chamada de BIST transparente.
BIST transparente pode transformar qualquer algoritmo de teste de memória de forma a não destruir o conteúdo da memória.
Os padrões de teste utilizados são o próprio conteúdo da memória que, durante o teste, é complementado um número par de vezes.
Antes de se iniciar o teste é feita uma varredura de toda a memória, de forma a criar uma assinatura de teste para o conteúdo da memória.
Depois de gerada a assinatura, é executado o procedimento de teste complementando- se um número par de vezes cada posição de memória.
Em, o autor afirma que a versão modificada do algoritmo march C com suporte a BIST transparente possui o acréscimo de área de 2% em relação a a implementação do algoritmo march C original.
O teste de processadores modernos tem se tornado um grande desafio devido a as seguintes razões:
Por possuir uma arquitetura diversificada e complexa (e.
g MIPS e Pentium IV); (
ii) por possuir memória embarcada (e.
g cache de vários níveis, banco de registradores, entre outros);
Por utilizar tecnologia submicrônica;
Por possuir alta velocidade de operação, que aumenta as restrições de tempo, impossibilitando adição de circuitos de BIST;
Por possuir partes com difícil observabilidade e controlabilidade como circuitos de exceção, alguns registradores de controle, entre outros.
Existem diversas abordagens para o problema de teste de processadores.
Duas de elas são apresentadas.
A primeira, apresentada a seguir, baseia- se na adição de módulos de hardware para geração de estímulos e compactação de respostas e no uso de Ate para inserir as instruções, ou seja, no uso combinado de BIST e Ate.
A segunda abordagem, apresentada posteriormente na Seção 4.2.1, baseia- se na execução de um código cuidadosamente desenvolvido para exercitar a maior parte das funcionalidades do processador, ou seja, um teste funcional.
A estrutura geral de uma implementação de BIST para processadores é apresentada na Figura (Bi) obtém o opcode da próxima instrução, baseado no endereço do contador de programa (PC).
Este opcode é decodificado por o bloco de controle, que gera sinais para o bloco de dados.
Baseado nestes sinais de controle, os operandos obtidos da memória do sistema são computados na unidade de execução.
A estrutura da Figura 21 (a) é então modificada para suportar auto-teste.
São adicionados três registradores conforme Figura 21 (b).
TCR (Test Control Register) que provê o opcode de uma instrução do processador.
O LFSR para gerar os operandos das instruções providas por TCR.
O MISR que compacta os resultados obtidos.
A seqüência de passos para executar o teste é:
Ativar o modo de teste;
Inicializar TCR, LFSR e MISR via sinais de controle de teste (omitidos da figura por questão de clareza);
Carregar o TCR serialmente por a entrada scan-in (por um testador externo) com opcode de uma instrução;
Executar um número fixo de ciclos de relógio, executando a instrução indicada por TCR várias vezes com diferentes operandos gerados pseudo-aleatoriamente a cada ciclo por o Serialmente, enviar por o pino scan-out o conteúdo do MISR para executar a comparação;
Comparar o conteúdo do MISR com um valor pré-computado numa simulação para verificar se a dada instrução foi executada com sucesso.
Um equipamento externo pode realizar esta comparação;
Repetir os passos 2 a 6 com diferentes instruções até que todas as instruções tenham sido testadas.
Não há um maior aprofundamento em teste de processadores por não estar diretamente ligado ao escopo deste trabalho.
As técnicas de teste apresentadas neste capítulo são utilizadas na indústria com sucesso há cerca de 20 anos.
Porém, como já salientado anteriormente, o crescente grau de integração e o conseqüente aumento de desempenho dos SOCs têm inviabilizado o uso de tais técnicas.
A criação de novos métodos de teste e modelos de falhas têm se tornado uma necessidade.
Os dois próximos capítulos apresentam propostas de soluções para o teste de SOCs.
Os recentes avanços na tecnologia de CIs permitem o desenvolvimento de um sistema completo num único componente, chamado de SOC, fornecendo como vantagem maior desempenho, menor consumo de potência, menor volume e peso comparado com o projeto baseado em múltiplos CIs.
O projeto de SOCs pode empregar módulos de hardware complexos reutilizáveis, chamados de núcleos de hardware.
O principal objetivo do uso de núcleos é a redução do time-- tomarket, uma vez que estes módulos são pré-desenvolvidos e pré-verificados.
O uso de núcleos envolve dois grupos, os provedores e os usuários de núcleos.
Em o projeto baseado em múltiplos componentes numa placa de circuito impresso, CIs já projetados, fabricados e testados são enviados para o usuário por o provedor.
O usuário precisa, neste caso, projetar, fabricar e testar a placa, utilizando os componentes como módulos livres de falhas.
O teste se resume a teste de interconexão dos CIs.
Em projeto de SOCs, os componentes são núcleos que não estão testados pois não estão fabricados.
O teste e a fabricação de todo o sistema é responsabilidade do usuário, incluindo o teste dos próprios núcleos.
O teste de sistemas baseados em núcleos apresenta vários desafios adicionais em relação a o teste de sistemas em placa de circuito impresso, entre eles:
Teste Interno de Núcleos:
Com a redução do tamanho do transistor e o aumento da freqüência de operação, o desenvolvimento de teste de alta qualidade, porém, de baixo custo, está se tornando um desafio maior.
Os modelos de falhas são inadequados, uma vez que defeitos de atraso e crosstalk induzidos por o roteamento estão se tornado mais freqüentes.
Transferência de Conhecimento de Teste do Núcleo: Uma vez que o teste envolve dois atores, o provedor e o usuário do núcleo, há uma necessidade de padronização na transferência de informação a respeito de teste.
Por exemplo, cobertura de falhas, padrões de teste, protocolos de teste, arquitetura de DFT, modos de teste, entre outros.
Acesso de Teste aos Núcleos Embutidos:
Núcleos são módulos internos aos SOCs, podendo conter diversos níveis de hierarquia.
Assim, é necessário definir um método para levar os padrões de teste às entradas e capturar as suas respostas.
A origem dos padrões e o destino das respostas podem ser um Ate ou módulos de BIST embutidos.
Integração e Otimização do Teste do Sistema:
SOCs são formados por núcleos, lógica definida por o usuário (LDU) e lógica de interconexão.
Desta forma, o integrador do sistema deve escolher um compromisso entre diversos parâmetros como tempo de teste, acréscimo de área de silício, degradação de desempenho, qualidade de teste, consumo de potência e custo total de teste.
Padronização da interface dos núcleos, geração de estímulos e captura de respostas são somente alguns dos desafios que envolvem o teste de SOCs.
Existe a necessidade de um conjunto de algoritmos que contemple escalonamento de teste, criação de mecanismo de acesso de teste (em inglês, Test Access Mechanism ­ Tam), seleção de conjunto de teste, permitir teste paralelo e encontrar o caminho de teste.
Estes algoritmos devem utilizar como restrições o tempo de teste, consumo de potência, área de hardware, número de pinos e recursos de teste.
Porém, este trabalho não contempla esta parte de teste de SOC.
O objetivo e organização deste capítulo são:
Apresentar uma revisão do estado da arte em teste de SOC na Seção 3.1.
A Seção 3.2 é voltada ao padrão P1500, que visa padronizar a interface entre distribuidor e usuário de núcleos.
Finalmente, a Seção 3.3 conclui este capítulo.
A arquitetura conceitual de teste de SOCs consiste em três elementos, conforme a Figura 22: Fonte e receptor de dados de teste, mecanismo de acesso de teste (Tam) e lógica envoltória.
A fonte gera estímulos para o núcleo.
O receptor compara as respostas geradas com as respostas esperadas.
Tanto a fonte quanto o receptor podem ser implementados fora de o circuito, através de um Ate, ou dentro de o circuito, através de BIST.
A escolha do tipo de fonte e receptor é determinado por:
Tipo de circuito do núcleo;
Tipo de teste desenvolvido no núcleo;
Considerações sobre custo e qualidade de teste.
Zorian classifica os tipos de circuitos em três categorias principais:
Lógica, memória e analógico.
Estes três tipos de circuito possuem defeitos diferenciados, portanto são testados diferentemente, requerendo fontes e receptores distintos tanto para Ate quanto para BIST.
As vantagens e desvantagens de uso de BIST em relação a o uso de Ate foram apresentadas anteriormente.
O mecanismo de acesso de teste (Tam) se encarrega de transportar as informações de teste tanto da fonte até o núcleo quanto do núcleo até o receptor.
O projeto de um Tam envolve um compromisso entre largura de banda e custo de teste.
Uma maior largura de banda implica em maior custo de área de silício e menor tempo de teste.
O tempo de teste também é resultante do volume de dados de teste.
Existem as seguintes opções no desenvolvimento de Tam:
Tam pode reutilizar caminhos já existentes como barramentos ou pode ser formado por um caminho exclusivo para teste;
Pode passar através dos núcleos ou passar em torno de os núcleos através da lógica envoltória;
Pode existir um Tam independente para cada núcleo ou o mesmo pode ser compartilhado;
Tam pode simplesmente transportar dados de teste ou também ter sinais de controle.
A lógica envoltória é a interface entre o núcleo e o restante do SOC e o Tam.
Ela permite o chaveamento entre o núcleo e os vários mecanismos de acesso.
A lógica envoltória obrigatoriamente deve possuir os seguintes modos de operação:
Modo Normal, onde a lógica envoltória é transparente para o restante do circuito;
Modo de Teste Interno do Núcleo, onde o Tam é conectado ao núcleo para transporte dos dados de teste para uma cadeia de varredura interna ao núcleo, por exemplo.
Em este caso o Tam pode ser serial ou paralelo, porém, devido a a pequena largura de banda do Tam serial, costuma- se utilizar Tam paralelo;
Modo de Teste Externo, onde o Tam é conectado à interconexão que provê os dados de teste para os núcleos, sem enviar padrões internamente ao núcleo.
Realiza teste das interconexões externas ao núcleo;
Modo de Passagem (bypass), permite que vários núcleos sejam conectados a um mesmo Uma vez que a largura do Tam é determinada por a largura de banda da fonte e do receptor, e o número de bits de entrada e saída do núcleo é determinado por a sua funcionalidade, uma função normalmente aplicada à lógica envoltória é o uso de conversores serial-paralelo e paralelo-serial para a adaptação das diferentes larguras de interface.
A Seção seguinte apresenta algumas definições de uma proposta de padrão para lógica envoltória chamada IEEE P1500.
Uma vez que o projeto baseado em núcleos envolve dois atores, o provedor e o usuário de núcleos, existe a necessidade de padronizar a interação entre eles.
Isto facilita a integração de núcleos de origem de diferentes provedores, aumentando a eficiência do usuário de núcleos.
Sendo assim, o padrão P15001 está focado na padronização da interface entre usuário e provedores de núcleos.
Esta padronização atende:
Uma linguagem padrão capaz de expressar todas as informações relacionadas a teste que devem ser transferidas para o usuário do núcleo;
Uma lógica envoltória de teste padrão configurável, que facilite a integração de núcleos no sistema.
P1500 não cobre:
Método de teste interno dos núcleos;
Integração do teste do sistema;
Otimizações do teste do sistema;
Mecanismos de acesso ao teste;
Fonte e receptor de teste.
Marinissen alega que estes pontos devem estar completamente nas mãos dos provedores e do usuário de núcleos, e sua padronização não é viável devido a as diferentes restrições dos núcleos e sistemas.
Os dois principais elementos do padrão P1500 são a linguagem de teste de núcleos (em inglês, Core Test Language -- CTL) e a arquitetura escalonável para teste de núcleos.
O objetivo da linguagem de teste de núcleos é expressar todas informações relacionadas a teste de forma explícita e concisa e transferir- las do provedor para o usuário de núcleos.
O escopo desta linguagem compreende:
Métodos de teste;
Modos de teste e seus correspondentes protocolos;
Padrões de teste, e.
g lista de vetores de teste, algoritmos de teste para memórias, polinômios primitivos para BIST;
Modelos e cobertura de falhas;
Informações da estrutura de DFT interna dos núcleos, e.
g tamanho da cadeia scan, BIST;
Informações de diagnóstico, e.
g localização física de pontos de teste.
Uma vez que as informações contidas nesta linguagem são mais importantes que a sintaxe da mesma, esforços estão sendo concentrados na expansão de uma linguagem já existente, o padrão P1450 (Standard Test Interface Language -- STIL).
Atualmente esta linguagem esta focada na descrição de padrões de teste no formato de forma de ondas, e necessita de várias extensões para expressar as informações de teste de núcleos.
Este padrão ainda está em fase de desenvolvimento Seu objetivo é definir uma interface de hardware uniforme mas flexível, capaz de transportar padrões de teste para o núcleo.
A padronização é necessária para garantir a fácil integração dos núcleos e, provavelmente, reduzir o time to market.
Por outro lado, deve ser flexível para permitir que o provedor e o usuário do núcleo possam explorar compromissos entre qualidade de teste, tempo de teste, área de silício e impacto no desempenho.
O trabalho de padronização está focado no desenvolvimento de uma lógica envoltória com as seguintes características:
Possuir múltiplos modos de operação, e.
g normal, teste de interconexão, teste interno e passagem;
Conectar qualquer número de portas do núcleo com qualquer tamanho de Tam, exigindo assim mecanismos de adaptação de largura;
Prevenir o escorregamento de relógio;
Caminho de controle de teste serial.
A Figura 23 apresenta um esquema da estrutura desta lógica envoltória.
Os terminais da lógica envoltória são:
As entradas e saídas funcionais correspondem às entradas e saídas do núcleo.
Sua largura é definida por o provedor;
Wc é uma porta de 6 bits que determina o modo de operação da lógica envoltória;
As interfaces de teste serial, si e so, são utilizadas para carregar o WIR e o Tam serial;
Zero ou mais TAMs paralelos, chamados de pi e po, que possuem largura parametrizável, podendo variar para cada lógica envoltória dentro de um sistema.
Além de isto pi e po não necessariamente precisam ter a mesma largura;
Controle de teste dinâmico, e.
g sinal de habilitação de varredura;
Funções especiais correspondem a sinais que não possuem células associadas como, por exemplo, sinais de relógio ou analógicos.
Os componentes mínimos de uma lógica envoltória são:
Registrador de instruções da lógica envoltória (WIR), que controla suas operações;
Múltiplas células de entrada e saída, que fornecem controlabilidade e observabilidade aos terminais do núcleo.
A cada terminal do núcleo é atribuída uma célula, sendo que alguns terminais especiais, e.
g relógio, não utilizam tais células;
Um registrador de um bit, denominado de passagem, que serve para transportar os padrões de teste por a Tam serial;
Fios de conexão e multiplexadores para selecionar os modos de operação.
A Figura 24 apresenta as conexões dos núcleos no nível do sistema.
Os núcleos, que utilizam uma lógica envoltória para teste, estão conectados por uma Tam serial, obrigatória, e uma Tam paralela, opcional, definida por o usuário.
O bloco de controle de teste também deve ser definido por o usuário.
Esta seção apresenta um exemplo de uso da lógica envoltória padrão ilustrada na Figura 25.
A lógica envoltória é adicionada a um núcleo simples.
Este núcleo, chamado de núcleo A, possui duas cadeias de varredura interna, duas portas de dois bits para acesso de entrada e saída destas cadeias, uma porta de entrada de cinco bits, uma porta de saída de três bits, uma porta para habilitar a varredura interna e a porta de relógio.
Em este caso específico o núcleo em questão possui um conjunto de padrões de teste pré-definidos, também chamados de padrões determinísticos.
Porém, qualquer outro método de geração de padrões, inclusive pseudo-aleatório, poderia ser utilizado.
Para tornar o núcleo A compatível com P1500, uma lógica envoltória e um programa CTL devem ser desenvolvidos.
O programa CTL foi omitido pois o objetivo desta linguagem é transportar conhecimento de teste do provedor ao usuário do núcleo.
Como neste caso o núcleo está sendo desenvolvido por quem irá utilizar- lo, não há necessidade deste programa.
A Figura 25 (a) apresenta a lógica envoltória conectada ao núcleo.
É importante destacar que esta descrição de lógica envoltória possui alguns detalhes de implementação.
O padrão P1500 define somente o comportamento da lógica envoltória, deixando a implementação a cargo de o projetista.
Em este exemplo em particular, a lógica envoltória possui um Tam serial, si e so, e um Tam paralelo de três bits, pi e po.
A implementação das células de entrada e saída são apresentadas, respectivamente, na Figura 25 (b) e Figura 25 (c).
Em esta implementação da lógica envoltória foram desenvolvidos seis modos de operação que compreendem o modo normal, de teste de conexão dos TAMs serial e paralelo e o modo de passagem serial.
Os caminhos ativados em cada um dos modos de operação são apresentados na Figura 26.
A Tabela 4 apresenta o estado dos multiplexadores em cada um dos modos de operação.
Os conceitos apresentados nesta seção visam facilitar o teste de SOC, criando caminhos de teste para os estímulos e respostas.
Porém, métodos de geração de estímulos e captura de respostas, que podem tanto ser origem externa quanto interna ao Ci, devem ser definidos por o usuário.
Ao longo de o próximo capítulo, propomos o uso de um processador como elemento de controle de teste de SOC, geração e captura dos estímulos gerados a partir de um software de teste.
A adição de módulos de teste, como BIST, pode vir a impedir que os sistemas computacionais operem na freqüência determinada por o projetista, dado o crescente aumento da freqüência de operação dos SOCs.
Assim, são necessárias novas metodologias de teste menos intrusivas, porém, que permitam teste em tempo de execução, pois a redução do tempo de teste também é importante.
Em uma revisão bibliográfica, apresentada na Seção 4.2, identificamos que teste de hardware baseado em software ameniza algumas desvantagens das técnicas de BIST.
Por este motivo, utilizamos esta técnica neste trabalho.
Durante o desenvolvimento deste trabalho não foi encontrada referência bibliográfica contemplando o teste de um SOC genérico por completo utilizando software para teste de hardware.
Rajski Utilizaram ABIST (arithmetic BIST) para testar a parte de dados de um processador.
Chen Exploraram a implementação de LFSR e MISR em software como método de geração de estímulos e compactação de respostas para teste de processadores, porém, assumem que a memória foi previamente testada.
Rajsuman explorou a programabilidade de processadores embutidos num SOC para testar memória e um DAC, porém, o processador era testado com técnicas estruturais de BIST lógico.
Hellebrand Exploraram a flexibilidade de software em relação a a hardware para implementar LFSRs com múltiplos polinômios e sementes, que foi demonstrado que pode aumentar a cobertura de falhas.
Papachristou Utilizaram processadores embutidos num SOC para testar núcleos, porém, o teste do processador e da memória não foram contemplados.
Este capítulo é organizado da seguinte forma:
A Seção 4.1 apresenta as vantagens e desvantagens do uso de software para teste de hardware.
A Seção 4.2 apresenta a arquitetura básica necessária para desenvolver teste de um SOC utilizando algoritmos implementados em software.
Ao longo de esta seção também é apresentada uma revisão bibliográfica do uso de software para testar os diferentes módulos de um SOC como processador, memória e núcleos não programáveis.
Finalmente, a Seção 4.3 conclui este capítulo.
Além de as vantagens já conhecidas de BIST tais como redução do volume de padrões de teste aplicado por o Ate, redução dos requisitos de freqüência e memória do Ate, realização de teste em tempo de execução, redução do tempo de teste, entre outras vantagens, o teste embutido baseado em software possui as seguintes vantagens adicionais:
Redução ou exclusão total de área de hardware adicional;
Redução ou exclusão total de perda de desempenho devido a a lógica de isolamento de teste (i.
e multiplexador);
Redução ou exclusão total da lógica de controle de teste.
O próprio processador se encarrega do fluxo de teste;
Fácil reutilização ou modificação das técnicas de teste;
Maior flexibilidade e programabilidade para realização do teste.
Basta modificar um código em software para modificar a rotina de teste.
Por exemplo, para diminuir a probabilidade de mascaramento das rotinas de compressão pode- se optar por executar o teste 2 vezes com polinômios diferentes;
Menor dissipação de potência durante o teste pois o teste ocorre em modo normal de operação.
Esta característica pode viabilizar o aumento do paralelismo, ou seja, múltiplos processadores testando o SOC, reduzindo o tempo de teste.
Porém, algumas das desvantagens inerentes ao uso de software para teste de hardware são:
SOC deve possuir um componente programável como processador, microcontrolador ou DSP.
Em este trabalho designamos de processador qualquer um destes três tipos de componente programável.
Para resolver esta desvantagem pode- se optar por o uso de um processador específico para teste.
Estes têm a vantagem de serem dedicados para teste, sendo assim menos custosos em área e degradação de desempenho em relação a processadores genéricos;
Requisitos de memória para armazenar programa de teste e padrões de teste.
A compactação dos vetores de teste determinísticos pode ser utilizada para reduzir esta desvantagem;
Tempo de teste.
Certos trabalhos demonstram que com a compactação de padrões de teste e a sua descompactação realizada por o processador pode diminuir o tempo de teste, uma vez que diminui a quantidade de tráfego de dados entre o Ate e o Ci.
O teste dos principais componentes de um SOC, utilizando técnicas de software é aplicado para:
Teste do processador;
Teste de memória;
Teste dos núcleos não programáveis.
Não faz parte do objetivo deste trabalho testar processador com teste baseado em software.
Porém, apresentamos esta revisão no intuito de mostrar as dificuldades de realizar tal teste e também como forma de guiar trabalhos futuros.
Teste do processador é uma tarefa difícil devido a os seguintes fatores:
Número de dispositivos lógicos existentes, e.
g ULA, controle, registradores, entre outros;
Número limitado de pinos para acessar o processador;
Existência de dispositivos que não podem ser escritos ou lidos diretamente;
Grande variação de implementação dos processadores.
Além de estas características inerentes da arquitetura de processadores, outros obstáculos dificultam a realização do teste, entre eles:
Uma vez que processadores utilizam o estado da arte da tecnologia para a sua concepção, a adição de cadeias de varredura compromete as restrições de área e desempenho do processador;
Métodos de detecção de falhas baseada em stuck-at são inviáveis devido a a grande área de hardware necessária para desenvolver um processador e ao fato de nem sempre ter disponível a descrição deste.
Dados os problemas relacionados, Thatte e Abraham propuseram um modelo de falhas funcional para processadores.
Uma vez que a metodologia tinha que ser geral, a arquitetura do processador foi abstraída.
Desta forma, este modelo pode ser aplicado a um grande número de processadores.
Verhallen e Van de Goor propuseram uma atualização ao modelo de falhas desenvolvido por Thatte, para suportar teste de cache de dados e instruções embutidos no processador.
Estes dois trabalhos, principalmente o escrito por Thatte, incentivaram a pesquisa de técnicas de teste funcional para validar processadores.
Esta técnica baseiase na escolha de um conjunto de instruções aleatórias ou de acordo com algum critério que tem como objetivo avaliar a funcionalidade do processador.
O método de escolha de instruções aleatórias mostrou que possui uma baixa cobertura de falhas, tornando- se inadequado.
Isto é devido a a complexidade da parte de controle dos processadores, que é altamente resistente a padrões pseudo-aleatórios.
Além de isto, o tempo de teste tende a ficar grande.
A alternativa defendida por diversos autores como Chen, Shen, Corno e Tehranipour é na escolha detalhada das instruções necessárias para testar o processador.
Lai e Cheng propõem a adição de instruções específicas para realizar teste.
Paschalis Defendem o uso de padrões determinísticos cuidadosamente escolhidos para testar o bloco de dados de processadores.
O critério utilizado varia de abordagem para abordagem, mas em geral, é proposto um método para alcançar um conjunto mínimo de instruções com uma maior cobertura de falhas.
Chen Desenvolveram um método efetivo de teste para processador através de uma análise de restrições espaciais e temporais do conjunto de instruções.
Porém, este cobre somente o bloco de dados do processador, não contemplando a parte de controle.
Shen Apresentaram uma ferramenta de geração automática de teste funcional que provê um ambiente único tanto para teste de fabricação quanto para validação do projeto.
Este ambiente suporta geração de código para validação, para teste com Ate e para auto-teste.
A diferença entre teste com Ate e auto-teste é que no modo com Ate os dados gerados são enviados para os pinos de entrada e, posteriormente, as respostas são lidas por os pinos de saída.
Já no autoteste os dados gerados são comprimidos por uma rotina similar a um MISR.
Utilizando tal ferramenta para teste do processador 8051, foi obtido 90.2% de cobertura de falhas, sendo que o programa de teste foi gerado em somente 3.1 minutos e o tempo de simulação foi 4.1 horas.
Corno Utilizaram o conceito de macro.
Uma macro é gerada para cada instrução que é testada.
Uma macro é um conjunto de instruções agrupadas de forma a carregar os operandos para a instrução alvo, executar a instrução alvo e propagar os resultados desta instrução para a memória.
Desta forma, existe um trabalho manual de criação das macros.
Corno alega que, para o processador 8051, leva- se 2 dias de trabalho de um programador assembly experiente para construir a biblioteca com 213 macros.
Por fim, a versão final do programa de teste é formada por uma seqüência de macros cujos operandos são selecionados por um algoritmo genético.
Esta seleção de operandos levou 24 horas numa estação de trabalho de 400 MHz com 2 Gbytes de RAM.
A metodologia proposta obteve uma cobertura de falhas de 85,19% contra 80,19% da metodologia baseada na escolha aleatória de instruções.
Tehranipour Atribuíram uma rotina de teste específica para cada bloco do processador, por exemplo ULA.
Os operandos destas rotinas são formados por padrões determinísticos e/ ou pseudo-aleatórios.
A seleção destes operandos é feita por uma investigação prévia não detalhada no artigo.
Também é realizado o teste de memória por software.
Porém, não foi detalhado o método de teste da parte de controle que é uma das partes mais difíceis.
Cheng Lai Alegam que a baixa cobertura de falhas atingida no teste de processadores é devida a circuitos de baixa controlabilidade e observabilidade como circuitos de controle de exceção, de interrupção e alguns registradores.
Desta forma eles propõem a adição de um pequeno número de instruções no processador para aumentar a controlabilidade e a observabilidade destes circuitos.
Algumas instruções também foram adicionadas para diminuir o tempo de teste e o tamanho do programa de teste.
Para isto, seqüências de instruções freqüentes foram condensadas numa única instrução.
Foram utilizados dois processadores para avaliar a técnica.
Em ambos os casos, 100% de cobertura foram alcançados com uma redução média de 20% no tempo de teste e tamanho do programa com o acréscimo de 1.6% de área.
Porém, esta metodologia não pode ser aplicada para processadores firm ou hard cores, pois implica na modificação do conjunto de instruções do processador.
Paschalis Demonstraram vantagens de aplicação de teste determinístico ao invés de teste pseudo-aleatório.
Teste pseudo-aleatório depende da arquitetura e da largura de barramento do processador.
Por exemplo, uma ULA de 16 e uma de 32 bits utilizam polinômios diferentes para implementar um LFSR.
Com isto, é necessário reescrever código e refazer a simulação.
Além de isto a execução de LFSR em software requer vários ciclos de relógio para a execução das rotinas.
Paschalis alega que com teste determinístico é possível escrever rotinas de teste para MAC, ULA e registradores de deslocamento independente da arquitetura interna e da largura do barramento e com número reduzido de padrões de teste, reduzindo tempo de teste.
Nedwal apresenta uma aplicação industrial de teste baseado em software.
Nedwal desenvolveu manualmente um programa de teste para o microcontrolador CR16B.
O tamanho deste código de teste é de cerca de 1 Kbyte.
Este reduzido tamanho foi alcançado pois não foram considerados todas as instruções e seus diferentes modos de operação.
Somente as instruções mais utilizadas (e.
g de carga, escrita, soma, comparação, salto, operações e ou lógico) foram testadas.
Esta lista de instruções foi selecionada varrendo diversos códigos em assembly gerados a partir de um compilador C. O tempo necessário para execução deste programa de teste não foi apresentado.
O teste da memória é o mais simples de realizar num SOC por um processador.
O motivo é que geralmente o teste de memória RAM utiliza padrões determinísticos, e.
g 55h e AAh.
Desta forma, somente um trecho de código é necessário.
Porém, o tempo de teste varia de acordo com os tipos de falhas que o algoritmo utilizado se propõem a encontrar.
A Tabela 3 na página 19 apresentou uma lista de algoritmos e suas complexidades.
Devido a a simplicidade de implementar teste de memória por processador, existem diversas referências que o implementam.
Rajsuman utiliza um código assembly do algoritmo March, apresentado na Figura 27, para testar uma memória de 2 Mbits.
Rajsuman argumenta que com esta técnica diminuíram- se significantemente os requisitos de um Ate, provendo teste em tempo de execução, sem degradação de desempenho, sem área de silício adicional e com a facilidade de poder trocar o algoritmo de teste sem alterar o hardware, provendo maior portabilidade.
O algoritmo apresentado na Figura 27 é simples e está bem comentado.
São executadas operações de leitura e escrita em ordem crescente e decrescente em toda a memória.
Assim que o valor lido divergir do valor escrito, é acusada uma falha e o procedimento é finalizado.
De acordo com, o algoritmo march é um algoritmo que, apesar de a simplicidade, apresenta as melhores coberturas de falha.
Zorian Apresentam um esquema para teste de ROM.
O autor considera uma memória ROM como um circuito combinacional, onde o endereço é a entrada e o conteúdo é a saída.
Desta forma é utilizado teste exaustivo, leitura de toda ROM, compactando a saída.
É utilizado um método de compactação baseado em MISR bidirecional e contador de 1s para diminuir a probabilidade de mascaramento.
Esta abordagem pode ser tanto utilizada em teste de fabricação quanto em teste de campo.
Para teste de campo, um algoritmo adequado para teste de memória RAM é o BIST transparente.
Este algoritmo de teste utiliza o próprio conteúdo da memória e seu complemento como padrão de teste.
Além de isto, este algoritmo pode ser facilmente implementado em software.
No caso de o teste de fabricação, deve haver um Ate simples que apenas carrega o programa de teste no momento em que o mesmo seja necessário.
Já em teste de campo, o programa de teste deve estar armazenado numa ROM, bastando, por exemplo, aplicar teste de ROM apresentado anteriormente para testar- la.
Para o teste dos núcleos não programáveis, o processador deve executar um programa que é responsável por acessar os núcleos, gerar os estímulos e capturar ou compactar as respostas dos núcleos testados.
Devido a a programabilidade dos processadores, é possível utilizar diferentes programas para geração de estímulos e compactação de respostas.
Esta flexibilidade permite utilizar um algoritmo diferenciado de geração e de compactação para cada núcleo a ser testado, de acordo com cada caso.
O acesso ao núcleo depende de alguma estratégia para levar os estímulos e capturar as respostas do núcleo.
A acessabiblidade dos núcleos por o processador depende da arquitetura do SOC.
Por exemplo, numa arquitetura de barramento provavelmente o acesso do processador é feito diretamente por o barramento.
Em arquiteturas onde o processador não tem acesso direto ao núcleo, estratégias de construção de TAMs são necessárias.
Alguns exemplos das técnicas de acesso, geração e compactação de dados são apresentados a seguir.
Gupta Propuseram um método de geração de padrões pseudo-exaustivo de teste baseado em somadores.
Uma vez que o somador existe na maioria das arquiteturas, não há degradação de desempenho e adição de hardware.
Os padrões são gerados acumulando continuamente um valor constante escolhido de acordo com um critério.
Estes padrões são gerados por a tripla onde a representa a constante a ser acumulada, X0 representa o valor inicial do acumulador e n a largura do acumulador.
Hellebrand Destacam o ganho de flexibilidade com uso de teste baseado em software.
Com esta flexibilidade adicional, é possível explorar configurações diferenciadas de LFSRs de forma a alcançar a maior cobertura de falhas possível, pois se sabe que existem circuitos resistentes a padrões pseudo-aleatórios.
Hellebrand alega que com esta flexibilidade é possível explorar a melhor configuração de gerador de padrões para cada módulo.
De entre as configurações estudadas estão o LFSR simples, LFSR com múltiplos polinômios e múltiplas sementes.
Foi demonstrado, utilizando benchmarks, que cada circuito responde melhor a uma dada configuração de LFSR.
Esta flexibilidade adicional e a escolha adequada de gerador de padrões para cada módulo acarretam uma maior cobertura de falhas.
Esta maior cobertura de falhas pode tornar viável armazenar em memória padrões determinísticos de forma a alcançar 100% de cobertura de falhas.
Por exemplo, o circuito c7552, pertencente ao ISCAS85, é conhecido por ser resistente a padrões pseudo-aleatórios.
Com um gerador de padrões baseado num LFSR simples, de um único polinômio, se atinge uma cobertura de falhas de 95.79%, restando 4.21% para padrões determinísticos.
Por outro lado, com o uso de um LFSR com múltiplos polinômios e múltiplas sementes é possível alcançar uma cobertura de falhas de cobertura de falhas acarreta uma economia de memória para armazenar padrões determinísticos.
Em conjunto com a abordagem de Hellebrand, pode ser aplicada a abordagem de Jas que utiliza um algoritmo embarcado para descomprimir as informações de teste que estão na memória.
Esta abordagem diminui ainda mais a necessidade de memória e a restrição de largura de banda de comunicação entre o Ate e o circuito.
Radecka Demonstraram, utilizando benchmarks, que um bloco de ULA ou multiply and accumulate (MAC) pode gerar estímulos com qualidade comparável a um LFSR.
Em certos casos, MAC precisa de menos vetores de teste para alcançar a mesma cobertura de falhas que um Stroele demonstra que somadores, subtratores e multiplicadores podem ser utilizados para a geração de padrões pseudo-aleatórios com uma cobertura de falhas e comprimento de teste similar a um LFSR.
Isto traz como vantagem o tempo de teste, pois a emulação de LFSR necessita de vários ciclos de relógio para a sua execução.
Para esta demonstração foram utilizados benchmarks, sendo medido o número de padrões necessários para alcançar 100% de cobertura.
Dorsch Aplicam a técnica baseada no uso de múltiplas sementes a somadores.
Esta técnica, normalmente atribuída a LFSR, é utilizada para codificar uma série de padrões determinísticos.
A vantagem é que basta armazenar somente um dado, que é a semente, para poder gerar pseudo-aleatoriamente uma série de padrões determinísticos.
Assim, algumas sementes são necessárias para codificar um conjunto inteiro de padrões determinísticos.
Os resultados obtidos são semelhantes aos obtidos com uso de LFSR, porém, sem o impacto em área de hardware e desempenho.
O problema deste esquema de múltiplas sementes é que o cálculo do número mínimo de sementes necessárias para representar um conjunto de padrões determinísticos é muito complexo, aplicado somente a conjuntos de padrões pequenos.
Rajski Avaliam, em termos de probabilidade de mascaramento, uma técnica de compactação de respostas baseada no uso de acumuladores.
Os autores provam que acumuladores com sinal de carry rotativo1 apresentam uma probabilidade de mascaramento semelhante a um MISR.
A vantagem desta abordagem em relação a o MISR é que não há necessidade de hardware adicional para a compactação de respostas, portanto não há degradação de desempenho.
Outra vantagem é o menor tempo de execução da compactação.
Stroele critica o método de avaliação do trabalho de Rajski E propõe outra avaliação.
Porém, o resultado final desta nova avaliação é compatível com os resultados de Rajski, ou seja, acumuladores com sinal de carry rotativo são semelhantes a MISRs em termos de probabilidade de mascaramento.
Papachristou Apresentam uma abordagem efetiva de teste de núcleos não programáveis utilizando um processador, porém, não cobrem teste do processador e de memória.
Em a arquitetura proposta existem três módulos principais:
O processador, a memória e um testador externo de baixo custo.
O processo de teste foi dividido em duas etapas:
De carga de memória e de teste.
A carga da memória é realizada por o testador através de acesso direto à memória (DMA).
O teste é realizado por o processador através da execução do programa de teste.
Desta forma, foi feita uma sobreposição no tempo destas duas etapas, carga e teste, diminuindo o tempo total de teste.
Antes da etapa de carga deve ocorrer uma análise do tempo estimado para carga e teste para cada núcleo do SOC, pois cada núcleo possui um tempo de carga e de teste diferentes e a ordem em que os dados são enviados para a memória também influência no tempo total de teste.
Outra proposta importante é um método baseado em grafos para escalonamento de teste.
As informações obtidas através da análise deste grafo são utilizadas para que o processador monte adequadamente o caminho que os estímulos e respostas devem traçar do processador ao núcleo e do núcleo ao processador, garantindo sempre o caminho com menor custo de tempo.
Uma das conclusões mais interessantes, obtidas através do exemplo demonstrado, é que o tempo de teste do sistema fica praticamente independente do testador externo.
Foi utilizado um testador simples de 50 MHz e constatou- se que o tempo de teste depende somente da freqüência de operação do processador.
Quando a freqüência do processador é duplicada, o tempo de teste praticamente é reduzido por a metade.
Teste baseado em software minimiza os dois principais problemas relacionados ao uso de Somadores onde o sinal de carry-out é rotacionado para o bit menos significativo.
BIST: Perda de desempenho devido a a inserção de circuitos no caminho crítico e o conseqüente aumento da área de hardware.
A minimização destes problemas possibilita o auto-teste de circuitos de alto desempenho e de difíceis restrições de concepção, tipicamente restrições de freqüência de operação e área.
Porém, novos desafios surgem como o aumento do tempo de teste e o aumento da área de memória para teste.
A redução do tempo de teste pode ser alcançada acrescentando instruções no processador específicas para teste e projetando um misto entre teste baseado em software e em hardware.
A área de memória para teste pode ser reduzida utilizando múltiplos polinômios e múltiplas sementes, e algoritmos de compactação.
Em a revisão bibliográfica apresentada, foram encontradas algumas aplicações acadêmicas e industriais de teste baseado em software.
Porém, se identificou que a automatização e avaliação deste processo ainda são incompletas.
No que diz respeito à automatização de técnicas de teste baseadas em software, nota- se que há várias abordagens de geração automática de código de teste para processadores.
Também se nota que existem várias aplicações de software para testar memórias.
Porém, não foram encontrados trabalhos que automatizassem o processo de geração de código de teste para núcleos não programáveis.
Em o que concerne à avaliação de técnicas de teste baseadas em software, não existem trabalhos que demonstrem de forma quantitativa suas vantagens e, principalmente, suas desvantagens.
Dadas estas lacunas, apresentamos no Capítulo 7 um ambiente de desenvolvimento que automatiza o processo de geração de código de teste de núcleos não programáveis.
Em seguida, no teste desenvolvido, apresentamos o ambiente de co-simulação implementado para validação das técnicas de teste baseadas em software (Capítulo 5) e o ambiente de prototipação (Capítulo 6) utilizado para validar as descrições de hardware geradas.
A atual complexidade dos sistemas computacionais é tal que o projeto não pode mais ser realizado com uma única linguagem nem com um único nível de abstração.
Desta forma, diferentes linguagens e modelos de computação são utilizados para cada um destes domínios.
Em a concepção de sistemas heterogêneos complexos, seus módulos são desenvolvidos e validados em separado.
Porém, em certas etapas do projeto é necessária uma validação total do sistema.
A validação deste tipo de sistema é uma tarefa complexa devido a sua heterogeneidade.
Entretanto, atualmente a técnica de validação mais utilizada é a simulação, uma vez que as técnicas de verificação formal ainda são imaturas.
O princípio da co-simulação heterogênea é a execução paralela dos simuladores necessários para a validação de um sistema.
Cada simulador é responsável por executar um módulo do sistema numa linguagem, modelo de computação e nível de abstração específico ao domínio tratado.
Em este capítulo apresentamos nas Seções 5.1 e 5.2 uma breve revisão de modelagem de sistemas computacionais e conceitos básicos de co-simulação.
A Seção 5.3 apresenta a implementação de um ambiente de co-simulação, sendo esta a primeira contribuição deste trabalho.
O desenvolvimento de um ambiente de co-simulação é necessário para validar a interação dos algoritmos de teste implementados em software com os núcleos implementados em VHDL.
A Seção 5.4 apresenta estudos de caso de aplicações e avaliações de desempenho da ferramenta desenvolvida.
Por fim, a Seção 5.5 conclui este capítulo e apresenta direções para trabalhos futuros.
A grande variedade de modelos de computação empregados é devido a as diferentes funcionalidades envolvidas num sistema computacional.
Por exemplo, para descrever um sistema composto de hardware e software poderíamos escolher as linguagens VHDL e C, respectivamente, para sua especificação.
Porém, se a decisão de projeto for adotar uma única linguagem (modelagem homogênea) de especificação, teríamos C descrevendo hardware e, naturalmente, software ou VHDL descrevendo software e hardware.
Esta abordagem teria a grande vantagem de necessitar de um único conjunto de ferramentas para validar todo o sistema.
É evidente que a escolha de uma única linguagem apresenta limitações devido a o fato do modelo de computação empregado não ser aplicado aos dois domínios (hardware e software).
Apesar de isto, algumas abordagens tentam diminuir estas limitações adicionando bibliotecas que incluem funcionalidades que implementam diferentes modelos de computação (e.
g SystemC).
Outras abordagens tentam definir uma única linguagem que possua todos os conceitos necessários para a especificação de um sistema, porém, é difícil definir tal linguagem (e.
g SpecC).
A alternativa defendida neste trabalho é manter as diferenças conceituais dos diferentes domínios.
Isto implica em modelar cada módulo do sistema numa linguagem específica e apropriada (modelo heterogêneo multi-linguagem).
Porém, o problema principal no uso desta abordagem é definir a semântica de interação de modelos computacionais diferentes e uma técnica de sincronização entre os processos (simuladores).
Desta forma linguagens como SDL e C+ podem ser utilizadas em descrições mais abstratas, matlab pode ser usado para modelar partes mecânicas e processos físicos, VHDL-AMS pode ser utilizado para descrição de hardware analógico, C e assembly para descrever software e VHDL para descrever hardware digital.
Motor de Simulação:
A Co-simulação mono-motor consiste em transformar os módulos descritos em diversas linguagens numa mesma representação que descreve todo o sistema.
A simulação ocorre nesta representação única do sistema, chamada de formato intermediário.
A vantagem desta abordagem é a necessidade de uma única ferramenta de simulação para validar todo o sistema.
Porém, tem a desvantagem de ser difícil converter qualquer linguagem com qualquer modelo de computação num formato intermediário, pois seria difícil desenvolver um formato intermediário que suporte tais variações.
Em a co-simulação multi-motor cada módulo é simulado por um simulador apropriado para cada linguagem.
A co-simulação se resume a uma troca de mensagens entre os simuladores.
A vantagem deste método é que permite a utilização das ferramentas existentes para realizar a simulação.
Porém, é necessário que os simuladores envolvidos permitam o envio de seus dados de simulação para o exterior, via uma interface de comunicação.
Além de isto surge a necessidade da utilização de um método de sincronismo para coordenar a troca de dados entre esses simuladores que estão executando paralelamente ou concorrentemente.
A Figura 28 ilustra estes conceitos.
Mecanismos de Comunicação:
Se for adotado o conceito de múltiplos motores de simulação, tem- se que escolher algum mecanismo de comunicação entre os diversos simuladores.
A escolha do mecanismo de comunicação é feita através de um compromisso entre desempenho, flexibilidade e suporte à cosimulação distribuída.
Mecanismos de comunicação entre processos como pipes, por exemplo, não suportam comunicação distribuída.
Por outro lado sockets além de suportar comunicação distribuída, é suportada por diversos sistemas operacionais e linguagens de programação.
Já mecanismos como corba e RMI conferem ao sistema uma maior flexibilidade e suporte à simulação distribuída.
Modelo Temporal: A validação funcional consiste em validar a funcionalidade do sistema sem sincronização temporal dos simuladores durante a execução da co-simulação.
A troca de dados entre simuladores é controlada por eventos.
A validação funcional é utilizada em níveis de abstração elevados onde a noção de tempo não é necessária à simulação.
A validação temporal permite realizar uma validação mais realista do sistema, considerando o tempo.
O envio e a recepção de dados acontece em instantes precisos de tempo, chamados de janelas temporais.
Assim, os diferentes simuladores são sincronizados e todos possuem o mesmo relógio global de simulação.
Quando um dado não é consumido num intervalo específico de tempo, este dado pode ser perdido.
Este tipo de validação é muito utilizado para a validação de um sistema especificado num baixo nível de abstração, onde se torna necessário realizar uma simulação precisa.
Modelo de Sincronização: A o se optar por o desenvolvimento de um ambiente que suporte co-simulação temporal devese definir qual é o modelo de sincronismo utilizado, já que os diversos simuladores que compõem o sistema podem ter velocidades de execução e modelos temporais diferentes.
A função básica do modelo de sincronização é coordenar o envio e recepção de dados através do ambiente de cosimulação.
A Figura 29 ilustra a diferença destas duas opções de sincronização.
Existem diversas abordagens para sincronização.
A mais simples é o modelo mestre/ escravo que consiste em designar um simulador como mestre da simulação, o qual fica responsável por invocar os outros simuladores escravos.
A maior vantagem deste modelo é a simplicidade.
Porém, este modelo restringe a arquitetura do sistema aos sistemas mestres/ escravos, formados por um processador mestre ao qual são associados coprocessadores escravos.
Este modelo de sincronização elimina toda forma de paralelismo entre os módulos.
O modelo distribuído permite a execução concorrente dos simuladores.
Cada simulador pode enviar e receber dados sem restrições.
Este modelo apresenta a vantagem de não restringir a arquitetura do sistema e assim permitir a sua utilização num grande número de aplicações.
No entanto, a coerência entre os dados compartilhados por os simuladores torna- se mais difícil de assegurar.
A principal metodologia empregada para este modelo consiste em conectar cada simulador num barramento de co-simulação, o qual é encarregado do envio e recepção de dados, e da sincronização entre os simuladores.
Este barramento age como um servidor de comunicação e sua implementação pode ser baseada em mecanismos de comunicação entre processos, como por exemplo ipc ou sockets no caso de o sistema UNIX.
Alguns dos principais desafios encontrados no desenvolvimento de um ambiente de cosimulação são sincronização, conversão de dados, desempenho de comunicação (tempo de simulação), facilidade de integração de novos simuladores e compromisso entre desempenho e precisão.
Algumas soluções da literatura para estas decisões de projeto do ambiente de cosimulação são apresentadas:
Sincronização: Quando se opta por desenvolver um ambiente de co-simulação distribuído com co-simulação temporal deve- se escolher o método de sincronização entre os simuladores.
Diversos algoritmos são encontrados na literatura como, por exemplo, o algoritmo &quot;synchronized handshake».
Conversão de dados:
Uma vez que o sistema é composto por diversos modelos de computação que podem conter tipos de dados diversos, é necessário definir uma estratégia responsável por a conversão de dados.
Em a Seção 5.3.2 apresentamos a solução de conversão de dados implementada.
Desempenho de comunicação:
A co-simulação distribuída é a capacidade de simular um projeto em máquinas geograficamente distribuídas conectadas por uma LAN (local area network) ou WAN (wide area network).
Algumas das motivações para a pesquisa neste campo são:
Descentralização do projeto, facilidade de cooperação entre diferentes grupos do projeto, gerenciamento de propriedade intelectual, gerenciamento de licenças de simuladores e compartilhamento de recursos.
Porém, a desvantagem desta abordagem é que o desempenho da simulação se torna dependente do desempenho da rede.
Por o fato de permitir a execução paralela e concorrente de diversos simuladores, outro problema que surge é a necessidade de sincronização destes simuladores.
Facilidade de integração de novos simuladores:
A abordagem tradicional para integração de novos simuladores é baseada na construção caso a caso das interfaces entre o ambiente de co-simulação e o simulador.
Quando um novo simulador for inserido no sistema, uma nova interface deve ser desenvolvida com pouco ou nenhum reuso de código, uma vez que na maioria dos casos as funções do simulador que provêm comunicação com o mundo externo são proprietárias.
De este ponto de vista, a maioria das abordagens são adequadas para atender um conjunto de problemas pré-definidos.
Porém, problemas inesperados podem exigir a integração de novos simuladores.
O desenvolvimento de uma nova interface pode ser inaceitável por questão de tempo ou por impor restrições ao simulador para se adaptar ao ambiente de co-simulação.
Estas restrições poderiam comprometer, por exemplo, a precisão da simulação.
Porém, em é apresentada uma abordagem mais flexível que as abordagens atuais.
Este trabalho utiliza um padrão IEEE, chamado HLA (high level architecture), que propõe regras e mecanismos para facilitar interoperabilidade de simuladores heterogêneos distribuídos.
Desta forma, todo o simulador que reconheça este padrão pode ser adicionado ao sistema de forma transparente.
Compromisso entre desempenho e precisão:
A maioria das abordagens que trata este item permite que seja possível escolher o nível de abstração em que cada módulo é simulado.
Desta forma, os módulos que podem ser simulados com menor número de detalhes contribuem com o desempenho do sistema uma vez que diminui a quantidade de comunicação.
Porém, em é apresentada uma abordagem diferenciada, chamada técnica de prototipação gradual, que tem como objetivo diminuir o tempo total de simulação (desempenho) prototipando em FPGA componentes de hardware já validados e sintetizados.
A cada módulo de hardware prototipado, menor fica o sistema que está sendo simulado, diminuindo a complexidade e o tempo de simulação.
Este processo de validação por prototipação gradual continua até que todos módulos de hardware tenham sido prototipados.
Desta forma, ao fim da validação do sistema, os componentes de hardware já estão prototipados.
A cosimulação com FPGA obteve um ganho de desempenho de 1.7 sobre a co-simulação baseada puramente em software.
Esta seção detalha a implementação e funcionamento do ambiente de co-simulação desenvolvido.
Esta, portanto, é a primeira contribuição deste trabalho.
Um ambiente de co-simulação distribuído é apresentado, de forma geral, como a estrutura mostrada na Figura 30 (a).
Em esta figura observa- se um barramento em o qual ocorre a comunicação entre os módulos.
Para simular a existência de um barramento utilizamos primitivas de sockets para comunicação entre os módulos e um roteador de mensagens.
Este roteador recebe as mensagens enviadas por os módulos e as envia para todos os módulos que devam tomar conhecimento de tal mensagem.
Esta organização está descrita na Figura 30 (b).
Para comunicar- se com o ambiente de co-simulação, cada módulo deve utilizar uma biblioteca especificamente desenvolvida para seu simulador.
Estas bibliotecas são partes integrantes do ambiente de co-simulação.
Módulos escritos em VHDL utilizam entidades para envio e recebimento de dados.
Descrições das bibliotecas de comunicação são encontradas na Seção 5.3.2.
A linguagem VHDL não possui interface com sockets.
No entanto, alguns simuladores permitem que se defina o comportamento de um módulo VHDL em código C, utilizando uma biblioteca.
O simulador ModelSim possui uma biblioteca chamada FLI (Foreign Language Interface).
Desta forma é possível utilizar a biblioteca de sockets do sistema operacional para realizar a comunicação com o ambiente de co-simulação.
Em a primeira parte deste arquivo é apresentada a definição dos módulos que compõem a cosimulação.
Em este exemplo, a co-simulação é realizada entre dois módulos, chamados VHDL e M2.
O módulo VHDL é especificado na linguagem VHDL e simulado na ferramenta ModelSim.
Este módulo é executado na máquina especificada por a diretiva machine, e possui as portas a, b, e resultado do tipo inteiro e comeca e pronto do tipo de controle.
O módulo M2 é descrito em linguagem C, tendo as portas necessárias para a comunicação com o módulo VHDL.
Este arquivo de coordenação é utilizado no estudo de caso da Seção 5.4.
A seguir temos as definições de interconexões entre os módulos, representados por a diretiva net.
Cada interconexão tem um nome.
Entre chaves são especificadas as portas que fazem parte da interconexão, no formato porta (módulo).
A partir de o arquivo de coordenação o roteador monta suas estruturas internas de controle da co-simulação, tornando possível o roteamento de mensagens entre os diversos módulos participantes da co-simulação.
A co-simulação é interrompida no momento em que algum dos módulos se desconecta do sistema.
Entretanto, o sistema garante que todas as mensagens já enviadas serão recebidas por os módulos que ainda estiverem conectados.
O roteador foi desenvolvido em C+.
De as classes que compõem o sistema, as seguintes são as mais importantes:
CCoSim: É a interface para todo o sistema.
Suas principais funções são:
Carregar o arquivo de coordenação, montando as estruturas internas do roteador;
Receber as conexões de todos os clientes, fazendo a autenticação dos mesmos;
E rotear as mensagens que chegam para os módulos que tem portas conectadas à mesma rede da porta do módulo que originou a mensagem.
CPort: Representa uma porta de um módulo.
Tem como atributos o tipo da porta e sua direção (que pode ser de entrada, saída ou entrada e saída).
CNet: Representa uma conexão entre as portas dos módulos.
CModule: Representa um módulo, contendo informações sobre suas portas e o socket que está sendo usado para se comunicar com o simulador do módulo.
Para a execução do ambiente é necessário que cada módulo saiba em qual endereço (IP+ porta) se localiza o roteador.
Em o momento da conexão o roteador verifica se o módulo faz parte da co-simulação através das definições fornecidas por o arquivo de coordenação.
Isto torna necessário que o módulo tenha conhecimento do seu nome para conseguir se autenticar junto ao roteador.
A Figura 32 mostra o diagrama de classes do ambiente.
A classe principal do roteador é CCoSim, através de ela é que são realizadas as operações de leitura do arquivo de coordenação, geração da estrutura de conexão dos módulos e o roteamento das mensagens.
Como pode ser visto no diagrama, a classe tem conexão com módulos (CModule) e redes (CNet).
Desta forma pode- se saber quais módulos fazem parte da co-simulação, e quais são as interconexões entre estes módulos.
As redes sabem quais portas (Cport) estão ligadas a ela, podendo saber qual módulo está ligado àquela porta.
Em o momento em que o roteador recebe uma mensagem, ele verifica em qual net a porta originadora está conectada, e procede enviando mensagens para todos os módulos que têm portas conectadas a esta net, através do método SendMsg da classe CModule.
A classe CModule guarda informação sobre qual socket mantem a conexão com o simulador do módulo correspondente, podendo assim enviar a mensagem ao mesmo.
A biblioteca de comunicação tem a função de integrar um simulador ao ambiente de cosimulação.
Esta biblioteca consiste basicamente de cinco funções:
Inicialização (csiInitialize()), inteiros.
Futuramente, utilizaremos um tipo unificado que possa representar qualquer tipo de dado que seja necessário.
A Figura 33 apresenta os módulos principais que compõem o ambiente de co-simulação, incluindo o projeto do usuário.
Os módulos em cor clara representam partes do ambiente de cosimulação, os dois módulos de cor escura compõem o projeto do usuário.
Para utilizar o ambiente de co-simulação o usuário deve incluir em cada módulo a ser simulado a biblioteca de comunicação com o ambiente de co-simulação.
Em a Figura 33 estas bibliotecas são designadas ComLib.
Cada simulador deve possuir a sua biblioteca de comunicação, pois geralmente a interface do simulador com o mundo externo é proprietária.
Atualmente o sistema possui bibliotecas que suportam C e VHDL.
Bibliotecas para a linguagem Java e SDL estão sendo desenvolvidas.
Em a parte de software do projeto usuário, o usuário deve inserir manualmente o código incluindo a primitiva de inicialização e as primitivas de envio e recebimento de dados.
Já na parte de hardware, o usuário deve mudar, também manualmente, a descrição do seu projeto de forma a adicionar um componente VHDL que instancia o módulo do usuário e os componentes (ComLib VHDL) que possibilitam a troca de mensagens entre as portas do hardware e do software.
Detalhando mais o funcionamento da primitiva de inicialização, o usuário deve incluir, no início do código, a função csiInitialize ().
Esta função é responsável por a conexão do simulador no ambiente.
Os parâmetros passados na sua execução são o nome do módulo do usuário e o endereço IP do roteador de mensagens do ambiente de co-simulação.
Cada simulador deve informar o seu nome (nome do módulo do usuário) para o roteador, pois este deve relacionar o nome do módulo à conexão do socket e verificar se o nome do simulador consta no arquivo de coordenação.
Uma funcionalidade atribuída às bibliotecas de comunicação é a conversão de dados.
Quatro alternativas foram analisadas em relação a o local onde esta conversão deve ser executada:
Conversão no módulo de origem da mensagem, no roteador, nos módulos de destino e conversão tanto nos módulos de origem quanto nos de destino.
Conversão de dados na origem não é possível pois uma mesma mensagem pode ser enviada para vários módulos que podem representar diferentes simuladores e linguagens.
Isto impossibilita que a conversão de dados ocorra no envio da mensagem, uma vez que os destinatários da mesma podem ser representados por tipos de dados diferentes;
Conversão de dados no roteador também não é uma abordagem adequada pois o roteador deve conhecer todos os tipos de dados de todos os simuladores do sistema.
Assim, sempre que um novo simulador for adicionado, o roteador deve ser recompilado de forma a incluir a biblioteca de conversões de dados deste novo simulador;
Conversão de dados no destino da mensagem recai no mesmo problema do item anterior, pois se um módulo receber dados num formato não suportado, a biblioteca de comunicação tem que ser alterada e recompilada para suportar este tipo de dado;
Conversão de dados na origem e no destino implica na adoção de um tipo de dado único e compatível a todos os módulos envolvidos na co-simulação.
Para este fim o tipo inteiro foi adotado.
Isto implica que quando o módulo de origem envia um dado, o mesmo deve ser convertido para inteiro.
Já o destinatário vai converter o dado de inteiro para o tipo de dado adequado.
Esta última opção de conversão, a qual foi implementada, torna o ambiente escalável e sem necessidade de recompilação.
Esta Seção apresenta três estudos de caso.
O primeiro visa validar o ambiente de cosimulação, o segundo avaliar o seu desempenho e o terceiro demonstrar sua aplicação a este trabalho.
Utilizamos como estudo de caso uma aplicação simples de multiplicação por somas sucessivas particionada manualmente em software (C) e em hardware (VHDL).
O módulo C gera operandos e recebe os resultados das multiplicações.
O módulo hardware executa a multiplicação propriamente dita.
É importante destacar que o objetivo desta aplicação é validar a metodologia de co-simulação, e não de validar um sistema complexo.
Uma avaliação de desempenho do ambiente de co-simulação é apresentada na Seção 5.4.2 utilizando um segundo estudo de caso.
De o ponto de vista do usuário, é necessário acrescentar as bibliotecas de comunicação nos módulos que participam da co-simulação.
Em relação a o módulo de hardware, deve ser construído um testbench que instancia o módulo VHDL (UUT) e um componente de comunicação para cada um dos sinais (operado A, operando B, começa, pronto e resultado) que se comunicam com o software.
A Figura 35 ilustra esta estrutura.
Existem os componentes de comunicação de entrada (CSI_ IN) e os de saída (CSI_ OUT).
Apesar de quatro componentes de comunicação serem instanciados, o sistema está escrito de maneira que apenas um canal de comunicação (socket) é aberto para se comunicar com o ambiente de co-simulação.
As portas do componente de comunicação de VHDL são apresentadas na Figura 34.
A porta porta é uma constante do tipo string que define qual porta do VHDL (a, b, resultado, pronto ou começa) que esta instância da biblioteca monitora.
Os componentes de entrada possuem uma porta de saída chamada dout.
Já os componentes de saída possuem uma porta de entrada chamada din.
O testbench apresentado na Figura 35 e Figura 36 possui uma estrutura regular, ou seja, um testbench formado por um simples conjunto de instâncias.
Isto facilita a sua criação que ainda é feita manualmente, mas que no futuro deve ser criado automaticamente.
Basicamente, deve ser instanciada a UUT e um componente de comunicação para cada porta da UUT.
As linhas 5-9 declaram os componentes de comunicação, respectivamente o de entrada e o de saída de dados.
O componente que é co-simulado é declarado e instanciado nas linhas 11 e 14.
Em as linhas 17-26 são instanciados um componente de comunicação para cada porta do UUT.
Finalmente, a linha 28 representa a geração dos sinais clock e reset.
Em a Figura 37 apresentamos o código fonte da parte de software deste estudo de caso.
De entre as modificações necessárias estão a inclusão da biblioteca de comunicação da linguagem C, a inicialização da comunicação e as chamadas de envio e recepção de dados.
Alterado o projeto do usuário, deve ser criado o arquivo de coordenação que conecta os módulos do usuário ao ambiente.
Este arquivo pode ser encontrado na Figura 31.
Feito isto, o ambiente pode ser inicializado.
Primeiro se inicializa o roteador, que começa aguardando a conexão de todos os módulos indicados no arquivo de coordenação para iniciar a etapa de roteamento.
Após, todos os simuladores que compõem o ambiente devem ser executados.
A o se conectarem, o roteador está pronto para rotear as mensagens para começar a simulação.
A Figura 38 apresenta a captura de tela da co-simulação do multiplicador.
Existem quatro janelas nesta figura.
A janela 1 é o console do simulador VHDL.
Em a área destacada desta janela apresentamos o momento em que este simulador recebe os três dados (a, b e começa nesta ordem) e envia o resultado da multiplicação.
A janela 2 representa a forma de onda da descrição de hardware.
Em o momento destacado, o módulo de hardware sinaliza na porta pronto o momento em que a multiplicação está realizada.
O resultado da multiplicação está na porta resultado.
Outros sinais desta descrição de hardware também são apresentados.
A janela 3 representa o módulo de software.
Cada linha impressa é o resultado de uma multiplicação.
Está destacado nesta janela o momento em que ocorre a multiplicação 14 x 14.
Por fim, a janela 4 representa o roteador.
Destacamos nesta janela o momento em que ocorre a mesma multiplicação.
Note que é impresso o momento em que o roteador recebe o dado 14 da porta a do módulo M2 (módulo de software).
Também é apresentado o momento em que o módulo chamado VHDL envia ao roteador, por a porta resultado, o valor 196.
Um segundo estudo de caso foi desenvolvido para avaliar o desempenho do ambiente de cosimulação.
O estudo de caso é um algoritmo de preenchimento de polígonos.
A Figura 39 ilustra o seu funcionamento.
Dado um polígono convexo, este algoritmo encontra todas as linhas horizontais, por exemplo y4, que preenchem este polígono.
Existe uma tabela de preenchimento que armazena as coordenadas x que representam cada linha horizontal.
A principal diferença entre as partições é o número de canais.
Esta diferença representa diferentes níveis de abstração empregados para descrever o sistema.
A primeira partição, chamada &quot;C + VHDL «possui 20 canais.
Esta partição possui canais de dados e controle, representando uma descrição arquitetural do sistema.
Por outro lado, a partição chamada &quot;C + VHDL otim «possui 15 canais de dados, sem canais de controle.
Esta partição representa uma descrição funcional do sistema.
Três experimentos foram realizados para avaliar o ambiente de co-simulação.
Em todos os casos diferentes números de polígonos foram executados para averiguar o tempo de validação com o aumento da complexidade do projeto.
As seguintes estações de trabalho foram utilizadas nestes experimentos:
WS1: Sun Ultra10, 333 MHz, 256 MB RAM;
WS2: Sun Ultra1, 167 MHz, 192 MB RAM;
WS3: Sun SPARC 4, 110 MHz, 72 MB RAM;
WS4: Sun SPARC 4, 110 MHz, 72 MB RAM;
WS5: Sun SPARC 4, 110 MHz, 64 MB RAM;
WS6: Sun Ultra2, 296 MHz, 256 MB RAM.
As estações WS1 até WS5 estão numa LAN Ethernet 10 Mbps.
Já a estação WS6 está numa WAN a uma distância de 5 hops.
O primeiro experimento, Tabela 5, compara os tempos de execução de diferentes abordagens de validação.
A primeira abordagem analisada é a implementação em software do estudo de caso.
Como o esperado, seu tempo de execução é muito menor que as outras abordagens, uma vez que se trata de código compilado e sem tráfego de mensagens.
Em a segunda abordagem o estudo de caso foi implementado em VHDL e simulado.
Como é código interpretado o tempo de execução é maior que a primeira abordagem.
Porém, este tempo de execução não é excessivamente maior porque não possui troca de mensagens.
A terceira abordagem representa o tempo de validação obtido na ferramenta desenvolvida.
Nota- se que o tempo de execução é muito maior que as outras abordagens devido principalmente às milhares de trocas de mensagens e chamadas do sistema operacional.
Além de isto, esta avaliação foi feita com o ambiente utilizando socket TCP para comunicação entre processos.
Este mecanismo foi utilizado para facilitar a construção do ambiente, pois o mesmo possui controle de erros.
Porém, sabe- se que este mecanismo é custoso em termos de tempo de conexão e transmissão.
As próximas versões da ferramenta visam suportar algum outro mecanismo de comunicação mais rápido.
O segundo experimento, Tabela 6, avalia a variação de tempo de co-simulação das duas partições implementadas, &quot;C+ VHDL «e «C+VHDL (otim).
Também é apresentado o número de trocas de mensagens executadas durante a co-simulação.
Nota- se que com a remoção de 5 canais da versão otimizada obtivemos um ganho de desempenho de cerca de três vezes.
Estes dados demonstram que o tempo de co-simulação varia com a quantidade de comunicação entre os módulos.
O terceiro experimento, Tabela 7, avalia o impacto da comunicação entre redes no tempo de co-simulação.
Três abordagens são comparadas:
Em a primeira abordagem todos os módulos do estudo de caso são executados numa única estação de trabalho;
Em a segunda abordagem estes módulos estão distribuídos em estações de trabalho diferentes conectados por uma LAN;
Em a última abordagem quatro módulos estão numa LAN e um está numa WAN a 5 hops de distância.
Observa- se que há um pequeno impacto no tempo de co-simulação, pois o tempo numa única estação é sempre menor do que nas outras abordagens.
Porém, o tempo adicional imposto por a distância da WAN é percebido somente quando se aumenta a complexidade do sistema.
Estes dados demonstram que é possível ter módulos distribuídos numa rede com pequeno impacto, corroborando as vantagens da co-simulação geograficamente distribuída.
A motivação para o desenvolvimento desta ferramenta de co-simulação foi a ausência de um ambiente que suportasse validação integrada entre módulos de hardware e software.
Desta forma, o objetivo deste estudo de caso é apresentar uma aplicação desta ferramenta no contexto de teste de núcleos utilizando teste baseado em software.
Este estudo de caso visa a validação funcional de uma lógica envoltória e do software de geração de estímulos e compactação de respostas.
Em este exemplo existe um módulo de hardware, que é a lógica envoltória, e um módulo de software, que implementa o LFSR e o MISR.
O arquivo de coordenação deste projeto é apresentado na Figura 41.
A lógica envoltória, chamada tb_ wr_ c880, possui a porta q de saída de dados, d de entrada de dados, addr de endereço, as portas cs, wr e rd para controle de escrita e leitura e a porta ready para sincronização da co-simulação.
O software, chamado lfsr32, possui interface equivalente.
A Figura 42 apresenta de forma simplificada um código de teste do núcleo c880 pronto para co-simulação.
Além de as modificações básicas para co-simulação com a inserção da biblioteca e a função de inicialização, este programa possui uma constante (POLY) que define o polinômio utilizado para gerar padrões e compactar respostas (linha 4).
Logo após segue a descrição do LFSR e do MISR (linhas 5 e 6).
Este programa gera padrões através de um LFSR e os envia para o núcleo sendo testado.
Este núcleo processa o padrão e gera uma saída que é lida e compactada por o software.
A Figura 43 apresenta a estrutura do testbench criado para co-simulação.
A lógica envoltória e os componentes de comunicação são instanciados.
Cada componente de comunicação conecta uma porta da lógica envoltória ao roteador.
Estes componentes de comunicação podem ser de entrada (CSI_ IN) ou de saída (CSI_ OUT), dependendo da direção da porta em questão da lógica envoltória.
A o final do testbench são descritos os sinais de clock e reset.
A Figura 44 representa uma captura de tela da co-simulação da de o sistema de teste particionado em hardware e software.
Existem três telas nesta figura.
Uma representa o simulador de hardware, a segunda representa a parte de software destacando o fim da execução e a assinatura gerada, e a última representa uma forma de onda da lógica envoltória.
Esta forma de onda destaca as operações de escrita e leitura em a/ da lógica envoltória.
Existem duas escritas pois o núcleo em questão possui mais de 32 bits de entrada.
Pode- se perceber que no momento em que ocorre uma escrita na lógica envoltória, os sinais cs e wr estão ativados.
Em a leitura os sinais cs e rd são ativados.
Este Capítulo apresentou o desenvolvimento de um ambiente de co-simulação, tendo como objetivo a validação da metodologia de teste de hardware baseado em software.
Esta validação é funcional, não informando o tempo necessário ao teste, apenas se a metodologia desenvolvida está funcionalmente correta.
A principal contribuição deste ambiente é a integração de diversos simuladores.
As principais características deste ambiente são:
Lançamento automático dos simuladores;
Suporta cosimulação geograficamente distribuída;
Suporta VHDL e C (inclusive suas variações como C+ e SystemC);
Suporta co-simulação de modelos arquiteturais e RTL;
Flexibilidade do roteador de co-simulação.
É importante ressaltar que a ferramenta de co-simulação, o roteador, é independente dos simuladores utilizados, pois o tipo de dados que trafega por a rede é único.
Desta forma, novas linguagens e simuladores podem ser facilmente suportados.
Por outro lado, este ambiente ainda apresenta desvantagens, as quais devem ser investigadas em trabalho futuro.
De entre as desvantagens citamos:
Baixa portabilidade, devido a o fato que cada nova linguagem ou simulador adicionado ao sistema deva possuir uma biblioteca de comunicação para troca de mensagens com o roteador;
Alto tempo de co-simulação, devido a o grande número de trocas de mensagens e chamadas ao sistema operacional.
Novas bibliotecas de comunicação estão sendo incorporadas ao sistema para dar suporte a SDL (Synchronous Description Language) e Java. Como
trabalhos futuros visamos principalmente diminuir o tempo de co-simulação.
Atualmente, o meio de comunicação utilizado é socket TCP, inclusive quando a co-simulação ocorre num único computador.
Em estes casos pode- se utilizar outro meio de comunicação como pipes ou memória compartilhada.
Também é possível o desenvolvimento de um gerador automático de testbench para VHDL, dada a regularidade do mesmo, cujo objetivo é facilitar a utilização do ambiente.
Em este Capítulo apresentamos o ambiente de prototipação que foi utilizado para desenvolver e validar a parte prática deste trabalho.
O ambiente de prototipação Excalibur, comercializado por a empresa Altera, é destinado à prototipação em FPGA de sistemas digitais baseados em módulos de hardware e software.
O ambiente Excalibur possui um conjunto de ferramentas para depuração e configuração de hardware e ferramentas para compilação e depuração de software, destinadas a um processador firm core proprietário, denominado Nios.
Este ambiente foi escolhido por possuir uma série de facilidades para prototipação e validação de um sistema composto por núcleos.
De entre as características que tornam o ambiente Excalibur uma plataforma adequada à avaliação das metodologias de teste de SOC enumeramos:
Ferramenta SignalTap, a qual possibilita capturar valores de sinais com o sistema em execução no FPGA.
Quando se desejam avaliar sinais internos de um dado sistema com um analisador lógico, é necessário levar estes sinais para o nível mais alto de hierarquia, modificando- se assim o circuito em desenvolvimento.
Já com a ferramenta SignalTap basta o projetista especificar os sinais que devem ser capturados e realizar a síntese do sistema.
Os blocos responsáveis por a captura dos dados são inseridos automaticamente.
Estes valores são enviados por a porta serial para um programa que mostra a variação dos valores da mesma forma que um analisador lógico o faria.
O objetivo deste capítulo é apresentar alguns recursos do ambiente Excalibur.
Desta forma, somente os pontos relevantes a este trabalho estão sendo destacados.
Entre estes pontos citamos:
A organização dos módulos de hardware, a estrutura do barramento e interface de hardware e software do sistema.
A Figura 45 apresenta a organização dos módulos do SOC implementados em FPGA.
Normalmente são instanciados o processador Nios, o barramento Avalon e alguns módulos de hardware.
Estes módulos de hardware podem ser instanciados dentro de o &quot;Módulo de Sistema Nios «(&quot;MSN&quot;) ou externamente ao MSN.
Os módulos internos ao MSN são:
O processador, o barramento e, geralmente, os módulos de hardware (firm core) responsáveis por a interface com algum recurso físico da placa (e.
g UART, LCD, memória, entre outros).
Por outro lado, os módulos externos ao MSN são necessariamente LDU (lógica definida por o usuário).
Em a Figura 45 temos o processador, o barramento e quatro módulos de hardware:
O módulo I/ O 1 está definido dentro de o MSN.
Ele não possui interface externa ao MSN nem ao FPGA.
Um exemplo de módulo com esta característica é um temporizador;
O módulo I/ O 2 também está definido internamente ao MSN.
Ele possui interface externa ao FPGA.
Um exemplo de módulo com esta característica é uma UART, com acesso aos pinos de entrada e saída serial;
A comunicação no ambiente Excalibur entre o mestre (geralmente é o processador) e os escravos (módulos de hardware) é feita por entrada e saída mapeada em memória.
Por exemplo, define- se para o módulo 1 a faixa de endereços 4 a 8.
Assim, sempre que o processador executar uma leitura ou escrita nesta faixa de endereços, estará sendo feito um acesso de leitura ou escrita no módulo 1.
Por este motivo que os sinais de interface entre processador e hardware são sinais padrão de comunicação processador/ memória (e.
g habilitador de dispositivo, de escrita, de leitura e barramento de dados e de endereços) mais o sinal de interrupção.
Apresentamos na Figura 46 uma representação esquemática de como ocorre a leitura e escrita executada por o processador no modelo de comunicação empregado entre o processador Nios e os módulos de hardware do sistema.
Uma vez que o FPGA utilizado não suporta barramento tri-state interno, a forma encontrada para direcionar o dado lido de um hardware é por meio de multiplexador (Figura 46 (b)).
Em o processo de leitura de um módulo por o processador, a lógica de decodificação de endereço ativa o sinal cs do módulo correspondente a este endereço, fazendo com que o dado do hardware seja transferido até o processador através do multiplexador.
Já quando o processador escreve (Figura 46 (a)) num hardware, novamente existe uma lógica no barramento que verifica o endereço de escrita e aciona o sinal cs do hardware em questão.
Desta forma, todos os módulos do sistema recebem os mesmos valores para suas portas de dados, endereço e sinais de controle como read e write.
O que diferência a que módulo o dado é endereçado é o sinal cs que é único a cada módulo.
A Figura 47 apresenta um exemplo de uma operação de escrita executada por o processador Nios: (
A) inicia a operação de escrita na borda de subida do clock; (
B) sinais writedata, address e writen registrados; (
C) barramento Avalon decodifica o endereço e seta chipselect para o módulo;
A Figura 48 apresenta um exemplo de uma operação de leitura executada por o processador Nios: (
A) inicia a operação de leitura na borda de subida do clock; (
B) sinais address e readn registrados; (
C) barramento Avalon decodifica o endereço e seta chipselect para o módulo; (
D) o módulo responde com dado válido; (
E) o barramento captura o sinal readdata na borda de subida do clock e a operação de leitura termina.
Pode- se notar em ambas figuras que há um certo atraso combinacional para a geração do sinal chipselect.
Isto ocorre pois este atraso é o tempo que a lógica interna do barramento Avalon necessita para decodificar o endereço.
Novas versões do ambiente Excalibur (SOPC Builder 2.52) possuem recursos que podem ser explorados em trabalhos futuros, como:
Múltiplos mestres, suporte a DMA (Direct Memory Access) e leitura e escrita em rajadas.
Com suporte a múltiplos mestres pode- se investigar teste de SOCs com múltiplos processadores Estes poderiam executar o programa de teste em paralelo, reduzindo o tempo de teste.
Com o suporte a DMA é possível que seja definida uma memória de teste reduzida, pois o programa de teste pode ser carregado somente quando for utilizado.
Com o suporte à escrita e leitura em rajada pode- se diminuir o tempo de transferência de dados entre o processador e o módulo.
Estes recursos não foram utilizados neste projeto por delimitação de escopo do projeto.
Para integrar um módulo de hardware no MSN é necessário que este módulo respeite algumas restrições do barramento Avalon.
Devido a o fato de desejarmos que o ambiente suporte módulos de propriedade intelectual, onde não é possível alterar a descrição de hardware de forma a se adequar ao barramento Avalon, há a necessidade do desenvolvimento de uma lógica envoltória para adequar a LDU ao barramento Avalon.
Esta lógica envoltória é descrita na Seção 7.3.3.
Em a Figura 49 é apresentado um trecho de código em linguagem C com as operações necessárias para acessar o hardware.
Inicialmente o usuário deve declarar um ponteiro para a posição de memória referente a o hardware (linha 4).
É importante destacar que a constante em a_ user_ hwif, definida na biblioteca nios.
H, contém este endereço de memória definido por o usuário.
Depois, basta ler e escrever nesta posição de memória para enviar e receber dados do hardware.
A Figura 50 mostra como as descrições de núcleos geradas por o ambiente de teste apresentado na Seção 7.3.3 são inseridas no ambiente Excalibur.
Estes núcleos devem possuir uma lógica envoltória para facilitar a integração do núcleo no sistema e facilitar o processo de teste.
Um sub-conjunto dos benchmarks ISCAS85 e ISCAS89 é utilizado como núcleos neste trabalho.
Por motivo de limitação do ambiente Excalibur e de delimitação de escopo do trabalho, a forma de comunicação deve ser baseada em barramento1.
Porém a metodologia empregada está sendo desenvolvida sem se limitar a esta arquitetura.
Em este sistema o testador é um PC conectado à placa de prototipação por uma porta serial.
A memória utilizada para armazenar o programa e dados de teste pode ser a memória disponível internamente ao FPGA ou uma memória externa.
O próprio barramento é usado como mecanismo de acesso ao teste.
Todos os núcleos do sistema possuem uma lógica envoltória responsável por a interface com o barramento.
Este Capítulo apresenta o desenvolvimento de uma ferramenta de CAD que tem por objetivo inserir teste de hardware a partir de software no fluxo de concepção de um SOC.
O ambiente desenvolvido possui quatro funções principais:
Inserir cadeias de varredura com DFTAdvisor nos núcleos, quando for necessário;
Parametrizar a lógica envoltória;
Gerar a interface do SOC e integrar os núcleos;
Gerar o código de teste.
A Figura 51 e a Figura 52 apresentam o fluxo de projeto dividido em duas partes:
O fluxo de integração de núcleos e o fluxo de geração de código de teste.
A próxima etapa é a geração do SOC -- Figura 51 (b).
Em esta etapa são executadas as seguintes ações:
Seleção dos núcleos e do processador;
Seleção do meio de comunicação;
Síntese da lógica envoltória para cada núcleo de hardware;
Geração da interface do SOC com meio externo.
Uma vez que o SOC é criado, a próxima etapa é a criação do código de teste que gera os padrões de teste, compacta as respostas e avalia cada núcleo de hardware.
A abordagem baseia- se no trabalho de Hellebrand, o qual utiliza diferentes configurações de LFSR para aumentar a cobertura de falhas obtida de padrões pseudo-aleatórios.
Desta forma, menos padrões determinísticos são armazenados na memória.
A maior vantagem desta abordagem é a minimização dos requisitos do testador como largura de banda, memória e freqüência de operação.
Entretanto, o desafio é encontrar um compromisso entre tempo de teste para padrões pseudo-aleatórios e requisitos de memória para padrões determinísticos.
A primeira ação do gerador de código de teste (Figura 52 (a)) é selecionar os polinômios e sementes para o gerador de padrões, sendo que múltiplos polinômios e sementes podem ser selecionados.
O usuário também deve selecionar o polinômio e a semente do compactador.
Em este caso somente um polinômio e uma semente podem ser selecionados.
O gerador de padrões e o compactador de respostas suportados são respectivamente o LFSR e o MISR modular.
Cada configuração de LFSR é simulada de forma a criar os padrões de teste -- Figura 52 (b).
Os padrões gerados são traduzidos para o formato do simulador de falhas (Flextest/ Fastscan) para avaliar a cobertura de falhas.
Se a cobertura resultante não for suficiente, o usuário pode executar a ferramenta de ATPG para gerar o restante dos padrões de teste ou escolher uma nova configuração de LFSR.
Este processo é repetido para cada núcleo de hardware do SOC de forma a gerar os padrões de teste para todo o sistema.
Finalmente, estes padrões de teste (pseudo-aleatório e determinísticos) são transformados para o código de teste em linguagem C -- Figura 52 (c).
Depois da geração de código, o usuário pode utilizar uma ferramenta de co-simulação temporal para avaliar o tempo de teste -- Figura 52 (d).
A ferramenta de co-simulação utilizada para este fim faz parte do ambiente Excalibur, a qual modela o processador no nível lógico de abstração.
A originalidade desta ferramenta encontra- se na integração das diversas ferramentas de CAD (Leonardo, ModelSim, FlexTest, FastScan e DFTAdvisor) e na criação de diversos módulos parametrizáveis, tanto de hardware como de software.
A ferramenta foi desenvolvida utilizado QT, o qual é um ambiente de programação que possibilita desenvolver interface gráfica multiplataforma em C+.
Este capítulo está organizado como segue.
A seção 7.1 apresenta como foram implementadas e validadas as rotinas de LFSR e MISR modular descritas em linguagem C. A seção 7.2 apresenta as ferramentas de geração automática de código C e VHDL.
Detalhamento das classes utilizadas no decorrer de o desenvolvimento são apresentadas no Anexo 10.1.
A Seção 7.3 apresenta o ambiente de desenvolvimento de SOCs baseados em barramento.
A seção 7.4 apresenta a ferramenta que automatiza o fluxo de teste de hardware baseado em software.
A Seção 7.5 apresenta algumas limitações das ferramentas desenvolvidas.
Por fim, a Seção 7.6 apresenta considerações finais.
Respostas em Software Conforme Bushnell e Agrawal, um LFSR modular genérico, apresentado na Figura 53, pode ser representado por a equação de matrizes representada na Figura 54.
Observando o comportamento do circuito de um LFSR modular desenvolveu- se o código em linguagem C apresentado na Figura 56.
Observa- se que existe um registrador de deslocamento no circuito LFSR, que é equivalente a uma instrução de rotação.
Porém, a linguagem C não possui esta instrução, somente a de deslocamento.
Portanto, uma instrução de rotação é emulada utilizando uma instrução de deslocamento e incremento.
Caso o bit mais significativo seja` 0' acontece apenas a propagação dos conteúdos dos flip-flops.
Caso o bit mais significativo seja` 1' ocorre a propagação dos conteúdos dos flip-flops, o incremento do bit menos significativo e a inversão dos valores onde o índice do polinômio for igual a 1 (operação xor).
É necessário descobrir uma forma genérica de representar o polinômio característico do LFSR.
Observando a Figura 54 pode- se chegar a conclusão que a representação desejada para o polinômio na rotina é semelhante à representação também utilizada para representar o polinômio na é fixo em` 0'.
Por exemplo, como pode ser verificado na Figura 55, o valor do vetor h que representa na matriz o polinômio 1+ x2+ x7+ x8 é 10100001.
Já na rotina este valor seria 00100001, onde o valor sublinhado representa o bit menos significativo.
O motivo desta diferença na implementação da representação do polinômio na matriz e na rotina é que na rotina o bit menos significativo é alterado em função de a instrução de incremento que é realizada antes da instrução ou65 exclusivo.
Com isto, o funcionamento de um LFSR modular foi simplificado a um conjunto de instruções simples, adequado para a execução num software embarcado o que, ao mesmo tempo, auxilia na redução do tempo total de teste do sistema e na redução do tamanho do código de teste.
De forma semelhante ao LFSR modular, um MISR modular, representado na Figura 57, pode ser emulado utilizando a equação de matrizes representada na Figura 58.
A Figura 59 apresenta a rotina de MISR desenvolvida.
A diferença em relação a a implementação do LFSR é que o MISR deve considerar a entrada de dados que representa as respostas do UUT.
Como pode ser observado, a única instrução adicionada é o ou- exclusivo com a resposta (input).
É importante destacar o motivo da escolha de implementação do LFSR e do MISR modular ao invés de o LFSR e do MISR padrão.
Como já apresentado na Figura 14 na página 15, tanto o LFSR quanto o MISR padrão possuem realimentações que dependem do estágio anterior.
Isto induz à implementação de um laço na rotina de software, o que aumentaria o tempo de teste do sistema.
Uma alternativa a estes métodos, como apresentado por Rajski e Tyszer, é através do uso de somadores como geradores de padrões pseudo-aleatórios e compactadores de respostas.
Esta abordagem diminui ainda mais o tempo de teste baseado em software pois, ao invés de o conjunto reduzido de instruções apresentadas na Figura 56 e Figura 59, somente uma instrução as substituiria.
Porém, esta abordagem ainda não é comum e também não é suficientemente documentada, pois, por exemplo, o próximo valor de um LFSR é definido por o seu polinômio característico.
Em somadores, como se define este próximo valor?
Somente somando, por exemplo, o valor 5 ao valor atual?
Existe alguma implicação na escolha deste valor somado?
Estas são algumas questões que impediam o uso de somadores.
Para validar as rotinas de LFSR e MISR modular desenvolvidas, foi utilizado o software matemático Maple V. O conjunto de instruções do Maple V, apresentado na Figura 60 e Figura 61, foi utilizado para emular um LFSR e um MISR modular, respectivamente.
Estas instruções criam uma matriz chamada Ts que representa o polinômio característico x3+ x+ 1 e um vetor chamado Xt que representa o estado atual do LFSR.
A multiplicação de Ts e Xt gera X, ou seja, o próximo estado do LFSR.
Em este caso, como o valor do estado atual é 110 b, o próximo estado deve ser 011 b, sendo o bit sublinhado menos significativo.
Estas instruções criam uma matriz chamada Ts que representa o polinômio característico x3+ x+ 1, um vetor chamado Xt que representa o estado atual do MISR e um vetor dt que representa a resposta do UUT.
A multiplicação de Ts e Xt somado com dt gera X, ou seja, o próximo estado do MISR.
Em este caso, como o valor do estado atual é 110b e da resposta é 111 b, então o próximo estado deve ser 100 b, sendo o bit sublinhado menos significativo.
Os exemplos validados com Maple V utilizaram um LFSR e um MISR de 32 estágios, o que representa uma multiplicação de uma matriz 32x32 e um vetor de 32 posições.
Uma das funções das ferramentas desenvolvidas é a geração automática de códigos.
De entre os códigos gerados citamos a lógica envoltória, o testbench de lógica envoltória, a interface do SOC e o código de teste.
Os três primeiros códigos são descritos em VHDL enquanto o último é descrito em linguagem C. De esta forma, foi desenvolvido em linguagem C+ um gerador automático de código reutilizável.
A Figura 62 resume a estrutura de um gerador de código específico.
Um gerador de código específico, chamado de xCodeGen, instancia dois módulos principais, o gerador de tabela de símbolos e o gerador de código genérico, respectivamente xTokenGen e CodeGen.
A interface entre estes módulos ocorre através dos métodos apresentados nesta figura, sendo que o tipo de dado principal é chamado de Token.
O módulo xTokenGen recebe todos os parâmetros necessários (basicamente o arquivo de projeto de SOC de extensão.
Soc) para atribuir valores aos símbolos existentes no arquivo de template (extensão.
Tpl). A linha tracejada entre xTokenGen e o arquivo de template significa que xTokenGen não manipula diretamente este arquivo, mas deve conhecer os símbolos existentes no mesmo.
Por outro lado, o módulo CodeGen recebe três parâmetros que são o arquivo de template, a tabela de símbolos preenchida por o xTokenGen e o nome do arquivo que deve ser gerado, representado por a extensão.
Out, mas que poderia ser um programa nas linguagens C ou VHDL.
O gerador de código possui três módulos de software descritos em C+ e um modelo (template) descrito na linguagem do código gerado (e.
g VHDL ou C).
A comunicação entre estes módulos ocorre através da passagem de um tipo de dados chamado de Token.
Este tipo de dados, número de símbolos da tabela.
A primeira coluna desta tabela contém o nome dos símbolos e a segunda coluna contém o seu valor.
O valor de um símbolo pode ser uma palavra ou várias linhas de código.
O primeiro módulo de software, chamado de xCodeGen ou gerador de código específico, é apresentado na Figura 63.
Este módulo instancia os outros dois módulos de software, chamados de xTokenGen ou gerador de tabela de símbolo específico e CodeGen ou gerador de código genérico.
Este código é diferente para cada gerador de código desenvolvido, porém, a estrutura básica é semelhante.
Basicamente as alterações que ocorrem são nas mensagens de saída e nos parâmetros que são passados para o gerador de tabela de símbolos;
O segundo módulo, chamado de xTokenGen ou gerador de tabela de símbolo específico, é apresentado na Figura 64.
Os três atributos básicos desta classe são m_ socFile, m_ errorMsg e m_ tokens, cujos significados respectivos são o nome do arquivo de projeto de SOC, a mensagem de erro e a lista de símbolos.
Os cinco métodos públicos são:
O capturar as mensagens de erros.
Outros métodos e atributos podem ser declarados como privados.
O código da Figura 64 reconhece os símbolos existentes no template do código.
Portanto, sua função é atribuir valores a estes símbolos de acordo com os parâmetros que são passados para esta classe.
Desta forma, este programa personaliza o template genérico gerando diferentes códigos.
A implementação deste módulo é caso a caso, dependendo do número de símbolos existente no template e da complexidade de montar os valores dos mesmos.
Somente a interface desta classe deve seguir um padrão para que a integração do módulo xCodeGen seja genérica.
De entre os O terceiro módulo de software, chamado de CodeGen ou gerador de código genérico, é apresentado na Figura 65.
Este módulo é responsável por substituir os símbolos enviados por xTokenGen e alterar- los no template de forma a gerar um código específico.
A Figura 66 apresenta um exemplo de template utilizado para demonstrar o funcionamento do gerador de código.
Os símbolos foram removidos de acordo com a tabela de símbolos de forma a criar uma descrição VHDL personalizada.
O principal objetivo desta ferramenta é integrar os núcleos e gerar a interface do SOC.
Para tanto, este fluxo de desenvolvimento é dividido em duas partes:
A criação de projetos de núcleos e a criação de projetos de SOCs.
Um projeto de núcleo de hardware é um arquivo como o apresentado na Figura 68.
Este arquivo concentra todas as informações necessárias para a posterior integração deste núcleo num SOC.
De entre as informações podemos citar:
O nome do núcleo;
Seu tipo (e.
g núcleo, processador ou memória);
Os arquivos VHDL que compõem o projeto;
O nome da entidade da interface;
E a descrição das portas da interface.
Este exemplo demonstra um arquivo de projeto de um núcleo pertencente ao bechmark ISCAS89.
Este núcleo se chama s1196, é composto por somente um arquivo VHDL e possui quatro portas:
Pinp, outp, clk e reset.
A definição de tipo de cada porta é apresentada no decorrer de a linha sendo que a última informação apresentada (sublinhado) é o uso da porta.
A Figura 69 apresenta as principais telas desta etapa de criação de projeto de núcleos.
As informações que devem ser preenchidas na primeira tela são o nome do núcleo, sua descrição, seu tipo e o local onde o projeto deve ser gravado.
Em a segunda tela o usuário deve informar os arquivos VHDL que compõem este núcleo.
As entidades existentes em cada arquivo adicionado são acrescentadas automaticamente na lista de seleção do campo top.
Em este campo o usuário deve selecionar qual das entidades do projeto é a interface do núcleo.
Esta ação executa automaticamente um analisador sintático desenvolvido para realizar a extração da interface.
Assim, todas as portas e generics desta entidade são apresentadas nas tabelas ports e generics, respectivamente.
O usuário deve obrigatoriamente definir o campo uso da porta (última coluna da tabela ports), pois é com base nesta informação que a ferramenta sabe distinguir a função de cada porta do núcleo.
Esta informação é utilizada para a integração dos núcleos e geração da interface do futuro SOC que utilizará este núcleo.
Todos os projetos de núcleos são acrescentados numa biblioteca que os disponibiliza para a futura integração num SOC.
Dada uma biblioteca de núcleos, a próxima tarefa é selecionar os núcleos que compõem o SOC, integrar- los de acordo com algum meio de comunicação e gerar a interface do SOC.
Estas tarefas fazem parte da geração de projeto de SOC que gera o arquivo apresentado na Figura 70.
De entre as informações mais importantes deste arquivo destacamos:
O nome, o tipo (e.
g ASIC ou FPGA), os núcleos que compõem o SOC e a definição da integração dos núcleos.
Este exemplo de projeto de um SOC, chamado de d281, foi retirado dos benchmarks ITC02.
Este exemplo é implementado em ASIC, possui uma descrição textual e é formado por nove núcleos.
Cada linha de definição de um núcleo é formada por o nome de instância do núcleo, o arquivo de projeto do núcleo e o número de generics que este núcleo possui.
Logo após segue o nome da biblioteca de DFT e de tecnologia usadas.
Por fim, seguem informações do tipo de comunicação selecionada para integrar estes núcleos.
A Figura 71 apresenta as principais telas que integram a criação de projeto de SOC.
Em a primeira tela) o usuário deve selecionar da biblioteca de núcleos quais são os núcleos que devem compor o SOC.
Esta biblioteca deve ter sido previamente gerada como especificado na seção anterior.
A o adicionar um núcleo ao projeto, o usuário deve informar o nome da instância do módulo e os valores dos generics, se os mesmos existirem.
Caso o núcleo que deve ser adicionado ao SOC não exista na biblioteca, o usuário pode adicionar- lo selecionando o módulo user defined.
Este módulo lança as telas de definição de um núcleo de hardware apresentadas na Figura 69.
Ainda na tela de seleção de núcleos, o usuário deve selecionar a forma de integração dos núcleos.
Atualmente somente integração por barramento é suportada.
Em o próximo passo, apresentado na Figura 71 (b), o usuário deve parametrizar o meio de comunicação especificado na etapa anterior.
No caso de barramento, o usuário deve informar o mestre do barramento e como este barramento deve ser implementado fisicamente (e.
g tri-state ou multiplexador).
Esta tela seria diferente se outro meio de comunicação fosse suportado, refletindo os parâmetros relacionados ao mesmo.
Acreditamos que esta forma de implementação confere flexibilidade à ferramenta uma vez que a mesma está preparada para suportar outros meios de comunicação.
A terceira e a quarta telas, apresentadas na Figura 71 (c, d), executam a ferramenta de geração de lógica envoltória, de testbench da lógica envoltória e da interface do SOC apresentadas na Seção compilação com Modelsim.
Uma vez que o protocolo de comunicação foi selecionado, a lógica envoltória é sintetizada para cada núcleo.
A lógica envoltória é gerada depois da definição do meio de comunicação, pois o protocolo de comunicação e a interface da lógica envoltória estão associados ao meio de comunicação.
A Figura 72 apresenta um exemplo de lógica envoltória.
Ela possui cadeias de varredura, chamadas de células de entrada (ci) e saída (com o), utilizadas para adaptar a largura do barramento (32 bits) à largura das entradas e saídas do núcleo.
Em este exemplo, a largura das portas de entrada é de 68 bits, e a saída é de 42 bits.
O teste é executado em duas etapas:
Carga dos padrões de teste e leitura das respostas.
Os seis bits menos significativos da porta addr são utilizados para ativar o modo de teste, isto é, a carga de padrões ou leitura de respostas.
Quando a lógica envoltória está no modo de teste e ocorre uma operação de escrita (porta wr), o sinal clockEnb é ativado durante um ciclo de relógio para processar os padrões atribuídos.
Observar que o sinal clockEnb é ativado somente quando a última parte do padrão é escrita na célula de entrada.
As respostas são armazenadas nas células de saída um ciclo após clockEnb ter sido ativado.
O processador lê as respostas quando a porta rd é ativada.
Em esta etapa do fluxo de projeto, dois arquivos são criados para cada núcleo:
A lógica envoltória e seu testbench.
Este testbench possui dois modos de funcionamento:
O modo com padrões gerados internamente e com padrões lidos de arquivo.
O modo de padrões internos é preferencialmente utilizado para validar a lógica envoltória enquanto o outro modo é utilizado para executar a simulação que gera a assinatura esperada.
É interessante destacar que o testbench gerado automaticamente foi o mesmo testbench utilizado durante a validação da lógica envoltória.
Desta forma, foi necessário apenas o esforço de adicionar- lo ao ambiente.
Ainda sobre a validação da lógica envoltória, a mesma foi validada por simulação funcional, temporal e por prototipação em FPGA.
Em os Anexos 10.2.1 e 10.2.2 é apresentado um exemplo de lógica envoltória e de seu testbench.
Wrapper Core n Portas Externas Barramento A última parte do processo de integração de núcleos é a geração da interface do SOC, a qual é obtida através da informação do uso das portas contida no arquivo de projeto de núcleos.
Quando uma porta é do tipo external, ela é conectada à interface do SOC conforme a Figura 73.
A lógica envoltória em destaque possui portas conectadas à interface de comunicação e portas do tipo external conectadas à interface do SOC.
Este tipo de porta impõe modificações (adição de portas) na lógica envoltória e na própria interface do SOC.
Um exemplo de interface de SOC é apresentado no Anexo 10.2.3.
O objetivo principal da ferramenta de teste é a geração do código de teste.
Para alcançar este objetivo diversas tarefas intermediárias são realizadas, conforme a Figura 74: Seleção de configuração do LFSR e do MISR, execução de simulações de falhas e de ATPG, diversas conversões de formatos de arquivo, geração de assinatura e co-simulação do código de teste.
A Figura 75 apresenta as principais telas que compõem este fluxo:
Tela de seleção de núcleo de hardware, de configuração do MISR, de seleção de polinômio e de semente para o LFSR.
Em a primeira etapa, apresentada na Figura 75 (a), o usuário deve selecionar o núcleo de hardware que vai ser configurado para geração de software de teste.
Existem dois campos de seleção, o primeiro é usado para selecionar o núcleo de hardware e o outro é usado para selecionar o processador que testará o núcleo.
O campo de seleção de processador deve conter somente um processador, pois atualmente o ambiente suporta somente barramento com um mestre.
É importante que exista este campo, pois indica que a interface gráfica está preparada para suportar múltiplos processadores no sistema.
Em a segunda etapa, apresentada na Figura 75 (b), o usuário deve selecionar a configuração do MISR.
Esta configuração suporta somente um polinômio e uma semente.
Os dados desta tela são gravados num arquivo com o seguinte formato Em a terceira e na quarta etapa, apresentadas na Figura 75 (c, d), o usuário deve selecionar a configuração do LFSR.
Primeiramente o usuário seleciona um ou mais polinômios e depois seleciona as sementes para cada polinômio.
A configuração de LFSR selecionada é salva num arquivo no formato apresentado na Figura 76.
Este arquivo de configuração é utilizado por um modelo de LFSR genérico desenvolvido em VHDL conforme apresentado na Figura 74 (b).
Usando o arquivo de configuração do LFSR este modelo cria os padrões que esta configuração geraria no momento do teste através de simulação (ModelSim).
Estes padrões são salvos no formato tabular que é suportado por o Modelsim.
Um exemplo do formato tabular é apresentado na Figura 77.
A primeira linha define o sinal que está sendo gravado.
Cada linha define o valor assumido por este sinal.
Este formato é utilizado principalmente devido a sua simplicidade para gerar os padrões de uma configuração de LFSR.
O formato tabular é convertido para o formato do simulador de falhas, apresentado na Figura 78, para que a configuração de LFSR seja avaliada.
O programa tab2ascii, apresentado na Figura 74 (c), foi desenvolvido para realizar esta conversão.
Um script contendo os comandos do simulador de falhas é personalizado por a ferramenta de teste para a execução da simulação -- Figura 74 (d).
É importante destacar que a simulação de falhas deve ter como entrada o circuito já sintetizado para a tecnologia alvo.
Isto é obtido através da síntese lógica da descrição VHDL do núcleo -- Figura 74 (a), gerando uma descrição no nível lógico de abstração de este (formato EDIF).
Este exemplo representa padrões de teste gerados para um circuito combinacional com uma entrada e uma saída, respectivamente PINP e OUTP, de dez bits cada uma.
Este formato é dividido em duas seções:
Setup e scan_ test..
A seção setup define, entre outras funções, as portas de entrada e saída do circuito.
A seção scan_ test define os padrões de teste.
O comando force atribui um valor ao circuito e o comando measuse lê os valores de saída.
A simulação da cobertura de falhas obtida com esta configuração de LFSR é detalhada num relatório.
Depois deste passo o usuário pode executar o ATPG para gerar padrões determinísticos aumentando a cobertura de falhas -- Figura 74 (e).
Porém, antes da execução de ATPG outro script deve ser personalizado para gerar padrões determinísticos para este núcleo de hardware.
É importante ressaltar que este script deve indicar ao simulador de falhas para gerar padrões apenas para as falhas não detectadas por os padrões do LFSR.
A o fim destes processos existem dois arquivos de padrões de teste no formato texto, um para padrões pseudo-aleatórios e outro para padrões determinísticos.
Com os padrões de teste gerados, a próxima etapa é a geração da assinatura do núcleo.
Para gerar a assinatura os padrões de teste são convertidos, conforme Figura 74 (f), para um formato mais simples apresentado na Figura 79.
Este formato concatena os padrões pseudo-aleatórios e determinísticos num único arquivo.
Este arquivo é utilizado por o testbench da lógica envoltória para simular o núcleo de hardware e apresentar ao final a assinatura esperada -- Figura 74 (g).
Este formato é gerado por o programa Ascii2Vector e é utilizado para geração de assinatura do módulo.
Os padrões pseudo-aleatórios e determinísticos são concatenados neste formato de arquivo.
Desta forma, o testbench do módulo pode ser utilizado para ler este arquivo e simular os valores de saídas esperados.
A etapa subseqüente é a geração de código de teste que utiliza a configuração de LFSR, do MISR e o arquivo de padrões de teste determinístico para criar o programa em linguagem C Figura 74 (h).
Um exemplo de código de teste gerado é apresentado e comentado no Anexo 10.2.4.
Embora um fluxo de projeto completo tenha sido desenvolvido, o mesmo possui diversas limitações apresentadas nesta Seção.
Não existem limitações quanto a a estrutura dos núcleos, mas existem limitações quanto a a interface do mesmo.
Estas limitações na interface existem pois há limitações no analisador sintático que executa extração da interface do núcleo.
Estas limitações são:
A interface não pode possuir portas de tipos compostos ou tipos definidos em packages;
Os tipos suportados na interface (i.
e nas portas e generics) são std_ logic, bit, std_ logic_ vector, bit_ vector, integer, natural e positive;
Somente generics do tipo integer, natural ou positive podem ser inicializados com valor padrão;
Não é possível declarar portas do tipo «data:
In std_ logic_ vector», pois a constante D não é suportada.
Uma limitação não relacionada à interface é que o núcleo deve ser descrito em VHDL (soft core).
Núcleos do tipo memória não foram avaliados em termos de cobertura de falhas por não possuirmos uma ferramenta de CAD que executa tal função.
Por este motivo memórias não são suportadas atualmente por o sistema.
Porém, este suporte pode ser facilmente implementado com a adição de uma biblioteca de algoritmos de teste de memória, conforme apresentado na Seção 4.2.2.
O único meio de comunicação que foi implementado é um modelo parametrizável de barramento que permite apenas um mestre.
Um processador não foi integrado à biblioteca de núcleos.
O processador utilizado para a validação do ambiente é o Nios que está disponível somente na forma de firm core.
Um dos trabalhos futuros é integrar o processador R81 ao sistema.
Assim, será possível descrever um SOC por completo.
Processador de 16 bits desenvolvido no GAPH (PUCRS).
As ferramentas de CAD suportadas são Modelsim 5.5 e, Leonardo 2001.1 d. 45, Flextest ser facilmente integradas ao ambiente, pois este possui uma interface genérica de configuração e lançamento de ferramentas de terceiros.
Embora a ferramenta de extração de interface de núcleos suporte diversos tipos de portas, a lógica envoltória suporta somente tipos std_ logic e std_ logic_ vector.
Para o suporte a outros tipos, deve- se acrescentar à ferramenta de geração de lógica envoltória recursos de conversão de dados;
Os núcleos testados possuem interface com uma porta de entrada e uma de saída e portas de clock e reset.
Outras interfaces devem ser testadas;
Não foi implementado o suporte às cadeias de varredura.
Através da ferramenta DFTAdvisor, a ferramenta desenvolvida é capaz de inserir cadeias de varredura num núcleo, porém, este recurso não foi testado adequadamente;
O fluxo de teste não suporta núcleos com cadeias de varredura.
Isto ocorre principalmente devido a a complexidade do formato do arquivo de padrões de teste do Flextest e Fastscan quando o mesmo possui estas cadeias.
Para suportar- las devem ser feitas modificações nos programas de conversão de formato de padrões como o tab2ascii e ascii2vec.
O modelo VHDL usado para gerar a assinatura e o gerador de código de teste também deve ser alterado para suportar as cadeias.
O ambiente de teste não possui uma ferramenta que auxilie na seleção de polinômios e sementes para um dado núcleo.
Esta seleção deve ser realizada por o usuário.
Este Capítulo apresentou um protótipo funcional de um ambiente integrado de projeto e teste de SOCs baseados em barramento.
Esta ferramenta automatiza o processo de geração de código de teste para núcleos de hardware não programáveis.
Este ambiente foi concebido como um arcabouço de projeto, capaz de incorporar vários outros módulos e técnicas de teste.
Isto porque os principais preceitos empregados no desenvolvimento do mesmo foram facilidade de uso, escalabilidade, flexibilidade e modularidade.
Acreditamos que devido a estas características esta ferramenta venha a contribuir no desenvolvimento de outros trabalhos como, por exemplo, o teste de núcleos em redes intra-chip (NOCs).
A originalidade desta ferramenta não está no desenvolvimento de novas técnicas ou algoritmos de teste, mas na integração de diversas técnicas conhecidas.
A complexidade deste ambiente pode ser avaliada, de forma relativa, através do número de linhas de código apresentado na Tabela 10.
A Figura 74 na página 76 demonstra que um fluxo de projeto relativamente complexo foi drasticamente simplificado com a criação desta ferramenta de teste.
Em o próximo capítulo avalia- se o código de teste gerado em termos de tempo de teste e requisitos de memória.
Como apresentado na introdução deste trabalho, na literatura são encontradas referências às vantagens do uso procedimentos de teste implementados em software e algumas avaliações preliminares destas.
Entretanto, as conclusões destes trabalhos não permitem avaliar a aplicabilidade desta técnica, principalmente por não avaliarem suas desvantagens.
Desta forma, a terceira contribuição deste trabalho é a avaliação das principais desvantagens do teste baseado em software (i.
e utilização de área de memória e tempo de teste) utilizando o código de teste gerado por a ferramenta descrita no capítulo 7.
Este capítulo está organizado como segue.
Em a Seção 8.1 apresentamos os estudos de caso utilizados na avaliação.
Em as Seções 8.2 e 8.3 avaliamos a área de memória e o tempo de teste, respectivamente.
Os estudos de caso utilizados fazem parte do conjunto de benchmarks ISCAS85 e ISCAS89.
O critério para a escolha deste subconjunto foi o tamanho reduzido dos núcleos, devido a o fato que a ferramenta de teste desenvolvida não possui um recurso que ajude na seleção de polinômios e sementes.
Para circuitos maiores é importante um recurso que ajude na seleção de polinômios e sementes, fora de o escopo deste trabalho.
A abordagem utilizada para selecionar os polinômios foi:
Seleciona- se diferentes polinômios e sementes, tendo cuidado com a minimização de número de padrões pseudo-aleatórios, pois este valor está diretamente relacionado ao tempo teste;
Seleciona- se polinômios e sementes somente quando sua contribuição à cobertura de falhas for significativa.
Esta abordagem necessita de diversas execuções de simulações de falhas e de ATPG até chegar a uma configuração de LFSR satisfatória.
Estas tarefas consomem muito tempo de execução se for considerado um circuito maior.
Isto justifica o uso destes estudos de caso menores.
De entre os núcleos selecionados citamos:
C1908, c1355, c3540 e s11961.
As características gerais dos mesmos são apresentadas na Tabela 11.
Os atributos apresentados são o número de portas de entrada, de saída e o número de portas lógicas equivalentes para a implementação do núcleo e do núcleo com a lógica envoltória gerada automaticamente.
Nota- se que a área da lógica envoltória é elevada.
Isto ocorre principalmente por a adição de células de entrada e saída para adaptar a interface do núcleo à largura do barramento (32 bits).
Em relação a a lógica envoltória podemos observar que seu uso corresponde à integração do núcleo ao barramento, não sendo específica para teste.
Esta área adicional da lógica envoltória pode parecer significativa, porém, deve- se observar que os núcleos são de baixa complexidade, não correspondendo a estudos de caso reais para SOC.
A Tabela 12 apresenta a configuração de teste utilizada no restante da avaliação dos resultados.
Esta tabela apresenta:
A cobertura de falhas obtida com a configuração de LFSR selecionada na primeira coluna;
A cobertura de falhas obtida com a configuração de LFSR e os padrões determinísticos na segunda coluna;
O número total de padrões de teste pseudo-aleatórios e determinísticos nas colunas 3 e 4, respectivamente.
Cobertura de falhas (Cf em%) e número de padrões rand Cf det+ rand Cf pat rand pat det É importante salientar que a qualidade da configuração de LFSR utilizada reflete- se nos dados desta tabela de duas formas diferentes:
Uma boa configuração de LFSR poderia atingir a mesma cobertura de falhas utilizando menos padrões pseudo-aleatórios.
Isto implica na redução dos dados da terceira coluna da tabela o qual implica no tempo de teste;
Uma boa configuração de LFSR poderia atingir uma cobertura de falhas maior, possivelmente aumentando o número de padrões pseudo-aleatórios, o que diminuiria o número de padrões determinísticos.
A redução de padrões determinísticos influenciaria pouco no tempo de teste, pois possivelmente o número de padrões pseudo-aleatórios aumentaria, mas influenciaria diretamente na área de memória utilizada para teste.
A Tabela 13 apresenta diferentes configurações do código de teste.
Três configurações principais foram analisadas:
A puramente software, o LFSR em software e o MISR em hardware e um somador como gerador de padrões e o MISR também em hardware.
Outras duas configurações otimizadas para o núcleo s1196 foram exploradas para destacar o compromisso entre flexibilidade do código de teste e tempo de teste.
O otimizado 1 e 2 refletem otimizações, respectivamente, nas implementações representadas por a coluna 1 e 3.
O tamanho do código de teste é independente do benchmark.
Como esperado, à medida que se remove funcionalidade do software para o hardware o tamanho do código de teste diminui, como apresentado na primeira e segunda coluna.
O código de teste também diminui quando o código é simplificado substituindo o LFSR por um somador, conforme colunas 2 e 3.
O núcleo s1196 possui portas de entrada e de saída de 14 bits.
Porém, um código de teste genérico deve considerar núcleos com largura maior, inclusive maior que a largura de barramento de 32 bits.
Entretanto, a generalidade de o código aumenta o tempo de teste de certos núcleos, e.
g E não otimizada para o núcleo s1196 com LFSR e MISR em software.
Como é observado na Tabela 14, o código de teste otimizado possui um conjunto de instruções significativamente menor que o código genérico.
Deve- se salientar que foram removidos da versão otimizada somente os laços específicos para leitura e escrita em núcleos com entrada e saída maiores que 32 bits, o que não alterou a funcionalidade do núcleo em questão.
Isto demonstra que pode haver um compromisso entre flexibilidade do código de teste, o que implica em aumento na área de memória, e tempo de teste.
A Tabela 15 apresenta o número de palavras de memória (32 bits) utilizadas para armazenar a configuração do LFSR e os padrões determinísticos.
Esta informação é complementar ao número de padrões apresentados na Tabela 12.
Por exemplo, o núcleo c1908 utiliza 40 padrões pseudoaleatórios e 68 padrões determinísticos.
A configuração de LFSR destes 40 padrões ocupa 9 palavras, enquanto que os 68 padrões determinísticos ocupam 137 palavras pois o núcleo possui mais de 32 bits de porta de entrada.
A Tabela 16 apresenta o número de bytes necessários para armazenar as funções do código de teste na configuração onde o LFSR e o MISR são implementados em software.
De acordo com os dados apresentados podemos concluir que o tamanho do código de teste não impõem uma restrição forte ao sistema, pois o código pode ser utilizado para testar diversos núcleos.
Porém, a generalidade de o código pode aumentar o tempo de teste de certos núcleos.
Para estes casos é interessante descrever funções de teste otimizadas.
Deve- se notar que o código de teste genérico necessita saber somente o número de portas de entrada e saída de cada núcleo para executar o teste caso- a- caso.
Desta forma, haverão apenas quatro possíveis configurações de núcleos:
Os de entrada e saída maior que 32 bits, os de entrada maior e saída menor (e vice-versa) e os de entrada e saída menor que 32 bits.
Em detrimento de o tempo de teste, pode ser interessante replicar as funções de teste para estas quatro configurações, uma vez que as mesmas ocupam pouca área de memória conforme demonstrado.
Finalmente, conclui- se que a área de memória para armazenar o código de teste não é uma restrição como alegado na literatura, pois com poucos bytes de memória armazena- se o programa de teste.
Isto porque, como o mesmo código é utilizado para testar diversos núcleos, o tamanho do código de teste permanece inalterado à medida que aumenta o número de núcleos no sistema.
Obviamente esta conclusão é aplicada no caso de um código de teste elaborado de forma a aumentar o reuso do mesmo, que é o caso do código gerado por a ferramenta desenvolvida.
Desta forma, no que diz respeito ao tamanho do código de teste, teste baseado em software pode ser aplicado em sistemas embarcados com baixo custo de memória.
Já no que diz respeito à área de memória necessária para armazenar a configuração de LFSR e determinísticos, é importante selecionar uma boa configuração de LFSR que possa minimizar os padrões determinísticos ou utilizar um algoritmo de compressão de dados.
Diversas implementações destas abordagens são encontradas na literatura.
Esta Seção avalia em termos de tempo de teste as três configurações de código de teste utilizadas anteriormente e a configuração puramente hardware.
Para avaliar o tempo de teste, utilizamos o ambiente de co-simulação específico para o processador Nios.
Este ambiente suporta co-simulação temporal considerando inclusive elementos como bolhas no pipeline do processador, tornando a medida de tempo precisa.
Por outro lado, a configuração puramente hardware foi modelada de forma que o tempo de teste total fosse representado por a equação Tt $= Pp+ 10* Pd, onde Tt é o tempo de teste, Pp é o número de padrões pseudo-aleatórios e Pd é o número de palavras de memória utilizadas para armazenar os padrões determinísticos.
A Tabela 17 apresenta o número de ciclos de relógio transcorridos entre a geração de um padrão determinístico e um padrão pseudo-aleatório, obtidos por co-simulação.
A primeira coluna desta Tabela mostra que um LFSR pode gerar um novo estímulo a cada ciclo de relógio, e devido a o atraso imposto por a leitura da memória de teste os padrões determinísticos são gerados em média a cada 10 ciclos de relógio (arbitragem do meio físico e geração de sinais de controle), resultando na equação Tt $= Pp+ 10* Pd.
Já a geração de padrões a partir de o processador embarcado, para os experimentos realizados, consumiu em média 180 ciclos de relógio, tanto para padrões randômicos quanto determinísticos.
Estes dados indicam que o tempo de teste para técnicas baseadas em software comparadas com técnicas baseadas em hardware possui um custo adicional de 1 a 2 ordens de grandeza.
As colunas 3 a 6 mostram a redução do tempo de teste obtida com otimizações no software e compromisso entre partições hardware e software dos módulos de teste.
A Tabela 18 apresenta o tempo total de teste para os três códigos de teste, para a abordagem puramente hardware e para as otimizações do núcleo s1196.
As colunas destacadas, segunda e quinta, apresentam os tempos de teste para as implementações puramente software e puramente hardware, respectivamente.
A relação entre os tempos destas colunas nos permite responder à questão &quot;quanto custa a implementação de técnicas de teste baseadas em software em termos de tempo de teste?».
Este tempo é inferior a duas ordens de grandeza, e está fortemente realcionado ao número de padrões determinísticos.
Se este for pequeno, como é o caso do benchmark c1908 o custo é mais elevado (54 vezes), porém, para os demais casos temos uma relação média de 25 vezes no tempo de teste.
Nota- se ainda que o tempo de teste diminui à medida que funcionalidades são passadas para o hardware ou o software é simplificado com a substituição de um LFSR por um somador.
Este fato justifica maiores estudos no particionamento hardware/ software da estratégia de teste e também justifica a inclusão de rotinas específicas de teste no processador como geradores de padrões ou compactadores de respostas.
O compromisso entre generalidade de código de teste e tempo de teste também é novamente evidenciado conforme dados apresentados nas versões de código otimizado do núcleo s1196.
Os resultados indicam que o tempo de teste é uma restrição importante.
Comparando a abordagem puramente hardware com a baseada em somador e MISR em hardware, pode- se notar que a diferença é um pouco mais que uma ordem de grandeza.
A abordagem de teste baseada em somador e MISR em hardware pode ser interessante se comparada com um teste baseado num Ate lento (i.
e baixa largura de banda e baixa freqüência de operação).
Exceto este caso, substituição de Ate lento, o tempo de teste da abordagem baseada em software comprometeria o teste de fabricação.
Entretanto, esta abordagem é adequada para teste de campo, pois os requisitos de memória são baixos e o tempo de teste está na ordem de milisegundo.
Em os objetivos apresentados na Seção 1.2, exploram- se duas perguntas que este trabalho se propunha a responder:
Resposta: Teste baseado em software pode ser aplicado em teste de fabricação com ganhos se o mesmo substituir um processo de teste baseado num Ate com baixa largura de banda e freqüência de operação.
Caso contrário, se comparado com BIST implementado em hardware, teste baseado em software aumenta o tempo de teste 10 a 40 vezes, dependendo da implementação do código de teste.
Resposta: Teste baseado em software pode ser aplicado em teste de campo, pois:
Os requisitos de memórias são baixos, em contrariando afirmações constantes na literatura recente.
Estes requisitos de memórias podem ser diminuídos ainda mais se algoritmos de compressão de dados e seleção de sementes forem explorados;
O tempo de teste é na ordem de mili-segundos, o que possibilita que o sistema seja interrompido para teste a intervalos regulares de acordo com a aplicação do sistema ou no momento da inicialização do mesmo.
O teste baseado em software minimiza as duas desvantagens principais do BIST:
Degradação do desempenho e aumento da área de hardware.
Porém, de acordo com a literatura, o teste baseado em software apresenta a desvantagem de aumentar os requisitos de memória e o tempo de teste.
O presente trabalho contribuiu no sentido de avaliar quantitativamente as desvantagens do teste baseado em software.
Como apresentado nos resultados, o tempo de teste realmente é uma desvantagem significativa quando se utiliza somente teste baseado em software.
Porém, foi demonstrado que este tempo de teste pode ser minimizado particionando os módulos de teste em hardware e software.
Por outro lado, o aumento dos requisitos de memória de teste não é desvantagem.
Um projeto do código de teste visando o reuso pode anular esta desvantagem visto que diferentes módulos do sistema são testados com o mesmo código.
É ainda possível reduzir mais as restrições da memória de teste utilizando algoritmos de compressão e seleção de sementes.
Porém, estes recursos não foram explorados neste trabalho.
Outra contribuição deste trabalho foi o desenvolvimento de uma ferramenta de auxílio ao teste baseado em software.
Esta ferramenta integra o fluxo de desenvolvimento de SOC ao fluxo de teste.
Como mostrado na Figura 74, página 76, um fluxo relativamente complexo foi drasticamente simplificado com a implementação destas ferramentas.
A ferramenta desenvolvida permite incorporar vários outros módulos e técnicas de teste.
Isto porque os principais preceitos empregados no desenvolvimento do mesmo foram facilidade de uso, escalabilidade, flexibilidade e modularidade Uma contribuição adicional deste trabalho foi o desenvolvimento de uma ferramenta de cosimulação funcional e geograficamente distribuída.
As principais características deste ambiente de co-simulação são:
Lançamento automático dos simuladores;
Suporte à co-simulação geograficamente distribuída;
Suporte à VHDL e C (inclusive suas variações como C+ e SystemC);
Suporte à co-simulação de modelos arquiteturais e RTL; (
v) flexibilidade do roteador de co-simulação.
A o fim deste trabalho aproximadamente 10000 linhas de código foram desenvolvidas, sendo 8250 linhas referentes ao ambiente de integração de núcleos e de teste e 1700 linhas do ambiente de co-simulação.
As limitações das ferramentas desenvolvidas foram destacadas nos Capítulos 5 e 7.
Estas limitações correspondem a motivações para trabalhos futuros.
De entre estes trabalhos podemos citar:
Avaliação das ferramentas desenvolvidas num conjunto maior dos benchmarks ISCAS 85/ 89 e ITC99/ 02.
Acrescentar na avaliação os critérios acréscimo/ redução da área de hardware, impacto na freqüência de operação e dissipação de potência.
Porém, é interessante ter uma ferramenta de inserção automática de BIST para simplificar a implementação de teste baseado em hardware.
Acrescentar novos recursos nas ferramentas desenvolvidas, como: (
i) diferentes meios de comunicação para diminuir o tempo de co-simulação; (
ii) implementar algoritmo de compressão de dados e de seleção de polinômios e sementes para reduzir ainda mais os requisitos de Memória do sistema; (
iii) integrar um processador descrito em VHDL ao ambiente de teste desenvolvido; (
iv) suportar cadeias de varredura; (
v) suportar outros meios de comunicação além de o barramento; (
vi) suportar interfaces padrão como OCP; (
vii) suportar múltiplos processadores.
