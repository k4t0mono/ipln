Mudanças são inevitáveis durante o ciclo de vida do software.
Estas mudanças são resultado de diferentes necessidades, como a evolução do conhecimento sobre os processos de negócio, alterações de ambiente, etc..
Nestas circunstâncias, é crucial ter controle sobre o que essas mudanças representam na aplicação.
A análise de impacto representa o processo que gera este conhecimento.
Essa análise possui um significado abrangente dentro de o desenvolvimento de software, incluindo desde a identificação de estruturas no código fonte até o controle das restrições de gestão de projeto.
Esta tese apresenta um modelo para analisar o impacto no código fonte de uma aplicação utilizando ontologias, visando melhorar a precisão e revocação de estruturas identificadas se comparadas a técnicas existentes.
O uso de ontologias integra uma perspectiva semântica nas técnicas tradicionalmente baseadas na análise sintática do código fonte.
Para o desenvolvimento do modelo de análise de impacto, foram definidos dois submodelos:
O de rastreabilidade e o de probabilidade.
O modelo de rastreabilidade recebe como entrada o código fonte e uma ontologia de domínio e gera como resultado uma ontologia de rastreabilidade populada automaticamente com elos entre conceitos do domínio (classes e propriedades) e estruturas do código (classes, métodos e atributos).
Estes elos são populados através de um analisador léxico e semântico que realiza a categorização, normalização (geração de tokens, expansão e eliminação) e comparação (stemming).
Com base na ontologia de rastreabilidade e num requisito de mudança, o modelo probabilístico classifica cada elo de rastreabilidade utilizando o modelo de Redes de Crenças Bayesianas.
Para o cálculo de probabilidade, a classificação dos nodos utilizou o algoritmo PageRank do Google e das arestas, a análise de frequência TFIDF e a dependência conceitual, definida nesta tese.
Este modelo de análise de impacto foi implementado como um plugin do eclipse e foi avaliado empiricamente através de três experimentos controlados.
Palavras chave:
Análise de impacto, ontologia, recuperação de informação.
A análise de impacto avalia os diversos riscos associados a uma mudança.
Essa análise possui uma abrangência significativa dentro de um projeto de software, podendo considerar aspectos econômicos, financeiros, de recursos humanos, entre outros.
Para o propósito deste trabalho, a análise de impacto se refere a mudanças em software, mais especificamente, a análise dos efeitos de determinada mudança no código fonte de uma aplicação.
Para uma definição clara do termo &quot;análise de impacto», este trabalho utilizará a apresentada por Bohner e Arnold que considera a análise de impacto como o processo de identificar potenciais consequências de uma mudança ou a estimativa do que precisa ser modificado para realizar uma mudança.
Outras definições serão discutidas na Seção 2.1.1, mas por enquanto é suficiente perceber que essa definição diferência explicitamente consequências e modificações.
Consequências típicas se referem ao impacto em algumas restrições do projeto, tais como custo, tempo e qualidade, enquanto modificações consideram aspectos de desenvolvimento, em o qual estruturas de código devem ser alteradas para atender certa demanda.
Modificações podem ter consequências positivas ou negativas, dependendo dos resultados de sua execução.
Essas consequências se referem a desvios de funções existentes que podem implementar novas necessidades, bem como gerar falhas no sistema.
Mudanças devem ser analisadas em duas perspectivas:
De o time de desenvolvimento que implementa a mudança e dos cenários de negócio que representam as partes interessadas, clientes e o mercado em geral.
O desenvolvimento de software engloba essas perspectivas através da execução de um conjunto de atividades que traduzem necessidades de negócio em código fonte.
Ao longo de os anos, diferentes estratégias e práticas vêm sendo desenvolvidas para superar as dificuldades inerentes à intersecção dessas perspectivas.
Estas práticas geralmente formalizam um conhecimento particular do sistema em artefatos de software, tais como código fonte, especificações e diagramas.
Esses artefatos representam uma perspectiva do sistema num período de tempo e tem o objetivo de compartilhar e alinhar esse conhecimento entre as partes interessadas.
Como softwares modelam essencialmente fenômenos do mundo real, é possível adotar modelos e técnicas de desenvolvimento fundamentadas na semântica.
O desenvolvimento orientado a ontologias representa a sinergia entre processos e artefatos da Engenharia de Software e ontologias, que especificam explicitamente conceitos do mundo real.
Para viabilizar a análise de impacto em código fonte utilizando uma perspectiva semântica, esta tese inclui modelos de rastreabilidade e de probabilidade.
O modelo de rastreabilidade estrutura e relaciona conceitos do domínio com o código fonte da aplicação.
Para isso, tanto o código fonte quanto o modelo de domínio são representados e relacionados utilizando ontologias.
A rastreabilidade utiliza técnicas de processamento de linguagem natural para extrair conhecimento a partir de dados brutos, populando a ontologia de domínio com elos de rastreabilidade.
O modelo de probabilidade, por sua vez, identifica a relevância de cada elo de rastreabilidade, analisando as relações de baixo nível extraídas do código fonte e ponderando com a organização semântica e conceitual modelada na ontologia.
O resultado é a identificação de estruturas de código relevantes frente a uma solicitação de mudança.
A perspectiva semântica da análise de impacto inclui a especificação de uma base de conhecimento do projeto de software por a manutenção de ontologias do domínio e do código fonte da aplicação.
Estas ontologias mantêm o conhecimento tanto do negócio quanto das entidades computacionais derivadas de seus conceitos.
Para apoiar essa tese, os experimentos discutidos em evidenciam empiricamente que a rastreabilidade de modelos UML orientada a conceitos é mais precisa e necessita de menos esforço do que abordagens tradicionais baseadas em requisitos.
Com base nessas evidências, esta pesquisa tem como objetivo desenvolver um modelo capaz de avaliar a relevância das entidades no código fonte impactadas por determinada solicitação de mudança, bem como avaliar as vantagens que podem ser obtidas por o uso de representações formais do conhecimento durante a manutenção de software.
Motivação A análise de impacto é uma atividade fundamental durante a evolução de software, pois cerca de 80% do ciclo de vida de um sistema equivale a sua manutenção.
O uso contínuo de qualquer sistema não trivial estimula um fluxo de mudanças.
A adição de novas funcionalidades provavelmente impactará as funcionalidades existentes, sejam elas manutenções corretivas, adaptativas ou de melhoria.
Mudanças podem ocorrer por diversas razões, desde adição de novas funcionalidades, correção de falhas, mudança de ambiente (como troca de servidor de aplicação ou banco de dados) ou pedidos de melhoria.
Essas mudanças são consequência do conhecimento que os usuários adquiriram sobre o problema resolvido por um software e esperam que o software evolua tanto quanto esse conhecimento Durante o desenvolvimento de software, diferentes necessidades aumentam sua complexidade, desde questões técnicas até de negócios.
As mudanças requeridas no software não devem ser negligenciadas ou implementadas incondicionalmente no sistema.
A análise de impacto fornece, neste sentido, meios para determinar as características e efeitos de uma proposta de mudança, fornecendo a base para sua aprovação ou não.
Um fator crítico da análise de impacto diz respeito ao encadeamento de mudanças.
A modificação numa unidade do código fonte geralmente impacta estruturas dependentes, podendo introduzir acidentalmente defeitos.
Um problema típico é a dificuldade de identificar essas estruturas, principalmente em sistemas com alto acoplamento e baixa coesão.
A análise de impacto, que inclui analisar as dependências entre estruturas, pode ser utilizada para definir o escopo e a complexidade de determinada modificação, bem como os riscos associados.
Modelos atuais para análise de impacto, que incluem análise de rastreabilidade e de dependência, geralmente utilizam apenas a estrutura sintática do código para calcular o particionamento do programa e gerar os grafos de chamadas, conforme apresentado na Seção 2.1.
Esses modelos apresentam algumas limitações exploradas nas Seções 2.2.5 e 2.3.5 que poderiam ser melhor resolvidas utilizando um modelo semântico para representação do conhecimento.
Em este contexto, a motivação desta tese é a possibilidade de recuperar estruturas de código com uma melhor precisão (precision) e revocação (recall), medidas estas exploradas na Seção 2.3.4, se comparada a abordagens existentes de análise de impacto.
Esta motivação se apoia na premissa de que o contexto semântico da aplicação, e não apenas a análise sintática do código, pode gerar uma análise mais apurada dos impactos resultantes de uma solicitação de mudança.
Para tanto, se sugere o uso de ontologias como formalismo primário para compartilhar e relacionar conhecimento.
Esta camada de semântica viabilizaria uma análise mais flexível e adaptativa, conforme apresentado no Capítulo 3.
Questão de Pesquisa Conforme Creswell, questões de pesquisa estão mais associadas ao paradigma qualitativo de pesquisa, enquanto o paradigma quantitativo faz uso de hipóteses e objetivos.
A presente pesquisa possui tanto métodos qualitativos em sua fase exploratória, quanto métodos quantitativos na fase confirmatória, conforme descrito na Seção 1.4.
Para apoiar a fase exploratória e nortear a proposta objeto deste trabalho, foi definida a seguinte questão de pesquisa:
&quot;Como melhorar, em relação a as abordagens existentes para análise de impacto, a recuperação de estruturas de código fonte relevantes a partir de uma solicitação de mudança?»
Esta questão de pesquisa engloba o principal aspecto explorado nesta pesquisa, que inclui identificar os impactos no código fonte resultantes de determinada solicitação de mudança.
Para esta análise, é proposto um modelo de rastreabilidade utilizando ontologias e um modelo de probabilidade que utiliza técnicas de recuperação de informação para identificação de estruturas relevantes, avaliando a relação entre os termos do domínio e estruturas da aplicação.
Objetivos Uma vez definida a questão de pesquisa, definiu- se o objetivo geral e os objetivos específicos deste trabalho.
O objetivo geral deste trabalho é desenvolver um modelo para identificação de estruturas do código fonte impactada por solicitações de mudanças, utilizando uma perspectiva semântica.
Para a consecução do objetivo geral proposto, identificam- se os seguintes objetivos específicos:
Metodologia de Pesquisa A metodologia compreende a classificação e definição da pesquisa, bem como a forma de abordar determinado problema, que inclui objetivos, procedimentos técnicos e coleta de dados.
A o se conduzir uma pesquisa, o primeiro critério a se considerar é sua classificação.
Conforme, pesquisas podem ser classificadas como confirmatórias, que testam relacionamentos pré-especificados, e exploratórias, que definem possíveis relacionamentos em sua forma mais genérica.
O método de pesquisa exploratório tem como objetivo:
A definição de questões e hipóteses de um estudo subsequente ou a determinação da viabilidade de procedimentos de pesquisa desejados.
Proporcionar maior familiaridade com o problema, com vistas a tornar- lo mais explícito ou a constituir hipóteses.
Pode- se dizer que estas pesquisas têm como objetivo principal o aprimoramento de ideias ou a descoberta de intuições.
Seu planejamento é, portanto, bastante flexível, de modo que possibilite a consideração dos mais variados aspectos relativos ao fato estudado.
De acordo com o descrito, este trabalho é caracterizado em sua maioria como exploratório, pois se pretende desenvolver um modelo para aprimorar técnicas convencionais de análise de impacto utilizando ontologias.
A análise de impacto é observada sob uma perspectiva diferente da usual, e a questão de pesquisa que guia este trabalho representa uma questão aberta, reforçando o caráter exploratório.
Esta tese tem como base a existência de abordagens que evidenciam os benefícios da rastreabilidade apoiada por ontologias, gerando indícios de que a análise de impacto também poderá se beneficiar deste apoio semântico.
O escopo dessa pesquisa inclui um aspecto descritivo, observando a prática e as limitações relacionadas à análise de impacto e, então, propondo melhorias.
Em este contexto, as melhorias são aplicadas com o objetivo de aprimorar as técnicas existentes de análise de impacto.
Oates fornece um sumário de diferentes tipos de produtos derivados de uma pesquisa:
Evidência, metodologia, conceitos e teorias, análise e produto.
De entre as classificações definidas por o autor, a estratégia de pesquisa utilizada nesta pesquisa é a chamada &quot;projeto e criação», que possui como foco o desenvolvimento de novos produtos de tecnologia da informação.
Foi escolhido o processo de desenvolvimento prototipal para verificar a viabilidade do modelo de análise de impacto proposto.
Apesar de a ênfase exploratória, este trabalho também inclui uma fase confirmatória, pois, conforme, &quot;estudos confirmatórios são utilizados para auditar a confiabilidade, validade do conteúdo e do construto».
Em este caso, a avaliação é realizada utilizando experimentos para validar empiricamente o modelo de análise de impacto proposto.
Estes experimentos isolam algumas variáveis independentes, no caso diferentes técnicas de análise de impacto, para avaliar quantitativamente as possíveis combinações de resultados através das variáveis dependentes de precisão, revocação e esforço.
Esta avaliação do modelo se caracteriza como empírica, pois evidência o impacto nas variáveis dependentes a partir de a manipulação das variáveis independentes.
Para desenvolver o modelo para análise de impacto proposto, planejou- se uma metodologia de pesquisa organizada em duas grandes fases:
Exploratória e confirmatória.
A Figura 1.1 apresenta o desenho de pesquisa que ilustra as fases adotadas por a pesquisa.
A primeira fase caracteriza- se como exploratória e consiste em duas etapas:
Identificação do Problema de Pesquisa e Desenvolvimento do Modelo de Análise de Impacto. A segunda fase caracteriza- se como confirmatória e é composta por uma etapa que inclui o desenvolvimento do protótipo e experimentos.
A Etapa Exploratória 1 ­ Problema de Pesquisa foi eminentemente exploratória em a qual foi identificado um problema real no contexto de gerenciamento de mudança de software:
A análise de impacto.
Foi formulado um tópico genérico de pesquisa dentro de o problema identificado e executada uma pesquisa em profundidade sobre a análise de impacto, bem como sua relação com ontologias e recuperação de informação.
Este estudo teve como objetivo aprofundar o entendimento nos modelos de análise de impacto existentes bem como das propostas que fazem uso de ontologias e recuperação de informação para apoiar a análise de impacto.
Após este estudo, foi realizada uma revisão sistemática para identificar a intersecção entre essas duas áreas.
O conceito de revisão sistemática foi introduzido na área de Engenharia de Software por e se mostrou bastante eficiente, justificando o seu uso neste trabalho.
Com isso, foi possível ampliar a cobertura dos estudos em profundidade e identificar lacunas de pesquisa.
O resultado desta etapa foi descrito no Capítulo 2 desse documento.
A Etapa Exploratória 2 ­ Modelo de Análise de Impacto representou o desenvolvimento de um modelo para apoiar a análise de impacto utilizando, para tanto, dois modelos:
De rastreabilidade e de probabilidade.
Para o modelo de rastreabilidade, as estruturas do código fonte foram relacionadas aos conceitos da ontologia de domínio.
Foi definido um modelo de representação de elos de rastreabilidade, flexível o suficiente para suportar quaisquer entidades representadas por artefatos de software.
Uma vez definido esse modelo, percebeu- se a necessidade de automação do mapeamento entre código e conceitos, diminuindo os esforços requeridos por a equipe de desenvolvimento nesta tarefa.
Foi desenvolvida uma arquitetura utilizando PLN para aprendizagem de ontologia e geração de instâncias de indivíduos a partir de o código fonte da aplicação, que representam os elos de rastreabilidade.
O modelo de probabilidade pondera as estruturas do código fonte que potencialmente serão impactadas em virtude de determinada mudança.
Este modelo considera a dependência entre a organização lógica e de baixo nível da aplicação (código fonte) e as dependências entre os conceitos do domínio.
Esse cálculo de relevância foi desenvolvido utilizando um modelo de recuperação de informação de Redes de Crenças Bayesianas que calcula a probabilidade de impacto para cada nodo do modelo de rastreabilidade.
Este cálculo inclui a frequência que cada conceito ocorre no código fonte, bem como a dependência conceitual existente entre a ontologia e o código fonte.
Essa dependência utiliza o grafo de chamadas em duas vias, isto é, analisa a dependência tanto da ontologia de domínio quanto do código fonte, em oposição às propostas atuais que consideram apenas a dependência do código fonte.
A o final, apresentam- se as classes que potencialmente serão impactadas por a solicitação de mudança;
A Etapa Confirmatória 1 ­ Avaliação teve como objetivo desenvolver um protótipo funcional para avaliar a viabilidade do modelo proposto bem como sua aplicação utilizando experimentos.
O processo de experimentação requer um preparo para conduzir e analisar corretamente o objeto de estudo.
Este esforço representa seu maior benefício:
Organização da Tese Esta tese está organizada da seguinte forma:
O Capítulo 2 apresenta a fundamentação teórica, que inclui uma visão geral sobre análise de impacto, bem como seu relacionamento com ontologias e recuperação de informação.
Este capítulo também apresenta uma revisão sistemática que apresenta o estado da arte relacionado aos objetivos deste trabalho.
O Capítulo 3 apresenta o modelo de análise de impacto e seus componentes.
Este modelo é exemplificado através de um exemplo prático, ilustrando cada um dos seus aspectos.
O Capítulo 4 apresenta a implementação do protótipo desenvolvido para analisar a viabilidade da tese, incluindo sua arquitetura e guia de uso.
Este modelo é avaliado empiricamente através de três experimentos controlados conforme apresentado no Capítulo 5.
As conclusões, contribuições e limitações da tese são exploradas, por fim, no Capítulo 6.
Atividades relacionadas à análise de impacto são comuns durante o ciclo de vida de software.
Porém a falta de uma definição comum para o termo e o caráter empírico muitas vezes empregado nessa avaliação evidenciam algumas limitações da área Para o desenvolvimento de uma proposta que analise os impactos de mudanças numa aplicação, primeiro é necessário entender os conceitos relacionados.
A Seção desenvolvimento de software, bem como alguns modelos de uso.
Também são apresentadas as etapas necessárias para essa análise e o contexto associado à evolução de software e gerenciamento de mudanças.
Mudanças em software são geralmente resultado de novas orientações sobre cenários de negócio e conceitos associados.
Para modelagem conceitual, diversos pesquisadores vêm sugerindo o uso de ontologias.
Com base nessa premissa, a Seção sua integração com o desenvolvimento de software, além de uma descrição sobre linguagens para manipulação de ontologias e seu relacionamento com análise de impacto.
A Seção 2.3 apresenta conceitos sobre Recuperação de Informação (Ri).
A Ri é uma estratégia eficiente para busca de informação não estruturada, como o código fonte da aplicação.
Para tanto, serão apresentados alguns modelos e algoritmos que podem ser aplicados à identificação de estruturas de código associadas a requisições de mudanças.
Em esta seção também é apresentado o processamento de documentos, algumas métricas para avaliar a eficiência de modelos de Ri e trabalhos relacionados à análise de impacto e recuperação de informação.
A Seção 2.4 apresenta uma revisão sistemática visando identificar a intersecção entre as duas áreas foco da fundamentação teórica:
Ontologia e recuperação de informação no contexto de análise de impacto, visando identificar propostas semelhantes ao objeto desta tese.
Por último, na Seção 2.5, além de as considerações sobre o capítulo, é discutido o escopo e abrangência da tese.
Análise de Impacto Esta seção discute análise de impacto no contexto de desenvolvimento de software, incluindo sua definição, conceitos e evolução.
Também são discutidas as estratégias manuais e automatizadas para analisar impactos, com ênfase nas análise de rastreabilidade e dependência.
O gerenciamento de mudanças é uma atividade fundamental para a manutenção e evolução de software.
Um fator chave desse gerenciamento é determinar quais serão as implicações decorrentes de determinado requisito de mudança.
Essas implicações incluem identificar as partes do sistema que serão impactadas e quanto de código é necessário ser modificado.
Um impacto de uma mudança representa uma parte do software que é determinantemente afetada e que merece investigação.
Essa inspeção tem como objetivo manter a estabilidade do software, o que representa a sua resistência a um potencial efeito colateral que um programa poderia ter quando modificado.
Esse efeito colateral, por sua vez, é considerado como um erro ou outro comportamento inesperado resultante de uma modificação que geralmente é causado por fazer uma pequena alteração num sistema que afeta outras partes.
Embora a análise de impacto seja praticada há muitos anos, ainda não existe um consenso em sua definição.
O Glossário de Engenharia de Software da IEEE, por exemplo, não apresenta a definição do termo &quot;análise de impacto».
Apesar de não haver um consenso formal, alguns autores apresentam suas definições.
Definiu como a ação de examinar um impacto para determinar seus efeitos, isto é, representa a avaliação do efeito ou do resultado de fazer uma alteração num sistema ou software.
Pfleeger definiu análise de impacto como a avaliação de muitos riscos associados a uma mudança, incluindo estimativas dos efeitos nos recursos, esforço e cronograma.
O autor estende aqui a definição a um contexto mais amplo de desenvolvimento de software e não apenas a alterações em código fonte.
Evoluindo nesta definição, Arnold e Bohner descrevem:
A análise de impacto corresponde à identificação de potenciais consequências de uma mudança ou a estimativa do que precisa ser modificado com o objetivo de realizar uma mudança, incluindo estimativas de custos e cronogramas.
Por a definição dos autores, identifica- se que a análise de impacto é utilizada para estimar o que será afetado no software e nas documentações associadas em virtude de uma requisição de mudança.
As informações desta análise podem ser utilizadas para alterar o plano de desenvolvimento, realizar mudanças em software bem como rastrear efeitos dessas mudanças.
A análise de impacto fornece a visibilidade desses potenciais efeitos antes que mudanças sejam implementadas, podendo assim tornar mais fácil e precisa sua implementação.
Diante de o exposto e por a sua relevância, utilizaremos a definição de Arnold e Bohner como referência para este trabalho.
A análise de impacto é frequentemente utilizada para auditar os efeitos de uma mudança num sistema após a sua implementação, porém abordagens mais proativas utilizam a análise de impacto para prever os efeitos de uma mudança antes que a mesma seja executada.
Em este caso, a análise de impacto precede ou é utilizada em conjunto com uma determinada mudança, fornecendo as entradas necessárias para sua implementação.
Assim, a análise de impacto fornece o entendimento do escopo do que deveria ser alterado num software devido a uma solicitação de mudança.
Por exemplo, o time de desenvolvimento pode considerar diferentes abordagens para implementar uma mudança e escolher aquela que tiver menor impacto com relação a custos ou prazos.
Com isso, torna- se possível planejar a mudança ao invés de lidar com suas consequências.
A falta de uma definição comum do termo análise de impacto pode levar ao surgimento de alguns problemas, tais como os citados por Arnold e Bohner:
Apesar de as limitações a respeito de uma definição comum, a prática propriamente dita vem sendo largamente empregada.
De entre as principais razões, Ajila descreve:
Em 1967, durante conferência de NATO1, começou- se a se discutir a importância do software em diversos aspectos da sociedade, sugerindo que o desenvolvimento seja menos ad-hoc e fundamentado em teorias, padrões e disciplinas que visem obter software confiável, eficiente e economicamente viável.
Entre as várias atividades relacionadas com a Engenharia de Software, a manutenção foi discutida como sendo a atividade executada após sua entrega e implantação.
Esta atividade foi consolidada por Royce que propôs em 1970 o modelo em cascata para o desenvolvimento de software, formalizando a atividade relacionada com a manutenção e evolução do software.
A prática de análise de impacto também começou a ser discutida durante o início da história da engenharia de software.
Em 1972, Haney discute aspectos relacionados ao impacto de mudanças em software e propõe um modelo para propagação dos efeitos num sistema chamado Análise de Conexão de Módulos (Module Connection Analysis).
Este modelo pressupõe que para qualquer par de módulos (que pode ser entendido como componente, subsistema, etc.), existe uma probabilidade finita de que a mudança num primeiro módulo requer uma mudança num segundo módulo, definindo assim uma matriz de conexão probabilística.
Pouco mais tarde, Yau propõe em 1978 uma técnica para analisar os efeitos colaterais de uma mudança e estimar a complexidade da modificação.
A técnica proposta considera a perspectiva funcional, em a qual avalia o efeito colateral sob a perspectiva lógica do programa utilizando a análise léxica do código e o grafo de invocação do programa, que representa um grafo de controle de fluxo baseado em blocos.
Com base nesta análise léxica, Yau propõe um algoritmo que computa o efeito colateral através da propagação de um critério de mudança, representado por um conjunto de variáveis, sobre o grafo de invocação, sugerindo ao final quais são os módulos do programa impactados por este critério.
A técnica apresentada pode ser utilizada para determinar como uma mudança numa área do código fonte se propaga e causa mudanças em outras áreas.
Um ano mais tarde, Weiser apresenta uma técnica chamada particionamento de programa, cujo objetivo é reduzir um programa em sua forma mínima produzindo o mesmo resultado.
Esta técnica visa recuperar as partições executáveis que contém apenas o código em a qual uma variável em particular depende.
O particionamento, que será melhor explicado na Seção 2.1.5, pode ser utilizado para determinar as dependências no código, minimizando assim os efeitos colaterais.
O conceito de rastreabilidade de software também não é algo recente e geralmente é discutido como o elo entre os requisitos do sistema e seus conceitos definidos por a disciplina de Gerenciamento de Configuração.
A rastreabilidade e a análise de código representam a base dos modelos de análise de impacto.
Em o ano 2000, Bohner discute o problema das datas Y2K, que tornou evidente o esforço necessário para identificar as porções de código que necessitariam adaptações devido a a virada do milênio.
Esta necessidade serviu para alertar as organizações na época sobre a relevância de métodos de análise de impacto explícitos.
A análise de impacto, ainda hoje, possui como base as técnicas e estratégias fundamentadas há muito tempo e focadas na análise de código fonte, incluindo particionamento e análise de efeitos colaterais.
Atualmente é possível identificar a evolução dessas técnicas devido a o amadurecimento da Engenharia de Software e sua colaboração com demais disciplinas da Ciência da Computação.
Técnicas como Recuperação de Informação, Processamento de Linguagem Natural, Representação do Conhecimento, Redes Semânticas, entre outras, vêm sendo empregadas em conjunto com técnicas tradicionais para avaliar o impacto resultante de determinada mudança conforme apresentado nas Seções 2.2.4 e 2.3.5.
Esta tendência promove a abstração das estruturas internas do código, tais como variáveis e controle de dependência, para o nível conceitual, conforme será discutido na revisão sistemática apresentada na Seção 2.4.
Existem diversas estratégias para analisar o impacto de determinada mudança no software, com diferentes níveis de automação.
Arnold e Bohner apresentam algumas dessas estratégias:
Observa- se que estas estratégias podem ser manuais e automatizadas por algoritmos.
Pode- se argumentar que algumas estratégias pertencem as duas categorias, porém neste trabalho as quatro primeiras serão consideradas manuais enquanto as duas últimas automatizadas.
As estratégias manuais geralmente são desempenhadas por humanos e não necessitam de uma infraestrutura específica, enquanto as automatizadas necessitam dessa infraestrutura para rastrear objetos e estimar o impacto de uma mudança.
As estratégias manuais são discutidas por Jönsson que argumenta que as mesmas não são fortemente dependentes de especificações estruturadas.
Consequentemente, existe o risco de serem menos precisas na predição de impactos.
Por outro lado, são mais fáceis de serem introduzidas num processo de gerenciamento de mudanças e são geralmente empregadas na indústria sem grande preocupação quanto a sua precisão.
Estas estratégias são geralmente empregadas utilizando documentação de projeto e entrevistas para identificar as entidades primariamente impactadas.
As documentações de projeto podem considerar diferentes artefatos, como diagramas UML, descrições textuais de software e componentes, etc., e o seu sucesso depende de diferentes fatores, como o conhecimento e habilidade do responsável por a análise e a disponibilidade e confiabilidade da documentação.
Lindvall em apresenta que entrevistas com desenvolvedores que possuem conhecimento sobre o sistema representam provavelmente a forma mais comum de se estimar impacto e são mais efetivas do que a pesquisa em documentos e outras fontes.
A segunda forma mais comum de análise de impacto é a análise do código fonte.
Em o estudo apresentado, todos os participantes do experimento que realizaram a análise de impacto entrevistaram desenvolvedores e consultaram o código fonte, porém apenas metade consultaram documentações como casos de uso e modelos de objetos.
Quando questionados sobre os métodos de análise, os participantes argumentaram que a documentação não abrangia todos os detalhes necessários ou não estava atualizada, enquanto &quot;o código fonte, por sua vez, está sempre atualizado».
Estratégias de análise de impacto automatizadas geralmente empregam algoritmos para identificar a propagação de mudanças e seus impactos indiretos.
Para tanto, possuem como pré-requisito grafos de relacionamento, que constituem uma informação semântica que evidência as associações entre entidades computacionais.
Estratégias como análise de rastreabilidade, dependência e particionamento são geralmente utilizadas para identificar os impactos de mudanças e serão apresentadas em detalhes nas próximas subseções.
A primeira estratégia para a análise de impacto que pode ser automatizada é a identificação do relacionamento entre entidades de software.
Essa etapa permite que as relações sejam estabelecidas, por exemplo, entre o código fonte e requisitos de software.
A segunda estratégia automatizada se preocupa em analisar o relacionamento entre módulos de um programa, baseados em objetos, fluxo de dados, variáveis, invocação, etc..
A diferença entre a análise de rastreabilidade e análise de impacto de uma mudança é bastante tênue, mas significativa.
A rastreabilidade preocupa- se em definir a relação existente entre os artefatos do software.
A análise de impacto, por sua vez, determina quais ações devem ser tomadas para realizar a alteração, podendo estimar quais serão as consequências desta.
Apesar de a diferença, ambas as abordagens estão intimamente relacionadas, uma vez que a metodologia utilizada para analisar o impacto é baseada na forma como a rastreabilidade foi construída.
Em este sentido, a análise de impacto pode ser dividida em duas etapas:
Análise de rastreabilidade e análise de dependência.
Fundamentalmente a diferença entre estas duas estratégias é o nível de detalhe:
Análise de rastreabilidade se preocupa em analisar as associações entre todos os tipos de entidades de software, tais como código fonte, requisitos, casos de teste, etc., enquanto a análise de dependência analisa em baixo nível as dependências extraídas do código fonte Rastreabilidade é a capacidade de inter-relacionar e recuperar entidades que se identificam durante um processo.
Segundo o Glossário de Engenharia de Software da IEEE, rastreabilidade é:
O grau em o qual relacionamentos podem ser estabelecidos entre dois ou mais produtos de um desenvolvimento de software, especialmente quando existem relações de predecessor e sucessor ou generalização e especialização entre esses produtos;
Por exemplo, o grau em que os requisitos e o projeto de um determinado componente de software se associam.
O grau em que cada elemento num produto do desenvolvimento de software estabelece para sua razão de existir;
Por exemplo, o grau em que cada elemento num gráfico de bolhas referência os requisitos que o satisfaz.
Análise de rastreabilidade se refere à habilidade de definir e recuperar associações entre entidades, incluindo requisitos ou outros tipos de artefatos de software.
A rastreabilidade de informações é representada por elos que relacionam requisitos a outros requisitos, artefatos que satisfazem esses requisitos (elos de satisfação) ou as suas fontes.
Um ponto fundamental para rastreabilidade consiste em perceber um sistema de informação como um conjunto de documentos e artefatos que descrevem o software em diferentes níveis de abstração e não apenas como uma implementação.
Em este contexto, Pfleeger define rastreabilidade como a habilidade de relacionar os itens dependentes dentro de um modelo e seus correspondentes em diferentes modelos.
Adicionalmente, esta técnica provê um apoio essencial para o entendimento da organização estrutural entre os requisitos e seus artefatos derivados.
O sucesso da análise de rastreabilidade depende fortemente da consistência e completeza dos relacionamentos identificados.
A rastreabilidade possui diversas classificações bem aceitas na literatura para avaliar a qualidade e o alcance dos relacionamentos.
A primeira dimensão de rastreabilidade diz respeito à interconexão dos itens relativos aos modelos.
Em esta perspectiva, Pfleeger apresenta os seguintes tipos:
Ambos os tipos de rastreabilidade são necessários para compreender um conjunto completo de relacionamentos pertinentes à análise de impacto.
Para o mapeamento, tanto vertical quanto horizontal, Pfleeger sugere o uso de grafos, onde um conjunto de objetos corresponde aos nodos e os relacionamentos correspondem a arestas.
É sugerida uma modularização entre requisitos, projeto, código e teste que estruturam uma organização vertical de seus componentes, e estes componentes se relacionam horizontalmente com os demais módulos.
A Figura 2.1 apresenta a proposta de organização horizontal entre componentes de software.
Segundo Gotel e Finkelstein, um termo mais comum para rastreabilidade é apresentado como Rastreabilidade de Requisitos que corresponde à habilidade de relacionar requisitos específicos com suas implementações.
Em esta perspectiva, é apresentada uma divisão de rastreabilidade em duas dimensões principais:
Lindvall, em, apresenta duas classificações para rastreabilidade levando em conta o tipo de relacionamento entre os elementos:
Uma abordagem explorada por Bianchi et al.
Diz respeito a relações obtidas através de sintaxe e semântica dos modelos que consistem em:
Uma abordagem comum para manter elos de rastreabilidade é o uso da matriz de rastreabilidade.
Uma matriz de rastreabilidade é uma matriz em que cada linha e cada coluna correspondem a uma entidade de software como, por exemplo, um requisito.
A relação entre as duas entidades é expressa colocando uma marca entre a intersecção da linha e coluna.
Por exemplo, o relacionamento entre um requisito, organizado numa linha da matriz, e uma classe do software, representado por uma coluna, pode ser representado por um símbolo de intersecção na matriz entre a linha e coluna associada.
Este símbolo representa que o requisito é implementado parcialmente ou totalmente por aquela a classe.
Lindvall define que rastreabilidade é uma propriedade de um sistema e um fator de qualidade que corresponde ao entendimento do software e ao acompanhamento da documentação de seus modelos.
Apesar disso, Ramesh e Jarke apresentam um estudo que evidência que as práticas de rastreabilidade não são em geral completas, pois não endereçam a informação semântica (rationale) destes elos de rastreabilidade.
Um relacionamento entre um requisito e uma classe, por exemplo, pode resultar em interpretações diferentes para interessados diferentes.
Ramesh e Jarke argumentam que, por exemplo, o relacionamento entre um requisito e uma classe pode ser interpretado como que a &quot;classe implementa o requisito «ou que a &quot;classe representa uma restrição para o requisito».
Um elo de rastreabilidade simples representa apenas uma associação entre entidades, isto é, representa que uma entidade afeta outra, sem incluir a semântica que motivou essa associação.
O fator de qualidade definido por Lindvall necessita de um suporte semântico para capturar o entendimento e razão que motivaram o elo de rastreabilidade.
De posse de um sistema rastreável, essa análise pode apoiar o desenvolvimento e manutenção do software, permitindo assim uma compreensão do sistema, apoiando a análise de impacto de mudanças.
Em este caso, uma forma tradicional de estimar o impacto de mudanças é utilizar informações estáticas sobre o código, isto é, dependências, para identificar partes do código potencialmente afetadas.
Segundo Law e Rothermel, as três principais técnicas de análise de impacto baseada em dependências são grafos de chamadas, particionamento estático e particionamento dinâmico.
Um grafo de chamadas é um grafo direto em o qual os nodos representam funções (ou métodos) e uma aresta transitiva entre A e B significa que A pode chamar B.
Através da definição de dependência direta entre os métodos, é possível avaliar o impacto por a manutenção de uma função específica.
O particionamento de um programa consiste em reduzir esse programa para a sua forma mínima, mantendo ainda seu comportamento inicial.
Por exemplo, inserindo mudanças no código e aplicando particionamento, que se baseia na construção de um grafo de dependência entre diversas entidades computacionais para determinar a dependência transitiva de determinada entidade, permite- se identificar um conjunto de pontos de impacto decorrentes de determinada mudança.
A ideia por traz do particionamento é justamente identificar apenas o código de interesse para determinado comportamento para, então, modificar apenas essa porção de código.
Por exemplo, considere a Figura 2.2.
A Figura 2.2 (a) apresenta um exemplo de código que lê um número n e computa a soma e o produto dos primeiros n números positivos.
A Figura 2.2 (b) representa apenas uma porção do código relacionada ao cálculo do produto, ignorando todos os demais trechos do código que não dizem respeito a essa operação.
O particionamento estático realiza a identificação do conjunto de todas as declarações transitivas relativas a determinado fluxo de controle ou de dados, considerando apenas as informações estáticas presentes no código.
O particionamento estático reduz o número de análises se comparado ao grafo de chamadas, pois considera apenas os valores potencialmente afetados em algum ponto de interesse, também chamado de critério de particionamento.
Critérios de particionamento geralmente são definidos numa parte de um programa contendo a ocorrência, por exemplo, a linha 10, write (product), da Figura 2.2, e uma variável, por exemplo, a variável product da Figura 2.2.
Em este exemplo, o critério de particionamento é e o código resultante é representado por a Figura 2.2 (b).
Em este exemplo, qualquer referência a variável sum é considerada fora de o escopo da partição analisada.
De entre os particionamentos, o estático é computado sem realizar suposições sobre as entradas de um programa enquanto o dinâmico utiliza casos de testes específicos em sua definição para determinar suas possíveis entradas.
Em este caso, apenas as dependências que ocorrem numa execução específica (caso de teste) devem ser levadas em consideração.
Tipicamente, o critério de particionamento dinâmico é composto por três elementos:
Entrada do programa, ocorrência e variável.
A ocorrência deve levar em consideração dados históricos, isto é, o número de vezes que esta declaração ocorreu no código.
Por exemplo, a Figura 2.3 apresenta um exemplo de programa e o critério de particionamento.
Esse critério é definido por, sendo que n $= 2 representa a entrada, 81 representa a primeira ocorrência da declaração e x a variável de interesse.
Note que como n $= 2, o loop será executado duas vezes passando uma única vez por as primeira iteração do loop é ignorada devido a a atribuição do valor 17 da segunda iteração.
Em o exemplo da Figura 2.3, o particionamento estático, cujo critério é, irá considerar todo o algoritmo, isto é, a Figura 2.3 (a), enquanto o particionamento dinâmico irá considerar apenas a partição representada por a Figura 2.3 (b).
Para o particionamento entre procedimentos, existem algumas técnicas comuns, como o Grafo de Fluxo de Controle (Figura 2.4), que representa a ordem de invocação das instruções, o Grafo de Dependência (Figura 2.5), que representa a subordinação das estruturas, e Grafo de Fluxo de Dados (Figura 2.6), que representa o uso de cada variável ao longo de o programa.
Segundo Law e Rothermel, cada uma dessas abordagens para análise de dependência tem suas vantagens e desvantagens:
O particionamento num grafo de chamadas pode gerar estimativas inseguras sobre o conjunto de operações afetadas por as mudanças, gerando resultados imprecisos e superestimando os efeitos de mudanças.
Recentemente, pesquisadores vêm considerando formas mais precisas de auditar os impactos utilizando informações capturadas durante a execução dessas mudanças.
Por exemplo, Law e Rothermel definiram uma técnica para análise de impacto que utiliza uma técnica chamada caminho completo de programas.
Esta técnica captura a dinâmica de um programa através de seus fluxos de controle e, em oposição às técnicas comuns de perfis de caminho que registram caminhos intraprocedurais e acíclicos, esta proposta captura os fluxos completos de programas incluindo ciclos e interações interprocedurais.
Em esta abordagem, se um procedimento p é alterado, todos os procedimentos que são invocados após p, bem como qualquer procedimento que está na pilha de chamada aguardando o retorno deste procedimento, são incluídos no conjunto de procedimentos potencialmente impactados.
Embora técnicas baseadas no rastreamento de execução possam atingir melhores resultados do que técnicas de análise de impacto tradicionais, tais como grafo de chamadas e particionamento de programas estático e dinâmico, essas técnicas são restringidas por a qualidade dos dados que são utilizados.
Como restrições, tem- se que a quantidade de rastros de execução utilizados pode crescer exponencialmente, e algoritmos que comprimem estes rastros a um nível computacionalmente razoável podem se tornar custosos, comprometendo o seu uso em tempo de execução na medida em que os rastros são produzidos.
Conforme apresentado, a análise de impacto corresponde à identificação de potenciais consequências de determinada mudança ou a estimativa do trabalho necessário para completar essa mudança.
Em o que se referem a impacto, duas perspectivas são pertinentes:
Perspectiva de negócio, relacionada às preocupações das partes interessadas em verificar se as regras de negócio existentes são impactadas por uma mudança, necessitando adequação;
Perspectiva do software, relacionada com as alterações no código fonte e documentação associada.
Oliveira, em, apresenta um estudo sobre diferentes técnicas de rastreabilidade e evidência que nenhuma das avaliadas lida diretamente com a perspectiva de negócio com razoável nível de precisão para os impactos descobertos.
Para ambas as perspectivas, dois tipos de medidas são consideradas importantes para a avaliação dos impactos:
Precisão, para medir a exatidão ou fidelidade, e revocação, como uma medida de completude.
A análise de impacto é uma atividade fundamental para a evolução de software.
Essa evolução corresponde à etapa posterior a primeira implantação do sistema em produção, em a qual novas necessidades são identificadas.
A evolução do software é inevitável, pois, conforme definido por a primeira lei de Lehman, chamada Mudança Contínua, um programa usado num ambiente real deve necessariamente mudar ou irá se tornar progressivamente menos útil.
Isso se deve em contextos em os quais as funcionalidades oferecidas por os sistemas requerem evolução contínua para manter a satisfação do usuário.
Como evidência da importância da evolução do software, estudos como apresentados por Erlikh sugerem que 90% dos custos do software estão em sua evolução.
De forma geral, para sistemas encomendados e específicos de domínio, os custos de manutenção geralmente excedem os custos de desenvolvimento numa proporção de 80% para 20%.
A evolução do software é direcionada por propostas de mudanças.
Essas propostas podem envolver requisitos não implementados, solicitação de novos requisitos ou reparos.
Este processo inclui a análise de mudanças, planejamento de entregas, implementação e liberação do software, considerando também o custo e impacto dessas mudanças.
Para ilustrar o processo de evolução de software, a Figura 2.4 apresenta o processo de evolução de sistemas proposto por Sommerville.
A Figura 2.7 ilustra que após a solicitação de mudanças, é necessário realizar uma análise do impacto dessa solicitação para então realizar um planejamento, implementação e liberação do sistema.
Esse processo varia consideravelmente entre organizações, desde um processo informal decorrente de conversas entre usuários e desenvolvedores, até um processo formalizado com documentação estruturada e produzida em estágios definidos, chamado de gerenciamento de mudanças.
O gerenciamento de mudanças no contexto do desenvolvimento de software representa o processo de solicitar, analisar a viabilidade, planejar, implementar e avaliar as mudanças num sistema.
A finalidade de ter um processo padrão de controle de mudanças é assegurar que as mudanças feitas num projeto sejam consistentes e que as partes interessadas sejam informadas do estado do produto, das mudanças feitas em ele e do impacto de custo e cronograma gerados por essas mudanças.
Conforme Sommerville, procedimentos de gerenciamento de mudanças dizem respeito à análise de custo e ao benefício de mudanças propostas, à aprovação de mudanças viáveis e à rastreabilidade de quais componentes de software foram alterados Conforme o Kruchten, a primeira etapa desse processo consiste em preencher um formulário formal de solicitação de mudanças chamado Formulário de Requisição de Mudança (RdM) que descreve as informações sobre a mudança, tais como finalidade, breve resumo, ocorrência, responsabilidade e adaptação.&amp;&amp;&amp;
Em este formulário deve ser inicialmente descrita a mudança numa perspectiva de negócios para que, uma vez validada e aprovada, seja encaminhada para a equipe técnica avaliar o impacto dessa solicitação no restante do sistema, identificando os componentes afetados por esta mudança.
Para essa análise, são utilizadas informações do banco de dados de configurações e do código fonte do software.
Após a análise de impacto, essa mudança é avaliada e o custo para realização é estimado.
Após esse levantamento, o RdM é encaminhado para um comitê de controle de mudanças, que corresponde a um grupo formalmente constituído de partes interessadas responsáveis por a revisão, avaliação, aprovação, atraso ou rejeição de mudanças feitas num projeto, com registro de todas as decisões e recomendações.
Esse comitê considera o impacto mais sobre o ponto de vista técnico do que estratégico e organizacional.
A evolução do software é consequência de mudanças das necessidades de negócio que, muitas vezes, refletem uma evolução do conhecimento sobre o domínio da aplicação.
O conhecimento a cerca de o domínio é geralmente capturado através de produtos de trabalho, tais como especificações funcionais, diagramas e código fonte.
Todos esses artefatos representam uma perspectiva de conhecimento sobre o sistema num determinado período.
Um fator crítico de sucesso durante as etapas de desenvolvimento e manutenção de software é a comunicação e colaborações do time para definir uma visão comum sobre o que é e como deve se comportar o sistema.
Este conhecimento compartilhado desempenha um papel fundamental para nivelar o entendimento entre as partes interessadas sobre quais são os requisitos de negócios e as entidades computacionais derivadas desses requisitos.
Ontologias e Análise de Impacto O termo &quot;ontologia «surgiu na filosofia através de Aristóteles e significa &quot;uma explicação sistemática da existência», isto é, a definição de um domínio do conhecimento num nível genérico, utilizada para especificar o que existe ou o que se pode dizer sobre o mundo.
Em Ciência da Computação, ontologias representam aquisição do conhecimento a partir de dados semiestruturados utilizando um conjunto de métodos, técnicas ou processos automáticos ou semiautomáticos.
Dentro de Ciência da Computação, o termo &quot;ontologia «teve sua origem na comunidade de Inteligência Artificial.
Segundo Thomas Gruber, aquilo que existe é aquilo que pode ser representado.
Quando o conhecimento sobre um domínio é especificado numa linguagem declarativa, o conjunto de objetos que pode ser representado é denominado universo de discurso.
Pode- se descrever uma ontologia de um programa por a definição de um conjunto representacional de termos.
As definições associam os nomes de entidades do universo de discurso (como classes, relacionamentos, funções ou outros objetos) com um texto que descreve o que os nomes significam e com os axiomas formais, que tanto restringem a interpretação destes termos quanto a boa formação no seu uso.
Ontologias são utilizadas para definir os termos usados para descrever e representar um domínio de informação.
Entende- se por domínio de informação uma específica área de conhecimento.
Para tanto, ontologias incluem definições de conceitos e seus relacionamentos, tornando assim o conhecimento compartilhado e, de certa forma, reusável.
Gruber, em, define ontologias como uma &quot;especificação explícita de uma conceituação».
Uma conceituação é uma abstração simplificada da realidade que se deseja representar, isto é, um conjunto de objetos, restrições, relacionamentos e entidades que se assumem necessárias em alguma área de aplicação.
A conceituação de Gruber foi modificada por Borst, em, definindo ontologias como uma &quot;especificação formal de uma conceituação compartilhada».
Esta definição enfatiza o fato que deve haver um acordo na conceituação do que é especificado.
Conforme Nino Cocchiarella, ontologias formais representam o desenvolvimento sistemático, formal e axiomático da lógica de todos os modos da existência.
Ela define as propriedades formais, a classificação de entidades no mundo (objetos físicos, eventos, etc.) e as categorias que modelam o mundo (conceitos, propriedades, etc.).
Quanto a a categorização de ontologias, Guarino classifica ontologias com relação a o seu nível de dependência a uma tarefa particular ou ponto de vista.
Guarino distinguiu as seguintes ontologias:
De alto nível, de domínio, de tarefa e de aplicação.
Ontologias de alto nível descrevem conceitos gerais como espaço, tempo, objetos, eventos, ações, etc., que são independentes de um problema particular.
Ontologias de domínio e de tarefa descrevem, respectivamente, o vocabulário genérico de um domínio (como medicina, automóveis) ou uma tarefa ou atividade em particular (como diagnosticar ou vender).
Ontologias da aplicação descrevem os conceitos de um domínio ou tarefa particular, correspondendo aos papeis executados por entidades do domínio enquanto executando certa atividade.
O uso de ontologias pode fornecer benefícios potenciais para várias aplicações, porém é sabido que sua construção é custosa e geralmente referenciada como um gargalo de aquisição de conhecimento.
Em este contexto, diferentes técnicas e métodos para criar e manter ontologias vêm sendo propostas.
A engenharia de ontologia se refere ao conjunto de atividades relacionadas ao processo de desenvolvimento de ontologias, seu ciclo de vida e as metodologias, ferramentas e linguagens para sua construção.
A modelagem de ontologias de domínios não triviais é uma tarefa de fato difícil e que necessita de bastante tempo e esforço.
A compensação entre modelar uma grande quantidade de conhecimento e fornecer o máximo de abstrações possíveis para manter o modelo conciso faz com que a engenharia ontológica se torne uma tarefa desafiadora.
Uma solução interessante é desenvolver uma abordagem para a aprendizagem automática da ontologia a partir de dados existentes.
O termo Aprendizagem de Ontologias (ontology learning) foi inicialmente definido por Maedche e Staab em e descrito como a aquisição de um modelo de domínio a partir de dados.
Para que a aprendizagem de ontologias seja realizada, são necessários dados representativos para o domínio em o qual a ontologia pretenda modelar.
Estes dados podem ser, por exemplo, diagramas UML ou esquemas de banco de dados.
O nome dado a essa aprendizagem de ontologias é lifting como basicamente consiste em mapear definições a partir de o esquema para suas definições ontológicas.
O aprendizado de ontologias também pode ser executado utilizando como fonte dados semiestruturados como documentos XML, Html ou estruturas tabuladas.
Cimiano em apresenta um conjunto de tarefas necessárias para a aprendizagem de ontologias, como a aquisição de terminologia relevante, identificação de termos sinônimos ou variações linguísticas, formação de conceitos e sua organização hierárquica, bem como a de propriedades, instanciação de esquemas e axiomas.
O processo de encontrar instâncias de conceitos e relacionamentos é comumente chamado de população de ontologias, que representa a seleção de fragmentos de texto a partir de documentos para designar- los a conceitos da ontologia.
Isso representa conectar conceitos e relacionamentos aos símbolos que são utilizados para referenciar- los e que, neste caso, implica na aquisição de conhecimento a partir de o conhecimento linguístico sobre os termos que são utilizados para se referir a um conceito específico ou sinônimo potencial destes termos.
Para se definir e manipular ontologias se sugere a utilização de linguagens que suportem estruturas para representação do conhecimento.
Esta representação é realizada através da descrição formal de um conjunto de termos sobre um domínio específico.
A definição de uma linguagem é necessária para a representação e descrição formal da estrutura que especifica uma conceituação.
Um termo que possui determinado significado pode variar sua semântica conforme o contexto empregado.
Para solucionar este problema de ambiguidade e propor uma estrutura formal para a representação do conhecimento, surgiram algumas propostas e OWL.
A linguagem OWL foi recomendada por a W3C em fevereiro de 2004 como linguagem para manipulação de ontologias e seu diferencial é a capacidade de processamento semântico através de inferência.
Uma semântica formal e o seu apoio lógico são geralmente providos através do mapeamento de uma linguagem ontológica num formalismo.
Estes requisitos foram a base para uma divisão da linguagem OWL em três sublinguagens:
OWL Full, OWL Dl e OWL Lite.
A linguagem OWL é construída sobre RDF e RDF Schema e baseada na sintaxe XML.
O modelo básico de dados do RDF, e herdado por OWL, é definido através de:
Para se estruturar um documento OWL, define- se em alto nível:
Classes: Conjunto de instâncias com características comuns, podendo ser consideradas superclasses, relacionamentos e disjunções.
Em é proposta uma abordagem de rastreabilidade orientada a ontologias e integrada ao Processo Unificado.
O objetivo dessa proposta é relacionar elementos que compõem certos diagramas UML com os conceitos de seu domínio de aplicação, utilizando para isso uma ontologia como o principal artefato para representação do conhecimento.
O objetivo dessa integração é adicionar uma camada semântica entre diferentes modelos de software, apoiando assim os desenvolvedores na busca de informação relevante entre elementos de diagramas da UML.
Com o objetivo de formalizar a proposta de rastreabilidade orientada a ontologia, os seguintes objetivos específicos foram desenvolvidos:
Inicialmente foi proposta a criação de uma nova disciplina no Processo Unificado, chamada Modelagem do Conhecimento, a qual foi estruturada em três macro atividades:
Projeto, Manutenção e Verificação.
A ontologia é projetada nas fases iniciais de desenvolvimento utilizando como base o Modelo de Domínio, documento este produzido durante a disciplina de Modelagem de Negócios.
Este modelo utiliza a sintaxe do diagrama de classes da UML para representar os conceitos do mundo real.
Com isso, a disciplina de Modelagem de Conhecimento refina a ontologia de domínio introduzindo novos relacionamentos que não podem ser expressos em diagramas de classes convencionais, como subPropertyOf.
Durante a disciplina de Requisitos, novos conceitos e relacionamentos sobre o domínio vão surgindo, fornecendo assim insumos para refinar o Modelo de Domínio e a ontologia em desenvolvimento.
Uma vez que este modelo é considerado completo, os Engenheiros do Conhecimento (EC) podem iniciar suas atividades de manutenção.
Esta macro atividade é responsável por manter a consistência entre o modelo de conhecimento gerado e a semântica dos artefatos produzidos durante o ciclo de vida de desenvolvimento.
Além disso, o EC estabelece manualmente os elos de rastreabilidade entre conceitos do domínio e elementos UML.
Cada modificação na ontologia deve ser verificada por as atividades descritas na macro atividade de Verificação.
Estas atividades consistem num processo de controle que analisa cada nova versão do modelo de conhecimento procurando por inconsistências.
Foi apresentada também uma estrutura ontológica chamada ONTrace responsável por a associação entre os recursos do domínio e os elementos dos diagramas UML.
ONTrace representa um recurso descrito numa classe OWL associada a uma propriedade (objectProperty) chamada ontraceRecover que mapeia elementos UML a conceitos do domínio.
Este mapeamento inclui o relacionamento entre casos de uso, classes num diagrama de classes e outros elementos descritos por a UML, a recursos de conhecimento, tais como classes e propriedades descritas em OWL.
Para cada elo de rastreabilidade, uma instância da classe ONTrace é criada ou atualizada, mapeando algum conceito a elementos da UML.
A Figura 2.8 exemplifica um elo de rastreabilidade entre o caso de uso &quot;UC01 ­ Manter Funcionário «e o conceito da ontologia &quot;Funcionário «através da propriedade &quot;ontraceRecover».
Adicionalmente, este elo especifica que este indivíduo é do tipo &quot;UseCase», que é um dos tipos definidos por a taxonomia estabelecida sobre a classe &quot;Model».
Para ilustrar a rastreabilidade orientada a ontologias, considere um cenário de comércio eletrônico.
Suponha que o caso de uso &quot;Manter Cliente «está relacionado ao conceito da ontologia &quot;Cliente», enquanto outro caso de uso &quot;Realizar Pedido «está relacionado aos conceitos &quot;Cliente», &quot;Pedido «e &quot;Produto».
Em este cenário, é possível inferir que &quot;Manter Cliente «e &quot;Realizar Pedido «se relacionam explicitamente através do conceito &quot;Cliente «por um elo de rastreabilidade direto.
Em este mesmo exemplo, suponha que exista outro caso de uso chamado &quot;Manter Funcionário», que se relaciona com o conceito &quot;Funcionário».
Existe também uma propriedade chamada &quot;buscaInformacao «que relaciona os conceitos &quot;Empregado «e &quot;Cliente».
Em este cenário, é possível inferir um relacionamento implícito entre os artefatos &quot;Manter Cliente «e &quot;Manter Funcionário», pois os conceitos que relacionam ambos estão associados por a propriedade &quot;buscaInformacao».
É importante perceber que o uso de regras semânticas complexas em motores de inferência permite recuperar cenários muito menos óbvios que este exemplo, estendendo- se inclusive para diferentes produtos de trabalho.
Para apoiar esse trabalho, foi desenvolvida uma ferramenta chamada ONTrace Ide como uma extensão da ferramenta open source de modelagem ArgoUML.
A ferramenta permite a extração da ontologia a partir de o Modelo de Domínio, a geração de elos de rastreabilidade utilizando o recurso ONTrace da ontologia e a recuperação de elos implícitos e explícitos.
A Figura 2.9 apresenta a interface da ferramenta que permite recuperar elos de rastreabilidade explícitos e implícitos.
A avaliação da proposta foi feita por dois experimentos controlados aplicados a populações e em épocas diferentes.
Este estudo empírico foi executado comparando a rastreabilidade orientada a ontologias com a tradicional rastreabilidade orientada a requisitos, considerando as perspectivas de esforço e precisão.
O objetivo foi caracterizar a aplicabilidade e relevância dessa abordagem comparada a modelos amplamente empregados em empresas de desenvolvimento de software.
Embora os resultados sejam limitados ao contexto dos experimentos, os mesmos parecem promissores em termos de aplicabilidade, mostrando um melhor desempenho nas perspectivas consideradas para a rastreabilidade orientada a ontologias tanto na primeira execução do experimento quanto em sua replicação.
Maiores detalhes estão descritos em.
Recentemente é possível perceber o crescimento do número de artigos científicos relacionados à integração de semântica formal nas metodologias de desenvolvimento de software.
Diferentes aspectos do desenvolvimento vêm sendo melhorados por o uso de técnicas de representação do conhecimento.
Devido a a similaridade entre ontologias e modelos orientados a objetos, alguns pesquisadores vêm focando em como melhorar o projeto e qualidade usando Lógica Descritiva, que é um subconjunto da lógica de predicados em a qual OWL é parcialmente mapeada, com o objetivo de utilizar motores de inferência como o Jena.
O trabalho de discute a rastreabilidade no processo de desenvolvimento de software utilizando técnicas de recuperação da informação para avaliar a similaridade entre produtos de trabalho.
Eles apresentam como a Indexação Semântica Latente (LSI) pode ser utilizada para recuperar elos de rastreabilidade baseada em similaridade de texto presente nos artefatos de software.
O trabalho também apresenta uma ferramenta que ajuda os engenheiros de software a descobrir elos de rastreabilidade emergentes durante a evolução do software, bem como monitorar elos previamente definidos.
A proposta foi avaliada por um estudo de caso envolvendo 150 estudantes em 17 projetos.
Os autores identificaram que a proposta recupera muitos falso positivos se a similaridade entre os artefatos vai além de o intervalo mínimo definido.
Por isso, a proposta é muito dependente de contexto, visto que este intervalo deve ser adaptado para cada projeto, ameaçando seu uso prático.
Por outro lado, existe um efeito colateral de detectar possíveis artefatos mal documentados, visto que a ferramenta facilita a inspeção dos artefatos através de elos de rastreabilidade.
Em, os autores apresentam uma abordagem que melhora o léxico do código fonte comparando anotações e comentários à terminologia da especificação de requisitos.
Esta abordagem utiliza os benefícios de em aplicar técnicas de recuperação da informação para avaliar a similaridade entre requisitos e código fonte.
A premissa geral neste artigo é que a similaridade entre artefatos de alto nível e o código fonte é um indicador da qualidade das anotações e comentários no código.
A ideia é fornecer uma ferramenta para desenvolvedores (um plugin do Eclipse) que sugere termos do domínio que possam ser usados no código fonte durante as atividades de desenvolvimento.
O artigo também apresenta uma avaliação empírica da ferramenta utilizando dois experimentos controlados, tendo como participantes alunos de mestrado e graduação.
Por a sugestão de termos durante a codificação do software, a proposta tenta minimizar o impacto de termos não significantes introduzidos durante a implementação, os quais reduzem a identificação de similaridade na definição de elos de rastreabilidade.
O trabalho de Zhang et al.
Utiliza técnicas de PLN e ontologias para mapear automaticamente código fonte e documentação.
Os autores propõem uma ontologia de dois níveis:
Código fonte e documentação.
A ontologia do código fonte contém conceitos tais como Classe, Variável e Comentários.
A ontologia da documentação é construída utilizando técnicas de PLN, incluindo algoritmos de reconhecimento de nomes de entidades e análise de estrutura sintática.
Por o elo entre estas ontologias, os autores são capazes de buscar no modelo relações entre o código fonte e documentação.
A avaliação da proposta foi realizada utilizando um exemplo de uso e consultas neste exemplo.
Os autores não apresentam nenhuma avaliação empírica da proposta nem como aplicar sua proposta em produtos de trabalho no nível de projeto.
A documentação utilizada (JavaDoc) e o código fonte possuem um relacionamento bastante forte visto que ambos são gerados durante a fase de implementação.
Seria interessante explorar o mapeamento do código fonte com os requisitos da aplicação utilizando ontologias geradas a partir de o modelo de domínio.
Em Song et al.
Propõe- se um método que combina LD (Lógica Descritiva) e Jena para desenvolver representações ontológicas uniformes para vários artefatos de software, desde o nível de requisitos, projeto e código fonte.
Este componente analisa as relações correspondentes entre LD e OWL e desenvolve um conjunto de funções utilizadas para buscar informações na ontologia de manutenção do software.
Os autores não apresentam como essa ontologia é desenvolvida nem mantida.
Além disso, o envolvimento do papel do analista de domínio não é discutido conforme apresentado na Seção 3.1.
Apesar de os autores sugerirem o mapeamento do código fonte com os requisitos, não é explorado como existentes técnicas poderiam ser utilizadas para extrair estas associações conforme explorado na Seção 3.3.2.
O trabalho de Assawamekin et al.
Também propõe utilizar ontologias para representar requisitos e especificações do software.
Os autores exploram a perspectiva estrutural da arquitetura de software (usando diagramas de classe) para automaticamente gerar elos de rastreabilidade.
A abordagem não cobre a criação e refinamento da ontologia, rastreabilidade de modelos de software comportamentais e a comparação léxica e semântica entre os termos.
Não é apresentada também nenhuma avaliação empírica para verificar se a proposta gera elos de rastreabilidade falso positivos.
Recuperação de Informação e Análise de Impacto A Recuperação da Informação lida com a representação, armazenamento, organização e acesso a informação.
Diferente da recuperação de dados, que está preocupada em obter dados que satisfaçam determinado critério, a Ri lida com a obtenção de informação que nem sempre está estruturada e pode ser semanticamente ambígua.
O campo de Ri nasceu na década de 50, principalmente devido a a crise em bibliotecas.
A o longo desses 50 anos, a área teve um avanço significativo devido a os inúmeros sistemas que utilizam Ri diariamente para reduzir a sobrecarga de informação.
Muitas universidades e bibliotecas públicas utilizam sistemas de Ri para acessar livros, jornais e outros documentos.
Mecanismos de busca como o Google, Amazon e outros representam aplicações que se destacam na área.
Segundo Oliveto, um processo de Ri inicia quando um usuário submete uma consulta a um sistema.
Consultas são declarações formais de uma necessidade como, por exemplo, busca de palavras em motores de busca.
Em Ri, uma consulta não identifica unicamente um objeto numa coleção, mas vários objetos, tais como documentos texto, imagens ou vídeos, que podem satisfazer esta consulta com diferentes níveis de relevância.
Atualmente, a maioria dos sistemas de Ri utiliza a classificação para estimar a utilidade de um documento dada uma consulta, qualificando objetos quanto a o seu valor e relevância.
Segundo, o processo associado a Ri é composto por cinco estágios:
Recuperadas, incluindo documentos e suas estruturas;
O último estágio corresponde à avaliação se os documentos retornados numa consulta são aqueles de interesse, isto é, se o sistema conseguiu retornar os documentos que o usuário gostaria.
Apesar de não ser um processo recente, este modelo é válido até os dias de hoje, mas com uma melhor performance computacional e efetividade.
Em vias gerais, Marmanis e Babenko sugerem que para a captura de documentos é necessário realizar um parser para transformar os documentos alvo da busca numa estrutura comum visando à indexação.
Baeza-Yates e Ribeiro-Neto apresentam em alguns modelos clássicos de O Modelo Booleano é um modelo de recuperação simples baseado na teoria de presentes ou não em documentos, o que implica que os pesos de cada índice se tornam binários, isto é, o documento é ou não relevante a busca.
A pesquisa é realizada utilizando apenas por conectores lógicos como and, or e not.
Este modelo sofre grandes limitações, tais como decisão binária que implica em recuperação de dados e não de informação, semelhante a bancos de dados convencionais.
Além disso, este modelo carece de semântica, pois nem sempre documentos podem ser traduzidos como sim ou não.
O Modelo Vetorial reconhece que pesos binários são muito limitados e propõe um framework que torna possível mapear parcialmente consultas a documentos.
Isso se torna viável por a atribuição de pesos não binários a índices de termos, computando assim um grau de similaridade entre consultas e documentos, recuperando informação de forma mais precisa que o Modelo Booleano.
Em o Modelo Vetorial, pesos são atribuídos aos vetores consulta e documentos e sua correlação é definida através do ângulo entre os dois vetores através do grau de similaridade.
Como vantagem, tem- se que:
Como desvantagem, os índices dos termos são assumidos como independentes uns dos outros, não considerando seu contexto semântico.
O modelo de Indexação Semântica Latente (Latent Semantic Indexing ­ LSI) é um mecanismo que induz a compreensão do significado de palavras por a análise das relações entre as palavras em grandes textos.
Este modelo visa recuperar a ideia de um texto por os conceitos descritos no mesmo ao invés de termos índices em sua descrição.
O modelo LSI foi originalmente definido para resolver os problemas de polissemia e sinonímia que ocorre no Modelo Vetorial.
O método utilizado para capturar a semântica essencial é a redução da dimensão, selecionando as dimensões mais importantes a partir de a matriz de coocorrência utilizando Singular Value Decomposition (SVD).
SVD é uma forma de análise de fator que visa reduzir as dimensões de um espaço de uma funcionalidade sem perder especificidade.
Uma das abordagens mais expressivas do SVD é o motor de busca do Google, em a qual qualquer matriz pode ser decomposta para ser posteriormente recomposta utilizando a menor dimensão da matriz original.
O Modelo Probabilístico se baseia que dada uma consulta do usuário, existe um conjunto de documentos que contém exatamente os documentos relevantes e nenhum outro, considerados como o conjunto ideal.
O processo de busca, neste contexto, corresponde a identificar exatamente as propriedades deste conjunto ideal.
Como não é possível determinar essas propriedades durante a execução da consulta, o modelo sugere sua suposição.
A partir desse momento, é necessária uma interação com o usuário para validar e sugerir melhorias desse conjunto inicialmente sugerido.
Com base nessa avaliação do usuário, o sistema refina sua descrição do conjunto ideal.
O Modelo Probabilístico é baseado fundamentalmente no princípio da pressuposição.
Como desvantagem deste modelo, pode- se destacar:
Sobre os modelos apresentados, o Modelo Booleano geralmente é considerado o modelo mais fraco.
Seu maior problema é a incapacidade de reconhecer o mapeamento parcial, gerando assim uma performance inferior.
Existem controvérsias entre o Modelo Vetorial e Probabilístico, pesquisadores como apresentam situações em que modelos possuem melhor desempenho em determinados problemas.
Um Modelo Probabilístico clássico é a Rede de Crenças Bayesiana, utilizada para evidenciar o relevância de um documento.
Redes Bayesianas são grafos acíclicos direcionados em o qual os nodos representam variáveis aleatórias e as arestas relações casuais entre essas variáveis.
A força dessas influências casuais são expressadas através de probabilidades condicionais.
Para expressar esta relação, considere xi como um nodo de uma rede Bayesiana G e Txi o conjunto de nodos pai de xi.
A influência de Txi em xi pode ser especificada por qualquer função Fi (xi, Txi) que satisfaça:
Onde xi também se refere aos estados aleatórios da variável associada ao nodo xi.
Redes de crenças fornecem os meios estatísticos para lidar com processos incertos, vagos e propensos a erro.
A teoria fundamental por trás dessas redes é o teorema de Bayes, representado por a equação:
Onde P (A, B) $= P (B| A) P (A) é baseado na regra da cadeia.
P (A) é a probabilidade inicial ou probabilidade marginal de um vetor A, o qual é independente do vetor B. P (A| B) é a probabilidade condicional de A, dado B. As probabilidades condicionais de uma rede Bayesiana podem ser definidas através de diferentes estratégias.
A estratégia de ordenação TFIDF, por exemplo, considera a normalização de frequência de termos (TF, term-- frequency) e de documento inversa (IDF, inverse-document-frequency), cuja fórmula é:
Onde é o número total de documentos do sistema, documentos em o qual o índice dos termos frequência bruta do termo aparece no documento é o número de é a no texto da informação recuperada por.
A recuperação de informações através de consultas pode ser justificada por o principio probabilístico de ordenação conforme modelo apresentado anteriormente.
Os modelos clássicos pressupõe a ausência de conhecimento sobre os documentos.
Contudo, algoritmos modernos vêm explorando o conhecimento prévio de documentos incorporando preferências de ordenação estática, isto é, ordenação independente da consulta executada, tais como o Hits e PageRank.
Dada uma consulta (um conjunto de palavras ou outros termos), Hits invoca um tradicional motor de busca para obter um conjunto de páginas relevantes, expandindo este conjunto utilizando links de entrada e saída e, então, tenta identificar dois tipos de páginas:
Hubs, páginas que apontam para várias páginas de alta qualidade, e authorities, que representam paginas de alta qualidade.
O algoritmo PageRank, famoso algoritmo de busca do Google, foi introduzido em 1998 e representa um modelo de análise de elos.
O algoritmo original melhora a ordenação dos resultados de uma consulta computando um simples vetor usando a estrutura de elos da Web para capturar a importância de cada página, independente de uma consulta particular.
Esta independência é um grande avanço comparado ao Hits, cuja ordenação é calculada em tempo de consulta.
Medidas de Ri tradicionais, como TFIDF, ordenam o documento baseado na frequência de termos.
PageRank, no caso, ordena documentos baseado em sua popularidade, como quantos documentos apontam para um segundo documento e, intuitivamente, o mais popular possui uma maior relevância.
A ideia chave do PageRank é considerar hiperlinks de uma página para outra como recomendações ou popularidade.
Então, quanto maior a popularidade de uma página, isto é, existem várias páginas a referenciando, maior sua importância.
A Figura 2.10 adaptada de apresenta um grafo orientado para um domínio qualquer.
A seta representa que a página de origem referência a página de destino.
Baseado nesta estrutura, é possível definir a Matriz de Hiperlinks H e um vetor p..
Cada linha da matriz H é construída contando o número referências que uma página faz (digamos) e atribuindo a coluna j o valor caso a página possua alguma referência, ou Zero caso contrário.
Então para o grafo da Figura 2.11, tem a matriz H apresentada por a Figura 2.11.
Para calcular o PageRank, é utilizada a fórmula abaixo.
Sendo os valores de p o valor do PageRank de cada página do grafo e.
Por questões técnicas, a matriz H é geralmente substituída por o Google por a matriz G, que possui melhores propriedades matemáticas.
Em a Web, existe um problema em que páginas não referenciam outras páginas, o que leva a problemas no algoritmo impedindo a navegação para outras páginas e correspondem as células que possuem valor Zero na matriz H. Para correção deste problema, o PageRank inclui um fator de ajuste que corresponde a um pulo aleatório, referenciado como um ajuste estocástico, atribuindo aleatoriamente as células com Zero o valor, onde n, é o número de páginas no grafo.
Outra restrição é que é possível alterar a navegação através de saltos arbitrários, chamados de alpha, que corresponde à primitiva de ajuste.
O valor de alpha usado por o Google é 0.85.
Conforme, durante o processamento de documentos, nem todas as palavras são igualmente significantes para representação semântica de documentos, o que significa que na linguagem escrita, algumas palavras carregam mais significado que outras.
Geralmente substantivos são as palavras mais representativas num documento devido a sua carga semântica.
Então, é geralmente considerado útil o pré-processamento de texto de documentos numa coleção visando determinar quais os termos candidatos a índices.
O pré-processamento pode ser dividido em cinco principais operações:
Análise léxica, eliminação de stopwords, stemming, seleção de termos índices e construção das estruturas de categorização dos termos.
Análise léxica que corresponde basicamente em converter uma sequência de caracteres numa sequência de palavras ou tokens.
Tokens, neste caso, são grupos de caracteres com uma significância coletiva candidatos a termos índices.
Em resumo, o maior objetivo da análise léxica é identificar palavras num texto.
Entretanto, existem palavras que, apesar de aparecerem inúmeras vezes no documento, não apresentam significado de relevância, como os artigos, as preposições, os pronomes e outras classes de palavras auxiliares.
A esse conjunto de palavras não significantes é dado o nome de stopwords.
Eliminação de stopwords que corresponde a filtrar palavras que possuem uma discriminação muito baixa no processo de recuperação.
O objetivo deste processo é evitar que palavras insignificantes interfiram no processo de Ri, reduzindo, assim, o tamanho do texto e do documento, o que facilita o armazenamento destes termos.
A utilização de uma lista de stopwords bem elaborada permite que sejam eliminados muitos termos irrelevantes, aumentando, assim, a eficiência do resultado obtido por o processo de indexação.
Apesar de reduzir o texto, esta técnica pode apresentar problemas, pois nem sempre as palavras de maior ou menor frequência não são significativas para o contexto.
As palavras que compõem um documento podem ser variações de outras palavras utilizadas nas consultas (plural, grau, etc.).
As diversas formas de flexionar palavras podem não alterar seu valor semântico e isso pode ser um problema no processo de busca.
Para minimizar esse problema, foram desenvolvidas diversas técnicas que permitem aos buscadores identificar relações semânticas entre consultas e documentos.
A conflação, o ato de fusão ou combinação, para igualar variantes morfológicas, é a técnica utilizada por os buscadores que amplia essas relações semânticas.
De acordo com Croft, o stemming é o processo de reduzir a grande dimensionalidade de termos que são extraídos num conjunto de documentos.
Ele possui como objetivo remover os afixos (prefixos e sufixos) reduzindo o termo (token) a sua provável raiz (stem), possibilitando assim recuperar documentos equivalentes apesar de pequenas variações sintáticas como, por exemplo, &quot;conectando», &quot;conectado», &quot;conexão», etc..
A seleção de termos índices visa determinar quais palavras/ stems (ou grupos de palavras) podem ser utilizadas para indexar elementos.
A decisão na escolha de uma palavra em particular para ser usada como termos índices está relacionada à natureza sintática da palavra e, de fato, substantivos são aquelas que frequentemente possuem uma maior semântica, se comparadas a adjetivos, advérbios e verbos.
Por último, a construção das estruturas de categorização dos termos, tais como tesauros, permitem a expansão da consulta do termo para termos correlatos.
Um tesauro provê um vocabulário preciso e controlado que serve para coordenar a indexação e recuperação de documentos, viabilizando assim a seleção dos termos mais apropriados.
Basicamente, as vantagens correspondem à definição dos relacionamentos entre termos, identificação do número de ocorrência, especificação e normalização de um vocabulário comum.
A definição de credibilidade de classificação de informação é geralmente o ponto inicial para avaliar os métodos empregados.
Quando se submete uma consulta a um sistema de Ri, o resultado pode não ser o esperado.
Existem várias métricas para avaliar o grau de sucesso desses sistemas, sendo as mais comuns revocação e precisão.
A Figura 2.12 adaptada de apresenta todas as possibilidades de resultados de uma consulta típica.
Conforme Figura 2.12, dado um conjunto com todos os documentos, um subconjunto desses documentos será relevante para a consulta do usuário e outro subconjunto será aquele recuperado por sua consulta.
Claramente o objetivo é recuperar todos os documentos relevantes, mas raramente este é o caso, portanto a intenção é recuperar a intersecção conforme ilustrado por a figura.
Revocação, por sua vez, mede a proporção de documentos relevantes recuperados, isto é, o número de documentos relevantes recuperados divididos por o número total de documentos relevantes (Rt) conforme:
Quantitativamente, estas duas métricas respondem questões diferentes.
A precisão responde a extensão em que se obtém aquilo que se gostaria e revocação responde se foi recuperado tudo aquilo que deveria ter sido.
Percebe- se que é mais fácil encontrar a precisão que revocação, visto que para a segunda é necessário saber de antemão quais são todos os documentos relevantes dada uma consulta.
Uma medida que combina precisão e revocação é sua média harmônica, também conhecida como medida F, e computada conforme:
Onde r (j) é a revocação para o elemento j, P (j) é a precisão para o elemento j e F (j) é a média harmônica de r (j) e P (j).
Em os últimos anos, uma série de abordagens automatizadas para analisar impacto utilizando recuperação de informação vem sendo propostas.
Helm et al.
Apresenta uma abordagem e uma ferramenta para construção automática de grandes bibliotecas de software a partir de documentação de seus componentes de software, fazendo uso do código fonte e documentação associada.
A abordagem proposta combina dois tipos de análise:
Técnicas de Ri baseadas em análise de documento e abordagens específicas de domínio, em a qual a maioria da informação é fornecida por um especialista de domínio baseadas na análise de código.
Em esta técnica, é realizado o parser do código fonte para derivar informação estrutural essencial como a relação entre entidades (por exemplo, classes, métodos e variáveis) ­ &quot;derivadas de», &quot;membro de», com base em um modelo de dados da linguagem de programação.
Por outro lado, Ri é utilizada para indexar documentação de software.
Mais especificamente, o mecanismo de indexação da ferramenta desenvolvida é centralizado ao redor de os conceitos de relações léxicas e noção de quantidade e qualidade da informação disponível ao especialista de domínio.
A partir de essa abordagem, é possível a realização de consultas em linguagem natural para encontrar informação relevante no código fonte e documentação das aplicações analisadas.
Antoniol et al.
Propõe um método para avaliar um conjunto de componentes de sistema inicialmente impactados por uma mudança a partir de uma requisição textual.
Duas abordagens foram aplicadas para recuperar os documentos relevantes a partir de uma requisição de mudança:
O modelo vetor espacial e probabilístico.
A abordagem foi estruturada em três passos:
A proposta foi aplicada em classes de domínio público da biblioteca C+.
Os autores chegaram à conclusão de que os maiores valores de revocação foram mais interessantes do que de precisão no que compete a qualidade dos resultados, visto que é importante não perder nenhum documento impactado por uma requisição de mudança.
A identificação de conceitos pode também ser útil para detecção de clones.
Marcus e Maletic utilizam Indexação Semântica Latente (LSI ­ Latent Semantic Indexing) para analisar estaticamente o software e determinar similaridades semânticas entre documentos (funções, arquivos ou segmentos de código).
Estas similaridades são usadas para o processo de detecção de clones de código fonte.
Em Chen et al.
É apresentada uma abordagem e ferramenta chamada CVSSearch que aproveita as características de ferramentas de gestão de configuração, em a qual os comentários tipicamente descrevem as linhas de código que foram impactadas durante a persistência de um código fonte.
Em particular, o CVSSearch permite que desenvolvedores pesquisem em comentários do CVS e fragmentos de código associados.
A abordagem assume vantagem de que os comentários do CVS tipicamente descrevem as linhas de código envolvidas durante a operação de persistência e esta descrição é mantida para futuras revisões.
Métodos de Ri como stemming e classificadores são usados para armazenar e buscar as informações extraídas dos logs do CVS e linhas de código associadas na revisão dos arquivos através de análise de similaridade.
Canfora e Cerulo em propõem um método que explora algoritmos de Ri e um modelo probabilístico para associar uma descrição de requisição de mudança e um conjunto histórico de revisões de código fonte impactados por antigas requisições de mudanças similares.
O raciocínio por traz da abordagem proposta é que a descrição textual mantida numa ferramenta de gestão de erros (bug tracking) durante a resolução de uma requisição de mudança é um descritor útil dos arquivos impactados para requisições de mudanças futuras e similares.
O artigo possui como hipótese de que um conjunto de comentários de revisão de um arquivo e um conjunto de requisições de mudanças que anteriormente impactaram esse arquivo são bons descritores dos arquivos que apoiam a análise de impacto de novas requisições de mudanças.
O artigo descreve duas contribuições:
Uma das tarefas mais importantes da análise de impacto é a localização de conceitos.
Esta localização é geralmente realizada por um processo intuitivo e informal cujo objetivo é identificar partes do software que implementam certa funcionalidade ou conjunto de conceitos.
Esta atividade compreende um pré-requisito para diversas tarefas de evolução e compreensão do programa, sendo ela uma atividade desempenhada comumente por desenvolvedores (identificar como conceitos e entidades do domínio são implementadas).
A atividade de localização de conceitos é uma das aplicações mais comuns dos métodos de Ri no contexto da Engenharia de Software.
A localização de conceitos no código fonte utilizando LSI e PageRank é abordado por Marcus et al.
Em particular, o usuário formula uma consulta, que consiste numa série de palavras que descreve um conceito a ser localizado.
A técnica usa esta consulta para classificar e recuperar os documentos de um sistema, como arquivos, classes, métodos ou funções.
Por exemplo, para a frase &quot;Adicionar pagamento por cartão de crédito no sistema de vendas», a técnica identificaria os conceitos &quot;pagamento «e &quot;cartão de crédito «no código fonte do programa.
Em é apresentada uma ferramenta para localização de conceitos que utiliza a ferramenta off the shelf Google Desktop Search.
A abordagem é implementada como um plugin do Eclipse e chamada Google Eclipse Search (GES).
GES permite aos desenvolvedores pesquisar nos projetos de software de forma similar a uma busca na internet.
Esta abordagem visa resolver problemas utilizando para tanto ferramentas e frameworks existentes (como a API -- Application Programming Interface -- do Google Desktop Search) para busca de conceitos.
JSearch é uma ferramenta projetada para localizar partes comum de código fonte num repositório utilizando Ri para indexar informações extraídas do código fonte.
JSearch não indexa os arquivos como documentos planos, ele realiza o parser do código fonte utilizando um Parser Java AST e extrai os campos de cada classe para indexação, fazendo uso da API Java Lucene.
A ferramenta não processa linguagem natural para realizar a consulta, mas possui uma linguagem específica de domínio em a qual os desenvolvedores podem especificar os membros do código, tais como classes, nomes de métodos, em os quais a busca será feita.
Uma abordagem estática que não necessita de interação com o usuário, chamada estática para automaticamente associar descrições textuais de funcionalidades com sua localização no código fonte.
SNIAFL usa Ri para revelar conexões básicas entre das funcionalidades e unidades computacionais (como funções) no código fonte.
Como a recuperação inicial pode ser imprecisa, SNIAFL usa a representação do grafo de chamada para restringir funções que não são relevantes identificadas inicialmente.
A novidade da abordagem é o uso combinado dos tipos de análise para endereçar a identificação de funcionalidades:
Ri e análise estática.
A performance da localização de conceitos pode ser melhorada combinando diferentes técnicas.
Poshyvanyk et al.
Combinam duas técnicas para localização de conceitos:
O estudo de caso apresentado demonstrou que a abordagem combinada teve um melhor desempenho que o uso das mesmas técnicas isoladas.
Poshyvanyk e Marcus também apresentam uma abordagem combinando LSI e Formal Concept Analysis (FCA) em conjunto.
FCA é utilizada para definir o conceito como uma unidade de duas partes:
Extensão e intenção.
A extensão de um conceito recupera todos os objetos que pertencem ao conceito, enquanto a intenção referência todos os atributos que são compartilhados por os objetos em consideração.
Em a abordagem proposta, LSI é utilizada para mapear os conceitos recuperados em consultas escritas por o programador com as partes do código fonte, apresentando uma lista ordenada dos resultados.
Dada esta lista ordenada, a abordagem seleciona os atributos mais relevantes destes documentos e organiza os resultados via FCA.
Em particular, a proposta é utilizada para mapear conceitos expressos por consultas escritas por os programadores para as partes de código relevantes, apresentando ao fim uma lista ordenada de resultados de busca.
Dada esta lista ordenada, a abordagem seleciona os atributos mais relevantes destes documentos, organizando os resultados da busca em tópicos (categorias).
Duas técnicas de localização de conceitos são combinadas em Shepherd et al.:
Uma abordagem leve de PLN e análise de programa.
O usuário interage com a ferramenta da seguinte forma:
Formulação da consulta inicial, a ferramenta expande esta consulta capturando ações relacionadas entre os identificadores do programa (considerando a relação verbo-ação), resultando em grafo orientado em o qual não apenas os conceitos são identificados, mas as ações associadas a estes conceitos.
A consulta é realizada sobre este grafo para descobrir relações estruturais entre os métodos utilizando como critério inicial os conceitos presentes na consulta.
O diferencial da proposta é que utiliza técnicas de PLN para buscar a morfologia, sinônimos e estrutura das sentenças.
Revisão Sistemática Com o objetivo de identificar o estado da arte em análise de impacto e sua integração com ontologias e recuperação de informação, foi desenvolvida uma revisão sistemática.
A motivação para sua realização surgiu de estudos preliminares que não identificaram uma proposta que, a partir de uma perspectiva de negócio, identificassem informações relacionadas a estruturas de código fonte impactadas por determinada requisição de mudança.
Surgiu então o interesse de relacionar ontologias, que capturam essa perspectiva de negócios, recuperação de informação, que obtém informações que nem sempre estão estruturadas e que podem ser semanticamente ambíguas, com o código fonte de uma aplicação.
Assim, definiu- se a seguinte questão de pesquisa para nortear a revisão:
&quot;Quais são os principais resultados encontrados em pesquisas sobre análise de impacto utilizando ontologias e recuperação de informação?».
Em se tratando de análise de impacto, as Seções 2.2.4 e 2.3.5 apresentaram respectivamente um conjunto extensivo de trabalhos relacionados a ontologias e recuperação de informação.
A o responder a questão definida por esta revisão sistemática, estará se verificando o estado da arte sobre propostas que representam a intersecção dessas duas áreas e se há contribuição suficiente para caracterizar- la como uma área de pesquisa.
Adicionalmente às palavras chave, considerou- se como critério de intervenção o contexto de desenvolvimento de software e código fonte.
Após a realização de alguns testes comparativos para avaliar a relevância dos resultados, foram utilizadas as expressões descritas na Tabela 2.2.
Simultaneamente ao processo de elaboração dos critérios de busca, foram selecionadas as seguintes os seguintes domínios para pesquisa:
O número de artigos retornados em Março e Abril de 2012 em cada fonte é apresentado na Tabela 2.3.
Para definirmos um critério de inclusão e exclusão dos trabalhos a serem selecionados, foi feita uma classificação com três níveis para enquadrar os trabalhos que para inclusão e exclusão dos artigos na revisão foram elaborados para serem aplicados em todos os artigos que foram retornados nos mecanismos de busca selecionados, como apresentado na Tabela 2.4.
O foco do artigo é análise de impacto utilizando ontologias e técnicas de recuperação de informação.
O foco do artigo é análise de impacto utilizando ou ontologias ou técnicas de recuperação de informação.
O foco do artigo é análise de impacto não utilizando ontologias nem técnicas de recuperação de informação.
O foco do artigo não é análise de impacto.
Para a seleção dos trabalhos, foi definido um processo que iniciou com a escolha dos artigos de onde foram retiradas as sínteses e as contribuições para a revisão sistemática.
Não foi feita nenhuma análise prática das aplicações ou protótipos descritos nos trabalhos selecionados.
Também não foi incluída a identificação de ferramentas durante a fundamentação deste estudo.
A Tabela 2.5 mostra o processo de seleção dos estudos e o número de artigos identificados em cada estágio.
Em[ Ary11] é apresentado uma abordagem para predizer dependências de software baseado no acoplamento de informações de domínio.
Para tanto, são definidos clusters conceituais por a identificação de variáveis e funções de domínio no código e componentes de interface com usuário.
A definição destes clusters corresponde à ocorrência de conceitos na camada da aplicação que faz a interface com o usuário, identificando o acoplamento de variáveis do domínio.
Este acoplamento é representado com um grafo, o qual é ponderado usando o coeficiente de Jaccard para calcular o peso da ocorrência do termo num contexto.
Esta proposta utiliza a frequência de termos de conceitos do domínio para a análise de dependência, que inclui a identificação de classes, atributos e métodos navegando por o grafo de conceitos até o código fonte.
Esta abordagem foi avaliada utilizando uma conhecida ferrramenta ERP (Enterprise Resource Planning) de código aberto para predizer dependências, porém não foi apresentada nenhuma avaliação empírica comparando os resultados da proposta com resultados onde desenvolvedores predizem os impactos, baseando em sua experiência.
A proposta apresentada em cria e utiliza uma ontologia específica do projeto derivada a partir de a informação textual de projeto com o objetivo de apoiar o processo de criação de elos de rastreabilidade.
Para tanto, os autores propõem uma arquitetura composta por um Analisador de Artefato, que extrai informação de instâncias de artefatos para identificar subordinação de entidades, como atributos de classes.
O módulo de Extração de Entidade Global utiliza um tagger para identificar substantivos de estruturas.
Estes substantivos são utilizados por o Construtor de Artefato Ontologia e Construtor Genérico de Ontologia para criação da ontologia a partir de os módulos anteriores, bem como a rastreabilidade entre os elementos.
O módulo de Raciocínio Ontológico de Projeto combina as ontologias geradas para recuperar estruturas de código relevantes considerando frequência e importância.
Não é apresentada nenhuma avaliação empírica.
Em, os autores apresentam um modelo de processo formal para apoiar a manutenção de software.
O modelo fornece uma representação ontológica que suporta o uso de serviços de raciocínio entre diferentes fontes de conhecimento.
O modelo é composto utilizando o Racer e sistemas de mineração de texto entre a ontologia do código fonte e da documentação.
Não foi apresentado nenhuma avaliação empírica.
Os autores de conduziram experimentos de abordagens existentes de rastreabilidade que fazem uso de recuperação de informação para avaliar a qualidade e acurácia das propostas.
O resultado do experimento foi inconclusivo devido a o pequeno número de participantes, não permitindo rejeitar a hipótese nula de que a acurácia da rastreabilidade das ferramentas diferem.
Os autores sugerem que é necessário maior investimento para obter retorno de investimento significativo por o uso de técnicas de Ri para rastrear objetos.
Em, os autores propõem o uso de recuperação de informação para explorar a informação linguística encontrada no código fonte tais como identificadores e comentários.
Os autores propõem clusters semânticos para agrupar artefatos de software que utilizam um vocabulário similar, técnica essa baseada em LSI.
Os elos de rastreabilidade são definidos por os clusters semânticos, classificados por a frequência dos termos.
A o final são apresentados alguns estudos de caso que evidenciaram que a abordagem proposta fornece um bom apoio para desenvolvedores não familiarizados com o sistema de encontrar estruturas de código relevantes.
Os autores em propõem o uso de ontologias para melhorar a recuperação e rastreabilidade do conhecimento sobre o software fazendo uso de anotação semântica em código.
O processo de indexação inclui a construção de uma ontologia preliminar gerada automaticamente a partir de requisitos funcionais e não funcionais, identificando instâncias de conceitos através de substantivos.
A recuperação das estruturas impactadas se dá por uma descrição de cenários, os quais conceitos são identificados.
Não foram apresentados resultados empíricos do uso da proposta.
Analisando todas as propostas resultado da revisão sistemática, não foi possível identificar trabalhos que utilizam efetivamente os recursos fornecidos por linguagens de representação do conhecimento, modelos de recuperação de informação e que tenham uma avaliação empírica relevante e consistente.
Considerações sobre o Capítulo As seções anteriores apresentaram uma fundamentação teórica buscando contextualizar as atividades relacionadas à análise de impacto no contexto do desenvolvimento de software orientado a ontologias.
Inicialmente foram apresentadas algumas definições sobre o termo análise de impacto bem como a relevância da área.
A seguir, foram apresentados alguns métodos para a execução da análise de impacto e suas principais etapas.
Com relação a as etapas, foram descritas as análises de rastreabilidade e dependência.
A primeira define o relacionamento entre termos, enquanto a segunda avalia a relevância desses termos frente a uma solicitação de mudança.
Com relação a as etapas descritas, a primeira apresentou um estudo sobre as perspectivas, classificações, sintaxe e semântica dos elos de rastreabilidade.
A análise de dependência, por sua vez, apresentou a sistemática das três principais técnicas:
Grafos de chamadas, particionamento estático e particionamento dinâmico.
A análise de impacto é consequência da evolução do software, direcionada por propostas de mudanças.
Assim, foram apresentados os conceitos e procedimentos relacionados à gestão de mudanças durante o desenvolvimento de software.
Mudanças são geralmente resultado de necessidades de negócio do mundo real, isto é, onde o software é posto em operação.
Para modelar conceitos do mundo real, ontologias emergem como o principal formalismo para especificação explicita de conceitos.
Foi então apresentada uma definição comum para o termo bem como a estrutura da linguagem OWL para sua representação.
Em Ciência da Computação, apesar de ontologias terem origem na área de Inteligência Artificial, este modelo é bastante difundido na área de Engenharia de Software.
Foram então discutidos trabalhos relacionados a ontologias no contexto de análise de impacto e rastreabilidade.
Para contextualizar o escopo dessa proposta, foi apresentado o trabalho desenvolvido em a respeito de rastreabilidade ontológica sobre o Processo Unificado.
Observou- se que os elos de rastreabilidade interferem na análise de dependência e, consequentemente, são determinantes para uma adequada análise de impacto.
Este trabalho tem como hipótese que ao aprimorarmos os elos de rastreabilidade utilizando ontologias, a análise de impacto também poderá ser aprimorada.
Sugere- se utilizar como base elos de rastreabilidade apoiados por conceitos com semântica associada, ao invés de simples e estáticos, como normalmente descritos nas propostas de análise de impacto, para melhorar assim a exatidão e cobertura na recuperação de alterações no código fonte frente a uma requisição de mudança.
O uso de ontologias para esse fim se mostra interessante, pois cada entidade computacional é derivada, direta ou indiretamente, de conceitos do mundo real.
Relacionando estes conceitos a estruturas de código fonte, permite- se uma análise dinâmica, por a dependência semântica entre os termos, bem como estática, por a organização estrutural do código.
Para analisar estas dependências, foram apresentados conceitos sobre Ri para busca de informação não estruturada no código fonte da aplicação.
Foram apresentados modelos e algoritmos, técnicas de processamento de documentos e linguagem natural, métricas para avaliar a eficiência de modelos de Ri.
Também foi apresentada uma extensa relação de trabalhos que associam análise de impacto e recuperação de informação.
Este capítulo também apresentou uma revisão sistemática que visou identificar propostas de análise de impacto que integrem ontologias e modelos de recuperação de informação.
Os resultados não foram satisfatórios ao responder a questão de pesquisa definida, evidenciando que a intersecção entre as áreas apresenta lacunas, conforme sumarizado por a Tabela 2.7.
Este resultado encoraja o desenvolvimento de pesquisa na área.
Esta tese tem como base algumas evidências empíricas positivas quanto a a definição e recuperação de elos de rastreabilidade entre conceitos da ontologia e elementos de diagramas da UML.
Estas evidências instigam um estudo mais específico de quão relevante é a lista de termos associados frente uma necessidade de negócio, bem como a avaliação desta dependência em nível de código fonte conforme descrito no próximo capítulo.
Este trabalho apresenta um modelo de análise de impacto em código fonte fazendo uso de ontologias.
O objetivo é automatizar a rastreabilidade entre estruturas de código com o domínio da aplicação e identificar a relevância dessas associações frente a um requisito de mudança, considerando as categorias de manutenções definidas por A Figura 3.1 ilustra o Modelo de Análise de Impacto proposto.
Como entrada deste modelo, deve- se informar o código fonte da aplicação, uma ontologia que descreve seu domínio e um requisito de mudança.
Apesar de o desenvolvimento da ontologia de domínio fugir do escopo desta tese, uma proposta de processo de integração de ontologias com a engenharia de software é discutida no Apêndice C. A saída irá evidenciar as estruturas de código (classes, métodos ou atributos) que potencialmente serão impactadas.
Este modelo é composto por dois submodelos:
Modelo de Rastreabilidade e Modelo Probabilístico. O Modelo de Rastreabilidade é responsável por interpretar a ontologia do domínio e o código fonte da aplicação.
A árvore de sintaxe abstrata do código fonte é traduzida numa nova ontologia, que possui seus componentes rastreados com os conceitos do domínio.
Os elos de rastreabilidade são populados automaticamente na ontologia de domínio através de um analisador de similaridade léxico e semântico, relacionando estruturas do código fonte (classes, métodos e atributos) às suas respectivas estruturas conceituais.
Esta análise de similaridade inclui eliminação de stopwords, a categorização de cada termo do código fonte como substantivo, normalização conceitual (WordNet e glossário) e comparação utilizando Stemming.
Este modelo recebe o código fonte e a ontologia do domínio como entrada e popula esta ontologia com elos de rastreabilidade.
O Modelo de Probabilidade avalia a relevância de cada elo de rastreabilidade utilizando o modelo probabilístico de recuperação de informação de Redes de Crenças Bayesianas.
Este modelo utiliza diferentes técnicas para identificar a relevância de código, desde o algoritmos de classificação como o PageRank e TFIDF, até a análise de dependência conceitual utilizando grafos de chamada.
Esta análise considera toda a estrutura de baixo nível do código fonte, respeitando sua organização léxica e semântica, sugerindo ao final a probabilidade de impacto das classes ou membros identificados como relevantes.
O Modelo de Análise de Impacto é relevante em diferentes aspectos.
Primeiro, ele relaciona conceitos do domínio com o código fonte da aplicação através de uma rastreabilidade orientada a ontologias que viabiliza a recuperação de relacionamentos implícitos e explícitos utilizando motores de inferência.
Segundo, ele provê uma abordagem de população de ontologias automática usando processamento de linguagem natural para identificar instâncias de conceitos do domínio a partir de produtos de trabalho de software.
Esta abordagem cria elos de rastreabilidade entre conceitos do domínio e o código fonte da aplicação.
Terceiro, ele pondera todos os elos de rastreabilidade utilizando um modelo probabilístico de Redes de Crenças Bayesianas.
Além disso, ele avalia diferentes aspectos, desde a frequência que termos do domínio são referenciados numa classe específica do código fonte, a frequência que um conceito específico aparece em todas as classes (frequência inversa do documento) e se a distância entre conceitos do domínio é equivalente a distância entre as classes do código referenciadas por estes conceitos (dependência conceitual).
Este capítulo também apresenta um cenário motivacional que descreve o problema relacionado à análise de impacto e que será utilizado para ilustrar cada aspecto do modelo proposto.
Cenário Motivacional Para ilustrar o contexto deste trabalho, esta pesquisa utilizou um sistema descrito e implementado em, que fornece descrições de casos de uso, diagramas UML e código fonte da aplicação.
Trata- se de um sistema de apropriação de horas de funcionários chamado Sistema de Cartão Ponto (Timecard).
O objetivo do sistema é apoiar o registro de horas em determinadas atividades, chamadas de Códigos de Débito (charge code).
Em este sistema, os usuários podem realizar a manutenção desses códigos, a manutenção de empregados, bem como o registro de horas nesses códigos.
Para ilustrar melhor o universo de discurso, a Figura 3.2 apresenta o Modelo de Domínio com os principais conceitos da aplicação e seus relacionamentos.
Para compreender as principais funcionalidades oferecidas por o sistema, o diagrama de casos de uso é apresentado na Figura 3.3.
O exemplo seguido neste trabalho se refere ao caso de uso Registrar Horas, cujo fluxo principal se encontra descrito na Figura 3.4.
Durante a evolução do software, devido a uma necessidade de negócio, surge um requisito de mudança formalizado num RdM.
Este requisito caracterizado como mantenção evolutiva consiste em alterar o sistema a partir de a declaração representada por a Figura 3.5.
Para entender um pouco o desafio em avaliar o impacto dessa mudança no código fonte, é necessário considerar aspectos técnicos bem como a complexidade do sistema.
Com relação a a tecnologia utilizada, o sistema foi desenvolvido em Java utilizando Enterprise Java Beans (EJB), versão 1.1, o qual faz parte da especificação Java 2 Enterprise Edition (J2EE).
O código é estruturado por os seguintes componentes:
Entity bean, Home interface, Session bean, Remote interface, Implementation, Deployment descriptor, Bean-managed persistence e Container-managed persistence.
Fluxo de eventos principal:
Empregado registra suas horas.
Passo&amp; Ação O empregado visualiza as horas previamente entradas para o período atual.
O empregado seleciona um código de débito de entre os disponíveis, organizados por clientes e projeto.
O empregado seleciona um dia na semana corrente.
O empregado informa horas trabalhadas como um número decimal positivo.
Durante o registro de horas, o usuário poderá informar horas para o mês corrente até a data atual.
Com o objetivo de avaliar a complexidade do código no momento de analisar o impacto da mudança, foram extraídas algumas métricas apresentadas nas Tabelas 3.1 e como a quantidade de código fonte passível de inspeção.
Tabela 3.2 ­ Análise de Linhas de Código (LoC) do Sistema de Cartão Ponto.
Linguagem Arquivos Linhas em Branco Linhas de Comentários Linhas de Código Java DOS Batch Por a análise das Tabelas 3.1 e 3.2, identificam- se 75 classes organizadas em 12 pacotes.
Para realizar a análise de impacto da mudança solicitada, a Seção 2.1.3 apresenta algumas estratégias.
A primeira opção é a força bruta, navegando no programa, abrindo e fechando arquivos relacionados.
Os desenvolvedores geralmente alteram o código para avaliar o impacto na aplicação durante sua execução.
Esta opção é uma atividade manual fortemente baseada na experiência.
Em o pior caso, o desenvolvedor deveria considerar as 3414 linhas de código, incorrendo num exercício de tentativa e erro.
Uma alternativa à navegação num programa é apoiar a análise na especificação do software.
Para tanto, é necessário que a documentação do software esteja atualizada.
De posse destes documentos, pode- se navegar por todos os modelos produzidos para analisar o impacto.
Considerando a UML como referência, esta atividade inclui investigar Diagramas Estruturais, como Diagrama de Classes, Objetos, Componentes, Infraestrutura, Pacotes, etc., e Diagramas Comportamentais, como Diagramas de Atividades, Casos de Uso, Máquina de Estados, Sequência, Comunicação, etc..
Uma vez identificado os impactos nos modelos, cada elemento deve estar mapeado adequadamente com as estruturas de código fonte para, então, realizar a análise de impacto.
O uso de matrizes de rastreabilidade convencionais pode apoiar a análise de impacto, relacionando requisitos com estruturas de código.
Devido a a granularidade do caso de uso, é possível identificar que um conjunto de classes esteja relacionado a um requisito.
Não se pode, porém, considerar aspectos mais específicos do comportamento como, no exemplo em questão, o impacto específico em métodos e atributos.
Além de essa restrição, deve- se avaliar se os relacionamentos entre requisitos e classes do sistema estão completos e corretos.
Por último, a análise de dependência é uma alternativa viável quando se refere a código fonte.
Esta análise pressupõe navegar por os métodos através do grafo de chamadas.
Para tanto, é necessário conhecer os métodos e considerar o impacto um a um.
Com o objetivo de melhor entender a dinâmica da análise de dependência e particionamentos, a seguir será apresentada a ferramenta JRipples2 descrita em[ Buc05, Pet09].
Esta ferramenta é utilizada para a análise de impacto e propagação de mudanças onde dependências transitivas são avaliadas.
O método de grafo de chamadas, conforme ilustrado por a Figura 3.6, é utilizado por a ferramenta JRipples para analisar as dependências.
Em este exemplo, são apresentados os tipos de dependência que a classe RecordTimeWorkflowBean possui com outras classes do projeto.
É navegando por o encadeamento dessas dependências que o grafo de chamadas é executado.
A ferramenta JRipples, em oposição às abordagens anteriores, tem cobertura em código e sistematiza a atividade de análise.
A ferramenta analisa a dependência dos métodos do programa e solicita ao usuário abrir cada classe dependente indicada por a ferramenta.
O usuário abre a classe, analisa o código fonte e decide se é necessária alguma alteração.
Caso seja necessário, ele marca na ferramenta a necessidade de revisão e a ferramenta recupera todas as classes relacionadas a este impacto identificado.
Caso contrário, a ferramenta ignora e apresenta as próximas classes dependentes.
O uso da ferramenta para a análise de impacto está ilustrado em detalhes no Apêndice 2.
Seguindo o exemplo apresentado para a mudança proposta (Figura 3.5), são necessárias alterações na classe de controle RecordTimeWorkflowBean e suas interfaces RecordTimeWorkflow e RecordTimeWorkflowHome, bem como na classe de fronteira RecordTimeServlet para validação da data.
Para localizar apenas as classes utilizando a ferramenta JRipples, foi necessário analisar 22 diferentes classes (aproximadamente 30% aproximadamente 50%), correndo o risco de uma análise equivocada devido a a dependência de interpretação do desenvolvedor quanto a o código alterado.
Avaliando as abordagens existentes para a análise de impacto, percebe- se que ainda é uma atividade manual e fortemente baseada na percepção humana, bem como na confiança numa documentação de apoio.
Conforme apresentado, mudanças são decorrentes de necessidades de negócio.
Por a análise do requisito de mudança definido na Figura 3.5, percebe- se que esta necessidade se relaciona a alguns conceitos de domínio apresentados na Figura 3.2.
De entre estes conceitos, pode- se considerar &quot;usuário «(user) e &quot;registro de horas «(time record), bem como algumas propriedades desses conceitos como &quot;registrar «(register) e &quot;horas «(hours).
Modelo de Rastreabilidade A rastreabilidade vem sendo reconhecida como um fator significante para qualquer fase de um processo de desenvolvimento e manutenção de software, contribuindo assim para a qualidade final do produto desenvolvido.
Apesar de o reconhecimento de sua importância, a rastreabilidade geralmente não é utilizada em escala na indústria de software.
A falta de rastreabilidade entre os artefatos pode ser devido a diversos fatores, tais como:
O fato que existem artefatos descritos em linguagens diferentes, incluindo linguagem natural, especificação formal, linguagens de programação e modelagem;
Descreve um sistema computacional em diferentes níveis de abstração;
Muitos processos não determinam a manutenção de elos de rastreabilidade existentes;
E a falta de ferramentas adequadas para apoiar a definição e manutenção de elos de rastreabilidade entre diversos artefatos.
Este fenômeno pode ser resultado da dificuldade de automatizar a geração de relações de rastreabilidade com uma semântica precisa e clara, visto que a maioria dos ambientes e ferramentas assumem que os elos de rastreabilidade devem ser identificados e mantidos manualmente.
A identificação de elos de rastreabilidade é claramente custosa e não satisfatória.
As pesquisas existentes em rastreabilidade de software focam em reduzir o custo e esforço associados à identificação manual através do desenvolvimento de um apoio automático em estabelecer e manter a rastreabilidade entre diferentes artefatos.
Em os últimos anos, ontologias vêm se destacando como o principal formalismo para compartilhar e associar conhecimento entre diferentes modelos.
Esta camada de conhecimento habilita o desenvolvimento de sistemas mais flexíveis e adaptativos fazendo uso dos últimos progressos da comunidade de Web Semântica, incluindo OWL.
A rastreabilidade orientada a ontologias se refere à efetiva integração de uma camada semântica nas existentes abordagens de desenvolvimento de software.
Ontologias podem nivelar a semântica entre diferentes modelos de software, apoiando os desenvolvedores a encontrar informação relevante entre os diferentes produtos de trabalho, tais como diagramas, casos de teste e código fonte.
Em este contexto, a rastreabilidade orientada a ontologias representa a capacidade de rastrear as relações semânticas entre os artefatos ao invés de agrupar- los baseados nos requisitos que motivaram seu desenvolvimento.
Com o uso de uma análise de rastreabilidade orientada a ontologias, dois grandes benefícios podem ser atingidos se comparados às demais técnicas de rastreabilidade:
Com base em um análise de rastreabilidade orientada a ontologias, é possível associar estruturas de código à conceitos do domínio e, com o uso de técnicas de recuperação de informação e análise de dependência, apoiar a identificação do impacto de determinada mudança.
Com base no apresentado, este trabalho propõe um modelo para rastreabilidade orientada a ontologias a ser utilizada durante o desenvolvimento de software.
O objetivo é permitir a identificação, uso e manutenção de relacionamento semântico entre o código fonte e conceitos deste domínio.
A abordagem utiliza uma ontologia como o principal artefato para representação do conhecimento.
Como existem diferentes tipos de ontologia conforme seu nível de generalidade, tais como ontologias de alto nível, de domínio e de aplicação, esta tese sugere o uso de ontologias de aplicação para manutenção do conhecimento.
A motivação na escolha se deve que para sistemas de informação específicos e contextualizados, ontologias de aplicação são utilizadas para representar o papel das entidades da aplicação desempenhado por entidades de domínio.
Ontologias podem manter a semântica entre modelos, ajudando desenvolvedores a encontrar informações relevantes em diferentes produtos de trabalho.
Com o objetivo de obter benefícios a partir de a rastreabilidade orientada a ontologias, torna- se necessário:
Para que seja possível utilizar ontologias para rastreabilidade de artefatos, é necessário definir uma abordagem sistemática para projetar e manter uma ontologia de aplicação durante o desenvolvimento de software.
A introdução de ontologias no processo de desenvolvimento de software foi apresentada em trabalhos anteriores, cujo escopo foi restrito ao Processo Unificado no que compete a atividades e produtos de trabalho.
Este trabalho sugere um novo modelo de processo, desvinculado agora do Processo Unificado e adaptado a um contexto mais genérico de desenvolvimento.
Além de a evolução deste processo, esta tese propõe novos aspectos relacionados com a população automatizada de ontologias.
O processo de modelagem de conhecimento está descrito em detalhes no Apêndice 3 e é organizado em três fases:
Projeto, Manutenção e Verificação.
A ontologia é projetada durante a primeira fase, utilizando como base o Modelo de Domínio, representado por um diagrama de classes conceitual.
Uma vez que a primeira versão do primeiro modelo de conhecimento é considerada completa, o Engenheiro de Conhecimento estará apto para iniciar a fase de Manutenção.
Esta atividade é responsável por manter consistente os elos semânticos entre o conhecimento modelado e os produtos de trabalho do ciclo de vida do software.
Cada modificação na ontologia deve ser verificada durante a fase de Verificação.
Esta fase consiste num processo de controle que analisa cada nova versão da ontologia, procurando por inconsistências com suas prévias versões.
Ontologias podem ser utilizadas para diferentes finalidades, visto que capturam o conhecimento produzido durante o desenvolvimento de software.
Para o propósito deste trabalho, os conceitos presentes em ontologias serão utilizados para indexar estruturas de código fonte.
Uma visão geral do Modelo de Rastreabilidade é apresentado na Figura 3.7.
O modelo proposto recebe como entrada o código fonte da aplicação e uma ontologia do domínio.
O Modelo de Rastreabilidade realiza a transformação da árvore sintática do código fonte numa ontologia de código fonte para ser vinculada aos conceitos do domínio.
Para a população automática, duas perspectivas foram consideradas:
A semântica, que categoriza substantivos, elimina stopwords, normaliza os termos utilizando a WordNet e glossário da aplicação e gera tokens para comparação;
E a léxica, que compara cada token utilizando o algoritmo de stemming.
O resultado desta análise é a ontologia de rastreabilidade, que vincula estruturas do domínio com código fonte.
Linguagens representam formas de comunicação.
Como qualquer outra, as linguagens de programação também possuem essa função mantendo duas formas de comunicação:
Entre humanos e máquinas, através de instruções, e entre humanos e humanos, através de convenções, nomes de identificadores, métodos ou comentários.
Para ilustrar esta situação, considere o código ilustrado por a Figura 3.8, que representa um gerador de identificador para todas as classes de domínio da aplicação.
Esta classe gera o identificador baseado num atributo estático concatenado numérico equivalente a hora corrente.
Existe muito conhecimento sobre sistemas computacionais que estão associados a conceitos do domínio e são capturados de forma implícita.
A maioria dos modelos de rastreabilidade visa formalizar estas relações, associando requisitos a artefatos de software derivados desses requisitos.
Quando se discute código fonte, a granularidade associada a métodos e atributos é muito específica para ser gerenciada tendo como base requisitos.
Além disso, o conhecimento dos desenvolvedores sobre o domínio é formalizado através da transposição de conceitos em nomes de atributos, métodos e comentários.
Este trabalho assume que uma quantidade razoável de conhecimento sobre o domínio está presente no código fonte da aplicação, relacionando o conhecimento formal da solução com o informal do domínio.
O papel de uma ontologia é crucial para formalizar e explicitar este conhecimento, associando- o com estruturas de código.
Em este contexto, é proposto um modelo de rastreabilidade para relacionar e recuperar entidades num sistema tendo como base conceitos do domínio.
Este modelo teve como objetivo desenvolver uma estrutura que seja flexível o suficiente para suportar o mapeamento de quaisquer entidades representadas por artefatos de software, inclusive o código fonte da aplicação.
Para tanto, a proposta apresentada em foi estendida para manter elos de rastreabilidade entre diferentes artefatos e não apenas entre elementos UML.
O recurso &quot;Associacao «é uma classe OWL associada a uma propriedade do tipo objectProperty chamada &quot;ehAssociadaA».
A finalidade dessa propriedade é relacionar elementos de software, tais métodos, atributos, classes, casos de uso, casos de teste, diagramas, etc., com conceitos do domínio, que podem ser classes OWL ou propriedades (object e datatype properties).
Para cada elo de rastreabilidade criado, uma instância (indivíduo) do recurso Associacão é criada ou atualizada, mapeando conceitos do domínio a produtos de trabalho.
A estrutura de associação é exemplificada na Figura 3.10 utilizando a notação da.
A Figura 3.10 apresenta um exemplo de rastreabilidade obtida através da estrutura Associacao.
Java RecordTimeWorkflowBean, no caso uma instância da classe Associacão, com o conceito Timecard existente na ontologia de domínio.
A figura também ilustra o atributo Employee da mesma classe associado ao conceito da ontologia User.
Por sua vez, o conceito User se relaciona com o conceito Timecard na ontologia através da propriedade getCurrentTimecard (objectProperty).
Com esta estrutura, é possível afirmar que ao ser solicitado alguma mudança relacionada ao conceito User, possivelmente a classe Java RecordTimeWorkflowBean será impactada, pois um de seus atributos se relaciona ao conceito User (relacionamento de dependência).
Adicionalmente, o conceito da associação RecordTimeWorkflowBean­ Timecard se relaciona indiretamente através de transitividade ao conceito User através da propriedade getCurrentTimecard.
Observa- se então que existe uma relação entre a classe Java RecordTimeWorkflowBean e o conceito User, tanto por a dependência de classe e atributo, quanto por o mapeamento da ontologia.
Essa dependência conceitual refletida no código fonte será melhor discutida no Modelo de Probabilidade.
Com esta estrutura, é possível mapear outros artefatos de software, como ilustrado por a Figura 3.11.
A ontologia é representada no topo da figura e abaixo diferentes artefatos de software, como código fonte, descrição e diagrama de casos de uso.
Este exemplo não é exaustivo, onde poderiam ser apresentados diversos outros diagramas, glossário, documentação de regras de negócio, etc..
Também não são apresentados neste exemplo possíveis relacionamentos entre as propriedades OWL com classes da ontologia.
Como o objetivo deste trabalho inclui o mapeamento de conceitos do domínio com o código fonte de uma aplicação, torna- se necessário a utilização de uma estrutura para representação desse código.
Para tanto, o modelo proposto utiliza o conceito de árvore de sintaxe abstrata, ou Abstract Syntax Tree (AST), para representar a estrutura do código fonte.
Esta árvore representa um construto de um programa, isto é, a estrutura sintática e semântica de um código fonte, onde cada nó representa um de seus operadores, tais como operações e atributos, e seus filhos os operandos.
A vantagem em se utilizar uma AST para analisar a estrutura do código, e não uma busca textual simples (leitura de arquivo), é a possibilidade de identificar estruturas bem definidas, pois a AST já traz uma semântica implícita sobre o tipo de construto.
Esta estrutura permite identificar, por exemplo, tipos e nomes de atributos, retorno e argumentos de métodos, declaração de variáveis, etc..
Com isso, pode- se ignorar stopwords como modificadores de visibilidade ou palavras reservadas da linguagem (try, catch, for, private, etc.).
A semântica da estrutura do código não poderia ser identificada de forma simples por uma busca textual, a menos que subsidiada por heurísticas.
A abordagem adotada compreende a geração automática de uma ontologia que representa o código fonte da aplicação a partir de a AST, dissociada da ontologia do domínio.
O objetivo de manipular duas ontologias, uma para código fonte e outra para o domínio, diz respeito diferença de suas naturezas.
Existem relações semânticas que fazem sentido no contexto do domínio da aplicação e que não podem ser mapeadas para o código fonte, como, por exemplo, relações de hierarquia de propriedades (subPropertyOf), relacionamento de instâncias e classes num mesmo modelo, etc..
Adicionalmente, relações como implementação de interfaces, polimorfismo, padrões de projeto, entre outros, fazem parte da solução da aplicação que, apesar de poderem ser mapeadas a estruturas do domínio, possuem uma organização característica da solução.
Com o intuito de manter dissociadas estas estruturas, foi definido o uso de duas ontologias.
Apesar de dissociadas, a atividade de população de elos de rastreabilidade entre a ontologia de código fonte e de domínio irá manter os relacionamentos semânticos numa nova ontologia de rastreabilidade.
O modelo desenvolvido inclui a geração automática da ontologia do código fonte utilizando o JDT (Java Development Tools), que é um parser Java fornecido por o Eclipse.
O sistema analisa a AST do código fonte Java e identifica todas as entidades e seus relacionamentos para gerar a ontologia do código fonte.
Este procedimento permite, por exemplo, a identificação de instâncias de classes, isto é, relacionamento entre entidades do código fonte que representam instâncias de outras classes, através da análise estática.
O processo de criação da ontologia do código fonte inclui, para cada classe do projeto, os seguintes procedimentos:
Um exemplo de uma ontologia do código fonte é ilustrado por a Figura 3.12.
Em ela existe a taxonomia de classes, datatype properties e object properties.
Inicialmente a ontologia do código fonte possuirá as classes da aplicação representadas por classes OWL, operações representadas por objectProperties e atributos por datatypeProperties.
Esta representação é obtida através do mapeamento da AST de cada classe da aplicação, que irá gerar um único modelo OWL com as classes da ontologia equivalentes à aplicação.
As relações de referências externas a classes (associações) fazem parte desta ontologia.
A ontologia de rastreabilidade e a ontologia do código fonte podem ser visualizadas no Apêndice 4.
O modelo de rastreabilidade desenvolvido relaciona estruturas do domínio com o código fonte conforme apresentado por a Figura 3.14.
Esta figura ilustra a rastreabilidade de alguns conceitos e propriedades da ontologia com classes e atributos do código fonte.
Como é possível observar na ontologia do código fonte, existem estruturas que não podem ser associadas a conceitos do domínio devido a pertencerem exclusivamente ao escopo da tecnologia empregada como, por exemplo, a classe BasicEntityBean.
Em a solução apresentada, a ontologia de domínio permanece com sua estrutura inalterada e mantém o conhecimento modelado por o especialista do domínio (e não da solução), adicionado apenas elos de rastreabilidade a esta estrutura.
Toda a inferência realizada por o Modelo de Probabilidade será realizada utilizando exclusivamente a ontologia do domínio.
A granularidade da rastreabilidade orientada a ontologias é mais específica se comparada às tradicionais abordagens utilizando requisitos.
Essa especificidade possui peculiaridades:
Quanto mais específico, maior a precisão dos elementos recuperados para um individuo;
E quanto mais específico, maior o esforço para manutenção dessas associações.
Para viabilizar o modelo de rastreabilidade, é necessário o apoio de ferramentas para população da ontologia e criação automática de elos de rastreabilidade.
Estas ferramentas devem apoiar os desenvolvedores a encontrar novos relacionamentos entre o software e sua especificação, conforme ilustrado previamente.
Para tanto, foi desenvolvida uma abordagem baseada em medidas de similaridade entre conceitos da ontologia e especificações de software, apoiando a população da ontologia com indivíduos que mantém os elos de rastreabilidade.
Com o objetivo de viabilizar esta proposta, é necessário fornecer ferramentas para esta definição automática.
Estas ferramentas podem apoiar os desenvolvedores a encontrar novos conceitos e relações, bem como evoluir a ontologia baseada em produtos de trabalho, incluindo elos de rastreabilidade.
Para apoiar especialistas de domínio e engenheiros do conhecimento na complexa atividade de construir e popular ontologias, algumas técnicas de aprendizagem de máquina e recuperação de informação foram aplicadas para descobrir e explicitar a semântica a partir de dados brutos.
O resultado foi descrito em e compreende um modelo e ferramenta para popular uma ontologia de aplicação, capaz de relacionar os conceitos do domínio com estruturas do código fonte.
Aprendizagem de ontologias explora um conjunto de recursos existentes, tais como textos, tesauros, dicionários ou banco de dados, para desenvolver uma ontologia de uma forma semiautomática.
O processo de aprendizagem que estende conceitos da ontologia é chamado de População da Ontologia e é realizado por a seleção de fragmentos de texto, associando aos mesmos conceitos da ontologia.
Em este contexto, o mapeamento linguístico é discutido na literatura como sendo um processo de três passos:
Glossário; Nome dos elementos normalizados em cada categoria, incluindo a recuperação do significado semântico e avaliação parcial do termo (substring).
Além de os três passos descritos, existem diferentes níveis para medir a similaridade entre os termos.
Esta proposta utiliza uma visão em duas camadas, conforme sugerido por, sendo elas:
Léxica, que investiga como os termos se relacionam ao significado, e semântica, que investiga as relações conceituais entre os termos.
Análise de Similaridade Léxica Dois métodos de comparação bem estabelecidos são Distância de Edição, para ponderar a diferença entre duas palavras medindo o número mínimo de caracteres a serem inseridos, excluídos ou atualizados para transformar uma palavra em outra;
E Stemmer, para redução da palavra a sua forma canônica, removendo flexões ou derivações a partir de a sua raiz morfológica.
O trabalho de apresenta um estudo que avalia o algoritmo de Stemmer.
Três diferentes métodos foram comparados, incluindo Distância de Edição, Distância de Edição com Edição Complexa e o Prefixo Stemmer.
O corpora utilizado para avaliação dos métodos foi o das palavras mais comuns do ano de 2000 da Associated Press and Reuter's.
Apesar de a tendência do método de Distância de Edição possuir um melhor desempenho em palavras com erros de digitação e tipográficos, o estudo experimental evidenciou que o Prefixo Stemmer é mais preciso que os demais métodos.
Associado a isso, num típico cenário de desenvolvimento, em o qual cada produto de trabalho passa por diferentes revisões e verificações evitando erros tipográficos, a ideia da utilização do algoritmo de Stemmer é natural.
Baseado nestas evidências, esta proposta utilizou o método de Stemmer para comparação léxica.
Análise de Similaridade Semântica O nível de comparação semântico avalia as relações conceituais entre os termos.
Esta análise considera o mapeamento entre conceitos e não o mapeamento entre termos, realizada por a análise léxica.
O mapeamento entre conceitos consiste em analisar o significado das palavras através de suas relações semânticas obtidas durante a normalização, permitindo a recuperação de termos mesmo com grafias diferentes.
A normalização utiliza um tesauro que avalia as relações semânticas e léxicas entre os termos.
A representa uma grande base de dados léxica que contém verbos, substantivos, adjetivos e advérbios organizados em categorias.
Estas categorias são relacionadas por sentido de relações léxicas e semânticas.
O resultado é uma rede que associa palavras aos seus conceitos.
A análise de similaridade utiliza a WordNet para recuperação de relações de sinonímia entre termos.
Para cada termo na especificação de software, é analisada a similaridade léxica com os conceitos da ontologia e, caso não identifique, é realizada a mesma análise com os termos sinônimos.
Além de a WordNet, é realizada a busca de sinônimos num glossário da aplicação.
Análise de Similaridade e População de Ontologias A solução desenvolvida para a população automática dos elos de rastreabilidade entre ontologia e estruturas do código fonte inclui a categorização, normalização e comparação léxicas e semânticas, ilustrada por a Figura 3.15.
Como a proposta tem o objetivo de identificar os conceitos do domínio a partir de o código fonte do programa, apenas substantivos são considerados para as categorias linguísticas.
O fluxo de processo inclui a avaliação de cada elemento da especificação de software e seus sinônimos, considerando apenas aqueles pertencentes à categoria linguística de substantivos.
A avaliação inclui uma análise léxica de cada elemento da especificação com os recursos da ontologia utilizando o algoritmo de Stemmer.
Em caso de similaridade, este relacionamento é adicionado na matriz de rastreabilidade da ontologia.
Se os termos não são lexicamente equivalentes, o próximo passo é normalizar o termo, reduzindo a redundância.
A normalização inclui a recuperação de termos sinônimos na WordNet e glossário para execução da análise léxica previamente descrita.
A busca por sinônimos não é cíclica e é realizada uma única vez para cada elemento analisado.
A população da ontologia para os elos de rastreabilidade irá comparar cada termo categorizado como substantivo no código fonte (também considerando individualmente nomes compostos) com todas as classes e propriedades da ontologia.
Caso a raiz morfológica (stem) dos termos seja equivalente, o sistema cria uma instância da classe OWL Associação, relacionando a estrutura do código com o elemento da ontologia.
Essa análise deve considerar cada componente de palavras compostas, separadas por caracteres maiúsculos, números e underscore.
Se os termos não são equivalentes, o sistema irá buscar na WordNet e glossário uma lista de sinônimos do termo e analisar com todos os conceitos da ontologia.
O algoritmo que representa este processo é apresentado na Figura 3.16.
Como exemplo de uso deste algoritmo, sugere- se considerar o termo da ontologia de domínio &quot;user».
Comparando lexicamente este termo, cujo stem é &quot;user», com a estrutura de código &quot;Java «&quot;Employee», cujo stem é &quot;empl», percebe- se que não existe similaridade.
A o recuperar da WordNet o sinônimo (Synset Semantic Relation) do termo &quot;user», something)».
O termo &quot;employs «será primeiro categorizado quanto a sua classe gramatical.
Recuperando da WordNet, o mesmo possui o estado de substantivo (noun.
State) como &quot;employment «e &quot;employ», podendo então ser candidato a comparação léxica.
Analisando os stems, identifica- se a equivalência léxica entre o termo &quot;employs», que é sinônimo do conceito &quot;user», com a referência do código &quot;Java «&quot;Employee».
Com isso, gera- se um elo de rastreabilidade entre &quot;user «(domínio) e &quot;Employee «(código fonte).
O modelo de rastreabilidade gera como resultado uma matriz que relaciona conceitos a suas estruturas de código, isto é, um grafo direcionado dos conceitos do domínio para as estruturas de código.
Para gerar tal matriz, deve- se percorrer todas as classes do código fonte da aplicação e verificar se existe equivalência com classes OWL e, para cada classe da aplicação, verificar se seus atributos e métodos possuem equivalência com datatype properties object properties, respectivamente.
A Figura 3.17 apresenta um exemplo do modelo de rastreabilidade relacionando respectivamente a classe OWL ChargeCode, datatype property day da classe Entry e object property get_ hours da classe User e Entry com suas estruturas definidas por a ontologia do código fonte.
As ontologias de domínio populada com os elos de rastreabilidade e de código fonte estão disponíveis no Apêndice 4.
Modelo Probabilístico O Modelo de Rastreabilidade define uma relação de dependência entre conceitos do domínio e o código fonte da aplicação através de uma ontologia de rastreabilidade.
Esta relação pode ser representada por um modelo de recuperação de informação fonte da aplicação.
Com este modelo, é possível identificar se uma classe ou seu membro classes que estiverem vinculadas a um conceito serão recuperadas por esta busca.
O de conceitos, ou suas propriedades, no código da aplicação.
Analisando o modelo vetorial, os índices dos termos (conceitos, no caso) são assumidos como independentes uns dos outros, não considerando seu contexto semântico e a dependência conceitual entre as estruturas de código, um conhecimento valioso que pode ser representado numa ontologia.
Para avaliar essa influência e diagnosticar o impacto de uma mudança de forma quantitativa, foi escolhido utilizar o modelo de raciocínio probabilístico de Redes de Crenças Bayesianas (RCB).
RCB representam um método bem estabelecido na comunidade de Inteligência Artificial para raciocínio em situações de incerteza, utilizando uma estrutura de grafo direcionado representando relacionamentos de causa e efeito e um cálculo de probabilidade para quantificar essas relações, atualizando as crenças dada uma nova informação.
Esta rede representa a união de uma distribuição de probabilidade e um conjunto de variáveis, consistindo numa parte qualitativa (grafo direcionado) e outra quantitativa (modelos de dependência).
Para um domínio em particular, os vértices do grafo representam variáveis enquanto as arestas direcionadas descrevem as probabilidades de transição entre estas variáveis.
Utilizando um modelo RCB associado a uma matriz de rastreabilidade, viabiliza- se a predição do impacto de uma mudança num sistema quando certos conceitos da aplicação são identificados.
A RCB é um grafo cujas arestas conectam os nodos sem formar ciclos diretos, isto é, um grafo acíclico direcionado do conceito para o código fonte.
Os nodos representam variáveis aleatórias e as arestas representam a dependência direta entre essas variáveis.
Este trabalho assume como dependência os elos de rastreabilidade entre estruturas de código e conceitos da ontologia.
Para ilustrar melhor o modelo proposto, a Figura 3.19 apresenta uma visão geral.
Como entrada deste modelo, a matriz de rastreabilidade, representada por a ontologia de rastreabilidade, e um requisito de mudança são fornecidos.
O requisito de mudança é então analisado léxica e semanticamente em busca de referência a conceitos do domínio.
Uma vez identificados, os mesmos são utilizados para rastrear as estruturas do código fonte da aplicação e realizar o cálculo de relevância.
Este cálculo é realizado utilizado RCB incluindo duas análises distintas:
Probabilidade independente de cada conceito da aplicação e código fonte, utilizando o algoritmo de classificação PageRank, e probabilidade dependente, utilizando o algoritmo de frequência TFIDF e análise da dependência conceitual fazendo uso de grafos de chamada.
Segundo, a definição de uma RCB para um domínio particular envolve as seguintes tarefas:
Identificação de variáveis relevantes e seus valores, equivalendo aos nodos e seus estados;
Identificação e representação do relacionamento entre estes nodos (o que significa quais nodos devem ser conectados);
E (3) a parametrização da rede, que consiste em a (a) definição de probabilidades iniciais para cada nodo e (b) em probabilidades condicionais associadas a cada aresta.
Em a teoria, variáveis podem ser discretas ou contínuas.
Os modelos criados nesse trabalho foram construídos utilizando exclusivamente variáveis discretas, isto é, o espaço amostral se restringe a um conjunto finito de classes e propriedades das ontologias.
É importante relembrar que este trabalho utiliza dois conjuntos distintos de variáveis que equivalem ao domínio e contradomínio dos elos de rastreabilidade.
O domínio (D) consiste no conjunto finito de conceitos do domínio do problema e o contradomínio (CD) os recursos extraídos a partir de o código fonte da aplicação.
A imagem (Im) representa, neste contexto, todas as estruturas do código fonte que foram rastreadas a conceitos.
Cada nodo possui uma probabilidade condicional calculada dinamicamente conforme os conceitos identificados por o requisito de mudança, quantificando assim a relevância de cada elo de rastreabilidade.
Caso sejam identificados alguns conceitos referentes a uma solicitação de mudança, então é provável que as estruturas de código dependentes se tornem instáveis e suscetíveis a mudanças.
Antes de representar esta dependência, é importante representar os possíveis estados que os conceitos do domínio e código fonte podem assumir.
Para tanto, foi definido que ambos os conjuntos possuem dois estados que representam quando um nodo é estável ou instável (possivelmente será modificado devido a solicitação).
Os dois estados que um nodo pode assumir são:
Estável (E):
O nodo não é identificado como impactado;
Instável: O nodo é identificado como passível de mudança.
Para ilustrar, considere que na Figura 3.20 apenas o conceito Conc01 foi identificado como instável, então apenas a classe Class01 será considerada como instável.
Cada elo de rastreabilidade é representado por a associação de um nodo pertencente ao domínio com um nodo pertencente à imagem.
Essa organização foi definida para prevenir ciclos, fazendo com que RCB pudesse ser aplicado de forma válida, isto é, num grafo acíclico direcionado.
Quando o modelo de rastreabilidade é formado, ele é sempre baseado em nodos interconectados com a estrutura fundamental:
Esta estrutura representa as relações direcionadas a partir de os conceitos do domínio para o código fonte da aplicação.
Estas relações podem definir diferentes situações:
Um conceito x é considerado estável se não for identificado numa requisição de mudança;
Um conceito x é considerado instável se for identificado numa requisição de mudança;
Um código fonte y é considerado estável se y pertence ao conjunto Im e não existe nenhum conceito x instável pertencente a D que seja rastreável para y ou se Y pertence ao CD e não pertence a Im;
Um código fonte y é considerado instável se e somente se y pertence ao conjunto Im e existe algum conceito x instável pertencente a D que seja rastreável para y.
As relações são definidas e quantificadas utilizando uma matriz de probabilidade baseada em nodos de D, Im e suas relações.
A intenção em usar RCB é definir a relevância de uma estrutura de código em particular baseado num requisito de mudança.
Para tanto, a RCB provê uma probabilidade condicional de observar um evento y dado x ­ uma probabilidade que o evento y ocorra (código potencialmente impactado) uma vez que o evento x tenha ocorrido (conceito identificado como relevante).
Em particular, essa classificação utiliza o seguinte:
A parte essencial de um classificador é o cálculo da probabilidade observada de um evento y relacionado ao evento x, também conhecida como probabilidade posterior e referenciada por.
O cálculo é realizado por o teorema de Bayes:
O classificador de NaiveBayes pode fornecer uma medida de quão provável uma estrutura de código y será impactada quando submetida uma consulta incluindo o conceito x.
Este classificador não é utilizado para efetivamente classificar, mas produzir uma medida de relevância na análise do impacto.
Para tanto, serão discutidas as probabilidades marginal e condicional.
Cálculo da Probabilidade Marginal P Cada nodo raiz possui uma probabilidade previamente definida como Estabeleceu- se que cada nodo raiz é definido por conceitos da ontologia pertence a, tal que.
A arquitetura definida para o modelo de probabilidade deve estimar e atribuir as probabilidades para cada um dos nodos, que indica o quão relevantes são os conceitos entre si, independente de suas dependências com o código fonte (y).
Para calcular a probabilidade marginal de forma independente de y, optou- se por o algoritmo de classificação PageRank, originalmente utilizado por o motor de busca do Google.
Este algoritmo avalia a relevância, ou popularidade, de um nodo de um grafo com base nas dependências atribuídas ao mesmo.
Maiores detalhes podem ser obtidos no Assumindo que classes nas ontologias são associadas por propriedades do tipo object properties e que relacionam domínio e imagem, é possível identificar a relevância de um conceito dentro de o seu universo por a quantidade de conceitos que o referência e que são referenciados por ele.
Essa medida fornece a probabilidade em a qual um determinado conceito pode ser modificado baseado em suas dependências.
Para a obtenção da relevância usando PageRank, primeiro é necessário definir o grafo em o qual esta relevância será extraída.
Percorre- se então todas as object properties da ontologia e inclui- se no grafo do PageRank uma aresta cuja origem é o domínio (domain) da propriedade e destino sua imagem (range).
O valor alfa (ajuste de primitiva) utilizado para o algoritmo é 0.14 conforme se estima ser o valor utilizado por o Google.
Observando a ontologia apresentada na Figura 3.13, o resultado da aplicação do PageRank pode ser observado na Tabela 3.4.
O algoritmo de classificação PageRank também foi utilizado para o cálculo da probabilidade marginal de y, utilizando o mesmo valor alfa descrito anteriormente.
Como o código fonte também é representado por uma AST, o grafo do código fonte é gerado navegando por esta AST e identificando todos os tipos utilizados por cada classe ou interface do código fonte.
Toda a análise de P (y) é independente dos conceitos da ontologia.
Para a geração do grafo necessário ao PageRank, para cada classe do código fonte do projeto, verifica- se:
Cada atributo de instância ou de classe (estático), verificando se o seu tipo pertence a alguma classe do projeto.
Cada método, verificando:
Assinatura, avaliando tipo de retorno e tipo dos argumentos;
E Corpo, avaliando a declaração de atributos em (a) blocos de (d) todos os subblocos do método, recursivamente.
RemoteException, CreateException try TimecardHome thome $= (TimecardHome) initialContext.
Lookup (&quot;TimecardBean&quot;);
A Figura 3.21 ilustra um trecho de código que irá gerar uma aresta no grafo de cálculo do PageRank da classe EmployeeBean para as classes sublinhadas na figura BasicEntityBean, Timecard e TimecardHome.
O resultado do cálculo de relevância de todo o código fonte se encontra na Tabela 3.5.
Duas análises complementares foram definidas para o cálculo de probabilidade condicional das classes do código fonte dado um conceito:
TFIDF e análise de Dependência Conceitual (DC).
A medida TFIDF é uma análise que verifica a ocorrência do conceito em todas as estruturas da classe, enquanto a DC representa uma análise da distância entre os conceitos impactados comparada com a distância entre as classes da aplicação associadas a estes conceitos.
O cálculo da probabilidade condicional é dado por a média geométrica das duas medidas devido a pertencerem a amostras diferentes e definido conforme:
O algoritmo TFIDF descrito no Capítulo 2 foi utilizado para calcular a probabilidade condicional de alterar a classe do código fonte y caso o conceito x seja instável.
Este algoritmo combina a definição de frequência de termos e a frequência inversa dos documentos para ponderar cada classe y no projeto em questão.
Em este trabalho, a frequência de termos é definida por a quantidade de referências a conceitos da ontologia identificados como impactados e que existem em cada classe do código fonte.
A frequência inversa dos documentos avalia quantas classes do código fonte fazem referência aos conceitos impactados.
Para exemplificar, considere que uma classe do código fonte possui referência a 10 conceitos da ontologia, sendo 3 equivalentes ao valor de x.
Em este caso, TF (termfrequency) representa 3/10 $= 0,3.
O IDF (inverse document frequency) é gerado percorrendo todas as classes da aplicação buscando por o conceito x.
Caso existam 75 classes e o conceito x apareça em 10 dessas classes, então TFIDF é calculado como log $= 0,87.
O valor de TFIDF, neste caso é 0,3* 0,87 $= 0,26.
Os procedimentos para busca de conceitos em cada AST (classe do código fonte) incluem percorrer todas as classes, identificando respectivamente a equivalência dos itens abaixo com os conceitos da ontologia.
Atributos: Nome;
E Tipo.
Métodos: Nome;
Tipo de retorno;
Argumentos, considerando nome ou tipo;
E Declarações de variáveis no corpo do método, e subblocos, recursivamente.
Diferente da definição do grafo para o algoritmo do PageRank referente a P (y), toda a busca por conceitos em classes do código fonte agora é realizada utilizando o algoritmo de análise de similaridade descrito na Seção 3.2.2, pois não se consegue garantir apenas a equivalência léxica dos conceitos da ontologia com as estruturas do código fonte.
Cada referência ao conceito encontrado em cada classe ou interface é considerada para o cálculo de TF.
Para ilustrar, considere o código parcial da interface TimecardHome ilustrado por a Figura 3.22.
CreateException, RemoteException;
FinderException, RemoteException;
Para o código fonte da Figura 3.22, são identificadas as estruturas de dados:
Timecard, employeeId, findAllForEmployee, employeeId, findTimecard, employeeId, Timecard, timecardId, que fazem referência aos conceitos Employee e Timecard.
A quantidade de referências ao conceito Timecard neste exemplo é 4 (Timecard, findTimecard, Timecard, timecardId) de uma quantidade total de 8, então TF de Timecard nesta interface é 4/8 $= 0,5.
O cálculo de IDF percorre todas as classes da aplicação contabilizando a ocorrência de determinado conceito de forma similar a busca de frequência de termos.
Em o exemplo apresentado, num universo de 75 classes, o conceito Timecard aparece em 11 classes (ExportFile, RecordTimeWorkflowBean, Test, AddEmployeeWorkflowBean, RecordTimeWorkflow, TimecardHome, TimecardBean, ExportTimeEntriesApplication, Employee, EmployeeBean, TextEntryFrame).
O cálculo de IDF é log (75/11) $= 0,83.
O valor TFIDF do conceito Timecard para interface TimecardHome é 0,5* 0,83 $= 0,41, o que representa a relação entre a frequência de vezes que o conceito Timecard é identificado na classe TimecardBean (TF) e o número total de classes que este conceito é referenciado (IDF).
A segunda análise foi utilizada para aumentar a acurácia da recomendação.
Para tanto, foi aplicada a análise de dependência para calcular a distância conceitual entre as estruturas do código.
Esta análise é similar as tradicionais análises estáticas do código fonte, porém considerando agora uma perspectiva semântica.
A Dependência Conceitual é uma métrica proposta por esta tese, resultado da combinação do modelo de Sobreposição Taxonônica (Taxonomic Overlap), que avalia a sobreposição de taxonomias definidas por diferentes ontologias, com a análise de dependência, definida por grafos de chamada no código fonte.
Essa métrica é utilizada para avaliar a distância entre as diferentes classes da aplicação (dependência) com relação a os conceitos do domínio instáveis.
A análise de Dependência Conceitual (DC) é calculada por a seguinte equação:
Onde representa a menor distância entre dois conceitos medida por a quantidade de object properties e, a menor distância entre duas classes associadas a conceitos diferentes e medida por suas dependências.
A dependência de classe é considerada quando uma classe possui uma referência a outra classe do mesmo projeto declarada em seu corpo.
Esta análise não considera a distância entre classes pertences ao mesmo conceito e só é empregada quando mais de um conceito da ontologia é considerado instável.
Para exemplificar, considere a Figura 3.23.
Supondo que os conceitos Conc01, Conc03 e Conc04 foram identificados como instáveis por uma mudança.
Considere também que Conc01 está relacionado a Class01 e Class02, Conc03 está relacionado a Class04 e Conc04 está relacionado a Class05.
Em este exemplo, percebe- se que a distância entre Conc01 e Conc03 é 2;
Conc01 e Conc04 é 3;
Class01 e Class04 é 3;
e Class01 e Class05 é 4.
A distância conceitual da Class01 considerando Conc01 pode ser calculada por as suas distâncias relativas do conceito base com os demais impactados, bem como da classe base com as demais rastreadas, conforme:
Para calcular DC, o modelo primeiro avalia se existe mais de um conceito impactado.
Se sim, percorre- se cada par de conceitos impactados e recupera- se suas respectivas classes do código fonte.
O cálculo da distância entre conceitos é realizado encontrando o menor caminho entre o conceito base e os demais conceitos impactados considerando todos os objects properties.
A análise de dependência do código fonte é realizada identificando se na classe base (y) existe referência a uma classe rastreada ao segundo conceito.
Caso não encontre, a mesma busca é realizada em todos os tipos encontrados na segunda classe (segundo nível) e assim por diante, recursivamente.
A identificação dos tipos nas classes considera:
Atributos: Tipo;
E Tipo declarado em subbloco do tipo (declaração estática).
Métodos: Tipo de retorno;
Tipo dos argumentos;
E Declarações de variáveis no corpo do método, considerando exceções (bloco try-- catch), Para ilustrar, considere os seguintes conceitos instáveis:
Timecard e RecordTime.
As seguintes estruturas foram identificadas como impactadas:
Conc01: Timecard o Classes:
TimecardHome, TimecardBean, Timecard.
Conc2: RecordTime o Classes:
RecordTimeWorkflowBean, RecordTimeWorkflow, ExportTimeEntriesApplication.
RecordTimeWorkflowHome, RecordTimeServlet, A distância conceitual de Employee e RecordTime é 1 devido a o relacionamento dos conceitos por a object property hasCurrent (Timecard hasCurrent RecordTime).
O próximo passo é calcular a menor distância entre todas as classes associadas a Conc01 e Conc02.
O resultado é apresentado na Tabela 3.6.
Caso não seja encontrada a dependência conceitual no código fonte, optou- se por atribuir um redutor para 10% na distância para o cálculo da probabilidade condicional.
Este valor foi obtido através de uma calibragem empírica com o objetivo de aumentar a probabilidade de determinada classe caso a equivalência seja detectada.
O resultado final pode ser observado na Tabela 3.7.
Esta medida é calculada para todas as classes do código fonte dada a definição de um conceito como instável, isto é, que suas estruturas serão possivelmente impactadas por o requisito de mudança.
Foi identificado que a distribuição da amostra pode possuir uma dispersão significativa dos valores, gerando um conjunto de artefatos com baixa probabilidade de impacto.
Para eliminar a elementos com baixa relevância, foram excluídos àqueles abaixo de o desvio padrão.
Revendo o cenário motivacional, a solicitação de mudança apresentada na Figura De posse destes conceitos e com a ontologia e código fonte informados, o sistema:
O resultado resumido da execução pode ser visto na Figura 3.24.
Conforme apresentado no cenário motivacional, a inspeção do código fonte apontou que, para o requisito de mudança, seria necessário alterar as classes RecordTimeWorkflowBean e suas interfaces RecordTimeWorkflow e RecordTimeWorkflowHome, bem como na classe de fronteira RecordTimeServlet para validação da data.
Todas estas classes foram identificadas como relevantes por o modelo apresentado.
Observa- se também que a classe ExportTimeEntriesApplication foi definida como relevante, o que reforça a necessidade de inspeção do resultado por um especialista.
O motivo de sua relevância é a grande frequência que os conceitos instáveis foram identificados em seu construto.
Ainda assim, este modelo de análise de impacto proporciona uma redução das classes inspecionadas de 75 para 5, o que equivale analisar apenas 6,67% da quantidade total classes do sistema, com uma taxa de erro, neste exemplo, de 1,33% do escopo total.
Considerações sobre o Capítulo A análise de impacto em software é uma tarefa de estimar as partes do software que serão impactadas quando uma mudança proposta é realizada.
A informação de análise de impacto pode ser utilizada em diferentes momentos durante o desenvolvimento de software como o planejamento, execução e acompanhamento de mudanças, ou mesmo a análise de efeitos colaterais (no caso de testes de regressão).
Uma forma tradicional de estimar impacto é usando informações estáticas do código fonte (isto é, suas dependências) para identificar as partes do software que podem ser afetadas por determinadas mudanças.
Por exemplo, inserindo mudanças no código e buscando as suas dependências usando grafos de chamada.
Estas técnicas tradicionais são geralmente imprecisas e tendem a superestimar os efeitos de uma mudança.
O modelo de análise de impacto proposto combina uma série de abordagens e técnicas visando melhorar a acurácia na recuperação de estruturas relevantes frente a um requisito de mudança.
Esta combinação foi resultado de uma série de análises empíricas sobre o objeto de estudo.
Para tanto, foram definidos dos submodelos:
Modelo de Rastreabilidade e Modelo Probabilístico. O primeiro visa rastrear conceitos do domínio com estruturas do código fonte e o segundo calcular a probabilidade de mudança frente a um requisito.
De forma geral, este modelo necessita de três informações para realizar a análise de impacto e cálculo da influência dos conceitos:
O projeto da aplicação e seu código fonte;
A ontologia do domínio;
E um texto livre com a requisição de mudança (RdM).
O modelo de análise de impacto realiza então a carga do código fonte da aplicação, organizados em AST e traduzidos numa ontologia OWL, e a ontologia do domínio, organizado num modelo OWL.
Após a carga, é instanciado o Modelo de Rastreabilidade que percorre todas as estruturas do código fonte, populando a ontologia de domínio com referências a instâncias da ontologia do código fonte.
Este mapeamento percorre todas as classes do código fonte, analisando seus atributos e métodos, bem como suas estruturas aninhadas.
Este procedimento realiza a busca de referências a conceitos do domínio no código fonte utilizando um analisador de similaridade léxico e semântico.
Este analisador realiza a categorização, normalização e comparação de cada um dos termos de busca, fazendo uso de PLN.
O resultado deste modelo é uma ontologia de domínio rastreada a estruturas do código fonte.
Com base no RdM e no Modelo de Rastreabilidade, o Modelo de Probabilidade é instanciado.
Este modelo identifica inicialmente no RdM os conceitos na ontologia do domínio através do mesmo analisador de similaridade léxico e semântico descrito.
Para o cálculo da probabilidade, foi utilizado o modelo de recuperação de informação RCB que é organizado em probabilidade independente e condicional.
Para o calculo de probabilidade independente dos conceitos do domínio e código fonte, foi utilizado o algoritmo de classificação PageRank com uma configuração semelhante ao motor de busca do Google.
Para a probabilidade condicional, foi utilizada uma análise combinada de TFIDF, para análise de frequência dos conceitos em cada código fonte, e DC, para identificação da distância conceitual através da sobreposição de grafos de chamadas do código fonte com a organização estrutural dos conceitos da ontologia.
Como resultado deste modelo, é apresentado um cálculo de probabilidade associado a cada classe do código fonte, indicando a relevância de cada estrutura do código, incluindo atributos e métodos que potencialmente serão impactados por o requisito de mudança.
Este capítulo detalha a arquitetura da ferramenta SEmantics (Semantic Software Engineering) desenvolvida para verificar a viabilidade do Modelo de Análise de Impacto sugerido.
Este detalhamento será estruturado conforme os Modelos de Rastreabilidade e Probabilístico apresentados no Capítulo 3, visando relacionar a parte conceitual com sua implementação.
A implementação da ferramenta SEmantics foi realizada como uma extensão (plugin) do Ide Eclipse utilizando a linguagem de programação Java.
O propósito de estender uma Ide (Integrated Development Environment) foi integrar o modelo de análise de impacto proposto com uma ferramenta tradicional de desenvolvimento e manutenção do código fonte.
Antes de decidir por qual Ide, foi necessário realizar um estudo de viabilidade técnica que incluiu analisar Apis de manipulação de código fonte, ontologias, recuperação de informação e processamento de linguagem natural.
Os resultados desses estudos foram inicialmente publicados em e posteriormente descritos em.
Quanto a o escopo de implementação, neste primeiro momento a ferramenta apresenta suporte apenas para aplicações que utilizam a linguagem de programação Java.
Esta restrição é devido a a dependência das bibliotecas utilizadas para o parser do código fonte.
Durante a implementação da ferramenta, foram observados aspectos para permitir a extensão dos modelos relacionados a diferentes produtos de trabalho.
O modelo de rastreabilidade foi estruturado para que fosse flexível o suficiente para processar documentos UML (XMI -- XML Metadata Interchange), casos de uso e de teste descritos em linguagem natural, entre outros, expandindo o escopo de código fonte desenvolvido neste trabalho.
Foram realizados testes de viabilidade com diferentes produtos de trabalho conforme descritos em, porém a primeira versão da ferramenta SEmantics manteve apenas elos de rastreabilidade entre ontologia e código fonte.
Por último, acredita- se que, apesar de as diversas otimizações realizadas, ainda serão necessários aperfeiçoamentos com relação a performance e desempenho.
Para a primeira versão da ferramenta, é improvável que se consiga chegar numa arquitetura ideal em termos de flexibilidade e desempenho.
Estas e outras limitações serão melhor discutidas no Capítulo 6, que apresenta as considerações finais e os trabalhos futuros.
Arquitetura SEmantics A ferramenta SEmantics foi definida como uma arquitetura de referência visando possíveis extensões.
O objetivo é estabelecer um vocabulário comum para servir como base de discussão em futuras implementações.
Para tanto, a descrição da arquitetura SEmantics concentra- se principalmente na definição das responsabilidades dos elementos que a compõe, suas Apis e aspectos estruturais e comportamentais.
Esta seção apresenta uma visão geral da arquitetura desenvolvida através de diferentes perspectivas da aplicação.
Será apresentada uma visão estrutural, que descreve a infraestrutura básica e bibliotecas, e comportamental, que descreve os principais aspectos dinâmicos associados a atividade de análise de impacto.
Para o desenvolvimento, foram utilizadas as seguintes bibliotecas e Apis:
Buscas precisas por declarações, referências a pacotes, tipos, métodos e campos no escopo do workspace de um projeto Java.
Os complementos JDT fornecem as seguintes Apis:
Platform Debugger Architecture); C. Text:
Que fornece editores Java com uma série de funcionalidades, como cores conforme sintaxe e palavras reservadas, formatação de JavaDoc, assistente e seleção de código, etc.;
Package Explorer, visão hierárquica de código, outline, etc.;
E e.
Core: Utilizada para construção de código Java e navegação em sua árvore AST, como fragmentos de pacotes, unidades de compilação, classes, tipos, métodos e atributos, além de indexação para busca e computação de tipos hierárquicos;
Java para construção de aplicações para Web Semântica.
Jena fornece uma coleção de ferramentas bibliotecas Java de apoio ao desenvolvimento de aplicações que inclui:
XML, N-tripples e formato Turtle;
B. Uma API para manipulação de ontologias OWL e RDF;
RDF ou OWL como origem de dados;
D. Armazenamento em disco de grande quantidade de triplas RDF;
As Apis descritas acima foram utilizadas respectivamente para:
Documento, sua conversão para AST e navegação em sua estrutura;
Foi definida uma arquitetura centralizada para a ferramenta SEmantics utilizando um modelo clássico baseado em camadas.
Esta arquitetura é chamada monolítica, pois é estruturada em camadas lógicas que obrigatoriamente residem no mesmo nó físico, isto é, a Ide onde o plugin está sendo executado, e seus componentes não podem ser distribuídos.
A arquitetura da aplicação é organizada em camadas lógicas conforme o diagrama de pacotes ilustrado na Figura 4.1.
A Figura 4.1 apresenta a visão lógica da arquitetura, ilustrando a dependência entre os pacotes em nível de camadas.
Entre as camadas da aplicação, destacam- se três equivalentes ao padrão de projeto MVC (Model-View--Controller), identificadas por os pacotes view, control e dao.
Estas camadas são responsáveis por tratar questões relativas a interface com o usuário (view), via plugin do Eclipse ou linha de comando, regras para análise de impacto (control), incluindo os controladores de rastreabilidade e probabilidade, e persistência (DAO ­ Data Access Object), responsável por o acesso e parser do código fonte e ontologia a partir de o sistema físico de arquivos.
Camada de Visão.
A camada de visão é definida por o pacote view e ilustrada por a Figura 4.2.
Esta camada é responsável por o acesso a funcionalidade de análise de impacto.
Duas classes principais são definidas nesta visão:
SEmanticsView, responsável por o acesso a funcionalidade de análise de impacto através de linha de comando, SEmanticsPlugin, invocada por a Ide para carregar os painéis de comunicação com o usuário.
A classe SEmanticsView viabiliza o acesso a funcionalidade de forma direta, através dos métodos:
Startup, responsável por inicializar contextos e controladores;
A classe SEmanticsPlugin é outra alternativa de acesso a aplicação SEmantics através de um plugin do Eclipse.
Ela é responsável por inicializar os contextos e controles de painéis com base no arquivo de configuração plugin.
Xml. O arquivo de configuração do plugin define as classes responsáveis para a entrada e saída de dados referentes a análise de impacto.
Este painéis incluem uma nova entrada no menu do Eclipse e ícone no painel default para acesso à funcionalidade, bem como um painel na parte inferior com os resultados da execução.
A Seção 4.2 apresentará o uso da ferramenta, onde cada componente será explicado em detalhes.
Camada de Controle.
A camada de controle é definida por o pacote control e ilustrada por a Figura 4.3.
Esta camada é responsável por o acesso a funcionalidade de análise de impacto e possui quatro classes principais:
SEmanticsController, TraceabilityModelController, ProbabilityModelController e SimilarityController.
A classe SEmanticsController é a classe base da camada de controle.
Ela é responsável por instanciar os modelos (TraceabilityModelController) rastreabilidade (ProbabilityModelController), (SimilarityController) de bem utilizada como por estes probabilidade análise dois de similaridade modelos.
SEmanticsController recebe da camada de visão as referências aos arquivos do projeto Java e ontologia e utiliza suas classes de persistência (DAO) para recuperação dos modelos.
Estes modelos são convertidos por os DAOs em objetos DTOs (Data Transfer Object -- objetos de transferência de dados) para definição da matriz de rastreabilidade e cálculo de relevância.
TraceabilityModelController, que mantém a referência a JWNL para acesso a WordNet.
Este acesso permite a categorização de palavras, identificando substantivos, e recuperação de sinonímia entre termos.
Ela também recebe controlador SimilarityController responsável por a análise léxica e semântica entre termos, que será descrito em detalhes em seguida.
De entre os principais métodos da classe TraceabilityModelController, destaca- se o match, responsável por o mapeamento do código fonte com a ontologia.
Este método percorre cada unidade de compilação do código fonte, identificando classes, atributos e métodos para realizar sua rastreabilidade com classes, datatype properties e object properties da ontologia, respectivamente.
TraceabilityModelController é popular a ontologia de domínio com as referências as estruturas de baixo nível do código fonte (atributo, método e classe).
A classe ProbabilityModelController é responsável por a identificação da relevância dos elos de rastreabilidade da ontologia de rastreabilidade.
Ela possui o método getImpact responsável por a recuperação do impacto, que inclui:
Definir o cálculo da probabilidade marginal P de cada classe na ontologia utilizando o algoritmo PageRank;
Definir a probabilidade marginal P (y) de cada classe do código fonte utilizando o algoritmo PageRank;
E definir a probabilidade condicional P (y| x) por a classificação de cada aresta do nodo, utilizando a análise TFIDF e DC.
A o final, o cálculo de probabilidade do modelo RCB é realizado com base nas probabilidades previamente identificadas.
Camada de Persistência e Dados. A Figura 4.4 ilustra a relação das camadas de persistência (definida por o pacote dao) e de dados (definida por o pacote dto).
As classes DTO (Data Transfer Object) são referenciadas em todas as camadas, conforme Figura 4.1, porém optou- se por apresentar as relações estruturais associadas às classes DAO para ilustrar sua instanciação.
O pacote dão inclui as classes OntModelDAO, JavaProjectDAO e JavaAstDAO.
O pacote dto possui as classes OntModelDTO, OntClassDTO, JavaAstDTO, JavaClassDTO e JavaProjectDTO.
A classe OntModelDAO recupera e realiza o parser da ontologia informada por a camada de visão, criando uma instância do OntModelDTO.
Essa ontologia é carregada utilizando a classe OntModel da API Jena e suas classes, datatype e object properties são identificadas para facilitar posteriores buscas e comparações.
O processo de carga da ontologia será melhor descrito por a visão comportamental.
A classe JavaProjectDAO é responsável por fazer a carga e parser dos arquivos do código fonte.
Para tanto, a mesma recupera todos os arquivos com extensão java a partir de o caminho informado por a camada de visão, incluindo subdiretórios.
Os arquivos são armazenados numa lista de File e recuperados por uma classe utilitária chamada FileListing.
Em seguida, a classe JavaAstDAO realiza o parser de cada arquivo numa unidade de compilação AST utilizando a API JDT, carregando numa lista de JavaAstDTO.
Esse DTO inclui todas as compilações de classes do código fonte, incluindo classe, atributos e métodos.
Uma vez carregada todas as classes do projeto Java, é gerada a ontologia do código fonte com base nos atributos e métodos de cada classe.
Uma visão comportamental de alto nível da aplicação SEmantics pode ser ilustrada por o diagrama de atividades da Figura 4.5.
O fluxo de processo para a análise de impacto inicia quando o usuário informa o endereço do código fonte e da ontologia, bem como um texto livre com a requisição de mudança.
Em este momento, a aplicação é instanciada identificando qual classe de visão iniciou o processo (linha de comando ou plugin) para carga adequada dos painéis de interface.
A configuração inicial inclui a instanciação dos DAOs OntModelDAO e JavaProjectDAO para as respectivas cargas dos modelos nos DTOs OntModelDTO e JavaProjectDTO.
Esta configuração inicial também instancia os controles de rastreabilidade (TraceabilityModelController), probabilidade (ProbabilityModelController) e similaridade (SimilarityController), bem como as classes utilitárias ConceptualDependency, RankOntology RankSourceCode.
A carga da ontologia inclui a leitura e parser do arquivo OWL num objeto OntModel da API Jena e sua atribuição num objeto OntModelDTO, que categoriza a priori classes e propriedades OWL para otimizar buscas em comparações posteriores.
Esta carga também inclui os mecanismos necessários para geração dos elos de rastreabilidade utilizando a classe OWL Associacao.
A carga das classes do código fonte ocorre percorrendo a estrutura de diretório informada e identificando arquivos com extensão Java.
Em seguida, a classe JavaAstDAO realiza o rastreamento de cada arquivo identificado, gerando uma AST correspondente para ser armazenada num objeto JavaAstDTO.
Este objeto também categoriza o nome da classe, seus atributos e métodos para otimizar as buscas em comparações posteriores.
Cada elemento é recuperado percorrendo a árvore do código e identificando o nome do método e o nome e tipo dos atributos.
Por fim, a ontologia do código fonte é gerada para fins de rastreabilidade.
A Figura 4.6 apresenta o diagrama de sequência que ilustra o processo descrito.
A Figura 4.7 ilustra o processo executado para cada classe do código fonte, em o qual é realizada uma análise de similaridade com seus membros, considerando se existe equivalência da classe Java com alguma Classe OWL, de um atributo Java com algum datatype property OWL ou método Java com object property OWL.
Para cada similaridade identificada, é gerado ou atualizado um elo de rastreabilidade na ontologia de domínio.
Toda a comparação entre estruturas de código e recursos da ontologia, seja no modelo de rastreabilidade ou dependência, realizada utilizando classe SimilarityController, conforme ilustrado por a Figura 4.8.
A análise ocorre entre duas strings, ignorando getters e setters, normalizando e gerando tokens de todas as strings para posterior comparação.
As comparações consideram uma perspectiva léxica por o stem dos termos e uma perspectiva semântica, categorizando elementos como substantivos e recuperando seus sinônimos da WordNet e glossário da aplicação.
A classe SimilarityController é responsável por receber duas strings e verificar se são similares.
Esta análise inclui:
Para cada elemento das duas listas (associadas aos dois termos da comparação), (a) comparar os stems dos termos, (b) os stems dos sinônimos dos termos substantivos recuperados da WordNet e (c) os stems dos termos recuperados do glossário da aplicação.
Após geração da matriz de rastreabilidade, classe ProbabilityModelController avalia a relevância de cada elo conforme Figura 4.9.
A classe SEmanticsController recebe como argumento a matriz de rastreabilidade e um texto livre com o requisito de mudança.
A partir deste texto, o sistema identifica as classes e propriedades da ontologia consideradas instáveis e propensas a mudanças.
Com a matriz de rastreabilidade e recursos da ontologia impactados, o sistema inicia o cálculo de probabilidade.
Inicialmente é realizada a classificação utilizando o algoritmo PageRank de ambos os modelos:
Ontologia e código fonte.
O grafo da ontologia é gerado com base em seus object properties.
O grafo do código fonte é realizado percorrendo cada classe do projeto e verificando suas dependências com outras classes do mesmo projeto.
Esta análise ignora classes definidas em bibliotecas como, por exemplo, &quot;ArrayList», &quot;Set», etc..
A análise de dependência é realizada em:
Logo em seguida, é realizado o cálculo de frequência (TF) que um conceito aparece numa classe utilizando o controle comparando a quantidade total de conceitos que existem nesta mesma classe.
Após é identificado quais os conceitos que aparecem em cada classe do projeto Java (IDF).
A Figura 4.10 ilustra o fluxo para o cálculo da probabilidade condicional TFIDF.
Após o cálculo de TFIDF, a dependência conceitual (DC) é calculada com base no grafo de chamada do código fonte e na distância entre dois conceitos na ontologia conforme descrito por a Figura 4.11.
A distância conceitual avalia a equivalência entre dois conceitos na ontologia relacionados por object properties, comparado a distância entre as classes do código fonte associadas a estes conceitos.
O resultado, juntamente com o valor de TFIDF, é utilizado para o cálculo da probabilidade condicional de cada classe no código fonte impactada por um conjunto de recursos da ontologia.
A o final, a classe de SEmanticsController retorna para a camada de visão um relatório com as classes, atributos e métodos relevantes para a mudança e suas respectivas probabilidades de impactado.
Guia de Uso Após a apresentação dos aspectos técnicos relacionados a solução da ferramenta SEmantics, é necessário fornecer um guia com a sequência de passos que devem ser executados para a utilização do modelo de análise de impacto desenvolvido.
Para a instalação do plugin no Ide Eclipse, basta copiar o arquivo br..
Pucrs. Semantics_ 1.0.0.
Jar exportado do projeto SEmantics para a pasta de plugins da Ide e reiniciar.
Uma vez carregado, os componentes do plugin são apresentados conforme Figura 4.12.
As marcações, e da Figura 4.12 representam formas alternativas de acessar a funcionalidade SEmantics.
A marcação apresenta o painel em o qual o resultado da análise de impacto é fornecido ao usuário.
Esta estrutura apresenta uma lista de classes hierarquizadas por métodos e atributos que são identificados como potencialmente impactados.
Para ilustrar um exemplo de uso, foi importado o projeto Timecard, explorado no é apresentado na Figura 4.13.
A Figura 4.13 apresenta a tela após a solicitação de análise de impacto.
Note que o projeto importado possui uma ontologia chamada Timecard.
Owl conforme marcador.
A o solicitar a análise de impacto, a janela &quot;SEmantics analysis «é apresentada para seleção de projeto, ontologia e descrição de mudança.
Todos os projetos no workspace do Eclipse estão disponíveis para seleção ao clicar em Browse do marcador.
Uma vez que o projeto foi escolhido, é possível selecionar qual ontologia deste projeto será utilizada para análise de impacto através da modal de seleção ilustrada por o marcador.
Esta modal percorre todo o projeto e lista todos os arquivos OWL.
A Figura 4.14 apresenta os resultados da análise, informando as classes potencialmente impactadas, sua probabilidade de impacto e seu caminho no projeto em questão.
É possível expandir a árvore das classes caso seja identificado um método ou atributo potencialmente impactado, conforme ilustrado por a figura.
A o se clicar na classe, método ou atributo, os mesmos serão abertos para edição.
Considerações sobre o Capítulo Esse capítulo descreveu os principais aspectos relacionados ao desenvolvimento da ferramenta SEmantics, que implementa o Modelo de Análise de Impacto proposto.
Foram descritas duas visões:
A estrutural e a comportamental.
A visão estrutural descreveu as Apis utilizadas, bem como detalhes de implementação.
Foram discutidas as camadas da aplicação, os padrões de projeto adotados e a organização geral das classes.
O propósito de cada componente da arquitetura também foi explorado em detalhes, bem como sua relevância para o Modelo de Análise de Impacto.
Toda a dinâmica associada à execução da análise de impacto foi apresentada por a visão comportamental, incluindo detalhes e fluxos dos algoritmos.
Os processos dos Modelos de Rastreabilidade e Probabilístico foram descritos, bem como os eventos relacionados a análise de similaridade léxica e semântica.
Este capítulo também apresentou um guia de uso da ferramenta através da descrição do plugin desenvolvido para o Ide Eclipse.
O cenário motivacional explorado por o Capítulo 3 foi utilizado para ilustrar a dinâmica de uso da ferramenta.
O modo de operação consiste em abrir no Eclipse um projeto Java e selecionar acesso a funcionalidade SEmantics Analysis.
A partir deste momento, o usuário deve informar um projeto existente em seu workspace, uma ontologia pertencente a este projeto e a descrição da mudança.
Informado estes dados, o plugin atualiza um painel na ferramenta com a relação de classes e suas respectivas probabilidades de impacto.
Estas classes e seus membros ficam disponíveis para inspeção através de uma seleção direta, facilitando os procedimentos de manutenção.
O modelo de análise de impacto em código fonte usando ontologias e recuperação de informação apresenta duas perspectivas para sua avaliação.
A primeira está relacionada com a necessidade de desenvolver um produto de software, demonstrando sua viabilidade conforme apresentado no Capítulo 4.
A segunda perspectiva está relacionada com a avaliação de sua eficiência e eficácia quando comparadaa abordagens manuais que representam o estado da prática, conforme discutido por.
Conforme apresentado em Lindvall, a análise manual do código fonte representa uma das estratégias mais comuns para analisar o impacto de mudanças.
Para avaliar o modelo de análise de impacto proposto, se sugere um estudo comparando os resultados obtidos por a ferramenta SEmantics e por abordagens convencionais.
Em este contexto, experimentos são determinantes para uma boa avaliação, provendo uma disciplinada, sistemática, quantificada e controlada forma de avaliar as atividades desempenhadas por humanos.
A literatura provê algumas abordagens baseadas numa estratégia experimental para avaliar um processo onde o fator humano é considerado.
Em, as seguintes abordagens são definidas para avaliação de processos, produtos e recursos:
Análise das características, pesquisa de opinião (survey), estudo de caso e experimentos.
Esse último representa o tipo de estudo mais controlado, geralmente realizado em laboratórios.
Em esta abordagem, os valores das variáveis independentes (entradas do processo) são manipulados para se observar as mudanças nos valores das variáveis dependentes (saídas do processo).
A o término da execução do experimento, os resultados são analisados, interpretados, apresentados e, por fim, empacotados.
Em, é observado que as diferenças entre os métodos de pesquisa são refletidas em suas escalas.
Experimentos tendem a ser pequenos, envolvendo um número reduzido de pessoas ou eventos devido a sua natureza que exige maior controle.
Pode- se pensar em experimentos como &quot;pesquisas num ambiente restrito».
Por outro lado, estudos de caso geralmente abordam um projeto típico em vez de tentar obter informações sobre todos os possíveis casos;
Eles podem ser considerados como &quot;pesquisas num ambiente típico».
O objetivo deste trabalho é investigar métodos alternativos para a análise de impacto, identificando relações como &quot;mais preciso que», &quot;maior revocação que «ou &quot;mais esforço que».
É possível, assim, isolar variáveis que determinam esta relatividade do resto do processo e manipular- las, avaliando os resultados a partir de os tipos de combinações possíveis.
Para isso, torna- se necessário um controle sobre as variáveis independentes do experimento.
Frente a o apresentado, optou- se por a utilização de experimentos para avaliar empiricamente o modelo proposto por este trabalho.
Para conduzir o experimento, o processo aplicado utilizou como referência as propostas de, de maneira complementar.
A avaliação do experimento foi apoiada por.
Este trabalho inclui três experimentos distintos.
O primeiro experimento está relacionado à precisão e revocação dos elementos recuperados por o modelo de análise de impacto proposto.
O segundo está relacionado à análise de similaridade, que corresponde a um subprocesso chave da automação da análise de impacto.
Essa análise é central em todo o modelo e corresponde ao processo que vincula código fonte aos conceitos da ontologia.
A automação da análise de similaridade viabiliza o modelo de rastreabilidade utilizado para definição de seus elos, e o modelo probabilístico, utilizado para análise de probabilidade condicional que inclui TFIDF e DC.
Devido a a criticidade da análise de similaridade, a mesma foi considerada relevante para uma avaliação minuciosa.
O terceiro experimento foi executado para caracterizar o esforço, avaliando o tempo necessário para analisar e implementar uma mudança utilizando ou não a ferramenta SEmantics.
Experimento sobre medida F associada à Análise de Impacto A abordagem GQM (Goal-Question-Metric) foi utilizada para a definição deste experimento.
O objetivo geral foi comparar o resultado da análise de impacto manual, realizada por engenheiros de software, com a obtida por a execução da ferramenta SEmantics.
A partir deste objetivo, derivou- se a seguinte questão:
A abordagem automática para análise de impacto utilizando SEmantics (aut) identifica o mesmo conjunto de classes do código fonte que a análise realizada manualmente por engenheiros de software (man)?
O objetivo da medição foi analisar o impacto identificando, numa manutenção de software, quais as classes do código fonte relevantes para uma mudança.
Para tanto, a métrica associada à questão previamente definida foi a medida F, que equivale a média harmônica entre a precisão, que mede quantos documentos relevantes foram recuperados, e a revocação, que mede a proporção de documentos relevantes recuperados, conforme descrito na Seção 2.3.4.
A definição das classes relevantes para a requisição de mudança foi definida por um especialista através da efetiva implementação das mudanças no código fonte da aplicação.
O objetivo do estudo foi definido como:
Comparar a análise de impacto manual com a análise de impacto automatizada por a ferramenta SEmantics, Com o propósito de avaliar a eficiência de ambas as abordagens, Com foco na precisão e revocação, Sob o ponto de vista de um programador, Em o contexto de uma implementação de um requisito de mudança durante a manutenção evolutiva de software.
Foi escolhido o contexto de uma universidade para a condução do experimento.
Embora diversos autores argumentem sobre a necessidade de conduzir experimentos em ambientes realistas, semelhantes aos encontrados na indústria, esta cooperação demanda custos e riscos não previstos no escopo desta pesquisa.
O processo para o experimento foi a abordagem em a qual o conjunto de participantes executou o experimento num ambiente controlado.
Este experimento não foi executado durante o desenvolvimento de software industrial, isto é, ele foi off-line.
Os participantes do experimento foram alunos do Programa de Pós Graduação em Ciência da Computação da PUCRS.
Com relação a realidade do sistema, foi utilizado um software de código aberto chamado Memoranda, desenvolvido em Java para a gestão e organização pessoal de atividades, tarefas e recursos.
O objetivo desta escolha foi a utilização de um sistema real, desenvolvido por terceiros sem a intervenção do pesquisador, e que seja diferente do exemplo utilizado durante a definição do trabalho, desacoplando assim a solução aqui proposta de um problema particular (como o sistema de cartão ponto).
Para o experimento, foi definida informalmente a hipótese de que a identificação de classes do código fonte resultantes da análise de impacto automatizada é igual a prática manual desenvolvida por engenheiros de software.
Com base nessa definição informal, viabiliza- se a formalização das hipóteses do experimento e suas medidas para avaliação.
A Hipótese Nula definida foi que, utilizando um mesmo requisito de mudança (RdM), a medida F de ambas as abordagens (aut e man) é a mesma se comparadas a definição do impacto por um especialista que efetivamente implementou as modificações no código fonte, onde:
Sendo que precisão) e (medida F) representa a média harmônica entre (média da revocação) do conjunto de classes relevantes e recuperadas por os sujeitos (participantes) do experimento.
A definição de Onde Sendo a quantidade de sujeitos se dá conforme:
Para o experimento, assumiu- se como variável independente a técnica para análise de impacto e como variável dependente a corretude na definição do impacto da mudança no código fonte utilizando a medida F. A seleção dos sujeitos participantes foi por conveniência (não probabilística) e incluiu dezoito (18) profissionais graduados e com experiência prévia em desenvolvimento de software.
De entre os princípios genéricos de projeto do experimento, foi avaliada a obstrução, em a qual nem todos os participantes possuem conhecimento equivalente em Java.
Para minimizar o efeito da experiência, a análise do impacto foi realizada em duplas, em as quais os participantes foram agrupados por conveniência e quota por o condutor do experimento, utilizando como critério suas experiências na linguagem Java.
O tipo de projeto do experimento foi um fator (medida F) com dois tratamentos (aut e man).
Para a realização dos testes das hipóteses no contexto do experimento (um fator e dois tratamentos aleatórios), a literatura sugere o teste de significância chamado Teste T para duas amostras independentes, caso seja realizado um teste paramétrico, ou Mann--Whitney, caso o teste seja não paramétrico.
A definição do teste a ser aplicado ocorrerá após a análise da normalidade e variância dos dados obtidos por a execução do experimento.
A instrumentação do experimento incluiu como objeto o código fonte da aplicação Memoranda, a Ide Eclipse para análise do código e três requisições de mudanças.
Como guia para man, a equipe foi motivada e treinada, sendo apresentada a arquitetura do software e como importar e executar a aplicação.
Também foi fornecido um documento sobre como proceder durante o experimento.
As métricas foram coletadas em formulários preenchidos por os participantes (Relatório de Análise de Impacto) conforme Figura 5.1.
Para a abordagem aut, a análise de impacto foi executada utilizando a ferramenta SEmantics.
As requisições de mudanças foram definidas em língua Portuguesa para man e em língua Inglesa para aut, conforme Tabela 5.1.
Foi utilizada a língua Portuguesa para man com o objetivo de facilitar o entendimento da mudança, eliminando o viés da língua estrangeira.
A língua inglesa foi utilizada em decorrência do parser realizado por o modelo.
A instrumentação completa está disponível no Apêndice E, junto com a ontologia utilizada para abordagem aut.
Com relação a análise da validade interna, alguns critérios foram considerados, tais como:
Histórico, em a qual a data de aplicação do experimento foi criteriosamente definida para evitar períodos em os quais os participantes pudessem sofrer influências externas, como final de semestre;
Maturação, para motivar positivamente os participantes durante o treinamento;
Seleção dos grupos, utilizado para nivelar o conhecimento dos participantes através de treinamento e agrupamento conforme experiência;
E difusão, para incentivar os participantes a não interagirem entre os grupos, aliado ao policiamento do condutor do experimento.
A validade externa incluiu a interação da seleção, através da identificação de participantes com perfil apto ao tratamento do experimento, apresentando, em sua maioria, conhecimento prévio de programação e experiência na indústria.
A validade de construção considerou a explicação operacional do experimento, visando maturar a forma em a qual a análise de impacto foi conduzida e sua extração de dados.
A expectativa do condutor do experimento também foi considerada, fazendo com que o mesmo não exerça influência sobre as variáveis envolvidas ou sobre o material elaborado.
A validade da conclusão considerou as seguintes perspectivas:
Manipulação dos dados:
Como os dados resultantes do experimento serão manipulados por o pesquisador, é possível que os mesmos sofram algumas variações, tal como o coeficiente de significância para validação dos resultados;
Confiabilidade das medidas:
Esta perspectiva sugere que medidas subjetivas possam ser influenciadas por o pesquisador.
Em nossa proposta, as medidas foram objetivamente definidas, não dependendo do critério humano;
Confiabilidade na implementação dos tratamentos:
Consiste no risco em que diferentes participantes possam implementar de forma distinta os processos estabelecidos por o experimento.
Este risco não será evitado, visto que não se pode interferir no caráter subjetivo de analisar os impactos de determinada mudança;
Configurações do ambiente do experimento:
Consiste nas interferências externas do ambiente que podem influenciar os resultados durante a execução do experimento.
O experimento foi executado num laboratório isolado, onde foi proibida a interação externa como celulares, saídas, etc.;
Heterogeneidade aleatória dos participantes:
A escolha de diferentes participantes com diferentes experiências pode exercer um risco na variação dos resultados.
Para preparar a execução do experimento, os participantes consentiram com o experimento, pois se os participantes não concordam com os objetivos da pesquisa ou não tem conhecimento sobre o experimento, corre- se o risco de que sua participação não ocorra em encontro aos objetivos.
Durante a experimentação, a preparação dos participantes forneceu o embasamento necessário sobre o experimento, esclarecendo os objetivos e metas almejadas.
Resultados sensitivos também foram considerados, pois é possível que o resultado obtido por o experimento seja influenciado por questões pessoais, como a sensibilidade dos participantes por estarem sendo avaliados.
Foi adotada uma postura de anonimato em toda a execução do experimento.
Com relação a a instrumentação, todas as definições apresentadas foram criteriosamente estabelecidas durante a execução, incluindo o ambiente e infraestrutura.
Foi realizado um treinamento específico de como proceder a análise de impacto, contextualizando os objetivos, a técnica, a motivação e o procedimento operacional.
A execução do experimento foi realizada num curto período de tempo (3 horas), em o qual o pesquisador esteve envolvido em todos os detalhes da execução.
Durante este período, foi realizada a coleta dos dados por os próprios participantes, utilizando formulário específico.
Esta opção é justificada, pois o pesquisador não poderá se envolver na definição dos dados resultantes do experimento.
Outro critério considerado foi o anonimato, onde os nomes dos participantes não foram divulgados.
Durante a execução do experimento, o responsável ficou a disposição dos participantes para o esclarecimento das dúvidas relacionadas ao processo, abstendo- se de influência técnica nas respostas.
A primeira análise apresentada diz respeito à classificação das escalas das variáveis definidas no experimento, apresentada na Tabela 5.2.
Com esta classificação, é possível determinar as operações que podem ser aplicadas sobre as variáveis.
A Figura 5.2 representa o gráfico de linhas da medida F associada a cada abordagem de análise de impacto.
Todas as análises apresentadas neste experimento foram realizadas utilizando o pacote estatístico SPSS.
Conforme apresentado, as variáveis dependentes estão caracterizadas na escala razão, o que permite o cálculo da normalidade e homocedasticidade necessárias para definir o tipo de teste das hipóteses (paramétrico ou não paramétrico).
Para a análise da hipótese, os dados foram caracterizados, visualizando tendências centrais e dispersões.
Posteriormente, sugere- se a eliminação de dados anormais ou incertos, que distorcem a integridade da conclusão, através da redução do intervalo de dados.
Por último, foi realizado o teste das hipóteses que compreende a avaliação estatística dos dados até certo nível de significância.
O nível de significância adotado (p- value) para todos os testes foi de 5%.
O p- value compreende o menor nível de significância com que se pode rejeitar a hipótese nula.
Uma análise inicial da distribuição é eficiente para avaliar o comportamento das amostras.
Foi utilizado o gráfico de dispersão boxplot, apresentado na Figura 5.3, para identificação dos outliers.
Conforme apresentado na Figura 5.3, a variável medida F possui outliers moderados e portanto nenhum sujeito foi eliminado da amostra para o teste de hipótese.
Outra observação sobre o gráfico de dispersão é a variabilidade entre as duas amostras, em a qual a dispersão da abordagem manual é muito pequena.
A próxima etapa consiste em identificar se os dados seguem uma distribuição normal.
Para se avaliar a normalidade, é definida uma hipótese nula e uma hipótese alternativa, conforme:
H0: A distribuição é normal;
H1: A distribuição não é normal.
Existem duas formas para se avaliar a distribuição normal dos dados, que compreendem o Teste de Kolmogorov--Smirnov e o Teste de Shapiro-Wilk.
O primeiro é utilizado para identificar a normalidade em variáveis com pelo menos 30 valores e o segundo em variáveis com menos de 50 valores.
A Tabela 5.5 apresenta os testes de normalidades para a amostra utilizando o Teste de Shapiro-Wilk.
Com base na Tabela 5.5, observa- se que a significância dos dados do teste de Shapiro-Wilk é superior, em ambas as amostras (man e aut), ao nível de significância definido.
Com esta informação, não há indícios para rejeitar a hipótese nula sobre a distribuição da normalidade, conseguindo assim o primeiro requisito para utilização de teste paramétrico para duas amostras independentes.
O segundo requisito requer a análise da homocedasticidade, tornando necessário analisar a variância das duas amostras.
Com este objetivo, definem- se duas hipóteses:
H0: As variâncias são iguais;
H1: As variâncias não são iguais.
O teste das hipóteses acima é realizado com a significância obtida diretamente do Teste de Levene.
O Teste de Levene é usado para testar se k amostras têm a mesma variância.
A Tabela 5.6 apresenta os resultados obtidos para este teste.
Com base na Tabela 5.6, verifica- se que o nível de significância para variâncias iguais é superior ao nível de significância definido.
Com esta informação, não se consegue rejeitar a hipótese nula para variâncias, conseguindo o segundo requisito para utilização do teste paramétrico.
Conforme definido no planejamento do projeto do experimento, o teste indicado para avaliação das hipóteses é o Teste T para duas amostras independentes.
Conseguese validar sua utilização por ser um teste paramétrico, em o qual seus requisitos foram explicitamente atendidos.
Com base na declaração das hipóteses, têm- se:
Aut $= man man\&gt; aut aut\&gt; man O critério para rejeição de H0 em favor de H1 é:
H1: (man\&gt; aut):
Rejeita- se H0 se t0\&gt; t, n+ m-2, onde t0:
É o valor t obtido através da aplicação do Teste T. O Teste T para duas amostras independentes foi aplicado neste contexto e o resultado está apresentado na Tabela 5.7.
Com base na Tabela 5.7, obtemos o valor de t0 e, com base no Anexo A, obtemos o valor de t, n+ m-2.
Como t0 t, n+ m-2, então não se consegue rejeitar a man\&gt; hipótese nula a um nível de significância de 5% em favor de H1:
O critério para rejeição de H0 em favor de H2 é:
H2 :(aut\&gt; man):
Rejeita- se H0 se t0\&gt; t, n+ m-2 O Teste T foi aplicado neste contexto e o resultado está descrito na Tabela 5.8.
Com base na Tabela 5.8, obtemos o valor de t0 e o valor de t, n+ m-2.
Como t0\&gt; t, n+ m-2, consegue- se rejeitar a hipótese nula a um nível de significância de 5% em favor de H2:
Por as análises aut\&gt; man.
O ponto fundamental para a automação da análise de impacto é a habilidade de identificar quais as estruturas de código fonte impactadas por uma mudança.
Este experimento foi realizado com o intuito de avaliar o quão próximo de a identificação de código relevante é o resultado obtido por a automação do modelo de análise de impacto em código fonte usando ontologia proposto comparado a uma estratégia manual de análise impacto.
Para esta avaliação, foi utilizada a medida F definida por a média harmônica entre precisão, que mede quantas classes relevantes foram recuperadas, e revocação, que mede a proporção de classes relevantes recuperadas.
Os dados relativos à medida F obtiveram uma distribuição normal com uma diferença estatisticamente insignificante entre suas variâncias (homocedasticidade).
Diante de esta circunstância, foi executado o Teste Paramétrico T que evidenciou a validade da hipótese alternativa que estabelece que a medida F da análise de impacto automática é maior que a obtida manualmente por engenheiros de software.
Como resultado, pode- se comprovar estatisticamente a hipótese de que a precisão e a revocação da análise de impacto automática definida por a ferramenta SEmantics é maior que a recuperada por a análise de impacto manual.
Um ponto fundamental para análise de impacto é a análise de similaridade que automatiza a identificação dos elos de rastreabilidade, população automática da ontologia e análise da probabilidade condicional necessária para ponderar cada elo.
Esta análise de similaridade corresponde a habilidade de identificar quais conceitos do domínio estão presentes numa especificação de software.
Este mapeamento não é trivial e idealmente deveria ser executado por um especialista do domínio, discernindo e relacionando a perspectiva conceitual das demais perspectivas de um sistema de informação.
O esforço associado a esta identificação é significativo e não é esperado que, durante o processo de desenvolvimento de software, pessoas relacionem manualmente estes conceitos a cada elemento que constituem os artefatos, principalmente um especialista do domínio que desempenha um papel chave neste processo.
Para avaliar aspectos relacionados a precisão e revocação da ferramenta desenvolvida para análise de similaridade comparada a uma avaliação humana e não especialista, que tipicamente caracterizaria desenvolvedores de software, foi realizado um experimento controlado conforme descrito a seguir.
Experimento sobre Análise de Similaridade associada à Análise de Impacto A abordagem GQM também foi utilizada para a definição do estudo.
Tendo como base a possibilidade de melhorar o processo de identificação de conceitos em artefatos de software, o objetivo deste estudo foi avaliar a precisão e revocação da ferramenta que analisa a similaridade entre termos, comparando- a com a atividade humana não especialista de capturar e associar estas mesmas estruturas.
A partir deste objetivo, derivou- se a seguinte questão:
A precisão e revocação da definição de elos de rastreabilidade entre artefatos e conceitos utilizando aut (análise de similaridade automática) é a mesma que utilizando man (análise de similaridade manual) se comparadas à definição realizada por um especialista do domínio?
A métrica utilizada para responder esta questão foi a medida F definida no experimento anterior.
O objetivo de estudo foi definido como:
Comparar a identificação manual de conceitos obtido por engenheiros de software não especialistas no domínio da aplicação, com o mapeamento automático obtido por o processo proposto de análise de similaridade, Com o propósito de avaliar a eficiência de ambas as abordagens, Com foco na precisão e revocação, Sob o ponto de vista de um especialista do domínio, Em o contexto da definição de elos de rastreabilidade entre conceitos do domínio e artefatos de software.
O experimento foi conduzido numa universidade, utilizando um ambiente controlado e off-line.
A população definida para o experimento foi formada por estudantes dos cursos de graduação e pós-graduação em computação.
A amostra da população foi não probabilística, escolhida por quota e conveniência.
A questão previamente definida originou a seguinte Hipótese Nula:
Utilizando um mesmo artefato, a medida F de ambas as abordagens é a mesma se comparada a identificação dos conceitos por um especialista, onde:
H0: F aut $= F man H1:
F man\&gt; F aut H2:
F aut\&gt; F man As mesmas métricas definidas para o primeiro experimento foram utilizadas para avaliar a análise de impacto associada aos modelos de rastreabilidade e probabilístico.
As considerações sobre as análises de validade interna, externa, de construção e conclusão também foram aplicadas nesta execução do experimento.
A instrumentação do experimento utilizou seis casos de uso (CdU) descritos na língua inglesa relacionado ao sistema de Cartão Ponto discutido no Capítulo 3.
Como o objetivo da rastreabilidade é identificar conceitos em artefatos, a motivação em utilizar casos de uso e não o código fonte foi por a facilidade na interpretação e identificação dos conceitos por os participantes nessa especificação.
Como casos de uso são escritos em linguagem natural, o viés da questão técnica associada ao conhecimento da linguagem de programação seria minimizado, focando essencialmente na eficiência da definição de elos.
A relação entre as dimensões de conceitos do domínio e casos de uso foi definida utilizando um formulário de mapeamento, conforme Figura 5.4.
As variáveis independentes foram aut e man e a variável dependente foi a medida F obtida em cada descrição de caso de uso.
O projeto experimental utilizado foi de um fator com dois tratamentos.
Para a abordagem aut, a análise de similaridade do Modelo de Rastreabilidade foi executada utilizando os artefatos descritos.
Para a abordagem man, foi oferecido um treinamento, apoiando e motivando os participantes para a execução do experimento.
Antes da execução do experimento, as variáveis independentes e dependentes foram cuidadosamente preparadas, atentando para o consenso com o experimento, resultados sensitivos e anonimato.
A execução do treinamento teve como objetivo contextualizar o estudo, motivar os indivíduos e demonstrar o modo operacional para execução do experimento.
O experimento possuiu 13 participantes que receberam a descrição de seis casos de uso e a relação completa dos conceitos do domínio referentes aos mesmos casos de uso.
Foi solicitado que cada participante interpretasse os requisitos e avaliasse quais seriam os conceitos presentes nestas especificações.
Após a execução do experimento, obtiveram- se os resultados apresentados a seguir.
Para este experimento, as variáveis independentes foram caracterizadas na escala nominal e as dependentes na escala razão.
A escala razão permite avaliar a distribuição dos dados e a definir qual teste de hipótese pode ser aplicado (paramétrico ou não paramétrico).
O nível de significância adotado (p- value) para todos os testes foi de 5%.
A execução do experimento produziu os dados apresentados no Apêndice G e sumarizados por a Tabela 5.9.
A análise inicial teve como objetivo avaliar a distribuição dos dados.
Para tanto foi gerado o gráfico de linhas conforme Figura 5.5 e o gráfico de boxplot conforme Figura 5.6.
Mesmo identificando o outlier 07 (equivalente ao CdU 4 de man) na Figura 5.6, nenhum sujeito foi retirado da amostra para o teste de hipóteses.
Em uma amostra de apenas seis pontos, a eliminação de um sujeito não deve ser feita apenas com base em gráficos de boxplot, além de que nenhuma evidência externa foi identificada durante a execução do experimento que justificasse esta exclusão.
O próximo passo foi identificar se os pontos seguem uma distribuição normal, onde as duas hipóteses relacionadas a normalidade definidas no experimento anterior foram consideradas.
A Tabela 5.10 apresenta os testes de normalidades para a amostra utilizando o Teste de Shapiro-Wilk.
Com base na Tabela 5.10, observa- se que a significância dos dados do teste de Shapiro-Wilk é inferior em aut com relação a o nível de significância definido.
Sendo assim, há indícios para rejeitar a hipótese nula e, consequentemente, não se pode aplicar um teste paramétrico para avaliação das hipóteses.
Optou- se por aplicar o teste Mann--Whitney, para duas amostras independentes, por se tratar de uma alternativa não paramétrica para o Teste T. Com base na declaração das hipóteses, sugere- se:
H0: Não há diferença entre as médias (aut $= man) H1:
Há diferença entre as médias (aut man) O resultado do teste Mann--Whitney foi aplicado sobre as amostras e está apresentado na Tabela 5.11.
Como o grau de significância associado (Sig.
Assimpt.) é 0,034 e é menor que a significância assumida de 0,05, pode- se rejeitar H0.
Frente a os resultados apresentados para a variável medida F, existe diferença de média entre a abordagem manual e automática.
Por a análise estatística dos dados, consegue- se recuperar duas informações:
A distribuição da medida F não é normal, o que implica na execução de testes não paramétricos;
Utilizando o teste Mann--Whitney, conseguiu- se verificar que existem diferenças entre as médias das duas amostras aut e man.
Utilizando o teste de Mann--Whitney, conseguiu- se apenas rejeitar a hipótese nula, isto é, medida F de ambas as abordagens é diferente se comparadas ao mapeamento de um especialista.
Porém, como existe diferença entre as médias das abordagens, sugerese analisar melhor esta relação, conforme a Tabela 5.12.
Comparando os dados apresentados na Tabela 5.12, observa- se que a média descritiva da medida F na amostra automática é maior do que na manual.
Esta evidência é pontual para este experimento e em outras execuções há a possibilidade estatística desta relação mudar.
O ponto fundamental para a identificação do mapeamento automático é a habilidade de identificar quais conceitos do domínio estão presentes numa especificação de software.
Em a especificação textual, foram considerados durante o experimento todos os problemas inerentes da língua natural, como ambiguidade, termos implícitos, pronomes, etc..
Este mapeamento não é trivial e idealmente deveria ser executado por um especialista do domínio, discernindo e relacionando a perspectiva conceitual das demais perspectivas de um sistema de informação (no caso deste estudo, a perspectiva comportamental foi avaliada).
Este experimento foi conduzido com o intuito de avaliar o quão próximo é o resultado obtido por a análise de similaridade automática e manual entre conceitos do domínio e artefatos de software.
Em este experimento, buscou- se o estado da prática em diversas de fábrica de software no Brasil, utilizando documentação especificada na língua inglesa e avaliada por pessoas não nativas neste idioma.
Para apoiar os indivíduos na execução de suas tarefas, foi permitido o uso de dicionários ou consulta exclusiva ao condutor do experimento com relação a os termos da linguagem.
Uma das evidências encontradas neste estudo foi à perda de revocação da abordagem manual na descrição a qual os conceitos não estavam explicitamente definidos.
Como exemplo, tem- se o Caso de Uso 04, em a qual foram identificados apenas 2 conceitos num universo de 8.
Existe a descrição neste caso de uso de termos sinônimos a alguns conceitos, e esta sutileza não foi percebida por alguns dos participantes.
De a mesma forma, algumas características foram evidenciadas para o caso automático, como a diminuição da precisão por a identificação de falsos positivos.
Caso existam termos não relacionados a especificações, referenciados através de sinônimos ou explicações descontextualizadas, este mapeamento é identificado por a ferramenta.
Durante a especificação da análise de similaridade, não foi estabelecido um ponto de corte para avaliar falsos positivos.
Este ponto de corte foi implementado especificamente no modelo probabilístico ao final do processo completo de análise de impacto.
O fenômeno de falsos positivos também foi identificado no caso de o mapeamento manual, onde indivíduos inferiram relacionamentos que efetivamente não fazem parte do contexto da especificação.
Como resultado do estudo experimental, foi possível evidenciar que, comparado o mapeamento automático com o efetuado por um especialista, ambas as abordagens geram amostras com diferença estatística entre suas médias.
Através da análise descritiva, conseguiu- se identificar que a média geral da abordagem automática possui uma medida F maior que a média da abordagem manual.
Considerando a média parcial da abordagem automática e manual, verifica- se que a medida F obtida por a ferramenta é maior em 83% das especificações consideradas, isto é, em 5 das 6 especificações avaliadas, o que traz bons indícios da aplicabilidade da análise em consideração.
Experimento sobre Esforço associado à Análise de Impacto A abordagem GQM também foi utilizada para a definição do estudo.
O objetivo foi avaliar o esforço necessário para analisar e implementar uma mudança numa aplicação utilizando ou não a ferramenta SEmantics.
Este objetivo derivou a seguinte questão:
&quot;o esforço para a implementação de uma mudança utilizando aut (abordagem automatizada utilizando SEmantics) é o mesmo que utilizando man (abordagem manual)?».
A métrica adotada foi o tempo gasto em minutos em cada atividade.
O objetivo de estudo foi definido como:
Comparar o tempo necessário para analisar e implementar mudanças, Com o propósito de avaliar o desempenho da ferramenta SEmantics, Com foco no esforço, Sob o ponto de vista de um programador de software, Em o contexto de uma manutenção evolutiva.
O experimento foi conduzido numa universidade utilizando um ambiente controlado e off-line.
A população definida para o experimento foi formada por graduandos de computação finalizando o curso de linguagem de programação orientada a objetos (Java).
A amostra da população foi não probabilística, escolhida por quota e conveniência.
A questão previamente definida originou a seguinte Hipótese Nula:
O esforço necessário para a implementação de uma mudança num software utilizando a ferramenta SEmantics é igual ao esforço necessário utilizando uma abordagem manual.
O esforço foi avaliado como sendo o tempo gasto em minutos para implementar a mudança em cada abordagem, isto é, a diferença entre o tempo final e o tempo inicial, onde:
I. Taut: Representa a variação de tempo gasto em minutos para implementação da mudança utilizando a ferramenta SEmantics;
II. Tman:
Representa a variação de tempo gasto em minutos para implementação da mudança utilizando a abordagem manual.
H0: Taut $= Tman H1:
Tman\&gt; Taut H2:
Taut\&gt; Tman A instrumentação utilizou uma aplicação Web na linguagem Java disponível em JavaFree.
Org, responsável por gerenciar locações de veículos.
Trata- se de uma aplicação composta por 105 classes e estruturada numa arquitetura MVC que faz uso de servlets, JSPs, DAOs e classes de controle.
A aplicação é responsável por gerenciar dados como veículos, locação de veículos, clientes, estados, cidades, usuários do sistema, bem como marcas, categorias e modelos de veículos.
A ontologia utilizada para ferramenta SEmantics foi extraída do modelo de domínio disponível por a aplicação.
Para esta aplicação, foram definidos os seguintes requisitos de mudanças:
Quando cadastrar ou alterar o modelo de um veículo, garantir que a descrição não tenha mais que 10 caracteres.
Quando inserir um usuário ou alterar um usuário, validar que a senha tenha mais que 6 caracteres.
As variáveis independentes foram aut e man e a variável dependente foi o tempo gasto por cada participante para compreender e implementar a mudança.
O projeto experimental utilizado foi de um fator (esforço) com dois tratamentos (aut e man).
Para a abordagem aut, a análise de impacto e implementação da mudança foi executada utilizando o Ide Eclipse com o plugin SEmantics.
Para a abordagem man, foi fornecido apenas o Eclipse.
Para coleta dos dados, os participantes preencheram um formulário registrando a hora de início e término de cada atividade.
Após registrado o tempo, os participantes apresentaram o software executando com a mudança requisitada para o condutor do experimento, validando assim a implementação.
A instrumentação completa do experimento é apresentada no Apêndice H. O experimento possuiu 13 participantes, cada um executando duas requisições de mudanças (RdM), resultando em duas amostras de 13 pontos.
Os participantes receberam toda a infraestrutura do software pronta para execução da análise de impacto e implementação da mudança.
A infraestrutura incluiu o software executando no ambiente de desenvolvimento (Eclipse) configurado com o servidor de banco de dados (HSQLDB) e de aplicação (Tomcat).
Os participantes foram divididos por técnicas e fornecido treinamentos específicos (ferramenta SEmantics e análise de impacto manual).
Foi solicitado que cada participante interpretasse e implementasse cada mudança, aplicando ou a técnica aut ou man, definida a priori de forma aleatória.
Para este experimento, a variável independente foi caracterizada na escala nominal (abordagem) e a dependente na escala razão (esforço).
O nível de significância adotado foi de 5%.
A análise foi estruturada a partir de as duas RdMs:
Durante a primeira, os participantes não possuíam nenhum conhecimento sobre a arquitetura da aplicação, enquanto durante a segunda, os mesmos já possuíam familiaridade na arquitetura do software devido a primeira implementação (ocasionando uma diferença significativa no esforço).
A execução do experimento produziu os dados apresentados na Tabela 5.13.
A análise inicial avaliou a distribuição dos dados através dos gráficos de linhas, conforme Figura 5.7, e boxplots, conforme Figura 5.8.
Como os outliers na Figura 5.8 foram considerados moderados, nenhum participante foi retirado da amostra.
O próximo passo foi avaliar a normalidade da amostra, considerando as duas hipóteses definidas no experimento anterior.
A Tabela 5.14 apresenta os resultados do Teste de Shapiro-Wilk.
Com base na Tabela 5.14, observa- se que as significâncias dos dados do teste de Shapiro-Wilk são superiores nas duas RdMs considerando o nível de significância definido.
Com esta informação, não se pode rejeitar a hipótese nula sobre a distribuição da normalidade.
O segundo requisito para o teste paramétrico requer a análise da homocedasticidade.
Para tanto, foram utilizadas as mesmas hipóteses definidas anteriormente para o Teste de Levene.
A Tabela 5.15 apresenta os resultados deste teste.
Com base na Tabela 5.15, verifica- se que os níveis de significância para variâncias iguais na 1a RdM e 2a RdM são superiores ao nível de significância definido, viabilizando a utilização do teste paramétrico para duas amostras independentes.
Com base na declaração das hipóteses, têm- se:
H0: Taut $= Tman H1:
Tman\&gt; Taut H2:
Taut\&gt; Tman O Teste T para duas amostras independentes foi aplicado neste contexto e o resultado está apresentado na Tabela 5.16.
Observando a Tabela 5.16, percebe- se que as significâncias são inferiores ao p- value definido, permitindo assim rejeitar H0 (Taut $= Tman).
Isso significa que as médias de esforço das duas amostras (aut e man) são estatisticamente diferentes para as duas RdMs.
A Tabela 5.16 também apresenta que os valores de t0 na 1a RdM e t0 ($= 2,443) na 2a RdM são inferiores ao valor de t, n+ m-2 ($= 2,20) para o grau de liberdade 11.
Como t0 t, n+ m-2 nas duas RdMs, então não se consegue rejeitar a hipótese nula a um nível de significância de 5% em favor de H1:
Tman\&gt; Taut para os dois requisitos de mudança.
Tabela 5.17 ­ Teste T para esforço agrupado aut e man e por RdMs RdM Grau de Liberdade Significância (bicaudal) 1a RdM Assumindo variâncias iguais 2a RdM Assumindo variâncias iguais Com base na Tabela 5.17, obtêm- se que os valores de t0 na 1a RdM e t0 ($= 2,443) na 2a RdM são maiores que t, n+ m-2 ($= 2,20).
Como t0\&gt; t, n+ m-2 nas duas RdMs, então se consegue rejeitar a hipótese nula a um nível de significância de 5% em favor de H2:
Taut\&gt; Tman para os dois requisitos de mudança.
Por as análises apresentadas, pode- se concluir que existe diferença estatisticamente significativa com relação a o esforço necessário em analisar o impacto e implementar uma mudança utilizando a ferramenta SEmantics comparada a uma abordagem manual.
O esforço utilizando a abordagem manual é maior que o esforço utilizando a ferramenta SEmantics.
Isso representa que a ferramenta, no contexto deste experimento, ajudou os participantes a analisar o impacto e implementar a mudança se comparada a uma abordagem manual.
O terceiro experimento teve como objetivo avaliar a eficiência do modelo de análise de impacto numa requisição de mudança, considerando desde a interpretação do requisito até sua efetiva implementação.
Para tanto, foi identificado um software desenvolvido por terceiros e definido duas alterações.
Durante a execução da primeira modificação, os participantes não possuíam nenhum conhecimento sobre o software objeto deste estudo nem de sua arquitetura.
Quando foi solicitada a segunda alteração, o conhecimento adquirido por a primeira execução (fator experiência) fez com que o esforço fosse consideravelmente reduzido em ambas as abordagens.
Cada requisição de mudança foi avaliada de forma independente para caracterizar especificamente esses dois momentos.
A hipótese de que o tempo necessário para a implementação das mudanças é o mesmo nas duas abordagens foi rejeitada utilizando o teste paramétrico T. Este teste deixa claro que existe diferença significativa entre analisar e implementar uma mudança utilizando ou não a ferramenta SEmantics.
Em seguida, foram avaliadas duas hipóteses alternativas para identificar qual abordagem requer maior esforço para ser executada e, por a análise dos dados, foi evidenciado que a abordagem que utilizou a ferramenta SEmantics levou menos tempo que a manual/ tradicional.
Observando a distribuição dos dados, percebe- se que a dispersão diminuiu significativamente para a abordagem automática conforme os participantes adquiriam conhecimento sobre a arquitetura do software.
Durante o primeiro requisito de mudança, quando os participantes não possuíam nenhum conhecimento, houve uma dispersão significativa de esforço.
Este fato não ocorreu para a segunda mudança, gerando inclusive a uma sobreposição quase nula de valores entre as amostras, a exceção de um outlier.
Com isso, evidencia- se que quanto maior o conhecimento do software, menor a dispersão dos dados, gerando conjuntos praticamente disjuntos entre a abordagem manual e automática.
Este fato sugere que a ferramenta SEmantics tende a tirar a variabilidade humana do processo de análise de impacto, visto que o esforço não variou muito entre os participantes do experimento a medida que eles vão adquirindo conhecimento sobre o código fonte do software.
Considerações Finais Este capítulo apresentou uma avaliação quantitativa do modelo de análise de impacto em código fonte utilizando ontologias através de três experimentos controlados.
Estes experimentos exploraram aspectos relativos a revocação e precisão da análise de impacto resultante da ferramenta SEmantics e da análise de similaridade que identifica automaticamente conceitos do domínio a partir de artefatos de software.
Também foi avaliado o esforço referente a analisar e implementar mudanças utilizando ou não a ferramenta SEmantics.
O objetivo geral foi avaliar a aplicabilidade e relevância da automação do modelo de análise de impacto, considerando diferentes aspectos quando comparada a atividade equivalente desempenhada por humanos.
Durante o experimento que analisou a precisão e revocação de três diferentes requisições de mudanças, evidenciou- se estatisticamente que a medida F é maior utilizando a ferramenta SEmantics do que a obtida manualmente por engenheiros de software.
As estratégias manuais são dependentes de especificação estruturada e, conforme descrito por Jönsson, correm o risco de serem menos precisas na predição de impacto.
Este fenômeno foi evidenciado neste experimento onde, para as três requisições mudanças analisadas, em nenhum caso a abordagem manual teve melhor desempenho com relação a precisão ou revocação.
Vale ressaltar que a ferramenta SEmantics possui um melhor desempenho na predição de impacto quando realizada a referência adequada aos conceitos e propriedades do domínio para identificar estruturas do código que sejam equivalentes no escopo da aplicação.
Isto requer que haja uma coerência semântica entre a definição da ontologia e da estrutura da aplicação, e que as estruturas corretas sejam evidenciadas por a requisição de mudança.
A análise de similaridade, que corresponde a um processo chave para a automação do modelo de análise de impacto, também foi especificamente avaliada.
Este experimento utilizou a medida F para comparar a precisão e revocação na identificação automática e manual de conceitos do domínio associados a artefatos de software.
Uma vez associados os conceitos à estrutura do software, esta análise de similaridade é utilizada para definir as métricas de probabilidade aplicadas para avaliar a relevância de cada associação, como TFIDF e DC.
Este é um processo crítico do modelo que foi julgado como relevante para uma análise minuciosa através de experimentos controlados.
Com relação a o segundo experimento, o resultado de sua execução definiu um contexto diferente do primeiro, onde a amostra não apresentou uma distribuição normal, impossibilitando a utilização do Teste T. O teste Mann--Whitney foi à escolha definida para avaliação das hipóteses.
Este teste apenas informa se dois grupos independentes procedem da mesma população.
O resultado foi que existe diferença entre as medidas F associadas a análise de similaridade manual e automatizada.
Em este contexto, não foi possível avaliar estatisticamente as hipóteses alternativas e se optou por comparar a análise descritiva das médias.
Após comparar as médias, verificou- se que a medida F na análise de similaridade automática é aproximadamente 15% maior que a análise manual e que medida F obtida por a ferramenta é maior em 83% das especificações consideradas.
Analisando cada caso, em todas as amostras, a revocação foi maior na análise de similaridade automática e a precisão em 83% dos casos.
O terceiro experimento teve como objetivo caracterizar o esforço necessário para interpretar e implementar uma mudança no código fonte considerando dois momentos:
Considerando a análise descritiva do terceiro experimento, identifica- se que a média de tempo necessária para implementar o primeiro requisito de mudança, utilizando a abordagem manual, ficou em aproximadamente 36 minutos, enquanto a automatizada em 19 minutos.
Esta diferença equivale a quase o dobro do tempo se não for utilizada a ferramenta SEmantics.
Esta variação foi mantida durante o segundo requisito de mudança, que obteve uma média em torno de 10 minutos para a abordagem manual e 5 para a automatizada.
Percebe- se que a proporção de 50% para implementar a mudança foi mantida.
Essa diferença é significativa e, mesmo num contexto em o qual o programador já possui conhecimento da arquitetura do software, o benefício na utilização da ferramenta com relação a o esforço se mostrou igualmente interessante, reduzindo o tempo, em média, a metade.
É importante evidenciar que os resultados obtidos através destes experimentos não são generalizáveis para qualquer análise de impacto em software.
Os resultados foram específicos para seus respectivos escopos, incluindo a limitação no tamanho de cada amostra.
Para aumentar o conhecimento sobre precisão e revocação em diferentes contextos, definindo a validade da experimentação, sugere- se replicações do experimento em modelos distintos e em amostras mais significativas (tanto em quantidade quanto qualidade).
Generalizando o experimento, possibilita- se a extração de novas informações sobre a proposta em diferentes perspectivas no contexto de manutenção de software.
A relevância da análise de impacto durante a manutenção de software é notória.
Mudanças são comuns num cenário evolutivo, em o qual novas funcionalidades são adicionadas ou adaptadas.
Para tanto, modelos para analisar impacto se tornam fundamentais.
Conforme revisão apresentada, percebe- se que as propostas de automação de análise de impacto em código fonte se baseiam em estratégias manuais e automatizadas.
De entre as automatizadas, os modelos que se destacam utilizam a análise léxica do código fonte associadas a modelos de recuperação de informação ou por a própria análise de dependência das estruturas do código, o que gera algumas limitações sob o ponto de vista de negócios.
Estas limitações estão associadas a distância que existe entre o problema, definido por o conhecimento de um especialista do domínio, e a solução, definida por a equipe de desenvolvimento, que irá gerar em sua última instância o código fonte da aplicação.
Em este contexto, o modelo de análise de impacto proposto tem como objetivo automatizar o processo de recuperação de estruturas do código fonte relevantes dada uma solicitação de mudança.
Este modelo foi organizado em dois submodelos:
De rastreabilidade e probabilístico, ambos apoiados por ontologias.
A motivação em utilizar ontologias reside na ideia de que o contexto semântico da aplicação, e não apenas a análise das estruturas estáticas do código, pode gerar uma análise mais apurada dos impactos resultantes de determinada solicitação de mudança.
Foi formulado um tópico genérico de pesquisa dentro de o problema identificado e desenvolvidos estudos em abrangência, tanto sobre análise de impacto quanto seu relacionamento com ontologias e recuperação de informação.
Uma vez desenvolvida a fundamentação da área, foi realizada uma revisão sistemática para verificar o estado da arte com relação a o foco desta pesquisa, não identificando explicitamente nenhuma proposta completamente equivalente.
Logo em seguida, foi apresentado o modelo de análise de impacto que é constituído por um modelo de rastreabilidade e probabilístico.
O modelo de rastreabilidade permite a associação de conceitos do domínio com o código fonte da aplicação através da população da ontologia de domínio com elos de rastreabilidade.
Devido a a granularidade específica dessas associações em nível de conceitos e suas propriedades, foi identificada a necessidade de automatizar o processo de geração de elos de rastreabilidade.
Para tanto, foi desenvolvido um processo automatizado de análise de similaridade entre conceitos e artefatos de software.
Este processo é realizado considerando duas perspectiva:
A semântica, responsável por a categorização e normalização, e a léxica, responsável por a comparação.
O modelo probabilístico foi desenvolvido utilizando Redes de Crenças Bayesianas, que pondera os elos de rastreabilidade conforme as estruturas da ontologia identificadas em determinado requisito de mudança.
Para tanto, são calculadas probabilidade independentes, utilizando o algoritmo de classificação PageRank, e dependentes, realizando análise de frequência e cálculo de dependência conceitual, que considera a sobreposição de grafos de chamada no código fonte com a distância conceitual na ontologia.
O modelo de análise de impacto foi implementando como um plugin do Eclipse e sua arquitetura foi apresentada em detalhes.
Após o desenvolvimento da ferramenta, foram realizados três experimentos controlados para avaliar a precisão, revocação e esforço do modelo de análise de impacto com relação a estratégias convencionais.
A execução dos dois primeiros experimentos evidenciou que tanto a análise de impacto quanto seu subprocesso de análise de similaridade, responsável por a automação de toda a proposta, obtiveram um desempenho bastante interessante considerando o critério de precisão e revocação.
Foi verificada estatisticamente que a medida F associada a análise de impacto é maior no modelo proposto nesta tese, com relação a recuperação de código relevante, quando comparada a abordagem manual de análise de impacto.
A execução do terceiro experimento evidenciou estatisticamente que o esforço para implementar uma mudança num software é maior utilizando a abordagem manual se comparada a abordagem utilizando a ferramenta SEmantics, tanto com o conhecimento prévio da arquitetura do software quanto sem este conhecimento.
Com relação a metodologia, observa- se que a questão de pesquisa definida para esta tese, que corresponde em &quot;Como melhorar a recuperação de estruturas de código fonte relevantes a partir de uma solicitação de mudança comparada a abordagens existentes de análise de impacto?»,
foi respondida integralmente ao longo desde trabalho.
Este trabalho não só apresentou em detalhes o modelo para automatizar a análise de impacto, mas também o avaliou em duas perspectivas:
Viabilidade técnica e eficiência.
A primeira avaliação considerou um protótipo funcional enquanto a segunda, três experimentos controlados.
Os experimentos exploraram a precisão e revocação das estruturas de código recuperadas, comparando- as com estratégias convencionais de análise de impacto, bem como o esforço necessário para interpretar e implementar uma mudança com ou sem a ferramenta SEmantics, que implementa o modelo de análise de impacto proposto.
Com relação a o objetivo geral de desenvolver um modelo para identificar estruturas do código fonte impactadas por solicitações de mudanças, utilizando uma perspectiva semântica, verifica- se que este objetivo também foi atendido completamente conforme descrito no Capítulo 3.
A seguir, serão analisados cada objetivo específico definido nesta pesquisa:
Crenças Bayesianas, através da definição de nodos (e estados) e arestas (com as probabilidades relativas).
As probabilidades independentes foram definidas utilizando o algoritmo de classificação PageRank para os conceitos do domínio e classes do código fonte.
As probabilidades condicionais utilizaram a análise de frequência (TFIDF) e dependência conceitual (sobreposição do grafo de chamada do código fonte com a distância dos conceitos na ontologia).
O modelo de análise de impacto foi implementado integralmente conforme descrito no Capítulo 4.
A solução, por se tratar de uma análise no código fonte, incluiu a criação de um plugin do Eclipse chamado SEmantics.
Através desse plugin, o usuário pode selecionar o projeto Java, a ontologia e descrever a mudança.
A o final, as estruturas do código fonte (classes, atributos e métodos) são ponderadas por o modelo de análise de impacto.
Foram realizados três experimentos controlados visando avaliar a precisão e revocação dos elementos recuperados, bem como o esforço necessário para analisar e implementar uma mudança.
O primeiro experimento avaliou o modelo de análise de impacto como um todo, enquanto o segundo avaliou especificamente a análise de similaridade, que representa uma parte central deste modelo, responsável por a automação na identificação de estruturas do código associadas a conceitos do domínio.
O terceiro foi responsável por avaliar o esforço durante a manutenção do software, aplicando o modelo de análise de impacto proposto.
Contribuição da Tese A principal contribuição da tese proposta é o desenvolvimento de um modelo para análise de impacto e sua avaliação empírica num típico cenário de desenvolvimento, considerando os desafios existentes relacionados evolução de software gerenciamento de mudanças.
Hass em evidência que qualquer organização que inicia um processo de gerenciamento de mudanças e configuração deverá focar na perspectiva do processo para entender uma melhoria, e obviamente a análise de impacto faz parte desta análise.
Como contribuições específicas desta tese, destacam- se:
A identificação e análise dos impactos em estruturas do código fonte a partir de requisições de mudanças;
Ainda como contribuições desta pesquisa, a Tabela 5.1 descreve as produções direta ou indiretamente relacionadas com algum aspecto da tese.
Como todo processo de pesquisa, foram identificadas algumas restrições.
Estas limitações são derivadas tanto da metodologia quanto modelos utilizados.
Apesar de a necessidade do desenvolvimento de uma ontologia de domínio, não é foco primário desta proposta fornecer os meios nem os processos de construção dessa ontologia.
Para a construção da ontologia, engenheiros do conhecimento poderão utilizar as guias apresentadas no Apêndice C. É importante destacar que os profissionais que utilizarão o modelo de análise de impacto não necessitam de conhecimento sobre a engenharia de ontologias, eles apenas a utilizam como insumo para identificação de software relevantes.
Não é escopo deste trabalho a manutenção dos elos de rastreabilidade, que caracteriza a evolução de ontologias.
Uma vez criado os elos, os mesmos serão utilizados para recuperação de estruturas de código.
A definição dos elos de rastreabilidade ocorre em tempo de execução, sempre que uma requisição de mudança é solicitada.
Estes elos não são armazenados fisicamente.
Qualquer mudança no contexto da aplicação ou no domínio requer nova análise de impacto.
Como esse mapeamento em geral é feito automaticamente, o esforço para gerar novo modelo de análise de impacto não é crítico.
Por último, a linguagem de programação utilizada para análise de impacto no código fonte é Java, devido as bibliotecas utilizadas para avaliar a viabilidade da proposta.
Como trabalhos futuros, sugere- se evoluir o modelo de análise de impacto com a manutenção e persistência física dos elos de rastreabilidade, bem como sua manutenção em tempo de execução.
Sugere- se também considerar diferentes linguagens de programação e Ides.
Por último, é sugerido também evoluir o modelo para analisar o impacto em diferentes artefatos de software, não apenas no código fonte.
