Em a área da Informática na Educação, é cada vez mais presente o uso de técnicas de Inteligência Artificial e de Sistemas Multiagentes.
Não obstante os avanços obtidos por a utilização dessas técnicas no que tange à flexibilidade e modularidade, nota- se que ainda há limitações quando se trata de domínios altamente dinâmicos ­ por exemplo, sistemas tutores baseados em simulações de algum ambiente natural.
Em este trabalho, apresentamos uma extensão ao sistema MCOE, um ambiente de ensino de ecologia para crianças, em a qual aplicamos uma técnica de aprendizagem por reforço (Negotiated W--Learning) a fim de tornar o ambiente mais dinâmico em termos de situações que pode apresentar ao aluno.
Em a área da Informática na Educação, os Sistemas Tutores Inteligentes e os Ambientes de Ensino Inteligentes constituem- se modalidades de sistemas de ensino suportados por computador.
Em esta classe de sistemas, é crescente o uso de tecnologias de Sistemas Multiagentes.
Várias técnicas de Aprendizagem Automática (Machine Learning) vêm sendo aplicadas gradativamente nestes sistemas, e entre elas está Aprendizagem por Reforço (Reinforcement Learning), atualmente uma das mais promissoras, especialmente em ambientes de múltiplos agentes.
Diversos sistemas têm sido desenvolvidos dentro de o paradigma de sistemas multiagentes, geralmente reunindo agentes reativos e cognitivos.
Enquanto que os módulos que exigem constante adaptação ­ como o tutor e os modelos dos alunos ­ são geralmente implementados usando- se agentes cognitivos (por suas capacidades de raciocínio e aprendizagem), os demais agentes do sistema são constituídos de agentes meramente reativos, que respondem a estímulos através de comportamentos predefinidos.
Ainda que caracterizados como estado da arte, alguns destes sistemas oferecem lacunas ainda não preenchidas, como por exemplo a pequena flexibilidade na variação das situações de simulação apresentadas aos alunos.
Este fato deve- se geralmente por conseqüência de comportamentos fixos atribuídos aos agentes, definidos à mão por a equipe de desenvolvimento.
Este trabalho apresenta um estudo direcionado de Aprendizagem por Reforço em Sistemas Multiagentes no contexto de uma aplicação com objetivos educacionais.
Utilizando- se o sistema MCOE como exemplo concreto, explora- se a potencialidade de se expandir sistemas de ensino suportados por computador através da implementação de algoritmos de aprendizagem por reforço.
O sistema MCOE, Multi Cooperative Environment, um ambiente de ensino cooperativo de Ecologia para crianças, constitui- se um caso típico desta classe de sistemas.
Em este ambiente, dois alunos são expostos a uma simulação de um ecossistema contendo elementos tais como um lago com peixes e microorganismos, e também elementos poluidores, como esgoto e lixo urbano.
Em o MCOE, o objetivo é manter o equilíbrio ecológico do ecossistema, e, para isto, os alunos assumem diferentes papéis e interagem entre si, trocando experiências.
É importante deixar claro que este trabalho não pretende apresentar uma nova versão do sistema MCOE, mas sim apontar diretivas para uma futura atualização do sistema, que envolva características de adaptação e aprendizagem por parte de os agentes que o compõem.
A proposta baseia- se no fato de que um ambiente mais dinâmico (que ofereça mais possibilidades de ambientes para os alunos) possa ser utilizado de forma mais variada do ponto de vista pedagógico.
Para isto, foram realizadas alterações na arquitetura do sistema para acomodar um método de aprendizagem por reforço denominado Negotiated W--Learning, que apresentou resultados bastante motivadores.
Em o capítulo seguinte, apresenta- se uma revisão de ambientes de ensino suportados por computador, incluindo Sistemas Tutores Inteligentes e Ambientes de Ensino Inteligentes.
O capítulo 3 apresenta o sistema MCOE, descrevendo seu funcionamento e arquitetura.
O capítulo 4 apresenta o problema abordado, enquanto que os capítulos 5, 6 e 7 apresentam, seqüencialmente, uma revisão de aprendizagem em sistemas multiagentes, conceitos de Aprendizagem por Reforço e a técnica Negotiated W--Learning, usada para a implementação.
A descrição das extensões ao sistema encontram- se no capítulo 8.
As conclusões e as indicações de trabalhos futuros apresentam- se nos capítulos 9 e 10, respectivamente.
Em este capítulo apresenta- se brevemente os principais conceitos de Sistemas Tutores Inteligentes (STIs) e de Ambientes de Ensino Inteligentes (ILEs).
Existem inúmeros sistemas voltados especificamente para o ensino através de computador.
Em a realidade, porém, constatou- se que qualquer software pode ser utilizado como ambiente de ensino e aprendizagem, desde que a metodologia seja adequada para este fim.
Apesar de a variedade, segundo os programas educacionais podem ser classificados em duas categorias:·
Categoria 1: Programas com a aprendizagem do aluno dirigida a um conjunto de habilidades específicas.
Exemplos: Tutoriais, ExercícioPrática, Demonstração, Jogos e Simulação;·
Categoria 2: Programas para aprendizagem de habilidades cognitivas amplas.
Exemplos: Micromundos e Sistemas de Autoria.
Em a primeira categoria estão os programas onde a aprendizagem proporcionada por o ambiente está centrada na aquisição de habilidades específicas (motricidade fina, percepção, identificação e outras).
Os programas educacionais desta modalidade se dividem em dois grandes grupos:·
Cai -- Computer Assisted Instruction;
Surgidos na década de 50 e oriundos de projetos da área de Educação;
E· ICAI -- Intelligent Computer Assisted Instruction;
Surgidos na década de 70, a partir de projetos de pesquisas da área de Ia.
Conforme, «os Cai, nas versões iniciais, apresentavam instruções programadas que repetiam na máquina os mesmos métodos utilizados por o paradigma vigente na época (behaviorista), ou seja, o aluno utilizava um conjunto de lições previamente organizadas por o professor (instrutor) de forma seqüencial e com pouca interação.
Os Cai foram criados para oferecer suporte ao ensino de habilidades específicas sem a utilização do modelo do aluno para orientar a forma de interação».
Em o grupo dos ICAI encontram- se os Sistemas Tutores Inteligentes (STIs), apresentados no item 2.1.
Além destes, existem ainda (extraído de):·
Jogos educacionais, que apresentam uma concepção diferenciada daqueles apresentados na primeira categoria.
Em estes ambientes, existe um modelo de simulação onde o tipo de ação executada por o aluno fará diferença no resultado do jogo.
Não é mais uma mera questão de ganhar ou perder:
O aluno atinge ou não um determinado grau de controle do jogo através do resultado que aparece na tela no final do tempo da partida.
Caracterizam- se como ambientes mais sofisticados envolvendo um maior grau de complexidade tanto no seu projeto, como na sua implementação e requerem uma equipe interdisciplinar mais diversificada do que os jogos da primeira categoria;·
Ambientes de Ensino Inteligentes (ILEs).
São ambientes de aprendizagem social que utilizam técnicas de Ia no seu projeto e no seu desenvolvimento.
Caracterizam- se por considerarem mais de um aluno ou mais de um tutor trabalhando no mesmo ambiente.
Podem incluir características das modalidades encontradas na categoria ICAI.
Os ILEs também são conhecidos como Sistemas Tutores Cooperativos ou Sistemas de Aprendizagem Social, combinando aspectos das modalidades STI e Micromundos, e podendo agregar os elementos de simulações (jogos educacionais).
Em as seções seguintes será dada maior ênfase aos Sistemas Tutores Inteligentes e aos Ambientes de Ensino Inteligentes.
Em seguida, serão apresentadas algumas considerações sobre a utilização de agentes na modelagem destes sistemas.
Sistemas Tutores Inteligentes (Intelligent Tutoring Systems) são softwares educacionais que utilizam técnicas de Inteligência Artificial no seu projeto e que se diferenciam dos tradicionais Cais por apresentarem uma estrutura modular em que se tem o conteúdo (domínio) separado da forma como este será trabalhado.
Um STI interage com o aluno exercendo o papel de um tutor computadorizado com poder indutivo similar ao do professor humano, sendo baseado numa modelagem cognitiva constante e progressiva.
Através desta interação, o sistema percebe as intervenções dos alunos, podendo modificar a sua base de conhecimento, sendo, assim, capaz de aprender e de adaptar as estratégias de ensino conforme o progresso dos mesmos.
Os STIs armazenam o conhecimento numa base de conhecimentos construída com o auxílio de um especialista na área de aplicação (domínio).
A arquitetura básica de um STI possui cinco elementos:·
uma base de conhecimento, que armazena o conhecimento do tutor a ser trabalhado com o aluno;·
um modelo do aluno, que armazena informações sobre os alunos, inferidas por a interação com os mesmos, ao mesmo tempo que dá suporte para a criação de situações de ensinoaprendizagem diferenciadas para cada um;·
um conjunto de estratégias de ensino, descrevendo as maneiras como o conteúdo é apresentado;·
um módulo de controle geral;
E· um módulo de interface, responsável por capturar as ações do aluno e fornecer feedback das mesmas.
Segundo, o módulo de controle funciona como um articulador e coordenador dos demais módulos a fim de garantir um sincronismo adequado entre todas as partes.
Em os ambientes mais modernos, esta função fica transparente para o usuário.
Em sistemas que utilizam uma arquitetura multiagente, o controle fica distribuído entre os diversos agentes.
As formas de comportamentos que o sistema aplica para auxiliar o aluno na utilização do conhecimento são armazenadas sob a forma de estratégias de ensino.
Conforme, «atualmente, a base do domínio pode apresentar um grau de sofisticação bastante complexo e diferenciado.
Os sistemas mais modernos utilizam recursos de hipermídia e simulação para que o aluno aceda e manipule os conteúdos.
As interfaces estão cada vez mais sofisticadas, a ponto de algumas utilizarem os recursos de realidade virtual».
Os Ambientes de Ensino Inteligentes (Intelligent Learning Environments) podem ser vistos como uma derivação dos STIs.
Segundo, a diferença básica entre um STI e um Ile está no estilo da interação entre o sistema e o estudante.
Um STI tradicional é baseado num estilo rígido de interação, de forma que o sistema detém sempre o controle da interação.
Em um Ile, contudo, considera- se um estilo mais flexível, onde a iniciativa de interação pode vir tanto do aluno quanto do sistema.
Apesar de esta argumentação, não existe consenso nesta diferenciação e alguns autores afirmam que os ILEs são STIs sociais.
De entre os vários requisitos de um Ile, destacam- se:·
construir uma amostra suficientemente grande do discurso da comunidade-alvo;·
analisar, descrever e indexar a maneira mais apropriada para converter esta amostra num conjunto de experiências de aprendizagem coerentes e cumulativas, criando uma participação pró-ativa ou reativa;·
gerar experiências de aprendizagem e perceber- las em termos de parâmetros de uma interface apropriada ao aluno;·
permitir que o aluno adquira um determinado nível de confiança no seu próprio conhecimento, para que a situação de vivência real torne- se para ele um ambiente de aprendizagem.
Em esta abordagem identificam- se três tipos de alunos:
Os não-- cooperativos, os cooperativos e os pró-ativos.
Embora a metodologia aplicada deva ser específica para cada um destes casos, os ambientes de ensino inteligentes são mais adequados à terceira categoria de alunos (os pró-ativos) por conhecerem melhor os seus objetivos e por buscarem eles mesmos assistência para tornar mais simples as tarefas.
Através de atualizações constantes no modelo do aluno ­ considerando variáveis como a sua experiência anterior ­ o sistema visa auxiliar o ensino, baseandose numa abordagem de Ensino Centrado no Aluno.
Com os avanços nas tecnologias de agentes, muitos sistemas de suporte ao ensino hoje em dia estão sendo modelados utilizando- se o paradigma de agentes (o Sistema MCOE, descrito posteriormente, é um exemplo de utilização desta abordagem).
Esta seção discute brevemente o uso de tecnologias multiagentes no contexto de STIs e ILEs.
Segundo, a razão fundamental para introduzir- se agentes como elementos representando o conhecimento do tutor deve- se às suas capacidades de comunicação e interação, já que os agentes podem se adaptar e aprender durante uma seção instrutiva.
Características como estas são fundamentais para a sobrevivência de um agente num ambiente educacional.
Em geral, as arquiteturas de agentes distribuídas apresentam inúmeras vantagens em relação a os métodos convencionais de construção de STIs.
Entre elas destacam- se a modularidade e a utilização de padrões abertos.
A modularidade permite que se adicione ou que se substitua componentes de forma mais suave e facilitada.
Além disso, permite também facilidades para se trabalhar com domínios mais complexos, onde cada agente pode ser especializado para as suas próprias tarefas dentro de o domínio do problema.
O desenvolvimento de um sistema de apoio ao ensino suportado por computador depende de várias etapas e principalmente de uma forte interação entre educadores e programadores.
Depois de projetado, o sistema deve ser validado através de sessões de avaliação com os estudantes.
Uma vez testado, o sistema é modificado e refinado baseando- se nas informações obtidas por os testes.
Como estudo de caso para a aplicação das idéias propostas neste trabalho, escolheu- se o Sistema MCOE, descrito no capítulo seguinte.
O Sistema MCOE é um ambiente inteligente de ensino cooperativo de Educação Ambiental para crianças e que pode também ser visto como um jogo educacional multimídia baseado no paradigma dos Sistemas Tutores Inteligentes Conforme, o projeto MCOE foi precedido por dois outros projetos.
Com base na experiência prévia foi possível adquirir- se um conjunto de conhecimentos que viabilizou o ambiente atual.
A fase inicial foi composta por um jogo educacional com recursos multimídia, Eco-Lógico, em que foi possível observar- se os alunos trabalhando e retirar- se subsídios para se definir os diálogos e os estados mentais que compõem o conjunto de dados utilizados como teste para a simulação.
A segunda etapa ocorreu com o sistema Multi-Ecológico, uma versão redesenhada do primeiro sistema para compor uma sociedade de agentes heterogêneos (reativos e cognitivos).
Este sistema acomodava ainda apenas um aluno por vez, isto é, o sistema foi criado utilizando- se agentes, porém não se caracterizava como um sistema distribuído de fato.
A versão atual apresenta muitos avanços em relação a os anteriores.
O sistema consiste de um ambiente distribuído em que dois alunos participam e são monitorados por um agente tutor.
A Figura 1 apresenta a sua interface.
O jogo apresenta um ecossistema, caracterizado por um lago habitado por peixes, plantas e microorganismos, onde, com o passar do tempo, elementos poluidores são ativados aleatoriamente.
O objetivo do jogo é agir para manter o sistema em equilíbrio ecológico.
Participam do jogo dois alunos, sendo que cada um escolhe um de entre os personagens disponíveis:
Prefeito, Cidadão, Turista e MãeNatureza.
Cada personagem possui um conjunto distinto de ferramentas a serem usadas no combate aos elementos poluidores, adequadas à sua função no ambiente.
Cada aluno tem a liberdade de escolher a ferramenta que julgar necessário a cada momento e pode interagir com o colega a fim de construir uma estratégia comum de combate à poluição.
Durante o jogo, um aluno pode ou não cooperar com seu parceiro, porém isto ocorre externamente ao sistema.
O MCOE, contudo, é capaz de perceber as ações dos alunos e os seus estados mentais associados.
Baseando- se nestas informações, o componente tutor do sistema decide a forma de se comportar frente a os mesmos, apresentando informações de acordo com cada situação.
A arquitetura do sistema MCOE foi cuidadosamente planejada durante todo o projeto, observando- se os resultados obtidos nas implementações dos projetos anteriores, Eco-Lógico e Multi-Ecológico.
Como evolução destes sistemas, a arquitetura hoje reúne diversos agentes cognitivos e reativos operando em conjunto, caracterizando- se, assim, como uma arquitetura multiagente híbrida.
Os agentes cognitivos, por terem capacidade de raciocínio e de aprendizagem, são capazes de cooperar e negociar com outros agentes na sociedade em que estão inseridos (possuem uma representação explícita do conhecimento).
Os agentes reativos, por sua vez, possuem uma representação comportamental, sendo as suas atividades produzidas por a interação com o ambiente.
Os agentes do sistema estão distribuídos conforme a lista a seguir:·
Agentes Cognitivos· Tutor Inteligente Modelos dos alunos Agentes Reativos (elementos do cenário)· Peixes· Microorganismos· Plantas· Elementos poluidores Também fazem parte do sistema dois outros elementos:
O fundo do lago e a água.
Estes elementos, em conjunto com os demais, são usados para se avaliar a qualidade do ambiente (nível de poluição) e também fazem parte da cadeia alimentar do ecossistema, descrita posteriormente.
O Tutor do sistema recebe informações do ambiente e de cada aluno por meio de sensores e utiliza estas informações para enviar mensagens aos estudantes.
O tutor possui três comportamentos diferenciados (reativo, treinador e assistente), que são ativados em situações específicas de ensino.
Cada comportamento está associado a um conjunto de regras, as quais determinam táticas específicas a serem utilizadas.
Por meio de mensagens, o tutor aplica as suas táticas a fim de desencadear reações nos alunos.
Embora seja um elemento sempre ativo no sistema, o tutor não dispõe de ferramentas para combater os agentes poluidores.
Seu papel consiste em apenas desenvolver ações pedagógicas, induzindo os alunos a refletir sobre determinadas ações.
Os demais agentes cognitivos do sistema implementam os modelos dos alunos.
A função dos modelos é a de armazenar informações inferidas por a interação com os mesmos, e dar suporte para a criação de situações de ensino-aprendizagem diferenciadas para cada estudante.
Os peixes, plantas e microorganismos possuem uma espécie de relógio biológico, que controla o seu envelhecimento, e também um nível de energia, de 0 a 100%.
A o atingirem uma determinada idade, as plantas e os microorganismos morrem.
As plantas estão em contato permanente com o fundo do lago, portanto seu nível de energia permanece sempre aumentando, exceto quando o fundo do lago &quot;morre «devido a a poluição.
Quando este fato ocorre, as plantas param de absorver nutrientes e sua energia começa a cair até que morram.
Os microorganismos, de forma semelhante às plantas, absorvem nutrientes da água.
Se a água torna- se muito poluída, os microorganismos param de absorver energia e também morrem.
Plantas e microorganismos, em condições normais, absorvem constantemente energia até atingirem um determinado limite.
Em este ponto reproduzem- se, perdendo parte da energia.
Os peixes estão permanentemente perdendo energia, e quando estão com fome, procuram por alimento.
Se não encontrarem alimento logo, primeiramente param se reproduzir e, após mais alguns instantes, param de se mover.
Periodicamente (conforme seus relógios biológicos) os peixes entram em ciclo reprodutivo, cuja freqüência varia de espécie para espécie.
Em esse período, o peixe nada para o local de reprodução da sua espécie e &quot;desova», causando o nascimento de novos peixes.
A reprodução causa perda de parte da energia do peixe.
Sendo o principal foco do trabalho, os agentes reativos do sistema são apresentados detalhadamente a seguir.
Os agentes reativos possuem um papel importante no contexto do MCOE, pois implementam os elementos do cenário, incluindo os seres vivos e os elementos poluidores.
A implementação original da arquitetura dos agentes reativos é inspirada na organização proposta por Minsky, que afirma que vários processos mentais podem ser descritos como um conjunto de agentes que interagem entre si.
Em este contexto, um agente representa uma unidade simples capaz de executar uma única tarefa.
Para resolver problemas complexos é necessário que vários agentes interajam de uma forma organizada.
Um exemplo de estrutura que segue esta organização é apresentada na Figura 4.
Em este exemplo, o agente que representa um indivíduo é composto por um agente Base (Builder, na denominação de Minsky), que possui a capacidade de criar uma organização ­ por meio de agregação de comportamentos ­ para realizar uma determinada tarefa.
Se, por exemplo, a tarefa for localizar um determinado objeto para pegar- lo, então uma organização possível poderia envolver agentes responsáveis por os comportamentos de &quot;procurar», &quot;mover- se «e &quot;pegar o objeto».
Os agentes que implementam os comportamentos são chamados de &quot;agentes componentes «1.
Os comportamentos podem ser decompostos sucessivamente, como ocorre no caso de o agente &quot;procurar», que depende de um subagente capaz de &quot;enxergar «o ambiente.
Em esta forma de organização, os agentes não possuem percepção global de todo o ambiente.
Cada agente possui somente percepção e responsabilidades locais.
O cenário do sistema MCOE possui diversos agentes baseados nesta estrutura, tais como os peixes, as plantas e os microorganismos.
Cada tipo de agente possui seus comportamentos definidos por diferentes combinações de agentes componentes, isto é, cada agente do cenário é composto por uma instância do agente base e de diferentes instâncias e organizações de agentes componentes.
Nem todos os pesquisadores concordam em chamar estes componentes de agentes.
Entretanto, para efeito de padronização, neste trabalho será utilizada esta terminologia.
O agente base reúne funções e características comuns a todos os agentes reativos.
Os demais agentes constituem um conjunto de agentes específicos para as ações ou comportamentos específicos de cada elemento do cenário (suas classes são derivadas da classe Cação).
Os peixes, por exemplo, são compostos por os agentes Reproduzir, Nadar, Absorver alimento, entre outros.
Esta forma de implementação, contudo, apresenta algumas desvantagens como por exemplo a necessidade de se realizar reconfigurações para a substituição de ações (implementadas por os comportamentos).
Quando um novo indivíduo é colocado no ambiente, a única ação que contém é a de nascer.
Esta ação é responsável por configurar os demais parâmetros do agente, inclusive reconfigurar suas ações adicionando comportamentos específicos para aquele indivíduo.
No caso de os peixes, a ação de nascer acrescenta ações de reprodução, nadar, absorver alimento, entre outras.
Estas reconfigurações são também necessárias quando o indivíduo morre ou quando é preciso mudar o tratamento de um determinado evento (ao qual passará a responder com outro comportamento).
Assim, apesar de a flexibilidade, esta estrutura pode apresentar um overhead significativo de processamento.
A alternativa proposta neste trabalho, além de adicionar aprendizagem aos agentes, simplifica a adição e a remoção de comportamentos aos mesmos.
Em os capítulos seguintes, apresenta- se a descrição do problema abordado e como este foi solucionado.
Como afirmado no Capítulo 1, alguns sistemas de ensino suportados por computador oferecem lacunas ainda não preenchidas, como por exemplo a pequena flexibilidade na variação das situações de simulação apresentadas aos alunos.
Geralmente este fato ocorre por conseqüência de comportamentos fixos atribuídos aos agentes, definidos à mão por a equipe de desenvolvimento.
Os agentes reativos do Sistema MCOE também apresentam esta característica, o que restringe o conjunto de situações que o sistema pode expor aos alunos.
Com a introdução de uma técnica de aprendizagem automática ­ nomeadamente, Aprendizagem por Reforço ­ buscou- se aumentar o leque de possibilidades em termos de variedade de situações de simulação no ambiente, tornando o sistema ainda mais próximo de a realidade.
O trabalho visa, então, estudar a aplicação de Aprendizagem por Reforço em Sistemas Multiagentes num ambiente educacional.
A seguir estão relacionados os objetivos da dissertação:
Objetivo geral:·
fazer com que os agentes reativos do sistema MCOE alterem seus padrões de comportamento em função de as mudanças no ambiente.
Objetivos específicos:·
estudar a necessidade de se propor mudanças na arquitetura do ambiente para acomodar agentes reativos que aprendem via aprendizagem por reforço;·
propor mudanças na arquitetura interna dos agentes reativos usando aprendizagem por reforço;·
implementar as alterações propostas num protótipo (separado do ambiente existente) para avaliar- se os efeitos das alterações.
Para se avaliar a contribuição das mudanças propostas neste trabalho sobre os agentes reativos, foram realizados experimentos especificamente sobre a classe de agentes reativos do ambiente.
Por serem os elementos reativos que apresentam mais possibilidades em termos de movimentação, quantidade de estados perceptíveis e ações, os peixes do MCOE foram escolhidos como estudo de caso para o trabalho.
As tecnologias apresentadas aqui, contudo, podem também ser adaptadas para os demais agentes do sistema.
É importante deixar claro que este trabalho não pretende apresentar uma nova versão pronta do sistema MCOE, mas sim apontar diretivas para uma futura atualização do sistema, que envolva características de adaptação e aprendizagem por parte de os agentes que o compõem.
O protótipo desenvolvido chama- se RLMCOE, uma junção do nome original do sistema com a sigla RL (Reinforcement Learning), Aprendizagem por Reforço.
Os próximos capítulos apresentam revisões e conceitos sobre as tecnologias utilizadas para a elaboração do protótipo.
Inicialmente é apresentada uma revisão sobre aprendizagem automática em sistemas multiagentes.
Em seguida, são apresentados os conceitos de aprendizagem por reforço.
Em o capítulo 7 apresenta- se o algoritmo escolhido para se implementar a aprendizagem nos agentes reativos do MCOE, chamado Negotiated W--Learning, que caracteriza- se por ser uma extensão do método de aprendizagem por reforço, especialmente projetado para sistemas multiagentes.
As tecnologias de múltiplos agentes têm ocupado cada vez mais espaço nos atuais sistemas.
Algumas subáreas da Inteligência Artificial, como a de Aprendizagem Automática, também passaram a ganhar mais atenção neste contexto.
A crescente demanda por sistemas multiagentes alimenta a pesquisa, que passa a apresentar os benefícios da descentralização e especialização das diversas partes que compõem um sistema.
Muitas vezes, o uso de sistemas multiagentes encontra justificativas na própria natureza.
Como observa, &quot;em as sociedades, a evolução através de atividades coletivas e colaborativas sustentou o progresso individual e coletivo».
Para isto, são necessárias técnicas e modelagens de interação, comunicação e adaptação ao ambiente por parte de os agentes Conforme, a Aprendizagem em Sistemas Multiagentes, além de estender a aprendizagem dos sistemas monolíticos, também reúne as características dos primeiros no sentido de que um agente pode aprender de forma independente dos outros agentes.
Um aspecto importante relacionado à aprendizagem em sistemas multiagentes é a escolha de quando os agentes devem cooperar e de que forma.
Embora a pesquisa alavanque os estudos em torno de vários aspectos da aprendizagem em sistemas multiagentes, sempre acaba- se voltando a questões de interação e comunicação.
Em este sentido, é necessário que se definam mecanismos que evitem a recodificação de tais características para cada novo problema que surge.
Estudos nesta área demonstram que, como esperado, o tipo de abordagem depende da aplicação que se deseja modelar.
Paralelamente, afirma que a verificação cruzada (cross checking) dos resultados entre os diferentes métodos é o que permite que se alcancem resultados mais precisos.
Em os sistemas baseados em aprendizagem automática, os agentes podem aprender durante a execução do sistema (chamada de aprendizagem on-line) ou numa fase prévia explícita de treinamento (aprendizagem off-line).
Muitos sistemas de reconhecimento de padrões ou escrita são modelados por aprendizagem off-line, onde um conjunto de características é previamente especificado por o projetista.
Existem sistemas, contudo, que adaptam- se continuamente baseando- se em retroalimentação a partir de o ambiente ou a partir de o próprio usuário.
Também é interessante citar que, nos sistemas baseados em sociedades de agentes, os mesmos podem aprender coletivamente (compartilhando informações) ou de forma independente.
Weiss entende esta classificação como Aprendizagem Interativa e Aprendizagem Isolada, respectivamente.
Em um caso, os agentes aprendem de forma solitária, enquanto que no outro, aprendem em conjunto e o aprendizado é influenciado por trocas de suposições, convenções sociais e culturais, visões do ambiente criadas em conjunto e normas que regulam seus comportamentos e interações.
Para Weiss, portanto, a Aprendizagem Interativa é aquela baseada ­ ou a que requer ­ a presença de vários agentes e sua interação.
Em relação a a classificação de Weiss, a expressão &quot;aprendizagem em sistemas multiagentes», num contexto mais específico, aplica- se apenas às situações em que vários agentes coletivamente buscam um objetivo comum.
Já num contexto menos específico, aplica- se às situações em que um agente possui seu próprio objetivo, mas é afetado por o conhecimento dos demais (crenças, intenções, e assim por diante).
Alguns sinônimos para esta expressão incluem:
Aprendizado Mútuo (Mutual Learning), Aprendizado Cooperativo (Cooperative Learning), Aprendizagem Colaborativa (Collaborative Learning), Aprendizagem em Equipes (Team Learning) e Aprendizagem Social (Social Learning).
Em ambientes de múltiplos agentes, a necessidade de adaptação é claramente expressa por:
&quot;Para a maioria das aplicações, ou mesmo em ambientes que parecem ser mais ou menos simples, é extremamente difícil ou mesmo impossível determinar corretamente os comportamentos e as atividades completas de um sistema multiagente em tempo de projeto e antes de utilizar- lo.
Este problema pode ser evitado (ou pelo menos reduzido) proporcionando aos agentes as habilidades de aprendizagem e de adaptação ao ambiente, aumentando o desempenho de todo o sistema».
Existem diversas formas possíveis de aprendizagem em sistemas multiagentes, e existem também alguns critérios para distinguir- los.
Dois exemplos são· o método ou estratégia de aprendizagem (por ordem crescente de esforço de aprendizagem):
&quot;Rote Learning», Aprendizagem por Instrução, Aprendizagem por Exemplos, Aprendizagem por Analogia, Aprendizagem por Descoberta;·
a informação de retorno disponível (feedback), que indica o nível de desempenho atingido até o momento:
Aprendizagem Supervisionada, Aprendizagem por Reforço e Aprendizagem Não-Supervisionada.
A informação de retorno é fornecida por o ambiente ou por outros agentes no papel de instrutores (para o caso da Aprendizagem Supervisionada) ou como críticos (no caso de Aprendizagem por Reforço).
Em a Aprendizagem NãoSupervisionada, o ambiente e os outros agentes são meros observadores passivos.
É interessante mencionar que Weiss classifica Aprendizagem por Reforço como uma terceira categoria além de a Aprendizagem Supervisionada e Não-Supervisionada.
Para concluir, segundo, as vantagens dos agentes que aprendem em ambientes multiagentes são claras:
Em os sistemas cooperativos de resolução de problemas (Cooperative Problem Solving Systems), o comportamento cooperativo pode se tornar mais eficiente quando os agentes se adaptam às informações do ambiente e às de seus parceiros.
Em sistemas multiagentes competitivos, a performance dos agentes em relação a o ambiente pode resultar em bons ganhos se eles puderem aprender sobre as estratégias e preferências dos seus oponentes».
O uso de sistemas multiagentes geralmente traz benefícios às aplicações, principalmente às que envolvem aprendizagem.
No entanto, sempre é preciso avaliarse as características individuais de cada sistema, levando- se em conta características como a provável sobrecarga (overhead) imposta por a comunicação entre os agentes, quais informações devem circular e qual o papel de cada agente na sociedade.
Em as abordagens tradicionais de aprendizagem automática (Machine Learning), geralmente os sistemas aprendem através de exemplos de pares de entrada e saída, que fornecem indicativos do comportamento esperado do sistema.
Em estes casos, a tarefa resume- se em aprender uma determinada função que poderia ter gerado tais pares.
Estes métodos são apropriados quando existe alguma espécie de &quot;professor «fornecendo os valores corretos ou quando a saída da função representa uma predição sobre o futuro que pode ser verificada por as percepções do agente no próximo passo de iteração.
Existem casos, porém, em que não se dispõe de exemplos de pares de entrada e saída, e muito menos de um &quot;professor «fornecendo ajuda.
Muitas vezes isto ocorre porque deseja- se que o agente tenha total autonomia.
Em estas situações, o sistema deve ser capaz de aprender com base em outras informações ­ neste caso, recompensas ou reforços fornecidos por um &quot;crítico «ou por o próprio ambiente.
Em alguns casos também é possível que o próprio agente determine as suas recompensas através da observação das transições de estado que realiza no ambiente.
Em todos estes casos, o agente passa a &quot;experimentar «autonomamente o ambiente em o qual está inserido.
Exemplos desta outra classe de métodos que não necessitam de pares de entrada e saída são os métodos de Aprendizagem por Reforço (Reinforcement Learning).
A informação que se obtém do ambiente é adquirida na forma de reforços, também denominadas recompensas.
Geralmente, os reforços (que são informações do tipo &quot;boa escolha «ou &quot;ação inadequada neste momento&quot;) são fornecidos a cada passo ou somente no final de uma tarefa, isto é, o agente pode receber recompensas durante a execução da tarefa ou somente quando chega a um estado final predeterminado.
Em termos gerais, aprendizagem por reforço baseia- se na idéia de que a tendência por parte de um agente a produzir uma determinada ação dever ser reforçada se ela produz resultados favoráveis e enfraquecida se ela produz resultados desfavoráveis.
Segundo, existem três classes fundamentais de métodos para a solução de problemas de aprendizagem por reforço:
Programação Dinâmica (Dynamic Programming), Métodos de Monte Carlo e Temporal Difference Learning (TD).
Os métodos de programação dinâmica são matematicamente bem desenvolvidos, no entanto necessitam de um modelo completo e preciso do ambiente.
Os métodos de Monte Carlo não necessitam de um modelo, possuem conceitos mais simples, mas não são adequados para uma computação incremental passo a passo.
Os métodos baseados em TD, por sua vez, também não necessitam de um modelo e são métodos incrementais, porém são mais complexos de se analisar.
Naturalmente, estes métodos apresentam também diferenças quanto a a eficiência e à velocidade de convergência.
Algumas destas características serão discutidas mais adiante.
Em relação a o MCOE, pode- se dizer que o seu sistema caracteriza- se por um ambiente relativamente complexo (com vários agentes interagindo) e bastante dinâmico (na medida em que os elementos do sistema alteram constantemente o ambiente).
Por não necessitarem de um modelo completo do ambiente e por serem incrementais, os métodos de Temporal Difference Learning são os mais adequados para se resolver o problema de aprendizagem no MCOE.
As seções seguintes apresentam mais detalhadamente o método de aprendizagem por reforço, dando ênfase aos métodos baseados em Q-Learning, uma técnica de aprendizagem por reforço da classe de algoritmos de Temporal Difference Learning.
Segundo, o projeto de um sistema de aprendizagem automática (Machine Learning) envolve várias escolhas, incluindo o tipo de experiência de treinamento, a função a ser aprendida e sua representação, além de um algoritmo de aprendizagem que opere sobre os exemplos de treinamento.
Esta abordagem convencional, contudo, pode se tornar complicada para alguns sistemas.
Como alternativa aos mecanismos de planejamento propõe- se métodos baseados em cálculo de estimativas da melhor ação a ser tomada em cada estado.
É claro que não basta apenas construir uma tabela para ações instantâneas, mas o que se deseja obter é a melhor seqüência de ações realizadas no tempo.
Segundo Russell, o campo de aprendizagem por reforço trata de como um agente pode se tornar proficiente num ambiente desconhecido, apenas por sua percepção recompensas ocasionais (rewards) Para aprendizagem por reforço é o estudo de como um agente autônomo, que percebe e atua no seu ambiente, pode aprender a escolher ações adequadas a fim de atingir os seus objetivos.
Já para, aprendizagem por reforço preocupa- se com agentes que aprendem um determinado comportamento através de interações de tentativa-eerro com um ambiente dinâmico.
Em geral, aprendizagem por reforço é um método de aprendizagem automática em o qual um agente relaciona- se com o ambiente via percepção e ação, e em que, a cada passo da iteração, o agente recebe indicações do estado atual do ambiente, e escolhe uma ação para gerar como saída.
A Figura 7 demonstra este processo.
Em um sistema de aprendizagem por reforço, o estado do ambiente é representado por um conjunto de variáveis de estado percebidas por o agente através de sensores;
O conjunto das combinações de valores dessas variáveis forma o espaço de estados do agente.
Uma ação escolhida por um agente muda o estado do ambiente e o valor desta transição de estados é passado ao mesmo através de um sinal de reforço, denominado recompensa.
O objetivo do método é levar o agente a escolher a seqüência de ações que tendam a aumentar a soma dos valores de reforço, isto é, que produzam a maior recompensa acumulada no tempo.
Este é um processo iterativo, em o qual o agente se move autonomamente no espaço de estados, aprendendo sobre o ambiente.
Como pode- se observar, o agente aprende ações por experimentação.
Cada vez que o agente realiza uma ação, uma entidade externa treinadora (ou o próprio ambiente) pode dar- lhe uma recompensa ou uma penalidade para indicar o quão desejável é alcançar o estado resultante.
Assim, o valor de reforço nem sempre indica situações de avanço, podendo, então, ter um significado negativo (ou inibidor) em relação a a ação executada.
Em aprendizagem por reforço, não se especifica como as tarefas são executadas.
Esta questão é delegada aos agentes.
Uma vantagem desta técnica de aprendizagem é que a aprendizagem em si pode ser feita on-line, isto é, não é necessária uma fase explícita de treinamento do sistema, o que permite com que o mesmo permaneça em constante adaptação2.
A aprendizagem por métodos de tentativa-e-erro sistemáticos pode ser implementada por vários algoritmos já existentes.
Segundo, os algoritmos de aprendizagem por reforço estão relacionados com algoritmos de programação dinâmica, freqüentemente utilizados para solucionar problemas de otimização.
Existem outras definições para aprendizagem por reforço na literatura.
Para, por exemplo, Aprendizagem por Reforço é uma abordagem para a inteligência de máquina que combina duas outras técnicas (Programação Dinâmica e Aprendizagem Supervisionada) para resolver problemas de forma eficaz.
Segundo Harmon, Programação Dinâmica é um campo da matemática derivado da Pesquisa Operacional muito usado para resolver problemas de otimização e controle.
Porém, a programação dinâmica tradicional é limitada em tamanho e complexidade dos problemas que pode resolver.
Conforme, a Programação Dinâmica reduz um problema a um sistema de equações e tenta resolver este sistema através de um método numérico.
Aprendizagem Supervisionada, por sua vez, é um método genérico para treinar um &quot;aproximador de função «(redes-neurais, por exemplo) a representar funções.
Entretanto, a aprendizagem supervisionada necessita de pares de entrada e saída como exemplos da função a ser aprendida, isto é, o método necessita das &quot;perguntas «e das &quot;respostas».
Infelizmente, nem sempre se conhece as &quot;respostas», e, neste caso, a aprendizagem supervisionada simples pode não ser suficiente.
A argumentação de Kaelbling, contudo, é bastante forte:
&quot;Em relação a a forma de aprendizagem, a Aprendizagem por Reforço difere- se das técnicas de aprendizagem supervisionada de várias maneiras.
A mais importante é que na primeira não existe apresentação de conjuntos de exemplos.
São informados ao agente Alguns sistemas de aprendizagem por reforço contam com uma fase de treinamento off-line;
Outros ­ depois deste escolher uma ação ­ a recompensa imediata e o estado subseqüente, mas não qual ação teria sido melhor de acordo com seus interesses.
Outra diferença reside no fato de que a avaliação do sistema geralmente concorre com o aprendizado, enquanto que nos outros métodos existe uma fase de treinamento e outra de execução.
Por outro lado, a Aprendizagem por Reforço assume que todo o espaço de estados possa ser enumerado e armazenado na memória».
É interessante ressaltar que para Kaelbling o agente aprende através de um &quot;crítico «e não de um professor.
Ainda sobre definições alternativas, para aprendizagem por reforço é uma terceira forma de aprendizagem além de a supervisionada e da nãosupervisionada.
De acordo com as suas definições:·
em a Aprendizagem Supervisionada, a informação de retorno (feedback) especifica a ação desejada em relação a o &quot;aluno «e o objetivo da aprendizagem é aproximar- se desta ação desejada o máximo possível;·
em a Aprendizagem Não-Supervisionada, nenhum feedback explícito é fornecido e o objetivo é encontrar ações úteis e desejadas na base de tentativas-e-erro e processos de autoorganização;·
em a Aprendizagem por Reforço, o feedback especifica apenas a utilidade da ação realizada e o objetivo é maximizar esta utilidade.
Embora as definições de Weiss estejam corretas, ainda parece que a Aprendizagem por Reforço ora se comporta de forma semelhante à supervisionada (quando de a presença de um &quot;crítico&quot;), ora como não-supervisionada.
Considerando- se o caso de um cão sendo treinado para andar em duas patas através de punição e recompensa, a forma de aprendizagem pode ser vista como supervisionada.
Entretanto, o caso de uma criança encostar numa panela quente a faz memorizar que esta ação lhe traz prejuízos, e neste caso, a aprendizagem é não-supervisionada.
Em ambos os casos houve informação de retorno (feedback), mas a ação correta (ou desejada) não foi explicitada.
A diferença então reside no fato de que no primeiro exemplo, o sistemas são inicializados com valores pré-calculados.
&quot;crítico «era um treinador (um outro agente) e no segundo, o &quot;crítico «era o próprio ambiente.
Uma das grandes vantagens da aprendizagem por reforço é, sem dúvida, a sua capacidade de transformar recompensas em comportamento, visto que, em muitos casos, funções de recompensa são mais fáceis de se projetar do que os próprios comportamentos.
Conforme, aprendizagem por reforço continua a ser uma das áreas mais ativas em aprendizagem automática, devido a o seu potencial de eliminar estratégias de controle escritas à mão.
Além de isto, segundo, em muitos domínios mais complexos, aprendizagem por reforço é a única maneira viável de se treinar um programa para atingir um bom desempenho.
Em o contexto de jogos, por exemplo, é muito difícil para uma pessoa fornecer avaliações exatas e consistentes sobre um número elevado de posições, que seria necessário para treinar uma função de avaliação diretamente através de exemplos.
Em vez de isto, pode- se informar ao programa quem venceu e quem perdeu, e o programa pode usar esta informação para aprender uma função de avaliação que retorna estimativas razoavelmente corretas da probabilidade de vencer a partir de qualquer posição do espaço de estados.
Harmon, enfim, destaca que «aprendizagem por reforço não é um tipo de rede neural ou uma alternativa às mesmas:
É uma abordagem ortogonal que ataca questões diferentes e mais difíceis».
Assim, muitos pesquisadores estão apostando nesta tecnologia, que apresenta a possibilidade de se resolverem problemas que antes eram insolúveis.
De acordo com, algumas das características que tornam Aprendizagem por Reforço (RL) interessante são:·
RL é um método em o qual· a aprendizagem ocorre através de tentativa-e-erro no ambiente;·
o feedback usado para a aprendizagem tem a forma de valores escalares ­ não é necessário um &quot;professor «ensinando as respostas certas;·
pouco ou nenhum conhecimento prévio é necessário;·
RL é incremental e pode ser usada on-line (não necessita de uma fase explícita de treinamento);·
RL pode ser usada para aprender mapeamentos sensorio-motores diretos, sendo apropriada para tarefas altamente reativas em que o agente deve responder rapidamente a eventos inesperados no ambiente;·
RL é válida em ambientes não-determinísticos;·
As arquiteturas de RL são extensíveis.
Existem aplicações de RL que incorporam aspectos de planejamento, exploração inteligente, aprendizagem supervisionada e controle hierárquico.
Conforme, uma das principais vantagens do uso de aprendizagem automática nos sistemas é que muitas vezes é impossível prever- se todas as situações que podem se apresentar e fazer com que as situações previstas funcionem sempre.
Esta característica vale também para os métodos de aprendizagem por reforço, o que os torna ótimos candidatos para estes sistemas.
Um problema de aprendizagem por reforço possui três partes fundamentais:
O ambiente, em o qual o agente está inserido;
A função de reforço, que fornece o feedback das ações para o agente;
E a função de valoração, um mapeamento de estados para valores (ou qualidades) de estado.
Em este contexto, toda a atenção está em como se encontra de forma eficiente a função de valoração ótima.
Em as subseções seguintes resume- se as principais características de cada uma.
Todo sistema de Aprendizagem por Reforço aprende um mapeamento de situações para ações por experimentação num ambiente dinâmico.
Conforme, o ambiente em o qual está inserido o sistema deve ser pelo menos parcialmente observável através de sensores, descrições simbólicas, ou situações mentais.
Também é possível, entretanto, que toda a informação do ambiente esteja perfeitamente disponível.
Em este caso, então, o agente poderá escolher ações baseadas em estados reais do ambiente ­ o caso ideal.
O objetivo de um sistema de aprendizagem por reforço é definido por uma função de valoração, que representa a função exata dos valores de reforços futuros que o agente tenta maximizar.
Em outras palavras, existe um mapeamento de pares estado-ação para valores de reforço.
Nem sempre o agente precisa ou deve tentar maximizar a função de reforço.
O agente pode também aprender a minimizar- las.
Isto é útil quando o reforço é uma função para recursos limitados e o agente deve aprender a conservar- los ao mesmo tempo que alcança o objetivo. Como
exemplo de aplicação pode- se citar o caso de um avião em que se tenta poupar o máximo de combustível possível.
Em geral, todo problema cuja função de reforço baseia- se em preservação de recursos irá utilizar esta abordagem.
Definir uma função de reforço que reflita de maneira apropriada os objetivos do agente é função do projetista do sistema.
Conforme, as funções de reforço podem ser bastante complicadas;
Porém, existem pelo menos três classes de problemas freqüentemente usadas para criar funções adequadas a cada tipo de problema:
Reward Problems) Em esta classe de funções, as recompensas são todas zero, exceto no estado final, em que o agente recebe uma recompensa real ou uma penalidade.
Como o objetivo é maximizar o reforço, o agente irá aprender que os estados correspondentes a uma recompensa são bons e os que levaram a uma penalidade devem ser evitados.
Funções de reforço nesta classe fazem com que o agente realize ações que produzam o caminho ou trajetória mais curta para um estado objetivo.
Toda ação tem reforço 1,0, sendo que o estado final é zero.
Como o agente tenta maximizar valores de reforço, ele aprende a escolher ações que minimizam o tempo que leva a alcançar o estado final.
Uma função de reforço alternativa poderia ser usada no contexto de um jogo de dois os mais jogadores com objetivos opostos.
Em este cenário, o sistema pode aprender a gerar comportamento ótimo para os jogadores através de algoritmos MaxiMin e MiniMax, entre outros, em relação a a função de reforço.
Uma revisão mais aprofundada destes casos pode ser obtida em.
Duas questões ainda precisam ser respondidas.
A primeira é como o agente aprende a escolher &quot;boas «ações, e a segunda é como se pode medir a utilidade de uma ação.
Primeiro é necessário definir- se três termos:
Uma política determina qual ação deve ser realizada em cada estado, constituindo assim, um mapeamento de estados para ações.
O valor de um estado é definido como a soma das recompensas recebidas quando se parte deste estado e segue- se uma determinada política até o estado final (o valor é, então, dependente da política).
Por fim, a política ótima é o mapeamento de estados para ações que maximiza a soma das recompensas partindo- se de um estado arbitrário e realizando- se ações até um estado final.
Novamente é importante observar que pode- se querer minimizar a soma das recompensas.
No entanto, por simplicidade, neste trabalho será tratado o caso mais comum (maximizar a soma), exceto quando explicitamente indicado.
Uma vez apresentados estes termos, define- se a função de valoração como um mapeamento de estados para valores de estado, que pode ser aproximado usando- se qualquer tipo de &quot;aproximador de função «(quando a quantidade de estados é muito elevada utiliza- se, por exemplo, uma rede neural;
Em outros casos, uma simples matriz é suficiente).
A questão que merece discussão neste momento é como se planeja um algoritmo que encontre de forma eficiente a função de valoração ótima.
A seção seguinte apresenta uma solução.
Em o intuito de se aproximar a função de valoração, a idéia fundamental é que, como o sistema deve aprender com a experiência, se uma ação executada causou uma situação não desejada, o sistema aprende a não mais executar tal ação naquela situação.
Para tornar este comportamento possível utilizam- se conceitos de Programação Dinâmica, onde o processo de aprendizagem consiste em se encontrar uma solução em que, por sucessivas atualizações dos valores dos estados de uma tabela, estes tenham convergido, ou seja, não mudem mais a partir de certo momento.
Em a maioria dos casos, porém, a complexidade do problema é tamanha que a tabela ficaria extremamente grande.
Novamente a solução é usar um aproximador de função que seja capaz de generalizar e interpolar valores de estados, geralmente uma rede neural.
Existem várias técnicas para se realizar uma aproximação de função no contexto da aprendizagem por reforço.
Uma das técnicas mais utilizadas (chamada Q-Learning) é baseada em métodos de Temporal Difference Learning.
Ambos são apresentados a seguir.
Os métodos de Temporal Difference Learning (TD) são uma combinação de características dos métodos de Monte Carlo e de Programação Dinâmica, que buscam estimar valores de utilidade para cada estado no ambiente.
Em outras palavras, quanto mais próximo de a convergência do método, mais o agente tem certeza de qual ação tomar em cada estado.
Assim, a idéia básica de todos os métodos de Temporal Difference Learning consiste em primeiro se definir as condições que se mantêm localmente quando as estimativas de utilidade estão corretas, e então escrever uma equação de atualização que leva as estimativas em direção de esta equação de &quot;equilíbrio «ideal.
A regra de atualização para este método usa a diferença em utilidades entre estados sucessivos (i e j), onde é a razão ou taxa de aprendizagem:
U+ (R (i)+ U (j) -- U (i)) Segundo, apesar de sua grande aplicabilidade, deve- se tomar cuidado no uso deste método visto que a maioria dos resultados de convergência para estes métodos residem na suposição de que o ambiente é Markoviano.
Um Processo de Decisão Markoviano (Markovian Decision Process, ou MDP) consiste de um conjunto de estados X, um conjunto de estados iniciais S (subconjunto de X), um conjunto de ações A, uma função de reforço R, em que R (x, a) é a recompensa imediata esperada por escolher a ação a no estado x, e um modelo de ação P, onde P (x'| x, a) representa a probabilidade de que executar a ação a no estado x irá levar ao estado x'.
Nota- se que é necessário que a escolha da ação seja dependente somente da observação atual.
Se o conhecimento de ações anteriores ou estados afetar a escolha atual da ação, então o processo de decisão não é Markoviano.
Por fim, um MDP permite múltiplas ações possíveis num determinado estado.
Já uma cadeia de Markov permite apenas uma ação possível em cada estado.
Maiores detalhes sobre estes conceitos podem ser obtidos em, e A seguir apresenta- se o método Q-Learning, um algoritmo baseado em TD, cujo objetivo é determinar e atribuir valores-qualidade para cada par de estado e ação do ambiente.
Em o método Q-Learning, a cada iteração o agente observa o estado atual &quot;i», realiza uma ação &quot;a», observa o novo estado &quot;j», e recebe uma recompensa imediata &quot;r».
O agente tem interesse não apenas nas recompensas imediatas, como também na recompensa total descontada (total discounted reward), em que as recompensas recebidas &quot;n «iterações no futuro têm menos valor do que as recebidas no estado atual, por um fator de desconto», onde 0.
O agente, então, tenta determinar Valores-Qualidade (Q-Values) para cada par (i, a).
A cada passo, deve- se aproximar a expressão Q (i, a) (R (i, a)+ max Q (j, b) bA onde:·
A é o domínio das ações;·
Q (i, a) é o valor (qualidade) da ação &quot;a «no estado &quot;i&quot;;·
R (i, a), a recompensa da ação &quot;a «no estado &quot;i&quot;;·,
o fator de desconto;·
&quot;Q(i, a) åd&gt; «significa ajustar a estimativa &quot;Q(i, a) «na direção de &quot;d».
Em esta função de atualização, quanto menor for o valor de, mais homogeneamente os valores Q são atualizados, isto é, menos a segunda parte da expressão (relacionada com os reforços) influência o valor atual.
O fator, também um número entre 0 e 1, é usado para atribuir mais valor às recompensas mais próximas ou às recompensas mais distantes no tempo.
Assim, quanto mais perto de 1 for o valor de, mais importância é dada aos reforços mais distantes no tempo.
O último termo da fórmula evidência o fato de que o valor da ação atual é também influenciado por o valor da melhor ação no estado para o qual se vai no futuro (j).
Isto significa que a ação é interessante não apenas por si mesma, mas também por o fato de que levará a um estado em o qual se poderá executar ações também interessantes.
Os Q-Values estão diretamente relacionados com valores de utilidade de estados (TD) por a equação:
U $= max Q (i, a) ou seja, a utilidade de um estado é equivalente ao maior valor Q da tabela para aquele estado.
Segundo, os Q-Values têm um papel importante na aprendizagem por reforço por duas razões:
Primeiro, assim como regras condiçãoação, eles colaboram para a tomada de decisões sem o uso de um modelo;
E, por outro lado, de forma diferente de regras condição-ação, estes valores podem ser aprendidos diretamente por informações de resposta (feedback) das recompensas.
Além de os métodos tradicionais de aprendizagem por reforço, como Q-Learning, existem inúmeros outros métodos, como:
Average Reward Reinforcement Learning, Actual Return Reinforcement Learning, Feudal Q--Learning, Compositional Q--Learning, Hierarchical Q--Learning, Recurrent Q--Learning, entre outros[ DIE 96;
KAE 95]. Um dos motivos que torna o método Q-Learning um dos mais utilizados ocorre porque, para se aprender utilidades de estados (necessários para outros métodos), seria necessária uma descrição bastante detalhada do ambiente, o que nem sempre é possível.
Em este sentido, considera- se Q-Learning um método mais &quot;econômico», porque baseia- se apenas em valores de qualidade para cada ação em cada estado, muitas vezes mais fáceis de se determinar.
Um dos fatores críticos dos algoritmos de aprendizagem é a garantia de convergência dos métodos.
Segundo, o algoritmo de Q-Learning converge garantidamente com probabilidade 1,0 (um) se:·
o ambiente é estacionário e Markoviano;·
uma tabela é usada para armazenar os Q-Values;·
nenhum par estado-ação é omitido para sempre;
E· a taxa de aprendizado é decrementada de forma apropriada no tempo.
Por estacionário entende- se um ambiente em o qual as probabilidades de transições entre estados não variam no tempo.
Em um ambiente Markoviano, a escolha da próxima ação depende apenas do estado atual.
O protótipo desenvolvido, RLMCOE, caracteriza- se por possuir um ambiente estacionário e Markoviano, mas também dinâmico ­ devido a a aprendizagem, os agentes modificam o seu comportamento constantemente.
Em relação a a taxa de aprendizagem, (learning rate), o problema está na expressão imprecisa &quot;decrementada de forma adequada».
É muito comum encontrar- se valores e atualizações como estas (chamadas na literatura de &quot;tunning «do sistema) nos sistemas de Inteligência Artificial.
Geralmente não são triviais de se determinar.
No caso de o RLMCOE, os valores foram escolhidos experimentalmente através de tentativa-e-erro.
A terceira condição de convergência merece bastante atenção, pois diz respeito à capacidade de um agente explorar o seu ambiente.
Segundo, exploração é definida como a escolha intencional de se executar uma ação que não é considerada a melhor no estado atual com o objetivo de se adquirir conhecimento de estados ainda desconhecidos ou pouco visitados.
Em o contexto de aprendizagem por reforço, geralmente um agente não possui conhecimento prévio dos efeitos de suas ações no ambiente.
Em aprendizagem por reforço os agentes vivem um dilema conhecido na literatura como &quot;the explore/ exploit dilemma», que consiste em decidir quando se deve aprender e quando não se deve aprender sobre o ambiente mas usar a informação já obtida até o momento.
Segundo, para que um sistema seja realmente autônomo, esta decisão deve ser tomada por o próprio sistema.
Conforme, a decisão é fundamentalmente uma escolha entre agir baseado na melhor informação de que o agente dispõe no momento ou agir para obter novas informações sobre o ambiente que possam permitir níveis de desempenho maiores no futuro.
Isto significa que, de um lado, o agente deve aprender quais ações maximizam os valores das recompensas obtidas no tempo, e, de outro, deve agir de forma a atingir esta maximização, explorando ações ainda não executadas ou regiões pouco visitadas do espaço de estados.
Conforme, este é um problema crucial no contexto da aprendizagem por reforço, pois agir para obter informação pode aumentar o desempenho a longo prazo, embora faça com que o desempenho a curto prazo diminua.
Tomando- se estes cuidados, quanto mais tempo o agente estiver atuando no ambiente, mais corretas serão suas ações no decorrer de sua tarefa.
A necessidade de exploração deve ser ainda mais forte quando a especificação do ambiente é fraca, permitindo uma variedade de diferentes tipos de ambientes, cujas características mudam no tempo.
Em este caso, nenhum aprendizado off-line por parte de o projetista irá permitir que uma estratégia correta e fixa seja obtida, porque os comportamentos corretos irão variar a cada execução e mesmo ao longo de o tempo durante uma única execução.
&quot;Um dos tipos mais simples de informação que pode aumentar o desempenho dos algoritmos de aprendizagem por reforço é o reforço esperado obtido a partir de a política ótima.
O agente que possuir esta informação poderá usar- la para auxiliar na decisão entre agir para obter informações ou agir para obter recompensas.
O agente, assim, será capaz de determinar quando encontrou a melhor política e então não precisará experimentar mais no ambiente».
Afinal, um dos objetivos da aprendizagem por reforço é aumentar o grau de autonomia em ambientes nãodeterminísticos.
Outra consideração interessante diz respeito ao ambiente do RLMCOE não ser determinístico.
Para que um ambiente seja determinístico, a transição de um estado &quot;x «através da ação &quot;a «sempre deve resultar no estado &quot;y «com probabilidade.
Em o RLMCOE, algumas ações não resultam nas condições esperadas do ponto de vista dos agentes:
Quando estão próximos aos limites físicos do ambiente (bordas), as ações que os levariam para fora de a área de simulação são ignoradas, o que torna o ambiente não-determinístico do ponto de vista dos agentes.
Este fato, contudo, não traz prejuízos ao sistema e é bastante comum nas aplicações que envolvem movimentação de agentes dentro de um espaço de simulação.
Apesar de todos estes critérios de convergência, para a aplicação em questão não é absolutamente necessário que o algoritmo convirja:
Não se deseja criar &quot;peixes ótimos», mas permitir que seus comportamentos sejam auto-adaptáveis no tempo.
De qualquer forma, dificilmente o ambiente irá estabilizar- se, devido a a sua dinâmica.
O curioso é que, para a aplicação proposta, esta característica pode ser vista como uma vantagem, visto que a introdução de capacidades de aprendizagem visa proporcionar experiências sempre novas para os alunos.
Existem inúmeras aplicações para as técnicas de aprendizagem por reforço.
Alguns exemplos são listados a seguir:·
Aplicações Industriais:
Indústria de empacotamento, certificando que os containers estejam preenchidos de acordo com a especificação numa linha de montagem;·
Controle de Elevadores: Controle de um conjunto de elevadores usando uma combinação de Q-Learning e redes neurais -- melhores resultados do que os métodos heurísticos tradicionais;·
Escalonamento de Tarefas: Problemas de otimização combinatória, minimizando o tamanho de um escalonamento;·
Robótica: Controle de braços mecânicos, coordenação de comportamentos múltiplos usando agentes baseados em Q-Learning independentes;·
Jogos, como Gamão:
Uma das implementações utiliza algoritmos paralelos em arquiteturas RISC.
Com as novas descobertas na área e com as recentes extensões para sistemas multiagentes, as técnicas de aprendizagem por reforço começam a ser aplicadas em domínios cada vez mais amplos.
A próxima seção apresenta a aplicação de RL em sistemas multiagentes.
Em alguns problemas, o espaço de estados a ser tratado por os métodos de Aprendizagem por Reforço é muito grande.
Para lidar com este fato, uma estratégia consiste em tratar o espaço de estados como uma hierarquia de problemas de aprendizagem, em que se tem um agente (ou um conjunto de agentes) para cada subdivisão e nível hierárquico do espaço de estados.
Conforme, em muitos casos, no entanto, soluções hierárquicas introduzem uma pequena &quot;sub-otimalidade «na performance, apesar de potencialmente apresentarem boa eficiência em termos de tempo de execução, tempo de aprendizagem e espaço.
Alguns métodos hierárquicos presentes na literatura são:
Feudal Q--Learning, Compositional Q--Learning e Hierarchical Distance to Goal.
Segundo, abordagens hierárquicas a problemas de RL trazem muitos benefícios:
Melhoria na exploração (pois a exploração pode dar &quot;passos maiores «nos níveis mais altos de abstração), aprender a partir de menos tentativas (pois poucos parâmetros devem ser aprendidos) e aprendizagem mais rápida para novos problemas (pois as subtarefas aprendidas anteriormente podem ser reutilizadas).
Apesar de tudo, verificou- se que as abordagens hierárquicas ainda são demasiadamente complicadas na maioria dos casos.
Existem soluções alternativas, como as de ­ que propõe agentes cooperativos que sabem da presença e também cooperativo, mas onde os agentes são capazes de observar as recompensas imediatas recebidas por os demais agentes e as ações tomadas anteriormente (um agente mantém duas tabelas de Q-Values para cada estado:
Uma para os seus próprios valores e outra para os dos outros agentes).
Sendo um método que pode ser usado on-line e que não necessita de um modelo fixo do ambiente, RL torna- se muito interessante para sistemas multiagentes, onde agentes conhecem muito pouco sobre os demais agentes e o ambiente varia durante a aprendizagem.
Alguns sistemas tratam os outros agentes do ambiente como sendo parte do próprio ambiente, ignorando a diferença entre agentes que respondem a estímulos e este ambiente passivo.
É preciso também diferenciar classificações de sistemas multiagentes (Mas, Multi Agent Systems) baseados em aprendizagem por reforço.
Alguns autores classificam Mas em RL somente quando são vários agentes independentes e iguais agindo para um objetivo em comum, como o problema de predadores e presa proposto por, em que um conjunto de Q-Learners sintetizam comportamento cooperativo, especializando- se.
Apesar de os avanços, confirma a dificuldade de aprendizagem por reforço em sistemas multiagentes, especialmente quando formas de operações em conjunto são necessárias aos agentes.
No caso de o presente trabalho, a configuração segue princípios diferentes.
Os detalhes serão apresentados no capítulo seguinte.
Tan comenta o custo da cooperação em sistemas multiagentes baseados em aprendizagem por reforço, através do compartilhamento de informações instantâneas (como sensações, ações e recompensas), episódios (seqüências de sensações, ações e recompensas) e políticas de decisão completas.
Como vantagem tem- se que novos agentes podem aprender mais rapidamente por usar conhecimentos extraídos dos agentes mais experientes, através de exemplos.
Vários sistemas baseados em múltiplos agentes foram desenvolvidos para otimizar a velocidade ou a exatidão.
Entretanto, estudos de complexidade de algoritmos demonstraram que um número n de agentes baseados em RL que são capazes de observar tudo sobre os demais agentes podem reduzir significativamente o tempo de aprendizagem numa razão.
Assim, por outro lado, constatou- se que:·
informação sensorial extra pode interferir com a aprendizagem;·
compartilhamento comunicação;
E de informações produz um custo de· é necessário um espaço de estados maior para se aprender comportamentos cooperativos para tarefas conjuntas.
Um estudo interessante é o realizado por, um novo algoritmo chamado &quot;incremental self-improvement», em que os agentes não apenas melhoram continuamente a sua performance, como, em princípio, melhoram também a forma da própria melhoria.
Este algoritmo foi especialmente projetado para ambientes dinâmicos, onde estas mudanças ocorrem justamente porque os demais agentes do ambiente também aprendem e modificam o seu comportamento.
Mais uma vez, porém, sua aplicação é geralmente complicada.
Connell e Mahadevan destacam que a decomposição de um problema complexo de aprendizagem num conjunto de pequenos problemas e o fornecimento de sinais de reforço úteis para os subproblemas é uma técnica muito poderosa para se direcionar (bias) a aprendizagem.
Muitos exemplos interessantes de aprendizagem por reforço na área da robótica utilizam esta técnica (Connell &amp; Mahadevan, 1993 apud Segundo, alguns exemplos de aplicações de aprendizagem por reforço em sistemas multiagentes são:
Jogos de futebol, jogos de perseguição (pursuit games), e jogos de coordenação.
Embora o algoritmo Q-Learning aplicado a sistemas monolíticos seja ainda um dos mais utilizados, a crescente demanda por sistemas multiagentes criou a necessidade de se estender as técnicas para que estas fossem também utilizadas eficientemente em sistemas de múltiplos agentes.
A técnica descrita no próximo capítulo acompanha esta necessidade de evolução e foi escolhida para resolver o problema de aprendizagem no protótipo RLMCOE.
Muitos dos problemas tratados por os métodos de aprendizagem por reforço podem ser vistos como problemas de seleção de ações (action selection).
Por seleção de ações entende- se a tarefa de se escolher qual entre as possíveis ações em cada estado é a mais adequada naquela situação.
No caso de o RLMCOE, esta mesma idéia pode ser aplicada aos peixes, pois os mesmos devem, a cada instante, escolher ações tendo em vista satisfazer objetivos múltiplos (alimentação, fuga de predadores, etc) e eventualmente conflitantes (por exemplo, quando o alimento e o predador encontram- se na mesma direção).
De certa forma, o Sistema MCOE já possui um mecanismo de seleção de ações.
No entanto, as ações selecionadas são ou meramente aleatórias, ou (comportamentos fixos).
Em o novo protótipo, o critério de seleção de ações varia com a aprendizagem.
Em um sistema de aprendizagem por reforço tradicional, temos tipicamente um único agente ou processo sendo responsável por toda a tarefa de aprender no seu ambiente.
Todavia, existem problemas em que uma solução descentralizada é mais adequada.
Seleção de ações em presença de objetivos múltiplos é um caso típico desta situação, pois é possível aprender estratégias para cada objetivo em separado e combinar- las dependendo do estado em que o agente se encontra.
Em o caso específico do RLMCOE, optou- se por uma técnica de aprendizagem derivada da aprendizagem por reforço monolítica tradicional.
Esta técnica, denominada Negotiated W--Learning (NWL), é uma versão descentralizada do algoritmo de aprendizagem Q-Learning (que combina RL com Seleção de Ações) na medida em que se tem vários subagentes independentes ligados a um agente principal (o Switch), que determina qual de eles deve ser atendido em cada ciclo de iteração.
Este algoritmo permite que se desenvolva coordenação entre os agentes, porém sem ser necessário compartilhar informações.
Um estudo similar focado em coordenação apresentado por agentes desenvolvem políticas complementares com o objetivo comum de empurrar um bloco entre dois pontos num ambiente.
O interessante de sua pesquisa é que os agentes desconhecem a existência uns dos outros.
A Figura 8 ilustra o domínio do algoritmo Negotiated W--Learning dentro de o contexto de aprendizagem automática em sistemas multiagentes.
Pode- se dividir os sistemas baseados em agentes em dois grandes grupos:
Os sistemas monolíticos, compostos de um único agente, e os sistemas multiagentes.
Como apresentado na figura, os métodos de aprendizagem automática são amplamente aplicados em ambos os grupos.
Aprendizagem por reforço é um tipo de aprendizagem automática, que também pode ser usada por um ou mais agentes.
Conforme a seção Difference Learning.
O algoritmo Negotiated W--Learning, por sua vez, implementa um tipo de aprendizagem por reforço baseado em Q-Learning, porém especialmente projetado para sistemas multiagentes.
De acordo com, este algoritmo baseia- se numa competição entre agentes baseados em Q-Learning.
O algoritmo também pode ser visto como um esquema de seleção de ações auto-organizável para sistemas com múltiplos objetivos paralelos, tais como robôs autônomos móveis.
Basicamente, a idéia do W--Learning puro vem de uma técnica chamada Q-Learning Hierárquico, onde divide- se um problema complexo em subproblemas, tendo um conjunto de agentes Q-Learners aprendendo os sub-problemas, e um único controlador (o Switch) que aprende qual agente escolher em cada estado.
Em este modelo baseado em &quot;negociação», tem- se, na verdade, literalmente uma competição entre agentes que atendem a diferentes estímulos, poupando esforços de se desenvolver um complicado mecanismo de cooperação entre todas as partes.
Além deste aspecto, cada subagente sensora ou percebe apenas uma parte do estado (um subconjunto das variáveis de estado), o que traz benefícios diretos ao desempenho do sistema, diminuindo o risco de explosão combinatória.
O problema do Q-Learning Hierárquico puro é que o Switch deve conhecer e ter acesso a todo o espaço de estados, o que não ocorre no NWL.
É importante ressaltar que Q-Learning e W--Learning resolvem problemas diferentes.
Enquanto que o primeiro resolve o problema de aprendizagem por reforço, o segundo resolve o problema de seleção de ações.
Assim, W--Learning não é apenas mais um outro método de Q-Learning;
Porém, o algoritmo NWL reúne ambos.
A Figura 9 apresenta o algoritmo executado por o Switch.
A cada passo, o agente observa o estado corrente e cada subagente si, baseado em sua percepção parcial, sugere uma ação ai.
O Switch elege um vencedor k e executa a ação ak.
Segundo, a competição entre os agentes no Switch tem limite de iterações finito.
De acordo com o algoritmo proposto, o menor loop possível seria:·
iniciar com um líder qualquer;·
ter todos os outros Wi iguais a zero (isto significa que todos os outros agentes concordam com a melhor ação a tomar, ak $= ai, i);·
loop termina com número de iterações igual a 1.
Por outro lado, o maior loop possível seria:·
algum outro agente possuir Wi\&gt; 0 e tornar- se o líder;·
testar outros agentes, mas voltar ao líder original, cujo W agora é diferente de zero;·
o líder original, então, vence a competição.
Em este último caso, tentou- se o agente original, mas logo em seguida outros agentes, e então o agente original novamente, o que resulta num número de iterações igual a, onde n é o número de agentes.
A partir de estas constatações, chega- se à conclusão de que a competição é limitada em iterações.
No caso de o RLMCOE, o Switch reúne quatro agentes distintos, o que limita o comprimento do loop de competição em 5 iterações.
Apesar deste ser o limite teórico, na prática a competição é resolvida em média entre uma ou duas iterações.
Segundo, este algoritmo é capaz de descobrir em apenas um passo o que o W--Learning tradicional apenas aprende ao longo de várias iterações.
Em termos de arquitetura, no RLMCOE o algoritmo NWL foi aplicado a uma coleção de peixes, cujo objetivo é aprender a escolher as ações corretas em cada estado observado.
Os peixes devem evitar predadores, caçar peixes menores, e ser capazes de nadar para áreas mais seguras a fim de se reproduzir ou de fugir da poluição.
Pode- se fazer uma analogia do algoritmo NWL com um conjunto de pessoas com desejos possivelmente diferentes num bote salva-vidas em o qual só existe um remo.
Como não chegam a um consenso, decidiu- se que, a cada instante, todos os integrantes devem observar a situação atual e discutir quem vai ter o direito de usar o remo naquele momento.
O vencedor dirige o bote para a direção condizente com seus objetivos pessoais, mas todos sofrem as conseqüências da sua escolha.
Pode ser que alguns fiquem satisfeitos, enquanto outros não.
Com o tempo, seus desejos podem ir sendo alterados em função de todo o contexto, e suas atitudes reavaliadas.
Como o objetivo de todos é a sobrevivência, um comportamento coletivo tende a surgir para o benefício do grupo.
Em suma, de acordo com, as abordagens hierárquicas para aprendizagem por reforço apresentam várias vantagens, incluindo exploração melhorada (visto que a exploração pode dar &quot;passos maiores «nos níveis de abstração maiores), capacidade de aprender com menor número de tentativas (menos parâmetros precisam ser aprendidos) e aprendizagem mais rápida para novos problemas (as subtarefas aprendidas anteriormente podem ser reutilizadas).
Os métodos W--Learning e Negotiated W--Learning podem ser aplicados em diversos domínios.
No entanto, não se pode aplicar- los em quaisquer problemas que envolvam múltiplos objetivos ­ por exemplo, nos casos em que os objetivos são tão diferentes a ponto de não poderem ser interrompidos ou intercalados (interleaved).
Apesar de isto, é importante deixar claro que estes algoritmos não forçam a intercalação de objetivos, mas são capazes de descobrir quando esta intercalação é possível.
Em a prática, a competição pode resultar até mesmo na execução serial dos objetivos O próximo capítulo descreve as extensões ao Sistema MCOE e comenta os resultados do protótipo desenvolvido.
Este capítulo apresenta as alterações propostas para o sistema MCOE, bem como as características do protótipo desenvolvido para a sua avaliação.
São apresentadas as classes desenvolvidas, a forma como o algoritmo de aprendizagem foi implementado e resultados de experimentos.
Para que fosse possível agregar- se capacidades de aprendizagem aos agentes reativos do sistema MCOE, uma nova estrutura de classes foi desenvolvida em C+.
As novas classes, independentes do resto do sistema, serviram de base para se implementar todas as funcionalidades e componentes necessários para a arquitetura dos agentes.
Além disso, foi também desenvolvido um novo módulo de simulação, baseado num sistema de mensagens que emula a execução paralela dos agentes.
Basicamente, foram implementadas classes específicas para o ambiente e para o loop de simulação, e também classes genéricas para os agentes (o usuário das classes tem a liberdade de atribuir quaisquer características aos agentes, incluindo capacidades de aprendizagem, ou não).
Os agentes do protótipo são todos derivados destas classes e incluem o algoritmo de aprendizagem NWL.
A arquitetura do protótipo RLMCOE é baseada numa estrutura de classes, que implementa o ambiente de simulação, os agentes reativos, e algumas outras classes de apoio (helper classes).
A Tabela 1 resume as principais funções de cada uma.
Para efeito de padronização, os nomes de variáveis, funções, classes e estruturas foram preferencialmente grafados em língua inglesa, e, sempre que relevante, utilizando a notação Húngara, seguindo as normas da Microsoft e facilitando, assim, a legibilidade.
Para uma rápida referência sobre a notação Húngara, recomenda- se consultar a biblioteca de desenvolvimento (Development Library) da MSDN (Microsoft Developer Network), disponível no pacote do compilador Microsoft Visual C+.
Reúne as definições do ambiente, a lista de agentes, e outras funções.
Embora a arquitetura implementada suporte a simulação de toda a cadeia alimentar do MCOE, para efeito de avaliação o protótipo foi desenvolvido com um conjunto mínimo de agentes, visando simplificar a coleta de resultados.
Vários testes foram realizados, variando- se parâmetros como o número de agentes, a taxa de exploração, a taxa de aprendizagem, o fator de desconto e o valor das recompensas.
Os experimentos foram realizados testando- se a interação de exemplares dos peixes, simulando competições do tipo predador-presa.
Embora não se tenha implementado toda a cadeia alimentar do MCOE, a figura a seguir apresenta as relações entre as classes implementadas e as classes previstas para as plantas, microorganismos e elementos poluidores, todas derivadas de CAgent.
O ambiente suporta um número ilimitado de agentes derivados da classe CAgent.
Também não há restrição quanto a o número de subagentes por instância da classe CRLSwitchAgent.
Todos os elementos do sistema são identificados por tipos predefinidos.
A relação dos tipos é apresentada na Tabela 2.
O ambiente de simulação do jogo (o lago) é um espaço de 200 por 100 pixels subdividido por uma grade em pequenas regiões (células) de 10 por 10 pixels, conforme a Figura 12.
Embora cada agente no ambiente seja mapeado no espaço &quot;fino «de 200 por 100 posições (para que a animação fique suave), para efeitos de cálculo de distância e proximidade, suas posições são convertidas para as coordenadas de grade.
Desta forma, dois agentes nas posições e estarão respectivamente nas células e da grade.
As bordas do ambiente são tratadas como limites físicos.
Se os agentes tentarem ultrapassar- las, suas ações serão ignoradas.
Em o início da simulação, os agentes são adicionados ao ambiente em posições aleatórias, e em quantidades também aleatórias.
A simulação é baseada num &quot;loop «de mensagens contínuo, responsável por disparar eventos de simulação para cada agente do ambiente (broadcast).
O conjunto de eventos de simulação é apresentado na Tabela 3.
A cada passo de simulação, o ambiente primeiramente dispara o evento para todos os agentes, em seguida dispara evento também para todos os agentes, e assim por diante até o evento Em seguida, o ciclo recomeça.
Esta forma de disparar eventos emula a execução paralela dos agentes, como se cada um fosse implementado por um processo concorrente independente.
Uma grande vantagem desta forma de organização é que a adição e a remoção de agentes no ambiente é bastante simplificada e transparente para o mecanismo de simulação.
Um novo agente passa a receber eventos de simulação tão logo seja inserido no ambiente.
As ações executadas por os agentes são específicas para cada tipo de elemento.
Em o caso específico dos peixes, contudo, o conjunto de ações possíveis deve ser rigorosamente o mesmo internamente, devido a as características do algoritmo NWL.
Desta forma, definiu- se as ações dos peixes como deslocamentos numa das oito possíveis direções mapeadas por a grade, conforme a Figura 13.
A Tabela 4 apresenta a codificação das direções possíveis.
Figura 13:
Conjunto de ações possíveis para os peixes A classe CAgent é uma classe virtual (não pode ser instanciada) utilizada como base para a implementação de todos os agentes do sistema.
Algumas de suas funções compreendem o armazenamento de informações gerais sobre os agentes (como posição no ambiente, tipo de agente e nível de energia, implementados na classe CAgentInfo) e um procedimento de callback para responder aos eventos de simulação.
Cada agente implementado é uma derivação da classe CAgent, especializada para a sua função.
No caso de os peixes, uma classe chamada CRLSwitchAgent foi criada especialmente para o algoritmo NWL.
Sendo o foco deste trabalho, a descrição completa da implementação do algoritmo de aprendizagem para os peixes é apresentada nas seções seguintes.
Por questão de clareza, os elementos previamente chamados de &quot;agentes «no algoritmo NWL serão tratados como &quot;subagentes «a partir de esta seção.
A função do Switch no algoritmo NWL é agregar um conjunto de subagentes, repassar os eventos de simulação para os mesmos e resolver a competição.
Em o protótipo, o Switch é implementado por a classe CRLSwitchAgent.
Uma lista encadeada dinâmica o torna capaz de agregar um número qualquer de subagentes.
Para o protótipo RLMCOE, definiu- se que os peixes teriam quatro subagentes.
Com base no método NWL, definiu- se o modelo dos peixes como o apresentado na Figura 14.
Cada indivíduo possui a sua própria instância da estrutura, e, portanto, aprende independentemente.
Vale lembrar que um subagente não &quot;sabe «que os demais existem e que está participando de uma competição por a atenção do Switch.
Conforme comentado, cada subagente sensora somente aquilo que é importante para tomar as suas decisões.
Desta forma, o subagente &quot;Fugir- dos- Predadores «leva em conta somente a distância relativa ao predador visível mais próximo, enquanto que o subagente &quot;caçar-Presas «sensora a presa visível mais próxima ao mesmo tempo em que leva em conta a quantidade de fome do peixe naquele instante.
Os demais subagentes funcionam de forma análoga.
Para os subagentes, definiu- se a classe virtual CRLSubagent.
Esta classe armazena o algoritmo completo de Q-Learning, mantendo a tabela de Q-Values, os estados e as ações do subagente, as funções de exploração e recompensa e também os fatores de desconto e a taxa de aprendizagem individuais.
A Tabela 5 resume as classes derivadas de CRLSubagent, que implementam cada subagente.
Para se adicionar um indivíduo da classe dos peixes ao ambiente, é necessário que se instancie uma classe para o Switch (CRLSwitchAgent) e um conjunto de classes de subagentes (derivadas de CRLSubagent).
Em seguida, deve- se &quot;registrar «o agente na instância da classe do ambiente (CEnvironment).
A Figura 15 exemplifica este processo.
A cada iteração da simulação, cada subagente realiza o passo de percepção do estado atual do ambiente, atualiza o seu conhecimento, e escolhe uma ação a ser executada ­ já que pode ser eleito como vencedor da competição por o Switch.
Em seguida, o Switch entra em ação e o subagente vencedor daquela rodada ganha o direito de executar a sua ação previamente escolhida.
Naturalmente, os subagentes podem escolher ações contraditórias, sendo isto resolvido por a competição.
Em relação a as recompensas, cada subagente possui a sua própria função de reforço, muitas vezes complementares.
Por exemplo, conseguir fugir de um predador resulta num reforço positivo, enquanto que permanecer numa área poluída resulta num reforço negativo.
Há uma particularidade interessante no problema de predadores-e-presas.
A o ser capturada, uma presa deveria receber uma punição e ser removida do ambiente, enquanto que o seu predador continuaria vivo e somaria pontos no seu histórico de recompensas.
Entretanto, obviamente não faz sentido que a presa receba uma recompensa e seja imediatamente removida do ambiente, porque este conhecimento adquirido (de que predadores devem ser evitados) seria perdido.
Considerando- se este fato, definiu- se que uma instância de um peixe representaria, na realidade, uma coleção de indivíduos (um cardume).
Em este caso, quando predado, um contador de indivíduos é decrementado numa unidade, o que permite que o conhecimento adquirido seja compartilhado por o cardume.
Quando o número de indivíduos chega a zero, o peixe (instância da classe CRLSwitchAgent) é, então, removido do ambiente.
As subseções seguintes detalham as características dos subagentes, apresentando os conjuntos de estados e as funções de recompensa.
O subagente Fugir- dos Predadores possui dez estados, variando entre percepções da presença ou não de predadores e sua localização e da condição de capturado.
Seu diagrama de estados e recompensas reúne três principais condições:
Não haver predadores visíveis, haver um predador e saber a sua direção (Px) e a condição de ser capturado (PC).
Para que todos os subagentes sejam igualmente competitivos, os valores das recompensas foram normalizados no intervalo contínuo de.
Tipicamente atribui- se os extremos do intervalo aos eventos mais significativos (positivamente e negativamente) e valores menos influentes ou intermediários para as demais recompensas, de acordo com as suas importâncias.
Em este caso, utilizou- se variações positivas e negativas de 1,0 e 0,1;
e 0,0.
Para exemplificar, serão analisadas algumas transições e suas recompensas associadas:·
estar no estado P0 e realizar uma ação que resulte em permanecer no mesmo estado resulta numa recompensa pequena positiva (peixe livre de predadores);·
estar no estado Px (predador próximo a a vista) e realizar uma ação que leve o peixe para o estado P0 na sua percepção (livre de predadores) resulta numa recompensa positiva grande(+ 1,0):·
permanecer no estado de capturado (PC) é extremamente nocivo:
Resulta em recompensa grande negativa.
O subagente &quot;Caçar Presas «possui um conjunto de estados similar ao anterior.
A novidade é a condição do peixe estar ou não com fome.
Assim, seu diagrama de estados e recompensas é um pouco mais complicado.
As transições diagonais entre estados utilizam linhas tracejadas e valores de recompensa em itálico para facilitar a leitura.
Os valores de recompensas destacados com um asterisco em princípio não deveriam ocorrer.
Em o primeiro caso, um peixe com fome e sem presa passa a ficar sem fome;
E, no segundo, um peixe com fome e com presa à vista passa a ficar sem fome sem ter- la capturado.
Como as recompensas são armazenadas numa tabela de transição de estados, não haveria maneira de serem ignoradas.
Assim, foram atribuídos valores simbólicos às mesmas.
Analisando- se algumas transições:·
estar com fome e com uma presa à vista e manter- se neste mesmo estado resulta num reforço pequeno positivo:
O peixe está seguindo a sua presa;·
todas as ações que levam o peixe ao estado F0Px (sem fome) resultam em recompensas grandes e positivas:
O peixe alimentouse;·
há também mais duas transições curiosas:
F=0  Px-F1PC e F 1P0-F 1 PC.
Em o primeiro caso, o peixe passou do estado sem fome para o estado com fome, mas já se alimentou.
Em o segundo caso, o peixe estava com fome mas não havia presas à vista, e teve sorte de capturar uma presa que se moveu na sua direção.
O subagente Fugir- da- Poluição possui um conjunto de estados praticamente idêntico ao primeiro subagente.
É bom lembrar que pode- se variar parâmetros como o raio de visão, além de os valores para atualização dos Q-Values (fator de desconto e taxa de aprendizagem).
O diagrama de estados e recompensas para este subagente é idêntico ao do subagente Fugir- dos Predadores.
Analisando- se algumas transições e suas recompensas associadas:·
estar no estado Px (poluição próxima) e realizar uma ação que leve o peixe para o estado P0 (livre de poluição) resulta numa recompensa positiva grande:
Sua ação o fez escapar da área poluída;·
permanecer no estado de contato com a poluição (PC) traz prejuízos ao agente, resultando numa recompensa grande negativa.
O subagente nadar é o mais simples dos subagentes.
Quando os demais subagentes não se apresentam muito influentes (não há predadores, poluição ou presas por perto), o subagente Nadar ganha mais importância.
Sua função auxilia na locomoção e na exploração do ambiente e seu conjunto de estados é bastante reduzido.
O subagente Nadar poderia ter sido implementado por rotinas que não envolvessem aprendizagem.
No entanto, para &quot;fazer parte do peixe «e competir no ciclo do algoritmo de aprendizagem, sua implementação deve ser uma instância da classe CRLSubagent, possuindo, assim, o mesmo conjunto de ações dos demais.
De qualquer forma, também é interessante observar que o próprio comportamento de nadar e explorar o ambiente pode ser aprendido, em vez de ser puramente aleatório ou predefinido.
Seus dois únicos estados produzem um diagrama de estados e recompensas bastante simples.
Analisando- se as transições e as suas recompensas associadas:·
estar sem energia suficiente para nadar é ruim, portanto é reforçado negativamente;·
sendo a condição normal e desejada, permanecer no estado E1 não influência diretamente as suas ações;·
ações que levarem a transições de E0 para E1 serão sempre positivas, portanto resultam num reforço de+ 1,0.
A seção seguinte apresenta algumas experiências realizadas.
Devido a a dificuldade de se avaliar um sistema de simulação com muitos elementos (em que a maioria de eles envolve algoritmos de Inteligência Artificial independentes) por causa de a enorme quantidade de parâmetros envolvidos, os principais experimentos de avaliação foram realizados em escalas reduzidas.
Vários testes foram realizados, variando- se parâmetros como a taxa de aprendizagem, o fator de desconto, a taxa de exploração, o número de agentes e o raio de visão dos subagentes.
Três testes foram selecionados para discussão:·
Teste A:
Um predador com comportamento fixo (escrito à mão) contra uma presa baseada em NWL.
Objetivo: Verificar se a presa é capaz de aprender um comportamento de fuga.·
Teste B: Situação Predador--Presa.
Dois agentes baseados em NWL.
Objetivo: Verificar os comportamentos desenvolvidos simultaneamente.
A dificuldade é que os dois nascem erráticos, mas têm de aprender um com o outro.·
Teste C: Predador-Presa com três peixes baseados em NWL.
Objetivo: Verificar se os agentes são capazes de aprender em grupo comportamentos individuais satisfatórios.
Em os testes, as tabelas de Q-Values são sempre inicializadas com 0,0;
isto significa que os agentes nascem sem comportamentos predefinidos.
Considerou- se, também, que os predadores estão sempre com fome e que os peixes não gastam energia.
Além disso, os peixes predados nunca são removidos da simulação.
Por questões de espaço, os valores nas tabelas são apresentados com apenas duas casas decimais (a simulação é realizada com variáveis de ponto flutuante de precisão double de 64 bits).
Além de isto, em algumas tabelas, os maiores valores (melhores ações) para cada estado são mostrados em negrito, e os menores valores, em negrito-itálico para efeito de análise.
Em o primeiro teste simulou- se a execução de 25000 iterações de um ambiente com dois agentes:
Um predador com comportamentos de caça definidos à mão e uma presa que aprende por reforço.
Os parâmetros da simulação são apresentados na Tabela 10.
A listagem a seguir ilustra as tabelas de Q-Values dos dois subagentes, o número de vezes em que o subagente se encontrou em cada estado e o número de ações aleatórias realizadas para exploração.
Embora o subagente caçar-Presas do predador não utilize os Q-Values como critério de decisão de suas ações, sua tabela é apresentada apenas para se verificar que o agente executou as ações pré-programadas corretamente.
Algumas conclusões derivadas a partir de os resultados:·
observando- se a tabela, pode- se verificar que uma política simples de fuga foi desenvolvida.
Um exemplo interessante é o estado 5 (predador identificado abaixo e à direita), em o qual a melhor ação é deslocar- se para cima e para a esquerda;
E a piores ações seriam 0, 4 5 e 6;·
conforme o número de experiências aumenta, a política de fuga torna- se melhor definida;·
de acordo com os resultados, de forma geral, este teste prova que o algoritmo de Q-Learning implementado, em conjunto com a modelagem de estados e recompensas, funciona como previsto e apresenta resultados satisfatórios.
Em o segundo teste, para testar o mecanismo de Q-Learning simulou- se uma situação de predador-presa entre dois peixes, cada um com apenas um subagente:
O predador com uma instância do subagente caçar-Presas e a presa com uma instância do subagente Fugir- dos- Predadores.
Apesar de um número de 50.000 iterações já oferecer resultados satisfatórios, foi realizada a simulação de 1.000.000 de iterações para se verificar a estabilidade dos comportamentos adquiridos.
A Tabela 11 apresenta os parâmetros do segundo teste.·
como assumiu- se que o predador sempre está com fome, o estado 0 nunca foi visitado;·
em quase 93% das iterações, o predador não localizou a presa, considerando um raio de visão de 4 posições;·
observando- se o estado 6 do subagente caçar-Presas (peixe com fome vendo presa abaixo e à direita), nota- se que os valores aprendidos refletem o esperado:
A melhor ação a ser tomada é a ação 5 (para baixo e para a direita), com valor+ 0,52 e a pior ação é 1, com valor ­0, 22 (para cima e para a esquerda);·
quando uma presa é capturada, ficar parado é a pior opção, pois provavelmente ela irá tentar escapar mais vezes.
Nota- se também que os demais valores são homogêneos, indicando que a presa costuma fugir para qualquer direção;·
quando o subagente Fugir- dos- Predadores encontra- se no estado 0 (sem predadores à vista), qualquer direção torna- se igualmente interessante;·
de forma geral, para o subagente Fugir- dos- Predadores, os valores estão próximos às melhores estratégias.
No entanto, ficou claro que a sua função de recompensa precisa de alguns ajustes (é fácil notar como a tabela deste subagente está menos organizada do que a do subagente caçar-Presas);·
também é muito interessante observar que existe uma relação direta do número de visitas dos estados entre os dois subagentes.
Pode- se facilmente encontrar os pares correspondentes.
Isto ocorre porque ambos os subagentes percebem- se corretamente em posições opostas· de forma geral, este teste demonstra que ambos subagentes são capazes de aprender simultaneamente os comportamentos esperados para uma situação de predador-presa simples.
O terceiro teste simulou a execução de 500.000 iterações para um ambiente com três peixes baseados em NWL (tipos A, B e C), cada um com dois subagentes (caçar-Presa e Fugir- dos- Predadores).
Lembra- se que o peixe grande (A) é predador dos demais, e o peixe médio (B) é predador apenas do peixe pequeno (C).
O teste verifica os algoritmos de aprendizagem individuais (Q-Learning) e a ação do Switch (competição entre os subagentes).
Seus parâmetros encontram- se na Tabela 12.·
Uma primeira constatação é que o subagente fugir-dosPredadores do peixe Grande permaneceu sempre no estado 0 (sem predadores) porque não possui um predador.
Assim, todas as ações neste estado têm a mesma importância;·
Outra fácil constatação é que o peixe Pequeno desenvolveu uma estratégia puramente defensiva, enquanto que o peixe Grande, uma estratégia puramente agressiva;·
O comportamento do peixe Médio é mais difícil de se descrever (comparar seus subagentes com os subagentes dos outros peixes).
Sua estratégia mais elaborada deu- se provavelmente porque o agente é tanto presa como predador ao mesmo tempo, e teve que levar em conta as duas situações em cada tomada de decisão;·
No entanto, um fato curioso é que o peixe Médio desenvolveu a política de caça melhor do que a de fuga, um comportamento inesperado e surpreendente.
Questiona- se se uma codificação de comportamentos realizada à mão se traduziria num resultado similar;·
O fato anterior demonstra como cada indivíduo é capaz de desenvolver um comportamento próprio baseado na sua própria experiência de vida;·
Se no futuro for acrescentado um predador para o peixe Grande, certamente seu comportamento passará a ser um pouco mais defensivo, mostrando que os indivíduos são capazes de se adaptar continuamente.
Além destes, vários outros experimentos foram realizados.
Para efeito de comparação com a metodologia utilizada na versão original do MCOE, a seção a seguir apresenta as diferenças entre as estratégias &quot;fixa «e &quot;baseada em aprendizagem», em relação a o Teste A. O Teste A demonstrou a eficácia do algoritmo de aprendizagem implementado, representado por a simulação de uma presa baseada em aprendizagem por reforço contra um predador com comportamentos fixos.
Com o objetivo de comparar as estratégias desenvolvidas por o algoritmo de aprendizagem proposto com a estratégia típica definida manualmente, desenvolveu- se um comparativo entre:·
simulação de um predador fixo versus uma presa também com comportamento fixo;·
simulação de um predador fixo versus presa baseada em RL.
É importante destacar que os testes foram executados com o mesmo número de iterações e com a mesma semente aleatória.
As tabelas seguintes apresentam os resultados obtidos, onde pode- se verificar o número de vezes que cada estado possível ­ para cada peixe ­ foi experimentado por cada indivíduo, ou seja, o número de vezes que cada agente encontrou- se em cada estado.
Observando- se individualmente as colunas e, tem- se os resultados para cada um dos testes.
Comparando- se as execuções, tanto para o predador quanto para a presa, pode- se representar as informações conforme os gráficos a seguir:
Em o eixo horizontal apresentam- se os estados possíveis para o predador e, no eixo vertical, o número de vezes em que o agente encontrou- se em cada estado.
Nota- se facilmente, através da linha pontilhada, que o comportamento do predador manteve- se conforme o esperado, já que suas ações são fixas em ambas execuções.
O único estado que apresentou mudança significativa foi o estado 1, o que é um indicativo de que, na segunda simulação, onde a presa aprendeu via RL, o predador teve mais dificuldade de localizar- la.
Também, com relação a a presa, pode- se verificar na Figura 21 a seguir, que o único estado que sofreu mudança significativa foi o estado 0, sem predadores à vista), o que indica novamente ­ e de forma complementar ­ que a estratégia de fuga desenvolvida por a presa baseada em Aprendizagem por Reforço foi superior à estratégia definida à mão, visto que o número de vezes que a mesma encontrou- se em localizações seguras, longe do predador, foi muito superior em relação a o algoritmo fixo.
Como no gráfico para o predador, pode- se ainda observar que o comportamento desenvolvido autonomamente por a presa na segunda simulação foi, em termos comparativos, praticamente idêntico ao comportamento fixo para os demais casos, conforme esperado (ver linha pontilhada):
As visitas a cada estado obedecem distribuições similares nos dois casos.
Uma outra comparação interessante (Figura 22, a seguir) apresenta um gráfico contendo as recompensas ganhas e acumuladas no tempo por os dois agentes, durante as 25.000 iterações.
Conforme esperado, as recompensas para a versão da presa com comportamentos fixos formaram praticamente uma linha reta entre a primeira e a última iteração, indicando um comportamento constante para todos os estímulos.
Já a versão baseada em aprendizagem por reforço apresentou variações no valor das recompensas obtidas na simulação, conforme o agente corrigia o seu próprio comportamento ao longo de o tempo.
Além de isto, é fácil perceber na Figura 22 que as recompensas acumuladas no tempo da solução baseada em RL refletem e reafirmam uma tendência ao comportamento esperado nestas situações;
Isto é, em geral, a presa aprendeu boas estratégias pois manteve- se recebendo recompensas positivas ao longo de o tempo, e, quando necessário, realizou alguns ajustes na sua política de fuga.
Esta seção discute brevemente algumas dificuldades encontradas durante a elaboração desta dissertação.
As principais dizem respeito à bibliografia e a esforços de otimização dos algoritmos para a simulação.
Principais dificuldades:·
bibliografia não muito extensa e pouco detalhada sobre aplicação de Aprendizagem por Reforço em Sistemas Multiagentes para problemas com objetivos conflitantes:
Trabalhos Individuais, que antecederam a dissertação;·
necessidade de redução do número de estados por subagente:·
falta de bibliografia específica sobre NWL:
­ embora tenha- se obtido acesso a uma vasta bibliografia sobre RL nas mais diversas aplicações, pouco material foi encontrado especificamente sobre Negotiated W--Learning.
A principal referência foi a tese de doutorado de Mark Humphrys;
Outras dificuldades:·
algoritmo NWL:
Entretanto, sua completa compreensão tomou uma boa porção de tempo.·
tunning dos parâmetros da fórmula de RL:
­ devido a a natureza da própria área de Inteligência Artificial, a maioria das soluções baseia- se em heurísticas e em refinamentos por tentativa-e-erro.
A fórmula de atualização dos Q-Values é complexa, especialmente nestes ambientes multiagentes, o que exigiu um bom esforço para determinar- los.
Apesar de isto, de qualquer forma não foi necessário descobrir os melhores valores, visto que a convergência do algoritmo nesta aplicação também não é absolutamente necessária;
A listagem a seguir apresenta as limitações da solução proposta.·
Alcance e extensibilidade· número ilimitado de subagentes por Switch;·
número ilimitado de estados e ações por subagente;·
número ilimitado de agentes no ambiente;
Restrições· subagentes devem possuir o mesmo conjunto de ações;·
valores são armazenados em matrizes;·
valores das recompensas são fixos.
Em relação a o alcance e à extensibilidade, a solução apresentada pode fazer uso de um número ilimitado de agentes, subagentes e número de estados e de ações.
Naturalmente, a expressão &quot;número ilimitado «refere- se à capacidade teórica de armazenamento;
Sabe- se, contudo, que a memória disponível representa o limite físico real.
Além deste aspecto, o mesmo tipo de solução pode ser aplicado a outros contextos, como controle de semáforos de tráfego urbano, controle de elevadores, entre outras aplicações similares, demonstrando a flexibilidade do algoritmo implementado.
Com relação a as restrições, deve- se tornar claro que os subagentes de um mesmo Switch devem possuir necessariamente o mesmo conjunto de ações.
Esta é uma restrição do algoritmo NWL, visto que os subagentes devem competir sobre um mesmo conjunto de &quot;regras».
Outras duas restrições, com relação a o armazenamento e a geração dos valores para o algoritmo de aprendizagem, são apresentadas.
Em o primeiro caso, atualmente os Q-Values do algoritmo NWL são armazenados em matrizes, o que eventualmente pode limitar as dimensões das soluções desenvolvidas com a solução proposta.
Uma solução possível, como comentado em diversos pontos anteriormente, seria a substituição das matrizes por um mecanismo aproximador de função, como uma rede neural.
A outra restrição diz respeito aos valores das recompensas fornecidos aos subagentes.
Em a atual implementação, estes valores são fixos e armazenados e uma tabela de rápida consulta.
Alguns pesquisadores apresentam alternativas a esta classe de algoritmos de aprendizagem, em que inclusive os valores das recompensas são aprendidos ao longo de o tempo (ver próxima seção).
Com respeito a trabalhos relacionados na área, pode- se citar:·
Em Aprendizagem por Reforço -- várias iniciativas, abordando aspectos ou construções diferentes para sistemas baseados em RL.
Destacam- se (extraído de -- Maes e Brooks; --
Mahadevan e Connell; --
Lin; -- Dorigo &amp; Colombetti; --
Singh; -- Kaelbling. --
Kaelbling, em, identifica que, na verdade, todas estas soluções podem ser vistas como diferentes alternativas a um modelo ao qual chama de &quot;Hierarchical Learners&quot;:
Todas as soluções possuem um conjunto de &quot;behaviors «que alimentam um &quot;gate», o qual gera uma ação de saída (&quot;Gated Behaviors solution&quot;).
Em algumas, os behaviors aprendem e o gate é fixo, em outras ocorre o oposto;
Há também soluções em que todos os módulos incorporam aprendizagem.·
Específicos para W--Learning -- principal referência:; --
comparação com soluções de (extraído de): --
Tyrrell; -- Minsky; --
Brooks (Subsumption Arch).; --
Maes, -- Whitehead;·
Em I_ A. na Educação -- não foram encontradas referências de uso do algoritmo proposto ou de soluções similares na literatura; --
sendo assim, em princípio este fato classificaria o presente estudo como inovador, por tornar o ambiente de aprendizagem mais dinâmico com baixo custo computacional, se comparado a soluções que envolvem comunicação e coordenação entre os agentes.
Durante o desenvolvimento desta dissertação, os estudos parciais foram apresentados em dois eventos:·
SBIE99, Simpósio Brasileiro de Informática na Educação, em Curitiba, PR ­;
Intelligence, em Acapulco, México ­.
O próximo capítulo apresenta as conclusões.
Em este trabalho apresentou- se o sistema MCOE e as alterações propostas a fim de tornar- lo mais dinâmico do ponto de vista educacional, através da geração de um conjunto maior de efeitos próximos aos que acontecem na natureza (adaptação).
Estas alterações tendem a enriquecer a forma como os alunos resolvem os problemas, uma vez que ­ através da aprendizagem ­ as regras do sistema terão de ser adaptadas e o jogo ganhará um nível de complexidade maior.
Isto poderá ampliar a faixa etária de utilização e a forma com que o sistema pode ser explorado do ponto de vista pedagógico.
Assim, este estudo contribui, em nível geral, para a área de Ia aplicada à Educação, ao estudar a aplicação de Aprendizagem por Reforço em Sistemas Multiagentes num ambiente educacional.
Em termos de os objetivos definidos para esta dissertação, o objetivo geral foi atendido, na medida em que foi possível implementar- se o algoritmo proposto e verificar que os agente são de fato capazes de corrigir continuamente os seus comportamentos.
Com relação a os objetivos específicos, estes também foram atingidos, pois apresentou- se uma nova arquitetura para os agentes reativos do sistema MCOE e coletou- se resultados bastante animadores a partir de sua implementação.
A modularidade da arquitetura permitiu que as alterações fossem realizadas na parte reativa do sistema de forma independente da parte cognitiva.
No entanto, é claro que o conhecimento do agente tutor deve ser adaptado aos novos parâmetros.
Tais ajustes ficam caracterizados como sugestão a trabalhos futuros na área, no intuito de adicionar ao tutor a capacidade de levar em conta as mudanças de comportamento dos agentes reativos.
Em relação a a convergência do método NWL, não se obteve elementos para provar se o algoritmo converge ou não, embora para o RLMCOE a convergência não seja necessária.
Apesar de tudo, os resultados obtidos demonstraram que o algoritmo apresenta uma boa tendência nesta direção.
Também foi interessante verificar que não se pode negar a enorme vantagem de se poder modelar um sistema através de descrições de comportamentos desejáveis e indesejáveis, o que torna- se muito útil quando não se dispõe de ferramental necessário (ou o problema é demasiadamente complexo) para se resolver o problema através de uma modelagem tradicional.
As soluções geradas por técnicas aprendizagem automática como Aprendizagem por Reforço em muitos casos podem apresentar- se superiores às definidas à mão, apesar de serem difíceis de se visualizar ou de se traduzir para um conjunto conciso de instruções.
Por final, Weiss destaca que é possível aplicar aprendizagem para se melhorar um sistema, ou aplicar sistemas multiagentes para se melhorar a aprendizagem.
Em o protótipo desenvolvido atingiu- se as duas afirmações:
O sistema passou a ser mais dinâmico devido a a aprendizagem e o uso de sistemas multiagentes permitiu um bom nível de aprendizagem de uma forma mais simplificada.
Com o objetivo de melhorar continuamente o Sistema MCOE, contribuindo para os ambientes de ensino inteligentes em geral, propõe- se algumas sugestões para trabalhos futuros.
Inicialmente seria interessante realizar mais experimentos com a atual arquitetura, visando afinar os parâmetros do algoritmo de aprendizagem.
Uma mudança também importante seria a integração do protótipo aos outros módulos do MCOE, incluindo alterações nas regras do Tutor, de forma a adaptas- lo ao algoritmo de aprendizagem.
Outra pesquisa interessante seria aplicar aprendizagem por reforço a outros elementos do ambiente, como por exemplo, os elementos poluidores.
Concluídas estas etapas, uma nova avaliação pedagógica do sistema teria de ser realizada com alunos e professores.
Além de esta alteração na arquitetura, seria interessante desenvolver uma segunda contribuição para a melhoria do sistema:
Uma modificação no processo de atuação das ferramentas dos personagens sobre os elementos do cenário.
Em o MCOE, cada ser vivo possui um nível de energia, que varia de zero a 100.
Quando este valor chega a zero, o indivíduo morre e é removido da simulação.
No caso de os peixes, especificamente, a própria ação de nadar gasta energia.
Para recuperar- la, o peixe deve alimentar- se de espécies menores, de microorganismos ou de algas.
A poluição também remove energia dos seres do ambiente, e por isto precisa ser combatida.
Em a implementação original do sistema, as ferramentas de apoio dos personagens atuam diretamente sobre os seres vivos.
Para exemplificar, vamos supor que um determinado peixe esteja com o nível de energia igual a 76% e que um poluente acaba de agir no lago.
Supondo que este poluente remova 5 unidades de energia deste indivíduo, o peixe irá ficar com 71% de energia.
Se um aluno decidir usar uma ferramenta que repõe, digamos, 3 pontos de energia, o peixe irá ficar com 74% de energia ao final.
O problema desta abordagem sob o ponto de vista educacional é que o aluno pode não identificar uma situação de causa-efeito direta entre as ferramentas e o resultado de sua aplicação.
Para solucionar este problema, seria necessário que as ferramentas atuassem sobre os elementos poluentes e não diretamente sobre os seres vivos.
Assim, o aluno, com auxílio do tutor, poderia vir a perceber que as ações que ele realiza se refletem posteriormente sobre os seres vivos, mas que isto ocorre como conseqüência da atuação sobre a causa da degradação ambiental, ou seja, a poluição.
Através do algoritmo NWL embutido nos peixes, estes gradualmente perceberiam as mudanças na poluição do ambiente, adaptando seus comportamentos.
