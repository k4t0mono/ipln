Em esse trabalho é apresentada uma estratégia para diminuir a intrusão do teste de software em programas paralelos baseados em troca de mensagens.
Para isso, um ambiente de teste foi desenvolvido utilizando técnicas de teste de software funcional e abordagens de depuração.
O ambiente, que utiliza Java como linguagem de programação e MPI como biblioteca para troca de mensagens, baseia- se na idéia de utilizar Rede de Autômatos Estocásticos (SAN) para a representação do modelo comportamental da aplicação e, com isso, criar casos de teste que exercitem a aplicação paralela na busca por falhas de comunicação entre os processos.
Essas falhas são identificadas por os módulos de monitoração e análise on-line, que observam a execução da aplicação, verificando inconsistências entre os estados atingidos e os estados esperados do modelo.
Para a diminuição da intrusão foi dada ênfase tanto para a definição e geração de casos de teste, quanto para as abordagens utilizadas no engine de teste nas etapas de monitoração e análise.
Busca- se com essa estratégia, validar as abordagens utilizadas no processo de teste e identificar eventuais problemas.
Com o passar dos anos o computador se tornou uma das ferramentas mais importantes já desenvolvidas por a humanidade.
Ele atualmente está inserido na vida de muitas pessoas, provendo inúmeras facilidades que automatizam e aceleram tarefas tediosas do dia-a-dia, fazendo com que seus usuários consigam tempo para realizar atividades consideradas mais produtivas e interessantes.
O universo de utilização dos computadores alcançou lugares onde há poucos anos atrás nem se imaginam o uso de tal tecnologia.
A disponibilidade da Internet em boa parte das residências e empresas ao redor de o mundo, os telefones celulares, assim como diversos outros dispositivos móveis e de entretenimento, providos de um grande poder de processamento, são exemplos desse crescimento.
Muitas pessoas acreditam que existe um número excessivo de computadores e que alguns são usados de maneira desnecessária, e até mesmo incorreta, algo que muitas vezes põe em dúvida a funcionalidade dessas máquinas.
Essa desconfiança se agravou, principalmente, durante o Bug do Milênio (Y2K problem), e continua influenciada por a disposição atual do mercado.
Uma forte indicação disso é a crescente utilização de softwares embarcados e tecnologias sem fio em dispositivos eletroeletrônicos de última geração, o que cultiva nos usuários dessas tecnologias, a incerteza de onde toda essa explosão tecnológica irá parar.
Além de essas tendências atuais e futuras de mercado, a computação é aplicada também em áreas mais tradicionais da ciência e engenharia.
A simulação é uma de elas, e compõe o campo científico e metodológico ao lado de categorias teóricas e experimentais, sendo através desses domínios mais científicos que a computação torna- se indispensável na solução de grandes desafios.
Até o presente momento, diversos problemas no campo científico ainda não foram solucionados, ou foram resolvidos de maneira parcial, devido a limites no poder computacional existente.
Uma visível tendência, é que essa demanda por alto desempenho computacional nunca termine, e que os problemas sempre venham a persistir.
Esse grupo de problemas também chamado de &quot;grandes desafios «afeta áreas como:
Pesquisas climáticas, análises químicas, estudos do espaço, entre outras.
Conseqüentemente há uma crescente demanda por um maior poder computacional, fazendo com que pesquisas sejam estimuladas na busca por alto desempenho, objetivando desenvolver computadores com maior capacidade de processamento e memória, mais espaço em disco, e principalmente com dimensões reduzidas.
Diversas arquiteturas de computadores estão disponíveis atualmente para os mais variados tipos de aplicações.
A mais utilizada é a arquitetura seqüencial tradicional que foi idealizada por Von Neumann.
Desde sua criação até hoje, muita coisa mudou nessa arquitetura:
Os processadores adquiriram um maior processamento;
Houve um aumento na capacidade de memória;
E a transmissão de dados recebeu barramentos mais rápidos, tornando- a mais eficiente.
Tudo isso acompanhado por uma diminuição do custo, o que tornou a tecnologia computacional mais acessível.
No entanto, atualmente está se tornando cada vez mais difícil aumentar o desempenho de máquinas baseadas nesse modelo de Von Neumann.
Aplicações que necessitam de um grande poder computacional buscam suas soluções em computadores que utilizam outras arquiteturas, como as arquiteturas paralelas.
Contudo, melhorar o desempenho não depende somente do uso de dispositivos de hardware mais rápidos e seguros, depende também de uma melhoria na arquitetura dos computadores e nas técnicas de processamento.
Em esse ponto, há uma grande diferença entre usuários que estão interessados em eficiência e confiabilidade, e usuários de tecnologias tradicionais do dia-a-dia, e que buscam apenas solucionar problemas de maneira eficaz.
Os usuários que buscam alto desempenho sabem como programar suas aplicações de maneira mais eficiente para a solução de determinados problemas.
Além disso, as máquinas usadas por esses usuários são capazes de trabalhar dias, semanas ou até mesmo meses na solução de um problema em particular.
Desde que surgiu há 20 anos atrás a computação paralela vem permitindo que problemas complexos sejam computacionalmente solucionados, e aplicações de alto desempenho sejam desenvolvidas.
A utilização desses computadores de larga escala, juntamente com suas aplicações, introduzem problemas que são estabelecidos principalmente devido a sua complexidade, ao grande número de processos envolvidos e também por a quantidade de dados a serem processados.
Muitas dessas complexidades adicionais são inseridas devido a o fato de que programar aplicações paralelas é mais complicado quando comparado com programas seqüenciais.
Muitos desses problemas são observados durante o ciclo de vida do desenvolvimento de programas paralelos, especialmente durante as fases de teste, depuração e ajustes de desempenho (tuning phase).
O objetivo dessas fases é detectar falhas e corrigir erros críticos e gargalos de desempenho, aumentando a confiabilidade e eficiência do código da aplicação, e resultando numa aplicação com maior qualidade.
Esse propósito é conseguido basicamente através da análise dos estados do programa durante sua execução e da interpretação desses resultados.
Entretanto, a quantidade de dados que descreve esses estados depende do número de processos paralelos envolvidos.
Conseqüentemente, o tamanho dos resultados depende da quantidade de estados monitorados e também do tempo total da execução da aplicação.
Portanto, devido a essa tendência em gerar grandes quantidades de dados a serem analisados é que muitas vezes se excede o limite de memória disponível, limitando e dificultando assim as atividades de teste e depuração.
Outra conseqüência, é que essas atividades de análise e monitoração podem influenciar de maneira negativa o comportamento do programa testado, alterando a ordem e o tempo de execução dos eventos.
Além desses problemas, o teste e a depuração de programas paralelos tornam- se bem mais complicados do que em programas seqüenciais, principalmente por causa de o nãodeterminísmo e seus problemas relacionados.
Muitos pesquisadores tentam estender as técnicas e abordagens utilizadas no teste e depuração de programas seqüenciais para serem aplicadas em programas paralelos.
Entretanto, dada a complexidade desse tipo de arquitetura de software, inúmeros problemas ainda persistem.
Em esse trabalho definiu- se uma estratégia para a diminuição da intrusão do teste de software em programas paralelos, onde um ambiente de teste foi desenvolvido para validar as técnicas propostas.
Espera- se com essa estratégia conseguir criar uma abordagem que seja menos intrusiva no comportamento da aplicação em teste.
No entanto, para isso, a intrusão tem que ser diminuída não apenas nas etapas de teste, mas principalmente na técnica de monitoração a ser utilizada.
De essa forma, foi definida uma estratégia híbrida utilizando tanto técnicas de teste de software funcional quanto técnicas de depuração e monitoração de programas paralelos.
O ambiente de teste desenvolvido é composto basicamente por três módulos:
Um módulo para geração automática de casos de teste e scripts, um módulo de análise, e um módulo de monitoração (on-line) e execução da aplicação em teste.
Os casos de teste são gerados automaticamente através de um modelo abstrato da aplicação (modelo de uso).
O formalismo utilizado para essa modelagem é SAN (Stochastic Automata Network), ou Rede de Autômatos Estocásticos, o qual através da associação de uma distribuição de probabilidade nas transições entre os estados do modelo, passa a descrever o uso operacional do software, ou o comportamento esperado.
Além de o ambiente implementado, foram definidos também:
Um processo de teste, que estipula as etapas e técnicas utilizadas;
E um ciclo de teste, que especifica as abordagens utilizadas em cada etapa do processo.
A principal motivação desse trabalho é pesquisar métodos e técnicas importantes relacionados ao teste e a depuração de programas paralelos e, com isso, adquirir um conhecimento científico capaz de nos permitir propor uma nova estratégia de teste de software, focando as abordagens utilizadas no teste de software funcional, assim como na diminuição da intrusão dessas etapas de teste.
Outra forte motivação é a carência de ferramentas de teste para ambientes paralelos, algo que é sentido nas bibliografias da área.
No entanto, nos sentimos motivados também, e principalmente, por o fato das pessoas envolvidas nesse projeto fazerem parte de centros de pesquisa relacionados tanto ao teste de software quanto a o processamento paralelo e distribuído (CAP2), algo que promove a integração e a extensão de projetos.
Como é o caso do STAGE, uma ferramenta de teste desenvolvida no CPTS, e que nesse trabalho permite adaptar as suas funcionalidades de geração automática de casos de teste para interfaces de aplicações web, a fim de gerar também, casos de teste para aplicações paralelas nãodeterminísticas.
O objetivo principal desse trabalho é propor uma estratégia para diminuir a intrusão do teste de software em programas paralelos.
Para isso, é necessária a definição de um processo de teste e a construção de um ambiente que implemente as técnicas utilizadas nesse processo.
Como objetivo secundário, busca- se validar as funcionalidades desse ambiente e levantar as dificuldades que eventualmente impossibilitem essa validação.
Este trabalho está organizado da seguinte maneira:
Paralela. Em ele é feita uma revisão da história dos computadores, dando ênfase às arquiteturas paralelas, e descrevendo as razões para o estabelecimento de arquiteturas de alto desempenho, onde são citados os termos &quot;supercomputador «e &quot;grandes desafios».
CPTS: Centro de Pesquisa em Teste de Software ­ Cooperação PUCRS/ HP-Brasil.
CAP: Centro de Pesquisa em Aplicações Paralelas ­ Cooperação PUCRS/ HP-Brasil.
Em o Capítulo cinco é apresentado o ambiente desenvolvido para a validação da estratégia proposta (objetivos do trabalho).
Em ele é apresentado o processo de teste, o qual define as etapas e técnicas utilizadas, além de um ciclo de teste, que oferece mais detalhes às abordagens de cada etapa envolvida.
São apresentadas também:
As funcionalidades do ambiente de teste;
E os resultados e dificuldades encontradas na tentativa de validar essas funcionalidades.
Em o Capítulo seis, a estratégia proposta para a diminuição da intrusão do teste de software em programas paralelos é apresentada.
Experimentos são discutidos e seus resultados avaliados, visando medidas de desempenho que comparem a nossa abordagem de monitoração com uma outra ferramenta de monitoração comercial.·
Por fim, o Capítulo sete apresenta as conclusões e trabalhos futuros.
Este capítulo introduz o ambiente de trabalho descrito nessa dissertação.
O domínio do problema faz parte da computação paralela, e é importante para descrever as razões para o estabelecimento de arquiteturas de alto desempenho, o que é indispensável para a solução de grandes problemas ligados à ciência e à engenharia.
São apresentados os termos &quot;supercomputador «e &quot;grandes desafios», assim como uma visão geral da situação atual da área de computação paralela de alto desempenho.
Desde a invenção dos primeiros computadores que o poder computacional oferecido por essas máquinas não satisfaz as exigências de engenheiros, cientistas e usuários que necessitam de alto desempenho para a solução de problemas complexos.
Conseqüentemente, essa necessidade se tornou uma busca incansável por desempenho, resultando numa grande evolução na história dos computadores.
Basicamente já se passaram quatro séculos de tentativas na construção de máquinas computacionais.
Muitos projetos tiveram sucesso, entretanto, diversos nem saíram do papel.
Mesmo assim, todos os esforços foram válidos, pois algo que no passado era apenas um sonho, hoje é a realidade, a qual tornou os computadores indispensáveis à vida das pessoas.
Os principais avanços na computação vieram com a criação das primeiras máquinas de calcular e com os primeiros computadores, os quais vêm sendo desenvolvidos e aperfeiçoados ao longo de os anos.
Uma das mais importantes investidas na área computacional, e que merece registro histórico, foi a do inglês Charles Babbage por volta de 1830.
Ele projetou dois computadores, o Difference Engine (Dispositivo Diferencial) e o Analytical Engine (Dispositivo Analítico), sendo o primeiro para realizar computações matemáticas mais especificas, e o segundo para propósitos matemáticos gerais.
Ambos representaram grandes avanços científicos para a época, embora não tenham sido implementados.
Outro importante avanço, e que foi uma das primeiras tentativas de construção de computadores eletrônicos, ocorreu por volta de 1930, por John Atanasoff.
Seu computador era baseado em válvulas, e tinha o propósito de resolver equações lineares.
Entretanto, os primeiros computadores de propósitos gerais, foram provavelmente o COLOSSUS e o ENIAC, construídos entre 1940 e 1946.
Parte da motivação do ENIAC, e analogamente à primeira máquina de Babbage, foi a necessidade de construir tabelas de forma automática, algo que despertou o interesse do exército americano.
Fisicamente, o ENIAC era uma máquina enorme que pesava trinta toneladas e empregava cerca de 1800 válvulas.
Com o avanço das pesquisas e o conseqüente desenvolvimento tecnológico, houve grandes modificações nos computadores ao longo de o tempo.
Como reflexo dessa evolução, a tecnologia e os estilos usados na construção e programação de computadores formaram várias gerações, conforme a Tabela 1.
Como pode ser visto na Tabela 1, em termos de tecnologias de hardware, a primeira geração criada foi a das válvulas.
A segunda geração foi a dos transistores.
A terceira geração foi marcada por os circuitos integrados como o SSI (Small- Scale Integrated).
Já na quarta geração, surgiram os microprocessadores de larga escala, ou circuitos VLSI (Very Large-- Scale Integrated).
E por último, veio a geração atual caracterizada por os computadores paralelos e circuitos ULSI (Ultra Large-- Scale Integrated).
Em as tecnologias de software, a primeira geração foi marcada por o uso de linguagens de máquina e assembly.
A segunda geração foi marcada por as linguagens de alto nível como Algol e Fortran.
Em a terceira geração, predominou o uso da linguagem C, com sistemas multiprogramados e com time-sharing.
Já na quarta geração, a programação paralela se firmava com os compiladores paralelos e bibliotecas para troca de mensagens.
E na geração atual, as tecnologias de software existentes estão baseadas no suporte à programação orientada a objetos, e à programação paralela e distribuída.
Desde os primeiros trabalhos de Von Neumann, que as arquiteturas paralelas vêm sendo apresentadas como uma forma de obter uma maior capacidade de processamento para a execução de tarefas complexas.
Esse aumento de processamento só podia ser obtido através de processadores mais velozes ou através do aumento do número de processadores empregados em conjunto.
Entre os anos 50 e 60, as tecnologias de fabricação de máquinas monoprocessadas3 eram suficientes para atender à demanda de processamento daquela época.
Entretanto, a partir de os anos 70, as necessidades crescentes não eram mais suportadas por tais arquiteturas, tornando- se necessário a utilização de técnicas de concorrência4 para alcançar o desempenho requerido.
Como o aumento da velocidade dos processadores esbarrava no custo e no limite da capacidade tecnológica para o desenvolvimento de circuitos mais rápidos, a tendência para o emprego de vários processadores trabalhando em conjunto foi a solução encontrada.
De essa forma, obteve- se uma maior capacidade de processamento, e definiu- se o termo processamento paralelo, o qual designa o uso de diferentes técnicas de concorrência para suportar as exigências de desempenho.
O desenvolvimento da área de processamento paralelo levou ao surgimento de máquinas especializadas em realizar grandes quantidades de operações por segundo, conhecidas como supercomputadores.
Essas máquinas proporcionaram um maior desempenho na resolução de problemas, algo que encorajou os usuários a tentar solucionar problemas cada vez mais complexos.
Áreas em particular como, modelagem numérica e simulação, são consideradas extensões da ciência e engenharia, e que exigem alto desempenho.
Além de essas áreas, diversas outras apresentam problemas que necessitam de supercomputadores para serem solucionados.
Máquinas monoprocessadas:
Arquitetura de computador formada por um único processador (máquinas convencionais).
Técnicas de concorrência:
Técnicas usadas para a sobreposição computacional, visando obter um melhor desempenho.
As principais formas de concorrência são:
Temporal e espacial.
Em a primeira existe uma sobreposição das execuções no tempo, criando um ganho no desempenho final do processamento.
Enquanto que na segunda, ocorre a sobreposição de recursos através da utilização de vários processadores, ou elementos de processamento que trabalham em paralelo na execução das tarefas que compõem o processamento.
O ganho de desempenho ocorre devido a quantidade de unidades de processamento que trabalham em conjunto.
Alguns dos mais importantes grupos de problemas são os chamados grandes desafios, e não são capazes de serem solucionados em tempo hábil por os computadores seqüenciais atuais.
Um problema típico e considerado um grande desafio é a previsão do tempo, onde cálculos complexos são necessários para a simulação de modelos atmosféricos.
Conseqüentemente, para a obtenção de resultados precisos e rápidos, computadores extremamente poderosos são necessários.
Outros exemplos são:
Dinâmica de fluídos, cálculos astronômicos e aplicações de banco de dados, como sistemas de informações gerenciais (SIG), os quais também necessitam além de alto desempenho, grande quantidade de memória e armazenamento em disco.
O grupo chamado &quot;grandes desafios», é exemplo de problemas que necessitam de computadores de alto desempenho para serem solucionados.
Uma saída eficaz dá- se através do uso de computadores paralelos.
Um computador paralelo consiste, basicamente, num ou mais processadores, com um ou mais núcleos de processamento, que realizam operações computacionais simultâneas.
Com o uso de tal arquitetura, um problema pode ser teoricamente dividido em n subproblemas (onde n é o número de processadores disponíveis), sendo cada parte solucionada por um processador.
O tempo ideal para o término desse processamento deve ser t/ n, onde t é o tempo de processamento realizado num computador seqüencial.
A computação paralela não é uma área nova.
Em 1920, Vannevar Bush apresentava um computador analógico capaz de resolver equações diferenciais em paralelo.
O próprio Von Neumann, em seus artigos por volta de 1940, sugere, para resolver equações diferenciais, uma grade em que os pontos são atualizados em paralelo.
Entretanto, uma das primeiras máquinas a empregar o paralelismo do tipo sobreposição (overlap) foi o UNIVAC I. Essa máquina suportava a sobreposição de execuções de programas com as operações de entrada e saída (E/ S).
A velocidade dos computadores dependia da velocidade de execução das instruções, e da velocidade de transferência dos dados entre a memória e a CPU.
Em 1960, a técnica de acelerar a busca de instruções na memória foi empregada na máquina IBM 7094, onde cada acesso à memória para busca de uma instrução já trazia a próxima instrução junta.
De essa forma, era eliminada metade dos acessos à memória para busca de instruções.
Posteriormente, surgia o entrelaçamento de memória como uma técnica que permitia o acesso simultâneo à memória por a divisão desta em diversos bancos.
Cada um destes bancos podia ser acessado separadamente através de canais, por os quais os dados podiam ser transferidos simultaneamente.
O emprego da técnica de pipeline5 aparece nos computadores STRETCH e LARC.
O primeiro dividindo a execução da instrução em dois estágios:
Uma fase de busca e decodificação da instrução, e uma fase de execução da operação;
Enquanto no segundo, a execução das instruções foi dividida em quatro estágios:
Busca de instrução, operação de endereçamento e índice, busca do operando e execução da instrução.
Uma forma de melhorar a velocidade de transferência dos dados entre a memória e os processadores, foi o emprego de pequenas memórias rápidas, servindo como uma área de armazenamento temporário.
Essas memórias rápidas foram chamadas de memórias cache, e permitiam que as CPUs acessassem os dados nas memórias com mais velocidade.
Seu funcionamento se baseia no princípio de que normalmente o acesso a dados e instruções na memória ocorre na forma seqüencial ou em endereços próximos.
Em a evolução das arquiteturas, um dos problemas que apareceram e que dificultava o ganho de velocidade, foi a dependência entre instruções.
Uma instrução, num determinado estágio do pipeline, pode depender do resultado de uma instrução prévia que ainda não completou seu processamento, e que, portanto, impedirá a instrução dependente de passar para o próximo estágio.
Em as primeiras máquinas essa situação simplesmente causava um atraso na execução do pipeline de instruções.
Para minimizar esse problema, surgiram computadores como o CDC 6000, que minimizavam o número de possíveis dependências através do emprego de um formato de instruções simples, além de hardware específico para funções separadas.
Isto levou ao projeto de uma arquitetura com varias unidades funcionais, tais como somadores e multiplicadores, que executando em paralelo permitia um ganho na velocidade do processamento de instruções.
Posteriormente, surgiram máquinas para a resolução de tarefas que necessitavam de uma grande quantidade de processamento repetitivo sobre um conjunto de elementos individuais de dados (os vetores de dados).
Essas máquinas foram criadas para executar diretamente uma nova classe de instruções, as instruções vetoriais.
De essa forma são eliminadas as tarefas de controle do laço de instruções, simplificando a forma de funcionar dos pipelines.
Esses computadores ficaram conhecidos como máquinas vetoriais ou processadores vetoriais, cujos primeiros exemplos foram o IBM 2938, o CDC Star-100 e o Pipeline:
O princípio da técnica de pipeline baseia- se no conceito de paralelismo temporal, onde a execução de uma nova tarefa pode ser iniciada antes que o resultado da tarefa anterior tenha sido gerado, desde que existam recursos disponíveis no processador para tal (diversos estágios no pipeline).
As técnicas de pipeline, de memória cache e de memória entrelaçada, são atualmente empregadas de forma usual em computadores seqüenciais SISD.
No entanto, possuem limitações de hardware, devido a a tecnologia e ao software inerente ao tipo de programa utilizado (conjunto de instruções sendo executadas).
Uma alternativa a essa situação é o emprego de diversas CPUs com suas respectivas memórias interconectadas, dando origem às máquinas com vários processadores, ou máquinas multiprocessadas.
Os computadores que empregam diversos processadores (MIMD), ou elementos de processamento (SIMD) para aumentar seu poder computacional, têm no ILLIAC IV, seu marco inicial.
Com seu projeto iniciado nos anos 60 e tornado operacional em 1972, o ILLIAC IV, que possuía uma arquitetura SIMD, foi considerado o primeiro supercomputador.
Em os anos 80, as máquinas conhecidas como supercomputadores eram o CRAY, o FUJITSU e outras que agregavam, além de a capacidade vetorial, técnicas de sobreposição, permitindo que várias operações em paralelos fossem desenvolvidas em pipelines diferentes.
O CRAY-1, por exemplo, além de trabalhar com registradores vetoriais ao invés de diretamente com a memória (o que já acelerava tempos de busca de instruções), possuía a capacidade de encadear operações vetoriais, minimizando os tempos de espera por o término de uma operação para iniciar uma próxima.
Esses computadores evoluíram para o emprego de várias unidades de processamento em conjunto, gerando as máquinas multiprocessadas, como o CRAY XMP.
Por último, chegam ao mercado nos anos 90 as máquinas compostas por agregados de computadores, denominadas clusters computacionais.
Primeiramente, eles eram formados de transformaram em clusters de PCs, em os quais máquinas comuns são interligadas através de redes de interconexão de alta velocidade, formando um conjunto de alto desempenho.
Os computadores paralelos podem ser classificados por diversos aspectos segundo sua arquitetura.
Em essa seção, são apresentados três diferentes esquemas de classificação.
O primeiro esquema classifica os computadores paralelos através de sua organização entre processador e memória.
O segundo esquema, conhecido como classificação de Flynn, organiza os computadores através da combinação do fluxo de instruções e fluxo de dados.
E o último esquema classifica os computadores em cinco arquiteturas paralelas diferentes, baseado nos principais modelos físicos existentes.
Em termos de organização entre processador e memória como um critério de classificação de máquinas paralelas, três grupos principais podem ser identificados:
Arquitetura de memória compartilhada, arquitetura de memória distribuída e arquitetura de memória compartilhada e distribuída.
A principal característica da arquitetura de memória compartilhada (Shared Memory), é que todos os processadores (P) têm acesso à mesma memória, existindo apenas um único espaço de endereçamento global.
Em tal arquitetura, a comunicação e a sincronização entre processos são feitas implicitamente através de variáveis compartilhadas.
Em essa arquitetura, os processadores são conectados aos módulos de memória através de algum tipo de interconexão.
Esse tipo de computador paralelo é também chamado de Uma (uniform memory access) ou multiprocessado, permitindo acesso uniforme à memória, uma vez que todos os processadores a acessam com mesma latência e largura de banda.
Uma grande vantagem desse tipo de computador, é que devido a o compartilhamento de memória, a programação é mais conveniente, visto que os dados ficam disponíveis para todos os processos, não sendo necessário uma cópia destes.
Outra vantagem, é que o programador não precisa se preocupar com questões de sincronização, uma vez que isso é realizado por o sistema automaticamente (o que deixa o hardware mais complexo e conseqüentemente com um custo mais elevado).
Entretanto, é difícil obter altos níveis de paralelismo com máquinas paralelas de memória compartilhada (a maioria dos sistemas não permite mais que 64 processadores), pois uma vez que a máquina já esteja construída, é muito difícil agregar mais processadores, devido a o hardware dedicado.
No caso de os computadores de memória distribuída (Distributed Memory), também conhecidos na literatura como multicomputadores, cada processador (P) possui sua própria memória privada (M), não existindo um espaço de endereçamento comum.
A comunicação e a sincronização entre os processadores são feitas por troca de mensagens, através de uma rede de interconexão.
A Figura 2 mostra a organização entre os processadores (P) e os módulos de memória (M) em computadores de memória distribuída.
Ao contrário de os computadores de memória compartilhada, que não são muito escaláveis, os computadores de memória distribuída conseguem bons níveis de escalabilidade, visto que não apresentam problemas de conflito no acesso a memória.
Através do uso desse tipo de arquitetura, os computadores paralelos maciços (MPPs) podem ser construídos com centenas ou milhares de processadores.
Uma representação típica dessa arquitetura são os clusters computacionais, os quais se tornam mais importantes a cada dia.
Em um cluster, cada nodo é um computador e a interconexão desses nodos dá- se com redes de baixo custo, como Ethernet e Myrinet.
A maior vantagem dos clusters sobre as máquinas MPPs, é que os clusters apresentam um custobenefício muito melhor.
Entretanto, ainda não conseguem os desempenhos conseguidos por as máquinas MPPs.
Para combinar as vantagens das outras duas arquiteturas citadas acima (fácil programação e alto nível de escalabilidade), uma terceira categoria de computadores foi estabelecida:
Em essa categoria, cada processador (P) tem sua própria memória local (M), conforme pode ser observado na Figura 3.
Entretanto, ao contrário de as máquinas de memória distribuída, todos os módulos de memória disponíveis formam um único espaço de endereçamento, onde cada célula de memória mantém um sistema de endereçamento único.
Para evitar o baixo nível de escalabilidade dos sistemas de memória compartilhada, cada processador usa um cache de memória, o que sustenta os possíveis conflitos existentes, e também as possíveis latências de interconexão.
Porém, o uso de cache introduz uma série de problemas, como, por exemplo, manter os dados atualizados tanto na memória quanto no cache.
Para solucionar esse problema, utilizam- se técnicas de consistência e coerência de cache, conforme podem ser verificadas em.
Uma alternativa de implementação para essa coerência de cache, chama- se Memória Virtual Compartilhada (Shared Virtual Memory -- SVM).
Em ela o mecanismo de gerenciamento de memória de um sistema operacional tradicional é modificado para suportar esses serviços em nível de páginas ou de segmentos.
A vantagem dessa abordagem é que não são necessárias alterações nas aplicações.
Como exemplo, temos o projeto SHRIMP.
Outra possibilidade é não alterar o sistema operacional e utilizar bibliotecas de funções.
Com essas bibliotecas, é possível converter o código desenvolvido para um espaço de endereçamento único num código que suporte múltiplos espaços de endereçamento.
Em esse caso, o código original tem que ser modificado para incluir compartilhamento de dados, sincronização e primitivas de coerência.
O projeto TreadMarks utiliza esse tipo de abordagem.
A classe das máquinas SISD (fluxo único de instrução e fluxo único de dados) referese às máquinas tradicionais de fluxo seqüencial, ou chamados computadores seqüências, os quais são baseados na arquitetura de Von Neumann.
Conforme a Figura 4, o fluxo de instruções (linha contínua) alimenta uma unidade de controle (C) que ativa a unidade central de processamento (P).
A unidade P, por sua vez, atua sobre um único fluxo de dados (linha tracejada), que é lido, processado, e reescrito na memória (M).
Em essa classe de computadores, estão incluídas as máquinas monoprocessadas, como:
Microcomputadores pessoais e estações de trabalho.
A classe MISD (fluxo múltiplo de instrução e fluxo único de dados) é uma classe que nunca foi implementada.
Em essa classe, múltiplos fluxos de instruções atuariam sobre um único fluxo de dados.
Conforme a Figura 5, múltiplas unidades de processamento (P), cada uma com sua unidade de controle própria C, recebem um fluxo diferente de instruções.
Essas unidades de processamento executam suas diferentes instruções sobre o mesmo fluxo de dados.
Em a prática, diferentes instruções operariam a mesma posição de memória simultaneamente, executando instruções diferentes, o que até os dias de hoje é tecnicamente impossível.
Os computadores paralelos concentram- se nas duas classes restantes:
SIMD e MIMD.
As representações mais importantes da classe SIMD (fluxo único de instrução e fluxo múltiplo de dados) são os computadores e processadores vetoriais.
Essas máquinas têm apenas uma unidade de controle e instrumentação de memória, a qual controla múltiplos processadores.
Tais processadores não foram projetados para uso geral, e sim para aplicações específicas como processamento de imagens.
Conforme a Figura 6, uma única instrução é executada ao mesmo tempo sobre múltiplos dados.
O processamento é controlado por uma única unidade de controle (C), alimentado por um único fluxo de instruções.
A mesma instrução é enviada para os diversos processadores (P) envolvidos na execução, e todos os processadores executam suas instruções em paralelo de forma síncrona sobre diferentes fluxos de dados.
Em a prática, pode- se dizer que o mesmo programa está sendo executado sobre diferentes dados, o que faz com que o princípio de execução SIMD, assemelhe- se ao paradigma de execução seqüencial.
É importante ressaltar que, para que o processamento das diferentes posições de memória possa ocorrer em paralelo, a unidade de memória (M) não pode ser implementada como um único módulo de memória, o que limitaria a somente uma operação por vez.
A classe MIMD (fluxo múltiplo de instrução e fluxo múltiplo de dados) é a classe a qual pertence a maioria dos computadores paralelos atuais.
Enquanto numa classe SIMD, apenas um fluxo de instruções, ou seja, um único programa pode ser executado, numa máquina MIMD, cada unidade de controle (C) recebe um fluxo de instruções próprio, conforme pode ser visto na Figura 7.
De essa forma, cada processador executa o seu próprio programa sobre seus próprios dados de forma assíncrona.
Assim como na classe SIMD, a unidade de memória (M) não pode ser implementada como um único módulo de memória, o que permitiria apenas uma operação por vez.
Em a classe MIMD se enquadram servidores com múltiplos processadores, as redes de estações de trabalho e máquinas como Cm-5, nCube, Intel Paragon e Cray T3D.
Segundo os autores, é possível identificar as seguintes classes de arquiteturas paralelas, conforme a Tabela 3: Atualmente, a maioria das máquinas paralelas está sendo comercialmente produzida, exceto aquelas baseadas na arquitetura PVP, as quais possuem diversos componentes customizados.
As máquinas PVPs, ou Processadores Vetoriais Paralelos (Parallel Vector Processors), são sistemas compostos por poucos e poderosos processadores vetoriais, que são especialmente desenvolvidos para esse tipo de máquina.
A comunicação entre os processadores é feita através de memória compartilhada, formando um espaço de endereçamento único que engloba todos os módulos de memória.
Exemplos dessa arquitetura são:
Cray C-90, Cray T-90 e NEC SX-4.
Em contraste com as máquinas PVPs, as SMPs, ou Máquinas de Multiprocessadores Simétricos (Symmetric Multiprocessors Machines), possuem microprocessadores com caches on- board e off-board, e acesso à memória compartilhada através de barramentos de alta velocidade.
Estes sistemas são considerados simétricos, porque todos os processadores possuem acessos semelhantes tanto à memória compartilhada quanto a os dispositivos de E/ S e também aos serviços oferecidos por o sistema operacional.
Um problema dessa arquitetura é a falta de escalabilidade, que é limitada principalmente por o uso centralizado de memória compartilhada, e também devido a os barramentos de interconexão, o que dificulta expansões de hardware, uma vez que a máquina já esteja construída.
Como exemplos:
IBM R50, SGI Power Challenge e DEC Alpha server 8400.
Os últimos três grupos, MPPs, DSMs e COWs, são máquinas de memória distribuída, o que permite vantagens de escalabilidade perante as limitações de arquiteturas de memória compartilhada, principalmente em aplicações que exigem um alto grau de paralelismo.
As máquinas maciçamente paralelas (Massively Parallel Processors ­ MPPs), geralmente referem- se a sistemas de computação de larga escala.
Esses sistemas possuem diversos nodos individuais de processamento, compostos de microprocessadores e memória local, interconectados com outros nodos através de redes de alta velocidade e com baixa latência de comunicação, o que pode ser escalável a centenas ou milhares de processadores.
Os programas para essas máquinas consistem em múltiplos processos, cada um com seu próprio espaço de endereçamento de memória, e se comunicando através de troca de mensagens.
A sincronização entre processos dá- se por operações de troca de mensagens, ao invés de operações sincronizantes com variáveis compartilhadas.
As máquinas Intel Paragon, Connection Machine Cm-5 e a IBM SP2, são exemplos dessa arquitetura.
As máquinas DSMs (Distributed Shared Memory), ou máquinas com memória compartilhada e distribuída, provêem extensões de software e hardware que permitem um único espaço de endereçamento de memória, enquanto a memória física fica distribuída entre os nodos do sistema.
O que diferência essas máquinas das arquiteturas MPP, é a possibilidade de acesso às memórias remotas e a utilização de coerência de cache.
Como exemplos dessa arquitetura, temos:
Stanford DASH e CRAY T3D.
O último grupo de máquinas de memória distribuída (COW ­ Cluster of Workstations), também conhecida como máquinas agregadas, são baseadas no conceito de cluster, como os sistemas implementados em Digital´ s TruClusters, IBM´ s SP2 e Berkeley NOW.
De certo modo, os clusters são versões de custo mais baixo das máquinas MPPs, o que se torna uma alternativa interessante para instituições de pesquisa.
Uma característica importante desse grupo, é que cada nodo é uma estação de trabalho completa (provavelmente sem alguns periféricos como:
Teclado, mouse e monitor), e que são conectados a outros nodos através de redes também de baixo custo (Ethernet, FDDI, ATM ou Myrinet).
Em contraste com máquinas MPPs, as interfaces de rede são fracamente acopladas ao barramento de E/ S, e cada nodo do cluster contém seu disco local e um sistema operacional completo, enquanto as MPPs não possuem disco local e apenas microkernels integrados.
Outro conceito importante e que possibilita estender a idéia de sistemas de larga escala através de conexões de máquinas distribuídas, são:
&quot;Metacomputers», &quot;MetaSystems «e &quot;Computing Grids «ou &quot;Information Power Grid».
Um exemplo é o projeto multi-institucional conhecido como &quot;Globus Project», que busca alto desempenho através de grids computacionais.
Tais sistemas são compostos por arquiteturas de hardware fracamente acopladas, visando solucionar problemas particulares, e são considerados os sistemas mais poderosos já existentes.
Construir máquinas poderosas como as apresentadas nas seções anteriores, é apenas uma parte da história dos sistemas de computação paralelos.
Tais supercomputadores devem ser programados, e softwares dedicados devem ser criados para que essas máquinas obtenham o desempenho desejado.
Para isso, nessa seção, sobe- se um nível na abstração desses sistemas, e trata- se dos processos que são executados nos processadores dessas máquinas paralelas.
Desenvolver programas para qualquer sistema de computador é difícil e exige bastante tempo.
Entretanto, essa tarefa de desenvolvimento torna- se ainda mais complicada quando se trata dos detalhes inerentes ao paralelismo.
Enquanto a programação seqüencial já é largamente aceita e praticada por a maioria das pessoas, a programação paralela ainda é vista, em alguns casos, como uma rara e exótica subárea da computação, interessante e intelectualmente desafiadora, mas pouco relevante para muitos programadores.
Em o paradigma seqüencial de programação, o programador possui uma visão simplificada da máquina como sendo um único processador que pode acessar certa quantidade de memória.
Já o paradigma de troca de mensagens é uma busca da portabilidade para a programação paralela.
Em este paradigma, várias instâncias do paradigma seqüencial trabalham juntas.
Isto significa que o programador pode imaginar vários processadores, cada um com a sua própria memória, e escrever um programa para executar em cada processador.
Entretanto, a programação paralela, por definição, requer a cooperação entre os processadores para resolver o problema, o que torna necessário a implementação de algum meio de comunicação.
Uma maneira natural de se programar em máquinas de memória distribuída é através da troca de mensagens, assim como, nas máquinas de memória compartilhada, é natural o uso de variáveis compartilhadas.
Entretanto, é possível também inverter ou mesclar esses paradigmas, onde se usa troca de mensagem em máquinas de memória compartilhada e variável compartilhada em máquinas de memória distribuída.
Ambas as abordagens são praticadas.
Contudo, o uso de variáveis compartilhadas em máquinas de memória distribuída exige camadas adicionais de software para prover uma visão compartilhada da memória que se apresenta fisicamente distribuída, conforme foi apresentado na seção 2.3.
A principal característica do paradigma da troca de mensagens é que os processadores comunicam- se através do envio e recebimento de mensagens.
Assim, nesse modelo não há o conceito de memória compartilhada ou de processadores que possam acessar a memória de outro processador diretamente.
O paradigma da troca de mensagens vem se tornando cada vez mais popular.
Uma das razões é o grande número de plataformas que oferecem suporte à troca de mensagens.
Programas escritos neste paradigma podem executar em multiprocessadores de memória distribuída ou compartilhada, redes de computadores e sistemas com um único processador.
Segundo, existem três maneiras de programar num ambiente de troca de mensagens:
Desenvolvendo uma linguagem de programação paralela.
Modificando uma linguagem seqüencial existente para manipular a troca de mensagens.
Usando uma linguagem seqüencial de alto nível já existente, juntamente com uma biblioteca que forneça rotinas de troca de mensagens.
Existem exemplos para as três abordagens citadas.
A linguagem OCCAM, que foi criada para ser usada no Transputer, é um exemplo para a primeira abordagem.
Para a segunda abordagem, diversas linguagens seqüenciais foram estendidas para prover funcionalidades à troca de mensagem.
Como exemplo, é possível citar CC+, que é uma extensão da linguagem C+.
E Fortran M, como extensão da linguagem Fortran.
Em essa dissertação, os estudos estão concentrados na terceira abordagem, onde são utilizadas linguagens de alto nível como Java e C, apoiadas em bibliotecas que ofereçam suporte a comunicação por troca de mensagem.
Existem algumas bibliotecas para troca de mensagem disponíveis, sejam elas comerciais ou não, e de plataformas especificas ou independentes.
Duas bibliotecas muito usadas e que são para plataformas independentes, são:
PVM (Parallel Virtual Machine)[ 18, 1920].
Um exemplo de biblioteca comercial para plataforma específica é NCube.
Em essa seção, o paradigma da troca de mensagem não será abordado em detalhes.
Apenas uma descrição abstrata das funções mais usadas será apresentada.
Mais detalhes pode ser conseguido em.
As funções básicas do paradigma de troca de mensagem são o envio e o recebimento como forma de comunicação.
Essas primitivas podem ser tanto síncronas como assíncronas, conforme a Tabela 4: Um conhecimento importante que se deve ter quando se trabalha com funções de troca de mensagens, é a diferença entre síncrona e assíncrona.
As funções síncronas, ou bloqueantes os processos ficam bloqueados, conforme a Figura 8.
Esse comportamento não bloqueante não apresenta problemas para a função send, mas requer alguns cuidados para a função irecv, pois uma vez que a função de recebimento não bloqueante retorna imediatamente, nada garante que a mensagem foi entregue ou não.
Para esse propósito, existe uma outra função importante e que pode ser usada para verificar se uma mensagem a ser enviada está disponível ou não, que é a:
Através dessa função esse problema pode ser solucionado, retornando true caso a mensagem esteja disponível, ou false caso contrário.
Essa função de teste geralmente é usada num laço para o recebimento, onde a função irecv só é chamada quando a função de teste retornar true, indicando que a mensagem está disponível no seu buffer.
A maioria das bibliotecas de troca de mensagem oferece bem mais funções do que as apresentadas acima.
Por exemplo, é possível especificar grupos de processos para enviar mensagens de maneira coletiva como:
Gather, scatter, reduce, entre outras.
Mais detalhes e a lista completa de funções de comunicação podem ser conseguidos em.
Entretanto, para esse trabalho, as funções apresentadas são suficientes.
A principal diferença desse paradigma para o paradigma da troca de mensagens é o uso de variáveis compartilhadas, ao invés de troca de mensagens.
Como já foi visto na Seção global, com um único espaço de endereçamento.
Conseqüentemente, qualquer local da memória pode ser acessado por qualquer processo.
Isso requer mecanismos de sincronização para evitar que uma mesma variável seja alterada ao mesmo tempo por mais de um processo, num mecanismo chamado de exclusão mútua.
Como nesse trabalho o paradigma utilizado é o baseado em troca de mensagens, mais detalhes sobre as variáveis compartilhadas podem ser conseguidos em.
Em este capítulo é apresentada uma revisão sobre teste de software e depuração.
Inicialmente são apresentados alguns conceitos básicos relacionados ao ciclo de vida de um software, onde é dada ênfase a um modelo de desenvolvimento, assim como modelos de maturidade do processo de teste.
Com isso, contextualizam- se as atividades de teste e depuração no ciclo de desenvolvimento de software.
No decorrer de o capítulo são feitas considerações gerais sobre teste, e uma revisão e classificação das principais nomenclaturas envolvidas.
Em seguida são apresentadas às fases do teste e as principais técnicas e critérios de teste, onde é dada uma atenção especial para o teste de software funcional, dando destaque aos modelos de uso por constituírem o alvo do presente trabalho.
O processo de desenvolvimento profissional de sistemas de software é chamado de engenharia de software6.
Esse processo cobra a aplicação e o uso de métodos e ferramentas na construção de sistemas de software.
Alguns dos principais aspectos da engenharia de software são:
O gerenciamento e a coordenação de projetos, seus componentes e seus relacionamentos, assim como o entendimento do processo de desenvolvimento de sistemas.
Basicamente todo esse conhecimento deve consistir em modelos de ciclo de vida, convenções de trabalho e garantia de qualidade de processo.
Como conseqüência desses aspectos, um sistema de software deve incluir não somente programas de computador, mas também a documentação necessária para o seu desenvolvimento, operação e manutenção.
Modelos de ciclo de vida definem a ordem entre diferentes atividades e suas relações no processo de desenvolvimento de software.
Basicamente três modelos podem ser Engenharia de Software:
Disciplina que aplica técnicas da engenharia para produzir softwares de alta qualidade e baixo custo.
Essas etapas de validação determinam o prosseguimento para as próximas fases do processo.
Em casos de resultados insatisfatórios durante alguma dessas fases de validação, é necessário que o processo retorne à fase anterior.
Entretanto, a cada avanço de fase é indispensável que os resultados sejam bem compreendidos e, de preferência, documentados.
Se essas regras não forem seguidas, erros críticos podem ser detectados demasiadamente tarde, geralmente durante a fase de teste e depuração, e quanto mais tarde se der a correção desses erros, mais esforços serão necessários para a conclusão do projeto.
As fases de teste e depuração são os pontos de interesse dessa dissertação e serão abordadas em mais detalhes nas próximas seções.
A busca por produtos de software de alta qualidade tem pressionado os profissionais ligados a área da engenharia de software a identificar e quantificar fatores de qualidade, como:
Como complemento à identificação dessas técnicas de desenvolvimento e melhores práticas individuais de gerenciamento da qualidade7, pesquisadores ligados à engenharia de software concluíram que é importante integrar todos esses esforços dentro de um contexto relacionado a um processo de desenvolvimento de software de alta qualidade.
Com isso, surgiram alguns modelos de maturidade e qualidade de processo, como CMM (Capability Maturity Model), Iso-15504-SPICE e Bootstrap.
Durante anos esses modelos de melhoria de qualidade de processo vêm sendo aplicados e aperfeiçoados com o apoio tanto das indústrias quanto dos centros de pesquisas acadêmicas.
Entretanto, esses modelos são modelos de alto nível e visam a melhoria do processo como um todo, não oferecendo um suporte adequado para a progresso de etapas intermediárias do processo, como o teste e a depuração.
Principalmente na última década, quando os grupos de teste de software começaram a surgir, tornando- se o teste uma atividade independente do processo de desenvolvimento, embora sempre com etapas integradas.
A criação de um processo independente de teste demandou necessidades de melhorias metodológicas e métricas, visto que estas já existiam no processo de desenvolvimento de software original, mas que precisavam ser adaptadas a este novo processo.
Em 1996 foi o modelo TMM (Test Maturity Model).
Esse modelo passou a permitir a realização de diagnósticos do processo de teste dentro de as empresas, possibilitando medidas dos seus níveis de maturidade, e com isso definir um roteiro gradativo de melhorias.
Atualmente existem vários modelos de avaliação da maturidade do processo de teste.
Alguns como o TPI (Test Process Improvement) que é bastante usado na Europa, e outros como o TMM (Test Maturity Model) e TCMM (Test Capability Maturity Model), que são mais populares nos Estados Unidos.
Entretanto, já existe um grande esforço no sentido de integrar os modelos de maturidade de teste (TMM), com os modelos de maturidade da capacitação para software (CMM).
Enquanto isso não ocorre, é preciso tratar o teste como um processo separado e com características próprias, o que justifica a existência de modelos como o TMM.
Mais informações sobre modelos de maturidade e qualidade de software pode ser encontrado em.
Os fatores de qualidade serão abordados na Seção 3.2.2.
O processo de desenvolvimento de software é descrito como uma série de fases, procedimentos, e passos que resultam na criação de um produto de software.
Conforme o escopo dessa dissertação, nosso interesse está focado principalmente nas fases de &quot;teste e depuração», as quais são etapas responsáveis por &quot;encontrar falhas no programa em teste «e &quot;corrigir essas falhas encontradas», respectivamente.
A fase de teste é geralmente iniciada sempre que um módulo ou um programa completo tenha sido implementado, uma vez que seja necessária a revisão deste na busca por comportamentos incorretos.
Por esta razão, a atividade de teste está fortemente relacionada com as atividades de verificação e validação.
Enquanto a primeira é utilizada para provar o quanto um programa está correto de acordo com as suas especificações de comportamento, a segunda visa avaliar a execução do programa através de um conjunto limitado de entradas.
Mais especificamente, a verificação é um conjunto de atividades que garante que o software implementa corretamente as funções de comportamento especificadas.
Em esse sentido, um programa é dito correto, quando este cumpre com as suas especificações, permitindo aos programadores terem certeza de que &quot;construíram certo o produto».
Em contraste com a verificação, o processo de validação é aplicado para conferir se um sistema ou algum módulo em particular, correspondente a um determinado requisito, está de acordo com o resultado observado.
Para tal processo, o programa deve ser executado num ambiente de monitoração ou simulação.
Em esse contexto, a etapa de teste pode também ser descrita como a aplicação interativa da validação, ajudando os programadores a ter certeza de que estão &quot;construindo o produto certo».
Para uma melhor compreensão dos conceitos tratados nessa dissertação, nesta seção será definido um vocabulário relacionado aos processos de teste de software e depuração.
O conhecimento desses termos básicos é essencial para assegurar que os conceitos apresentados estão baseados num vocabulário comum, e que este é aceito tanto por a academia quanto por a indústria.
VV&amp; T (Verificação, Validação e Teste):
Atividade relacionada com garantia de qualidade agregada ao processo de desenvolvimento de software.
Muitos dos conceitos a serem tratados estão baseados em termos descritos no IEEE Standards Collection for Software Engineering.
Esses padrões incluem o IEEE Standard vocabulários referentes à engenharia de software, e de onde foram retiradas as seguintes definições:
Erro (Error) 9: Um erro é um engano, uma ação humana que produz um resultado incorreto.
Esse erro causa uma diferença entre o valor obtido com a execução do programa e o valor esperado na especificação.
O erro pode ser cometido por programadores, analistas e testadores em situações adversas.
Como exemplo, pode- se citar um documento com uma especificação errada, ou um código mal escrito por falta de entendimento de sua especificação.
Falha (Fault):
Uma falha é a incapacidade de um software ou componente de software de realizar as funções esperadas com o desempenho exigido por as suas especificações.
Também é considerada como a produção de uma saída incorreta em relação as suas especificações.
Defeito (Failure):
Um defeito é introduzido num software como o resultado de um erro.
É um passo, processo ou definição de dados incorretos.
Algumas vezes os defeitos são considerados &quot;bugs».
Casos de Teste (Test Case):
A abordagem básica para a detecção de falhas num programa é selecionar um conjunto de dados de entrada (input data), e então executar esse programa com esses dados num conjunto de condições específicas.
A definição do resultado do teste só é possível, caso se tenha os resultados desejados (output data).
Assim, torna- se possível comparar o que realmente aconteceu na execução do programa com o que era esperado.
Um caso de teste deve conter todas essas informações, ou seja, além de os dados externos à aplicação que servem como um estímulo ao teste (dados de entrada), exercitando a aplicação em teste, este deve conter também parâmetros de execução, e os resultados esperados para as saídas da execução do programa.
Conjunto de Teste (Test Set):
O conjunto de teste é um grupo de casos de testes relacionados, ou um grupo de casos de teste e procedimentos de teste.
Entretanto, um grupo de testes relacionados e que são executados em conjunto é conhecido como test suite.
Oráculo de Teste (Test Oracle):
Um oráculo de teste é um documento ou um módulo (componente) de software, que permite determinar quando um teste é bem sucedido ou não, ou seja, quando um caso de teste encontrou falhas durante o teste ou não, Em o contexto dessa dissertação, os termos defeito e erro são tratados como erro (causa).
E o termo falha, como sendo a conseqüência, ou a identificação, a manifestação de um comportamento incorreto do programa devido a algum tipo de erro.
Engineering Terminology, há duas definições para o termo qualidade.
&quot;A primeira refere- se ao quanto um sistema, componente de sistema ou um processo cumpre com seus requisitos especificados».
&quot;E a segunda refere- se ao quanto um sistema, componente de sistema ou um processo cumpre com as necessidades e expectativas de usuários e clientes».
Para determinar a qualidade de um sistema, são usados alguns atributos, os quais são características que inspiram qualidade.
Como essas definições são esforços internacionais, muitos desses atributos não possuem uma tradução literal para o português.
De essa forma, nesse trabalho, os nomes serão apresentados em inglês, e apenas o significado será traduzido para o português.
Alguns exemplos desses atributos são:
Correctness: Certeza de que um sistema executa as suas funções pretendidas.
Reliability: Certeza de que um software é capaz de realizar suas funções requeridas sobre certas condições pré-definidas, e por um período de tempo pré-definido.
Usability: Relaciona- se à quantidade de esforço necessário para aprender a operar, preparar entradas e interpretar as saídas do software.
Integrity: Relaciona- se à habilidade do sistema de suportar ataques intencionais e acidentais.
Portability: Relaciona- se a capacidade do software de ser transferido de um ambiente computacional para outro diferente.
Maintainability: Relaciona- se a capacidade de manutenção de software.
Interoperability: Relaciona- se ao esforço necessário para interligar ou acoplar um sistema a outro.
Testability10: Este atributo desperta mais interesse aos programadores e testadores, do que aos clientes, e pode ser definido de duas maneiras:
Os testadores devem trabalhar em conjunto com analistas, projetistas e desenvolvedores durante o ciclo de vida do software para assegurar que os requisitos de testabilidade são atingidos.
É importante lembrar que os termos falha, erro e defeito, podem ser encontrados na literatura da área de teste com outras interpretações, embora às apresentadas acima façam parte de um padrão mundial estabelecido.
Entretanto, esses termos também podem ser utilizados em outras áreas não relacionadas ao teste de software, como a Tolerância a Falhas ou Dependabilidade.
Diferente da abordagem do teste de software, que busca encontrar as falhas existentes no software e corrigir essas falhas, a tolerância a falhas busca garantir a capacidade de um sistema em manter seu correto funcionamento mesmo na presença de falhas.
Essa capacidade aumenta a confiabilidade desses sistemas, visto que as falhas são inevitáveis, mas as conseqüências destas podem ser evitadas.
A principal diferença, ou a principal confusão causada por esses conceitos, é que no teste, o termo erro é usado como causa de uma falha.
Enquanto na tolerância a falhas, esses termos são tratados de maneira inversa, onde a falha é a causa dos erros, podendo esta ser considerada de hardware, software, humana ou intencional (falha maliciosa).
E ainda ter uma causa, natureza, duração, extensão e valor.
Embora a tolerância a falhas seja uma área importante quando se trata de sistemas computacionais, uma revisão dos seus conceitos foge um pouco do escopo dessa dissertação.
O importante mesmo é delimitar esses conceitos para a área de teste de software, que será tratada no resto do texto.
Um dos principais problemas do teste de software é que, de maneira geral, este pode no máximo demonstrar a existência de falhas, mais nunca provar a ausência destas.
Além disso, testes exaustivos são geralmente impossíveis de serem executados, pois o domínio funcional de uma aplicação em teste pode ser infinito, resultando numa quantidade muito grande de casos de teste.
O custo financeiro das etapas de teste também é um fator que dificulta esse processo, visto que este pode chegar a custar mais de quarenta por cento do valor total de um sistema.
Outro fator importante, é que geralmente as primeiras versões de um programa quase nunca estão corretas.
Conseqüentemente é provável que qualquer programa contenha algum tipo de erro.
Uma razão para isso é a própria complexidade envolvida no processo de desenvolvimento, algo existente tanto na interpretação de uma especificação, como no sincronismo entre as diversas equipes envolvidas.
O critério de parada também é outra razão que dificulta o teste.
Basicamente existem duas possibilidades:
Uma estatística, onde estatísticas de erros indicam certo grau de confiabilidade;
E outra sistemática, que busca determinar métodos para identificar propriedades funcionais, além de classes de possíveis erros.
A atividade de teste de software consiste basicamente numa análise dinâmica do produto a ser testado, sendo considerada uma atividade relevante para a identificação de falhas e eliminação de erros existentes num sistema.
O conjunto de informações oriundas da atividade de teste é significativo para as atividades de depuração, manutenção e estimativa de confiabilidade de software.
O teste de software envolve basicamente quatro etapas que são executadas durante o ciclo de desenvolvimento do software, que são:
Planejamento do teste, projeto de casos de teste, execução do teste e coleta dos resultados, e avaliação dos resultados coletados.
A concretização dessas etapas se dá em três fases:
O teste de unidade, o teste de integração e o teste de sistema.
O teste de unidade concentra seus esforços na menor unidade de projeto, identificando falhas relacionadas a erros de lógica e de implementação em cada módulo individual de software.
O teste de integração é uma atividade sistemática aplicada durante a integração da estrutura do programa, visando descobrir erros associados às interfaces entre os módulos, ou seja, constrói a estrutura do programa determinada por o projeto a partir de os módulos testados no nível de unidade.
E o teste de sistema, que deve ser realizado após o teste de integração, visando descobrir erros de funções e características de desempenho que não estejam de acordo com a especificação.
As etapas do teste, assim como as etapas da depuração, geralmente ocorrem em ciclos, onde o programa pode ser executado sucessivamente até que se obtenha uma confiabilidade desejada.
O ciclo do teste é usado para detectar computações incorretas, onde o programa é testado com um conjunto de casos de teste que exercitam o seu comportamento.
Enquanto o ciclo da depuração provê informações sobre os estados do programa e também sobre resultados intermediários durante a execução.
A idéia dessas etapas é identificar as O ciclo do teste e da depuração será apresentado no Capítulo 4.
Uma questão importante da atividade de teste, independentemente da fase, é a avaliação da qualidade de um determinado conjunto de casos de teste, visto que é impraticável utilizar todo o domínio de dados de entrada para avaliar os aspectos funcionais e operacionais de um produto em teste.
Assim, o objetivo do teste é utilizar casos de teste que tenham alta probabilidade de encontrar a maioria dos erros com um mínimo de tempo e esforço.
Portanto, um teste bem sucedido é aquele que consegue determinar casos de teste para os quais o programa em teste falhe.
A aplicação do teste segue diversas técnicas que oferecem perspectivas diferentes, abordando diferentes classes de erros, e devendo ser utilizadas de forma complementar.
No entanto, elas diferenciam- se umas das outras por a origem das informações utilizadas no processo de teste, ou seja, quais objetos de teste devem ser testados, e o que deve ser testado nesses objetos.
Para a engenharia de software, essas informações representam os critérios e requisitos de teste, e são indispensáveis para a especificação de um caso de teste.
Existem diversas técnicas para geração de casos de teste, e entre as principais estão:
A funcional (Black
Box), a estrutural (White
Box ou Glass
Box) e a baseada em métodos estatísticos.
Em a técnica funcional, os critérios e requisitos de teste são estabelecidos a partir de uma função de especificação do software, onde o objetivo é determinar se o programa satisfaz aos requisitos funcionais e não-funcionais que foram especificados.
O termo Black
Box se deve ao fato da aplicação ser vista como uma caixa fechada, ou seja, uma caixa onde não se tem acesso ao seu conteúdo, ou a sua estrutura e comportamento interno (código do programa).
Em a técnica estrutural, os critérios e requisitos de teste são derivados essencialmente a partir de as características de uma particular implementação em teste, o que requer a inspeção do código fonte, e a seleção de casos de teste que exercitem partes do código e não de sua especificação.
Por essa técnica permitir o acesso a sua estrutura interna (transparência do código ­ Glass
Box), é que é chamada de caixa-branca (White
Box). Em a técnica baseada em métodos estatísticos, os critérios e requisitos de teste permitem o uso de inferências estatísticas para computar aspectos probabilísticos do processo de teste (resultados do teste), tais como confiabilidade, tempo médio para a ocorrência de falha (MTTF) e tempo médio entre falhas (MTBF).
Observa- se também o uso de métodos estatísticos no estabelecimento de critérios para geração de casos de teste baseados em máquinas de estados finito (FSM), cadeias de Markov (MC) e mais recentemente baseados em Rede de Autômatos Estocásticos (SAN).
O teste estatístico não deve ser entendido como uma técnica de teste de software que venha a substituir o teste funcional ou estrutural.
Em a verdade, ele visa empregar princípios estatísticos e probabilísticos no processo de teste, utilizando- se das técnicas tradicionais abordadas (funcional e estrutural).
Assim, as inovações do teste estatístico se apresentam mais em termos de o processo de teste do que efetivamente em técnicas de teste.
Em, é apresentado o processo típico do teste estatístico baseado em modelos de uso.
Este é dividido em algumas etapas, conforme a Figura 10: Análise da especificação:
O modelo de uso deve ser desenvolvido partindo- se da especificação do comportamento correto do sistema.
Este pode ser definido através de uma especificação formal, documentação dos requisitos, manual do usuário, protótipo, etc..
Desenvolvimento da estrutura do modelo:
São identificados os estados e os arcos de transição entre eles através de um processo manual.
Atribuição de probabilidades:
As probabilidades de transição entre os estados do modelo são atribuídas manualmente ou calculadas automaticamente a partir de a análise de uso.
Verificação e análise do modelo:
Em esta etapa são realizados cálculos sobre o modelo, no intuito de apoiar o planejamento dos testes, a validação do modelo e assim por diante.
Execução de testes não aleatórios:
São gerados casos de teste para cobrir todos os arcos do modelo, seguindo a ordem das probabilidades de ocorrência.
Execução de testes pseudo-aleatórios (random-testing):
São gerados testes pseudo-aleatórios a partir de o modelo, podendo ser executados de maneira automática ou manual.
Estimativa da confiabilidade:
É realizada a análise dos registros do teste aleatório no intuito de estimar a confiabilidade a partir de o registro das falhas ocorridas e dos estados em que ocorreram.
Decisão de parada do teste:
Consiste na avaliação do registro de teste, decidindo quando o teste deve parar ou prosseguir.
Relato dos resultados:
Depois de encerrados os testes, seus resultados podem ser utilizados para decidir sobre a liberação de um produto, avaliar o grau de controle do processo de desenvolvimento, avaliar o desempenho de novos elementos integrados ao sistema e uma série de outros usos.
O teste estatístico com o passar dos anos passou a despertar grande interesse, pois viabiliza a superação de alguns pontos fracos de outras estratégias de teste.
Ele geralmente está baseado em modelos de uso, os quais podem descrever possíveis comportamentos de um determinado software.
A estrutura de um modelo de uso é composta por um conjunto de estados e transições entre esses estados, constituindo um grafo.
Os nodos do grafo representam os estados da aplicação, e os arcos do grafo representam as transições entre os estados.
Esta estrutura descreve os possíveis usos do software.
Entretanto, ao associar uma distribuição de probabilidade à estrutura do modelo, este passa a descrever o uso esperado do software.
Devido a o modelo de uso constituir uma representação formal do software modelado, este pode ser aplicado a diversas fases do ciclo de vida do software.
Ele pode ser utilizado para validar os requisitos do sistema, avaliar a complexidade de um sistema, apoiar o processo de verificação do software, gerar automaticamente casos de teste, direcionar testes, identificar freqüência de eventos, projetar custos e recursos para o teste, definir critérios quantitativos do teste, critérios de parada, confiabilidade, entre outras aplicações.
Em o âmbito do teste estatístico, tais modelos permitem ao engenheiro de teste12 visualizar caminhos críticos mais suscetíveis a falha, direcionando os esforços do teste nesse sentido.
Essa vantagem se dá através da análise das probabilidades de ocorrência, associadas a cada uso do software.
Os modelos de uso geralmente são representados por algum tipo de formalismo.
O primeiro formalismo utilizado no teste estatístico foi Cadeias de Markov.
Entretanto, existem estudos recentes em torno de um outro formalismo chamado Rede de Autômatos Estocásticos Um processo estocástico é definido como um conjunto de variáveis aleatórias definidas num espaço de probabilidades e indexadas por um parâmetro.
Esse parâmetro geralmente refere- se a um conjunto de índices do tempo do processo, ou um intervalo de tempo.
Se tivermos tempo discreto, temos um processo estocástico discreto.
Se tivermos tempo contínuo, temos um processo estocástico de tempo contínuo.
Um processo estocástico é definido como um processo markoviano, quando não possuir memória em relação a o passado do sistema.
Isso significa que apenas o estado atual do sistema influência o próximo passo para a atingibilidade de estados futuros.
É possível representarmos o comportamento de um sistema descrevendo todos os diferentes estados que este venha a apresentar e indicando as transições possíveis de um estado para outro durante a sua execução.
Este sistema pode ser representado como um processo markoviano, quando o tempo gasto em cada estado apresenta- se exponencialmente distribuído.
A este processo markoviano está associado um conjunto de estados, sendo que este pode assumir apenas um estado em qualquer momento.
A evolução do sistema é representada por transições do processo de um estado para outro, transições estas que, assume- se, ocorrem de maneira instantânea (sem consumir tempo).
Quando o espaço de estados de um processo markoviano é discreto (número finito de estados enumeráveis), o processo é chamado Cadeia de Markov.
As cadeias de Markov são classificadas também em relação a uma escala de tempo, sendo cadeias de Markov de tempo discreto (DTMC) e cadeias de Markov de tempo contínuo (CTMC).
Em as cadeias de tempo discreto (DTMC), temos probabilidades condicionais de ocorrem transições de um Engenheiro de Teste:
Pessoa responsável por a criação dos testes e gerência de sua execução.
As probabilidades de transição das cadeias de Markov são representadas através de matrizes de dimensões n x n, sendo n o número de estados desta cadeia.
Estas matrizes são chamadas estocásticas devido a o fato de que cada linha que as compõe é uma distribuição.
No caso de as cadeias de tempo discreto, esta matriz é chamada de matriz de probabilidade de transição, e no caso de as cadeias de tempo contínuo ela é chamada de matriz de taxas de transição.
No caso de as matrizes das cadeias de tempo contínuo, devido a os valores das taxas representarem a freqüência com que as transições ocorrem, a soma dos valores das linhas da matriz não tem sua soma igual a um.
Para solucionar esta diferença, a diagonal principal da matriz é composta por valores negativos, fazendo com que a soma das linhas seja zero.
Estas matrizes estocásticas apresentam correspondência de uma para um com o diagrama que representa uma mesma cadeia de Markov.
A análise do processo estocástico é dita estacionária quando analisa as características estatísticas do modelo de maneira independente do tempo t em que sua observação é iniciada, ou seja, quando o processo não varia, julgando- se um tempo próximo a o infinito.
Já a análise transitória realiza a análise de um estado em função de outro estado prévio, ou seja, a análise probabilística de uma trajetória no espaço de estados.
Através da modelagem do sistema sob forma de uma cadeia de Markov, apesar de o fato desta não possuir memória com relação a estados anteriores da cadeia, pode- se conhecer as probabilidades de se estar em determinado estado, ou conjunto destes, em determinado momento posterior ao início do processo.
É possível também estimar quanto tempo é necessário para atingir- se determinado estado pela primeira vez, e uma série de outras métricas relativas ao sistema e sua evolução.
Em o exemplo da Figura 11, temos um pequeno modelo de uso utilizando cadeias de Markov para a autenticação de usuários numa aplicação.
Em esse exemplo o usuário precisa digitar uma senha válida para acessar o menu opções.
A cadeia é composta por quatro estados (Start, Passwd, PNotOK e Menu) e sete eventos de transição entre esses estados.
É possível notar que em cada evento de transição há uma probabilidade associada, a qual determina o possível comportamento do sistema (uso operacional).
Uma rede de autômatos estocásticos (SAN) consiste num conjunto de autômatos estocásticos individuais que operam de maneira quase independente.
Cada autômato é representado por um determinado número de estados, juntamente com regras ou funções de probabilidade que regem os movimentos de um estado para outro do autômato.
O estado local de um autômato em determinado tempo t é o estado que este autômato ocupa no tempo t..
Já o estado global da rede de autômatos estocásticos é dado por o estado local que cada um dos autômatos constituintes ocupa neste mesmo tempo t.
As taxas de transição de um estado para outro nas redes de autômatos estocásticos são representadas nos próprios arcos do modelo, sendo exponencialmente (no caso de tempo contínuo) ou geometricamente (no caso de tempo discreto) distribuídas.
Uma SAN pode possuir certo grau de interação entre seus autômatos, sendo esta representada por as taxas funcionais e por os eventos sincronizantes.
No caso de taxas funcionais, que representam uma taxa que varia de acordo com o estado global da rede, estas devem ser avaliadas (calculadas) a cada ocorrência, através da sua fórmula de definição.
Os eventos sincronizantes afetam a rede como um todo, fazendo com que uma transição num autômato dispare simultaneamente uma transição em outros autômatos, sincronizando a interação entre eles.
O estado global de uma SAN pode ser mudado por eventos locais ou eventos sincronizantes.
Os eventos locais, como o próprio nome sugere, alteram o estado individual de determinado autômato da rede, alterando o seu estado local.
Logo, se tem um novo estado global, cuja diferença em relação a o anterior se dá em apenas um autômato.
Já os eventos sincronizantes podem alterar simultaneamente mais de um estado local, ou seja, promovendo alteração de estado em mais de um autômato ao mesmo tempo.
A Figura 12 ilustra estes dois tipos de eventos.
Em ela podemos ver os eventos locais l1, l2 e l3 (autômato A), e o evento local l4 (autômato B), representando as transições que ocorrem unicamente no seu próprio autômato, representando uma alteração no estado local de cada autômato.
Já o evento sincronizante S, representa transições disparadas simultaneamente nos autômatos A e B, significando uma alteração no estado global da SAN através da alteração dos estados locais de ambos os autômatos.
Em o autômato A, cabe ressaltar que apenas uma das transições do evento S é disparada por vez, conforme a probabilidade de ocorrência assumida.
Um evento sincronizante faz com que todos os autômatos afetados por ele disparem uma transição correspondente a este evento.
Uma informação importante acerca de os eventos sincronizantes é que, a condição principal para a ocorrência destes eventos é que eles possam ser disparados em todos os autômatos que os contêm.
Em o caso, por exemplo, de um evento estar presente em 3 autômatos, caso num de eles a transição relativa ao evento seja impossível, não é permitido a ocorrência do evento em toda a SAN.
Os eventos locais são representados por um identificador e a respectiva taxa do evento.
Já as transições representando eventos sincronizantes são identificadas por um nome ou identificador, uma taxa de disparo e a probabilidade de ocorrência do evento.
Logo, temos:
A soma das probabilidades de ocorrência dos arcos de um mesmo evento sincronizante em cada autômato é sempre igual a 1.
No caso de um evento possuir apenas uma transição em determinado autômato, a probabilidade de ocorrência é igual a 1, e pode ser emitida.
Em a Figura 12 a probabilidade de ocorrência é mostrada como terceiro item da tripla do evento S. As taxas relativas aos tipos de evento acima citados podem ser de dois tipos:
Fixa ou funcional.
As taxas fixas, como o próprio nome sugere, são representadas por um número real não negativo, não apresentando variação.
Já as taxas funcionais representam juntamente com os eventos sincronizantes, os dois modos de interação entre autômatos de uma SAN.
Estas taxas não são mais representadas por um número real não-negativo unicamente, mas por uma função discreta dos estados locais de alguns autômatos sobre estes números.
Logo, os estados locais da SAN é que indica qual será a taxa utilizada no momento da transição.
Em a Figura 13, temos, no autômato B, o evento local l4 apresentando taxa funcional f, definida como:
Assim, o valor da taxa será definido por a avaliação da função f, ou seja, em função de o estado local atual do autômato A. As taxas funcionais não se apresentam apenas em eventos locais, podendo ser utilizadas até mesmo em eventos sincronizantes, inclusive para representar sua probabilidade de ocorrência.
A taxa é indicada através da avaliação da função no momento de seu disparo.
A vantagem do uso de taxas funcionais frente a as taxas constantes é a possibilidade de representação compacta de estruturas complexas.
A estrutura da SAN que descreve o possível comportamento do sistema possui os seguintes componentes:
O comportamento do sistema modelado na Figura 15 pode ser observado através de dois casos de teste para esse modelo, conforme a Figura 16.
Em esses casos de teste são apresentados passo a passo, os estados globais da rede e o evento que gerou a transição de estados.
É possível notar que sempre que ocorre um evento sincronizante, os estados dos autômatos envolvidos nesse evento são alterados simultaneamente.
Já quando ocorrem eventos locais, apenas estados locais ao evento são alterados.
Esses casos de teste foram criados num ambiente integrado para geração de casos de teste e scripts para teste estatístico de software ­ STAGE.
O ambiente STAGE foi desenvolvido no CPTS/ PUCRS, num projeto em colaboração com a Hp Brasil.
A principal contribuição desse sistema foi o uso desse formalismo para a representação dos modelos de uso e um modelo intermediário (ISEM ­ Modelo) para mapear a abstração do modelo de uso com a interface dos componentes do sistema.
De essa forma, a utilização de SAN permitiu uma representação modular de sistemas com complexo comportamento não-deterministico, minimizando a explosão de espaço de estados apresentado em cadeias de Markov.
Em a presente dissertação, busca- se adaptar as funcionalidades do STAGE para comportar a complexidade de programas paralelos, e com isso gerar automaticamente casos de teste e scripts para teste funcional de programas paralelos.
Além de o trabalho apresentado em, outros trabalhos demonstram a utilização de modelos de uso e métodos estatísticos em etapas de teste de software, seja em programas seqüenciais ou em programas paralelos.
Em os autores propõem a utilização das cadeias de Markov como um formalismo para a modelagem de uso de aplicações para teste estatístico de software.
A utilização do teste estatístico e dos modelos de uso empregando cadeias de Markov é embasada na necessidade de métodos estatísticos e modelos de confiabilidade na aplicação do teste funcional.
Para tal, é proposto o uso de duas cadeias de Markov diferentes nesse processo, uma cadeia de uso e outra de teste.
A cadeia de uso consiste no modelo de uso em si, ou seja, a representação dos estados e transições do sistema, juntamente com suas probabilidades de ocorrência.
O objetivo desta cadeia é o de apoiar o processo de geração dos casos de teste, possibilitando a aplicação de diversos critérios de cobertura.
Já a cadeia de teste é montada a partir de os dados obtidos no processo de teste.
A cadeia é alimentada por os registros de teste, constituindo uma espécie de histórico do processo.
Até mesmo as situações de falha passam a fazer parte da cadeia (um diferencial em relação a a cadeia de uso), possibilitando inferir a respeito de a confiabilidade da aplicação de acordo com a evolução da cadeia, à medida que novas versões da aplicação são testadas, e incorporadas à cadeia.
Em, diversos modelos SAN foram criados e comparados com modelos de cadeias de Markov em termos de números de estados, escalabilidade e capacidade de leitura desses modelos.
Em essa comparação, SAN apresentou- se superior.
Em, casos de teste foram gerados automaticamente através de modelos SAN.
Já em, SAN é utilizada na construção de modelos de desempenho aplicados a programas paralelos, representando a comunicação entre processos paralelos com comportamentos síncronos e assíncronos.
Como a proposta dessa dissertação está baseada na utilização de modelos de formalismo discreto para a representação comportamental de aplicações paralelas, fundamenta- se a utilização de SAN para essa abstração, devido a sua capacidade em comportar a complexidade dos programas paralelos.
Entretanto, as etapas de teste tornam- se um pouco mais complicadas do que em programas seqüenciais, principalmente devido a o grande número de comunicação e sincronismo necessários.
De essa forma, torna- se indispensável a utilização de técnicas de depuração para a manipulação desses eventos de comunicação.
Após as falhas terem sido identificados durante a fase de teste, o propósito do processo de depuração é encontrar e corrigir erros no código do programa que causaram as falhas, sendo considerada uma tarefa típica de programador.
Uma estratégia para isso é a aplicação de abordagens de depuração cíclica.
Em essa abordagem, após a detecção da falha, o programador re-executa o programa usando os mesmos dados de entrada, fazendo com que o programa reproduza o mesmo comportamento falho.
Esta re-execução acontece sob o controle de uma ferramenta de depuração, o depurador, que permite ao programador executar o programa passo a passo.
Após cada passo, a execução do programa pode ser interrompida, e o programador tem a possibilidade de conferir o estado do programa (inspecionando o conteúdo de variáveis, etc), o que permite obter mais informações sobre as razões do comportamento incorreto do programa.
Se a informação conseguida durante esta re-execução não for suficiente para encontrar e corrigir os erros, o programador pode executar o programa novamente (com os mesmos dados de entrada), tentando obter mais informações.
Este procedimento pode se repetir, até que a informação coletada seja suficiente para localizar e corrigir a falha encontrada.
Para interromper a execução do programa após cada instrução, usa- se breakpoints.
Estes podem ser inseridos manualmente por o usuário, e são consideradas funcionalidades importantes, principalmente se o programador já tem idéia de onde possa estar o erro.
De essa ser responsável por a falha.
Assim, quando a execução do programa for interrompida, o usuário pode continuar a execução passo a passo, tentando identificar o erro.
A depuração cíclica é uma abordagem de depuração genérica, podendo ser vista como uma base para o teste e depuração de qualquer programa.
Em o Capítulo 4, mais detalhes sobre depuração serão apresentados, assim como técnicas adicionais necessárias para o teste e depuração de programas paralelos, principalmente programas não-determinísticos, os quais possuem diversas propriedades que dificultam essas etapas.
Após ter descrito a área de aplicação, &quot;Computação Paralela «(Capítulo 2), e também os principais conceitos sobre teste e depuração (Capítulo 3), neste capítulo é apresentado em mais detalhes os ciclos de teste e depuração, tanto para programas seqüenciais, quanto para programas paralelos.
Iniciando com uma abordagem tradicional sobre teste, no decorrer de as Seções serão introduzidos alguns problemas e dificuldades encontradas ao se analisar programas paralelos, o que nos levará a uma revisão sobre os obstáculos da depuração devido a os comportamentos não-determinísticos, assim como possíveis extensões no ciclo tradicional de depuração cíclica.
De essa forma, nesse capítulo serão revisados conceitos importantes que servem como um breve referencial teórico, assim como algumas soluções e suas ferramentas relacionadas.
As fases de teste e depuração aplicadas no ciclo de vida de um software ocorrem, geralmente, sempre que o final do estágio de implementação é alcançado, onde se obtêm uma versão do software ou componente de software em seu estado executável.
Isso significa que todos os erros sintáticos foram removidos, permitindo que o código seja compilado de maneira correta.
O teste e a depuração sempre operam em dois ciclos, onde o programa é executado várias vezes até que um grau suficiente de confiabilidade tenha sido obtido.
O ciclo do teste é usado para detectar computações incorretas, enquanto o ciclo da depuração permite obter mais informações sobre os estados e resultados intermediários durante a execução do programa.
A idéia principal é identificar, localizar e corrigir os erros existentes no programa, baseado na comparação dos resultados esperados com os resultados observados na execução.
Em esse contexto de teste e depuração, os pesquisadores Blum e Wasserman apresentaram o termo simple checking, como sendo uma abordagem básica para o teste de software, e podendo ser definida como:
&quot;seja f uma função matemática qualquer, x uma entrada, y uma saída e P um programa em teste, simple checking determina que um programa esteja correto quando y $= f, ou seja, quando as saídas esperadas (y) são iguais às saídas executadas (f (x)), identificando uma correta computação da função f».
Em este caso, a função f descreve o comportamento do programa P, que está baseada nas especificações do mesmo.
Em complemento ao simple checking, na Figura 18 é mostrado o ciclo tradicional de teste e depuração.
Esse ciclo é representado por um diagrama de fluxo, onde o fluxograma da esquerda representa o ciclo do teste de software, e o da direita o ciclo da depuração.
A fase de teste inicia com a seleção dos dados de entrada (casos de teste).
Conseqüentemente, os usuários escolhem diversas entradas de um conjunto X de entradas válidas.
Estas entradas são selecionadas para a execução do programa P, a qual deve computar um conjunto de saídas (y) correspondentes.
Após o término da execução do programa, esta é verificada na busca por falhas que possam ter ocorrido.
Essas falhas podem ser:
Um término inesperado da execução do programa, ou uma computação de um resultado incorreto.
Assim, essas duas hipóteses devem ser verificadas, imediatamente após o término da execução do programa.
Aqui, situações distintas podem ser observadas, como:
A execução do programa pode falhar antes que o programa tenha atingido um estado esperado, ou a execução terminou corretamente.
Claro que uma falha é sempre uma indicação de um comportamento incorreto, não importa se gerou um resultado incompleto, ou se não gerou resultado.
Conseqüentemente, o ciclo da depuração inicia- se imediatamente logo após essa verificação.
Em situações em que o programa termina de maneira correta, a situação é mais complexa, porque o programa produz certa quantidade de dados de saída, o que parece correto a primeira vista.
O problema é que esses resultados devem ser verificados, o que em muitos casos é complicado, dependendo da técnica usada.
O uso de técnicas estatísticas foi utilizado em, onde o autor utilizou modelos de uso para a verificação dos resultados, através da detecção de erros freqüentes no uso operacional do software.
Uma abordagem semelhante foi proposta por, onde o autor utilizou métodos estatísticos para a caracterização da execução de programas.
Em essa abordagem, as execuções obtidas são sempre amostras do conjunto de possíveis execuções, assim, métodos estatísticos são aplicados para a análise do programa.
Uma diferença importante entre o ciclo do teste e da depuração, é que o teste opera sobre um conjunto de entradas diferentes, enquanto a depuração analisa a execução do programa baseado numa única entrada que causou a falha no teste.
De essa forma, o ciclo da depuração termina sempre quando um conjunto particular de dados de entrada, selecionado durante o teste, for avaliado.
Conforme o ciclo da depuração, apresentado no lado direito da Figura 18, a depuração tem inicio com a instrumentação do programa.
Isso significa que um código extra é adicionado ao código do programa, o qual serve para observar a execução da aplicação durante a depuração.
Com isso, é possível analisar a execução do programa de maneira interativa, visando a identificação da linha do código que contem o erro.
A abordagem tradicional do teste e depuração apresentado nessa seção provê uma breve revisão de suas principais características, e também uma visão geral de sua aplicação em programas seqüenciais.
Diversas ferramentas para esse propósito foram implementadas, dando suporte aos usuários nessas etapas.
Entretanto, inúmeros pesquisadores aplicam essas técnicas, usadas em programas seqüenciais, para o teste e depuração de programas paralelos, com o objetivo de obter resultados similares.
No entanto, existem muitos problemas na depuração de programas paralelos, o que dificulta o uso de abordagens de depuração tradicionais.
Esses problemas serão discutidos nas próximas Seções.
Em relação a as questões básicas relacionadas aos depuradores seqüenciais, é possível definir as seguintes restrições:
O depurador deve afetar o mínimo possível o comportamento da execução de uma aplicação (princípio de Heisenberg);
O depurador deve ser confiável, permitindo que os usuários tenham certeza de suas funções;
A apresentação das informações do contexto do programa deve ser clara e confiável (variáveis e estados);
As funções do depurador devem comprometer ao mínimo o comportamento da aplicação.
Esse princípio de intrusão ou probe-effect inserido em sistemas de depuração foi avaliado por alguns pesquisadores, e os erros estabelecidos por esses efeitos foram definidos como Heisenbugs.
Em esse caso, a intrusão da depuração deve ser mínima, para que se possa garantir a validade dos dados analisados.
Em particular às aplicações de depuração, onde o comportamento da execução do programa não deve ser alterado.
Em as etapas de depuração, o depurador viola completamente o princípio de Heisenberg.
Simplesmente porque o depurador e o programa depurado estão em memória, sendo controlados por o mesmo sistema operacional, isso pode afetar o comportamento da execução da aplicação de muitas formas, especialmente se o programa for uma aplicação paralela e nãodeterminística.
Segundo, os erros encontrados no teste podem até mesmo ser mascarados, ou não serem percebidos nas etapas de depuração.
Isso é possível, visto que a intrusão causada por a inserção de código extra no programa, pode alterar de alguma forma as disposições dos objetos na memória, fazendo com que os bugs fiquem mascarados.
Isso pode ocorrer até mesmo com um simples printf adicionado ao código.
Outro fator importante é que os depuradores devem oferecer informações confiáveis aos usuários, visto que estes utilizam essas ferramentas para observar e tentar inferir como e onde as falhas aconteceram.
Qualquer informação incorreta provida por o depurador guia o usuário em direções erradas, prejudicando, ou até mesmo, impedindo uma correta investigação do erro.
A principal questão relacionada aos usuários dessas ferramentas de depuração é como estas apresentam o contexto das informações, ou os estados intermediários durante as etapas de depuração, como:
Código fonte, valores de variáveis, informações sobre threads, entre outras.
Segundo, a depuração deve acontecer apenas se puder ser estabelecida uma conexão entre o comportamento observado e o código fonte original, permitindo que este seja monitorado.
Um programa paralelo é considerado basicamente, uma coleção de diversos processos seqüenciais que são executados simultaneamente e se comunicam de alguma forma.
Conseqüentemente, as dificuldades básicas encontradas em depuradores seqüenciais, são também evidentes para qualquer depurador paralelo.
Entretanto, além de essas dificuldades, os programas paralelos apresentam outros problemas.
Em, o autor apresenta três justificativas para esses problemas:
O aumento da complexidade dos programas paralelos, a quantidade de dados depurados e os efeitos anômalos adicionais.
O aumento da complexidade dos programas paralelos refere- se basicamente ao fato de que em programas paralelos se têm vários processos simultâneos, o que dificulta a estratégia tradicional utilizada em programas seqüenciais.
A segunda razão relaciona- se às dificuldades de se observar o que realmente interessa na depuração, evitando o acúmulo desnecessário de informações coletadas.
E a última razão diz respeito às dificuldades que ocorrem devido a a concorrência e ao sincronismo entre os processos, o que não acontece nos programas seqüenciais.
Algumas dessas dificuldades são:
Race-conditions e probeeffects.
E outras se referem ao não-determinísmo e suas implicações nos programas paralelos como irreproducibility effect e completeness problem.
Um comportamento não-deterministico é caracterizado por violar o determinismo de execução, ou seja, múltiplas execuções de um mesmo programa, em condições iguais, resultando em diferentes seqüências de eventos sincronizantes e podendo produzir diferentes caminhos de execução.
Um exemplo de função de comunicação que introduz o nãodeterminismo é a função de recebimento de mensagens MPI_ Recv (buffer, any_ source) da biblioteca de funções do MPI, onde o parâmetro any_ source permite o recebimento aleatório de várias mensagens.
De essa forma não é possível determinar qual das mensagens será aceita primeiro.
Isso vai depender da ordem de chegada dessas mensagens, o que caracteriza uma situação não-determinística, e com condição de corrida (race-condition).
Existem dois problemas principais causados por o não-determinismo, o irreproducibility effect e o completeness problem.
O primeiro é caracterizado como sendo a incapacidade de realizar depuração cíclica (cyclic debugging), ou seja, a incapacidade de reprodução de um erro encontrado na fase de teste, visto que o programa está sujeito a gerar a cada execução subseqüente, seqüências diferentes de eventos sincronizantes, ou diferentes caminhos de execução.
Já o completeness problem está diretamente ligado a critérios de cobertura de teste, sendo um problema encontrado quando se tenta testar o comportamento de um programa paralelo em todos os caminhos possíveis de execução, o que dependendo do número de caminhos a serem testados pode se tornar impraticável.
Segundo, a solução para o irreproducibility effect, é a criação de um mecanismo que crie re-execuções equivalentes a uma execução anterior &quot;observada».
Esse mecanismo foi chamado de record&amp; replay, e é composto por duas etapas.
A primeira etapa é a fase de coleta (record fase), onde a ordem das mensagens são armazenadas num histórico de execução segunda etapa é a fase de re-execução (replay), onde os dados armazenados são utilizados para criar re-execuções equivalentes à execução &quot;observada».
Para o tratamento do completeness problem, algumas técnicas também foram criadas como controlled execution, event manipulation e artificial replay.
Controlled execution é uma abordagem proposta por, que oferece um método para a realização de teste automático.
Baseada numa técnica que descreve o comportamento desejado da comunicação entre os processos, o programa paralelo é executado num comportamento forçado, evitando assim situações de condição de corrida.
Uma outra abordagem dessa técnica foi proposta por, onde padrões de controle são aplicados para estabelecer as ordens entre os eventos de comunicação.
Tais padrões são regras dinâmicas que definem a ordem de interação entre os processos.
Outros trabalhos tratam o completeness problem mais diretamente, conforme pode ser visto em.
Aqui os autores apresentam uma técnica chamada event manipulation e artificial replay.
A idéia básica consiste em coletar informações referentes aos eventos de comunicação sincronizantes numa execução inicial da aplicação, e assim identificar todas as condições de corrida existentes.
Com isso, numa segunda etapa, re-execuções artificiais são criadas através da manipulação das ordens dos eventos, permitindo que a aplicação tenha um outro comportamento.
Os autores afirmam que conseguem investigar facilmente diferentes execuções para um mesmo conjunto de entradas.
Além disso, todas as combinações possíveis de execução do programa podem ser criadas através da manipulação dos eventos, caso todas as combinações das mensagens de recebimento forem testadas.
Inicialmente esse processo era manual, mas em ele foi automatizado sem a necessidade de interação de usuários.
Algumas outras técnicas foram propostas para solucionar os problemas do nãodeterminismo, mas a maioria está baseada em record&amp; replay.
Como o processo de coleta e armazenamento das informações do programa paralelo é feito com a utilização de monitores, um monitor é basicamente um código extra (sonda) adicionado ao código fonte do programa, visando coletar informações.
Essa estratégia também é conhecida como &quot;observar um programa».
O uso de monitores introduz o probe-effect, ou overhead causado por a intrusão do processo de monitoração.
Isso significa que a coleta de informações em tempo de execução pode influenciar o resultado do programa, seja na alteração dos tempos ou na ordem dos eventos.
Em a próxima seção um resumo das principais ferramentas existentes para a construção e análise de programas paralelos será apresentado.
Inicialmente um modelo genérico para a classificação dessas ferramentas será mostrado.
Logo após, estas ferramentas serão classificadas de acordo com suas principais características.
Segundo, qualquer ferramenta de análise de programas paralelos consiste em dois itens principais, um componente de observação (monitoração) e um componente de análise.
Outra característica importante é como e quando esses componentes interagem com a aplicação, sendo classificados em dois grupos:
On-line e off-line.
Quando a interação entre os componentes ocorre de maneira on-line, os dados do monitor são transportados durante a execução do programa para o componente de análise.
Já no caso off-line, os dados necessários para análise são coletados por o monitor enquanto o analisados somente após o término do programa.
Ambas as abordagens tem características diferentes.
Inicialmente métodos on-line são mais flexíveis devido a a capacidade de poder determinar e inspecionar cada estado durante a execução do programa.
Por outro lado, os métodos off-line permitem algumas facilidades, principalmente porque ferramentas on-line apresentam apenas estados referentes ao passado e ao presente, enquanto ferramentas off-line permitem uma análise completa dos estados da aplicação, feita do início até o fim da execução do programa.
Um exemplo dessa diferença pode ser percebido em técnicas para detecção de condições de corrida, onde métodos on-line são incapazes de identificar um conjunto completo de condições de corrida durante a execução do programa.
A maioria das ferramentas de monitoração são partes integrantes de ferramentas de análise, especialmente as ferramentas de análise on-line.
Como exemplo é possível citar algumas ferramentas de depuração como P2 D2, PDBG e PDT que utilizam o Gnu debugger ­ gdb como depurador de baixo nível.
Em esses casos, o monitor é completamente integrado no ambiente de depuração.
Baseado no modelo, nas características e nos problemas apresentados nas Seções anteriores, diversos trabalhos introduzem soluções inovadoras para tais problemas.
Essas soluções correspondem ao estado da arte no contexto de ferramentas para criação, manipulação e análise de programas paralelos.
Em ambientes de teste e depuração existem trabalhos relacionados à descrição de ambientes completos que incluem a depuração como parte integral de suas estratégias de desenvolvimento.
Alguns trabalhos discutem as vantagens dessa estratégia.
Em é descrito uma combinação entre depuradores distribuídos ­ DDBG com ferramentas de teste estrutural ­ STEPS.
Essa estratégia permite localizar erros através de análises simbólicas do código realizado com inspeções do código em execuções controladas.
Uma estratégia parecida é oferecida por num ambiente integrado de ferramentas automatizadas para análise de programas que utilizam padrões de comunicação baseados em Outros exemplos de ambientes integrados podem ser vistos em Grade (Graphical Application Development Environment), e MAD (Monitoring and Debugging Environment).
A idéia básica em Grade é prover um suporte gráfico de alto nível para programação baseada numa linguagem gráfica ­ GRAPNEL para ambientes PVM.
Esse ambiente oferece módulos para a construção, execução, depuração, monitoração e visualização de programas paralelos baseados em troca de mensagens, podendo ser acessado por os usuários através de uma interface gráfica.
Como ferramenta de depuração, Grade integra o DDBG como depurador distribuído.
Outro forte ambiente integrado que combina um conjunto de ferramentas para monitoração e análise de programas paralelos é o MAD.
De acordo com um modelo genérico para ferramentas de análise, os autores classificam os componentes de MAD em módulos de monitoração e análise.
Diversas possibilidades são oferecidas em termos de monitoração.
Uma implementação inicial chamada EMU (Event Monitoring Utility) gera históricos de execução para análise off-line num formato de dados proprietário.
Uma das idéias principais é usar essa ferramenta para medir os efeitos causados na execução da aplicação com as técnicas de monitoração empregadas.
Com isso, diversas estratégias foram implementadas para correção do overhead (intrusão) causado por os monitores, objetivando remover a perturbação tanto no tempo de ocorrência dos eventos quanto em suas ordens.
Outra ferramenta pertencente ao MAD, e que se integra ao EMU, chama- se PARASIT (Parallel Simulation Tool), responsável por gerar execuções equivalentes em programas paralelos não-determinísticos.
NOPE (Nondeterministic Program Evaluator) que representa um mecanismo completo para a técnica de record&amp; replay é outra ferramenta integrada ao ambiente.
Ela apresenta uma característica adicional, a possibilidade de gerar manipulações automáticas de eventos com o objetivo de solucionar o completeness problem, conforme apresentado nas Seções anteriores.
Além de essas ferramentas, MAD apresenta ainda um módulo central ­ ATEMPT (A Tool for Event Manipulation), capaz de visualizar execuções paralelas utilizando diagramas baseados em técnicas de tempo e espaço (spacetime diagrams).
Para a representação gráfica do comportamento de programas paralelos, existem diversos trabalhos que descrevem abstrações de modelos comportamentais em diversos tipos de diagramas.
Segundo, uma das melhores formas é através de Redes de Petri, o que pode ser aplicado em diferentes níveis de abstração de programas com características diferentes.
Outro tipo de representação gráfica é o grafo de dependência (dependency graph).
Ele é capaz de caracterizar visualmente dependências de dados e controle de operações executadas por um programa.
Existem outros tipos de representações gráficas, mas um dos mais usados é o diagrama de tempo e espaço, que foi introduzido por para expressar ordem entre os eventos distribuídos.
Esse diagrama é usado em muitas ferramentas para depuração paralela e ajuste de desempenho.
Um bom exemplo do uso desse diagrama é Paragraph, que possui além de diagramas de tempo e espaço, muitas outras formas de visualização.
Outra ferramenta que incorporou o diagrama de tempo e espaço foi AIMS.
AIMS é baseado em instrumentação em nível de código e provê portabilidade entre diferentes tipos de hardware.
Uma ferramenta similar a AIMS é XPVM, e possui uma interface gráfica para análise de programas paralelos baseados em PVM, abstraindo graficamente tempos de computação, overheads de comunicação e tempos de espera, podendo ser visualizados tanto de maneira online quanto off-line.
Uma comparação similar para programas MPI pode ser vista em Upshot e Vampir, e mais recentemente em Jumpshot4.
Existem outros trabalhos com propostas semelhantes às citadas acima, entretanto uma revisão exaustiva foge ao objetivo do presente trabalho.
Essa seção apresentou os trabalhos mais importantes e que servem de base para a definição da estratégia que está sendo proposta.
Em a próxima seção, o ambiente de teste implementado, será apresentado.
Suas funcionalidades serão exemplificadas, assim como o levantamento dos problemas encontrados.
Após ter descrito a área de aplicação desse trabalho, &quot;Computação Paralela «(Capítulo 2), e também os principais conceitos sobre teste e depuração (Capítulos 3 e 4), aqui será apresentado o ambiente de teste que foi implementado para dar suporte à estratégia proposta.
Esse ambiente será detalhado e suas funcionalidades serão avaliadas assim como as dificuldades encontradas na avaliação dos resultados.
Assim como em outros processos de teste de software, o processo utilizado nesse trabalho consiste basicamente em aplicar um conjunto de casos de teste na execução de uma aplicação, e verificar se há inconsistência entre o resultado obtido e o resultado esperado.
A diferença é que, nesse trabalho, as aplicações a serem testadas são aplicações paralelas, e conforme apresentado no Capítulo 4, exigem muito mais cuidados nas etapas de teste do que os programas seqüenciais.
Além de o não-determinismo, o qual possibilita que uma aplicação paralela apresente caminhos de execução diferentes, existe também o fato de estarmos tratando de programas com diversos processos simultâneos e com a ausência de um relógio global, o que torna indispensável a utilização de técnicas de depuração e monitoração, principalmente para a identificação dos estados atingidos em cada processo envolvido durante a execução do programa a ser testado.
Uma vez que a proposta deste trabalho é uma estratégia para diminuir a intrusão do teste de software em programas paralelos, um ambiente de teste foi desenvolvido, e teve sua proposta formalizada e apresentada na Conferência Latino Americana de Informática, conforme Anexo A. Para a implementação desse ambiente de teste, foi definido um processo o qual define as etapas e técnicas utilizadas, além de um ciclo de teste, que apresenta mais detalhes às abordagens de cada etapa envolvida.
A técnica de teste utilizada é a técnica de teste de software funcional baseada em modelos de uso.
Ela, juntamente com um modelo de falhas (que será apresentado na Seção 5.3.4), visa testar o comportamento do programa, buscando falhas na comunicação entre os processos.
Para a representação dos modelos de uso, adotouse SAN como o formalismo capaz de suportar a complexidade dos programas paralelos.
Em as próximas seções, o processo de teste será apresentado, assim como a metodologia utilizada, as principais funcionalidades, além de as dificuldades encontradas na avaliação dos resultados.
O processo de teste é mostrado na Figura 20.
As técnicas utilizadas nesse processo norteiam as contribuições mais importantes do ambiente de teste, e estão baseadas na geração automática de casos de teste e scripts para teste de software funcional, bem como, na criação de um módulo de análise on-line que implemente o teste funcional baseado nos estados do modelo (casos de teste).
Conforme a Figura 19, a geração dos casos de teste é feita automaticamente através do modelo SAN, como sendo um possível ou esperado comportamento operacional da aplicação em teste.
Com essa abordagem, busca- se criar através das estimativas probabilísticas do modelo, casos de teste não-determinísticos.
Para a implementação do processo de teste definiu- se um ambiente integrado conforme a Figura 20.
O ambiente é composto por três módulos principais:
Módulo gerador de casos de teste e scripts, módulo de análise e módulo de monitoração e execução, todos desenvolvidos em JAVA.
O módulo gerador de casos de teste e scripts estende os conceitos utilizados em.
A interação entre os módulos do engine e a aplicação em teste é feita de maneira online, onde os dados monitorados são transferidos para o módulo de análise em tempo de execução.
Esses dados são coletados através de uma biblioteca de funções de monitoração, que é associada à execução do programa no instante da instrumentação do código da aplicação.
Com a integração desses módulos, é possível monitorar e analisar a execução da aplicação através dos estímulos oferecidos por o script de teste, testando o comportamento do programa na busca por situações de inconsistência entre os estados atingidos na execução e os estados do modelo.
Assim, sempre que for identificada uma inconsistência (falha) entre os estados e eventos testados, se fixa essa falha, a qual é reportada num histórico da execução intermediários que ajudem na identificação do erro que causou a falha identificada.
Em as próximas seções, as principais funcionalidades serão explicadas, objetivando apresentar mais detalhes as técnicas utilizadas.
Em essa etapa, a especificação do programa é usada para a modelagem da aplicação em SAN.
A estrutura de uma SAN foi apresentada no Capítulo 3, e é composta por diversos autômatos, onde cada autômato corresponde a um processo da aplicação modelada.
Como exemplo, utilizaremos uma aplicação paralela não determinista para ordenação de vetores.
Como o próprio nome diz, esse programa consiste em diversos processos, onde um de estes (o mestre) fica responsável por controlar a distribuição e o recebimento dos vetores a serem ordenados por os outros processos (escravos), os quais ordenam os vetores recebidos e os devolvem para o mestre.
O modelo SAN dessa aplicação é apresentado na Figura 22.
Esse modelo é composto apenas por três autômatos, um mestre e dois escravos.
Conseqüentemente, o código desse modelo deve ser implementado para executar em três processos.
Após a criação do modelo, as probabilidades de ocorrência devem ser inseridas, as quais determinam a ordem de acontecimento dos eventos, e conseqüentemente, a ordem dos estados globais atingidos por o modelo.
Em a Figura 22, esses valores não estão explicitados.
Para auxiliar na geração dos casos de teste e scripts, algumas convenções foram estabelecidas, e podem ser verificadas na própria Figura 22: Cada autômato terá seu estado inicial aleatório, a menos que contenha um estado do tipo start, sendo identificado visualmente através de uma seta ao lado esquerda do estado.
É necessário que apenas um autômato tenha um estado do tipo start_ master, o qual iniciará primeiro que os demais (Mestre.
Not_ set).
É necessário também que algum autômato tenha um estado final (quit_ master) (estado QUIT de todos os autômatos).
Apenas os eventos de início, fim e sincronizantes devem ser do tipo master ou slave.
O restante dos eventos deve ser local.
Eventos do tipo master chamam eventos do tipo slave, alterando os estados de ambos os autômatos de maneira sincronizante.
Eventos locais alteram apenas estados locais de autômatos.
Cada autômato possui um conjunto de estados locais, e o estado global da rede é identificado através do conjunto de estados locais de cada autômato.
A descrição dos eventos de transição de estados do modelo é a seguinte:
O Start (master):
Evento para inicialização do processo.
Logo após o modelo SAN ter sido criado, e as probabilidades terem sido atribuídas, gera- se então os casos de teste.
Estes são gerados aleatoriamente cobrindo todos os arcos (eventos) do modelo, seguindo a ordem das probabilidades de ocorrência atribuídas na etapa anterior.
Com os casos de teste criados, gera- se o script de teste, onde são incluídos apenas os casos de teste não repetidos.
Como a técnica de teste utilizada é a técnica de teste funcional baseada em estados do modelo, cada caso de teste contem uma seqüência de estados globais e eventos, os quais têm sua atingibilidade testada na execução da aplicação.
Para isso, o script de teste foi definido num formato de arquivo XML, conforme a Figura 23.
Em o script são definidos o número de autômatos (linha 4), um nome para o mapeamento entre os autômatos do modelo e os processos da aplicação, um tempo máximo de espera do teste, e um conjunto de casos de teste gerados.
Em cada caso de teste devem estar definidos os dados a serem testados (conjunto de estados globais e eventos), os quais devem obedecer a seguinte ordem:
É importante lembrar que, como na geração dos casos de teste os autômatos são percorridos de um estado inicial até um estado final, cada caso de teste contem uma seqüência de dados equivalente a um caminho de execução.
Conseqüentemente, um script formado por diversos casos de teste contem, na verdade, um conjunto de caminhos esperados no uso operacional da aplicação em teste.
E quando se testa o programa, verifica- se se o caminho percorrido na execução é igual ao caminho contido no caso de teste.
Como todo o ambiente de teste foi implementado em JAVA, optou- se por usar JAVA e MPI/ JAVA, para a codificação da aplicação paralela, e também para a biblioteca de troca de mensagens, respectivamente.
Tanto a codificação quanto a instrumentação da aplicação devem basear- se no modelo SAN.
A Figura 24 apresenta um trecho do código de uma aplicação, já instrumentado.
Em ele, é possível notar que são inseridas algumas funções de método do engine de teste, capaz de identificar a ocorrência de eventos em tempo de execução, e logar os estados atingidos por os processos envolvidos.
Tal função deve ser inserida na etapa de instrumentação do código, e possui os seguintes parâmetros:
Esses parâmetros indicam, através do evento ocorrido (Event), qual o estado que está sendo atingido (State), o tipo de evento (Type -- sincronizante (SYN) ou local (LOC)), e em qual processo esse evento ocorreu.
De essa forma, essa é a abordagem utilizada para monitorar o comportamento da aplicação em teste, o qual é identificado automaticamente em tempo de execução.
Em termos de arquitetura e implementação, o engine de teste é instanciado num processo que roda em paralelo aos processos da aplicação.
Assim, sempre que algum processo o processo do engine, a qual contem os dados referentes ao evento e estado atingido.
De essa forma, o engine fica constantemente recebendo essas mensagens e salvando os estados globais atingidos no histórico de execução.
Para a etapa de teste, o script é lido por o engine, e os casos de teste são armazenados em memória.
Com isso, a aplicação é executada e monitorada, e os estados de execução atingidos são identificados e comparados automaticamente com os estados e eventos lidos do caso de teste.
O teste é considerado bem sucedido, quando o caso de teste aplicado identifica alguma falha, ou seja, quando ocorre alguma inconsistência entre os estados testados.
Um modelo de falhas foi definido baseado nas falhas tradicionais de sistemas paralelos e distribuídos.
Como exemplo dessas falhas, temos:
Crash faults, fail-stop, omission faults, delay faults, e bizantine faults.
Entretanto, em abordagens de teste funcional (black-box), que utilizam modelos de uso para a geração automática de casos de teste, os modelos de falhas geralmente baseiam- se na cobertura dos possíveis estados atingidos a partir de o modelo.
Como o foco desse trabalho é aplicar o teste funcional para testar o comportamento de aplicações paralelas baseadas em troca de mensagens, e também por o fato do ambiente de execução (Cluster/ MPI) ser considerado um ambiente &quot;comportado «ou facilmente controlado, e assim menos propenso a falhas, o modelo assumido busca as seguintes falhas:
Incapacidade de atingir estados definidos no modelo.
Atingibilidade de estados não permitidos por o modelo.
Possíveis deadlocks e livelocks (time-out).
Baixo desempenho (gargalos de comunicação).
Falhas de omissão.
Inicialmente, falhas vinculadas a atingibilidade de estados, são automaticamente identificadas.
Demais tipos de falhas, devem ser verificadas manualmente através do log de resultados do teste, que contêm o conjunto de estados e eventos atingidos na execução até o instante da falha.
Em essa seção, um experimento será feito visando validar as funcionalidades do ambiente de teste e identificar problemas relacionados.
A aplicação utilizada para isso será a ordenação de vetores, já mencionada nas Seções anteriores.
E o experimento consiste basicamente em aplicar o processo de teste nessa aplicação paralela.
Para validar as funcionalidades de monitoração e análise, foram criados manualmente (para evitar problemas relacionados ao não-determinísmo), casos de teste contendo seqüências de dados de teste, os quais se tinham certeza que seriam atingidos na execução da aplicação.
Buscou- se com isso, evitar problemas com o não-determinismo, e garantir dessa forma, que os estados e eventos contidos nos casos de teste fossem atingidos na execução da aplicação, ou seja, que o comportamento esperado nos casos de teste fosse igual ao comportamento monitorado.
Para se ter essa certeza, diversos scripts foram criados contendo inúmeros casos de teste, e todos tiveram 100% de atingibilidade.
Com isso, é possível validar as abordagens de monitoração e instrumentação utilizadas no engine.
Entretanto, esse experimento inicial serviu para validar as funções de monitoração, buscando provas de consistência na atingibilidade de estados.
Os problemas surgiram quando se iniciaram os testes para verificar a capacidade do engine em identificar falhas, ou seja, durante a etapa de validação do processo de teste, onde casos de teste foram gerados automaticamente a partir de o modelo SAN.
Essas dificuldades foram sentidas quando os resultados dos testes indicavam uma presença muito grande de falhas, mesmo em casos de teste que não deveriam encontrar falhas, caracterizando uma situação de &quot;falsos bugs».
A análise desses &quot;falsos bugs», nos levou a descobrir que o problema que estávamos tendo, era um problema já enfrentado por muitos pesquisadores da área na tentativa de solucionar dificuldades relacionadas ao não-determinísmo.
Algo que muitas vezes pode ser solucionado com técnicas baseadas em record/ replay, conforme já foi mencionado no problema está na geração dos casos de teste, pois geramos casos de teste determinísticos para testar aplicações não-determinísticas.
A ocorrência desses &quot;falsos bugs «dá- se, pois para cada caso de teste a ser testado (Figura 25 -- A), executa- se a aplicação, e testa- se a consistência entre os estados atingidos na execução (Figura 25 -- B), e os estados contidos no caso de teste.
Algo que seria correto em aplicações com comportamento determinístico, visto que é possível testar a aplicação várias vezes com um mesmo caso de teste, que o resultado provavelmente será sempre o mesmo.
Entretanto, como em aplicações paralelas a cada execução podemos ter comportamentos diferentes (caminhos de execução diferentes), conclui- se que essa abordagem para geração de casos de teste não suporta programas paralelos não-determinísticos, o que dificulta a validação do processo de teste, ou seja, a interpretação dos resultados do teste.
Como solução para esse problema, definiu- se como trabalho futuro, a criação de scripts de teste que contenham casos de teste agrupados, ou seja, várias possibilidades de caminhos de execução num único caso de teste (Figura 26 -- A).
De essa forma, quando se testa uma aplicação, verifica- se se o caminho percorrido por esta (Figura 26 -- B) é igual a algum dos caminhos contidos no caso de teste.
Com isso, os casos de teste passam a ser nãodeterminísticos, diminuindo assim os efeitos causados por o completeness problem.
Para que isso seja possível, algumas modificações são necessárias na a geração dos casos de teste.
A principal de elas é a criação de regras que definam na própria criação dos casos de teste, quais caminhos são válidos para aquele modelo, o que é muito difícil de se saber, visto que o número de caminhos possíveis que uma aplicação paralela pode executar cresce conforme seus parâmetros de execução (dados de entrada e número de processos).
Entretanto, para verificar a viabilidade dessa proposta de trabalho futuro, implementou- se um protótipo baseado em perfis de execução.
Com ele, executa- se a aplicação milhares de vezes, e cria- se um rank probabilístico com todos os caminhos de execução possíveis que acontecerem, e através desse perfil operacional, gera- se casos de teste para todos os caminhos identificados.
Através dessa abordagem, verificou- se que a taxa de identificação de &quot;falsos bugs «diminuiu para pouco mais de 3%, o que não interfere no processo de identificação de falhas.
Como a abordagem utilizada nesse protótipo para a geração de casos de teste não é derivada de um modelo abstrato, e sim da execução de uma aplicação aparentemente correta, nada garante que os casos de teste gerados estejam corretos.
Entretanto, os dados obtidos com o protótipo ajudam a perceber que a proposta de casos de teste agrupados é uma solução viável para a geração de casos de teste não-determinísticos, e assim, uma possível solução para a validação do processo de teste.
Em o Capítulo anterior, não foi abordada nenhuma questão referente a a diminuição da intrusão do teste de software.
Apenas foi apresentado o ambiente de teste e suas funcionalidades, assim como os problemas encontrados.
Em esse capítulo, a estratégia utilizada para a diminuição da intrusão será apresentada.
Um experimento será avaliado, visando medidas de desempenho que comparem a nossa abordagem de monitoração com uma outra ferramenta comercial de monitoração.
Tanto as etapas de teste quanto de depuração necessitam de algum mecanismo de monitoração, seja para controlar e analisar a execução do programa a ser testado, ou até mesmo para re-execuções controladas, em caso de depuração cíclica.
Como já foi apresentado no Capítulo 4, as funções do monitor devem influenciar o mínimo possível o comportamento da aplicação.
Essa influência chama- se probe-effect ou intrusão, e deve ser minimizada, para que o comportamento da execução do programa não seja alterado.
A intrusão está diretamente ligada com a quantidade de dados monitorados, e é um problema que se agrava quando se trata de programas paralelos.
Isso ocorre, basicamente, por ter vários processos envolvidos, exigindo mais tempo para a monitoração e também mais memória para o armazenamento desses dados coletados.
Tanto a utilização quanto os tipos de arquiteturas envolvidas na estrutura de uma ferramenta de monitoração foram tratados no Capítulo 4.
Aqui, busca- se apresentar a nossa abordagem de monitoração utilizada no engine de teste e, principalmente, mostrar como estamos buscando a diminuição da intrusão no processo de teste.
Como já mencionado no capítulo anterior, o monitor utilizado nesse trabalho é um processo central que fica constantemente controlando e observando o comportamento dos processos envolvidos na execução da aplicação, conforme pode ser observado na Figura 27.
De essa forma, a cada evento ocorrido, este recebe o estado atingido por o processo que executou o evento, armazenando e atualizando o estado global da aplicação.
Os dados coletados durante a monitoração são dados referentes apenas às mudanças de estado ocorridas, o que já torna essa abordagem menos intrusiva.
Outra maneira de diminuir o problema da intrusão se dá na escolha do que analisar no instante do teste.
Algo que envolve a estrutura dos casos de teste e scripts, assim como a instrumentação da aplicação, para que ambas sejam compatíveis com a abordagem utilizada na monitoração.
Em esse caso, a diminuição da intrusão não ocorre apenas na redução dos dados de teste (estados a serem testados), mas também no que monitorar na execução da aplicação.
Visto que o engine de teste apresenta problemas na geração dos casos de teste, poucos testes foram feitos envolvendo as estratégias relacionadas aos casos de teste.
Entretanto, a concentração maior dos testes se deu nas medidas de desempenho da técnica de monitoração utilizada no engine, as quais serão analisadas na próxima seção13.
Esse experimento foi formalizado num artigo científico e será publicado nos anais do LATW 2006 (7 th IEEE LatinAmerican TestWorkshop), conforme o Anexo B. Existem diversas ferramentas de monitoração que podem servir de modelo para uma comparação de desempenho com o monitor implementado, entretanto, poucas possuem uma estrutura semelhante de dados a ser coletada.
Sendo assim, uma vez que o MPE (MultiProcessing Environment) possui essa semelhança, testes de desempenho comparativos com este foram realizados, buscando validar a diminuição da intrusão do nosso monitor.
O MPE é uma biblioteca pertencente ao pacote do MPI que fornece métodos de monitoração para análise off-line.
Através de ele, é possível coletar informações de transições de estados e sincronização entre processos.
E o que é mais interessante, ele possui o conceito de estado global, visto que este é necessário para a visualização do comportamento descrito no log gerado.
Embora o engine de teste não crie log de execução, pois possui uma abordagem de teste on-line, para esse experimento, algumas adaptações no engine foram No entanto, o formato do log utilizado no MPE é binário, sendo lido apenas por uma ferramenta específica, a Jumpshot (Capítulo 4).
Já em nossa abordagem, o log é em ASCII, podendo ser lido facilmente em qualquer editor de texto.
A aplicação escolhida para os experimentos continua sendo a ordenação de vetores utilizada no capítulo anterior.
Em esse experimento, vinte vetores são ordenados numa abordagem mestre-escravo.
Os testes foram feitos num cluster composto por quatorze máquinas, sendo todas do tipo Pentium III 500Mhz, com 256 MB RAM e interconectadas por uma rede FastEthernet de 100 Mb.
Foram usados três tamanhos diferentes de vetores:
5000, 10000, e 20000 elementos do tipo &quot;inteiro».
O número de escravos utilizados variou de 3, 5, 7, 9, e 11.
As Figuras 28, 29 e 30 apresentam os resultados obtidos a partir de uma média feita através de vinte execuções para cada item avaliado.
Para facilitar a interpretação dos resultados, três gráficos diferentes foram gerados, onde as coordenadas horizontais representam o número de escravos, e as verticais, o tempo de execução obtido em segundos.
Cada gráfico apresenta quatro linhas (itens):
Uma obtida para a versão da aplicação implementada em C/ MPI;
Outra para a implementação em JAVA/ MPI, outra para a implementação em C/ MPE, e outra para a implementação em JAVA usando nosso engine.
A Figura 28 apresenta os resultados obtidos com vetores de 5000 elementos.
Em esse caso, a versão implementada em C/ MPI apresentou o melhor resultado.
Também é possível notar que a intrusão nas versões da aplicação que utilizam JAVA aumenta à medida que o número de escravos também cresce devido a necessidade de uma maior comunicação entre os processos e também a baixa computação exigida.
De acordo com as Figuras 29 e 30, as versões em JAVA, com e sem o engine, apresentam uma curva decrescente constante, isso porque a quantidade de computação necessária torna- se mais significante que intrusão causada quando os dados crescem, chegando a ser quase equivalente no 11º escravo da Figura 29.
No entanto, nesse mesmo ponto, a quantidade de computação por processo não é suficiente para compensar a intrusão causada por a Máquina Virtual do JAVA mais o engine.
E por último, é importante lembrar que, quando o tamanho do problema e o número de processos trabalhadores aumentam, a intrusão do engine torna- se menos significativa, tornando todos os tempos mais próximos, conforme o gráfico da Figura 30.
Esses resultados comprovam outros trabalhos já realizados em relação a o desempenho do JAVA na tentativa de integrar- lo a uma implementação do MPI.
Isso se dá através de chamadas a métodos nativos, como é o caso do mpiJAVA, e acaba impondo restrições de desempenho quando comparadas com implementações em C e FORTRAN.
No entanto, segundo, o mpiJAVA apresenta desempenhos mais próximos a versão nativa (MPI) quando executada com uma maior quantidade de computação e com tamanho maior de mensagem, o que pode ser percebido nas Figuras 29 e 30.
Assim conclui- se que, com os resultados obtidos, e por se tratar de uma ferramenta de monitoração implementada em JAVA, acredita- se que esta é capaz de apresentar baixa intrusão, mesmo que os gráficos demonstrem situações de desempenho inferior ao MPE.
No entanto, isso pode ser visto de maneira positiva, pois o MPE quase não gera intrusão em relação a o MPI e principalmente por estas serem ferramentas que vêm sofrendo melhorias de desempenho ao longo de os anos.
A atividade de teste consiste numa análise dinâmica do produto a ser testado, sendo relevante para a identificação e eliminação de erros que persistem constituindo um dos elementos para fornecer evidências da confiabilidade do software.
O principal objetivo do teste de software é revelar a presença de falhas no produto testado, portanto, o teste bem sucedido é aquele que consegue determinar casos de teste para os quais o programa em teste falhe.
Um programa paralelo é considerado, basicamente, uma coleção de diversos processos seqüenciais que são executados simultaneamente e se comunicam de alguma forma.
Conseqüentemente, as dificuldades básicas encontradas no teste e depuração de programas seqüenciais, são também evidentes para qualquer programa paralelo.
Entretanto, além de essas dificuldades, os programas paralelos apresentam outros problemas relacionados com o aumento da complexidade, a quantidade de dados depurados e os efeitos anômalos adicionais, os quais muitas vezes acabam gerando uma intrusão excessiva na monitoração da aplicação em teste.
Em essa dissertação, foi proposta uma estratégia para diminuir a intrusão do teste de software em programas paralelos não-determinísticos.
Busca- se com essa estratégia, validar as abordagens de monitoração assim como as funcionalidades do ambiente de teste implementado.
Conforme os resultados apresentados nos Capítulos 5 e 6, a estratégia proposta obteve resultados positivos, tendo a diminuição da intrusão validada perante as comparações de desempenho com o MPE, além de a validação das funções de monitoração e atingibilidade de estados durante os experimentos realizados.
Entretanto, os problemas encontrados com os &quot;falsos bugs «impediram que nesse trabalho fosse validada também, a capacidade do engine de teste de apresentar o resultado do teste, o que nos incentivou a buscar novos experimentos para identificar possíveis soluções para esse problema.
Essas soluções representam no escopo do projeto, alterações nas etapas de geração de casos de teste e scripts, sendo inviáveis de implementar- las em tempo hábil, dado a etapa final do projeto em que nos encontramos.
Com isso, definiu- se como trabalho futuro, a criação de scripts de teste que contenham casos de teste agrupados.
Espera- se que dessa forma seja possível dar continuidade a todos os esforços dedicados nessa dissertação.
