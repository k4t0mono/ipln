Para várias áreas de aplicação, a construção semi-automática ou automática de ontologias seria extremamente útil.
Abordagens semi-automáticas para a extração de ontologias a partir de textos têm sido propostas na literatura, as quais sugerem a extração de conhecimento encontrado nos textos de um domínio, com o apoio de técnicas de processamento da língua natural.
Este trabalho propõe uma abordagem para suportar algumas fases do processo de aquisição de estruturas ontológicas, mais especificamente as fases de extração de conceitos e relações taxonômicas, de modo a semi-automatizar os passos da construção de ontologias a partir de textos na língua portuguesa do Brasil.
O resultado obtido serve como ponto de partida ao engenheiro de ontologia.
Para avaliação da abordagem proposta, foi desenvolvido um protótipo que incorpora mecanismos de importação de corpus, identificação de termos relevantes, identificação de relações taxonômicas entre esses termos e geração da estrutura ontológica em OWL.
Este protótipo foi utilizado num estudo de caso sobre o domínio do Turismo, possibilitando a avaliação com relação a diferentes aspectos do processo de aquisição de conceitos e relações.
Palavras-Chave: Construção de ontologias a partir de textos, Processamento da língua Portuguesa.
Há alguns anos o termo &quot;ontologia «remetia apenas a um campo da Filosofia.
Porém, a partir de o início dos anos 90, ontologias se tornaram um tópico de investigação e aplicação em comunidades de pesquisa de Inteligência Artificial (Ia), incluindo engenharia do conhecimento, representação do conhecimento e processamento de linguagem natural.
Hoje, encontramos pesquisas em torno de ontologias sendo desenvolvidas em diversas áreas, tais como sistemas de informação cooperativos, recuperação de informação, comércio eletrônico e gestão do conhecimento, entre outras.
Segundo Fensel, a principal razão para esse interesse é que ontologias &quot;prometem «entendimento comum e compartilhado de um mesmo domínio, que pode ser comunicado entre pessoas e sistemas de aplicação.
Noy e McGuinness, em, também citam algumas razões para se construir uma ontologia:
Compartilhar entendimento comum da estrutura de informação entre pessoas e agentes de software;
Permitir reuso do conhecimento de um domínio;
Criar compreensão do domínio explicitamente;
Separar o conhecimento do domínio do conhecimento operacional;
Permitir a análise do conhecimento do domínio.
Mas, apesar de o número de pesquisas sobre o tema ontologia, sua construção ainda é uma questão crítica.
Segundo Blázquez em, não existe uma metodologia generalizada e suficientemente testada para construção de ontologias e, dessa forma, os engenheiros de ontologias acabam criando seu próprio processo.
A afirmação de Blázquez ainda pode ser considerada válida hoje, visto que várias propostas de metodologias têm sido apresentadas na literatura nos últimos anos como, por exemplo, as encontradas em,,, e.
De acordo com Noy e McGuinness em, não existe uma metodologia &quot;mais correta «a ser usada na construção de ontologias e não há modo &quot;mais correto «para modelar um domínio, pois sempre existem alternativas viáveis.
A escolha da solução vai depender da aplicação que se tem em mente e do escopo desejado para a ontologia.
Metodologias como as encontradas em,,, e incluem a construção manual de ontologias suportada por algumas ferramentas computacionais.
Porém, conforme e, a construção manual de ontologias é um processo complexo, tedioso e de alto custo, e por ser um processo extremamente artesanal é também propenso a erro.
Além disso, a manutenção de uma ontologia, inclusão ou alteração de conceitos existentes, também pode ser onerosa.
Sendo assim, para várias áreas de aplicação, a construção semi-automática ou automática de ontologias seria extremamente útil e este fato tem levado pesquisadores a propor, num primeiro momento, abordagens para construção semi-automática de ontologias.
Entretanto, para se construir uma ontologia de modo automático ou semi-automático, é necessária uma fonte a partir de a qual possa ser extraído conhecimento relevante.
Em esse contexto, textos eletrônicos são fonte de informação bastante interessante, pois a cada dia que passa, mais textos estão disponíveis ao acesso das pessoas, representando o conhecimento dos mais diversos domínios.
Porém, de acordo com Davies Em, existem alguns problemas, não triviais, relacionados à construção de ontologias a partir de textos como, por exemplo:
O que deve ser representado?
Um outro problema, apontado em, é a identificação de fontes corretas e apropriadas para a extração do conhecimento.
Apesar destes problemas, abordagens e métodos utilizados para a construção (automática ou semi-automática) de ontologias a partir de textos têm sido propostos na literatura como, por exemplo, os encontrados em,,, e.
Estas frentes propõem a construção semi-automática a partir de o conhecimento encontrado nos textos de um dado domínio, com o apoio de técnicas de processamento de linguagem natural (PLN).
Face a o exposto, constituem a motivação deste trabalho:
O significado e a importância de ontologias na solução de problemas de Inteligência Artificial;
Os problemas relacionados à construção manual de ontologias;
E o surgimento de abordagens semi-automáticas para a construção de ontologias a partir de textos.
Outra importante motivação para este trabalho é que, durante a pesquisa realizada, não foram encontradas abordagens para a construção automática ou semi-automática de ontologias a partir de textos da língua portuguesa do Brasil.
O trabalho mais próximo, em se tratando de textos escritos em nosso idioma, é o apresentado em, onde a autora propõe um processo semi-automático para a extração de conhecimento a partir de textos da língua portuguesa do Brasil, voltado à construção de mapas conceituais.
Em esse sentido, a proposta de uma abordagem para semi-automatizar os passos típicos do processo de construção de ontologias (identificação de conceitos, de relações taxonômicas e não-taxonômicas e identificação de instâncias) a partir textos na língua portuguesa do Brasil é o que nos vem em mente.
Porém, devido a a complexidade e diversidade de técnicas envolvidas em cada um destes passos, fica inviável neste trabalho uma solução completa que integre todos os passos mencionados.
Em o contexto desta dissertação, &quot;conceito «e &quot;termo «são usados sem distinção, isto é, tem o mesmo significado.
Visando evoluirmos em direção a uma abordagem para semi-automatizar os passos da construção de ontologias a partir textos na língua portuguesa do Brasil, este trabalho apresenta uma proposta para suportar algumas fases deste processo, mais especificamente a extração de conceitos e de relações taxonômicas (ou relações hierárquicas entre conceitos).
O resultado da execução destas fases será o que chamamos de estrutura ontológica, ou seja, uma estrutura inicial que servirá como ponto de partida ao engenheiro da ontologia.
Apesar de a variedade de relações entre palavras que podem ser encontradas quando analisando um corpus, a escolha por trabalhar com relações taxonômicas se deve ao fato de que, segundo, na maioria dos casos, ontologias são estruturadas como hierarquias de conceitos (taxonomias), ou seja, arranjam conceitos dos mais gerais aos mais específicos.
Em este sentido, emergiu a questão de pesquisa deste trabalho:
&quot;É possível avançar na proposta de uma abordagem para aquisição de estruturas ontológicas a partir de textos da língua portuguesa do Brasil?».
Partindo- se desta questão de pesquisa, tínhamos a seguinte hipótese: --
Existem algoritmos e técnicas que podem ser adaptados para o processo de construção de estruturas ontológicas a partir de textos da língua portuguesa do Brasil, apoiando este processo através da geração de resultados parciais.
Visando evoluirmos em direção a a solução do problema colocado, o principal objetivo deste trabalho foi desenvolver uma abordagem para suportar fases do processo de aquisição de estruturas ontológicas a partir de textos na língua portuguesa do Brasil, mais especificamente as fases de extração de conceitos e relações taxonômicas.
Propor/ adaptar mecanismos que facilitem a extração de conceitos relevantes ao domínio ao qual os textos pertencem;
Propor/ adaptar mecanismos que facilitem a extração de relações taxonômicas entre os conceitos extraídos previamente;
Propor/ adaptar mecanismos que auxiliem a criação de uma estrutura ontológica a partir de os dados extraídos previamente;
Definir um ambiente de apoio aos mecanismos anteriores.
A Figura 1.1 representa as principais etapas desenvolvidas na condução deste trabalho.
Inicialmente foram estudadas as principais fontes sobre o processo de construção de ontologias, donde se obteve um entendimento geral sobre o que são ontologias e suas aplicações.
Posteriormente, o estudo restringiu- se às abordagens para a construção de ontologias a partir de textos, onde o problema motivador para esta pesquisa, a falta de uma abordagem para a construção de ontologias a partir de textos da língua portuguesa do Brasil, foi identificado.
Estas abordagens forneceram subsídios para a definição das funcionalidades de apoio necessárias para a consecução da proposta aqui apresentada.
O passo seguinte consistiu no desenvolvimento de um ambiente de apoio através de um protótipo, o qual teve seus resultados analisados através de dois estudos de caso.
Avaliação da abordagem proposta através dos resultados dos estudos de caso.
Este trabalho está organizado em 7 capítulos.
O Capítulo 2 apresenta conceituações e definições para o termo ontologia e a temática da sua classificação, fornecendo ainda uma visão geral sobre diferentes áreas de aplicação de ontologias.
O Capítulo 3 aborda uma questão crítica em relação a ontologias:
Sua construção.
Em ele são apresentados aspectos e abordagens propostos na literatura relacionados à construção de ontologias a partir de textos, sendo aqui reportados os principais trabalhos relacionados à proposta deste trabalho.
Este capítulo apresenta definições e classificações de ontologias, relacionando também algumas áreas de aplicação.
O termo &quot;ontologia «originou- se na Filosofia, através de Aristóteles, que o utilizava no intuito de especificar o que existe ou o que podemos dizer sobre o mundo.
Segundo Alexander Maedche em, &quot;Ontologia é um ramo da Filosofia que lida com a natureza e a organização do ser, e foi introduzido por Aristóteles na Metafísica».
Aristóteles estava interessado nas definições das coisas.
Sua noção de definição não era simplesmente o significado de uma palavra.
Uma definição tinha a intenção de explicar claramente o que uma coisa é «por ser um relato da essência da entidade.
Ele acreditava que, para dizer o que uma coisa é», precisava- se sempre dizer &quot;por que alguma coisa é».
Em a Ciência da Computação, o uso do termo ontologia teve origem na comunidade de Ia.
Segundo Thomas Gruber em, para sistemas de Ia o que existe é aquilo que pode ser representado e, quando o conhecimento de um domínio é representado através de um formalismo declarativo, o conjunto de objetos que pode ser representado é chamado de universo do discurso.
Mas nem sempre esse conceito precisa ser tão abrangente:
Russel e Norvig, em, definem ontologia como uma teoria da natureza do ser ou da existência, expressa por meio de um vocabulário.
Tais autores consideram uma ontologia apenas como um vocabulário, ou seja, uma lista informal dos conceitos num domínio.
O termo ontologia pode assumir diferentes significados para a Filosofia e para a comunidade de Ciência da Computação.
Em o sentido filosófico, pode- se referir a uma ontologia como um sistema particular de categorias a considerar para uma certa visão do mundo e, no seu uso mais comum na Ciência da Computação, uma ontologia refere- se a um artefato de engenharia, constituído por uma espécie de vocabulário usado para descrever uma certa realidade.
De entre as várias definições existentes para o termo ontologia na literatura, a mais citada é a oferecida por Gruber:
&quot;Uma ontologia é uma especificação explícita e formal de uma conceituação compartilhada».
Segundo Fensel, que remonta esse assunto:
Conceituação: Refere- se a um modelo abstrato de algum fenômeno do mundo, por terem sido identificados os conceitos relevantes para aquele fenômeno.
Explícito: Significa que o conjunto de conceitos utilizados e as restrições aplicadas são previamente e explicitamente definidos.
Formal: Refere- se ao fato de que se espera que uma ontologia seja processável por computador, o que exclui definições diretamente em linguagem natural, por exemplo.
Compartilhada: Descreve um conhecimento consensual, que é utilizado por mais de um indivíduo e aceito por um grupo.
Várias outras definições para o termo ontologia têm sido adotadas conforme o contexto de sua utilização.
Para Neches e co-autores, &quot;uma ontologia define termos e relações compreendendo o vocabulário de um tópico de uma área assim como as regras para combinar termos e relações para definir extensões para o vocabulário».
Já para Chandrasekaran Uschold e co-autores, em, afirmam que uma ontologia é uma reunião explícita de conhecimento compartilhado numa área específica e que, conseqüentemente, pode resolver problemas de comunicação entre pessoas, organizações e aplicativos.
Studer comenta que uma das finalidades das ontologias é permitir a interoperabilidade de fontes heterogêneas de informação e, assim, realizar o compartilhamento de conhecimento.
Uma ontologia, segundo Noy e McGuinness em, é formada basicamente por os seguintes componentes:
Classes (organizadas numa taxonomia), relações (representando os tipos de interação entre os conceitos de um domínio), axiomas (usados para modelar sentenças sempre verdadeiras) e instâncias (utilizadas para representar elementos específicos, ou seja, os próprios dados).
Maedche, em, oferece uma definição mais formal, em a qual uma ontologia é C refere- se a um conjunto de conceitos, muitas vezes chamados de classes;
R refere- se a um conjunto de relações;
HC representa a hierarquia dos conceitos, também chamada de taxonomia.
Por exemplo, HC significa que C1 é um sub-conceito de C2.
Por exemplo, (R) $= (C1, C2) significa que C1 e C2 estão relacionados de forma nãotaxonômica através de (R).
A o é um conjunto de axiomas da ontologia, expressos em linguagem lógica apropriada (por exemplo, lógica de primeira ordem).
As ontologias apresentam propriedades distintas mas, mesmo assim, é possível identificar tipos bem definidos de ontologias.
Existem diferentes classificações para ontologias na literatura, sendo que algumas propostas definem tipos de ontologias relacionando a capacidade de uma ontologia modelar um domínio ou, por exemplo, tipos de ontologia quanto a sua aplicação.
A seguir apresentamos algumas destas classificações.
Segundo Vergara em, ontologias podem ser classificadas de acordo com a forma como são capazes de modelar as informações de um determinado domínio.
Sendo assim, ontologias que dificultam as inferências, por não possuírem axiomas e restrições, são classificadas como superficiais (lightweight), enquanto que ontologias que incluem todos os elementos que permitem inferências sobre o conhecimento que representam, são classificadas como profundas (heavyweigth).
Em, Heijst classifica ontologias nas quatro categorias apresentadas a seguir:
Ontologias de representação do conhecimento:
Capturam a representação primitiva usada para formalizar conhecimento em paradigmas de representação de conhecimento.
Explicam as conceituações que fundamentam os formalismos de representação de conhecimento.
Meta-ontologias: Definem conceitos tais como estado, evento, processo, ação, etc., com o intuito de serem especializadas na definição de conceitos numa ontologia de domínio.
Esse tipo de ontologia é também chamado de ontologia genérica.
Ontologias de domínio:
Fornecem um vocabulário dos conceitos (dentro de um domínio) e seus relacionamentos, das atividades que acontecem naquele domínio e das teorias e princípios elementares que governam aquele domínio.
Ontologias de aplicação:
Contêm o conhecimento necessário para modelar uma aplicação em particular.
Guarino, em, apresenta uma classificação de ontologias com base no conteúdo, que se organiza em:
Ontologias de alto nível:
Provêem noções gerais às quais todos os termos nas ontologias estão relacionados.
Equivalem basicamente as meta-ontologias genéricas de Heijst.
Ontologias de domínio:
Têm a mesma característica encontrada na definição de Heijst.
Descrevem o vocabulário relacionado a um domínio genérico por a especialização dos conceitos introduzidos na ontologia de alto nível.
Ontologias de tarefas:
Descrevem o vocabulário relacionado a uma tarefa ou atividade genérica por a especialização das ontologias de alto nível.
Seu objetivo é facilitar a integração dos conhecimentos da tarefa e do domínio numa abordagem mais uniforme e consistente, tendo por base o uso de ontologias.
Ontologias de aplicação:
São as ontologias mais específicas.
Os conceitos de uma ontologia de aplicação devem ser especializações dos termos das ontologias de domínio e de tarefa correspondentes.
Guarino em fornece também outra distinção em relação a ontologias.
Para ele, uma ontologia pode ser &quot;refinada «ou &quot;não refinada».
Ontologias não refinadas têm um número mínimo de axiomas e o objetivo de serem compartilhadas por usuários que concordem sobre uma determinada visão do mundo.
Uma ontologia refinada precisa de uma linguagem de alta expressividade e tem um grande número de axiomas.
Ontologias não refinadas têm mais chance de serem compartilhadas e deveriam ser usadas on-line para dar suporte à funcionalidade de sistemas de informação.
Já as ontologias refinadas deveriam ser usadas off-line e somente para referência.
Como dito anteriormente, muitas pesquisas vêm sendo desenvolvidas em torno de ontologias em diversas áreas, tais como integração de informação inteligente, sistemas de informação cooperativa, recuperação de informação, comércio eletrônico e gerenciamento do conhecimento.
Para Fensel, a razão por a qual ontologias se tornaram populares é, em grande parte, o que elas prometem:
Entendimento comum e compartilhado de um mesmo domínio que pode ser comunicado entre pessoas e sistemas de aplicação.
Segundo Noy e McGuinnes em, uma ontologia define um vocabulário comum para pesquisadores que precisam compartilhar informação num domínio.
Seguem algumas visões de outros autores sobre a utilidade de ontologias.
Ontologias habilitam o compartilhamento de conhecimento e a análise ontológica clarifica a estrutura do conhecimento.
Ontologias são essenciais para o desenvolvimento e uso de sistemas inteligentes bem como para interoperabilidade de sistemas heterogêneos.
Ontologias são úteis de muitas maneiras para o entendimento e interação humana.
Uma razão típica para a construção de ontologias é oferecer uma linguagem comum para compartilhamento e reuso de informações sobre um fenômeno de interesse no mundo.
Maedche aponta a Web semântica, a compreensão da linguagem natural, a gestão do conhecimento e o comércio eletrônico como as principais áreas de aplicação de ontologias.
Cita ainda outras áreas de aplicação, como modelagem de processos de negócios, bibliotecas digitais e agentes inteligentes, entre outras.
A seguir são tecidos comentários a respeito de áreas de aplicação mencionadas.
A Web trouxe novas possibilidades de acesso à informação e às aplicações em geral.
Porém essa facilidade tem levado a um crescimento exponencial do material disponibilizado, gerando alguns problemas para os usuários.
Um problema encontrado hoje está na busca de informação a qual, na maioria das vezes, ocorre através do uso de palavras-chave, ou seja, uma busca por comparação lexical.
Devido a a grande quantidade de informação disponível e à falta de significado desta, as buscas na Web retornam hoje uma grande quantidade de informações, em diversos contextos irrelevantes para os usuários.
O uso de semântica na Web tem sido visto, então, como um fator fundamental para encontrar uma saída para os problemas causados por essa expansão &quot;o desenvolvimento da World Wide Web está a ponto de amadurecer de uma plataforma técnica que permite o transporte de informação de fontes da Web para humanos (embora em muitos formatos) para uma plataforma que permite a comunicação do conhecimento de fontes Web para máquinas «apud.
Segundo Grüninger, o criador da Web, Tim Berners-Lee, considera o uso de ontologias como uma parte crítica da Web semântica (Ws), que necessita ontologias formais para definir o significado dos dados.
Segundo Maedche, já existem vários exemplos de protótipos de aplicações fazendo uso de ontologias nesta área.
A área de comércio eletrônico vem crescendo muito ultimamente.
Porém, segundo Fensel, de forma geral, a automatização de transações nesta área não cumpriu as expectativas anunciadas.
Essa automatização requer descrições formais de produtos, além de uma sintaxe para formatos de troca.
Assim, um entendimento comum dos termos e suas interpretações deve ser capturado na forma de ontologias, permitindo interoperabilidade e meios para integração de informação inteligente.
Outro problema no comércio eletrônico é o fato de muitas informações sobre produtos estarem disponibilizadas somente em linguagem natural e, portanto, serem entendidas somente por humanos.
Em esse caso, ontologias poderiam ser usadas para descrever os produtos, facilitando a navegação e recuperação automática de informações.
A área de gestão do conhecimento trata da aquisição, manutenção e acesso ao conhecimento de uma organização.
No que se refere à gestão do conhecimento, a tecnologia da Web semântica, que inclui as ontologias, permitirá definições semântica e estrutural de documentos fornecendo possibilidades completamente novas:
Busca inteligente ao invés de busca por palavra-chave;
Resposta a consultas ao invés de recuperar documentos;
Troca de documentos via mapeamento de ontologia;
Definição de visões personalizadas de documentos.
Como exemplo de aplicação do uso de ontologias na gestão do conhecimento, pode- se citar o projeto On-To--Knowledge1, que constrói um ambiente para gerenciamento do conhecimento em grandes intranets e web sites e usa ontologia para modelar as semânticas de fontes de informação de maneira processável por máquina.
Compreensão de linguagem natural requer uma integração de muitas fontes de conhecimento.
Sendo assim, o conhecimento do domínio, na forma de ontologias, é essencial para entender profundamente os textos.
A extração de informação também é uma aplicação da área de processamento da linguagem natural que pode usar a noção de ontologia para preencher modelos com instâncias, e assim retornar as informações solicitadas.
Ontologias também têm sido aplicadas na área de Engenharia de Software.
Falbo Em, apresentam uma ontologia de qualidade de software visando apoiar o entendimento sobre o domínio em questão.
Esta ontologia foi formalizada em lógica de primeira ordem e durante sua construção ocorreu integração com uma ontologia de processo de software.
Já em, Falbo Apresentam Ode (Ontology--based Software Development Environment), um Ambiente de Desenvolvimento de Software (ADS) baseado em ontologias.
Segundo tais autores, por ser baseado em ontologias, este ambiente possui algumas vantagens como a criação de um repositório de conhecimento que proporciona ao ambiente uma uniformidade de conceitos, primordial na integração de ferramentas.
Em relação a reuso de software, Falbo Também sugerem o uso de ontologias.
Porém, a falta de abordagens para inserir ontologias num processo de desenvolvimento de software mais convencional é uma das principais desvantagens, quando se busca um uso mais amplo de ontologias na Engenharia de Software.
Os autores apresentam o ambiente Ode como uma abordagem ontológica para o domínio da engenharia de software com o objetivo de unir ontologias com a tecnologia de orientação a objetos.
Essa abordagem está baseada em duas fases:
Construção de ontologias, seguida da derivação de frameworks de objetos a partir de essas ontologias.
Já Edward Hovy em apresenta o uso de ontologias com o objetivo de simplificar acesso a dados em bases do governo dos EUA.
Segundo o autor, devido a a divisão em muitas esferas (federal, estadual e local;
Executivo, judicial e legislativo;
Etc), os dados do governo acabam sendo coletados por diferentes pessoas, em tempos e locais diferentes.
Muitos dos dados coletados envolvem outros dados ou então são complementares, e a heterogeneidade resultante desta coleta acaba criando incompatibilidade no compartilhamento dos dados.
O que se buscava então era um mecanismo para padronizar os tipos de dados de forma a permitir compartilhamento.
Para tal, foi definida uma ontologia simplesmente como uma taxonomia de termos, abrangendo desde termos mais gerais, no topo, até termos mais específicos, na base.
Outro trabalho interessante é o que John Everett Descrevem em.
Os autores apresentam um sistema de compartilhamento de conhecimento da Xerox, denominado Eureka, que contém aproximadamente 40.000 documentos textuais.
Seu objetivo é construir um sistema que possa identificar documentos conceitualmente similares, de forma a permitir fazer manutenção em várias coleções de documentos.
Para isso, os autores estão utilizando ontologias, que devem suportar a normalização de diferentes representações de conteúdo similar, para permitir a detecção de similaridades.
Em o projeto das ontologias, desenvolveram critérios que suportam comparações de textos em linguagem natural.
Segundo os autores, foi construído um protótipo do sistema, mas que está muito longe, ainda, de estar completo.
Já em, Kalfoglou Apresentam uma forma de aplicar ontologias em Memória Organizacional (Me O).
O objetivo de uma Me o é prover acesso fácil e recuperação de informações relevantes aos seus usuários.
Um dos problemas encontrados quando uma Me o está sendo desenvolvida diz respeito a sua população inicial.
Outro problema encontrado é a identificação de Comunidades de Prática (CoPs) dentro de uma organização.
Essas comunidades criam e compartilham conhecimento, e armazenar- lo é muito importante para a memória da organização.
Para resolver esses problemas, os autores propuseram o uso de ontologias, aplicando um método denominado Análise de Rede de Ontologia.
Assim, os autores criaram uma instanciação particular desse método, denominada Ontocopi, para tentar identificar CoPs dentro de as organizações.
Ontocopi é implementada como um plugin para a ferramenta Protégé2 e também está disponível via Web.
Os autores aplicaram um algoritmo de ativação de propagação, para identificar quais objetos são mais importantes numa ontologia de modo a usar- los para povoar inicialmente a Me o.
Para operacionalizar a Ontocopi, assumiram que ontologias populadas existiam na organização.
Este capítulo apresentou um embasamento teórico sobre o que são ontologias, suas diferentes classificações e sua aplicação em diferentes áreas, entre as quais engenharia do conhecimento, recuperação de informação e comércio eletrônico.
A partir de a pesquisa realizada constatou- se que o tema, ontologia tem sido abordado em várias pesquisas, em diversas áreas, confirmando sua importância no contexto tecnológico atual.
O principal objetivo nestas pesquisas tem sido fornecer um entendimento comum e compartilhado sobre um determinado domínio.
Porém, apesar de a importância e da grande quantidade de pesquisas realizadas sobre o tema, a construção de ontologias não conta com metodologias e modelos maduros e bem aceitos para este processo.
Em o próximo capítulo apresentamos algumas abordagens encontradas na literatura para a construção de ontologias a partir de textos, que serviram de base para a abordagem que está sendo proposta neste trabalho.
Este capítulo descreve os principais trabalhos relacionados à nossa pesquisa, principalmente no que se refere a construção de ontologias a partir de textos.
Para várias áreas de aplicação, a construção automática de ontologias a partir de um determinado conjunto de textos seria extremamente útil.
Porém, segundo Davies Existem alguns problemas, não triviais, relacionados à geração de ontologias a partir de textos, alguns de eles já conhecidos do campo da representação do conhecimento.
De entre estes problemas está a identificação de qual conhecimento deve ser representado, neste caso, em nível da ontologia.
Um problema adicional está em determinar uma linha divisória entre o que é expresso explicitamente num texto e o que é assumido implicitamente.
Por exemplo, quando um texto está sendo escrito, seu autor assume várias questões como sabidas, isto é, ele assume que o leitor compartilha do mesmo, ou quase o mesmo, conhecimento, deixando assim de explicitar conhecimento.
Conseqüentemente, é muito difícil construir um processo computacional que capturará o que em essência não está presente no texto.
Segundo Brewster Em, alguns aspectos nos textos podem afetar uma ontologia de domínio.
Estes aspectos são:
Um texto pode reforçar as suposições e estruturas de conhecimento de uma ontologia através da aproximação de conceitos;
Um texto pode alterar as ligações, associações e instanciações de conceitos existentes.
Este tipo de atividade pode ser visto como uma tentativa de re-estruturar a ontologia de domínio.
A maneira mais óbvia como um texto afeta uma ontologia de domínio é adicionando conceitos novos.
Ainda em, segundo os autores, a experiência tem mostrado que um certo número de contextos textuais é necessário para que o conhecimento ontológico esteja explicitamente disponível.
Assim, haverá sempre uma quantidade de termos de baixa freqüência para os quais é difícil achar contextos suficientes ou apropriados dentro de o corpus.
Em esses casos, poderiam ser procuradas fontes textuais externas para superar a ausência de comunicação explícita de conhecimento no corpus específico do domínio.
O problema encontrado nesse caso é identificar a fonte externa correta e apropriada para um determinado conjunto de textos.
Existem várias fontes potenciais de conhecimento ontológico:
Enciclopédias: Parecem fontes ideais para o conhecimento ontológico, pois incluem definição e textos explicativos que poderiam ser explorados.
Seu principal problema é o fato de que elas provavelmente não sejam muito atualizadas.
Livros didáticos e manuais:
Associados ao domínio, têm utilidade potencial.
Aqui o problema principal é identificar os textos relevantes e obter- los eletronicamente.
Internet: Esta é a fonte mais óbvia:
Vantagens: Devido a o seu tamanho, a informação desejada provavelmente será encontrada;
Devido a o seu dinamismo, é provável que conceitos novos estejam disponíveis prontamente;
Ela é facilmente acessada.
Desvantagens: O conceito de determinado termo pode aparecer em diversos textos de domínios diferentes;
A Internet tende a repetir a mesma informação em muitos lugares porque as pessoas freqüentemente copiam informações umas das outras;
É difícil determinar critérios para decidir se um web site será confiável ou não.
A seguir apresentamos abordagens encontradas na literatura que estão relacionadas a nossa proposta de construção de estruturas ontológicas a partir de textos.
Em, é apresentada uma abordagem para extração ou extensão (também denominada enriquecimento) de ontologias a partir de documentos textuais.
Segundo Buitelaar, esta abordagem segue os passos típicos de aprendizado de ontologia, porém objetivando integrar mais diretamente a engenharia de ontologia com análise lingüística, através da definição de regras de mapeamento, que relacionam entidades lingüísticas, em coleções de texto anotadas, a conceitos e atributos.
A seguir provemos uma descrição em alto nível dos passos desta abordagem, que é implementada como um plugin, denominado OntoLT3, para a ferramenta de desenvolvimento de ontologia Protégé.
A primeira parte do processo de extração consiste em realizar anotação lingüística nos textos do corpus.
Este passo é realizado por um sistema baseado em regras para análise do alemão e do inglês denominado Schug4 que, segundo os autores, provê as seguintes informações:
Part-of-speech (categoria gramatical), informação morfológica (flexão, derivação ou composição de uma palavra), estrutura sintática da frase e da sentença.
A próxima etapa desta abordagem consiste em definir regras de mapeamento entre a estrutura lingüística e o conhecimento ontológico.
Previamente são providas algumas regras de mapeamento, mas o usuário tem a liberdade para criar novas regras se achar necessário.
A seguir apresentamos dois exemplos de regras de mapeamento previamente definidos:
HeadNounToClass_ ModToSubClass:
Mapeia o substantivo principal para uma classe (conceito) e, em combinação com seus modificadores, para uma ou mais sub-classes.
SubjToClass_ PredToSlot:
Mapeia um sujeito para uma classe (conceito), e seu predicado para um atributo (slot) dessa classe.
A idéia é executar as regras de mapeamento coletivamente e, à medida que as précondições sejam satisfeitas, gerar os conceitos e atributos para uma nova ontologia ou integrálos numa ontologia existente, sempre de forma automática.
Deve- se observar que as regras de mapeamento somente serão executadas para aquela informação considerada relevante no pré-processamento estatístico (vide próxima sub-seção).
Por fim, os conceitos e atributos extraídos são validados por o usuário.
Maiores informações sobre o sistema Schug podem ser encontradas em.
Este passo serve para filtrar, a partir de a informação lingüística extraída, aquela relevante ao domínio.
Para realizar essa tarefa a abordagem baseia- se numa função denominada qui-quadrado5, a partir de a qual é determinada a relevância da informação ao domínio através da comparação da sua freqüência no corpus do domínio com sua freqüência num corpus de referência.
A partir de o passo anterior é possível a geração semi-automática de regras de mapeamento, as quais simplesmente poderiam ser geradas para todos os possíveis elementos da anotação lingüística, porém limitados às palavras que foram selecionadas por a medida quiquadrado.
O uso de mapeamentos entre a estrutura lingüística e o conhecimento ontológico, similar ao proposto por Velardi e co-autores em, é uma abordagem que poderia ser utilizada, principalmente, no que diz respeito à construção de ontologias de domínio, onde algumas informações importantes precisam de regras específicas para ser extraídas.
Porém, é nosso objetivo focar em regras mais genéricas, que possibilitem a geração de estruturas ontológicas para diferentes domínios.
Quanto a o processamento estatístico, nosso objetivo inicial era trabalhar somente com o corpus do domínio para o qual a estrutura ontológica deveria ser gerada.
Sendo assim, o processamento estatístico utilizado em não poderia ser utilizado em nossa proposta, pois a medida qui-quadrado precisa de um corpus de referência para determinar qual informação é relevante ao domínio.
Assim, utilizaríamos a medida TFIDF (subseção 3.5.2.2) para determinar os termos relevantes do domínio.
Porém, a medida TFIDF retorna apenas uma classificação dos termos, não ocorrendo nenhuma poda e nenhum valor ou regra para poda são sugeridos.
Isso acaba resultando numa quantidade muito grande de termos não relevantes sendo apresentados ao engenheiro de ontologia.
Maiores informações sobre a função qui-quadrado podem ser encontradas em.
Em esse caso, a medida qui-quadrado poderia ser utilizada para podar termos através da sua comparação num corpus de referência.
Uma medida alternativa a medida qui-quadrado é a medida Log--Likelihood.
De acordo com Rayson, a medida Log--Likelihood é muito semelhante à medida qui-quadrado, com uma ligeira melhora nos resultados para algumas situações.
A medida Log--Likelihood calcula a relevância de um termo do corpus do domínio com base na sua freqüência no corpus do domínio e no corpus de referência.
É possível calcular a medida Log--Likelihood (G2) com a seguinte fórmula:
Onde, a:
É a freqüência da palavra observada no corpus de referência;
Quanto mais alto o valor de G2, mais significante é a diferença entre duas freqüências.
Vale observar que o valor de G2 será sempre um número positivo.
Para definir se o termo no corpus de referência é mais significante que no corpus do domínio, ou vice-versa, calcula- se como segue:
Se a* 0 então o termo é mais relevante no corpus do domínio.
A medida Log--Likelihood será utilizada em nossa abordagem para poda de termos na fase de identificação de termos relevantes.
Segundo os autores, algumas pesquisas desenvolvidas no intuito de semiautomatizar a construção de ontologias a partir de textos ou, até mesmo, automatizar completamente algumas tarefas, estão apoiadas no uso de ontologias já existentes, thesauri e dicionários on-line, entre outros.
Diferentemente, os autores propõem um método para construir ontologias a partir de textos escritos em linguagem natural, não anotados.
As fases deste método, que são descritas a seguir, foram implementadas numa ferramenta denominada Ontostruct.
A primeira parte do pré-processamento é transformar diferentes formatos de texto, por exemplo, PDF ou Html, num formato único.
Porém, segundo os autores, nesta transformação existe perda significativa de informação de formatação.
Os marcadores de lista, por exemplo, assim como parágrafos aninhados, provêem valiosa informação sobre a divisão e estrutura do texto e, a partir de eles, pode- se descobrir se dois termos compartilham uma mesma estrutura de lista, e isso pode servir como um indicador de relação entre esses termos.
Para resolver este problema, os autores usam uma ferramenta denominada MXTERMINATOR6 e um algoritmo que procura por seqüências crescentes de contadores e aprende padrões que extraem estes tipos de seqüências.
Depois, são usados um etiquetador de categorias gramaticais e um chunk parser7 para etiquetar o conteúdo, e identificar sintagmas nominais e sintagmas verbais nas orações.
A partir de os sintagmas nominais, identificados no pré-processamento, são extraídos os candidatos a termos.
Com o objetivo de obter somente a forma singular dos substantivos e seus modificadores adjetivais (variantes), são aplicados filtros sintáticos e morfológicos a cada sintagma nominal.
Os autores optam por considerar essas variantes como termos candidatos, não as eliminando quando encontradas.
Por exemplo, num texto, o termo &quot;profit «(lucro) pode vir acompanhado de adjetivos, como em &quot;domestic profit «(lucro nacional) ou &quot;foreign profit «(lucro estrangeiro).
Estas variantes para o termo &quot;profit «são então aceitas como termos Maiores informações podem ser encontradas em.
Em resumo, um chunk parser é um tipo de parser que identifica grupos lingüísticos (como sintagmas nominais) em texto irrestrito, tipicamente uma sentença de cada vez.
Por fim, de entre os termos candidatos que ficam, são selecionados todos aqueles que aparecem pelo menos duas vezes num dos documentos da coleção, ou aparecem em múltiplos documentos.
Em esta fase, para derivar relações conceituais entre os termos extraídos do corpus, primeiramente são analisados vários padrões sintáticos que acontecem no texto.
Segundo os autores, esses padrões são usados para recuperar relações is-a, equivalências, atributos gerais de termos (inclusive relações has- a (tem um) ou part-of (parte de), e outras relações gerais entre termos.
A seguir brevemente descrevemos os tipos de relações extraídas no processo.
Relações hierárquicas: São extraídas do texto usando padrões léxico-sintáticos prédefinidos por Hearst em.
Exemplo de um padrão: Como exemplo, considere o seguinte trecho de texto:
Relações de equivalência: Por o uso de padrões parentéticos9 podem ser extraídas relações de equivalência entre dois termos, ou um termo e seu acrônimo como, por exemplo, &quot;standard industrial code (SIC)».
Relações de equivalência ainda podem ser extraídas por a seguinte regra: De acordo com Church em, informação mútua é uma medida que compara a probabilidade de se observar dois pontos (palavras), x e y, juntos, com a probabilidade de se observar x e y independentemente.
De o inglês parenthetical.
Expresso entre parênteses.
Atributos de termos e propriedades: Podem ser extraídos usando informação atributo de &quot;pessoa».
O objetivo desta fase é colocar termos de significado equivalente, ou quase equivalente, num mesmo grupo.
Isso é feito com base na similaridade entre as relações das quais os termos participam. Williams.
O resultado final, calculado entre as três listas, corresponde ao valor de desigualdade lexical média entre os dois termos.
Tal valor sofre ainda interferência por a ocorrência ou não destes dois termos juntos num documento.
Em o final, os termos com valor de desigualdade mínimo são então agrupados.
As classes de termos semanticamente agrupados e as relações de equivalência e relações hierárquicas aprendidas são parte da entrada para o algoritmo de construção de hierarquia, que gerará como resultado um conjunto de hierarquias formando um grafo acíclico Em o experimento relatado em, foram usados documentos como formulários e suas correspondentes instruções de preenchimento, desenvolvidos sem nenhuma estrutura e formatação, com grande variedade no seu conteúdo.
A partir de informações do domínio e definições para os diferentes tipos de relações fornecidas, avaliadores humanos julgaram a precisão.
Como resultado, a ferramenta alcançou a precisão de 71-83% na extração de relações e obteve agrupamentos corretos de termos em 54% das vezes.
Computar uma forma padrão para os termos (como o lema da palavra) é um aspecto muito importante na fase de identificação de termos, pois num corpus um conceito normalmente aparece com diferentes formas (com variações de gênero, número e grau).
Em um texto de esportes, por exemplo, o conceito &quot;goleiro «pode aparecer no singular (goleiro) ou no plural (goleiros).
Computando-se o lema das palavras obtemos apenas um conceito (goleiro), o que faz mais sentido, pois este é o conceito envolvido nas duas situações.
Dada a importância desta abordagem, ela será utilizada em nossa proposta para identificação de termos.
Além disso, por se tratar da construção de uma ontologia de domínio, é interessante, como em, não eliminar as variantes de um termo (substantivo+ modificadores adjetivais), como &quot;profit», &quot;domestic profit «e &quot;foreign profit».
Tais variantes podem representar conceitos mais específicos dentro de um domínio e, portanto não serão eliminadas.
A diferença em nossa proposta é que adotaremos todas as variantes dos termos como possíveis candidatas, deixando que sua permanência seja decidida por o engenheiro de ontologia, enquanto que em é utilizada a medida de informação mútua para determinar se a variante deve ser considerada ou não.
Em relação a a freqüência dos termos, em são selecionados todos aqueles termos que aparecem ao menos duas vezes num dos documentos da coleção, ou que aparecem em múltiplos documentos.
Nosso entendimento é que assim podem ser selecionados muitos termos, sendo que grande quantidade não relevante ao domínio.
Nossa estratégia, como descrito anteriormente, será a de utilizar uma medida para filtrar parte dos candidatos a termos e deixar ao engenheiro de ontologia a tarefa de definir, conforme sua percepção, quais termos são ou não relevantes.
O método apresentado por Lame em tem por objetivo a identificação de conceitos e relações semânticas entre esses conceitos.
Segundo o autor, as técnicas utilizadas são automáticas e contam com ferramentas tais como analisadores sintáticos e estatísticos, porém não substituem os projetistas de ontologias, apenas os ajudam no processo.
Em o contexto apresentado em, o método foi utilizado para a construção de uma ontologia referente a a lei francesa, a partir de textos de Códigos (leis), e foi aplicada para a construção de uma ontologia dedicada a recuperação de informação.
Segundo Lame, o método também pode ser usado para construir ontologias dedicadas a outras tarefas.
De modo geral, a idéia aqui consiste em identificar conceitos, através de termos (palavras ou grupos de palavras) encontrados nos textos, e identificar relações semânticas entre esses conceitos, através da busca de relações sintáticas entre os termos.
Para atingir esses objetivos, o método conta com as etapas descritas a seguir.
Consiste na análise de textos com o objetivo de identificar a categoria gramatical das palavras (substantivos, verbos, etc).
Para realizar esta etapa o autor utiliza uma ferramenta denominada Syntex10, obtendo como resultado uma lista de termos etiquetados com as respectivas categorias gramaticais.
A ferramenta inclui um conjunto de regras sintáticas e uma lista de &quot;stopwords11 «e estabelece também dependências sintáticas entre termos.
Em o contexto onde foi aplicado este método, o autor optou por trabalhar somente com substantivos pois, segundo ele, a maioria dos conceitos está rotulada por substantivos.
Além disso, são excluídos os termos que contêm caracteres não-alfabéticos, como números e símbolos.
De essa forma, ao final da análise estatística obtém- se como resultado uma lista de termos relevantes (substantivos) que são considerados nas próximas etapas.
A análise das relações de coordenação tem como objetivo identificar termos separados por conjunções &quot;e «ou &quot;ou «como, por exemplo, &quot;nome e sobrenome «ou &quot;carro ou ônibus».
A partir de a lista de termos, é realizada a análise dos documentos identificando ocorrências desses termos separados com &quot;e «ou &quot;ou», pois isso pode indicar uma relação entre eles.
O resultado desta etapa, uma lista de possíveis relacionamentos, deve ser manualmente conferido para validar quais relações são semanticamente relevantes e quais não são.
Maiores informações sobre a ferramenta Syntex podem ser encontradas em.
Stopwords são palavras comuns que não agregam valor por possuírem significado semântico limitado; Tem como objetivo identificar relações entre os termos relevantes identificados na análise sintática.
De acordo com, dois termos semanticamente relacionados ocorrem com freqüência em contextos similares.
Esses contextos estão relacionados com as palavras que cercam um termo.
Por exemplo, o termo &quot;nacionalidade «sempre aparece no mesmo contexto que &quot;registro», &quot;data de nascimento», etc..
Cada palavra do contexto recebe uma medida de informação mútua que quantifica sua dependência em relação a um determinado termo.
Conforme Lame, deve- se definir um limiar mínimo com o objetivo de validar as relações obtidas.
Uma validação manual desses resultados também poderia ser realizada.
Muitas vezes um termo está dentro de outro termo como, por exemplo, o termo &quot;contrato «está em &quot;contrato de depósito «ou em &quot;contrato de locação».
Sendo assim, esta etapa consiste em relacionar tais termos (no exemplo, &quot;contrato&quot;), com os termos de os quais fazem parte (neste caso, &quot;contrato de depósito «e &quot;contrato de locação&quot;).
Segundo Lame, esse método é grosseiro mas, se aplicado a uma lista de termos bem identificados, pode dar bons resultados, especialmente no contexto de uma ontologia dedicada à recuperação de informação.
O resultado desta etapa será um conjunto de relações hierárquicas entre os termos identificados na análise sintática.
Lame, em suas pesquisas, optou por trabalhar somente com substantivos que, segundo ele, representam a maioria dos conceitos.
Como dito anteriormente, iremos trabalhar com os substantivos e também com seus modificadores.
Lame utiliza uma lista de stopwords. A utilização dessas estratégias nos permitirá excluir palavras que não acrescentam valor para a construção da estrutura ontológica.
A o observar o termo &quot;contrato «como parte do termo &quot;contrato de depósito», identificamos que &quot;contrato de depósito «é um tipo de &quot;contrato «e, portanto, os termos estão relacionados hierarquicamente (relação taxonômica).
A identificação de termos que fazem parte de outros, como proposto por Lame, auxilia na identificação de relações taxonômicas e também será utilizada em nossa proposta.
Em, Maedche e co-autores apresentam um método semi-automático para obter uma ontologia de domínio baseada em fontes de texto de uma intranet.
As etapas desse método, que são suportadas por uma ferramenta denominada Text-To-Onto, são descritas a seguir.
De acordo com os autores, a ontologia de domínio resultante deste processo pode, se necessário, ser refinada e melhorada através da repetição do processo de aquisição.
Para extração de informação, é usado um processador superficial de texto para o idioma alemão denominado SMES12, o qual é composto por um tokenizador, um analisador lexical e um chunk parser.
O resultado deste processamento são textos do domínio lingüisticamente anotados.
Esta etapa consiste em escolher uma ontologia genérica (ou ainda redes léxicosemânticas ou uma ontologia relacionada ao domínio) para ser usada como uma estrutura base para a ontologia específica que se deseja construir.
De acordo com os autores, os algoritmos trabalham sem a necessidade de qualquer estrutura de conhecimento;
Entretanto, se algum tipo de conhecimento estiver disponível, ele será utilizado como base para o restante do processo.
O objetivo desta etapa é extrair conceitos específicos do domínio.
Os autores trabalham com duas abordagens nesta etapa.
Em uma de elas utilizam um dicionário contendo termos importantes do domínio e a outra é baseada em medida de freqüência.
Maiores informações sobre o processador de textos SMES podem ser encontradas em.
Aquisição de conceitos via dicionário de domínio O primeiro passo consiste em converter em conceitos as headwords13. Quando duas entradas possuírem uma mesma definição, elas são unidas e as headwords são consideradas sinônimos e, quando uma entrada contiver referência à outra entrada, então esta ligação é convertida numa relação conceitual.
A partir de os conceitos criados são então extraídos stems14.
Se um stem já existe na ontologia, é necessário verificar se a entrada extraída do dicionário descreve de forma diferente o mesmo conceito contido na ontologia e, caso isso aconteça, tal fato é classificado como conflito.
Para tentar resolver esse tipo de conflito de forma automática, os autores propõem o uso de algumas heurísticas.
Caso o conflito não seja resolvido de forma automática é então solicitada intervenção do usuário.
Aquisição de conceitos baseada em freqüência Consiste em obter a freqüência das entradas lexicais num conjunto de documentos do domínio com o objetivo de indicar possíveis conceitos deste domínio. A técnica está baseada na idéia de que uma entrada lexical freqüente pode sugerir um conceito.
Porém, segundo, existem meios mais efetivos para se obter a classificação dessas entradas além de a simples contagem de freqüência.
De essa forma, os autores usam então uma combinação de medidas denominada tfidf (term frequency inverse document frequency), a qual pesa a freqüência de uma entrada lexical num documento.
Uma headword, neste caso, é uma palavra-chave do dicionário, isto é, uma palavra que representa uma entrada específica do dicionário.
Stem, no contexto desta dissertação, é o mesmo que radical de uma palavra.
Depois de calculada a freqüência de cada entrada lexical em cada documento, através da medida mencionada anteriormente, tem- se uma lista de todas as entradas lexicais.
As stopwords são retiradas desta lista.
De acordo com os autores em, para calcular o valor tfidf para uma determinada entrada lexical no corpus, basta fazer um somatório das freqüências desta entrada em cada documento do corpus como segue:
Para auxiliar na extração de entradas lexicais relevantes ao domínio, um limiar mínimo pode ser definido e para a entrada ser considerada, o valor tfidfl deve alcançar este limiar.
A determinação da freqüência também pode ser feita com a utilização de um segundo corpus contendo documentos genéricos.
Assim, as freqüências dos conceitos em ambos os corpora são comparadas.
Aqueles conceitos que tiverem maior freqüência no corpus específico do domínio permanecem como relevantes, enquanto os demais são removidos.
Segundo os autores, é dada liberdade ao usuário para intervir no processo de modo, por exemplo, a excluir conceitos que permaneceram ou incluir conceitos não selecionados.
Os autores utilizam duas técnicas para a extração de taxonomias (ou hierarquia de conceitos), sendo uma a partir de textos lingüisticamente anotados, usando agrupamento hierárquico, e outra a partir de definições de dicionários pré-processados lingüisticamente, usando padrões.
Técnica de agrupamento hierárquico Fazer um agrupamento significa formar grupos de objetos semelhantes em algum ponto. Existem duas formas de se criar um agrupamento hierárquico:
Bottom-up e top-down. O algoritmo bottom-up adotado por os autores inicia com cada objeto formando um grupo e, a cada iteração, os grupos mais similares são determinados e agrupados num novo grupo, terminando quando somente um grande grupo existir.
Já o algoritmo top-down adotado por os autores inicia com um grupo contendo todos os objetos, a partir de o qual, a cada iteração, são selecionados e divididos em grupos menos coerentes, ou seja, aqueles com objetos menos similares.
Os autores trabalham com três estratégias em relação a a medida de similaridade para agrupamentos hierárquicos:
Exploração de dicionário baseada em padrões O objetivo é usar a informação estruturada contida em dicionários específicos de domínio como entrada para a extração de relações taxonômicas. A idéia é definir uma expressão regular que captura trechos recorrentes e mapear os resultados para uma estrutura semântica como HC.
Em seu framework, os autores definem várias expressões regulares para adquirir relações taxonômicas.
Um problema, porém, é que domínios específicos requerem padrões específicos.
Mesmo assim, segundo os autores, várias heurísticas pode ser reusadas.
A desvantagem do uso dessa abordagem está na necessidade de definição desses padrões, pois esta é uma atividade que consome tempo.
Para descobrir relações conceituais, os autores utilizam uma abordagem estatística a partir de um algoritmo baseado em regras de associação.
A idéia consiste em descobrir quais uniões entre conceitos aparecem freqüentemente nas orações, pois estas podem representar relações relevantes entre conceitos.
Por exemplo, o processamento lingüístico poderia indicar que a entrada lexical &quot;costs «freqüentemente aparece junto com palavras como &quot;hotel», &quot;guest house», e &quot;youth hostel».
Os dados estatísticos indicam então que pode existir uma relação relevante entre &quot;costs «e cada um desses conceitos, e esta relação pode ser proposta para inclusão na ontologia.
Esses dados estatísticos são denotados por duas medidas, support e confidence15, para as quais um valor mínimo aceitável deve ser especificado.
Segundo os autores, o resultado deste passo são sugestões de relações, devendo o usuário manualmente selecionar e nomear as relações desejadas.
Assim como Lame, Maedche também utiliza uma lista de stopwords para filtrar palavras que não representam conceitos do domínio e também exclui palavras que incluem caracteres não-alfabéticos como números e símbolos.
Em relação a a extração de termos relevantes do domínio, Maedche propõe duas abordagens.
Uma de elas é baseada num dicionário contendo termos importantes do domínio o que, dada nossa proposta de não utilizar outras fontes além de os textos, não é viável.
A segunda abordagem é baseada na combinação de medidas tfidf (term frequency x inverted document frequency), a qual tem por objetivo pesar a freqüência de uma entrada lexical num conjunto de documentos.
Essa medida é adotada em nossa proposta para determinar a ordem de relevância dos termos do domínio.
Para ficar somente com os termos relevantes do domínio, um limiar mínimo para esta medida ainda poderia ser definido.
Porém, como nossa idéia é possibilitar que nossa abordagem seja utilizada para diferentes domínios e, portanto, muito provavelmente para diferentes tamanhos de corpus, um limiar fixo não será muito adequado.
De essa forma, como dito anteriormente, nossa proposta é deixar a definição de tal limiar para o engenheiro de ontologia.
Para identificar as relações taxonômicas, Maedche também utiliza duas abordagens.
Uma abordagem, denominada agrupamento hierárquico, é o processo de organizar objetos em grupos em os quais os membros são similares de alguma forma.
A outra abordagem usada é denominada pattern-matching, onde a partir de a informação sintática, várias heurísticas (padrões) para extrair relações taxonômicas são aplicadas.
Maedche usa nesta abordagem os padrões léxico-sintáticos identificados por Hearst. A mesma abordagem e padrões são utilizados também por Degeratu e Hatzivassiloglou em.
Os padrões de Hearst inicialmente propostos para textos escritos na língua inglesa, como veremos adiante, foram Maiores informações sobre as medidas de support e confidence podem ser encontradas em.
Em, Velardi Apresentam técnicas de mineração de texto com o intuito de melhorar a produtividade humana durante o processo de construção de ontologia.
Tais técnicas foram implementadas numa ferramenta, denominada OntoLearn16, com o objetivo de extrair conceitos importantes do domínio e descobrir as relações semânticas entre eles.
Para a mineração dos textos, os autores usam um processador de corpus chamado Ariosto17, o qual teve seu desempenho melhorado com a adição de um reconhecedor de entidades mencionadas18 e um chunk parser chamado Chaos19.
Segundo os autores, conceitos são denotados por termos que são classificados em três classes:
Entidades mencionadas do domínio ­ nesta classe encontram- se, por exemplo, Termos multi-palavra específicos do domínio ­ ou seja, termos compostos por mais de uma palavra como travel agent, preservation área, etc..
Palavras singulares específicas do domínio -- por exemplo, hotel, reservation, etc..
Segundo os autores, identificar instâncias de conceitos significa identificar termos da classe &quot;entidades mencionadas do domínio «como, por exemplo, nomes próprios, os quais representam instâncias de conceitos do domínio.
Entidades mencionadas são muito comuns em textos e, de acordo com Velardi Em, na maioria dos domínios, representam mais de 20% do total de palavras de um texto.
Maiores informações sobre a ferramenta OntoLearn podem ser encontradas em.
Maiores informações sobre o processador de corpus Ariosto podem ser encontradas em.
De o inglês named entities.
Maiores informações sobre o chunk parser Chaos podem ser encontradas em.
Através de Ariosto, os termos desta classe são identificados e etiquetados semanticamente conforme uma das três categorias conceituais:
Lugares, organizações e pessoas.
Essa identificação é baseada em regras contextuais anotadas manualmente ou aprendidas por máquina.
O método proposto por os autores explora propriedades lingüísticas e estatísticas para construir um glossário terminológico específico do domínio.
Assim, expressões terminológicas candidatas são capturadas por meio de técnicas executadas em quatro passos através do chunk parser Chaos:
Etiquetagem de categorias gramaticais;
Chunking; Comparação de estruturas de argumentos de verbos;
Análise gramatical superficial20.
Segundo os autores, o problema de usar abordagens puramente sintáticas é que os termos candidatos são extraídos numa quantidade muito maior do que as verdadeiras entradas terminológicas.
Então, existe a necessidade de filtrar candidatos não-terminológicos e, para tanto, duas novas métricas foram definidas.
Essa filtragem é executada por a combinação de ambas:
Relevância de Domínio:
Um termo candidato é medido por a análise comparativa entre diferentes domínios.
Uma definição quantitativa da relevância de domínio pode ser dada de acordo com a quantia de informação capturada dentro de o corpus designado, em relação a a coleção inteira.
Consenso de domínio:
Mede o uso distribuído de um termo num domínio, isto é, sua distribuição ao longo de todos os documentos pertencentes ao domínio.
O passo anterior retorna uma lista de termos sem nenhuma relação hierárquica.
Esta lista então é processada no intuito de identificar relações verticais entre os termos (em outras Maiores detalhes sobre esta análise gramatical superficial podem ser encontradas em.
O método extrai relações taxonômicas a partir de a do núcleo do sintagma de termos multi-palavras.
Segundo Velardi, a estruturação hierárquica dos termos reduz significativamente o trabalho manual, pois somente núcleos de termos devem ser ligados diretamente à ontologia.
Considere, por exemplo, que uma sub-árvore para o termo card foi encontrada com vários termos associados (credit card, golden card, business card, etc).
Em esse caso, só card deverá ser ligado diretamente a um nodo da ontologia.
Essa ligação é feita de modo manual.
De acordo com os autores, relações tais como hiponímia e hiperonímia são difíceis de extrair a partir de um corpus.
Sendo assim, os autores sugerem que relações entre Sujeito, Verbo e Objeto sejam mais facilmente detectadas a partir de técnicas de mineração de texto.
Um ponto considerado importante em, e que também adotaremos, é a extração dos termos multi-palavra (ou termos compostos) como, por exemplo, &quot;pano de prato».
As variantes de termos, identificadas em, também são classificadas como termos compostos.
Para identificar relações taxonômicas, Velardi e co-autores usam uma heurística especificamente para termos multi-palavra, baseada no núcleo do sintagma desses termos.
Por exemplo: A partir de dois termos, t1 e t2, se t1 é igual a t2, e t1 é modificado por um adjetivo, então é possível derivar a relação taxonômica Is-A.
Considerando os termos credit card como t1 e card como t2, a partir de a heurística temos a relação Is-A (credit card, card).
Visto que estaremos identificando termos compostos em nossa abordagem, é interessante utilizarmos também essa heurística para identificar relações taxonômicas.
Segundo os autores, mesmo sendo um método grosseiro, pode dar bons resultados se for aplicado sobre uma lista de termos bem definidos.
Um trabalho correlato a esta proposta foi desenvolvido por Pérez, onde a autora apresenta um processo semi-automático para a extração de conhecimento a partir de textos da língua portuguesa do Brasil, objetivando a construção de Mapas Conceituais.
Estes mapas têm o objetivo de representar o conhecimento como armazenado na estrutura cognitiva, ou seja, como um conjunto de conceitos, organizados de forma hierárquica, representando o conhecimento e as experiências adquiridas por uma pessoa.
O relacionamento entre os conceitos é formado por palavras de ligação, que são usadas para formar proposições simples (união entre dois ou mais termos conceituais) formando uma unidade semântica.
A seguir são brevemente descritas as etapas utilizadas por a autora na geração dos Mapas Conceituais.
O corpus selecionado para a realização dos experimentos é composto por artigos jornalísticos da Folha de S. Paulo do ano de 1994 e textos didáticos da Editora Scipione do ano de 1983.
De acordo com, para a análise sintática foi utilizado o parser Palavras, que realiza as etapas de tokenização, processamento léxico-morfológico, e a análise sintática propriamente dita.
A identificação das estruturas para a aquisição de conhecimento a partir de os textos está baseada no formato de mapas conceituais, ou seja, são representadas por triplas (conceito ­ relação ­ conceito), que em textos da língua natural tendem a aparecer como Sujeito ­ Verbo ­ Objeto.
O verbo tem a função de estabelecer o relacionamento entre dois conceitos.
Para a representação das triplas no formato de mapas conceituais foi utilizada a Co-referência é o termo usado para especificar que duas ou mais expressões de um texto se referem a uma mesma entidade do discurso.
Depois dos textos estarem anotados, são usadas folhas de estilo XSL para extrair as cadeias de co-referência, que podem ser usadas para identificar os termos relevantes dos textos, a partir de os quais é possível observar aqueles que são mais referenciados ao longo de o texto e também podem ser úteis na identificação de termos equivalentes presentes nos mapas.
A partir de o arquivo no formato de triplas Relação, aplicam- se scripts na linguagem shell (Linux) para a gerar a codificação XTM.
O primeiro experimento da autora foi baseado apenas no núcleo dos sintagmas nominais que exercem a função de sujeito e objeto direto, e o núcleo dos sintagmas verbais, formando a tripla ­ verbo ­ objeto direto\&gt;, representando as relações entre os conceitos.
Após a análise qualitativa das triplas formadas por essas estruturas, a autora constatou que os mapas conceituais gerados automaticamente não apresentaram relações conceituais tão relevantes como nos mapas construídos manualmente.
A autora então observou que a extração poderia melhorar ao levar em consideração pronomes que exercem o papel de sujeito ou objeto, verbos com complementos proposicionais, nomes compostos, e ambigüidade.
Assim, para o segundo experimento foi utilizado um corpus, com estruturas de seleção mais complexas e foi realizada uma avaliação quantitativa com base em mapas gerados manualmente.
Assim, as estruturas que passaram a ser de interesse foram:
Verbos, sujeito, objeto direto ou indireto, agente da passiva, predicativo do sujeito, adjuntos adverbial e adnominal.
A relação das estruturas consideradas para a formação de triplas foi:
Argumento1 -- Relação.
Argumento2. Conceitos a..
Argumento 1 (Sujeito):
Sintagma nominal que exerça a função de sujeito, pronome relativo que exercendo a função de sujeito, e verbo no particípio que exerça a função de sujeito.
Argumento 2 (Complemento):
Núcleo do sintagma nominal que exerce a função de objeto direto e o adjetivo se existir, sintagma preposicional que exerce a função de objeto indireto, predicativo do sujeito, etc..
Relação: Núcleo do sintagma verbal e advérbio que antecede um verbo, pois a retirada do mesmo pode produzir um sentido oposto à tripla.
De acordo com a autora, a consideração das novas estruturas possibilitou a formação de mais triplas completas, refletindo no aumento do número de triplas que constituem os mapas.
As triplas extraídas automaticamente de cada texto foram então avaliadas utilizando as medidas de abrangência e precisão.
Segundo Pérez, questões semânticas como a ambigüidade, a resolução anafórica de pronomes da terceira pessoa (ele, eles, etc) e complementos proposicionais ficaram fora de o escopo do trabalho.
Dois pontos diferenciam o trabalho realizado por Pérez em relação a o que está sendo aqui proposto.
O primeiro diz respeito ao fato da autora não mencionar o uso de medida para selecionar os conceitos.
Um segundo ponto refere- se ao tipo de relação que é identificado em.
A autora trabalha relações predicativas, ou seja, onde os conceitos estão ligados a verbos ao quais servem de sujeito ou objeto.
Diferentemente, nossa proposta é a de identificar as relações taxonômicas existentes entre os conceitos.
Além disso, segundo, apesar de serem construções similares e com muitas características em comum, mapas conceituais são mais flexíveis que ontologias e possuem proposições simples, enquanto ontologias possuem classes, sub-classes e outras relações bem definidas como, por exemplo, meronímia e hiperonímia.
Assim, apesar de os trabalhos serem similares e igualmente voltados à língua portuguesa, nossa abordagem não está baseada nesta pesquisa.
Apesar de a variedade de relações que podem existir entre conceitos, segundo na maioria dos casos ontologias são estruturadas como hierarquias de conceitos (taxonomias).
Esse tipo de relação é identificado nos textos por meio de a relação semântica hiponímia e seu inverso, hiperonímia, que arranjam conceitos dos mais gerais aos mais específicos.
Hiponímia: Segundo é a relação em que um conceito denota uma subclasse do outro.
O conceito mais específico é um hipônimo do mais genérico.
Por exemplo, a relação entre &quot;turista «e &quot;pessoa «é uma hiponímia, em que &quot;turista «é um hipônimo de &quot;pessoa».
Hiperonímia: É uma relação semântica inversa à hiponímia, ou seja, é a relação em que um conceito denota uma generalização do outro.
O conceito mais genérico é um hiperônimo do mais específico.
Em este caso, &quot;pessoa «é um hiperônimo de &quot;turista».
Apesar de não estarem diretamente relacionadas à construção de ontologias, os padrões léxico-sintáticos identificados por Hearst e por Morin e Jacquemin serão apresentados a seguir, pois tais padrões serão utilizados como parte do processo para recuperação de relações taxonômicas em nossa abordagem.
Em, Hearst apresenta uma lista com seis padrões léxico-sintáticos que indicam a relação hiponímia a partir de textos escritos na língua inglesa.
Esses padrões são também utilizados em e.
A Tabela 3.1 apresenta tais padrões, com sua tradução/ adaptação para uso em textos escritos na língua portuguesa do Brasil.
Seguem, como exemplo, dois trechos de texto provenientes de[ HEA92], a partir de os quais pretendemos demonstrar como esses padrões podem ser empregados em nosso trabalho:
Podemos observar que o primeiro trecho se enquadra no primeiro padrão da Tabela 3.1 (Np such as Np), tendo &quot;bow lute «e &quot;Bambara ndang «como Np, gerando, neste caso, a relação:
Em relação a o segundo trecho, podemos observar que se trata do sexto padrão (Np, relações:
Em, Morin e Jacquemin apresentam padrões para a aquisição de relações de hiperonímia para um corpus de textos escritos na língua francesa.
A Tabela 3.2 apresenta tais padrões, onde as linhas subseqüentes a cada padrão referem- se a nossa adaptação para uso em textos escritos na língua portuguesa do Brasil.
Significa que o texto entre chaves anterior ao símbolo não é obrigatório na identificação do padrão A partir de os exemplos que seguem, provenientes de, demonstraremos como tais padrões funcionam.
Goupia glabra et Eperua &quot;grandiora «A partir de o padrão em questão, podemos identificar no trecho de texto, as seguintes relações:
A partir de o padrão em questão, podemos identificar as seguintes relações no trecho de texto:
Podemos observar, por o disposto na Tabela 3.3, que alguns padrões de Morin e Jacquemin se equivalem a padrões identificados por Hearst.
Desta forma, somente alguns padrões de Morin e Jacquemin serão acrescentados ao nosso conjunto de regras.
Como pode ser visto na Tabela 3.4, os padrões 1, 2, 3, e 4 foram generalizados, pois entendemos que esta medida nos permitirá recuperar relações que não seriam identificadas caso mantidos os padrões no formato original.
Visto que estes padrões podem auxiliar na identificação de relações taxonômicas de interesse para uma ontologia de domínio, os mesmos serão utilizados em nossa proposta com as adaptações realizadas para seu uso em textos na língua portuguesa do Brasil.
Nossa primeira etapa, neste trabalho, foi fornecer uma visão geral sobre abordagens existentes para construção de ontologias a partir de textos.
Em a Tabela 3.5 apresentamos uma visão integrada dessas abordagens.
Podemos ver que, de modo geral, a construção de ontologias passa por os seguintes objetivos:
Extrair conceitos, extrair relações taxonômicas e não-taxonômicas e popular a ontologia com instâncias.
De essa forma podemos classificar abordagens como &quot;mais completas «ou &quot;menos completas «conforme a quantidade de objetivos que elas se propõem a alcançar.
Por esse ponto de vista, e conforme a Tabela 3.5, a abordagem de Buitelaar seria a menos abrangente (menos completa) entre as estudadas, tratando apenas da identificação de conceitos.
Já a abordagem de Maedche, devido a o número de objetivos que visa alcançar, estaria entre as mais completas.*
As abordagens estão identificadas por o primeiro autor.
Intervenção do usuário Outro ponto importante em relação a construção de ontologias é o nível de automatização proposto por a abordagem.
Quanto a esse aspecto, a abordagem de Degeratu e Hatzivassiloglou propõe um processo totalmente automatizado.
Isso, porém, nos deixa receosos quanto a a qualidade da ontologia resultante, pois podemos verificar que as demais abordagens estudadas propõem a construção semi-automática de ontologias a partir de textos, requisitando intervenções do usuário em alguma parte importante do processo como, por exemplo, validação de conceitos ou de relações extraídas.
Em a Tabela 3.6, podemos ver outras características mais específicas das abordagens estudadas.
O reuso ou não de ontologias existentes, bem como o uso de fontes de conhecimento adicionais (por exemplo, dicionários ou corpora de texto mais genéricos) para auxiliar na extração de conceitos e relacionamentos, são aspectos muito importantes a serem considerados.
A Tabela 3.7 refere- se a avaliação da ontologia resultante.
Através de ela podemos ver que a avaliação manual é a mais utilizada de entre as abordagens estudadas, tanto para a validação da ontologia resultante quanto para a validação das saídas em cada etapa como, por exemplo, na abordagem de Lame, que apresenta apenas avaliação dos resultados de cada etapa, sem uma avaliação final.
Segundo Maedche em, não existe medida padrão para avaliação de ontologias extraídas de texto, e então o autor propõe uma abordagem de avaliação baseada nas medidas de precisão e recall.
Uma plataforma para avaliação de ontologias extraídas é um dos seus trabalhos futuros.
Avalia somente precisão.
A avaliação foi realizada por dois especialistas.
Cita somente validação manual das saídas de cada etapa e não de avaliação da ontologia resultante.
Através das medidas de precisão e recall e com validação humana.
Avaliação realizada por um especialista.*
As abordagens estão identificadas por o primeiro autor O próximo capítulo descreve a abordagem proposta nesta pesquisa, a qual foca nas atividades de identificação de termos relevantes do domínio e relações taxonômicas entre esses termos, bem como a fase relacionada à geração da estrutura ontológica.
Descreve ainda as etapas de cada uma das fases.
Este capítulo apresenta nossa proposta para construção de estruturas ontológicas, fornecendo um detalhamento sobre cada etapa do processo.
Em este capítulo apresentamos nossa proposta para construção de estruturas ontológicas a partir de textos na língua portuguesa do Brasil.
A abordagem, que tem como foco as atividades de identificação de termos relevantes do domínio e identificação de relações taxonômicas entre esses termos, surgiu da combinação de abordagens e técnicas apresentadas no capítulo anterior.
A Figura 4.1 representa, de forma simplificada, as fases da abordagem que está sendo proposta.
Em as próximas seções apresentamos uma descrição de cada uma dessas fases.
O primeiro ponto importante a ser considerado sobre nossa proposta é a entrada utilizada por a mesma.
As abordagens estudadas no Capítulo 3 incluem uma fase inicial para realizar anotação lingüística do texto (com auxílio de alguma ferramenta), o que envolve tokenização, processamento léxico-morfológico e análise sintática do corpus, entre outras ações.
Sabemos que a eficácia de nossa proposta, assim como é o caso das abordagens estudadas, depende da correta identificação das etiquetas gramaticais.
Embora existam ferramentas de pré-etiquetagem gramatical do corpus, desenvolvidas dentro de o Grupo de Pesquisa em Processamento da Linguagem Natural da PUCRS, como descrito em, as mesmas ainda não se encontram com níveis de confiabilidade compátiveis com o desejado ao escopo deste trabalho.
Por não dispormos de uma ferramenta com alta confiabilidade na etiquetagem e nem de tempo hábil para construir- la, optamos por não realizar a etapa de anotação lingüística.
Assim, assumimos como entrada um corpus com textos já anotados lingüisticamente, com as seguintes informações associadas a cada palavra do documento:
A palavra no seu formato original;
O lema da palavra original, ou seja, a palavra em sua forma singular e masculina e;
A etiqueta gramatical da palavra (exemplo:
Substantivo, adjetivo, etc).
O padrão de etiquetas considerado neste trabalho é o utilizado por o NILC23.
A Tabela 4.1 apresenta tais categorias.
Núcleo Interinstitucional de Lingüística Computacional -- NILC (www_ Nilc_ Icmc_ Usp_ Br/ nilc/).
A primeira fase de nossa abordagem é a identificação de termos relevantes do domínio, que é constituída por as cinco etapas apresentadas na Figura 4.2.
Esta fase utiliza como entrada um corpus com as características apresentadas na seção anterior.
A execução das etapas 1 a 4 resulta numa lista de termos relevantes simples (termos mono palavra).
A execução da quinta etapa resulta numa lista de termos compostos.
A seguir descrevemos cada etapa desta primeira fase.
Eliminar termos que não representam conceitos de domínio:
Através de uma lista de stopwords são excluídas do corpus palavras comuns que possuem significado semântico limitado e, portanto, não são relevantes o domínio.
Atualmente essa lista conta com aproximadamente 550 palavras (entre artigos, preposições, advérbios, etc).
A lista de stopwords encontra- se no Anexo A. Em esta etapa, além de as stopwords, são também removidos do corpus todos os termos contendo caracteres não-alfabéticos como números e símbolos, os quais estão rotulados por as etiquetas apresentadas na Tabela 4.2.
Tais termos são excluídos para serem identificados como possíveis termos relevantes.
Porém vale salientar que os termos excluídos nesta etapa ainda podem ser utilizados em regras para identificação de termos compostos e relações taxonômicas entre os termos.
Outra característica da proposta é o fato de não trabalharmos com nomes próprios como termos (por exemplo,, Robert,, Kyoto,, Espanha,, Sergipe e, Brasil), visto que os mesmos representam instâncias de um domínio e, por conseqüência, seriam também instâncias numa ontologia.
Por exemplo,, Brasil e, Espanha seriam provavelmente instâncias do conceito (classe), país.
Como a extração de instâncias não faz parte do escopo desta pesquisa, nesta etapa, palavras que representam nomes próprios são também desconsideradas.
A identificação de nomes próprios no texto é realizada a partir de as heurísticas apresentadas na Tabela 4.3.
Além disso, também são identificadas, a partir de heurísticas, palavras abreviadas como, por exemplo, jr, &quot;tel», av, &quot;sr «e &quot;pág».
Assim como acontece com os nomes próprios, estas abreviaturas são igualmente desconsideradas nas etapas seguintes.
A heurística usada para identificar uma palavra abreviada consiste em verificar se a palavra termina com um ponto(».&quot;).
Em caso positivo (é abreviatura), a mesma é desconsiderada nas próximas etapas.
Resumindo, esta etapa consiste em identificar e desconsiderar nas próximas etapas, palavras que estejam na lista de stopwords, palavras contendo caracteres não-alfabéticos e palavras que representem nomes próprios e abreviaturas, pois estas palavras tendem a não constituir termos de um domínio.
Pesagem dos termos:
A segunda etapa consiste em pesar as palavras candidatas a termos relevantes do domínio.
Para isso utilizamos duas medidas:
TFIDF (term frequency x inverted document frequency) e Log--Likelihood.
Inicialmente apenas a medida TFIDF foi utilizada para pesar os termos e apresentar- los em ordem de relevância ao engenheiro de ontologia.
Porém, tal medida retornava apenas a classificação de todos termos, resultando numa quantidade muito grande de termos não relevantes sendo apresentados ao engenheiro de ontologia.
Foi então adicionada a medida Log--Likelihood para comparar a freqüência dos termos no corpus do domínio face a sua freqüência num corpus de referência, promovendo assim a exclusão automática de termos não relevantes ao domínio (ou seja, termos que aparecem em maior proporção no corpus de referência).
Em esta etapa, utilizamos o lema da palavra (disponível no corpus etiquetado) para realizar a pesagem.
A utilização do lema na pesagem se deve ao fato de que os termos normalmente aparecem no corpus com diferentes propriedades (gênero, número e grau).
Assim, considerando- se o lema da palavra, evita- se que um mesmo termo, representado com diferentes propriedades, receba distintos pesos como se fossem diferentes termos.
Por exemplo, podem aparecer nos textos os termos &quot;praia «e &quot;praias».
Se não fosse utilizado o lema da palavra para computar os pesos, teríamos dois termos diferentes, cada um com seu peso associado.
Utilizando- se o lema, estes termos passam a ser pesados como um único termo (&quot;praia&quot;).
Definição de limiar mínimo para termos:
A terceira etapa consiste em definir uma freqüência (tfidf) mínima aceitável para um termo no corpus ser considerado relevante ao domínio.
Com a definição desse limiar, os termos com freqüência abaixo de o mesmo são excluídos.
A definição desse limiar é responsabilidade do engenheiro de ontologia.
Entretanto, a poda conforme a freqüência deve ser aplicada com cautela, visto que termos que aparecem poucas vezes ou apenas uma vez num texto podem ser mais relevantes para o domínio do que termos mais freqüentes.
Excluir e Incluir termos:
A idéia aqui é sugerir ao engenheiro de ontologia, como termos relevantes, a lista de termos resultantes das etapas anteriores.
O engenheiro de ontologia pode então excluir os termos que julgar desnecessários ou por ventura incorretos.
Em esta etapa é permitido ao engenheiro de ontologia incluir termos relevantes não selecionados previamente.
Todos os termos resultantes desta etapa serão considerados nas etapas subseqüentes.
Identificar termos compostos:
A quinta etapa corresponde à identificação de termos compostos (ou termos multi-palavra).
A partir de a lista de termos relevantes, resultantes da execução das etapas 1 a 4, são selecionados termos compostos que contenham ao menos um termo relevante em sua composição.
A identificação dos termos compostos é realizada com base em regras expressas por seqüências de etiquetas que, quando encontradas no texto, podem representar termos compostos.
A Tabela 4.4 apresenta as seqüências de etiquetas utilizadas na identificação de termos compostos.
Os termos compostos resultantes desta etapa são também considerados termos relevantes do domínio.
A validação dos termos compostos extraídos deve ser realizada por o engenheiro de ontologia.
A Tabela 4.5 apresenta as entradas e saídas de cada uma das etapas envolvidas na fase de identificação de termos e ainda observações quanto a sua automatização.
A segunda fase da abordagem refere- se à identificação de relações taxonômicas entre os termos relevantes derivados da fase anterior.
Cada etapa desta fase procura extrair um conjunto de relações taxonômicas a partir de uma determinada abordagem.
A Figura 4.3 apresenta as etapas desta fase.
Identificar relações taxonômicas através dos padrões de Hearst:
Este segundo passo tem por objetivo a identificação de relações taxonômicas nos textos através dos padrões léxico-sintáticos propostos por Hearst em.
A idéia aqui é encontrar no corpus os padrões de Hearst onde exista ao menos um termo relevante envolvido.
A validação dessas relações também é realizada por o engenheiro de ontologia.
Os padrões de Hearst foram criados inicialmente para a língua inglesa.
Para sua utilização em textos da língua portuguesa do Brasil eles precisaram ser adaptados.
A Tabela apresenta os padrões originais e as traduções/ adaptações realizadas.
Primeiramente vale salientar que Hearst trabalha com sintagma nominal (noun phrase ­ Np) em seus padrões e que não dispomos desta de informação nos arquivos utilizados como entrada para a abordagem.
Em esse sentido, nossa adaptação foi substituir a informação de Np diretamente por um substantivo (Su).
Os padrões de Morin e Jacquemin foram desenvolvidos para a língua francesa, e por isso foi necessário traduzir- los/ adaptar- los para a língua portuguesa do Brasil.
Os padrões com suas adaptações são apresentados na Tabela 4.7.
De a mesma forma que Hearst, Morin e Jacquemin trabalham em seus padrões com informação em nível de sintagma nominal e, da mesma forma, substituímos essa informação diretamente por um substantivo A numeração que acompanha Su, Np ou LIST_ SU refere- se a sua posição na relação:
Hiperonímia e hiponímia.
A Tabela 4.8 apresenta as entradas e saídas de cada uma das etapas envolvidas na fase de identificação de relações taxonômicas.
Em esta fase, o objetivo é utilizar o conhecimento adquirido, termos relevantes e relações taxonômicas, para gerar uma estrutura ontológica numa linguagem de representação ontológica.
A linguagem escolhida foi a OWL, suportada por o framework Jena, o qual é um projeto open-source desenvolvido por o Hp Labs Semantic Web Programme.
Em OWL os termos são representados por classes e as relações são representadas por propriedades, mapeadas através de relações hierárquicas (rdfs:
SubClassOf). A linguagem OWL permite que a ontologia seja editada ou estendida numa ferramenta para edição de ontologias.
Em esta etapa o engenheiro de ontologia poderá gerar a estrutura ontológica com as seguintes informações:
Termos simples;
Termos compostos;
Relações baseadas em termos compostos;
Relações baseadas nos padrões de Hearst;
Relações baseadas nos padrões de Morin e Jacquemin.
A geração da estrutura ontológica em OWL está disponível no protótipo desenvolvido e apresentado no próximo capítulo.
Como pôde ser visto durante a descrição da proposta, existe a necessidade de avaliação dos resultados em algumas etapas.
A forma avaliação proposta segue a forma mais utilizada de entre as abordagens estudadas:
A validação manual por um especialista.
A Tabela 4.9 referese à forma de avaliação da ontologia resultante proposta por diferentes autores, em comparação à proposta de avaliação desta dissertação.
Avaliação manual realizada por um especialista nas saídas de cada etapa e também da ontologia resultante.
Uma plataforma para avaliação de ontologias extraídas é um dos seus trabalhos futuros.
Avalia somente precisão.
A avaliação foi realizada por dois especialistas.
Cita somente validação manual das saídas de cada etapa e não de avaliação da ontologia resultante.
Através das medidas de precisão e recall e com validação humana.
Avaliação realizada por um especialista.*
As abordagens estão identificadas por o primeiro autor Em nossa proposta, a avaliação dos resultados se torna necessária a partir de a quarta etapa da primeira fase, logo após a pesagem dos termos e definição do limiar.
Em esta etapa são apresentados ao engenheiro de ontologia (especialista) os termos identificados e considerados relevantes ao domínio.
Cabe ao engenheiro de ontologia avaliar os termos e então excluir os aqueles julgados incorretos.
Caso algum termo considerado relevante por o engenheiro não tenha sido selecionado, o mesmo pode incluir- lo nesta etapa.
A quinta etapa desta mesma fase corresponde à identificação de termos compostos.
A validação dos termos compostos extraídos também deve ser realizada por o engenheiro de ontologia.
Uma avaliação correta neste ponto (termos simples e termos compostos) é muito importante, pois todos os termos resultantes desta etapa servirão de base para a segunda fase.
Em a segunda fase da abordagem, relacionada à identificação de relações taxonômicas, a avaliação dos resultados por o engenheiro de ontologia deve ocorrer em todas as três fases:
Identificar relações taxonômicas com base em termos compostos;
Identificar relações taxonômicas através dos padrões de Hearst;
Identificar relações taxonômicas através dos padrões de Morin e Jacquemin.
A avaliação dos resultados das etapas citadas nesta sessão implicará na qualidade da ontologia resultante.
A abordagem aqui proposta, de modo geral, está baseada nas abordagens e medidas estudadas durante o desenvolvimento deste trabalho.
A Tabela 4.10 mostra em destaque as principais contribuições de cada autor para a definição desta proposta.
As tabelas 4.11 e 4.12 posicionam a abordagem aqui proposta face a características gerais e características mais específicas das abordagens estudadas.
O usuário interage na integração de subárvores a nodos apropriados na ontologia e na definição de regras Hearst Relações taxonômicas Identificar relações Identificação baseada em padrões Semi-automático O usuário valida as relações extraídas Morin Relações taxonômicas Identificar relações Identificação baseada em padrões Semi-automático O usuário valida as relações extraídas Identificar termos relevantes simples e compostos Identificar relações taxônomicas Abordagem estatística Identificação baseada em padrões Semi-automático O usuário valida saídas e determina alguns limiares se desejado Velardi Baségio Relações taxonômicas Termos relevantes Relações taxonômicas* As abordagens estão identificadas por o primeiro autor.&amp;&amp;&amp;
Este capítulo descreve o protótipo desenvolvido no intuito de auxiliar na validação e avaliação da abordagem para identificação de estruturas ontológicas proposta nesta dissertação.
Em este capítulo é descrito o protótipo de software desenvolvido no contexto deste trabalho.
O protótipo foi desenvolvido com o objetivo principal de verificar a aplicabilidade da abordagem para identificação de estruturas ontológicas a partir de textos na língua portuguesa do Brasil proposta nesta dissertação.
É importante ressaltar que nosso objetivo, durante a construção do protótipo, foi exclusivamente desenvolver a abordagem proposta, sendo que questões relacionadas à modelagem do protótipo e otimização dos algoritmos foram tratadas com menor prioridade.
Assim, neste capítulo apresentamos, de modo geral, o funcionamento dos módulos do protótipo sem entrar em maiores detalhes sobre as características específicas da implementação.
Características gerais da implementação O protótipo de software foi implementado na linguagem Java, versão 1.5.0, através do Ide Net Beans versão 4.1.
Durante seu desenvolvimento, ele foi testado num ambiente com plataforma Windows XP utilizando um computador Pentium 3, com 1,2 GHz e 256 MB de memória.
Funcionalidades As funcionalidades do protótipo estão descritas na Tabela 5.1.
O ator corresponde ao engenheiro de ontologias que fará uso do ambiente de apoio para gerar uma estrutura ontológica.
As funcionalidades em negrito são aquelas onde existe especificidade quanto a a língua portuguesa.
O engenheiro de ontologia deve ser capaz de importar um arquivo texto contendo o corpus de um determinado domínio.
O engenheiro de ontologia deve ser capaz de visualizar os textos do corpus importado.
O engenheiro de ontologia deve ser capaz de excluir do processo de pesagem, as stopwords e palavras contendo caracteres não-alfabéticos.
O engenheiro de ontologia deve ser capaz de pesar os termos através da medida TFIDF.
O engenheiro de ontologia deve ser capaz de visualizar os termos com seus pesos associados.
O engenheiro de ontologia deve ser capaz de filtrar os termos através da definição de um limiar para seu peso.
O engenheiro de ontologia deve ser capaz de excluir termos considerados irrelevantes.
O engenheiro de ontologia deve ser capaz de incluir termos não selecionados.
O engenheiro de ontologia deve ser capaz de identificar termos compostos.
O engenheiro de ontologia deve ser capaz de excluir termos compostos identificados.
O engenheiro de ontologia deve ser capaz de identificar relações taxonômicas a partir de termos compostos.
O engenheiro de ontologia deve ser capaz de identificar relações taxonômicas a partir de os padrões de Hearst.
O engenheiro de ontologia deve ser capaz de identificar relações taxonômicas a partir de os padrões de Morin e Jacquemin.
O engenheiro de ontologia deve ser capaz de excluir relações identificadas.
O engenheiro de ontologia deve ser capaz de identificar relações duplicadas.
O engenheiro de ontologia deve ser capaz de gerar a estrutura ontológica em OWL.
O engenheiro de ontologia deve ser capaz de visualizar a estrutura ontológica gerada.
O engenheiro de ontologia deve ser capaz de salvar a estrutura ontológica num arquivo OWL.
A Figura 5.1 fornece uma visão geral do processo de identificação de estruturas ontológicas do protótipo, caracterizando as entradas e saídas geradas em cada etapa.
Em a Figura 5.2 apresentamos a arquitetura do protótipo e o modo como seus módulos estão estruturados para atender às funcionalidades propostas.
Módulo de Importação e Gerenciamento de Textos Este módulo é o primeiro a ser utilizado no protótipo, pois possibilita ao usuário selecionar um arquivo texto contendo os textos de um determinado domínio;
Obter informações sobre o tamanho do corpus em questão e visualizar os textos importados, que são utilizados nos demais módulos.
A) Arquivo de entrada Assume- se como entrada um arquivo texto contendo artigos de um determinado domínio, sendo cada artigo formado por um título e um texto.
Um título é identificado por a informação id\&gt;, onde id é o número que identifica o artigo no corpus.
O texto do artigo inicia por a informação id\&gt; onde id tem a mesma identificação do título do artigo.
Cada artigo termina quando um novo título é identificado.
Além disso, cada linha do arquivo, exceto aquelas que definem o início do título ou do texto do artigo, contém as seguintes informações, separadas por um espaço em branco:
A palavra no formato original, o lema e a etiqueta gramatical da palavra.
A Figura 5.3 representa um arquivo texto contendo as informações citadas.
Em nossa abordagem, apenas a palavra no formato original, o lema e a etiqueta gramatical da palavra são utilizados, sendo as demais informações desconsideradas.
B) Seleção do Arquivo do Corpus A Figura 5.4 representa a interface do protótipo responsável por a importação do arquivo do corpus.
Primeiramente, o protótipo permite selecionar um arquivo texto contendo o corpus.
Uma vez selecionado o arquivo, é permitido ao usuário visualizar cada texto importado numa pequena tela.
Este módulo possibilita ao usuário identificar os termos relevantes contidos no corpus do domínio selecionado.
Módulo de Identificação de Termos Relevantes A Figura 5.5 apresenta a interface para a identificação dos termos relevantes simples, juntamente com os aspectos considerados durante o processo.
Remover Stopwords: A seleção deste item implica que, durante a pesagem dos termos, serão desconsideradas todas as palavras identificadas como stopwords e palavras contendo caracteres não-alfabéticos.
Apesar de a proposta estar baseada na exclusão de Stopwords, o protótipo possibilita, para fins de testes, a não seleção deste item, resultando na não execução deste passo.
Pesar termos:
Este botão, quando acionado, ativa o processo responsável por a pesagem dos termos.
A pesagem é realizada através das medidas Log--Likelihood e tfidf.
Como resultado, os termos com seus respectivos pesos (tf, Log--Likelihood, tfidf) são apresentados numa tabela.
Filtrar: Este botão, quando acionado, exclui os termos relevantes da tabela que possuírem um peso abaixo de o limiar definido para a medida tfidf.
Porém, também é possível utilizar no filtro, a simples freqüência do termo (tf) ou o peso Log--Likelihood associado ao termo.
Excluir selecionados:
Este botão, quando acionado, exclui todos os termos selecionados por o usuário.
A idéia é possibilitar a exclusão de termos julgados desnecessários ou irrelevantes para o domínio.
Incluir termo:
Tem por objetivo possibilitar ao usuário incluir termos relevantes para o domínio, que não tenham sido selecionados por a ferramenta.
A o clicar no botão o sistema abre uma nova janela permitindo a entrada de uma palavra e sua adição.
Módulo de Identificação de Termos Compostos A Figura 5.6 apresenta a interface para a identificação de termos compostos, juntamente com os aspectos considerados durante o processo.
Identificar: Este botão, quando acionado, identifica termos compostos que contenham ao menos um termo relevante em sua composição.
A extração é baseada em regras explícitas de seqüências de etiquetas nos textos.
Como resultado são apresentados os termos compostos juntamente com sua freqüência no texto.
Excluir selecionados:
Este botão, quando acionado, exclui os termos selecionados por o usuário que foram julgados desnecessários ou irrelevantes.
As interfaces disponibilizadas na ferramenta para a identificação de relações taxonômicas apresentam as funcionalidades de identificação e exclusão de relações.
Em ambas, são apresentadas como resultado, as relações identificadas, suas freqüências no texto e a regra por a qual foram extraídas.
Como exemplo, a Figura 5.7 apresenta a interface para identificação de relações taxonômicas a partir de os padrões de Morin e Jacquemin.
Este módulo tem como objetivo fornecer funcionalidades que possibilitem ao usuário a geração da estrutura ontológica numa linguagem de representação de ontologias.
Módulo de Geração da Estrutura em OWL O protótipo utiliza o framework Jena para criar, em OWL, uma estrutura inicial da ontologia do domínio.
A Figura 5.8 mostra a interface para geração do código OWL, tendo- se como base os termos e relações selecionados.
Termos: Possibilita ao usuário selecionar quais termos (Relevantes e Compostos) serão incluídos na estrutura ontológica a ser gerada.
Relações: Possibilita ao usuário selecionar quais relações (Compostas, Hearst e/ ou Morin e Jacquemin) serão incluídas na estrutura ontológica a ser gerada.
Gerar: Ativa a geração do código OWL, de acordo com os termos e relações selecionados.
Cada termo é representado em OWL por uma classe e as relações entre os termos (propriedades em OWL) são mapeadas através de relações hierárquicas (rdfs:
SubClassOf). Módulo de Exportação para Arquivo OWL O módulo de exportação possibilita, através do botão &quot;Salvar «apresentado na interface mostrada na Figura 5.8, salvar a estrutura ontológica gerada num arquivo OWL.
A partir de o arquivo OWL gerado é possível estender a ontologia manualmente ou com uso de ferramentas que apóiam a edição de ontologias.
Como exemplo, na Figura 5.9 apresentamos parte de um código OWL gerado sendo visualizado na ferramenta Protégé.
Este capítulo destacou o protótipo de software desenvolvido no contexto desta pesquisa.
Foram apresentados os módulos do protótipo, com suas características, funcionalidades e interfaces, bem como o formato utilizado no arquivo de entrada.
O próximo capítulo apresenta o estudo de caso realizado com o objetivo de validar a abordagem proposta com a ferramenta desenvolvida.
Serão ainda descritos o processo de condução da avaliação e seus resultados.
O presente capítulo descreve os estudos de caso realizados sobre o domínio do Turismo com o objetivo de validar a abordagem para identificação de estruturas ontológicas proposta nesta pesquisa.
Para validar a abordagem proposta nesta pesquisa, foi definido um estudo de caso consistindo na identificação de termos e relações taxonômicas a partir de um corpus do domínio do Turismo, fazendo uso do protótipo apresentado no Capítulo 5.
Seguindo a abordagem proposta, em cada etapa ocorreu a validação dos resultados por um especialista na área do Turismo.
Os dados selecionados numa etapa serviram de entrada e direcionaram os resultados das etapas subseqüentes, ou seja, termos excluídos numa etapa foram desconsiderados nas demais.
O estudo de caso teve como objetivo analisar o modo como a abordagem proposta auxilia na identificação de estruturas ontológicas.
Um segundo estudo de caso foi realizado utilizando o mesmo corpus do domínio do primeiro estudo de caso.
A diferença deste para o primeiro está na forma como os dados foram apresentados ao especialista.
Após a seleção do corpus, a abordagem foi executada sem a intervenção do especialista, isto é, sem que termos ou relações fossem excluídos entre as etapas, fazendo com que um número maior de termos e relações fossem extraídos.
A o final do processo, todos os termos e relações extraídos foram apresentados ao especialista para exclusão daqueles não relevantes ao domínio.
Este segundo estudo de caso teve como objetivo verificar a viabilidade da abordagem proposta ser executada sem intervenção humana, possibilitando maior automatização, com validação do resultado por o especialista apenas no final do processo.
O especialista que participou dos estudos de caso aqui apresentados, sendo responsável por a validação tanto das saídas de cada etapa quanto da ontologia final, é um profissional altamente especializado24 na área do Turismo.
Em as seções seguintes, discorremos sobre o corpus utilizado, bem como os resultados obtidos em cada estudo de caso.
Doutor na área do Turismo, coordenador do curso de pós-graduação em Turismo da PUCRS, tendo grande atuação profissional na área em questão.
O corpus de referência (ou corpus geral), utilizado no estudo de caso é formado a partir de uma coleção de documentos do Jornal Folha de São Paulo do ano de 199425, já no formato apresentado na seção 4.1 do Capítulo 4.
Os textos utilizados pertencem a diferentes seções do jornal e compreendem um total de 3.862 documentos com 974.685 palavras.
O corpus do domínio utilizado para o estudo de caso é constituído por um total de 294 documentos com 88.601 palavras, documentos estes extraídos do Caderno de Turismo da Folha (retirados do corpus de referência citado).
Um ponto importante a ser considerado em relação a o corpus utilizado é o fato de que textos de jornal, de modo geral, mesmo que de uma seção específica, não podem ser considerados textos especializados de um domínio.
Textos com essa característica são considerados semi-especializados.
Assim, por não ser um corpus especializado do domínio do Turismo, os textos utilizados no estudo de caso não contêm apenas termos relacionados ao domínio do Turismo.
A utilização desse corpus se deu por o fato de não dispormos de um corpus específico de domínio etiquetado conforme nossa necessidade.
A execução deste estudo de caso iniciou com a seleção do corpus do Turismo descrito na seção anterior.
A primeira etapa realizada foi a identificação de termos, sendo o especialista do domínio responsável por selecionar, de entre os termos extraídos por a ferramenta (simples e compostos), aqueles considerados relevantes ao domínio do Turismo.
Depois o especialista selecionou as relações taxonômicas relevantes, identificadas por a ferramenta com base nos termos selecionados previamente.
Por fim gerou- se a estrutura ontológica em OWL.
Em as seções seguintes detalhamos os resultados obtidos no desenvolvimento do estudo de caso.
Este corpus foi gentilmente disponibilizado por o Núcleo Interinstitucional de Lingüística Computacional (NILC), ao grupo de pesquisa em PLN da PUCRS.
Maiores informações sobre o NILC podem ser encontradas em www_ Nilc_ Icmc_ Usp_ Br /nilc/. Em as subseções seguintes são apresentados os resultados referentes a cada passo da etapa de identificação de termos relevantes do domínio.
Eliminar termos que não representam conceitos de domínio Neste passo foram excluídas as palavras que não têm significado para o domínio, caracterizadas como stopwords, palavras contendo caracteres não-alfabéticos (números e símbolos), nomes próprios e abreviaturas.
A Tabela 6.1 apresenta a quantidade de palavras identificadas como não representando conceitos do domínio, explicitando a categoria por a qual foram desconsideradas.
A coluna &quot;Diferentes «apresenta quantas palavras distintas foram excluídas, ou seja, considera cada palavra apenas uma vez, ignorando as repetições.
A coluna &quot;Total «apresenta a quantidade total de palavras excluídas, contabilizando todas as vezes que a palavra aparece.
Por exemplo, se a palavra &quot;aquele «foi excluída 5 vezes, ela será contabilizada 5 vezes na coluna &quot;Total «e apenas 1 vez na coluna &quot;Diferentes».
A coluna&quot;% do corpus &quot;refere- se ao percentual de palavras excluídas do corpus do domínio utilizando como referência a coluna «Total «em relação a o total de palavras do corpus.
Como podemos verificar na Tabela 6.1, grande parte do corpus do domínio não foi considerada como candidata a termo relevante.
Em o total foram excluídas 61.374 palavras do corpus.
O maior percentual de palavras excluídas foi classificado como stopwords, contabilizando mais de 50% do corpus utilizado.
Também chama à atenção, o grande número de nomes próprios encontrados no texto, representando possíveis instâncias do domínio.
O Gráfico 6.1 apresenta a distribuição das palavras do corpus, excluídas de acordo com sua categoria, bem como o total de palavras restantes após a execução deste passo.
Nomes próprios Não-alfabéticas Restantes Stopwords Abreviaturas Gráfico 6.1: Distribuição das palavras excluídas por categoria e palavras restantes Pesagem dos termos Com a eliminação das palavras não relevantes ao domínio, restaram 27.227 palavras, de entre as quais 14.485 são substantivos, correspondendo a 16,35% do corpus.
Os 4.047 substantivos restantes, que são os verdadeiros candidatos a termos relevantes, tiveram sua freqüência no corpus do domínio comparada a sua freqüência no corpus de referência com a utilização da medida Log--Likelihood.
Este passo nos possibilitou excluir automaticamente 3.635 diferentes substantivos não específicos ao domínio do Turismo, ou seja, substantivos que aparecem em maior proporção no corpus de referência.
Assim, restaram apenas 412 substantivos candidatos a termos relevantes do domínio.
O Gráfico 6.2 apresenta a proporção entre os termos filtrados por a medida LogLikelihood e os termos que permaneceram após a filtragem.
Para apresentar os termos ao especialista em ordem de relevância, foi utilizada a medida TFIDF, associando- se um peso a cada uma das palavras restantes.
A utilização da medida TFIDF se deve ao fato de que essa medida mostrou um melhor desempenho em testes realizados do que a simples freqüência do termo no corpus.
Excluídos Não excluídos Gráfico 6.2: Distribuição dos substantivos candidatos a termos, resultantes da pesagem Definição de limiar mínimo para termos Nossa intenção neste passo foi possibilitar a poda de termos a partir de a definição de uma freqüência mínima para o termo ser considerado relevante.
Em a ferramenta foi disponibilizada a poda por a medida TFIDF, por a simples freqüência (TF) ou ainda por a medida Log--Likelihood.
Como mencionado anteriormente, a poda através da freqüência deve ser seguida com cautela, visto que termos que aparecem poucas vezes ou apenas uma vez num texto podem ser mais relevantes, para o domínio, do que termos mais freqüentes.
Foi possível verificar tal afirmação através da utilização da medida Log--Likelihood, onde termos com menor freqüência no corpus do domínio foram considerados mais relevantes do que termos com maior freqüência, quando comparados com o corpus de referência.
O especialista teve a mesma percepção e optou por não realizar uma poda por a definição de um limiar.
Excluir/ Incluir termos:
Esse passo foi muito importante na execução deste estudo de caso.
Em este ponto o especialista pôde excluir termos extraídos que não considerou relevantes para o domínio do Turismo.
De os 412 termos resultantes apresentados ao especialista, 362 foram excluídos, ou seja, 50 termos foram considerados relevantes, correspondendo a 12,14% dos termos extraídos.
Vale ressaltar que não houve interesse do especialista na inclusão de termos além de os identificados por o protótipo como relevantes.
A Tabela B. 1 do Anexo B apresenta os termos selecionados por o especialista.
Selecionados Excluídos Gráfico 6.3: Distribuição dos candidatos a termos excluídos e selecionados Os 412 candidatos a termos do domínio foram apresentados ao especialista na ordem de relevância identificada por a medida TFIDF.
A Tabela 6.2 apresenta o percentual de termos selecionados por o especialista em comparação à quantidade de termos extraídos, considerando- se a ordem de relevância.
A Tabela 6.2 nos mostra que mais de 50% dos termos selecionados por o especialista estavam entre os 100 termos mais relevantes, de acordo com a medida TFIDF, e 80% dos termos selecionados estavam entre os 200 termos mais relevantes, ou seja, na primeira metade dos termos apresentados ao especialista.
Considerando o total de termos selecionados por o especialista, todos estão entre os 300 termos mais relevantes.
Identificar termos compostos a partir de a lista de termos relevantes Este passo se propôs a identificar termos compostos a partir de a lista de termos relevantes selecionados previamente por o especialista.
Foram extraídos 284 termos compostos, de entre os quais 154 foram selecionados por o especialista, ou seja, 54,23% dos termos extraídos.
A Tabela B. 2 do Anexo B apresenta os termos compostos selecionados por o especialista.
Em a Tabela 6.3 podemos verificar os resultados obtidos por cada regra utilizada na identificação de termos compostos.
E os resultados obtidos diferem bastante entre as regras, tanto na quantidade de termos extraídos quanto na quantidade de termos selecionados.
A regras 5, por exemplo, não extraiu nenhum termo composto.
Já as regras 1 e 2, por exemplo, foram responsáveis por a extração de poucos termos e as regras 8 e 9 extraíram um número maior de termos.
Em relação a os termos selecionados por o especialista, as regras também tiveram diferentes resultados.
Algumas regras não tiveram termo algum selecionado e outras tiveram um grande número de termos selecionados como, por exemplo, a regra 8, que também obteve o melhor percentual de termos selecionados entre as regras utilizadas.
Já a regra 9 foi responsável por o maior número de termos extraídos e também por o maior número de termos selecionados, ou seja, uma quantidade maior que a soma dos resultados obtidos nas demais regras.
O Gráfico 6.4 apresenta a distribuição dos termos compostos selecionados por o especialista de acordo com a regra por os quais foram extraídos.
Regra 1 Regra 2 Regra 3 Regra 4 Regra 5 Regra 6 Regra 7 Regra 8 Regra 9 Gráfico 6.4: Distribuição dos termos compostos selecionados por o especialista Com exceção da regra 9, as demais regras têm como base uma preposição(_ PR), sendo que a mesma pode assumir 3 diferentes valores (&quot;de», &quot;de a e «de o).
Assim, analisamos como se comportaram as regras para cada uma dessas preposições.
A Tabela 6.4 apresenta a relação entre termos extraídos e termos selecionados, de acordo com a preposição por a qual foram extraídos.
Podemos verificar na Tabela 6.4 que o número de termos compostos extraídos utilizando a preposição &quot;de «é bem maior que o número de termos compostos extraídos para as preposições &quot;de a e «do, chegando a 59,05% do total de termos extraídos.
O mesmo pode ser dito em relação a os termos selecionados por o especialista, onde o número de termos selecionados com a preposição &quot;de «é maior que a soma do número de termos selecionados para as preposições &quot;de a e «do, chegando 75,75% dos termos selecionados entre as regras que utilizam preposição como base.
O Gráfico 6.5 apresenta a distribuição dos termos selecionados, com relação a a preposição por a qual foram extraídos.
Selecionados 'de` Selecionados' de a` Selecionados'do'Gráfico 6.5: Distribuição dos termos compostos selecionados de acordo com a preposição A segunda fase da abordagem refere- se à identificação de relações taxonômicas entre os termos relevantes (simples e compostos) derivados da fase anterior.
Em as subseções seguintes são apresentados os resultados referentes a cada passo desta etapa.
A Tabela B. 3 do Anexo B apresenta os termos compostos selecionados por o especialista.
Identificar relações taxonômicas com base em termos compostos.
Esse passo buscou identificar relações taxonômicas a partir de o núcleo do sintagma de termos compostos, relacionando cada termo composto ao termo relevante que faz parte da sua composição.
Em esta etapa foram extraídas 284 relações taxonômicas.
De as relações extraídas, o especialista selecionou 152, o que representa 53,52% do total extraído.
Apesar de ser um método grosseiro, esta regra para identificação de relações taxonômicas foi a que obteve o melhor resultado de entre as regras utilizadas.
O Gráfico 6.6 apresenta a proporção entre relações selecionadas e relações excluídas.
Relações selecionadas Relações excluídas Gráfico 6.6: Proporção entre relações taxonômicas selecionadas e excluídas Identificar relações taxonômicas através dos padrões de Hearst Apesar de o grande número de padrões utilizados nesta etapa, para identificação de relações taxonômicas, foram extraídas por a ferramenta apenas oito relações, sendo que apenas uma de elas foi selecionada por o especialista.
A Tabela 6.5 apresenta os padrões de Hearst (adaptados), a partir de os quais as oito relações foram extraídas.
Identificar relações taxonômicas através dos padrões de Morin e Jacquemin Em este passo foram extraídas por a ferramenta apenas quatro relações taxonômicas e nenhuma de elas foi selecionada por o especialista como relevante.
A Tabela 6.6 apresenta os padrões de Morin e Jacquemin (adaptados), a partir de os quais as relações foram extraídas.
Em esta fase o objetivo foi utilizar os termos e relações taxonômicas selecionados por o especialista para gerar uma estrutura ontológica na linguagem de representação ontológica denominada OWL.
O resultado deste passo foi a criação de um arquivo OWL, que pode ser utilizado em editores de ontologias como Protégé, permitindo ao especialista do domínio continuar o desenvolvimento da ontologia.
A execução do segundo estudo de caso seguiu basicamente as mesmas etapas do primeiro, porém sem a intervenção do usuário no que se refere à exclusão de termos e relações no decorrer de o processo.
Como foi utilizado o mesmo corpus nos dois estudos de caso, os resultados dos primeiros passos são exatamente os mesmos e, portanto, não serão repetidos neste segundo estudo de caso.
Aqui serão apresentados os resultados a partir de o passo relativo à primeira intervenção do especialista no primeiro estudo de caso, ou seja, a exclusão/ inclusão de termos.
A o final do processo, coube ao especialista selecionar os termos e as relações taxonômicas relevantes extraídas por a ferramenta.
A diferença, neste caso, é que os dados foram apresentados ao especialista uma única vez e em maior quantidade, pois não ocorreu exclusão de termos no decorrer de o processo.
Por fim gerou- se a estrutura ontológica em Em as seções seguintes detalhamos os resultados obtidos na execução deste estudo de caso.
Apesar de os termos e relações terem sido validados somente no final do processo, para melhor compreensão e visualização os resultados das validações por o especialista serão apresentados juntamente com os resultados referentes à extração dos termos e relações.
Excluir/ Incluir termos Ao contrário de o primeiro estudo de caso, em o qual termos não relevantes foram excluídos neste passo, neste estudo de caso os 412 termos resultantes dos passos anteriores foram utilizados nas etapas subseqüentes como termos relevantes ao domínio.
Somente no final do processo o especialista selecionou os 50 termos tidos como relevantes ao domínio do Turismo.
Identificar termos compostos a partir de a lista de termos relevantes Este passo se propôs a identificar termos compostos a partir de os 412 termos relevantes resultantes do passo anterior.
Foram extraídos 1.245 termos compostos, de entre os quais 203 foram selecionados por o especialista ao final do processo, ou seja, 16,31% dos termos extraídos.
Em a Tabela 6.7 podemos verificar que existem regras com poucos ou nenhum termo composto extraído e regras a partir de as quais foi extraído um número maior de termos.
Em a regra 1, por exemplo, temos poucos termos extraídos, porém com maior precisão.
Por outro lado temos a regra 3 a partir de a qual foram extraídos 23 termos e nenhum foi selecionado por o especialista.
Já a regra 9 foi responsável por um grande número de termos extraídos e também por um grande número de termos selecionados (56,65% do total de termos selecionados).
O Gráfico 6.7 apresenta a distribuição dos termos compostos selecionados por o especialista de acordo com a regra por os quais foram extraídos.
Regra 1 Regra 2 Regra 3 Regra 4 Regra 5 Regra 6 Regra 7 Regra 8 Regra 9 Gráfico 6.7: Distribuição dos termos compostos selecionados por o especialista Assim como no primeiro estudo de caso, com exceção da regra 9, as demais regras têm como base uma preposição(_ PR), que pode assumir 3 diferentes valores (&quot;de», &quot;de a e «de o).
A Tabela 6.8 apresenta a relação entre termos extraídos e termos selecionados de acordo com a preposição por a qual foram extraídos.
Podemos verificar na Tabela 6.8 que o número de termos compostos extraídos utilizando a preposição &quot;de «é bem maior do que aquele para as preposições &quot;de a e «do, chegando a 53,58% do total de termos compostos extraídos.
O mesmo pode ser dito em relação a os termos selecionados por o especialista, onde o número de termos com preposição &quot;de «é maior que a soma do número de termos selecionados para as preposições &quot;de a e «do, chegando a 71,59% dos termos selecionados entre as regras que utilizam preposição como base.
O Gráfico 6.8 apresenta a distribuição dos termos selecionados, de acordo a preposição por a qual foram extraídos.
Selecionados 'de` Selecionados' de a` Selecionados'do'Gráfico 6.8: Distribuição dos termos compostos selecionados de acordo com a preposição A segunda fase da abordagem refere- se à identificação de relações taxonômicas entre os termos relevantes derivados da fase anterior.
Em as subseções seguintes são apresentados os resultados referentes a cada passo desta etapa.
Identificar relações taxonômicas com base em termos compostos Este passo buscou identificar relações taxonômicas a partir de o núcleo do sintagma de termos compostos, relacionando cada termo composto ao termo relevante que faz parte da sua composição.
Em esta etapa foram extraídas 1.104 relações taxonômicas tendo como base os termos compostos da Tabela 6.7.
De as relações extraídas, o especialista selecionou 161, o que representa 14,58%.
Neste passo foram extraídas por a ferramenta apenas 17 relações taxonômicas e apenas uma de elas foi selecionada por o especialista, representando 5,89%.
A Tabela 6.10 apresenta os padrões de Hearst (adaptados) a partir de as quais as relações foram extraídas.
Identificar relações taxonômicas através dos padrões de Morin e Jacquemin Em este passo foram extraídas por a ferramenta apenas 13 relações taxonômicas e nenhum foi selecionada por o especialista como relevante.
A Tabela 6.11 apresenta os padrões de Morin e Jacquemin (adaptados), a partir de os quais as regras foram extraídas.
Como mencionado no primeiro estudo de caso, este passo serviu para gerar uma estrutura ontológica em linguagem de representação ontológica com base nos termos e relações taxonômicas selecionados por o especialista.
O resultado deste passo foi a criação de um arquivo OWL, que pode ser utilizado em editores de ontologias como Protégé, permitindo ao especialista do domínio continuar o desenvolvimento da ontologia.
Esta seção apresenta uma análise sobre o desempenho da abordagem proposta, discutindo sobre resultados obtidos, identificando problemas e levantando possíveis causas.
A idéia é analisar a eficiência da abordagem no que diz respeito a sua capacidade em identificar termos e relações taxonômicas corretamente.
Em esse contexto, foi objetivo dos estudos de caso apresentados neste capítulo possibilitar uma avaliação do modo como a abordagem proposta auxilia na identificação de estruturas ontológicas.
Um ponto importante a ser considerado, como descrito no inicio do capítulo, é o fato de que textos de jornal, devido a sua característica, não são considerados específicos de um domínio, mas semi-especializados para o domínio.
Assim, por não ser um corpus especializado do domínio do Turismo, os textos utilizados no estudo de caso não contêm apenas termos relacionados ao domínio do Turismo.
De essa forma, a utilização do corpus nos estudos de caso pode ter gerado um resultado não tão preciso quanto se fosse utilizado um corpus específico do domínio.
Durante a execução dos primeiros passos, que foram iguais para os dois estudos de caso, alguns dados nos chamaram a atenção.
O primeiro diz respeito ao grande número de palavras que não representam conceitos no domínio, chegando a quase 70% do corpus utilizado.
Outro dado que merece atenção foi o grande número de nomes próprios encontrados no texto, pois nomes próprios podem representar instâncias (indivíduos) de um domínio.
A identificação dessas instâncias e das classes de uma ontologia às quais estão relacionadas pode gerar outro trabalho interessante.
A utilização da medida Log--Likelihood foi muito importante para os resultados obtidos nos estudos de caso.
Com o uso da medida foram excluídos muitos termos sem significado para o domínio do Turismo, pelo menos quando comparados ao corpus de referência.
Para se ter uma idéia, numa execução como a do segundo estudo de caso se, ao invés de a medida Log--Likelihood, utilizássemos apenas a medida TFIDF, obteríamos um total de 3.308 candidatos a termo relevante ao invés de os 412 retornados com o uso da medida LogLikelihood.
E isso traria conseqüências para as etapas seguintes.
A Tabela 6.12 apresenta os dados resultantes de uma execução como a do segundo estudo de caso, porém apenas com a utilização da medida TFIDF.
Como podemos ver na tabela, a diferença de termos e relações extraídas é relativamente grande quando comparadas aos resultados obtidos com o uso da medida Log--Likelihood.
Um ponto importante a ser considerado é o fato de que o tamanho do corpus utilizado é pequeno, e que um corpus maior irá conter possivelmente muito mais termos e relações do que os apresentados na Tabela 6.12.
A utilização da medida TFIDF para apresentar os candidatos a termo relevante do domínio em ordem de relevância obteve um bom resultado.
mais de 50% dos termos selecionados por o especialista estavam entre os 100 termos mais relevantes e 80% dos termos selecionados encontravam- se na primeira metade dos termos apresentados ao especialista.
A exclusão de termos através da definição de um limiar, como proposto na abordagem, não foi executada nos estudos de caso apresentados neste capítulo, devido a a possibilidade de termos com grande relevância serem excluídos por possuírem um peso menor.
Talvez o tamanho do corpus utilizado nos estudos de caso não tenha gerado a necessidade de uma exclusão por limiar, mas num corpus maior este passo pode vir a ser importante, mesmo que alguns termos relevantes possam vir a ser perdidos com sua utilização.
Este passo também poderia ser utilizado numa possível execução automática da abordagem, semelhante ao que ocorreu no segundo estudo de caso.
Porém seriam necessários vários testes até encontrar um bom ponto de corte.
O baixo número de termos selecionados por o especialista nos leva a entender que uma solução totalmente automatizada e com alto grau de precisão não seja viável quando se utilizando apenas de técnicas estatísticas para identificação de termos.
Esse baixo resultado pode ser conseqüência do uso de um corpus semiespecializado como o utilizado nos estudos de caso.
A identificação de termos compostos obteve um bom resultado no primeiro estudo de caso, visto que o especialista selecionou 57% dos termos extraídos.
De entre as regras utilizadas, tem grande destaque a regra 9(_ Su_ AJ) que foi responsável por mais da metade dos termos compostos extraídos por a ferramenta e também por o maior número de termos selecionados.
Por outro lado, algumas regras não tiveram nenhum termo extraído, ou tiveram poucos termos extraídos e nenhum termo selecionado.
Outro ponto importante na identificação de termos compostos está relacionado às regras que utilizam preposição como base.
Como visto anteriormente, a preposição aqui modelada pode assumir três diferentes valores (&quot;de», &quot;de a e «de o).
Os resultados do primeiro estudo de caso nos mostraram que a preposição &quot;de «foi responsável por 59% dos termos extraídos e 77% dos termos selecionados.
Analisando os termos extraídos com uso das preposições &quot;de a e «do, é possível observar que grande parte de eles poderia ser identificada como atributos de outros termos extraídos.
Já no segundo estudo de caso o número de termos compostos extraídos foi bem maior e o número de termos selecionados por o especialista também aumentou.
De a mesma forma que o estudo de caso anterior, algumas regras continuaram sem termos extraídos ou selecionados neste estudo de caso, e a regra 9 continuou sendo a responsável por a maioria dos termos extraídos e selecionados.
A mesma observação quanto a os termos compostos identificados por regras que utilizam preposição como base foi confirmada no segundo estudo de caso.
Quanto a as relações taxonômicas identificadas no primeiro estudo de caso, foram observados resultados bons e ruins.
A identificação de relações taxonômicas com base em termos compostos, apesar de ser um método relativamente grosseiro, teve um grau de acerto de 53%.
Já os padrões adaptados de Hearst e também de Morin e Jacquemin tiveram um resultado muito abaixo de o esperado, tanto na extração de termos quanto na seleção dos termos por o especialista.
Isso pode ser devido a a característica de escrita dos textos.
Em o segundo estudo de caso, a identificação de relações taxonômicas por meio de os termos compostos resultou basicamente na mesma quantidade de termos obtidos no primeiro estudo de caso, ou seja, um maior número de termos compostos gerados não resultou num ganho significativo.
De a mesma forma, as relações identificadas através dos padrões adaptados de Hearst e também de Morin e Jacquemin tiveram um resultado muito abaixo de o esperado.
Como pôde ser visto no decorrer de o capítulo, apenas a medida Precisão (Precision) foi utilizada para análise dos resultados.
Outras medidas como Recall e Accuracy poderiam ser interessantes nesta análise, mas para sua utilização seriam necessários dados sobre acerto e erro de entre os termos não selecionados por a ferramenta.
Estes dados porém não foram produzidos por o especialista.
Apenas os termos selecionados foram validados.
Este capítulo apresenta as considerações finais quanto a o trabalho realizado, descrevendo suas principais contribuições e limitações.
Ainda, destaca rumos para futuras pesquisas na área.
Para o desenvolvimento do trabalho, inicialmente realizou- se um embasamento teórico sobre o que são ontologias, suas classificações e sua aplicação em diferentes áreas.
Este embasamento nos propiciou constatar que o tema, ontologia tem sido abordado em várias pesquisas, em diversas áreas, confirmando sua importância no contexto tecnológico atual.
Posteriormente, o estudo restringiu- se às alternativas para a construção de ontologias a partir de textos, onde um dos problemas motivadores para esta pesquisa, a falta de uma abordagem automática ou semi-automática para a construção de ontologias a partir de textos da língua portuguesa do Brasil, foi identificado.
Como descrito no trabalho, apesar de a variedade de relações entre palavras que podem ser encontradas num corpus, nossa escolha em lidar com relações taxonômicas se deve ao fato de que, de acordo com, na maioria dos casos ontologias são estruturadas como hierarquias de conceitos (taxonomias).
Frente a o exposto, nosso objetivo neste trabalho foi propor uma abordagem para semiautomatizar passos do processo de aquisição de estruturas ontológicas a partir de textos escritos na língua portuguesa do Brasil, mais especificamente as fases de extração de conceitos e relações taxonômicas.
Um protótipo de software foi desenvolvido com o objetivo principal de validar a abordagem proposta para identificação de estruturas ontológicas.
Com auxílio do protótipo foi realizado um estudo de caso sobre o domínio do Turismo.
Os resultados obtidos a partir de a aplicação da abordagem sobre o corpus do Turismo foram considerados relevantes à pesquisa, não se tendo noticia de outro trabalho que tivesse realizado essa tarefa.
No entanto, algumas considerações precisam ser feitas.
Um ponto importante a ser considerado é o fato de que textos de jornal, como os utilizados no estudo de caso, não são considerados textos especializados de um domínio, mas sim textos semi-especializados.
Assim, a utilização desse corpus pode ter gerado alguma distorção nos resultados, tanto na identificação de termos quanto na identificação de relações.
Para obtermos resultados mais precisos, será necessário um estudo de caso com corpus específico de um domínio.
Outra consideração importante diz respeito ao tamanho do corpus utilizado para o estudo de caso.
Um corpus contendo em torno de 100 mil palavras é considerado pequeno, mas relevante, para um estudo de caso como o executado neste trabalho.
Porém, a quantidade de termos e relações resultantes da aplicação da abordagem proposta num corpus maior (e específico) pode vir a ser considerável, e um novo estudo de caso nesse sentido poderá ser desenvolvido, dando continuidade à pesquisa.
Durante a realização deste trabalho, como já descrito, foram realizados dois estudos de caso, cujo resultados estão sintetizados na Tabela 7.1.
O objetivo destes estudos de caso foi analisar a eficiência da abordagem no que diz respeito a sua capacidade em identificar corretamente termos e relações taxonômicas.
Em o primeiro estudo de caso os resultados de cada etapa foram avaliados por o especialista.
Isto significa que apenas os dados selecionados como corretos numa etapa, serviram de entrada e direcionaram os resultados das etapas seguintes.
Já no segundo estudo de caso, os dados foram apresentados ao especialista somente ao final do processo, sem que termos e relações fossem excluídos entre as etapas.
Conseqüentemente, como pode ser visto na Tabela 7.1, mais termos e relações foram extraídos.
A exclusão dos dados irrelevantes por o especialista ocorreu apenas ao final do processo.
Como pôde ser visto nos resultados dos estudos de caso, a maior parte do corpus do domínio, quase 70% do total de palavras no que se refere à possibilidade de serem termos relevantes, foi desconsiderado já na primeira etapa da abordagem proposta.
Consideradas apenas as stopwords, estas já representam mais de 50% do corpus.
Logo após a eliminação das palavras não relevantes ao domínio, as palavras restantes tiveram sua freqüência no corpus do domínio comparada a sua freqüência no corpus de referência, com a utilização da medida Log--Likelihood.
Esse processo resultou na exclusão, de forma automática, de 3.635 diferentes substantivos não específicos ao domínio do Turismo, ou seja, substantivos que aparecem em maior proporção no corpus de referência.
Desta forma restaram apenas 412 candidatos a termos relevantes do domínio.
Como pode ser visto na Tabela 7.1, o especialista selecionou, entre estes, apenas 50 termos, correspondendo a 12,14% dos termos extraídos.
De estes termos compostos identificados, 154 foram selecionados por o especialista, ou seja, 54,23% dos termos extraídos.
De entre as regras utilizadas para essa identificação, se destaca a regra_ Su_ AJ (o termo é formado por um substantivo seguido de um adjetivo), sendo esta responsável por mais de 50% dos termos extraídos e também dos termos selecionados.
Em o segundo estudo de caso, como não houve exclusão de termos por o especialista na primeira etapa, o número de termos compostos selecionados foi muito superior (1245 termos compostos).
No que diz respeito às relações taxonômicas identificadas no primeiro estudo de caso, foram observados resultados bons e também resultados ruins.
Durante a identificação de relações taxonômicas a partir de os termos compostos, foram extraídas 284 relações taxonômicas, sendo que 152 destas foram selecionadas por o especialista, representando 53,52% do total extraído.
Apesar de ser este um método não muito sofisticado, a identificação de relações taxonômicas baseada em termos compostos foi a que obteve o melhor resultado de entre as regras utilizadas.
Já no segundo estudo de caso pode ser visto que o número de termos e relações extraídas foi bem maior, pois não ocorreram cortes durante o processo.
Já a identificação de relações taxonômicas, seja através dos padrões de Hearst seja por os padrões de Morin e Jacquemin, não trouxe resultados significativos nos dois estudos de caso.
Houve poucas relações identificadas, e apenas uma selecionada por o especialista, em cada estudo de caso.
Este resultado pode ser consequência da característica da linguagem empregada na escrita dos textos.
Apesar de serem relevantes, os resultados dos estudos de caso nos levam a acreditar que uma solução totalmente automatizada, não levaria a um alto grau de precisão, quando se utilizando apenas técnicas estatísticas para identificação de termos e relações taxonômicas.
A abordagem proposta, juntamente com o protótipo desenvolvido, constitui a principal contribuição deste trabalho, auxiliando na identificação de termos e relações taxonômicas, oferecendo apoio ao engenheiro de ontologia em fases importantes do processo de construção.
Em uma visão mais detalhada, as contribuições deste trabalho são:
Levantamento e análise de abordagens e técnicas para identificação de termos e relações taxonômicas e sua aplicação ao português;
Mecanismos que auxiliam a identificação de conceitos relevantes a partir de um corpus de domínio específico;
Mecanismos que auxiliam a identificação de relações taxonômicas entre os conceitos extraídos previamente;
Mecanismos que auxiliam a criação de uma estrutura ontológica a partir de os conceitos e relações identificadas previamente e;
O protótipo desenvolvido como uma contribuição prática que possibilita validar a abordagem proposta.
As principais limitações da pesquisa são mencionadas a seguir:
Utilização de um corpus que, devido a suas características, é considerado semiespecializado;
Dependência de um corpus anotado como entrada para a abordagem;
Utilização de um corpus de tamanho limitado por falta de um parser que permitisse realizar a etiquetagem de textos do domínio;
Validação dos resultados dos estudos de caso realizada por apenas um especialista do domínio;
Realização de estudos de caso para apenas um domínio.
A partir de o trabalho apresentado e das limitações colocadas acima é possível identificar novos trabalhos como continuidade da pesquisa:
Ampliar o processo de construção de modo a tornar- lo recorrente, permitindo a ampliação do número de níveis da ontologia;
Enriquecer ontologias:
Utilizar uma ontologia existente para, a partir de um corpus, adicionar novos conceitos e relações;
Utilizar um dicionário de sinônimos evitando que termos com mesmo significado sejam identificados como conceitos diferentes;
Identificar outros tipos de relações, como relações parte-de, entre outras citadas no decorrer de o trabalho;
Descobrir automaticamente novos padrões que indiquem relacionamento entre termos;
Incorporar um etiquetador e um lematizador à ferramenta, possibilitando a utilização de corpora que não estejam marcados;
Identificar restrições e propriedades para conceitos de uma ontologia;
Identificar instâncias e relacionar- las a conceitos da ontologia;
Desenvolver um estudo de caso com corpus específico de um domínio;
Executar automaticamente um número maior de tarefas.
