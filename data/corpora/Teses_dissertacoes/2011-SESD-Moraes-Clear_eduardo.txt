Com o aumento da densidade de transistores num mesmo circuito integrado, possibilitou- se o desenvolvimento de sistemas computacionais completos num único chip (Systems-on-Chip).
Atualmente, o poder computacional exigido por as aplicações freqüentemente demanda uma arquitetura SoC com mais de um processador.
Surgiram então as arquiteturas multiprocessadas, denominadas MPSoCs (Multiprocessor Systemson-Chip).
As arquiteturas MPSoCs encontradas na literatura não apresentam grande variedade de tipos de núcleo de processamento.
Apesar deste fato, a literatura no tema de MPSoCs argumenta com freqüência que MPSoCs heterogêneos (mais de um tipo de núcleo de processamento) apresentam um melhor desempenho.
Um dos problemas para desenvolver arquiteturas heterogêneas é a dificuldade de se integrar processadores em plataformas MPSoC.
Este trabalho tem por objetivo suprir a lacuna de integração de processadores pré-validados, adaptando uma plataforma de hardware e software para um novo processador.
Como ponto de partida para o trabalho utiliza- se uma plataforma MPSoC estado-da-arte homogênea.
Esta plataforma é modificada, tornando- se possível a sua prototipação, resultando na primeira contribuição desta Dissertação.
A segunda contribuição reside no desenvolvimento de um novo elemento de processamento para o mesmo MPSoC, utilizando como estudo de caso o processador MB-Lite ­ que adota a arquitetura Microblaze.
Além de o desenvolvimento deste módulo, o sistema operacional responsável por a execução multitarefa é portado para este processador, identificando- se os mecanismos dependentes de arquitetura no mesmo.
Por fim, são apresentados resultados da integração deste processador, e a avaliação do MPSoC heterogêneo com aplicações sintéticas executando tarefas em processadores distintos (Plasma e MB-Lite), validando- se assim a proposta de integração de novos processadores em arquiteturas MPSoC.
Palavras Chave: MPSoCs heterogêneos baseados em NoC, Processadores Embarcados, Prototipação em FPGAs, MPSoCs em FPGA.
Com o crescente aumento de número de transistores num único circuito integrado, tornou- se possível desenvolver sistemas completos num único chip.
Estes sistemas são chamados de SoCs (System-on-Chip).
Um SoC é um circuito integrado que implementa a maioria, senão todas, as funções de um sistema eletrônico completo.
Uma característica de um SoC é a complexidade de projeto e validação.
Hoje um SoC contém centenas de milhões de transistores.
Um SoC contém vários elementos de processamento, gerando assim um grande poder computacional num único circuito integrado.
Estes elementos de processamento (PEs, processing elements) podem ser processadores de propósito geral (GPPs), processadores dedicados (DSP e VLIW), ou blocos de hardware dedicados (IP, Intelectual Property).
A arquitetura de um SoC é feita geralmente visando uma aplicação e não um circuito integrado de propósito geral Os consumidores de sistemas embarcados exigem produtos com desempenho cada vez mais elevado, baixo consumo de energia e baixo custo.
Os projetistas devem atender a estes requisitos conflitantes de projeto, além de um time- to-- market curto.
Para suprir estes requisitos de projeto, surgiram arquiteturas com mais de um processador, chamadas de MPSoC (Multiprocessor Systems-on-Chip).
MPSoCs já deixaram de ser apenas tema de pesquisa e encontram- se implementações comerciais na área de telefonia celular, entretenimento, jogos eletrônicos, entre outros.
Entre os temas de pesquisa em MPSoCs, atualmente, pode- se citar:
Mapeamento de tarefas, migração de tarefas, modelagem no nível de transação, modelagem com atores, técnicas para estimar potência dissipada, entre outros.
MPSoCs podem ser divididos quanto a o meio de interconexão que empregam:
Barramentos ou redes intra-chip (NoCs, do inglês Networks-on-Chip).
Quando o barramento é utilizado como interconexão, este pode ser monolítico ou hierárquico (com a utilização de pontes).
O problema desta arquitetura está diretamente relacionado ao número de módulos conectados em ela.
Quanto mais módulos são conectados, maior a perda na taxa de comunicação entre eles.
Entretanto, um MPSoC possui requisitos rígidos de comunicação que podem não ser atendidos por estruturas baseadas em barramentos.
Por outro lado, NoCs utilizam uma estrutura de comunicação distribuída, com múltiplos caminhos para a transferência de dados.
O uso de NoCs permite o uso de MPSoCs para uma maior variedade de aplicações, devido a a sua escalabilidade e paralelismo na comunicação entre os módulos conectados à rede.
De entre as vantagens oferecidas por esta estrutura de comunicação, pode- se destacar:
Paralelismo de comunicação entre pares distintos de núcleos;
Compartilhamento de fios, por o fato da largura de banda ser escalável, comportando fluxos concorrentes;
Possibilita priorizar mecanismos de confiabilidade e gerência do consumo de energia;
Reusabilidade. Sob o ponto de vista do multi-processamento, um MPSoC é dito homogêneo quando os PEs que o compõem são todos iguais.
Por exemplo, um sistema composto por processadores idênticos que permitem exclusivamente a execução de tarefas de software compiladas para tal arquitetura de processador.
De outra forma, quando os PEs são de naturezas distintas, o MPSoC é dito heterogêneo.
Isso pode ocorrer, por exemplo, quando diferentes tipos de processadores (GPPs, DSPs, etc) fazem parte de um mesmo MPSoC.
Enquanto MPSoCs homogêneos tendem a simplificar a aplicação de técnicas como migração de tarefas, MPSoCs heterogêneos tendem a suportar uma variedade maior de aplicações.
Para garantir qualidade de serviço (QoS) e desempenho, um decodificador de TV digital (MPEG4), por exemplo, deve ser heterogêneo o suficiente para integrar vários processadores (RISC), núcleos de hardware dedicados (Upsampler) e memórias (SDRAM).
Cada um desses componentes possui diferentes funcionalidades, área de silício e necessidades de comunicação.
Atualmente existem produtos empregando MPSoCs, como os propostos por a Intel e por a Tilera.
O primeiro de eles é composto por 80 núcleos de processamento idênticos, enquanto o outro por até 100 núcleos também idênticos.
No que diz respeito à comunicação, ambos MPSoCs citados são baseados em NoCs com topologia malha.
Como exemplo de MPSoC comercial heterogêneo pode- se citar a arquitetura CELL, apresentada em, desenvolvido por a IBM.
Este MPSoC visa a execução de diversas aplicações, incluindo processamento científico e multimídia.
Sua primeira utilização foi para o videogame PlayStation 3.
Motivação Devido a a alta complexidade do projeto de um MPSoC e os requisitos de mercado por um time- to-- market curto, a sua construção baseia- se no reuso de PEs pré-projetados e pré-validados.
Estes PEs podem ser compostos por processadores de diversos tipos, e em alguns casos, módulos de hardware dedicados.
Um dos problemas de projeto reside na integração de processadores a um MPSoC.
Entre os problemas na integração podem- se citar:
Desenvolvimento de interfaces entre o processador e a arquitetura de comunicação;
Desenvolvimento de um sistema operacional (SO) ou µkernel (software básico para execução do SO) para execução de software no processador;
Desenvolvimento de métodos para comunicação entre os processadores (que podem ser de outros tipos em arquiteturas heterogêneas).
Autores apontam que as restrições para projeto de MPSoCs conduzem a arquiteturas heterogêneas.
Por exemplo, Nikolov Em apresentam um fluxo de projeto no nível de sistema onde estimativas de desempenho apontam que arquiteturas heterogêneas atingem um speed-up maior se comparado com arquiteturas homogêneas.
Desta forma, pode- se inferir que existe a necessidade de um método para que a integração de diferentes tipos de processadores a um MPSoC se torne mais fácil e rápida.
Como ponto de partida para este trabalho, temos a plataforma HeMPS Station (HeMPS-S).
Esta plataforma é composta por uma infra-estrutura de hardware e software capaz de gerenciar a execução de múltiplas tarefas simultaneamente.
A plataforma é um ambiente dedicado para projeto de MPSoCs homogêneos, capaz de permitir a avaliação do desempenho de aplicações embarcadas distribuídas em determinada arquitetura executando num FPGA, e também simulável no nível RTL e ISS (apenas para o processador).
Como interface para comunicação entre a plataforma e um computador hospedeiro, a HeMPS-S oferece uma interface ethernet com uma pilha TCP/ IP implementada em hardware para depuração e avaliação do desempenho.
A partir de a plataforma de referência, o presente trabalho busca definir os requisitos de hardware e software necessários à integração de novos processadores a esta plataforma.
A partir de esta especificação, utiliza- se o processador MB-Lite como estudo de caso para integração deste à plataforma HeMPS.
Além de isto, este trabalho visa dar mais flexibilidade na escolha do PE.
Objetivos Os objetivos deste trabalho compreendem objetivos estratégicos e específicos, definidos a seguir.
Domínio da tecnologia de projeto de sistemas multi-processados em chip (MPSoCs);
Domínio de diferentes arquiteturas de processadores;
Domínio do SO (µkernel) que cada um dos processadores executa;
Integração de diferentes processadores a um MPSoC.
Domínio de pelo menos duas arquiteturas de processadores distintas, para o desenvolvimento de um MPSoC heterogêneo;
Desenvolver técnicas que facilitem o porte do µkernel do MPSoC HeMPS, permitindo para outros processadores;
Definir os requisitos de hardware e software necessários à integração de novos processadores ao MPSoC HeMPS, sendo esta a principal contribuição da presente Dissertação;
Integrar o processador escolhido ao MPSoC HeMPS, de forma a obter- se um MPSoC heterogêneo.
Estrutura do Documento Este documento é organizado da seguinte forma.
O Capítulo 2 apresenta o estado da arte de arquiteturas MPSoCs, apresentando em seu final uma tabela comparativa entre as arquiteturas estudadas.
O Capítulo 3 apresenta processadores soft core de código aberto disponíveis na literatura.
Os processadores são comparados em termos de área ocupada em FPGA e freqüência de operação.
O Capítulo 4 apresenta a plataforma HeMPS-S, bem como as modificações necessárias para sua prototipação em FPGA.
O capítulo 5 apresenta a infra-estrutura de software (µkernel) utilizada no processador Plasma.
O Capítulo 6 apresenta as modificações de hardware e software propostas neste trabalho, de forma a portar o µkernel para o processador MB-Lite.
O Capítulo 7 apresenta exemplos de execução de aplicações no MPSoC heterogêneo.
Por último, o Capítulo 8 apresenta conclusões e direção para trabalhos futuros.
Em este Capítulo é apresentado o estado-da-arte em arquiteturas MPSoCs.
A o final deste é apresentada uma tabela com um comparativo entre estas.
O objetivo de apresentar o estado- da arte é mostrar ao leitor as principais arquiteturas MPSoC em desenvolvimento, os aspectos de gerenciamento de cada uma e identificar os PEs utilizados.
Tomahawk Os autores em apresentam uma plataforma heterogênea para tratamento de sinais de antena para as próximas tecnologias de telefonia celular, tais como 3 GPP LTE e WiMAX.
Esta plataforma é gerenciada por:
Dois processadores RISC Tensilica DC212GP que executam o sistema operacional;
Seis processadores SIMD DSPs ponto fixo para fazer processamento paralelo e vetorial, tais como FFTs e DCTs;
Dois processadores DSP de ponto flutuante, para processamento de fluxo de dados (stream);
Módulos de hardware para funções específicas como controle de paridade, decodificadores de entropia entre outros.
A arquitetura é apresentada na Figura 1.
Os blocos em cinza escuro representam IPs externos.
Os blocos são interconectados por duas NoCs com topologia crossbar, sendo uma mestre e outra escravo, com largura de flit igual a 32 bits e arbitragem por prioridade estática.
A NoC suporta transferências em burst de até 63 palavras.
O modelo de programação da plataforma é similar ao utilizado no processador Cell.
Segundo os autores, o escalonador implementado na plataforma, o CoreManager, por ser implementado em hardware, consegue um melhor desempenho e eficiência em energia quando comparado com o CellSS.
O programador da plataforma deve anotar as funções em linguagem C que devem executar como tarefas em cada PE (ou seja, o mapeamento é realizado de forma manual).
Além disso, devem- se definir os tipos de entradas e saídas de cada tarefa.
Fica a cargo de um script fazer as chamadas para os diferentes compiladores para cada PE e mapear o código binário em memória.
O repositório com os códigos binários das tarefas, é gerenciado por o CoreManager.
Este também é responsável por o envio da tarefa, somente quando as dependências de tarefas são resolvidas e quando existir um PE específico disponível.
Portanto, se uma tarefa T2 depende do término da execução da tarefa T1, ela só é enviada quando T1 terminar.
O CoreManager então copia as tarefas da memória global (repositório) para a memória local do PE.
Quando uma tarefa termina sua execução, a memória local do PE é copiada de volta à memória global.
Os autores apresentam em três implementações de MPSoCs, comparandoas em termos de área e dissipação de potência, usando benchmarks de aplicações de rede celular 4G.
A Figura 2 apresenta estas arquiteturas.
A arquitetura heterogênea, denominada MAGALI, é composta por dois elementos de processamento diferentes:
Um DSP e um módulo MMC.
O DSP é processador VLIW de 32 bits, otimizado para operações com números complexos, contendo ainda um módulo para operações MAC (multiplicação e acumulação).
A unidade MMC é um controlador de memória microprogramável para manipulação intensiva de dados, incluindo sincronização, bufferização, duplicação e reordenamento.
A segunda arquitetura é denominada homoGENEous Processor array (GENEPY).
Esta é composta por os módulos Smart ModEm Processors (SMEP) interconectados por uma NoC.
O SMEP é implementado numa tecnologia de 65 nm low-- power CMOS, executando a 400 MHz.
A arquitetura GENEPY v0 é composta de uma matriz de unidades SMEP v0 e um processador hospedeiro.
Cada SMEP v0 é composto por um SME (Smart Memory Engine) e um cluster de processamento composto por dois DSPs.
O SME gerência quatro buffers mapeados numa memória local de 32 KB.
O DSP é encarregado das leituras de uma FIFO de entrada.
Os valores de processamento intermediário são salvos na memória local e os resultados, escritos na FIFO de saída.
O fluxo de saída é lido na saída da FIFO por o SME.
A arquitetura GENEPY v1 é composta apenas por módulos SMEP v1.
Esta manteve os mesmos blocos de processamento e gerenciamento, mudando apenas o módulo de controle (CCC).
O bloco de controle, nas arquiteturas anteriores era um módulo de hardware responsável por:
Configurações de transferência de dados, armazenagem dos microprogramas para a comunicação de entrada e saída processamento dos dados, escalonamento da Ni e do núcleo de processamento.
Em esta arquitetura, o módulo responsável por estas é um processador MIPS 32 bits extensível, de forma que é possível a reconfiguração dinamicamente, escalonamento em tempo real e sincronizações.
Com a adição deste módulo não é mais necessário o processador hospedeiro.
A implementação homogênea apresentou desempenho melhor comparado com a plataforma MAGALI heterogênea.
A homogênea GENEPY v1 é aproximadamente 14% menor em termos de área de silício.
Para uma aplicação de telefonia 4 G, os comparativos mostraram um speed-up de 3%, com uma economia de potência de 10% e 18%.
Os autores mencionam que arquiteturas heterogêneas são soluções para os padrões de wireless.
Entretanto, argumentam que estas arquiteturas têm uma flexibilidade limitada e reiteram que abordagens homogêneas se justificam para os futuros terminais móveis de telefonia.
Zhang Este trabalho apresenta um MPSoC heterogêneo implementado num FPGA Altera.
O sistema é composto por (Figura 3):
Quatro processadores soft core Nios II;
Um processador ARM hard core, um subsistema de memória e um barramento do sistema.
O processador ARM é o controlador central do sistema e os módulos NIOS II responsável por o ambiente de processamento.
Ambos dividem o acesso a um espaço de endereçamento, que inclui uma memória principal (SDRAM) e um subsistema de comunicação.
O módulo contendo o processador ARM é composto por um processador ARM com barramento AMBA para comunicar- se diretamente com uma memória local.
O sistema operacional e as aplicações podem ser executados desta memória local.
Cada NIOS II contém uma cache de instruções e uma memória local.
Cada um destes módulos tem acesso à memória compartilhada e ao barramento principal do sistema.
Um dos módulos NIOS II contém uma UART JTAG para comunicação com o computador hospedeiro.
O sistema é interligado através de uma arquitetura de barramento hierárquico.
Esta arquitetura permite diferentes subsistemas acessarem sua camada do barramento e operar em paralelo se não existir conflito de recursos.
O sistema foi implementado no dispositivo EP2 S180.
Este dispositivo contém freqüência de relógio de 60 MHz, utiliza 13% da área de ALUTs e 39% da área de memória.
Como resultado é apresentado uma aplicação utilizando multiplicação de matrizes.
ZeBu-XXL Os autores apresentam um MPSoC que tem como alvo a prototipação de uma plataforma multi-FPGA.
A arquitetura é composta por 672 PEs, conectados através de uma NoC com topologia malha de tamanho 7x8.
Cada roteador é conectado a um cluster de PEs idênticos tratando- se, portanto de uma plataforma homogênea.
A Figura 4 apresenta a arquitetura proposta.
Cada cluster é de fato um MPSoC interligado por NoC, contendo:
12 processadores Xilinx MicroBlaze;
8 memórias geradas a partir de o software Xilinx Coregen;
4 roteadores da NoC e um módulo de comunicação entre os clusters (ICM).
Estes dois últimos IPs foram gerados a partir de a biblioteca VHDL Arteris Danube.
Cada cluster é implementado numa placa e contém os quatro roteadores interconectados por topologia malha.
Cada roteador conecta três processadores e duas memórias SRAM.
A arquitetura com 672 processadores requer uma abordagem multi-FPGA.
O ICM é responsável por conectar o sistema multi-FPGA.
Cada FPGA contem 2 ICMs que conectam os FPGAs numa topologia malha.
A emulação de todo o sistema foi feita numa plataforma chamada ZeBu-XXL, contendo 56 FPGAs.
Os autores ainda apresentam um fluxo para mapear o projeto nos diversos FPGAs.
Como crítica, hoje não existe nenhuma aplicação que necessita um número tão elevado de processadores.
Como o elemento de processamento é uma Microblaze compatível com a norma OCP, os autores afirmam que o processador pode ser substituído por outro também compatível, deixando o restante do projeto idêntico.
Ninesilica Os autores apresentam um projeto de MPSoC para aplicações de soft radio denominada Ninesilica.
A arquitetura é formada por nove PEs, compostos por processadores RISC COFFEE interconectados através de uma NoC.
O sistema multiprocessado é baseado no template Silicon Café.
Este template provê um modelo configurável na linguagem VHDL para criar um MPSoC baseado em NoC interconectando vários nodos de processamento.
Os autores utilizaram uma rede 3x3 com uma topologia star-mesh com o mestre no centro, sendo o único conectado as entradas e saídas do circuito integrado.
Cada escravo, por sua vez, é conectado diretamente ao mestre criando comunicações ponto a ponto entre cada escravo e o mestre (Figura 5).
Este trabalho apresentou resultados para implementações em ASIC e FPGA.
A implementação em ASIC utilizou uma biblioteca de 65 nm, com uma ocupação de 630K Gates com uma freqüência máxima de relógio de 200 MHz.
A implementação em FPGA teve como alvo um dispositivo Altera Stratix II, utilizando 76.780 ALUTs e 50.482 registradores, com uma freqüência máxima de 75 MHz.
Hs-Scale Os autores em apresentam um MPSoC homogêneo como um componente de um sistema heterogêneo (Figura 6).
O MPSoC é baseado numa arquitetura escalável com memória distribuída.
A arquitetura proposta é feita de um arranjo de PEs interconectados por uma NoC.
A partir de a premissa que a Hs-Scale é um componente de um sistema, alguns dos PEs são responsáveis de estabelecer a comunicação com o restante do sistema (PEs de interface).
O MPSoC é formado por um arranjo de PEs comunicando- se através de uma rede, denominados de NPU (Network Processing Unit).
Cada NPU é composto por:
Um roteador, uma interface de rede (Ni), (iii) um processador RISC e (iii) periféricos (UART, timers e controlador de interrupção).
Cada PE executa um µkernel capaz de executar múltiplas tarefas que são escalonadas através de um time-slice.
A comunicação entre os PEs é feita de forma assíncrona.
Como resultados foram apresentados resultados de duas aplicações:
Um filtro FIR e um decodificador MJPEG.
Ainda foram apresentados resultados de ocupação e potência para uma tecnologia de 0,35 µm e ocupação para um dispositivo FPGA Xilinx.
Os autores em apresentam um ambiente de hardware e software para projeto de MPSoCs baseados em NoC.
É apresentada uma ferramenta de EDA (do inglês, Eletronic Design Automation) chamada NoCWizard, para gerar modelos de hardware de NoCs a partir de uma descrição XML.
A plataforma xENoC é composta por um conjunto de elementos de comunicação (roteadores e NIs) agrupados numa biblioteca de componentes e um framework de software.
O fluxo de projeto xENoC é composto por cinco tarefas para implementar um MPSoC baseado em NoC:
Especificação do sistema:
É feita a definição das especificações em nível de sistema (HW-SW) e da composição do sistema.
Em este ponto são definidas as restrições do sistema, tais como desempenho do sistema como um todo, latência, largura de banda disponível, consumo de potência e parâmetros da NoC entre outros.
Exploração arquitetural:
O principal objetivo nesta tarefa é realizar um refinamento, a fim de escolher uma arquitetura otimizada de acordo com os requisitos do sistema.
Isto implica na escolha otimizada dos PEs (VLIW, DSP, IPs específicos) e a infraestrutura de comunicação (barramento, NoC ou ponto a ponto), preenchendo assim os requisitos iniciais.
Projeto da arquitetura e co-projeto de HW-SW:
A partir de a arquitetura obtida, o próximo passo no fluxo de projeto é a implementação através de alguma HDL, para obter- se um código RTL sintetizável ou criando um protótipo em SystemC.
Os autores dão ênfase à tarefa de integração entre HW-SW como chave para explorar as capacidades do hardware através de Apis e drivers de software.
Projeto de Software: Em esta tarefa, os projetistas de software devem otimizar as interfaces com os PEs, drivers de software e o sistema operacional.
Em o caso específico de MPSoCs baseados em NoCs, é importante estudar e definir, através de um grafo acíclico dirigido (DAG, directed acyclic graph), a paralelização do código da aplicação seqüencial para tarefas distribuídas e concorrentes.
O grafo é então implementado utilizando primitivas da API de software MPI (message passing interface).
Integração e Verificação do Sistema:
A última parte do fluxo de projeto é a integração e verificação do sistema todo.
De este modo, a arquitetura de hardware e os componentes de software devem ser verificados concomitantemente, numa plataforma física ou utilizando um ambiente de co-simulação.
O ambiente para projeto automatizado propõe uma especificação em XML para especificar a NoC.
Esta especificação descreve a comunicação das NIs, a localização e composição dos PEs, o tipo de roteador (e seu tamanho de FIFO e tipo de chaveamento) e também a toplogia.
Este arquivo XML é a entrada para a ferramenta NoCWizard.
Como estudo de caso, os autores geraram uma NoC com topologia malha com chaveamento por circuito.
Esta NoC conecta um número parametrizável de PEs.
Cada PE é composto por um processador NIOSII com um barramento AMBA conectando a interface de rede (Ni) e a memória do processador como apresentado na Figura 7.
Foram apresentados resultados de área ocupada para diversos tamanhos de NoC e diferentes topologias.
Por fim é apresentado um diagrama de seqüência que demonstra as primitivas de comunicação MPI utilizadas na aplicação portada para o MPSoC.
Os autores defendem que para a aplicação apresentada, onde os dados gerados são independentes, o overhead de comunicação na NoC é insignificante.
Desta forma, o speed-up é sempre próximo a o número de processadores incluídos no MPSoC (e.
g HeMPS O MPSoC HeMPS é composto por uma infra-estrutura de hardware e software capazes de gerenciar a execução de múltiplas tarefas simultaneamente.
A HeMPS é composta por um número parametrizável de PEs interconectados por a NoC Hermes.
A NoC utiliza os seguintes parâmetros:
Topologia malha com chaveamento de pacotes tipo wormhole;
Controle de fluxo baseado em créditos;
Algoritmo de roteamento XY;
16 bits de tamanho de flit;
Buffer para armazenamento de 16 flits.
O tamanho do flit é metade do tamanho da palavra do processador para reduzir a área do MPSoC.
O módulo Plasma-IP é o que define a interconexão entre os sub-módulos do PE (processador, memória, DMA e Ni).
Em ele podem- se observar diversos registradores no mapa de memória, conectados à entrada de um multiplexador, e este por sua vez à entrada de dados do processador.
Há também um módulo, access repository, presente apenas no Plasma-IP Mestre, o qual tem por função acessar o repositório externo de tarefas e transmitir via DMA os códigos objeto para os processadores escravos.
Para otimizar o desempenho nos PEs, a arquitetura Plasma-IP separa o processo de comunicação e computação.
Os módulos Ni e DMA são responsáveis por enviar e receber pacotes para a rede, enquanto que o processador Plasma executa o código da tarefa.
A RAM local, por ser dupla porta, permite acesso simultâneo do processador e do DMA, evitando uso de hardware extra ou outra técnica, como roubo de ciclo.
Cada processador executa um µkernel com suporte a multitarefa e comunicação entre tarefas.
O µkernel divide a memória em páginas, alocando para ele mesmo a primeira página e para as tarefas as páginas subseqüentes.
Cada Plasma-IP contém uma tabela de tarefas, com o endereço de localização de cada uma de elas.
Um escalonador round robin provê o suporte a multitarefa.
A comunicação entre tarefas ocorre através de troca de mensagens.
A troca de mensagens é feita através de um pipe global localizado no µkernel de cada processador.
Conclusão A Tabela 1 apresenta um comparativo entre as arquiteturas avaliadas.
Os trabalhos apresentados mostram que NoC é o meio de interconexão mais utilizado nos MPSoCs revisados.
Isto se deve a sua escalabilidade e paralelismo na comunicação.
Outro aspecto relevante é que a variedade de processadores co-existindo no mesmo MPSoC não é pequena.
A maior variedade observada é no trabalho proposto por, o qual contém três tipos de processadores.
Módulos de hardware dedicados são mais raros, apenas presentes quando uma função específica é necessária por questões de desempenho, como módulos para decodificação de vídeo, por exemplo.
De o ponto de vista de gerenciamento, geralmente um dos processadores (Mestre) é encarregado por o mapeamento das tarefas e gerenciamento de recursos.
Em alguns casos o mestre apresenta uma arquitetura diferente dos escravos.
O controle centralizado, utilizando um único mestre, pode se tornar um gargalo para redes maiores, visto que apenas um processador ficaria encarregado por os dados de monitoração, mapeamento e migração de tarefas.
A plataforma HeMPS contém processadores RISC com baixa ocupação de área, tamanho do MPSoC configurável, interconexão por NoC malha, e uma arquitetura de software para execução multitarefa.
Por ser uma arquitetura estado-da-arte, disponível com código aberto, a arquitetura HeMPS é utilizada como base para o presente trabalho.
Este Capítulo tem por objetivo apresentar processadores soft core disponíveis na literatura.
Como existe uma grande variedade de processadores de diversos tipos, este trabalho focou- se em processadores que apresentam algumas características específicas para um sistema embarcado.
Entre estas, podem- se citar:
Tool chain disponibilidade de ferramentas para a compilação de código. (
de preferência linguagem C):
Compilador e montador que derivem do gcc seriam ideais;
Baixo consumo de área de silício e bom desempenho;
Código aberto:
Deve ser permitido ao projetista fazer modificações ou até mesmo a inserção de módulos no processador, de forma a otimizar ou adicionar funcionalidades.
Apesar de isto, um dos objetivos deste trabalho é não exigir do projetista a modificação interna do processador.
Kranenburg e Van Leuken em avaliaram vários processadores, considerando aspectos como:
Qualidade do projeto, desempenho, documentação disponível e as funcionalidades específicas de cada.
Em este trabalho, os autores avaliaram cinco processadores:
OpenFire, OpenRISC, LEON3, AeMB, e MB-Lite.
O processador MB-Lite foi projetado e proposto por os autores, como uma implementação da arquitetura Microblaze.
Adicionalmente, esta Dissertação avaliou os processadores Plasma (modificado em), COFFEE e o Plasma com uma unidade de ponto flutuante.
A avaliação destes oito processadores é detalhada a seguir.
AeMB Este processador de 32 bits está sob desenvolvimento por a Aeste Works.
O processador utiliza uma interface Wishbone para a memória de dados e instruções.
O AeMB é composto por um pipeline de cinco estágios e os barramentos de dados e instruções são separados (organização Harvard).
A organização é baseada nos cinco estágios clássicos de um pipeline RISC.
O processador provê suporte a uma única interrupção externa.
O banco de registradores é composto por 32 registradores de 32 bits.
Este processador é uma implementação da arquitetura Microblaze, com exceção das instruções WIC, WDC (escrita na cache), IDIV, IDIVU (divisão).
Além disso, o AeMB foi desenvolvido para ser um processador de duas threads, podendo fazer uma troca de contexto num ciclo.
Por ser semi-compatível com a Microblaze, pode- se utilizar a tool chain Microblaze para compilar o software para este processador.
De acordo com Kranenburg, alguns programas não executam como esperado.
Fazendo- se algumas modificações no código, a fim de tornar- lo compatível com ANSI C resolvem- se alguns problemas, mas não todos.
Por outro lado, a síntese em hardware não apresentou maiores problemas para utilizando- se a ferramenta ISE 10.
1. A família de processadores LEON é uma implementação da arquitetura Sparc V8, a qual não é proprietária e totalmente aberta.
O penúltimo processador desta família é o processador de 32 bits LEON3.
Sua implementação é baseada na arquitetura Harvard e utiliza o barramento Amba Advanced High--performance Bus (AHB).
Uma característica da arquitetura Sparc é o uso de uma janela de registradores para aumentar o desempenho principalmente na troca de contextos de programação.
Esta característica é implementada em hardware, ficando transparente para o desenvolvedor de software.
Por outro lado, a área ocupada aumenta devido a os requisitos dos registradores.
Uma implementação padrão tem oito registradores globais e oito janelas de registradores, cada uma contendo 24 registradores.
O número de registradores visíveis é, portanto, 32, enquanto que o total de registradores é 200.
Entretanto, a implemetação feita em não obteve benefício da janela de registradores.
Um benchmark mostra que tanto a MB-Lite quanto a implementação comercial Microblaze da Xilinx tem um tempo de execução menor que o LEON3.
Uma explicação é que a tool chain aplicada não se beneficia da janela de registradores.
O projeto do processador LEON3 é modular, de tal forma que pode- se desabilitar certas partes do processador para poupar recursos.
Uma ferramenta de configuração pode ser utilizada para gerar um SoC que inclua vários periféricos.
Até oito processadores podem ser ligados no barramento AHB.
Em termos de prototipação, o projeto apresenta várias bibliotecas de hardware para implementações em FPGA e ASIC.
Os autores ainda defendem que é difícil, senão impossível de se obter um processador com poucos recursos, pois as unidades de gerência de memória, caches e os controladores do barramento AHB não podem ser desabilitados.
Apesar de estas limitações, este processador é utilizado como arquitetura tolerante a falhas, como apresentado em e para aplicações espaciais, como apresentado em.
OpenRisc 1200 O processador OpenRisc 1200 faz parte da família de processadores embarcados OpenRisc 1000.
Este contém um pipeline de cinco estágios e implementa a arquitetura OpenRisc Basic Instruction Set (Orbis).
Esta arquitetura Harvard é composta por instruções de 32 bits e pode operar com dados de 32 e 64 bits.
Muitas extensões para esta arquitetura foram implementadas para melhorar, por exemplo:
O processamento de vetores (OpenRisc Vector/ DSP eXtension ORVDX) e (ii) instruções de ponto em conta estas extensões.
Tanto a memória de dados como a de instruções utilizam o barramento Wishbone.
OpenFire A arquitetura OpenFire 0.3b é uma implementação da arquitetura MicroBlaze 7.10, incluindo instruções e palavras de 32 bits, e todas as operações aritméticas.
O processador foi projetado para pesquisa em processadores configuráveis.
Características como interrupções, exceções e registradores especiais não foram implementados, pois o objetivo era manter o processador pequeno e simples.
O pipeline é composto por três estágios, diferentemente da especificação da arquitetura Microblaze.
Operações de load e store levam mais de um ciclo para executar, de forma que o processador não se torna compatível no nível de ciclo com a Microblaze.
Este projeto ainda permite interligar vários processadores utilizando o barramento Microblaze FSL (Fast Simplex Link).
A documentação do projeto é pobre e não se pode compreender o projeto internamente ao processador.
A documentação menciona que a instrução break não funciona corretamente, e este problema não é explicado em maiores detalhes.
O processador COFFEE (Core For FrEE) é um processador de 32 bits baseado na arquitetura MIPS.
COFFEE é composto por um pipeline de seis estágios e uma organização de memória Harvard.
O conjunto de instruções foi projetado para evitar bolhas no pipeline (e.
g a instrução de divisão foi removida).
Fazem parte do processador um controlador de interrupções, uma interface para um coprocessador e o switch mode.
O switch mode é utilizado para trocar o hardware de decodificação de instruções.
Desta forma, pode- se integrar um multiplicador e acumulador (MAC) em hardware.
Este MAC será executado quando a instrução adicionada por o switch mode for executada, por exemplo.
O COFFEE é composto por dois bancos registradores, cada banco contendo de 32 registradores.
Ambos os conjuntos são acessíveis no modo privilegiado enquanto que apenas um é acessível no modo usuário.
Os resultados avaliados nesta Dissertação mostram COFFEE é o mais caro em termos de área, comparado com os processadores aqui descritos.
A explicação para este fato é que o banco de registradores não está implementado utilizando- se LUTRAMs.
Plasma O Plasma é um processador RISC de 32 bits baseado no conjunto de instruções original do processador MIPS.
Diferentemente da arquitetura original do MIPS, o Plasma tem três estágios no pipeline e uma organização de memória Von Neumann.
O Plasma suporta todas as instruções do ISA (Instruction Set Architecture) MIPS-I, exceto as instruções de load e store desalinhadas, pois estas são patenteadas.
A tool chain é composta por um montador e um compilador de linguagem C;
O mips-elf- assembler e o mips- elf-gcc-- compiler, respectivamente.
O Plasma apresenta também uma unidade de multiplicação e divisão em hardware por somas e subtrações sucessivas, respectivamente.
Por ser uma arquitetura Von Neumann, existe apenas uma interface de acesso à memória RAM.
Por este motivo existe a unidade de controle de memória (Mem_ ctrl).
A Figura 6 apresenta o diagrama de blocos do processador Plasma.
O Plasma é o núcleo do PE da plataforma HeMPS.
Para a utilização na HeMPS, foram feitas modificações na arquitetura V2.
0 do Plasma, destas podem- se destacar:
Inserção de um mecanismo de paginação interno ao processador;
Exclusão do módulo da UART, adição de registradores mapeados em memória e inserção da instrução syscall.
Note que os resultados de área e freqüência mencionados aqui são baseados na versão modificada.
Esta versão inclui uma unidade de multiplicação, diferentemente dos outros processadores apresentados, que contém apenas o núcleo do processador.
Rodolfo Apresentaram implementações de unidades de ponto flutuante com precisão simples executando como co-processador do processador Plasma V3.
5. As unidades de ponto flutuante são organizadas a partir de implementações FPU100 do opencores e FPU da ferramenta Coregen da Xilinx.
Estas unidades de ponto flutuante são co-processadores com três organizações:
Plasma-HFP100: Baseada na FPU100 e Plasma-HFPMin e Plasma-HFPMax:
Entre as três arquiteturas avaliadas, a arquitetura Plasma-HFPMax apresentou melhor relação custo benefício entre área e desempenho.
Devido a a diferença entre as freqüências de operação do processador e do co-processador, os autores propuseram uma organização GALS.
Em a realidade foram propostas duas organizações, com relações de relógio de 4 e 8 vezes, sendo o relógio da FPU mais rápido que o do processador.
Em aplicações com uso intensivo de operações de ponto flutuante, a arquitetura Plasma-HFP-8X apresentou uma melhora de velocidade de execução até 22 vezes em relação a a emulação por software.
MB-Lite O MB-Lite é uma implementação da arquitetura MicroBlaze de 32 bits, baseada na arquitetura MIPS.
Este processador contém os mesmos cinco estágios do pipeline do MIPS.
Várias modificações na organização MIPS forma efetuadas para obter uma implementação compatível com a MicroBlaze.
O processador MB-Lite implementa um controle distribuído para eliminar a necessidade de um controlador de pipeline complexo e centralizado.
Todas dependências como stall, hazards e forwards são resolvidas localmente.
Por isso, é relativamente fácil de compreender o projeto, uma vez que as dependências foram feitas de forma clara e simples para entender.
O projeto foi descrito seguindo métodos de codificação rigorosos.
Caminhos combinacionais e seqüenciais são explicitamente separados para facilitar o entendimento das dependências de temporização.
Esta abordagem resultou num projeto que é fácil de entender, depurar e manter.
Conclusão A maioria dos processadores analisados utiliza métodos ad-hoc para o desenvolvimento do hardware.
Este tipo de metodologia é difícil de manter, entender e depurar.
Por este motivo, muitos dos processadores acabaram sendo descartados para utilização como novo PE a ser incluído na HeMPS.
Outra característica apresentada é de que não se consegue extrair facilmente apenas a unidade de processamento.
Ou seja, o processador está geralmente ligado a periféricos, controladores de barramento e módulos de hardware específicos, entre outros.
Para alguns tipos de aplicações este tipo de abordagem é importante, por exemplo, quando se necessita uma interface padrão de barramento.
Entretanto o objetivo do presente trabalho é um processador simples, sem módulos de hardware para manter a ocupação de área pequena.
Devido a o código bem escrito e a documentação disponível, o processador MB-Lite foi escolhido como a arquitetura para ser incluída na implementação heterogênea da HeMPS.
A Tabela 2 resume os processadores revisados, comparando- os.
Pode- se notar que a arquitetura dos processadores apesar de variar bastante, é muito ligada à arquitetura MIPS, visto que a Microblaze é baseada nesta.
Desta forma, dos oito processadores estudados, sete apresentam uma arquitetura similar à do processador MIPS.
A exceção é o processador LEON3.
A Figura 10 apresenta os resultados da síntese dos processadores.
De entre os processadores que consomem pouca área pode- se citar os processadores aeMB, Plasma, OpenFire e MB-Lite.
O processador OpenFire foi descartado devido a o problemas de documentação e da instrução break relatados anteriormente.
O processador aeMB foi descartado devido a os glitches apresentados no barramento de endereços, e a não correta execução de alguns programas.
A Plataforma HeMPS Station, utilizada para o projeto de sistemas MPSoC, é utilizada como base para o desenvolvimento deste projeto.
A Seção 4.1 descreve a arquitetura de hardware da HeMPS-S. A prototipação da plataforma constitui a primeira contribuição desta Dissertação, descrita na Seção 4.2.
Por último, a Seção 4.3 descreve a ferramenta HeMPS-S Generator, utilizada para geração e controle da plataforma.
Descrição da Arquitetura de Hardware A plataforma HeMPS Station (HeMPS-S) é um ambiente que permite a avaliação de aplicações embarcadas distribuídas em MPSoCs.
Esta avaliação pode ser feita numa plataforma FPGA, bem como por simulação no nível RTL.
Através das ferramentas executando num computador hospedeiro, e uma interface de comunicação entre este e o MPSoC é possível acessar a HeMPS-S. Desta forma, pode- se avaliar o sistema em tempo de execução e monitorar o sistema para obtenção de dados de desempenho.
A arquitetura do ambiente de prototipação é apresentada na Figura 11.
HeMPS-S é composta por o MPSoC HeMPS, o módulo ConMe (um controlador de DDR2), o módulo ComEt (Interface de comunicação MAC Ethernet), todos, interconectados por o módulo Main Control.
A HeMPS-S é controlada por o usuário no computador hospedeiro.
O hospedeiro contém aplicações descritas em linguagem C, modeladas utilizando grafo de tarefas.
O usuário pode escolher em qual processador cada tarefa irá executar, ou pode escolher deixar para que o sistema (processador mestre) dinamicamente mapeie estas tarefas.
O computador hospedeiro compila cada tarefa, e o código binário resultante é armazenado num arquivo, denominado repositório de tarefas.
O repositório é composto por o cabeçalho e os códigos binários das tarefas.
O cabeçalho identifica onde cada tarefa será mapeada (ou se esta será mapeada dinamicamente), o tamanho do código da tarefa e em qual posição do repositório o código da tarefa inicia.
A Figura 12 apresenta a estrutura do repositório de tarefas.
A Figura 13 ilustra o processo de armazenamento do repositório na plataforma HeMPS-S. Depois da compilação, o computador hospedeiro envia o repositório de tarefas para o MPSoC.
O repositório é enviado através de pacotes UDP por a interface MAC Ethernet.
O módulo ComEt é responsável por processar estes pacotes, através de uma pilha TCP/ IP implementada em hardware.
Esta pilha implementa os protocolos ARP, IP e UDP.
O Main Control recebe então os dados do repositório e os armazena na memória DDR2 através do módulo ConMe.
Depois que todo o repositório de tarefas foi armazenado na memória, o hospedeiro inicia a execução das aplicações no MPSoC HeMPS.
O início da execução é feito por o usuário (via ferramentas no computador hospedeiro) através de um comando para o módulo Main Control, que controla o MPSoC.
A Figura 14 ilustra o processo de execução de aplicações na plataforma HeMPS-S. Com o repositório armazenado, o processador mestre do MPSoC inicia a leitura deste através do módulos Main Control e ConMe.
O mestre pode então enviar as tarefas para os escravos de acordo com o mapeamento indicado no cabeçalho do repositório.
Quando a tarefa é recebida por um escravo, esta é escalonada para ser executada.
As tarefas podem comunicar- se entre elas, no mesmo ou em diferentes processadores, e ainda enviar dados de depuração para o mestre.
O mestre envia estes dados de depuração para o hospedeiro através dos módulos Main Control e ComEt.
Em o hospedeiro, uma aplicação de controle da plataforma interage com o usuário através de interface gráfica.
Esta aplicação, denominada HeMPS-S generator, mostra ao usuário os dados de depuração, habilitando a avaliação de desempenho.
A Figura 23, ao final deste Capítulo, apresenta a HeMPS-S generator.
HeMPS-S. O módulo ConMe tem por objetivo ser um intermediário entre a HeMPS e um controlador de memória DDR2.
O ConMe é dividido em dois blocos:
Uma interface e um controlador DDR2.
O Controlador DDR2 foi gerado por a ferramenta Xilinx Coregen e é responsável por interagir fisicamente com a memória.
A interface tem por objetivo simplificar o protocolo de comunicação com o controlador e auxiliar no endereçamento dos dados para o módulo Main Control.
A DDR2 deve armazenar o repositório de tarefas.
A escrita é feita com os dados do repositório contidos nos pacotes UDP enviados por o hospedeiro.
A leitura é feita por o Plasma-IP mestre.
Tanto a leitura como a escrita são controladas por o módulo Main Control.
Este módulo é responsável por a comunicação entre a HeMPS e o computador hospedeiro.
A pilha TCP/ IP é implementada em hardware, sem a necessidade da utilização de um processador dedicado ou de uma pilha em software para processar os pacotes com a rede.
Esta estratégia é adotada para melhorar o desempenho e reduzir área ocupada.
O ComEt é composto por os módulos de transmissão e recepção de pacotes Ethernet e um núcleo MAC Ethernet.
Estes módulos se comunicam com o MAC e com o módulo Main Control.
Para a comunicação com o MAC, é implementada então, uma pilha de TCP/ IP com suporte aos protocolos UDP, IP e ARP.
O módulo de recepção remove o cabeçalho dos pacotes IP e transmite a carga útil para o Main Control.
O módulo de transmissão é responsável por inserir o cabeçalho nas mensagens de depuração do Plasma-IP mestre para gerar pacotes IP.
Estas mensagens, em sua maioria, contêm dados de depuração gerados por as tarefas que estão executando no MPSoC.
Vale salientar que estas mensagens devem ser geradas por o projetista através de software por ele desenvolvido.
Este módulo é responsável por controlar a interação entre os módulos da HeMPS-S. Entre suas funções, podem- se citar:
Processar os comandos recebidos por o ComEt;
Escrever e ler dados no ConMe;
Transmitir o repositório de tarefas para a HeMPS e transferir as mensagens de depuração da HeMPS para o ComEt.
O Main Control recebe apenas a carga útil dos pacotes enviados por o hospedeiro via ComEt.
A carga útil é composta por um comando e os dados, responsáveis por gerenciar a execução da HeMPS-S. Os comandos são detalhados na Tabela 3.
Esta Seção descreve a primeira contribuição deste trabalho, o processo de prototipação em FPGA da plataforma HeMPS-S, indicando as modificações no projeto necessárias para atender às restrições de temporização.
O dispositivo utilizado é um FPGA Xilinx Virtex 5 5vlx330 tff1738-2, escolhido devido a a grande quantidade de blocos de memória embarcada (324 blocos BRAM) e à disponibilidade de núcleos MAC Ethernet no seu interior, o qual é necessário ao módulo ComEt.
As principais ferramentas de CAD empregadas foram Xilinx o ambiente PlanAhead restrições de projeto, depuração e análise dos resultados de área e temporização.
Diferentemente do ISE, no PlanAhead pode- se sintetizar múltiplas estratégias de implementação facilmente no mesmo projeto.
De este modo, se torna possível avaliar um maior número de alternativas de projeto, acelerando- se o processo de prototipação.
O Modelsim, apesar de não utilizado para prototipação, foi utilizado para simular a plataforma.
Para implementar um número maior de PEs, foi necessário modificar a plataforma HeMPS-S original.
A arquitetura original utilizava a NoC Hermes como um módulo IP.
Esta abordagem não é prática, pois todos os roteadores estarão próximos entre si, e não necessariamente próximos aos seus devidos processadores.
Os roteadores devem estar fisicamente próximos aos processadores com os quais se comunicam, de forma a reduzir o atraso nas interconexões locais.
Por este motivo, um novo módulo foi criado, chamado Plasma_ Router.
O Plasma_ Router contem um Plasma-IP e o roteador central da NoC Hermes (RouterCC).
Esta modificação evita fios longos, o que comprometeria o desempenho do MPSoC.
O RouterCC é um dos nove diferentes tipos de roteadores existentes em qualquer instância de uma NoC Hermes com dimensões maiores que 3x3.
Este roteador contém cinco portas, uma porta local para comunicação com o processador e outras quatro para comunicação com os vizinhos (NoC malha).
A Figura 16 apresenta a ocupação de área em termos de flip-flops e LUTs de cada módulo Plasma_ Router.
Note que o roteador avaliado tem cinco portas operacionais, sendo portanto o pior caso em termos de área.
E Plasma-IP. (
b) Ocupação de LUTs para o Roteador e Plasma-IP.
A Figura 18 apresenta parte do arquivo de restrições do usuário (User Constraints File ­ UCF), detalhando as restrições aplicadas para cada sinal de relógio.
Por exemplo, a linha 1 indica que o sinal hemps_ clk_ 50 pertence ao TNM_ NET denominado SYS_ clk50.
O código da linha 2 atribui à TNM_ NET SYS_ clk50 uma restrição temporal de um sinal de relógio com período de 20 ns e ciclo de serviço de 50%.
O número de PEs no MPSoC neste trabalho é limitado por o número de blocos de memória (FPGA) disponíveis no dispositivo FPGA.
O dispositivo selecionado contém 324 BRAMs.
Cada memória RAM local do processador Plasma necessita de 16 BRAMs.
Desta forma, o dispositivo alvo pode conter até 20 processadores.
A Figura 19 apresenta o resultado de uma síntese com a ferramenta de planta baixa no ambiente PlanAhead para uma instancia 2x3 da plataforma HeMPS-S. Cada PE utiliza duas colunas de 8 BRAMs.
A localização dos módulos ComEt e ConMe é escolhida para tirar partido da proximidade de pinos no dispositivo por eles utilizados.
PlanAhead. O sinal de reset do MPSoC HeMPS é gerado por o módulo Main Control, depois de finalizada a transmissão do repositório de tarefas.
Um problema encontrado durante o processo de síntese é que a ferramenta não reconhecia o fio de reset para cada processador como um fio especial.
Isto ocasionava falha na síntese, pois o atraso nestes fios excedia a restrição temporal.
Para evitar erros de temporização, foram utilizados explicitamente componentes Xilinx do tipo buffer BUFG para ligar o sinal de reset para o MPSoC.
O módulo ComEt é composto por dois domínios de relógio.
De um lado temos o Main Control lendo/ escrevendo à 200 MHz e de outro lado o PHY lê/ escreve à 25 MHz.
Além disso, não existe nenhuma relação de fase entre os relógios, apesar de as freqüências serem múltiplas.
A solução adotada utiliza uma LUTRAM com escrita síncrona e leitura assíncrona para armazenar os dados e um sincronizador de dois flip-flops para cada sinal de controle.
A Figura 20 apresenta o módulo de fronteira para processar a transmissão dos dados UDP, provindos do computador hospedeiro.
O sinal tx_ ack sinaliza que os dados estão disponíveis para serem consumidos na LUTRAM.
O sinal é sincronizado no outro domínio de relógio e a leitura dos dados tem início.
Depois do conteúdo da LUTRAM ter sido lido por o Main Control, o sinal tx_ done_ in indica o final da leitura no domínio de relógio de 25 MHz, e o sinal de controle é sincronizado com o outro domínio.
Uma estrutura análoga é utilizada na direção oposta.
A outra fronteira de relógio se situa na interface entre o Plasma-IP mestre e o Main Control.
O mestre requisita os dados do repositório com um relógio de 50 MHz, enquanto o Main Control acessa a memória DDR2 à 200 MHz.
Entretanto estes relógios têm a mesma fase, um aspecto garantido por o DCM.
Para sincronizar esta interface um protocolo de quatro fases é utilizado.
Em o processador requisita a leitura do endereço 0x04000000 através do sinal read_ req.
O Main Control baixa então o valor de data_ valid, pois o dado lido ainda não está disponível no barramento.
Em, após o endereço ser lido na DDR2, pode- se sinalizar sua validade (sobe data_ valid).
Em o mestre sinaliza que a leitura foi feita normalmente (baixa read_ req).
A primeira leitura é mais lenta que as subseqüentes, pois o controlador DDR2 lê palavras de 256 bits, executando portanto uma leitura de 16 palavras de 8 bits simultaneamente e, bufferizando- as.
Ferramenta de Configuração e execução da HeMPS-Station O usuário pode controlar a plataforma HeMPS-S através do computador hospedeiro, utilizando a ferramenta HeMPS-S Generator, cuja janela gráfica inicial aparece na Figura a e processo de execução da plataforma HeMPS-S. De entre as suas funcionalidades, pode- se citar:
Configuração da Plataforma:
Pode- se gerar um MPSoC:
Número arbitrário de Plasma_ Routers, através dos parâmetros X e Y;
Número máximo de tarefas executando por escravo;
O tamanho da página (e conseqüentemente o tamanho da RAM);
E o nível de abstração da descrição do processador (apenas para simulação).
O usuário pode escolher entre versões ISS ou VHDL.
Estas são equivalentes, sendo que o ISS simula mais rápido e o VHDL permite observar mais dados de depuração (sinais internos).
Inserção de aplicações no MPSoC:
Em o painel esquerdo da Figura 22 podemos encontrar as aplicações e mapear- las manualmente nos processadores escravos ou deixar- las no mestre para que este as mapeie.
Em a Figura 22, como exemplo, apresenta- se as aplicações MPEG e communication inseridas na Plataforma.
A aplicação communication tem 4 tarefas, enquanto que a MPEG contém 5 (start, ivlc, idtc, iquant e print).
Definição do mapeamento inicial de tarefas:
Note que na Figura 22 a tarefa start (tarefa inicial da MPEG) é mapeada no processador 01, e as tarefas taskA e taskB (tarefas iniciais da communication) estão mapeadas nos processadores 11 e 12, respectivamente.
As tarefas restantes, designadas para o processador mestre, serão mapeadas dinamicamente durante a execução do MPSoC.
Geração do código binário:
Através do botão Generate, as tarefas e o µkernel são compilados e o repositório é gerado com o mapeamento de cada tarefa.
Em este momento o VHDL topo do MPSoC HeMPS é gerado com os dados parametrizados de X e Y para a síntese.
Definição dos endereços de rede:
IP e MAC.
Atualmente não está implementado o protocolo DHCP, em o qual a plataforma poderia receber um número de IP dinâmicamente.
Botão Debug: Este botão gera uma janela gráfica (GUI) com os resultados de simulação, que são lidos a partir de um arquivo texto gerado por o testbench da plataforma.
Conecta o hospedeiro com o FPGA;
Preenche a DDR2 com os códigos binários presentes no repositório;
Inicia a execução da HeMPS, inicializando o MPSoC através do comando Start;
E recebe as mensagens de depuração.
Após inicializar o MPSoC, mensagens de depuração são geradas por o Plasma-IP mestre.
Estas mensagens são exibidas na tela da ferramenta, habilitando avaliação de desempenho em tempo de execução, como ilustra a Figura 23.
Conclusão O ambiente apresentado neste Capítulo, a plataforma HeMPS-S, mostrou ser um framework operacional para execução de aplicações distribuídas.
Como contribuição deste Capítulo do trabalho, pode- se citar todo o processo de prototipação, descrito na Seção 4.2.
Este ambiente é capaz de gerar um MPSoC de tamanho parametrizável que pode ser simulado no nível RTL e prototipado em hardware.
Além de isto, o framework contém uma arquitetura de software para execução multitarefa e a parte de hardware com a arquitetura MPSoC.
Um aspecto importante deste framework é que a plataforma é disponível de forma livre e com código aberto.
Por este motivo, o projetista pode utilizar a HeMPS-S como base para o desenvolvimento de novas técnicas tanto de software como de hardware.
Como se trata de plataforma prototipada em hardware, as modificações propostas por o projetista podem servir como prova de conceito.
Como extensões para a plataforma HeMPS, atualmente estão sendo realizados os seguintes trabalhos:
Generalização do PE, de forma que a execução das tarefas possa ser feita em processadores distintos, generalização esta descrita nos próximos Capítulos;
Inclusão de uma arquitetura de memória distribuída na HeMPS, com suporte à memórias cache, trabalho em andamento por o mestrando Tales Marchesan Chaves;
Controle dinâmico de freqüência dos processadores, trabalho em andamento por o mestrando Thiago Raupp da Rosa;
Inclusão de heurísticas de mapeamento dinâmico no µkernel, trabalho em andamento por o mestrando Marcelo Mandelli;
Inclusão de políticas de Qualidade de Serviço na NoC e no µkernel, trabalho em andamento por o doutorando Everton Carara.
Este Capítulo apresenta a segunda contribuição deste trabalho:
O detalhamento do µkernel da HeMPS utilizado no processador Plasma, e a identificação dos pontos onde este µkernel é dependente da arquitetura do processador.
Assume- se que as aplicações que executam no MPSoC são modeladas como um grafo de tarefas.
Em este grafo, os vértices representam as tarefas das aplicações, e as arestas representam a comunicação entre estas.
Por exemplo, o grafo da Figura 24 apresenta uma aplicação composta por quatro tarefas, onde as tarefas A e B enviam informações para a tarefa C, e esta por sua vez para a tarefa D. Para uma execução multitarefa a memória de cada um dos processadores é dividida em páginas.
Cada uma destas páginas pode conter o código de uma tarefa ou o próprio µkernel.
O tamanho de cada página pode ser parametrizado de acordo com o tamanho da memória disponível.
Em este trabalho, devido a as restrições de memória na prototipação, a memória utilizada tem tamanho de 16 kB e é dividida em quatro páginas de 4 kB.
O µkernel escravo atualmente ocupa 6 kB e por esta razão são utilizadas duas páginas para este.
As outras duas páginas ficam disponíveis para tarefas do usuário.
O µkernel que executa em cada Plasma-IP é um núcleo sistema operacional (SO) o qual tem por objetivo gerenciar e dar suporte à execução de tarefas em cada processador.
O µkernel da HeMPS é preemptivo, ou seja, cada tarefa utiliza o processador por um período pré-definido de tempo chamado timeslice.
Existem duas versões do µkernel:
Uma que executa no Plasma-IP mestre, que tem por objetivo coordenar a distribuição e gerenciamento das tarefas, e não a execução propriamente de uma tarefa de uma aplicação.
A outra versão é a que executa no Plasma-IP escravo, que tem como responsabilidade o controle da execução multitarefa e o tratamento de interrupções (tanto por software como por hardware) do processador local a ele, ou onde ele executa.
Por efetivamente executar o processamento das tarefas, apenas o µkernel do Plasma-IP escravo é avaliado quanto a a dependência da arquitetura do processador.
A Figura 25 apresenta a estrutura do µkernel e a estrutura genérica de dada tarefa, ambas apresentadas em camadas.
Estas camadas são discutidas a seguir.
MPSoC HeMPS.
A tarefa é composta por as camadas de &quot;boot», chamadas de sistema, e o próprio código da tarefa.
A camada de boot é responsável por a inicialização da área de dados enquanto as chamadas de sistema apenas executam chamadas para funções externas à página da tarefa.
O µkernel, por sua vez, é composto por as camadas de boot, drivers de comunicação a terceira camada com várias funções.
Estrutura de Dados Específicas do µkernel Para a execução e comunicação entre tarefas, o µkernel utiliza estruturas de dados específicas.
Estas estruturas são responsáveis por o controle do contexto da tarefa (Task Control Block -- TCB), armazenagem e identificação de mensagens (Pipe e Request Message), e localização das tarefas (Task Location).
Task Control Block (TCB) Esta estrutura (Figura 26) é responsável por o armazenamento do contexto referente a a execução da tarefa.
Em ela são armazenados os valores dos registradores do processador, o endereço de retorno para a tarefa (pc) após o atendimento de interrupção ou chamada de sistema, o identificador da tarefa (id), o endereço inicial da tarefa na memória (offset), e seu status.
Note que a TCB (neste exemplo para o Plasma) contém apenas 30 registradores.
Isto se deve ao fato de que o registrador 0 é a constante zero e o registrador 1 é reservado ao montador).
Um vetor de TCBs é alocado estaticamente no µkernel, sendo que cada posição do vetor corresponde a uma página que pode receber uma dada tarefa.
O campo status tem por função representar o estado atual da tarefa que está alocada numa página, podendo esta tarefa encontrar- se em diferentes estados, conforme detalhado na Tabela 4.
A tarefa requisitou uma mensagem e está esperando por a resposta Indica que a TCB está livre e uma tarefa pode ser alocada Indica que a TCB está sendo alocada A troca de contexto entre tarefas é feita através de rotinas de salvamento e recuperação dos conteúdos dos campos da TCB.
É importante salientar que no momento em que ocorre uma troca de contexto, o conteúdo dos registradores não deve ser modificado até o salvamento destes na TCB.
A rotina de salvamento de contexto armazena todos os valores dos registradores na TCB referente a a tarefa, enquanto que a recuperação de contexto carrega todos os registradores com os valores armazenados na TCB.
Estas duas rotinas são descritas em linguagem de montagem, pois devem ter controle sobre o valor de cada um dos registradores.
Se descritas em linguagem C, por exemplo, o compilador poderia modificar o conteúdo de alguns registradores ao longo de esta operação, algo indesejado na troca de contexto.
Por exemplo, quando uma tarefa A já executou durante o tempo definido no timeslice, se existir outra tarefa com status READY, esta deve ser escalonada.
Em este momento, todos os registradores da tarefa A são salvos na sua TCB e uma tarefa B pode ser escalonada para execução.
Os registradores da tarefa B são carregados da sua TCB e então a tarefa B pode iniciar sua execução.
Pipe O Pipe é uma área de memória do µkernel destinada à comunicação entre tarefas.
Em ele ficam armazenadas (até serem consumidas) as mensagens que as tarefas enviam entre si.
O Pipe é dividido em slots cuja estrutura é apresentada na Figura 27.
Cada slot responsável por armazenar as informações de cada mensagen.
O Pipe é implementado em software como um vetor com acesso aleatório.
Desta forma, problemas como bloqueio por head-of-line (FIFO) e deadlocks são evitados.
De entre os campos do PipeSlot, diversos são utilizados para identificação da mensagem.
A mensagem propriamente dita contém tamanho parametrizável em tempo de compilação de até MSG_ SIZE palavras (message), a indicação se o slot está sendo ou não utilizado (status), e a ordem da mensagem (order).
O campo order é importante para garantir a entrega de mensagens na ordem em as quais estas foram geradas.
TaskLocation, Request Message e current A estrutura TaskLocation é responsável por vincular o endereço do processador na rede ao identificador da tarefa.
Como as tarefas se comunicam através de um identificador de tarefas, fica transparente para o programador em qual processador a tarefa está alocada.
Em o modelo de comunicação utilizado na HeMPS as escritas são não bloqueantes, escrevendo- se diretamente no Pipe, enquanto que as leituras são bloqueantes.
As leituras geram um pacote de request_ message, e o processador destino possuindo em seu Pipe a mensagem, esta é enviada à origem.
Caso contrário, estes pedidos de mensagens são armazenados na estrutura RequestMessage.
Em o momento em que há uma escrita no Pipe, a estrutura RequestMessage é consultada, afim de se realizar, se possível, o envio da mensagem.
Além de estas, um ponteiro na área de dados global, denominado current, é responsável por indexar a TCB da tarefa em execução.
Através deste ponteiro é feito o salvamento e recuperação de contexto dos registradores nas TCBs e o escalonamento de tarefas.
Chamadas de Sistema Para prover uma infra-estrutura de comunicação entre as tarefas e depuração de código, utilizam- se chamadas de sistema (system calls), as quais são de fato interrupções causam chamadas de sistema ­ WRITEPIPE e READPIPE, respectivamente.
Para depuração, comunicação com o mundo externo.
A Tabela 5 apresenta as chamadas de sistema implementadas no µkernel do Plasma-IP escravo.
Indica que uma tarefa terminou sua execução.
Não deve mais ser escalonada para execução e espera a leitura de todas as mensagens de ela no Pipe, se houver.
Escreve uma mensagem no Pipe, ou se esta já foi requisitada por a tarefa consumida a envia para tarefa destino.
Procura por uma mensagem no Pipe, se esta não existir, envia uma requisição à tarefa com a mensagem de origem e bloqueia a tarefa enquanto a mensagem não for recebida.
Retorna o valor de um contador global de ciclos de relógio, habilitando medidas de tempo de execução.
Envia uma mensagem ao mestre, que a repassa para o mundo externo.
Para a execução de chamadas de sistema no Plasma, Woszezenki adicionou uma nova instrução ao processador, syscall, que efetua uma interrupção de software (trap).
Esta instrução é parte do ISA do MIPS, mas não é nativa na versão 2.0 do Plasma, usada como base para o Plasma-IP.
Ou seja, quando a instrução é executada, ocorre um salto para uma posição de memória pré definida, onde existe uma instrução de salto para a rotina de tratamento desta interrupção de software.
Deve- se ressaltar que para a inserção desta instrução foram feitas modificações internas ao processador, o que é algo indesejado para a inclusão de novos processadores num MPSoC.
Para ilustrar o funcionamento, a Figura 28 apresenta o fluxo da execução da interrupção de software no Plasma.
Em tem- se o código de uma tarefa qualquer executando.
A primitiva de comunicação é mapeada através de uma macro numa chamada da função SystemCall.
A função SystemCall é escrita em linguagem de montagem no próprio código da tarefa na área de chamada de sistema da mesma, e executa a instrução syscall.
A syscall executa um salto para a posição 0X044, localizada na página do µkernel.
Em esta posição existe outro salto para a rotina system_ service_ routine que faz o salvamento de contexto e chama a função Syscall do µkernel, implementada em C, que efetivamente processa a requisição da chamada de sistema.
Em este exemplo, a primitiva de executar a chamada de sistema, o µkernel recupera o contexto da tarefa e esta retorna a execução.
BOOT (Primeira Camada do µkernel) As primeiras posições de memória do código executável de um Plasma na HeMPS contém programação responsável por a inicialização do apontador de pilha (SP) e do ponteiro global (GP).
Além disso, uma vez que tanto as tarefas como o µkernel são escritos em C, o código de boot é responsável por inicializar toda a seção de dados com zero para todas posições de memória.
Este procedimento é adotado para garantir que as variáveis não incializadas por o desenvolvedor de software conterão apenas zeros (o compilador também garante isso ao programador).
Este código está descrito em linguagem de montagem e é dependente da arquitetura do Plasma e da forma como é gerado o código executável do µkernel.
Depois de finalizado, o código de boot executa a chamada da função main, escrita em linguagem C, presente nas posições de memória subseqüentes.
A inicialização da área de dados, do SP e do GP é comum tanto para o µkernel, quanto para as tarefas.
Este processo é feito por uma ferramenta (convert_ bin) que automatiza o processo.
O processo de geração do código da uma tarefa e do µkernel é explicado em detalhe na Seção 5.7.
Drivers de Comunicação (Segunda Camada do µkernel) Estes drivers são responsáveis por a comunicação entre do processador, através do µkernel, e os módulos de hardware que compõe o Plasma-IP.
Os drivers de comunicação são acessados apenas no espaço de endereçamento do µkernel, evitando que as tarefas do usuário se comuniquem diretamente com módulos de hardware.
Terceira Camada do µkernel Esta camada é responsável por o tratamento das interrupções, comunicação entre as tarefas, e escalonamento de tarefas.
Dois tipos de interrupção de hardware são implementadas no Plasma-IP:
Interrupção por o módulo Ni:
Sempre que o PE recebe algum dado da rede, é gerada uma interrupção provinda do módulo Ni;
Interrupção de timeslice:
Indica que a tarefa atual executou durante o tempo pré-determinado e outra tarefa deve ser escalonada.
O tratamento de interrupções no processador Plasma ocorre da seguinte forma:
Quando o pino de interrupção é ativado, ocorre um salto para a posição de memória 0x04C, a qual contém um salto para a rotina que executa o tratamento de interrupção.
O valor do PC é armazenado no registradorra.
Desta forma depois de executado o tratamento de interrupção, sabe- se o endereço onde ocorreu a interrupção e pode- se voltar a execução neste ponto.
A Figura 29 apresenta um exemplo que ilustra o fluxo da rotina de tratamento de interrupção.
Antes de iniciar o tratamento da interrupção é feito o salvamento e no retorno a recuperação de contexto, assim como é feito nas chamadas de sistema.
Apesar de serem procedimentos com o mesmo objetivo, no µkernel do Plasma-IP foram implementados por Woszezenki dois tipos de salvamentos de contexto:
Um para chamada de sistema e outro para o tratamento de interrupções.
Apesar de semelhantes, são duas implementações distintas, onde o salvamento de contexto para chamadas de sistema é parcial (salvam- se apenas alguns registradores do total), e para interrupções é completo.
A interrupção pode ocorrer em qualquer momento da execução do código de uma tarefa.
Se interrupções não estiverem mascaradas, o Plasma interrompe a execução e salta para a posição 0x4C da memória.
Em esta posição de memória há um salto para a interrupt_ service_ routine, que é a rotina responsável por fazer o salvamento de contexto e chamar a rotina de tratamento de interrupção Os_ interrupt_ service.
Esta última é a rotina que efetivamente faz o tratamento da interrupção.
O retorno da interrupção é feito através da rotina ASM_ RunScheduledTask, que recupera o contexto da tarefa (que pode ser diferente da que gerou a interrupção, no caso de uma interrupção timeslice) e retorna.
Como mencionado anteriormente, um dos tipos de interrupção é a de timeslice.
Por a Ni.
Os pacotes que trafegam na NoC contêm no cabeçalho a espeficicação de um dos serviços, detalhados na Tabela 6.
Alocação de tarefas:
Uma tarefa deve ser transferida por o DMA 0 x00000040 para a memória do processador 0x00000050.
Aviso de que uma nova tarefa está alocada no sistema 0x00000060.
Requisição de uma tarefa 0x00000070.
Aviso de que uma tarefa terminou a sua execução e pode ser 0x00000080 liberada.
Aviso que o nodo mestre terminou a alocação inicial das tarefas FINISHED_ ALLOCATION 0x00000090 (alocação estática).
A comunicação entre tarefas envolve chamadas de sistema, tratamento de interrupção (no caso de tarefas em processadores diferentes) e a utilização do pipe por o µkernel.
A comunicação entre tarefas é baseada na troca de mensagens através das armazenada no Pipe deste processador, e o processamento da tarefa continua.
Isto bloqueante.
Se a tarefa cuja mensagem foi requisitada está localizada no mesmo processador, a tarefa executa uma leitura diretamente no Pipe.
Se a tarefa está localizada em outro processador, o µkernel envia uma requisição através da NoC (utilizando o serviço REQUEST_ MESSAGE).
O escalonador então carrega o status da tarefa com valor WAITING e esta não é mais escalonada, ou seja, bloqueia a tarefa aguardando a mensagem.
Quando a mensagem é recebida (identificada por o serviço DELIVER_ MESSAGE) por a NoC, acontece uma interrupção e o escalonador muda o status da tarefa de WAITING para READY e pode assim escalonar novamente a tarefa.
A Figura 30 ilustra este processo.
Em (a) assume- se que a tarefa t2 escreveu uma mensagem no Pipe, endereçada para a tarefa t4 (Send(&amp; msg, t4), e a tarefa t4 está requisitando a mensagem da tarefa t2 (Receive(&amp; msg, t2).
Em (b) o processador 1 envia a mensagem requisitada para o processador 2 através da NoC.
O sistema assegura o ordenamento na entrega de foram escritas.
Processor 1 Processor 2 t2 t4 Send(&amp; msg, t4);
Receive(&amp; msg, t2);
T1 t2 Pipe request_ msg (a) Receive(&amp; msg, t2);
Pipe Pipe kernel Send(&amp; msg, t4);
Identificação de Rotinas dependentes da Arquitetura do Processador Esta Seção tem por identificar as rotinas de software dependentes da arquitetura.
Esta identificação é importante, pois quando se quer portar o µkernel para outro processador, estas rotinas devem ser modificadas.
A Figura 32 apresenta a descrição da rotina de salvamento de contexto nos processadores MB-Lite (b) e Plasma (a).
Por serem descritas em linguagem de montagem, estas rotinas devem ser totalmente reescritas de acordo com a linguagem de montagem do processador alvo.
A rotina de salvamento de contexto utiliza um registrador para indexar a TCB da tarefa em execução através do ponteiro currrent.
Note que nas primeiras linhas das duas rotinas de salvamento de contexto, as instruções carregam um registrador e 27 em (b)) com o valor do ponteiro current.
Em o Plasma o registrador 27 é destinado ao Os, ou seja, não existem dados válidos da tarefa em ele, e por isso pode ser sobrescrito sem ser antes salvo.
Em o MB-Lite não existe um registrador destinado ao Os, por este motivo o valor do registrador r18 é armazenado na variável global reg na primeira linha do código.
Depois de carregado o valor de current, o salvamento de todos os registradores é feito deslocando- se apenas na estrutura da TCB, pois os registradores estão organizados numa estrutura de vetor (TCB.
Reg). Por exemplo, quando acontece o recebimento de uma mensagem previamente requisitada por uma chamada de sistema, a tarefa que requisitou esta não está sendo escalonada, pois a leitura é bloqueante.
O bloqueio acontece quando é executada a chamada de sistema, que não retorna à execução da tarefa.
Esta rotina é responsável por a inicialização de todas as estruturas de dados utilizadas no µkernel, incluindo as TCBs.
Por este motivo esta rotina também deve ser modificada para o porte do µkernel.
De entre suas ações, podem- se citar:
Inicialização do SP e GP do kernel, e armazenamento destes em variáveis globais na área do kernel;
Leitura do endereço de rede do processador (leitura no registrador Ni_ CONFIG);
Inicialização do Pipe, zerando todas suas posições;
Inicialização das estruturas TaskLocation e RequestMessage;
Inicialização das TCBs;
Esta rotina é responsável por a recuperação de contexto e salto para a execução de uma tarefa.
A rotina além de recuperar o contexto salvo previamente na TCB, deve carregar o valor do deslocamento na memória (registrador PAGE) da tarefa.
Totalmente escrita em linguagem de montagem (como mostra a Figura 34), esta rotina deve ser reescrita de acordo com a linguagem de montagem do processador alvo.
RunScheduledTask. Note- se que a recuperação de contexto é análoga ao salvamento de contexto, executando- se cargas indexadas em relação a o registrador 27 (registrador do SO que aponta para a TCB corrente).
Em a antepenúltima instrução desta rotina, o registrador PAGE é carregado através da instrução mtc0 27, 10.
Notar também a carga do valor, 1 no registrador 1, para posterior reabilitação de interrupções.
A o final da rotina ocorre o salto para onde a tarefa parou a execução antes da interrupção (através do registrador 26).
Em este momento têm- se um código no formato ELF (executable and linking format) (4).
O formato ELF identifica seções como a área de dados e a área de instruções.
Desta forma, a ferramenta convert_ bin pode extrair as instruções e dados sem os cabeçalhos e determinar onde a área de dados inicia.
É importante determinar onde a área de dados inicia, pois a convert_ bin é responsável por inicializar esta com zeros, bem como inicializar o apontador de pilha (SP) e o ponteiro global (GP).
Depois que estes foram inicializados, a ferramenta convert_ bin pode gerar um formato em hexadecimal pronto para ser carregado na memória RAM.
A ferramenta convert_ bin também deve verificar se o código da tarefa não ultrapassa o tamanho da página.
Este cálculo é feito levando em consideração uma pilha com no mínimo 128 bytes.
O objetivo deste Capítulo é tornar o processo de inclusão de um processador diferente no MPSoC HeMPS mais fácil e rápido para o projetista, de acordo com as restrições abaixo.
Para atingir este objetivo, primeiramente é preciso identificar os requisitos mínimos do processador.
Estes requisitos, neste trabalho especificamente, são pertinentes à arquitetura do MPSoC HeMPS.
Depois de definidos os requisitos, são apresentadas as modificações necessárias para o porte do µkernel para o processador MB-Lite.
O requisitos mínimos para incluir suporte a um novo processador na plataforma HeMPS-S envolvem:
Uma interface processador -- memória local (seja por organização Von Neumann ou Harvard):
Esta interface e o software associado, deve permitir mapear registradores em memória.
Estes registradores são utilizados por o µkernel para comunicação com os módulos de hardware do PE.
Um processo de tratamento de interrupções:
De o ponto de vista de hardware é necessário no mínimo um pino de interrupção no processador.
De o ponto de vista de software é necessária uma rotina em software para executar o tratamento de interrupção.
Uma cadeia de ferramentas de geração de código:
O µkernel que executa nos PEs é praticamente todo escrito em linguagem C. Algumas rotinas são descritas em linguagem de montagem.
Para tanto é necessário um compilador, um montador, e algumas ferramentas de desmontagem para análise do código gerado, para depuração.
Uma suíte GCC é o ideal.
Largura da palavra:
32 bits.
O MPSoC utilizado supõe que o processador conectado à rede tenha palavra de 32 bits.
Interface do Processador no Elemento de Processamento O objetivo é manter a arquitetura do novo PE o mais similar possível do PE da HeMPS-S. Entretanto, a organização de memória do processador interfere na organização interna do PE, como ilustra a arquitetura apresentada na Figura 37.
Em (a) tem- se uma organização para um processador com arquitetura Von Neumman (e.
g Plasma) e em (b) para organização Harvard (e.
g MB-Lite).
A diferença entre estas é o acesso à memória.
Enquanto que em (a) o processador tem apenas um barramento para acesso a RAM, em (b) tem- se dois barramentos, e um de eles deve ser compartilhado com o DMA.
A Figura que apresenta a arquitetura Harvard é de fato uma simplificação da organização real, pois o DMA pode escrever tanto na área de instruções quanto na de dados.
Estas diferenças de implementação devem ficar transparentes para o desenvolvedor de software.
O Anexo I apresenta a descrição do hardware do elemento de processamento com o processador MB-Lite.
É importante ao leitor acompanhar como os módulos Ni, DMA e RAM são interligados com o processador.
Como dito anteriormente, o principal fator que influência na organização interna do PE é a arquitetura de memória do processador, pois nesta dissertação decidiu- se não modificar os módulos Ni, DMA e RAM internamente.
As modificações de acordo com a arquitetura de memória são detalhadas no decorrer deste Capítulo.
Organização do PE Dependente da Arquitetura Esta Seção tem por objetivo apresentar a organização interna do elemento de processamento, identificando as modificações executadas para o porte do µkernel para o MB-Lite.
O µkernel da plataforma HeMPS-S deve implementar um sistema onde um processador possa executar mais de uma tarefa, ou seja, um sistema multitarefa.
Em esta plataforma, o que possibilita esta operação é o mecanismo de paginação.
Este método permite que os códigos das tarefas sejam compilados independentemente.
Durante a execução de cada tarefa, o endereço gerado por o compilador, endereço lógico, é concatenado com o conteúdo de um registrador (neste caso, o registrador PAGE), resultando num endereço físico, como ilustrado na Figura 38.
Este mecanismo habilita que uma memória seja dividida em páginas, onde cada uma destas páginas pode conter o código de uma tarefa.
Este método implica numa proteção de acesso aos dados das tarefas, pois uma tarefa não tem mecanismos para acessar áreas de outras tarefas.
O mecanismo de paginação é controlado por o µkernel (controle do registrador PAGE).
A troca entre páginas ocorre através de uma interrupção.
Por exemplo, assuma- se um processador com tarefas mapeadas nas páginas 2 e 3.
A tarefa da página 2 executa durante um período de tempo, definido por a constante timeslice, e após este período gera- se uma interrupção.
O tratador de interrupção, no µkernel, escalona a próxima tarefa, alterando o registrador PAGE para o valor 3.
A paginação no processador Plasma foi feita da seguinte forma.
A o banco de registradores foi adicionado o registrador, PAGE.
A configuração da página é realizada através da instrução mtc0 reg, 10.
Em esta instrução, o endereço inicial de uma tarefa da memória (offset) contido no registrador reg é carregado para o registrador 10 do CP0, que corresponde ao registrador PAGE.
Esta abordagem exige do projetista do hardware o conhecimento do funcionamento interno do processador e a validação destas modificações em hardware.
Para evitar que seja necessário alterar- se o processador para implementar este procedimento em outras arquiteturas, optou- se por efetuar uma abordagem no nível de software, que consiste em mapear o registrador PAGE em memória.
Sendo assim, toda vez que há a necessidade de troca de página na memória, chama- se a função:
MemoryWrite (PAGE, value).
Outro requisito para a execução do µkernel da HeMPS é a existência de rotinas para o tratamento de interrupção.
Em a parte de hardware é necessária a existência de no mínimo um pino de interrupção de E/ S no processador.
O tratamento de interrupção no MB-Lite é semelhante ao processo executado no Plasma, onde o µkernel é responsável por o tratamento de interrupções.
Como uma interrupção pode acontecer quando o processador está executando o código de uma tarefa, deve- se saltar para a página que contém o µkernel.
Por este motivo o tratamento de interrupção é bastante atrelado ao mecanismo de paginação.
Em o MB-Lite, semelhantemente ao Plasma, quando o pino de interrupção é ativado, ocorre um salto para a posição de memória 0x010, a qual contém um salto para a rotina que executa o tratamento de interrupção (Os_ interrupt_ handler).
Paralelamente a esta operação, o valor do PC é armazenado no registrador r14.
Desta forma depois de executado o tratamento de interrupção, sabe- se o endereço onde ocorreu a interrupção e pode- se voltar à execução neste ponto.
Quando o processador acessa o endereço 0x010, o hardware carregado com o valor zero, ocorrendo um salto para a faixa de endereços do µkernel atual da tarefa.
Então a rotina Os_ interrupt_ handler pode executar o tratamento da interrupção.
Em o processador MB-Lite, o retorno da rotina de interrupção é feito através da instrução rtid rA, IMM, de forma que o processador execute um salto para ao endereço de memória especificado por o conteúdo do rA somado com a constante IMM.
Por convenção utiliza- se aqui o registrador r14 como rA.
Para o retorno da interrupção à página da tarefa, o hardware do PE monitora o barramento de instruções.
Quando o opcode rtid r14, 0 é detectado, o hardware provoca o retorno à tarefa que estava executando, carregando- se o PC com o valor de r14 e CURRENT_ PAGE com o valor de O procedimento de tratamento de interrupção, juntamente com a paginação é ilustrado na Figura 41.
Em ocorre uma interrupção e o registrador CURRENT_ PAGE é carregado com zero, causando acesso à área do µkernel.
Em esta posição existe um salto para a rotina de salvamento de contexto (denominada interrupt na figura) (2).
O tratamento da interrupção é executado por a rotina Os_ interrupt_ handler de forma equivalente à implementação no Plasma.
Depois de executado o tratamento de interrupção, pode- se retornar à execução da tarefa, através da rotina ASM_ interrupt_ handler que executa a recuperação de contexto.
Como descrito na Figura 39, o registrador NEXT_ PAGE recebe o valor da página em a qual a tarefa se encontra.
Os valores dos registradores são recuperados e em, ao acessar o opcode de retorno de interrupção, o registrador CURRENT_ PAGE é carregado com o valor de NEXT_ PAGE e retorna à faixa de endereços da tarefa.
Chamadas de Sistema Como explicado no Capítulo anterior, a comunicação entre os PEs é feita através de chamadas de sistema implementadas como interrupções de software ­ traps.
Em o processador Plasma esta interrupção foi implementada adicionando- se uma instrução específica (syscall) que executa um salto para a rotina de chamada de sistema.
Esta abordagem foi descartada para o MB-Lite, pois implicaria na modificação interna do processador.
Em o processador MB-Lite, a chamada de sistema é implementada como uma nova interrupção de hardware.
Desta forma, ao executar uma chamada de sistema, a tarefa força a execução de uma interrupção.
Esta abordagem, além de não necessitar a modificação do processador, elimina a necessidade de utilização de duas rotinas de salvamento de contexto (parcial e completo), como implementado no Plasma.
Para implementação da chamada de sistema como uma interrupção, adicionou- se ao PE do MB-Lite o registrador mapeado em memória, denominado SYS_ CALL.
O bit menos significativo deste registrador é ligado ao registrador IRQ_ STATUS.
A escrita de um valor, 1 neste bit do registrador, gera uma interupção de hardware.
A Figura 42 apresenta a nova configuração do registrador IRQ_ STATUS, onde o bit 5 adicionado identifica uma interrupção de chamada de sistema.
Os bits 4 e 3 identificam interrupção de Ni e timeslice, respectivamente, já existentes na implementação do Plasma.
Como exemplo do fluxo de chamadas de sistema no MB-Lite, a Figura 43 ilustra parâmetros nos registradores e escreve o valor, 1 no registrador SYS_ CALL.
Esta escrita gera uma interrupção em hardware no processador, ocorrendo um salto para a página do µkernel.
Em ocorre o salto para a rotina em linguagem de montagem (interrupt) responsável por o salvamento de contexto.
Em este ponto, é executada a rotina tratamento de chamadas de sistema é executada.
Feito isto, pode- se recuperar o contexto da tarefa e retornar para a tarefa.
O passo 9 é equivalente aos passos 5-6 da Figura 41.
O DMA é o módulo de hardware responsável por controlar a escrita/ leitura de grandes blocos de informação da memória.
Por exemplo, quando o µkernel recebe o código de uma tarefa, este código deve ser escrito numa página da memória.
Esta operação não requer computação, apenas escritas na memória.
Desta forma, comunicação e a computação são operações separadas.
A arquitetura do Plasma-IP foi projetada utilizando memórias BRAM com porta dupla, presentes nos FPGAs da Xilinx.
Estas memórias possuem dois barramentos que podem ser usados simultaneamente para escrita ou leitura.
Note- se que a arquitetura do processador Plasma é Von Neumman, ou seja, existe apenas um barramento para a memória de instruções e de dados.
Por este motivo, o processador ocupa apenas um dos barramentos de acesso a memória RAM, disponibilizando o outro barramento para o módulo de DMA.
O processador MB-Lite, por sua vez, apresenta uma arquitetura Harvard, que supõe a existência de duas memórias:
Uma para instruções e outra para os dados.
Dado que a memória RAM utilizada é dupla porta, uma porta é conectada ao barramento de instruções e a outra porta ao barramento de dados.
Desta forma, a existência de duas memórias disjuntas é virtualizada por o uso de uma memória de dupla porta.
Uma vantagem desta abordagem é que se consegue carregar o conteúdo das instruções e dados por um único barramento, dado que qualquer um dos barramentos tem acesso a todo o espaço de memória.
Em esta abordagem, nenhuma das portas da memória RAM fica disponível unicamente para o DMA.
Por este motivo foram necessárias modificações no acesso à memória por o DMA.
Um dos barramentos da memória RAM teve seu acesso compartilhado conforme apresentado na Figura 44.
A memória de instruções é utilizada com muita freqüência, visto que a cada ciclo uma nova instrução é lida da memória.
Por este motivo, o barramento da memória de dados (porta A da Figura 44) foi selecionado para ser compartilhado com o DMA.
Esta modificação em hardware é descrita nas linhas 58 a 64 do Anexo I. DMA e por o barramento de dados do MB-Lite.
É importante salientar que a configuração do DMA é feita por o processador, mas quem efetivamente controla o acesso à memória é o DMA (através do sinal DMA_ ACTIVE).
Desta forma, deve- se garantir que quando o DMA está acessando a memória, o processador não tente executar acessos à memória de dados.
A solução encontrada foi adicionar depois das operações de start no DMA uma pausa na leitura do barramento de dados.
Esta pausa é implementada através de um laço de leitura de um registrador mapeado em memória que contém o próprio sinal DMA_ ACTIVE.
Esta abordagem tem por desvantagem uma perda de desempenho se comparado com o processador Plasma, pois o processador MB-Lite fica ocioso enquanto o DMA está operando, diferentemente do Plasma.
Entretanto, há ganho de desempenho se comparado à uma execução com acessos a memória por software.
Processo de Geração do Binário para o Processador MB-Lite O processo de geração do código binário para o processador MB-Lite é semelhante ao processo do processador Plasma.
Os dois processadores têm uma parte do código em linguagem de montagem e outra em linguagem C. A geração de código objeto para o MBLite requer apenas um arquivo com descrição em linguagem de montagem, contendo o salvamento de contexto.
O restante das rotinas em linguagem de montagem (recuperação de contexto, inicialização das TCBs, entre outras) estão contidas no código C através de pragmas.
Poderia- se- utilizar o salvamento de contexto e o salto para a rotina interrupt_ handler gerados por o compilador mb-gcc.
Porém, esta rotina realiza modificações na pilha e em registradores, causando perda no contexto da tarefa (TCB).
A solução adotada foi escrever o salvamento de contexto e o salto para a rotina de tratamento de interrupção (Os_ interrupt_ handler) num rotina escrita em linguagem de montagem, denominada interrupt e descrita no arquivo interrupt.
S. A Figura 46 apresenta o fluxo para geração do código binário, a carregar memória RAM do processador MB-Lite.
Após a montagem e a compilação dos códigos em linguagem de montagem e C, respectivamente, uma ferramenta desenvolvida durante no escopo desta Dissertação (denominada convert) modifica (4) o código binário gerado a partir de o arquivo ELF.
Esta ferramenta escreve o endereço da rotina interrupt na posição 0x10 da memória.
Desta forma, quando ocorrer uma interrupção, haverá um salto para esta rotina de salvamento de contexto em linguagem de montagem.
Diferentemente da abordagem utilizada por o Plasma, neste fluxo a área de dados não é inicializada por a ferramenta convert.
Este processo é feito em tempo de execução, por rotinas geradas automaticamente por o compilador mb-gcc.
Desta forma, quando o µkernel ou uma tarefa tem sua execução iniciada, antes do início da rotina main, o processador executa a inicialização da área de dados.
Esta inicialização é executada somente no ínicio do µkernel e para cada tarefa.
Esta abordagem tem como vantagem a não necessidade do desenvolvimento de uma ferramenta específica para inicialização de área de dados.
Como desvantagem, temos a ocupação da memória por rotinas que não executam processamento propriamente dito, além de o aumento no tempo do início da execução do µkernel/ tarefa.
Repositório de Tarefas com Códigos para Processadores Distintos O repositório de tarefas para o MPSoC heterogêneo não necessitou modificações.
Isto se deve ao fato do processador mestre enviar o código binário das tarefas independentemente da arquitetura.
Sendo assim, não há diferenciação se o código que este está enviando é relativo a um processador MB-Lite ou Plasma.
A responsabilidade de mapear as tarefas de acordo com a arquitetura do processador é do usuário.
O fluxo de geração de um repositório é feito de forma automática para arquiteturas homogêneas Plasma ou MB-Lite.
A criação de um repositório contendo códigos objetos de ambos processadores de arquiteturas heterogêneas também é feita de forma automática.
Ressalta- se que não há mapeamento automático de tarefas no presente trabalho.
O projetista deve indicar a posição e o tipo de cada processador conectado à rede, e mapear manualmente as tarefas nos processadores.
Existe também a necessidade do processador mestre ser do tipo Plasma, pois como dito anteriormente, portou- se apenas o µkernel escravo.
MPSoC HeMPS-S Heterogêneo O MPSoC heterogêneo proposto é formado por elementos de processamento cujo núcleo pode ser o processador Plasma ou o MB-Lite.
Cada um destes PEs é composto por o roteador e por o núcleo de processamento (MB-Lite-IP ou Plasma-IP).
Os módulos MB-Lite-IP e Plasma-IP são compostos por o processador (MB-Lite ou Plasma), DMA, Ni e RAM.
A geração de um MPSoC homogêneo é feita de forma automatizada.
Depois de gerado o arquivo de topo com o MPSoc, o usuário deve escolher manualmente qual o tipo de núcleo de processamento de cada PE.
A escolha é feita através de um atributo generic no código VHDL.
A hierarquia com os tipos de PE é apresentada na Figura 47.
Comparação Plasma-IP -- MB-Lite-IP Esta Seção de conclusão deste Capítulo tem por objetivo identificar as características de cada um dos processadores, bem como das implementações e comparar- los.
A Tabela 8 apresenta um comparativo entre estes.
Entre as características de cada um dos processadores, pode- se citar a unidade de multiplicação/ divisão do Plasma, que não existe no MB-Lite.
Por outro lado o MB-Lite apresenta uma arquitetura Harvard, com barramentos dedicados para a memória instruções e para a memória de dados, diferentemente do Plasma.
Por utilizar as duas interfaces da memória RAM, o processador MB-Lite compartilha o barramento de dados com o módulo de DMA.
Esta abordagem causa perda de desempenho se comparado com o Plasma, mas um ganho de desempenho se comparado com uma implementação em software.
Em termos de implementações do elemento de processamento, o Plasma apresenta modificações internas do hardware do processador, que atrapalham a portabilidade do processador.
Entre estas modificações pode- se citar a adição da instrução syscall, do registrador PAGE.
Em termos de software, o Plasma apresenta duas rotinas para salvamento de contexto, uma quando acontece uma interrupção de software e outra quando de uma chamada de sistema.
A implementação do elemento de processamento com MB-Lite por sua vez não modifica o hardware interno do processador, visto que o registrador PAGE é um registrador mapeado em memória, e a chamada de sistema foi implementada como uma interrupção de hardware.
Destaca- se como trabalhos futuros a necessidade de automatização de alguns processos, como o mapeamento dinâmico de tarefas e a geração automática do MPSoC heterogêneo.
Este Capítulo apresenta os resultados da implementação da arquitetura HeMPS-S heterogênea.
São apresentados dois cenários de execução:
O primeiro onde se apresenta uma execução multitarefa nas arquiteturas MB-Lite e Plasma, e outro com a execução num MPSoC heterogêneo com diversas organizações de heterogeneidade mostrando comunicação entre o Plasma e o MB-Lite.
Por fim, um comparativo de ocupação de área e freqüência de operação é apresentado.
Execução Multitarefa Este cenário tem por objetivo validar a execução multitarefa no processador MB-Lite, utilzando um MPSoC de tamanho 3x3.
Os processadores 22 (escravo) e 10 (mestre) têm como núcleo de processamento o processador Plasma e o restante destes o processador MB-Lite.
Uma aplicação simples, composta por duas tarefas, executa numa instância de cada tipo de processador.
Em esta aplicação a tarefa A envia uma mensagem M1 à tarefa B, que ao receber- la responde com uma mensagem M2.
Uma instância da aplicação é mapeada no processador MB-Lite, e outra instância da aplicação no processador Plasma, como ilustra a Figura 49.
Este cenário permite validar as principais funcionalidades do µkernel, como:
Chaveamento de contexto, troca de página e troca de mensagem (apenas local).
A Figura 50 apresenta o código para a tarefa A. Note que existem duas chamadas Cada tarefa executa 10 repetições de envio de 10 pacotes.
Assim, cada tarefa envia 100 pacotes, totalizando 200 pacotes trocados entre as tarefas em cada instância da aplicação.
A Figura 51 apresenta o número de ciclos de relógio entre a execução de cada envio e o recebimento de pacotes por as tarefas.
O processador MB-Lite apresenta 12% a mais no número de ciclos de relógio se comparado com a execução do Plasma.
Este número se deve aos fatores anteriormente citados, tais como:
Multiplicação por software no processador MB-Lite;
Compartilhamento do barramento de acesso à memória de dados.
Apesar deste desempenho, todas as funcionalidades o µkernel foram validadas, permitindo assim a validadação de cenários mais complexos.
Para efeitos comparativos, o binário para o Plasma contém 181 palavras, enquanto que o binário do MB-Lite contém 688 palavras.
Isto mostra o impacto das rotinas de inicialização das variáveis da área de dados no código executável do MB-Lite.
Para complexificar a execução, utilizou- se o mapeamento das tarefas de acordo com a Figura 53.
Este mapeamento propõe a divisão dos recursos com execução multitarefa, e distribuída, utilizando 4 processadores para a aplicação.
Este cenário também apresenta uma concorrência por os canais de comunicação, visto que mensagens de depuração são enviadas ao processador mestre.
Por estes motivos, o mapeamento proposto pode ser considerado ruim, de forma que esta execução valida, além de as funcionalidades da Sessão anterior, a comunicação entre os processadores.
Para demonstrar a funcionalidade do sistema, independentemente da arquitetura do processador, foram propostos 8 cenários de teste, com cada um dos processadores configurado como apresentado na Tabela 9.
Vale salientar que um mesmo binário não é equivalente aos dois processadores.
Por este motivo, as tarefas foram compiladas separadamente para os dois processadores, utilizando- se o mesmo código fonte.
Portanto, no nível de software o repositório varia entre os cenários.
Já em hardware o que varia é a configuração dos tipos de PE (MB-Lite ou Plasma).
Para mensurar as diferenças de implementação entre os PEs, o tempo de execução se deve ao fato de que esta se comunica com o maior número de tarefas neste exemplo, anterior, os resultados com o processador Plasma apresentaram um número menor de ciclos de relógio para execução da tarefa C, como apresenta a Figura 54.
A execução do cenário 4, obteve os melhores resultados, levando em média 80 mil ciclos de relógio para execução de uma rodada.
Os cenários com PEs do tipo MB-Lite (Figura 55) apresentaram mais ciclos de relógio para a execução, se comparados com o PEs do tipo Plasma.
Em o pior caso (cenário 2), a diferença foi de 10% no número de ciclos de relógio.
Este cenário, com o MPSoC homogêneo com PEs do tipo MB-Lite, apresentou em média 89 mil ciclos de relógio para execução de uma rodada.
A execução destes 8 cenários de teste permitiu avaliar de forma extensa o µkernel.
Todos os 8 cenários concluíram a execução de forma correta.
Ressalta- se que apesar de os cenários com o processador MB-Lite apresentarem resultados inferiores, o objetivo deste trabalho é o porte tanto do hardware como do software do MPSoC HeMPS para uma nova arquitetura, apenas utilizando o MB-Lite como estudo de caso.
Comparativo dos Resultados entre PE Plasma x PE MB-Lite Os resultados apresentados nesta Seção se referem à implementação do elemento de processamento do MPSoC HeMPS heterogêneo.
Este elemento de processamento é composto por o núcleo de processamento (Plasma-IP ou MB-Lite-IP, Ni e DMA).
A memória RAM por utilizar apenas módulos BRAMs foi excluída dos resultados.
Os resultados referentes ao Plasma, apresentados anteriormente, foram dispostos novamente nesta Seção para facilitar a comparação entre com o MB-Lite.
A Figura 56 apresenta os resultados de ocupação do PE contendo o MB-Lite e o Plasma.
O que se pode notar é que as duas arquiteturas ocupam praticamente a mesma área em termos de flip-flops.
Entretanto em termos de LUTs, o MB-Lite ocupa aproximadamente 50% a mais.
Por outro lado em termos de freqüência de operação o MB-Lite apresenta um ganho de 70%.
Isto se explica por o fato do MB-Lite apresentar 5 estágios de pipeline, diferentemente do Plasma que apresenta apenas 3 estágios.
Desta forma, os resultados das execuções do MB-Lite devem ser reavaliados.
Enquanto que número de ciclos de relógio das execuções para o MB-Lite são maiores em 10% na média, a freqüência de operação deste é 70% maior se comparado com o Plasma.
Assim, o tempo de execução para uma mesma tarefa executando no MB-Lite será menor se comparado com o Plasma.
Como exemplo, utilizaremos a média do número de ciclos de relógio das rodadas do cenário 4 e 2 da execução heterogênea (o melhor e o pior resultado em termos de ciclos de relógio, respectivamente).
Para cada um destes cenários, utilizaremos os valores de freqüência de operação dos processadores Plasma e MB-Lite.
De esta maneira pode- se calcular o tempo de execução da mesma tarefa para os dois processadores, como apresentado na Tabela 10.
Média de&amp; de ciclos de relógio das rodadas 79865 Tempo de Execução 0,627 ms 0,968 ms A Tabela 10 mostra que apesar de obter um número de ciclos menor para execução, o Plasma, por operar numa freqüência menor, obtém um tempo de execução maior se comparado com o MB-Lite.
O Processador MB-Lite apresenta um tempo de execução 35% menor se comparado com o Plasma.
Esta Dissertação teve dois principais objetivos:
A prototipação em FPGA da plataforma HeMPS-S e o porte da arquitetura de software e hardware do PE do MPSoC HeMPS para uma nova arquitetura de processador.
Como ponto de partida para este trabalho, temos a plataforma HeMPS-S. Esta plataforma foi modificada afim de ser prototipada em FPGA como apresentado no de forma a unir o módulo Plasma-IP e o roteador, facilitando o processo de síntese em hardware.
O processo de prototipação da plataforma HeMPS-S é a primeira contribuição desta Dissertação.
Depois da plataforma prototipada, apresentou- se a integração de um novo processador à plataforma HeMPS-S:
O processador MB-Lite.
A integração de novos processadores envolve modificações de software (porte do kernel) e de hardware (adaptação do elemento de processamento).
Estas modificações foram discutidas durante os Capítulos 5 e 6 desta dissertação.
O importante deste processo é o fato de que nenhum dos módulos do PE foram modificados (Ni, DMA, MB-Lite) e que o processo de porte do µkernel foi desenvolvido pensando- se no porte para uma arquitetura genérica de processador.
Por estes motivos, o trabalho de integração de uma nova arquitetura de processador ao MPSoC é facilitado.
Em este processo situa- se a segunda contribuição deste trabalho.
Em o final do Capítulo 6 uma comparação entre as vantagens e desvantagens de cada arquitetura é apresentada.
Modificações internas ao processador, como na implementação do Plasma-IP, foram excluídas.
Desta forma o porte para um processador genérico não requer conhecimento e validação da arquitetura interna do processador.
Plasma é 10% mais rápido que o MB-Lite em termos de número de ciclos de relógio executados por uma mesma tarefa.
Em termos de área ocupada em FPGA os dois são equivalentes.
Já em termos de freqüência de operação, o MB-Lite apresenta uma freqüência 70% maior que o Plasma.
Em um dos cenários de teste propostos, o processador MB-Lite apresentou um tempo de execução 35% menor que o Plasma, levando- se em conta o pior caso do MB-Lite comparado com o melhor caso do Plasma.
A contribuição mais relevante deste trabalho é que a plataforma heterogênea está validada e operacional.
Ou seja, os dois processadores executam o mesmo µkernel multitarefa com suporte a comunicação entre tarefas.
De forma a facilitar o porte do µkernel para novos processadores, a Tabela 11 identifica as funções do µkernel que exigiram modificações para o porte ao processador MB-Lite.
Para MB-Lite ou outro processador Harvard, adiciona- se um laço de espera no final Igual Igual Igual Igual Igual Igual Igual Diferente Plasma está em assembly, MB-Lite em C com Pragmas Diferente, com passagem de parâmetros, devido a a chamada de sistema Igual Igual Igual Diferente Trabalhos Futuros Como trabalhos futuros podem- se enumerar as seguintes atividades:
Excluir o código de inicialização gerada por o compilador do MB-Lite:
A inicialização da área de dados pode ser feita em tempo de compilação.
A parte de dados pode ser inicializada de forma semelhante àquela do processador Plasma.
Esta atividade pode ser realizada de duas formas:
Ou o desenvolvimento de uma ferramenta para exclusão das rotinas de inicialização ou a utilização de alguma ferramenta presente na tool chain Microblaze que exclua estas.
Portar o µkernel para outras arquiteturas:
De a forma como foi desenvolvido o elemento de processamento (conforme Anexo I), o processador pode ser tratado como um IP genérico.
Serão necessárias modificações nas leituras/ escritas dos registradores mapeados em memória, pois o barramento pode variar de acordo com o processador.
Automatizar a geração de repositórios heterogêneos:
Seriam necessárias modificações ao software HeMPS-S Generator de forma que a compilação, a escolha do tipo de nodo de processamento e a geração do binário fosse automatizada.
Prototipar a arquitetura HeMPS heterogênea em FPGA:
Vale salientar que tanto o processador MB-Lite quanto seu elemento de processamento estão descritos em VHDL no nível RTL, facilitando este processo.
Adicionar um método que permita o mapeamento dinâmico de tarefas no MPSoC heterogêneo.
Para isto, o mestre deve conhecer qual o tipo de processador em todos os endereços de rede e o tipo de processador em a qual a tarefa mapeada pode executar.
Uma abordagem simples seria adicionar no cabeçalho de cada tarefa do repositório um identificador com o tipo de tarefa ou o tipo de processador.
Já no MPSoC, os processadores poderiam enviar uma mensagem identificando seu tipo no início da execução do µkernel.
