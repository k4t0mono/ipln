A engenharia de requisitos trata fundamentalmente de como descobrir, analisar, documentar e verificar as funções e restrições que um software deve contemplar.
Em este processo o projetista se concentra em entender as necessidades, metas e convicções dos interessados e em como transformar- las em artefatos de software.
Isso é conhecido como ciclo de desenvolvimento e é realizado até que o software atenda todos os requisitos dos interessados.
Este trabalho descreve como os recursos do processamento da língua natural foram utilizados na construção de uma solução para recuperação semântica de documentos de caso de uso e apresenta os resultados alcançados.
Para a construção da solução, foi especificado um método que organiza os trabalhos de preparação e recuperação em duas fases.
A primeira descreve a forma como o corpus deve ser preparado e como os termos utilizados na preparação podem ser utilizados na definição das palavras-chave do domínio.
A segunda fase explica como a recuperação de documentos é realizada, e mostra como os relacionamentos descritos na ontologia são utilizados para melhorar os resultados da recuperação.
Os resultados apresentados mostram que o método descrito neste trabalho é promissor, visto que ele apresentou cobertura de 100% em ambos os testes.
Quanto a medida de precisão, que apresentou resultado inferior a 50%, o resultado foi compensado por o algoritmo de ranking que ordenou os documentos de forma similar a classificação manual feita por os usuários.
Palavras-chave: Recuperação semântica.
Informações. Engenharia de requisitos.
Conhecimento do domínio.
Processamento da língua natural.
Atualmente é vertiginoso o crescimento da quantidade de textos armazenados em formato digital.
Isso tem incentivado o aumento de pesquisas que exploram estratégias para recuperação de informações relevantes aos usuários.
Diferentes estratégias foram pesquisadas e implementadas em mecanismos de recuperação no sentido de resolver o problema da recuperação de informações:
Como disponibilizar e classificar os melhores resultados quando o usuário realizar uma busca?
A motivação em responder a essa pergunta, provém da constatação de que o usuário ao formular a expressão de busca, o faz sugerindo ao sistema de recuperação o tipo de informação de seu interesse e espera como resposta, documentos que tratem deste e de outros assuntos relacionados à sua pesquisa.
Notavelmente as consultas formuladas são ambíguas e, em alguns casos, trazem com si o uso de jargões específicos a um determinado domínio.
O uso de jargões específicos e ambigüidade de termos são uma realidade nos documentos de requisitos que utilizam a língua natural como meio para a especificação de projetos de software.
Mas quando se trata de desenvolvimento de software, normalmente trabalha- se com escopo fechado e termos que tragam ambigüidades não são desejáveis.
Para resolver este problema, recomenda- se a criação de um dicionário onde todos os termos ambíguos ou jargões são colocados com seus respectivos significados.
Em estudo anterior, disponível em, foram avaliadas as ferramentas Case IBM Requisite-Pro e Borland Caliber RM e observou- se que, mesmo com a adoção do dicionário de termos, o módulo de busca dessas ferramentas Case para o gerenciamento de requisitos de software não utiliza este dicionário para realizar as pesquisas no corpus.
Pior que isso, o referido módulo desconsidera que os termos fornecidos para a consulta, indicam a necessidade de informação desejada, e não a informação em si.
O resultado das consultas realizadas apresenta- se então como um subconjunto dos requisitos procurados.
Para que o usuário tenha um resultado satisfatório, terá que fazer tantas consultas quantos termos relacionados à consulta original existirem.
Mas como saber todos os termos presentes no domínio que se relacionam com a consulta realizada?
A resposta para essa pergunta não é trivial, pois ela trás consigo a complexidade inerente à engenharia de requisitos.
Mas pode- se começar a levantar os termos e seus relacionamentos através do modelo de domínio e do dicionário de termos.
Esses são artefatos normalmente presentes numa metodologia de desenvolvimento.
Em esta pesquisa nos concentraremos nos artefatos disponíveis no Processo Unificado.
O objetivo deste estudo é propor um método para recuperação semântica de documentos que se utiliza dos recursos do Processamento da Língua Natural (PLN) e de uma ontologia para representação do seu domínio.
Em um projeto de software típico são criados inúmeros documentos que representam os requisitos para o desenvolvimento do sistema.
Quando se utiliza o Processo Unificado (PU) como metodologia de desenvolvimento, os documentos de requisitos são conhecidos como casos de uso.
Após os casos de uso serem implementados, eles são armazenados num repositório e ficam disponíveis para os envolvidos no projeto.
Em referido estudo anterior, verificou- se que as ferramentas para gestão de projetos de software dispõem de módulos para localização de informações em documentos de seus repositórios, e estes trabalham com pesquisa direta por palavras-chave.
As buscas por palavras-chave apenas localizam e recuperam documentos que apresentam em seu conteúdo os termos especificados na consulta, muitas vezes deixando de fora documentos relevantes ou mesmo sobrecarregando o usuário com documentos não relevantes.
A busca por melhores resultados no processo de recuperação tende a agregar recursos semânticos para melhora do sistema de recuperação.
Por isso a necessidade de se modelar um ambiente computacional que recupere informações tal qual um especialista humano o faria, ou seja, um ambiente que considere os termos utilizados na pesquisa e o contexto em que esses termos se inserem.
Pesquisas atuais mostram que o uso das relações entre os termos e o uso do contexto em que a consulta está sendo realizada é fator determinante para satisfazer a necessidade de informação do usuário.
Estas pesquisas têm concentrado seus esforços na aplicação de ontologias para aquisição e uso das relações entre os termos, e descoberta do contexto.
Os resultados alcançados mostramse promissores.
Este trabalho vai ao encontro de essas pesquisas ao fazer a recuperação semântica utilizando as informações presentes numa base ontológica, no sentido de alavancar os resultados da recuperação de documentos num ambiente de desenvolvimento de software.
Oferece ainda um protótipo para preparação do corpus e outro para recuperação semântica, ambos integrados na ferramenta Case Enterprise Architect (EA).
O estudo de caso onde são realizados os testes da proposta são documentos de caso de uso que tem como domínio o departamento de pós-graduação de uma universidade.
A validação da proposta é realizada de forma empírica, por equipes formadas por analista de sistemas que desempenharão o papel de analistas desenvolvedores e de analistas de negócios deste estudo de caso.
Face a o crescente interesse em recuperação da informação e às suas aplicações em diversas áreas, optou- se por realizar uma pesquisa exploratória.
De acordo com Santos, a pesquisa exploratória é utilizada quando se deseja verificar a real importância do problema a ser pesquisado e o estágio em que se encontram as informações disponíveis a respeito de o assunto.
Fase 1: Em esta fase fez- se o levantamento bibliográfico sobre a área de recuperação de informação e dos recursos do processamento da língua natural disponíveis que nos ajudasse o obter bons resultados na implementação de um método de recuperação, foco desta pesquisa.
Buscou- se complementar o estudo inicial referente a essa áreas do conhecimento.
Após, verificou- se o funcionamento do módulo de recuperação de duas ferramentas Case utilizadas para gerência de requisitos de software, o que permitiu identificar uma oportunidade de pesquisa.
Fase 2: Em esta fase especificou- se uma arquitetura para recuperação de documentos de casos de uso.
A arquitetura é composta de um método que define como o corpus é preparado, como a recuperação é realizada e por uma ferramenta, criada como um plugin, que executa o método proposto.
Essa fase é detalhada na seção 5.
Fase 3: Em esta fase definiu- se o corpus para o estudo de caso onde os testes foram executados e o protocolo utilizado na validação da ferramenta.
Os resultados são discutidos na seção 6.
Esta dissertação está organizada em 6 capítulos.
O capítulo 2 apresenta a fundamentação conceitual que apóia o desenvolvimento desta pesquisa.
O capítulo 3 mostra os módulos existentes na solução, explicando a interação entre eles.
O capítulo 4 descreve o método proposto neste trabalho, para recuperação semântica de documentos e o desenvolvimento de um protótipo que apóia a aplicação do método.
O capítulo 5 apresenta os experimentos realizados e fornece uma análise dos resultados obtidos.
O capítulo 6 faz as considerações finais discutindo a aplicabilidade do método e apresenta sugestões de trabalhos futuros.
Por último, encontram- se as referências bibliográficas e anexos.
O modelo vetorial de recuperação propõe um ambiente em o qual é possível obter documentos que respondam parcialmente a uma expressão de busca.
Isso é feito associando- se pesos aos termos de índice e aos termos da busca, que posteriormente são utilizados para calcular o grau de similaridade entre a expressão de busca e cada um dos documentos do corpus.
Como resultado tem- se um conjunto de documentos ordenados por o grau de similaridade em relação a a expressão de busca.
Em o modelo vetorial, um documento é representado por um vetor de termos onde cada elemento representa a relevância do respectivo termo para o documento.
Cada elemento do vetor é normalizado de forma a assumir valores entre zero e um, sendo os termos, cujo peso mais se aproximar de um, os de maior importância na descrição do documento.
Analogamente, o corpus é descrito através de uma matriz onde cada linha representa um documento e cada coluna representa a presença de um determinado termo nos diversos documentos.
Assim, um corpus contendo n documentos e m termos de índice pode ser representado conforme a Tabela 1.
O modelo vetorial representa da mesma forma os documentos e as expressões de busca.
Essa característica faz com que se possa calcular o percentual de similaridade entre a expressão de busca e os documentos do corpus.
A função para cálculo de similaridade é definida como:
Equação 1 onde:
Os valores de similaridade entre a expressão de busca e cada um dos documentos do corpus são utilizados no ordenamento dos documentos resultantes.
Assim, no modelo vetorial o resultado de uma busca é um conjunto de documentos ordenados por o percentual de similaridade entre cada documento e a expressão de busca.
Esse ordenamento permite restringir o resultado a um número máximo de documentos desejados ou ainda definindo um limite mínimo para o valor da similaridade.
Desta forma o usuário pode definir que a máquina de busca recupere somente os documentos com um valor mínimo de relevância em relação a a expressão de consulta.
Baeza-Yates e Ribeiro-Neto definem como vantagens principais do modelo vetorial:
O modelo probabilístico de recuperação propõe um framework em que os problemas de recuperação são tratados com a utilização de princípios probabilísticos.
Desta forma, dada uma expressão de busca, o corpus é dividido em quatro subconjuntos distintos (Figura 2):
O conjunto de documentos relevantes (dRel), o conjunto de documentos recuperados (dRec), o conjunto dos documentos relevantes que foram recuperados (dRR) e o conjunto dos documentos não relevantes e não recuperados.
O conjunto de documentos relevantes e recuperados é o resultado da interseção dos conjuntos dRel e dRec.
O resultado de uma busca ideal é o conjunto que contenha apenas os documentos relevantes para o usuário, o conjunto dRR.
Entretanto os documentos que formam este conjunto não são previamente conhecidos.
Assim, através da formulação de uma expressão de busca, tenta- se encontrar uma descrição probabilística inicial que descreva este conjunto.
Os resultados obtidos após a execução da primeira buscam podem gradativamente ser melhorados.
Seja dRel o conjunto de documentos relevantes e¬ dRel o conjunto dos documentos não relevantes.
A probabilidade de um documento d ser relevante em relação a uma expressão de busca é dada por p (dRel| d) e a probabilidade de um documento ser considerado não relevante é representada por p(¬ dRel| d).
Já a similaridade de um documento em relação a a expressão de busca (y) é definida como:
Equação 2 Usando a função de Bayes obtém- se a seguinte expressão:
Equação 3 Onde:
Baeza-Yates e Ribeiro-Neto observam que, caso se considere p (dRel) e p(¬ dRel) iguais para todos os documentos do corpus, a fórmula de similaridade pode ser simplificada e escrita como:
Equação 4 O modelo probabilístico representa os documentos do corpus por um vetor binário, onde os valores um e zero são utilizados para representar a presença ou ausência de um determinado termo de índice, onde wn, m assumirá o valor zero para indicar a ausência de um termo, ou o valor um para indicar a presença do termo de indexação Tm no conjunto de termos do documento Doc..
A probabilidade de um termo Tm estar presente num documento selecionado do conjunto dRel é representada por p (Tm| dRel) e p(¬ Tm| dRel) é a probabilidade do termo Tm não estar presente num documento selecionado de dRel.
Desta forma, temos a função de similaridade fundamental para ordenar os resultados do modelo probabilístico:
Equação 5 Uma vez definida a função de similaridade, é necessário que o modelo seja treinado de forma a reconhecer, baseado numa pesquisa do usuário, os documentos que são relevantes e os que não são relevantes.
Como no início do processo de busca não se sabe qual o conjunto de documentos relevantes, Baeza-Yates e Ribeiro-Neto sugerem que alguns valores de referência sejam assumidos:
A) p (Tm| dRel) igual a 0.5, onde este valor é constante para todos os termos Tm;
B) assumir que a distribuição dos termos de indexação dos documentos é uniforme.
Assim, o usuário realiza a busca e todos os documentos com similaridade LimiteCorte são apresentados em ordem descendente.
Tendo esse conjunto de documentos, o usuário pode selecionar alguns documentos que considera relevantes para a sua necessidade.
O sistema então pode utilizar esta informação para tentar melhorar os resultados subseqüentes.
O usuário poderá repetir este processo de seleção de documentos relevantes até que o conjunto de documentos recuperados satisfaça a sua necessidade de informação, a esse processo denomina- se genericamente feedback de relevância.
As principais virtudes do modelo probabilístico estão em reconhecer que a atribuição de relevância é uma tarefa do usuário e o de apresentar os documentos numa ordem que é definida de forma probabilística, onde são apresentados os de maior relevância primeiro.
As suas principais desvantagens incluem:
Os sistemas de recuperação de informação realizam o processo de recuperação baseando- se em expressões de consulta que traduzem as necessidades de informações dos usuários do sistema.
Em a literatura encontra- se vários modelos de recuperação que foram desenvolvidos com o objetivo de criar um ambiente que maximize a recuperação de informações relevantes, sem com isso aumentar a complexidade na formalização da expressão de consulta.
Com desenvolvimento de novos sistemas de recuperação, por vezes, na programação destes sistemas surge a dúvida:
Qual modelo de recuperação deve ser adotado?
Em este contexto, aplicam- se metodologias de avaliação, que visam fornecer subsídios aos desenvolvedores sobre o quão bons são os resultados apresentados por cada modelo de recuperação.
Em a subseção 2.3.1 discutiremos brevemente o problema da relevância e nas subseções seguintes apresentaremos o método de julgamento de documentos recuperados comuns, técnica também conhecida como pooling, assim como outras variáveis utilizadas no processo de avaliação dos métodos de recuperação.
Gonzalez e seus co-autores afirmam que os problemas de um sistema de recuperação de informação não são menos complexos que aqueles inerentes à interpretação de significado.
Este fato permite antever as dificuldades de se avaliar os resultados da recuperação, que ainda podem se agravar devido a a noção de relevância.
Assim, para ser possível avaliar se um sistema teve sucesso em recuperar um determinado documento em resposta a uma expressão de consulta, é necessário determinar o quão relevante é esse documento em relação a a expressão de consulta.
A questão fundamental é que a relevância precede os sistemas de recuperação de informação, tendo como objeto principal o usuário.
O seu julgamento é que define o quão relevante é o resultado de um sistema de recuperação em relação a as suas necessidades de informação.
Desta forma, diversos documentos podem ser relevantes.
O desafio do sistema de recuperação é encontrar quais documentos são os mais relevantes, considerando a necessidade de informação do usuário e o contexto em que ele está inserido.
Já o desafio de se avaliar estes sistemas é o de fazer- lo sem considerar o contexto em que o usuário está inserido ao formular uma expressão de busca ­ já que cada usuário tem contextos e necessidades de informações diferentes ­ mas, ainda assim, os sistemas melhor avaliados se comportam como se considerassem esse contexto.
Estratégias para minimizar os problemas da determinação de relevância na avaliação de sistemas de recuperação de informações incluem a construção de coleções de referência e o uso do método de julgamento de documentos recuperados comuns, que são tratados nas próximas subseções.
A avaliação consiste em submeter os sistemas de recuperação de informações a um conjunto de testes, onde os procedimentos de indexação, recuperação e classificação de relevância são executados e os resultados apresentados por o sistema são comparados, levando em conta uma coleção de referência.
Uma coleção de referência é, normalmente, constituída por um conjunto de documentos, um conjunto de consultas já formuladas e a indicação dos documentos relevantes.
Para avaliar a qualidade da relevância gerada por os sistemas de recuperação, uma possibilidade está em adicionar a uma lista, em ordem de relevância, os documentos que foram recuperados, classificando- os como relevantes ou não relevantes.
Método conhecido como julgamento de documentos recuperados comuns.
Os sistemas de recuperação são julgados, manualmente, por uma equipe de avaliadores cuja área de conhecimento corresponde aos temas das consultas realizadas.
Após este julgamento, são utilizadas algumas medidas de avaliação, de entre as quais, se destacam as medidas de precisão e cobertura, necessárias para definir o quão bom é o sistema de recuperação de informações.
Estas medidas são utilizadas para avaliar o desempenho dos sistemas de recuperação em relação a a relevância dos documentos recuperados no processo de consulta e são discutidas com mais detalhes na próxima seção.
Em a tarefa de recuperação de informação, a cobertura consiste na relação entre o total de documentos corretamente recuperados por o sistema e o total de documentos corretos presentes na coleção.
Precisão consiste na relação entre a quantidade de documentos corretamente recuperados por o sistema e o número total de documentos recuperados (relevantes+ não relevantes).
Portanto, cobertura refere- se a quantidade de informações relevantes que foram corretamente recuperadas, enquanto precisão referese à confiança da informação recuperada.
Essas medidas, contudo, levam a objetivos conflitantes.
Quando se tenta aumentar a cobertura, a precisão tende a piorar e vice-versa.
Por isso, muitas vezes adota- se uma média harmônica que avalia o desempenho geral de um sistema.
As medidas de precisão e cobertura são definidas, respectivamente, na Equação 6 e Equação 7: Equação 6 Equação 7 Onde:
Combinando- se as duas medidas anteriores, encontramos a medida-F, média harmônica das anteriores, que é definida na Equação 8: Equação 8 Onde, o parâmetro é um fator que modifica a preferência da cobertura sobre a precisão.
O mais freqüente é usar $= 1, com o objetivo de se avaliar os sistemas de recuperação balanceando- se as duas medidas.
Assim, temos a Equação 9 como uma simplificação da Equação 8.
Equação 9 O PLN envolve um conjunto de técnicas computacionais para a análise de textos num ou mais níveis lingüísticos, com o propósito de simular o processamento humano da língua.
Esta abordagem surge como uma possível solução a alguns problemas relacionados à recuperação de informações, visto que tanto o corpus como as expressões de consulta formuladas por os usuários se apresentam em língua natural.
O desenvolvimento de sistemas de recuperação de informação que possam &quot;interpretar «os documentos exige um alto custo computacional.
Por esta razão, as técnicas de PLN são utilizadas para melhorar o desempenho geral do sistema, atacando problemas lingüísticos que possam interferir nos resultados da recuperação, como por exemplo:
Jurafsky e Martin propõem a abordagem do PLN em seis níveis, a saber:
Fonológico; Morfológico;
Sintático; Semântico;
Pragmático e discurso.
A) Fonológico:
É o de interpretação dos sons e fala;
O de maior interesse na implementação dos sistemas de reconhecimento da fala, onde o usuário pode expressar verbalmente um comando, ou receber resposta de forma audível.
B) Morfológico:
Em este são analisadas as variações que podem ocorrer numa palavra.
As variações são detectadas observando- se os prefixos, sufixos e radicais que compõem a palavra analisada.
Um exemplo de processamento morfológico na recuperação de informação são as técnicas de extração de radicais (stemming) que visam substituir as variantes de uma palavra por uma forma normalizada.
C) Sintático:
Em o qual é determinada a estrutura sintática das frases de um texto.
Por causa de a enorme quantidade de estruturas frasais presentes num texto, determinar precisamente a estrutura de uma frase requer um alto custo computacional, degradando a performance do sistema.
Por este motivo o processamento sintático é evitado nos modelos tradicionais de recuperação de informação.
D) Semântico:
Busca interpretar o significado de palavras individuais e também o significado de expressões ou frases.
Um exemplo do processamento neste nível é a resolução de ambigüidades, visto que muitas vezes as ambigüidades só podem ser solucionadas quando analisadas dentro de um frase ou parágrafo.
E) Pragmático:
Em este, o sistema de recuperação utiliza ontologias, dicionários ou quaisquer outros conhecimentos externos aos documentos e expressões de busca executadas anteriormente.
Este conhecimento pode ser específico a um determinado domínio ou pode versar sobre as necessidades dos usuários, como preferências e objetivos na formulação das expressões de busca.
F) Discurso:
Aqui são analisados as estruturas e os princípios organizacionais de um documento.
Entre os níveis (a) e (b), se insere o nível lexical onde é tratada individualmente a palavra.
O exemplo mais comum de processamento neste nível é a construção de lista de palavras ­ stopwords ­ de pouco valor semântico, como artigos e preposições.
Este nível está relacionado, por exemplo, com a geração e uso de vocabulários controlados ­ tesauro ou ontologias ­ na indexação de documentos e na formulação e expansão de expressões de busca.
A importância de reconhecer as variações lingüísticas dentro de um texto se dá, principalmente, por a possibilidade de controle de vocabulário, o que permite melhorar o desempenho geral do sistema, visto que a quantidade de palavras que são processadas diminuirá.
A normalização lingüística pode ser tratada em três casos distintos:
A normalização morfológica produz a redução dos itens lexicais de forma que dois ou mais termos são representados através de uma única forma.
Assim, todas as variantes de uma palavra são percebidas da mesma forma por o sistema de recuperação.
Para realizar esta normalização, as técnicas mais conhecidas são o stemming e a lematização, processo que reduz uma palavra à sua forma canônica.
Esta pesquisa utiliza o algoritmo RSLP para fazer stemming.
Em está disponível os experimentos e os resultados alcançados por este algoritmo.
A normalização sintática ocorre quando há a normalização de frases semanticamente equivalentes numa forma única e representativa das mesmas, como &quot;a casa foi pintada de azul e amarelo «e &quot;a casa foi pintada de amarelo e azul».
A normalização léxico-semântica ocorre quando são utilizados relacionamentos semânticos entre os itens lexicais de forma a criar um agrupamento de similaridades semânticas, que são identificadas por um item lexical que representa um conceito único.
Esta é a forma utilizada quando o sistema emprega um tesauro para melhorar os resultados de busca de expressões formuladas por os usuários.
A ambigüidade é a propriedade que faz com o que um termo, uma palavra ou todo um texto, possa ser interpretado de modos diferentes.
A ambigüidade pode ser do tipo sintático ou semântico.
A ambigüidade sintática ocorre quando um termo pertence a mais de uma classe gramatical, como &quot;forte», que pode ser um substantivo &quot;o forte no alto do morro «ou um adjetivo &quot;o café é forte».
Já a ambigüidade semântica ocorre quando um termo apresenta mais de um significado, por exemplo, o verbo passar, que pode significar &quot;passar a ferro», &quot;passar no vestibular «e &quot;passar no trabalho».
As ambigüidades podem ser classificadas como lexicais, quando é possível a um termo assumir múltiplos significados;
E estruturais, quando é possível mais de uma estrutura sintática para a sentença.
Jurafsky aponta que a ambigüidade lexical pode ser resolvida com abordagens cognitivas ou lingüísticas.
A primeira procura investigar como fatores semânticos, sintáticos e neuropsicológicos podem contribuir na resolução desse tipo de ambigüidade.
A abordagem lingüística considera estratégias em nível sintático e semântico.
Em nível sintático, são levadas em consideração as palavras vizinhas da palavra ambígua.
Já a abordagem semântica considera metodologias para representação do conhecimento sobre os termos, sendo necessário especificar contextos ou domínios restritos.
Nota- se que, em determinados casos, a ambigüidade sintática somente pode ser resolvida com a utilização da abordagem semântica.
Abordagens atuais procuram resolver a ambigüidade de forma semântica.
Desta forma, os termos relacionados encontrados na base ontológica são utilizados como fatores contextuais ao termo ambíguo.
As abordagens semânticas têm como principal característica o enriquecimento da expressão de consulta com informações contextuais de interesse do usuário.
O enriquecimento normalmente é realizado adicionando- se ao conjunto de termos da pesquisa, outros termos relacionados ao domínio em questão.
Normalmente o enriquecimento é utilizado com o objetivo de aumentar a medida de cobertura, mantendo a medida de precisão em padrões aceitáveis por o usuário.
Esta seção apresenta as abordagens utilizadas nesta pesquisa.
Ontologia é o ramo da filosofia que tem por objeto o estudo das propriedades mais gerais do ser.
Este termo foi adotado por a comunidade de Inteligência Artificial (Ia) para se referir aos conceitos e termos que podem ser usados para descrever alguma área do conhecimento ou construir uma representação desse conhecimento.
Ao longo de o tempo, diversas áreas do conhecimento têm emprestado da filosofia este termo quando se deseja uma estrutura que descreva alguma coisa.
Por esse motivo, é comum encontrar diversas definições para ontologia.
Breitman faz um apanhado geral das definições nas mais diversas áreas do conhecimento e faz uma discussão sobre o tema.
Conclui dizendo que «independente da definição escolhida, é necessário entender que ontologias têm sido utilizadas para descrever artefatos com variados graus de Em a literatura encontramos algumas abordagens para o desenvolvimento de ontologias.
As abordagens foram analisadas sob o ponto de vista de construção e do produto resultante.
Para construção, elas fornecem um conjunto de técnicas e atividades para o desenvolvimento de um modelo que represente o domínio modelado.
As abordagens indicam quais atividades devem ser executadas, mas não indicam a ordem em que devem ser executadas.
Como o desenvolvimento de ontologias não é um processo linear, a ordem de execução dos passos é definida por a equipe de construção, e devem ser executadas de acordo com o refinamento do modelo que se está construindo.
De o produto resultante, temos que a ontologia é formada por classes, relações e instâncias.
Classes representam os conceitos, no seu sentido mais geral.
Relações representam as associações entre os conceitos no domínio.
E as Instâncias são utilizadas para definir um elemento dentro de a ontologia.
Classes, relações e as instâncias são partes de uma estrutura ontológica, sobre qual se pode construir uma base de conhecimentos.
A ontologia fornece um conjunto de conceitos para descrever um determinado domínio, enquanto a base de conhecimento usa esses conceitos para descrever uma determinada realidade.
Caso essa realidade seja modificada, a base de conhecimentos também o é;
Porém a ontologia permanecerá inalterada desde que o domínio se mantenha inalterado.
De entre as vantagens do uso de ontologias na Ciência da Computação, destacamse:
Fornecem um vocabulário para representação do conhecimento, que tem por trás uma conceituação que o sustenta, evitando, assim, interpretações ambíguas.
Permitem o compartilhamento de conhecimento.
Sendo assim, caso exista uma ontologia que modele adequadamente certo domínio de conhecimento, essa pode ser compartilhada e usada por pessoas que desenvolvam aplicações dentro desse domínio.
Para exemplificar, considere que exista uma ontologia para o domínio de enciclopédias.
Uma vez que essa ontologia está disponível, vários sistemas podem ser desenvolvidos no sentido de recuperar informações baseados em consultas semânticas, sem a necessidade de se fazer, para cada sistema de recuperação, uma análise do domínio de enciclopédia.
Fornecem uma descrição exata do conhecimento.
Diferente da língua natural, em que as palavras podem ter semântica totalmente diferente conforme o seu contexto, a ontologia por ser escrita em linguagem formal, não deixa espaço para as ambigüidades existentes na linguagem natural.
Possibilitam fazer o mapeamento da linguagem da ontologia sem que com isso seja alterada a sua conceituação, ou seja, um mesmo conceito pode ser expresso em várias línguas.
Essas são as principais vantagens da utilização de ontologias.
Existem outras, mas todas derivadas das citadas anteriormente.
De as diversas abordagens estudadas, optamos por a abordagem proposta por Natalya Noy e Deborah McGuiness, por sua simplicidade e fácil adaptação à aplicação que esta pesquisa propõe.
Noy e McGuiness propõem que uma ontologia pode ser construída através de sete passos que são executados de forma iterativa:
O objetivo da expansão de consulta é encontrar, a partir de a análise dos termos utilizados numa consulta, novos termos relacionados que possam ser de interesse do usuário.
Uma vez descobertos, esses novos termos são agregados à consulta original e a busca é então realizada.
A literatura aponta duas abordagens principais para a expansão de consulta:
A abordagem probabilística e a ontológica.
A abordagem probabilística se utiliza do conjunto de termos mais freqüentes, encontrados nas consultas realizadas previamente, para apresentar os termos candidatos para a expansão.
Já a abordagem ontológica sugere a utilização das relações semânticas presentes na ontologia para encontrar os termos para a expansão.
O benefício da adoção da abordagem ontológica advém do fato que se pode escolher a relação semântica que é utilizada para fazer a expansão ou, ainda, um conjunto de relações que estão ligados a um termo.
Essa característica permite desenvolver sistemas que utilizem a expansão para obter resultados mais relevantes para uma consulta realizada.
Esta seção apresentou os recursos do processamento da língua natural que forneceram subsídios para a concepção desta proposta.
A área de recuperação de informações se preocupa, principalmente com a organização da informação e mecanismos que facilitem recuperar a informação solicitada.
Há vários modelos para recuperação de informações.
Discutimos o modelo vetorial e o modelo probabilístico de recuperação, mostrando que cada modelo tem suas especificidades e que elas devem ser levadas em consideração no desenvolvimento de um sistema de recuperação.
Escolhemos o modelo vetorial para o desenvolvimento dessa proposta, pois ele mantém independência entre os termos de indexação, característica que nos permitiu integrar uma ontologia para auxiliar o mecanismo de recuperação.
Uma ontologia provê mecanismos que permitem que o conhecimento sobre um domínio seja descrito.
A descrição do domínio é feita na forma sujeito-predicado- objeto, onde o predicado representa o relacionamento entre um termo e o seu significado.
Essa forma de descrição pode alavancar os resultados do sistema de recuperação quando este se utiliza desse conhecimento para recuperar informações relacionadas às necessidades do usuário.
A próxima seção apresenta o desenho da solução desenvolvida nesta pesquisa.
Apresenta também trabalhos recentes de recuperação semântica e faz uma discussão sobre a aplicabilidade da nossa proposta de recuperação semântica.
A solução proposta é dividida em duas fases que são executadas em momentos distintos.
Cada fase é constituída por componentes específicos e outros compartilhados, conforme descreve a Figura 3: Fase de recuperação de documentos:
Caso de uso, contextualização, recuperação e ranking e a resposta do sistema;
Fase de preparação da ontologia:
Configuração do ambiente e elicitação de palavras-chave;
Componentes compartilhados:
Usuário, pré-processamento, extração de termos, ontologia e corpus.
A fase de preparação da ontologia tem o objetivo de auxiliar o usuário a criar ou expandir uma ontologia que descreva o corpus.
A etapa de configuração do ambiente é onde os especialistas do projeto identificam os casos de uso com potencial de reutilização.
A etapa de pré-processamento aplica ao corpus a remoção de stopwords.
A etapa de extração de termos tem o objetivo de montar uma lista na forma &quot;radical $= conjunto de termos».
Essa etapa faz isso agrupando todos os termos extraídos por o radical dos termos.
Todos que tem o mesmo radical são agrupados juntos.
A etapa de elicitação de palavras-chave apresenta numa interface todos os termos extraídos e permite ao usuário descartar os termos que não são importantes para descrição do domínio.
A ontologia é onde o usuário faz o enriquecimento semântico do corpus, ou seja, usa a lista de termos resultante para descrever os termos e relações presentes no domínio.
A fase de recuperação de documento se inicia com a necessidade do usuário em recuperar casos de uso similares a outro caso de uso informado.
Em esse processo são aplicadas as técnicas de pré-processamento e extração de termos.
A lista de termos é então enriquecida com as relações presentes na ontologia e então é utilizada para recuperar casos de uso similares.
Após é aplicada uma função de similaridade e os casos de uso em ordem de similaridade são apresentados ao usuário.
Os atuais sistemas de recuperação de informações são baseados em pesquisa por palavra-chave, onde dada uma expressão de busca, o sistema retorna um conjunto de documentos que contenham alguns ou todos os termos presentes na expressão de busca e apresenta esses documentos ordenados por algum critério de relevância.
Bast e seus co-autores dizem que, se por um lado esses sistemas já foram considerados suficientes para resolver os problemas de recuperação, hoje em dia estão superados, pois a expectativa é que os novos sistemas sejam capazes de recuperar informações considerando também a semântica presente na expressão de busca.
Os autores apresentam um sistema que usa uma ontologia aliada a uma interface interativa.
O papel da interface é apresentar as relações semânticas presentes na ontologia que sejam relacionadas aos termos informados por o usuário e desta forma conduzir- lo a formular uma expressão de consulta que representa o seu real interesse de informação.
Hu e seus co-autores apresentam num método que utiliza o Wikipédia como fonte para desenvolvimento de uma base semântica.
A base semântica é constituída por sinonímia, hiperonímia e outras relações entre termos, que são extraídas de forma automática dos artigos do Wikipédia.
A idéia que fundamenta a pesquisa é que os relacionamentos presentes na base semântica alavancariam os resultados de um sistema de clusterização de documento.
Os resultados da pesquisa apresentam uma melhora na faixa de 16,20 a 18,80% na clusterização quando utilizadas a base semântica.
Chu-Carroll e seus co-autores desenvolvem num método de recuperação de informações que usa um corpus etiquetado para melhorar os resultados da recuperação.
O método de eles se interessa por etiquetas sobre conceitualização, restrições e outras relações entre os termos.
A etiqueta de conceitualização é utilizada como fator contextual, expandindo a consulta com termos relacionados.
A etiqueta de restrição é utilizada para direcionar a consulta a um assunto específico, e as outras relações entre os termos são apresentadas ao usuário para que este faça a escolha dos fatores contextuais, controlando assim a abrangência da expansão de termos.
Apesar de existirem diferenças entre os sistemas apresentados na subseção 3.1, tem- se como pontos principais:
Recursos de pré-processamento do corpus:
A utilização de listas de stopwords com o intuito de diminuir a quantidade de termos utilizados na indexação e uso de stemming, agrupando os termos sob um mesmo radical.
Em stemming é utilizado como estratégia para encontrar as relações semânticas similares e em é utilizado na construção dos relacionamentos da base semântica.
Indexação do corpus:
A indexação do corpus é realizada utilizando variações da lista invertida de termos, onde o peso do termo num documento é definido em relação a sua freqüência no corpus.
Ontologias como núcleo do sistema de recuperação:
As ontologias são utilizadas como base para o mecanismo de recuperação, pois disponibilizam aos sistemas relações que são utilizadas para expandir ou para se especializar os resultados da recuperação, de acordo com a preferência do usuário.
Esta pesquisa compartilha os pontos em comum apresentados, fornecendo uma metodologia que organiza os recursos do processamento da língua em duas fases:
A construção da ontologia e a recuperação dos documentos.
A metodologia é apresentada na próxima seção.
Essa seção descreve como os recursos do processamento da língua natural foram utilizados na construção de um sistema para recuperação semântica de documentos de caso de uso.
Para a construção do sistema, especificou- se um método que organiza os trabalhos de preparação e recuperação em duas fases.
A primeira descreve a forma como o corpus deve ser preparado e como os termos utilizados na preparação podem ser utilizados na definição das palavras-chave do domínio.
A segunda fase explica como a recuperação de documentos é realizada, e mostra como os relacionamentos descritos na ontologia são utilizados para melhorar os resultados da recuperação.
A etapa de configuração do ambiente está interessada na identificação de casos de uso com potencial de reutilização em diversos domínios e deve ser executada por os especialistas do projeto.
Entenda- se por potencial de reutilização, os casos de uso que elicitam comportamentos sistêmicos que podem ocorrer em diversos domínios com poucas variações em sua forma, e estejam implementados e testados.
O exemplo mais comum de caso de uso com alto potencial de reutilização são os que descrevem os cadastros, normalmente denominados de CRUD1.
Acrônimo da expressão Create, Retrieve, Update e Delete A forma geral de executar essa etapa é verificar se, ao abstrair o domínio do cenário descrito no caso de uso, o que resta pode ser reaproveitado em outras situações.
O problema dessa forma geral é a dificuldade de executar- la em projetos onde o volume de casos de uso seja muito grande, problema este que deverá ser gerenciado por a equipe para que o resultado dessa fase seja satisfatório.
Uma vez identificados os casos de uso com potencial de reutilização, os especialistas do projeto devem verificar se os componentes de software desenvolvidos para implementálos são genéricos o bastante para que possam ser reutilizados em outros domínios.
Caso não sejam, esses componentes devem ser refatorados2, a fim de tornar- los genéricos.
Uma dúvida comum nessa etapa é como identificar todos os componentes de software que implementam um determinado caso de uso.
A resposta a esse questionamento é que o projeto de software deve contar com um processo de rastreabilidade forte.
Para que o método descrito neste trabalho funcione, rastreabilidade é um requisito necessário.
Isso pode causar um impacto inicial, mas deve- se ter em mente que qualquer empresa que deseje alcançar CMMI nível 2 deverá atender a prática específica REQM3 SP 1.
4-2 ­ Manter a rastreabilidade bidirecional dos requisitos.
Essa etapa é executada dentro de o EA.
Uma vez que os especialistas do projeto tenham identificado os casos de uso desejados, estes devem ser selecionados.
A o executar o protótipo ­ mostrado no item A da Figura 5 ­ o conjunto selecionado é enviado como entrada para a etapa extração de termos e a etapa de configuração de ambiente termina.
Uma vez concluída a etapa de configuração do ambiente, passamos para a etapa de extração dos termos presentes nos casos de uso identificados, descrito com detalhes na próxima subseção.
A realização dessa etapa é feita de forma automatizada, onde a entrada para o software é um conjunto de casos de uso de um mesmo domínio, como mostrado na Figura 5.
Por ser controlada por software, sugerimos que um analista de sistema se responsabilize por essa etapa.
Descreve- se abaixo o funcionamento interno do software e as técnicas utilizadas para construir- lo.
Conceitualmente, o caso de uso é um modelo que descreve como diferentes tipos de usuários interagem com um sistema para resolver um problema.
Para tal, ele descreve as metas dos usuários, as interações entre os usuários e o sistema, bem como o comportamento necessário do sistema para satisfazer estas metas.
Estruturalmente, um caso de uso é um documento composto por seções, parágrafos e itens.
A Figura 6 mostra um documento de caso de uso mínimo (parcialmente preenchido por questões de espaço).
Em o PLN a construção da lista de termos do corpus se inicia com análise léxica, passa por a eliminação de stopwords e conclui com a normalização dos termos.
A análise léxica tem o objetivo de tratar números, hífens, símbolos, pontuação, e maiúsculas e minúsculas.
A forma geral de aplicação da análise léxica é converter todos os termos em minúsculas, eliminar hífens, números e demais símbolos e remover termos que tenham seqüência de dígitos.
Devido a as especificidades dos documentos de caso de uso, chamamos a atenção para o tratamento de números e símbolos.
Em documentos de caso de uso, é corriqueira a utilização do primeiro e/ ou segundo caractere quando se deseja referenciar um item específico de alguma seção.
Por exemplo, os termos e (onde é um seqüencial numérico) mostrados na Figura 6, se referindo as seções Regras de Negócio e Fluxos de Exceção, respectivamente.
Essas seções descrevem requisitos não funcionais.
Requisitos não funcionais são regras de domínio que devem ser satisfeita quando uma determinada operação for executada e, por esse motivo, uma mesma regra pode ser referenciada em vários casos de uso.
A análise léxica preservará termos que estejam envolvidos por colchetes.
É de utilização corriqueira também, o uso de símbolos, como a barra(/), quando se deseja relacionar especificidades relacionadas a um determinado termo.
Aproveitaremos esse estilo de escrita para extrair automaticamente relações do tipo &quot;é um «dos títulos dos casos de uso.
Essas relações serão sugeridas ao usuário quando a etapa de enriquecimento semântico da lista de termos for executada.
A Tabela 2 mostra um exemplo de extração de relações.
Stopwords são termos não significativos, como artigos e preposições, mas não limitado somente a estes.
Por exemplo, a seção descrição da Figura 6 se inicia com a frase:
Esta é uma palavra comum na seção de introdução dos casos de uso e que deve ser tratada como stopword por não agregar valor à seção de introdução.
Para fins de implementação, as stopwords podem ser tratadas como uma lista, onde os seus elementos representam termos que devem ser retirados do documento que está sendo processado, abordagem utilizada nesta pesquisa.
Removidas as stopwords, a lista de termos é obtida através de um algoritmo guloso, que utiliza os espaços em branco presentes entre os termos como delimitador.
Assim que se extrai um termo, o software verifica de qual seção aquele termo foi extraído, vincula ao termo o nome da seção e os termos da lista são normalizados (stemming).
Aplicar a técnica de remoção de stopwords é necessária já que melhora o resultado da lista de palavras, mas não garante a qualidade dessa lista, pois ainda podem aparecer termos que não tem representatividade no domínio, às vezes por serem genéricos demais ou específicos demais.
Esta etapa está interessada em melhorar a qualidade dessa lista e é realizada por o especialista no domínio.
A principal tarefa desse especialista é descartar todos os termos que, no seu entendimento, não agrega informações para representação do domínio.
Em este ponto existe uma discussão pertinente:
Existem termos que só fazem sentido no domínio quando analisados em conjunto, são os chamados sintagmas, que podem ser nominais ou verbais.
A etapa de extração de termos considera somente os termos, não extraindo sintagmas.
Por esse motivo, o especialista no domínio deve ter cuidado ao analisar os termos, pois a falta de extração de sintagmas é um desafio que o especialista no domínio terá que vencer para que o resultado desta etapa e da próxima seja satisfatório.
Para auxiliar o usuário na escolha de &quot;melhores «termos que representem o documento, os termos são normalizados e posteriormente utilizamos a medida de cálculo de freqüência inversa (TF-IDF) para calcular o peso que um determinado stem tem num documento em função de os outros documentos do conjunto.
A Figura 7 mostra no protótipo desenvolvido a visualização dos stems, dos termos agrupados, a freqüência calculada e a opção para o usuário manter ou descartar termos.
Uma vez descartado os termos, o que resta é uma lista com termos de alta representatividade no domínio.
Esses termos são usados como produto para a criação de uma ontologia, apresentada na próxima seção.
Esta etapa, a última da preparação do corpus, está interessada em enriquecer a lista de termos resultante da etapa anterior com relações semânticas.
Ela deve ser executada por os analistas de sistemas em conjunto com os especialistas do domínio.
O resultado final é uma ontologia que descreve o domínio, sob o ponto de vista dos documentos de caso de uso e dos especialistas de domínio envolvidos no projeto.
As relações de sinonímias dizem respeito aos sinônimos.
Ou seja, busca- se identificar os sinônimos dos termos, incluindo aqui os jargões utilizados no domínio.
Por exemplo, no domínio de pós-graduação de uma universidade, é comum que os termos &quot;aluno_ regular «seja utilizado como sinônimo do termo &quot;aluno_ cursando_ disciplina».
A linguagem recomendada por o W3C para descrição de uma ontologia é o OWL4.
Esta especificação é baseada em XML e descreve num arquivo a ontologia desenvolvida.
Trabalhar com arquivos numa arquitetura concorrente causaria impacto no desenvolvimento do protótipo de apoio ao método proposto.
Desta forma modelamos a ontologia utilizando um banco de dados relacional.
Existe na literatura discussões sobre a utilização do modelo relacional para expressar uma ontologia, mostrando assim que é possível gerar o arquivo OWL de uma ontologia a partir de um modelo relacional.
Sobre esse assunto sugere- se a leitura do trabalho desenvolvido por Gomez-Perez et al em Uma ontologia é composta por classes, propriedades e indivíduos, onde:
Classes descrevem o que existe em determinado domínio;
Propriedades descrevem relacionamentos e outras informações de uma classe;
E Indivíduos descrevem as instâncias das classes existentes.
Uma vez que se tenha concluído essa etapa, tem- se uma ontologia que descreve o domínio modelado nos casos de uso, enriquecido com conhecimentos de um especialista.
A recomendação é encontrada em Essa ontologia é utilizada na recuperação de documentos de casos de uso, descrita na próxima seção.
A fase de recuperação de documentos é subdividida em quatro etapas:
Caso de uso, expansão dos termos de busca, resolução de ambigüidades e ranking de resultado.
A o final dessa fase o usuário terá como resposta todos os casos de uso potencialmente reutilizáveis que sejam semanticamente similares ao caso de uso utilizado como entrada na pesquisa, desde que o conjunto de casos de uso potencialmente reutilizáveis já tenham sido previamente indexados por a ferramenta.
Em a seqüência explicaremos em detalhes cada etapa dessa fase.
Em esta etapa o usuário especifica um caso de uso dentro de o EA.
É importante dizer que o caso de uso precisa ser especificado o mais completo possível, pois o método utiliza as informações presentes nas seções do caso de uso para gerar a lista de termos, essa lista é o princípio da recuperação.
Após especificar o caso de uso, chama- se o protótipo de recuperação, passo explicado a seguir.
O objetivo dessa etapa é expandir a lista de termos que são utilizados para a pesquisa.
A expansão é realizada utilizando os relacionamentos de sinonímia, hiperonímia e hiponímia que foram definidos na etapa de enriquecimento semântico da lista de termos.
Para que ocorra a expansão é necessário inicialmente construir a lista de termos.
Essa lista é construída utilizando o mesmo algoritmo descrito na etapa de extração de termos.
Com a lista de termos pronta, temos que:
Expansão utilizando sinonímias:
Os sinônimos são relações com propriedade de simetria.
Isso quer dizer que, se um dado termo &quot;A «é sinônimo do termo &quot;B», &quot;B «também é sinônimo de &quot;A».
Assim, para cada termo da lista, busca se na ontologia os seus respectivos sinônimos e este é adicionado ao final da lista de termos, caso ainda não exista.
Quando a lista de termos é expandida utilizando sinonímias, diz- se que a lista de termos está contextualizada.
Expansão utilizando hiperonímia:
As relações de hiperonímia são realizadas com o objetivo de generalizar uma consulta.
Desta forma, uma lista de termos enriquecida com essas relações tem tendência a ser mais abstrata, aumentando a cobertura da pesquisa em relação a os corpora.
Quando a lista de termos é expandida utilizando as hiperonímias, diz- se que a lista de termos está generalizada.
Expansão utilizando hiponímia:
Já as relações de hiponímia, que são automaticamente especificadas no momento em que se define a relação de hiperonímia, têm o objetivo de especializar os termos da consulta a um assunto ou jargão.
Quando a lista de termos é expandida utilizando hiponímia, diz- se que a lista de termos está especializada.
Chama- se a atenção para o comportamento antagônico das relações de hiperonímia e hiponímia quando utilizadas na expansão de termos de consulta.
Desta forma, é necessário que o usuário escolha o tipo de comportamento que a expansão terá, podendo ser mais especializada ou mais genérica.
A ambigüidade é um desafio enfrentado por sistemas que lidam com a língua natural e diz respeito ao fenômeno lingüístico que faz com que um termo tenha significados distintos.
Recomendações para a escrita de documentos de caso de uso chamam a atenção do designer para que este evite o uso de termos ambíguos, e quando o mesmo se fizer necessário, deve- se adotar um dicionário de terminologias onde o termo é descrito e todo o seu uso se refere àquela definição adotada.
Ou seja, todo o projeto que adotar aquele dicionário tem uma definição única para os termos que no uso cotidiano são ambíguos.
Como os dicionários são orientados a projetos, projetos diferentes podem ter dicionários que definam um mesmo termo de forma distinta.
Problema semelhante foi relatado em no desenvolvimento de perfis que refletissem os interesses e necessidades do usuário num sistema de recuperação de informações de uso geral.
A solução adotada por os autores foi dividir os interesses dos usuários em perfis organizados por assuntos.
A conclusão obtida no estudo é que dividir os perfis por assunto melhorou o resultado do módulo de desambiguação da solução.
De forma similar, nesta pesquisa cada projeto indexado diz respeito a um domínio (ou outro ponto de vista de um mesmo domínio) e cada domínio deve ter a sua própria base ontológica.
Em o momento da recuperação, a ferramenta apresenta os casos de uso recuperados e para qual projeto aquele caso de uso foi especificado.
Visto que os usuários que utilizarão esta solução estarão interessados em artefatos de software utilizados na realização dos casos de uso recuperados e não no caso de uso em si, essa pode ser uma solução viável para o problema da ambigüidade de termos entre projetos de software distintos.
Como resultado da etapa Expansão dos termos de busca (4.2.2), temos uma lista de termos, a sua seção e o respectivo peso do termo no documento.
Essas informações são consultadas na ontologia com o objetivo de recuperar documentos similares.
A consulta às instâncias dos casos de uso presentes na ontologia é realizada utilizando a linguagem SQL, com restrições no formato:
Onde: Stem:
Propriedade da classe caso_ uso Secao:
Propriedade da classe caso_ uso O uso dos sinais de maior e menor denota que o comando é opcional.
Uma vez recuperado casos de uso potencialmente similares ao caso de uso informado para consulta é necessário aplicar uma função que defina o quanto cada caso de uso é similar ao procurado.
Em a literatura consultada, boa parte dos trabalhos de recuperação de informações adotou com sucesso a função de similaridade por o cálculo do cosseno e por esse motivo também a utilizaremos.
A função do cosseno é definida como:
Equação 10 onde:
Os valores de similaridade entre a expressão de busca e cada um dos documentos do corpus são utilizados no ordenamento dos documentos recuperado.
Assim o resultado da busca é um conjunto de documentos ordenados por o grau de similaridade entre cada documento e a expressão de busca.
Esse ordenamento permite restringir o resultado a um número máximo de documentos desejados ou ainda definindo um limite mínimo para o valor da similaridade.
Desta forma o usuário pode definir para a máquina de busca recuperar somente os documentos com um valor mínimo de relevância em relação a a expressão de consulta.
Para que o método proposto fosse avaliado, desenvolvemos dois protótipos:
Um que executa a fase de preparação, guiando os especialistas por as etapas descritas e apoiando a construção da ontologia, e outro que executa as etapas da fase de recuperação, apresentando ao usuário casos de uso previamente preparados que sejam similares a um caso de uso dado como entrada para a ferramenta.
Em as seções que seguem, detalhamos questões tecnológicas e apresentamos as interfaces construídas.
O EA é uma ferramenta Case que suporta o desenvolvimento de sistemas utilizando a UML como linguagem padrão.
É altamente configurável e extensível, oferecendo uma API que permite a construção de plugins que expandam o funcionamento da ferramenta.
As principais funcionalidades do sistema relacionadas a esta pesquisa são:
Caso de uso:
A ferramenta disponibiliza um formato de documento de caso de uso conforme o modelo mínimo encontrado em mostrado na Figura 6, além de oferecer possibilidade de customização da estrutura do documento de caso de uso, respeitando a forma descrevendo a seção\&gt; discutida na seção 4.1.2.
Rastreabilidade de artefatos:
A ferramenta suporta links de rastreabilidade que integram o caso de uso com todos os artefatos construídos em sua implementação.
Requisito necessário para cumprir a etapa Configuração de ambiente, discutido na seção 4.1.1.
Suporte a metodologia:
Pode ser configurado para trabalhar apoiando uma metodologia de desenvolvimento, como o processo unificado.
Suporte a plugins:
O modelo de componentes oferecido com a ferramenta permite acesso aos objetos gerenciados por a ferramenta de duas formas:
Objetos Com+ e acesso direto ao modelo de componentes através de ODBC.
Existem outras ferramentas Case com suporte a especificação de casos de uso, de entre as mais conhecidas estão o IBM RequisitePro e o Borland Caliber RM.
Ambas as ferramentas provêm funcionalidades similares ao EA.
A escolha por o EA foi feita após um levantamento sobre as ferramentas Case utilizadas nas empresas desenvolvedoras de software de Cuiabá-MT, onde constatamos uma boa aceitação dessa ferramenta.
O plugin de preparação guia o especialista nas etapas de Extração de termos, Elicitação de palavras-chave e Enriquecimento semântico da lista de termos e faz isso através de duas interfaces.
A primeira, mostrada na Figura 12, faz a extração dos termos e apresenta ao usuário, permitindo que ele descarte termos que não tem representatividade no domínio.
Para auxiliar o usuário na escolha de termos, o sistema usa a medida TF-IDF para apresentar ao usuário o peso que cada termo tem nos documento do corpus.
O plugin de recuperação utiliza os mesmo módulos desenvolvidos para o préprocessamento e para a extração de termos utilizados no plugin de preparação.
A lista de termos resultante da extração de termos é contextualizada, conforme explicado na seção a recuperação, os casos de uso são apresentados em ordem de similaridade.
A interface desenvolvida para a recuperação é mostrada na Figura 14.
Em esta seção apresentamos os experimentos de avaliação do método e da ferramenta desenvolvida.
Os experimentos foram realizados sobre um corpus formado por documentos de casos de uso que especificam um sistema para gestão de cursos e de professores de pósgraduação stricto sensu de uma universidade.
O objetivo do sistema é o «desenvolvimento de um novo sistema que contemple as funcionalidades necessárias para a gestão das operações envolvidas na Pós-graduação Stricto Sensu da universidade, possibilitando o controle de calendário, processos, professores, turmas, disciplinas, alunos, bolsas.
A disponibilidade e usabilidade do sistema devem ser umas das principais características, para que o sistema possa ser utilizado por diferentes Secretarias de Programa a qualquer momento, possibilitando uma visualização fácil e rápida de informações e relatórios.»
O corpus é formado por 81 casos de uso, sendo:
Como não tivemos acesso à equipe que construiu os documentos de caso de uso utilizados neste trabalho, convidamos três analistas de sistemas com forte atuação no mercado de Cuiabá-MT, para utilizarem o método e o sistema proposto.
Um dos analistas (analista de configuração) ficou responsável por criar os conjuntos de casos de uso que deveriam ser recuperados.
Fez isso seguindo as recomendações da etapa de configuração do ambiente apresentadas na seção 4.1.1.
Os outros dois analistas (analistas de preparação) ficaram responsáveis por a criação da base ontológica e seguiram o método de preparação conforme descrito nas seções 4.1.2, 4.1.3 e 4.1.4.
O analista de configuração recebeu a ferramenta Case Enterprise Architect 7.0 e o corpus de avaliação.
Para fins de uma avaliação preliminar, o analista de configuração analisou os casos de uso do tipo CRUD, e destes separou quatro casos de uso que foram separados em dois conjuntos (Tabela 3).
A estes anexamos mais alguns casos de uso escolhidos de forma aleatória antes de passar- los aos analistas de preparação.
Cada analista de preparação recebeu a ferramenta Case Enterprise Architect 7.0, o plugin de preparação e dois corpora, cada corpus se referindo a um dos conjuntos apresentados na Tabela 3.
Os analistas tiveram uma semana para concluírem a fase de preparação.
Esse tempo foi sugerido por os próprios analistas para que eles tivessem contato com os casos de uso e entendessem o negócio.
Após a conclusão da fase de preparação, os analistas nos entregaram duas bases contendo a ontologia e os índices para os documentos, essas bases foram utilizadas conforme nos foram entregues, não sendo aplicado nenhum tipo de revisão.
Passamos então para a fase de recuperação.
A fase de recuperação se inicia com a necessidade do designer em conhecer casos de uso previamente preparados e que sejam similares a um caso de uso em fase de especificação.
Para que fosse possível avaliarmos a fase de recuperação, solicitamos ao analista de configuração que nos sugerisse um caso de uso similar para cada um dos conjuntos criados na fase de preparação.
Utilizamos o corpus completo em conjunto com as ontologias criadas, para o processo de recuperação.
Para efetivar a recuperação, escolhemos o caso de uso de entrada e executamos o plugin de recuperação.
Os resultados são mostrados na Tabela 4 e Tabela recuperação;
Os casos de uso retornados na consulta, sendo destacados os casos de uso separados na fase de configuração de ambiente (Tabela 3);
A similaridade entre o caso de uso retornado e o caso de uso de entrada;
E as medidas de precisão, cobertura e média harmônica.
Os casos de uso recuperados foram entregues para os analistas de preparação, sem uma ordem de similaridade definida.
Para o analista de preparação 1, entregamos os casos de uso recuperados com a utilização da ontologia que foi preparada por o analista de preparação 2.
Foi adotado o mesmo critério para o analista de preparação 2.
Solicitamos que eles analisassem o caso de uso utilizado como entrada e definissem uma ordem para os casos de uso recuperados, sendo permitido formar conjuntos e definir uma mesma ordem ao conjunto.
O resultado da ordenação é mostrado na Tabela 6 e Tabela 7.
Tabela 6 ­ Resultado da ordenação manual do conjunto de teste A Ordenação manual, conjunto de testes A Analista 1 Ordem Casos de uso Analista 2 Ordem Manter requisitos de inscrição e Casos de uso andamento Manter requisitos de inscrição e andamento Emitir informações gerenciais de inscrição Manter áreas Manter programas O nome do caso de uso está grafado conforme recebemos originalmente.
Apesar de preliminares, os resultados apresentados mostram que o método descrito neste trabalho é eficaz, visto que ele apresentou cobertura de 100% em ambos os testes.
Quanto a medida de precisão, que apresentou resultado inferior a 50%, o resultado foi compensado por o algoritmo de ranking que ordenou os documentos de forma similar a classificação manual feita por os usuários.
Existem ainda alguns pontos que devem ser considerados:
Etapa de elicitação de palavras-chave:
A escolha de termos é feita de forma subjetiva, e varia segundo o entendimento que o analista que a está executando tem sobre os documentos que estão sendo preparados.
Em esta fase, até o papel do analista influência.
Percebe- se (Tabela 4) que quando o analista tem um papel mais voltado para o negócio o comportamento do sistema tende a ser mais generalista ­ caso do Analista 2.
O oposto ocorre quando o papel do analista é mais técnico ­ caso do Analista 1.
Etapa de enriquecimento semântico da lista de termos:
Esta etapa captura o conhecimento que os analistas detêm sobre o domínio.
Esse conhecimento tende a variar de acordo com o analista.
Por esse motivo, seria mais interessante que a ontologia fosse criada a partir de o conhecimento de um grupo de analistas.
Etapa de recuperação:
A recuperação de documento é realizada a partir de os índices e do conhecimento descrito na ontologia.
Mesmo com o viés causado por a utilização de um único analista para a criação dos índices e da ontologia, os documentos recuperados satisfizeram ambos os analistas (Tabela 6 e Tabela 7).
Isso demonstra que cada analista detém, minimamente, o conhecimento consensual do domínio.
Este trabalho objetivou o desenvolvimento de um método para recuperação semântica de documentos de casos de uso.
Para tal, fundamentamos o trabalho com técnicas para construção e avaliação de sistemas de recuperação de informações e discutimos como a utilização de uma ontologia poderia melhorar os resultados do sistema.
Esses recursos foram então organizados em etapas, de forma a guiar os envolvidos num projeto de software na construção de uma ontologia para representação do domínio e na criação dos índices que seriam utilizados na fase de recuperação.
Um protótipo de software foi desenvolvimento com o objetivo principal de validar o método proposto.
Com o auxílio do protótipo foi realizado experimentos de recuperação sobre o corpus no domínio de um departamento de pós-graduação de uma Universidade.
Os resultados apresentados foram considerados relevantes a essa pesquisa.
No entanto percebemos que o desempenho da fase de recuperação semântica de informações é dependente da fase de preparação da ontologia, que é feita por um especialista do domínio.
Como nem sempre se tem um especialista do domínio disponível na fase de implementação, ainda no contexto desta pesquisa, é desejável verificar qual o impacto na eficácia do sistema se forem utilizados outros analistas na preparação da base ontológica.
Em o desenvolvimento deste trabalho, identificamos pontos em abertos que deixamos como sugestão para trabalhos futuros:
Pesquisar mecanismos que ajudem o analista de configuração a selecionar os documentos na fase de configuração do ambiente;
Pesquisar mecanismo para a utilização de sintagmas na fase de extração de palavras-chave do domínio;
Investigar se a utilização do algoritmo C- value/ NC- value na preparação dos índices melhora o resultado do algoritmo de recuperação apresentado neste trabalho;
Implementar algoritmos para construção semi-automática da ontologia a partir de os documentos de caso de uso do domínio.
