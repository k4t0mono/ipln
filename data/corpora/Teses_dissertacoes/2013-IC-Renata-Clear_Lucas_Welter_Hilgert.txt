Ferramentas e serviços de tradução de máquina (automática) em tempo real têm sido investigadas como uma alternativa à utilização de idiomas comum (Lingua Franca) durante reuniões de equipes com diferentes idiomas nativos.
No entanto, como demonstrado por diferentes pesquisadores, este tipo de tecnologia ainda apresenta alguns tipos problemas que dificultam a sua utilização neste contexto, de entre os quais destaca- se neste trabalho as traduções inconsistentes (diferentes traduções atribuídas a uma mesma palavra num mesmo contexto).
De entre as soluções apontadas na literatura para melhorar a qualidade das traduções, destaca- se a construção de vocabulários multilíngues específicos de domínios.
Sendo assim, neste trabalho é proposto um processo para a extração de vocabulário multilíngue a partir de documentos de software.
O processo proposto seguiu um conjunto de etapas consolidadas na literatura, tendo apresentado, como principal diferencial a forma por a qual o vocabulário de domínio é identificado:
Mediante a utilização de softwares extratores de terminologia.
Uma avaliação manual dos dicionários gerados por o processo demonstrou uma precisão de 81% na tradução de palavras simples e 39% na tradução de expressões multipalavras.
Estes valores demonstraram- se condizentes com os trabalhos relacionados.
Palavras-Chave: Vocabulário Multilíngue, Desenvolvimento Global de Software, Tradução de Máquina.
Serviços de tradução de máquina em tempo real têm sido considerados como alternativas promissoras ao auxílio na comunicação durante a execução de tarefas colaborativas envolvendo equipes multilíngues.
De as áreas em as quais este tipo de tecnologia tem sido explorada, destacam- se neste trabalho as pesquisas relacionadas ao Desenvolvimento Global de Software, área em a qual a comunicação entre times com diferentes idiomas nativos é frequentemente realizada por intermédio de idiomas comuns (inglês, por exemplo), em os quais nem sempre os participantes apresentam a fluência necessária.
No entanto o desempenho dos serviços e ferramentas de tradução ainda está longe da perfeição, apresentando um alguns problemas a serem solucionados.
De entre os problemas identificados, tanto na bibliografia quanto numa análise conduzida sobre registros do experimento de Calefato, destacam- se neste trabalho aqueles relacionados ao vocabulário empregado como, por exemplo, a inconsistência de tradução de palavras e termos.
Este tipo de inconsistências, segundo Yamashita, tende dificultar o estabelecimento de um conhecimento comum (common ground) entre os participantes da reunião, condição necessária ao sucesso da execução de tarefas colaborativas.
Uma das possíveis soluções apontadas na literatura para o problema das inconsistências, e consequente melhoria da qualidade das ferramentas e serviços de tradução, consiste na especialização dos vocabulário bilíngue utilizado por essas mediante a ampliação de seus dicionários com vocabulário bilíngue específico dos domínios sobre as quais serão utilizadas.
Sendo assim, este trabalho propõe um processo para a extração de vocabulário multilíngue (português-inglês) a partir de a documentação de software, tendo esse como objetivo a ampliação dos dicionários da ferramenta Apertium, (ferramenta de tradução) a ser utilizada em futuras etapas do projeto em o qual este trabalho encontra- se inserido.
Este documento encontra- se organizado de modo que o Capítulo 2 apresenta o contexto em o qual este trabalho encontra- se inserido, o Capitulo 3 apresenta o corpus construído e demais recursos levantados, o Capítulo 4 apresenta os principais conceitos relacionados ao processo proposto, o proposto por este trabalho e, por fim, os capítulos 7 e 8 apresentam respectivamente uma análise dos resultados obtidos e as principais discussões em relação a esse.
Este capítulo descreve o contexto em o qual o presente trabalho de pesquisa encontra- se inserido.
Em esse, são apresentados o projeto de o qual este trabalho faz parte, os problemas identificados e alternativas encontradas na literatura para a solução de esses (Seção 2.2).
Por fim, são apresentados os objetivos do trabalho e os métodos utilizados no decorrer de a pesquisa (Seção 2.4).
Projeto O Desenvolvimento Global de Software consiste numa abordagem de desenvolvimento em a qual as equipes envolvidas e os stakeholders (interessados no software) encontram- se geograficamente dispersos a nível global.
Este tipo de desenvolvimento explora questões como a diversidade cultural e as diferenças de fusos horários visando diminuir o tempo e os custos do processo de desenvolvimento de softwares, além de promover a aproximação das empresas com os clientes ao redor de o globo.
Assim, esta estratégia tem se demonstrado como um diferencial competitivo para as empresas de desenvolvimento.
A dispersão geográfica, no entanto, tende a trazer uma série de dificuldades, de as quais destacam- se as linguísticas principalmente no que tange a diferença de idiomas.
Esta diferença é apontada tanto como um dos maiores obstáculos à execução de atividades colaborativas quanto como um dos principais fatores de sucesso dos projetos de desenvolvimento em países como Índia, Filipinas, Irlanda e Singapura.
Estes países se destacam por seus altos índices de proficiência na língua inglesa, frequentemente empregada como idioma comum (common language) durante reuniões de equipes multilíngues.
No entanto, existem países considerados competidores no mercado global de desenvolvimento de software que não possuem um número suficiente de profissionais fluentes na língua inglesa.
No caso de o Brasil, por exemplo, tem- se um crescimento médio anual de 6,5% de empregos no setor de Ti (Tecnologia da Informação) sendo que apenas aproximadamente 5,4% da população tem fluência no idioma inglês, contra 90 milhões da Índia.
Este trabalho ocorre no contexto do projeto &quot;O Efeito do Processamento da Linguagem Natural no Desenvolvimento da Capacidade do Brasil no Mercado Global de Desenvolvimento de Software», que tem como objetivo auxiliar na inserção de equipes brasileiras no mercado global de desenvolvimento de software mediante a experimentação e utilização de métodos, técnicas e ferramentas da área de Processamento da Linguagem Natural (PLN).
O projeto tem como foco principal a experimentação, utilização e adaptação de serviços de tradução de máquina em tempo real, tendo como principais objetivos:
Mapear as etapas e práticas usuais na engenharia de requisitos, identificando tarefas de comunicação;
Fazer um levantamento de vocabulário específico destas práticas;
Propor métodos de aquisição de vocabulário nas comunicações entre equipes distribuídas;
Investigar, propor e avaliar da geração de vocabulário controlado multilíngue orientado a tarefas;
Estudar o uso de ferramentas off the shelf na comunicação entre equipes distribuídas, em especial envolvendo idiomas distintos;
Estudar o incremento de ferramentas através do mapeamento de vocabulários específicos.
Tradução de Máquina em Atividades Colaborativas Distribuídas A utilização de serviços de tradução de máquina em tempo real tem sido considerada por pesquisadores como uma alternativa promissora à utilização de idiomas comuns (lingua franca) durante a comunicação envolvendo participantes com diferentes línguas nativas.
Em o contexto da execução de tarefas colaborativas entre participantes geograficamente distribuídos, destacam- se os trabalhos de Calefato, Yamashita e Ishida e Yamashita, cujas constatações motivaram a realização do presente trabalho.
Calefato Investigaram a utilização de tradutores automáticos (Google Translator) durante reuniões de equipes distribuídas (e multilíngues) de desenvolvimento de software.
A investigação foi realizada através de experimentos controlados e teve como alvo reuniões em as quais eram discutidos requisitos de softwares.
Yamashita e Ishida pesquisaram os efeitos da tecnologia de tradução de máquina em tarefas colaborativas relacionadas à ordenação de imagens.
Em o experimento realizado, um participante organizava um conjunto de imagem de acordo com as instruções (automaticamente traduzidas) fornecidas por um segundo participante.
Posteriormente em Yamashita Um novo experimento foi conduzido (nos moldes do experimento anterior), porém, utilizando trios de participantes no lugar de duplas, levantando uma novo conjunto de dificuldades.
Os trabalhos anteriormente apresentados demonstram que não houve uma diferença significativa entre a utilização do inglês como um idioma comum e da utilização de serviços de tradução simultânea durante a intermediação da comunicação entre participantes (e equipes) multilíngues.
Este fato é atribuído, de forma unânime, aos problemas de tradução encontrados durante a utilização dos serviços de tradução automática.
Estes problemas (traduções incorretas ou inconsistentes) foram apontados como a principal causa das dificuldades no estabelecimento de um conhecimento comum entre os participantes (common ground), levando a atrasos na comunicação uma vez que era necessário um maior número mensagens de esclarecimento para que os participantes pudessem compreender de forma clara a informação passada por os demais participantes.
Uma análise (qualitativa) dos registros de comunicação do experimento de Calefato Demonstrou problemas de tradução ordem estrutural (referentes à reestruturação incorreta de sentenças) e relacionados ao vocabulário (traduções incorretas ou inconsistentes).
Em relação a a traduções incorretas foram identificados casos em os quais essas foram provocadas por características da sentença de entrada como, por exemplo, erros de digitação e abreviações.
Como exemplo deste tipo de problemas pode ser mencionada a abreviação das palavras &quot;ring tone «(abreviado como &quot;ring&quot;) e «(bluetooth) «(abreviado como &quot;blue&quot;) para as quais foram geradas, respectivamente, as traduções &quot;anel «e &quot;azul», ambas corretas em relação a versão abreviada, porém incorretas de acordo com o contexto.
Outro problema identificado decorre da tradução inconsistente de palavras.
Considera- se neste trabalho como inconsistente a atribuição de diferentes traduções para uma mesma palavra dados contextos iguais.
Como exemplo de tradução inconsistente pode ser destacado o termo &quot;release «(versão), para o qual foram atribuídas três diferentes traduções:
&quot;versão», &quot;lançamento», &quot;entrega», &quot;liberação».
Foram, ainda, identificados casos para os quais o termo não foi traduzido.
Por fim, observou- se, ainda, a ocorrência frequente de erros de digitação (typos) sendo que, para o termo &quot;Bluetooth «(tecnologia para transmissão de dados), por exemplo, foram encontradas 5 diferentes grafias:
&quot;bluetooth», &quot;blutooth», &quot;bluetoth», &quot;blutoofh «&quot;bluetooh».
Estes erros, apesar de simples, podem vir a ter impacto na qualidade da tradução e mesmo no entendimento dos participantes.
Motivação Os problemas anteriormente mencionados dificultam a adoção de ferramentas e serviços de tradução de máquina para a intermediação da comunicação entre equipes multilíngues.
Sendo este tipo de tecnologia apontado como uma solução promissora para a falta de profissionais proficientes na língua inglesa, buscou- se neste trabalho colaborar para o aumento de qualidade destes servições e ferramentas.
Tendo sido observado que os problemas relacionados ao vocabulário (traduções incorretas ou inconsistente) ocorreram mais frequentemente e causaram maiores impactos ao entendimento entre os participantes, optou- se, inicialmente, por buscar soluções para esses.
De entre as possíveis soluções apontadas na bibliografia, destaca- se a construção de vocabulários bilíngues específicos do domínio.
Apesar deste objetivo ser encontrado em outros trabalhos, esses não tem como foco a especialização do vocabulário para um domínio específico.
Sendo assim, este trabalho propõem um processo para auxiliar na extração de vocabulário multilíngue inglês-português, tendo como foco (domínio) manuais de software do nível de usuário.
Objetivo Geral Dado o contexto do projeto e os estudos experimentais iniciais, este trabalho tem como objetivo auxiliar no tratamento dos problemas de tradução relacionados ao vocabulário.
Em este sentido, propomos o estudo e desenvolvimento de um processo para a extração de vocabulário multilíngue a partir de textos paralelos de um domínio específico.
Assim, pretende-se colaborar com o projeto mediante a utilização do processo proposto na extração de vocabulário multilíngue a partir de documentação de software, sendo que neste trabalho foram investigados manuais de usuário multilíngues, principal fonte de material encontrada dentro de o domínio abordado.
Objetivos Específicos A extração de vocabulário multilíngue foi mapeada nos seguintes objetivos específicos:
Levantamento de materiais relacionados à documentação de software;
Construção de um corpus paralelo para os idiomas inglês-português (Brasil);
Investigação de métodos e técnicas para extração de vocabulário multilíngue a partir de corpora paralelo;
Proposta de um método para a extração de vocabulário multilíngue a partir de a documentação de software;
Avaliação dos resultados obtidos a partir de a execução do processo;
O objetivo inicial deste trabalho era a construção do vocabulário bilíngue relacionado às práticas usuais de Engenharia de Software, no entanto, devido a a escassez de material encontrado, optou- se por focar na extração de vocabulário a partir de manuais de usuário (maior número de documentos paralelos), que constituem uma das principais fontes de vocabulário do domínio de aplicação de softwares.
Escopo do Trabalho Como pode ser identificado mediante a descrição do projeto, o mesmo pode ser dividindo em duas principais linhas de pesquisa, sendo uma relacionado à experimentação com tecno-logias relacionadas à tradução de máquina e a outra à pesquisa linguística para melhoramentos ou especializações das tecnologias empregadas.
Este trabalho se encontra inserido na fase inicial da pesquisa linguística, estando voltado à coleta e construção de recursos a serem empregados nas etapas futuras do projeto.
A Figura 2.1 apresenta o escopo do trabalho proposto.
Como pode ser visto na Figura 2.1, o trabalho proposto é composto por as seguintes etapas:
Construção do Corpus:
Em esta etapa foi construído um corpus paralelo português-inglês composto por manuais de software;
Extração do Vocabulário Bilíngue: A partir de a aplicação do processo proposto sobre o corpus criado, foi extraído um vocabulário bilíngue;
Construção de Dicionário Bilíngue: O vocabulário bilíngue extraído foi filtrado eliminando símbolos não úteis ao dicionário como, por exemplo, símbolos de pontuação e numerais.
Posteriormente as entradas mais relevantes do vocabulário bilíngue restante foram formatadas de acordo com a estrutura dos dicionários do bilíngues do Apertium.
Assim, a saída do processo proposto consistem num dicionário bilíngue, construído a partir de o vocabulário extraído dos manuais de software.
O dicionário criado pode tanto ser utilizado de forma individual em conjunto com a ferramenta Apertium ou utilizado para expandir os dicionários padrão da ferramenta com vocabulário específico de um domínio (tendo em vista que os dicionários padrão são de domínio genérico).
Como o corpus encontrado é escasso, o processo aqui apresentado tem como um dos objetivos permitir sua aplicação sobre os diferentes tipos corpus que possam vir a ser construídos no decorrer de o projeto.
No entanto, vale ressaltar que a avaliação do vocabulário extraído mediante sua utilização na ferramenta selecionada (avaliação extrínseca), bem como a realização de experimentos utilizando o vocabulário em questão, são atividades que encontram- se fora de o escopo deste trabalho, sendo executada em outras etapas do projeto.
O vocabulário, bem como os demais materiais levantados durante a execução deste trabalho, podem ser utilizados em outras etapas do processamento linguístico como, por exemplo, a construção automática de corpora multilíngues, construção de estruturas hierárquicas e ontologias multilíngues, entre outras.
No entanto, como anteriormente ressaltado, o escopo deste trabalho termina na geração dos dicionários, sendo a utilização dos recursos realizada durante outras etapas do processo.
Em este capítulo serão apresentados os recursos linguísticos levantados durante a primeira etapa deste trabalho, tendo como foco principal a apresentação do corpus bilíngue compilado a partir de manuais de softwares.
O corpus aqui apresentado foi compilado com o objetivo de ser utilizado como recurso linguístico inicial para estudos relacionados à extração de vocabulário multilíngue e para a avaliação do processo de extração proposto neste trabalho.
Apesar de o foco na construção de um corpus, no decorrer de a busca por documentos bilíngues, outros recursos considerados como úteis ao escopo do trabalho foram encontrados como, por exemplo, vocabulários da área de Engenharia de Software e registros de comunicação entre desenvolvedores.
O capítulo encontra- se organizado de modo que, inicialmente, é apresentado o corpus paralelo construído e utilizado durante este trabalho.
A seguir, a Seção 3.2 apresenta um conjunto de materiais complementares relevantes ao projeto que foram levantados durante a busca por documentos para o corpus.
Por fim (Seção 3.3), são justificadas as principais escolhas tomadas durante a construção do corpus.
Corpus Bilíngue Um corpus pode ser definido como uma coleção de textos, compilada de acordo com critérios de seleção preestabelecidos, de modo a tornar- la empregável na pesquisa linguística.
Este tipo de recurso é imprescindível a pesquisas na área de Processamento da Linguagem Natural (PLN) sendo utilizado em diferentes atividades de as quais destaca- se, no contexto deste trabalho, a extração de vocabulário multilíngue.
Em o que tange à questão multilíngue, um corpus pode ser classificado como paralelo ou comparável.
Corpus paralelo (também conhecido como bitext) consiste num conjunto de textos acompanhados por suas respectivas traduções para outros idiomas.
Corpus comparável, por sua vez, consiste num conjunto de textos em diferentes idiomas que compartilham determinadas características como, por exemplo, assunto (tema), autor, época, entre outras.
De entre os corpus multilíngues encontrados, o único relacionado ao domínio de documentos de software foi o Opus.
No entanto, apesar de ser um corpus grande em número de palavras (aproximadamente 3 milhões) esse é composto apenas por dois manuais.
Assim sendo, optou- se por a construção de um novo corpus paralelo.
Corpus Construído O objetivo inicial desta etapa foi a construção de um corpus a partir de a documentação relacionada a projetos de desenvolvimento de software como, por exemplo, documento de requisitos, documento de análise, documento de projeto, registros (logs) de reuniões multilíngues, entre outros.
No entanto, devido a pequena quantidade de material técnico bilíngue encontrado, optouse por recorrer aos manuais de usuário, mais facilmente encontrados em múltiplos idiomas, sendo priorizados manuais de projetos de código livre (open source).
Em projetos open source, manuais de usuário são construídos e traduzidos de forma colaborativa por usuários, muitas vezes, geograficamente dispersos.
Em este processo, inicialmente, uma versão em inglês do manual é disponibilizada para posteriormente ser traduzida para diferentes idiomas.
A utilização de manuais de usuário de projetos open source para a construção de corpus paralelo não é novidade, tendo sido anteriormente utilizada por Tiedemann no Opus Corpus, tendo como principal vantagem a possibilidade de disponibilização posterior desse, assim como dos resultados do seu processamento, devido a o tipo de licenças utilizadas nestes projetos.
Esta decisão foi tomada de modo a consolidar, inicialmente, o processo de extração do vocabulário multilíngue enquanto a busca por material de nível técnico é realizada de forma paralela para utilização em futuras etapas do projeto.
As dificuldades associadas à construção de corpus para domínios específicos, principalmente quando relacionados a trabalhos multilíngues (especialmente para documentos paralelos), é destacada na literatura por diferentes pesquisadores.
Para a construção do corpus, optou- se por a utilização de uma abordagem manual devido a o baixo desempenho obtido durante o teste de abordagens automáticas (utilização da ferramenta Bootcat).
Em relação a o tamanho do corpus construído, pode- se observar, de acordo com a Tabela por Tiedemann.
Em relação a a disparidade no tamanho do corpus utilizado por Tiedemann deve- se ressaltar que aproximadamente 2,6 milhões das palavras utilizadas por o referido autor foram retiradas a partir de os arquivos de tradução do ambiente gráfico KDE (mesmo documento), logo, existindo um viés no que tange a extração de vocabulário.
A Tabela 3.3 apresenta o número de sentenças componentes do corpus para cada um dos idiomas envolvidos.
Tabela 3.3 ­ Tamanho do corpus construído em número de sentenças.
Manual Inglês Português Android 2.3.4 Blender 2.6 Debian LibreOffice 3.3 Slackbook TortoiseSVN 1.7 Ubuntu Total Como pode ser observado na Tabela 3.3, o número de sentenças dos textos em português (segunda coluna) e inglês (terceira coluna) é similar, sendo que a diferença foi resolvida posteriormente durante o alinhamento sentencial mediante a atribuição de alinhamentos n:
M ou o descarte de sentenças.
Materiais Complementares Em esta seção serão apresentados outros tipos de materiais linguísticos encontrados durante a construção do corpus.
Apesar de não terem sido empregados diretamente no processo proposto (da mesma forma que o corpus), esses serviram como material de apoio durante etapas como, por exemplo, a avaliação do vocabulário extraído em relação a a terminologia de domínio.
Esta encontra- se organizada de modo que a Seção 3.2.1 apresenta um conjunto de termos extraídos a partir de glossários de livros técnicos de Engenharia de Software e de documentos técnicos publicados por agências como, por exemplo, a IEEE.
A Seção 3.2.2 apresenta um conjunto de documentos técnicos referentes a diferentes etapas do processo de desenvolvimento de software.
Por fim, a Seção 3.2.3 apresenta um conjunto de registros de comunicação entre equipes desenvolvidas.
Glossários Os glossários encontrados consistem em listas de palavras (simples e compostas) empregadas nas diferentes práticas da Engenharia de Software, acompanhadas por suas respectivas definições.
O principal atrativo deste material é o fato de ter sido construído por especialistas da podendo ser considerado como relevante e confiável.
A Tabela 3.4 apresenta os vocabulários encontrados bem como a quantidade de termos componentes de esses (segunda coluna).
Os vocabulários apresentados na Tabela 3.4 foram encontrados apenas para o idioma inglês.
Mesmo monolíngues, estes vocabulário podem ser utilizados como sementes (seeds) para a construção automática de corpus multilíngue (etapas futuras do projeto).
Outra fonte identificada para a extração de vocabulário da área foram o glossários e listas de assuntos de livros relacionados à Engenharia de Software como, por exemplo, o livro &quot;Software Engineering «de o qual foram extraídos 167 termos a partir de o glossário e 1.600 a partir de a lista de assuntos.
No entanto, a utilização deste tipo de recurso implica questões relacionadas a diretos autorais (copyright), bem como a dificuldade de obtenção de cópias digitais desses materiais para a língua portuguesa.
Outra possível utilização destes glossários é na ampliação dos dicionários morfológicos utilizados por ferramentas de extração morfológica do processo de extração de vocabulário (Seção 6) para a identificação de expressões multipalavras.
Padrões Técnicos Além de os vocabulários apresentados na Seção 3.2.1, foram encontrados padrões técnicos relacionados às diferentes etapas do processo de desenvolvimento de software, também compilados por instituições como a IEEE, a Iso, e outras.
A Tabela 3.5 apresenta exemplos desses.
Descreve o formato e o conteúdo de plano para a garantia IEEE Std 730-2002 da qualidade de software.
Descreve o formato e conteúdo para planos de gerência de IEEE Std 1058-1998 software.
Especifica o conteúdo de um plano de gerenciamento de IEEE Std 828-2005 configuração do processo de desenvolvimento software, assim como da atividade de especificação de requisitos.
Os padrões apresentados na Tabela 3.5 foram selecionados a partir de a lista apresentada eletrônico.
Assim como no caso de os vocabulários, apenas versões em inglês dos padrões técnicos puderam ser encontradas.
Sendo assim, esses não puderam ser utilizados na compilação do corpus construído.
Registros de Comunicação Levando em consideração o fator comunicação entre equipes (escopo do projeto) outro recurso buscado nesta primeira etapa foram registros de comunicação entre equipes distribuídas, preferencialmente envolvendo diferentes idiomas.
O objetivo da coleta deste tipo de recursos foi a identificação do vocabulário utilizado em situações reais de comunicação durante processos de desenvolvimento.
Assim como nos recursos anteriormente apresentados, optou- se por a pesquisa em projetos de desenvolvimento open source, devido a a maior disponibilidade de material por parte destes projetos bem como ao tipo de licença de utilização adotada por esses.
Como resultados desta busca, dois tipos de registros foram encontrados:
Síncronos (mensagens instantâneas) e assíncronos (listas de discussão).
Registros de comunicação assíncrona encontrados constituem- se, basicamente, por listas de e-mail (mail list) coletadas a partir de diferentes projetos.
Em relação a os registros de comunicação síncronos (mensagens instantâneas), esses foram obtidos a partir de gravações de conversas entre desenvolvedores utilizando o IRC (Internet Relay Chat), protocolo de mensagens instantâneas na Internet.
Entre os registros síncronos, destaca- se o registro de comunicação entre desenvolvedores participantes de projetos da fundação Mozilla, mais especificamente do Thunderbird (gerência de e-mails) composto por aproximadamente 161.316 mensagens e 1.669.000 palavras.
Listas de discussão podem ser obtidas em diferentes níveis, variando do nível mais leigo, em o qual usuários discutem problemas encontrados no software e requisitam novas funcionalidades, até o nível mais especialista, em o qual desenvolvedores discutem aspectos do desenvolvimento do software.
De as listas encontradas, destaca- se a do Debian (distribuição Linux) que possui um repositório contendo aproximadamente 18 anos de registros de discussão para múltiplos idiomas.
Quanto a o critério multilíngue, registros de comunicação síncrona foram encontrados somente para o idioma inglês, enquanto registros de comunicação assíncrona foram encontrados para diferentes idiomas.
Mediante uma análise do vocabulário encontrado nos registros levantados, pode- se constatar uma maior incidência de termos do domínio de aplicação do software do que do vocabulário da Engenharia de Software.
Por exemplo, a partir de o vocabulário extraído dos registros do experimento de Calefato, cujo domínio foi telefonia móvel, termos como &quot;ring tone «e &quot;Bluetooth «apresentaram maior incidência do que termos da Engenharia de Software como, por exemplo, &quot;release «(versão).
Corpus Utilizado na Pesquisa Em este capítulo foram apresentados quatro tipos de recursos levantados:
Corpus paralelo construído a partir de manuais de software, conjunto de padrões técnicos, vocabulários monolíngues da área de Engenharia de Software e registros de comunicação entre desenvolvedores.
De os recursos anteriormente listados, os conjuntos de padrões técnicos e os vocabulários de domínio são os que apresentam uma maior relação com a área de Engenharia de Software, no entanto, apenas versões monolíngues para o idioma inglês puderam ser encontradas, sendo assim, esses não puderam ser utilizados no processo de extração do vocabulário multilíngue.
A comparação do vocabulário coletado com as listas de termos extraídos a partir de os registros de comunicação demonstrou um baixo índice de intersecção, o que indica que o vocabulário técnico é pouco empregado nas conversas entre desenvolvedores, enquanto o vocabulário do domínio de aplicação é mais frequentemente empregado (o termo &quot;agenda «para o domínio de telefones móveis, por exemplo).
A principal utilidade dos registros de comunicação coletados, foi a identificação do tipo de vocabulário mais utilizado durante a comunicação entre os desenvolvedores, sendo que apenas registros assíncronos comparáveis puderam ser encontrados para múltiplos idiomas, e sua utilização na construção de corpora não foi possível devido a dificuldades de alinhamento.
Sendo assim, optou- se por a compilação e utilização de um corpus multilíngue (portuguêsinglês) a partir de documentos de software (a nível de usuário) para estudos da extração de vocabulário multilíngue e avaliação do processo proposto.
A construção de outros corpora, de domínio mais técnico, está prevista para etapas futuras do projeto, tendo sido priorizada neste primeiro momento, a consolidação do processo proposto.
Vocabulários multilíngues (também conhecidos como translation lexicons) são recursos de grande importância para pesquisas na área de Processamento de Linguagem Natural (PLN) e fundamentais para estudos em tradução de máquina.
Este tipo de recurso linguístico especifica relações de correspondência entre palavras de diferentes idiomas, sendo aplicado em outras áreas de pesquisa multilíngues como, por exemplo, na construção de corpora, recuperação de informações, tradução assistida por computadores, entre outras.
Em este capítulo serão apresentados os principais conceitos relacionados ao processo de extração de vocabulário multilíngue.
Inicialmente, um processo geral (baseado na literatura) é apresentado, sendo suas etapas descritas de modo que a Seção 4.2 apresenta a etapa de alinhamento sentencial, a Seção 4.3 a análise morfológica e a Seção 4.4 a etapa de alinhamento léxico.
A Seção 4.5 apresenta a estratégia empregada para o tratamento de expressões multipalavras e, por fim, são apresentadas estratégias para a avaliação de léxicos bilíngues (Seção 4.6).
Processo de Extração A construção de recursos multilíngues (vocabulário, terminologia, modelos de tradução, regras de tradução, etc.) encontra- se bem consolidada na literatura, não podendo ser considerada como algo inédito.
Sendo assim, a partir de os trabalhos relacionados, com destaque para os trabalhos de Caseli e Nunes, Tiedemann e Ha, um processo genérico para a extração de vocabulários bilíngues foi identificado.
Esse é composto por 3 etapas:
Alinhamento Sentencial;
Análise Morfológica; Alinhamento Lexical;
De acordo com o processo, os documentos componentes de um corpus paralelo são decompostos em sentenças para as quais são buscadas correspondências entre os documentos considerados paralelos (diferentes idiomas).
Esta atividade é conhecida como alinhamento sentencial.
Após alinhadas, as sentenças são decompostas em suas unidades léxicas (palavras, expressões multipalavras, etc.) para as quais correspondências são buscadas na sentença correspondente (alinhada) do documento paralelo (alinhamento lexical).
No entanto, como muitas vezes uma palavra (ou grupo de palavras) pode estar relacionada à mais de uma possível tradução, é importante que se estabeleça uma forma de diferenciar o contexto em o qual cada uma dessas é empregada (desambiguação).
O objetivo da anotação morfológica (segunda etapa) é fornecer informações (rótulos morfossintáticos) que permitam ao alinhador lexical realizar esta distinção, ainda durante o alinhamento lexical.
Após a etapa de alinhamento lexical, os trabalhos relacionados divergem, de acordo com seu objetivo, em relação a a utilização dos recursos produzidos.
De as diferentes utilidades desses, destacam- se o auxílio à extração terminológica, a indução de léxicos bilíngues e a construção de dicionários multilíngues, como apresentado dos principais trabalhos relacionados.
Além de as etapas anteriormente citadas, costuma- se, ainda, realizar uma etapa de préprocessamento do corpus, em a qual são executadas atividades como, por exemplo, conversão de formato de arquivos e codificação dos textos, remoção de ruídos (símbolos indesejados), separação de sentenças e unidades léxicas, entre outras.
O processo apresentado é considerado genérico pois, além de comumente encontrado em trabalhos da área, diferentes abordagens podem ser adotadas na implementação das etapas que o compõem, como melhor exemplificado nas seções posteriores.
Alinhamento Sentencial O alinhamento sentencial consiste no estabelecimento de correspondências (links) entre sentenças de textos paralelos, ou seja, em determinar possíveis traduções para essas.
A Tabela 4.1 apresenta exemplos de alinhamentos sentenciais extraídos a partir de versões paralelas (português-inglês) do manual de usuário do Android (sistema operacional de dispositivos móveis).
&quot;Como calcular soluções de problemas mate «Calculating the solutions to math problems.»
máticos. &quot;zados diretamente na tela «Página inicial».»
As sentenças demonstradas na Tabela 4.1 foram retiradas de textos paralelos sentencialmente alinhados.
Sendo assim, presumindo- se o alinhamento como correto, pode- se estabelecer a sentença &quot;Como calcular soluções de problemas matemáticos «(português) como uma provável tradução da sentença &quot;Calculating the solutions «(inglês).
Em relação a esta etapa, duas principais questões devem ser consideradas: (
a) tipos dos alinhamentos; (
b) métodos de alinhamento.
Estas questões serão apresentadas nas subseções que seguem.
Tipos de Alinhamento O alinhamento sentencial nem sempre é realizado entre exatamente uma sentença do texto fonte e uma sentença do texto alvo, podendo variar em diferentes tipos como, por exemplo, 1:2 (uma sentença do texto fonte com duas do texto alvo), 2:1 (duas sentenças do texto fonte com uma sentença do texto alvo), entre outras possíveis combinações.
No entanto, como demonstrado em, a maioria dos alinhamentos são do tipo 1:1.
Em, Caseli demonstrou que, em alinhamentos entre sentenças do português e do inglês a taxa de alinhamentos 1:1 foi de 93,97% enquanto para alinhamentos português-espanhol foi de 98,32% (manualmente revisados).
Existem, ainda, alinhamentos do tipo 1:0 e 0:1, conhecidos como alinhamentos vazios (empty) ou omissões, que ocorrem quando não é possível determinar as equivalências de uma determinada sentença entre os idiomas envolvidos.
Sentenças com alinhamento vazio costumam ser descartadas durante o alinhamento sentencial, sendo assim, nem sempre o número de sentenças alinhadas será o mesmo que o número total de sentenças dos textos originais.
Métodos de Alinhamento Sendo inviável o alinhamento manual de corpora e tendo- se como objetivo a automatização do processo, métodos automáticos (assim como ferramentas) foram propostos.
Os métodos de alinhamento sentencial pesquisados baseiam- se em três abordagens básicas:
Tamanho das sentenças (length-- based);
Conteúdo das sentenças (lexical-based);
Híbridas, combinação das duas abordagens anteriores;
Abordagens baseadas no tamanho da sentença (length-- based) partem do princípio de que existe uma relação estatística entre o tamanho de sentenças paralelas de determinados idiomas Esta relação pode ser calculada tanto a partir de o número de caracteres, quanto do número de unidades léxicas (palavras, números, sinais de pontuação, etc.) componentes das sentença.
De estas alternativas, Gale e Church relatam ter obtido melhores resultados com cálculos baseados no número de caracteres.
Abordagens baseadas no conteúdo das sentenças (lexical-based) exploram os símbolos (palavras, numerais, etc.) componentes dessas.
Em relação a o tipo de informações utilizadas, essas podem ser baseadas em: (
a) dicionários bilíngues (dictionary based), (b) símbolos âncora (anchor based).
Métodos baseados em dicionários utilizam dicionários bilíngues iniciais (domínio geral) para realizar uma tradução de palavras conhecidas (existentes no dicionário) da sentença fonte, gerando uma tradução inicial de essa (ainda que pouco precisa).
Esta tradução inicial é então comparada, mediante cálculo de similaridade, com as prováveis sentenças alvo.
A principal desvantagem desta técnica é a necessidade da utilização de dicionários bilíngues externos, os quais podem nem sempre encontrar- se disponíveis.
Métodos baseados em símbolos âncora (anchor) calculam a similaridade de duas sentenças de acordo com a ocorrência desses.
Símbolos âncora são símbolos que mantêm sua forma original (não são traduzidos) em diferentes idiomas como, por exemplo, números e palavras cognatas.
Os símbolos âncora podem ser obtidos tanto a partir de listas externas, quanto a partir das próprias sentenças alinhadas, mediante algoritmos como sequência comum mais longa (Longest Common Sequence), por exemplo.
Por fim, abordagens híbridas combinam (de diferentes formas) as duas abordagens anteriormente apresentadas.
Em o Bilingual Sentece Aligner, por exemplo, o alinhamento baseado no tamanho das sentenças é inicialmente aplicado para posteriormente ser refinado utilizando- se um alinhamento baseado no conteúdo das sentenças.
Torna- se importante conhecer os métodos de alinhamentos pois estes delimitarão o tipo de pré-processamento que será empregado sobre o corpus.
Métodos baseados em símbolos âncora, por exemplo, são diretamente influenciados por a etapa de remoção de ruído.
A remoção de determinados símbolos (como número, por exemplo) impacta de forma direta no desempenho destes métodos.
Já métodos baseados no tamanho da sentença são diretamente influenciados por o procedimento de separação de sentenças.
Sentenças incorretamente separadas (divididas ao meio, por exemplo) tem seu tamanho alterado, influenciando diretamente no estabelecimento de correspondências.
Ferramentas No decorrer de os estudos relacionados ao alinhamento sentencial, um conjunto de ferramentas foi identificado, destacando- se entre essas as apresentadas na Tabela 4.2.
Além de as ferramentas apresentadas por a Tabela 4.2 foram identificados, ainda, o WinAlign e o GMA (Geometric Mapping and Alignment).
No entanto, estas ferramentas não foram consideradas neste trabalho.
Bilingual Sentence Aligner O Bilingual Sentence Aligner (ferramenta empregada por este trabalho), utiliza uma abordagem híbrida, valendo- se tanto do tamanho da sentença quanto do conteúdo da mesma (palavras), sendo independente de conhecimento prévio das línguas envolvidas.
O método proposto por Moore é composto por três passos:
Um alinhamento sentencial inicial é realizado, utilizando uma abordagem baseada no tamanho das sentenças (em relação a a quantidade de caracteres), adaptado a partir de o trabalho de Brown;
Os alinhamentos de maior pontuação (atribuída de acordo com o cálculo de tamanho) são utilizados para o treinamento de um modelo estatístico, em o qual um alinhamento lexical inicial é realizado entre as unidades léxicas componentes da sentença, criando assim um dicionário bilíngue inicial;
O dicionário criado durante o passo anterior é utilizado num realinhamento das sentenças, agora baseado no conteúdo sentencial.
O Bilingual Sentence Aligner foi testado sobre um corpus formado por manuais de software, domínio relacionado ao presente trabalho.
Os testes, em os quais foram utilizadas as métricas de Precision Error e Recall Error, demonstrou que a abordagem híbrida utilizada obteve um desempenho melhor do que a abordagem baseada somente no tamanho sentencial sem um aumento substancial no tempo.
Em um dos casos de avaliação realizados, 300 sentenças foram aleatoriamente removidas de um dos textos.
Em esta avaliação os valores de Precision Error e Recall Error da abordagem híbrida foram, respectivamente, 0,042% e 0,052%, enquanto para a abordagem baseada no tamanho sentencial esses foram de 0,552% e 1.967%.
A ferramenta permite, ainda, a definição de um valor threshold que delimita um valor mínimo para a probabilidade de alinhamento entre duas sentenças para que estas sejam consideradas alinhadas.
Em relação a este parâmetro, quanto maior o valor utilizado, maior a acurácia dos alinhamentos produzidos, no entanto, o número de alinhamentos tende a diminuir.
TCAalign O TCAalign, baseado no Translation Corpus Aligner, é uma ferramenta construída durante o projeto Pesa (Portuguese-English Sentence Alignment) e utilizada em Caseli e Nunes para alinhamentos português-inglês/ inglês-português e\&gt; Uma das características do TCAalign é a possibilidade da utilização de listas de palavras âncoras durante o alinhamento sentencial.
Ainda que opcional, a utilização deste recurso é apontada como um dos fatores que melhoram o desempenho do sistema.
Apesar de constituírem um recurso externo, além de sua utilização ser opcional, listas de palavras cognatas para a dupla português-inglês, produzidas durante o projeto Pesa, encontram- se disponíveis no sítio do projeto1.
Os alinhamentos realizados de forma automática durante os experimentos apresentados em foram manualmente revisados e corrigidos.
Os alinhamentos corrigidos foram posteriormente utilizados como padrão de referência no teste da ferramenta.
Após o teste foi observada uma precisão de 97.10% e uma abrangência (cobertura) de 98.23%.
O TCAalign possui uma versão para download (standalone), e outra versão para utilização online, conhecida como VisualTCA2.
Hunalign O Hunalign é uma ferramenta que emprega abordagens baseadas em dicionários e no tamanho das sentenças (sendo que pode- se optar por a utilização de apenas um destes modos), tendo sido originalmente proposto para o alinhamento do idioma húngaro devido a a falta de similaridade desse com os demais idiomas.
Quando utilizado o modo orientado a dicionários, o dicionário inicial é utilizado para a realização de uma tradução inicial do texto, posteriormente comparada com o texto alvo.
Esta comparação pode ser conduzida de acordo com o número de palavras ou com o tamanho da sentença.
Quando utilizado o modo orientado ao tamanho, a contagem de caracteres do texto original é incrementada numa unidade e a pontuação é baseada na variação da mais longa para a mais curta.
Os marcadores de limite de parágrafo são tratados como sentenças com pontuação especial.
A pontuação de similaridade é calculada para cada par de sentenças ao redor de a diagonal da matriz de alinhamento.
Quando um dicionário inicial não é disponibilizado, um alinhamento 1:1 entre os textos fonte e alvo é estabelecido, de modo a gerar um dicionário inicial.
Neste passo, qualquer sentença com probabilidade de alinhamento maior do que 0.5 é considerada.
Como pode ser observado, a abordagem empregada neste caso é muito similar à empregada por Moore.
Uma das principais vantagens desta ferramenta em relação a as demais é à velocidade de alinhamento, diretamente relacionada à sua implementação na linguagem de programação C+, enquanto as demais encontram- se implementadas em Perl (linguagem interpretada).
Análise Morfológica Dependendo do contexto, uma palavra (ou grupo de palavras) pode assumir mais de uma tradução válida, sendo assim, torna- se necessária a utilização de algum tipo de informação para a distinção entre estes contextos (desambiguação).
Informações morfológicas são frequentemente empregadas com esta finalidade.
Durante etapa de análise morfológica podem ser atribuídos diferentes tipos de informações morfológicas às palavras formadoras de um documento, destacando- se aquelas empregadas no presente trabalho:
Forma superficial:
Forma por a qual as palavras aparecem no texto;
Lema: Forma canônica, ou seja, palavra desprovida de sufixos modificadores de gênero, número, etc.;
Classe Gramatical: Classe gramatical à qual a palavra pertence (Part-of-speech) por exemplo, substantivo, adjetivo, etc;
Flexões: Variações de número (singular e plural), gênero (masculino e feminino), entre outras.
A Tabela 4.3 apresenta um exemplo de análise morfológica da palavra &quot;região «(&quot;region&quot;) para o inglês e para o português, proveniente da saída da ferramenta Lttoolbox (analisador morfológico do Apertium).
Variação de gênero, &quot;feminino «para este caso;
Variação de número, &quot;plural «para este caso.
Como observado, pode haver uma diferença entre o número de características disponibilizadas por os analisadores, sendo que para o inglês o número de informações foi menor.
Esta diferença pode ser atribuída a limitações dos dicionários utilizados por o analisador morfológico.
Além de a desambiguação de sentido, estas informações podem, ainda, ser empregadas para outras finalidades, como, por exemplo, no trabalho de Ha, em o qual os rótulos morfológicos (PoS) também são utilizados no processo de identificação de terminologia.
Tiedemann, por sua vez, utiliza informações morfossintáticas durante a geração de árvores de dependência, empregadas na visualização do corpus, assim como de seus termos.
Ferramentas A Tabela 4.4 apresenta uma lista de ferramentas que podem ser utilizadas para diferentes tipos de anotação morfológica.
Em a tabela, um &quot;X «representa que a determinada ferramenta apresenta a característica definida por a coluna.
De entre as ferramentas, destaca- se o Apertium (descrito na próxima seção), tendo esta sido selecionada para a utilização neste trabalho.
Vale ressaltar que várias das ferramentas apresentadas possuem particularidades a serem consideradas.
O Apertium, por exemplo, depende da utilização de dicionários externos para análise morfológica, enquanto outras ferramentas como o NLTK, por exemplo, necessitam que modelos próprios para cada idioma sejam treinados.
Outra particularidade que deve ser considerada é o conjunto de rótulos (tagset) utilizados para representar cada tipo de informação.
Frequentemente, ferramentas utilizam diferentes padrões como, por exemplo, no caso de o Lttoolbox que utiliza a letra &quot;n «(minúscula) para representar substantivos, enquanto o Freeling utiliza &quot;N «(maiúscula).
Sendo assim, os conjuntos de rótulos devem ser padronizados.
Apertium O Apertium é uma plataforma de código livre (open source) para tradução de máquina, sendo composta por um motor de tradução, um conjunto de ferramentas e um conjunto de dados para a implementação de sistemas de tradução de máquina baseada em regras.
De entre as funcionalidades disponibilizadas por esta plataforma, destaca- se, nesta seção, a análise morfológica, conduzida em três etapas:
Desformatação: Texto (sentenças) é separado da formatação que o acompanha;
Análise Morfológica: O texto de entrada é decomposto em suas unidades léxicas (palavras) para as quais são atribuídas informações (através de rótulos) como, por exemplo, forma superficial, lema, categoria gramatical e flexões (gênero, número, etc.).
Em esta etapa são realizadas, ainda, a resolução de casos de contração (&quot;de o igual a «de+ o&quot;) e a identificação de expressões multipalavras;
Desambiguação Categorial: Unidades para as quais mais de um rótulo gramatical (PoStag) foi atribuído são processadas por um etiquetador morfológico (PoS-tagger) para a definição do rótulo e ou forma canônica mais adequado ao contexto.
A Tabela 4.5 apresenta um exemplo de sentença anotada morfologicamente por o Apertium.
A primeira coluna da tabela apresenta a sentença original enquanto a segunda coluna apresenta o resultado da anotação.
Uma relação dos rótulos utilizados por a ferramenta para a anotação e seu significado pode As etapas de anotação morfológica e desambiguação categorial são realizadas, dentro de a arquitetura do Apertium, por ferramentas distintas.
A análise morfológica é conduzida por o pacote Lttoolbox 4, enquanto a desambiguação categorial é realizada por a ferramenta apertium-tagger.
O Lttoolbox é um conjunto de ferramentas do Apertium para o processamento lexical, e é composto por três ferramentas:
Lt-comp: Ferramenta utilizada para a compilação de dicionários morfológicos;
Lt-proc: Ferramenta de processamento léxico, utilizada para funcionalidades como, por exemplo, lematização e análise morfológica;
Lt-expand: Ferramenta utilizada para a expansão dos dicionários morfológicos.
Utilizada principalmente para a visualização completa de seu conteúdo.
O apertium-tagger, por sua vez, é um rotulador morfológico estatístico (statistical Partof-Speech) baseado num modelo de Markov oculto (HMM) de primeira ordem.
Para realizar a escolha, esta ferramenta necessita de um modelo treinado especificamente para o idioma utilizado.
Modelos treinados para vários idiomas podem ser obtidos no repositório do Apertium e encontram- se identificados por a terminação».
Prob». As ferramentas descritas são utilizadas em sequência (pipeline) de modo que a saída do analisador morfológico (Lttoobox) é fornecida como entrada para o desambiguador categorial apertium-tagger.
Alinhamento Lexical O alinhamento lexical (word alignment) consiste no estabelecimento de correspondências entre palavras (ou grupos de palavras) pertencentes a textos considerados como paralelos Partindo desta definição, torna- se importante ressaltar a diferença entre alinhamento lexical, extração de léxicos bilíngues e extração de terminologia bilíngue.
O alinhamento léxico, como previamente apresentado, consiste no alinhamento de todas as unidades léxicas (palavras, sintagmas, sinais de pontuação, etc.) componentes dos textos paralelos A extração de léxicos bilíngues, por sua vez, tem por objetivo a identificação de traduções de palavras (ou grupos de palavras) específicas, que posteriormente poderão ser utilizadas fora de seu contexto.
Sendo assim, são filtrados elementos como, por exemplo, sinais de pontuação, funções gramaticais, traduções incertas, etc..
A extração de um subconjunto de palavras pertencentes a um domínio específico, a partir de um léxico bilíngue, é conhecida como extração de terminologia bilíngue.
Segundo Tiedemann, existem duas principais abordagens para o alinhamento léxico: (
a) associativas e (b) estimativa.
Essas são apresentadas nas próximas seções.
Abordagens Associativas As abordagens associativas, também conhecidas como abordagens heurísticas ou abordagens de teste de hipótese, constituem uma das principais técnicas utilizadas por lexicógrafos no início dos estudos linguísticos em corpus paralelos.
Este tipo de abordagem, baseado na coocorrência de palavras equivalentes em documentos paralelos, em geral, é composta por três etapas:
Segmentação léxica:
Os limites das unidades léxicas (sentenças, palavras, sintagmas, etc.) são identificados para ambos os idiomas envolvidos;
Correspondências: Identificação de possíveis associações entre unidades léxicas de acordo com algum critério de correspondência (frequência, contexto, etc.).
Em esta etapa, muitas vezes, são criados dicionários bilíngues em os quais palavras são vinculadas a suas respectivas traduções, utilizando algum tipo de peso para cada vínculo;
Alinhamento e Extração:
Mediante o dicionário criado no passo anterior, são selecionadas as traduções mais confiáveis.
Esta extração pode ser realizada através de estratégias &quot;gulosas «(greedy) como a &quot;Best First», por exemplo.
A etapa de segmentação léxica também pode ser referenciada como tokenization e pode ser executada de diferentes formas, sendo a forma mais comum a utilização de espaços em branco como delimitadores de palavras.
No entanto, existem ferramentas específicas para esta finalidade como, por exemplo, as disponibilizadas por as bibliotecas NLTK e OpenNLP 5.
Essas, baseiam- se em modelos treinados que, além de identificar os limites das palavras, são capazes de resolver casos de contração (&quot;de o $= de+ o&quot;) e abreviações.
Após identificados os limites das unidades léxicas, o próximo passo consiste em medir o nível de associação entre as palavras dos documentos.
De entre as medidas mais utilizadas na literatura para o cálculo de associação, destacam- se as medidas de co-ocorrência e as de similaridade de strings.
Medida de Coocorrência Medidas de coocorrência assumem que palavras equivalentes de textos paralelos co-ocorrem significativamente mais frequentemente devido a caracterizarem um alinhamento do que ao acaso.
Sendo assim, para este tipo de medida, cria- se uma hipótese inicial de que as unidades léxicas co-ocorrem ao acaso e, posteriormente tenta- se refutar- la através da utilização de cálculos de associação como o t-test e o coeficiente de Dice.
Ambas as mediadas de associação estatística são utilizadas de forma que, quanto mais forte a evidência para rejeitar a hipótese de independência, maior os valores resultantes do cálculo.
Abordagens baseadas em medida de coocorrência podem ser vistas em.
Ha Faz uso da medida de Loglikelihood enquanto Zhang utiliza um conjunto de medidas:
Mi (Mutual Information), Dice, X 2 e LogLikelihood.
Vale ressaltar que a medida de LogLikelihood é utilizada em ambos os trabalhos devido a sua capacidade de capturar associações de baixa frequência Esta abordagem demanda a utilização de textos alinhados sentencialmente, para as quais são construídas tabelas de contingência que servem como base para a aplicação das métricas apresentadas.
A Tabela 4.6 representa um exemplo de tabela de contingência para um termo &quot;I «(inglês) e seu suposto termo equivalente &quot;P «(português).
&quot;I «não, o valor &quot;c «ao número de sentenças em as quais o termo &quot;I «ocorre e o termo &quot;P «não e, por fim, o valor &quot;d «representa a quantidade de sentenças em as quais nenhum dos termos ocorre.
A utilização de abordagens associativas costuma ser empregada para a extração de palavras ou expressões multipalavras (Multiword Expressions) previamente conhecidas.
Sendo assim, costuma ser precedida por um processo de extração terminológica, como pode ser observado nos A aplicação das métricas anteriormente apresentadas faz uso destes valores para o cálculo de co-ocorrência.
O cálculo de Dice, por exemplo, utiliza a seguinte fórmula:
Dice (E, P) $= 2 a/ (a+ b) (a+ c) Técnicas relacionadas a medidas de similaridade de strings, utilizam- se de medidas como, por exemplo, LCSR (Longest Common Subsequence Ratio), subsequência comum mais longa, para a extração de unidades léxicas similares, também conhecidas como cognatas.
A identificação de cognatas é utilizada principalmente durante o processo de alinhamento sentencial.
De entre os sistemas encontrados, apenas o LIHLA faz utilização desta abordagem, ainda que de forma auxiliar.
Abordagens Estimativas Em este tipo de abordagem, modelos probabilísticos (estatísticos) treinados a partir de corpus paralelos são utilizados para a identificação de correspondências entre palavras.
Um exemplo de método que utiliza abordagens estimativas é a tradução de máquina estatística (Statistical Machine Translation), que consiste na aplicação do modelo do canal com ruído (noisy channel model) da teoria da informação à tradução de máquina.
Em relação a abordagens estimativas, duas questões devem ser levadas em consideração:
Alinhamentos não simétricos, identificação de expressões multipalavras (Multi-Word Expressions).
O fato dos alinhamentos não serem simétricos implica resultados diferentes de acordo com a direção de alinhamento escolhida (português-inglês ou inglês-português).
Sendo assim, dados dois textos paralelos nos idiomas português e inglês, o alinhamento de um termo X do texto em inglês com um termo Y do texto em português (na direção inglês-português), não implicará que o termo Y seja alinhado ao termo X se a direção do alinhamento for invertida (português para inglês, por exemplo).
Esta diferença de alinhamento ocorre principalmente em palavras compostas, ou expressões multi-palavras uma vez que neste tipo de alinhamento uma palavra fonte só pode ser alinhada a uma palavra alvo por vez.
Uma das possíveis soluções para este problema são os algoritmos de simetrização propostos por Och e Ney e utilizados por Tiedemann e Caseli e Nunes.
Esses realizam a intersecção ou a união entre as matrizes de alinhamento geradas em ambos os sentidos fonte/ alvo e alvo/ fonte.
Outra técnica utilizada durante o alinhamento de expressões multipalavras é a conexão das palavras formadoras dessas mediante a utilização de símbolos como « «(sublinhado) como em &quot;telefone_ móvel», por exemplo.
Esta conexão deve ser realizada antes do processo de alinhamento léxico.
Existem duas principais desvantagens associadas a esta técnica (conexão de palavras).
A primeira consiste na necessidade que as expressões sejam previamente conhecidas, o que exige a utilização de métodos ou ferramentas para a identificação deste tipo de estrutura.
A segunda desvantagem é que, por unir as palavras, o número de ocorrências individuais dessas é decrementado, podendo interferir em seu processo de alinhamento.
Ferramentas trabalhos pesquisados, optou- se por sua utilização.
A ferramenta selecionada é apresentada na próxima seção.
A ferramenta permite, ainda, que arquivos de configuração previamente treinados e dicionários bilíngues sejam utilizados como auxílio durante o processo, sendo que estas informações influenciam diretamente na qualidade do alinhamento.
Dicionários externos influenciam tanto no alinhamento das palavras multilíngues quanto na identificação de expressões multipalavras durante o alinhamento.
Este, constitui- se de um arquivo indicando uma determinada palavra e seu respectivo equivalente para um segundo idioma.
Em sua configuração padrão, a ferramenta executa 5 iterações dos modelos 1,2 e 4 da IBM seguido por o processo de alinhamento utilizando Modelos Ocultos de Markov (HMM).
Identificação do Vocabulário Como apresentado na Seção 4.4, o alinhamento lexical, quando conduzido através de abordagens estimativas, estabelece relações de equivalência entre todas as unidades léxicas de textos paralelos, sem distinguir entre os tipos de símbolos alinhados.
Sendo assim, ao final do processo, tem- se uma lista de equivalências multilíngues em a qual podem ser encontrados tanto termos de domínio quanto símbolos de menor importância (para o contexto do trabalho) como sinais de pontuação, por exemplo.
Logo, é importante que sejam utilizados métodos que distinguam entre as entradas de acordo com sua importância, sendo o vocabulário do domínio priorizado.
Ainda no que tange a identificação do vocabulário, deve- se considerar expressões multipalavras (Multi-Word Expressions), combinações de palavras para as quais as propriedades sintáticas ou semânticas da expressão como um todo não podem ser obtidas a partir de suas partes constituintes.
Este tipo de estrutura compõe, segundo Ramish, pelo menos, 50% do vocabulário de um domínio especializado.
Vale ressaltar que a necessidade de identificação do vocabulário do domínio, bem como das expressões multipalavras (componentes desse) não é uma exclusividade das abordagens estimativas (estatísticas) sendo empregada também em abordagens associativas.
Em o restante desta seção serão apresentadas as principais técnicas utilizadas para a extração de palavras relevantes ao domínio e expressões multipalavras.
A o final, serão apresentadas as duas ferramentas utilizadas neste trabalho para extração dessas.
Identificação do Vocabulário de Domínio A identificação do vocabulário do domínio, que também inclui o conjunto de expressões multipalavras pertencentes a esse, pode ser realizada de acordo com três principais abordagens:
Abordagens estatísticas;
Abordagens heurísticas;
Abordagens híbridas.
Abordagens estatísticas baseiam- se na frequência de ocorrência de ngramas (unigramas, bigramas, trigramas, etc.), que consistem em grupos formados por n palavras contínuas.
Este tipo de abordagem, em geral, é composto por 3 etapas:
Construção de ngramas;
Contabilização da frequência dos ngramas;
Filtragem dos ngramas.
Inicialmente, a partir de o corpus definido, são construídos grupos formados por n palavras, também conhecidos como ngramas sendo que cada um desses é considerado como um candidato à expressão multipalavra.
Em a segunda etapa do processo, os conjuntos de palavras (ngramas) são contabilizados de acordo com sua frequência de ocorrência e organizados numa lista ordenada (em geral com o elemento mais frequente no topo).
Por fim, a lista de ngramas é filtrada de acordo com algum critério, de modo que apenas um sub-conjunto dos elementos considerados como mais representativos será extraído.
O critério mais comumente empregado é a frequência de ocorrência dos elementos que pode ser considerada de forma estática ou dinâmica.
A lista de ngramas pode, ainda, ser refinada de acordo com heurísticas que realizam tarefas como a remoção de artigos e símbolos indesejados, por exemplo.
Abordagens heurísticas (ou linguísticas), por sua vez, baseiam- se em características linguísticas das palavras como, por exemplo, rótulos gramaticais (Part-of-Speech).
Este tipo de abordagem costuma ser realizado de acordo com os seguintes passos:
Anotação morfossintática;
Extração de palavras candidatas;
Filtragem dos candidatos extraídos.
Inicialmente, os documentos são anotados (por ferramentas específicas) com informações morfossintáticas como, por exemplo, rótulos gramaticais (substantivo, verbo, etc.).
Em seguida, padrões sintáticos (&quot;substantivo+ substantivo», por exemplo) são utilizados sobre os rótulos atribuídos para a identificação e extração dos ngramas candidatos à entradas do vocabulário.
Por fim, os candidatos extraídos são filtrados de acordo com métricas e heurísticas para a seleção dos mais adequados a serem incluídos no vocabulário.
Assim como na etapa anterior, os candidatos podem ser pré-processados de acordo com heurísticas.
No entanto, neste tipo de abordagem, as heurísticas costumam utilizar informações linguísticas no lugar de estatísticas como a frequência, por exemplo.
Por fim, abordagens híbridas combinam diferentes aspectos das abordagens anteriormente apresentadas.
Uma das possíveis configurações (utilizada por o TTC TermSuite) é a utilização de uma abordagem linguística baseada em padrões gramaticais para a extração dos termos candidatos, seguida do cálculo de frequência destes para a realização da filtragem.
Posteriormente, as unidades léxicas extraídas são filtradas mediante um cálculo de termhood (baseado métricas específicas como TF/ IDF, por exemplo) para determinar o grau de relevância desses para um determinado domínio.
As unidades mais relevantes são considerados como termos do domínio.
O cálculo de Termhood mede o quanto um determinado termo é significativo para um domínio específico, sendo utilizado para verificar quais das palavras (e expressões multipalavras) podem ser considerados como termos do domínio em questão.
A identificação do vocabulário da área e de expressões multipalavras, pode ser realizada através de ferramentas extratoras de terminologia como, por exemplo o ExATOlp e o TTC Term Suite, utilizados neste trabalho.
As ferramentas mencionadas serão a seguir apresentadas.
Ferramentas Para a identificação do vocabulário de domínio e de expressões multipalavras foram utilizadas as ferramentas ExATOlp (português) e TTC TermSuite (inglês).
Cogitou- se, ainda, a utilização da ferramenta MWEtoolkit, no entanto não foram encontrados os padrões morfológicos necessários para a extração das expressões multipalavras, optando- se por inserir a utilização dessa na lista de trabalhos futuros.
ExATOlp O ExATOlp é uma ferramenta para a extração da terminologia de domínio a partir de corpora em português.
De entre os recursos linguísticos disponibilizados por esta ferramenta, destacam- se:
Lista de termos, juntamente com sua anotação morfossintática e de frequência;
Concordanciador que disponibiliza uma lista de todas as sentenças em as quais um determinado termo está contido;
Nuvem de conceitos (Concept Cloud), recurso visual que permite a visualização dos termos relevantes num ambiente em o qual o tamanho da fonte é proporcional à relevância desses;
Hierarquia de conceitos, apresenta os conceitos extraídos hierarquicamente dispostos numa árvore hiperbólica (hyperbolic tree).
De os recursos anteriormente apresentados, aquele com maior relevância para este trabalho é a extração das lista de termos, utilizado na ampliação dos dicionários monolíngues da língua portuguesa.
A ferramenta utiliza uma abordagem linguística, em a qual, um corpus anotado por um analisador sintático (parser) é investigado de acordo com padrões linguísticos e, para os candidatos extraídos são aplicadas heurísticas de refinamento e, posteriormente, filtros estatísticos baseados em frequência.
Por fim, a ferramenta realiza uma cálculo de termhood que leva em consideração a frequência do termo tanto no documento a partir de o qual este foi extraído quanto em documentos de diferentes domínios para determinar, assim sua especificidade.
Uma descrição mais detalhada do processo utilizado por a ferramenta pode ser obtida em.
TTC TermSuite O TTC TermSuite é uma ferramenta originalmente criada para a extração de terminologia bilíngue a partir de corpus comparável.
Esta ferramenta encontra- se inclusa no escopo do projeto TTC Project que tem por objetivo a extração de terminologia de domínios específicos, a partir de documentos comparáveis extraídos da Web.
Como parte de processo, o TTC TermSuite possui funcionalidades para a extração da terminologia monolíngue, utilizando uma abordagem híbrida em a qual padrões sintáticos são empregados para a extração de candidatos que posteriormente são filtrados de acordo com critérios como, por exemplo, frequência, especificidade, entre outros.
O processo de extração utilizado é composto por cinco passos:
Reconhecimento de candidatos a termos simples e compostos;
Cálculo de suas frequências relativas e especificidade no domínio;
Detecção de neoclassical words (palavras técnicas) a partir de o conjunto de palavras simples;
Agrupamento das variantes do termo (flexões);
Filtro de candidatos utilizando um limiar (treshold) que pode ser especificado tanto em relação a a frequência quanto a a especificidade num domínio.
Como o desenvolvimento da ferramenta em questão ainda encontra- se em andamento, sendo que o processo de validação ainda não foi realizado, dados em relação a o seu desempenho não puderam ser encontrados.
No entanto, este fato não impede sua utilização uma vez que a funcionalidade de extração monolíngue encontra- se implementada e é baseada em métodos conhecidos.
Avaliação de Léxicos Bilíngues Existem duas formas principais de avaliação para recursos linguísticos como léxicos bilíngues:
Intrínseca e extrínseca.
Essas podem, ainda, ser executadas de forma manual ou automática.
Em uma avaliação intrínseca, o léxico é avaliado de acordo com o conteúdo que apresenta (palavras e expressões multipalavras) mediante a utilização de alguma métrica que pode variar de acordo com o objetivo da avaliação.
Quando executada de forma manual, juízes humanos realizam a avaliação das entradas classificando- as de acordo com categorias predefinidas.
Esta avaliação pode ser binária (correta ou incorreta) ou ainda levar em consideração outras categorias como, por exemplo, parcialmente correta.
Já quando realizada de forma automática, o léxico induzido é comparado com um léxico de referência, sendo que as entradas do léxico induzido também presentes no léxico de referência são consideradas como corretas.
Em uma avaliação extrínseca o léxico bilíngue é utilizado em alguma aplicação de PLN (extração de informações multilíngue, por exemplo).
Em esta abordagem, a avaliação é realizada sobre os resultados da aplicação e, a partir desses, determina- se o quanto o léxico induzido contribuiu para a aplicação.
Assim como na avaliação intrínseca manual, a avaliação extrínseca é conduzida por juízes humanos que, no entanto, ao invés desses avaliarem o conteúdo do léxico, avaliam as saídas produzidas por a tarefa de PLN em a qual esse foi utilizado.
Por fim, numa avaliação automática extrínseca, assim como na intrínseca, o que é a avaliado são as saídas da atividade de PLN, no entanto, diferentemente da avaliação manual, esta é conduzida de forma automática empregando padrões de referência da saída avaliada.
A escolha entre uma abordagem intrínseca (utilizada neste trabalho) ou extrínseca depende diretamente do que se deseja avaliar.
Uma avaliação intrínseca avalia o léxico como um produto final, enquanto a avaliação extrínseca avalia a colaboração desse na realização de uma determinada tarefa.
Em este trabalho, como o objetivo da avaliação foi avaliar o desempenho do processo de extração proposto (diretamente relacionada ao vocabulário extraído), optou- se por a utilização de uma abordagem intrínseca, em a qual foi avaliada a identificação das equivalências multilíngues.
A metodologia utilizada encontra- se descrita na Seção 7.1.
Em relação a a escolha entre abordagens manuais ou automáticas, esta depende diretamente dos recursos disponíveis.
Avaliações automáticas dependem de padrões de referência (léxico, dicionários, listas, etc.), enquanto avaliações manuais dependem da disponibilidade de juízes humanos.
A principal vantagem de avaliações automáticas sobre manuais é a possibilidades de avaliar o impacto de modificações no processo de forma rápida e imediata.
No entanto, léxicos de referências para textos de domínios específicos são recursos escassos e sua construção pode ser custosa.
Já a avaliação manual depende da disponibilidade de juízes humanos e na existência de um nível de concordância aceitável entre esses (podendo esta ser mensurada por o cálculo de Kohens Kappa).
No entanto, a abordagem manual proporciona uma avaliação mais detalhada, podendo auxiliar na identificação de questões até então não investigadas, como em relação a problemas na identificação das expressões multipalavras ocorridos neste trabalho, descritos na Seção 7.2.
Como para o presente trabalho a alocação de juízes humanos era menos custosa do que a construção de léxicos de referência, optou- se por a realização de uma avaliação manual como apresentado no Capítulo 7.
A extração automática de vocabulário multilíngue é um tema bastante explorado na literatura relacionada à Tradução de Máquina (MT).
Em este capítulo serão apresentados os principiais trabalhos que embasaram a construção do processo proposto (Capítulo 6).
Este capítulo encontra- se organizado de modo que inicialmente são apresentados os trabalhos de Caseli e Nunes, na Seção 5.2 são apresentados o processo de extração de vocabulário multilíngue e o corpus propostos por Tiedemann e, na Seção relação destes trabalhos com o presente (Seção 5.4).
ReTraTos O ReTraTos é uma ferramenta que tem por objetivo a indução automática de léxicos bilíngues e regras de tradução, ambos representados num formato compatível com a plataforma de tradução de máquina Apertium.
De as publicações relacionadas ao ReTraTos, destaca- se a de Caseli e Nunes que trata especificamente da indução de léxicos bilíngues a partir de textos paralelos, objetivo esse diretamente relacionado ao presente trabalho.
Maiores informações sobre a indução das regras de tradução e detalhes da ferramenta podem ser obtidos em.
O processo proposto por Caseli e Nunes tem início numa etapa de pré-processamento dos documentos componentes do corpus.
Em essa, os documentos foram decompostos em suas sentenças formadoras (identificadas por rótulos) e essas foram organizadas no formato de uma sentença por linha (arquivo de saída).
Em a segunda etapa, os conjuntos de sentenças (um português e outro inglês) foram sentencialmente alinhados (de forma automática) mediante a utilização da ferramenta TCAalign, em conjunto com uma lista auxiliar de palavras cognatas.
Os resultados desta etapa foram manualmente verificados e os alinhamentos corrigidos foram utilizados na construção de uma lista de referências.
Em a terceira etapa foi conduzida a anotação morfológica das palavras componentes das sentenças.
Para cada palavra foram atribuídos rótulos gramaticais (POStag), formas canônicas e flexões de número e gênero.
O processo de anotação foi realizado através de ferramentas da plataforma Apertium (Lttoolbox).
Vale ressaltar que a ferramenta de análise morfológica utilizada é baseada em dicionários externos.
Os dicionário monolíngues utilizados para anotação dos documentos foram obtidos a partir de o repositório da ferramenta Apertium 1 e, posteriormente ampliados com entradas de outros dicionários.
Os dicionários foram ampliados a partir de entradas extraídas dos dicionários produzidos por o projeto Unitex2, passando a abranger 337.861 formas superficiais (forma por a qual uma palavra se apresenta no texto) para a língua portuguesa e 61,601 para a língua inglesa.
Em a quarta etapa do processo foi realizado o alinhamento lexical (a nível de palavra) dos documentos.
Para esta tarefa, foi utilizada a ferramenta LIHLA no alinhamento dos documentos (sentenças) gerados durante a segunda etapa, e utilizada de acordo com sua configuração padrão.
Os alinhamentos lexicais obtidos foram, ainda, submetidos ao algoritmo de simetrização (união) proposto por Och e Ney, em busca de maior precisão.
Em esta etapa, Caseli e Nunes ressaltam ter obtido uma precisão de 90,47% e uma cobertura (recall) de 92,34% (obtido após avaliação manual de 500 sentenças).
Por fim, as saídas dos alinhadores lexicais foram utilizadas para a indução dos léxicos bilíngues que, por sua vez, foram representados de acordo com os formalismos da ferramenta Apertium.
A Figura 5.1 demonstra um exemplo de saída do módulo de pós processamento.
O processo de indução é descrito na Seção 5.1.1.
Vale ressaltar que a indução dos léxicos bilíngues difere da construção de dicionários bilíngues probabilísticos.
Enquanto dicionários probabilísticos retornam a probabilidade duas palavras serem equivalentes, léxicos induzidos retornam padrões de tradução em os quais uma palavra, anotada com determinadas informações morfossintáticas, é considerada como equivalente de outra palavra (em outro idioma) quando essa encontrar- se anotada de uma forma específica (determinado conjunto de informações).
Para a avaliação do processo apresentado, Caseli e Nunes conduziram um experimento.
Em esse, dois corpus paralelos foram utilizados, um para os idiomas português (do Brasil) e Inglês, e o outro para os idiomas português (do Brasil) e espanhol.
O corpus português/ espanhol utilizado era composto por 18.236 sentenças sendo formado por 594.391 palavras para o português e 645.866 palavras para o espanhol, enquanto o segundo corpus era composto por 17.397 sentenças paralelas contabilizando 494.391 palavras para o português e 532.121 palavras para o inglês.
Os documentos utilizados foram levantados a partir de a revista Pesquisa FAPESP.
A avaliação do léxico bilíngue português-inglês gerado ao final do experimento utilizou uma abordagem manual intrínseca, devido a a padrões de referência (Golden Standards não se encontrarem disponíveis.
As 19.191 entradas do léxico foram classificadas em 8 classes de acordo com seus atributos.
De essas foram avaliadas somente aquelas cujos atributos fossem iguais para as entradas fonte e alvo ou que apresentassem pequenas diferenças em relação a estes (atributos mais específicos ou gerais).
Por fim, aproximadamente 10% das entradas resultantes da seleção foram avaliadas.
A escala de avaliação utilizada classificou as entradas avaliadas em três categorias:
Válidas: A parte fonte é uma possível tradução da parte alvo, considerando- se o sentido de tradução especificado;
Parcialmente Válidas (PV):
A especificação seria válida se alguma alteração nas informações morfológica ou no sentido da tradução fossem realizadas;
Não Válidas (NV):
A correspondência entre as duas partes não é válida.
A avaliação foi conduzida por dois juizes humanos.
Cada juiz avaliou manualmente 618 entradas sendo que parte dos conjuntos de avaliação era comum aos dois avaliadores.
Uma verificação do nível de concordância entre os juízes (medida por o cálculo de kappa) apresentou um valor de 0,48, indicando baixa concordância entre os avaliadores.
No entanto, devido a os tipos de discordância (principalmente relacionado à expressões multipalavras) encontrados, os juízes foram considerados aptos para o procedimento.
Processo de Indução O processo de indução dos léxicos bilíngues utilizado por a ferramenta ReTraTos é composto por 7 passos:
Criação de um léxico bilíngue para o sentido fonte-alvo;
Criação de um léxico bilíngue para o sentido alvo-fonte;
União dos léxicos criados nos passos anteriores;
Generalização das entradas do léxico bilíngue;
Tratamento de diferenças de gênero e número (opcional);
Tratamento de multipalavras;
Antes da execução dos passos anteriormente apresentados, é executado um passo de préprocessamento, em o qual os textos paralelos são lidos e armazenados em estruturas de dados específicas do processo.
Em o início do processo, são buscadas todas as possíveis traduções para uma determinada entrada do texto fonte (palavra ou expressão multipalavra), considerando as diferentes combinações de informações morfológicas para ambos os idiomas.
Após construída a lista de possíveis alinhamentos, a frequência de ocorrências desses é verificada, sendo que o par de maior frequência é considerado como a tradução mais adequada.
Este procedimento é adotado durante o primeiro e segundo passos, alternando apenas os textos fonte e alvo.
Em o terceiro passo, os alinhamentos produzidos nos dois primeiros passos (fonte-alvo e alvo-fonte) são unidos (mediante algoritmo de união de Och e Ney) e as ambiguidades são resolvidas.
Neste passo é realizada a filtragem de expressões multipalavras mediante sua frequência de ocorrência.
A frequência mínima para a inclusão de multipalavras pode ser manualmente definida por o usuário da ferramenta.
Em o quarto passo tenta- se generalizar entradas similares do léxico bilíngue, as quais possuam apenas um atributo (informações morfológicas) com valores diferentes.
Durante a generalização, estas entradas são unificadas e os possíveis valores para o atributo diferente são inseridos no mesmo campo, separados por o símbolo&quot;|».
As frequências de ocorrência das entradas agrupadas são somadas.
O passo 5 (opcional) é responsável por o tratamento de entradas em as quais o valor do atributo de gênero ou número não pode ser determinado de acordo com as informações contidas na entrada (palavras que possuem a mesma forma em diferentes contextos como thesis, por exemplo).
Para este tratamento, é necessário que o usuário forneça uma lista contendo possíveis valores dos atributos de número e gênero de acordo com um formato de representação específico.
Ainda durante este passo as entradas válidas para ambos os sentidos são divididas em duas novas entradas com uma indicação para o sentido de tradução.
Por fim, no passo 6 é realizada a impressão das entradas do léxico bilíngue no arquivo de saída.
Durante esta impressão, as entradas são formatadas de acordo com o formalismo dos dicionários de entrada da ferramenta Apertium.
Uplug e Opus Os trabalhos de Tiedemann diferem entre si no processo utilizado, no entanto, ambos possuem elementos relacionados a este trabalho.
O projeto Opus tem por objetivo suprir a comunidade científica com corpora paralelo para múltiplos idiomas e livremente acessível.
Além de corpora, o projeto também agrega um conjunto de ferramentas para processamento, anotação e gerenciamento deste tipo de recurso.
De entre os domínios dos corpora disponibilizados por o Opus, destacam- se o de legendas de filmes (construído a partir de o repositório open subtitles), biomedicina (Emea) e documentação de softwares (open source).
Antes de serem inseridos ao repositório os textos passam por um processo em o qual são pré-processados (de acordo com seu tipo), convertidos para o formato XML (eXtensible Markup Language), seguindo uma estrutura própria do Opus, e alinhados sentencialmente com seus textos equivalentes mediante a utilização da ferramenta Hunalign.
A etapa de pré-processamento varia de acordo com o tipo de texto a ser processado uma vez que as características de um documento contendo legendas de filmes é diferente das encontradas em documentos de software.
Entre as técnicas de pré-processamento aplicadas por Tiedemann, a utilizada em documentos do corpus de biomedicina (Emea) foi considerada a mais similar à empregada sobre os documentos do corpus apresentado no Capítulo 3 uma vez que os documentos de ambos os corpora possuem o mesmo formato de entrada (PDF).
Em Tiedemann conduz o alinhamento utilizando o Hunalign, já no Uplug (extrator automático de dicionários multilíngues) permite que seja utilizado o Hunalign, o GMA ou um alinhador padrão baseado no tamanho das sentenças.
Os textos (no formato XML) são então convertidos para o modelo de entrada de uma ferramenta de gerenciamento de corpus (Corpus Workbench) que disponibiliza um conjunto de funcionalidades para auxiliar na visualização de corpora como, por exemplo, concordanciadores multilíngues.
A maior parte das ferramentas de pré-processamento e conversão utilizadas por Tiedeman são provenientes do Uplug que apresenta um conjunto de ferramentas para a extração de dicionários bilíngues probabilísticos.
A principal diferença encontrada no Uplug é a forma por a qual este executa o alinhamento lexical, utilizando uma abordagem baseada em pistas (&quot;cues&quot;) que se baseia numa série de características da palavra (pistas) para a execução do alinhamento.
Uma descrição detalhada do projeto pode ser encontrada em.
Ha E Zhang Os trabalhos de Ha E Zhang são bastante similares, utilizando abordagens associativas para alinhamento lexical e tendo como objetivo principal auxiliar processos de extração terminológica.
Como corpus paralelo, Ha Utilizaram um subconjunto do MedlinePlus construído a partir de a Medline, considerada a maior biblioteca médica disponível.
O subconjunto utilizado era composto por 9.250 sentenças paralelas sendo essas constituídas por 31.498 palavras para o inglês e 30.344 palavras para o espanhol.
Zhang, por sua vez, utilizou um corpus composto por textos acadêmicos sendo este composto por 460.000 documentos para o idioma chinês e 130.000 documentos para o inglês, distribuídos entre 23 categorias com uma média de 4.733 registros paralelos por categoria.
Em relação a o processo de extração utilizado, Ha Inicialmente executaram uma etapa de pré-processamento em a qual os documentos originais foram convertidos para texto sem formatação (plain text) tendo sido removidas as quebras de linha para evitar problemas posteriores com alinhadores sentenciais.
Zhang não apresenta o procedimento de pré-processamento empregado.
Em a segunda etapa, ambos os trabalhos realizaram a identificação da terminologia de domínio.
Esta identificação foi realizada de forma individual para cada um dos idiomas envolvidos.
Em esta etapa, Ha Utiliza uma abordagem baseada em padrões morfossintáticos para a extração dos termos, enquanto Zhang utiliza uma abordagem baseada em palavras-chave.
Posteriormente, em ambos os trabalhos, foi calculado o termhood (especificidade de um termo em relação a um domínio) dos candidatos a termo previamente extraídos.
Para este cálculo, foram utilizados critérios como frequência e métricas como TF/ IDF (Term Frequency/Inverse Document Frequency), por exemplo.
A terceira etapa dos processos consistiu no alinhamento sentencial do corpus, procedimento para o qual Ha Utilizaram a ferramenta Trados WinAlign, enquanto Zhang não apresentaram a ferramenta utilizada.
Em a quarta etapa, tabelas de contingência foram construídas para os termos candidatos (extraídos nos passos anteriores), como apresentado na Seção 4.4.
A partir de as tabelas de contingência, métricas de associatividade foram utilizadas para a determinação equivalências multilíngues (pares de palavras).
Zhang fez uso de um conjunto de métricas composto por LogLikeliHood, Dice, X e Mi (Mutual Information.
Ha, utilizaram apenas LogLikeliHood.
Vale ressaltar que a métrica de LogLikeliHood é utilizada pois, segundo os autores previamente referenciados, é capaz de identificar associações de baixa frequência.
Em relação a a metodologia de avaliação utilizada por Han E Zhang, esta tem por objetivo avaliar a extração terminológica e não a extração de equivalências multilíngues.
Sendo assim, a metodologia em questão não foi considerada por este trabalho.
Relação com o Presente Trabalho O processo proposto neste trabalho (Capítulo 6), segue um conjunto de passos bastante similar aos dos trabalhos apresentados no decorrer deste capítulo, destacando- se a semelhança com o trabalho de Caseli e Nunes.
De as etapas componentes do processo, o pré-processamento e o alinhamento sentencial seguiram abordagens muito similares às empregadas nos demais trabalhos, tendo como principais alterações as ferramentas utilizadas.
O procedimento para a análise morfológica foi baseado nos trabalhos de Caseli e Nunes, Ha E Zhang.
De esses, a expansão dos dicionários e a escolha das ferramentas utilizadas basearam no trabalho de Caseli e Nunes, enquanto a utilização de termos mais específicos aos documentos do corpus foi baseada nos trabalhos de Ha E Zhang.
Em a etapa de alinhamento lexical, assim como em Caseli e Nunes e Tiedemann, ferramenta.
Por fim, para a indução dos léxicos multilíngues e geração dos dicionários no formato de entrada da ferramenta Apertium foi utilizada a ferramenta ReTraTos, proposta e utilizada no trabalho de Caseli e Nunes.
Em relação a a similaridade do presente trabalho com o de Caseli e Nunes, a principal diferença que pode ser apontada é forma por a qual a ampliação dos dicionários morfológicos é conduzida, sendo que o presente trabalho utiliza extratores automáticos de terminologia ao invés de listas previamente construídas.
Outra diferenças entre os dois trabalhos é relacionada ao domínio das palavras utilizadas para a ampliação dos dicionários.
Enquanto Caseli e Nunes utilizam dicionários de domínio geral, este trabalho utiliza palavras (e expressões multipalavras) mais específicas do domínio, extraídas por ferramentas de extração terminológicas.
Este capítulo apresenta o processo proposto para a extração de vocabulário multilíngue, cujo o objetivo é a extração de vocabulário multilíngue a partir de o corpus paralelo formado por documentos de software.
O processo segue as etapas apresentadas no Capítulo 4, sendo adicionadas uma etapa de pré-processamento (Seção 6.2) e outra de indução dos léxicos bilíngues (Seção 6.6).
Este capítulo encontra- se organizado de modo que inicialmente é apresentada uma visão geral do processo, seguida por a descrição das etapas de pré-processamento (Seção 6.2), alinhamento sentencial (Seção 6.3), análise morfológica (Seção 6.4), alinhamento léxico (Seção 6.5) e, por fim, a indução do léxico (Seção 6.6).
Visão Geral do Processo Em esta seção é apresentada uma visão geral do processo proposto, sendo demonstrada a interação entre as etapas apresentadas nas seções posteriores.
A Figura 6.1 apresenta o processo como um todo.
A entrada do processo consiste num corpus paralelo formado por dois conjuntos de textos equivalentes (paralelos) nos idiomas português (&quot;Corpus PT&quot;) e inglês (&quot;Corpus EN&quot;).
O corpus utilizado neste trabalho encontra- se descrito no Capitulo 3.
Os textos componentes do corpus são submetidos a uma etapa de pré-processamento em a qual são convertidos para texto puro (&quot;txt&quot;) e reorganizados no formato de uma sentença por linha (como descrito na Seção 6.2).
A segunda etapa do processo consiste no alinhamento sentencial do conjunto de documentos pré-processados.
Em essa, são estabelecidas as equivalências entre as sentenças dos documentos em português com seus correspondentes em inglês.
O alinhamento é realizado com os textos de cada conjunto ainda separados.
O processo de alinhamento encontra- se descrito na Seção 6.3.
A próxima etapa consiste na anotação morfológica, em a qual os documentos de cada um dos conjuntos produzidos são analisados e anotados com informações morfossintáticas como, por exemplo, rótulos gramaticais (Part-of-Speech Tags) e flexões de número e gênero.
Ainda durante esta etapa é realizada a identificação das expressões multipalavras (Multi-Word Expressions).
Esta etapa encontra- se descrita na Seção 6.4.
Após morfologicamente anotados, os arquivos fonte e alvo são utilizados como entrada para a etapa de alinhamento léxico, em a qual são determinadas as equivalências entre os símbolos (palavras, expressões multipalavras etc.) componentes desses.
O processo de alinhamento léxico é apresentado na Seção 6.5.
Como saída da etapa de alinhamento léxico são gerados dois arquivos, cada um contendo uma direção de alinhamento (português/ inglês e inglês/ português).
Esses são então simetrizados através do algoritmo de união proposto por Och e Ney e utilizados como entrada para a última etapa do processo, referente a indução de léxicos conduzida por a ferramenta &quot;ReTraTos».
A etapa de indução de léxicos consiste na extração de entradas bilíngues a partir de os arquivos de alinhamento, sendo estas compostas por palavras simples e expressões multipalavras.
O processo de indução é descrito na Seção 6.6.
Por fim, a etapa de indução dos léxicos gera como saída dois dicionários bilíngues, uma na direção português/ inglês e outro na direção inglês/ português.
Estes dicionários são gerados de acordo com modelo utilizado por o Apertium.
Como pode ser observado na Figura 6.1 os conjuntos de documentos formadores do corpus são submetidos à ferramentas extratoras de terminologia (ExATOlp e TTC TermSuite) com o objetivo de identificar o vocabulário relevante do domínio e utilizar- lo para a ampliação dos dicionários morfológicos empregados.
Uma descrição mais completa do processo proposto é apresentada nas próximas seções.
Pre-processamento Em esta seção serão apresentados os procedimentos de pré-processamento empregados na preparação do corpus construído para sua utilização em etapas posteriores do processo de extração do vocabulário bilíngue.
Esta etapa é constituída por três passos, a seguir apresentados:
Conversão dos documentos para texto sem formatação (plain text);
Remoção dos símbolos de formatação (eliminação do ruído);
Separação e organização das sentenças;
As próximas seções apresentam descrições detalhadas dos passos apresentados.
Conversão de Documentos Os manuais de usuário, componentes do corpus, foram coletados em três formatos distintos:
PDF (Portable Document Format), Html (Hypertext Markup Language) e &quot;txt «(texto sem formatação).
Em esta etapa, os documentos do corpus foram convertidos para arquivos de texto no formato &quot;txt «codificados em &quot;UTF-8», de acordo com os procedimentos a seguir apresentados.
Conversão de Arquivos PDF A conversão de documentos no formato PDF foi realizada através da ferramenta pdftotext (assim como em) configurada de acordo com os seguintes parâmetros:
nopgbrk: Remove marcadores de quebras de página;
enc UTF-8:
Define a codificação do arquivo de saída para &quot;UTF-8&quot;;
eol unix:
Define o símbolo para quebra de linha de acordo com o padrão Unix (também utilizado por distribuições Linux);
raw: O arquivo original é lido no formato de uma sequência de palavras (stream).
Uma descrição mais detalhada da utilização da ferramenta, bem como dos parâmetros utilizados e seus possíveis valores pode ser obtida no manual da ferramenta2.
O parâmetros &quot;raw «merece destaque pois delimita a forma por a qual o arquivo original é lido, sendo que, para o valor adotado, o texto é lido no formato sequencial desfazendo assim sua formatação original.
Tiedemann defende a utilização do parâmetro &quot;layout «(que tenta manter a formatação original do arquivo), relatando ter obtido melhores resultados durante a conversão de documentos.
No entanto, após alguns testes realizados sobre os documentos, optou- se por manter o valor &quot;raw», devido a este apresentar melhores resultados de acordo com o tipo de formatação apresentado por os documentos do corpus.
Para ilustrar a diferença entre os dois parâmetros utiliza- se como exemplo um texto formatado em duas colunas.
O parâmetro &quot;raw «converterá o texto para apenas uma coluna, enquanto a opção &quot;layout «manterá o formato de duas colunas.
O principal problema associado à opção &quot;layout», no contexto deste trabalho é que, ao manter a formatação original de duas colunas, as sentenças do texto são fragmentadas em várias linhas, dificultando o processamento.
Conversão de Documentos Html Inicialmente, as páginas Html componentes dos manuais foram capturadas com o auxílio da ferramenta wget 3 que disponibiliza um conjunto de funcionalidades para esta finalidade.
Documentos Html, diferentemente dos arquivos PDF, não precisam ter seu formato convertido.
No entanto, precisam ter a marcação Html (rótulos) removida, uma vez que esta não pertence ao texto original.
A remoção dos marcadores Html foi realizada através de funcionalidades (método Tratamento do Corpus A conversão de arquivos PDF para texto puro(».
Txt&quot;) frequentemente acrescenta ruído ao texto resultante.
Este ruído, em geral, se demonstra na forma de símbolos desconhecidos às ferramentas, utilizando codificações ou fontes não suportadas por essas.
Além de o ruído inserido durante a conversão dos arquivos PDF, existem outros símbolos, frequentemente pertencentes à formatação dos documentos como, por exemplo, sequências de pontos utilizados para reconstruir a estrutura de sumário nos documentos.
Remover este tipo de símbolos torna- se necessário pois esses tendem a causar problemas durante a leitura e processamento dos arquivo convertidos nas etapas posteriores do processo de extração (alinhamento léxico, por exemplo).
No entanto, vale ressaltar que o tipo de ruído (símbolos) varia, de acordo com o tipo de documento convertido, não sendo possível assim, a automatização completa do processo de remoção.
Antes que o processo de remoção fosse realizado, duas questões foram levantadas: (
a) Quais os símbolos a serem definidos como ruído; (
b) O quanto é aceitável modificar corpus original mediante sua remoção.
A questão da definição de símbolos como ruído foi resolvida com a criação manual de uma lista contendo símbolos já identificados como tal (mediante observação de conversões anteriores).
Exemplos de símbolos removidos podem ser visualizados na Tabela 6.1.
Em relação a o conteúdo a ser removido, foi definido que apenas símbolos relacionados à estrutura do documento (estilo de formatação visual) seriam removidos evitando assim, a remoção de informações relevantes ou a introdução de algum tipo de viés ao corpus.
Outra questão, posteriormente levantada, foi o momento em o qual os símbolos deveriam ser removidos.
Como mencionado no Capítulo 4, ferramentas de alinhamento sentencial frequentemente utilizam símbolos (pontuações e números, por exemplo) como âncoras (anchors), ou seja, símbolos comuns a ambos os idiomas que auxiliam nas identificação de sentenças correspondentes/ equivalentes.
Testes utilizando diferentes abordagens de remoção conduziram a uma estratégia, em a qual, parte dos símbolos são removidos antes do alinhamento sentencial e o restante após esse ser executado.
A Tabela 6.1 apresenta os símbolos removidos durante a primeira etapa da remoção.
Um dos tratamentos considerado de suma importância é a remoção dos símbolos utilizados para a quebra de linha(&quot;\ n», em ambientes Unix/ Linux)[ 48], especialmente em documentos convertidos a partir de o formato PDF.
Esta remoção corrige grande parte das quebras de linha incorretamente inseridas por o software de conversão que, frequentemente, quebram sentenças antes de sua finalização, prejudicando a identificação e separação posterior dessas.
Outras informações também removidas durante a primeira etapa da limpeza dos documentos foram os marcadores (bulletmarks) e os números das páginas, sendo que esses números não colaboram para o processo de alinhamento.
Durante a segunda etapa do alinhamento foram removidos os seguintes símbolos:&amp;,
%, $ ,+,*, utilizados, principalmente, na formatação do layout dos arquivos.
Esses foram eliminados após a observação de sua interferência negativa tanto no alinhamento léxico quanto na indução dos léxicos.
Vale ressaltar que a remoção dos símbolos deve ser realizada antes do alinhamento léxico, caso contrário corre- se o risco de interferir no alinhamento realizado.
A remoção dos símbolos foi realizada através de expressões regulares implementadas na linguagem de programação Python.
Separação de Sentenças Como última etapa do pré-processamento, os documentos foram decompostos em suas sentenças constituintes e os arquivos foram reorganizados no formato de uma sentença por linha, como exigido por os alinhadores sentenciais considerados.
A separação das sentenças (sentence detection ou sentence splitting), foi realizada através idiomas envolvidos, disponibilizados por a própria biblioteca.
A saída deste passo (que corresponde à saída da etapa) consistiu nos documentos do corpus convertidos em arquivos de texto sem formatação organizados no formato de uma sentença por linha.
Vale ressaltar que, diferentemente do trabalho de Caseli e Nunes, nenhum tipo de notação foi utilizada para demarcar os limites (início e fim) das sentenças.
Alinhamento Sentencial A partir de o conjunto de ferramentas apresentado na Seção 4.2, o Bilingual Sentence Aligner foi selecionado para a realização do alinhamento sentencial.
Testes iniciais realizados com as ferramentas levantadas demonstram que, apesar de a ferramenta selecionada ter obtido um desempenho muito similar às demais, essa se demonstrou mais adequada ao contexto (sem correções manuais).
O fato da ferramenta realizar apenas alinhamentos 1:1 não demonstrou ter causado impactos negativos significativos no resultado do alinhamento.
O Hunalign demonstrou uma vantagem significativa em relação a o tempo de alinhamento, no entanto, seu desempenho foi inferior às demais ferramentas.
Estima- se que o baixo desempenho tenha sido causado por a falta de dicionário para os idiomas envolvidos.
Como entrada, o Bilingual Sentence Aligner exige dois arquivos paralelos organizados no formato de uma sentença por linha, sendo que os arquivos gerados durante a etapa de préprocessamento foram utilizados.
O valor de &quot;threshold «(probabilidade mínima de alinhamento para a inclusão) padrão da ferramenta é de 0.75 no entanto, para este trabalho, foi adotado um valor de 0.85.
Testes preliminares demonstraram uma diferença muito pequena no número de alinhamentos produzidos devido a o aumento deste valor.
Como saída, esta ferramenta produz uma lista de arquivos sendo os mais relevantes aqueles identificados por o sufixo &quot;aligned «que correspondem às sentenças resultantes do alinhamento.
Sentenças para as quais não foram encontrados alinhamentos (vazias) foram automaticamente descartadas por a ferramenta.
Os arquivos alinhados encontram- se estruturados de modo que cada linha do arquivo».
Aligned &quot;corresponde diretamente à linha de mesmo número do arquivo «Aligned».
Sendo assim, a linha 3 do arquivo por exemplo, encontra- se alinhada à linha 3 do arquivo Como recomendado em trabalhos relacionados os arquivos foram alinhados de forma individual, ou seja, cada manual foi alinhado com seu correspondente paralelo sem que seu conteúdo fosse concatenado ao dos demais manuais de mesmo idioma.
Análise Morfológica De entre as ferramentas listadas na Seção 4.3, optou- se por a utilização do Apertium para a anotação morfológica das sentenças alinhadas.
Além de prover os níveis de anotação desejados, este conjunto de ferramentas suporta diferentes idiomas e permite que seus dicionários morfológicos sejam ampliados ou mesmo especializados para um domínio específico.
Sendo baseadas em dicionários externos, as ferramentas componentes do Apertium são capazes de anotar textos em diferentes idiomas, tendo- se, assim, um padrão de anotação para os idiomas envolvidos (sem necessidade de adaptação).
Em este aspecto, outra vantagem deste pacote é que dicionários para diferentes idiomas encontram- se livremente disponíveis no repositório do projeto.
Apesar de os dicionários disponibilizados serem de domínio geral, estes podem ser ampliados ou especializados mediante a inserção de termos de um domínio específico.
Esta especialização do dicionário, muitas vezes, se torna necessária (como no caso deste trabalho) para que as palavras componentes do vocabulário específico de um domínio sejam corretamente anotadas.
A Seção 6.4.1 descreve o processo de ampliação realizado neste trabalho.
O dicionário probabilístico utilizado por o rotulador morfológico (PoS-tagger) do Apertium foi obtido a partir de os mesmos pacotes linguísticos que os dicionários morfológicos originais, previamente mencionados.
O Apertium é, ainda, utilizado em trabalhos relacionados, o que permite que informações sobre sua utilização, otimização, desempenho e recursos sejam obtidas, além de validar sua utilização no contexto do trabalho.
Ampliação do Dicionário Como apresentado na Seção 4.3, o Lttoolbox (pacote do Apertium para análise morfológica) utiliza dicionários externos para realizar a anotação morfológica dos documentos.
Estes dicionários são compostos por palavras, juntamente com suas respectivas formas canônicas e flexões gramaticais (gênero e número).
Os dicionários padrão do Apertium podem ser obtidos a partir de o repositório do projeto5.
Os dicionários monolíngues encontram- se inclusos juntamente com os dicionários bilíngues nos pacotes de tradução.
Sendo assim, no pacote apertium-es-pt, por exemplo, encontram- se os dicionários morfológicos monolíngues dos idiomas português e espanhol.
Inicialmente, cogitou- se a utilização dos mesmos dicionários utilizados em Caseli e Nunes, no entanto, testes preliminares demonstraram que estes não continham palavras e expressões multipalavras consideradas importantes ao domínio, optando- se assim por a expansão dos dicionários padrão com o vocabulário considerado de domínio extraído do corpus construído.
Sendo que a identificação das expressões multipalavras é realizada nesta etapa (seguindo o processo padrão do Apertium), constatou- se a necessidade da ampliação dos dicionários com a inserção do vocabulário dos manuais de usuário, obtido através da utilização das ferramentas ExATOlp e TTC TermSuite.
O processo de extração deste vocabulário é apresentado na Seção 4.5.
Como apenas as versões compiladas dos dicionários utilizados por Caseli e Nunes foram obtidas, estes não puderam ser ampliados, sendo assim, optou- se por realizar a ampliação dos dicionários disponibilizados no repositório do Apertium.
Para a língua portuguesa, foi utilizado o dicionário «pt_ BR.
Dix &quot;proveniente do pacote «apertium-es-pt», enquanto para o inglês foi utilizado o dicionário «apertium-enca.
En. Dix &quot;contido no pacote «apertium- en- ca».
Durante a ampliação foram adicionados 78.062 formas superficiais ao dicionário da língua portuguesa, sendo esta adição composta por 41.757 expressões multipalavras e 36.307 palavras simples.
Já para o dicionário da língua inglesa esta adição foi menor, tendo sido adicionadas 10.408 palavras de as quais 6.466 expressões eram expressões multipalavras e 3.937 palavras simples.
Apesar de o número de entradas adicionadas ter sido inferior ao adicionado por Caseli e Nunes, as entradas adicionadas são mais específicas ao domínio, tendo sido extraídas a partir de os próprios documentos do corpus.
A estrutura de um dicionário do Apertium é composta, basicamente por três tipos de definição:
Alfabeto e conjunto de símbolos:
Conjunto de símbolos a serem encontrados no dicionário, de entre os quais destacam- se os rótulos morfológicos utilizados na anotação das palavras;
Paradigmas: Padrões de anotação que podem ser aplicados a diferentes palavras com o intuito de otimizar a organização do dicionário.
Por exemplo, sendo que as palavras carro e passo possuem a mesma flexão de número (introdução do s para o plural), cria- se um paradigma para esta flexão que passa a ser empregado por ambas as palavras, evitando sua duplicação;
Entradas: Em esta seção são declaradas as palavras (simples e compostas) em conjunto com suas formas canônicas, rótulos morfológicos (PoStags) e flexões como número (singular e plural) e gênero (masculino e feminino), entre outras.
A ampliação dos dicionários morfológicos padrão do Apertium consistiu na adaptação das saídas dos programas de extração de terminologia (previamente mencionados) para o formato padrão dos dicionários utilizados por esse.
Durante esta adaptação, os rótulos morfológicos utilizados por as ferramentas de extração tiveram de ser convertidos para o padrão utilizado por o Lttoolbox 6.
A notação utilizada por a ferramenta ExATOlp pode ser visualizada em:
Vale ressaltar que, uma vez que tanto a extração dos termos quanto sua inserção foram realizadas de forma automática, os dicionários resultantes não foram otimizado com a utilização de paradigmas.
A utilização deste tipo de recurso encontra- se listada como um dos possíveis trabalhos futuros.
A não utilização dos paradigmas pode implicar que varições das palavras (alguma flexão não prevista) não sejam reconhecidas, diminuindo assim a abrangência do dicionário.
No entanto, por o vocabulário ter sido extraído a partir de os textos paralelos empregados no processo, suas formas superficiais (forma por a qual a palavras se apresenta no texto) serão identificadas podendo, assim, seus equivalentes multilíngues serem extraídos.
Outra questão que deve ser ressaltada é que, nem sempre, a ferramenta de extração terminológica empregada fornecerá todos os dados necessários.
O TTC Term Suite, por exemplo, fornece apenas o rótulo morfológico (PoStag), de forma que apenas esta informação será utilizada para o vocabulário do domínio.
Apesar de existirem rotuladores morfológicos para a língua inglesa, estas ferramentas, em geral, não realizam o reconhecimento de expressões multipalavras ou mesmo a priorização dos sintagmas extraídos.
Sendo assim, preferiu- se utilizar o TTC TermSuite.
O TTC TermSuite baseia- se numa abordagem híbrida, sendo assim, a extração da terminologia consistiu na aplicação automática dos padrões sintáticos já definidos por a ferramenta precedido por a aplicação manual de filtros de frequência para os quais, baseado no trabalho de Lopes Utilizou-se uma frequência mínima de 5 ocorrências.
Mesmo o rótulo gramatical (PoStag) atribuído por o extrator pode não estar correto ou, ainda, não ser determinado (principalmente em expressões multipalavras).
Para esta etapa, estimasse que poderia ser utilizada a estratégia proposta por Tiedeman, em a qual três etiquetadores são utilizados num sistema de votação, sendo que o rótulo que tiver o maior número de votos é atribuído a palavra avaliada.
Esta abordagem segue como dica de melhoria para trabalhos futuros.
Expressões multipalavras, no entanto, devem ser conectadas de forma que a ferramenta Assim como em Caseli e Nunes, o símbolo «&quot;foi utilizado para conectar as palavras como em «mobile_ phone», por exemplo.
Após ampliados, os dicionários morfológicos foram compilados utilizando- se, para esta finalidade, a ferramenta lt-comp «pertencente ao Lttoolbox.
Em o processo de compilação os dicionários, até então definidos no formato XML, foram compilados em arquivos binários, identificados por a extensão».
Bin». Anotação Morfológica A análise morfológica foi então conduzida utilizando o` lt-proc «7 ferramenta componente do Lttoolbox, em conjunto com os dicionários morfológicos ampliados.
Por fim, o rotulador gramatical (PoS-tagger) do apertium-tagger foi utilizado para determinar o rótulo gramatical mais adequado para cada palavra, principalmente para casos em os quais mais de um rótulo encontrava- se disponível.
Estas duas ferramentas foram utilizadas em conjunto na forma de um pipeline (sequência) como demonstrado por o exemplo a seguir:
&quot;cat Txt| lt-proc morfologico\&gt;|
