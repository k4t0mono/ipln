No últimos anos, os depósitos turbidíticos vêm-se tornando cada vez mais importantes alvos de exploração e produção de companhias do setor petrolífero.

A maior parte das reservas de petróleo encontradas nas bacias marginais brasileiras são encontradas em águas profundas, nos turbiditos.

A exploração e o desenvolvimento desse tipo de reservatório requer que se entenda detalhadamente suas formas de camadas e sua distribuição faciológica no espaço tridimensional.

A modelagem numérica tridimensional de corpos sedimentares é uma técnica relativamente recente na exploração de petróleo, embora tenha sido objeto e instrumento de pesquisa de geocientistas nas duas últimas décadas.

Fundamentada em conceitos de Sedimentologia, Estratigrafia e de Geologia Estrutural, esta área vem tomando força nos últimos anos devido à necessidade de otimização das atividades exploratórias e de produção das empresas do setor petrolífero.

Os avanços computacionais fazem surgir novas metodologias para a interpretação geológica.

Neste contexto, surgiram algumas ferramentas importantes, hoje muito difundidas como aplicativos computacionais, como é o caso dos softwares SEISWORKS TM, STRATWORKS TM e STRATAMODEL TM da Landmark Graphics Company, que vêm sendo utilizados por algumas empresas do setor petrolífero, pois auxiliam nos processos interpretativos anteriores à modelagem, e posteriormente na própria construção do modelo 3D.

Eles disponibilizam algumas facilidades, tais como a interpretação e a integração de dados sísmicos, o georreferencimento de um objeto geológico em 3D, sua visualização geométrica interna e externa, bem como o mapeamento tridimensional, a análise geoestatística e operações matemáticas, entre outras.

O objetivo central deste trabalho é representar espacialmente uma parte de um dos reservatórios petrolíferos da Bacia de Campos, Estado do Rio de Janeiro, tirando o máximo proveito da informação contida nos dados.

Para isso, propõe-se uma metodologia que leva em conta a continuidade espacial de propriedades petrofísicas e atributos estratigráficos e estruturais, bem como modelos geológicos conceituais.

Um objetivo mais amplo consiste em fornecer realizações tridimensionais equiprováveis, simuladas, da estratigrafia e estruturas dos reservatórios para fins de modelagem de fluxo e gerenciamento da produção dos campos de petróleo.

Para atingir o objetivo proposto, este trabalho aplica uma metodologia de mapeamento 3D a sistemas turbidíticos canalizados de uma pequena área, e também restrita seqüência estratigráfica, no contexto da Megasseqüência Marinha Transgressiva do Cretáceo da margem continental brasileira.

Diante das limitações dos dados disponíveis, desenvolve-se um modelo sedimentológico simplificado do reservatório, suplementando os dados sísmicos e de poços através de estudos de análogos.

A primeira fase do trabalho consiste na consulta bibliográfica referente a turbiditos, Geoestatística, Geofísica, caracterização de reservatórios, levantamentos de dados publicados a respeito da Geologia da Bacia de Campos e sobre o campo estudado.

Também são feitos levantamentos bibliográficos sobre áreas das quais se possam extrair analogias, utilizadas como base para estudos de depósitos de fluxos gravitacionais de águas profundas.

Aplicam-se conceitos de Estratigrafia, Geologia Estrutural e evolução tectônica, a fim de obter um modelo numérico tridimensional que agregue de forma geologicamente coerente os dados de poços e de levantamentos sísmicos, obtidos diretamente do reservatório modelado, e os dados complementares de modelos geológicos, extraídos da literatura ou de análogos.

A fase seguinte consiste na análise e interpretações de dados cedidos pela Agência Nacional do Petróleo (ANP), relativos ao campo estudado, tais como descrições de testemunhos, perfis geofísicos e dados sísmicos.

Os dados existentes são reinterpretados principalmente através de conceitos da Sedimentologia e Estratigrafia de Seqüências, e de consultas a trabalhos publicados sobre a área em estudo.

Na fase de interpretação dos dados, utilizam-se recursos geoestatísticos para estimar correlação entre dados de poços e os atributos sísmicos, analisar a continuidade espacial e realizar simulações a partir das eletrofácies identificadas, a fim de minimizar erros de estimativa nos espaços não amostrados.

Por fim, a modelagem será concluída a partir dos dados interpretados, gerando seções interpretadas da área de interesse.

Posteriormente, será feita a entrada dos dados e a modelagem dos objetos geológicos com o uso do programa STRATAMODEL TM.

Nesta etapa, os dados geológicos interpretados em outros softwares compatíveis serão incorporados, formando objetos tridimensionais.

Modelo esquemático da integração dos dados sísmicos com dados de poços para simulação condicional, gerando mapas de riscos imagens simuladas.

Adaptado de. WOLF, 1996.

O Método Científico é uma seqüência de procedimentos envolvendo o estudo de sistemas naturais, adotada por pesquisadores, muitas vezes de forma intuitiva, visando a compreensão de seu funcionamento e, como conseqüência prática, a capacidade preditiva ou corretiva sobre os mesmos.

No entanto, Bacon, considerado o pai da filosofia científica, propôs uma estrutura básica no processo de construção de modelos preditivos de sistemas naturais.

Assim, o método científico pode ser descrito por 5 etapas. observação do fenômeno ( sistema físico).

Formulação de uma hipótese, ou de um modelo matemático.

Predição do comportamento do sistema, baseado nessa hipótese.

Validação do modelo.

Aplicação preditiva.

Etapas do Método Científico.

O fluxograma mostra as 4 fases descritas por Bacon que descreve o processo de construção de um modelo qualquer.

Partindo da observação do sistema real o pesquisador formula uma hipótese (ou um modelo matemático), faz as predições do sistema baseado nessa hipótese por dedução matemática ou lógica, e por último, realiza experimentações para testar a validade da hipótese formulada.

A primeira etapa consiste na observação do sistema natural.

Em geral, os sistemas naturais estudados pelas geociências encontram-se em escalas e níveis de complexidade tais que o pesquisador fica muito limitado em suas observações.

É impossível observar o sistema como um todo, com todas as suas características.

Ainda nesta primeira etapa, o pesquisador organiza o conjunto de dados obtidos e submete-o à chamada "Análise Exploratória", na qual já começa a visualizar relações entre os componentes de seu sistema.

A segunda etapa consiste na elaboração de um ou mais modelos para representar seu sistema.

Um modelo, a ser conceituado de modo mais preciso adiante, tem a finalidade de representar os aspectos do sistema natural que são mais relevantes para os objetivos da pesquisa.

Esta etapa tem a característica de uma inferência indutiva, isto é, de observações localizadas, ou fragmentadas, chega-se a uma representação global do sistema.

A terceira etapa consiste na resolução de cada modelo elaborado, com base na dedução matemática ou lógica, no sentido de prever, com base no modelo, o comportamento do sistema em locais ou momentos em que não é conhecido.

A última fase é a validação de cada modelo elaborado.

É a etapa de experimentação, para saber se cada modelo alternativo, com sua simplificação em relação ao sistema, reproduz satisfatoriamente as observações feitas no sistema real.

O método é iterativo.

Cada vez que a etapa de validação não retorna um modelo coerente com as características observadas no sistema real, em pontos onde o mesmo é conhecido, retorna-se à coleta de dados ou ao ponto onde pode ter havido uma falha.

Apenas atendendo aos critérios de validação é que um modelo pode servir ao uso preditivo.

Antes de conceituar modelo, deve-se entender o significado de sistema.

Denomina-se sistema um conjunto de componentes dinamicamente interligados dentro de uma fronteira, que pode ser aberta ou fechada.

As fronteiras de um sistema são ditas abertas quando o conjunto interage com o exterior, influenciando e sendo influenciado.

Caso contrário, a fronteira é dita fechada.

Na realidade, não existem sistemas naturais com fronteiras fechadas, pois sempre há interações com o meio adjacente.

O grande número de componentes, a complexidade das interações e a dificuldade de se prever as conseqüências de alterações em componentes ou na estrutura de um sistema dinâmico motivaram os pesquisados a desenvolver o conceito de modelo.

Modelo é uma representação simplificada de um sistema, contendo alguns de seus componentes básicos, sobre os quais se pretende realizar simulações para fins preditivos.

Naylor definem modelo científico como uma abstração de um sistema real, que possa ser utilizada com propósitos de predição e controle, tendo como finalidade a de permitir ao analista determinar em que proporções uma ou mais mudanças em determinados aspectos de um sistema, representado pelo modelo, poderão afetar o sistema ou parte dele.

Modelo, portanto, é um sistema artificial, concebido para representar de forma simplificada um sistema real.

Os modelos podem ser conceituais, físicos ou matemáticos.

Modelo conceitual é uma imagem mental de um sistema natural, podendo ser expresso na forma diagramática.

Nesta categoria se enquadram os blocos-diagrama, comuns em comunicações técnicas e científicas das geociências.

Os físicos são representações físicas de sistemas reais, como exemplo, os simuladores físicos (túneis aerodinâmicos, maquetes, etc).

Os modelos matemáticos são abstrações de sistemas reais, representados por expressões contendo variáveis, parâmetros e constantes.

Estes últimos são classificados em. probabilísticos e determinísticos.

O modelo matemático é probabilístico quando contém variáveis aleatórias.

No caso determinístico, ao contrário, a variável aleatória é ausente.

O processo de construção do modelo envolve as quatro primeiras etapas do método científico.

Num primeiro momento, são observados os fenômenos e selecionados os aspectos de maior relevância, de modo que o modelo não se torne tão complexo quanto o sistema natural.

Na Análise Exploratória, o pesquisador já começa a descobrir padrões e relações que o direcionam para a escolha de modelos.

Antes de ser utilizado preditivamente, um modelo deve passar pela etapa de validação, conforme explicado anteriormente.

Nesta etapa, costuma-se escolher entre modelos alternativos, recaindo a escolha na alternativa que otimize uma função objetivo.

Esta pode combinar, com distintos pesos, critérios tais como custos financeiros, ambientais e sociais, riscos materiais e pessoais e índices econômicos, além de possíveis aspectos estratégicos.

A Estatística contribui para a boa realização de todas as etapas do método científico, começando pelo plano de amostragem, ou coleta de dados, o qual deve dar o máximo de informação pelo mínimo custo.

Daí a necessidade de se proceder a uma revisão dessa disciplina, que constitui um dos alicerces da metodologia utilizada nesta tese.

Durante a etapa de mapeamento geológico, o geólogo de campo procura por afloramentos e estruturas que lhe possam indicar a continuidade e a disposição das rochas no subsolo.

Os afloramentos muitas vezes fornecem dados pontuais, que o geólogo utiliza para fazer interpretações e gerar o mapa geológico de sua área de trabalho.

O mapa gerado é uma representação da população, no caso todo o volume de rochas e seu arranjo espacial.

Como a população não se apresenta de forma visível em todos os pontos, o geólogo utiliza os afloramentos disponíveis, que representam a amostra, no sentido estatístico.

Segundo Bussab & Morettin, a "População é todo o conjunto de indivíduos (ou objetos), tendo pelo menos uma variável comum observável".

Segundo esses mesmos autores, "amostra é qualquer subconjunto da população".

As medidas relacionadas a uma população qualquer recebem o nome de parâmetros.

Um parâmetro é uma medida usada para descrever uma população.

Já as medidas relacionadas às amostras recebem o nome de estatísticas ou estimadores.

Isto significa que a mesma propriedade pode descrever um parâmetro ou uma estatística, dependendo de sua origem, seja da população ou de uma amostra.

Os valores numéricos que as estatísticas assumem na amostra são chamados de estimativas.

Medida de população e seus correspondentes na amostra.

Esquema simplificado da representação de população, amostra e amostragem de um dado qualquer.

Quando uma variável X, associada a uma população, toma valores de um intervalo ou de um conjunto discreto, a freqüência com que ocorrem subintervalos ou valores pontuais pode ser apresentada na forma de tabelas e gráficos conhecidos como distribuições de freqüência.

Numa amostra de tamanho n, o número de vezes com que ocorre um valor, ou o número de valores dentro de um subintervalo ou classe, é a freqüência absoluta do valor ou da classe.

A razão entre a freqüência absoluta e o tamanho da amostra é a freqüência relativa.

Assim, se n é o número total de observações, e ni é a freqüência absoluta na i-ésima classe, então a freqüência relativa. Em análise de dados quantitativos, há situações em que é necessário saber a proporção dos elementos que se encontram abaixo ou acima de um valor crítico ou de referência.

Para tal, usa-se a medida de freqüência relativa acumulada.

Segundo Bussab & Morettin, "dadas n observações de uma variável quantitativa e um número real qualquer, x, indicar-se-á por N o número de indicações menores ou iguais a x, e chama-se de função distribuição acumulada (FDA) a função definida por. Antes de conceituar probabilidade, convém abordar alguns conceitos preliminares, que se referem a experimentos aleatórios, espaços amostrais e eventos aleatórios.

Experimento aleatório é o "processo de coleta de dados relativos a um fenômeno que acusa variedade em seus resultados".

Distingue-se do experimento determinístico pela impossibilidade de se prever o resultado preciso de cada realização, por mais que se controlem as condições em que é operado.

Repetições do mesmo experimento aleatório em idênticas condições conduzem a uma variedade de resultados.

Exemplo. lançamento de um dado equilibrado sobre uma superfície plana horizontal, cem vezes.

Bussab & Morettin descrevem como espaço amostral "o conjunto de todos os resultados possíveis de um experimento, e tem o evento como subconjunto".

No exemplo acima, o espaço amostral é o conjunto? = {f1, f2, f3, f4, f5, f6 }, em que fk designa a face identificada com o número k? {1,2,3,4,5,6}.

Qualquer subconjunto de? é um evento aleatório.

Uma vez definido o objeto de estudo, o espaço amostral, deve-se então procurar entender como é o comportamento dos eventos na natureza, criando funções que descrevam a distribuição de freqüência relativa dos eventos aleatórios.

A probabilidade é o grau de certeza, quantificado na escala entre zero e a unidade, que o observador tem quanto à ocorrência de um evento aleatório.

Um evento impossível de ocorrer, na avaliação do observador, tem probabilidade nula.

Um evento, de cuja ocorrência o observador tem certeza, tem probabilidade unitária.

Há várias definições de probabilidade na literatura.

A primeira a ser descrita neste trabalho é a definição clássica.

Esta definição diz que a probabilidade de um evento ocorrer é a razão entre o número de casos favoráveis ao mesmo, no espaço amostral, e o número de casos possíveis, ou seja. se A é o evento de interesse, a probabilidade de A ocorrer, sendo representada por P, é dada por. Esta definição admite que todos os casos possíveis sejam equiprováveis e que se conheça completamente o mecanismo gerador dos resultados possíveis.

Não tem valor prático nas geociências.

A segunda definição é a freqüencial.

Esta definição baseia-se na regularidade estatística, quando um experimento é repetido um grande número de vezes, em condições semelhantes.

Sejam. A o evento de interesse, N o número de realizações do experimento aleatório e n o número de vezes em que A ocorreu.

Então. Esta expressão significa que a freqüência relativa do evento A aproxima-se da probabilidade de A, à medida que cresce o número de realizações do experimento.

Conseqüentemente, quanto maior o número de realizações, mais precisa é a estimativa da probabilidade de ocorrência do evento considerado.

Na exploração do petróleo, o índice de sucesso de poços exploratórios numa bacia pode ser usado como uma probabilidade (a priori) de sucesso de um novo poço exploratório na mesma bacia.

A terceira definição, probabilidade subjetiva, é muito útil em geociências, pois leva em consideração o nível de informação que o profissional tem em relação ao fenômeno estudado.

No caso da exploração de petróleo, cada poço perfurado traz novas informações que afetam a avaliação da chance de sucesso das perfurações subseqüentes.

Notação. Se A e B são eventos do mesmo espaço amostral, P[A|B] é a probabilidade de A ocorrer, dado que B já ocorreu.

Então, no caso da probabilidade subjetiva. A notação descrita anteriormente, P[A|B], representa uma probabilidade condicional.

A probabilidade condicional é a razão entre a probabilidade de ocorrência conjunta dos eventos considerados e a probabilidade de ocorrência (a priori) do evento condicionante. Dois eventos aleatórios podem apresentar dependência mútua, ou mostrar que são independentes entre si.

Dados dois eventos A e B, num espaço amostral?, diz-se que são independentes quando. P(A|B) = P e P(B|A) = P.

Significa que a probabilidade de um ocorrer, dado que o outro já ocorreu, é igual a probabilidade individual de A ou de B, para o primeiro e o segundo caso respectivamente.

Isso mostra que a informação sobre a ocorrência de um evento não altera a avaliação da probabilidade de ocorrência do outro.

Nas aplicações das geociências, costuma-se reconhecer dois tipos de variáveis. qualitativas e quantitativas.

As variáveis qualitativas são assim definidas por representarem atributos dos componentes de um conjunto de amostras do objeto pesquisado, os quais possibilitam a classificação dos objetos em categorias, ordenáveis ou não.

As variáveis qualitativas, por sua vez, subdividem-se em nominais e ordinais.

As variáveis nominais podem ser, por exemplo, cor, nome de rocha, fácies, etc.

Nas variáveis nominais, as categorias têm uma ordenação natural, como é o caso da escala de dureza dos minerais (escala de Mohs).

As variáveis quantitativas resultam de processos de contagem ou medição.

Subdividem-se em discretas e contínuas.

Num dado intervalo estratigráfico, o número de camadas de uma dada litologia exemplifica uma variável discreta.

A sinuosidade, a largura e o comprimento de canais formados por fluxos gravitacionais são exemplos de variáveis contínuas.

A conseqüência metodológica que advém da classificação de variáveis reside na conjugação dos seguintes fatos. (1) o nível de informação aumenta das qualitativas nominais para as quantitativas contínuas e (2) há métodos específicos para tratar cada tipo, tirando o máximo proveito da informação contida nos dados.

Geralmente, no tratamento de sistemas complexos, são feitas análises utilizando mais de uma variável de forma conjunta.

São chamadas de variáveis multidimensionais.

Os conjuntos multivariados (variáveis multidimensionais) podem ser analisados através de tabelas, matrizes de covariância, gráficos e diagramas de distribuição conjunta, de modo a facilitar a identificação de relações entre as variáveis.

Os diagramas de duas ou mais variáveis servem para análise, como também para mostrar se existe uma relação entre as variáveis envolvidas, ou seja, destacar a existência e a forma de dependência entre as mesmas.

É interessante destacar que essa dependência, quando do tipo linear, pode ser medida através de um único número, chamado coeficiente de correlação.

O coeficiente de correlação varia entre -1 e +1, sendo que, quanto mais próximo do valor 1, em valor absoluto, maior será a correlação entre as variáveis, e quanto mais próximo do valor 0, menor a correlação entre as variáveis.

É importante notar que dependência e correlação não são sinônimos.

Havendo correlação, as variáveis são dependentes, mas a recíproca nem sempre vale.

Pode haver uma forte relação não-linear entre duas variáveis com baixa ou nenhuma correlação linear.

O coeficiente de correlação experimental entre as variáveis X e Y é calculado pela seguinte expressão. Se a distribuição dos pontos amostrados no gráfico estiver disposta de tal modo a tomar a forma aproximada de uma nuvem elíptica alongada no sentido do terceiro para o primeiro quadrante, a correlação linear é elevada e positiva, isto é, quando uma variável cresce, a outra tende a crescer.

Se o alongamento for do segundo para o quarto quadrante, a correlação é negativa.

À medida que a nuvem de pontos se torna menos alongada, aproximando-se da forma circular, a correlação diminui.

É mostrada uma correlação de 62% entre duas variáveis (primária e secundária).

Quanto mais próximo do valor 1, maior será a correlação, e conseqüentemente, mais próximo será a nuvem de uma reta traçada a 45o a partir da origem (0,0).

Fonte. Imagem gerada pelo software de Geoestatística GSLIB.

As variáveis aleatórias são funções matemáticas que atribuem valores numéricos aos resultados possíveis dos experimentos aleatórios.

Esses valores podem ser inteiros ou reais, dando origem a variáveis aleatórias discretas e contínuas.

Uma variável aleatória (VA) discreta é representada pela função X, definida sobre um espaço amostral? e que toma valores num subconjunto enumerável de pontos do conjunto dos números inteiros.

Há várias formas de expressar esta função chamada variável aleatória. em forma de conjunto de pares ordenados para conjuntos limitados, pequenos. em forma de conjunto de pares ordenados genéricos, para conjuntos grandes ou enumeráveis, mas infinitos. É mostrada a relação dos conjuntos domínio e o contradomínio pela variável aleatória X.

Cada elemento w de? tem um correspondente X em RX, denominado imagem.

Subconjuntos de? também têm suas imagens em RX.

Ambos os subconjuntos em? e RX são considerados eventos aleatórios, tendo ambos a mesma probabilidade de ocorrer.

Portanto, os valores possíveis de uma variável aleatória têm uma probabilidade associada.

O conjunto de pares ordenados de valores da VA e respectivas probabilidades formam uma nova função, denominada função de probabilidade da VA discreta ou função massa de probabilidade.

No exemplo dado anteriormente, a VA X tem a seguinte distribuição de probabilidade. em forma de conjunto de pares ordenados para conjuntos limitados, pequenos. em forma de conjunto de pares ordenados genéricos, para conjuntos grandes ou enumeráveis, mas infinitos. tabela. Conjunto de números discretos x com suas respectivas probabilidades uniformes.

Distribuição de probabilidade da VA discreta X igual ao número da face livre após lançamento de um dado.

Cada distribuição de probabilidade pode ser resumida através de medidas, denominadas parâmetros.

Apresentam-se aqui apenas os mais usuais, que serão utilizados no desenvolvimento dos modelos apresentados neste trabalho, a saber. O valor esperado. Numa distribuição de probabilidade discreta, a dispersão numérica dos valores possíveis pode ser quantificada pela variância. O desvio padrão é a raiz quadrada da variância e tem a vantagem de ser expresso nas mesmas unidades da variável X.

Se X é uma VA discreta, dá-se o nome de Função de Distribuição Acumulada (FDA) a. Existem vários modelos de distribuição de probabilidade para VAs discretas, porém, neste trabalho, serão apresentadas apenas as distribuições uniforme e de Bernoulli, por serem largamente empregadas nos procedimentos geoestatísticos.

Dos vários modelos, a distribuição é o caso mais simples.

A distribuição de VA X será uniforme se. O gráfico mostra a função distribuição de probabilidade.

Em a função de distribuição.

Fonte. Morettin & Bussab, Estatística Básica, 1997.

Esta distribuição descreve a probabilidade de ocorrência de situações binárias, ou seja, a probabilidade de um evento ocorrer ou não, de haver sucesso ou haver fracasso, sim ou não.

Variável aleatória discreta X, toma valores 0 para o caso de ocorrer fracasso, e 1 para ocorrência de sucesso.

Sua função de probabilidade é. Distribuição de Bernoulli.

Acima é mostrado o sistema binário, o evento ocorre ou não.

A probabilidade de um evento não ocorrer, ou seja x = 0, é 1-p, enquanto que a probabilidade deste evento ocorrer, x = 1, é p.

As variáveis quantitativas contínuas tomam valores dentro de intervalos de números reais, assim como a própria definição de variáveis quantitativas.

Contudo, estão associadas a modelos probabilísticos.

Resultam de medidas de comprimento, massa, tempo e concentrações, bem como de números puros, tais como proporções e porcentagens.

Não existe uma função de probabilidade semelhante à função massa de probabilidade, para variáveis contínuas.

Não faz sentido calcular a probabilidade de valores pontuais de uma VA contínua.

Para caracterizar o aspecto probabilístico, neste caso, define-se a função densidade de probabilidade.

Uma função pode ser utilizada como função densidade de probabilidade da VA X, se satisfizer ambas as condições a seguir. A função de densidade não fornece a probabilidade de um evento aleatório diretamente.

Para uma VA contínua, só faz sentido calcular a probabilidade de eventos representados por intervalos do domínio de X.

Para fornecer probabilidades diretamente, é necessário definir a Função de Probabilidade Acumulada, como segue. Extraindo-se ao acaso um valor x da VA X, a probabilidade de x pertencer ao intervalo (a,b), simbolizada por P[a = X = b], é igual à área sob a curva no intervalo (a,b), tendo a e b como pontos distintos do domínio de X (abscissa). Assim como no caso das variáveis discretas, existem diversos modelos elaborados para variáveis contínuas.

Descreveremos apenas os mais utilizados nos procedimentos geoestatísticos. as distribuições uniforme, normal e lognormal.

Fonte. Fonte. Morettin & Bussab, Estatística Básica, 1997.

Trata-se da distribuição de probabilidade mais importante das variáveis aleatórias contínuas.

Diversos fenômenos naturais, assim como procedimentos experimentais, como é o caso dos erros de medida, conduzem a distribuições que se aproximam da normal.

Um dos teoremas fundamentais da estatística, o Teorema do Limite Central, diz que, se a flutuação total de uma certa variável aleatória decorrer da somatória das flutuações de muitas variáveis independentes e de importância aproximadamente igual, sua distribuição tenderá para a normalidade, não importando qual seja a natureza das distribuições das variáveis individuais.

A variável aleatória X tem distribuição normal, com parâmetros µ e s é normal se sua função densidade de probabilidade for. A variável aleatória X com distribuição normal é denotada por N(µ, s2).

A probabilidade em cada intervalo é igual à área sob a curva, naquele segmento.

O cálculo dos valores de probabilidade para qualquer intervalo de X na distribuição normal é obtido através de sua transformação em evanto equivalente na distribuição normal padrão, cuja média é zero e cujo desvio padrão é unitário.

A VA normal padrão Z, denotada por Z. N(0,1) encontra-se tabulada em textos de Estatística e programada na maioria dos pacotes estatísticos computacionais conhecidos.

Para transformar um intervalo (x1, x2) pertencente a X num evento aleatório equivalente (z1, z2) de Z, utiliza-se a transformação de padronização, expressa abaixo. Uma VA X tem distribuição Lognormal quando a VA Y=lnX tem distribuição normal, isto é, Y. N(a,ß), onde a é a média dos logaritmos de X e ß é o desvio padrão dos logaritmos de X.

A função densidade de probabilidade da distribuição lognormal é. Este tipo de distribuição é muito comum em geociências.

Sua função densidade de probabilidade tem forma assimétrica com cauda alongada à direita, caracterizando uma assimetria positiva.

Significa que apresenta alta freqüência de valores baixos e poucas medidas com valores muito altos, quando comparados aos valores da classe mais freqüente.

Diversos fenômenos conhecidos nas geociências, tais como a distribuição de tamanhos de grãos em depósitos sedimentares, tamanhos de campos petrolíferos descobertos em bacias sedimentares e teores de elementos químicos de baixa concentração no solo, apresentam este tipo de distribuição.

Histograma com distribuição lognormal.

A Geoestatística é um ramo da Estatística Aplicada, também chamado de Estatística Espacial, que se caracteriza pela modelagem da continuidade espacial de fenômenos naturais.

Segundo Matheron, dentro da construção de um modelo matemático, o primeiro nível de abstração consiste em representar a interpretação de um fenômeno natural e regionalizado através de uma função numérica, denominada variável regionalizada (VRe).

Variáveis regionalizadas, segundo a explicação de Yamamoto, são aquelas que representam fenômenos referenciados geograficamente, e que foram introduzidas para descrever quantitativamente variações espaciais de jazidas minerais.

Trata-se de uma função numérica pertencente ao conjunto Rn, que representa fenômenos espacialmente contínuos.

A variável regionalizada z é uma variável mapeável como, por exemplo, uma superfície topográfica ou o topo de uma formação de rochas sedimentares.

Quando se trata de uma variável z com distribuição geográfica em superfície, com os pontos de observação, representados por um par de coordenadas (x,y), o domínio reduz-se a um subconjunto do R2.

Neste caso, a notação de um valor específico de z no ponto (u1) fica. A VRe z = z(x,y) tem um valor único em cada ponto de seu domínio.

Entretanto, após uma campanha de amostragem, que corresponde à primeira etapa do Método Científico, torna-se conhecida em apenas alguns pontos.

Para ser estimada em pontos não amostrados (terceira e quinta etapas do método científico, descrito anteriormente), será necessário representá-la através de um modelo probabilístico, que quantifica a incerteza em todos os pontos do domínio de interesse.

Em cada ponto = (x,y) em que z é desconhecida, define-se a variável aleatória local Z = Z(x,y), cuja distribuição quantifica o nível de incerteza sobre seu valor único, porém desconhecido.

Sua distribuição é condicionada às informações dos pontos vizinhos, sendo que, quanto maior for o nível de informação, menores serão as incertezas em torno do ponto estimado.

O valor verdadeiro de z representa, portanto, um atributo da população, enquanto que o valor estimado da variável aleatória local representa um estimador daquele valor desconhecido.

O conjunto de variáveis aleatórias locais sobre todo o domínio de z é chamado de função aleatória (FA).

Para cada par de pontos e (u+h), separados pelo vetor h < a, e pertencentes a um subconjunto do domínio de z, as correspondentes VAs locais Z e Z(u+h), não são independentes, mas ligadas por uma correlação que exprime a continuidade espacial da VRe.

A distância a que limita a existência dessa correlação chama-se alcance ou amplitude, e constitui um dos principais parâmetros utilizados pela Geoestatística para quantificar a continuidade especial da variável z na direção do vetor h.

Uma das funções mais utilizadas na Geoestatística para representar a continuidade espacial da FA Z na direção do vetor h é o chamado semivariograma. O semivariograma representa a semivariância da diferença da mesma propriedade Z medida em pares de pontos e (u+h), separados pela distância h.

Quanto mais próximos os pontos, menor a variância da diferença.

Portanto, para valores de h menores que o alcance, o semivariograma é uma função crescente, para fenômenos ditos "de transição", descritos a seguir.

Semivariograma teórico de um fenômeno transição, ilustrando o alcance e o patamar, onde se estabiliza a semivariância.

Fonte. Yamamoto, J.

K.

Avaliação e Classificação de Reservas Minerais, 2001.

Se a função aleatória tem a mesma distribuição em todos os pontos de uma região, diz-se que é estacionária.

Armstrong, explica que uma variável ou função aleatória é estacionária quando sua distribuição não varia sob translação.

Existem 4 ordens de estacionariedade.

Contudo, neste trabalho será descrita apenas a hipótese de estacionariedade de 2a ordem.

Uma FA é estacionária de segunda ordem se. E{Z } não depende do ponto x, então E{Z } = µ.

Se, para cada par de variáveis aleatórias [Z, Z(x+h)], existe a covariância, esta depende apenas do vetor h, sendo. a estacionariedade da covariância implica a existência e a estacionariedade da variância e do semivariograma.

Fenômenos que satisfazem esta hipótese são chamados fenômenos de transição, e caracterizam-se por um semivariograma com patamar.

Segundo, o variograma é a ferramenta básica que permite descrever quantitativamente a variação no espaço de um fenômeno regionalizado.

Campozana descreve a análise variográfica como a etapa mais importante da caracterização espacial de uma variável regionalizada.

O variograma é uma forma numérica de representar a continuidade espacial de um fenômeno natural, como a continuidade de camadas sedimentares ou tipos de solos.

Leva em consideração a anisotropia e as feições estruturais do meio.

A variabilidade, ou correlação espacial, de uma função aleatória, num determinado espaço geográfico, é caracterizada pela covariância C ou pelo semivariograma.

Admitida a hipótese de estacionariedade de segunda ordem, o semivariograma e a covariograma são complementares em relação à altura do patamar. onde C(0) é a altura do patamar, que corresponde à variância a priori de Z, isto é, a variância dos valores de Z quando se ignoram as suas posições espaciais.

Semivariograma hipotético de um fenômeno de transição, ilustrando sua relação com o covariograma C.

O semivariograma experimental é definido sobre os dados amostrais da seguinte forma. Componentes do Variograma. No gráfico do variograma? versus h, chama-se alcance ou amplitude a distância a partir da qual as amostras deixam de ter correlação e tornam-se independentes umas das outras.

Certos variogramas apresentam descontinuidades na origem.

Isto pode ocorrer devido a erros de amostragem, imprecisões de medida ou microvariabilidade local.

Essas descontinuidades na origem recebem o nome de efeito pepita.

Existem outros comportamentos que o variograma pode apresentar junto a origem, além do efeito pepita, como o linear e o parabólico.

O comportamento linear apresenta uma forma retilínea, tangente e obliqua à origem.

O comportamento parabólico representa alto grau de continuidade espacial do fenômeno estudado.

Um bom exemplo de deste tipo são as camadas de carvão.

Um outro fator de importância relevante na análise estrutural é a anisotropia.

A anisotropia exerce grande influência na continuidade espacial dos fenômenos naturais, pois, num meio anisotrópico, as propriedades físicas do meio são diferentes ao longo de direções distintas.

Isso requer que a análise do variograma se faça em diversas direções, para identificar os eixos principais de uma elipse ou de um elipsóide, conforme a dimensão do domínio, para representar a anisotropia.

A anisotropia pode ser zonal, geométrica ou uma combinação de ambas.

Na anisotropia geométrica, o patamar é constante e os alcances são distintos.

Na anisotropia zonal, o alcance é único, mas com patamares distintos em diversas direções.

É mista quando a análise resulta em variogramas com patamares e alcances distintos em direções distintas.

Os tipos de anisotropias.

Segundo Campozana, existem 3 tipos de semivariograma para cada variável regionalizada, que são experimental ou observado.

O verdadeiro e o teórico.

O semivariograma experimental é gerado a partir do conjunto de dados disponíveis das variáveis locais.

O semivariograma verdadeiro é aquele que representa a variável regionalizada, contudo é desconhecido.

O terceiro é o semivariograma teórico, que é descrito por funções matemáticas, as quais são utilizadas para ajustar o semivariograma experimental.

Os modelos variográficos teóricos podem ser divididos segundo o patamar em dois grupos, os que atingem um patamar, sendo coerentes com a hipótese de estacionariedade de 2a ordem, e os que não têm patamar.

Para os primeiros, pode-se fazer uma subdivisão em 3 categorias.

Modelos com comportamento linear próximo à origem.

Modelos que apresentam comportamento parabólico junto à origem.

Modelos de comportamento constante, o efeito pepita.

Os modelos com patamar são Gaussiano. na origem tem comportamento parabólico, por isso representa fenômenos altamente contínuos.

Tende ao patamar assintoticamente, atingindo 0,95 c no alcance prático A= a 31/2.

Esférico. é o modelo mais comum, apresenta comportamento linear próximo à origem.

Sua expressão é. O parâmetro a é o próprio alcance.

O parâmetro c é o patamar.



Exponencial. também apresenta comportamento linear próximo à origem.

Seu alcance prático é igual a A = 3a.

Este modelo tende ao patamar c assintoticamente.

Atinge o valor de? =? = 0,95c no alcance prático.

Sua expressão é. 4.

Efeito buraco. fenômenos cíclicos, que são comuns em Geologia, podem afetar de forma significativa o variograma causando depressões.

Campozana explica que isso ocorre pelo fato de que, a partir de uma distância h correspondente a um ciclo, os pontos começam a ter correlação com outros ciclos.

Efeito pepita. corresponde a um fenômeno puramente aleatório, sem correlação entre os pontos vizinhos. Modelos experimentais de variogramas.

Modelo variográfico gaussiano, modelo esférico, modelo esférico com efeito pepita, e efeito pepita puro.

Segundo Deutsch & Journel, a krigagem inicialmente era utilizada como um estimador de média local para pontos não amostrados.

Atualmente, porém, na versão indicatriz, vem sendo usada também para construir modelos probabilísticos de distribuição de incertezas dos valores dos atributos nos pontos desconhecidos.

A krigagem é um método de interpolação geoestatístico, não tendencioso, de mínima variância, que se baseia na análise e na modelagem da variabilidade espacial do atributo, a partir de um conjunto de amostras.

Trata-se de um método que permite a estimação do valor desconhecido Z*(uo) associado a um ponto, área ou volume, a partir de um conjunto de dados amostrados vizinhos, representados pela variável Z(ui).

O estimador de krigagem é uma "coleção de técnicas de regressão linear generalizadas para minimizar a variância do erro de estimação a partir de um modelo de covariância pré-definido".

A estimação do valor médio Z*(uo), utilizando as informações dos atributos numa área vizinha, é dada pela fórmula. A krigagem nada mais é do que uma média ponderada que leva em consideração o número de amostras envolvidas, a distância entre os pontos conhecidos e o ponto a ser estimado, o arranjo espacial das amostras dentro do raio de busca e a continuidade espacial da propriedade através da variografia.

Contudo, há vários tipos de krigagens que são utilizados em distintas situações.

A krigagem simples necessita do conhecimento da média global da propriedade sendo estimada.

Para o cálculo dos pesos?i, deve-se assumir a seguinte premissa. A FA é estacionária de 2a ordem, o que significa que a média µ = µ, ou seja, a média é igual em todo o espaço e que a covariância é dependente apenas do vetor de separação h.

Considerando-se um ponto Z a ser estimado a partir de n vizinhos conhecidos, o sistema de krigagem simples é expresso pelo seguinte conjunto de n equações. Outra versão da Krigagem, chamada Ordinária, não necessita do conhecimento prévio da média estacionária.

A média local é calculada através de informantes vizinhos.

Neste caso, a obtenção de um estimador não tendencioso requer que a soma dos pesos seja unitária. Onde. C(ui, ua) é a covariância entre as amostras ui e ua.

C(u, ua) é a covariância entre os valores de Z observados em ua e u.

O multiplicador de Lagrange, necessário para minimização do erro e associado com a restrição A modelagem de atributos geológicos como litologia, geometria de estratos sedimentares, dentre outros, está longe de ser uma questão de simples solução.

É um meio complexo e o profissional conta com poucos dados para resolvê-la.

Então, a simulação aparece como ferramenta útil que integra o conhecimento do profissional com técnicas numéricas.

A simulação probabilística é um processo de construção de realizações alternativas equiprováveis, das variáveis aleatórias que compõem um modelo de uma função aleatória.

Trata-se de uma técnica numérica de geração de mapas, com distribuição de probabilidade, na qual os mapas gerados honram os dados, o histograma e o variograma utilizados como entrada.

O resultado da simulação é uma distribuição de probabilidade da variável de interesse nos pontos simulados dentro de uma malha definida.

A simulação pode ser usada para auxiliar no estudo de interações complexas, na tomada de decisões, na estimativa de distribuições de probabilidades, etc.

No desenvolvimento de reservatórios de petróleo, a simulação torna-se muito útil na caracterização espacial de atributos, a princípio não "mapeáveis" entre os poços.

A simulação seqüencial condicionada usa a função distribuição acumulada de probabilidade com intervalo [0,1] para obter os valores Z.

Este método para geração de números aleatórios é conhecido como Método da Transformação Inversa ou Método de Monte Carlo.

O Método da Transformação inversa ou de Monte Carlo utiliza a função distribuição acumulada, gerada através da distribuição condicional local.

Para cada ponto simulado, é sorteado um número p no intervalo [0,1] que será a probabilidade acumulada naquele ponto.

Através da inversa da distribuição acumulada, obtem-se o valor simulado zs, onde. Método de Monte Carlo.

Fonte. Campozana, F.

P.

Modelagem Probabilística e Simulação de Reservatórios.



O condicionamento da simulação considera os dados amostrais originais e também os valores pré-simulados dentro da vizinhança de u.

Abaixo, segue o algoritmo da simulação seqüencial condicionada, para obtenção de valores em n posições não amostradas. definir a malha.

Definir a função densidade acumulada representativa de toda área de interesse.

Normalizar os dados da função densidade acumulada Fz barb2right Fzn(Zn).

Definir um caminho aleatório de visita em cada nodo grid.

Para cada nó u(x,y), reter um valor específico dos informantes originais e dos previamentes simulados.

Krigar os dados originais e simulados para a determinação da função densidade acumulada condicionada (ccdf) em cada nó do grid.

Usar Monte Carlo (sortear um valor simulado a partir da ccdf).

Adicionar o valor simulado na base de dados.

Realizar os passos anteriores em cada nó seguinte.

Retornar os valores Fzn(Zn) barb2right Fz Em cada realização pelo procedimento de simulação seqüencial condicionada, obtém-se, para cada posição u da malha, um valor numérico para a variável aleatória.

O conjunto de valores obtidos para uma localização u, forma a distribuição condicionada local.

Esquema representativo dos valores equiprováveis obtidos pela simulação seqüencial entre dois poços (barras verticais amarelas).

Os depósitos turbidíticos são gerados por fluxos gravitacionais de sedimentos, em meio a correntes de turbidez.

São chamados de fluxos gravitacionais de sedimentos quando, na mistura de fluido com sedimentos, estes últimos carregam o fluido pela ação da gravidade.

Turbiditos são depósitos resultantes de correntes de turbidez.

Segundo Walker, essas correntes de turbidez são causadas por diferença de densidade, ocorrendo principalmente pela presença de sedimentos clásticos de granulometria variada, sob ação da gravidade, em meio aquoso ou subaéreo.

O mesmo autor acrescenta outros fatores importantes que podem influenciar o fluxo turbulento, como a diferença de salinidade e a temperatura.

Existem vários fatores que podem iniciar correntes de turbidez, como terremotos, descargas de sedimentos fluviais, tempestades e correntes de fundo.

Os modelos de facies são descrições baseadas nos conjuntos de atributos litológicos e estruturas sedimentares que tem por finalidade auxiliar na classificação do depósito.

Os modelos de fácies de depósitos turbidíticos iniciaram-se na década de 50 com o trabalho de Kuenen & Miglione, que, através de observações de depósitos de Flysch, postularam que uma corrente de turbidez é capaz de formar depósitos com granulometria ascendente.

Em 1962, Bouma estudando o arenito Anott, na França, constituiu o primeiro modelo de fácies turbidítica.

O modelo de Bouma ficou conhecido como o modelo "Clássico de Fácies" e alavancou o estudo de novas propostas de padrões de faciológicos.

A seqüência de Bouma, como ficou conhecida, é caracterizada por seqüência gradacional contendo 5 camadas, segundo a definição litológica.

Essas camadas, da base para o topo, foram denominadas de camadas A, B, C, D e E.

A camada A é composta por arenito grosso sem estrutura.

A camada B por arenito médio com laminação plano-paralela.

A camada C por arenito fino com climbing ripples, e D e E são rochas hemipelágicas.

Outros autores que criaram modelos geológicos amplamente aceitos dentre os profissionais de geociências, foram Mutti e Ricci Lucchi.

Em 1972, Mutti e Ricci Lucchi criaram um modelo faciológico que variava de A a D, porém mais subdividido.

Os autores descreveram as fácies com a seguinte nomenclatura. A1, A2, B1, B2, C1, C2, D1, D2, D3.

Onde. A são conglomerados.

B areias de granulomentrias grossa e média.

C areias média e fina.

D areias muito finas e muito finas com ripples.

A problemática é que as fácies B e C não caracterizavam o depósito como fluxo turbidítico e poderiam ser encontrados em outro tipos de depósitos.

Em 1983, Fisher desenvolveu uma teoria de transformação de fluxos gravitacionais.

A teoria diz que as mudanças no comportamento de fluxos gravitacionais ocorrem entre os estados laminares e turbulentos, e que podem ocorrer mais de uma vez e de modos diferentes durante a movimentação.

Um fluxo pode ser laminar, entrar em turbulência e voltar a ser laminar.

Por vezes, durante a perda de energia do fluxo turbulento, podem ocorrer fluxos laminares e turbulentos simultaneamente.

Em 1992, Mutti, com base no trabalho de Fisher, reformulou seu modelo de fácies.

Criou uma nova classificação de facies turbidítica segundo o decréscimo de energia, que vai de F1 até a F9.

Assim como na Seqüência de Bouma, o modelo considera uma desaceleração no fluxo registrada nos depósitos e estruturas formadas pelas alternâncias entre fluxos turbulentos e laminares.

As fácies e os processos relacionados de Mutti, resumidamente, são os seguintes. As fácies de Mutti descritas por Della Fávera.

Mutti fizeram algumas mudanças.

Introduziram o conceito de fluxos eficientes na deposição turbidítica e retiraram as fácies F4 e F7.

Contudo, os modelos não eram universais.

Isto levou ao COMFAN (Comitê para estudos de Leques Submarinos), 1982, à conclusão que não existe modelos turbidíticos, sendo cada caso um caso.

Os primeiros modelos sedimentológicos surgiram no início da década de 70 a partir de descrições de afloramentos.

O modelo de sedimentação de turbiditos inicialmente proposto por Mutti & Ricci Lucchi postula que o sistema turbidítico seria formado por.

Canais entrelaçados.

Região de intercanal (crevasse splay, diques marginais).

Barra de embocadura.

Lobos deposicionais.

Franja de leque e (6) planície batial.

Abaixo, segue o layout do modelo deste autores em 1972.

Os modelos criados até a aparição da estratigrafia de sequências de Vail consideravam apenas processos internos como formadores dos depósitos turbidíticos.

Com a "Revolução" da Sismoestratigrafia, que culminou no surgimento da Estratigrafia de Sequências a partir do trabalho de Vail, 1977, os modelos posteriores a esse período histórico da Geologia de rochas sedimentares, consideravam também em seus modelos os fatores externos influenciando de forma efetiva os processos formadores destes depósitos.

Destacam-se entre os fatores externos as variações do nível do mar, a climatologia e a tectônica de placas.

Dos modelos propostos após 1977, tem destaque o modelo de Mutti.

O modelo proposto pelo autor foi fortemente influenciado pelo surgimento da Estratigrafia de Seqüências originada a partir do trabalho de Vail.

Mutti passou a considerar que a variação do nível relativo do mar controlaria a distribuição dos sedimentos.

Mutti classificou 3 tipos de turbiditos de acordo com sua posição no talude e sua geomentria, e podem ser. canalizados (tipo III), que constituem a parte proximal.

A parte transicional (tipo II), que é canalizada e lobada e a parte distal que é lobada (tipo I).

Modelo sedimentológico de deposição turbidítica.

Adaptado de. Mutti 1999.

Os depósitos turbidíticos são formados nos tratos de mar baixo que ocorrem durante o rebaixamento relativo do nível do mar, e são vários os contextos onde esses depósitos podem ser encontrados.

Vales incisos surgem com o rebaixamento relativo do nível do mar, devido a exposição da plataforma marinha.

Nestas áreas ocorrem passagem de correntes vindas de desembocadura de rios.

Tais correntes, além de erodirem os vales por transportarem sedimentos, funcionam como alimentadores de complexos mais distais.

Os sedimentos depositam-se na porção superior do talude, mas devido a sua forte inclinação, os sedimentos permanecem em instabilidade.

Estes podem sofrer escorregamentos ou movimentações pela ação da gravidade se houver algum tipo de perturbação no sistema, que pode ser causado por terremotos, marés ou correntes de fundo, dentre outros fatores.

Com a erosão dos taludes (slopes) durante a fase de rebaixamento do nível do mar em áreas próximas à desembocadura de rios, na qual ocorre passagem de correntes, é comum haver preenchimentos por canais arenosos, mas com o aumento do nível do mar há uma tendência de haver preenchimento do canyon por sedimentos finos e o abandono do complexo formado durante o nível de mar baixo.

A grande maioria dos depósitos turbidíticos são encontrados em complexos de leques de talude, turbidito tipo II.

O talude é erodido, formando canyons no próprio talude, e os depósitos formados são complexos de canais e lobos.

Outros tipos de depósitos são complexos de leques de fundo de talude (Basin Floor Fan), onde se depositam turbiditos tipo I, e canal overbank (Channel Levee) ou turbidito tipo III.

Os complexos de leques de fundo de talude são encontrados no sopé de taludes, onde os depósitos são em forma de lobos de geometria tabular.

O modelo de Channel Levee é mais canalizado e se forma em nível de mar não tão baixo quanto os demais.

As formas canalizadas apresentam sinuosidades de acordo com as condições deposicionais e fisiográficas do local de deposição, e apresentam transversalmente formas côncavas (em forma de cunha), enquanto que a parte lobada, apresenta-se em formas tabulares.

Clark & Pickering, apresentam em seu trabalho um modelo interessante para deposição dos canais turbidíticos.

Os autores, através de casos reais, determinam padrões deposicionais através de interações dos processos de amalgamação lateral e vertical durante a formação do sistema canalizado.

Esses processos apresentam um forte controle na interconectividade das unidades genéticas e na razão largura/profundidade das rochas reservatórios.

O empilhamento dos canais pode ser gerado por processos autocíclicos que fazem com que haja mudanças de eixos de canais, resultando em migração lateral, e ocasionam conectividade entre seus depósitos.

Quando há agradação dos sedimentos, geralmente causada por sedimentação confinada, o empilhamento é vertical.

O confinamento, a sinuosidade, a agradação e a migração lateral são fatores que influenciam na sobreposição dos canais.

Quanto mais confinada for a deposição, maior será sobreposição vertical, que também ocorre com o incremento do aporte sedimentar e com o aumento da sinuosidade.

Existem outros fatores controladores para este último tipo, tais como canyons submarinos, domos de sal, diápiros ígneos e falhamentos.

Segundo Clark & Pickering, a proporção areia/lama também é fundamental para a ocorrência da sobreposição dos depósitos turbidíticos.

Quanto maior for o aporte de areia em relação à lama, maior será o empilhamento, tanto vertical como horizontal.

Os sedimentos mais antigos da Bacia de Campos datam do Cretáceo inicial, onde teve início sua evolução tectono-sedimentar, com a separação dos continentes africano e sul-americano.

A evolução da Bacia de Campos e o contexto tectônico em que está inserida levou Chang, a classificarem-na como Bacia Marginal do Tipo Atlântico.

Guardado, descrevem em seu trabalho 3 megasseqüências, levando em consideração as fases rift e pós-rift.

Cainelli & Mohriak identificaram 4 megasseqüências durante as fases evolutivas tectono-sedimentares, levando em consideração a fase pré-rift, além das descritas anteriormente.

Chang, apresentam 5 fases na evolução tectono-sedimentar.

Essas 5 fases correspondem a 5 megassequências da bacia de Campos.

Neste trabalho, segue a descrição das megasseqüências de acordo com a tese de Doutorado de Souza Jr e com o trabalho apresentado por Bruhn na AAPG-1998.

A Megasseqüência Rift Continental marca o início da movimentação tectônica que resultou na separação dos continentes.

Em sua base são encontradas rochas vulcanoclásticas de idade barremiana e sedimentos siliciclásticos e carbonáticos de época neocomiana que compõem a fase pré-rift.

Os principais depósitos são de sedimentos lacustres e flúvio-deltaicos depositados sobre grabens assimétricos.

Os lamitos lacustrinos e margas formam as principais rochas geradoras de hidrocarbonetos das bacias brasileiras.

Durante o Aptiano inicial, houve deposição carbonática (coquinas porosas) associada aos altos sindeposicionais.

São rochas produtoras de HC nos Campos de Badejo, Pampo, Linguado e Trilha.

A Megasseqüência Evaporítica (transicional) contém depósitos terrígenos e evaporíticos marcando a transição, do continente para o oceano, de forma gradativa e persistente.

A deposição de sedimentos nesta fase inicia-se no Aptiano inicial.

Uma superfície erosiva (discordância) separa a Megasseqüência Continental da Megassequência Evaporítica.

A deposição terrígena é formada por folhelhos e conglomerados oriundos de ambientes aluviais, fan-deltas, e de sabkhas, que passam lateralmente para sedimentação carbonática de águas rasas representada por estromatólitos e carbonatos nodulares.

Sua acumulação é acompanhada de um sistema de falhamento sindeposicional.

Segundo Babinski & Santos, e Mello & Maxwell, os folhelhos do Aptiano são as rochas geradoras de segunda maior importância das bacias marginais brasileiras.

Esta deposição compõe a porção superior da Formação Lagoa Feia, composta de sedimentos argilosos a conglomeráticos, e tem grande importância na formação de hidrocarbonetos da Bacia de Campos.

Os depósitos evaporíticos da Megassequência Evaporítica marcam o topo desta Megasseqüência transicional e ocorrem durante o Aptiano tardio.

São formados principalmente por anidrita e halita e depositados sobre ambiente lagunar, tectonicamente calmo.

A importância dessa seqüência para a Bacia de Campos deve-se ao movimento halocinético que controla alguns dos seus sistemas petrolíferos.

A Megasseqüência Marinha inicia-se durante o Albiano médio e tem continuidade até o Holoceno.

Entre o Albiano inicial ao Albiano médio, desenvolveu-se a Megassequência Plataforma Carbonática da Bacia de Campos.

A sequência é formada por depósitos clásticos de fan-deltas (compostos por arenitos e conglomerados) misturados com carbonatos plataformais.

Esta porção basal da Megasseqüência corresponde à deposição inferior da Formação Macaé, formada por sedimentos carbonáticos de águas rasas, lamitos e margas.

A sedimentação carbonática desenvolveu-se sob clima quente e seco, em ambiente nerítico, fundo oxigenado e águas hipersalinas, o que é sugerido pelo escasso conteúdo fossilífero de baixa diversidade específica.

A porção basal é composta por dolomitas que gradativamente diminuem em direção ao topo e por depósitos de leques deltaicos (fan-deltas), que ocorrem em partes costeiras e são formados por conglomerados e arenitos, os quais apresentam-se em estruturas canalizadas.

Em regiões plataformais ocorrem carbonatos na forma de oolitos, peletes e bioclastos.

Em resposta à movimentação salífera (Albiano inicial ao médio), sob o efeito da carga sedimentar, formam-se depressões na margem da plataforma rasa, e originam-se diápiros e falhas de crescimento.

Na bacia de Campos, durante o Albiano tardio e o Terciário inicial, ocorreu deposição da Megasseqüência Transgressiva.

Esta é contemporânea ao período em que os oceanos e o Atlântico Sul e Norte se conectam (Albiano tardio ao Turoniano inicial).

Há novas fases de movimentação halocinética devido ao progressivo basculamento da bacia e da sobrecarga sedimentar.

Há também o aumento relativo do nível do mar causado pela subsidência termal que sofre a bacia, somado à carga sedimentar e a própria separação dos continentes.

Neste contexto, inicia-se a deposição de sedimentos detríticos, de fraca energia, onde ocorrem intercalações de argila, margas, calcilutitos e alguns aportes turbidíticos que se depositam em baixos adjacentes às estruturas dônicas formadas durante a movimentação halocinética.

O sistema turbidítico do Campo em estudo caracteriza-se por depocentros limitados e controlados por falhas originadas da movimentação halocinética durante o Albiano tardio e o Cenomaniano inicial.

A partir do Terciário inicial, aumenta o aporte sedimentar, baixa a taxa de subsidência, causando uma queda eustática do nível do mar, o que provoca uma regressão marinha.

A movimentação halocinética é menos intensa, porém continua a produzir calhas deposicionais confinadas.

Tais calhas serviram aos depósitos de fluxos gravitacionais durante o Oligoceno inicial.

A Megassequência Marinha Regressiva inicia-se no Turoniano tardio e vai até o Eoceno médio.

O desenvolvimento sedimentar na bacia, durante o período Terciário, foi influenciado pela variação global do nível do mar, pelo soerguimento terciário da Serra do mar e pelos movimentos halocinéticos.

O Cretáceo tardio e o Eoceno inicial são marcados por rochas vulcanoclásticas, devido aos movimentos distensivos da crosta.

É grande o aporte sedimentar, e com a baixa taxa de subsidência, é gerado um padrão progradante.

Predomina sedimentação detrítica, tendo intercalações de folhelhos e arenitos de leques submarinos.

Ocorre o preenchimento das calhas formadas durante a fase anterior por fluxos gravitacionais.

Destes, destacam-se os turbiditos, que são importantes reservatórios na bacia, pois possuem boa permeabilidade e porosidade.

O Campo em estudo faz parte de um conjunto de plays exploratórios da Bacia de Campos, na porção Norte do Estado do Rio de Janeiro.

Situa-se cerca de 80 km da costa, em cotas batimétricas que variam de 110 a 250m.

A empresa operadora do campo, a Petrobrás, furou um total de 56 poços e realizou uma malha sísmica 2D e 3D.

O reservatório do Campo em estudo, o arenito reservatório, é um depósito turbidítico de idade cenomaniana/turoniana.

O campo é controlado por falhas normais, contudo o bloco principal do reservatório é pouco afetado pelos falhamentos.

A acumulação de óleo também é controlada pelas estruturas de falhamentos e pelo acunhamento ( pinch out) das deposições turbidíticas.

A rocha selante que marca o topo do reservatório é composta por folhelhos e margas, enquanto que sua base é marcada pela presença de carbonato.

O arenito é caracterizado por intercalações de areia com folhelho.

Segundo a descrição de Guardado, trata-se de arenitos arcósicos, maciços, de granulometria areia média e localmente conglomerática.

Sua porosidade varia de 20 a 30% e a permeabilidade é muito alta, acima de 1 Darcy.

O campo tem disposição espacial de NW para SE, assim como os corpos arenosos que compõem o reservatório.

Com a intenção de fomentar pesquisa e formação de mão-de-obra para a indústria do petróleo, a Agência Nacional do Petróleo (ANP) disponibilizou um conjunto de dados para realização de trabalhos de pesquisa no setor de petróleo e gás.

Parte dos dados utilizados neste trabalho foram cedidos pela ANP, e parte pela PetrobrAs.

Sendo que esta última contribuiu com dados já interpretados.

Fazem parte do conjunto de dados cedidos pela ANP os seguintes. uma malha sísmica 3D de 8 bits, uma linha sísmica 2D, 18 poços com perfis com perfis GR, DT, ILD,?B e fN, de 20 em 20cm, sendo que nem todos os perfis estavam presentes em todos os poços.

Imagens em arquivos binários (tif) de perfis interpretados de testemunhos de dois poços.

As imagens contêm informações incompletas dos respectivos perfis granulométricos.

Apesar de estas imagens estarem incompletas, serviram como referência ao ajuste dos perfis.

A PETROBRÁS cedeu alguns dados interpretados para realização deste trabalho de tese de mestrado.

Dentre os dados liberados pela empresa e utilizados neste trabalho incluem-se perfis de Vsh, fe, Sw e o horizonte sísmico da base.

Esses dados foram tratados em estação de trabalho com estrutura avançada, RISC 6000, é facilmente encontrada em ambientes de trabalho nos quais o conjunto de dados é gigantesco e o processamento é intensivo.

A Landmark Graphics Company ofereceu hardware e software para processamento e análise dos dados supracitados, dentre eles o SeisWorks, o StrataWorks, o Stratamodel e o Postack.

O trabalho de modelagem realizado neste projeto iniciou-se através de consulta ao acervo bibliográfico existente sobre o campo estudado, de maneira que todas as informações geológicas e estruturais coletadas pudessem ser utilizadas como guias.

Posteriormente, utilizaram-se dos dados e equipamentos fornecidos pela ANP, PETROBRAS e Landmark.

A primeira etapa da modelagem, que foi a análise dos dados sísmicos, foi um tanto limitada, devido ao fato de que faltaram as tabelas tempo-profundidade para realizar interpretações do intervalo do reservatório em questão.

No entanto, a PETROBRAS liberou a interpretação do horizonte sísmico da base do reservatório, o que permitiu não só a identificação do reservatório, como também, o reconhecimento de estruturas descritas na bibliografia consultada, relativas ao intervalo de interesse.

Os passos seguintes foram. (1) analisar o conjunto de dados de poços, (2) realizar interpretações e (3) associá-las à interpretação sísmica, para construção da base de dados tridimensional.

Como os dados fornecidos pela ANP foram insuficientes, não foi possível criar um modelo de estruturas complexas contendo falhamentos.

O modelo criado deve ser aperfeiçoado por outros alunos que desejarem ingressar nesta área, que une conceitos de Geologia e Matemática, com a modelagem numérica funcionando como "ponte" de comunicação entre o geólogo e o engenheiro.

A partir das curvas de GR, DT e?B, foi possível delimitar o reservatório em profundidade.

No perfil de raios gama (GR), o topo do reservatório aparece muito bem marcado pela presença da rocha selante que, segundo o perfil, pode variar de aproximadamente 10 a 25 metros de espessura.

A base também foi reconhecida através de perfis, além da seção sísmica interpretada.

Para tanto, utilizaram-se os perfis DT e?B.

A calcita tem densidade 2,71 g/cm3, o quartzo 2,61 g/cm3.

Por analogia, o carbonato não poroso possui densidade maior que a de um arenito não poroso, seja ele arcosiano ou quartzo-arenito.

Ao passar de um domínio de rochas siliciclásticas para um de rochas carbonáticas, a tendência é de que a curva de?B aumente, enquanto o tempo de trânsito da onda P (DT) diminui.

É uma relação inversa.

Segundo a literatura, a base do reservatório é marcada por carbonatos da Plataforma Carbonática.

Os perfis do campo estudado, mostram a passagem de uma plataforma carbonática para uma sucessão de depósitos turbidíticos, e no topo, depósitos argilosos.

É bem perceptível o aumento da densidade e a conseqüente redução do tempo de trânsito da onda P (perfil DT).

Dessa forma, pode-se marcar a base do reservatório estudado, enquanto que o topo é bem marcado pela curva de raios gama (GR).

A seta aponta para a curva de?B (azul escuro), indicando um aumento significativo na densidade do meio.

Delimitação do topo e base do intervalo turbidítico superior (em verde) através de correlação lateral, com base nos perfis de poços.

Para delimitação dentro do reservatório, na camada de interesse, foi preciso recorrer a outras curvas disponíveis, como Vsh e fe.

A curva de fe mostra a porosidade efetiva da rocha, que é o espaço ocupado por fluido móvel dentro dos poros, refletindo o grau de intercomunicação entre os poros de uma rocha.

Esta é uma curva calculada em função de fN, fD e do perfil sônico, porém o perfil sônico tem pouca contribuição, podendo este ser retirado do cálculo.

A curva de Vsh mede a argilosidade, mas esta também é uma curva calculada. seus parâmetros são GR, fD e fN.

As curvas foram calculadas a partir das fórmulas de Archie.

Para melhor visualização do intervalo, foi criado um modelo de visualização de curvas (template), no qual as curvas ficaram com ascensão de escala em mesma direção, ocupando a mesma área.

A análise foi feita partindo do princípio de que o reservatório estudado é composto por depósitos turbidíticos, como informa a literatura.

No momento em que a curva de Vsh apresenta um baixo valor de argilosidade, enquanto que na curva de fe ocorre um aumento da porosidade efetiva, interpretou-se como um intervalo arenoso com porosidade.

Visualmente, ocorre um cruzamento entre as curvas, com valores baixos de Vsh e valores altos para fe.

A partir desses cruzamentos visuais, foram identificados o topo e a base do intervalo turbidítico.

O método empírico descrito foi comparado com algumas imagens de perfis granulométricos de testemunhos cedido pela ANP dos poços P04, P37D e P22, e o resultado foi satisfatório.

Os perfis granulométricos cedidos pela ANP são dados interpretados por geólogos da PETROBRAS.

Os autores de sua descrição identificaram a camada superior do reservatório como sendo arenito médio gradado, de composição arcosiana, com cimentação pontual, bem selecionado e sem estruturas (maciço).

Segundo as análises de perfis geofísicos, a espessura do estrato varia entre 20 a 30 metros.

A geração do mapa de interesse foi feita a partir da análise descrita acima.

O mapa de topo e base do intervalo de interesse, inicialmente, era composto de uma região com densidade de poços informantes suficientes para simulação e uma pequena região com pouquíssimas informações, o que poderia gerar valores sem controle.

Essa sub-região foi cortada do mapa.

Pois aqui, o interesse está na aplicação da metodotologia às áreas com um mínimo de informações.

O modelo tridimensional criado com essas informações compõe-se de uma malha cuja célula elementar mede 20x20x1m, o que dá um volume de 400m3/célula.

A finalidade da modelagem é a representação da porção superior do reservatório estudado.

O tratamento de dados para quantificação da continuidade espacial de atributos petrofísicos da rocha em questão, no caso do arenito arcosiano, pode ser feita através de variografia, utilizando-se perfis de poços relacionados à litologia, como GR e?B e variogramas.

Pode-se ainda utilizar a porosidade, ou um outro atributo que esteja relacionado com a porosidade.

Neste trabalho, foi utilizada a porosidade preenchida por hidrocarboneto fH (phiH).

A análise foi feita com um conjunto de 16 poços localizados na porção SE do campo estudado.

Dos 16 poços, 5 são verticais e 11 direcionais.

O modelo utilizado foi o esférico.

Observa-se a presença de estrutura cíclica no semivariograma vertical.

Eventos periódicos, como os que ocorrem em muitos dos depósitos sedimentares, podem apresentar esse efeito, conhecido como efeito buraco.

No caso do depósito estudado, que é um turbidito, era de se esperar o aparecimento deste efeito, já que é composto por intercalações de rochas arenosas com pelíticas formadas por eventos cíclicos.

Variograma vertical de fH do modelo esférico, gerado pelo programa RC2.

Através da análise é perceptível que a anisotropia da propriedade modelada é mista.

O semivariograma mostra o efeito pepita com valor de 0,15, aproximadamente.

Este valor é o mesmo nas direções distintas, pelo fato de que o software RC2 TM realiza a modelagem variográfica de forma solidária.

Uma de suas vantagens é que ele permite a modelagem da anisotropia de maneira simples, sem a necessidade de construção de um modelo variográfico complexo.

Para ilustrar a etapa da análise variográfica, abaixo, mostram o ajuste dos variogramas para o conjunto de poços da área de estudo.

Ajuste variográfico de forma solidária dos dados de perfis de poços (fH) da área de interesse.

A análise foi realizada nas direções de azimutes 20, 40, 60, 80, 100, 120, 140 e 160, com tolerância angular de 20o, o que permite uma varredura em todas as direções.

A elipse no canto superior esquerdo representa a anisotropia no plano horizontal.

Semivariograma do conjunto de poços na direção de azimute 20.

Semivariograma do conjunto de poços na direção de azimute 40.

Semivariograma do conjunto de poços na direção de azimute 60.

Semivariograma do conjunto de poços na direção de azimute 80.

Semivariograma do conjunto de poços na direção de azimute 100.

Semivariograma do conjunto de poços na direção de azimute 120 Semivariograma do conjunto de poços na direção de azimute 160 A partir da correlação dos intervalos arenosos dos perfis, foram gerados mapas de topo e base do intervalo de interesse, a partir do qual foi feito o tratamento geoestatístico para gerar mapas 3D equiprováveis de continuidade espacial do intervalo e seu respectivo cálculo de volume de hidrocarboneto.

O tratamento original proposto para este trabalho consistia em simular um conjunto de atributos (espessura, porosidade e saturação de água), utilizando o seguinte algoritmo, denominado algoritmo(1). simular separadamente cada um dos atributos volumétricos do intervalo modelado (espessura, porosidade efetiva e saturação de água).

Mapear a incerteza e a variabilidade dos atributos através de um pós-processamento.

Com esses dados simulados, é possível estimar o volume de hidrocarboneto in place (óleo e gás), pois. A primeira dessas etapas foi totalmente concluída no software RC2.

Entretanto, este mesmo sistema não dispõe de procedimentos de pós-processamento das simulações, para compor mapas de quantis e de probabilidade.

Tampouco está programado para exportar as malhas (grids) para outros pacotes computacionais abertos, tais como a GSLIB, que dispõem de recursos para produzir mapas de probabilidade e de quantis a partir da coleção de imagens simuladas.

Em vista desta limitação computacional, para a conclusão da etapa de quantificação da incerteza, foi adotado o seguinte procedimento, denominado algoritmo(2). Simular um único atributo significativo para o cálculo de volume de hidrocarboneto.

Mapear a incerteza de todo o volume simulado.

O atributo utilizado foi o fH.

Este pode ser calculado a partir de curvas oriundas da perfilagem, ou pré-calculadas, como foi mostrado anteriormente.

Neste caso foi utilizado os cálculos com base nas fórmulas de Archie.

Assim, foi criada uma nova curva (fH) para cada poço.

Esta curva calculada apresenta uma relação inversa à densidade do meio, o que serviu como base para a simulação.

Os valores de fH mostram boa correlação com a densidade?H, tendo coeficiente de correlação de -0,92.

O resultado obtido da simulação Sequêncial Gaussiana foi a geração de um conjunto de 100 imagens equiprováveis, cada uma gerando um volume de hidrocarboneto distinto e possível.

O programa RC2 permitiu o cálculo de volume deste atributo para cada uma das imagens.

Este conjunto de volumes foi gravado num arquivo e exportado para ser processado em outros softwares (SAS e GSLIB).

Foi aplicado um tratamento estatístico para extrair a distribuição dos valores simulados.

O conjunto de 100 volumes simulados pelo software RC2 foi processado pelo aplicativo estatístico SAS, para cálculos intermediários, e pelos programas histplt e probplt da biblioteca aberta GSLIB, que representam o volume estimado e sua incerteza, sob diversas formas gráficas de distribuição de probabilidade.

Estes programas também fornecem algumas estatísticas básicas.

Observa-se a semelhança da distribuição obtida com o modelo normal.

No gráfico ("papel de probabilidade normal"), pontos alinhados sobre uma reta representam a distribuição normal.

Observa-se que os pontos plotados aproximam-se de uma reta, justificando a escolha do modelo normal para esta distribuição simulada.

Outro ponto a destacar é o desvio padrão baixo, quando comparado com o valor médio.

O desvio padrão foi de 0,93 milhão de m3 e a média dos volumes foi de 26,98 milhões de m3, o que dá um coeficiente de variação igual a 0,0348 ou 3,48%.

Apesar do conjunto de dados limitados, utilizando apenas informações extraídas de poços, o desvio padrão foi baixo.

Distribuição de freqüência relativa construído a partir das 100 imagens simuladas dos volumes de hidrocarbonetos presentes na região de estudo.

Nota-se que apresenta uma distribuição próxima da normal e com baixa dispersão dos valores.

A curva de probabilidade mostra-se quase que retilínea.

O que corresponde com a distribuição quase normal dos valores simulados.

A partir das curvas de probabilidade acumulada, pode-se calcular um intervalo de confiança aproximado, devido ao fato de que a curva de distribuição dos volumes aproxima-se de uma normal, tendo a média e mediana valores muito próximos.

O intervalo de confiança aproximado de 50% é (em milhões de m3). Da distribuição acumulada representada (ou de seu correspondente arquivo digital), pode-se extrair outros pares de quantis simétricos, para representar intervalos de confiança, levando em conta a simetria da distribuição normal.

Pode-se também extrair a probabilidade de superar um valor crítico ou a probabilidade de não atingi-lo.

Curva da probabilidade acumulada dos 100 volumes extraídos da simulação do fH.

Gráfico preparado através do programa Histplt da GSLIB.

É visível o efeito buraco nos semivariogramas em algumas direções, o que representa evento cíclico, típico de depósitos turbidíticos com suas intercalações de material arenoso com pelítico.

A análise variográfica mostra também uma tendência de maior continuidade das propriedades petrofísicas volumétricas do depósito, particularmente a porosidade, na direção de azimute 57 (SW-NE).

Através da anisotropia observada nos semivariogramas, é possível que haja um canal ou conjunto de canais turbidíticos nesta área com a direção mencionada, o que é confirmado pela simulação na porção NW do modelo, onde a informação é mais abundante.

A simulação seqüencial Gaussiana, mesmo com um número limitado de dados informantes, permite a estimação local de atributos volumétricos de um reservatório petrolífero, acompanhada da distribuição de probabilidade do erro.

No presente projeto, o mapeamento da incerteza na estimação do volume de óleo in situ, dentro do reservatório estudado, ficou incompleto, devido à incapacidade do software usado de pós-processar conjuntos de imagens equiprováveis simuladas.

Tal dificuldade teria sido superada, exportando-se as malhas simuladas para pacotes computacionais abertos, com opção de pós-processamento das imagens, como é o caso da GSLIB.

Entretanto, o software usado não permite a exportação de seus resultados.

A solução adotada através do algoritmo(2), com a utilização da curva phiH, tem a vantagem de economizar diversas etapas em relação ao algoritmo(1), que representa a forma tradicional de mapear a incerteza.

Entretanto, a distribuição de freqüência gerada representa apenas a incerteza global.

Os gráficos e as estatísticas obtidas a partir do pós-processamento dos valores simulados indicam uma distribuição normal.

A partir da curva de probabilidade acumulada, pode-se calcular intervalos de confiança aproximados.

Um ponto a destacar é o desvio padrão baixo, quando comparado com o valor médio.

O desvio padrão foi de 0,93 milhão de m3 e a média dos volumes foi de 26,98 milhões de m3, o que dá um coeficiente de variação igual a 0,0348 ou 3,48%.

Apesar do conjunto de dados limitados, utilizando apenas informações extraídas de poços, o desvio padrão, que quantifica a incerteza, foi baixo.

O mapeamento da incerteza local requer o uso de pós-processadores, não disponíveis no pacote computacional utilizado.

Um complemento natural dessa tese de mestrado seria a execução destes procedimentos em outro software que permita a quantificação da incerteza local, a partir da qual obtêm-se os mapas de quantis e de probabilidade.

