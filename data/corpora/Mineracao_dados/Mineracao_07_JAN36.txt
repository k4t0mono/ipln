Os atuais gerenciadores de banco de dados não são adequados para manipulação de dados complexos.

E entre eles destacamos os dados multimídia que, para agilizar as consultas usam a operação de igualdade sobre as estruturas de indexação.

Operações de igualdade são pouco utilizadas em operações que envolvem dados complexos, uma vez que, a existência de dois elementos extremamente iguais é rara.

Uma classe de operadores que se adequa melhor para manipulação desses dados são os operadores por similaridade.

Exemplo de operadores de seleção por similaridade são a consulta por abrangência (range queries) e consulta aos vizinhos mais próximos.

Exemplificando, o operador de seleção aos vizinhos mais próximos responde a consultas como, "selecione as cinco proteínas mais parecidas pelo alinhamento da proteína Sparc (responsável pelo câncer de pele)".

Existem muitos trabalhos desenvolvidos no sentido de prover operadores de seleção por similaridade envolvendo estruturas baseadas em árvores.

Entretanto, poucos estudos têm sido realizados envolvendo a utilização de operadores diferentes da seleção, por exemplo, a junção.

Um operador de junção compara pares de objetos de elementos pertencentes ao domínio dos dados, ao passo que um operador de seleção recebe uma constante para a comparação dos elementos.

Podemos ter assim, três operadores de junção por similaridade, operadores de junção por abrangência, por vizinhos mais próximos e sobre os pares de vizinhos mais próximos.

Exemplificando, uma consulta utilizando junção por abrangência responde a consultas do tipo, "Selecione as proteínas contidas no vírus da Hepatite B que diferem em até duas unidades de alinhamento das contidas no vírus da Hepatite C".

Este trabalho apresenta um novo método de acesso métrico em extrema quantidade de dados bem como, formas de implementação das formas de junção em estruturas métricas.

Os atuais gerenciadores de bancos de dados foram desenvolvidos inicialmente para manipular domínios de dados numéricos e/ou pequenas seqüências de caracteres (palavras) aproveitando a relação de ordem total que existe entre os elementos do domínio para executar as operações de consulta e atualização dos dados.

A estrutura de indexação que tem sido usualmente adotada pelos gerenciadores de banco de dados para recuperar os dados é a B Tree.

O surgimento de novas formas de captura de informação e o aumento da capacidade de armazenamento possibilitaram que se armazenem grandes quantidades de outros tipos de dados tais como, séries temporais, seqüências de proteínas, imagens, áudio, textos longos, entre outros.

Esses dados são chamados de dados complexos, pois não têm uma relação de ordem entre seus elementos.

A recuperação desses dados não é atendida de maneira adequada pelas estruturas de indexação tradicionais.

Para dar suporte às novas operações sobre esses dados, sugiram novas estruturas de indexação, como por exemplo, as estruturas temporais, onde existem relações de tempo entre seus elementos, tais como, Time Index, entre outras.

Um outro tipo de dados é o que apresenta a noção de dimensão, onde os elementos podem ser projetados em um espaço de dimensões menores ou, podem ter seus valores em nova dimensão restrita pelos valores em outra dimensão (definindo hiper-volumes) e também existe a noção de distância entre os elementos.

Neste caso, todos os elementos devem possuir a mesma dimensão.

As estruturas que indexam dados desse domínio são amplamente descritas na literatura, e são chamadas de Métodos de Acessos Espaciais (SAM Spatial Access Methods).

O desenvolvimento dessas estruturas iniciou-se com o trabalho pioneiro sobre as R-Trees e, prosseguiu com numerosas variações e adaptações.

Existem conjuntos de dados em que a única relação existente entre os elementos é a similaridade.

A similaridade é definida através de uma função de distância, ou função de "dissimilaridade", que retorna zero se ambos os objetos forem idênticos ou um valor positivo que aumenta quanto maior for a distância (ou dissimilaridade) entre os objetos.

Um fato que justifica a escolha das estruturas métricas para o desenvolvimento desse projeto é que as operações de comparação por igualdade têm pouca utilidade nas operações envolvendo dados complexos, uma vez que a existência de dois elementos exatamente iguais é rara.

Nesses casos, o grau de similaridade entre os dados é o fator mais importante.

Além disso, o conjunto dos dados complexos não respeitam a relação de ordem total.

Existem basicamente dois operadores de seleção por similaridade, consulta por abrangência (range queries) e consulta por vizinhos mais próximos (k-nearest neighbors queries).

Uma consulta por abrangência recebe como parâmetros um elemento do domínio de dados objeto da consult e um grau de dissimilaridade, e o resultado são todos os elementos da base de dados que diferem do objeto da consulta até no máximo o grau de dissimilaridade indicado.

Um exemplo de consulta por abrangência em uma base de dados de seqüências genéticas é "selecione as proteínas que diferem da proteína Sparc (responsável pelo câncer de pele) por até duas unidades de alinhamento".

Uma consulta por vizinhos mais próximos recebe como parâmetros um elemento do domínio de dados o objeto da consult e uma quantidade K, e obtém como resultado a quantidade K de elementos da base de dados que sejam os mais próximos do objeto da consulta.

Um exemplo de consulta por vizinhos mais próximos em uma base de dados de seqüências genéticas é "selecione as cinco proteínas mais semelhantes à proteína Sparc pelo alinhamento".

Pode-se considerar que ambos os operadores são equivalentes ao operador de seleção em domínios de dados que dão suporte a relações de ordem total.

Ambas as operações recebem um objeto de consulta, e retornam um conjunto de objetos como resposta.

Para suportar as consultas por similaridade surgiram várias estruturas de dados métricas sendo que, algumas são baseadas em árvores implementadas em memória primária como GH-Tree, VP-Tree.

Outras árvores são implementadas em memória secundária e baseadas em regiões de cobertura como M-Tree.

Outras estruturas implementadas em memória secundária são baseadas em particionamento disjunto de regiões como SH (Similarity Hashing). Essas estruturas foram desenvolvidas no sentido de prover operadores de seleção por similaridade em gerenciadores para dar suporte a dados multimídia.

No entanto, poucos estudos têm sido voltados a consultas por abrangência e vizinhos mais próximos em operadores diferentes da seleção, como por exemplo a junção.

Como explicado acima, um operador de junção difere de um operador de seleção, pois o operador de seleção recebe uma constante para a comparação, e um operador de junção compara pares de objetos pertencentes a um mesmo domínio de dados, um de cada conjunto.

Existem três operadores de junção por similaridade, operadores de junção por abrangência, operadores de junção por vizinhos mais próximos e operadores de junção por proximidade.

A junção por abrangência tem como resposta todos os pares de objetos, um de cada conjunto, que estão distantes entre si por no máximo uma distância máxima de busca.

A junção de vizinhos mais próximos tem como resposta para cada objeto do conjunto, uma quantidade de objetos mais próximos do conjunto.

Em caso de empate de distâncias na maior distância obtida, pode-se selecionar apenas o número necessário de objetos que satisfazem a regra, ou criar uma lista de empates.

A junção por proximidade tem como resposta os pares de objetos mais próximos entre os objetos do conjunto para os objetos do conjunto.

Esse também pode criar uma lista em caso de empates de distâncias.

O objetivo deste trabalho é implementar esses operadores de junção nas estruturas métricas baseadas em região de cobertura, como M-Tree e Slim-Tree.

Um problema que vem sendo tratado nessas estruturas é a sobreposição entre regiões de um mesmo nível.

Essa sobreposição prejudica os algoritmos de consulta e trabalhos como Slim-Tree tentam minimizar essa sobreposição em regiões de um mesmo nível.

Esse trabalho também explorou o problema da sobreposição de regiões de um mesmo nível, definindo uma nova estrutura baseada em particionamento disjunto de regiões, chamado de coordenada métrica.

As implementações das operações de junção possibilitam verificar a eficiência desses algoritmos de junção nas estruturas baseadas em regiões de cobertura e nas estruturas baseadas em particionamento disjunto de regiões.

São apresentadas algumas consultas por similaridade que avaliam o grau de dissimilaridade entre os objetos e técnicas de indexação para melhorar o desempenho das consultas por similaridade.

São apresentadas a operação de junção, os tipos de junção e as suas possíveis implementações.

Um novo conceito de particionamento extensível do espaço métrico, chamado de coordenadas métricas é apresentado.

Apresenta-se a junção por similaridade e seus algoritmos desenvolvidos no trabalho.

Os Sistemas de Gerenciamento de Bases de Dados (SGBDs) tradicionais realizam buscas sobre dados convencionais (números e textos curtos) por meio de relações de igualdade e de ordem existentes nesses dados.

Entretanto, quando se utilizam SGBDs para armazenar dados complexos, como por exemplo dados multímidia imagens, sons, et, as buscas por igualdade ou por intervalo não se aplicam, ou simplesmente têm pouca utilidade.

Exames de tomografia são um bom exemplo disso.

Dificilmente a tomografia do cérebro de um paciente é idêntica à de outro paciente, ou até mesmo duas tomografias do próprio paciente tomadas com alguns instantes de separação entre elas.

Neste caso, o grau de similaridade entre os dados complexos é o fator mais importante.

O tipo de busca denominado consulta por similaridade consiste em procurar em um conjunto por objetos que, segundo algum critério de similaridade, sejam mais "parecidos" ou mais "distintos" com/de um determinado objeto.

Em outras palavras, consultas por similaridade comparam todos os objetos do conjunto com um objeto escolhido, selecionando apenas os elementos que atendam a um certo critério de similaridade.

Por exemplo, procurando dar um diagnóstico mais preciso sobre o tumor no cérebro de um paciente, e até mesmo tentando encontrar subsídios para decidir como encaminhar o tratamento, um médico estaria interessado em encontrar as tomografias de cérebro que apresentem um tumor semelhante àquele em estudo.

Para se determinar a similaridade entre os objetos, eles podem ser comparados diretamente ou a partir de vetores de características (cor, forma e textura por exemplo) extraídos dos mesmos.

Nos dois casos, uma função de distância é definida para calcular o grau de similaridade, retornando um valor numérico que quantifica o quão similares dois objetos.

Por definição, esses valores são sempre maiores ou iguais a zero, tendo valores próximos de zero para objetos muito similares (zero para objetos iguais) e valores maiores para objetos menos similares.

Essa função é chamada de função de dissimilaridade ou de função de distância e deve sempre ser definida por um especialista no domínio em questão.

Na realidade, a função de distância indica a dissimilaridade entre dois objetos, e convém colocar que a similaridade é medida pelo seu inverso.

Existem vários tipos de consultas por similaridade, sendo a consulta por abrangência e a consulta aos k-vizinhos mais próximos as mais comuns.

Pesquisadores possuem grande interesse na elaboração de algoritmos de busca para executar consultas por similaridade.

Dois principais modelos relacionados com o domínio dos dados onde as consultas serão efetuadas têm sido propostos e estudados, modelo de espaço métrico e modelo de espaço vetorial.

São apresentadas uma discussão sobre espaço métrico e uma discussão sobre o espaço vetorial.

Tipos de consultas por similaridade.

São apresentados os principais métodos de acesso espacial e métrico.

Para alguns domínios a extração de vetores de características de dimensão definida pode ser uma tarefa muito complicada, ou até inviável, como no caso em que os tipos de características variam para cada objeto.

Isso ocorre, por exemplo, com impressões digitais, pois cada impressão tem um conjunto próprio de características, deltas, curvatura e espaçamento dos sulcos, sumidouros, entre outras.

Essa restrição ocorre com freqüência quando a noção de similaridade é complexa e altamente dependente do domínio.

No entanto, se ainda é possível avaliar a similaridade entre os objetos a partir de uma função de distância métrica então os dados podem ser considerados estar em um domínio métrico.

Seja um domínio de objetos.

O subconjunto finito representa o conjunto de objetos no qual as consultas são efetuadas.

A função corresponde à medida de "distância", também chamada de "similaridade", entre dois objetos.

Isso significa que quanto menor a distância entre dois objetos, mais próximos ou semelhantes (similares) eles são.

Se uma função de distância possuir as seguintes propriedades, Simetria, Desigualdade triangular, o espaço definido pelo par é chamado de espaço Métrico e a função de distância é chamada Função de Distância Métrica, ou simplesmente Métrica.

Note-se que nenhuma informação geométrica, como num domínio espacial, pode ser usada em um espaço métrico.

As funções métricas mais utilizadas são as métricas de Minkowski, utilizadas em domínios multidimensionais, e de Levenshtein, utilizada para o cálculo de similaridade entre cadeias de caracteres.

A métrica retorna o número mínimo de operações de edições (inserções, remoções e substituições de caracteres) necessárias para transformar a cadeia de caracteres na cadeia.

As propriedades dos espaços métricos, principalmente a desigualdade triangular, permitem a elaboração de técnicas de indexação capazes de responder a consultas por similaridade de modo eficiente.

Uma característica dos espaços métricos é a possibilidade de englobar os espaços vetoriais, bem como espaços a dimensionais (conjuntos de imagens, palavras, sons ou dados genômicos), desde que haja uma métrica adequada.

Um espaço é adimensional quando o domínio dos objetos não pode ser identificado por um conjunto de coordenadas em eixos ortogonais numa dimensão finita.

Se os objetos do domínio correspondem a vetores de valores numéricos então o espaço é chamado espaço Vetorial com Dimensão Finita, ou simplesmente espaço Vetorial.

Os objetos de um espaço vetorial de dimensão são representados por coordenadas de valores reais.

Várias métricas podem ser aplicadas para comparar os objetos, sendo as mais conhecidas as da família L (ou Minkowski).

No exemplo, a métrica L, conhecida como Bloco ou Manhattan, corresponde ao somatório do módulo das diferenças entre as coordenadas.

Nesse caso, o conjunto de pontos de mesma distância forma um losango com os diâmetros paralelos aos eixos das coordenadas.

A métrica L, mais conhecida como distância Euclidiana, corresponde à função usual para distância entre vetores.

O conjunto de pontos de mesma distância ao centro para a métrica forma uma circunferência.

Calculando-se o limite da métrica 21 quando tende ao infinito, obtém-se a métrica L, também conhecida como Chebychev, na qual o conjunto de pontos com mesma distância formam um quadrado com Representação dos pontos situados à distância a partir do objeto, considerando as diferentes métricas da família.

Exemplos de consultas por abrangência em um conjunto de objetos os lados paralelos aos eixos das coordenadas.

Conjunto de objetos, um objeto de consulta, o raio de consulta e três métricas para uma consulta que deseja obter quais são os objetos a uma distância do objeto de referência.

O conjunto de objetos contido no losango de consulta faz parte do conjunto resposta quando a métrica é utilizada.

Os objetos contidos na circunferência de consulta fazem parte do conjunto resposta considerando a métrica.

Os objetos contidos no quadrado de consulta fazem parte do conjunto resposta, quando a métrica é utilizada.

As três métricas citadas anteriormente foram escolhidas por serem muito utilizadas em consultas por similaridade.

As consultas que avaliam o grau de dissimilaridade entre os objetos são chamadas de consultas por similaridade e envolvem, uma função de dissimilaridade.

Um objeto de busca, também considerado como o centro da consulta, a partir do qual se deseja encontrar os mais semelhantes e um parâmetro que depende do tipo de consulta por similaridade em questão.

Os dois tipos mais comuns são a consulta por abragência e consulta aos vizinhos mais próximos.

Considere-se um conjunto de objetos pertencentes a um domínio.

Exemplos de consultas por similaridade, consulta por abrangência, vizinhos mais próximos métrica.

Os tipos mais comuns de consultas por similaridade são, considerando o conjunto de dados, o objeto de busca e o raio de busca para responder a consulta, cada objeto é comparado com o objeto de busca e, então é inserido no conjunto resposta.

Se a cardinalidade do conjunto for muito alta ou se a métrica utilizada envolver algoritmos muito custosos, o tempo para a obtenção da resposta pode se tornar inaceitável, sendo interessante o uso de alguma técnica para agilizar tal processo.

Pode ser necessário também, armazenar os objetos em disco, dependendo de sua complexidade e tamanho.

O desempenho de consultas por similaridade em ambientes altamente dinâmicos pode ser afetado por dois processos principais, a quantidade de acessos a disco e a quantidade de comparações entre objetos efetuadas através de cálculos de distância.

Diversas técnicas de indexação têm sido propostas para melhorar o desempenho de consultas por similaridade e podem ser classificadas em Métodos de Acesso Espaciais, voltados para o modelo em que os objetos são representados por vetores em um espaço dimensional.

Métodos de Acesso Métricos, voltados para o modelo em que apenas a distância entre os objetos é levada em consideração.

Em geral, a estrutura dos métodos de acesso dinâmicos voltada para o armazenamento secundário é baseada em estruturas hierárquicas que organizam os dados em agrupamentos (clusters).

Métodos de Acesso Métrico como MVP-Tree, entre outras apresentadas, podem melhorar o desempenho de consultas por similaridade sobre dados complexos.

O método de acesso espacial mais encontrado na literatura é a R-Tree, escolhido por ser o mais referenciado e utilizado para a indexação de dados vetoriais.

Outro ponto importante é que a R-Tree é sempre comparada com os outros métodos e pode ser vista como uma referência para demonstrar e validar as melhorias propostas.

Dentre os vários métodos de acessos existentes, o método R-Tree é apresentado a seguir.

A R-Tree é um método de acesso espacial para indexar dados vetoriais não necessariamente pontuais, ou seja, que possuem extensão (área, volume, hiper-volume).

A organização da estrutura interna é semelhante à da B-Tree e atende às premissas.

Exemplo da representação estrutural de uma R-Tree.

Representação estrutural de uma R-Tree.

A R-Tree é uma estrutura de indexação dinâmica, portanto, os objetos podem ser inseridos e removidos da árvore sem reconstruir a árvore completamente.

Cada nó na R-Tree corresponde a uma página de disco que armazena um conjunto de objetos geométricos k-dimensional representando-os pelo limite mínimo de retângulos.

Cada nó da R-Tree corresponde ao MBR (Mini-mum Bounding Rectilinear Rectangle) que contém seus filhos.

Uma MBR é caracterizada por pontos "min" e "max" dos super retângulos com faces paralelas aos eixos coordenados.

Usando a MBR em vez da representação geométrica do objeto, sua complexidade representacional é reduzida para dois pontos, onde as características mais importantes do objeto (posição e extensão) são mantidas.

Conseqüentemente, o MBR é uma aproximação amplamente empregada.

As folhas da árvore R-Tree contêm ponteiros para os objetos da base de dados, em vez de ponteiros para os nós filhos.

Deve ser notado que os retângulos que cercam nós diferentes podem ser sobrepostos.

Além disso, um retângulo pode estar incluído (no sentido geométrico) em muitos nós, mas pode estar associado a somente um deles.

Isto significa que uma busca espacial pode demandar a visita a muitos nós, antes de confirmar a existência ou não de um dado retângulo.

As regras obedecidas pela R-Tree são as seguintes, cada folha contém pares da forma, tal que é o retângulo mínimo que contém espacialmente o objeto determinado pelo identificador.

Todos outros nós contêm pares da forma, onde é um ponteiro para um filho do nó e é o retângulo mínimo que contém espacialmente todos os retângulos contidos em seus filhos.

A R-Tree de classe tem a característica que todo nó, exceto a raiz que contém entre pares.

A raiz contém no mínimo dois pares, senão ela é um folha.

A inserção é feita da seguinte maneira, considerando inicialmente uma árvore onde só existe o nó raiz, os objetos são inseridos até que a raiz fique totalmente cheia.

Nesse momento, a inserção de um novo objeto faz com que ela seja dividida em dois e os objetos são re-arranjados de forma que o hiper-volume dos dois novos nós seja mínimo.

Caso o hiper-volume dos dois novos nós não esteja contido dentro do hiper-volume do nó acima, esse último deve ser ajustado para que contenha o hiper-volume dos novos nós.

A R-Tree são variações posteriores que visam melhorar o desempenho da R-Tree.

As principais melhorias apresentadas pela R-Tree são, o algoritmo de inserção passa a reinserir alguns objetos antes de particionar um nó.

O algoritmo de particionamento de nós que procura minimizar o perímetro e maximizar a ocupação dos novos nós.

O particionamento proposto pela R-Tree procura apenas, minimizar o volume dos nós.

Porém, a sobreposição dos nós em qualquer variante da R-Tree aumenta drasticamente à medida que a dimensão aumenta.

Procurando evitar a sobreposição, a R-Tree quebra os objetos e armazena seus pedaços em nós diferentes.

Como conseqüência, o particionamento de um nó interno precisa ser propagado tanto para seu nó pai quanto para sua subárvore.

Por sua vez, ter a subárvore particionada pode reduzir a taxa de ocupação da árvore como um todo.

Outra variante da R-Tree foi apresentada.

A X-Tree, como é chamada, define um super-nó para tratar a questão de sobreposição.

Quando o grau de sobreposição é alto, os nós são concatenados em um super-nó de tamanho variado (geralmente um múltiplo do tamanho original).

Essa abordagem é interessante apenas quando é possível acessar páginas de disco de tamanhos diferentes, diminuindo o número de acessos a disco.

No entanto, se o tamanho da página for fixo, como é o caso da maioria dos SGBD, o número de acessos aumenta e o desempenho passa a ser próximo ao da R-Tree.

A consulta por similaridade baseia-se apenas no grau de dissimilaridade fornecido pela comparação entre "pares de objetos".

A seguir serão apresentados vários métodos de acesso métrico que fornecem suporte para responder buscas por similaridade, isto é, buscas cujo propósito é recuperar objetos que são similares a um objeto de busca, e onde a dissimilaridade entre objetos é medida por uma função de distância métrica específica.

A indexação de dados complexos com base na distância entre pares de objetos foi proposta.

Três técnicas foram apresentadas para indexar cadeias de bits, nas quais o valor retornado pela métrica é restrito a números inteiros não-negativos.

O processo de indexação particiona recursivamente o conjunto de dados, sendo materializado como uma árvore.

As técnicas são apresentadas a seguir.

Na primeira técnica, cada nó possui uma entrada e um número pré-definido de filhos maior ou igual a dois.

Um elemento qualquer do conjunto é escolhido para ser colocado como raiz em cada subárvore.

Os demais elementos são organizados de acordo com sua distância para o objeto da raiz, ocupando uma mesma subárvore aqueles que estão à mesma faixa de distância.

Como a distância é discreta, o agrupamento é sempre possível.

Esse procedimento é repetido recursivamente para cada subárvore, até que só reste um objeto, ou até uma profundidade pré-definida.

Na segunda técnica, o conjunto de dados é dividido em um número fixo de subconjuntos e, para cada um, são definidos um objeto representante e um raio máximo.

Os demais objetos são distribuídos de forma que sua distância ao representante seja menor ou igual ao respectivo raio.

Esse procedimento é repetido recursivamente.

Os representantes são agrupados em nós.

A terceira técnica, corresponde a particionar o conjunto de objetos em cliques (um clique é um conjunto que define um grafo completo maximal, ou seja, que nenhum outro nó pode ser inserido sem que o mesmo deixe de ser completo), cada qual com um valor diferente para c (distância máxima entre os objetos), de modo a garantir que todos os objetos de S estejam em pelo menos um desses cliques.

Em seguida, um elemento arbitrário de cada clique é escolhido como seu representante.

A definição não impede que um mesmo objeto apareça em cliques diferentes.

Sugere-se armazenar uma matriz de distâncias entre todos os pares de objetos, aumentando ainda mais a exclusão de nós durante as consultas.

Os autores comentam que essa abordagem é indicada principalmente quando o cálculo da distância é muito custoso.

Porém, ela torna-se inviável se o conjunto de dados for muito grande, pois seria necessário armazenar a matriz triangular superior inteira, envolvendo uma complexidade da ordem. Um método de acesso métrico é a Slim-Tree, em que os pontos fortes são, a medida do fatfactor que avalia o grau de sobreposição entre os nós de um índice, permitindo indicar se a árvore é adequada para o processamento de consultas.

O algoritmo de pós-otimização slim-down, que reduz o número de acessos a disco durante as consultas.

Os métodos de acessos métricos Slim-Tree, M-Tree entre outros e a técnica Omni serão discutidos nas próximas subseções.

Decomposição do espaço na GH-Tree.

A GH-Tree (Generalized Hyperplane Tree), foi um dos primeiros trabalhos a sugerir a construção de um MAM.

Esta árvore divide o espaço métrico em hiperplanos generalizados.

A definição desses hiperplanos é a seguinte, Definição, Sejam dois pontos, um hiperplano generalizado é o conjunto de pontos que satisfazem.

Cada nó define dois ramos que representarão os subespaços particionados pelo hiperplano.

A mesma política é aplicada em cada subpartição, construída na abordagem top down.

Representação da partição em um espaço euclidiano bidimensional.

Nota-se que delimitam dois subespaços, divididos pelo hiperplano generalizado.

Assim, todos os objetos pertencem à região, caso contrário pertencem à região.

A GH-Tree é construída escolhendo dois objetos aleatoriamente como representativos e dividindo os objetos restantes associando-os aos representantes mais próximos.

Em seguida, este processo é aplicado em ambos subespaços, recursivamente.

Esta estrutura dá suporte à dinamicidade, ou seja, inserções individuais, seguindo a mesma política de escolha acima.

A dinamicidade vem do fato de que a estrutura não controla o balanceamento, então a inserção de um novo objeto não quebra nenhuma propriedade da estrutura.

Uma das desvantagens da GH-Tree é que somente consultas pontuais podem ser feitas, uma vez que somente pontos podem ser comparados às partições.

Uma outra desvantagem é que precisam ser feitos dois cálculos de distância por nó durante consultas e a cardinalidade dos nós é limitada a dois, o que inviabiliza seu uso para armazenamento em disco.

Particionamento de uma VP-Tree, representação no espaço, simplificação da árvore gerada.

Métodos de acessos métricos semelhantes à primeira técnica foram propostos, sendo que a idéia básica é particionar o conjunto de dados em dois subconjuntos de acordo com um representante e um raio máximo.

Geralmente, o valor do raio máximo corresponde à mediana das distâncias entre o representativo e os demais objetos.

Todos os objetos a uma distância do representativo menor ou igual ao raio são colocados na subárvore esquerda.

Os demais são colocados na subárvore direita.

A VP-Tree (Vantage Point Tree) foi implementada com melhorias e variações que visam criar uma estrutura para responder a consultas por abrangência e aos vizinhos mais próximos, com o objetivo de minimizar cálculos de distância.

A VP-Tree divide o espaço em cortes esféricos, a partir de um ponto escolhido, chamado de ponto de vantagem.

A política de construção a torna em princípio uma estrutura estática, onde novas inserções não são possíveis.

A construção da árvore é feita da seguinte maneira, a partir de um conjunto de objetos, é escolhido um objeto para atuar como ponto de vantagem e as distâncias dos objetos restantes são calculadas para este objeto.

Encontra-se um raio que seja a mediana das distâncias de cada objeto ao ponto de vantagem, que divide o espaço de maneira que metade dos objetos fique dentro do raio de cobertura e a outra metade fora do raio.

O mesmo princípio é realizado em ambas as partes recursivamente, formando uma hierarquia.

A VP-Tree tem sua construção e pesquisa análoga à ordenação e busca binária em uma dimensão, tanto por causa de sua estratégia quanto por sua complexidade.

Algumas variações foram sugeridas para melhorar o desempenho da estrutura ou mesmo para permitir dinamicidade.

Uma das variações sugeridas foi a VP-Tree que mantém em cada nó um histórico das distâncias dos objetos representativos ancestrais até a raiz, sendo que, as distâncias são calculadas durante a construção da árvore.

Cada nó da árvore mantém, além de seu identificador, uma lista de limites de distâncias superior e inferior, denotando o subespaço visto por cada ancestral.

Esta melhoria diminui o número de nós visitados durante as consultas por exemplo, aos vizinhos mais próximos, pois ajudam a escolher a ordem adequada de visita aos ramos, o que pode resultar em uma consulta mais rápida.

No entanto, esta técnica sofre se a dimensionalidade dos objetos for muito alta.

Outra variação da VP-Tree é a VP sb-Tree, onde os nós folhas formam buckets armazenando mais objetos, economizando assim mais espaço, o que resulta em menos acesso a disco para as consultas.

A MVP-Tree (Multiple Vantage Points Tree) foi apresentada com o objetivo de otimizar a utilização dos pontos de vantagem da VP-Tree.

O espaço métrico na MVP-Tree é particionado em cortes esféricos a partir dos ponto de vantagens.

Esta estrutura cria partições a partir de dois objetos representantes por nó e mantém informações extras nos nós folhas para filtragem de cálculos de distância, utilizando a desigualdade triangular.

A estrutura da MVP-Tree surgiu de algumas alterações de como utilizar os objetos representativos e seus cortes.

Duas motivações estimularam seu desenvolvimento, a primeira vem do fato de que é possível particionar uma região sem que o objeto representativo esteja contido na mesma região, o que significa que um mesmo objeto representativo pode particionar diferentes regiões associadas a nós em um mesmo nível.

Isto se aplica ao objeto representativo adicional em cada nó, quando comparada à VP-Tree.

A segunda motivação recai na possibilidade de manter informações extras nos nós folhas, ou seja, manter para cada objeto armazenado no nó folha, as distâncias aos objetos representativos no caminho da raiz até o nó folha onde cada objeto reside.

Tais distâncias podem ser aplicadas posteriormente em consultas através da desigualdade triangular para evitar cálculos de distâncias desnecessários.

A construção da MVP-Tree leva em consideração o número de partições criadas para cada ponto de vantagem.

O número máximo de objetos nos nós folhas.

O número de distâncias pré-calculadas a serem armazenadas para cada objeto nos nós folhas.

Uma vez definido o parâmetro, o grau de um nó intermediário é definido.

Em uma MVP-Tree binária, o primeiro ponto de vantagem divide o espaço em duas partes, e o segundo divide cada uma dessas partes em outras duas, obtendo um grau e regiões disjuntas.

Em uma MVP-Tree de ordem, o primeiro ponto de vantagem divide o espaço em partes e o segundo divide cada uma destas partes em outras partes, ou seja, cada nó interno de uma MVP-Tree binária é formado pelos atributos que seguem, é o primeiro ponto de vantagem, é o seu raio de cobertura, é o segundo ponto de vantagem e são seus raios de cobertura.

Cada nó folha armazena dois pontos de vantagem, suas distâncias aos pontos de vantagem, e uma lista das distâncias aos ancestrais para cada objeto.

Durante a construção da MVP-Tree binária, o primeiro ponto de vantagem é escolhido arbitrariamente e o segundo ponto de vantagem é escolhido como sendo o objeto mais distante.

A construção de uma MVP-Tree genérica tem ordem de complexidade.

Apesar de ser um método estático, é eficiente, pois supera as árvores VP-Tree em consultas por similaridade.

A M-Tree foi proposta para organizar e buscar grandes conjuntos de dados em um espaço métrico.

Os objetos são armazenados em nós de tamanho fixo de dois tipos, nós folhas e nós índices.

Os nós folhas da M-Tree armazenam todos objetos indexados, ao passo que os nós internos armazenam os objetos representantes.

Para cada rota do objeto (objeto representante) existe um ponteiro associado, que referencia a raiz da subárvore, chamado de cobertura da árvore.

Todos os objetos na cobertura da árvore estão dentro da distância, que é chamado de raio de cobertura e forma uma parte da entrada em cada nó da M-Tree.

Finalmente, um objeto representante está associado com uma distância, seu objeto pai, que é o objeto representante que referencia o nó onde a entrada está armazenada.

Esta distância não é definida para entrada na raiz da M-Tree.

Os algoritmos para construir a M-Tree especificam como os objetos são inseridos e removidos e como são gerenciadas as regiões de cobertura.

O algoritmo de inserção desce recursivamente a árvore para localizar o nó folha mais apropriado para acomodar o novo objeto.

Se o nó folha onde o novo objeto será inserido estiver cheio, provocará uma quebra.

O raciocínio básico usado para determinar o nó folha mais apropriado é descer em cada nível da árvore, ao longo da subárvore, em que o objeto a ser inserido pertença a região de cobertura, definida pelo raio do objeto representante.

Se existem múltiplas subárvores com esta propriedade, será escolhido o nível em que o objeto está mais próximo do objeto.

Se não existe nenhum objeto representante, a escolha é minimizar o aumento do raio de cobertura.

Isto está relacionado à heurística que sugere minimizar o volume total coberto pelos objetos representativos no nó atual.

Como qualquer árvore balanceada dinâmica, a M-Tree cresce de maneira bottom-up.

A quebra de um nó provoca a alocação de um novo nó no mesmo nível, particionando as entradas entre estes dois nós, e promovendo para o nó, dois objetos representantes para referenciar os dois nós o que pode ocasionar mais quebras.

Quando a raiz é quebrada, uma nova raiz é criada e a M-Tree cresce um nível.

Quando se insere um objeto, calculam-se e armazenam-se as distâncias entre esse objeto e seus representativos.

Estas distâncias são usadas durante as consultas para descartar sub-níveis através da propriedade de desigualdade triangular do espaço métrico, sem que seja necessário, um novo cálculo de distância.

Espera-se que a quantidade de cálculos de distância em uma consulta seja drasticamente reduzida, melhorando o desempenho do método.

O descarte por desigualdade triangular é feito da seguinte maneira, considerando o espaço métrico, o conjunto, o objeto de busca, o raio de busca e um representativo, o objeto poderá ser descartado caso uma das duas condições a seguir seja satisfeita sendo que a distância foi previamente obtida e armazenad.

Porém, nada pode ser presumido, forçando o cálculo de distância entre eles e o objeto de busca.

O resultado é que encontra-se dentro da região de busca.

O cálculo de distância final é necessário para garantir que não ocorram os objetos qualificados erroneamente.

A garantia de que não existem descartes falsos, ou seja, de que os objetos descartados efetivamente não fazem parte do conjunto de resposta, é embasada na propriedade da desigualdade triangular, sendo que pode ser comprovada.

O espaço vetorial foi utilizado no exemplo apenas para facilitar o entendimento do processo de descarte de objetos no espaço métrico.

Porém, como as equações apresentadas levam em consideração apenas a distância entre os objetos, pode-se garantir que esse processo aplica-se para qualquer tipo de dado complexo, desde que a função de distância seja métrica.

Durante as consultas, o raio de cobertura e as distâncias pré-calculadas são utilizados para podar subárvores e entradas nas folhas através da desigualdade triangular, reduzindo o número de cálculos de distância.

Descarte de objetos com o uso da desigualdade triangular.

Só se calcula a distância entre objetos quando a poda é possível.

Percorre-se uma subárvore se a região que define estiver sobreposta pela região de busca.

Ao se atingir uma folha, os objetos que não podem ser descartados são comparados por cálculos de distânci diretamente com a região de busca e os que estiverem dentro são inseridos no conjunto resposta.

O algoritmo para consultas aos k-vizinhos mais próximos é semelhante ao da R-Tree, ou seja, em uma consulta por abrangência na qual o raio diminui à medida que novos vizinhos são encontrados.

A idéia desse método de acesso métrico é semelhante ao da M-Tree, e consiste em selecionar um ou mais objetos e colocá-los como representantes do conjunto.

A diferença está na inclusão de um novo algoritmo de quebra na Slim-Tree, chamado Minimm Spanning Tree (MST), que é baseado na árvore de caminho mínimo de um grafo, sendo resumido da seguinte forma, os objetos de um nó são considerados vértices de um grafo e as distâncias entre eles são os pesos das arestas que os conectam.

O MST do grafo é construído e a aresta de maior peso é removida, gerando os dois agrupamentos correspondentes ao particionamento do nó.

Como representante de cada agrupamento é escolhido o objeto mais parecido com todos os outros ao mesmo tempo, ou seja, o centróide.

Conforme mostram seus autores, apesar de bem menos custoso que o algoritmo Minimum-Maximum (min-Max), o desempenho das consultas de índices construídos com o MST é igualável ao de um criado pela M-Tree.

Outra diferença é que a Slim-Tree apresenta algoritmos para pós-otimização do índice que procura minimizar o grau de sobreposição entre os nós da árvore.

Com esse processo, chamado pelos autores de slim-down (emagrecimento), o número de acessos a disco é drasticamente reduzido durante as consultas por similaridade.

O algoritmo slim-down é executado quando a árvore possui um alto grau de sobreposição, o que é indicado pelo fat-factor.

A Slim-Tree foi o primeiro método de acesso métrico que permitiu quantificar a sobreposição entre nós da árvore.

Descrições mais completas da estrutura interna da Slim-Tree podem ser encontradas.

Exemplo de uma Slim-tree com seus nós índices e folhas indexando 1elementos.

No exemplo, a Slim-tree contém níveis de índice.

No primeiro nível existem dois objetos anta e tatu, que serão representantes para o nó inferior na árvore.

O segundo nível da árvore com o objeto representante anta, indexa os objetos anta e cabra.

O segundo nível da árvore com o objeto representante tatu indexa os objetos leão, tatu e urso.

Os valores anta, bode, boi, etc, são usados apenas como ilustração, e a função de distância utilizada não é Ledit.

A técnica Omni propõe o uso de alguns representantes globais, chamados focos, para podar cálculos de distância durante as consultas por similaridade, melhorando o desempenho em relação à abordagem de usar o representante local do nó, existente nos métodos M-Tree e Slim-Tree.

A idéia principal da Omni é escolher um conjunto de focos, com elementos, e representar cada objeto armazenado através de suas distâncias (pré-calculadas) para os focos.

Os objetos são armazenados em um arquivo de acesso randômico (AAR), e acessados diretamente através de um identificador que é gerado pelo AAR no momento da inserção.

Os principais componentes da técnica Omni são, A técnica Omni pode ser combinada com a busca seqüencial, com B-Trees e com o método de acesso espacial R-Tree, gerando três novos métodos de acesso métrico, Omni-Sequential, OmniB-Tree e OmniR-Tree, os quais são descritos a seguir.

Representação de uma Slim-tree com seus nós índices e folhas.

Omni-Sequential.

Armazena as coordenadas Omni em um arquivo seqüencial, onde cada entrada é composta pelo vetor de coordenadas e pelo código interno.

O AAR que gerencia os objetos é representado por O-AAR e o arquivo de acesso seqüencial (AAS), que gerencia as coordenadas Omni, por CO-AAS.

A execução de uma consulta por abrangência começa com uma etapa de filtragem sobre as coordenadas Omni armazenadas no arquivo CO-AAS.

O primeiro passo é gerar as coordenadas Omni do objeto de busca, calculando as distâncias.

Em seguida, o arquivo CO-AAS é percorrido seqüencialmente e as coordenadas Omni indexadas são comparadas com as coordenadas.

Durante a comparação das coordenadas, se algum foco for válido para a relação, o objeto pode ser descartado sem precisar compará-lo diretamente com o objeto de busca.

Vale ressaltar que já estava calculada e armazenada no CO-AAS.

Uma consulta aos k-vizinhos mais próximos é semelhante à consulta por abrangência, com a diferença de que o raio de busca é atualizado sempre que um novo vizinho é encontrado.

O conjunto de candidatos corresponde a uma lista encadeada dos k-vizinhos encontrados ao longo do processo, e mantida constantemente ordenada pela distância entre os candidatos vizinhos.

O valor do raio de busca é inicialmente ajustado para o infinito.

Para os dois tipos de consulta por similaridade, um objeto só é recuperado do O-AAR e comparado com o quando não pode ser descartado de nenhum foco.

OmniB-Tree.

O método de acesso auxiliar usado para armazenar as coordenadas Omni pode ser substituído por um conjunto de B-Trees.

Esse método permite a indexação de conjuntos de dados métricos usando recursos que já existem em sistemas de gerenciamento de base de dados comerciais.

O método Omni B-Tree utiliza uma floresta de B-Trees para indexar as coordenadas Omni.

O arquivo que armazena os objetos é representado por O-AAR e o conjunto de B-Trees por CO-BF.

Na consulta por abrangência, obtém-se o conjunto de candidatos da etapa de filtragem através da interseção dos candidatos gerados individualmente através de cada foco.

Um vetor de contadores, onde cada posição corresponde a um objeto armazenado é usado para acelerar a operação de interseção.

Cada posição é acessada pelo código interno armazenado com as distâncias.

Este vetor é usado para contabilizar os focos que não puderam descartar os objetos durante as consultas.

A etapa de refinamento consiste em percorrer o vetor e, para cada posição que tenha valor (número total de focos), o respectivo objeto é comparado diretamente com o objeto de busca.

O algoritmo de consulta aos k-vizinhos mais próximos requer uma abordagem diferente da tradicional, que é executar uma consulta por abrangência com reduções sucessivas do raio.

Em geral, ajusta-se o valor inicial do raio para o infinito.

Porém, a consulta por abrangência na Omni B-Tree posicionará cada árvore exatamente na primeira chave da primeira folha.

Um problema é a separação dos elementos do vetor das coordenadas Omni, fazendo com que os objetos sejam contabilizados como candidatos em instantes diferentes, a depender de cada foco.

Na solução adotada a busca é iniciada em cada árvore exatamente onde os respectivos componentes do vetor de coordenadas Omni do objeto de busca seriam inseridos, prosseguindo com sucessivos passos para frente e para traz em cada árvore, alternando entre elas após estes dois passos.

Omni R-Tree.

Este método de acesso métrico utiliza uma R-Tree para indexar as coordenadas Omni.

Esse método mostra praticidade e a validade de se usar um método de acesso espacial para indexar as coordenadas Omni, requerendo poucas mudanças nos algoritmos de manipulação dos índices originais (inserção, remoção e consultas), proporcionando uma melhoria no desempenho do método de acesso métrico gerado com a técnica Omni.

Os algoritmos originais da R-Tree para inserção, particionamento dos nós, consulta por abrangência, entre outros, são os mesmos utilizados pela OmniR-Tree.

Consultas aos k-vizinhos mais próximos podem ser efetuadas diretamente com o algoritmo original de consultas por abrangência da R-Tree.

O algoritmo para consultas aos k-vizinhos mais próximos combina as etapas de filtragem e refinamento.

O processo é visto como uma consulta por abrangência com raio decrescente no índice armazenado na R-Tree.

Sempre que um objeto não consegue ser descartado pela coordenada Omni, deve-se comparar esse objeto diretamente com o objeto de busca.

Se o resultado indicar que este objeto está mais próximo que o último vizinho, ele é inserido na lista de vizinhos mais próximos e o raio é atualizado.

Caso contrário, o objeto é descartado.

O método VA-File (Vector Approximation File) reduz a quantidade de dados que deve ser lida durante buscas por similaridade.

Este método não usa uma estrutura em árvore, mas em vez disso, armazena uma aproximação do vetor de características de cada objeto em um arquivo seqüencial e realiza uma varredura seqüencial do vetor de aproximações.

O vetor de aproximações é uma representação comprimida dos vetores de características originais e são tipicamente 25% do tamanho do vetor de características.

Para obter o vetor de aproximações, o espaço de dados é dividido em vezes ao longo de cada dimensão, onde denota o número de bits usado por dimensão.

Isto divide o espaço de dados em células hiper-retangulares onde é a dimensionalidade do espaço de dados e cada objeto é aproximado pela string de bits da célula.

Para cada busca o VA-File é lido seqüencialmente e um limite inferior e superior da distância atual entre o vetor de características de cada objeto e o objeto de busca é computado.

A distância do limite inferior é computado do objeto de busca para o ponto mais perto na célula do objeto, e o limite superior é computado como a distância do objeto de busca para o ponto mais distante na célula do objeto.

Objetos com limites inferiores maiores do que o raio de busca ou o limite superior menor do que o objeto atual são podados do conjunto de candidatos porque sua distância atual para objeto de busca deve ser grande para o objeto estar no conjunto de resposta.

Isto permite que muitos objetos candidatos sejam descartados sem ter que recuperar o vetor de característica completo do objeto.

Para garantir a resposta correta, é realizado um passo de refinamento, onde a distância atual entre os objetos não podados e o objeto de busca é computado.

Se b escolhido é grande, este método requer a recuperação de um número pequeno de vetores de características no passo de refinamento.

Um problema com o VA-File é que b tem que ser escolhido antes que o VA-File seja construído.

Se b é mal escolhido, ele pode limitar o desempenho do VA-File.

Se b é muito pequeno, um grande número de vetores de características terá que ser recuperado do arquivo de dados original.

Isto é caro, por que cada vetor de características deve ser acessado aleatoriamente porque não existe nenhuma ordem no arquivo de dados original.

O VA-File deve também computar um grande número de cálculos de distância em alta dimensionalidade.

O número de cálculos de distância é linear com a cardinalidade do conjunto de dados.

O VA-File mostrou exibir desempenho linear em relação à cardinalidade e a dimensionalidade do conjunto de dados.

É importante notar que o VA-File pode somente indexar dados espaciais.

Em espaços de alta dimensionalidade, o VA-File mostrou-se mais eficiente do que a R-Tree e a X-Tree.

Foi apresentado o primeiro trabalho usando uma estrutura que usa uma função de mapeamento local para realizar a consulta por abrangência, chamada Similarity Hashing.

Essa estrutura foi melhorada para realizar consulta aos vizinhos mais próximos e rebatizada para D-Index.

Essa estrutura particiona o espaço em regiões disjuntas, ou seja, não existe sobreposição entre os nós de um mesmo nível.

Para armazenar os objetos, a estrutura de mapeamento local usa uma função mapeadora chamada split, onde é um número real limitado como é uma distância máxima.

Essa função mapeadora particiona o espaço métrico em regiões centradas em um objeto chamado ponto de vantagem ou pivô.

Particionamento ao Meio por Exclusão.

Adição de um novo arco para dividir ao meio a região entre os arcos através da distância média.

Assim, um objeto qualquer pode pertencer a regiões.

A idéia dessa estrutura é criar vários níveis onde cada nível usa uma função de particionamento.

O espaço métrico é particionado enquanto houver espaço no bucket.

Quando não houver espaço para o particionamento, cria-se um nível com uma outra função de particionamento que subdivide esse espaço.

Dessa forma, o primeiro nível usa uma função p-split para separar os objetos do conjunto.

Para qualquer outro nível, os objetos são mapeados pela exclusão do bucket do nível anterior.

Finalmente, a exclusão do bucket do último nível forma o bucket de exclusão de toda a estrutura.

Neste capítulo, foram discutidos os métodos de acessos espaciais e métricos.

Sobre os métodos de acessos espaciais foi apresentada a estrutura R-Tree que é a mais citada na literatura.

Essa estrutura permite realizar junção por similaridade, que será apresentada.

Os métodos de acessos métricos oferecem um suporte mais eficiente para responder consultas por similaridade, sendo que, foram apresentados os seguintes métodos, GH-Tree, VP-Tree, MVP-Tree, M-Tree, Slim-Tree, Omni-Family, VA-File e a estrutura.

Vários conceitos apresentados foram usados na criação de um novo método de acesso métrico chamado de coordenadas métricas que é uma contribuição inovadora deste trabalho e será apresentado.

No próximo capítulo serão apresentados vários tipos de junções, e as suas respectivas implementações.

A operação de junção é uma das operações fundamentais de busca em sistemas de gerenciamento de banco de dados relacional.

Ela possibilita a recuperação de dados em duas relações diferentes baseada no produto cartesiano de duas relações.

A operação de junção tem sido estudada e discutida na literatura porque ela é uma operação que consome mais tempo e tem acesso intensivo aos dados em um processamento de consulta relacional.

A operação de junção é usada para combinar tuplas relacionadas de duas relações em tuplas simples que são armazenadas na relação resultado.

O relacionamento desejado entre tuplas ou alguns atributos nas tuplas é especificado em termos da condição de junção.

Na forma mais simples, a junção é escrita, onde define-se a condição de junção.

O operador define a condição que se deve manter verdadeira entre os atributos, respectivamente.

Esta junção é chamada de theta-join.

Nas operações sobre conjuntos de dados que atendem a propriedade de relação de ordem total, o operador teta pode ser um dos seguintes.

A operação de junção é equivalente ao produto cartesiano seguida por uma operação de seleção, onde uma operação de seleção é implícita na condição de junção, ou seja, O resultado de unir duas relações é um subconjunto do produto Cartesiano de duas relações.

O resultado da junção das relações com atributos é a relação atributos.

A relação tem uma tupla para cada par de tuplas de R e S que satifaz a condição de junção.

O resultado da relação pode então ser definida.

Vários tipos de junção têm sido definidos.

O operador theta mais usado é o operador de igualdade e nestes casos a junção é chamada de equijoin.

Para todos outros operadores theta a junção é chamada de uma nonequijoin.

A relação resultante é definida.

A relação resultante contêm tuplas compostas de duas partes, onde deve ser uma tupla da relação e deve ser uma tupla da relação.

Em cada tupla, os valores dos atributos da junção são idênticos em todos aspectos aos valores dos atributos da junção.

A junção natural pode ser definida como a equijoin sobre os atributos com o mesmo nome em ambas as relações, seguido por projeção para remover um conjunto dos atributos da junção.

Essa junção natural pode ser escrita, onde são os atributos em relação.

A relação resultante é dada.

Na execução de uma operação de junção convencional, a relação resultante tem todos os atributos de ambas as relações de entrada.

Algumas vezes, é requerido que somente os atributos de uma das relações esteja presente na relação de saída.

A operação semijoin é projetada para realizar tal junção.

Ela também tem sido definida como uma operação que seleciona o conjunto das tuplas de uma relação que se relaciona com uma ou mais tuplas da outra relação.

O relacionamento é definido pela condição de junção.

Ela é equivalente à junção de duas relações seguidas por uma operação de projeção que resulta nos atributos da segunda relação.

A própria junção inicial pode ser realizada por qualquer uma das técnicas de junção.

A operação semijoin é escrita e a relação resultante é dada.

Diferente da maioria das operações de junção, a operação semijoin não é comutativa, isto é, embora o efeito seja o mesmo em ambos os casos, esta versão da semijoin reduz o tamanho da segunda relação participante na operação de junção.

Uma expressão alternativa para a operação de junção semijoin.

Nenhuma condição de junção é especificada nesta expressão por que ela representa o caso geral.

Semijoin pode ser usada para reduzir a carga de processamento de junção regular e para evitar a criação de grandes relações intermediárias.

Ela é especialmente útil em processamento distribuído, para diminuir a quantidade de dados que trafegam entre unidades de processamento.

Operação outerjoin é uma extensão da operação de junção.

Ela é também chamada de junção externa.

Três tipos de outerjoin são definidas, a left-outerjoin, a right-outerjoin e a full-outerjoin.

Os símbolos de junção correspondente.

Tanto a junção left-outerjoin quanto a right-outerjoin são coletivamente referidas como junção direcional.

Deve-se notar que somente a junção full-outerjoin é comutativa.

Embora as junções left/right-outerjoin não sejam comutativas, elas são apresentadas.

A relação resultante pode ser escrita, onde representa o enchimento das tuplas com valores nulos.

O self-join pode ser considerado um caso espacial do theta-join.

A única diferença é que a relação de entrada é unida com ela mesma.

A relação de saída é dada.

Composition.

A composição de duas relações é equivalente a junção de duas relações, e formalmente isto é descrito, onde representa o operador de composição sem a condição de junção representa qualquer junção entre as relações.

A composição natural é definida como a composição baseada na junção natural.

A relação resultante, da composição natural, é dada.

Division.

A operação de divisão não é um membro do conjunto de operações de álgebra relacional.

O operador de divisão permite uma tupla ser recuperada de uma relação se ela está relacionada a todas as tuplas em outra relação baseada em alguma condição pré definida.

Em outras palavras, a relação resultante é o subconjunto da relação, tal que o produto cartesiano das relações esteja contido no conjunto.

As técnicas e métodos usados para implementar junções são apresentados e discutidos nas próximas subseções.

Este é o método de junção mais simples, e segue da definição da operação de junção.

Uma das relações sendo unidas é designada como a relação mais interna, e a outra é a mais externa.

Para cada tupla da relação mais externa, todas as tuplas da relação mais interna são lidas e comparadas com a tupla da relação mais externa.

Quando a condição de junção é satisfeita, as duas tuplas são concatenadas e colocadas no buffer de saída.

Note que para melhorar o desempenho do algoritmo, a relação com a cardinalidade mais alta deve ser escolhida para ser a relação mais interna.

O algoritmo de junção nested-loops é implementado como uma junção nested-block.

Tuplas são recuperadas em unidades de blocos.

Esta implementação pode ser descrita da seguinte forma, a relação mais interna é lida em um bloco uma vez.

O número de blocos de memória principal disponível, determina o número de blocos lidos da relação mais externa.

Então todas as tuplas do bloco da relação mais interna são unidas com todas as tuplas do bloco da relação mais externa.

Este processo é repetido com todos os blocos da relação mais interna antes que o próximo conjunto do bloco da relação mais externa seja lido.

A junção sort-merge é executada em dois estágios.

Primeiro, ambas relações são ordenadas pelos atributos da condição de junção.

Então, ambas as relações são examinadas pela ordem dos atributos da junção, e as tuplas que satisfizerem a condição de junção são unidas para formar a relação resultado.

Quando as tuplas da primeira relação combinam com uma tupla da segunda relação, as tuplas são concatenadas e colocadas na relação de saída.

O algoritmo exato para realizar o sort-merge join depende de se os atributos da junção são ou não atributos sem chave e do operador theta.

Em todos os casos, no entanto, é necessário que as duas relações estejam fisicamente ordenadas por seus respectivos atributos de junção.

O Algoritmo 3mostra como realizar equijoins.

O processo merge varia ligeiramente dependendo se os atributos da junção são chaves primárias, secundárias ou sem chave.

Se os atributos da junção não são atributos chave primária, várias tuplas com o mesmo valor de atributo pode existir.

Isto necessita de vários passos sobre o mesmo conjunto de tuplas da relação interna.

O processo é descrito abaixo.

Suponha-se que existem duas tuplas, que tem um dado valor do atributo de junção e três tuplas que têm o mesmo valor do atributo de junção.

Se o algoritmo de junção acima é usado onde é a tupla corrente em R, a tupla corrente em S seria a tupla s3.

Agora a relação resultante deve também incluir a junção de rcom s1, se s3.

Para alcançar isto, o algoritmo acima deve ser modificado para relembrar o último valor e o ponto onde ele iniciou o último loop interno.

Quando encontra um valor duplicado, retrocede-se a recursão ao ponto inicial anterior.

Este retrocesso pode ser especialmente caro em termos, se o conjunto de tuplas não se adequa na memória principal disponível e as tuplas têm que ser recuperadas do armazenamento secundário para cada passo.

O algoritmo sort-merge join pode também ser usado para implementar a outerjoin completa.

O Algoritmo 3mostra como realizar a outerjoin completa usando o método sort-merge.

O sucesso da junção sort-merge está no fato de que ela reduz o número de comparações entre tuplas.

Uma tupla da primeira relação não é comparada com aquelas tuplas na segunda relação em que não é possível fazer a junção.

Os métodos de junção hash alcançam o mesmo efeito de outra maneira.

Eles tentam isolar as tuplas da primeira relação em que pode ser feita a junção com uma dada tupla da segunda relação.

Então, tuplas da segunda relação são comparadas com um conjunto limitado de tuplas da primeira relação.

Um grande número de métodos de junção usando hashing tem sido proposto e/ou implementado.

Alguns serão discutidos aqui.

Método de Junção Simples.

Com este método, os valores dos atributos da junção na primeira relação serão usados pela função hash.

Estes valores da chave hash apontam para a tabela hash em que cada entrada pode conter tuplas ou ids das tuplas.

Pode ser útil armazenar os valores das chaves também.

Dependendo da eficiência da função hash, uma ou mais entradas podem ser mapeadas para o mesmo bucket.

Então para cada tupla da outra relação participante na junção, os valores dos atributos da junção são mapeados usando a mesma função hash.

O Algoritmo 3mostra como realizar.

No algoritmo 34, a mesma função hash deve ser usada para ambas as relações.

O método de junção hash pode também ser usado para implementar as junções right/left outer-joins.

O algoritmo 35 mostra como realizar a right-outerjoin, usando a técnica de junção hash simples.

Junção Particionada.

A junção hash particionada tenta otimizar a execução da junção particionando o problema em partes.

É usado hash para decompor o problema de junção em vários subproblemas que são mais fáceis para resolver.

A abordagem dividir-e-conquistar tem sido usada por apresentar várias vantagens, não só a eficiência do processo como um todo é melhorada, mas também o particionamento faz com que a paralelização do algoritmo seja mais fácil permitindo que pares independentes sejam processados, o que resulta na melhora de desempenho.

Uma função hash é usada para particionar as tuplas em cada relação em um número fixo de conjuntos sem junção.

As tuplas no primeiro conjunto de uma relação podem combinar somente com as tuplas no primeiro conjunto na segunda relação.

Assim, o processamento de diferentes pares de partições correspondentes nos dois conjuntos são independentes de outros conjuntos.

Como resultado, esses pares de partições podem ser processados em paralelo.

Tuplas nos pares de partição de ambas as relações podem ser unidas por uma outra junção hash ou qualquer outro método de junção.

Junção Particionada Simples.

Na junção hash particionada simples nem todas as partições são criadas no início da operação.

Em vez disso, cada par de partição é criado e usado antes que a próxima seja gerada.

O número de partições e a área dos valores dos atributos em cada partição são percorridos.

Antes das relações serem percorridas, a área dos valores hash para a posição atual é escolhida.

Idealmente, a área deve ser tal que todas as partições sejam de tamanhos iguais.

As áreas são determinadas dividindo a área total dos valores dos atributos em subáreas de tamanhos iguais.

O número de subáreas é igual ao número de partições desejadas.

Isto depende de fatores como a quantidade de memória disponível e o número de processadores.

A relação externa é percorrida e os valores dos atributos são mapeados.

Se o valor mapeado cai na área de valores mapeados atuais, a tupla é inserida na tabela hash na partição atual.

Por outro lado, ela é escrita em um arquivo temporário.

A partição da tabela hash está completa quando toda a relação tiver sido percorrida.

A relação interna é percorrida e os atributos da junção são mapeados usando a mesma função hash.

Se o valor mapeado cai na área atual, o processo de junção é iniciado.

Por outro lado, a tupla é escrita em um arquivo temporário.

Temp-R e temp-S são relações temporárias usadas para evitar o processamento de todas as tuplas em cada passo no algoritmo.

A relação menor é particionada primeiro para reduzir o tempo de execução.

Junção.

Esta é uma variação da técnica de junção nested-loops.

Na primeira fase, a relação externa é dividida em um número fixo de partes.

Próximo passo, cada partição é lida na memória uma vez.

Uma tabela hash é construída para cada partição quando ela é lida da memória principal.

As tuplas na relação interna são lidas.

Os atributos da junção interna são mapeados, e os valores mapeados são usados para investigar a tabela hash para que a partição atual possa combinar as tuplas.

Assim, a relação interna é lida uma vez para cada partição, e a tabela hash é usada para acelerar o processo de encontrar uma combinação.

O algoritmo 37 apresenta como realizar.

Durante as últimas décadas, sistemas de base de dados espaciais têm tornado-se mais e mais importantes para indústria, administração pública, ciência e negócios.

Vários sistemas de base de dados espaciais, projetados para organizar dados espaciais de um sistema de informação geográfica, têm sido desenvolvidos para aplicações tais como cartografia, ciência ambiental e geografia.

Um objeto espacial consiste de (no mínimo) um atributo espacial que descreve a geometria do objeto.

Estes atributos contêm dados de duas ou três dimensões de um tipo comum como por exemplo, pontos, linhas, retângulos, polígonos e até tipos mais complexos compostos de tipos simples.

O processamento eficiente de uma junção espacial é importante, uma vez que seu tempo de execução é superlinear no número de objetos espaciais das relações participantes, e o número de objetos pode ser grande.

A junção espacial é definida sobre duas ou mais relações.

Ela computa um subconjunto do produto cartesiano e combina objetos espaciais destas relações de acordo com seus atributos geométricos, isto é, estes atributos têm que atender a um predicado espacial.

A junção espacial é uma das operações mais freqüentes em um sistema de bases de dados espaciais.

O desempenho de um sistema de bases de dados espaciais é provável ser mais afetado por uma operação de junção espacial do que por uma operação de seleção espacial, porque uma junção espacial tem que acessar os objetos várias vezes.

Seu tempo de execução é geralmente superlinear no número de objetos.

Apesar da importância da junção espacial, a maioria das pesquisas tem sido voltada para junção natural e equijoins.

Quase todos os métodos projetados para processamento de junção-relacional não podem ser usados para junções espaciais sem modificações.

Junções baseadas em hash não são apropriadas para realizar junções espaciais já que os objetos espaciais, os quais devem atender ao predicado da junção, não estão em um bucket comum no hash.

Junções sort-merge podem ser aplicadas para junções espaciais, mas estas requerem uma ordem dimensional dos objetos bi-dimensionais.

Assim, somente a abordagem da junção nested-loops pode ser usada sem quaisquer modificações para ambas as junções, espacial e relacional.

A primeira abordagem, para realizar uma junção espacial com R-Tree, é usar os retângulos MBR que redirecionam para as sub-árvores internas.

Se os retângulos das duas entradas não têm uma interseção comum, não existirá nenhum par de retângulos de dados intersectando onde rect está na subárvore e rect está na subárvore.

Caso contrário, deve-se navegar em um nível abaixo e continuar verificando quais retângulos se intersectam.

O Algoritmo 38 apresenta a abordagem descrita acima.

Aqui é assumido que R e S são do tipo nó na R-Tree, sendo que cada nó possui uma coleção de entradas.

Dois Conjuntos de Retângulos e sua Projeção no Eixo X.

Cada entrada consiste de um ponteiro ref e um retângulo rect.

O algoritmo percorre recursivamente ambas as árvores de modo top-down verificando cada entrada de um nó com todas as entradas do outro nó.

O Algoritmo 39 usa a técnica de restrição de espaço de busca para melhorar o Algoritmo 38.

Essa técnica consiste em usar o retângulo de interseção entre os nós do nível superior para restringir os espaços.

No Algoritmo 39, rect é o retângulo de interseção MBR dos nós R e S e é usado para restringir o espaço para todas as entradas de R e S.

A segunda abordagem apresentada, para o melhoramento da junção espacial, é ordenar as entradas em um nó da R-Tree de acordo com a localização espacial dos retângulos correspondentes.

O problema acontece pois retângulos bi-dimensionais não podem ser ordenados, isto é, mapeados para uma seqüência dimensional, sem perda de localização.

A solução adotada foi utilizar a interseção da seguinte maneira, considere uma seqüência de retângulos, sendo que um retângulo qualquer é composto por seu canto esquerdo inferior e seu canto direito superior.

A seqüência é ordenada em relação ao eixo.

Por exemplo, a seqüencia ordenada de 6 retângulos mostrada.

Uma outra abordagem chamada Plane Sweep (varredura de plano) é uma técnica comum para interseções.

A idéia básica é mover uma linha, também chamada de linha de varredura na perpendicular para um dos eixos, isto é, o eixo X da esquerda para direita.

Dado duas seqüências de retângulos, explora-se a técnica de varredura de plano sem o overhead de construir qualquer estrutura de dados dinâmica.

Primeiro os retângulos são ordenados.

Então move-se a linha de varredura para o retângulo.

Agora, conhecido o intervalo que intersecta o intervalo para todos.

Se o intervalo também intersecta o intervalo, então, o retângulo também intersecta o retângulo.

Depois disso, o retângulo é marcado para ser processado.

Então, a linha de varredura é movida para o próximo retângulo desmarcado inferior e o mesmo passo como foi descrito acima é repetido para todos os retângulos não marcados.

Quando as últimas entradas forem processadas, todas interseções são computadas.

A descrição formal do algoritmo é mostrada nos Algoritmos 310 e 311.

Um exemplo de como o algoritmo procede é ilustrado.

A linha de varredura pára nos retângulos.

Para cada parada, os pares de retângulos que são testados para interseção são mostrados no lado direito.

Exemplo para o Teste de Interseção Ordenada.

O algoritmo plane-sweep (varredura de plano) determina as interseções de dois conjuntos de retângulos.

Baseado na ordenação espacial, este algoritmo cria uma seqüência de interseção de pares de retângulos.

Esta seqüência pode também ser usada para determinar o escalonamento de leitura da junção espacial.

Para preservar a localidade espacial no buffer, esta abordagem é usada sem qualquer custo extra.

Esta abordagem é chamada de ordenação por varredura de plano local, "local plane-sweep order", e corresponde ao Algoritmo 312.

Existem duas outras variações de ordenação baseadas em varreduras de planos (plane-sweep) que são, Ordenação por Varredura de Planos Locais com Pinos, Primeiro, o algoritmo é igual ao de planos locais que determina um par de entradas.

Após processar as sub-árvores é computado o grau do retângulo de ambas entradas.

O grau de um retângulo de uma entrada é dado pelo número de interseção entre os retângulos, e os retângulos que pertencem às entradas da outra árvore não processadas até agora.

Segundo, fixa-se a página no buffer com o retângulo que tem o grau máximo.

Terceiro, a junção é realizada na página fixada com todas as outras páginas.

Então determina-se o próximo par de entradas usando novamente a ordenação por varredura de planos locais.

Ordenação em Local, a idéia básica é realizar a ordenação através dos centros do retângulos representado por pontos bidimensionais.

Uma técnica para ordenar pontos bidimensionais é baseada em curvas de preenchimento de espaço, por exemplo curvas de Peano, também chamada de ordenação-Z.

Essa técnico consiste em decompor o espaço em células de tamanho igual e realizar a ordenação nesse conjunto de células.

Ordenação espacial usando Ordenação-Z.

Um exemplo dessa ordenação é ilustrado.

No diagrama mais à esquerda, todas as interseções são computadas entre os retângulos.

Isto resulta no diagrama central, que representa os centros dos conjuntos de retângulos por pontos na grade.

A ordem das células da grade é determinada pela ordenção-Z que está ilustrada no diagrama mais a direita.

As operações de junção por similaridade combinam dois conjuntos de objetos complexos em que o resultado contém todos os pares de objetos similares.

Os tipos de junção por similaridade são, por abrangência, por vizinhos mais próximos e por proximidade (do inglês Closest Neighbors Join).

Diferença entre as Operações de Junção por Similaridade.

Diferenças entre as operações de junção por similaridade.

Como mostrado, a junção por abrangência de dois conjuntos, é o conjunto de pares onde a distância dos objetos não excede o parâmetro.

Junção por proximidade que recupera os pares com distância mínima.

Junção por vizinhos mais próximos que recupera para cada objeto do conjunto todos os objetos mais próximos do conjunto.

Pontos podem aparecer uma ou mais vezes se o objeto estiver entre mais próximos de vários pontos.

A junção por similaridade tem sido um mecanismo importante para auxiliar buscas por similaridade e data mining.

A seguir serão apresentadas as implementações das junções por similaridade usando a estrutura espacial R-Tree.

Formalmente, uma operação de junção por abrangência (Range Join) pode ser definida.

Assim, a junção por abrangência recebe como parâmetros dois conjuntos de dados, ambos pertencentes a um mesmo domínio, e uma distância máxima de busca, sendo que as respostas para essa consulta são pares de objetos, um de cada subconjunto, que estão distantes entre si por no máximo um valor igual ao raio dado.

O algoritmo apresentado pode ser adaptado para fazer consulta por abrangência usando a R-Tree.

Algumas estruturas baseadas em grids que melhoram a junção por abrangência são EGO (Epsilon Grid Order).

Formalmente a junção por vizinhos mais próximos (k-nn (k-Nearest Neighbor Join)) é definida como o menor subconjunto que contém para cada ponto no mínimo pontos para os quais a seguinte condição se mantém.
Assim, a junção de vizinhos mais próximos recebe como parâmetro dois conjuntos de dados R e S, ambos pertencentes a um mesmo domínio, e uma quantidade de k qualquer, sendo que, as respostas para essa consulta são os k pares de objetos mais próximos do conjunto R para cada objeto do conjunto S.

Em caso de empate de distâncias na maior distância obtida, pode-se selecionar apenas o número necessário de objetos que satisfazem a regra, ou criar uma lista de empates.

Algumas implementações de junção por vizinhos mais próximos usando a R-Treesão, o algoritmo Z-Order, GORDER (G-ordering KNN) que substitui a contagem das interseções na R-Tree pela análise de componentes principais (Principal Component Analysis).

Uma extensão feita na R-Tree originou a estrutura MUX (Multipage Index).

A operação de busca por proximidade recupera os pares que têm a menor distância entre os elementos de cada par.

Formalmente, uma junção por proximidade pode ser definida por, é o menor subconjunto que contém no mínimo pares de pontos e para o qual a seguinte condição se mantém.

Alguns algoritmos para busca de pares mais próximos, serão apresentados em seguida.

Os algoritmos apresentados utilizam a R-Tree.

Algoritmo Simples.

A introdução mais simples do problema de busca por proximidade é seguir uma simples solução recursiva para o sub-problema CP e para duas R-Trees de mesma altura.

Tal algoritmo consiste dos seguintes passos, Passo 1, começar das raizes das duas R-Trees e ajustar a distância mínima encontrada.

Passo2, se acessar um par de nós internos, processar os descendentes recursivamente para todos possíveis pares de MBR.

Passo3, se acessar dois níveis, calcular a distância de cada possível par de pontos.

Se esta distância é menor, atualizar.

Algoritmo Completo.

O melhoramento do algoritmo anterior é fazer uso da parte esquerda da desigualdade 1 e podar alguns caminhos nas duas árvores que não são adequados para conduzir para a melhor solução.

Isto é, processa os descendentes somente para aqueles pares de MBR que satisfazem esta propriedade.

O passo do algoritmo anterior agora seria, Passo 2, se acessar um par de nós internos, calcular MINMINDIST para cada possível par de MBR.

Processar os descendentes recursivamente somente para aqueles pares que têm MINMINDIST T.

Algoritmo Recursivo Simples.

Um melhoramente adicional é tentar minimizar o valor de T assim que possível.

Isto pode ser feito utilizando a desigualdade 2, isto é, quando um par de nós internos é visitado, examinar se a desigualdade aplicada para todos pares de MBR, pôde dar um valor de T menor.

Como a desigualdade mantém no mínimo um par de pontos, este melhoramento é produzido para o problema.

O passo agora seria, Passo 2, se acessar um par de nós internos, calcular o mínimo de MINMAXDIST para todos possíveis pares de MBRs.

Se este mínimo é menor que T, atualizar T.

Calcular MINMIN-DIST para cada possível par de MBR.

Processar os descendentes recursivamente somente para aqueles pares que têm MINMINDIST T.

Algoritmo Recursivo de Distâncias Ordenadas.

Pares de MBR que têm MINMINDIST menor são mais adequados para conter o CP e conduzir para um T menor.

Uma heurística que visa melhorar os algoritmos ainda mais quando dois nós internos são acessados, é ordenar os pares de MBR de acordo com a ordem ascendente de MINMINDIST e obedecer esta ordem processando os descendentes recursivamente.

Esta ordem de processamento é exigida para melhorar a poda dos caminhos.

O passo do algoritmo anterior seria, Passo 2, se acessar pares de nós internos, calcular o mínimo de MINMAXDIST para todos possíveis pares de MBR.

Se este mínimo é menor que T, atualizar T.

Calcular MINMINDIST para cada possível par de MBR e ordenar estes pares em ordem ascendente de MINMINDIST.

Seguindo esta ordem, processar os descendentes recursivamente somente para aqueles pares que tem MINMINDIST T.

Este algoritmo não é recursivo.

A fim de superar a recursão e manter a rotina de processamentos dos ascendentes enquanto acessa as duas árvores, é usado um heap.

Este heap mantém pares de MBR de acordo com seus MINMINDIST.

O par com menor valor consiste no topo do heap.

Este par é o próximo candidato a ser visitado.

O algoritmo é como segue, Passo 1, iniciar das raízes das duas R-Trees, ajustar T para 1 e inicializar o heap.

Passo 2, se acessar um par de nós internos, calcular o mínimo de MINMAXDIST para todos possíveis pares de MBR.

Se este mínimo é menor do que T, atualizar T.

Calcular MINMINDIST para cada possível par de MBR.

Inserir no heap aqueles pares que têm MINMINDIST T.

Passo 3, se acessar dois níveis, calcular a distância de cada possível par de pontos.

Se esta distância é menor que T, atualizar T.

Passo 4, se o heap está vazio então interromper, e Passo5, pegar o par no topo do heap.

Senão repetir o algoritmo do passo para este par.

Note que dois ou mais pares que tem o mesmo valor de MINMINDIST podem também aparecer no algoritmo.

Se este valor é o mínimo, então mais do que um par apareceria próximo ao topo do heap.

As rotinas de tratamento do heap são responsáveis por especificar o par que finalmente ocupa o topo.

Tratamento de Empates.

Nos algoritmos onde empates de valores de MINMINDIST podem aparecer, é possível alcançar um melhoramento adicional escolhendo o próximo par no caso de um empate usando alguma heurística (e não seguindo a ordem gerada pela ordenação ou pelo algoritmo de tratamento do heap).

A lista seguinte apresenta cinco critérios que são adequados para melhorar o desempenho.

Assim, no caso de um empate de dois ou mais pares, escolhe-se o par que tem, como um de seus elementos a maior MBR a área da MBR é representada como uma porcentagem da área da raiz apropriada.

O menor MINMAXDIST entre seus dois elementos.

A maior soma das áreas de seus dois elementos.

A menor diferença de áreas entre a MBR.

A maior área de interseção entre seus dois elementos.

Tratamento de Diferentes Alturas.

Quando as duas R-Trees armazenam os dois conjuntos de pontos que tem alturas diferentes os algoritmos são levemente mais complicados.

Em algoritmos recursivos, existem duas abordagens para tratamento de diferentes alturas.

A primeira abordagem é chamada de fix-at-root.

A idéia é, quando o algoritmo é chamado com um par de nós internos em diferentes níveis, o processamento aos ascendentes termina na árvore de nó de nível mais baixo, enquanto que o processamento na outra árvore continua até um par de nós no mesmo nível seja atingido.

Então o processamento continua em ambas as sub-trees, e processa os descendentes recursivamente como habitual.

A segunda abordagem é chamada de fix-at-leaves e trabalha de maneira oposta.

Quando o algoritmo é chamado com um nó folha de um lado e um nó interno de outro, o processamentos dos descendentes pára na árvore do nó folha, enquanto o processamento nas outras árvores continua.

Por exemplo, para o algoritmo Simples, a aplicação da estratégia fix-at-leaves resulta no seguinte passo extra, Passo 21, se acessa um par comum folha e um nó interno,calcula o mínimo de MINMAXDIST para todos possíveis pares de MBR que consistem da MBR deste folha e a MBR contida no nó interno.

Se este mínimo é menor do que T, atualiza T.

Calcula também MINMINDIST para todos estes pares.

Para cada par, faz uma chamada recursiva com parâmetros dos nós correspondente às MBR do par (note que um parâmetro será o nó folha e o outro parâmetro será um filho do nó interno).

Enquanto que a aplicação da estratégia fix-at-root, para o mesmo algoritmo resulta no seguinte passo extra, Passo 21, se acessa dois nós residindo em diferentes níveis, calcula o mínimo de MINMAX-DIST para todos possíveis pares de MBR que consistem de um MBR contido no nó de nível mais alto e a MBR do nó no nível mais baixo.

Se este mínimo é menor do que T, atualiza T.

Calcula também MINMINDIST para todos estes pares.

Para cada par que tem MINMINDIST T, faz uma chamada recursiva com parâmetros do nó correspondente à MBR do par (note que um parâmentro será o nó residente no nível mais baixo).

O algoritmo do Heap pode ser modificado para lidar com diferentes alturas pelas estratégias de fix-at-root e fix-at-leaves.

O passo extra que precisa ser adicionado ao algoritmo Heap é semelhante a um dos passos apresentados acima, dependendo da estratégia.

A única diferença é que a chamada recursiva é substituída pela inserção no heap.

Estendendo para os pares mais próximos para esclarecer o problema é necessário uma estrutura extra que mantém os pares mais próximos.

Esta estrutura é organizada com um k-heap e mantém pares de pontos conforme suas distâncias.

Os pares de pontos com a maior distância sempre ficam no topo do heap.

Inicialmente o k-heap está vazio.

Os pares de pontos explorados no passo são inseridos no k-heap até ele ficar cheio.

Então, quando um novo par de pontos é explorado no passo e se sua distância é menor do que o topo do k-heap, o topo é removido e seu par é inserido no k-heap.

As adições acima são necessárias e suficientes para os algoritmos simples e completo para resolver o problema K-CP e foram usados na implementação das versões K-CP destes algoritmos.

Estas adições não são suficientes para os algoritmos recursivo simples, recursivo de distâncias ordenadas e heap.

Estes algoritmos fazem uso da desigualdade que, em geral não mantém mais de um par de pontos.

Isto significa que atualizar T baseado nos valores de MINMAXDIST deve ser descartado.

Uma simples modificação que pode fazer esses três algoritmos adequados para resolver o problema K-CP é usar a distância do topo do k-heap como valor de T, logo após o k-heap ter k elementos.

Enquanto o k-heap tem slots vazios, infinito seria usado como T.

O melhoramento da modificação acima (que foi usada na implementação das versões k-CP dos três algoritmos) é fazer uso da parte direita da desigualdade 1, enquanto poda caminhos desnecessários nas duas árvores.

Isto é, entre um número de pares de MBR para encontrar um com MAXMAXDIST que pode atualizar o valor de T.

Note que um par, respectivamente, pode indicar no mínimo pares de pontos que são as saídas mínimas.

Isto leva à seguinte política quando o algoritmo trata um grupo de pares é calculado para cada par.

Os pares são ordenados de acordo com MAXAXDIST.

Os algoritmos recursivo simples e recursivo de distâncias ordenadas aplicam esta estratégia para atualizar T no passo(e passo 21 em caso de alturas diferentes).

O grupo de pares de MBR que pegam parte nos cálculos acima é o grupo de pares já usados neste passo.

O algoritmo Heap também aplica esta estratégia para atualizar T no passo (e passo 21 em caso de alturas diferentes).

De qualquer modo, o grupo de pares de MBR que pegam parte nos cálculos acima é o grupo de pares já usados neste passo mais os pares de MBR já inseridos no heap.

Neste capítulo, foram discutidos os tipos e implementações de junção.

Sobre as junção por similaridade, a literatura cita somente o uso da estrutura espacial R-Tree e variações.

Os tipos de junção por similaridade citados foram, por abrangência, por vizinhos mais próximos e por proximidade (do inglês closest neighbors join).

No próximo capítulo será apresentada nova estrutura métrica baseada em particionamento do espaço métrico, seus algoritmos de consulta por abrangência e por vizinhos mais próximos e experimentos.

Muitas estruturas de indexação utilizam o conceito de região de cobertura para estabelecer seus níveis hierárquicos, sendo que as regiões de um mesmo nível podem apresentar sobreposição entre si.

Esse conceito é aplicado tanto na construção de diversos métodos de acesso métrico quanto de métodos de acesso espacial.

Quando um algoritmo de consulta intercepta uma região de sobreposição, ele é obrigado a navegar por todas as regiões que compõem a sobreposição.

Dessa forma, deve-se evitar que exista muita sobreposição para não prejudicar os algoritmos de consulta.

No entanto, o algoritmo de quebra de nós da operação de inserção pode incrementar a cobertura de um nível mais baixo e processar o incremento até o nível raiz, uma vez que os níveis superiores mantêm a cobertura de todos seus níveis inferiores.

Esse incremento recursivo nas regiões de cobertura, originado pela quebra de nós da operação de inserção, aumenta a sobreposição entre regiões de um mesmo nível.

Quando uma estrutura apresenta diversos níveis e muita sobreposição entre as regiões de cobertura de um mesmo nível, os algoritmos de consultas acabam realizando um número maior de acessos a disco e cálculos de distância do que se a estrutura apresentasse poucos níveis.

Para não gerar estruturas altas que aumentam o custo das consultas por similaridade, adota-se um tamanho de bloco maior em disco fazendo com que a estrutura cresça em largura e diminua em altura.

A altura máxima que não prejudica o desempenho das consultas depende muito da quantidade de registros, do tipo de dado e da distribuição de inserção dos dados, o que dificulta a estimativa do tamanho de bloco.

Portanto, estruturas que não apresentam sobreposição entre os níveis não influenciam seus algoritmos de consulta por similaridade com o crescimento da altura da estrutura.

Um conceito que não apresenta sobreposição entre elementos de um mesmo nível é o de particionamento disjunto de região.

Esse conceito já é usado em algumas estruturas como a D-Index.

Neste capítulo dois novos métodos de acesso métrico que usam um novo conceito de particionamento disjunto de região, chamado de coordenada métrica.

Este trabalho introduz um novo método para particionar o espaço métrico em regiões disjuntas empregando o conceito, também desenvolvido neste trabalho, chamado de coordenadas métricas.

Uma coordenada métrica é um vetor de valores inteiros sendo que é a quantidade de mapeadores.

Um mapeador é um objeto escolhido entre os objetos armazenados no nó.

Inicialmente uma coordenada métrica usa dois mapeadores, sendo que a quantidade de mapeadores pode ser incrementada a qualquer momento.

A coordenada métrica é definida através da função que mapeia qualquer objeto com um conjunto de objetos mapeadores.

Formalizando, Para realizar esse mapeamento, a função usa as distâncias entre pares de mapeadores.

Essa distância é definida através de uma função de distância métrica.

As distâncias entre os mapeadores formam uma matriz onde cada linha dessa matriz apresenta intervalos de distâncias para um mapeador base.

Por definição se um elemento deve-se inverter os valores dos elementos para manter uma ordem crescente de distâncias para o mapeador base, ou cada coordenada c é definida através da distância entre um objeto e o objeto mapeador base.

O valor é determinado pelo índice do intervalo de distâncias em que a distância entre um objeto qualquer e um objeto mapeador base satisfaz a expressão.

Para ser armazenado, o vetor de inteiros das coordenadas métricas é convertido para um valor numérico, onde c é a coordenada métrica e n é a quantidade de objetos mapeadores.

As coordenadas métricas permitem extensão ou incremento da quantidade de coordenadas através da adição de um novo objeto mapeador.

Essa característica de extensibilidade será apresentada.

Os exemplos a seguir ilustram o conceito de coordenadas métricas desde a criação e a extensibilidade.

Inicialmente um bucket folha recebe objetos sem que seja necessário mapear para uma coordenada métrica, até que esse bucket folha fique cheio.

Quando o bucket folha fica cheio, é necessário dividir os objetos armazendos nesse bucket entre dois buckets.

Existem três formas de divisão do bucket folha, divisão com escolha de dois mapeadores, divisão de valores de coordenadas e divisão com incremento de mapeadores.

A divisão com escolha de dois mapeadores cria o esquema de coordenadas métricas através de dois objetos mapeadores que particionam o espaço métrico em regiões.

Particionamento com mapeadores.

Exemplo de um espaço bidimensional euclidiano que é inicialmente particionado através de dois mapeadores, e uma distância entre esses mapeadores.

Assim qualquer objeto terá coordenadas métricas que variam de 0 até 1.

Para todo objeto, pertencente ao bucket folha cheio, deverá ser calculada a sua coordenada métrica através da função.

A coordenada de cada objeto deve ser convertida para o valor numérico a ser armazenado, pela forma mencionada anteriormente.

Assim, o objeto que tem a coordenada pertence à região em relação ao primeiro mapeador e na região 1 em relação ao segundo mapeador.

Todos os objetos com suas respectivas coordenadas são divididos entre dois buckets folhas, sendo que um mesmo bucket deve receber todas coordenadas repetidas.

A estrutura de índice deve guiar os valores de coordenadas para os respectivos buckets.

A divisão de valores de coordenadas ocorre quando um bucket folha cheio possuir coordenadas diferentes permitindo a divisão das coordenadas métricas entre o bucket cheio e um novo bucket.

A divisão com incremento de mapeadores ocorre quando todas as coordenadas de um bucket cheio possuem o mesmo valor, não sendo mais possível realizar a operação divisão de valores de coordenadas.

Particionamento com Mapeadores.

Nesse momento deve-se estender a coordenada métrica escolhendo um novo mapeador, sendo que três mapeadores particionam o espaço métrico em regiões.

Duas representações de particionamento do espaço métrico usando mapeadores assumindo um espaço bidimensional euclidiano.

No lado esquerdo, o mapeador particiona o espaço com duas circunferências concêntricas, uma com raio que é sua distância. para o mapeador E e outra com raio r que é sua distância para o mapeador E.

Esse mesmo princípio é aplicado para os mapeadores que também particionam o espaço com duas circunferências concêntricas, delimitando as 27 regiões disjuntas.

O lado direito apresenta três semi-retas que indicam os possíveis valores para cada coordenada.

Os segmentos das retas representam uma escala de distâncias ordenadas entre o objeto mapeador base do segmento e os demais objetos mapeadores.

Dessa maneira, qualquer objeto terá coordenadas métricas que variam de 0 até 2.

O valor da primeira coordenada depende da distância do objeto para o mapeador podendo ser, As demais coordenadas obedecem ao mesmo principio, mudando a ordem para os mapeadores.

Um exemplo pode ser visto, onde um objeto qualquer é mapeado para a coordenada métrica.

Essa coordenada indica que o objeto encontra-se na primeira região em relação ao mapeador, na segunda região em relação ao mapeador e na terceira região em relação ao mapeador.

Objeto mapeado para a região.

Extensibilidade das Coordenadas Métricas.

Particionamento com Mapeadores.

Extensão para Mapeadores.

Espaço métrico particionado com objetos mapeadores.

Suponha-se agora que, um grande número de objetos mapeia para a região {2,1,0} e por isso essa região precisa ser dividida.

Assim, é escolhido um objeto mapeador da região dividindo essa coordenada, atualizando assim a matriz.

O importante é fazer o ajuste nas coordenadas antigas, sendo que, todas as regiões com numeração maior do que a região dividida deverão ser incrementadas em um, isso pode ser feito sem nenhum novo cálculo de distância.

Todas as regiões com numeração menor do que a região dividida deverão permanecer com a mesma numeração, o que também não causa nenhum cálculo de distância.

E para todas as regiões com numeração igual à região dividida, deve-se calcular a distância para o novo objeto, para determinar se esse permanece com o mesmo valor ou se incrementa em um.

Além de ajustar as coordenadas antigas, deve-se calcular um novo dígito em relação ao mapeador.

Por exemplo, um objeto com coordenada antiga em que o novo mapeador foi escolhido na região será convertida para, primeiro dígito permanece 1, pois essa coordenada é menor que a primeira coordenada do novo mapeador com valor dois.

Segundo dígito é 1 ou se a distância do objeto base para o objeto X for menor ou maior que o elemento da matriz, o dígito permanece 1.

Caso contrário o dígito torna-se 2.

Noterceiro dígito é incrementado para 2, pois essa coordenada é maior que a terceira coordenada do novo mapeador com valor zero.

No quarto dígito será realizado um cálculo de distância para determinar para qual região o objeto será mapeado.

Várias técnicas para a escolha do novo objeto mapeador podem ser adotadas, sendo que nesse trabalho utilizou-se as seguintes técnicas, randômica, variância e contagem.

A técnica randômica escolhe aleatóriamente um objeto mapeador entre os que estão armazenados no bucket folha cheio.

A técnica da variância escolhe o objeto mapeador que gera a maior variância de coordenadas métricas entre os objetos que estão armazenados no bucket folha cheio.

Essa técnica tem custo quadrático em relação ao acesso de objetos ao bucket dividido, pois todos os objetos armazenados no bucket são eleitos um a um como mapeador para verificar aquele que gera a maior variância.

A técnica da contagem escolhe o objeto mapeador que gera a maior variedade de coordenadas métricas diferentes entre os objetos que estão armazenados no bucket folha cheio.

Essa técnica também tem custo quadrático, pois todos os objetos armazenados no bucket são eleitos um a um como mapeador para verificar aquele que gera a maior variedade.

Problema das distâncias mínimas.

Para que ocorra a extensibilidade das coordenadas métricas é imprescindível que o novo objeto mapeador escolhido gere novas coordenadas para os demais objetos do conjunto.

No entanto, nem sempre isso pode ser garantido.

Isso acontece quando a diferença entre quaisquer dois elementos seguidos da matriz de distâncias possuir o menor valor de distância diferente de zero.

Assim não é possível mais dividir essa região e os objetos continuam sendo mapeados para a mesma região.

Esse problema pode acontecer em espaços métricos onde o domínio de valores possíveis para a função de distância é discreto.

Essa situação foi chamada de problema das distâncias mínimas e deve ser tratada pelos métodos de acesso métrico.

Neste trabalho foram implementados dois métodos de acessos métrico, de inserção top-down, baseados em funções de mapeamento local, a Partição Local e uma extensão chamanda de Partição Local.

Os blocos dessas estruturas são chamados de buckets e esses são de dois tipos, bucket índice e bucket folha e serão detalhados para cada estrutura nas próximas seções.

Nos métodos de acesso métrico baseado em particionamento local, cada bucket na camada índice da estrutura, define uma nova função.

Uma coordenada métrica de um bucket índice redireciona para um bucket inferior.

Se o bucket inferior é índice, ele define uma nova função.

Se o bucket inferior é folha, ele apresenta os objetos indexados.

Índice para Partição Local.

Para que um bucket índice possa definir a função, ele deve armazenar os mapeadores, as distâncias entre os mapeadores e identificadores para os blocos internos pois, cada coordenada direciona para um bucket sendo que, mais de uma coordenada pode redirecionar para o mesmo bucket.

Organização de um bucket índice que é composto por três partes, header, contêm informações do tipo do bucket (folha ou índice).

Próximo bloco vizinho que está no mesmo nível hierárquico que é atribuído no momento de quebra do bloco.

A quantidade de objetos mapeadores no bucket entries, contém um vetor de marcas de início (off-set) para as informações de cada mapeador.

Essa técnica foi adotada para evitar a reescrita das informações já existentes no bucket pois, a adição de um novo mapeador implica em aumentar a quantidade de distância entre os mapeadores e a quantidade de coordenadas méricas.

E informações dos mapeadores, cada seção marcada pelos marcadores (off-set) com exceção do primeiro contém um vetor de identificadores de blocos para os buckets internos, um vetor com as distâncias para os objetos anteriores, e o objeto.

O vetor de distâncias entre os mapeadores evita cálculos de distâncias já computados previamente.

Quando um bucket índice contém mapeadores e um novo mapeador é adicionado, deve-se inserir também duas distâncias para os mapeadores anteriores e 2identificadores de blocos que somados com os quatro identificadores anteriores totalizam coordenadas.

Quantidade de bytes segunda colun usada para armazenar a quantidade de mapeadores da primeira coluna sem contabilizar o tamanho dos objetos mapeadores.

Tamanho do Bucket.

Índice sem Objetos Mapeadores.

O incremento é ilimitado, mas o tamanho do bucket limita a quantidade de objetos mapeadores.

Quando não existe espaço suficiente no bucket para o incremento de mapeadores, cria-se um novo nível, executando a operação de divisão com escolha de dois mapeadores, mencionada anteriormente.

A grande vantagem é que essa organização faz com que as estruturas sejam maiores em largura do que as estruturas baseadas em região de cobertura.

Por exemplo, em blocos de 2048 bytes com mapeadores, a estrutura pode ter até ou 256 sub-buckets, pois mapeadores permitem 256 coordenadas métricas.

No entanto, um mesmo sub-bucket pode mapear mais de uma coordenada métrica podendo diminuir a quantidade de sub-buckets.

O método de inserção top-down faz com que as estruturas de particionamento fiquem mais altas em alguns ramos, comparada com as estruturas bottom-up, mas a maioria dos ramos apresenta poucos níveis por causa da largura da estrutura.

Bucket Folha para Partição Local.

O bucket folha apresentado, armazena todos os objetos indexados pelo método de acesso métrico, e é organizado pelos seguintes campos, header, contêm informações do tipo do bucket (folha ou índice).

Próximo bloco vizinho que está no mesmo nível hierárquico.
A quantidade de objetos mapeadores do nível superior.

A ocupação de objetos no bucket.

O identificador de blocos para uma lista de buckets folhas que tratam o problema das distâncias mínimas.

Entries, contém um vetor de marcas de início (off-set) para cada objeto, bem como sua coordenada métrica.

E objetos, contêm todos os objetos armazenados no bucket folha.

A inserção na estrutura descrita no Algoritmo 41 mostra que, para cada bucket índice navegado, deve-se calcular a coordenada métrica para selecionar o sub-bucket que mapeia a coordenada.

Quando o algoritmo chega em um bucket folha o objeto deve ser inserido.

Se um bucket folha não contém espaço para a inserção de um objeto, é realizado um dos métodos de divisão, divisão com escolha de dois mapeadores.

Divisão de valores de coordenadas e divisão com incremento de mapeadores.

A tática de divisão com escolha de dois mapeadores é realizada quando o único bucket folha está cheio ou um bucket índice não suporta mais ser estendido, portanto, deve-se criar uma nova função que escolhe os dois objetos mapeadores.

Essa tática possui etapas, Escolher dois objetos mapeadores que estão no bucket cheio, usando uma das técnicas, randômica, variância ou contagem.

Dividir as novas coordenadas métricas calculadas através dos dois mapeadores entre um bucket cheio e um novo bucket.

E criar um bucket índice e ajustar os identificadores de blocos entre os dois buckets, conforme a coordenada métrica que cada bucket armazena.

A tática divisão de valores de coordenadas é realizada quando o bucket cheio mapeia mais de um valor de coordenada.

Com ela é possível dividir o bucket folha sem que haja necessidade de incrementar o tamanho da coordenada métrica.

Essa tática possui etapas, Dividir as coordenadas métricas entre um bucket cheio e um novo bucket.

E ajustar os identificadores de blocos entre os dois buckets, conforme a coordenada métrica que cada bucket armazena.

A tática de divisão com incremento de mapeadores é realizada quando não é possível realizar a divisão de valores de coordenadas e há espaço no bucket índice para o incremento.

Esse algoritmo possui 5 etapas, Verificar se o bucket apresenta o problema das distâncias mínimas, ou seja, se não é possível escolher um novo mapeador que consiga gerar novos valores de coordenadas.

Se apresentar esse problema, deve-se criar uma lista de buckets folhas que mapeiam as coordenadas métricas que colidem e as demais etapas do algoritmo não são realizadas.

Verificar se o bucket índice que antecede o bucket folha na hierarquia, tem capacidade para armazenar um novo mapeador.

Se o bucket índice não tem capacidade para o novo mapeador, deve-se executar a tática de divisão com escolha de dois mapeadores e o algoritmo se encerra em seguida.

Se o bucket índice tem capacidade para o novo mapeador, deve-se escolher um objeto mapeador que está no bucket cheio, usando uma das técnicas, randômica, variância e contagem descritas anteriormente.

Ajustar os identificadores de blocos do bucket índice com o novo mapeador adicionado.

Dividir as novas coordenadas métricas recalculadas com o novo mapeador entre um bucket cheio e um novo bucket.

E ajustar os identificadores de blocos entre os dois buckets, conforme a coordenada métrica que cada bucket armazena.

Assim, o problema das distâncias mínimas é tratado nessa estrutura através de uma lista de buckets folhas com valores de coordenadas que colidem, ou seja, que não são estensíveis.

Para uma consulta por abrangência são dados um objeto central e um raio, e a estrutura retorna todos os objetos que estão no máximo a uma distância igual para o objeto.

Regiões qualificadas na consulta por abrangência.

Ilustra-se como é feita uma consulta por abrangência no espaço métrico particionado com mapeadores, onde é o objeto de consulta e o raio.

Pode ser observado que a área da circunferência da consulta qualifica no primeiro dígito as regiões 0, 1 e 2.

No segundo dígito as regiões 0 e 1 e no terceiro dígito somente a região 2.

Assim o algoritmo de consulta por abrangência deverá procurar os objetos que satisfazem a resposta nas coordenadas.

O princípio do algoritmo de consulta por abrangência é navegar nos buckets índices gerando o início e término das coordenadas de consulta, sem ler mais de uma vez um mesmo bucket.

Quando o bucket é folha, são descartados os objetos que não estão dentro da faixa de início e término das coordenadas de consulta e, aqueles que estão dentro da faixa, calcula-se a distância para o objeto de consulta para verificar se fazem parte da resposta.

Para uma consulta aos vizinhos mais próximos são dados um objeto e uma quantidade de objetos, e a estrutura retorna os objetos mais próximos do objeto.

O algoritmo tem etapas, criar uma primeira lista de resposta até completara quantidade k para passar ate um raio limite, e com o raio limite inicial, verificar nas regiões que se qualificaram se existem outros objetos com distância menor.

Ilustra-se como é feita a primeira etapa do algoritmo que cria uma lista de resposta com os objetos mais próximos do objeto que estão na coordenada, onde está mapeado o objeto de consulta.

Se existirem no mínimo objetos nessa região, consegue-se ter um raio inicial.

Caso não tenha k objetos, navega-se entre as regiões vizinhas até conseguir ter k objetos e assim ter um raio inicial.

O Algoritmo 4mostra a implementação da primeira etapa do algoritmo de consulta aos vizinhos mais próximos.

A segunda etapa do algoritmo de consulta aos vizinhos mais próximos é basicamente o algoritmo de consulta por abrangência modificado para que quando forem adicionados elementos na lista de resposta, remova-se os elementos excedentes ao k e diminuindo o raio máximo para maior distância dos elementos remanecentes na lista de resposta.

O Algoritmo 4mostra a implementação da segunda etapa do algoritmo de consulta aos vizinhos mais próximos.

A estrutura de partição local foi gerada a partir das modificações feitas na estrutura de Partição Local para armazenar no bucket folha a distância para o primeiro mapeador para cada entrada no bucket.

Essa modificação foi realizada depois de observar que nos blocos folhas das estruturas métricas com regiões de cobertura, a distância para o representante evita a realização de muitos cálculos de distâncias eliminando objetos que não fazem parte da resposta através da desigualdade triangular.

Entretanto, em um nível folha das estruturas de particionamento não existe um representante, mas deve existir dois ou mais mapeadores.

Dessa maneira, optou-se em armazenar apenas a distância do objeto ao primeiro mapeador.

Para isso, o algoritmo de inserção foi adaptado para armazenar no bucket folha a distância de cada objeto para seu primeiro mapeador.

Bucket Folha para Partição Local.

Os algoritmos de consulta por abrangência e consulta aos vizinhos mais próximos, além de verificar se a coordenada métrica do objeto no bucket folha está entre o início e término das coordenadas de resposta, devem também usar a distância para o primeiro mapeador para excluir objetos que não fazem parte da resposta através da desigualdade triangular.

Para avaliar a adequação dessas estruturas foram realizados experimentos com conjuntos de dados distintos que são, pontos geográficos do projeto GEOnet Names Server, proteínas de diversos seres vivos do projeto UNIPROT (Universal Protein Resource) e palavras de todas as línguas dos dicionários de ortografia do software OpenOffice.

O conjunto de dados do projeto UNIPROT (Universal Protein Resource) contém 92525 proteínas filtradas para ter no máximo 6aminoácidos (média de 4aminoácidos) de diversos seres vivos.

A função de distância criada realiza o alinhamento métrico entre duas proteínas.

Esse alinhamento usa programação dinâmica para implementar a função de distância Levenshtein tratando o problema da transposição.

Para realizar o alinhamento métrico, a função de distância Levenshtein é adaptada para usar uma matriz métrica de pesos entre os 20 aminoácidos, chamada MPAM.

Nessa implementação foi usado um peso constante para o deslocamento de partes da proteína chamado de "gap".

O valor do gap é a maior distância entre os aminoácidos da MPAM incrementado por um, pois a penalidade é maior para deslocar parte da proteína do que para trocar um aminoácido por outro.

Matriz MPAM.

A maior distância para essa função é o valor 448, que equivale a comparar uma proteína composta por 6aminoácidos do tipo C com outra proteína de 6aminoácidos do tipo W.

Logo, o domínio dos valores possíveis para essa função de distância métrica com esse conjunto de dados serão os números inteiros entre 0 e 448.

A utilização desse conjunto de dados se fez necessária pois, ele possui uma função de distância custosa, onde um cálculo de distância é muito mais caro que um acesso a disco.

Assim, espera-se verificar se as modificações que originaram a Partição Local+ garantem um bom desempenho mesmo ocupando mais espaço em disco e conseqüentemente, mais acessos a disco.

Esse conjunto de dados não possui dimensão fixa e somente pode ser indexado por um método de acesso métrico.

Foram usadas 9 estruturas para a comparação, sendo, Arquivo Seqüencial.

M-Tree com quebra pelo algoritmo MINMAX.

Slim-Tree com quebra pelo algoritmo MINMAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador randômico (Ran, com escolha de mapeador pela variância (Var) e com escolha de mapeador pela contagem (Diff).

E Partição Local+ com escolha de mapeador randômico (Ran, com escolha de mapeador pela variância (Var) e com escolha de mapeador pela contagem (Diff).

Para cada uma das 8 estruturas mencionadas anteriormente variou o tamanho do bloco em 102bytes, 2048 bytes e 4096 bytes totalizando 27 estruturas.

Características das estruturas geradas para indexar o conjunto de proteínas.

Esse conjunto só pode ser armazenado por estruturas métricas, já que não possuem dimensão fixa.

Informações sobre as Estruturas do Conjunto de Proteínas.

Campos, o tamanho do bloco em bytes.

A altura para as estruturas M-Tree, Slim-Tree e a altura máxima das estruturas Partição Local e Partição Local+, sendo que essa característia não se aplica para arquivo seqüencial.

A quantidade de nó índice sendo que, essa característica não se aplica para o arquivo seqüencial.

A quantidade de nós folhas.

O tamanho do arquivo gerado e a porcentagem de objetos em relação ao total de registros que estão em regiões de colisão, sendo que essa característica só se aplica para as estruturas baseadas em coordenadas métricas.

A colisão indica que aconteceu o problema das distâncias mínimas em um bucket índice e assim, não é possível estender as coordenadas métricas.

Esse problema deve ser evitado pois gera uma lista de buckets folhas e, se essa lista for grande prejudica as consultas.

A partir dos valores, que mostram as estruturas geradas a partir dos dados do UNIPROT usando uma função de distância de alinhamento métrico, com as características citadas acima, podemos concluir que, as estruturas baseadas em coordenadas métricas possuem mais níveis do que as estruturas baseadas em região de cobertura.

No entanto, a quantidade adicional de níveis não foi expressiva.

Essa diferença comprova que a escolha dos objetos mapeadores que compõem a função de mapeamento local foi boa em qualquer variação do tamanho do bloco.

A estrutura Partição Local possui em média 7% menos blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree e, conseqüentemente usa menos espaço em disco.

A estrutura Partição Local+ possui em média 6% mais blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree e, conseqüentemente usa mais espaço em disco.

As estruturas que usaram o algoritmo de escolha de novo mapeador pela maior variância de regiões e pela maior contagem de regiões diferentes possuem pelo menos 3% menos nó folha comparado com as estruturas que usam o algoritmo randômico.

A colisão não aconteceu nesse conjunto de dados, pois a escolha do objeto mapeador foi sempre boa.

Isto aconteceu porque a ordem dos registros no conjunto UNIPROT não apresenta um agrupamento de distâncias.

Análise do Número de Cálculos de Distância para Proteínas.

Apresenta-se os gráficos da média de números de cálculos de distância dos resultados obtidos utilizando a base de proteínas UNIPROT.

A primeira coluna mostra os gráficos obtidos utilizando consulta por abrangência e a segunda coluna apresenta os gráficos utilizando consulta aos vizinhos mais próximos.

Comparando os gráficos com os resultados nota-se que o tamanho do bloco influencia diretamente a altura da estrutura e que, quanto mais alta a estrutura maior é o custo para realização da consulta.

Os gráficos da primeira coluna mostram o número médio de cálculos de distância realizados para responder a 500 consultas por abrangência em que o raio varia de 1 até 5 unidades de alinhamento, onde os objetos de consulta foram sorteados aleatoriamente do conjunto UNIPROT.

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O número de cálculos de distância é comparado com a quantidade de proteínas que compõem o conjunto de dados, que é de 92575.

As estruturas que realizaram o menor número de cálculos de distância na consulta por abrangência no pior caso em que o raio vale 5 unidades de alinhamento foram, Partição Local+ para blocos de 102bytes, com escolha de mapeador pela variância, tendo realizado 22205 cálculos ou 24% de proteínas da base.

Gráficos da média de números de cálculos de distância para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun da base UNIPROT.

Partição Local+ para blocos de 2048 bytes, com escolha de mapeador randômico, tendo realizado 17699 cálculos ou 19% de proteínas da base.

E Partição Local para blocos de4096 bytes, com escolhade mapeadorrandômico, tendo realizado 1979cálculos ou 21% de proteínas da base.

As estruturas que realizaram o maior número de cálculos de distância na consulta por abrangência no pior caso em que o raio vale 5 unidades de alinhamento foram, Slim-Tree para blocos de 102bytes, tendo realizado 51817 cálculos ou 56% de proteínas da base.

Slim-Tree para blocos de 2048 bytes, tendo realizado 40789 cálculos ou 44% de proteínas da base.

E Slim-Tree para blocos de 4096 bytes, tendo realizado 36760 cálculos ou 40% de proteínas da base.

Portanto, o algoritmo de consulta por abrangência com o raio valendo 5 unidades de alinhamento nas estruturas baseadas em regiões de coberturas comparado com a melhor estrutura baseada em coordenadas métricas, foi pior obtendo os seguintes valores, 133% mais cálculos de distância para blocos de 102bytes, 130% mais cálculos de distância para blocos de 2048 bytes e 85% mais cálculos de distância para blocos de 4096 bytes.

Consulta aos Vizinhos mais Próximos.

Os gráficos da segunda coluna mostram o número médio de cálculos de distância realizados para responder a 500 consultas aos vizinhos mais próximos com lista de empate em que o k varia de 1 até 5 unidades de alinhamento, onde os objetos de consulta foram sorteados aleatoriamente do conjunto UNIPROT.

Novamente, as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O algoritmo de busca aos vizinhos mais próximos nas estruturas baseadas em região de cobertura realizou o número de cálculos de distância muito próximo ao da busca seqüencial, sendo que na média as duas estruturas para raio valendo 5 unidades de alinhamento realizaram, 83339 cálculos ou 90% de proteínas da base para blocos de 102bytes.

Os 77870 cálculos ou 84% de proteínas da base para blocos de 2048 bytes e 73976 cálculos ou 80% de proteínas da base para blocos de 4096 bytes.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 5 unidades de alinhamento nas estruturas baseadas em regiões de coberturas, comparado com a melhor estrutura baseada em coordenadas métricas, foi pior obtendo os seguintes valores, 24% mais cálculos de distância para blocos de 102bytes, 42% mais cálculos de distância para blocos de 2048 bytes e 33% mais cálculos de distância para blocos de 4096 bytes.

Análise do Número de Acessos a Disco para Proteínas.

Gráficos da média de números de acessos a disco dos resultados obtidos utilizando a base de proteínas UNIPROT.

A primeira coluna mostra os gráficos obtidos para realizar consulta por abrangência e a segunda coluna apresenta os gráficos para realizar consulta aos vizinhos mais próximos.

Gráficos da média de números de acessos a disco para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun para a base UNIPROT.

Comparando os gráficos com os resultados, nota-se novamente que o tamanho do bloco influencia diretamente na altura da estrutura e que quanto mais alta é a estrutura, maior é o custo para realização da consulta.

Os gráficos da primeira coluna mostram o número médio de acessos a disco realizados para responder a 500 consultas por abrangência em que o raio varia de 1 até 5 unidades de alinhamento, onde os objetos de consulta foram sorteados aleatoriamente do conjunto UNIPROT.

Pôde-se observar que as estruturas baseadas em coordenadas métricas novamente ganham em qualquer situação das estruturas baseadas em região de cobertura.

As estruturas que realizaram o menor número de acessos a disco na consulta por abrangência no pior caso, que é quando o raio vale 5 unidades de alinhamento, foram, Partição Local para blocos de 102bytes, com escolha de mapeador pela diferença tendo realizado 273acessos ou 55% dos blocos de um arquivo seqüencial.

Partição Local para blocos de 2048 bytes, com escolha de mapeador pela variância tendo realizado 1245 acessos ou 51% dos blocos de um arquivo seqüencial.

E Partição Local para blocos de 4096 bytes, com escolha de mapeador pela variância tendo realizado 536 acessos ou 45% dos blocos de um arquivo seqüencial.

Por outro lado, as estruturas que realizaram o maior número de acessos a disco na consulta por abrangência no pior caso em que o raio estava valendo 5 unidades de alinhamento foram, M-Tree para blocos de 102bytes, tendo realizado 1177acessos ou 2,38 vezes o número de blocos de um arquivo seqüencial.

M-Tree para blocos de 2048 bytes, tendo realizado 561acessos ou 2,31 vezes o número de blocos de um arquivo seqüencial e M-Tree para blocos de 4096 bytes, tendo realizado 2441 acessos ou 2,0vezes o número de blocos de um arquivo seqüencial.

Consulta aos Vizinhos mais Próximos O algoritmo de consulta aos vizinhos mais próximos com o k valendo 5 unidades de alinhamento foi pior nas estruturas baseadas em regiões de coberturas obtendo os seguintes valores, 3,vezes mais acessos a disco para blocos de 102bytes, 3,5 vezes mais acessos a disco para blocos de 2048 bytes e 3,55 vezes mais acessos a disco para blocos de 4096 bytes, comparado com as estruturas baseadas em coordenada métrica.

As estruturas que realizaram o maior número de acessos a disco na consulta aos vizinhos mais próximos, em que o k vale 5 unidade de alinhamento, foram, M-Tree para blocos de 102bytes, tendo realizado 13728 acessos ou 2,78 vezes o número de blocos de um arquivo seqüencial.

M-Tree para blocos de 2048 bytes, tendo realizado 6538 acessos ou 2,69 vezes o número de blocos de um arquivo seqüencial e M-Tree para blocos de 4096 bytes, tendo realizado 2858 acessos ou 2,37 vezes o número de blocos de um arquivo seqüencial.

O algoritmo de consulta aos vizinhos mais próximos, com k valendo 5 unidades de alinhamento nas estruturas baseadas em regiões de coberturas comparado com a melhor estrutura baseada em coordenadas métricas, foi pior obtendo os seguintes valores, 93% mais acessos a disco para blocos de 102bytes, 101% mais acessos a disco para blocos de 2048 bytes e 90% mais acessos a disco para blocos de 4096 bytes.

Análise do Número de Tempo de Execução para Proteínas.

Os gráficos apresentados indicam os tempos de execuções referentes às operações de consulta por abrangência e consulta aos vizinhos mais próximos no conjunto de proteínas.

Pode-se observar que não houveram ganhos constantes de um mesmo método de escolha de mapeador variando o tamanho de bloco e o tipo de consulta.

Por isso, recomenda-se o método de escolha de mapeador randômico que é mais rápido que os outros métodos (variância e contagem) para construir as estruturas.

Os gráficos da primeira coluna mostram o tempo médio em milisegundos (ms) para responder a 500 consultas por abrangência, em que o raio varia de 1 até 5 unidades de alinhamento, onde os objetos de consulta foram sorteados aleatoriamente do conjunto UNIPROT.

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham, em todas as situações das estruturas baseadas em região de cobertura, como nos casos de cálculos de distância e acessos a disco.

O algoritmo de consulta por abrangência com o raio valendo 5 unidades de alinhamento nas estruturas baseadas em regiões de coberturas, comparado com a melhor estrutura baseada em coordenadas.
Gráficos da média de tempo para 500 consultas por abrangência 1 coluna e de vizinhos mais próximos, coluna para a base UNIPROT métricas, foi pior, obtendo os seguintes valores, 2,66 vezes mais tempo de execução para blocos de 102bytes, 2,70 vezes mais tempo de execução para blocos de 2048 bytes e 1,66 vezes mais tempo de execução para blocos de 4096 bytes.

Os gráficos da segunda coluna mostram o tempo médio em milisegundos (ms) para responder a 500 consultas aos vizinhos mais próximos com lista de empate, em que o k varia de 1 até 5 unidades de alinhamento.

Os objetos de consulta foram sorteados aleatoriamente do conjunto UNIPROT, e novamente as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 5 unidades de alinhamento nas estruturas baseadas em regiões de coberturas comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 33% mais tempo de execução para blocos de 102bytes, 42% mais tempo de execução para blocos de 2048 bytes e 31% mais tempo de execução para blocos de 4096 bytes.

Os resultados mostraram que a organização das estruturas de particionamento realizam menor número de cálculos de distância nos blocos índices e folhas, e menor número de acessos a disco.

A justificativa para realizar um menor número de cálculos de distância nos blocos índices é que existe uma quantidade menor de mapeadores comparado com a quantidade de representantes nos níveis de índice das estruturas.

Por exemplo, nos blocos índices de tamanho de 2048 bytes, as estruturas baseadas em região de cobertura podem realizar até 40 cálculos de distância por nó índice, pois armazenam até 40 proteínas representantes com tamanho médio de 4aninoácidos.

No entanto, esse número é reduzido pelo uso da distância para seu representante do nível acim usando a desigualdade triangular.

No mesmo exemplo, para blocos índices de tamanho de 2048 bytes, as estruturas de particionamento sempre realizam seis cálculos de distância por bucket índice, pois armazenam no máximo proteínas mapeadoras com tamanho médio de 4aninoácidos.

O número de cálculos é dado através das combinações possíveis entre as distâncias de mapeadores.

Mesmo que as estruturas de particionamento sejam um pouco mais altas, o que deveria aumentar o número de cálculos de distância, a pouca quantidade de cálculos realizados no bloco índice compensa essa diferença de altura.

O uso da coordenada métrica de início e fim da consulta das estruturas baseadas em particionamento mostrou que evita mais cálculos de distância do que o uso da distância para o representante das estruturas de região de cobertura nos objetos dos buckets folhas que não fazem parte da resposta.

A justificativa para realizar um menor número de acessos a disco é que as estruturas baseadas em particionamento também são maiores em largura do que as estrutura baseadas em região de cobertura.

Ainda usando o exemplo acima, para blocos índices de tamanho de 2048 bytes, as estruturas baseadas em região de cobertura possuem exatamente 40 sub-árvores, uma para cada representante.

As estruturas baseadas em particionamento com proteínas mapeadoras podem ter no máximo ou 4256 sub-buckets, pois um mesmo sub-bucket pode ser mapeado por mais de uma coordenada métrica.

Mesmo sendo mais altas, em alguns ramos, as estruturas de particionamento são maiores em largura do que as estruturas baseadas em região de cobertura, o que deixa muitos ramos com uma quantidade menor de níveis do que as estruturas baseadas em região de cobertura.

O tempo mais baixo das consultas nas estruturas de particionamento é conseqüencia da menor quantidade de cálculos de distância e acessos a disco.

Os métodos de escolha de mapeadores por variância e por contagem não garantiu ganhos constantes em todos os experimentos.

Esses métodos tornam a inserção mais lenta e por isso recomenda-se utilizar o método de escolha randômica pois tornam a inserção mais rápida.

A maioria dos experimentos mostraram que as estruturas Partição Local realizam um menor número de cáculos de distância pois usam a distância para o primeiro mapeador, no bucket folha, para evitar cálculos de distância.

Como essa função de distância é mais custosa do que o acesso a disco, as estruturas de Partição Local+ são mais rápidas do que as de Partição Local, mesmo realizando uma quantidade maior de acessos a disco.

O conjunto de dados do OpenOffice contém 69476palavras filtradas com somente as que não possuíam acentuação no dicionário de ortografia de 61 línguas de todo o mundo.

A palavra mais longa tem 50 letras, mas na média elas têm 9 letras.

Para o conjunto de palavras do OpenOffice foi usada a função de distância Levenshtein.

Essa função de distância métrica atribui peso 1 para a troca ou deslocamento entre qualquer letra.

Logo, o domínio dos valores possíveis para essa função de distância métrica está com todos números inteiros entre 0 e 50, já que essa palavra não possui todas as letras do alfabeto.

A utilização desse conjunto de dados se fez necessária pois, ele possui uma função de distância menos custosa em relação ao conjunto de dados UNIPROT.

Esse conjunto apresenta 7,5 vezes mais dados do que o conjunto de dados anterior e o domínio dos valores possíveis da função de distância é pequeno.

Esse conjunto de dados não possui dimensão fixa e só pode ser indexado por um método de acesso métrico.

Características das 15 estruturas de indexação construídas com esse conjunto de palavras.

As estruturas usadas na comparação foram, M-Tree com quebra pelo algoritmo de MINMAX.

Informações sobre as Estruturas do Conjunto de Palavras.

Slim-Tree com quebra pelo algoritmo MINMAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador randômico Ran, e Partição Local+ com escolha de mapeador randômico Ran.

Para cada uma das 5 estruturas mencionadas anteriormente variou-se o tamanho do bloco em 51bytes, 102bytes e 2048 bytes.

Campos, o tamanho do bloco em bytes.

A altura para as estruturas M-Tree, Slim-Tree e a altura máxima das estruturas Partição Local e Partição Local+, sendo que essa característia não se aplica para arquivo seqüencial.

A quantidade de nó índice, sendo que essa característia não se aplica para arquivo seqüencial.

A quantidade de nó folha.

O tamanho do arquivo gerado e a porcentagem de objetos em relação ao total de registros que estão em regiões de colisão, sendo que essa característica só se aplica para as estruturas baseadas em coordenadas métricas.

Não foram testadas para esse conjunto de dados, dentre as estruturas de particionamento, os métodos de escolha de mapeadores pela variância e pela contagem pois, não se manteve constante a melhora no desempenho das consultas.

Outro motivo é que esses algoritmos tornam mais lenta a inserção de objetos para conjuntos com grande quantidade de dados como esse.

Medidas obtidas das estruturas geradas do conjunto de palavras do OpenOffice, usando a função de distância Levenshtein com as características citadas acima.

Pôde-se concluir que, as estruturas baseadas em coordenadas métricas chegam a ser mais altas mais de duas vezes quando comparadas com as estruturas baseadas em região de cobertura.

Essa diferença mostra que a escolha dos objetos mapeadores que compõem a função de mapeamento local não foi boa para as diversas variações de tamanho de bloco como ocorreu com o conjunto de proteínas.

A estrutura Partição Local possui em média 18% menos blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree e, conseqüentemente usa menos espaço em disco.

A estrutura Partição Local+ possui em média 13% mais blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree e, conseqüentemente usa mais espaço em disco.

E a colisão foi bem pequena nesse conjunto de dados, pois a escolha do objeto mapeador apesar de não ser a ideal, foi razoável.

Isto aconteceu porque a ordem dos registros no conjunto OpenOffice apresenta uma ordenação nas palavras, o que agrupa as distâncias.

Apresenta os gráficos da média de cálculos de distância obtidos utilizando a base de palavras do OpenOffice.

A primeira coluna mostra os gráficos obtidos para realizar consultas por abrangência e a segunda coluna apresenta os gráficos para executar consultas aos vizinhos mais próximos.

Consulta por Abrangência comparando os gráficos com os resultados nota-se que o tamanho do bloco em disco influencia diretamente a altura da estrutura e que, quanto mais alta a estrutura maior é o custo para realização da consulta.

Análise do Número de Cálculos de Distância para Conjunto de Palavras.

Os gráficos da primeira coluna mostram o número médio de cálculos de distância realizados para responder a 500 consultas por abrangência em que o raio varia de 1 até 5 unidades, onde os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

O algoritmo de consulta por abrangência com o raio valendo 5 unidades nas estruturas baseadas em regiões de cobertura comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 5% mais cálculos de distância para blocos de 51bytes, 1% mais cálculos de distância para blocos de 102bytes e 18% mais cálculos de distância para blocos de 2048 bytes.

Os gráficos mostram o número médio de cálculos de distância realizados para responder a 500 consultas aos vizinhos mais próximos com lista de empate, em que o k varia de 1 até 5 unidades, e os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

As estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

Gráficos da média de números de cálculos de distância para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun da base OpenOffice.

Nos experimentos realizados, o número de cálculos de distância foi comparado com a quantidade de palavras que compõem o conjunto de dados que é de 694762.

As estruturas que realizaram o menor número de cálculos de distância na consulta aos vizinhos mais próximos no pior caso, em que o raio vale 5 unidades, foram, Partição Local+ parablocos de 51bytes,com escolha de mapeador randômico, tendo realizado 276269 cálculos ou 40% de palavras da base.

Partição Local+ para blocos de 102bytes, com escolha de mapeador randômico, tendo realizado 294898 cálculos ou 42% de palavras da base.

E Partição Local+ para blocos de 2048 bytes, com escolha de mapeador randômico, tendo realizado 28771cálculos ou 41% de palavras da base.

As estruturas que realizaram o maior número de cálculos de distância na consulta aos vizinhos mais próximos no pior caso, em que o raio vale 5 unidades, foram, Slim-Tree para blocos de 51bytes, tendo realizado 492988 cálculos ou 71% de palavras da base.

M-Tree para blocos de 102bytes, tendo realizado 44053cálculos ou 63% de palavras da base e M-Tree para blocos de 2048 bytes, tendo realizado 40743cálculos ou 59% de palavras da base.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 5 unidades nas estruturas baseadas em regiões de cobertura, comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 78% mais cálculos de distância para blocos 51bytes, 49% mais cálculos de distância para blocos 102bytes e 41% mais cálculos de distância para blocos 2048 bytes.

Análise do Número de Acessos a Disco para Conjunto de Palavras.

Apresenta os gráficos da média do número de acessos a disco utilizando a base de palavras do OpenOffice.

A primeira coluna mostra os gráficos obtidos utilizando consulta por abrangência e a segunda coluna apresenta os gráficos utilizando consulta aos vizinhos mais próximos.

Consulta por abrangência comparando os gráficos com os resultados nota-se novamente que o tamanho do bloco influencia diretamente a altura da estrutura e que quanto mais alta a estrutura maior é o custo para realização da consulta.

Os gráficos da primeira coluna mostram o número médio de acessos a disco realizados para responder a 500 consultas por abrangência com o raio variando de 1 até 5 unidades, onde os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

Gráficos da média de números de acessos a disco para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun para a base OpenOffice.

O algoritmo de consulta por abrangência com o raio valendo 5 unidades na pior estrutura baseada em regiões de cobertura (M-Tree) comparado com a melhor estrutura baseada em coordenadas métricas, Partição Local, foi pior, obtendo os seguintes valores, 46% mais acessos a disco para blocos de 51bytes, 33% mais acessos a disco para blocos de 102bytes e 48% mais acessos a disco para blocos de 2048 bytes.

Os gráficos da segunda coluna mostram o número médio de acessos a disco realizados para responder a 500 consultas aos vizinhos mais próximos com lista de empate, em que o k varia de 1 até 5 unidades.

Os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

Novamente as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

As estruturas que realizaram o menor número de acessos a disco na consulta aos vizinhos mais próximos no pior caso, em que o raio vale 5 unidades, foram, Partição Local para blocos de 51bytes, com escolha de mapeador randômico tendo realizado 28310 acessos ou 108% dos blocos de um arquivo seqüencial.

Partição Local para blocos de 102bytes, com escolha de mapeador randômico tendo realizado 13598 acessos ou 106% dos blocos de um arquivo seqüencial.

E Partição Local para blocos de 2048 bytes, com escolha de mapeador randômico tendo realizado 6486 acessos ou 102% dos blocos de um arquivo seqüencial.

As estruturas que realizaram o maior número de acessos a disco na consulta aos vizinhos mais próximos no pior caso, em que o raio vale 5 unidades, foram, M-Tree para blocos de 51bytes, tendo realizado 7265acessos ou 2,79 vezes o número de blocos de um arquivo seqüencial.

M-Tree para blocos de 102bytes, tendo realizado 3113acessos ou 2,4vezes o número de blocos de um arquivo seqüencial e M-Tree para blocos de 2048 bytes, tendo realizado 13791 acessos ou 2,18 vezes o número de blocos de um arquivo seqüencial.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 5 unidades nas estruturas baseadas em regiões de cobertura comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 1,56 vezes mais acessos a disco para blocos de 51bytes, 1,29 vezes mais acessos a disco para blocos de 102bytes e 1,1vezes mais acessos a disco para blocos de 2048 bytes.

Análise do Número de Tempo de Execução para o Conjunto de Palavras.

Os gráficos da média de tempo de execução referentes às operações de consulta por abrangência e consulta aos vizinhos mais próximos no conjunto de palavras.

Gráficos da média de tempo para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun para a base OpenOffice.

Os gráficos da primeira coluna mostram o tempo médio em milisegundos (ms) para responder a 500 consultas por abrangência em que o raio varia de 1 até 5 unidades, onde os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura, como nos casos de cálculos de distância e acessos a disco.

O algoritmo de consulta por abrangência com o raio valendo 5 unidades nas estruturas baseadas em regiões de cobertura comparando com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 13% mais tempo de execução para blocos de 51bytes, 12% mais tempo de execução para blocos de 102bytes e 32% mais tempo de execução para blocos de 2048 bytes.

Os gráficos da segunda coluna mostram o tempo médio em milisegundos (ms) para responder a 500 consultas aos vizinhos mais próximos com lista de empate, em que o k varia de 1 até 5 unidades, e os objetos de consulta foram sorteados aleatoriamente do conjunto OpenOffice.

Novamente as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 5 unidades nas estruturas baseadas em regiões de cobertura, comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 93% mais tempo de execução para blocos de 51bytes, 63% mais tempo de execução para blocos de 102bytes e 46% mais tempo de execução para blocos de 2048 bytes.

Os resultados mostraram que a organização das estruturas de particionamento realizam menor número de cálculos de distância nos blocos índices e folhas, menor número de acessos a disco e tempo pelos mesmos motivos mencionados na discussão dos resultados do conjunto de dados de proteínas.

A maioria dos experimentos mostrou que as estruturas Partição Local+ realizam um menor número de cálculos de distância pois usam a distância para o primeiro mapeador, no bucket folha, para evitar cálculos de distância.

Essa função de distância é menos custosa do que a de proteínas, pois o tamanho médio das palavras é menor do que o tamanho médio das proteínas.

Dessa forma, o tempo das estruturas de Partição Local+ são próximos ao tempo da Partição Local, mesmo realizando uma O conjunto de dados do projeto GEOnet Names Server contém 3976221 pontos geográficos (latitude e longitude) filtrados pela unicidade do ponto geográfico.

Os pontos correspondem ao centro de cidades e pontos geográficos de interesse em todo mundo.

A função de distância para o conjunto de pontos geográficos descrita na fórmula abaixo, retorna a distância entre dois pontos na Terra em quilômetros, usando a lei dos cosenos na trigonometria esférica, onde o raio da terra é aproximadamente 6370 Km.

As variáveis lat e long é a latitude e a longitude do primeiro ponto geográfico representada em graus e, lat e long é a latitude e a longitude respectivamente, do segundo ponto geográfico.

O domínio dos valores possíveis para essa função de distância métrica inclui todos os números fracionários entre 0 e a maior distância entre dois pontos da Terra que é pouco mais de 20000 km.

A utilização desse conjunto de dados se fez necessária, pois ele possui uma função de distância barata.

Esse conjunto apresenta 5,7 vezes mais registros em relação a conjunto de palavras e o domínio dos valores possíveis da função de distância é muito maior que os conjuntos anteriores.

Esse conjunto de dados possui dimensão fixa e poderia ser indexado por um método de acesso espacial.

Informações sobre as Estruturas do Conjunto Pontos Geográficos.

Características da construção das estruturas, Arquivo Seqüencial, M-Tree com quebra pelo algoritmo MINMAX.

Slim-Tree com quebra pelo algoritmo MINMAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador randômico Ran, e Partição Local+ com escolha de mapeador randômico Ran.

Para cada estrutura mencionada anteriormente, variou o tamanho do bloco em 51bytes, 102bytes e 2048 bytes totalizando 15 estruturas.

Campos, o tamanho do bloco em bytes.

A altura para as estruturas M-Tree, Slim-Tree e a altura máxima das estruturas Partição Local e Partição Local+, sendo que essa característia não se aplica para arquivo seqüencial.

A quantidade de nós índice, sendo que essa característia não se aplica para arquivo seqüencial.

A quantidade de nós folha.

O tamanho do arquivo gerado e a porcentagem de objetos em relação ao total de registros que estão em regiões de colisão, sendo que essa característica só se aplica para as estruturas baseadas em coordenadas métricas.

Não foram testadas para esse conjunto de dados, dentre as estruturas de particionamento, os métodos de escolha de mapeadores pela variância e pela contagem pois, não se manteve constante a melhora no desempenho das consultas.

Outro motivo é que esses algoritmos tornam mais lenta a inserção de objetos para conjuntos com grande quantidade de dados.

A partir dos valores da ue mostra as estruturas geradas com os dados do GEOnet usando uma função de distância com as características citadas acima, podemos concluir que, as estruturas baseadas em coordenadas métricas possuem até três vezes mais níveis do que as estruturas baseadas em região de cobertura.

Essa diferença mostra que a escolha dos objetos mapeadores que compõem a função de mapeamento local foi ruim para todas as variações de tamanho de bloco, pois uma escolha boa deve manter a estrutura com a quantidade de níveis igual ou menor que as estruturas de região de cobertura.

A estrutura Partição Local, possui em média 14% menos blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree.

Isso acontece porque essas estruturas armazenam para cada objeto, somente a coordenada métrica local, que é menor do que armazenar uma distância com precisão.

A estrutura Partição Local+ possui em média 30% mais blocos do tipo folha que a melhor estrutura de região de cobertura Slim-Tree.

Isso acontece porque essas estruturas armazenam para cada objeto a coordenada métrica local e a distância para o primeiro objeto mapeado.

Como cada objeto é composto por apenas dois números de mesma precisão do número usado para mensagem e distância, o aumento proporcional ao consumo de memória é significativo (passa de 8 para 1bytes por objeto).

E a colisão foi até 2% da base, influenciadas pela escolha ruim dos objetos mapeadores.

Isto aconteceu porque a ordem dos registros no conjunto GEONet apresenta agrupamentos pelos estados e países, e portanto, prejudicam a escolha de mapeadores locais.

Análise do Número de Cálculos de Distância para o Conjunto de Pontos GeoGráficos.


Os gráficos apresentados mostram os resultados obtidos referentes aos testes realizados utilizando o conjunto de dados de pontos geográficos.

Na primeira coluna estão os gráficos obtidos utilizando consulta por abrangência e a segunda coluna utilizando consulta aos vizinhos mais próximos.

Comparando os gráficos com os resultados da Tabela, nota-se que o tamanho do bloco influencia diretamente na altura da estrutura e que, quanto mais alta a estrutura maior é o custo para realização da consulta.

Os gráficos mostram o número médio de cálculos de distância realizados para responder a 500 consultas por abrangência, onde os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet, e que o raio foi, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40 (km).

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O número de cálculos de distância será comparado com a quantidade de pontos geográficos que compõem o conjunto de dados que é 3976221.

As estruturas que realizaram o menor número de cálculos de distância na consulta por abrangência no pior caso, em que o raio vale 40 kilômetros, foram, Partição Local+ parablocos de51bytes,com escolhademapeadorrandômico,tendo realizado 1031 cálculos ou 0.

Os 025% de pontos geográficos da base.

Partição Local+ para blocos de 102bytes, com escolha de mapeador randômico, tendo realizado 1065 cálculos ou 0.

Os 026% de pontos geográficos da base e Partição Local+ para blocos de 2048 bytes, com escolha de mapeador randômico, tendo realizado 998 cálculos ou 0.

Os 025% de pontos geográficos da base.

As estruturas que causam o maior número de cálculos de distância na consulta por abrangência no pior caso, em que o raio vale 40 kilômetros, foram, M-Tree para blocos de 51bytes, tendo realizado 5936 cálculos ou 0.

Os 149% de pontos geográficos da base.

A Média de Números de Cálculos de Distância para 500 Consultas por Abrangência 1 coluna e de Vizinhos mais Próximos coluna para a base GEONet M-Tree para blocos de 102bytes, tendo realizado 2721 cálculos ou 0.

Os 068% de pontos geográficos da base e M-Tree para blocos de 2048 bytes, tendo realizado 210cálculos ou 0.

Os 052% de pontos geográficos da base.

Como nos demais conjuntos, o algoritmo de consulta por abrangência com o raio valendo 40 km nas estruturas baseadas em regiões de cobertura comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 5,07 vezes mais cálculos de distância para blocos de 51bytes, 1,65 vezes mais cálculos de distância para blocos de 102bytes e 1,18 vezes mais cálculos de distância para blocos de 2048 bytes.

Os gráficos mostram o número médio de cálculos de distância realizados para responder a 500 consultas aos vizinhos mais próximos com lista de empate em que o k foi, 3, 6, 9, 12, 15, 18, 21 e 24, e os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet.

Novamente, as estruturas baseadas em coordenadas métricas ganham em todas as situações das estruturas baseadas em região de cobertura.

As estruturas que realizaram o menor número de cálculos de distância na consulta aos vizinhos mais próximos, no pior caso em que o k vale 24, foram, Partição Local+ parablocos de51bytes,com escolhademapeadorrandômico,tendo realizado 221 cálculos ou 0.

Os 005% de pontos geográficos da base.

Partição Local+ para blocos de 102bytes, com escolha de mapeador randômico, tendo realizado 20cálculos ou 0.

Os 005% de pontos geográficos da base e e Partição Local+ para blocos de 2048 bytes, com escolha de mapeador randômico, tendo realizado 25 cálculos ou 0.

Os 005% de pontos geográficos da base.

As estruturas que realizaram o maior número de cálculos de distância na consulta aos vizinhos mais próximos no pior caso, em que o k vale 24, foram, M-Tree para blocos de 51bytes, tendo realizado 437cálculos ou 0.

Os 11% de pontos geográficos da base.

M-Tree para blocos de 102bytes, tendo realizado 1575 cálculos ou 0.

Os 04% de pontos geográficos da base e M-Tree parablocosde2048 bytes,tendorealizado 988 cálculos ou 0.

Os 03% de pontos geográficos da base.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 2nas estruturas baseadas em regiões de cobertura, comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 18,79 vezes mais cálculos de distância para blocos de 51bytes, 6,7vezes mais cálculos de distância para blocos de 102bytes e 2,9vezes mais cálculos de distância para blocos de 2048 bytes.

Análise do Número de Acessos a Disco para o Conjunto de Pontos Geográficos.

Os gráficos apresentados mostram os resultados obtidos dos testes realizados, utilizando o conjunto de dados de pontos geográficos.

A primeira coluna apresenta os gráficos obtidos utilizando consultas por abrangência e a segunda coluna mostra os gráficos utilizando consulta aos vizinhos mais próximos.

Comparando os gráficos com os resultados da ota-se que o tamanho do bloco influencia diretamente na altura da estrutura e que, quanto mais alta a estrutura maior é o custo para realização da consulta.

Os gráficos da primeira coluna mostram o número médio de acessos a disco realizados para responder a 500 consultas por abrangência, onde os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet, e que o raio foi, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40 (km).

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

As estruturas que realizaram o menor número de acessos a disco na consulta por abrangência no pior caso, em que o raio vale 40 kilômetros, foram, Partição Local para blocos de 51bytes, com escolha de mapeador randômico, tendo realizado 11acessos a disco ou 0.

Os 087% dos blocos de um arquivo seqüencial.

Partição Local para blocos de102bytes, com escolhade mapeadorrandômico, tendo realizado 68 acessos a disco ou 0.

Os 107% dos blocos de um arquivo seqüencial e Partição Local para blocos de2048 bytes, com escolhade mapeadorrandômico, tendo realizado 35 acessos a disco ou 0.

Os 111% dos blocos de um arquivo seqüencial.

As estruturas que realizaram o maior número de acessos a disco na consulta por abrangência no pior caso, em que o raio vale 38 kilômetros, foram, M-Tree para blocos de 51bytes, tendo realizado 1507 acessos a disco ou 1.

Os 174% dos blocos de um arquivo seqüencial.

Gráficos da média de números de acessos a disco para 500 consultas por abrangência 1 colun e de vizinhos mais próximos colun para a base GEONet.

M-Tree para blocos de 102bytes, tendo realizado 337 acessos a disco ou 0.

Os 533% dos blocos de um arquivo seqüencial e M-Tree para blocos de 2048 bytes, tendo realizado 129 acessos a disco ou 0.

Os 412% dos blocos de um arquivo seqüencial.

O algoritmo de consulta por abrangência com o raio valendo 40 km nas estruturas baseadas em regiões de coberturas comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 13,2vezes mais acessos a disco para blocos de 51bytes, 4,1vezes mais acessos a disco para blocos de 102bytes e 2,85 vezes mais acessos a disco para blocos de 2048 bytes.

Os gráficos da segunda coluna mostram o número médio de acessos a disco realizados para responder a 500 consultas aos vizinhos mais próximos com lista de empate em que o k foi, 3, 6, 9, 12, 15, 18, 21 e 24, e os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet.

Novamente, as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

As estruturas que realizaram o menor número de acessos a disco na consulta aos vizinhos mais próximos no pior caso, em que o k vale 24, foram, Partição Local para blocos de 51bytes, com escolha de mapeador randômico, tendo realizado 31 acessos a disco ou 0.

Os 02% dos blocos de um arquivo seqüencial.

Partição Local para blocos de102bytes, com escolha de mapeador randômico, tendo realizado 19 acessos a disco ou 0.

Os 03% dos blocos de um arquivo seqüencial e Partição Local para blocos de 2048 bytes, com escolha de mapeador randômico, tendo realizado 1acessos a disco ou 0.

Os 04% dos blocos de um arquivo seqüencial.

As estruturas que realizaram o maior número de acessos a disco na consulta aos vizinhos mais próximos no pior caso, em que o k vale 24, foram, M-Tree para blocos de 51bytes, tendo realizado 137acessos a disco ou 1.

Os 07% dos blocos de um arquivo seqüencial.

M-Tree para blocos de 102bytes, tendo realizado 287 acessos a disco ou 0.

Os 45% dos blocos de um arquivo seqüencial e M-Tree para blocos de 2048 bytes, tendo realizado 105 acessos a disco ou 0.

Os 34% dos blocos de um arquivo seqüencial.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 24, nas estruturas baseadas em regiões de cobertura comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 43,29 vezes mais cálculos de distância para blocos de 51bytes, 14,11 vezes mais acessos a disco para blocos de 102bytes e 7,08 vezes mais acessos a disco para blocos de 2048 bytes.

Análise do Número de Tempo de Execução para o Conjunto de Pontos GeoGráficos.

Os gráficos apresentados mostram os resultados de tempo de execução obtidos referentes aos dados do conjunto de dados de pontos geográficos.

A primeira coluna apresenta os gráficos obtidos utilizando consulta por abrangência e a segunda coluna mostra os gráficos obtidos utilizando consulta aos vizinhos mais próximos.

Os gráficos da primeira coluna mostram o tempo médio em milisegundos (ms) gasto para responder a 500 consultas por abrangência em que o raio foi, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40 (km), e os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet.

Pôde-se observar que as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura, como nos casos de cálculos de distância e acessos a disco.

O algoritmo de consulta por abrangência com o raio valendo 40 km nas estruturas baseadas em regiões de coberturas comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 185% mais tempo de execução para blocos de 51bytes, 36% mais tempo de execução para blocos de 102bytes e 39% mais tempo de execução para blocos de 2048 bytes.

Os gráficos da segunda coluna mostram o tempo médio em milisegundos (ms) para responder a 500 consultas dos vizinhos mais próximos com lista de empate em que o k foi, 3, 6, 9, 12, 15, 18, 21 e 24, e os objetos de consulta foram sorteados aleatoriamente do conjunto GEONet.

Novamente as estruturas baseadas em coordenadas métricas ganham em qualquer situação das estruturas baseadas em região de cobertura.

O algoritmo de consulta aos vizinhos mais próximos com k valendo 2nas estruturas baseadas em regiões de cobertura comparado com a melhor estrutura baseada em coordenadas métricas, foi pior, obtendo os seguintes valores, 177% mais tempo de execução para blocos de 51bytes, 7% mais Gráficos da média de tempo para 500 consultas de abrangência 1 colun e de vizinhos mais próximos colun para a base GEONet tempo de execução para blocos de 102bytes e 2% mais tempo de execução para blocos de 2048 bytes.

Os resultados mostraram que a organização das estruturas de particionamento realizam menor número de cálculos de distância nos blocos índices e folhas, menor número de acessos a disco e tempo pelos mesmos motivos mencionados na discussão dos resultados do conjunto de dados de proteínas.

Todos os experimentos mostraram que as estruturas Partição Local+ realizam um menor número de cálculos de distância pois usam a distância para o primeiro mapeador, no bucket folha, para evitar cálculos de distância.

Essa função de distância é menos custosa do que as anteriores, no entanto, o tempo das estruturas de Partição Local+ são próximos ao tempo da Partição Local, mesmo realizando uma maior quantidade de acessos a disco.

Neste experimento foi avaliada a escalabilidade das estruturas, M-Tree com quebra pelo algoritmo MINMAX.

Slim-Tree com quebra pelo algoritmo MINMAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador randômico Ran, e Partição Local+ com escolha de mapeador randômico Ran.

Para cada estrutura mencionada variou o tamanho bloco de 51bytes, 102bytes, 2048 bytes e 4096 bytes totalizando 16 estruturas.

O conjunto de dados escolhido foi o do projeto GEONet pois, é o maior conjunto de dados contendo 3976221 pontos geográficos.

Os gráficos da primeira coluna mostram o tempo gasto em segundos (s) para criar as estruturas (gráfico 420, o tempo médio em milisegundos (ms) para responder a 500 consultas dos 20 vizinhos mais próximos (gráfico 420 e o tempo médio gasto em milisegundos (ms) para responder a 500 consultas por abrangência com raio de 20 km (gráfico 420) variando o tamanho de blocos das estruturas.

Informações sobre as Estruturas do conjunto GEONet.

Características das estruturas com blocos de 4096 bytes pois, as informações sobre os blocos de 512, 102e 2058 bytes estão na tabela.

As características das estruturas são, o tamanho de blocos.

A altura para as estruturas M-Tree, Slim-Tree e a altura máxima das estruturas Partição Local e Partição Local+.

A quantidade de nó índice.

A quantidade de nó folha e a porcentagem de objetos em relação ao total de registros que estão em regiões de colisão, sendo que essa característica só se aplica para as estruturas baseadas em coordenadas métricas.

Gráficos de Escalabilidade variando o Tamanho de Blocos 1 colun e variando o Número de Objetos colun.

A partir dos gráficos da primeira coluna, pode-se concluir que, quanto maior o tamanho do bloco, mais tempo é gasto para construir as estruturas de região de cobertura, sendo que a estrutura M-Tree com blocos de 4096 bytes gastou 4 horas, 1 minuto e 5 segundos e a estrutura de Partição Local Rand.

Gastou 55 minutos e 30 segundos em sua construção.

Quanto maior a altura, mais tempo é gasto para responder uma consulta aos vizinhos mais próximos nas estruturas de região de cobertura sendo que, a estrutura M-Tree com blocos de 4096 bytes gastou 7,08 milisegundos para responder a consulta dos 20 objetos mais próximos e a estrutura de Partição Local Rand.

Gastou 0,7milisegundos para responder a consultas dos 20 objetos mais próximos e quanto maior a altura, mais tempo é gasto para responder uma consulta por abrangência nas estruturas de região de cobertura sendo que, a estrutura M-Tree com blocos de 4096 bytes gastou 10,86 milisegundos para responder a consulta com raio de 20 km e a estrutura de Partição Local Rand.

Gastou 4,28 milisegundos para responder a consulta com raio de 20 km.

Os gráficos da segunda coluna mostram o tempo gasto em segundos (s) para criar as estruturas com bloco de 51bytes (gráfico 420, o tempo médio em milisegundos (ms) para responder a 500 consultas dos 20 vizinhos mais próximos (gráfico 420 e o tempo médio gasto em milisegundos (ms) para responder a 500 consultas por abrangência com o raio de 20 km (gráfico 420(f)) variando o número de objetos indexados pelas estruturas.

Características da construção das estruturas com o bloco de tamanho de 51bytes, M-Tree com quebra pelo algoritmo MINMAX.

Slim-Tree com quebra pelo algoritmo MIN-MAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador randômico Ran, e Partição Local+ com escolha de mapeador randômico Ran.

Para cada estrutura mencionada anteriormente, foram criadas dez estruturas variando o número de objetos, com os seguintes valores, 397622, 795244, 1192866, 1590488, 1988110, 2385732, 2783354, 3180976, 3578598 e 3976221.

Optou-se em utilizar blocos de 51bytes pois como pode ser visto no gráfico 420, nota-se que a diferença no tempo de construção das estruturas não é muito grande comparado com blocos de 102bytes, 2048 bytes e 4096 bytes.

A partir dos gráficos, pode-se concluir que, na média de 500 consultas dos 20 vizinhos mais próximos às estruturas de região de cobertura, quanto maior o número de objetos na base, maior é o tempo para responder a consultas, sendo que no pior caso em que o número de objetos é 3276221 a M-Tree gasta 16,8vezes mais tempo que a estrutura de Partição Local.

E na média de 500 consultas por abrangência com o raio de 20 km, as estruturas de região de cobertura quanto maior o número de objetos na base, maior é o tempo para responder a consultas, sendo que no pior caso em que o número de objetos é 3276221 a M-Tree gasta 2,8vezes mais tempo que a estrutura de Partição Local.

Informações sobre as Estruturas do conjunto GEONet.

As estruturas de particionamento sempre ganham em tempo para responder consultas por abrangência e consultas aos vizinhos mais próximos das estruturas com sobreposição, sendo que esse ganho é maior à medida que aumenta a altura das estruturas de região de cobertura.

A altura nas estruturas de região de cobertura é influenciada pelo tamanho do bloco, mas blocos muito grandes aumentam o tempo de construção das estruturas.

O tempo gasto na construção das estruturas de particionamento não é influenciado pela variação do tamanho do bloco.

Entretanto, para a construção das estruturas de região de cobertura quanto maior é o tamanho do bloco, mais tempo é gasto.

A estrutura de partição+ perde em algumas situações nos acessos a disco para responder as consultas comparado com as estruturas de região de cobertura pois, possuem mais blocos folhas e conseqüentemente mais blocos índices.

Essa diferença acontece porque os blocos folhas das estruturas de particionamento possuem um campo adicional que é responsável por armazenar as coordenadas métricas.

No entanto, ela realiza menos cálculos de distâncias e assim possui um tempo inferior que as estruturas de região de cobertura.

As funções de distância (tal como a euclidiana usada no conjunto de dados GEONet) que têm custo pequeno e apresentam grande domínio de valores possíveis mostraram nos testes realizados que as estruturas de particionamento são mais rápidas, comparadas com as estruturas de região de cobertura em blocos de 51bytes, 185% para responder as consultas por abrangência com o raio de 40 km e 177% para responder os 2objetos mais próximos.

As funções de distância (conjunto de dados UNIPROT) que têm custo alto e apresentam domínio de valores possíveis pouco maior em relação ao conjunto de dados OpenOffice, mostraram nos testes realizados que as estruturas de particionamento são mais rápidas.

Comparado com as estruturas de região de cobertura em blocos de 102bytes, 266% para responder as consultas por abrangência com o raio de 5 unidades de alinhamento e 33% para responder os 5 objetos mais próximos em alinhamento.

As funções de distância (conjunto de dados OpenOffice) que têm custo médio e apresentam pequeno domínio de valores possíveis mostraram nos testes realizados que as estruturas de particionamento são mais rápidas, comparado com as estruturas de região de cobertura em blocos de 51bytes, 13% para responder as consultas por abrangência com o raio de 5 unidades e 93% para responder os 5 objetos mais próximos.

O próximo capítulo apresenta outra contribuição deste trabalho que é, a implementação da junção por abrangência, aos vizinhos mais próximos e por proximidade nas estruturas discutidas neste capítulo.

As operações de junção por similaridade combinam objetos de dois subconjuntos que pertencem a um mesmo domínio D, entre os quais uma condição de similaridade deve ser satisfeita.

As condições de similaridade podem ser as mesmas da seleção por similaridade, por abrangência e de vizinhos mais próximos.

Além dessas, a literatura cita um outro operador de junção que se chama junção por proximidade (do inglês closest neighbors join).

Neste capítulo serão apresentados as implementaçõs destas condições de junção por similaridade em, arquivo seqüencial.

Estruturas métricas de região de cobertura e estruturas métricas de particionamento.

As implementações dessas condições de junção nas estruturas métricas (Slim-Tree, Partição Local e Partição Local+) são contribuições inovadoras deste trabalho.

A junção por similiridade em arquivo seqüencial utiliza duas listas de blocos em que os dados são armazenados seqüencialmente.

Essa característica garante que as páginas tenham sempre a máxima quantidade de objetos, minimizando assim a quantidade de blocos usados.

Os algoritmos de junção para esse tipo de estrutura são caros, já que examinam todos os pares de objetos nas duas estruturas, direita e esquerda.

Apesar disso, neste trabalho servirão de base para a comparação com os algoritmos otimizados.

O princípio desses algoritmos é que, para cada bloco do arquivo seqüencial da direita deve-se navegar em todas os blocos do arquivo seqüencial da esquerda.

As implementações de qualquer condição de junção descritas nos Algoritmos 51, 52, 53, garantem que a quantidade de acessos a disco corresponde à multiplicação da quantidade de blocos de cada arquivo seqüencial, mais a quantidade de blocos da estrutura da esquerda.

Além disso, a quantidade de cálculos de distância corresponde à multiplicação da quantidade de objetos de cada arquivo seqüencial mais a quantidade de objetos da estrutura da esquerda.

Para tal, o consumo de memória varia de acordo com o algoritmo.

A junção por abrangência responde a seguinte consulta, "Selecione as proteínas contidas no vírus da Hepatite B que diferem em até duas unidades de alinhamento das que estão contidas no vírus da Hepatite C".

Essa operação é comutativa, ou seja, a ordem dos conjuntos entre o operador não altera a resposta considerando os pares encontrados desde que o raio permaneça o mesmo.

No entanto, a inversão da ordem das estruturas envolvidas pode refletir em um custo computacional diferente.

O algoritmo em arquivos seqüenciais da junção por abrangência é descrito no Algoritmo 51.

Ele recebe dois arquivos seqüenciais e a distância máxima entre os objetos.

Esse algoritmo necessita apenas que as páginas sejam alocadas por vez na memória, consumindo menos memória que os demais, pois um par de objetos selecionado certamente faz parte da resposta, e não há necessidade de memória adicional.

Como essa operação é comutativa, uma forma de otimizar esse algoritmo é usar no lação externo o arquivo seqüencial com o menor número de objetos.

A junção por vizinhos mais próximos responde a seguinte consulta, "Selecione as três proteínas mais parecidas pelo alinhamento entre as proteínas do vírus da Hepatite C para cada proteína do vírus da Hepatite B".

Essa operação não é comutativa, ou seja, a ordem dos conjuntos entre o operador pode alterar a resposta.

O algoritmo em arquivos seqüenciais da junção por vizinhos mais próximos, é descrito no Algoritmo 52.

Esse algoritmo recebe dois arquivos seqüenciais, uma quantidade de vizinhos qualquer e deseja-se manter os empates da última resposta.

O consumo de memória desse algoritmo é dado pela alocação de dois blocos (um de cada arquivo seqüencial) e listas com resultados locais, onde é a quantidade de objetos do bloco da esquerda.

Cada lista contém quantidade de vizinhos mais próximos e são mantidas ordenadas aumentando assim o custo da execução.

As listas de resultados locais são necessárias para garantir que, a quantidade de acessos a disco corresponde à multiplicação da quantidade de blocos de cada arquivo seqüencial, mais a quantidade de blocos da estrutura da esquerda.

E a quantidade de cálculos de distância, corresponde à multiplicação da quantidade de objetos de cada arquivo seqüencial mais a quantidade de objetos da estrutura da esquerda.

Isto acontece porque a inclusão de um par de objetos não é garantia de que tal par faça realmente parte da resposta.

A junção por proximidade responde a seguinte consulta, "Selecione os cinco pares de proteínas mais parecidas pelo alinhamento entre as contidas no vírus da Hepatite C e das contidas no vírus da Hepatite B".

Essa operação também é comutativa, ou seja, a ordem dos conjuntos entre os operadores não altera a resposta, desde que se considere que (r,s) e (s,r) correspondem ao mesmo par.

A junção por proximidade aplica o princípio da busca aos vizinhos mais próximos no produto cartesiano.

Primeiro, todos os pares são formados, e então, os melhores k são selecionados.

O algoritmo de junção por proximidade, descrito no Algoritmo 53, recebe dois arquivos seqüenciais, uma quantidade de pares qualquer e deseja-se manter os empates da última resposta.

Esse algoritmo consome duas páginas em memória e uma lista com a quantidade de pares de objetos mais próximos.

O problema é que essa lista é mantida ordenada, o que aumenta o custo da execução.

Este tipo de implementação de junção é proposto neste trabalho e pode ser utilizado nas estruturas Slim-Tree e M-Tree, sem nenhuma alteração.

A seguir serão apresentados os algoritmos para as condições de junção, por abrangência, por vizinhos mais próximos e por proximidade.

Todas as implementações consideram que ambas as estruturas usadas na junção podem conter quantidade de níveis diferentes e por isso, os algoritmos são um pouco extensos.

Na implementação da junção por abrangência navega-se simultaneamente em cada nível hierárquico de duas regiões de cobertura de ambas as estruturas envolvidas na junção.

Durante a navegação, o algoritmo de junção pode restringir os nós que serão navegados por ambos os lados estrutura esquerda e direit.

O algoritmo deve navegar em somente um dos lados quando chegar a um nó folha de uma das estruturas envolvidas.

Isso acontece quando as estruturas envolvidas na junção não possuem a mesma quantidade de níveis.

Ilustra-se a seguir como dois nós índices, cada um pertencente a uma Slim-Tree, cobrem o espaço métrico.

Todos os objetos representantes, com suas respectivas regiões de cobertura, da estrutura que está sendo usada do lado esquerdo da junção, são representados pela letra a e os da estrutura que está sendo usada do lado direito da junção são representados pela letra b.

As regiões de cobertura que têm os objetos representantes, a da estrutura do lado esquerdo, sobrepõem as regiões de cobertura que tem os objetos representantes da estrutura do lado direito da junção.

Regiões que se sobrepõem entre duas Slim-Tree, Regiões sem raio de consulta e Regiões com raio de consulta.

No entanto, quando é realizada uma operação de junção por abrangência, existe um raio que é passado para a consulta e todas essas regiões têm seu raio de cobertura incrementados pelo valor desse raio.

Além das regiões sobrepostas descritas anteriormente, o raio da consulta sobrepôs a região de cobertura que tem o objeto representante b da estrutura do lado 6 direito da junção.

Portanto, serão visitadas as sub-nós da estrutura do lado esquerdo da junção que têm os representantes e os sub-nós da estrutura do lado direito da junção, que têm os representantes.

Ilustra-se a seguir como é feita a poda na estrutura do lado direito usando a desigualdade triangular sem que seja necessário realizar cálculos de distâncias adicionais.

Para podar o nó, a distância entre os representantes de cada nó da esquerda e da direita calculados previamente menos a cobertura do nó da esquerda menos o raio deve ser menor que a distância entre o representante da direita e o objeto (que já está armazenado no nó) mais o raio de cobertura do objeto.

Pode-se observar que a desigualdade triangular consegue podar sem realizar cálculos de distância os objetos.

Os objetos serão podados realizando cálculos de distância.

Assim, somente a cobertura do objeto do lado direito da estrutura deve ser verificado se existem objetos para a resposta.

Usando o mesmo princípio, é realizada a poda do lado esquerdo da estrutura usando a cobertura do lado direito e o raio para cada distância entre o objeto do lado esquerdo e o representante da esquerda e sua cobertura.

Poda através da Desigualdade Triangular.

O Algoritmo 5mostra, como é feita a implementação da junção por abrangência nas estruturas de região de cobertura com a estrutura de buffer.

Para minimizar o número de acessos a disco no Algoritmo 5pode-se habilitar um buffer para a estrutura do lado direito da junção que mantém em memória primária todos sub-nós de um nó visitado.

A quantidade de blocos que deve ser mantida em memória primária é dada pela altura da estrutura vezes a quantidade de sub-nós por nó mais 1 que é o nó raiz.

Para exemplificar, são utilizadas duas estruturas Slim-Tree com tamanho de bloco de 51bytes, com três níveis sendo que cada nível referencia quatro sub-nós.

O consumo de memória para manter o buffer é 819bytes, ou seja, três blocos da altura da estrutura da direita mais treze blocos da estrutura de buffer da esquerda.

O Algoritmo 5pode ser dividido em quatro etapas, se ambos os nós da esquerda e da direita são índices, deve-se verificar quais coberturas de objetos da esquerda e da direita com o raio de consulta se sobrepõem com a cobertura do representante da direita e da esquerda respectivamente.

Utiliza-se a desigualdade triangular para podar os nós da esquerda e da direita.

Se o nó da esquerda é índice e o nó da direita é folha, deve-se verificar quais coberturas de objetos da esquerda com o raio de consulta se sobrepõem com a cobertura do representante da direita.

Utiliza-se a desigualdade triangular para podar os nós da esquerda.

Se o nó da esquerda é folha e o nó da direita é índice, deve-se verificar quais coberturas de objetos da direita com o raio de consulta se sobrepõem com a cobertura do representante da esquerda.

Utiliza-se a desigualdade triangular para podar os nós da direita e se ambos os nós da esquerda e da direita são folhas, deve-se verificar quais distâncias entre os objetos são menores que o raio.

Utiliza-se a desigualdade triangular para evitar cálculos de distância entre objetos da esquerda e da direita.

Na implementação da junção por vizinhos mais próximos navega-se em uma estrutura de cada vez, primeiro na estrutura do lação externo até chegar em um nó folha, e depois na estrutura do lação interno.

A junção por vizinhos mais próximos, mostrada no Algoritmo 55, deve descer somente do lação externo até chegar ao nó folha.

A partir desse momento, inicia-se a descida do lado direito da estrutura realizando uma consulta por vizinhos mais próximos para cada nó da estrutura do lação externo.

Quando o algoritmo completar a lista de respostas com k elementos, deve atualizar o raio máximo com o valor da maior distância de pares de objetos.

Para minimizar o número de acessos a disco no Algoritmo 55 pode-se habilitar um buffer para a estrutura do lado direito da junção que mantém em memória primária todos sub-nós de um nó visitado, como mencionado na seção anterior.

O consumo de memória sem o uso do buffer desse algoritmo é dado pela alocação de i blocos da estrutura da esquerda e j blocos da estrutura da direita, onde i e j são as alturas de cada estrutura respectivamente.

Além disso, deve-se manter listas com resultados locais, onde n é a quantidade de objetos do bloco folha da estrutura da esquerda.

Cada lista contém k quantidade de vizinhos mais próximos e estas listas são mantidas ordenadas aumentando assim o custo da execução.

As listas de resultados locais são necessárias para minimizar a quantidade de acessos a disco e de cálculos de distância.

Isto acontece porque a inclusão de um par de objetos não é garantia de que tal par faça realmente parte da resposta.

Na implementação da junção por proximidade mostrada no Algoritmo 56, deve-se navegar simultaneamente em cada nível hierárquico de duas regiões de cobertura de ambas as estruturas envolvidas na junção.

A junção por proximidade é dividida em dois algoritmos, o primeiro apresentado no Algoritmo 56 e o segundo é mostrado no Algoritmo 5que deve ser modificado para remover os objetos excedentes ao k, minimizando o raio máximo com o valor da maior distância de pares de objetos.

Para minimizar o número de acessos a disco, pode-se habilitar um buffer.

O consumo de memória sem o uso do buffer desse algoritmo é dado pela alocação de i blocos da estrutura da esquerda e j blocos da estrutura da direita, onde i e j são as alturas de cada estrutura respectivamente.

A implementação das consultas de junção nos métodos de acessos por Partição Local e Partição Local+ também está sendo proposto neste trabalho.

Essas estruturas usam a distância entre os objetos mapeadores para organizar os seus níveis hierárquicos.

Nessas implementações, cada bucket é visitado simultaneamente em ambas as estruturas que compõem a junção.

Essa visita irá restringir, pela sobreposição das estruturas, quais sub-buckets deverão ser navegados.

Na implementação da junção por abrangência apresentado, no Algoritmo 57, deve-se navegar simultaneamente em cada nível hierárquico de ambas as estruturas envolvidas na junção.

O Algoritmo 57 mostra, como é feita a implementação da junção por abrangência nas estruturas de região de cobertura.

A técnica que mantém um buffer para a estrutura interna não pode ser aplicada nessa estrutura, pois mais de uma chave pode redirecionar para um mesmo bucket.

Duas funções mapeadoras dividem o espaço métrico de dois buckets índice, cada uma pertencente a uma Partição Local.

Todos os objetos representantes da estrutura que está sendo usada do lado esquerdo da junção, com suas respectivas regiões de cobertura, são representados pela letra a e os da estrutura que está sendo usada do lado direito da junção são representados pela letra b.

Duas funções de mapeamento de duas estruturas de Partição Local sendo que, a do lado esquerdo tem objetos mapeadores e a do lado direito tem objetos mapeadores.

Pode-se observar que existe sobreposição entre os eixos dos objetos.

Regiões que se sobrepõem entre duas estruturas Partição Local, Regiões sem raio de consulta e Regiões com o raio de consulta aumentando o número de cobertura de cada nó No entanto, quando é realizada uma operação de junção por abrangência, existe um raio que é passado para a consulta que incrementa essa sobreposição.

Pode-se observar que as divisões que ficaram sobrepostas foram, 1 e do mapeador A entre 1 do mapeador B.

Mapeador A entre 1, e do mapeador B.

Mapeador A entre do mapeador B.

Portanto, serão visitados os sub-nós da estrutura do lado esquerdo da junção que têm as coordenadas métricas e da estrutura do lado direito da junção, os sub-nós que têm as coordenadas.

Na implementação da junção por vizinhos mais próximos navega-se em uma estrutura de cada vez.

Primeiro na estrutura do lação externo até chegar em um nó folha, e depois na estrutura do lação interno.

A junção por vizinhos mais próximos, mostrado no Algoritmo 58, deve descer somente do lação externo até chegar ao nó folha.

A partir desse momento, inicia-se a descida do lado direito da estrutura realizando uma consulta por vizinhos mais próximos para cada nó da estrutura do lação externo.

Quando o algoritmo completar a lista de resposta com k elementos, deve atualizar o raio máximo com o valor da maior distância de pares de objetos.

O consumo de memória desse algoritmo é dado pela alocação de i blocos da estrutura da esquerda e j blocos da estrutura da direita, onde i e j são as alturas de cada estrutura respectivamente.

Além disso, deve-se manter n listas com resultados locais, onde n é a quantidade de objetos do bloco folha da estrutura da esquerda.

Cada lista contém k quantidade de vizinhos mais próximos e são mantidas ordenadas aumentando assim o custo da execução.

As listas de resultados locais são necessárias para minimizar a quantidade de acessos a disco e cálculos de distância.

Isto acontece porque a inclusão de um par de objetos não é garantia de que tal par faça realmente parte da resposta.

Na implementação da junção por proximidade deve-se navegar simultaneamente em cada nível hierárquico de ambas as estruturas envolvidas na junção.

A junção por proximidade é muito parecida com o Algoritmo 55 que é dividido em dois algoritmos, o primeiro apresentado no Algoritmo 59 e o segundo é mostrado no Algoritmo 57 que deve ser modificado para remover os objetos excedentes ao k, minimizando o raio máximo com o valor da maior distância de pares de objetos.

O consumo de memória desse algoritmo, é dado pela alocação de i blocos da estrutura da esquerda e j blocos da estrutura da direita, onde i e j são as alturas de cada estrutura respectivamente.

Técnicas usando Árvore AVL para Lista Global de Resposta.

A literatura cita que, em todo algoritmo de consulta por vizinhos mais próximos, seja para qualquer estrutura, deve ser mantida em memória primária uma lista global.

Essa lista deve ser ordenada pelas distâncias do objeto de consulta para os objetos de resposta.

Durante a execução do algoritmo de consulta por vizinhos mais próximos, uma inserção ordenada na lista pode gerar a retirada de um objeto excedente à quantidade k, ou mesmo de vários objetos se a consulta deve recuperar também a lista de empate.

Existem duas formas tradicionais de implementação dessa lista, usando o vetor estático ou lista duplamente encadeada.

A implementação usando vetor estático apresenta um problema quando a consulta exige a lista, pois não é possível prever o tamanho do vetor.

A vantagem é que a operação de retirada não é custosa, pois pode-se navegar a partir do final do vetor para o início e remover os objetos excedentes.

O problema aparece nas operações de inserção, pois embora sejam beneficiadas pela possibilidade de se usar busca binária, existe a necessidade de deslocamento dos elementos até o final do vetor.

A implementação usando lista duplamente encadeada usa um pouco mais de memória, pois em cada nó devem ser armazenados também os endereços para os nós antecessor e sucessor, mas não precisa reservar memória para uma possível lista de empate.

A vantagem também está na operação de retirada que não é custosa, pois pode navegar do final da lista para o início removendo os excedentes.

O problema aparece novamente na inserção, pois embora não se desloquem elementos por causa da alocação dinâmica, não se pode usar agora a busca binária, obrigando que se realize uma busca seqüencial até encontrar a posição de inserção do novo elemento.

Assim, qualquer forma de implementação apresenta alto custo quando a consulta de seleção dos k vizinhos mais próximos retorna uma grande quantidade de resposta, ou quando a estrutura indexa vários níveis e assim, a todo momento acontece inserção e retirada de objetos excedentes na lista.

O algoritmo básico da consulta por abrangência não necessita dessa lista global.

Entretanto, se além da operação de consulta for requisitada, também a ordenação das distâncias de resposta, faz-se necessário a implementação dessa lista global.

Essa situação se agrava quando é realizada uma operação de junção, pois o conjunto de resposta normalmente é bem maior.

Para solucionar esse problema, optou-se por implementar uma lista global usando-se uma árvore binária balanceada AVL criada em 1962.

A inserção é feita através de navegação em altura na árvore e acessa elementos.

A remoção dos excedentes navega a partir do último elemento à direita para que ele seja removido.

Como a árvore está balanceada, essa operação também acessa elementos.

Se a consulta exigir a lista de empate, antes de remover todos os objetos repetidos, deve-se garantir que a lista não vai ficar menor que k objetos.

O conjunto de dados usado neste experimento é o conjunto de cidades do projeto GEOnet Names Server que contém 3976221 pontos geográficos (latitude e longitude).

Os dados foram filtrados para compor quatro subconjuntos, que contêm as cidades de quatro países, Brasil, com 47518 pontos.

Peru, com 36831 pontos.

Argentina, com 28027 pontos e o Equador, com 9230 pontos.

Embora a máquina tenha dois processadores, os algoritmos são seqüenciais ou seja, processados em um único processador.

Um dado importante da configuração é o cache de 51para o processador que permite realizar os cálculos de distância das proteínas e das palavras em cache do que em memória RAM.

A função de distância para o conjunto de cidades é a mesma utilizada anteriormente.

Para cada conjunto de dados foram construídas estruturas seqüencial e quatro métodos de acesso, Seqüencial.

Slim-Tree com quebra pelo algoritmo MINMAX e otimizado pelo algoritmo SlimDown.

Partição Local com escolha de mapeador pelo randômico, e Partição Local+ com escolha de mapeador pelo randômico.

Além disso, para cada método de acesso e conjunto de dados, foram construídas as estruturas usando diferentes tamanhos de bloco em disco, 51bytes, 102bytes e 2048 bytes.

As características mostradas são, a base que é o subconjunto de dados do GEONet.

O tamanho do bloco.

A altura para as estruturas Slim-Tree e a altura máxima das estruturas Partição Local e Partição Local+.

A quantidade de nó índice.

A quantidade de nó folha.

O tamanho do arquivo gerado.

A porcentagem de objetos em relação ao total de registros que estão em regiões de colisão, sendo que essa característica só se aplica para as estruturas baseadas em coordenadas métricas.

O número de acessos a disco realizados na construção das estruturas.

O número de cálculos de distância realizados na construção das estruturas e o tempo de execução para a construção das estruturas.

O tempo, número de acessos a disco e números de cálculos de distância mostrado para a estrutura Slim-Tree refere-se ao total para a construção e execução do algoritmo de Slim-Down.

A partir dos valores, podemos concluir que, os métodos de acesso baseado em coordenadas métricas geram estruturas com até o dobro de níveis do que as estruturas baseadas em região de cobertura.

Essa diferença mostra que a escolha dos objetos mapeadores que compõem a função de mapeamento local não foi boa para qualquer variação de tamanho de bloco.

As estruturas Partição Local possuem em média 25% menos blocos do tipo folha do que a estrutura Slim-Tree.

As estruturas Partição Local+ possuem em média 13% mais blocos do tipo folha do que a estrutura Slim-Tree.

A colisãoem algumas basechegou até7,7%da base, influenciadas pela escolha ruim dos objetos mapeadores.

As estruturas Partição Local são construídas em 1,6 vezes menos tempo do que a estrutura Slim-Tree, pois realiza menos cálculos de distância e menos acessos a disco.

Junção por Abrangência.

Os gráficos mostram o número de acessos a disco, número de cálculos de distância e o tempo de execução para a junção por abrangência entre os pontos geográficos da Argentina e do Brasil, onde o raio foi 10, 20, 30, 40, 50, 60, 70, 80, 90 e 100 km.

Pelos valores, podemos calcular o número de acessos a disco para qualquer tamanho de raio na junção por abrangência em bloco aninhado, em arquivos seqüenciais, multiplicando a quantidade de nó folhas dos arquivos envolvidos.

Portanto, o número de acessos a disco entre a base da Argentina e do Brasil é de, 1387365 para blocos de 51bytes.

Para blocos de 102bytes e 82875 para blocos de 2048 bytes.

Da mesma maneira, o número de cálculos de distância para qualquer tamanho de raio na junção por abrangência em bloco aninhado, em arquivos seqüenciais, é dado pela multiplicação da quantidade de objetos dos arquivos envolvidos.

Portanto, o número de cálculos de distância entre a base da Argentina e do Brasil é de 1331786986 independente do tamanho dos blocos.

Comparando os gráficos com os resultados pode-se concluir que, o número de acessos a disco é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o raio valendo 100 km, foi a Partição Local, com os seguintes números de acessos a disco, 370ou 0.

O 26% do acesso da junção por abrangência em bloco aninhado com bloco de tamanho de 51bytes.

Os 65% do acesso da junção por abrangência em bloco aninhado com bloco de tamanho de 102bytes e 1790 ou 2.

Os 16% do acesso da junção por abrangência em bloco aninhado com bloco de tamanho de 2048 bytes.

O número de cálculos de distância é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o raio valendo 100 km, foi a Partição Local+, com os seguintes números de cálculos de distância, 185117 ou 0.

Os 014% da junção por abrangência em bloco aninhado com blocos de tamanho de 51bytes.

Gráficos com números de acessos a disco 1 colun e de número de cálculos de distância colun para junção por abrangência entre as bases Argentina e Brasil abrangência em bloco aninhado com bloco de tamanho de 102bytes e 370570 ou 0.

Os 027% da junção por abrangência em bloco aninhado com bloco de tamanho de 2048 bytes e as estruturas obtiveram tempo de execução semelhantes pois, o número de cálculos de distância é superior ao acesso a disco.

Os gráficos mostram o número de acessos a disco, número de cálculos de Gráficos de tempo de execução para junção por abrangência entre as bases Argentina e Brasil distância e o tempo de execução para a junção por abrangência entre os pontos geográficos do Equador e Peru, onde o raio foi 10, 20, 30, 40, 50, 60, 70, 80, 90 e 100 km.

Pelos valores, podemos calcular o número de acessos a disco para qualquer tamanho de raio na junção por abrangência em bloco aninhado, em arquivos seqüenciais, multiplicando a quantidade de nó folhas dos arquivos envolvidos.

Portanto, o número de acessos a disco entre a base do Equador e do Peru é de, 35432para blocos de 51bytes.

Para blocos de 102bytes e 2124para blocos de 2048 bytes.

Da mesma maneira, o número de cálculos de distância para qualquer tamanho de raio na junção por abrangência em bloco aninhado, em arquivos seqüenciais, é dado pela multiplicação da quantidade de objetos dos arquivos envolvidos.

Portanto, o número de cálculos de distância entre a base do Equador e do Peru é de 339950130 independente do tamanho dos blocos.

Comparando os gráficos com os resultados, pode-se concluir que, o número de acessos a disco é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

Gráficos com números de acessos a disco 1 colun e de número de cálculos de distância colun para junção por abrangência entre as bases Equador e Peru.

A estrutura que obteve os melhores resultados com o raio valendo 100 km, foi a Partição Local, com os seguintes números de acessos a disco, 1426ou 4.

Os 02% do acesso da junção por abrangência em bloco aninhado de tamanho de 51bytes.

Os 21% do acesso da junção por abrangência em bloco aninhado de tamanho de 102bytes e 900 ou 4.

Os 23% do acesso da junção por abrangência em bloco aninhado de tamanho de 2048 bytes.

O número de cálculos de distância é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o raio valendo 100 km, foi a Partição Local+, com os seguintes números de cálculos de distância, 2077100 ou 0.

Os 61% da junção por abrangência em bloco aninhado de tamanho de 51bytes.

E 57% da junção por abrangência em bloco aninhado de tamanho de 102bytes e 298224ou 0.

E 87% da junção por abrangência em bloco aninhado de tamanho de 2048 bytes.

E as estruturas obtiveram tempo de execução semelhantes, pois o número de cálculos de distância é superior ao acesso a disco.

Os gráficos mostram o número de acessos a disco, número de cálculos de distância e o tempo de execução para a junção aos vizinhos mais próximos entre os pontos geográficos do Equador e Brasil em que o k foi 1, 2, 3, 4, 5, 6, 7, 8, 9 e 10 pontos geográficos.

Gráficos de Número de Acessos a Disco, Número de Cálculos de Distância e o Tempo de Execução para junção aos vizinhos mais próximos entre as bases Equador e Brasil.

Pelos valores, pode-se calcular o número de acessos a disco para qualquer tamanho de raio na junção aos vizinhos mais próximos em bloco aninhado, em arquivos seqüenciais, multiplicando a quantidade de nós folha dos arquivos envolvidos.

Portanto, o número de acessos a disco entre a base do Equador e do Brasil é de 45683para blocos de 51bytes.

Da mesma maneira, o número de cálculos de distância para qualquer tamanho de raio na junção aos vizinhos mais próximos em bloco aninhado, em arquivos seqüenciais, é dado pela multiplicação da quantidade de objetos dos arquivos envolvidos.

Portanto, o número de cálculos de distância entre a base do Equador e do Peru é de 438591140 independente do tamanho dos blocos.

Comparando o gráfico com os resultados, pode-se concluir que, o número de acessos a disco é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o k valendo 10 unidades, foi a Slim-Tree com 19352 ou 42.

Os 36% acessos da junção aos vizinhos mais próximos em bloco aninhado de tamanho de 51bytes.

E o número de cálculos de distância é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o k valendo 10 unidades foi a Slim-Tree com 371016 ou 0.

E 84% da junção aos vizinhos mais próximos em bloco aninhado de tamanho de 51bytes.

Junção por Proximidade.

Os gráficos mostram o número de acessos a disco, número de cálculos de distância e o tempo de execução para a junção por proximidade entre os pontos geográficos do Peru e Brasil em que o k foi 1, 2, 3, 4, 5, 6, 7, 8, 9 e 10 pontos geográficos.

Os gráficos mostram o número de acesso a discc, número de cálculos de distância e o tempo de execução para junção por proximidade entre as bases Peru e Brasil.

Pelos valores, pode-se calcular o número de acessos a disco para qualquer tamanho de raio na junção por proximidade em bloco aninhado, em arquivos seqüenciais, multiplicando a quantidade de nós folha dos arquivos envolvidos.

Portanto, o número de acessos a disco entre a base do Peru e do Brasil é de 1822737 para blocos de 51bytes.

Da mesma maneira, o número de cálculos de distância para qualquer tamanho de raio na junção por proximidade em bloco aninhado, em arquivos seqüenciais, é dado pela multiplicação da quantidade de objetos dos arquivos envolvidos que é de 1750135458 cálculos.

Comparando o gráfico com os resultados, pode-se concluir que, o número de acessos a disco é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o k valendo 10 unidades foi a Slim-Tree com 69783 ou 38.

Os 28% acesso da junção por proximidade em bloco aninhado de tamanho de 51bytes.

E o número de cálculos de distância é menor nas estruturas baseadas em região de cobertura e nas estruturas baseadas em coordenadas métricas comparado com o arquivo seqüencial.

A estrutura que obteve os melhores resultados com o k valendo 10 unidades foi a Slim-Tree com 371016 ou 0.

Os 21% da junção por proximidade em bloco aninhado de tamanho de 51bytes.

Discussão dos Resultados Os resultados mostraram que a organização das estruturas de baseadas em região de cobertura realizam menor número de cálculos de distância, menor número de acessos a disco e utilizam pouco tempo para realizar os algoritmos de junção.

As estruturas baseadas em particionamento apresentam um problema que deverá ser tratado na função mapeadora que determina as coordenadas de início e fim de ambas estruturas.

Como mencionado anteriormente, a função mapeadora usa as distâncias entre os mapeadores para determinar as coordenadas de início e fim.

O problema está em, como não existe uma distância máxima para cada mapeador, a função mapeadora pode habilitar uma região desnecessária a região extern em cada bucket navegado.

Isso faz com que as estruturas baseadas em particionamento façam mais acessos a disco, cálculos de distância e conseqüentemente gastam mais tempo.

Uma proposta para solucionar esse problema é alterar os buckets índices (responsáveis pelo armazenamento das funções mapeadores) nas estruturas de particionamento para armazenar essa distância.

Essa distância máxima não é necessária para os algoritmos de seleção, mas deve minimizar bastante o número de cálculos de distância, número de acessos a disco e tempo Os testes da junção por abrangência mostraram que as estruturas de particionamento e de região de cobertura apresentam tempos próximos.

As estruturas baseadas em região de cobertura sempre ganham para responder junção aos vizinhos mais próximos e junção por proximidade.

As estruturas baseadas em particionamento apresentam um problema na função de mapeamento para determinar as coordenadas de início e fim da junção.

As primeiras pesquisas com armazenamento de dados convencionais relatam a utilização de estruturas baseadas em árvore e, em uma segunda fase surgiram as estruturas baseadas em funções de mapeamento local.

Esse trabalho definiu duas funções de mapeamento local com a característica de extensibilidade que apresentou um menor custo em todos os aspectos, acessos a disco e cálculos de distância, refletindo um menor tempo de resposta.

Um outro aspecto é que as consultas por similaridade por abrangência e dos vizinhos mais próximos têm sido considerados como equivalentes ao operador de seleção.

Porém, para que essas consultas possam ser incorporadas aos gerenciadores, exige-se que essas suportem também as operações de junção.

Neste trabalho foram propostas técnicas para realizar junção por abrangência, aos vizinhos mais próximos e por proximidade nas estruturas de região de cobertura e de particionamento disjunto.

Os principais resultados deste trabalho foram, um novo conceito para particionar o espaço métrico extensivelmente, baseando em distância entre os objetos mapeadores que dividem o espaço métrico em regiões disjuntas.

Uma estrutura baseada em funções de mapeamento local onde cada bucket da estrutura particiona localmente o espaço métrico, chamada Partição Local.

Essa estrutura é a que realiza menos acessos a disco e assim, é recomendável para conjuntos de dados em que a função de distância não é custosa.

Uma extensão na estrutura Partição Local que armazena nos seus nós folhas a distância para o primeiro mapeador do bucket do nível acima, chamada Partição Local+.

Essa estrutura realiza menos cálculos de distância, sendo recomendável para conjuntos de dados em que a função de distância é custosa.

Uma melhora no custo de execução da consulta por abrangência usando estrutura baseada em particionamento sob as estruturas em região de cobertura, o que resultou em um ganho no conjunto de pontos geográficos, para blocos de 51bytes, de até 5,07 vezes menos cálculos de distância.


Acessos a disco e 185% menos tempo de execução.

Uma melhora no custo de execução da consulta dos vizinhos mais próximos usando estrutura baseada em particionamento sob as estruturas em região de cobertura, o que resultou em um ganho no conjunto de pontos geográficos, para blocos de 51bytes, de até 18,79 vezes menos cálculos de distância.

E 43,29 vezes menos acessos a disco.

E 177% menos tempo de execução.

As estruturas baseadas em particionamento, apresentaram melhor escalabilidade para a realização das consultas por abrangência e aos vizinhos mais próximos do que as estruturas baseadas em região de cobertura.

Isso acontece pois, a medida em que se aumenta a quantidade de objetos nas estruturas baseadas em região de cobertura aumenta o custo de realizar consultas de abrangência e de vizinhos mais próximos, comparado com as estruturas de particionamento.

Os algoritmos de junção por abrangência, junção aos vizinhos mais próximos e junção por proximidade nas estruturas de região de cobertura e nas estruturas de particionamento.

Os algoritmos de junção por abrangência apresentaram pouca diferença em tempo entre as estruturas de particionamento e as estruturas de região de cobertura.

Os algoritmos de junção aos vizinhos mais próximos e por proximidade apresentaram melhor desempenho nas estruturas baseadas em região de cobertura do que as estruturas de particionamento.

A técnica para manter em buffer todas as sub-páginas da estrutura do lado direito da junção de uma página que foi visitada que reduz o número de acessos a disco, mas consome mais memória.

Uma técnica que usa uma árvore AVL para a lista de resposta que miminiza o custo da lista global de resposta para as consultas por similaridade, vizinhos mais próximos e consulta por abrangência com ordenação das distâncias da resposta.

A árvore AVL foi aplicada nos algoritmos de junção por similaridade uma melhora no custo de execução da junção por abrangência usando a junção hierárquica nas estruturas Slim-Tree, Partição Local e Partição Local+ sob as estruturas seqüenciais.

Essa junção consumiu, 0.

E 26% do número de acessos a disco na junção por abrangência da seqüencial.

E 014% do número de cálculos de distância na junção por abrangência da seqüencial.

Duas formas de escolha do novo mapeador para a função de mapeamento local nas estruturas de particionamento do espaço métrico, além do randômico, baseadas na maior variância do valor da coordenada métrica e na maior contagem do valor da coordenada métrica diferentes.

Essas técnicas não apresentaram ganhos constantes e por isso esse trabalho recomenda a técnica de escolha randômica, pois torna mais rápida a inserção dos objeto na estrutura.

Os resultados desse trabalho abrem pesquisas para, Criação de uma estrutura de particionamento que usa uma única função de mapeamento global e extensível, ou seja, uma estrutura de hashing.

Priorização de sub-páginas para os algoritmos de junção aos vizinhos mais próximos e por proximidade.

Uma escolha melhor pode minimizar bastante o raio de busca nesses algoritmos e diminuir a quantidade de acessos a disco e cálculos de distância.

Suporte às atuais estruturas métricas para o armazenamento de objetos sem que seja necessário aumentar o tamanho de bloco.

Um exemplo são as proteínas que podem variar de poucos aminoácidos até mais que 20 mil aminoácidos.

Para indexar proteínas desse tamanho as atuais estruturas devem usar blocos superiores a 6kbytes, que são maiores do que os que têm suporte aos atuais sistemas de arquivos.

Indexar maior quantidade de dados e conseguir menos tempo de resposta para as consultas, distribuindo as estruturas em várias máquinas para criar uma estrutura de grid.

Suporte nas estruturas de particionamento local para resolver consultas usando os operadores lógicos, E, OU e NÃO.

Explorar novas formas de escolha dos mapeadores que melhorem o espalhamento dos valores de coordenadas métricas e que não geram colisões.

Explorar formas para restringir as sub-páginas que serão navegadas pelos algoritmos de junção por vizinhos mais próximos e por proximidade na técnica de junção hierárquica.
Novas formas de otimização da junção quando são utilizadas em expressões de consulta encadeando tanto com operadores lógicos quanto operadores de seleção.

