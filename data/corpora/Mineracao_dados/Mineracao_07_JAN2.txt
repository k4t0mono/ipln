A quantidade de dados submetida às aplicações de mineração de dados tem crescido consideravelmente como conseqüência indireta das reduções dos custos de coleta, transmissão e armazenamento de dados.

Portanto, as aplicações de mineração de dados devem ser escaláveis, isto é, as perdas em desempenho devem ser pequenas com o aumento do tamanho da entrada.

A mineração de conjuntos freqüentes é uma aplicação popular de mineração de dados para a qual há diversos algoritmos e implementações.

O EClaT está entre os algoritmos mais bem-sucedidos e conhecidos.

Seu tipo abstrato de dados que mais consome memória é o conjunto de números naturais.

Nesse trabalho, substituímos a implementação desse tipo abstrato de dados por outra, comumente empregada por algoritmos de recuperação de informação mas nunca antes empregada por algoritmos de mineração de dados, que economiza memória.

Também adaptamos para o novo contexto e/ou implementamos outras estratégias de economia de memória.

Obtivemos economia do consumo máximo de memória de até uma ordem de magnitude em relação à implementação original.

O conhecimento é um elemento fundamental para a tomada de decisões racionais.

Tanto a quantidade quanto a variedade de dados disponíveis para a sua aquisição podem ser enormes, dificultando ou impossibilitando análise humana não-assistida.

Knowledge Discovery in Databases, KDD, ou Descoberta de Conhecimento em Bancos de Dados, é o processo não-trivial de identificação de padrões válidos, novos, potencialmente úteis e, finalmente, inteligíveis.

O processo de KDD é interativo e iterativo, com várias decisões tomadas pelo usuário seguir.

Entendimento do domínio de aplicação e do conhecimento prévio relevante e identificação do objetivo do processo do ponto de vista do destinatário

Criação de um conjunto de dados alvo, selecionando um conjunto de dados ou focalizando em um subconjunto de variáveis ou amostras, sobre o qual a descoberta será realizada

Limpeza e pré-processamento dos dados, o que inclui a remoção de ruído, o tratamento de valores ausentes e a consideração de variações temporais

Redução e projeção dos dados, que consiste em encontrar características úteis para representar os dados dependendo do objetivo da tarefa.

Com redução de dimensionalidade ou métodos de transformação, o número efetivo de variáveis sob consideração pode ser reduzido, ou representações invariantes para os dados podem ser encontradas

Casamento dos objetivos do processo de KDD (primeiro passo) a um método particular de mineração de dados, por exemplo, sumarização, classificação, regressão ou agrupamento

Análise exploratória e seleção de modelos e hipóteses, que envolve a escolha dos algoritmos de mineração de dados e métodos de seleção a serem utilizados para a busca por padrões.

Este processo inclui a decisão de quais modelos e parâmetros poderiam ser apropriados e o casamento de um método particular de mineração de dados ao critério global do processo de KDD

Mineração de dados, a busca por padrões de interesse em uma forma representacional particular ou um conjunto de tais representações, incluindo regras, árvores de classificação, regressão ou agrupamento.

O usuário provê dados adequados ao método de mineração de dados realizando corretamente os passos precedentes

Interpretação dos padrões minerados, possivelmente retornando a qualquer dos passos 1 a 7 para iteração adicional.

Este passo pode também envolver visualização dos dados ou dos padrões e modelos extraídos

Atuação sobre o conhecimento descoberto, utilizando-o diretamente, incorporando-o a outro sistema para ação adicional, documentando-o ou reportando-o às partes interessadas.

Este passo também inclui o confronto e a resolução de potenciais conflitos com conhecimento previamente aceito ou extraído.

A entrada para os problemas de mineração de conjuntos freqüentes e de mineração de regras de associação é um multiconjunto de conjuntos de entidades.

Um conjunto freqüente é um conjunto de entidades cujo suporte, freqüência relativa que o conjunto é subconjunto de um elemento pertencente à entrada, é maior ou igual ao parâmetro de limite inferior de suporte, definido pelo usuário.

Uma regra de associação é uma implicação entre dois conjuntos de entidades, denominados antecedente e conseqüente, onde a união entre antecedente e conseqüente é um conjunto freqüente e a confiança, freqüência relativa que o conseqüente é subconjunto de um superconjunto do antecedente pertencente à entrada, é maior ou igual ao parâmetro de limite inferior de confiança.

A definição formal do problema e um exemplo simples serão apresentados.

O problema de mineração de regras de associação pode ser decomposto em dois subproblemas, o primeiro, a mineração de conjuntos freqüentes, e o segundo, relativamente simples, o cálculo das regras de associação em função da solução do primeiro sub-problema.

Fica assim estabelecida a importância dos conjuntos freqüentes ao processo de KDD e à mineração de regras de associação.

Os algoritmos de mineração de conjuntos freqüentes enfrentam problemas de desempenho e escalabilidade porque são intensivos em acessos a dispositivos de entrada e saída, armazenamento e processamento e a quantidade de informações resultante é exponencial em função do parâmetro de entrada.

O problema de escalabilidade dos algoritmos de mineração de conjuntos freqüentes é muito grave.

O consumo de memória por esses algoritmos eventualmente ultrapassa a capacidade de memória primária e o sistema operacional é obrigado a recorrer à memória secundária.

Acessos de pouca localidade de referência espacial à memória secundária são ordens de grandeza mais lentos que à memória primária.

Quando nem a memória secundária é suficiente o sistema operacional é obrigado a abortar o processo.

O consumo de memória por algoritmos de mineração de conjuntos freqüentes cresce invariavelmente à medida em que o limite inferior de suporte decresce.

O crescimento é sempre observável, mesmo que os valores absolutos do consumo sejam menores ou maiores dependendo de características das entradas, e quase sempre exponencial.

O gráfico exibe o consumo de memória máximo de uma implementação de algoritmo de mineração de conjuntos freqüentes em função do limite inferior de suporte para entradas de diferentes características.

Não apresentamos as curvas completas para todas bases de dados por termos limitado o tempo de execução dos processos.

Observamos que ao reduzir o limite inferior de suporte, o consumo máximo de memória para algumas bases de dados cresce rapidamente, comprometendo a escalabilidade quando o valor for maior que a quantidade de memória primária disponível.

Consumo de memória máximo limite inferior de suporte O objetivo deste trabalho é reduzir o consumo de memória sem afetar significativamente o desempenho de algoritmos de mineração de conjuntos freqüentes, pela aplicação inédita de uma técnica de compressão de conjuntos totalmente ordenados finitos de números inteiros positivos à principal estrutura de dados desses algoritmos.

A metodologia de desenvolvimento do trabalho envolveu a revisão bibliográfica das estratégias de redução de memória empregadas pelos algoritmos de mineração de conjuntos freqüentes e das técnicas de compressão aplicáveis a conjuntos totalmente ordenados finitos de números inteiros positivos.

A definição de algoritmos de mineração de conjuntos freqüentes em função de operadores sobre conjuntos totalmente ordenados finitos de números inteiros positivos, a implementação e a análise empírica de algoritmos de mineração de conjuntos freqüentes.

As implementações de algoritmos de mineração de conjuntos freqüentes baseados no EClaT o contêm.

Usualmente, esses conjuntos são representados por um vetor ordenado de números inteiros positivos.

Apresentamos nesse trabalho uma representação alternativa para esses conjuntos.

Várias outras representações alternativas já foram propostas para atenuar o problema de escalabilidade dos algoritmos de mineração de conjuntos freqüentes.

As mais bem-sucedidas são os bitmaps e os diffsets sentação, os d-gaps para a indexação de documentos.

Um bitmap é uma representação de um conjunto por uma seqüência de valores booleanos na qual cada valor representa a pertinência de um determinado elemento ao conjunto.

Os bitmaps são utilizados pelos algoritmos de mineração de conjuntos freqüentes DCI, implementado por Borgelt relacionado, MAFIA quando as ausências são muito mais freqüentes que as presenças de elementos no conjunto.

Um diffset representa um conjunto pela diferença a um superconjunto de referência e a cardinalidade da diferença normalmente é menor que a cardinalidade do conjunto.

A ausência de elementos é desnecessariamente representada pelos bitmaps, mesmo comprimidos por skinning, e nenhuma técnica de compressão é aplicada aos diffsets.

Um d-gap representa um conjunto totalmente ordenado finito de números inteiros positivos por uma seqüência de diferenças entre elementos consecutivos codificadas por um código para o qual números menores ocupam menos memória.

Normalmente, as diferenças são muito menores que os próprios elementos e os d-gaps economizam memória.

Alguns refinamentos relacionados ao consumo de memória ou ao desempenho dos algoritmos de mineração de conjuntos freqüentes são o curto-circuito, empregado pelo algoritmo EClaT e pelo algoritmo FP-growth ração de conjuntos maximais, MAFIA.

Os trabalhos a serem utilizados para a solução proposta são detalhados.

As contribuições desse trabalho são, Aplicação inédita à mineração de conjuntos freqüentes de técnica de compressão baseada em codificação de diferenças adjacentes de elementos de conjuntos totalmente ordenados

Revisão bibliográfica das estratégias de redução de consumo de memória aplicáveis à mineração de conjuntos freqüentes

Definição dos operadores sobre conjuntos necessários a algoritmos de mineração de conjuntos freqüentes

Implementação e análise empírica do algoritmo zEClaT e dos refinamentos de curtocircuito, diffsets, projeção e reordenação dinâmica.

Evidência de que houve redução do consumo de memória em relação a algoritmos existentes de até uma ordem de magnitude.

Apresentamos formalmente o problema de mineração de conjuntos freqüentes, os operadores sobre conjuntos e o algoritmo EClaT, que os utiliza.

Algumas estratégias de redução de consumo de memória aplicáveis à mineração de conjuntos freqüentes são abordadas.

A adaptação do algoritmo EClaT, denominada zEClaT, os experimentos e a análise dos resultados são os assuntos.

O problema de mineração de conjuntos freqüentes consiste em calcular, a partir da entrada D, um arquivo que representa um multiconjunto de conjuntos de entidades, e do parâmetro inf, os conjuntos de entidades cujos suportes, probabilidade de que o conjunto de entidades seja subconjunto de um elemento pertencente a D, são maiores ou iguais a e os respectivos suportes.

A mineração de conjuntos freqüentes é simultaneamente um problema de mineração de dados e uma etapa do problema de mineração de regras de associação.

A instância do problema na qual a entrada representa os cupons de vendas de um supermercado é denominada análise de cestas de compra.

Os conjuntos de produtos vendidos juntos freqüentemente pelo supermercado e a freqüência relativa da venda dos conjuntos de produtos são a solução dessa instância.

Exemplos de pares ordenados de conjunto freqüente e suporte para a análise de cestas de compra são (ftomateg, 1%), (fcebolag, 0, 8%) e (ftomate, cebolag, 0, 5%).

O conhecimento desses pares ordenados ajudaria o gerente do departamento de hortifrutigranjeiros do supermercado a decidir, de acordo com seu objetivo, por exemplo, aumentar os preçãos das cebolas e dos tomates, iniciar uma promoção das cebolas, finalizar uma promoção dos tomates ou aproximar ou distanciar as gôndolas das cebolas e dos tomates.

Outras instâncias do problema incluem a análise de compras com cartão de crédito, a análise de padrões de chamadas telefônicas, a identificação de solicitações fraudulentas de seguro médico e a análise de compras de serviços de telecomunicações.

Nesse capítulo apresentamos o problema de mineração de conjuntos freqüentes, a terminologia e os conceitos matemáticos envolvidos, os operadores sobre conjuntos empregados pelos algoritmos de mineração de conjuntos freqüentes e seus refinamentos, e o algoritmo de mineração de conjuntos freqüentes EClaT.

Nessa seção apresentamos a definição formal do problema de mineração de conjuntos freqüentes e introduzimos a terminologia e os conceitos auxiliares às descrições do problema, dos algoritmos e dos refinamentos.

Sejam a entrada D, um multiconjunto de conjuntos de entidades, e o parâmetro inf, um.

O objetivo da mineração de conjuntos freqüentes é calcular o conjunto F, expresso abaixo.

Os elementos pertencentes a D são chamados transações.

Os elementos pertencentes às transações são chamados itens.

O conjunto de itens que pertencem ao menos a uma transação é expresso por I = ceito 220 é expresso por P(I).

Quando o item i pertence à transação T, dizemos que o item ST D T e o conjunto de subconjuntos de I, o conjunto potência de I (Coni ocorre na transação T).

Um conjunto de itens é um itemset.

Quando o itemset I é subconjunto da transação T, dizemos que o itemset I ocorre na transação T.

Um itemset de cardinalidade k é um k-itemset.

A quantidade de transações nas quais o itemset I ocorre, a freqüência do itemset I em D, é expressa e a freqüência relativa do itemset I em D, o suporte de I, é expresso.

F é sintetizado pela expressão abaixo.

O identificador de uma transação é o tid da mesma.

O conjunto totalmente ordenado de tids das transações nas quais o item i ocorre é o tidset de i, simbolizado por L(i).

O conjunto totalmente ordenado de tids das transações nas quais o itemset I ocorre é o tidset de I, simbolizado por L(I).

O tidset do itemset I é igual à interseção dos tidsets dos itens pertencentes a I.

Multiconjunto de transações.

Considere o multiconjunto de transações.

Os itemsets freqüentes para o limite inferior de suporte 50% são exibidos.

Apresentamos agora diversos conceitos.

Uma relação é uma ordem parcial sobre um conjunto S se são válidas as propriedades de reflexividade, anti-simetria e transitividade.

Conjunto Parcialmente Ordenado, Um conjunto parcialmente ordenado é um par ordenado, onde S é um conjunto e é a ordem parcial de P.

O par ordenado, onde é o conjunto de números inteiros positivos e j é a relação de divisibilidade, é um conjunto parcialmente ordenado.

Ordem Total Uma relação, é uma ordem total sobre um conjunto S se ela é uma ordem parcial e é válida a propriedade.

Conjunto Totalmente Ordenado, Um conjunto totalmente ordenado é um par ordenado, onde S é um conjunto e é a ordem total de P.

O par ordenado, onde é conjunto de números inteiros positivos é a relação menor que, é um conjunto totalmente ordenado.

Dois conjuntos totalmente ordenados são isomórficos se, e somente se, existe uma bijeção.

Em outras palavras, as cardinalidades dos conjuntos são iguais e existe um mapeamento entre eles que preserva a ordem.

Quaisquer dois conjuntos totalmente ordenados finitos de cardinalidades iguais são isomórficos.

Relação de Equivalência, Uma relação de equivalência sobre um conjunto S é um subconjunto de S S, isto é, uma coleção de pares ordenados de elementos de S, satisfazendo as propriedades reflexiva (s1 s, 8 s1 S), simétrica (s1 s) s s1, 8 s1 S sS) e transitiva Uma classe de equivalência é definida como um subconjunto da forma fs1, s1 S s sg.

A notação s1 ssignifica que existe uma relação de equivalência entre s1 e s2.

Pode ser mostrado que quaisquer duas classes de equivalência são ou iguais ou disjuntas, então a coleção de classes de equivalência forma uma partição de S.

Os limites inferiores são os divisores comuns e os limites superiores são os múltiplos comuns.

Encontro e junção, Sejam (P,) um conjunto parcialmente ordenado e S um subconjunto de P.

A junção de S, simbolizada por b S, é o menor limite superior de S e o encontro de S, simbolizado por c S, é o maior limite inferior de S.

O encontro é o máximo divisor comum e a junção é o mínimo múltiplo comum.

Semitreliças encontro e junção.

Seja um conjunto parcialmente ordenado.

P é uma semitreliça encontro se existe o encontro de qualquer par de elementos pertencentes a P.

P é uma semitreliça junção se existe a junção de qualquer par de elementos pertencentes.

Treliça.
Seja um conjunto parcialmente ordenado.

P é uma treliça se P é simultaneamente semitreliça encontro e semitreliça junção.

Seja (P,) um conjunto parcialmente ordenado.

P é uma treliça completa se existem o encontro e a junção de qualquer subconjunto.

Sejam (P,) um conjunto parcialmente ordenadoe S um subconjunto próprio de P.

S é uma subtreliça de P se pertencem a S o encontro e a junção de qualquer par de elementos pertencentes.

Treliça distributiva, Uma treliça P é distributiva.

Seja P uma treliça com um elemento fundo.

Treliça booleana, Uma treliça P é uma treliça booleana.

Para o conjunto S, qualquer P P(S) é uma treliça de conjuntos se é fechado sob finitas uniões e interseções, isto é, é uma treliça com a ordem parcial especificada pela relação de subconjunto, conjunto potência P (S) do conjunto S é o conjunto de todos os subconjuntos de S.

Seja P um conjunto.

O conjunto ordenado e o conjunto potência são uma treliça completa para a qual junção e encontro são união e interseção, respectivamente.

Os diversos conceitos definidos nessa seção são subsídios para a apresentação do algoritmo EClaT e o algoritmo zEClaT.

Nessa seção são apresentados os operadores de cardinalidade, diferença, diferença simétrica, interseção e união de conjuntos.

O algoritmo zEClaT será descrito em função dos operadores de cardinalidade, diferença e interseção.

A cardinalidade e a interseção são empregadas pelo algoritmo EClaT e a cardinalidade e a diferença são empregadas pelo algoritmo dEClaT.

Nenhum algoritmo de mineração de conjuntos freqüentes conhecido emprega a diferença simétrica ou a união, que são apresentadas apenas para evitar perda de generalidade.

Apresentamos uma definição intuitiva, a simbologia, uma definição formal em função da pertinência de elemento a conjunto, exemplos, a expressão análoga da álgebra booleana e o diagrama de Venn para cada operador.

Cardinalidade, A cardinalidade do conjunto, a quantidade de elementos pertencentes, é simbolizada e definida. Análoga da álgebra booleana e o diagrama de Venn não se aplicam.

Diferença, A diferença entre os conjuntos, o conjunto de elementos pertencentes e não pertencentes, é simbolizada e definida.

A expressão análoga da álgebra booleana é A B e o diagrama de Venn é ilustrado.

Diferença Simétrica.

A expressão análoga da álgebra booleana e o diagrama de Venn.

A expressão análoga da álgebra booleana é A B e o diagrama de Venn é ilustrado.

União.

Intersecção.

A expressão análoga da álgebra booleana e o diagrama de Venn é ilustrado.

Os operadores sobre conjuntos são os operadores fundamentais dos algoritmos de mineração de conjuntos freqüentes.

Eles são responsáveis por grande parte do tempo de execução.

Portanto, sua implementação eficiente é essencial para a rapidez dos algoritmos.

A complexidade do operador de cardinalidade é O(1) quando um inteiro não-negativo adicional, que representa a cardinalidade do conjunto, é atualizado após a adição de elementos ao conjunto e após a remoção de elementos do conjunto.

A complexidade dos operadores de diferença, diferença simétrica, interseção e união, quando os conjuntos são totalmente ordenados.

O Algoritmo 21 detalha a implementação original dos operadores de diferença, diferença simétrica, interseção e união sobre conjuntos totalmente ordenados.

O algoritmo itera sobre os elementos de A e B segundo a ordem total e adiciona, conforme o operador, os elementos não pertencentes a A e pertencentes a B, os elementos pertencentes a A e não pertencentes a B e os elementos pertencentes a A e pertencentes a B ao resultado, preservando a ordem.

Operadores de diferença, diferença simétrica, interseção e união sobre conjuntos totalmente ordenados.

Mostra a treliça potência para o banco de dados da limite inferior de suporte de 50%.

Um vértice representa um itemset.

O primeiro conjunto representa os itens do itemset e o segundo conjunto representa o tidset do itemset.

Vértices preenchidos representam itemsets freqüentes.

As arestas representam a relação de cobertura subconjunto.

Para ilustrar a descrição, considere o itemset fAg.

O tidset de fAg é freqüente e fAg é subconjunto. Observe que o conjunto de todos os itemsets freqüentes forma uma treliça encontro porque é fechado sob a operação de encontro.

Todos subconjuntos de um itemset freqüente são freqüentes.

O lema acima é uma conseqüência do fechamento sob a operação de encontro para o conjunto de itemsets freqüentes.

Como um corolário, nós temos que todo superconjunto de um itemset infreqüente é infreqüente.

Essa observação forma a base de uma estratégia de poda bem poderosa em uma busca bottom-up por itemsets freqüentes, que foi aproveitada em muitos algoritmos de mineração de dados.

Nominalmente, somente os itemsets freqüentes do nível anterior precisam ser estendidos como candidatos para o nível corrente.

Entretanto, a formulação de treliça deixa aparente que não precisamos nos restringir a busca simplesmente bottom-up, ou seja, que são igualmente viáveis buscas top-down, que parte do itemset maximal e identifica itemsets infreqüentes, e híbridas, que é alternam bottom-up com top-down.

Começamos notando que a treliça potência no conjunto de itens é uma treliça booleana.

O conjunto de átomos da treliça potência corresponde ao conjunto de itens.

Associamos a cada átomo (item) X sua tidset, denotada L(X), que é a lista de todos os identificadores de transação que contém o átomo.

Em outras palavras cada elemento de uma treliça booleana é dado como uma junção de um subconjunto do conjunto de átomos.

Como a treliça potência é uma treliça booleana, com a operação junção correspondendo à união de conjuntos, nós temos.

O lema acima diz que se um itemset é dado como a união de um conjunto de itens em J, então seu suporte é dado como a interseção das tidsets dos elementos em J.

Em particular nós podemos determinar o suporte de qualquer k-itemset por simples interseção das tidsets de quaisquer dois de seus subconjuntos de tamanho (k 1).

A verificação da cardinalidade da tidset resultante nos diz se o novo itemset é freqüente ou não.

Ela mostra o banco de dados inicial com a tidset para cada item (isto é, átomos).

A tidset intermediária é obtida pela interseção das listas.

Assim, somente dois subconjuntos no nível anterior são necessários para computar o suporte de um itemset em qualquer nível.

Uma conseqüência importante e prática do lema acima é que cardinalidades de tidsets intermediárias encolhem à medida em que movemos para cima na treliça.

Isso resulta em interseções e contagem de suporte mais rápidos.

Se tivéssemos memória suficiente poderíamos enumerar todos os itemsets freqüentes caminhando pela treliça potência, e realizando interseções para obter o suporte dos itemsets.

Na prática, entretanto, temos somente uma quantidade limitada de memória primária, e todas as tidsets intermediárias não caberiam na memória.

Isto levanta uma questão natural, podemos decompor a treliça original em peças menores tal que cada porção possa ser resolvida independentemente na memória principal?

Endereçamos essa pergunta abaixo.




Nós chamamos k, uma relação de equivalência baseada em prefixo.

Uma característica interessante das classes de equivalência é que os elos entre classes denotam dependências.

Isso quer dizer, se quisermos podar um itemset se existir pelo menos um subconjunto infreqüente, então temos que processar as classes em uma ordem específica.

Em particular nós temos que processar as classes do fundo para o topo, que corresponde a uma ordem lexicográfica reversa, isto é, processamos, seguido por e finalmente Na prática a decomposição induzida por é suficiente.

Entretanto, em alguns casos, uma classe pode ser muito grande para ser solucionada em memória principal.

Nesse cenário, aplicamos a decomposição recursiva.

Assumamos que principal.

Como antes, cada classe pode ser resolvida independentemente, e nós podemos resolvê-las em ordem lexicográfica reversa para habilitar poda de subconjunto.

Como antes, os elos mostram as dependências de poda que existem entre as classes.

Dependendo da quantidade de memória principal disponível nós podemos particionar recursivamente classes grandes em classes menores, até que cada classe seja pequena o suficiente para ser resolvida independentemente em memória principal.

A busca bottom-up é baseada na decomposição recursiva de cada classe em classes menores induzidas pela relação de equivalência.

A treliça de classes de equivalência é atravessada em profundidade.

Isto é, primeiro é processada a classe e suas derivadas e depois são processadas as classes.

Para calcular o suporte de qualquer itemset, simplesmente realizamos a interseção dos tidsets de dois dos subconjuntos do nível anterior.

Uma versão não-recursiva do EClaT é detalhada pelo Algoritmo.

Os parâmetros de entrada são um, o conjunto de itemsets, e inf, o inteiro positivo dinf jDje, e a saída é F, o conjunto de itemsets freqüentes.

Os itemsets freqüentes são minerados nas linhas a e os demais itemsets freqüentes são minerados nas linhas 5 a 22.

A mineração dos itemsets freqüentes é trivial.

Quando a freqüência de um itemset pertencente a um é maior que inf, ele é adicionado a F na linha 4.

A mineração dos demais itemsets é iterativa.

As classes de equivalência corrente e anterior são, respectivamente, as classes de equivalência ao topo e imediatamente inferior ao topo da pilha de classes de equivalência a processar, denominada classes.

A classe de equivalência representada pelo conjunto vazio é empilhada na linha 5 e a cada iteração o maior itemset da classe de equivalência corrente é processado nas linhas 6 a 22.

Quando mais de um itemset freqüente pertence à classe de equivalência corrente, os itemsets correspondentes às junções entre o maior e os demais itemsets pertencentes à classe de equivalência corrente são calculados e eles são adicionados a F quando suas freqüências são maiores que inf nas linhas 10 a 14.

Quando apenas um itemset freqüente pertence à classe de equivalência corrente, a classe de equivalência corrente é desempilhada e o maior itemset é removido da classe de equivalência anterior nas linhas 20 a 22.

Quando ao menos um dos itemsets calculados é freqüente, a classe de equivalência.

Quando não é calculado nenhum itemset freqüente, o maior itemset é removido da classe de equivalência corrente na linha.

Um exemplo passo a passo da execução do algoritmo EClaT é mostrado.

O próximo capítulo apresenta as técnicas de redução de consumo de memória aplicadas ao algoritmo zEClaT, ou às demais especializações do algoritmo EClaT.

Nesse capítulo apresentamos algumas estratégias de redução de consumo de memória aplicáveis aos algoritmos de mineração de conjuntos freqüentes.

Nominalmente, bitmaps, curto-circuito, diffsets, projeção, reordenação dinâmica, skinning e d-gaps.

Algumas dessas estratégias já foram empregadas por algoritmos de mineração de conjuntos freqüentes e outras apenas por outros algoritmos.

Os bitmaps são um refinamento que procura reduzir o consumo de memória representando os tidsets não por vetores de números inteiros positivos mas por vetores de dígitos binários.

Um bitmap a ausência ou presença de um número é sinalizada por um dígito binário falso ou verdadeiro na posição correspondente a esse número.

A representação esparsa de conjunto demanda um inteiro para cada número pertencente ao conjunto.

Nas máquinas atuais, isso corresponde a 3bits.

Quando a razão entre dígitos binários verdadeiros e totais é maior que o inverso do comprimento do inteiro, a representação bitmap ocupa menos que a representação esparsa.

Por exemplo, o conjunto demanda 3bits em bitmap e 55 bits na representação esparsa.

As operações entre conjuntos são realizadas em bitmap palavra por palavra.

Cada uma dessas operações palavra por palavra substitui uma quantidade de operações igual ao comprimento da palavra em relação à representação tradicional.

Isso significa que essas operações são mais rápidas que as operações na representação tradicional em proporção igual a essa redução.

Os bitmaps são alinhados e as operações bit a bit são executadas sobre os bitmaps.

A correspondência entre conjuntos e álgebra booleana permite que isso seja feito.

Assim, qualquer operação deve ser seguida pela interseção com o conjunto universo.

A cardinalidade pode ser calculada rapidamente por uma tabela pré-computada da quantidade de dígitos verdadeiros para cada divisão da palavra.

Se a palavra tem 3bits e a its, a contagem dos elementos é potencialmente 16 vezes mais rápida.

Os bitmaps são aplicados nos algoritmos de mineração de conjuntos freqüentes EClaT implementado por Borgelt e no algoritmo de mineração de conjuntos maximais GenMax duas abordagens, de bitmaps e de d-gaps, para o propósito de representação de conjuntos de números naturais, o foco desse trabalho é apenas a abordagem de d-gaps.

O curto-circuito é um refinamento aplicado pelo algoritmo EClaT dor entre conjuntos quando antecipa que a cardinalidade do resultado será menor que um limite inferior ou maior que um limite superior.

O diffset ou tidset de um itemset é relevante somente se esse itemset é freqüente.

A informação de infreqüência de um itemset é suficiente para que ele seja podado.

A interseção entre os tidsets de dois itemsets é relevante quando a cardinalidade é maior ou igual ao suporte mínimo e a diferença entre os diffsets de dois itemsets é relevante quando a cardinalidade é menor que a diferença entre o suporte do itemset correspondente ao prefixo da classe de equivalência e o suporte mínimo.

Quanto antes a infreqüência de um itemset for descoberta melhor, porque poupa processamento e armazenamento desnecessários.

A verificação das condições de curto-circuito é relativamente simples em relação à recuperação de um elemento de um conjunto e ao armazenamento de um elemento a um conjunto, que podem incluir codificação e decodificação de elementos e ampliação de conjunto.

A implementação dos operadores sobre conjuntos totalmente ordenados refinada por operadores de diferença, diferença simétrica, interseção e união sobre conjuntos totalmente ordenados, refinados por curto-circuito.

Condições de curto-circuito para as operações.

O curto-circuito é um refinamento incorporado pelo algoritmo zEClaT.

Os diffsets são um refinamento que procura reduzir o consumo de memória representando um conjunto não pelos seus elementos mas pelos elementos da diferença entre esse conjunto e um determinado superconjunto muito parecido.

Os diffsets reduzem dramaticamente o consumo de memória para armazenar resultados intermediários e aumentam significativamente o desempenho.

O armazenamento do tidset de cada membro de uma classe é evitado pelos diffsets.

Ao invés disso, eles armazenam apenas as diferenças dos tids entre cada membro da classe e o itemset do prefixo.

Essas diferenças dos tids são armazenadas nos diffsets, que são as diferenças entre dois tidsets (nominalmente, o tidset prefixo e o tidset do membro da classe), e são propagadas pelo caminho de um nó para seus filhos começando pela raiz.

Os membros do nó raiz podem usar tidsets ou diferenças do prefixo vazio (que por definição aparece em todos os tids).

Em métodos verticais normais temos disponível para uma dada classe o tidset para o prefixo assim como os tidsets de todos membros da classe são dois membros quaisquer.

A primeira coisa a notar é que o suporte de um itemset não é mais a cardinalidade do diffset mas deve ser armazenada separadamente. Usamos diffsets recursivamente como mencionado acima, isto é, fácil de consertar, porque ao invés de computar computamos como a diferença dos diffsets tids relevantes.

Podemos escolher começar com o conjunto original de tidsets para os itens freqüentes, ou poderíamos converter da representação tidset para representação diffset no início.

Podemos observar claramente que, para bases de dados densas como a mostrada, uma grande redução no tamanho do banco de dados é alcançada usando esta transformação.

As representações dos itemsets consomem 2elementos na abordagem baseada em tidsets, enquanto que somente 7 elementos na abordagem baseada em diffsets (vezes melhor).

Se olharmos para o tamanho dos resultados, descobriremos que a abordagem baseada em tidset leva 76 tids no total, enquanto a abordagem diffset (com dados iniciais diffset) armazena apenas 2tids.

Se compararmos por comprimento, descobriremos que o tamanho médio de tidset para os itemsets freqüentes é 3,8, enquanto o tamanho do diffset é 1.

Para os itemsets o tamanho do tidset é 3,mas o tamanho médio do diffset é 0,6.

Finalmente, para tidsets o tamanho do tidset é e o tamanho do diffset é 0!

O fato que a representação inicial é menor e os diffsets encolhem quando os itemsets mais longos são encontrados.

Isso permite que métodos baseados em diffsets sejam extremamente escaláveis, e ordens de magnitude melhores que outras abordagens de mineração de associações existentes.

Os diffsets são empregados pelo algoritmo dEClaT próximo capítulo.

A projeção é um refinamento aplicado no algoritmo FP-Growth juntos freqüentes, e no algoritmo MAFIA não aplicado no algoritmo EClaT por não apresentar nenhuma vantagem para esse algoritmo, que procura reduzir o consumo de memória trabalhando apenas sobre transações relevantes ao contexto.

As transações para as quais o prefixo de uma classe de equivalência não ocorre são irrelevantes para essa classe de equivalência.

As transações restantes são renumeradas para que sua representação codificada seja menor.

Os identificadores associados a cada transação são irrelevantes contanto que as transações relevantes a um contexto sejam identificadas univocamente.

As transações relevantes a uma classe de equivalência mas essas transações são conhecidas pelos algoritmos somente após a realização de operações.

A aplicação de qualquer função injetora cujo domínio é um superconjunto atende aos requisitos.

Tomando como exemplo a base de dados apresentada, o conjunto das transações que contêm o itemset fAg, L (fAg), é f1, 3, 4, 5 g.

Todas as transações que contêm itemsets da classe de equivalência las durante o processamento dessa classe de equivalência.

Aplicando uma função injetora de f1,3,4,5 g para f1,2,3,g que preserve a ordem, temos L(fA,Bg), originalmente igual a f1,3,4,5 g, agora igual a f1,2,3,g e L(fA,Cg), originalmente igual a f3,g, agora igual a f2,g.

A projeção pode ser aplicada a qualquer classe de equivalência, independente de sua aplicação a outras classe de equivalência.

Isso quer dizer que sua aplicação à classe de equivalência, transformando f3, g ou f2, g em f1, g.

A corretude da projeção é garantida pela propriedade que a aplicação da função antes ou depois do cálculo das operações produz o mesmo resultado, que tem a mesma cardinalidade que teria se a projeção não fosse empregada.

O Algoritmo detalha o refinamento de projeção.

O resultado do operador refinado é o resultado do operador original projetado em U, o conjunto universo.

Operadores de diferença, diferença simétrica, interseção e união sobre conjuntos totalmente ordenados, refinados por projeção.

O algoritmo zEClaT emprega a projeção renumerando os tids relevantes a uma classe de equivalência.

O zEClaT é detalhado no próximo capítulo.

A reordenação dinâmica é um refinamento, introduzido pelo algoritmo de mineração de conjuntos freqüentes maximais Max-Miner resultados intermediários alterando a ordem segundo a qual as classes de equivalência são processadas e as operações (de diferença ou interseção) são executadas.

O resultado de uma interseção independe da ordem segundo a qual as interseções intermediárias são realizadas porque a interseção é uma operação associativa ((A\\C = A\(B \), mas as cardinalidades das interseções intermediárias dependem dessa ordem e a velocidade de processamento da interseção é relacionada a essas cardinalidades.

Assim, a escolha adequada da ordem de realização das interseções promove maior velocidade de processamento sem invalidar o resultado.

Espera-se que, realizando primeiro as interseções entre os conjuntos de menor cardinalidade, a cardinalidade dos resultados intermediários e a velocidade de processamento sejam, respectivamente, menores e maiores que para alguma outra ordem.

A situação é análoga à multiplicação de matrizes.

O algoritmo EClaT original processa as classes de equivalência em ordem lexicográfica reversa.

O processamento das classes de equivalência segundo a ordem crescente de freqüência dos itemsets promove maior velocidade de processamento por alterar a ordem das interseções intermediárias da tidset de um itemset e a utilização mais racional da memória porque quando as maiores classes de equivalência forem processadas, as menores classes de equivalência já o foram e a memória por elas ocupada já foi liberada.

A reordenação dinâmica é um dos refinamentos aplicados ao algoritmo zEClaT, detalhado no próximo capítulo.

Skinning é uma técnica de compressão, ajustada à mineração de conjuntos freqüentes, para reduzir ainda mais o consumo de memória quando aplicada aos bitmaps.

À primeira vista, poderia parecer que o clássico, e simples de implementar, Run Length Encoding (RLE) seria a escolha apropriada para comprimir os vetores de bits.

Entretanto, esperamos que enquanto podem existir longas seqüências de 0's, seqüências de 1's que implicam uma seqüência de consumidores consecutivos comprando o mesmo item pode ser incomum em bancos de dados transacionais.

No pior caso, onde todos os 1's ocorrem de maneira isolada, o vetor RLE conteria duas palavras para cada ocorrência de um 1uma palavra para a seqüência precedente de 0's e uma para o próprio 1.

Isso significa que o tamanho do vetor resultante seria o dobro do original, que teria somente uma palavra associada a cada 1 (porque os 0's não são explicitamente representados).

Resumindo, resultaria em expansão e não em compressão.

Shenoy baseada no esquema de codificação Golomb clássico.

Seqüências de 0's e 1's são divididas em grupos de tamanho W e W, respectivamente os W's são referenciados por "pesos".

Cada grupo completo é representado no vetor codificado por um único bit "peso" igual a 1.

O último grupo parcial, de tamanho R mod W, onde R é o comprimento total da seqüênci é representado por um campo de contagem que armazena o equivalente binário do comprimento restante, expresso em log W bits, por razões explicadas abaixo, este campo é armazenado mesmo se o comprimento do último grupo parcial for zero.

Finalmente um bit 0 "separador de campo" é posicionado entre o último bit peso e o campo de contagem para indicar a transição do primeiro para o último.

Note que um "separador de seqüência" para distinguir entre uma seqüência de 0's e uma seqüência de 1's, não é requerido porque é implicitamente conhecido que o símbolo da seqüência muda depois do campo de contagem e o número de bits usado para o campo de contagem (log W) é fixo.

Para indicar os grupos associados a esta configuração de pesos.

Após o Skinning, o vetor comprimido resultante é de 25 bits (1)A 00 (11)B (1)C 0 (1)DE 0 (01) F(1) G 0 (1)H0 (01) (1)K 0L0 (01) M onde os superescritos alfabéticos indicam correspondência entre o grupo no vetor de bits original e sua versão codificada, e os bits 0 não classificados são os separadores de campos.

No exemplo acima, a compressão é somente de 30 para 25 bits, entretanto, para valores práticos de W e W, taxas de compressão muito maiores são obtidas por Skinning.

Na realidade, com escolha apropriada de W e W, Skinning resulta em perto de uma ordem de magnitude de compressão do formato original para os bancos de dados considerados nos experimentos.

Este alto grau de compressão é suficiente para assegurar que, embora a representação completa ocupe muito mais espaço que a esparsa, as representações obtidas através da técnica de Skinning são por volta de um terção do tamanho obtido com estes formatos.

A técnica de Skinning é aplicada pelo algoritmo VIPER porque o último não emprega os bitmaps.

O refinamento de d-gaps procura reduzir o consumo de memória através de uma representação alternativa de conjuntos de números inteiros positivos.

O conjunto a ser representado é ordenado e as diferenças entre os elementos consecutivos são representadas por um código para o qual números menores requerem menor espaço de armazenamento.

A redução de consumo de memória é alcançada quando os números menores são suficientemente mais freqüentes que os números maiores.

Apesar do uso de d-gaps ser amplamente difundido na área de recuperação de informação, apresentando excelentes resultados, não conhecemos nenhum trabalho que aplique essa técnica para mineração de conjuntos freqüentes, sendo essa nossa principal contribuição.

A ordem dos elementos na representação de um conjunto não importa, ou seja, o conjunto f1,20,15,27,5,7 g, representado pela seqüência h1,20,15,27,5,7 i, é igual ao conjunto totalmente ordenado (f1, 20, 15, 27, 5, 7 g,), representado pela seqüência ordenada h1, 5, 7, 15, 20, 27 i.

As operações sobre os conjuntos A e B, representados por seqüências ordenadas, podem ser implementadas em O (jAj + jBj) quanto ao processamento e Oquanto ao armazenamento transiente.

A representação de um conjunto totalmente ordenado pela seqüência ordenada correspondente requer que os elementos estejam ordenados.

Os tids são inseridos em ordem nos tidsets dos itens, portanto, tal requisito é atendido sem custo de processamento adicional.

Quando as seqüências ordenadas são processadas seqüencialmente a partir do primeiro elemento, elas podem ser representadas pelas diferenças adjacentes, chamadas de d-gaps, e recuperadas pelas somas parciais.

Apresentamos agora, formalmente, as relações entre os elementos p da seqüência ordenada e os elementos a da seqüência de d-gaps.

A seqüência ordenada h1, 5, 7, 15, 20, 27 i, por exemplo, pode ser representada pela seqüência de d-gaps h1, 4, 2, 8, 5, 7 i.

As duas formas são equivalentes.

Nenhuma compressão foi atingida ainda.

O maior d-gap na segunda representação ainda é potencialmente igual ao maior elemento na primeira representação.

Se uma codificação binária é usada para representar os d-gaps e existem N transações, ambos métodos requerem dlog Ne dígitos binários por tid armazenado.

A compressão é atingida apenas quando utilizamos um código cuja redução do comprimento dos d-gaps mais freqüentes compensa o aumento do comprimento dos d-gaps menos freqüentes.

As probabilidades p(x) de d-gaps iguais a x para a uma seqüência ordenada de p inteiros positivos escolhidos aleatoriamente com distribuição uniforme no intervalo.

Mostra as probabilidades p 16,y(x) (eixo probabilidade) de d-gaps iguais a x (eixo diferença adjacente) para uma seqüência ordenada de y (eixo tamanho da seqüência inteiros positivos escolhidos aleatoriamente com distribuição uniforme no intervalo) serve que, à medida em que o tamanho da seqüência aumenta, as diferenças adjacentes menores tornam-se mais prováveis e as maiores tornam-se menos prováveis.

Assim, assumindo que os tids de um tidset seguem uma distribuição uniforme, podemos dizer que, para os itemsets mais freqüentes, são mais adequados os códigos que privilegiam os números menores em detrimento aos números maiores.

A premissa pode parecer muito forte, mas, como a ordem das transações não importa para o problema, mediante a uma permutação aleatória das transações, ela é razoável.

Probabilidades dos d-gaps para uma seqüência ordenada de inteiros positivos no intervalo.

Freqüência dos d-gaps dos itens de algumas bases de dados.

Os gráficos apresentam para os valores dos d-gaps dos tidsets dos itens, no eixo das abscissas, as freqüências correspondentes, no eixo das ordenadas, para diversas bases de dados.

Observe que d-gaps pequenos são freqüentes e d-gaps grandes são raros, o que valida a estratégia de codificação de d-gaps para redução de consumo de memória, mesmo que a distribuição dos tids nos tidsets não seja uniforme.

Nesse capítulo apresentamos várias estratégias de redução de consumo de memória aplicáveis aos algoritmos de mineração de conjuntos freqüentes.

Algumas delas serão empregadas pelo algoritmo zEClaT e avaliadas nesse novo contexto.

Nesse capítulo apresentamos o algoritmo zEClaT e os parâmetros que alteram seu comportamento e ainda os experimentos e a análise dos resultados.

O algoritmo zEClaT, a especialização do algoritmo EClaT que representa um conjunto por uma seqüência de diferenças entre elementos consecutivos, é uma das contribuições deste trabalho.

Cinco parâmetros alteram o comportamento do algoritmo zEClaT.

Os parâmetros especificam o código com o qual as diferenças entre elementos consecutivos serão codificadas e quais refinamentos, entre curto-circuito, diffsets, projeção e reordenação dinâmica, serão utilizados.

Observe que o zEClaT é equivalente ao EClaT e o zEClaT é equivalente ao dEClaT (EClaT com diffsets).

As combinações de parâmetros que incluem os refinamentos de diffsets e projeção não foram implementadas porque esses refinamentos em suas formas originais não são ortogonais.

Explicam os parâmetros e refinamentos implementados para o algoritmo zEClaT, respectivamente, os códigos para representação das diferenças entre elementos consecutivos dos conjuntos de números inteiros, o curto-circuito, refinamento que melhora o desempenho abortando antecipadamente operações quando seus resultados são irrelevantes.

Os diffsets, refinamento que armazena diferenças entre conjuntos similares, a projeção, refinamento que renumera os identificadores das transações para que as diferenças entre elementos consecutivos sejam menores e ocupem menos espaço, e a reordenação dinâmica, refinamento que reordena as operações para uma utilização mais racional da memória.

Explicamos que a complexidade computacional do problema é preservada por nosso algoritmo mesmo com os diversos refinamentos.

Finalmente, as descrições das bases de dados, do ambiente de execução, dos experimentos e os resultados experimentais são os assuntos.

Um código é um sistema de símbolos com que se representam dados para serem processados por computador.

Na implementação do zEClaT, os códigos disponíveis são o Binário, o Elias, o Elias, o Fibonacci e o Unário, que são explicados abaixo e exemplificados.

O código Binário é um código posicional para representar números inteiros não-negativos.

O valor do i-ésimo dígito binário é igual a 2, ou seja, o primeiro dígito binário vale 2, o segundo, 2, e assim sucessivamente.

A delimitação do código Binário é implícita porque todas as representações ocupam a mesma quantidade de dígitos binários.

A quantidade de dígitos binários necessária para representar o número inteiro positivo n no código de Fibonacci é igual a A ostra os códigos Binário, Elias, Elias, Fibonacci e Unário para os inteiros jlog1+25 round h(n + /2) p5 ik 1 positivos n no intervalo delimitada por parênteses se repete.

Não existe representação para o número 0 nos códigos Elias, Elias, Fibonacci e Unário, assim como não existe representação para números maiores ou iguais a dlog (nmax)e Exemplos dos códigos Binário, Elias, Elias, Fibonacci e Unário O Algoritmo 2é alterado da seguinte forma.

A linha 11, correspondente ao cálculo das junções, agora se torna a linha 1do Algoritmo 41, armazenando nos conjuntos, em lugar dos elementos em forma nativa, as diferenças adjacentes dos elementos, codificadas com o código escolhido através de um parâmetro do algoritmo zEClaT.

O curto-circuito é um refinamento que aborta um operador sobre conjuntos quando antecipa que o resultado será irrelevante, procurando evitar processamento desnecessário.

O resultado de um operador é irrelevante para um algoritmo de mineração de conjuntos freqüentes quando o itemset é infreqüente, ou seja, a cardinalidade da interseção entre os tidsets é menor que o limite inferior de freqüência ou a soma da freqüência do itemset prefixo e a cardinalidade da diferença é maior que o limite inferior de freqüência.

O Algoritmo 2é alterado da seguinte forma.

A linha 11 passa a utilizar os operadores refinados por curto-circuito detalhados no Algoritmo 31 e se torna a linha 1do Algoritmo 41.

O operador utilizado é o de interseção, no cálculo dos tidsets, ou o de diferença, no cálculo dos diffsets.

O parâmetro inf para a interseção é o limite inferior de freqüência.

O parâmetro sup para a diferença é o limite inferior de suporte menos a freqüência do itemset prefixo.

Os diffsets, detalhados na Seção 43, são um refinamento que armazena para um itemset a sua freqüência e o seu diffset, a diferença entre o seu tidset e o tidset do itemset prefixo, em detrimento ao seu tidset.

O Algoritmo 2é alterado da seguinte forma.

A linha 11, que calcula as junções por meio de interseções, agora passa a calculá-las por meio de diferenças, se tornando a linha 1do Algoritmo 41.

As junções entre itemsets são calculadas pelas diferenças entre os tidsets.

As demais junções são calculadas pelas diferenças entre diffsets.

A ordem dos argumentos da diferença é importante.

Quanto ao itemset prefixo, seu tidset deve ser o primeiro argumento da diferença entre tidsets e seu diffset deve ser o segundo argumento da diferença entre diffsets.

A projeção, detalhada na Seção 44, é um refinamento que renumera os tids relevantes a uma classe de equivalência de forma que as diferenças entre os valores consecutivos dos elementos dos tidsets dos itemsets dessa classe sejam menores.

Quando as diferenças são menores o consumo de memória é menor.

Os tids relevantes a uma classe de equivalência de prefixo P são os tids das transações que contêm P, que são renumeradas em seqüência por 1, 2.

O Algoritmo 2é alterado da seguinte forma.

A linha 11, que calcula as junções utilizando interseções, agora passa a utilizar o operador refinado por projeção detalhado no Algoritmo 3com o parâmetro U igual a itemsets A reordenação dinâmica, detalhada na Seção 45, é um refinamento que altera a ordem em que as junções entre itemsets são realizadas.

Os itemsets menos freqüentes são processados pelo algoritmo antes dos itemsets mais freqüentes.

A tendência é que os resultados intermediários sejam menores e que quando os itemsets mais freqüentes forem processados a memória alocada pelos itemsets menos freqüentes já tenha sido liberada.

O Algoritmo 2é alterado da seguinte forma.

Um comando que ordena os itemsets freqüentes é acrescentado ao final da linha 5, se tornando a linha 6 do Algoritmo 41, e outro comando que ordena os demais itemsets freqüentes de cada classe de equivalência ao final da linha 16, se tornando a linha 18 do Algoritmo 41.

Nesta seção apresentamos o problema de mineração de conjuntos freqüentes visto por outra perspectiva e sua complexidade computacional e explicitamos a manutenção da complexidade zEClaT não-recursivo, computacional do problema apesar dos diversos refinamentos dos algoritmos.

Um grafo bipartido tem dois conjuntos disjuntos de vértices, e um conjunto de arestas.

Um subgrafo bipartido completo, chamado clique bipartido, é denotado.

A entrada para o problema de mineração de conjuntos freqüentes é essencialmente um grafo bipartido, com V como o conjunto de itens, V como o conjunto de transações, e cada par ordenado (item, transação) como uma aresta.

O problema de mineração de conjuntos freqüentes corresponde à enumeração de cliques bipartidos restritos com espaço de busca para enumeração de todos os itemsets é 2, que é exponencial em I.

Entretanto, se assumirmos que existe um limite l no tamanho da transação, a tarefa de encontrar todos os itemsets freqüentes é essencialmente linear no tamanho do banco de dados, pois a complexidade global neste caso é dada, onde m é o número de itemsets freqüentes maximais. Nenhum dos refinamentos altera a complexidade computacional do problema de mineração de conjuntos freqüentes.

São apenas estratégias que, na prática, dependendo de características da entrada, podem reduzir por um fator polinomial o tempo de execução Nesta seção, avaliamos empiricamente as efetividades dos refinamentos para diversos parâmetros e bases de dados.

As bases de dados utilizadas foram accidents, BMS-POS, BMS-WebView-1, BMS-WebView-2, chess, connect, kosarak, mushroom, pumsb, pumsb_star, retail, T10 ID100 K e T40 I10 D100 K, disponíveis publicamente.

WebView-1 e BMS-WebView-contêm dados de seqüências de cliques e compras do varejista virtual de vestuário e medicamentos para pernas Gazzele.

A base de dados chess descreve o fim-de-jogo de uma partida de xadrez entre rei e torre brancos contra rei e peão em a7 pretos (usualmente abreviado KRKPA7).

O peão em a7 significa que a promoção de peão para rainha está a um movimento.

O jogador com as brancas deve movimentar uma peça.

O formato para instâncias nessa base de dados é uma seqüência de 37 valores de atributos.

Os 36 primeiros atributos descrevem o tabuleiro e trigésimo sétimo a classificação "branco pode vencer" ou "branco não pode vencer"

A base de dados connect contém posições legais do jogo de connect, nas quais nenhum jogador ganhou ainda, e nas quais o próximo movimento é não-forçado

A base de dados mushroom inclui descrições de exemplos hipotéticos correspondentes a 2espécies de cogumelos com hifas dos gêneros Agaricus e Lepiota.

Cada espécie é identificada como definitivamente comestível, definitivamente venenosa, ou desconhecida e não recomendada.

A última classe foi combinada com a venenosa

A base de dados kosarak foi provida por Ferenc Bodon e contém dados anônimos de seqüências de cliques de um portal de notícias online húngaro

As bases de dados pumsb e pumsb_star contém informações pessoais e habitacionais dos questionários distribuídos para uma amostra da população do censo norte-americano de 1980.

As variáveis habitacionais incluem propriedade, ano de construção, número e tipos de quartos, encanamento, equipamentos de aquecimento, custos de taxas e hipoteca, número de crianças e renda familiar.

As variáveis pessoais incluem sexo, idade, estado civil, origem espanhola, renda, profissão, meio de transporte ao trabalho e escolaridade

As bases de dados T10 ID100 K e T40 I10 D100 K foram geradas usando o gerador do grupo de pesquisa IBM Almaden Quest.

As características dessas bases de dados são mostradas.

Características das bases de dados.

O algoritmo zEClaT foi implementado na linguagem C++ e utiliza classes da STL (Standard Template Library).

Foram utilizados o compilador g++, versão 343, do projeto GNU, e os parâmetros de compilação, que gera instruções ajustadas para os processadores Intel Pentium, e IV, e O3, que habilita otimizações, que habilita todos os avisos sobre construções questionáveis fáceis de evitar.

O microcomputador utilizado possui processador Intel Celeron (Coppermine), de 565 MHz de freqüência com 128 kB de cache, 255708 kB de memória primária, 10,2GB de memória secundária com 51B de cache em um disco rígido Seagate ST31021A, 74843kB de swap e Linux kernel 2427.

Um programa executa a implementação do algoritmo zEClaT para combinações de entradas e parâmetros.

Qualquer execução que ultrapasse 5 minutos de tempo de execução é abortada para que a coleção de execuções seja concluída em tempo hábil.

A entrada é lida de um arquivo, a saída é escrita para outro arquivo.

O consumo de memória, o tempo de execução e outras medidas são registrados pelo próprio algoritmo em um terceiro arquivo.

Realizamos um conjunto de experimentos com o objetivo de avaliar empiricamente a efetividade do zEClaT e seus refinamentos.

Medimos o consumo de memória e o tempo de execução após a realização de cada junção de itemsets.

Para algumas bases de dados e limites inferiores de suporte, escolhidos para cada base de dados de forma que o tempo de execução fosse próximo do limite de tempo estipulado para os processos de cinco minutos, o consumo de memória máximo e o tempo de execução total é mostrado.

A primeira coluna exibe a configuração, na ordem, código, diffsets, projeção, curto-circuito e reordenação dinâmica.

Os símbolos d, p, c e r representam, respectivamente, os refinamentos diffset, projeção, curto-circuito e reordenação dinâmica.

Uma linha horizontal sobre o símbolo representa refinamento desabilitado.

A primeira linha exibe a base de dados e o limite inferior de suporte.

As demais células exibem o consumo de memória máximo, em bytes, ou o tempo de execução total, em segundos.

Os gráficos mostram, para a base de dados chess e limite inferior de suporte de 80%, o efeito, respectivamente, do código, dos diffsets, da projeção, do curto-circuito e da reordenação dinâmica, no consumo de memória e no tempo de execução.

Os gráficos à esquerda mostram o consumo de memória em função do tempo de execução e os gráficos à direita mostram o consumo de memória em função da quantidade de junções de itemsets.

Para avaliar a efetividade da alteração de um parâmetro de configuração p, do valor v para o valor v, quanto à medida M, calculamos a média geométrica das razões entre as medidas M válidas das configurações c e c.

Os parâmetros das configurações c e c são iguais, à exceção do parâmetro p, que nas configurações c assume o valor v e nas configurações c assume o valor v.

Ela aumenta a medida M em b(m 1)c vezes ou b100 (m 1)c% e quando m < 1, dizemos que ela reduz a medida M em b100 (1 m)c%.

Em relação ao consumo de memória máximo, para a base de dados accidents e limite inferior de suporte 90%, os códigos Elias, Elias, de Fibonacci e Unário o reduziram em, respectivamente, 89%, 88%, 87% e 55%, em relação ao código Binário.

A habilitação dos diffsets o reduziu em 10%.

As habilitações da projeção e da reordenação dinâmica o reduziram em menos de 1%.

Já para a base de dados BMS-POS e limite inferior de suporte 1%, foi pequena a quantidade de configurações que respeitaram o limite de trezentos segundos de tempo de execução.

O consumo de memória máximo foi reduzido em 16% pelas habilitações da reordenação dinâmica e da projeção e em 9% pelo código de Fibonacci em relação ao código Elias.

Para a base de dados BMS-WebView-1 e limite inferior de suporte 0,1%, a habilitação da reordenação dinâmica e a desabilitação dos diffsets reduziram o consumo de memória máximo em, respectivamente 85% e 80%, e os códigos de Fibonacci, Elias e Elias o reduziram em, respectivamente, 49%, 43% e 40%, em relação ao código Binário.

Finalmente, a habilitação da projeção o reduziu em 17%.

Quanto à base de dados BMS-WebView-e limite inferior de suporte 0,2%, os códigos de Fibonacci, Elias e Elias reduziram o consumo de memória máximo em, respectivamente, 41%, 35% e 28%, em relação ao código Binário.

A habilitação da reordenação dinâmica, a desabilitação dos diffsets e a habilitação da projeção o reduziram em, respectivamente, 26%, 15% e 10%.

O consumo de memória máximo, para a base de dados chess e limite inferior de suporte 80%, foi reduzido em, respectivamente, 87%, 87%, 80% e 75%, pelos códigos Elias, Elias, de Fibonacci e Unário, em relação ao código Binário.

A habilitação dos diffsets, da reordenação dinâmica e da projeção o reduziram em, respectivamente, 86%, 68% e 6%.

A maior redução do consumo de memória máximo foi obtida para a base de dados connect e limite inferior de suporte 98%.

Os códigos Elias, Elias, Unário e de Fibonacci o reduziram em, respectivamente, 93%, 93%, 89% e 87%.

A habilitação dos diffsets e da reordenação dinâmica o reduziram em, respectivamente, 61% e 56%.

Apenas os códigos fizeram diferença para a base de dados kosarak e limite inferior de suporte 1%.

As configurações de código Binário e Unário não respeitaram o limite de trezentos segundos de tempo de execução.

Os códigos de Fibonacci e Elias reduziram o consumo de memória máximo em, respectivamente, 19% e 13%, em relação ao código Elias.

Consumo de memória máximo (1/2).

Para a base de dados mushroom e limite inferior de suporte 30%, os códigos Elias, 47, de Fibonacci e Unário reduziram o consumo de memória máximo em, respectivamente, 85%, 84%, 79% e 79%, em relação ao código Binário.

As habilitações da reordenação dinâmica, dos diffsets e da projeção o reduziram em, respectivamente, 63%, 58% e 27%.

A redução do consumo de memória máximo para a base de dados pumsb e limite inferior de suporte 90% pelos códigos Elias, Elias e de Fibonacci foi de, respectivamente, 79%, 79% e 78%, em relação ao código Binário.

As habilitações dos diffsets, da reordenação dinâmica e da projeção o reduziram em, respectivamente, 65%, 36% e 3%.

Um resultado parecido foi alcançado para a base de dados pumsb_star e limite inferior de suporte 50%.

A redução do consumo de memória máximo pelos códigos Elias, Elias e de Fibonacci foi de, respectivamente, 87%, 87% e 83%, em relação ao código Binário.

As habilitações dos diffsets, da projeção e da reordenação dinâmica o reduziram em, respectivamente, 63%, 21% e 15%.

Algumas configurações de código Binário não respeitaram o limite de trezentos segundos de tempo de execução.

Já para a base de dados retail e limite inferior de suporte 0,5%, os códigos de Fibonacci, Elias e Elias reduziram o consumo de memória máximo em, respectivamente, 48%, 45% e 40%.

A desabilitação dos diffsets e a habilitação da reordenação dinâmica o reduziram em, respectivamente, 29% e 20%.

Finalmente, para as bases de dados sintéticas T10 ID100 K e T40 I10 D100 K, a redução do consumo de memória máximo foi obtido apenas pelo código.

A redução em relação ao código Binário, para a base de dados T10 ID100 K e limite inferior de suporte 20%, para os códigos de Fibonacci, Elias e Elias, foi de, respectivamente, 49%, 43% e 39%, e, para a base de dados T40 I10 D100 K e limite inferior de suporte 2%, para os códigos de Fibonacci, Elias e Elias, foi de, respectivamente, 63%, 59% e 58%.

Consumo de memória máximo (2/2).

Os resultados em função do tempo de execução podem ser encontrados.

Em resumo, para as instâncias avaliadas, os melhores códigos quanto à redução do consumo máximo de memória em relação ao código Binário, foram o Elias, redução de 76%, seguido pelo Elias, redução de 76%, depois o de Fibonacci, redução de 73%, e, por último, o Unário, redução de 45%.

A redução do consumo máximo de memória para a habilitação da reordenação dinâmica é de 40%, para a habilitação dos diffsets é de 33% e para a habilitação da projeção é de 8%.

Quanto ao tempo de execução, os códigos Elias, Elias, de Fibonacci e Unário, o reduzem em, respectivamente, 57%, 54%, 48% e 39%, em relação ao código Binário.

A habilitação dos diffsets o reduz em 41%, a habilitação do curto-circuito o reduz em 25%, a habilitação da reordenação dinâmica o reduz em 15% e a habilitação da projeção o aumenta em 11%.

Na maioria dos casos as reduções conjugadas são ainda maiores que as individuais.

A diferença de consumo máximo de memória entre a melhor configuração do zEClaT e a configuração equivalente ao EClaT ou ao dEClaT chega a mais de uma ordem de magnitude em alguns.

Esse é o caso para a base de dados chess, com limite inferior de suporte igual a 80%, código Elias e curto-circuito, diffsets e reordenação dinâmica habilitados, mais de 20 vezes melhor para a base de dados connect, com limite inferior de suporte igual a 30%.

Código Elias e curto-circuito e diffsets habilitados, mais de 10 vezes melhor e a base de dados mushroom, com limite inferior de suporte igual a 30%, código Elias e curto-circuito, diffsets e reordenação dinâmica habilitados, mais de 30 vezes melhor.

O padrão do consumo de memória do algoritmo independe do código e que, para a base de dados chess e limite inferior de suporte igual a 80%, os códigos Elias, Elias, Fibonacci e Unário foram responsáveis por reduções entre 82% e 90% do consumo de memória e entre 68% e 80% do tempo de execução.

O padrão do consumo de memória do algoritmo independe da habilitação de diffsets e que, para a base de dados chess e limite inferior de suporte igual a 80%, a habilitação dos diffsets foi responsável por reduções de 93% do consumo de memória e 92% do tempo de execução.

O padrão do consumo de memória do algoritmo independe da habilitação da projeção e que, para a base de dados chess e limite inferior de suporte igual a 80%, a habilitação da projeção, além de não reduzir o consumo de memória, aumentou o tempo de execução em 58%.

O padrão do consumo de memória do algoritmo independe da habilitação do curto-circuito e que, para a base de dados chess e limite inferior de suporte igual a 80%, a habilitação do curto-circuito não reduz o consumo de memória mas reduz o tempo de execução em 17%.

Nos gráficos podemos observar que o padrão do consumo de memória do algoritmo difere dependendo da habilitação da reordenação dinâmica e que, para a base de dados chess e limite inferior de suporte igual a 80%, a habilitação da reordenação dinâmica foi responsável por reduções de 61% do consumo de memória e de 40% do tempo de execução.

Neste trabalho realizamos a revisão bibliográfica de estratégias de redução de consumo de memória aplicáveis aos algoritmos de mineração de conjuntos freqüentes, implementamos e avaliamos um novo algoritmo que aplica algumas das estratégias, denominado zEClaT.

O algoritmo zEClaT efetivamente reduz o consumo de memória em relação ao algoritmo EClaT e ao algoritmo dEClaT.

A redução é superior a uma ordem de grandeza para algumas bases de dados e limites inferiores de suporte.

Um código cuja distribuição de probabilidade implícita é mais próxima ao decaimento exponencial é adequado a limites inferiores de suporte altos e um código cuja distribuição de probabilidade implícita é mais próxima à distribuição uniforme é adequado a limites inferiores de suporte baixos.

As características das bases de dados e o parâmetro de limite inferior de suporte influenciam diretamente a efetividade das estratégias de redução de consumo de memória e tempo de execução.

O código Unário é efetivo apenas para a redução de consumo de memória para bases de dados densas e limites inferiores de suporte grandes.

À medida que a densidade da base de dados ou o limite inferior de suporte diminui, os códigos Elias, Elias e Fibonacci, não necessariamente nessa ordem, tornam-se mais interessantes, até que o código Binário seja o mais econômico, para bases de dados esparsas ou limites inferiores de suporte pequenos.

Para um dado código, o consumo de memória do algoritmo com d-gaps nunca é maior que o do algoritmo sem d-gaps.

O curto-circuito é efetivo somente para a redução do tempo de execução, quando a quantidade de itemsets infreqüentes verificados é grande em relação à quantidade de itemsets freqüentes.

O consumo de memória do algoritmo com curto-circuito nunca é maior que o do algoritmo sem curto-circuito.

Os diffsets reduzem tanto o consumo de memória quanto o tempo de execução, especialmente quando as bases de dados são densas e os limites inferiores de suporte são grandes.

Há possibilidade que o consumo de memória do algoritmo com diffsets seja maior que o do algoritmo sem diffsets.

A projeção reduz marginalmente o consumo de memória e aumenta marginalmente o tempo de execução.

A redução é maior quando as bases de dados são esparsas e quando o limite inferior de suporte é pequeno.

O consumo de memória do algoritmo com projeção nunca é maior que o do algoritmo sem projeção.

A reordenação dinâmica altera o comportamento do consumo de memória tanto pela redução do consumo pelos resultados intermediários quanto pelo deslocamento do consumo dos picos provendo uma utilização mais racional da memória.

Quando a base de dados é longa o custo da reordenação dinâmica é compensado pelo pela redução de custo das operações.

Há possibilidade que o consumo de memória do algoritmo com reordenação dinâmica seja maior que o do algoritmo sem reordenação dinâmica.

Como trabalhos futuros podemos implementar e avaliar outros refinamentos, em particular, calcular os itemsets freqüentes a partir do arquivo de entrada e não a partir dos itemsets freqüentes.

Avaliar o algoritmo quando há memória secundária envolvida.

Inventar e avaliar métricas para escolha automática de configuração.

Caracterizar os tidsets mais propícios à economia de consumo de memória e aplicar compactação seletiva.

A princípio, os itemsets mais rasos na treliça são os que mais consomem memória e os mais propícios à economia e sua compactação já reduziria significativamente o consumo afetando menos o desempenho.

Agora mostramos um exemplo passo a passo do funcionamento do algoritmo EClaT.

Os vértices representam os itemsets.

O primeiro conjunto é o itemset e o segundo conjunto é o tidset.

Vértices de borda preta representam itemsets na memória.

Vértices de fundo cinza representam itemsets freqüentes.

As arestas representam a relação de cobertura.

Arestas pretas representam junções.

Classe de equivalência.

Classe de equivalência.

A primeira classe é e do itemset fAg, fAg g fBg, é freqüente.

Classe de equivalência.

A próxima junção da classe.

Classe de equivalência.

A próxima junção da classe.

Classe de equivalência.

A próxima junção da classe.

Classe de equivalência.

A próxima classe é e do itemset fA, Bg, fA, Bg g fA, Dg, é freqüente.

Classe de equivalência.

A próxima junção da classe A7.

Classe de equivalência.

A próxima classe é classe é é retomada e o itemset fA, B, Dg é liberado.

O único itemset restante é fA, B, Eg, que é liberado.

Classe de equivalência.

O próximo itemset é fA, Dg.

A primeira junção da classe fA,Dg g fA,Eg, é freqüente.

A próxima classe é é fA, D, Eg, que é liberado.

A classe itemset restante é fA, Eg, que é liberado.

Classe de equivalência.

O próximo itemset é fBg.

A primeira junção da classe freqüente.

Classe de equivalência.

A próxima junção da classe.

Classe de equivalência.

A próxima junção da classe.

Classe de equivalência.

A próxima classe é e do itemset fB, Cg, fB, Cg g fB, Dg, é infreqüente e é liberada.

Classe de equivalência.

A próxima junção da classe A14.

A próxima classe é é retomada e o itemset fB, Cg é liberado.

Classe de equivalência.

O próximo itemset é fB, Dg.

A primeira junção da classe fB,Dg g fB,Eg, é freqüente.

A próxima classe é é fB, D, Eg, que é liberado.

A classe itemset é fB, Eg, que é liberado.

Classe de equivalência.

O próximo itemset é fCg.

A primeira junção da classe infreqüente e é liberada.

Classe de equivalência.

A próxima junção da classe próxima classe é é retomada e o itemset fCg é liberado.

Classe de equivalência O próximo itemset é fDg.

A primeira junção da classe freqüente.

A próxima classe é A classe algoritmo chega ao fim.

Tempo de execução total (1/2).

