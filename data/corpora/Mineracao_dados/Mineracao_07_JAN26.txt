As perdas que ocorrem na agricultura são grandes, devido, principalmente, à ocorrência de sinistros climáticos que ocorrem nas plantações.

Muitas vezes, os impactos social e econômico causados pelos danos são significativos, uma vez que envolvem fatores como a produção e o preço de alimentos.

Como exemplos, têm-se a produção de café e a de cana-de-açúcar no Estado de São Paulo, que sofrem alternâncias motivadas por eventos climáticos adversos e, em especial, as geadas e as secas, que reduzem drasticamente as produções.

Neste sentido, este estudo propõe identificar relações entre parâmetros climáticos, como temperatura máxima, temperatura mínima, precipitação, entre outros atributos, visando descobrir eventuais novos conhecimentos, a partir do comportamento conhecido dos atributos climáticos já ocorridos no passado, com o propósito de desenvolver a previsão local de geada e a previsão de deficiência hídrica.

Para isso, foram aplicadas técnicas de descoberta de conhecimento em grandes bancos de dados climáticos.

Utilizaram-se as ferramentas WEKA e o DISCOVER, que foram consideradas satisfatórias, uma vez que os objetivos propostos foram atingidos.

As bases de dados disponíveis atenderam a necessidade para a realização do projeto, apresentando um volume de dados e atributos suficientes para que pudesse gerar resultados para a previsão local de geada e de deficiência hídrica.

Referente aos resultados, com até 1 dia de antecedência à geada, o modelo gerado foi considerado confiável.

A partir de dias de antecedência à geada, os resultados encontrados apresentam uma diminuição no grau de acerto quanto mais distante estiver de acontecer o evento geada.

Para o caso deficiência hídrica, os resultados encontrados foram diferenciados conforme a classe.

Para a classe não, com 1 dia até 15 dias de antecedência ao evento, o grau de acerto foi alto e aceitável.

A classe forte, em seguida à classe não, é a que apresenta melhores resultados de acerto, decaindo para as outras classes.

Até dias de antecedência ao evento deficiência hídrica e, dependendo do mês, o grau de acerto é aceitável.

De dias em diante, os resultados mostram que o modelo gerado não é aceitável.

A atividade agrícola é altamente dependente das condições climáticas que muitas vezes estão fora do controle do homem.

Comandada por variáveis climáticas, estas podem influenciar, sobretudo, o resultado final da safra, pois a planta depende principalmente do regime pluviométrico e da temperatura adequada em cada fase de desenvolvimento das culturas.

Fenômenos climáticos adversos, como má distribuição da chuva e, consequentemente, falta de água no solo e temperaturas baixas, podem levar a grandes prejuízos econômicos, acarretando em perdas no final da safra.

É interessante observar que esses fenômenos, se considerados catastróficos, apresentam efeitos drásticos, que podem ter severas conseqüências.

Em outras palavras, em Estados em que a atividade agrícola representa boa parte do PIB, a ocorrência de seca e geada, por exemplo, pode afetar muitos municípios ao mesmo tempo resultando em reduções consideráveis do PIB.

Sendo assim, surge a necessidade de se desenvolver sistemas que consigam prever a ocorrência de alguns fenômenos climáticos, entre eles, desenvolver um sistema de alerta para geada e para deficiência hídrica para auxiliar o produtor em tomadas de decisão reduzindo impactos severos.

Para prever a ocorrência de geada e de deficiência hídrica, é apresentada a metodologia de descoberta de conhecimento em grandes bancos de dados, por meio do processo conhecido como KDD (Knowledge Discovery in Database).

É fato que atualmente existem muitos dados climáticos armazenados que podem auxiliar no desenvolvimento deste projeto utilizando tal metodologia.

Cada vez mais, o volume de dados excede a capacidade de sua análise pelos métodos tradicionais (planilhas, consultas e gráficos).

Esses métodos podem gerar relatórios a partir dos dados, mas não conseguem analisá-los sob o enfoque conhecimento.

Para atender essa necessidade, surgiram novas técnicas e ferramentas, que permitem a extração de conhecimento a partir de grandes volumes de dados.

Descoberta de Conhecimento em Banco de Dados KD é a descoberta de conhecimento interessante, mas escondido, em grandes bases de dados.

É uma metodologia dentro da área de inteligência artificial que faz uso de algoritmos de aprendizado de máquina, baseada em uma nova geração de hardware e software que inclui análises estatísticas, exploração visual, árvores de decisão, entre outras, para explorar grandes bases de dados e descobrir relações e padrões existentes nesses dados.

Destaca-se que procurar padrões na área meteorológica, para previsão de geada e de deficiência hídrica, utilizando a metodologia KDD, é uma atividade que está em desenvolvimento, se iniciando.

A questão da integração de várias áreas do conhecimento para a resolução de problemas, ou seja, o uso da multidisciplinaridade entre as áreas agrícola e inteligência artificial é um desafio, justificando ainda mais o desenvolvimento deste trabalho.

Salienta-se também que com a percepção dos especialistas em climatologia, que possuem um claro sentimento de que é possível extrair conhecimento novo "escondido" e útil no grande volume de dados climáticos, a partir do comportamento conhecido dos atributos climáticos já ocorridos no passado, aumentam as chances de se descobrir padrões que podem explicar e ajudar a predizer o comportamento futuro dos fenômenos climáticos estudados neste trabalho.

Como objetivo geral, este estudo pretende desenvolver modelos de previsão local para geada e para deficiência hídrica, que dêem resultados com um grau de confiança satisfatório e num intervalo de tempo adequado.

Entre os objetivos específicos destacam-se.
Analisar bancos de dados climáticos disponíveis e identificar relações entre parâmetros climáticos, descobrir novos conhecimentos entre parâmetros climáticos.

Utilizar a metodologia de descoberta de conhecimento em banco de dados KDD e auxiliar os produtores na tomada de decisão, visando a proteção contra essas ocorrências, reduzindo os impactos causados.

Neste capítulo foi apresentado o contexto em que o trabalho está inserido, bem como os seus objetivos.

As demais partes do trabalho estão organizadas conforme os itens a seguir.

No capítulo será apresentada a fundamentação teórica do estudo, as definições de geada e deficiência hídrica e o processo KDD mostrando as suas diversas etapas e os principais trabalhos envolvidos na área, dando suporte a este trabalho.

No capítulo será apresentada a aplicação da tarefa de Classificação e a utilização da técnica de árvore de decisão para descobrir conhecimentos na base de dados e o material disponível para a realização do experimento.

No capítulo serão apresentados os resultados encontrados e discutidos após a aplicação do processo de mineração de dados.

E finalmente, no capítulo 5 é apresentada a conclusão deste trabalho.

Nos locais situados a médias e altas latitudes, a agricultura torna-se atividade de risco durante o inverno, devido à ocorrência de temperaturas baixas.

A proteção de plantas contra os efeitos letais causados pela geada é problema considerável na agricultura, especialmente para as lavouras de alta rentabilidade, entre as quais estão fruticultura de clima tropical, o cafeeiro, seringueira entre outros.

No Brasil, a geada é um fenômeno freqüente nas latitudes maiores que 19ºS, englobando os Estados de Minas Gerais (Triângulo Mineiro e região sul), São Paulo, Mato Grosso do Sul, Paraná, Santa Catarina e Rio Grande do Sul, onde sua ocorrência resulta em graves prejuízos econômicos, principalmente quando ocorrem precocemente no outono, ou tardiamente na primavera.

As taxas de sinistralidade agrícola devido às geadas são muito grandes.

A produção de café pelos Estados do Sudeste brasileiro sofre alternância motivada por eventos climáticos adversos (geadas e secas), que reduzem drasticamente as produções.

No Estado de São Paulo, em 2000, a produção de café ocupava aproximadamente 240 mil hectares, contra 710 mil hectares em 1975, ano de inflexão da produção de café, decorrente da pior geada da história da cafeicultura nacional.

A maior parte dos cafezais do Estado de São Paulo foi dizimada, iniciando o processo de perda relativa da participação da produção desse Estado no total produzido no Brasil.

Sob o ponto de vista físico, geada é a ocorrência de temperatura do ar abaixo de 0ºC, podendo ou não dar origem à formação de gelo sobre as superfícies expostas.

Sob o aspecto agronômico, considera-se geada qualquer redução de temperatura que acarrete na planta efeitos prejudiciais ao seu crescimento ou desenvolvimento.

Portanto, deve-se destacar que nem sempre a presença de gelo sobre as superfícies expostas significa que ocorreu geada do ponto de vista agronômico, pois a temperatura que provoca danos às plantas pode não ter sido atingida.

A geada ocorre devido à queda de temperatura abaixo do nível de dano de cada cultura.

A morte do tecido vegetal por frio é um processo físico-químico.

O processo inicia-se assim que a temperatura letal da planta é atingida, variando de espécie para espécie, havendo o congelamento da solução extracelular, que resulta em desequilíbrio do potencial químico da água da solução intracelular em relação ao potencial químico da solução extracelular, parcialmente congelada.

Isso gera um processo contínuo de perda de água no sentido intra para extracelular, até que o equilíbrio seja reestabelecido, provocando a desidratação da célula ou o congelamento da solução intracelular.

Os tipos de geada podem ser definidos quanto à sua gênese (origem) ou pelos efeitos visuais (aspecto das plantas) que elas produzem.

E, ainda, as geadas podem ocorrer em função de dois fenômenos meteorológicos, advecção de ar frio e perda de radiação terrestre.

Quanto à gênese, a geada de advecção ou de vento frio é aquela provocada por ocorrência de ventos fortes, constantes, com temperaturas muito baixas, por muitas horas seguidas.

O ar frio resseca a folhagem causando sua morte.

Portanto, os ventos são os causadores do dano à planta.

Em algumas situações, esse tipo de geada fica bem caracterizado por haver dano apenas em um lado da planta (aquele voltado para os ventos predominantes).

A advecção de ar frio resulta da entrada de massas de ar frio, provenientes da região polar, e que atingem as regiões subtropicais.

No seu deslocamento em direção ao equador, elas trazem ventos frios causando maiores danos durante o inverno, principalmente na face sul do relevo (geada de vento).

Os danos causados por esse tipo de geada são tanto pelas baixas temperaturas (queima das folhas) como pela injúria mecânica provocada pela agitação contínua das plantas.

Este tipo de geada é ilustrado.

Planta de café que foi atingida pela geada de advecção.

Produção da planta no ano subseqüente à ocorrência da geada.

Pode-se notar que parte dela produziu o fruto e parte não atingiu a produção, devido ser afetada pelo vento forte e constante, ressecando a folhagem.

Efeito da geada de advecção sobre uma plantação.

Ainda quanto à gênese, a geada de radiação ocorre quando há resfriamento intenso da superfície, que perde energia durantes as noites de céu limpo, sem vento, e sob domínio de um anticiclone estacionário, de alta pressão massa de ar polar fri, com baixa concentração de vapor d'água.

A perda radiativa da superfície faz com que o ar adjacente a ela também se resfrie.

Logo, o agente causador é a perda radiativa intensa.

Essa situação ocorre frequentemente em regiões de clima árido, em que a falta de vapor d'água atmosférico reduz o efeito estufa local.

Nesta situação, durante o dia, a temperatura na superfície se mantém acima do ponto de congelamento.

Porém, após o pôr-do-sol, durante a noite, a perda de energia da superfície por emissão de radiação de ondas longas se acentua, provocando queda rápida da temperatura do ar próximo à superfície, resultando no que se denomina inversão térmica, ou seja, a temperatura aumenta com a altura, nos primeiros metros, ao invés de diminuir (situação normal).

Um exemplo de geada de radiação é apresentado.

Ocorrência de geada de radiação sobre uma plantação.

A geada mista é a situação em que ocorrem os dois processos sucessivamente, a geada de advecção e a geada de radiação, ou seja, entrada de massa fria e seca, e subsequentemente estagnação sobre a região permitindo intensa perda radiativa noturna.

Referente ao aspecto visual, a geada pode ser classificada em geada negra e geada branca.

Estes tipos de geada são denominados em função da aparência.

A geada negra ocorre quando o ar está muito seco e a planta morre antes que ocorram formação e congelamento do orvalho.

Em outras palavras, a geada negra ocorre quando a atmosfera tem baixa concentração de vapor d'água e a perda radiativa é intensa, causando resfriamento acentuado da vegetação, chegando à temperatura letal.

Em função do baixo teor de umidade no ar, não há deposição de gelo, por falta de água.

Este tipo de geada é mais severo, pois a baixa umidade do ar permite ocorrência de temperaturas bem menores.

Nas condições brasileiras, normalmente se conhece como geada negra os danos de ventos frios que desidratam os tecidos expostos.

Por isso, também se chama a geada negra de geada de vento.

A geada branca é a típica geada de radiação, com deposição de gelo sobre as plantas, ou seja, é aquela que ocorre quando o intenso resfriamento noturno produz condensação de vapor d'água e seu congelamento sobre as plantas, conferindo uma coloração branca sobre a vegetação.

Nesse caso, a concentração de vapor d'água na atmosfera adjacente à superfície é mais elevada que na geada negra.

Quando há mais umidade no ar, primeiro ocorre a condensação com liberação do calor latente, fato que ajuda a reduzir a queda da temperatura.

Muitas vezes, a geada branca não provoca danos para culturas mais tolerantes, pois embora a água congele a 0ºC, a temperatura letal pode estar bem abaixo deste valor.

Portanto, a geada branca é menos severa que a negra.

Quando a temperatura mínima afeta o tronco da planta, causando danos aos tecidos externos, que podem levar a planta à morte, ocorre o evento chamado de "geada de canela" ou "canela de geada".

Pode-se ainda classificar geada quanto à sua ocorrência e danos causados na lavoura, categorizando em geada moderada, severa e severíssima.

As geadas severíssimas ocorrem, em média, três por século e provocam danos severos em toda a região devido à morte das plantas.

Um exemplo de geada severíssima na lavoura do café, ocorrida em 1975, é mostrado.

Exemplo de geada severíssima ocorrida 18/07/1975, Mandaguaçu/PR.

A geada de 1975 causou uma perda de 10,milhões de sacas, 48% da produção nacional.

O prejuízo chegou a US$ 1 bilhão US$ 100/sac 500 milhões de pés de café.

Na safra de 1976 ocorreu uma perda de 3,8 mil sacas, 0,1% da produção nacional.

As geadas severas ocorrem, em média, a cada 6 a 10 anos e afetam a produção dos anos vigente e subseqüente.

Exemplo, geadas ocorridas em 1979, 1981, 1994, 2000.

Um exemplo de geada severa ocorrida em julho de 2000, no Paraná, é mostrado.

Exemplo de geada severa, ocorrida em julho de 2000, no Estado do Paraná.

E as geadas moderadas são aquelas que ocorrem, em média, a cada a anos e provocam danos superficiais e geada de canela.

Entre os fatores que favorecem a ocorrência de geada, podem-se citar fatores macroclimáticos, topoclimáticos e microclimáticos.

Os fatores macroclimáticos são aqueles relacionados à escala regional ou geográfica do clima, que dependem das nuances climáticas impostas pelos fatores latitude, altitude, continentalidade (oceanidade) e circulação global (massa de ar polar).

Quanto à latitude, Pereira comentam que, quanto maior a latitude, maior a ocorrência de geadas, mas Camargo relata que ao nível do mar e dependendo da altitude, as geadas ocorrem somente em latitudes maiores que 23ºS.

Entre 23ºS e 27ºS, ocorrem geadas no inverno, e elas danificam as culturas tropicais perenes café, citrus e banan.

Em latitudes maiores que 27ºS, ocorrem geadas precoces no outono, geadas normais no inverno e geadas tardias na primavera.

As geadas normais, que ocorrem no inverno, não afetam as culturas de clima temperado, pois nessa época essas plantas estão dormentes, mas as geadas precoces e as tardias afetam também estas culturas.

Na primavera, as geadas afetam a florada, e no outono, a frutificação.

Referente à altitude, quanto maior a altitude, menor a temperatura, e maior a ocorrência de geada, interferindo também a latitude.

Entre 20ºS e 23ºS, a freqüência de geada aumenta com a altitude.

Esse é o caso do Estado de São Paulo, onde ao nível do mar não ocorre geada.

O oceano, por ser uma enorme massa de água, com alto poder calorífico, tem efeito moderador nas variações da temperatura.

No interior do continente, a variação da temperatura é maior, podendo atingir valores muito baixos, com aspectos negativos aos tecidos vegetais.

E quanto à massa de ar polar, pela configuração do continente sul-americano, em forma de cone, invasões de massas polares (frias), que atingem a região sudeste, via continente, com ventos que transpõem os Andes, soprando de oeste e sudoeste, abaixam repentinamente a temperatura local.

Essa transposição dos Andes resulta em massa fria e seca.

A rota continental também não permite que a massa fria ganhe umidade.

O ar frio e seco favorece a perda de energia por radiação durante a noite e, consequentemente, a ocorrência de geadas de radiação.

Quando há o efeito da radiação e vento frio juntos, os danos são bem maiores.

Se a massa polar penetra via Oceano Atlântico, ela ganha um pouco de umidade em sua trajetória, com os ventos soprando de sul e sudoeste.

Essa umidade permite uma certa tropicalização da massa polar, pois sempre há condensação do vapor d'água, liberando seu calor latente, aquecendo-a.

Portanto, as geadas mais severas são aquelas associadas a invasões de massa polar soprando de oeste-sudoeste.

Quanto aos fatores topoclimáticos, são aqueles referentes à topografia do terreno, ou seja, a configuração e exposição, afetando o acúmulo de ar frio.

Deve-se observar o relevo regional como o relevo local.

Muitas vezes, a situação local é agravada pelo relevo da região com um todo.

Vale de rio é o caminho natural também do ar frio mais denso, sendo as regiões de chapadas mais elevadas as fontes do ar frio.

Em situações de geada de radiação, locais mais baixos são os que estão sujeitos a maiores danos.

Terreno plano está sujeito à estagnação de ar frio, favorecendo a ocorrência de geada.

Em caso do terreno ser côncavo, sua configuração em forma de bacia facilita o acúmulo de ar frio, o que torna freqüente a ocorrência de geadas ninho de gead.

Terreno convexo tem geralmente menor freqüência de geadas, desde que não esteja circundado por terrenos mais elevados, pois essa configuração facilita o escoamento do ar frio para outras áreas.

A meia-encosta favorece o escoamento do ar frio formando a brisa catabática, isto é, ar mais denso que escorre morro abaixo, que pode afetar o caule das plantas, ocorrendo a geada de canela.

Terrenos com exposição voltada para a face sul recebem menos energia solar durante o inverno, sendo naturalmente mais frios, e consequentemente, mais sujeitos aos efeitos dos ventos predominantes de SE (frios).

De modo geral, a face norte é naturalmente mais quente, pois recebe mais energia durante o inverno, sendo também menos sujeita aos ventos frios, e às geadas no hemisfério Sul.

Referente aos fatores microclimáticos, são aqueles ligados à cobertura do terreno, pois cobertura com mato, mulch ou outro tipo de cobertura funciona como isolante térmico, impedindo a entrada de calor dos raios solares no solo.

Além dos fatores que favorecem a ocorrência de geadas, citados acima, deve-se levar em consideração outro fator muito importante na identificação da geada, o valor da temperatura mínima do ar.

Foi detectado em noites de geada, uma diferença entre a temperatura mínima do ar medida em abrigo meteorológico e a temperatura mínima do ar medida na relva.

Existe uma diferença significativa entre esses valores.

É importante notar esta diferença, uma vez que o objetivo deste trabalho é identificar a ocorrência de geadas a fim de fazer a prevenção e os valores registrados nos bancos de dados são os obtidos no abrigo.

Estudando valores observados de temperaturas em abrigo meteorológico e nos termômetros de relva, em noites de geadas ocorridas no Estado de São Paulo, verificaram que, estatisticamente, ocorre uma diferença de 5,7ºC em média, entre as medidas no abrigo e na relva e que essa diferença apresenta de forma significativa a variação térmica das folhas em noites de geadas.

Analisando a região de Santa Catarina chegou-se a resultados semelhantes ao Estado de São Paulo, variando a diferença média entre a temperatura mínima diária do ar medida no abrigo meteorológico e junto à relva, entre 2,1ºC a 4,8ºC entre as localidades analisadas, Campos Novos, Chapecó, Lages, Ponte Cerrada, São Joaquim, São Miguel D'Oeste, Urussanga e Videira.

Durante noites típicas de ocorrência de geadas, é comum observarem-se essas diferenças de temperatura da ordem de 5°C ou mais entre a superfície do solo e o abrigo termométrico.

A suscetibilidade das culturas agrícolas às baixas temperaturas varia muito de acordo com a espécie e o estádio de desenvolvimento fenológico.

Exemplos de temperatura letal, ao nível das folhas e abrigo, para diversas culturas anuais em diversos estádios de desenvolvimento fenológico.

Exemplos de temperatura letal, ao nível das folhas e abrigo, para diversas culturas perenes.

Temperatura letal º de culturas anuais em diferentes estádios fenológicos.

Temperatura letal º de algumas culturas perenes.

Condições favoráveis para ocorrer geada.

Entre elas estão, em noites de geadas, com ausência de ventos, o ar frio "escorre", acumulando-se no fundo de vales ou bacias.

As geadas de radiação ocorrem na ausência de ventos e sempre com céu claro.

O uso de cobertura morta ("mulch") favorece a formação de geadas de radiação em noites com temperaturas baixas.

Geadas fracas ocorrem em noites de céu claro, sem ventos e baixa umidade do ar, em baixadas, com temperatura do ar ao redor dos +4ºC ou +5ºC, geadas moderadas com temperatura entre +2ºC a +3ºC e severas, entre 0ºC e 2ºC, no abrigo meteorológico.

A ocorrência de geada na região Sudeste concentra-se nos meses de junho a agosto com casos excepcionais em maio e setembro.

O fenômeno de geadas nas Regiões Sul e Sudeste do Brasil é um assunto de muito interesse para meteorologistas ligados ao setor agropecuário, que procuram disseminar avisos alertando os agricultores sobre a aproximação de massas polares, causadoras de temperaturas mínimas extremas e ocorrência de geadas.

No Estado do Paraná, o IAPAR criou o sistema alerta geadas para elaborar previsões detalhadas, que são transferidas aos agricultores com antecedência mínima de 2horas, com o intuito de amenizar o problema que as geadas causam ao café.

As previsões são rapidamente difundidas para a EMATER, cooperativas, sindicatos rurais e meios de comunicação, que fazem com que a previsão chegue até o produtor em tempo hábil.

O potencial de retorno deste Sistema é de R$ 50 a 60 milhões por ano em economia de novos plantios.

A margem de acerto das previsões tem sido de 100%, dando total segurança ao produtor.

No inverno de 2000, todas as geadas ocorridas foram previstas, possibilitando que muitos agricultores evitassem prejuízos em viveiros e plantios recentes.

As atividades desenvolvidas neste programa de previsão de geadas do IAPAR consistem basicamente no desempenho da rotina operacional de um centro de previsão do tempo, contudo, sempre adotando uma operação conjunta com o Sistema Meteorológico do Paraná, onde são elaboradas as previsões.

Para elaboração das previsões, são utilizadas imagens do satélite GOES, modelos regionais de temperatura, pressão, precipitação, umidade relativa e vento para o Estado, com informações das 00 e 1UTC (horário padrão) e projeção de 6 em 6 horas, além de modelos de previsão de temperatura mínima para 24, 48 e 7horas.

Também foram utilizados dados de superfície, coletados na rede de estações meteorológicas do SIMEPAR e da rede do IAPAR, além de modelos globais disponíveis para a análise nos horários das 00 e 1UTC, com projeções de um até seis dias.

As previsões são realizadas diariamente pelo SIMEPAR e IAPAR.

Quando se observa um sistema que possa atingir o estado começa-se a monitorar a sua trajetória e potencial para causar geadas.

Com três a quatro dias de antecedência, um grupo de pesquisadores do IAPAR, que trabalham com a cultura do café, se reúne com os meteorologistas para uma análise mais profunda e, havendo risco de geadas, são divulgados aos meios de comunicação.

Discutiu-se as evidências sobre a ocorrência do fenômeno de geada, que poderiam ser detectadas com até dias de antecedência utilizando imagens de satélites.

Isso foi possível analisando a intensidade da massa de ar polar que estivesse penetrando no sul da América do Sul.

À medida que esta massa fria se deslocasse na direção do sul do Brasil, os agricultores seriam informados.

Um modelo estatístico foi utilizado para gerar a distribuição da temperatura.

Isto foi feito correlacionando-se os níveis de cinza temperatur transmitidos pelo satélite, com os valores previstos pelo modelo físico.

Esta rotina poderia ser reiniciada e atualizada periodicamente, de modo que a horas antes da ocorrência de geada, os agricultores, alertados, acionariam os esquemas de proteção.

Como importante precursor destacou-se a configuração de ondas longas, observada em altos níveis, deslocando-se lentamente no Pacífico e amplificando-se entre e 5 dias antes das geadas no Brasil.

Os resultados mostraram que quando uma onda longa do Pacífico amplifica-se, fornece um sinal da provável ocorrência de geadas no sul do Brasil com a dias de antecedência.

A expressão deficiência hídrica, ou a falta de água no solo, designa uma situação na qual as precipitações exibem valores inferiores aos da evaporação e a transpiração das plantas.

Para verificar a deficiência hídrica no solo é necessário o cálculo do balanço hídrico.

O balanço hídrico é um método para calcular a disponibilidade de água no solo.

Indica a contabilização da água do solo, representando o balanço entre o que entrou e o que saiu de água.

O cálculo do balanço hídrico pode ser feito pelos métodos de Thornthwaite, desenvolvido em 1948, e de Thornthwaite-Mather, desenvolvido em 1955.

O balanço hídrico, desenvolvido por Thornthwaite, em 1948, considera que a água do solo é igualmente disponível desde a capacidade de campo até o ponto de murcha permanente.

Isto significa dizer que a evapotranspiração ocorre potencialmente enquanto o armazenamento de água no solo não for nulo.

Sob armazenamento nulo, ocorre deficiência de água no solo, caracterizada como a água que falta para que a evapotranspiração real ocorra potencialmente.

O balanço hídrico, desenvolvido por Thornthwaite-Mather, permite determinar a variação do armazenamento de água no solo.

Considera que a disponibilidade da água no solo decresce com a diminuição do armazenamento, o que é levado em conta no cálculo da evapotranspiração real.

Os resultados do balanço hídrico indicam excedente hídrico ou deficiência hídrica.

Ocorre excedente de água sempre que a precipitação for superior à quantidade necessária para alimentar a evapotranspiração potencial e completar o armazenamento de água no solo.

A deficiência hídrica aparece sempre que o solo não conseguir suplementar a precipitação no atendimento da evapotranspiração potencial.

Na elaboração do balanço hídrico, o primeiro passo é a estimativa da CAD Capacidade de Água Disponível no solo, ou seja, a máxima quantidade de água, utilizável pelas plantas, que pode ser armazenada na sua zona radicular.

É a lâmina de água correspondente ao intervalo de umidade do solo entre a capacidade de campo e o ponto de murcha permanente.

Pode-se adotar valores de CAD entre 25 e 50 mm, para hortaliças.

Entre 75 e 100 mm, para culturas anuais.

Entre 100 e 125 mm, para culturas perenes e entre 150 e 300 mm, para espécies florestais.

No modelo do balanço hídrico climatológico utilizado por Thornthwaite-Mather são definidos alguns valores importantes para a contabilização da água no solo.

Entre eles estão, precipitação (P), evapotranspiração potencial, a diferença entre esses dois parâmetros (P-ETP), a negativa acumulada (NEG), o armazenamento de água no solo (ARM), a alteração no armazenamento (ALT), a evapotranspiração real, obtendo assim o excedente hídrico e a deficiência hídrica (DEF).

Vários trabalhos relacionados aos estudos de probabilidade de ocorrência de deficiência hídrica destacam-se a seguir.

Todos eles utilizam uma metodologia diferente da aplicada no presente trabalho.

Assad determinaram as probabilidades de ocorrência de veranicos para a região dos cerrados brasileiros, a partir da análise de dados diários de precipitação referentes a 100 estações pluviométricas, com séries históricas iguais ou superiores a 20 anos, sendo identificados os veranicos de períodos de 10, 15 e 20 dias, através da utilização de funções de distribuição de Gumbell & Weibull.

Pelos resultados, pode-se notar que na maioria das estações estudadas se observou alta freqüência de veranicos no mês de janeiro e os estados onde se observa a maior freqüência de veranicos de duração máxima de 20 dias são Piauí, Bahia e Minas Gerais (norte de Minas).

Com base também em um balanço hídrico climatológico diário seqüencial, Fietz estudaram a probabilidade de ocorrência de déficit hídrico na região de Dourados-MS.

O estudo baseou-se em dados diários de um período de aproximadamente 20 anos.

Os atributos utilizados foram a precipitação e os elementos meteorológicos utilizados na estimativa da evapotranspiração (temperatura e umidade relativa do ar, número de horas de brilho solar e velocidade do vento).

A evapotranspiração de referência foi estimada pelo método FAO Penman-Montheith, o mesmo utilizado neste trabalho.

Com base na análise dos dados, verificou-se que as menores e maiores probabilidades de déficit hídrico ocorrem de abril a julho e de agosto a setembro, respectivamente, enquanto de outubro a janeiro também podem ocorrer altos índices de déficit hídrico.

Viana estudaram a probabilidade de ocorrência de períodos secos e chuvosos para o município de Pentecoste-CE, a partir de uma série de 2anos de dados diários de precipitação.

As probabilidades de ocorrência foram estimadas através da cadeia de Markov.

A probabilidade de ocorrerem dias com déficit hídrico foi sempre superior a de dias chuvosos.

As maiores probabilidades de ocorrerem dias com déficit hídrico foram registradas no primeiro decêndio de janeiro, primeiro de fevereiro e segundo e terceiro de maio.

A maior probabilidade de ocorrência de dias chuvosos foi registrada no terceiro decêndio de março.

A probabilidade de ocorrência de quatro dias consecutivos chuvosos foi muito baixa, sendo a maior no terceiro decêndio de fevereiro.

Freitas e Grimm estudaram a probabilidade de ocorrência de veranicos no Estado do Paraná.

Foram utilizados dados de precipitação e evapotranspiração potencial diários para 20 estações meteorológicas distribuídas por todo o estado.

Determinou-se a diferença, dia a dia, entre precipitação e evapotranspiração, considerando-se dias em que a precipitação é igual ou ultrapassa a evapotranspiração como dias úmidos.

Em caso contrário, os dias foram considerados secos, independendo do valor de precipitação.

Foram calculadas as probabilidades de ocorrências de dias secos consecutivos após cada dia do ano.

Segundo os resultados alcançados, a probabilidade de ocorrência de períodos secos com mais de 30 dias foi muito pequena e está praticamente presente apenas no período de inverno.

No intervalo de 5 a 15 dias, as estações localizadas no norte do Estado, apresentaram altas probabilidades com máximos variando entre os dias 140 a 260.

A região sudoeste apresentou probabilidades inferiores a 50% em todas as estações, com uma distribuição mais uniforme ao longo do ano.

Para as estações do litoral, as probabilidades atingiram máximos durante o inverno, mas não ultrapassaram os 50%, decrescendo aproximadamente como uma distribuição gaussiana para as outras épocas do ano.

Para as outras regiões, verificou-se que o risco de ocorrência de veranicos também foi maior durante o inverno e bem distribuída no restante do ano, diminuindo no sentido norte-sul.

Vasconcellos analisaram um modelo climatológico para previsão de deficiências hídricas, induzidas nas culturas do algodão e do milho para os solos Latossolo Vermelho Escuro(LVE) e Latossolo Roxo (LR), representativos da região de Jaboticabal, SP.

Os dados foram obtidos a partir dos registros pluviométricos disponíveis na UNESP-Jaboticabal-SP, que tratam de 2anos de observação da estação.

Foi feita a adequação de um modelo para caracterização da freqüência de dias secos com diferentes durações, para as duas culturas da região.

Definiu-se "dias secos" como aqueles em que o armazenamento de água no solo, de acordo com o balanço hídrico, igualou-se ou ficou aquém de um certo valor crítico condicionado pela demanda atmosférica.

O valor do armazenamento crítico foi estabelecido considerando-se características físicas do solo, profundidade efetiva do sistema radicular e evapotranspiração.

Os resultados mostram que a ocorrência de períodos curtos, de 5 dias, com deficiência hídrica no solo é mais provável do que períodos mais extensos, de 10 a 20 dias, independentemente do tipo de solo ou cultura, principalmente nas estações primavera e verão.

Independentemente do tipo de solo estudado, a freqüência relativa de períodos de deficiência hídrica, maiores que 10 dias, fica abaixo de 35% durante todo o ano.

Assis analisou a ocorrência de períodos com ou sem chuva em Pelotas-RS.

Foi feita uma comparação entre vários modelos para descrever a ocorrência de chuva, entre eles as distribuições de probabilidade geométrica, logarítmica e binomial negativa truncada utilizados para modelar as seqüências de dias com ou sem chuva.

Os dados analisados pertencem a uma série com 95 anos de observação.

Foram determinadas as seqüências de dias com chuva ou sem chuv iniciadas nos 31 dias seguintes aos dias 1, 11 e 21 de cada mês e deste modo o ano foi dividido em 36 períodos.

Os resultados mostraram que a distribuição binomial negativa truncada e a distribuição geométrica são adequadas para descrever tanto a ocorrência de dias chuvosos quanto a de dias sem chuva.

A descoberta de conhecimento em bancos de dados é uma tecnologia para a descoberta eficiente de conhecimento em grandes bases de dados.

Utiliza-se de algoritmos de aprendizado de máquina para conseguir extrair conhecimento em grandes quantidades de dados.

O aprendizado de máquina (AM) é o campo da Inteligência Artificial que pode ser entendido como um conjunto de métodos, técnicas e ferramentas próprias para aquisição automatizada de conhecimento a partir de um conjunto de dados, melhorando seu desempenho por meio da experiência.

O sistema de aprendizado de máquina pode ser classificado de duas maneiras, Supervisionado.

Não-supervisionado.

O sistema supervisionado diz que o algoritmo de aprendizado (indutor) recebe um conjunto de exemplos de treinamento para os quais os rótulos da classe associada são conhecidos.

Cada exemplo (instânci é descrito por um vetor de valores (atributos) e pelo rótulo da classe associada.

O objetivo do indutor é construir um classificador que possa determinar corretamente a classe de novos exemplos ainda não rotulados.

Para rótulos de classe discretos, esse problema é chamado de classificação e para valores contínuos, de regressão.

No aprendizado não-supervisionado, o indutor analisa os exemplos fornecidos e tenta determinar se alguns deles podem ser agrupados de alguma maneira, formando agrupamentos ou "clusters".

Após a determinação dos agrupamentos, em geral, é realizada uma análise para determinar o que cada agrupamento significa no contexto do problema analisado.

Na aplicação deste projeto foi trabalhado o aprendizado de máquina supervisionado, em que um conjunto de exemplos de treinamento rotulados da classe já é conhecido.

Uma forma simplificada da classificação dos sistemas de AM é apresentada.

Classificação dos sistemas de aprendizado de máquina.

Existem diversos paradigmas de aprendizado de máquina, entre eles, os sistemas simbólicos, estatísticos, baseado em exemplos, conexionista e genético.

Os sistemas de aprendizado simbólicos buscam aprender construindo representações simbólicas de um conceito através da análise de exemplos e contra-exemplos desse conceito.

Atualmente, entre as representações simbólicas mais estudadas estão as árvores de decisão.

As árvores de decisão utilizam um tipo de algoritmo de aprendizado de máquina baseado na abordagem de dividir para conquistar.

Árvores de decisão ou de classificação são técnicas de indução usadas para descobrir regras de classificação para um atributo a partir da subdivisão sistemática dos dados contidos no repositório que está sendo analisado.

Em uma árvore de decisão, um problema complexo é decomposto em subproblemas mais simples, tornando o problema mais fácil de ser analisado.

Entre as vantagens encontradas em árvores de decisões podem-se destacar pouco tempo de processamento utilizado, a facilidade de compreensão do modelo, bem como identificar os atributos chaves no processo, e expressar facilmente as regras como instruções lógicas aplicadas diretamente aos novos registros.

Os sistemas estatísticos tendem a focar tarefas em que todos os atributos têm valores contínuos ou ordinais.

Muitos deles também são paramétricos, assumindo alguma forma de modelo, e então encontrando valores apropriados para os parâmetros do modelo a partir de dados.

Por exemplo, um classificador linear assume que classes podem ser expressas como combinação linear dos valores dos atributos, e então procurar uma combinação linear particular que fornece a melhor aproximação sobre o conjunto de dados.

Os classificadores estatísticos freqüentemente assumem que valores de atributos estão normalmente distribuídos, e então usam os dados fornecidos para determinar média, variância e co-variância da distribuição.

O paradigma baseado em exemplos assume que uma forma de classificar um caso é lembrar de um caso similar cuja classe é conhecida e assumir que o novo caso terá a mesma classe.

Esta filosofia exemplifica os sistemas instance-based, que classificam casos nunca vistos através de casos similares conhecidos.

A medida de similaridade para os casos onde todos os atributos são contínuos pode ser calculada por meio de uma distância entre esses atributos.

O paradigma conexionista utiliza Redes Neurais Artificiais (RNAs) que são modelos computacionais inspirados no sistema nervoso biológico, cujo funcionamento é semelhante a alguns procedimentos humanos, ou seja, aprendem pela experiência, generalizam exemplos por meio de outros e abstraem características.

De maneira geral, pode-se definir uma RNA como um sistema constituído por elementos de processamento interconectados, chamados de neurônios, os quais estão dispostos em camadas, uma camada de entrada, uma ou mais intermediárias e uma de saída.

São responsáveis pela não-linearidade da rede, através do processo interno de certas funções matemáticas.

Essas RNAs possuem alguma forma de regra de aprendizagem que é responsável pela modificação dos pesos sinápticos a cada ciclo de iteração, de acordo com os exemplos que lhe são apresentados.

Assim, pode-se dizer que as RNAs aprendem a partir de exemplos.

O sistema genético de aprendizado é derivado do modelo evolucionário de aprendizado.

Um classificador genético consiste de uma população de elementos de classificação que competem para fazer a predição.

Elementos que possuem uma performance fraca são descartados, enquanto os elementos mais fortes proliferam, produzindo variações de si mesmos.

Este paradigma possui uma analogia direta com a teoria de Darwin, onde sobrevivem os mais bem adaptados ao ambiente.

Um algoritmo genético é um procedimento iterativo que mantém uma população de indivíduos, cada um dos quais é um candidato à solução de algum problema específico.

A cada iteração (denominada geração), os indivíduos da população atual são avaliados quanto à sua aptidão para a solução do problema.

Com base nessas avaliações, aplicam-se alguns operadores genéticos aos indivíduos, formando-se uma nova população, que substituirá a população atual.

Isto é feito de modo que, quanto maior a aptidão de um indivíduo da população atual, maior a sua influência na formação dos indivíduos da nova geração.

Assim, com o passar do tempo, a seleção natural tende a fazer com que a população seja formada por indivíduos cada vez melhores soluções cada vez mais próximas da solução ótima para o problem.

Como critério de parada do algoritmo genético, geralmente define-se um limite no número de gerações.

Dentre os indivíduos da última geração, aqueles mais aptos representam a melhor solução encontrada pelo algoritmo.

Pode-se também especificar que o algoritmo encerrará quando for gerado algum indivíduo que satisfaça alguma condição mínima de aptidão.

Aprendizado de máquina é uma área de pesquisa que tem alcançado muitos resultados positivos a ser aplicado em descoberta de conhecimento em banco de dados.

Existem algumas definições importantes para o entendimento do processo de aprendizado de máquina supervisionado.

Entre eles destacam-se o indutor, o conjunto de exemplos, o atributo, a classe, o conhecimento de domínio, a distribuição das classes, a matriz de confusão e as regras geradas como resultados.

A respeito de matriz de confusão, será mais detalhada uma vez que o seu significado será de extrema importância para entendimento dos resultados.

Matriz de confusão consiste em oferecer uma medida efetiva do modelo de classificação, ao mostrar o número de classificações corretas versus as classificações preditas para cada classe, sobre um conjunto de exemplos T.

Como mostrado, os resultados são totalizados em duas dimensões, classes verdadeiras e classes preditas.

Cada elemento da matriz M (C,C), representa o número de exemplos de T que realmente pertencem à classe C, mas foram classificados como sendo da classes C.

Matriz de confusão de um classificador.

O número de acertos para cada classe se localiza na diagonal principal M(C,C) da matriz.

Os demais elementos M(C,C), para i j, representam erros na classificação.

A matriz de confusão de um classificador ideal possui todos esses elementos iguais a zero uma vez que ele não comete erros.

Para um problema de duas classes, os dois erros possíveis são denominados falso positivo (Fp) e falso negativo (Fn).

Para os exemplos positivos classificados corretamente, denomina-se verdadeiro positivo (Tp) e para os exemplos negativos classificados corretamente, denomina-se verdadeiro negativo (Tn).

Matriz de confusão para um problema de duas classes.

Matriz de confusão de um classificador para um problema de duas classes.

Partindo da definição de KDD, descrito como um processo geral de descoberta de conhecimento composto por várias etapas, incluindo, preparação dos dados, mineração de dados e avaliação do conhecimento, faz-se necessário uma distinção mais precisa entre dados, informação e conhecimento.

Define-se dado como uma seqüência de símbolos.

Isto significa que os dados podem ser totalmente descritos por meio de representações formais, estruturais.

Sendo ainda quantificados ou quantificáveis, eles podem obviamente ser armazenados em um computador e processados por ele.

Dado pode ser definido como um conjunto conhecido de registros, qualitativos ou quantitativos, que, organizado, agrupado, categorizado e padronizado adequadamente, transforma-se em informação.

No mundo científico, dados representam observações coletadas sobre algum fenômeno em estudo e o desafio é como explicar melhor o que foi observado.

Nos negócios, os dados capturam informações sobre os mercados, concorrentes e clientes.

Em sistemas de manufatura, dados capturam oportunidades de melhorar o desempenho, otimização e meios de melhorar processos e resolver problemas.

Informação pode ser definida como dados organizados, geralmente em forma de Tabelas, que pode ser obtida diretamente a partir do banco de dados através de alguma ferramenta desenvolvida especificamente para este fim e com o auxílio de um gerenciador de banco de dados.

Comenta-se a distinção fundamental entre dado e informação, o primeiro é puramente sintático e a segunda contém necessariamente semântica (implícita na palavra "significado" usada em sua caracterização).

O termo informação é conceituado por vários autores como "aquilo que leva à compreensão. O que constitui informação para uma pessoa pode não passar de dados para outra".

Da mesma maneira, conceitua-se informação como sendo "dados organizados de modo significativo, sendo subsídio útil à tomada de decisão".

O conceito de conhecimento possui um sentido mais complexo que o de informação.

Conhecer é um processo de compreender e internalizar as informações recebidas, possivelmente combinando-as de forma a gerar mais conhecimento.

O "conhecimento é a informação mais valiosa, e é valiosa precisamente porque alguém deu à informação um contexto, um significado, uma interpretação".

O conhecimento pode, então, ser considerado como a informação processada pelos indivíduos.

O valor agregado à informação depende dos conhecimentos anteriores desses indivíduos.

Assim sendo, adquirimos conhecimento por meio do uso da informação nas nossas ações.

Desta forma, o conhecimento não pode ser desvinculado do indivíduo.

Ele está estritamente relacionado com a percepção do mesmo, que codifica, decodifica, distorce e usa a informação de acordo com suas características pessoais, ou seja, de acordo com seus modelos mentais.

O processo de descoberta de conhecimento KD envolve os dados, os quais representam a "matéria-prima" bruta, a partir dos quais as operações lógicas criam informações e, finalmente, estas últimas são interpretadas para gerar conhecimento.

Esquema do significado de dados, informação e conhecimento, envolvidos pelo processo KDD.

Significado de dados, informação e conhecimento.

Ao se considerar a inter-relação entre os três elementos e efetuar a análise tendo como foco o presente estudo, podemos inferir que os dados por si só não significam conhecimento útil para a tomada de decisão, constituindo-se apenas o início do processo.

O grande desafio dos tomadores de decisão é o de transformar dados em informação e informação em conhecimento, minimizando as interferências individuais nesse processo de transformação, sendo este o objetivo do processo de descoberta de conhecimento em banco de dados.

A descoberta de conhecimento em bancos de dados Knowledge Discovery in Databases KD é uma tecnologia que possui técnicas poderosas para a descoberta eficiente de conhecimento valioso em uma grande coleção de dados, visando o auxílio no suporte à decisão.

KDD é o processo não trivial de identificação, a partir de dados, de padrões que sejam válidos, novos, potencialmente úteis e compreensíveis.

KDD é descrito como um processo geral de descoberta de conhecimento composto por várias etapas, incluindo, preparação dos dados, mineração de informação e avaliação do conhecimento.

O termo não trivial significa que envolve algum mecanismo de busca ou inferência.

Os padrões descobertos devem ser válidos diante de novos dados com algum grau de certeza.

Estes padrões podem ser considerados conhecimento, dependendo de sua natureza.

Os padrões devem ser novos, compreensíveis e úteis, ou seja, deverão trazer algum benefício novo que possa ser compreendido rapidamente pelo usuário para tomada de decisão.

KDD é uma área interdisciplinar específica que surgiu em resposta à necessidade de novas abordagens e soluções para viabilizar a análise de grandes bancos de dados.

O termo KDD é empregado para descrever todo o processo de extração de conhecimento de um conjunto de dados, enquanto que o termo mineração de dados M, ou data mining, refere-se a uma das etapas deste processo.

A relação existente entre KDD e MD pode ser visualizada graficamente.

Interligação entre KDD e Data Mining.

Mineração de dados é a interação entre o especialista do domínio, o analista e o usuário final.

O especialista do domínio deve possuir amplo conhecimento sobre o assunto da aplicação e deve fornecer apoio para a execução do processo.

O analista é o especialista no processo de "extração do conhecimento" e responsável por sua execução.

Ele deve conhecer profundamente as etapas que compõem o processo.

O usuário final representa aqueles que utilizam o conhecimento extraído no processo para auxiliá-lo em um processo de tomada de decisão.

O processo de mineração de dados envolve várias etapas complexas, que devem ser executadas corretamente, pois cada etapa é fundamental para que os objetivos estabelecidos e o sucesso completo da aplicação sejam alcançados.

O processo de mineração é tanto iterativo quanto interativo.

A iteratividade tem sua natureza justificada pelo fato de que o conhecimento descoberto apresentado ao analista pode ser usado da seguinte forma, como base para a medida de avaliação a ser aprimorada como base para a mineração a ser refinada.

Novos dados podem ser selecionados ou transformados ou, ainda, novas fontes de dados podem ser integradas para adquirir resultados diferentes e mais apropriados.

Portanto, o processo pode ser realizado em etapas seqüenciais de maneira que seja possível sua volta às etapas anteriores, criando laços de ligação entre elas.

O analista é também o responsável pela tomada de várias decisões, como na modelagem das informações, o tipo de algoritmo a ser usado e quais objetivos serão seguidos na busca do conhecimento, garantindo-se assim a sua natureza interativa.

Basicamente, mineração de dados se preocupa com a análise dos dados e com o uso de técnicas responsáveis por achar padrões e regularidades no conjunto de dados.

É o computador que é o responsável por achar os padrões identificando as regras subjacentes e as características nos dados.

A idéia é que é possível encontrar "ouro" em lugares inesperados, tal como os softwares de data mining extraem padrões não previamente encontrados ou tão óbvios que ninguém os notou antes.

Processo típico de extração de conhecimento em base de dados.

Visão geral das etapas que compõem o processo de Descoberta de Conhecimento em Banco de Dados.

O processo de descoberta de conhecimento possui três passos.

Inicialmente é preciso selecionar os tipos de dados que serão usados pelo algoritmo de mineração.

Dados crus geralmente são variados, não estão organizados e nem todos são necessários para a mineração.

Um grande esforço é necessário para se coletar uma boa quantidade de dados e transportá-los para um lugar onde se possa minerá-los.

O primeiro passo é pré-processar os dados para aprontá-los para a análise.

Usualmente os dados têm que ser formatados, amostrados, adaptados e, algumas vezes, transformados para que possam ser usados pelo algoritmo de mineração.

Ocorre, então, o desenvolvimento do entendimento do domínio da aplicação, avaliação do hardware e software disponíveis, seleção, limpeza e transformação dos dados.

Após o pré-processamento, os dados estão prontos para serem minerados por um algoritmo.

É a fase conhecida como a própria mineração de dados.

É definida a escolha da tarefa e das técnicas a serem utilizadas, identificação da ferramenta que satisfaça a essas condições e aplicação desta aos dados.

Este passo pode envolver técnicas muito diversas e a informação descoberta é usada principalmente para construção de modelos, extração automática de padrões e exploração visual de dados.

O último passo do processo de mineração de dados é assimilar a informação minerada, chamado pós-processamento.

É a interpretação dos resultados e a incorporação do conhecimento adquirido.

No caso da construção de modelos, este passo consiste em avaliar a robustez e efetividade dos modelos produzidos.

No caso da extração de padrões e exploração visual de dados, este passo consiste em tentar interpretar a informação extraída.

Além das três fases definidas existem ainda duas fases no processo de mineração de dados, uma anterior ao processo, que se refere ao conhecimento do domínio e identificação do problema, e uma fase posterior ao processo, que se refere à utilização do conhecimento obtido.A qualidade do conhecimento descoberto no final é dependente da qualidade do dado, do pré-processamento, do algoritmo de mineração e do processo de assimilação.

Mais do que isso, a qualidade do conhecimento extraído é altamente dependente e amplamente determinada pela qualidade dos dados fornecidos como entrada.

A maioria dos métodos utilizados nas áreas de aprendizado de máquina (AM) e descoberta de conhecimento em banco de dados KD induz conhecimento estritamente a partir dos dados, sem utilizar outro conhecimento externo.

Se os problemas presentes nos dados forem identificados e tratados antes dos dados serem fornecidos a um algoritmo de extração de conhecimento, então espera-se que o conhecimento extraído seja mais representativo e mais preditivo e consequentemente com qualidade.

A qualidade dos dados é comprometida quando une fontes heterogêneas de dados, como por exemplo, redundância de atributos ou quando o banco de dados é formado de uma única fonte contendo dados "sujos".

É necessário fazer a limpeza dos dados, como extração, transformação e integração dos dados em uma única base.

Os dados disponíveis para aplicação dos algoritmos de extração de padrões podem apresentar problemas advindos do processo de coleta.

Estes problemas podem ser erros de digitação ou erro na leitura dos dados pelos sensores.

Como o resultado do processo de extração possivelmente será utilizado em um processo de tomada de decisão, a qualidade dos dados é um fator extremamente importante.

Por isso, técnicas de limpeza, como tratamento para dados com ruídos, outliers, desbalanceamento de classes e redundantes, devem ser aplicadas aos dados a fim de garantir sua qualidade.

No processo KDD existe uma fase que tem como finalidade melhorar a qualidade dos dados.

Essa fase, conhecida como pré-processamento dos dados, tem como objetivo principal a identificação e remoção de problemas presentes nos dados antes que os métodos de extração de conhecimento sejam aplicados.

Um dataset, ou um conjunto de dados, é uma coleção de objetos e seus atributos.

Um atributo é uma propriedade ou característica de um objeto e também pode ser conhecido como variável, campo, parâmetro ou "feature".

São exemplos de atributos, temperatura, precipitação e fases da Lua.

Uma coleção de atributos descreve um objeto.

Objeto é também conhecido como registro, observação, ponto, entidade ou instância.

A disposição dos atributos e dos objetos em um exemplo de um dataset é mostrada.

Exemplos de objetos (linhas) e atributos (colunas) em um conjunto de dados.

Normalmente, existem dois tipos de atributos, nominal, quando não existe uma ordem entre os valores (por exemplo, direção do vento, El Nino) e contínuo, quando existe uma ordem linear nos valores (por exemplo, temperatura do ar).

Independente do tipo do atributo, um conjunto de atributos unido com um conjunto de instâncias representa o dataset.

Diversos tipos de dataset podem existir a fim de representar o conjunto de dados.

Ente eles, existem dataset baseados em registros, como matriz de dados, coleção de documentos e dados transacionais.

Dataset baseados em Gráficos.

Baseados em uma ordem, como dado espacial, dado temporal, dado seqüencial e dado com seqüência genética.

No mundo real, geralmente os dados são inconsistentes, incompletos, apresentam ruídos e outliers, são poluídos, redundantes e a maioria dos conjuntos de dados possui classes desbalanceadas.

A seguir, são descritos cada um dos problemas encontrados na natureza.

Inconsistências podem ocorrer quando dados diferentes são representados pelo mesmo rótulo, ou quando o mesmo dado é representado por rótulos diferentes.

Um exemplo de inconsistência ocorre quando um atributo assume diferentes valores, os quais representam, na verdade, uma mesma informação.

Dados inconsistentes são dados com informações desatualizadas oriundas de erros no momento de introdução dos dados.

Dados incompletos são aqueles que possuem ausência de valores de atributos, ausência de atributos de interesse, ou dados com valores agregados.

São exemplos de valores faltantes, quando a informação não é coletada como pessoas que não querem fornecer suas idades.

Dados diários de temperatura com alguns valores sem preenchimento.

Necessidade do atributo da temperatura média, mas com presença apenas do atributo da temperatura máxima e ausência do atributo temperatura mínima.

Os dados com ruídos são aqueles que têm erros ou com valores extremos, os chamados outliers.

São objetos com características diferentes da maioria dos outros objetos em um conjunto de dados, ou seja, que não seguem o mesmo padrão dos demais.

É necessário identificar esses valores extremos, uma vez que esses dados podem distorcer os resultados obtidos.

Porém, valores extremos nem sempre apresentam riscos ao conjunto de dados.

Os valores extremos, conhecidos como outliers, precisam ser tratados com cuidado, uma vez que casos que possuem valores extremos que, a princípio, parecem ser dados incorretos, podem ser dados válidos e ainda representar a informação mais valiosa e interessante, pela qual o analista está procurando.

Dependendo sempre do objetivo do trabalho, mesmo que se tenha um objeto diferente dos demais, às vezes é necessário que ele faça parte do conjunto de dados.

Por exemplo, no caso de detecção de ocorrência de geada.

Detectar geada é função do atributo temperatura mínima menor ou igual a um determinado valor.

No atributo temperatura mínima, existem pouquíssimas ocorrências com temperatura menor que aquele valor, o que seria detectado como um outlier e na verdade é o que caracteriza a ocorrência de geada.

Entende-se por poluição nos dados a presença de dados distorcidos, os quais não representam os valores verdadeiros.

Uma possível fonte de poluição de dados é a tentativa, por parte dos usuários do sistema que coletam os dados, de utilizar esse sistema além da sua funcionalidade original, como exemplo, armazenar informação no atributo errado.

Outra fonte que pode gerar poluição nos dados é a resistência humana em entrar com os dados corretamente.

Enquanto campos em um banco de dados podem ser incluídos para capturar informações valiosas, esses campos podem ser deixados em branco, incompletos ou simplesmente com informações incorretas.

Redundância ocorre quando uma informação essencialmente idêntica é armazenada em diversos atributos, ou seja, possuir atributos, em uma mesma Tabela, com informações iguais.

Podem ocorrer quando o dataset pode incluir objetos que são duplicados ou quase duplicados de outros.

Esse problema geralmente ocorre quando os dados são integrados de fontes heterogêneas.

O maior prejuízo causado pela redundância para a maioria dos algoritmos de mineração é um aumento no tempo de processamento.

Entretanto, alguns métodos são especialmente sensíveis ao número de atributos, e variáveis redundantes podem comprometer seu desempenho.

Os métodos de seleção de atributos podem ajudar a identificar e remover os atributos redundantes.

Conjuntos de dados com classes desbalanceadas são aqueles que possuem uma grande diferença entre o número de exemplos pertencentes a cada classe.

A maioria dos algoritmos de aprendizado de máquina tem dificuldades em "aprender" com precisão os exemplos da classe minoritária.

Nestas condições, modelos de classificação que são otimizados em relação à precisão têm tendência de criar modelos triviais, que quase sempre predizem a classe majoritária.

Um exemplo de classes desbalanceadas é o conjunto de dados de geada.

Apenas 3% dos dados pertenciam à classe sim ocorrência de gead e os outros 97% pertenciam à classe referente à não ocorrência de geada.

Se os dados do conjunto de dados apresentam características que irão atrapalhar o conhecimento extraído, é necessário atribuir qualidade aos dados do dataset.

Sem qualidade de dados, não há qualidade nos resultados da mineração de dados.

Identificar e entender o problema faz parte do processo KDD.

Entender o domínio dos dados é naturalmente um pré-requisito para extrair algo útil, o usuário do sistema final deve ter algum grau de entendimento sobre a área de aplicação antes de qualquer informação valiosa ser obtida.

É nesta fase que é feita a compreensão do domínio da aplicação de onde o conhecimento será extraído e são estabelecidos os objetivos e metas que serão alcançados no processo de extração de conhecimento em banco de dados.

Antes do início das tarefas do processo é imprescindível a realização de um estudo a fim de adquirir um conhecimento inicial do domínio.

O sucesso do processo de extração de conhecimento depende, em parte, da participação dos especialistas do domínio da aplicação no fornecimento de conhecimento sobre o domínio e apoio aos analistas em sua tarefa de encontrar os padrões.

O conhecimento do domínio irá auxiliar, principalmente, na etapa de pré-processamento, ajudando os analistas na escolha do melhor conjunto de dados para se realizar a extração dos padrões, saber quais valores são válidos para os atributos, os critérios de preferência entre os possíveis atributos, as restrições de relacionamento ou informações para geração de novos atributos.

Na fase da mineração propriamente dita, o conhecimento sobre o domínio pode auxiliar os analistas na escolha de um critério de preferência entre os modelos gerados, no ajuste dos parâmetros do processo de indução, ou mesmo na geração de um conhecimento inicial a ser fornecido como entrada do algoritmo de mineração para aumentar a eficiência no aprendizado dos conceitos e melhorar a precisão ou a compreensibilidade do modelo final.

Na fase de pós-processamento, o conhecimento extraído pelos algoritmos deve ser avaliado.

Alguns critérios de avaliação utilizam o conhecimento do especialista para saber, por exemplo, se o conhecimento extraído é interessante ao usuário.

Um especialista no domínio pode fornecer ao analista dados e informações sobre quais atributos são, na sua opinião, os mais relevantes para a criação do modelo.

Entretanto, este procedimento pode limitar a originalidade do conhecimento descoberto.

Sempre que possível, o analista de dados deve adicionar novos atributos e verificar a importância dessas variáveis no conhecimento gerado.

É importante também verificar se esses dados existem nos bancos de dados da instituição ou podem ser encontrados em fontes de dados externas.

As ações realizadas na fase de pré-processamento dos dados visam preparar os dados para que a fase seguinte, a fase de mineração de dados, seja mais efetiva.

Esta fase é um processo semi-automático, ou seja, entende-se que essa fase depende da capacidade da pessoa que a conduz em identificar os problemas presentes nos dados, além da natureza desses problemas, e utilizar os métodos mais apropriados para solucionar cada um dos problemas.

A preparação de dados exige uma dedicação de 60 a 80% do tempo envolvido num estudo de descoberta de conhecimento.

Comenta-se sobre a importância de cada fase do processo de mineração de dados e o tempo que cada fase implica na qualidade do conhecimento adquirido.

Divisão das etapas do processo de mineração de dados, mensurando o tempo necessário para o desenvolvimento e a importância de cada etapa.

As etapas iniciais representam uma importância de aproximadamente 80% no sucesso do trabalho de mineração de dados e equivalem a cerca de 20% do tempo dedicado.

Em contrapartida, a fase de pré-processamento deve ter uma dedicação maior em relação às outras fases, em torno de 60% do tempo necessário para garantir o sucesso no trabalho final.

A fase de pré-processamento inicia assim que os dados são coletados e organizados na forma de um conjunto de dados.

O pré-processamento é necessário, principalmente, devido à qualidade dos dados e consequentemente qualidade no conhecimento adquirido.

Em geral, os dados disponíveis para análise não estão em um formato adequado para a extração de conhecimento.

Além disso, em razão de limitações de memória ou tempo de processamento, muitas vezes não é possível a aplicação direta dos algoritmos de extração de padrões aos dados.

Dessa maneira, torna-se necessária a aplicação de métodos para tratamento, limpeza e redução do volume de dados antes de iniciar a etapa de extração de padrões.

É importante salientar que a execução das transformações deve ser guiada pelos objetivos do processo de extração a fim de que o conjunto de dados gerado apresente as características necessárias para que os objetivos sejam cumpridos.

Diversas transformações nos dados podem ser executadas na etapa de pré-processamento, entre elas, extração e integração, transformação e limpeza de dados.

Essas transformações são comentadas a seguir.

Na extração e integração, os dados disponíveis podem ser encontrados em diferentes fontes, como arquivos-texto, arquivos no formato de planilhas, banco de dados ou data warehouse.

Assim, é necessária a obtenção desses dados e sua unificação, formando uma única fonte de dados no formato atributo-valor que será utilizada como entrada para o algoritmo de extração de padrões.

Após a extração e integração dos dados, estes devem ser adequados para serem utilizados pelos algoritmos de extração de padrões.

Algumas transformações comuns que podem ser aplicadas aos dados são, substituição dos valores perdidos, construção de novos atributos, discretização dos dados, normalização de dados e transformação de dados qualitativos em quantitativos.

As transformações de dados são extremamente importantes em alguns domínios, como, por exemplo, em aplicações que envolvem séries temporais como predições no mercado financeiro.

Entre as técnicas para tratar os valores desconhecidos, destacam-se a substituição dos valores desconhecidos pela média do atributo (valores numéricos).

Pela moda do atributo (valores categóricos).

Pela média para observações pertencentes a uma mesma classe.

Preencher os valores faltantes por meio de uma regressão linear.

Usar o método KNN (k-Nearest Neighbor) vizinho mais próximo ou usar o valor mais provável que é baseado em inferência, como exemplo, usando uma árvore de decisão ou um modelo Bayesiano.

Construção de atributos é o processo de composição de atributos ditos primitivos, ou seja, atributos pertencentes ao conjunto de dados original, produzindo-se novos atributos possivelmente relevantes para a descrição de um conceito.

Os atributos podem ser considerados inadequados para a tarefa de aprendizado quando são fracamente ou indiretamente relevantes, condicionalmente relevantes ou medidos de modo inapropriado.

Se os atributos são inadequados, os algoritmos de aprendizado de máquina provavelmente criarão classificadores imprecisos ou complexos.

Atributos fracamente, indiretamente ou condicionalmente relevantes podem ser individualmente inadequados, entretanto, esses atributos podem ser convenientemente combinados gerando novos atributos que podem mostrar-se altamente representativos para a descrição de um conceito.

A construção de novos atributos utiliza o conhecimento do usuário ou do especialista no domínio para orientar sua composição.

Quando o conjunto de dados possui atributos quantitativos, alguns algoritmos possuem limitações em trabalhar e extrair algum conhecimento, pois tem a limitação de trabalhar somente com atributos qualitativos.

Discretizar significa transformar um atributo quantitativo em um atributo qualitativo, ou seja, transformá-lo em faixas de valores.

Métodos de discretização de atributos.

Significa transformar os valores dos atributos de seus intervalos originais para um intervalo específico.

Este tipo de transformação é valiosa para os métodos que utilizam da distância entre os atributos, como o K-vizinhos mais próximos ou em Redes Neurais, obtendo-se resultados melhores quando os valores dos atributos são pequenos.

Entretanto, normalização não é de grande utilidade para a maioria dos métodos que induzem representações simbólicas, tais como árvores de decisão e regras de decisão, uma vez que a normalização tende a diminuir a compreensibilidade do modelo gerado por tais algoritmos.

Existem algoritmos que não são capazes de manipular atributos qualitativos, sendo necessário converter os atributos qualitativos em quantitativos.

Dependendo de cada algoritmo, existem diversas formas de realizar esta operação.

De forma geral, atributos qualitativos podem ser transformados para números, porém esta transformação cria uma ordem nos valores do atributo que não é real.

A limpeza de dados é cara e é um trabalho extensivo, que exige o envolvimento humano e deve ser feito antes de criar uma grande base de dados.

Os problemas surgem de dados perdidos, dados errados e dados duplicados com inconsistências e heterogeneidades.

Essa execução minimiza problemas, eliminando consultas desnecessárias que seriam efetuadas pelo algoritmo minerador e que, possivelmente, afetariam o desempenho dos resultados.

A limpeza dos dados pode ser realizada utilizando o conhecimento do domínio.

Por exemplo, podem-se encontrar registros com valor inválido em algum atributo, granularidade incorreta ou exemplos errôneos.

Pode-se também efetuar alguma limpeza independente de domínio, remoção de dados redundantes e duplicados, remoção de ruído e tratamento de conjunto de exemplos não balanceados.

Entre os métodos que existem para identificar os outliers, destaca-se a análise de agrupamento (cluster).

Intuitivamente, os objetos que estão fora dos clusters, são outliers.

A análise de agrupamento determina em qual cluster o objeto se encaixa.

Objeto fora do cluster é identificado como outlier.

Ainda, para comparar atributos e verificar outliers faz-se uso da técnica de Box Plot.

O Box Plot (ou desenho esquemático) é uma análise gráfica que utiliza cinco medidas estatísticas, valor mínimo, valor máximo, mediana, primeiro e terceiro quartil da variável quantitativa.

É uma linha central mostrando a mediana, uma linha inferior mostrando o primeiro quartil (Q1 25º percentil), uma linha superior mostrando o terceiro quartil (Q75º percentil).

O gráfico Box Plot pode ser utilizado para fazer comparações entre várias distribuições.

Essa comparação é feita por meio de vários desenhos esquemáticos.

Representação esquemática do Box Plot.

Existem algumas formas para solucionar o problema de classes desbalanceadas em um conjunto de dados, ou seja, procurar por uma distribuição da classe que forneça um desempenho aceitável de classificação para a classe minoritária.

Existem vários métodos para lidar com classes desbalanceadas e, consequentemente, alterar a distribuição dessas classes de forma a tornar o conjunto de dados mais balanceado.

Entre todos, existem dois métodos básicos para balancear a distribuição das classes, remover exemplos das classes mais populosas e inserir exemplos nas classes menos populosas.

Na sua forma mais simples, essa adição ou remoção é feita de maneira aleatória, sendo chamados de métodos de over-sampling aleatório e under-sampling aleatório, respectivamente.

Por se tratar de métodos aleatórios, os métodos over-sampling e under-sampling aleatórios possuem limitações.

O under-sampling aleatório pode eliminar dados potencialmente úteis, e over-sampling aleatório pode aumentar as chances de ocorrer um overfitting, ou seja, superajustamento dos dados, uma vez que cópias exatas dos exemplos pertencentes à classe minoritária são duplicadas.

Como exemplo, em um modelo simbólico, como árvore de decisão, pode-se construir regras que são aparentemente gerais, mas que na verdade cobrem um único exemplo replicado.

Alguns trabalhos, citados nos próximos parágrafos, têm tentado superar as limitações desses métodos.

Utilizam heurísticas para selecionar os exemplos a serem removidos/acrescentados, cujo principal objetivo é tentar minimizar a quantidade de dados potencialmente úteis descartados.

Algumas pesquisas tentam transpor esses problemas para essas duas classes de métodos.

Combina-se os métodos de under-sampling e over-sampling e, ao invés de fazer over-sampling pela simples replicação de exemplos da classe minoritária, o faz pela interpolação de exemplos da classe minoritária que estão próximos.

Dessa forma, o overfitting é contornado e as fronteiras de decisão para a classe minoritária são estendidas no espaço de exemplos da classe majoritária.

Utiliza-se método de under-sampling para minimizar a quantidade de dados potencialmente úteis descartados.

Em todos os trabalhos, o objetivo principal é tentar minimizar a quantidade de dados potencialmente úteis descartados.

Para isso, os exemplos são divididos em quatro categorias, ruído, redundantes, próximos à borda e seguros.

Os ruídos são casos que, por algum motivo, foram rotulados incorretamente, isto é, eles estão do lado errado da borda de decisão.

Os redundantes são casos que podem ser representados por outros casos que estão presentes no conjunto de treinamento.

Os próximos da borda de decisão são casos poucos confiáveis, uma vez que uma pequena quantidade de ruído em um dos atributos pode mover esses exemplos para o lado errado da borda de decisão.

E os casos seguros são aqueles que não são ruídos, não estão excessivamente próximos à borda de decisão e, também, não estão muito distantes dela.

Os casos seguros são, a princípio, os melhores casos a serem retidos para o aprendizado.

Entre os métodos para balancear as classes podem-se citar Ligações Tomek, Regra do vizinho mais próxima condensada, Seleção unilateral, Regra de limpeza da vizinhança, SMOTE.

Mais detalhes sobre cada método podem ser explicado a seguir, Ligação Tomek, casos próximos à borda e ruído podem ser identificados por meio de ligações Tomek, e removidos do conjunto de dados.

Uma ligação Tomek pode ser definida da seguinte maneira, Sejam E e E dois exemplos de classes diferentes.

Seja d uma função de distância entre exemplos.

Um par de exemplos constitui uma ligação Tomek se não existe um exemplo E, tal que a distância d < d ou d < d.

Se dois exemplos formam uma ligação Tomek, então, ou Ei e Ej são exemplos próximos à borda de decisão, ou um desses exemplos é ruído.

No uso de ligações Tomek para o balanceamento de conjuntos de dados, apenas os exemplos da classe majoritária que possuem ligações Tomek são removidos.

Regra do vizinho mais próxima condensada CNN, Essa abordagem é geralmente conhecida como regra do vizinho mais próximo condensada CNN.

Parte dos casos redundantes pode ser removida por meio da identificação de um subconjunto consistente.

Um subconjunto Ê E é consistente com E se utilizando o algoritmo do vizinho-mais-próximo (NN) ele classifica corretamente os casos em E.

Um algoritmo para a geração de subconjunto consistente consiste nos seguintes passos, primeiro é selecionado aleatoriamente um exemplo da classe majoritária e todos os exemplos da classe minoritária, que são inseridos em Ê.

Após, usa-se o algoritmo NN sobre o conjunto Ê para classificar os exemplos em E.

Todo exemplo incorretamente classificado de E é movido para Ê.

A idéia por detrás desse algoritmo é remover exemplos que estão muito distantes da fronteira de decisão, uma vez que esses exemplos são geralmente considerados menos importantes no processo de aprendizagem.

Também é importante notar que esse algoritmo não encontra o conjunto consistente ótimo a partir de E.

Seleção unilateral, é um método que consiste da aplicação do algoritmo para a identificação de ligações Tomek seguido da aplicação do CNN.

Essa abordagem é conhecida como seleção unilateral.

Ligações Tomek são utilizadas para identificar exemplos da classe majoritária que se sobrepõem à classe minoritária e CNN é utilizado para remover exemplos muito distantes da fronteira de decisão.

O restante dos exemplos, isto é, exemplos da classe majoritária "seguros" e todos os exemplos da classe minoritária, são utilizados para a aprendizagem.

CNN mais as ligações Tomek, propõem a inversão dos passos da seleção unilateral.

Como a identificação de ligações Tomek é computacionalmente custosa, ela é computacionalmente mais efetiva se aplicada em um conjunto reduzido.

Regra de limpeza da vizinhança, o método da limpeza do vizinho NCL (Neighborhood Cleaning Rule) utiliza a regra do vizinho mais próximo ENN de Wilson para remover exemplos da classe majoritária.

ENN remove exemplos cuja classe difere da classe por, pelos menos, dos seus vizinhos mais próximos.

SMOTE,método que não replica os exemplos de treinamento.

Nesse trabalho, o método do over-sampling cria novos exemplos da classe minoritária por meio da interpolação de diversos exemplos dessa classe que se encontram próximos.

Esse método é chamado SMOTE.

SMOTE mais as Ligações Tomek, mesmo que os métodos de over-sampling possam balancear a distribuição de exemplos entre as classes, outros problemas presentes em conjuntos de dados desbalanceados não são resolvidos.

Frequentemente, os exemplos que compõem a classe minoritária não estão bem agrupados e pode haver uma grande sobreposição entre as classes.

Com o objetivo de melhorar a definição dos agrupamentos de dados no conjunto de treinamento, Batista propõem a aplicação do método para a identificação de ligações Tomek após a aplicação do método SMOTE.

Diferentemente da aplicação de ligações Tomek como método de under-sampling, nesse caso remove-se exemplos de ambas as classes.

SMOTE mais ENN, consiste na aplicação do método ENN após a aplicação do método SMOTE.

A motivação é similar à do método SMOTE + ligações Tomek.

Entretanto, ENN tende a remover mais exemplos do que as ligações Tomek, promovendo uma maior limpeza no conjunto de dados.

Similarmente ao método SMOTE mais as ligações Tomek, ENN é utilizado para remover exemplos de ambas as classes.

Em muitos casos, datasets possuem um número elevado de atributos e observações (objetos).

Deve-se levar em consideração que conjunto de dados com muitos atributos pode possuir atributos redundantes, ou seja, variáveis altamente correlacionadas e consequentemente não agregam informação para a construção de um modelo.

Os atributos irrelevantes não contêm informação útil ao processo de mineração.

Entre as desvantagens de se trabalhar com datasets extensos podem-se citar, ficam muito caro computacionalmente e os algoritmos podem não rodar satisfatoriamente.

Faz-se necessário reduzir o banco de dados, selecionando somente os atributos que são relevantes para o processo KDD.

Torna-se necessário, então, a identificação desses atributos.

A idéia de se trabalhar selecionando os principais atributos é obter uma representação reduzida do dataset, que é muito menor em volume, mas que produza os mesmos (ou quase os mesmos) resultados analíticos.

Podem-se dividir as abordagens mais utilizadas para selecionar um subconjunto de atributos relevantes em três grupos, embutida, filtro e Wrappers.

A abordagem embutida consiste na seleção de atributos realizada como parte do processo de criação do modelo por parte de um algoritmo de aprendizado de máquina.

Neste método a seleção ocorre naturalmente como parte dos algoritmos de mineração.

Exemplo desse método é o ganho de informação.

Pela abordagem filtro, consiste em aplicar um método de seleção de atributos anterior à aplicação do algoritmo de aprendizado de máquina, geralmente analisando características do conjunto de exemplos que podem levar a selecionar alguns atributos e excluir outros.

Exemplos deste método são Projeção Aleatória e PCA (Análise dos Componentes Principais).

E por fim, o método Wrappers, que consiste em selecionar um subconjunto de atributos e medir a precisão do classificador induzido sobre esse subconjunto de atributos.

Usa algoritmos de mineração como uma caixa preta para encontrar os melhores subconjuntos de atributos.

Essa abordagem é totalmente dependente do algoritmo de aprendizado.

Entre os métodos citados, a abordagem baseada em filtro será descrita a seguir, a qual foi utilizada neste trabalho.

Nesta abordagem, entre as várias técnicas existentes para selecionar os melhores atributos, podem-se citar os métodos Qui-quadrado, Info Gain, Gain Ratio e CFS Correlation-base O teste do é utilizado para definir a importância dos atributos descritivos em relação ao atributo-classe e todos são testados individualmente em relação a este.

Cria-se uma lista ordenada de todos os atributos da base de dados, na qual os com maior grau de dependência em relação ao atributo-classe, ou seja, maior valor de 2, têm prioridade em relação aos outros.

Este teste é muito eficiente para avaliar a associação existente entre variáveis qualitativas (dados do tipo categórico).

O princípio básico deste método não paramétrico é comparar as divergências entre as freqüências observadas e as esperadas.

De uma maneira geral, pode-se dizer que dois grupos se comportam de forma semelhante se as diferenças entre as freqüências observadas e as esperadas em cada categoria forem muito pequenas, próximas a zero.

Observe que é a diferença entre a freqüência observada e a esperada, que deverá ser calculada para cada célula da Tabela.

Quando as freqüências observadas são muito próximas às esperadas, o valor é pequeno, no entanto, quando as discrepâncias são grandes, passa a ser grande e, consequentemente, o assume valores altos.

O pesquisador estará sempre trabalhando com duas hipóteses, H0, não há associação entre os grupos H1, há associação entre os grupos As freqüências observadas são obtidas diretamente dos dados das amostras, enquanto que as freqüências esperadas são calculadas a partir destas.

Na prática, a freqüência esperada em uma determinada célula é calculada pela multiplicação do total de sua coluna (T, pelo total de sua linha (T1), dividindo-se o produto pelo total geral da Tabela (N).

Uma vez calculado o 2, procura-se na Tabela de distribuição de o valor do crítico considerando o nível de significância adotado e os graus de liberdade.

Os graus de liberdade (gl)são obtidos por, Se o obtido for igual ou maior que o crítico, H0 deverá ser rejeitada.

O método Info Gain avalia atributos individualmente através da medição de ganho de informação com respeito à classe, ou seja, para cada atributo é medida a quantidade de ganho de informação para a classe em questão.

É uma medida que indica o quanto um atributo irá separar os exemplos de aprendizado de acordo com a sua classe.

Um valor numérico quantifica o ganho.

Para determinar o ganho é preciso calcular a entropia dos dados.

A entropia é uma medida que indica a homogeneidade dos exemplos contidos em um conjunto de dados.

A entropia permite caracterizar a "pureza" de uma coleção arbitrária de exemplos.

A entropia de um conjunto de dados é indicada por, Porém, esta abordagem se torna impraticável quando o número de atributos é muito grande.

A medida ganho de informação tem um vício natural (bias), ela favorece atributos com muitos valores.

Por exemplo, um atributo tendo diferentes valores em diferentes amostras gera uma medida (ganho de informação) pobre viciad.

Para solucionar este problema, usa-se a taxa de ganho de informação (information gain ratio).

O método Gain Ratio realiza avaliação de atributos individualmente por meio da medição de relação de ganho com respeito à classe.

A medida taxa de ganho de informação tenta corrigir o "vício" dos atributos que contêm muitos valores através da incorporação de quantidade de informação segmentada.

A taxa de ganho da informação é dada por, E por fim, o método CFS (Correlation-based feature selection), baseado em correlação, avalia os subconjuntos de atributos.

Neste método a avaliação dos subconjuntos leva em consideração a capacidade de discriminação dos atributos com relação às classes e o grau de correlação entre eles.

É um método em que um conjunto de atributos é considerado bom quando contém atributos altamente correlacionados com a classe e quando contém atributos não correlacionados entre si.

Este método CFS é uma heurística de avaliação de subconjuntos que considera não somente a utilidade de atributos individuais, mas também o nível de correlação entre eles.

Mais detalhes em Hall.

Nesta fase é definida a escolha da tarefa e das técnicas a serem utilizadas, identificação da ferramenta que atinja os objetivos propostos e aplicação desta ferramenta aos dados.

Este passo pode envolver técnicas muito diversas e a informação descoberta é usada principalmente para construção de modelos, extração automática de padrões e exploração visual de dados.

É importante distinguir o que é uma tarefa e o que é uma técnica de mineração.

A tarefa consiste na especificação do que está querendo buscar nos dados, que tipo de regularidades ou categoria de padrões tem interesse em encontrar, ou que tipo de padrões poderia surpreender por exemplo, ocorrência de deficiência hídrica na época em que não é esperad.

A técnica de mineração consiste na especificação de métodos que garantam como descobrir os padrões que interessam.

As tarefas de mineração podem ser classificadas em duas grandes áreas, predição e descrição.

Dentre essas áreas, dependendo do objetivo procurado, dividem-se as principais categorias.

São elas, classificação, regressão, associação e clusterização.

Resumo das principais tarefas de mineração de dados.

Principais tarefas de mineração de dados.

As atividades de predição envolvem o uso dos atributos de um conjunto de dados para prever o valor futuro do atributo-meta, ou seja, essas atividades visam principalmente à tomada de decisões.

Já as atividades de descrição procuram padrões interpretáveis pelos humanos que descrevem os dados antes de realizar a previsão.

Essa tarefa também visa o suporte à decisão.

A predição consiste em examinar atributos de um conjunto de entidades e, baseado nos valores destes atributos, assinalar valores e atributos de uma nova entidade que se quer caracterizar.

A predição usa atributos para predizer o desconhecido ou os valores futuros de outras variáveis.

Os dois principais tipos de tarefas para predição são classificação e regressão.

A descrição tem por objetivo descrever o que está acontecendo em uma base de dados complicada no intuito de aumentar o entendimento sobre as pessoas, produtos ou processos que produziram os dados.

Atividades de descrição consistem na identificação de comportamentos intrínsecos do conjunto de dados, sendo que estes dados não possuem uma classe especificada.

Algumas das tarefas de descrição são clusterização e regras de associação.

Classificação é o processo de encontrar um conjunto de modelos que descrevem e distinguem classes, com o propósito de utilizar o modelo final (refinado) para predizer a classe de objetos que ainda não foram classificados.

O modelo construído baseia-se na análise prévia de um conjunto de dados de amostragem ou dados de treinamento, contendo objetos corretamente classificados.

A classificação consiste na predição de um valor categórico como, por exemplo, predizer se irá ocorrer a geada ou não.

Na regressão, o atributo a ser predito consiste em um valor contínuo como, por exemplo, predizer o valor da temperatura.

A descoberta de associação consiste em identificar quais atributos estão associados com outros em um dado ambiente.

A tarefa da associação determina quais são os atributos meteorológicos que estão associados com outros, por exemplo.

Assim, a associação pode ser usada para identificar ocorrências meteorológicas.

O número de regras de associação que podem ser encontradas quando se aplica a associação em um banco de dados é praticamente infinito e muitas dessas regras podem não ser interessantes.

Para contornar esse problema foram introduzidas duas medidas de interesse que distinguem as regras que são interessantes das que não são.

Essas medidas são, suporte que indica a freqüência com que uma regra aparece no banco de dados e confiança que indica o grau de acerto da regra.

A descoberta de agrupamento, ou clusterização, consiste em dividir uma população heterogênea em subgrupos homogêneos, com base na semelhança entre os registros do subgrupo.

Esta técnica agrupa informações homogêneas de grupos heterogêneos entre os demais e aponta o item que melhor representa cada grupo, permitindo, desta forma, perceber as características de cada grupo.

Esta técnica se caracteriza por trabalhar sobre dados onde os rótulos das classes não estão definidos.

Diferentemente da técnica de classificação, em que os dados de treinamento estão devidamente classificados e os rótulos das classes são conhecidos.

A escolha de quais técnicas de mineração de dados usar depende das metas do perito no domínio e das tarefas para atingir estas metas.

As técnicas para a execução dessas tarefas são variadas, entre elas, mineração visual de dados, árvores de decisão, redes neurais artificiais, associação e clusterização.

A adequabilidade do tipo da função de mineração de dados ao tipo de problema que se está querendo solucionar, juntamente com a quantidade e qualidade dos dados são os fatores fundamentais para definir a técnica mais adequada de execução.

Normalmente, os produtos para mineração de dados combinam as diversas técnicas, para se construir um produto mais preciso e mais rápido.

Neste trabalho destacam-se as técnicas de mineração visual dos dados e árvore de decisão.

Pode-se pensar na visualização de dados como técnicas que mapeiam volumes de dados multidimensionais para a tela bidimensional de um computador.

Visualização é uma ferramenta importante para mineração de dados porque seres humanos são muito bons em processar informação visual e muito ruins em processar informação numérica e/ou tabular.

Mineração visual de dados engloba técnicas que combinam visualização e exploração interativa de dados.

Estas técnicas descrevem conjuntos complexos de dados por meio de gráficos envolvendo múltiplas variáveis simultaneamente.

Elas normalmente permitem a exploração inteligente destes dados por meio de controle dos gráficos e seleção interativa da informação a ser analisada.

Neste caso, o perito necessita interagir diretamente com a ferramenta para que possa extrair informações úteis dos dados explorados.

Árvores de decisão ou de classificação são técnicas usadas para descobrir regras de classificação para um atributo a partir da subdivisão sistemática dos dados contidos no repositório que está sendo analisado.

São simples representações de conhecimento e classificam exemplos em um número finito de classes.

A árvore de decisão consiste de uma hierarquia de nós internos e externos que são conectados por ramos (arcos).

O nó interno é a unidade de tomada de decisão que avalia por meio de teste lógico qual será o próximo nó descendente ou filho.

Em contraste, um nó externo (não tem nó descendente), também conhecido como folha, está associado a um rótulo ou a um valor, que indica a classe predita para um determinado conjunto de dados.

Em geral, o procedimento de uma árvore de decisão é o seguinte, apresenta-se um conjunto de dados ao nó inicial (ou nó raiz que também é um nó interno) da árvore, dependendo do resultado do teste lógico usado pelo nó, a árvore ramifica-se para um dos nós filhos e este procedimento é repetido até que um nó folha é alcançado.

A repetição deste procedimento caracteriza a recursividade da árvore de decisão.

Um exemplo de árvore de decisão é apresentado.

Exemplo de árvore de decisão para ocorrência de geada.

Os classificadores geralmente são construídos por programas chamados indutores, que implementam algoritmos computacionais especiais, que operam sobre uma massa de dados inicial considerada representativa do domínio do problema e na qual tanto o valor dos atributos comuns quanto da classe de cada objeto são conhecidos.

Um programa indutor de classificadores procurará, com base nas ocorrências desse conjunto de dados inicial, chamado de conjunto de treinamento (training set), estabelecer qual a ligação entre os valores dos atributos não-categóricos e as classes encontradas na massa de dados.

A expectativa é de que essas relações encontradas, chamadas de regras de classificação e que representam em última instância o classificador em si, possam ser empregadas posteriormente para determinar o valor da classe para objetos onde essa informação é desconhecida, num tipo de atividade chamada predição (da classe).

Para que o grau de acerto de um classificador assim produzido possa ser avaliado antes de sua efetiva utilização prática, geralmente procura-se aplicá-lo sobre um segundo conjunto de dados onde o valor da classe é igualmente conhecido, chamado conjunto de teste (test set).

O conjunto de teste possui valores para os atributos de predição e também valores de classe para cada caso, ou seja, é estruturalmente idêntico ao conjunto de treinamento, mas seus dados não são os mesmos.

Posteriormente compara-se o grau de concordância entre a classe prevista pelo classificador para cada objeto e a classe realmente observada.

Sendo assim, dados um conjunto de exemplos de tamanho finito (training set) e um indutor, é importante estimar o desempenho futuro do classificador induzido utilizando o conjunto de exemplos (test set).

A partir do conjunto de treinamento, treina-se um indutor e testa-se seu desempenho com esse conjunto teste, que são exemplos fora da amostra utilizada para treinamento.

Entre os métodos para estimar uma medida verdadeira, bastante comentado na literatura, é o método Cross-Validation, que se baseia na idéia de amostragem.

O Cross-Validation (CV) é um estimador entre outros dois estimadores holdout e leave-one-out.

O estimador holdout divide os exemplos em uma porcentagem fixa de exemplos para treinamento e para teste.

O estimador leave-one-out é um caso especial de Cross-validation.

É computacionalmente dispendioso e frequentemente é usado em amostras pequenas.

Para uma amostra de tamanho n uma hipótese é induzida utilizando exemplos.

A hipótese é então testada no único exemplo remanescente.

Este processo é repetido n vezes, cada vez induzindo uma hipótese deixando de considerar um único exemplo.

O erro é a soma dos erros em cada teste dividido por n.

Em r-fold cross-validation CV, os exemplos são aleatoriamente divididos em r partições mutuamente exclusivas (folds) de tamanho aproximadamente igual a n/r exemplos.

Os exemplos nos folds são usados para treinamento e a hipótese induzida é testada no fold remanescente.

Este processo é repetido r vezes, cada vez considerando um fold diferente para teste.

O erro na Cross-validation é a média dos erros calculados em cada um dos r folds.

A respeito dos algoritmos, muitos são os algoritmos de classificação que elaboram árvores de decisão.

Não há uma forma de determinar qual é o melhor algoritmo, um pode ter melhor desempenho em determinada situação e outro algoritmo pode ser mais eficiente em outros tipos de situações.

A forma mais tradicional para a indução de regras de classificação baseia-se em uma estratégia de "divisão-e-conquista" conhecida por TDIDT (Top-Down Induction of Decision Trees) ou ID3.

Entre os principais algoritmos de indução de regras de classificação, destacam-se o ID3, C45 e C50, comentado a seguir, O algoritmo ID foi um dos primeiros algoritmos de árvore de decisão, tendo sua elaboração baseada em sistemas de inferência e em conceitos de sistemas de aprendizagem.

Logo após foram elaborados diversos algoritmos, sendo os mais conhecidos, C45, CART (Classification and Regression Trees), CHAID (Chi Square Automatic Interaction Detection), entre outros.

O algoritmo IDinicial foi posteriormente aperfeiçoado, dando origem ao Ce C45, que estenderam suas funcionalidades ao permitir o tratamento de conjuntos de treinamento com valores de atributo desconhecidos.

Valores de atributos contínuos.

Implementação de estratégias de poda de árvore e recursos para a extração de regras SE-ENTÃO a partir da árvore inicialmente induzida.

É provavelmente a família de algoritmos de indução de regras mais conhecida na área do aprendizado computacional.

Mais recentemente uma nova geração desse algoritmo foi desenvolvida, oferecendo diversos aprimoramentos e passando a ser oferecida como um produto comercial denominado C50 ou See5, conforme o sistema operacional a que se destina.

Ainda para o mesmo autor, o C45 é uma melhora do ID3, ou seja, além de possuir as mesmas características, ele possui a vantagem de poder lidar com a poda (prunning) da árvore para evitar o overfitting, com a ausência de valores e com a presença de ruídos nos dados.

O algoritmo J48 é uma reimplementação em Java do algoritmo C45 e faz parte do pacote de algoritmos de aprendizado de máquina da ferramenta Weka (Waikato Environment for Knowledge Analysis).

O J48 constrói um modelo de árvore de decisão baseado num conjunto de dados de treinamento, e usa esse modelo para classificar outras instâncias num conjunto de teste.

Durante o processo de utilização do algoritmo J48 é interessante conhecer alguns parâmetros que podem ser modificados para proporcionar melhores resultados como, por exemplo, o uso de podas na árvore, o número mínimo de instâncias por folha e a construção de árvore binária.

A escolha central de um algoritmo está em selecionar qual atributo será usado em cada nó da árvore.

É interessante selecionar o atributo que é mais útil para classificar exemplos.

Em outras palavras, uma boa subdivisão no momento da construção da árvore, é aquela que produz para os dados disponíveis os grupos mais homogêneos com relação ao atributo classe, enquanto que as más subdivisões caracterizam-se por formar grupos com pouca identidade com relação à classe.

Assim, é definida uma propriedade estatística chamada ganho de informação, que mede como um determinado atributo separa os exemplos de treinamento de acordo com a classificação deles.

É utilizado o ganho de informação para selecionar, entre os candidatos, os atributos que serão utilizados a cada passo, enquanto constrói a árvore.

O melhor atributo é aquele com o ganho de informação maior.

Resumidamente, a estratégia para determinar o melhor atributo de split é expressa em calcular o ganho de informação para cada atributo não categórico disponível e escolher aquele que apresentar o maior valor, descartando-o em seguida do processo de escolha para os próximos níveis da árvore de decisão.

O interessante é descobrir o atributo que forme os subconjuntos mais homogêneos no que diz respeito ao atributo classe.

Em outras palavras, aquele que forma os grupos menos "confusos" com relação à classe.

A Teoria da Informação utiliza um conceito originado da Termodinâmica chamado Entropia, para representar o grau de confusão presente nos dados disponíveis.

A entropia, que é inversamente proporcional ao grau de informação presente no contexto analisado, é expressa por meio de um valor situado entre 0 e 1.

Munari mostra os valores de entropia obtidos para algumas probabilidades possíveis de A e B, e pode-se observar que quanto mais uniforme a distribuição de probabilidade, maior o grau de entropia.

Se, no subconjunto analisado, todos os elementos pertencem a uma mesma classe, a entropia (ou "confusão") é mínima, ou seja, zero.

Valores de entropia obtidos para probabilidades possíveis de A e B.

Após construir a árvore de decisão é possível que tenha sido gerado um classificador que é muito específico para o conjunto de treinamento.

Neste caso, a precisão do classificador poderá ser alta para o conjunto de treinamento, mas é possível que sua precisão para novos exemplos seja ruim.

Quando isto ocorre, é dito que a árvore de decisão "decorou" os dados (overfitting).

Para evitar esta situação, os indutores fazem uma poda da árvore após sua indução.

Neste processo, alguns nós da árvore são removidos, reduzindo a complexidade da árvore, mas aumentando sua capacidade de generalização, possibilitando uma melhor performance para exemplos não utilizados no conjunto de treinamento.

Uma forma de poda da árvore de decisão é assumir que um nó pode se tornar folha sempre que uma certa porcentagem dos exemplos pertencem a uma mesma classe.

Por exemplo, em um problema que envolve duas classes e que existe ruído nos dados, um nó pode se tornar terminal folh se este possuir 50 exemplos de uma classe e apenas 1 da outra classe.

Isto pode ser considerado um critério de parada, no qual se decide não mais expandir este nó da árvore.

Este tipo de poda é aplicado durante a indução da árvore de decisão, sendo por isso chamado de pré-poda.

Uma outra abordagem de poda consiste em analisar a árvore após ela ter sido completamente induzida, sendo conhecida como pós-poda.

Nesta abordagem, um conjunto de exemplos, preferencialmente de teste, deve ser fornecido à árvore para ser classificado.

A partir dos nós folhas da árvore, cada subárvore formada por um nó não terminal ligado diretamente aos nós folha é analisada.

Caso a taxa de erro neste conjunto de exemplos seja menor pela substituição desta subárvore por um único nó folha, então esta substituição é realizada, ou seja, a árvore é podada.

Este processo continua até que nenhuma melhora possa ser feita nas subárvores.

A partir de uma árvore de decisão é possível derivar regras.

As regras são escritas considerando o trajeto do nó raiz até uma folha da árvore.

As regras são do tipo, IF L AND L.


Para cada caminho, da raiz até uma folha, tem-se uma regra de classificação.

Cada par (atributo,valor) neste caminho dá origem a um L.

Por exemplo, a árvore de decisão corresponde ao seguinte conjunto de regras de classificação, da árvore de decisão como, construção da árvore de decisão para classificar todos os exemplos.

Poda da árvore de decisão para eliminar subárvores não confiáveis.

Processamento da árvore podada para melhorar a sua legibilidade construção de regras a partir da árvore.

Após a fase de mineração de dados, o processo KDD entra na última fase, a fase do pós-processamento.

Esta fase é conhecida como a etapa de avaliação e interpretação dos resultados.

Na etapa de pós-processamento, o conhecimento extraído pelos algoritmos de extração de padrões deve ser avaliado, bem como a qualidade do classificador.

Alguns critérios de avaliação utilizam o conhecimento do especialista para saber, por exemplo, se o conhecimento extraído é interessante ao usuário.

De forma geral, essa fase envolve todos os participantes.

O analista de dados tenta descobrir se o classificador atingiu as expectativas, analisando os resultados de acordo com algumas métricas.

O especialista irá verificar a compatibilidade dos resultados com o conhecimento disponível do domínio.

E, por último, o usuário, que é responsável por dar o julgamento final sobre a aplicabilidade dos resultados.

No processo KDD, o resultado encontrado no final deve ser compreensível, isto é, pode ser avaliado pela simplicidade do modelo, como por exemplo, número de nós de uma árvore de decisão ou então o especialista valida o conhecimento para descoberta de novos padrões, para sugestão de melhores atributos e para o refinamento do conhecimento.

Entre as medidas para avaliar um classificador, analisada pelo analista de dados, destacam-se a acurácia (precisão global) e o erro, que são calculadas avaliando a precisão com que o trabalho de predição é executado.

Um classificador tem sua acurácia avaliada em termos do grau de acerto de suas previsões tanto sobre o conjunto de treinamento como, principalmente, sobre dados novos, no conjunto de teste, medida numa fase imediatamente posterior ao treinamento, analisada na fase de pós-processamento.

Essas medidas podem ser extraídas a partir da matriz de confusão.

Como mencionado anteriormente, uma matriz de confusão mostra o número de classificações corretas em oposição às classificações preditas para cada classe.

Assim, podem-se avaliar os valores encontrados como falsos positivos (FP), falsos negativos (FN), verdadeiros positivos (TP) e verdadeiros negativos (TN).

Desta forma, essas medidas, a acurácia e a taxa de erro, calculadas a partir da matriz de confusão, avaliam o desempenho de um classificador para sistemas de aprendizado.

Abaixo segue a demonstração destas medidas.

Precisão global acurácia A, corresponde à proporção de previsões corretas do classificador.

É determinada pela equação, Taxa de erro, conhecida também como taxa de classificação incorreta.

Essa medida compara a classe verdadeira de cada exemplo com o rótulo atribuído pelo classificador induzido.

Dada por, Comenta-se a respeito da avaliação de um modelo de classificação, que é baseada na análise da matriz de confusão (ou de suas derivações).

Uma das maneiras mais comuns de avaliar modelos é a derivação de medidas que, de alguma maneira, tentam medir a "qualidade" do modelo.

Reduzir a matriz de confusão a uma única medida tem algumas vantagens aparentes.

A principal delas é que é mais fácil escolher o "melhor" modelo em termos de um único valor.

Entretanto, é comum encontrar casos em que uma dada medida é apropriada para um problema, mas ela é irrelevante para outros.

Também é comum encontrar situações em que a avaliação é um problema de múltiplas faces, nas quais é possível definir várias medidas, sendo perfeitamente possível que um modelo seja melhor que outro para algumas dessas medidas, mas seja pior com relação a outras medidas.

Nesses casos, utilizar uma única medida pode dar a falsa impressão de que o desempenho pode ser avaliado utilizando apenas essa medida.

Por exemplo, quando se utiliza a análise da precisão global acurácia de classificação.

Existem várias situações em que a acurácia de classificação não é apropriada para a avaliação de modelos de classificação.

Uma situação comum se dá quando o número de exemplos em cada uma das classes é muito desbalanceado.

Por exemplo, supondo que em um dado domínio o número de exemplos de uma das classes seja 99%.

Nesse caso, é comum se obter alta acurácia um modelo que sempre retorna a classe majoritária terá uma acurácia de 99%.

No entanto, esse modelo que sempre classifica um novo exemplo na classe majoritária não irá acertar nenhuma classificação de exemplos na classe minoritária.

Quando existe um desbalanceamento entre as classes, tal medida pode ser enganosa.

De forma geral, qualquer medida que tenha como objetivo reduzir a avaliação de um modelo de classificação a um único valor terá, em maior ou menor grau, uma perda de informação.

Geralmente, a avaliação de um modelo utilizando uma única medida pode levar a conclusões errôneas.

Outro problema referente ao uso da precisão ou da taxa de erro para classes desbalanceadas, é que essas medidas consideram erros de classificação diferentes como igualmente importantes.

Como exemplo, um paciente doente diagnosticado como sadio pode ser um erro fatal, enquanto que um paciente sadio diagnosticado como doente pode ser considerado um erro menos sério, uma vez que esse erro pode ser corrigido em exames futuros.

Para conjuntos de dados com classes desbalanceadas, como é o caso do conjunto de dados para prever a ocorrência de geada, uma medida de desempenho mais apropriada deve demonstrar os erros, ou acertos, ocorridos para cada classe.

A partir dos parâmetros da matriz de confusão é possível derivar outras medidas de desempenho de um classificador nas classes negativa e positiva, independentemente.

Entre essas medidas destacam-se a taxa de verdadeiro positivo (TP), taxa de falso positivo (FP), taxa de verdadeiro negativo (TN) e a taxa de falso negativo (FN).

Uma alternativa à avaliação do classificador, principalmente para classes desbalanceadas, é utilizar gráficos e/ou diagramas.

Gráficos permitem uma melhor visualização da multidimensionalidade do problema de avaliação.

O gráfico ROC é uma medida adotada para avaliar a construção dos modelos.

Análise ROC do inglês Receiver Operating Characteristics é um método gráfico para avaliação, organização e seleção de sistemas de diagnóstico e/ou predição.

Recentemente, foram introduzidos em aprendizagem de máquina e mineração de dados como uma ferramenta útil e eficiente para a avaliação de modelos de classificação.

Ela é particularmente útil em domínios nos quais existem uma grande desproporção entre as classes ou quando se deve levar em consideração diferentes custos/benefícios para os diferentes erros/acertos de classificação.

Análise ROC também tem sido utilizada para a construção e refinamento de modelos.

Para se construir o gráfico ROC plota-se a taxa de verdadeiro positivo (TP) no eixo das ordenadas eixo y e a taxa de falso positivo (FP) no eixo das abscissas eixo x.

Espaço ROC.

Analisando o gráfico ROC, o ponto (0%,0%) representa a estratégia de nunca classificar um exemplo como positivo, ou seja, de classificar todos os exemplos como pertencentes à classe negativa.

Modelos que correspondem a esse ponto não apresentam nenhum falso positivo, mas também não conseguem classificar nenhum verdadeiro positivo.

A estratégia inversa, de sempre classificar todos os exemplos como pertencentes à classe positiva, é representada pelo ponto (100%,100%).

O ponto (0%,100%) representa o modelo perfeito, isto é, todos os exemplos positivos e negativos são corretamente classificados.

E o ponto (100%,0%) representa a estratégia de tentar adivinhar a classe aleatoriamente.

Modelos próximos ao canto inferior esquerdo podem ser considerados "conservativos", isto é, eles fazem uma classificação positiva somente se têm grande segurança na classificação.

Como conseqüência, eles cometem poucos erros falsos positivos, mais frequentemente têm baixas taxas de verdadeiros positivos.

Modelos próximos ao canto superior direito podem ser considerados "liberais", isto é, eles predizem a classe positiva com maior freqüência, de tal maneira que eles classificam a maioria dos exemplos positivos corretamente, mas geralmente, com altas taxas de falsos positivos.

A linha diagonal ascendente (0%,0%) a (100%,100%) representa um modelo de comportamento estocástico, cada ponto pode ser obtido pela previsão da classe positiva com probabilidade p e da classe negativa com probabilidade 100% p.

Pontos pertencentes ao triângulo superior esquerdo a essa diagonal representam modelos que desempenham melhor que o aleatório e pontos pertencentes ao triângulo inferior direito representam modelos piores que o aleatório.

A diagonal descendente (100%,100%) a (0%,0%) representa modelos de classificação que desempenham igualmente em ambas as classes.

À esquerda dessa linha estão os modelos que desempenham melhor para a classe negativa em detrimento da positiva e, à direita, os modelos que desempenham melhor para a classe positiva.

A partir de um gráfico ROC é possível calcular uma medida geral de qualidade, a área sob a curva (AUC Área under the ROC curve).

A AUC é a fração da área total que se situa sob a curva ROC.

Essa medida é equivalente a diversas outras medidas estatísticas para a avaliação de modelos de classificação.

A medida AUC fatora o desempenho do classificador sobre todos os custos e distribuições.

Diversas medidas para avaliação de conhecimento têm sido pesquisadas com a finalidade de auxiliar o usuário no entendimento e na utilização do conhecimento adquirido.

Estas medidas podem ser divididas entre medidas de desempenho e medidas de qualidade.

Algumas medidas de desempenho são as medidas comentadas anteriormente, como precisão, erro, taxa de verdadeiro e falso positivo e taxa de verdadeiro e falso negativo.

As medidas de qualidade são necessárias, pois um dos objetivos do processo de extração de conhecimento é que o usuário possa compreender e utilizar o conhecimento descoberto.

Entretanto, podem ocorrer casos em que os modelos são muito complexos ou não fazem sentido para os especialistas.

Assim, a compreensibilidade do conhecimento extraído é um aspecto bastante importante para o processo de extração de conhecimento.

A compreensibilidade de um dado conjunto de regras está relacionada com a facilidade de interpretação dessas regras por um ser humano.

A compreensibilidade de um modelo pode ser estimada, por exemplo, pelo número de regras e número de condições por regra.

Nesse caso, quanto menor a quantidade de regras de um dado modelo e menor o número de condições por regra, maior será a compreensibilidade das regras descobertas.

Em Pazzani e Pazzani é discutido que outros fatores, além do tamanho do modelo, são importantes na determinação da compreensibilidade de um conhecimento.

Um fator citado é que os usuários especialistas possuem tendência a compreender melhor modelos que não contradizem seu conhecimento prévio.

Após a análise do conhecimento, caso este não seja de interesse do usuário final ou não cumpra com os objetivos propostos, o processo de extração pode ser repetido ajustando-se os parâmetros ou melhorando o processo de escolha dos dados para a obtenção de resultados melhores numa próxima iteração.

Para analisar e avaliar o conhecimento obtido, o modelo criado deve ser validado em outras bases de dados, diferentes das utilizadas no treinamento ou no teste.

Os padrões descobertos devem ser válidos em novos dados com algum grau de certeza.

É muito importante que os resultados e modelos possam ser avaliados e comparados.

Alguns elementos relevantes neste domínio são, teste e validação, que fornecem parâmetros de validade e confiabilidade nos modelos gerados e indicadores estatísticos para auxiliar a análise dos resultados.

O software WEKA, permite analisar e avaliar o conhecimento obtido, validando os resultados através do teste e validação, como cross-validation validação cruzad, supplied test set, use training set e percentage split.

Em cross-validation, o conjunto de treino será dividido em n partes.

Uma parte (1/n) é reservada como conjunto de testes, enquanto o restante é usado para n treinos, onde em cada, uma parte é usada para testes.

Em supplied test set, um outro conjunto de dados para teste, em arquivo separado, deve ser fornecido e o algoritmo será aplicado neste novo dataset fornecido.

Em use training set, todo o dataset fornecido é utilizado como conjunto de treino e testes.

E em percentage split, p% dos dados são usados para o procedimento de treino, sendo que o restante será usado na etapa de testes.

A análise e avaliação do conhecimento obtido podem ser feitas também, através do WEKA, por meio de indicadores estatísticos como, matriz de confusão, área sob a Curva ROC AU, índice de correção e incorreção de instâncias mineradas, erro médio absoluto, erro relativo médio, precisão, dentre outros.

No processo KDD, destacam-se as participações atuantes do especialista do domínio, do analista e do usuário final.

Os especialistas do domínio envolvidos possuem amplo conhecimento sobre o assunto da aplicação, além de fornecerem apoio para a execução do processo.

Como analista do processo KDD, a autora deste trabalho atuou como especialista no processo de extração do conhecimento, sendo responsável por sua execução, conhecendo profundamente as etapas que compõem todo o processo.

Como usuário final, os próprios especialistas atuaram representando aqueles que utilizam o conhecimento extraído no processo para auxiliá-lo na tomada de decisão.

Nos tópicos que seguem, são explicadas as atividades contínuas do processo KDD, descritas por etapas, aplicadas nos conjuntos de dados presentes.

Partindo do princípio que entender o domínio dos dados é naturalmente um pré-requisito para extrair algo útil, fez-se necessária a discussão entre os especialistas do domínio e o analista dos dados para identificar e entender o problema do projeto, sempre focalizando as metas e objetivos a serem alcançados.

Para este estudo, as metas e objetivos envolvem identificar as relações entre parâmetros climáticos, visando descobrir eventuais novos conhecimentos, possibilitando desenvolver alertas de geada e alertas de deficiência hídrica, com maior grau de confiança e num intervalo de tempo satisfatório, com o objetivo de auxiliar os produtores nas tomadas de decisão.

Nesta fase, foram definidas as escolhas da tarefa e das técnicas a serem utilizadas no desenvolvimento deste projeto.

Neste sentido, a tarefa definida para o problema e que atingisse o objetivo deste trabalho foi a CLASSIFICAÇÃO, isto é, processo para encontrar um conjunto de modelos que descrevem e distinguem classes, com o propósito de utilizar o modelo para predizer a classe de objetos que ainda não foram classificados.

A escolha de quais técnicas de mineração de dados usar depende das metas do especialista no domínio e das tarefas para atingir estas metas.

Para este trabalho, as técnicas de mineração visual dos dados e árvore de decisão foram definidas como sendo as utilizadas.

A técnica de mineração visual foi utilizada na parte de pré-processamento, como comentado anteriormente, para auxiliar na identificação dos melhores atributos para compor o conjunto de dados.

A técnica árvore de decisão foi aplicada aos conjuntos de dados a fim de obter as regras de classificação para cada conjunto de dados referente aos dias de antecedência aos eventos de geada e de deficiência hídrica.

A respeito de deficiência hídrica, segundo os especialistas, o interessante é identificá-la principalmente nos meses em que não é comum a sua ocorrência, ou seja, que não se espera por ela, como por exemplo nos meses de janeiro, fevereiro e março.

Nos meses em que a deficiência hídrica já é esperada, como junho, julho e agosto, acredita-se apenas em uma confirmação do acontecimento.

É importante destacar que neste trabalho existem dois objetivos que são completamente independentes, sendo necessárias duas bases de dados, uma para prever a ocorrência de geada e outra para prever a ocorrência de deficiência hídrica.

Os especialistas também auxiliaram o analista na escolha do melhor conjunto de dados para se realizar a extração dos padrões, saber quais valores eram válidos para os atributos, os critérios de preferência entre os possíveis atributos e as restrições de relacionamento ou informações para geração de novos atributos.

Neste sentido, a fim de gerar o sistema de alerta para geada, a base de dados selecionada compreende dados climáticos diários de Ponta Grossa/PR, de 1986 a 200e de Londrina/PR, de 1986 a 2006, disponibilizados pelo IAPAR Instituto Agronômico do Paraná.

O conjunto de dados de Ponta Grossa constitui uma matriz de 31 atributos (incluindo a classe) e 6056 linhas e de Londrina possui uma matriz dos mesmos 31 atributos e 7517 linhas.

Nos dois bancos de dados, as classes já vieram categorizadas como geada severíssima, severa, moderada e não ocorrência.

Os atributos selecionados juntamente com os especialistas para atingir o objetivo de prever geada são mostrados.

Atributos utilizados no conjunto de dados para previsão de geada.

Constituindo a base de dados para desenvolver o sistema de alerta para deficiência hídrica estão os dados climáticos diários de Sete Lagoas/MG, de 1960 a 2005 e de Piracicaba, de 1960 a 2004, disponibilizados pelo AGRITEMPO Sistema de Monitoramento Agrometeorológico desenvolvido pelo CEPAGRI Centro de Previsões Meteorológicas e Climáticas Aplicadas à Agricultura e pela Embrapa Informática Agropecuária.

O conjunto de dados de Sete Lagoas constitui uma matriz de 11 atributos (incluindo a classe) e 1653linhas e de Piracicaba possui uma matriz dos mesmos 11 atributos e 15556 linhas.

As classes deste conjunto de dados foram preenchidas a partir do cálculo do balanço hídrico.

Os atributos selecionados em conjunto com os especialistas para atingir o objetivo de prever deficiência hídrica são mostrados.

Atributos utilizados no conjunto de dados para previsão de deficiência hídrica.

Tanto no conjunto de dados para geada como no conjunto de dados de deficiência hídrica, o atributo "Fases da Lua" foi extraído do site e os atributos "El Niño" e "La Niña" foram extraídos do site.

Todos são dados diários e reais.

A respeito dos atributos Temperatura, máxima e mínima são medidas de grande importância na agrometeorologia.

A temperatura é medida por meio de termômetros, graduados em graus centígrados (ou Celsius) ou em graus Fahrenheit.

Ambas as escalas têm como referências o ponto de congelamento e o ponto de ebulição da água, com a temperatura de congelamento em 0ºC ou 32ºF, e a temperatura de ebulição em 100ºC ou 212ºF.

Os termômetros medem temperaturas pela dilatação (ou contração) da substância nele empregada, que é proporcional à variação da temperatura.

Toda substância reage a temperaturas diferentes (dilatando-se ou contraindo-se).

Os termômetros são baseados, principalmente, na dilatação, contração e condutividade elétrica de certas substâncias.

O fenômeno ENOS El Niño Oscilação Sul faz parte de uma variação irregular em torno das condições normais do oceano e da atmosfera na região do Pacífico tropical.

O El Niño representa o componente oceânico do fenômeno, enquanto a Oscilação Sul (OS) representa a contrapartida atmosférica.

O fenômeno ENOS é atualmente monitorado principalmente através da Temperatura da Superfície do Mar (TSM) em regiões definidas ao longo da região equatorial do Oceano Pacífico.

O componente atmosférico OS expressa a correlação inversa existente entre a pressão atmosférica nos extremos leste e oeste do Oceano Pacífico.

Quando a pressão é alta a leste, usualmente é baixa a oeste, e vice-versa.

O Índice de Oscilação Sul (IOS) é utilizado no monitoramento deste componente.

Um extremo dessa variação é representado pelas condições de El Niño, quando se verifica um aquecimento das águas simultaneamente com diminuição da pressão atmosférica no Pacífico leste, e o outro extremo da variação é representado pelas condições de La Niña, quando ocorre um resfriamento das águas e aumento na pressão atmosférica na região leste do Pacífico.

A Amplitude de temperatura é dada pela diferença da temperatura máxima e mínima.

A Umidade é um termo geral que descreve o conteúdo de vapor-d'água existente no ar atmosférico.

O aquecimento ou o resfriamento da água causa sua mudança de um para outro de seus três estados, sólido, líquido e gasoso vapor-d'águ.

A aplicação contínua de calor derrete o gelo, tornando-o líquido águ, que, por sua vez, evapora, transformando-se em vapor-d'água.

A retirada contínua de calor do vapor-d'água causa sua condensação e a passagem para o estado líquido.

A água, por seu turno, transforma-se em gelo, com o prosseguimento do processo de remoção de calor.

Estas mudanças de estado são sempre acompanhadas de ganho ou perda de calor pelos ambientes próximos.

A capacidade do ar atmosférico de conter umidade é diretamente proporcional à sua temperatura.

Esta é uma das principais propriedades do ar atmosférico.

Quanto maior a temperatura do ar, maior a quantidade de vapor-d'água que poderá conter.

Diz-se que o ar atmosférico está saturado quando contém a quantidade máxima de vapor-d'água, possível a uma dada temperatura.

Então, em temperaturas mais elevadas é necessária maior quantidade de vapor-d'água para tornar o ar saturado, ocorrendo o inverso em temperaturas mais baixas.

A umidade relativa é definida como a relação, em percentagem, existente entre a quantidade de vapor-d'água presente no ar e a quantidade máxima de vapor-d'água que ele poderá conter, a uma determinada temperatura.

Numa situação em que a quantidade de vapor-d'água contido no ar permaneça constante, ou seja, sem acréscimo ou retirada de umidade, se a temperatura do ar aumenta, a sua capacidade de conter vapor-d'água até se saturar também aumenta, logo, a sua umidade relativa diminui.

Se a temperatura do ar diminui, o seu limite de conter umidade até se saturar também diminui, logo, a sua umidade relativa aumenta.

Assim, constata-se que a umidade relativa varia de modo inversamente proporcional à variação da temperatura.

A condição de saturação do ar é importante porque qualquer resfriamento adicional do ar saturado força o vapor-d'água a mudar de estado, retornando à forma líquida.

Assim se formam as nuvens, os nevoeiros, as neblinas e até as geadas.

Se o processo continua o bastante, ocorre precipitação, ou seja, descida de uma parcela do vapor-d'água condensado, sob a forma de chuva, geada, neve, saraiva, chuvisco, ou de uma combinação deles.

Outro atributo é o vento.

Os ventos são deslocamentos de ar no sentido horizontal, originários de gradientes de pressão.

A intensidade e a direção dos ventos são determinados pela variação espacial e temporal do balanço de energia na superfície terrestre, que causa variações no campo de pressão atmosférica, gerando os ventos.

O vento se desloca de áreas de maior pressão (áreas mais frias) para aquelas de menor pressão (áreas mais quentes), e quanto maior a diferença entre as pressões dessas áreas, maior será a velocidade de deslocamento.

O regime dos ventos é expresso por sua velocidade e direção.

A velocidade dos ventos é afetada, também, pela rugosidade da superfície criada pelos obstáculos vegetação, construções, relevo montanhoso, et, e pela distancia vertical acima da superfície em que ela é medida.

Quanto mais próximo da superfície, maior o efeito do atrito com o terreno, desacelerando o movimento e diminuindo a velocidade de deslocamento do ar.

A direção dos ventos é resultante da composição das forças atuantes (gradiente de pressão, atrito, força de Coriolis), mas o relevo predominante na região também afeta a direção próximo à superfície.

Entre as direções dos ventos, ilustram Norte (N), Sul (S), Leste, Oeste (W), Noroeste (NW), Nordeste (NE), Sudoeste, Sudeste e Calmaria.

A etapa de pré-processamento dos dados consiste em "arrumar" os dados para a próxima etapa no processo KDD, análise do processamento dos dados.

Nesta fase foram executadas as etapas de extração e integração das diferentes bases de dados formando um único conjunto de dados.

Transformação dos dados e limpeza dos dados, com o objetivo de garantir a qualidade dos dados e consequentemente a qualidade do conhecimento adquirido.

Nesta fase, os atributos 1 a 27, disponíveis para geada, numerados no item anterior, encontravam-se no formato de planilhas eletrônicas.

Como os atributos de 28 a 30 eram provenientes de outras fontes, foi necessário fazer a unificação desses dados, formando uma única fonte de dados no formato atributo-valor que foi utilizada como entrada para o algoritmo de extração de padrões a fim de prever a ocorrência de geada.

Quanto ao banco de dados de deficiência hídrica, os atributos numerados de 1 a 7 também encontravam-se no formato de planilhas eletrônicas.

Com o mesmo objetivo de se obter a base de dados no formato atributo-valor para ser utilizada na fase de processamento dos dados, foi necessário unificar os outros atributos restantes, de 8 a 10 para conseguir prever a ocorrência de deficiência hídrica.

Alguns tratamentos foram aplicados analisando os valores faltantes, transformação de dados qualitativos em quantitativos, construção de novos atributos, discretização dos dados e outras transformações que fossem necessárias a fim de preparar o banco de dados com qualidade para conseguir extrair informações na fase de mineração de dados.

Sempre antes de aplicar qualquer tipo de tratamento nos dados, os experimentos no formato original, sem aplicação de nenhum tratamento prévio, foram rodados a fim de obter resultados preliminares da situação de cada experimento e assim notar a necessidade do tratamento em cada fase da etapa do pré-processamento.

Com os resultados preliminares, sem tratamento e com os resultados dos experimentos com os dados tratados, pode-se comparar e observar o ganho nos resultados obtidos.

Não existiam valores faltantes nos conjuntos de dados de geada e de deficiência hídrica, consequentemente não existiu a necessidade de realizar esta atividade.

Todos os atributos que eram seqüenciais e categóricos foram transformados em numéricos.

Isto se aplicou para os atributos "Mês", "Fases da Lua", "El Niño" e "La Niña".

Foram realizados experimentos preliminares antes de realizar a transformação dos dados qualitativos em quantitativos para comparar os resultados sem o tratamento e após a aplicação do tratamento nos dados.

Em função dos experimentos realizados com o tratamento apresentarem resultados melhores, optou-se pela aplicação do mesmo.

Os atributos "Direção do pico" e "Direção do vento" não foram transformados em dados numéricos porque não eram seqüenciais, uma vez que seria impossível interpretar os resultados.

Fez-se necessário a construção de novos atributos a partir dos atributos originais, fornecidos pelas instituições e indicados pelos especialistas.

Quando se trabalha com séries históricas, é necessária a criação de janelas para que se possam obter informações de todos os atributos em uma única linha com o objetivo do algoritmo de extração de conhecimento compreender o conjunto de dados e gerar o conhecimento esperado.

Sendo assim, para o conjunto de dados de geada, os atributos originais foram trabalhados e transformados em novos atributos, seguindo também orientação dos especialistas.

A temperatura mínima de 1 a 8 dias antes.

Temperatura máxima de 1 a 8 dias antes.

Temperatura média de 1 a 8 dias antes.

Amplitude de temperatura do dia, de um dia antes até amplitude de temperatura de 8 dias antes.

Temperatura do bulbo seco de 1 a 8 dias antes, para as 9 h, 15 h e 21 h.

Temperatura do bulbo úmido de 1 a 8 dias antes, para as 9 h, 15 h e 21 h.

Umidade relativa de 1 a 8 dias antes, para as 9 h, 15 h e 21 h.

Umidade relativa média de 1 a 8 dias antes.

Vento acumulado a 10 metros de 1 a 8 dias antes.

Precipitação acumulada de até 8 dias antes.

Precipitação de 1 até 8 dias antes.

Insolação de 1 até 8 dias antes.

Direção do vento de 1 até 8 dias antes, das 9 h, 15 h e 21 h.

Velocidade do vento de 1 até 8 dias antes, das 9 h, 15 h e 21 h.

Pico máximo do vento de 1 até 8 dias antes e direção do vento de 1 até 8 dias antes.

De 31 atributos originais, o conjunto de dados passou a ter 236 atributos.

Quanto ao conjunto de dados para deficiência hídrica, os atributos também foram transformados resultando nos atributos que seguem, temperatura média do dia, de um dia antes e até 15 dias antes.

Amplitude de temperatura do dia, de um dia antes e de até 15 dias antes.

Precipitação acumulada do dia, de um dia antes e de até 15 dias antes.

Diferença da precipitação e evapotranspiração potencial do dia, de um dia antes e de até 15 dias antes.

Precipitação acumulada média de 10 dias.

Precipitação acumulada média de 15 dias.

Precipitação acumulada média de 20 dias, precipitação acumulada média de 30 dias.

Dias acumulados com precipitação.

Dias acumulados sem precipitação.

Precipitação de um dia antes até precipitação de 15 dias antes.

Evapotranspiração de um dia antes até evapotranspiração de 15 dias antes.

Evapotranspiração acumulada de um dia antes até evapotranspiração acumulada de 15 dias antes.

Temperatura mínima de um dia antes até 15 dias antes e temperatura máxima de um dia antes até 15 dias antes.

De 10 atributos originais, o conjunto de dados passou a ter 150 atributos.

A amplitude de temperatura, diferença entre a temperatura máxima e mínima, foi calculada com a idéia de auxiliar na percepção de mudanças bruscas entre as duas temperaturas.

Se a temperatura máxima apresentar uma diferença significativa da temperatura mínima, em um intervalo de tempo pequeno, provocará mudanças bruscas na temperatura e isto poderá prejudicar a lavoura, podendo não haver tempo hábil para tomada de decisões.

Por outro lado, se a temperatura máxima e mínima for próxima, dentro de um intervalo de tempo curto, não haverá mudanças bruscas.

A precipitação acumulada foi calculada para os devidos períodos, conforme sugestões de especialistas da área e indicação da literatura.

Deve-se também levar em consideração que as melhores condições hídricas para o cultivo das culturas não é o total de precipitação anual, mas sim a sua distribuição ao longo do ano ou, ainda melhor, a disponibilidade de água no solo à disposição da planta, durante o período exigido.

Por exemplo, a cana-de-açúcar, na fase de crescimento, exige um período de água abundante e na fase de maturação exige um período mais seco.

O café, no período de dormência exige pouca chuva e no período de vegetação e frutificação exige muita água.

Chegou-se à conclusão que a precipitação acumulada deveria constar ao longo de 10, 15, 20 e 30 dias.

Esta foi calculada através da média móvel.

Também foram calculados os dias consecutivos que apresentaram precipitação e dias consecutivos que não apresentaram precipitação, ou seja, dias acumulados com precipitação e dias acumulados sem precipitação.

Esse atributo é de fundamental importância, pois algumas culturas necessitam de um período seco, ou seja, sem precipitação, e, portanto passa ser necessário conhecer o período que ficou sem precipitação.

Foi decidido transformar os atributos com até 8 dias de antecedência ao evento geada e até 15 dias de antecedência ao evento deficiência hídrica, por sugestão dos especialistas.

Com relação à discretização dos dados, ou seja, o processo de transformar um atributo em faixas de valores, foi aplicada ao atributo-classe no conjunto de dados geada e no conjunto de dados deficiência hídrica, conforme sugestões dos especialistas.

No conjunto de dados de Ponta Grossa existiam 236 atributos e 6056 linhas, sendo 24ocorrências de geada, divididas em 9pertencentes à classe severíssima, 9casos na classe severa, 57 moderada e 581pertencentes à classe não e no conjunto de dados de Londrina haviam os mesmos 236 atributos e 7517 linhas, sendo 25 pertencentes à classe severíssima, 27 casos na classe severa, 2moderada e 7441 pertencentes à classe não.

Essa classificação já fazia parte do banco de dados enviado para este trabalho.

Como a quantidade de dados na classe positiva era muito inferior, o especialista sugeriu unir todas as classes positivas em uma única classe, transformando o conjunto de dados em apenas duas classes, um problema de classe binária.

Além disso, a classificação de geada passou ser referente ao atributo temperatura mínima do ar.

Quando este for inferior ou igual a 4ºC, considerou-se da classe sim e o contrário considerou-se como da classe não.

Esta medida é referente à temperatura mínima do ar tomada no abrigo.

O banco de dados de geada de Ponta Grossa, ficou com 186 ocorrências da classe sim e 5870 da classe não, perfazendo o mesmo total anterior, 6056 linhas.

O banco de dados de Londrina ficou com 5sim e 7465 pertencentes à classe não, com o mesmo total anterior de 7517 linhas.

Quanto às classes de deficiência hídrica, também foi necessário a sua discretização dos valores numéricos para valores categóricos, em forte, moderada, fraca e não ocorrência de deficiência hídrica.

Estas classes do conjunto de dados para deficiência hídrica não foram fornecidas, sendo necessário calculá-las a partir do método do balanço hídrico, método para calcular a disponibilidade atual de água no solo (DAAS).

Em função da DAAS é que as classes foram criadas.

O primeiro passo para o cálculo do balanço hídrico climatológico é a seleção da Capacidade de Água Disponível no solo CA.

Para este trabalho, não foi necessário realizar o cálculo da CAD, pois o especialista sugeriu que fosse utilizado o valor da CAD igual a 125 mm.

Após a seleção da CAD, foi necessário conhecer a precipitação e a evapotranspiração potencial, atributos fornecidos pelo IAPAR.

A precipitação (P) é medida através da altura pluviométrica, que é a altura de água precipitada, expressa em milímetros (mm).

Essa medida é definida como sendo o volume precipitado por unidade de área horizontal do terreno ou por unidade de tempo.

O equipamento básico de medição de chuva é o pluviômetro, que é constituído de uma área de captação (100 cm) e de um reservatório onde a água da chuva é armazenada até o momento da leitura.

A evapotranspiração potencial representa a perda natural de água do solo vegetado para a atmosfera através da ação conjunta da evaporação e da transpiração.

A ETP é processo oposto à precipitação, representa a água que retorna forçosamente para a atmosfera, em estado gasoso, e depende da energia solar disponível na superfície do terreno para vaporizá-la.

Os valores de ETP podem ser estimados a partir de elementos medidos na estação meteorológica, existindo vários métodos para tal estimativa.

A escolha do método depende de uma série de fatores.

O primeiro fator é a disponibilidade de dados meteorológicos, pois métodos complexos, que exigem grande número de variáveis, somente terão aplicabilidade quando houver disponibilidade de todos os dados necessários.

O segundo fator é a escala de tempo requerida, pois métodos empíricos, estimam bem a ETP na escala mensal, ao passo que os métodos que envolvem o saldo de radiação apresentam boas estimativas também na escala diária.

Abaixo será mais detalhado o método de Penman-Monteith, pois foi o utilizado neste trabalho.

O método é um método micrometeorológico para estimativa da evapotranspiração potencial na escala diária.

Atualmente, este é o método-padrão da FAO, sendo ETP mm/di dada pela seguinte fórmula, A partir desses dois atributos, precipitação e evapotranspiração potencial, foram feitos os cálculos dos outros parâmetros necessários para conhecer a disponibilidade de água que havia no solo no dia de interesse e consequentemente conhecer a deficiência hídrica e o excedente hídrico.

O parâmetro P ETP é a diferença da precipitação menos a evapotranspiração potencial, mantendo-se o sinal positivo ou negativo.

Valor positivo indica chuva em excesso, e valor negativo representa perda potencial de água nos meses secos quando o solo apresenta armazenamento restrito de água.

A negativa acumulada NEG.

ACUM representa o somatório da seqüência de valores negativos de P ETP.

O armazenamento de água solo (ARM) deve ser calculado pela fórmula abaixo, As colunas de NEG.

ACUM e ARM devem ser preenchidas simultaneamente.

Inicia-se o preenchimento da coluna NEG ACUM no primeiro mês em que aparecer o valor negativo de P-ETP, após um período de valores positivos de P-ETP.

Nesse primeiro mês o NEG ACUM será igual a P-ETP.

Com esse valor calcula-se o valor da coluna ARM pela equação descrita acima.

Se o primeiro mês também apresentar valor negativo de P-ETP, acumula-se este com o valor do mês anterior e utiliza-se esse valor para o cálculo de ARM.

Isso prossegue enquanto P-ETP for negativo.

Quando aparecer um mês com P-ETP positivo, após uma seqüência de P-ETP negativos, procede-se da seguinte forma, soma-se o valor positivo de P-ETP ao ARM do mês anterior, obtendo-se o ARM do mês em questão, que não deve ultrapassar o valor da CAD.

Com esse valor de ARM obtém-se o NEG ACUM do mês pela inversão da equação abaixo.

Após o preenchimento das colunas de NEG ACUM e ARM, prossegue-se com o preenchimento dos demais parâmetros, um de cada vez.

ALT significa a alteração no armazenamento.

É obtida pela diferença entre o ARM do mês em questão e o ARM do mês anterior, se o balanço hídrico for mensal.

Adota-se a questão temporal que estiver trabalhando.

No caso deste trabalho, a escala é diária, então ALT é a diferença entre o ARM do dia em questão e o ARM do dia anterior.

ETR representa a evapotranspiração real, aquela que realmente ocorre em função da disponibilidade de água no solo.

DEF representa a deficiência hídrica, ou seja, a falta de água no solo, sendo calculada por, E finalmente, o EXC.

Representa o excedente hídrico, que é a quantidade de água que sobra no período chuvoso e se perde por percolação drenagem profund e/ou escorrimento superficial.

A planilha com os cálculos do balanço hídrico está disponibilizada no anexo 1 (anexo digital).

A partir desse cálculo do balanço hídrico, passou-se a conhecer o atributo DAAS (disponibilidade de água atual no solo), utilizado para calcular as classes do conjunto de dados de deficiência hídrica.

Tendo em mãos os valores numéricos de DAAS foi preciso discretizá-los em classes categóricas para distribuir os dados nas respectivas classes adotadas para deficiência hídrica.

Como sugestão do especialista, os valores do atributo DAAS foram divididos em 10%, 25% e 50% dos dados e foi adotado o seguinte critério, Deficiência hídrica é forte quando obtiver valores menores ou iguais a 10% dos valores de DAAS.

Deficiência hídrica é moderada quando obtiver entre 10 a 25% dos valores de DAAS.

Deficiência hídrica é fraca quando obtiver valores entre 25% a 50% dos valores de DAAS e Não existe deficiência hídrica quando a DAAS ocupar valores maiores que 50% dos valores de DAAS.

Sendo assim, constituindo o banco de dados de deficiência hídrica de Sete Lagoas, haviam 8245 casos na classe não, 1697 na classe forte, 248na classe moderada e 4108 na classe fraca.

Para o conjunto de dados de Piracicaba, as classes ficaram divididas em 1211 não, 436 forte, 1209 na classe moderada e 12700 na fraca.

Neste conjunto de dados as classes positivas não poderiam ser unificadas, como foi o caso de geada, pois o interessante deste problema de deficiência hídrica era saber quando a classe fraca seria atingida, uma vez que ainda haveria tempo hábil para tomar alguma decisão antes que as classes mais comprometidas fossem alcançadas, como a moderada e a forte.

Isto compreende um programa de sistema de alerta.

Como sugestões dos especialistas, foram necessárias outras transformações nos conjuntos de dados, tanto para geada como para deficiência hídrica.

Foi necessário o deslocamento, em até 8 dias de antecedência ao evento, dos valores no atributo-classe.

Em cada linha do conjunto de dados, a informação era do dia, ou seja, referia-se ao dia atual.

Deslocando a saída (classe) um dia para trás, tinha-se a mesma informação na linha do conjunto de dados, porém a saída se referia ao dia seguinte.

Isto foi feito até 8 dias de antecedência para o conjunto de dados geada e até 15 dias de antecedência ao conjunto de dados deficiência hídrica.

Sendo assim, transformou-se o banco de dados original para 8 bancos de dados para geada e 15 bancos de dados para deficiência hídrica.

Mais tarde foi proposta uma nova construção de novos conjuntos de dados para deficiência hídrica, descrito mais adiante.

Os problemas nos dados surgem a partir de dados com ruídos e outliers, dados redundantes e/ou duplicados e conjunto de exemplos não balanceados.

Sendo assim, foi necessária uma limpeza nos dados antes de ir para a fase de extração de padrões.

Referente à análise de ruído e outliers, verificou-se cada atributo dos conjuntos de dados geada e deficiência hídrica, aplicando um filtro do próprio software Excel e não foram encontrados ruídos nos dados.

Quanto ao conjunto de dados geada, percebeu que existiam outliers no atributo temperatura mínima e seus dependentes, porém, não categorizava como um ruído.

Por exemplo, poucas eram as ocorrências de temperaturas mínimas inferiores a2ºC, mas quando existiam era este atributo que fazia que a classe fosse identificada como ocorrência de geada positiva, sendo então de importante valor.

Referente ao desbalanceamento das classes, foram aplicadas algumas técnicas de balanceamento de classes a fim de deixar as classes equiparadas, tanto no conjunto de dados geada como no de deficiência hídrica.

Ficou definido, juntamente com os especialistas, que o conjunto de dados geada de Ponta Grossa seria utilizado para o treinamento do modelo e o conjunto de dados de Londrina seria utilizado para a validação do modelo, devido o de Ponta Grossa conter mais casos positivos à ocorrência de geada.

O mesmo ocorreu com o conjunto de dados deficiência hídrica, utilizou-se o conjunto de dados de Sete Lagoas para o treinamento e o de Piracicaba, para a validação do modelo.

Sendo assim, a base de Ponta Grossa e de Sete Lagoas é que receberam tratamento de balanceamento das classes.

Os conjuntos de dados ainda não eram ideais para constituir um banco de dados para extrair padrões, os resultados não possuíam qualidade, eram incompreensíveis.

Ainda existia uma diferença significativa entre as classes, isto é, a proporção entre as classes era muito desbalanceada.

Sendo assim, referente ao banco de geada, os especialistas sugeriram retirar os meses que eram muito difíceis de ocorrer geada, como janeiro, fevereiro, março, outubro, novembro e dezembro.

Portanto, análise foi realizada somente com os meses de abril até setembro.

Referente ao conjunto de dados de deficiência hídrica, os dados presentes referiam-se a dados diários e a todos os meses do ano (janeiro a dezembro).

Conforme discussão entre os especialistas, ficou concluído que era necessário dividir o banco de dados, construindo um banco de dados para cada mês do ano, ou seja, um banco de dados para janeiro, outro para fevereiro e assim por diante, até dezembro, pois cada mês apresentava um comportamento diferente quanto à disponibilidade de água atual no solo e conseqüentemente influenciava na classificação de deficiência hídrica.

Portanto, foi refeita a classificação da deficiência hídrica para cada mês, de acordo com o critério adotado, isto é, forte deficiência para 10% ou menos dos valores de DAAS do mês.

Moderada quando obtiver entre 10 a 25% dos valores de DAAS.

Fraca quando obtiver valores entre 25% a 50% dos valores de DAAS e não existe deficiência hídrica quando a DAAS ocupar valores maiores que 50% de DAAS.

A partir desta preparação dos dados, foram obtidos 180 conjuntos de dados para deficiência hídrica.

Isto é explicado devido anteriormente ter construído 15 conjuntos de dados devido ao deslocamento da classe para um dia antes do dia ocorrido o fenômeno e, a partir deste processamento, novos 1conjuntos de dados, um para cada mês, resultando assim em 180 conjuntos de dados.

Mesmo com as sugestões dos especialistas, o conjunto de dados de geada ainda apresentava uma diferença significativa entre as classes.

Sendo assim, fez-se necessário trabalhar com o desbalanceamento das classes, aplicando os métodos sugeridos pela literatura a fim de balancear as classes nos conjuntos de dados.

A princípio, os seguintes métodos de balanceamento foram aplicados no conjunto de dados geada (Ponta Gross para 1 dia de antecedência, over-aleatório, under-aleatório, ENN (Neighborhood Cleaning Rule), SMOTE, CNN (Condensed Nearest Neighbor Rule) e NCR (Neighborhood Cleaning Rule).

Esses tratamentos de balanceamento das classes foram aplicados no ICMC-USP, utilizando o ambiente DISCOVER.

Este ambiente aplica o algoritmo C45, pois é um dos métodos mais utilizados na construção de modelos de classificação e muito utilizado na avaliação de algoritmos de aprendizado em domínios com classes desbalanceadas.

Todos os experimentos foram realizados utilizando validação cruzada com 10 partições.

Os resultados foram avaliados utilizando-se a AUC (Area under the ROC curve) e serão apresentados no próximo capítulo.

Após aplicação do método de balanceamento no conjunto de dados de geada obteve-se uma quantidade de linhas muita reduzida em relação ao conjunto de dados apresentado inicialmente, aproximadamente 90%.

Quanto ao conjunto de dados de deficiência hídrica, foram aplicados alguns métodos de balanceamento artificial e os resultados não apresentaram melhoras surpreendentes como no banco de dados geada, em torno de 1% no acerto geral, no máximo.

Além disso, o conjunto de dados deficiência hídrica não apresentava um desbalanceamento, como no caso de geada.

Sendo assim, não foi aplicado nenhum método artificial para o balanceamento das classes.

Para tratar os atributos que eram redundantes ou duplicados, foram utilizados vários métodos para selecionar aqueles que eram importantes para conseguir extrair padrões das bases de dados.

A princípio, os atributos foram analisados através da mineração visual dos dados, ou seja, descrevendo os conjuntos de dados por meio de gráficos envolvendo múltiplas variáveis simultaneamente e sempre comparando a influência de cada atributo com o atributo-classe.

Isto foi feito no programa WEKA e, discutindo com os especialistas, percebeu-se que alguns atributos não tinham influência na classificação do problema.

Para confirmar os melhores atributos para constituir o conjunto de dados, foram aplicados alguns métodos de seleção de atributos.

Os seguintes métodos foram testados nos conjuntos de dados geada e deficiência hídrica, InfoGain, GainRatio, Qui-quadrado e CFS.

Comparando os melhores resultados, o teste do Qui-quadradofoi escolhido para atuar em todos os banco de dados em questão.

Todos os testes de seleção de atributos também foram realizados no programa WEKA.

Após aplicação do teste Qui-quadrado, os conjunto de dados ficaram reduzidos.

Todos os atributos do conjunto de dados geada e do conjunto de dados de deficiência hídrica, assim como os selecionados após a aplicação do método do Qui-quadrado em relação a cada dia de antecedência ao evento geada e deficiência hídrica, até 8 dias e 15 dias de antecedência, respectivamente, são apresentados no próximo capítulo, na seção dos resultados.

A partir de então, os conjuntos de dados, de geada e de deficiência hídrica, ficaram prontos para serem minerados, ou seja, prontos para serem inseridos na fase de processamento dos dados a fim de descobrir padrões dos conjuntos dos dados.

Nesta fase de mineração de dados, normalmente são definidas, escolha da tarefa e das técnicas a serem utilizadas, identificação da ferramenta e aplicação desta aos dados.

A escolha da tarefa e da técnica a ser aplicada ao problema deste trabalho já foi definida na etapa da identificação e entendimento do problema.

Nesta fase de mineração de dados foi definida somente a identificação da ferramenta e a aplicação desta aos dados.

Todos os conjuntos de dados foram separados em conjuntos de treinamento e em conjuntos de testes, utilizando-se validação cruzada com 10 partições.

Para o problema geada foi utilizado o algoritmo C45 para a construção das árvores de decisão, no ambiente DISCOVER, desenvolvido no Instituto de Ciências Matemáticas e de Computação ICM da USP São Carlos.

Referente ao problema deficiência hídrica, como não foi adotado o balanceamento das classes, todos os pré-processamentos e processamentos dos dados foram realizados no software WEKA, utilizando o algoritmo J48, resultando nas árvores de decisão.

Como resultados, foram gerados 8 conjuntos de regras de classificação para o conjunto de dados geada e 180 conjuntos de regras de classificação para o conjunto de dados deficiência hídrica.

Estes resultados estão apresentados no próximo capítulo.

Neste capítulo serão abordados os resultados obtidos durante o processo KDD.

Inicialmente, serão mostrados os resultados obtidos durante a preparação dos dados, no pré-processamento e, em seguida, os resultados obtidos no pós-processamento dos dados e na validação do conhecimento obtido.

Os resultados serão apresentados e discutidos separadamente para os experimentos de geada e de deficiência hídrica.

O conjunto de dados "crus" de geada, ou seja, da maneira que foi fornecido inicialmente pelo IAPAR, foi submetido à fase de mineração de dados, a fim de conhecer os resultados preliminares.

O atributo temperatura mínima do ar foi retirado do conjunto de dados inicial.

Com a presença deste atributo, o conjunto de dados de geada apresentou um resultado de 100% de acerto para a classe sim e para a classe não e uma árvore de decisão de linhas.

Isto ficou evidente que era devido à presença deste atributo, uma vez que era este que fazia que a classe sim fosse identificada como ocorrência de geada positiva, ou seja, a presença do atributo temperatura mínima do ar menor ou igual a 4ºC ser referência na classificação de geada, para este conjunto de dados.

Com a ausência deste atributo, o conjunto de dados foi processado no minerador de dados, obtendo um resultado precário, acertando somente 3198% na classe sim.

A partir deste resultado, tomado como referência inicial, percebeu-se a necessidade de aplicar todos os recursos necessários da fase de pré-processamento.

Todos os resultados mostrados a seguir foram gerados a partir de experimentos sem a presença do atributo temperatura mínima do ar do dia.

Quando este atributo se refere aos dias anteriores ao evento geada, este foi mantido presente no conjunto de dados.

A partir daí, foi dado continuidade aos experimentos, porém com tratamento específico nos dados para cada fase do pré-processamento.

O conjunto de dados de geada, de Ponta Grossa, foi submetido a alguns tratamentos a fim de que suas classes fossem balanceadas, deixando-as niveladas.

No primeiro tratamento, conforme sugestão dos especialistas, foram retirados alguns meses em que era difícil a ocorrência do fenômeno geada, janeiro, fevereiro, março, outubro, novembro e dezembro.

A quantidade de valores existentes em cada classe para cada banco de dados para geada de Ponta Grossa, constando apenas os meses que ocorre o evento geada é apresentada.

Quantidade de valores nas classes sim e não para o conjunto de dados geada de Ponta Grossa, após retirada dos meses janeiro, fevereiro, março, outubro, novembro e dezembro.

Mesmo retirando os meses que os especialistas sugeriram e com isso aumentando o número de casos positivos, estes ainda não estavam em quantidade suficiente para constituir um banco de dados ideal para conseguir extrair padrões, isto é, a proporção entre as classes positivas e negativas ainda estava muito desbalanceada.

Sendo assim, foi necessário aplicar um método de balanceamento de classes.

O grau de acerto em cada classe quando aplicado os métodos de balanceamento, over-aleatório, under-aleatório, under-aleatório e ENN, under-aleatório e SMOTE, under-aleatório e CNN e under-aleatório e NCR, são apresentados.

Resultados da aplicação dos métodos de balanceamento ao conjunto de dados de geada de Ponta Grossa para 1 dia de antecedência, sem os meses que eram difícil a ocorrência de geada.

Pode-se perceber pelos resultados apresentados, que mesmo sem os meses críticos, ou seja, meses em que o fenômeno geada é de difícil ocorrência, quando o conjunto de dados não recebe nenhum tipo de tratamento de balanceamento das classes, os resultados ainda não foram aceitáveis pelos especialistas, acertando apenas 5646% na classe sim.

Aplicando os métodos de balanceamento das classes, pode-se notar que os resultados melhoraram.

O melhor resultado na classe sim foi encontrado com o método under aleatório mais ENN.

Assim, este foi o método adotado para os conjuntos de dados para previsão de geada com até 8 dias de antecedência.

Após a aplicação do método de balanceamento no conjunto de dados de geada obteve-se uma quantidade de linhas muita reduzida em relação ao conjunto de dados apresentado inicialmente.

A quantidade de linhas para cada dia de antecedência ao evento geada após o balanceamento das classes é mostrada.

Tamanho dos conjuntos de dados de geada após balanceamento das classes através do método ENN.

Para tratar os atributos que eram redundantes foram utilizados alguns métodos para selecionar os atributos que eram relevantes para a base de dados.

Foram aplicados os métodos Qui-quadrado, InfoGain, GainRatio e CFS.

O grau de acerto geral para cada método é apresentado.

Resultados do tratamento no conjunto de dados de geada de Ponta Grossa para selecionar os atributos relevantes.

Entre eles, o método Qui-quadrado foi o que apresentou melhor resultado (8766%) quando comparado com os outros métodos, InfoGain, GainRatio e CFS.

Os resultados do teste Qui-quadrado estão apresentados no anexo (anexo digital).

A quantidade de atributos em cada dia de antecedência ao evento geada, após a aplicação do método Qui-quadrado, é apresentada.

Quantidade de atributos no conjunto de dados geada de Ponta Grossa, antes, no modelo original, e após a aplicação da seleção de atributos com o método Qui-quadrado.

Pode-se perceber que a quantidade de atributos ficou bem reduzida em relação ao banco de dados original, sinalizando que existiam muitos atributos que eram redundantes e que poderiam estar interferindo na qualidade do conhecimento adquirido.

Os atributos do conjunto de dados de geada, assim como os selecionados após a aplicação do método do Qui-quadrado em relação a cada dia de antecedência ao evento geada são apresentados no anexo (anexo digital).

Todos os atributos que foram selecionados, mostrando-se relevantes para o conjunto de dados estiveram presentes no corpo da árvore de decisão.

Aqueles que não são importantes, obviamente não fizeram parte da construção do modelo.

De maneira geral, quanto mais próximo ao momento de ocorrência de geada, alguns atributos são mais relevantes, o que significa dizer que quanto maior a antecedência da ocorrência de geada, os atributos interferem menos na sua previsão.

É o caso, por exemplo, dos atributos Temperatura mínima do ar, Temperatura do bulbo seco e do bulbo úmido das 9, 15 e 21 horas, Precipitação acumulada, Direção do vento das 9 e 15 horas e Direção do pico.

Alguns atributos não são importantes na previsão de geada, incluindo ruídos no conjunto de dados quando presentes.

É o caso dos atributos El Niño, La Niña, amplitude de temperatura, umidade relativa das 9, 15 e 21 horas, umidade relativa média com exceção para predições com até dias de antecedência ao evento gead, vento acumulado a 10 metros, precipitação, insolação, velocidade do vento das 9, 15 e 21 horas e pico máximo do vento.

E ainda existem alguns atributos que são importantes na previsão de geada, mesmo distante do evento.

São eles, Mês, Fase da Lua com exceção nas predições de 1 e dias de antecedênci, Temperatura máxima do ar, Temperatura média do ar e Direção do vento das 21 horas.

Conforme sugestão dos especialistas, a classificação para deficiência hídrica foi calculada segundo os valores de disponibilidade de água atual no solo (DAAS) para cada mês em questão, de janeiro a dezembro.

Os valores de DAAS são apresentados.

Classificação da deficiência hídrica em função dos valores de disponibilidade de água atual no solo (DAAS), adotado para cada mês do ano, a partir do cálculo do percentil de 10%, 25% e 50% dos valores de DAAS.

A partir da definição das classes para deficiência hídrica, pode-se calcular a quantidade de ocorrências de casos em relação às classes não, forte, moderada e fraca, até 15 dias de antecedência ao evento deficiência hídrica para cada mês do ano.

Lagoas, após calcular os valores de DAAS e atribuí-los até 15 dias de antecedência ao evento deficiência hídrica para cada mês do ano.

Hídrica de Sete Lagoas, após calcular os valores de DAAS e atribuí-los até 15 dias de antecedência ao evento deficiência hídrica para cada mês do ano.

Em todos os meses, e para todos os dias de antecedência ao evento deficiência hídrica, a classe não é a que apresenta o maior número de casos, seguida da classe fraca, moderada e forte.

As classes moderada e forte estão em quantidades equivalentes na quantidade de casos.

A partir dessa divisão é que foi proposto o balanceamento das classes para o conjunto de dados de deficiência hídrica.

Foram aplicados alguns métodos de balanceamento artificial de classes no conjunto de dados de deficiência hídrica, porém os resultados não apresentaram melhoras significativas como no caso de geada, sendo aproximadamente 1% a diferença no acerto geral, no máximo.

Além disso, o conjunto de dados deficiência hídrica não apresentava um desbalanceamento drástico, como no caso de geada.

Sendo assim, optou-se por não aplicar qualquer método artificial para o balanceamento das classes no conjunto de deficiência hídrica.

Para notar os atributos que eram relevantes no banco de dados de deficiência hídrica, foram aplicados alguns métodos de seleção de atributos, entre eles InfoGain, GainRatio, Qui-quadrado e o CFS para o conjunto de dados do mês de janeiro.

O resultado do grau de acerto geral é mostrado.

Resultados do tratamento no conjunto de dados de deficiência hídrica de Sete Lagoas para selecionar os atributos relevantes.

Entre todos esses métodos, o método do Qui-quadrado é que apresentou melhor resultado (787%) e, assim, foi o utilizado para apontar os melhores atributos no conjunto de dados de deficiência hídrica.

A quantidade de atributos para deficiência hídrica após a aplicação do método do Qui-quadrado é mostrada.

Quantidade de atributos no conjunto de dados deficiência hídrica de Sete Lagoas, após a aplicação da seleção de atributos com o método Qui-quadrado.

A segunda coluna mostra que todos os bancos de dados de deficiência hídrica, independente do dia de antecedência ao evento, continham 150 atributos.

As demais colunas mostram que após a aplicação do método, os números de atributos foram reduzidos.

Pode-se perceber que, após a aplicação do método de seleção de atributos Qui-quadrado, a quantidade de atributos ficou reduzida em relação ao banco de dados original, interferindo e melhorando a qualidade do conhecimento obtido no final do processo KDD.

Após a aplicação da seleção dos melhores atributos, resultaram 180 conjuntos de dados, sendo um para cada mês do ano e para cada mês até 15 dias de antecedência.

No anexo (anexo digital) são mostrados os atributos selecionados para cada mês até 15 de antecedência.

Pelos resultados apresentados no anexo digital, pode-se perceber quais os atributos que são relevantes na previsão de deficiência hídrica, comentados a seguir.

De maneira geral, os atributos, quando estão mais próximo de acontecer o evento deficiência hídrica, mais importantes eles se tornam na previsão desse evento, isto é, quanto mais distante do evento, menos relevantes esses atributos são para o conjunto de dados, sendo que em alguns meses eles são considerados ruídos.

Para cada mês, existem alguns atributos que influenciam mais e outros menos na previsão da deficiência hídrica.

No mês de janeiro, temperatura média, amplitude de temperatura, precipitação acumulada, precipitação menos evapotranspiração, precipitação média de 15, 20 e 30 dias, dias acumulados com e sem precipitação, precipitação, evapotranspiração, evapotranspiração acumulada, temperatura mínima e temperatura máxima são atributos que próximos ao evento têm importância na previsão.

O atributo fases da lua não se mostra importante para a previsão de deficiência hídrica até 15 dias antes do evento.

E alguns atributos são importantes na previsão do evento de 1 até 15 dias de antecedência, como é o caso de precipitação média de 10 dias, El Niño e La Niña.

Para o mês de fevereiro, o comportamento dos atributos se repete como no caso de janeiro, apenas entre os atributos que são importantes de 1 até 15 de antecedência incluem a precipitação média de 15 e 20 dias, dias acumulados com e sem precipitação e evapotranspiração acumulada.

Para o mês de março, a precipitação acumulada, a precipitação média de 10, 15, 20 e 30 dias, dias acumulados com e sem precipitação e evapotranspiração acumulada apresentam como atributos importantes na previsão de deficiência hídrica de 1 até 15 dias de antecedência ao evento.

Neste mês o atributo fases da lua também mostra não influenciar na previsão de deficiência hídrica até 15 dias antes.

No mês de abril, os atributos que se dizem importantes na previsão, de 1 até 15 dias de antecedência, são amplitude de temperatura, precipitação acumulada, precipitação menos evapotranspiração, precipitação média de 10, 15, 20 e 30 dias, dias sem precipitação, evapotranspiração, evapotranspiração acumulada e temperatura máxima.

Os outros, quanto mais próximo do evento, mais importantes, e à medida que se vão distanciando do evento, pouco ou nada influenciam na previsão de deficiência hídrica.

Neste mês, até 5 dias de antecedência, o atributo fases da lua mostra ter influência na previsão de deficiência hídrica no solo.

Em maio, o comportamento dos atributos se repete como no mês de abril.

Em junho, o atributo temperatura média se comporta de maneira diferente dos outros meses e dos outros atributos.

Quanto mais distante do evento, mais ela se mostra importante para o modelo.

Entre os atributos importantes de 1 até 15 dias de antecedência, destacam-se precipitação acumulada, precipitação menos evapotranspiração, precipitação média de 10, 15, 20 e 30 dias, El Niño e La Niña, evapotranspiração, evapotranspiração acumulada.

Nos primeiros dias e próximos a 15 dias de antecedência, o atributo fases da lua tem influência na previsão.

Em julho, a temperatura média e a temperatura máxima mostram ter pouca ou quase nenhuma influência na previsão do fenômeno.

A precipitação acumulada e a evapotranspiração acumulada apresentam ter importância na previsão de 1 até 15 dias de antecedência ao evento.

Os demais atributos, quanto mais próximo do evento da deficiência hídrica, mais têm importância.

O comportamento de julho se repete em agosto, incluindo que a temperatura mínima também participa como um atributo que tem pouca influência na previsão.

Em setembro, a amplitude de temperatura, a precipitação acumulada, a precipitação menos a evapotranspiração, a precipitação média de 10, 15, 20 e 30 dias, dias sem precipitação, evapotranspiração, evapotranspiração acumulada e temperatura máxima mostram ter grande influência na previsão de 1 até 15 dias.

O atributo fases da lua volta a não ter influência em nenhum dia de antecedência.

Nos meses de outubro, novembro e dezembro a precipitação acumulada, a precipitação média de 10, 15, 20 e 30 dias, dias acumulados com e sem precipitação e evapotranspiração acumulada se apresentam como atributos importantes na previsão de deficiência hídrica de 1 até 15 dias de antecedência ao evento.

Neste mês o atributo fases da lua também mostra não influenciar na previsão de deficiência hídrica até 15 dias antes.

Para dezembro, pode-se perceber que o atributo El Niño não tem influência na previsão do evento.

Nesta etapa de pós-processamento, o conhecimento extraído pelos algoritmos de extração de padrões foi avaliado, bem como a qualidade do classificador.

Esta fase, conhecida como a etapa de avaliação e interpretação dos resultados, foi realizada juntamente com os especialistas e o analista dos dados.

Cada vez que os conjuntos de dados de geada e de deficiência hídrica foram modificados em alguma etapa do pré-processamento dos dados, foram também processados no minerador de dados.

Os resultados foram analisados pelos especialistas e avaliados pelo analista dos dados.

Após realizar todas as modificações sugeridas e necessárias foram obtidos todos os resultados.

Utilizou-se o conhecimento do especialista para avaliação do conteúdo das regras e o analista de dados avaliou se o classificador atingiu as expectativas, analisando os resultados de acordo com algumas métricas.

Essas métricas foram extraídas a partir da matriz de confusão de cada árvore de decisão gerada.

A matriz de confusão mostrou o número de classificações corretas em oposição às classificações preditas para cada classe.

Assim, foi possível avaliar os valores encontrados como falsos positivos (FP), falsos negativos (FN), verdadeiros positivos (TP) e verdadeiros negativos (TN) em cada árvore gerada.

Foram geradas 8 árvores de decisão, uma para cada dia de antecedência ao evento geada.

Todas as árvores, até 8 dias de antecedência, estão apresentadas no anexo 5 (anexo digital).

A árvore de decisão para o conjunto geada com 1 dia de antecedência ao evento geada é apresentada.

Árvore de decisão para o conjunto de dados de geada, de Ponta Grossa, para 1 dia de previsão.

Árvore de decisão, na forma estrutural, para o conjunto de dados de geada com 1 dia de antecedência.

A partir da árvore gerada foram criadas as regras apresentadas a seguir.

Se a temperatura do bulbo úmido das 21 horas do dia (TU21) for > 98ºC, então não ocorre geada no dia seguinte.

Se a temperatura do bulbo úmido das 21 horas do dia for 8ºC, então ocorre geada no dia seguinte.

E se, a temperatura do bulbo úmido das 21 horas do dia estiver entre 8 ºC e 98ºC e a velocidade do vento das 21 horas do dia (V_V21) for m/s, então ocorre geada no dia seguinte.

Pode-se notar que para prever geada com 1 dia de antecedência os atributos relevantes são a temperatura do bulbo úmido das 21 horas do dia e a velocidade do vento das 21 horas do dia.

Destaca-se que para conseguir prever geada para o dia seguinte é necessária a informação do dia e ainda próxima ao evento (21 horas).

Estas regras apresentam um grau de acerto geral de 8766% para 1 dia de previsão, sendo 8877% para a classe sim e 8695% para a classe não.

Segundo a visão dos especialistas, este resultado apresenta um ganho de informação surpreendente na literatura, valorizando o modelo desenvolvido, tanto o acerto para a classe sim como para a classe não, uma vez que contribui para o controle do sistema de previsão.

Destaca-se, também, que o modelo aqui desenvolvido tem influência na previsão local de geada e com este grau de acerto para previsão local, o resultado encontrado é excelente.

Comparando com os modelos para previsão de geada existentes atualmente, que chegam a prever o evento em até 100% de acerto com 1 dia de antecedência, os resultados deste trabalho são inferiores.

Porém valem ressaltar que aqueles modelos levam em consideração atributos diferentes, como massa de ar polar, imagens de satélites, entre outros e empregam metodologia distinta da utilizada na construção do modelo neste trabalho.

Outra diferença é que os modelos já existentes empregam a previsão de uma forma global e o modelo aqui desenvolvido é para uma aplicação local, como comentado acima.

Quando analisados os resultados das árvores geradas para dias de antecedência à geada, nota-se que se a temperatura do bulbo úmido das 21 horas do dia foi ainda menor TU21 7º, ocorrendo geada depois de dias.

Caso essa temperatura seja maior que 7ºC e ainda a temperatura do bulbo úmido das 15 horas de dias antes seja 10ºC, então se confirma a ocorrência de geada depois de dias.

Se a temperatura do bulbo úmido das 21 horas do dia estiver entre 7 ºC e 84ºC e a temperatura do bulbo úmido das 15 horas de dias antes for >10ºC e a umidade relativa das 15 horas do dia for 84% e ainda esta mesma umidade relativa das 15 horas, mas de 1 dia antes, for > 549%, então irá ocorrer geada depois de dias.

Se a temperatura do bulbo úmido das 21 horas do dia for > 84ºC e temperatura do bulbo seco das 9 horas de dias antes for 76ºC e a temperatura média de dias antes for > 81 ºC, então irá ocorrer geada depois de dias.

E ainda, se a temperatura do bulbo úmido das 15 horas de dias antes continuar menor que 10 ºC, mas a umidade relativa das 15 horas do dia for > 84% e a temperatura mínima de 5 dias antes for 15ºC, então irá ocorrer geada Árvore de decisão, na forma estrutural, para o conjunto de dados de geada com dias de antecedência.

Para prever geada com dias de antecedência é necessário mais atributos do que para prever geada com 1 dia de antecedência.

Leva-se ainda em consideração atributos de outros horários mais distantes do evento, como das 15 horas e 9 horas e ainda é necessária a informação de atributos de até 5 dias antes do dia que se está analisando, como é o caso do atributo temperatura mínima.

Isto implica em um grau de acerto geral menor.

As regras geradas para dias de antecedência têm um grau de acerto geral de 696%.

Percebe-se que a confiança do modelo gerado a partir de dias de antecedência em diante diminui, pois o seu grau de acerto começa a diminuir, resultando para 8 dias de antecedência igual a 61%.

Para dias de antecedência, as regras geradas mostram que a partir de abril, se a temperatura do bulbo seco das 21 horas do dia for 94ºC, então haverá geada depois de dias.

Se a temperatura do bulbo seco das 21 horas do dia for > 9ºC e a umidade relativa média de 1 dia antes for > 944%, então haverá geada depois de dias.

Se a umidade relativa de 1 dia antes diminuir e estiver entre abril e agosto e a direção do vento das 21 horas do dia for noroeste, sul, oeste, central e sudoeste então haverá geada depois de dias.

Se a direção do vento das 21 horas do dia for nordeste e a temperatura do bulbo úmido das 15 horas do dia for 119 m/s, então haverá geada depois de dias.

Se essa temperatura do bulbo úmido das 15 horas do dia aumentar e a precipitação acumulada de 6 dias for > 74mm, então haverá geada depois de dias.

Se essa precipitação acumulada de 6 dias diminuir e a temperatura do bulbo úmido das 21 horas de 8 dias atrás for 72ºC, então haverá geada depois de dias.

Se a direção do vento das 21 horas do dia for norte e a umidade relativa média de 1 dia antes for 752%, então haverá geada depois de dias.

Se a direção do vento das 21 horas do dia for leste e a temperatura do bulbo seco das 21 horas de 5 dias antes for 9ºC, então haverá geada depois de dias.

E se a direção do vento das 21 horas do dia for sudeste e a temperatura máxima do dia for 22ºC, então haverá geada depois de dias.

Nota-se que para prever geada com dias de antecedência, outros atributos são destacados.

O mês, a temperatura do bulbo seco das 21 horas, a direção do vento, a precipitação acumulada e a temperatura máxima fazem parte dos atributos para conseguir prever com antecedência de dias.

Essas regras apresentam 589% de acerto geral.

Comparando o grau de acerto com 1 dia de antecedência, 8766%, a diferença é bastante significativa.

Da mesma maneira foi feito para as árvores de 4, 5, 6, 7 e 8 dias de antecedência ao evento geada e foi mostrado ao especialista para discussão dos resultados.

Como comentado anteriormente, quanto mais distante do evento, menor é o acerto da previsão de geada e os atributos que são mostrados como relevantes mais distantes estão do dia da ocorrência da geada.

Vale ressaltar que o modelo criado vem contribuir juntamente com outros modelos já existentes na literatura indicando a ocorrência ou não do fenômeno.

A seguir, encontram-se as matrizes de confusão, com os valores de FP, FN, TP e TN e o grau de acerto geral para o conjunto de dados de geada para cada dia de antecedência.

Entre parênteses é apresentado o grau de acerto para cada classe.

Resultados da matriz de confusão, comparando os valores reais com os preditos com até 8 dias de antecedência ao evento geada.

Entre parênteses é apresentado o grau de acerto em cada classe.

A partir da matriz de confusão gerada para o caso geada, foi calculada a Área sob a Curva ROC AU, sendo esta uma medida da qualidade global do modelo.

Os resultados encontrados no cálculo da AUC para geada, na classe sim, para cada dia de antecedência, bem como o grau de acerto em verdadeiro positivo (TP) na classe sim e na classe não são mostrados.

Valores da AUC (Área sob a curva RO, mostrando a qualidade global do modelo gerado e o grau de acerto para o verdadeiro positivo (TP) para cada classe referente a cada dia de antecedência ao evento geada.

Pelos resultados apresentados, para o conjunto de dados de geada, pode-se notar pela AUC da classe sim e pelo grau de acerto nos casos verdadeiros positivos (TP) em cada classe, que para até 1 dia de antecedência têm-se valores consideráveis pela análise do especialista (8877%).

Porém, a partir de dias de antecedência ao evento geada, a qualidade do modelo diminui.

O grau de acerto na classe sim, a partir de dias de antecedência, é de aproximadamente 65%, diminuindo à medida que aumentam os dias de antecedência ao evento geada.

Pode-se perceber que a partir de 6 dias de antecedência ao evento, o grau de acerto na classe sim aumenta e praticamente se mantém na faixa de 65% de acerto.

Foram feitos outros experimentos com mais de 8 dias de antecedência, com até 15 dias de antecedência e foi notado que existia uma oscilação no grau de acerto, entre aumentar e diminuir, mas nunca atingiu e ultrapassou o grau de acerto de 1 dia de antecedência, mantendo-se também entre 60% de acerto.

Na literatura consultada e segundo os especialistas que contribuíram com este trabalho, seria impossível ter um grau de acerto como 1 dia de antecedência para muitos dias afastados ao evento geada e desconhecem o motivo da oscilação de acerto a partir de 6 dias de antecedência.

Foram geradas, para cada mês, 15 árvores de decisão, sendo uma para cada dia de antecedência ao evento.

As árvores correspondentes ao mês de janeiro até dezembro, com até 15 de antecedência ao evento, encontram-se no anexo 6 (anexo digital).

Referente ao conjunto de dados de deficiência hídrica, a árvore de decisão para 1 dia de antecedência à deficiência hídrica, para o mês de janeiro é apresentada.

O desenho da árvore não foi mostrado por ser de difícil interpretação, uma vez que a árvore é muito grande.

Árvore de decisão, na forma estrutural, para o conjunto de dados de deficiência hídrica, para o mês de janeiro, com 1 dia de antecedência.

Dependendo da classe, os atributos envolvidos são diferentes e são necessários dias anteriores para conseguir prever a deficiência hídrica do dia seguinte.

Foram feitas as análises de até 15 dias de antecedência, para todos os meses e apresentadas aos especialistas para discussões, como se fez para 1 dia de antecedência.

As matrizes de confusão para o mês de janeiro, de 1 até 15 dias de antecedência ao evento deficiência hídrica no solo estão apresentadas.

São apresentados também a acurácia referente ao acerto geral e o percentual de acerto para o verdadeiro positivo referente a cada classe.

As demais matrizes de confusão, referentes aos meses de fevereiro até dezembro se encontram no anexo 7 (anexo digital).

Matriz de confusão, comparando os valores reais com os preditos para 1 dia de antecedência ao evento deficiência hídrica (acurácia, 782%).

Matriz de confusão, comparando os valores reais com os preditos para dias de antecedência ao evento deficiência hídrica (acurácia, 706%).

Matriz de confusão, comparando os valores reais com os preditos para dias de antecedência ao evento deficiência hídrica (acurácia, 634%).

Matriz de confusão, comparando os valores reais com os preditos para dias de antecedência ao evento deficiência hídrica (acurácia, 577%).

Matriz de confusão, comparando os valores reais com os preditos para 5 dias de antecedência ao evento deficiência hídrica (acurácia, 542%).

Matriz de confusão, comparando os valores reais com os preditos para 6 dias de antecedência ao evento deficiência hídrica (acurácia, 531%).

Matriz de confusão, comparando os valores reais com os preditos para 7 dias de antecedência ao evento deficiência hídrica (acurácia, 5031%).

Matriz de confusão, comparando os valores reais com os preditos para 8 dias de antecedência ao evento deficiência hídrica (acurácia, 516%).

Matriz de confusão, comparando os valores reais com os preditos para 9 dias de antecedência ao evento deficiência hídrica (acurácia, 497%).

Matriz de confusão, comparando os valores reais com os preditos para 10 dias de antecedência ao evento deficiência hídrica (acurácia, 505%).

Matriz de confusão, comparando os valores reais com os preditos para 11 dias de antecedência ao evento deficiência hídrica (acurácia, 502%).

Matriz de confusão, comparando os valores reais com os preditos para 1dias de antecedência ao evento deficiência hídrica (acurácia, 513%).

Matriz de confusão, comparando os valores reais com os preditos para 1dias de antecedência ao evento deficiência hídrica (acurácia, 508%).

Matriz de confusão, comparando os valores reais com os preditos para 1dias de antecedência ao evento deficiência hídrica (acurácia, 483%).

Matriz de confusão, comparando os valores reais com os preditos para 15 dias de antecedência ao evento deficiência hídrica (acurácia, 480%).

A partir das matrizes de confusão foi possível calcular o grau de acerto para cada classe.

Para os conjuntos de dados de deficiência hídrica, de 1 até 15 dias de antecedência, para todos os meses, os resultados de acerto das árvores de decisão são apresentados, mostrando o grau de acerto geral e o grau de acerto dos verdadeiros positivos (TP), em cada classe.

Grau de acerto geral e específico para cada classe, demonstrando os valores de Verdadeiros Positivos (TP), para cada dia de antecedência ao evento deficiência hídrica, para cada mês do ano.

Pelos resultados encontrados, a classe de maior acerto, ou seja, a que apresentou maiores verdadeiros positivos (TP) é sempre a classe não para todos os dias de antecedência, com um grau de acerto igual ou maior a 70%.

Isto significa que o modelo criado consegue prever a não ocorrência do evento deficiência hídrica com um grau de acerto satisfatório, segundo o especialista, para até 15 dias de antecedência ao evento.

Para 1 e dias de antecedência ao evento, pode-se perceber que o grau de acerto geral é maior durante os meses em que a deficiência hídrica não é comum, como janeiro a maio e setembro a dezembro e apresenta resultado menor nos meses em que a deficiência hídrica já é esperada, como junho, julho e agosto.

É importante destacar que prever deficiência hídrica, através da metodologia KDD, é uma atividade que está em desenvolvimento, se iniciando, não apresenta comparações com outros modelos existentes na literatura para fins de relato de benefícios.

Segundo os especialistas, esses resultados são de grande valia.

Saber que ocorrerá a deficiência hídrica nos meses em que ela já é esperada auxilia, porém saber que acontecerá a deficiência nos meses em que não se espera por ela, contribui na tomada de decisão.

Para 1 até dias de antecedência, o maior acerto, após a classe não, está a classe forte seguida da classe fraca e após a classe moderada, com exceção nos meses de junho, julho e agosto.

Nesses meses a ordem entre as classes fraca, moderada e forte se alterna.

Como comentado anteriormente, já se sabe que nesses meses a deficiência hídrica é fato de acontecer, independente da classe que ela apareça.

Nos outros meses, principalmente em janeiro, fevereiro e março, é importante que o modelo consiga prever a classe forte com o grau de acerto em torno de 70%, com até dias de antecedência, julgado muito satisfatório para os especialistas da área.

A partir de dias de antecedência ao evento deficiência hídrica, o grau de acerto nas classes forte, moderada e fraca é muito baixo, 50% ou menos.

Isto equivale dizer que o modelo gerado para prever a ocorrência de deficiência hídrica após dias de antecedência não é confiável.

DIFERENTES Para analisar e avaliar o conhecimento obtido, o modelo criado foi validado em outras bases de dados, diferentes das utilizadas no treinamento ou no teste.

Assim, com a validação realizada em bases de dados diferentes pode-se concluir que o modelo criado poderia ser utilizado em outras bases de dados, usufruindo do conhecimento obtido.

Para o caso geada, o modelo foi treinado na base de dados de Ponta Grossa e validado na base de Londrina.

Os valores da AUC e o grau de acerto são apresentados.

Valores da AUC Área sob a curva RO, grau de acerto geral e específico para cada classe para a base de dados de Londrina, utilizada na validação dos dados no conjunto de dados de geada.

Analisando os resultados encontrados na validação dos dados, pode-se notar que para 1 dia de antecedência, o valor da AUC e o grau de acerto nas classes sim e não são bastante satisfatórios, condizendo com a análise do especialista.

Para 1 dia de antecedência, o modelo validado acerta 100% para a classe sim, 968% para a classe não, apresenta um grau de acerto geral de 968% e a AUC com valor de 983%.

Isto mostra que o modelo gerado pode ser utilizado para a previsão de geada para 1 dia de antecedência utilizando bases de dados diferentes.

O mesmo não acontece para dias de antecedência.

Apesar de aparecer um valor alto para a classe não para dias de antecedência (911%), um grau de acerto geral também alto (906%) e o valor de AUC de aproximadamente 74%, o grau de acerto na classe sim é muito baixo (596%), sendo esta a principal classe no objetivo do trabalho.

Com dias de antecedência ao evento geada, os resultados são altos tanto para a classe sim como para o valor da AUC, 865% e 741%, respectivamente.

Porém para a classe não e acerto geral do modelo, os resultados foram em torno de 67%.

Vale ressaltar que no conjunto de dados de Londrina havia muito poucos casos positivos, eram 5pertencentes à classe sim e 7465 pertencentes à classe não.

Estes resultados concordam com os mesmos gerados pela árvore de decisão quando o modelo foi criado, utilizando o conjunto de dados de Ponta Grossa e ainda condiz com o que os especialistas analisaram.

Apesar dos resultados oscilarem a partir de dias de antecedência ao evento, como é o caso do grau de acerto na classe sim decrescer até 6 dias de antecedência e com 7 dias de antecedência a geada o grau de acerto aumentar, foram realizados alguns experimentos com mais de 8 dias de antecedência, com até 15 dias de antecedência, como se fez na criação do modelo a fim também de verificar tal variação.

A oscilação continua a acontecer com tendência sempre com um grau de acerto baixo, nunca igual ou maior que 1 dia de antecedência.

É importante destacar que para o caso geada, a base de dados utilizada na criação do modelo é da mesma região geográfica em que a base de dados que o modelo foi validado Londrina e Ponta Gross, ou seja, ambas são do Estado do Paraná.
O que significa dizer que os atributos possuem o mesmo comportamento, os valores nos atributos, nas duas bases de dados, não são discrepantes, o que contribuiu para que a validação tivesse resultados muito bons pelo menos para 1 dia de antecedência, como o modelo criado.

Balanço hídrico de Londrina e de Ponta Grossa.

Nota-se que os meses em que existem altos e baixos nos valores de excedentes hídricos são equivalentes nas duas estações, tanto de Londrina como de Ponta Grossa, e em ambas não existe deficiência hídrica.

Esses gráficos são os balanços hídricos normais, ou seja, considerando as médias mensais de temperatura e precipitação.

Para deficiência hídrica, o modelo foi treinado na base de Sete Lagoas e validado na base de dados de Piracicaba.

Os valores de acerto quando o modelo foi treinado na base de Sete Lagoas e validado na base de dados de Piracicaba são mostrados.

Valores de grau de acerto para a base de dados de Piracicaba, utilizada na validação dos dados.

Pelos resultados apresentados na validação do conjunto de dados de deficiência hídrica na base de dados de Piracicaba, pode-se perceber que não são satisfatórios.

O grau de acerto durante os meses do ano é muito baixo.

Apenas na classe não, e ainda, apenas em alguns meses, destacado em negrito, existe um grau de acerto satisfatório conforme análise do especialista.

E mesmo assim, esse acerto é mais freqüente nos dias mais próximo ao evento, dias mais distante a ocorrência desses acertos diminui, prejudicando a validação do modelo.

Ressalta-se também que a distribuição de dados nas classes na base de dados de Piracicaba é equivalente com a distribuição de dados nas classes da base de dados de Sete Lagoas.

Isto conclui que a quantidade de dados em cada classe não interferiu nos resultados apresentados.

Um dos motivos, no caso de deficiência hídrica, é que o modelo foi criado em uma base de dados do Estado de Minas Gerais e foi validado em outra base de dados de outro Estado, de São Paulo, existindo um comportamento discrepante entre os valores dos atributos.

Isto pode ser notado, com a demonstração dos gráficos do balanço hídrico de Sete Lagoas e Piracicaba.

Balanço hídrico de Sete Lagoas e de Piracicaba.

Nota-se que o comportamento nos meses em que existe a deficiência hídrica são diferentes para as duas estações, apresentando comportamento diferente nos dados e nos valores dos atributos.

Além disso, os valores em que ocorrem a deficiência hídrica também são diferentes, para Sete Lagoas a deficiência chega a aproximadamente 50 mm e em Piracicaba este valor não ultrapassa 20 mm.

Esses gráficos são os balanços hídricos normais, ou seja, considerando as médias mensais de temperatura e precipitação.

Este trabalho mostrou-se satisfatório, pois o objetivo proposto foi atingido, ou seja, o desenvolvimento de um modelo para previsão local de geada e de deficiência hídrica com um grau de confiança aceitável e um intervalo de tempo adequado.

Além disso, os bancos de dados climáticos foram analisados e foram identificadas relações entre os parâmetros climáticos, descobrindo novos conhecimentos entre esses parâmetros.

Para isso, foi utilizada a metodologia de descoberta de conhecimento em banco de dados KDD, que mostrou-se adequada para o desenvolvimento de um modelo de previsão local de geada e deficiência hídrica, auxiliando os produtores na tomada de decisão, visando a proteção contra essas ocorrências e reduzindo impactos severos.

No desenvolvimento deste trabalho pode-se perceber que é possível a aplicação do processo KDD na área agrícola, possibilitando a previsão local de geada e de deficiência hídrica.

Foi possível conceituar e detalhar os passos da descoberta de conhecimento em banco de dados e sua aplicação na área agrícola.

O processo KDD foi compreendido, delimitando as etapas que o compõem, apresentando as fases de identificação e entendimento do problema, o pré-processamento, a mineração de dados, o pós-processamento e a utilização do conhecimento obtido.

O material disponível para a realização do experimento foi considerado satisfatório no desenvolvimento do trabalho.

A utilização das ferramentas WEKA e do DISCOVER foram considerados eficientes, permitindo que se atingissem os objetivos propostos.

As bases de dados disponíveis atenderam a necessidade para a realização do projeto, tiveram tamanho de série e qualidade suficientes para análises consistentes, apresentando um volume de dados e atributos adequados para que pudesse gerar resultados para a previsão local de geada e de deficiência hídrica.

Os atributos selecionados em conjunto com os especialistas da área de geada e de deficiência hídrica mostraram-se ser suficientes para a realização do trabalho.

A aplicação de uma metodologia para selecionar os atributos mais relevantes para a aplicação da mineração nos dados também se mostrou eficiente e eficaz para a realização do trabalho, melhorando significativamente os resultados obtidos.

No caso de geada, como existiam poucos casos da classe sim, ficou evidente a necessidade de se fazer o balanceamento das classes, uma vez que criar um modelo com a quantidade original dos dados produzia um resultado insatisfatório.

Referente aos resultados apresentados no Capítulo 4, para até 1 dia de antecedência à geada, o modelo gerado é confiável.

Porém, a partir de dias de antecedência à geada, a qualidade dos resultados encontrados decai, diminuindo o acerto quanto mais distante estiver de acontecer o evento geada.

Para o caso deficiência hídrica, os resultados encontrados foram interessantes, na visão dos especialistas, dependendo da classe.

Para a classe não, independentemente ao número de dias de antecedência, o grau de acerto foi alto.

A classe forte, após a classe não, é a que apresenta melhores resultados de acerto, decaindo para as outras classes.

Para até dias de antecedência ao evento deficiência hídrica e, dependendo do mês, o grau de acerto é aceitável.

De dias em diante, os resultados mostram que o modelo gerado não é aceitável.

Em relação à validação dos modelos criados, fica claro que a base de dados treinada deve ter o mesmo comportamento nos dados, ou seja, a base de dados treinada deve apresentar atributos que tenham escala de valores semelhantes à base de dados que será validada e que esses valores não sejam discrepantes.

Isto aconteceu no caso de geada apresentando um grau de acerto, de acordo com os especialistas, muito bom, para até 1 dia de antecedência.

Neste caso, as duas bases pertenciam à mesma região geográfica.

Diferentemente do caso deficiência hídrica, onde foram utilizadas duas bases de dados diferentes, sendo uma do Estado de Minas Gerais e outra do Estado de São Paulo e resultou em um grau de acerto muito baixo, sendo insatisfatório para a previsão de deficiência hídrica.

Com os resultados encontrados, ressalta-se que este trabalho vem contribuir com outros trabalhos utilizados atualmente na previsão de geada e de deficiência hídrica, auxiliando na tomada de decisão.

Fica evidente que não é um trabalho que pretende substituir outros sistemas atuais de previsão.

Os modelos atuais são muito mais complexos e com esta pesquisa foram encontrados resultados razoavelmente bons com um modelo muito mais simples.

As dificuldades encontradas neste trabalho foram na preparação dos dados e na análise dos resultados encontrados, uma vez que as árvores geradas eram muito grandes, dificultando a criação das regras a serem mostradas aos especialistas.

Descoberta de conhecimento em banco de dados é um processo muito complexo, que exige dedicação do analista de dados e principalmente dos especialistas, mas que, ao mesmo tempo é uma ferramenta de grande utilidade para explorar o conhecimento que está escondido nas grandes bases de dados.

Foi tratada, neste trabalho, a aplicação do processo KDD na área agrícola, possibilitando a previsão local de geada e de deficiência hídrica.

Deve-se ressaltar que outras aplicações são relacionadas, possibilitando a continuidade do assunto com os seguintes trabalhos futuros, Zoneamento de risco climático, garantindo a probabilidade de sucesso na colheita para quem segue o zoneamento agrícola de risco climático, utilizando a metodologia KDD como ferramenta de gestão de risco.

Alertas agroclimáticos, possibilitando a previsão de outros fenômenos climáticos, além de geada e de deficiência hídrica que influenciam no sucesso da agricultura.

Avaliação de zonas críticas de produção, tendo em vista a identificação de áreas ou zonas, agrupando as regiões que são consideradas aptas ou inaptas para a produção agrícola.

Recomendação de doses de irrigação, identificando quanto e quando irrigar através de uma análise de previsão de deficiência hídrica.

Auxílio no controle de pragas e doenças, no desenvolvimento de sistemas de alerta contra doenças e pragas, através da metodologia KDD, auxiliando os produtores em tomada de decisões.

