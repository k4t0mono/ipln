A descoberta de conhecimento em bases de dados Knowledge Discovery in Databases KD visa a apoiar os processos de tomada de decisão através da extração automática de conhecimento oculto, útil e estratégico, em grandes bases de dados.

Este conhecimento precisa ser analisado e facilmente entendido por usuários e gestores para que se torne realmente relevante nas operações cotidianas ou em planejamento de ações no contexto do problema analisado.

O conhecimento descoberto pode ser apresentado de diversas formas.

Entretanto, estas formas muitas vezes não são compreendidas pelo usuário ou não permitem análises detalhadas e validações de novas hipóteses.

Para auxiliar a interpretação de resultados obtidos na mineração de dados, técnicas gráficas de Visualização de Informações têm contribuído significativamente para a representação inteligente de grandes volumes de dados, para a aplicação de técnicas estatísticas na análise de dados e para a manipulação visual dos dados.

À aplicação dessas técnicas sobre o processo de KDD dá-se o nome de Visual Data Mining.

Os principais objetivos deste trabalho são a investigação de técnicas de Visualização de Informações aplicadas no processo de KDD, o desenvolvimento de uma ferramenta de software que tenha foco principal em Visual Data Mining, com a proposição e implementação de técnicas e métodos que melhor se adaptem à interpretação de resultados minerados, e a realização de um estudo de caso com um problema em larga escala para validação da ferramenta desenvolvida.

A ferramenta desenvolvida, denominada Visual DATAMINER, atua sobre a interpretação de regras de indução, permite a integração com ferramentas de mineração de dados, possibilita a visualização dos resultados de mineração de dados em diversas visões e a interação com estas visualizações através de métodos de interação.

Desenvolvida na linguagem Java, a Visual DATAMINERapresenta todos os benefícios do paradigma de orientação a objetos como re-usabilidade, manutenibilidade e encapsulamento.

A investigação experimental realizada usando uma base de dados com um grande volume de dados, no domínio de análise de crédito ao consumidor, mostrou o refinamento do conhecimento descoberto através da aplicação das técnicas de visualização de informações e dos métodos de interação propostos na ferramenta, atestando a eficácia e a eficiência da ferramenta desenvolvida.

A principal área de concentração desta dissertação é a descoberta de conhecimento oculto na etapa de interpretação de resultados do processo de descoberta de conhecimento em bases de dados Knowledge Discovery in Databases KD.

O objetivo maior desta pesquisa é utilizar as técnicas de visualização de informações para entender, explorar e enriquecer o conhecimento descoberto na etapa de mineração de dados no processo de KDD.

Para tanto, foram realizados estudos sobre o processo de KDD, sobre as técnicas de visualização de informações existentes e sobre Visual Data Mining, especialização criada a partir da junção das áreas de Visualização de Informações e KDD.

Estes estudos constituíram a base para a proposta de novas técnicas a serem empregadas no contexto do Visual Data Mining.

Para consolidar a utilização das técnicas existentes e propostas nesta pesquisa, uma ferramenta denominada Visual DATAMINERfoi desenvolvida.

Esta ferramenta concentra-se em visualizar os resultados de algoritmos de mineração de dados que extraem regras de indução e em proporcionar mecanismos que facilitem o seu entendimento e a sua exploração.

Através da aplicação de técnicas de visualização de informações sobre regras de associação e regras de classificação clássicas ou fuzzy busca-se descobrir conhecimento relevante e útil sobre o problema analisado.

Com o objetivo de atestar a contribuição da ferramenta Visual DATAMINERao entendimento e enriquecimento da etapa de interpretação dos resultados, um estudo de caso com um problema de larga escala foi realizado.

O domínio deste problema é o contexto da análise de risco de crédito, atividade amplamente presente nos serviços prestados por empresas das áreas financeira e comercial.

A descoberta de conhecimento em bases de dados é uma área de pesquisa que tem atraído a atenção de muitos pesquisadores, gestores e profissionais de diversos setores, devido a tratar de processos que visam a obtenção de informação oculta em grandes bases de dados.

Estas informações descobertas podem ser muito valiosas tanto para os processos gerais de tomada de decisão do dia a dia quanto para o diferencial estratégico que podem proporcionar no planejamento de ações futuras.

O processo de descoberta de conhecimento em bases de dados KD foi formalizado em três etapas principais que são a preparação dos dados, a mineração dos dados e a interpretação dos resultados obtidos.

A etapa que tem recebido mais investimento é a etapa de mineração de dados, onde vários algoritmos e técnicas têm sido propostos.

Estes algoritmos apresentam o conhecimento minerado de diversas formas tais como árvores de decisão, regras de classificação, regras de associação, regras fuzzy, respostas numéricas, entre outras.

A etapa de interpretação dos resultados tem se mostrado cada vez mais relevante.

Isto se deve ao fato do conhecimento minerado precisar ser explicado, analisado e entendido por usuários que muitas vezes não têm facilidade de abstração com as formas de representação utilizadas para expressá-los.

Muitas vezes, os algoritmos de mineração de dados geram resultados que o usuário por não ter conhecimento, por não entender a linguagem utilizada ou por não ter condições de analisar refinadamente o conhecimento descoberto, não consegue tirar todo o proveito possível do trabalho realizado.

Outro fator é o tempo necessário para analisar todo o conhecimento minerado, que demanda um esforço por parte do especialista na sua análise.

Expressar o conhecimento minerado de uma forma que possa ser entendido e facilmente extrair dele todo o potencial disponibilizado é um desafio que precisa ser vencido.

Ampliando o foco da etapa de interpretação de resultados para todo o processo de KDD, observa-se que a participação de especialistas do negócio junto com a equipe técnica em todas as etapas do processo de KDD mostra-se de extrema valia.

Para esta proposta, as dificuldades de entendimento e manipulação que existem na etapa de interpretação dos resultados são potencializadas.

É conhecida a capacidade humana de interpretação de gráficos.

Já há muito houve-se dizer que "uma imagem vale por mil palavras".

Esta afirmação se deve ao fato do ser humano ter mais facilidade de entender imagens do que textos ou palavras propriamente ditas.

O esforço cognitivo humano para compreender informações expressas em gráficos é inferior ao dispendido para compreender textos ou números.

Além desta facilidade, gráficos possibilitam a expressão de muitas informações condensadas, que se fossem expressas através de textos, inviabilizariam sua interpretação, devido ao seu tamanho e capacidade de abstração necessárias ao seu entendimento.

O desenvolvimento da capacidade de processamento dos computadores e da capacidade de armazenamento de dados favorece os estudos e propostas de técnicas gráficas para a análise de dados.

Inúmeras técnicas têm sido propostas com o objetivo de representar enormes quantidades de dados e possibilidades de interação com estes dados.

Aproveitando todos esses fatores favoráveis, investimentos na utilização de gráficos para expressar informações relevantes são cada vez maiores.

À esta área de estudo dá-se o nome de Visualização de Informações.

Juntar os benefícios da mineração de dados com toda a potencialidade de expressão das técnicas de Visualização de Informações é a grande solução que se apresenta para diminuir as dificuldades expostas.

Por isso, a utilização de técnicas de visualização de informações na etapa de interpretação dos resultados é o grande foco desta pesquisa.

A possibilidade de facilitar o entendimento do conhecimento minerado, aplicar técnicas de visualização de informações e métodos de interação que possibilitam a descoberta de conhecimento escondido ainda na fase de interpretação de resultados e promover métodos de manipulação para potencializar o poder de análise dos usuários é a maior motivação desta pesquisa.

Para tanto, uma ferramenta de software denominada Visual DATAMINER foi desenvolvida com o objetivo de experimentar a aplicação das técnicas de Visualização de Informações sobre os resultados obtidos na etapa de mineração de dados.

Para realizar esta pesquisa, regras de classificação clássicas, regras de classificação fuzzy e regras de associação foram as formas de representação de conhecimento escolhidas para serem investigadas.

Estas regras, também conhecidas como regras de indução, são expressas na forma Se-Então e apresentam as variáveis do problema na sua formação.

As regras de indução são uma das principais formas de resposta produzidas por diversos algoritmos de mineração de dados.

Apesar de serem as formas de representação do conhecimento mais compreensíveis ao entendimento humano, algumas dificuldades existem na sua interpretação e análise.

As dificuldades de análise se devem ao fato de normalmente um número grande de regras serem mineradas e precisarem ser analisadas, a necessidade de concluir um raciocínio a partir de todo o conjunto de regras gerado e a existência de regras similaridades, demandando um tempo grande em sua análise.

Outro fator que incentivou a escolha de regras de indução como as formas de representação do conhecimento minerado a serem trabalhadas foi a possibilidade de se fazer inferências dos perfis determinados pelas regras sobre os dados que as originaram.

As técnicas de Visualização de Informações escolhidas para o desenvolvimento desta pesquisa são as Coordenadas Paralelas, os Histogramas, e as técnicas Net Diagram e Rule Relationship, estas duas últimas fruto do desenvolvimento deste trabalho.

Técnicas de Interação e distorção também foram utilizadas tais como a implementação de Filtros de Seleção, detalhamento dos relacionamentos entre as regras e utilização de Tabelas de Dados para facilitar a análise das informações e enriquecimento do trabalho.

Estas técnicas foram escolhidas com o objetivo de atender alguns objetivos da análise de regras, análise do texto da regra de forma gráfica, correlação do perfil da regra com outros atributos da base de dados original, análise do conjunto de regras quanto a índices de relevância da regra tais como índice de suporte, índice de confiança e índice de importância da regra e análise de similaridade entre as regras geradas.

Como relatado no início deste capítulo, o objetivo maior desta pesquisa é utilizar as técnicas de Visualização de Informações para entender, explorar e enriquecer o conhecimento descoberto na etapa de mineração de dados do processo de KDD.

As técnicas de Visualização de Informações aplicadas à KDD possibilitam a interação do especialista na interpretação dos resultados de forma mais amigável, permitindo que este possa usar de seu conhecimento e abstração para retirar do conhecimento minerado todo o potencial a ser aplicado no problema analisado.

A etapa de interpretação dos resultados no processo de KDD precisa dispor de métodos que facilitem a análise, a exploração e o entendimento do conhecimento minerado.

Precisa promover a possibilidade de que novas hipóteses, criadas a partir da análise dos resultados minerados, sejam validadas.

Precisa permitir que a criatividade do especialista possa ser utilizada na análise dos resultados minerados, através da aplicação de formas distintas de visualizar o conhecimento minerado e precisa possibilitar a correlação do resultado minerado com os dados originais, para validação de hipóteses.

Tendo em vista o direcionamento e os fatores expostos na seção anterior, os objetivos específicos desta pesquisa são, Investigar um conjunto de paradigmas de descoberta de conhecimento em bases de dados e visualização de informações e verificar as possibilidades de união dos benefícios das duas áreas.

Como resultado, propor soluções e técnicas que facilitem a integração entre as duas áreas.

A aplicação de técnicas de Visualização de Informações no processo de KDD caracteriza a criação de uma especialidade que abrange as duas áreas, denominada Visual Data Mining.

A aplicação de técnicas de Visualização de Informações enriquece o processo de KDD, ajuda a interação do especialista da área de negócios que está sendo analisada no processo de KDD, por facilitar o entendimento de todo o processo, e permite a ação e a análise do conhecimento descoberto por parte deste especialista.

Como unir duas áreas de conhecimento não é uma tarefa fácil, devido a cada uma delas apresentar peculiaridades que exijam conhecimento detalhado de ambas as áreas, as duas áreas de conhecimento precisam ser amplamente estudadas e uma cuidadosa análise sobre a junção das técnicas de ambas as áreas deve ser realizada.

A adaptação das técnicas de Visualização de Informações existentes ou a proposição de novas técnicas provavelmente deve ser realizada, para que se adeqüem à aplicação sobre as formas de representação do conhecimento minerado.

Modelar e implementar uma ferramenta de software com os procedimentos e técnicas que mais se adequem à proposta desta pesquisa.

O desenvolvimento de uma ferramenta de software que tenha o foco principal em Visual Data Mining e que aplique técnicas de Visualização de Informações, adaptadas à realidade e às necessidades da etapa de interpretação de resultados, é uma necessidade nos dias de hoje.

Criar ferramentas com o foco na interpretação dos resultados, com integração com ferramentas de mineração de dados, onde há a possibilidade de visualizar os resultados de mineração de dados em diversas visões e interagir com estas visualizações seria uma contribuição relevante à área de Visual Data Mining.

Outra funcionalidade que agrega potencialidade à interpretação dos resultados é a possibilidade de ter interação do resultado minerado com os dados originais, para que através de técnicas estatísticas, o conhecimento minerado possa ser refinado.

O desenvolvimento de software deve ser organizado de forma que seja executado conforme as melhores práticas de desenvolvimento de sistemas hoje existentes, para que os padrões de manutenibilidade, reusabilidade e encapsulamento sejam atendidos.

Para tanto, a metodologia de desenvolvimento segundo o paradigma de orientação à objetos deve ser utilizada.

Aplicar um problema de larga escala no domínio de análise de crédito, como ambiente de teste para a ferramenta Visual DATAMINER e identificar suas vantagens e deficiências quando aplicada na solução de um problema real relevante.

A aplicação da ferramenta Visual DATAMINER a um problema de larga escala tem o objetivo de validar a eficácia e eficiência da aplicação de técnicas de Visualização de Informações sobre a etapa de interpretação dos resultados de um processo de KDD.

A validação a ser atestada com este estudo de caso deve considerar o entendimento, o refinamento e a complementação que pode ser adquirida sobre o assunto a ser estudado com a aplicação da ferramenta.

O problema de análise de crédito apresenta a complexidade, importância e volume necessários a uma validação relevante, que mostre os benefícios da utilização das técnicas escolhidas.

Este estudo de caso é realizado sobre um conjunto de regras de indução que apresentam um número de regras de classificação e regras fuzzy suficientes para esta validação.

Os dados originais a serem avaliados apresentam um número de registros satisfatório para as análises necessárias.

Outro aspecto a ser avaliado neste estudo de caso é a facilidade de utilização da ferramenta, a importância dos métodos de interação sobre os dados e métodos de distorção sobre os métodos de visualização, e o desempenho da ferramenta.

A estrutura desta dissertação é organizada de forma a apresentar o embasamento teórico e todo o trabalho desenvolvido para se chegar aos resultados obtidos com a pesquisa.

Visão geral desta dissertação, com as motivações da pesquisa, seus objetivos e a estrutura do trabalho realizado.

Principais fundamentos do processo de descoberta de conhecimento em bases de dados.

Pesquisa realizada sobre a área de Visualizações de Informações bem como os estudos de integração das áreas de KDD e Visualizações de Informações, a saber, o Visual Data Mining.

Ferramenta desenvolvida nesta pesquisa, o Visual DATAMINER, com todas as suas telas, funcionalidades e possibilidades de interação.

Para consolidar a pesquisa realizada, um estudo de caso com a utilização do Visual DATAMINER é apresentada, com os resultados obtidos e as conclusões geradas a partir destes resultados.

As conclusões e sugestões de continuação desta pesquisa são apresentadas.

Finalmente, descreve-se os formatos dos arquivos utilizados pela ferramenta Visual DATAMINER para ler dados de entrada ou gravar as saídas.

O processo de tomada de decisões é um elemento cada vez mais crítico para a sobrevivência das empresas.

Decisões tomadas rapidamente e embasadas em informações corretas são o alvo de toda empresa.

A alta concorrência praticada nos mercados empresariais, a necessidade de sobrevivência em um mercado globalizado e a exigência de atendimento de metas arrojadas levam à busca de informações que possam agregar conhecimento, direção e precisão ao processo de tomada de decisões.

A quantidade de dados gerada e armazenada através dos diversos sistemas automatizados utilizados hoje é grande.

Com certeza, a possibilidade de analisar estes dados poderia gerar uma série de informações valiosas sobre os domínios de aplicação a que eles se referem.

Devido à grande quantidade e complexidade dos dados a serem analisados, o tempo necessário a esta análise se torna cada vez maior, impedindo que realmente se extraia todo o potencial dos dados analisados.

Para auxiliar o processo de análise de dados, a tecnologia de Data Warehousing foi desenvolvida com o objetivo de facilitar o acesso e manipulação destas bases de dados.

Nesta tecnologia, cria-se um repositório de dados (Data Warehouse) contendo dados corretos, agregados e consolidados que possam ser analisados por ferramentas específicas, chamadas de OLAP (On-Line Analitical Processing).

Estas ferramentas apresentam facilidades para a realização de consultas complexas em bases de dados multidimensionais.

As consultas são dirigidas pelos usuários, que possuem hipóteses a serem validadas ou, simplesmente, as executam aleatoriamente.

Através de várias iterações, o usuário pode formular uma nova hipótese ou refinar a hipótese atual.

A análise via OLAP caracteriza-se como um processo dedutivo cuja habilidade de descobrir padrões e tendências é limitada pelas hipóteses e perguntas feitas pelos usuários.

Por esta característica, a chance de padrões escondidos não serem encontrados é grande.

Pesquisas com o objetivo de criar técnicas que otimizem o processo de análise de dados foram desenvolvidas em diversas áreas como banco de dados, estatística e inteligência artificial.

Um dos resultados obtidos a partir destas pesquisas é o desenvolvimento de um processo para a descoberta de conhecimento em bases de dados.

Este processo denominado de KDD (Knowledge Discovery in Databases) trouxe uma grande avanço para a análise dos dados, criando a possibilidade de informações novas e desconhecidas serem apresentadas a partir do processamento dos dados.

A descoberta de conhecimento em bases de dados (KDD Knowledge Discovery in Databases) é formalmente definida como um processo não trivial de busca e análise de dados de forma a achar informação implícita e potencialmente útil sobre o problema sendo analisado.

Seu objetivo é transformar dados brutos em informação útil ou conhecimento a serem utilizados na aplicação que está sendo tratada.

Este processo é complexo e é constituído por várias etapas.

Entre elas, pode-se citar a preparação dos dados, a mineração de dados e a interpretação do conhecimento descoberto.

A etapa de preparação dos dados é um processo que consiste na adequação dos dados de entrada ao processo de descoberta de conhecimento, preparando e representando os dados em formato útil para os algoritmos de mineração de dados.

A etapa de mineração dos dados busca descobrir conhecimento útil, relevante e desconhecido, a serem aplicados em processos de tomada de decisão.

É desenvolvida através do uso de algoritmos ou combinações de técnicas de Inteligência Computacional sobre os dados preparados.

A etapa de interpretação dos resultados tem como objetivo analisar o conhecimento obtido na fase de mineração de dados, através de mecanismos que permitam visualizar informações úteis que possam suportar o processo de tomada de decisão.

Estas etapas serão detalhadas nas próximas seções.

Etapas do Processo de Descoberta de Conhecimento em Bases de Dados.

O processo de preparação dos dados é a primeira etapa do processo de descoberta de conhecimento em bases de dados.

O objetivo desta etapa é garantir a integridade das bases de dados que serão trabalhadas nas outras etapas do processo de KDD, eliminando os possíveis ruídos.

Os dados de entrada ao processo de mineração de dados podem apresentar uma série de falhas.

As mais comuns são inconsistências, a presença de dados fora dos padrões esperados e dados ausentes.

Estas falhas ocorrem devido às falhas no processo de geração, aquisição, integração e transformação dos dados.

Esta é uma das etapas mais trabalhosas, pois envolve analisar, limpar, integrar, consolidar e transformar os dados para subseqüente submissão destes aos algoritmos de mineração de dados.

Ao executar o processo de limpeza dos dados, busca-se tratar e eliminar dados ausentes, dados fora dos padrões e inconsistências que existam nos dados a serem trabalhados.

A qualidade do resultado deste processo impacta diretamente na qualidade do conhecimento descoberto ao fim do processo de KDD.

Dados sem qualidade podem prejudicar a mineração de dados, resultando em respostas não confiáveis nos processos de tomada de decisão.

Embora alguns métodos de mineração possuam procedimentos para lidar com dados ruidosos e incompletos, eles nem sempre são robustos.

Portanto, é essencial realizar um tratamento dos dados antes da etapa de mineração.

Para o caso de dados ausentes, várias são as técnicas a serem empregadas para preencher o valor ausente, dependendo do tipo de atributo a ser trabalhado.

Para os casos de atributos discretos, a técnica mais usada é o preenchimento do valor ausente com o valor mais predominante no atributo.

Para os casos de atributos numéricos, usa-se a média dos atributos para preenchê-lo, podendo a média ser calculada por classe ou por amostra ou usar o valor mais provável para o atributo, determinado por técnicas estatísticas.

Em algumas situações, opta-se por codificar a ausência do valor como uma informação, da mesma maneira como um outro valor qualquer de domínio.

Para o tratamento de dados fora dos padrões ou dados ruidosos, várias técnicas foram desenvolvidas.

As mais utilizadas são a suavização e a regressão.

A técnica de suavização consiste em distribuir dados ordenados em grupos.

Os valores ruidosos são considerados outliers.

Os valores ruidosos podem ser substituídos pelo valor da mediana, pelo valor da média ou pelo valor das fronteiras de cada conjunto de dados separado.

Na técnica de regressão, os dados podem ser modificados através da utilização de uma função.

Esta função pode ser linear, onde um atributo pode ser usado para prever o valor de outro atributo ou linear múltipla, onde dois ou mais atributos são utilizados nesta previsão.

Em muitas situações, outliers são simplesmente eliminados do conjunto de dados, se observado que sua presença pode prejudicar o trabalho, mais do que ajudar.

A preparação de dados para o processo de mineração geralmente exige que seja executado algum tratamento de integração de dados.

É comum encontrarmos dados armazenados separadamente e em diversas formas como arquivos textos, bases de dados relacionais, planilhas, Data Warehouse, entre outras.

No trabalho de integração de dados, duas maiores áreas devem ser cuidadosamente trabalhadas.

Estas são as áreas de semântica dos dados e redundância dos dados.

A semântica dos dados diz respeito à unificação de conceitos das entidades envolvidas.

Nesta etapa, devem ser considerados problemas como atributos representando o mesmo conceito com nomes diferentes, valores diferentes para uma mesma entidade e técnicas diferentes de codificação, representação e medição.

O aspecto de redundância refere-se a armazenar determinado atributo, o qual pode ser derivado através de outros atributos ou armazenar diferentes graus de agregação dos dados.

Utilizando a análise de correlação, redundâncias podem ser identificadas.

Alguns algoritmos de mineração de dados exigem que os dados a serem trabalhados sejam apresentados de determinada forma.

Para tanto, operações de agregação, generalização e normalização são realizadas com o objetivo de transformar o dado na forma mais apropriada a ser trabalhada por cada técnica de mineração.

Nas operações de agregação, busca-se sumarizar ou agregar os dados.

Esta operação é normalmente executada quando há a necessidade de alteração na granularidade do dado, isto é, no seu nível de detalhe, ou a necessidade da redução do número de atributos.

O processo de generalização é aplicado quando há a necessidade de substituir os valores dos atributos por conceitos de mais alto nível através de uma hierarquia de conceitos.

Podemos citar como exemplo o atributo idade, que pode ser substituído pela classificação discreta jovem, adulto ou sênior.

Um dos processos mais utilizados na transformação dos dados é a normalização.

A normalização transforma o valor do atributo de forma que este esteja entre uma escala definida de valores.

Esta escala normalmente é definida.

Existem vários métodos que produzem a normalização dos dados.

Entre eles podemos citar a normalização por min-max, a normalização z-score e a normalização por escala decimal.

A normalização min-max realiza uma transformação linear nos dados originais, sendo baseada nos valores mínimos e máximos dos atributos.

Este método preserva o relacionamento entre os valores dos dados originais.

A normalização z-score utiliza-se da média A e do desvio padrão A dos atributos para transformá-los.

Este método é empregado quando os valores mínimos e máximos de determinado atributo são desconhecidos ou dominados por ruídos.

Normalização Z-Score.

A normalização por escala decimal é realizada pelo deslocamento do ponto decimal dos valores dos atributos.

O número de casas decimais movidas depende do valor absoluto máximo do atributo em questão.

Normalização por Escala Decimal.

O método de normalização min-max é o mais utilizado devido ao fato de ser o método que menos altera os valores originais dos dados.

O principal papel da normalização é garantir que todos os valores dos atributos se encontrem dentro de um mesmo intervalo, tanto no desenvolvimento da solução quanto no seu uso.

É particularmente útil quando utilizamos algoritmos de classificação, tais como redes neurais ou algoritmos de medidas de distância, tais como k-vizinhos ou métodos de agrupamento.

Nas redes neurais, a normalização ajuda a diminuir o tempo da fase de aprendizado, enquanto que nos algoritmos de medida de distância, a normalização impede que atributos com valores iniciais grandes dominem os atributos com valores iniciais pequenos.

A etapa de mineração de dados é definida como o núcleo do processo de KDD, sendo responsável pelo mapeamento dos dados para algum tipo de informação ou modelo.

Esta é a segunda fase do processo de KDD.

Nesta fase, busca-se aplicar um conjunto de técnicas e algoritmos com o objetivo de extrair informações ocultas e relevantes de grandes bases de dados, de forma automática.

Há vários tipos de informações que podem ser extraídas, informações relevantes para apoio às decisões das empresas, previsões de comportamento de mercado, análise do comportamento passado e atual do mercado, entre outros.

O passo denominado mineração de dados envolve a identificação de subconjuntos de um conjunto de dados e de hipóteses sobre os mesmos.

Pode, então, ser definido como um processo interativo de geração de hipóteses.

De forma simplificada, a mineração de dados é o processo de pesquisar e analisar grandes volumes de dados identificando estruturas regulares e irregulares.

Em função do tipo de informação extraída dos dados, tarefas de mineração de dados podem ser agrupadas em, regras de associação, generalização e sumarização dos dados, classificação de dados, regressão e agrupamento de dados.

Estas tarefas podem ser classificadas como preditivas, quando já se conhece o dado ou atributo e se deseja achar o valor deste atributo num próximo estado.
Como nas tarefas de classificação ou regressão ou descritivas, onde se busca encontrar informação escondida na base de dados que não seria facilmente encontrada, nos casos de regras de associação, agrupamento de dados, generalização e sumarização dos dados.

Essas técnicas podem ser aplicadas em duas formas básicas no aprendizado do conhecimento escondido.

São elas a aprendizagem supervisionada e a aprendizagem não supervisionada.

A aprendizagem supervisionada é a aprendizagem por meio de exemplos onde um "professor" ajuda o sistema a construir um modelo definindo classes e exemplos representativos de cada classe.

O sistema tem que achar uma descrição de cada classe, isto é, as propriedades comuns nos exemplos.

Uma vez formulada a descrição e a forma da classe, uma regra de classificação pode ser usada para predizer a classe a que pertencem os objetos não vistos previamente.

Conhece-se a priori a classe desejada para cada instância.

Na aprendizagem não-supervisionada, a aprendizagem é realizada por meio de observações e descobertas.

O alvo da previsão não é definido nem conhecido e não há qualquer tipo de supervisor que indique se a resposta está certa ou errada.

Assim, tem-se que observar os exemplos e reconhecer padrões (descrição de classe) por si só.

O modelo é desenvolvido a partir das similaridades, regularidades ou relacionamentos existentes entre os dados.

Este sistema resulta em um conjunto de descrições de classes, uma para cada classe descoberta no ambiente.

As principais tarefas de mineração de dados serão descritas na próxima subseção.

Regras de Associação.

A mineração de dados através de regras de associação tem o objetivo de descobrir relacionamentos relevantes entre os atributos de um conjunto de dados.

Uma regra de associação é uma implicação na forma, onde estão contidos no conjunto de dados a ser trabalhado.

Esta regra é interpretada como tuplas (conjunto de valores de atributos) de banco de dados.

Para definir o quanto esta regra é representativa para o conjunto de dados utilizam-se dois fatores conhecidos como suporte e confiança.

O fator suporte é definido como o percentual de transações no conjunto de dados e indica a freqüência de ocorrências dos padrões usados nos dados.

O fator confiança mostra a força de representatividade da implicação e é definida pela porcentagem de transações no conjunto de dados (probabilidade condicional).

Normalmente, especifica-se um valor mínimo de suporte e precisão para que a regra possa ser considerada interessante.

As regras que possuem alto índice de confiança e suporte são referenciadas como "regras fortes".

A tarefa de descobrir regras de associação é decomposta em duas etapas.

Na primeira etapa, busca-se determinar o conjunto de itens de dados que têm a freqüência mínima superior ao limiar pré-estabelecido.

Em seguida, usa-se este subconjunto para gerar as regras de associação.

O desempenho do processo de mineração de dados é determinado pela primeira etapa, pois a derivação das regras de associação é realizada de uma maneira muito simples e direta.

Sumarização e Generalização de Dados.

De acordo com, a generalização dos dados é um processo que abstrai um grande conjunto de dados relevantes de uma base de dados de um conceito hierarquicamente baixo para um conceito hierarquicamente mais alto.

Esta tarefa pode ser realizada através de duas abordagens distintas.

Estas abordagens são denominadas abordagem do cubo de dados e abordagem por indução orientada a atributos.

A abordagem do cubo de dados tem como objetivo processar previamente certas instruções que são computacionalmente caras e freqüentemente utilizadas e armazená-las em visões em banco de dados multidimensionais, chamados cubos de dados.

As instruções processadas são contadores, somas, médias, valor máximo, valor mínimo, entre outras.

Estas funções agregadas podem ser processadas para vários grupos ou subgrupos de atributos e são acessadas por operações de roll-up e drill-down.

Nas operações roll-up, reduz-se o número de dimensões de um cubo de dados ou generaliza-se os valores dos atributos para níveis mais altos.

A operação drill-down funciona de maneira inversa, aumentando o número de dimensões de um cubo de dados ou detalhando os valores dos atributos.

É necessário ressaltar que esta abordagem trabalha de forma "off-line", uma vez que o processamento dos dados é realizado e armazenado, havendo a possibilidade de não estar manipulando os dados mais atualizados.

A abordagem por indução orientada a atributos é uma técnica de análise de dados baseada em generalização.

Alternativamente à abordagem do cubo de dados, esta técnica é processada "on-line".

É realizada em duas etapas, onde primeiramente os dados relevantes de um conjunto de dados são selecionados através do uso de uma linguagem de pesquisa em banco de dados.

Após a seleção, técnicas de generalização de dados são utilizadas, tais como remoção de atributos, controle de limiar de atributos, propagação de contagem, entre outras.

Os dados generalizados são expressos na forma de relação generalizada, nas quais muitas outras operações ou transformações podem ser processadas.

Os dados generalizados podem então ser apresentados em diversas formas tais como tabelas sumarizadas, gráficos e curvas.

Classificação de dados Segundo, classificação dos dados é o processo que busca as propriedades comuns entre um conjunto de objetos em uma base de dados e os classifica em diferentes classes, de acordo com um modelo de classificação.

Esta função tem sido objeto de estudo em diversas áreas.

Podemos citar as áreas de estatística, aprendizado de máquina, redes neurais e sistemas especialistas como as principais.

Vários métodos de implementação de algoritmos de classificação já foram propostos entre os quais podemos ressaltar as árvores de decisão, as redes neurais e os classificadores bayesianos.

Os classificadores baseados em árvores de decisão são métodos de aprendizado supervisionado que constroem árvores de decisão a partir de um conjunto de exemplos.

As árvores de decisão desejadas são aquelas em que cada folha contém uma classificação e cada nó interior especifica um atributo com ramos correspondente a cada valor possível daquele atributo.

Na construção da árvore, primeiramente é escolhido um subconjunto dos exemplos de treinamento para sua formação.

Caso a árvore não consiga classificação correta para todos os objetos, um novo conjunto de exceções é adicionado ao conjunto anterior.

Este processo é continuado até que se consiga uma boa performance de classificação.

Os classificadores baseados em redes neurais são conjuntos de unidades de processamento conectadas, onde cada unidade processa uma função matemática e cada conexão possui um peso associado a ela.

Este método possui três fases distintas de evolução, fases de aprendizado, onde a rede neural aprende através do ajuste de seus pesos e se torna capaz de definir uma função aproximada que mapeia os exemplos de entrada em respostas que representam classes.

Fase de validação, onde se valida o desempenho do classificador e a fase de teste, onde realmente a rede está apta a ser utilizada para classificação.

As redes neurais demandam períodos de treinamento que podem variar de acordo com o modelo e definição de um conjunto de parâmetros que vão determinar o seu modo de funcionamento.

Normalmente a definição dos parâmetros é realizada de forma empírica, entretanto, muitos algoritmos recentes definem boa parte dos parâmetros de forma automática.

Alguns destes parâmetros são a topologia da rede, os pesos iniciais das conexões, as taxas de aprendizado, entre outros.

As redes neurais têm sido criticadas pela dificuldade de interpretação de seus resultados, uma vez que os usuários devem decifrar o significado simbólico de seus pesos após a fase de teste.

Em contrapartida, as redes neurais apresentam alta tolerância a dados ruidosos e uma grande capacidade de classificar padrões que não foram submetidos ao algoritmo anteriormente.

O algoritmo de rede neural mais popular é o MLP (Multi-Layer Perceptron).

Os classificadores bayesianos são classificadores estatísticos.

Baseado no teorema de Bayes, esse classificadores prevêem a probabilidade de uma instância pertencer a uma classificação em particular.

Possuem grande precisão e velocidade ao ser aplicado em grandes bases de dados.

Um exemplo é o classificador Naive Bayesian, que assume a premissa que o efeito de um valor de atributo em determinada classe é independente dos valores dos outros atributos.

Esta premissa é chamada de independência condicional da classe.

Este classificador possui, em alguns domínios, uma performance comparável às redes neurais e aos classificadores baseados em árvores de decisão.

Regras de classificação são formas de representação do conhecimento minerado.

As regras de classificação são regras formadas por partes antecedente e conseqüente.

A parte antecedente ou pré-condicional de uma regra expressa os testes ou condições em comum encontradas em determinada classificação, enquanto a parte conseqüente, ou conclusiva, expressa a própria classificação, que se aplica às instâncias de dado sob a abrangência daquela regra.

Geralmente, as pré-condições são logicamente unidas pelo operador lógico "E" e todas as condições devem ser satisfeitas para que a regra seja verdadeira para determinada instância de dado.

Exemplo de Regra de Classificação.

As regras de classificação podem ainda ser categorizadas como clássicas e fuzzy.

As regras clássicas apresentam a classificação de instâncias de dados baseada na lógica convencional, a qual restringe-se somente a dois valores possíveis, verdadeiro ou falso.

As regras fuzzy apresentam a classificação de instâncias de dados baseadas na lógica fuzzy, multivalorada, possibilitando a expressão da classificação em termos lingüísticos e imprecisos.

Na lógica fuzzy, uma premissa varia de grau de verdade de 0 a 1, o que a leva a ser parcialmente verdadeira ou parcialmente falsa.

Com a incorporação do conceito de grau de verdade, a Teoria dos Conjuntos Fuzzy amplia a Teoria dos Conjuntos Tradicionais.

Os grupos nos conjuntos fuzzy são rotulados qualitativamente, com valores tais como alto, magro, morno, pequeno.

Os elementos deste conjunto são caracterizados variando o grau de pertinência, isto é, o valor que indica o grau em que um elemento pertence a um conjunto.

Por exemplo, um homem de 1,80 m de altura e um homem de 1,75 m de altura são membros do conjunto alto, embora o homem de 1,80 m tenha um grau de pertinência maior neste conjunto.

É o processo de agrupar um conjunto de objetos físicos ou abstratos similares entre si.

Um agrupamento é uma coleção de objetos de dados que são similares uns aos outros dentro do mesmo agrupamento e dissimilares aos objetos em outros agrupamentos.

A análise de agrupamentos tem sido usada em várias aplicações, incluindo reconhecimento de padrões, análise de dados, processamento de imagens, entre outros.

Pertencendo ao ramo da estatística, a análise de agrupamento tem sido intensamente estudada e vários métodos estatísticos têm sido usados em sua execução.

Entre eles, se destacam os métodos k-means e k-medoids como os mais utilizados.

Sendo um método que adota o aprendizado não-supervisionado, a análise de agrupamento é classificada como uma forma de aprendizado por observação ao invés do aprendizado por exemplos.

Interesse e Similaridade de Regras Geradas pela Mineração de Dados A etapa de mineração de dados pode gerar um grande número de regras a serem analisadas na etapa de interpretação dos resultados.

Conforme o número de regras geradas, o processo de análise e aprendizado do conhecimento torna-se caro e demorado.

Normalmente, apesar do número de regras geradas ser grande, somente algumas delas demonstram ser de algum interesse para os especialistas das áreas de negócios.

A razão para esta análise é o fato de muitas regras serem irrelevantes ou óbvias e não apresentarem fatos inovadores aos negócios.

Para auxiliar a análise das regras, podem ser calculados e atribuídos índices de relevância às regras.

Algumas medidas de importância são propostas na literatura, com o objetivo de medir e indicar a relevância das regras geradas na etapa de mineração de dados.

Hilderman e Hamilton fazem uma exposição detalhada de uma série destas técnicas.

As técnicas Piatetsky-Shapiro s Rule Interest Function, Agrawal and Srikant s Itemset Measures são algumas das técnicas mencionadas neste trabalho.

Alguns índices que são utilizados na medição da distribuição de renda de uma população são o índice de Lorenz e índice de Gini.

Estes índices podem ser adaptados na mensuração da importância de determinada regra gerada, ao comparar a freqüência das classes de determinado problema com o seu índice de confiança.

Formalmente, o índice de Gini é definido como "a área entre a curva de igualdade e a curva de Lorenz, dividida pela área abaixo da curva de igualdade" A similaridade entre as regras geradas é um outro fator a ser levado em consideração.

Muitas regras geradas são similares e o conhecimento desta similaridade pode ajudar e acelerar o processo de análise.

A similaridade entre regras pode ser definida através da aplicação de funções de similaridade sobre o conjunto de regras a ser manipulado.

Estas funções podem determinar um coeficiente de similaridade no relacionamento entre cada duas regras, gerando informação sobre como estas se relacionam em relação à base de dados.

Estas funções de similaridade podem avaliar a similaridade entre as regras de duas formas.

A similaridade entre as regras pode ser avaliada sintaticamente, onde é levada em consideração somente a sintaxe da parte antecedente das regras e semanticamente, onde além da sintaxe, são levados em consideração a parte conclusiva da regra e a representatividade de cada regra sobre a base de dados.

Várias medidas foram desenvolvidas para cada um dos casos de similaridade.

Com uma pequena adaptação dos conceitos das fórmulas para os conceitos das regras, estas medidas podem ser aplicadas à analise de similaridade de regras.

Para a avaliação de similaridade sintática de regras, a medida de similaridade de Jaccard, a medida de similaridade Dice e a medida de similaridade Simple Matching podem ser usadas.

Estas medidas foram desenvolvidas para trabalhar com dados binários e seus conceitos são baseados na presença ou ausência de variáveis em dois objetos.

Se considerarmos as partes antecedentes das regras como variáveis, a ausência ou presença das partes antecedentes das regras podem ser avaliadas e um índice de similaridade pode ser calculado.

Os valores obtidos se situam em escala entre 00 e 10, sendo o maior valor de similaridade retratado pelo valor 10.

Medida de Similaridade de Jaccard.

A medida de similaridade de Jaccard é conhecida como a razão de similaridade.

A quantidade de partes antecedentes similares entre as duas regras é dividida pelo somatório da quantidade de partes similares entre as duas regras, da quantidade de condições presentes na primeira regra, mas ausentes da segunda e da quantidade de condições presentes na segunda regra, mas ausentes da primeira.

Desta forma, a similaridade entre as regras é avaliada levando em consideração as partes similares e dissimilares dos dois objetos.

A medida de similaridade Dice ou Czekanoswski ou Sorenson é similar à medida de similaridade de Jaccard, sendo a única diferença o peso atribuído às partes similares entre as duas regras.

Medida de Similaridade de Dice.

Medida de Similaridade Simple Matching.

A medida de similaridade Simple Matching é a razão entre o número de partes antecedentes similares e o número total de características.

Esta medida considera as partes do conjunto total de variáveis que estão ausentes das duas regras.

A etapa de interpretação dos resultados é a etapa seguinte à mineração de dados.

Seu objetivo é analisar o conhecimento minerado, buscando extrair deles o conhecimento oculto e novo a ser aplicado no problema sendo analisado.

O conhecimento minerado precisa ser analisado, compreendido e colocado em prática para a melhoria dos processos.

Na etapa de mineração dos dados é extraído conhecimento e apresentado em diversas formas.

As formas mais comuns de apresentação do conhecimento minerado são valores numéricos, regras de classificação, regras de associação e árvores de decisão.

Muitas vezes, estes resultados são difíceis de serem entendidos pelo usuário por estarem em linguagem científica ou pelo volume e diversidade de materiais a serem analisados.

Existem pontos principais a serem observados na etapa de interpretação dos resultados para que o conhecimento minerado possa ser considerado relevante.

Entre eles a evidência do fato, que indica a significância do fato, medida por um critério estatístico.

A redundância ou similaridade ou derivação do fato em relação ao conhecimento conhecido ou já descoberto.

A utilidade que exprime o relacionamento do conhecimento descoberto com o objetivo da mineração do dado.

A inovação que refere-se à diferença em relação ao conhecimento original do sistema ou usuário.

A simplicidade, que refere-se a complexidade sintática da apresentação do conhecimento e a generalidade, que é determinada pela fração da população que o conhecimento abrange.

O grau de interesse de determinado conhecimento minerado é determinado como uma função dos critérios relatados e depende fortemente do usuário e de seu conhecimento de domínio.

Apresentar resultados obtidos na etapa de mineração nestas formas de representação gera alguns inconvenientes.

A falta de familiaridade dos usuários finais em lidar com regras, a necessidade de selecionar regras relevantes em meio ao conjunto de regras descoberto, a necessidade de análise de similaridade das regras mineradas são alguns dos pontos que dificultam a análise e a aplicação do conhecimento minerado.

Soluções para facilitar o entendimento e possibilitar a aplicação do conhecimento especialista na análise do conhecimento minerado têm sido objeto de estudo.

Umas das soluções que tem apresentado grandes resultados ao ser aplicada na etapa de interpretação de resultados é a utilização de métodos gráficos.

O ser humano apresenta uma grande capacidade de entendimento e exploração de fatos e esta capacidade é potencializada com a utilização de representação de fatos através de gráficos, tabelas e desenhos geométricos.

A visualização de informações foi categorizada por Ware como "a representação gráfica de dados ou conceitos", que pode ser visto como um artefato externo de apoio ao processo de tomada de decisão.

A aplicação de técnicas de visualização na interpretação dos resultados obtidos na etapa de mineração de dados tem ajudado o ser humano na sua análise, compreensão e aplicação deste conhecimento.

Esta ajuda é caracterizada como suporte cognitivo e explora o potencial da percepção humana, como o processamento visual paralelo, enquanto compensa as deficiências cognitivas, como a memória limitada.

A possível e útil junção das áreas de Descoberta de Conhecimento em Bases de Dados e Visualização de Informações tem sido estudada amplamente na busca de melhorias e aplicação conjunta do potencial das duas áreas.

O processo de KDD tem sido amplamente utilizado pela indústria da informação.

Os principais motivos para isso são a necessidade de analisar bases de dados e transformar os dados armazenados em conhecimento útil.

O conhecimento descoberto pode ser aplicado em áreas que vão desde o gerenciamento de negócios, controle de produção e análise de mercado até projetos de engenharia e pesquisas científicas.

As áreas que mais tem utilizado o processo de descoberta de conhecimento em bases de dados são os setores de marketing e vendas, financeiro, químico e farmacêutico, genético e médico.

As áreas de marketing e vendas são grandes áreas de potencial utilização de aplicações de KDD devido ao alto volume de armazenamento de dados advindo de suas aplicações operacionais.

As tarefas mais utilizadas são as preditivas.

As áreas de marketing e vendas buscam determinar perfis de comportamento de seus clientes, características determinantes dos clientes que pretendem parar de utilizar os serviços da empresa trocando-as pelos concorrentes (attrition ou churn prediction), previsão de vendas e alocação de estoque, entre outras funções.

Outra área que utiliza aplicações de KDD é a área financeira.

A área financeira diariamente necessita tomar uma série de decisões inerentes às atividades do seu dia a dia.

Concessão de créditos, previsão de índices de bolsa de valores, prospecção e retenção de clientes são tarefas que exigem um grande conhecimento do especialista para realizá-las com segurança e eficiência.

Algumas vezes, os especialistas analisam uma série de informações advindas de diversas fontes e não conseguem, de uma forma sistêmica e formal, determinar a razão de suas decisões.

O critério subjetivo é bastante utilizado e muitas vezes os especialistas tomam as suas decisões baseadas principalmente em seu conhecimento prévio ou mesmo em sua intuição.

As aplicações de KDD têm sido amplamente utilizadas nesta área.

Destacam-se, principalmente, as aplicações para detecção de fraudes, análise de perfil para concessão de crédito, criação de modelos para segmentação dos perfis de clientes, entre outras.

Vários sistemas comerciais de KDD têm sido desenvolvidos e comercializados com grande sucesso.

Destacam-se o Intelligent Miner, da IBM, o Enterpriser Miner, do SAS Institute, o MineSet, da Silicon Graphics Inc, o Clementine, da SPSS Inc e o DBMiner, desenvolvido pelo DBMiner Technology Inc.

Todos estes se destacam por apresentar múltiplas funções de mineração de dados e técnicas de descoberta de conhecimento.

A proposta de junção da tecnologia OLAP e Data Mining criou a tecnologia OLAM, On-Line Analitycal Mining.

A tecnologia OLAM busca utilizar a potencialidade das ferramentas OLAP ao se procurar conhecimento em bases de dados.

Esta integração proporciona flexibilidade ao processo de descoberta de conhecimento em bases de dados porque permite a realização da mineração de dados sobre abstrações multidimensionais e em vários níveis.

As funções de agregações e sumarizações e as funções de mineração de dados podem ser integradas e separadas, tornando o processo de descoberta de conhecimento em bases de dados altamente interativo e interessante.

Grandes volumes de dados têm sido gerados diariamente a partir dos sistemas e equipamentos de aquisição de imagens, dados e sinais em geral e dos computadores que permitem simular sistemas cada vez mais complexos.

Esses dados são uma grande fonte em potencial de informações valiosas sobre os ambientes de onde eles foram extraídos e têm sido armazenados com o objetivo de serem analisados em busca de conhecimento e informações que possam trazer aprimoramento dos processos de origem.

Estas informações são armazenadas a partir de diversas origens, possuem formatos variados e graus de complexidade distintos.

A complexidade dos dados gerados, bem como o grande volume de informações a serem analisadas, pode tornar a análise destes dados uma atividade desafiadora, longa e complexa.

Para evitar o desperdício de tempo, recursos e dinheiro, e ajudar na eficiência do processo, várias técnicas são utilizadas com o objetivo de facilitar esta análise.

Uma abordagem para contornar as dificuldades de analisar grandes blocos de informações é utilizar técnicas de Visualização de Informações.

Visualização de Informações é o processo de mapeamento de dados e informações em um formato gráfico, baseando-se em representações visuais e em mecanismos de interação, fazendo uso de suporte computacional e objetivando a ampliação da cognição ou uso do conhecimento.

O propósito da visualização é a percepção, não propriamente figuras em si, sendo que os principais objetivos dessa percepção são a descoberta, a tomada de decisões e o entendimento da situação sendo analisada.

Combinando aspectos de computação gráfica, interfaces homem computador e mineração de dados, a visualização de informações permite a apresentação de dados em formas gráficas de modo que o usuário possa utilizar sua percepção visual para melhor analisar e compreender as informações.

Devido ao seu grande potencial, a área de Visualização de Informações tem sido alvo de inúmeras pesquisas desde o seu surgimento.

Divide-se o desenvolvimento da área de Visualização de Informações em quatro estágios.

O primeiro estágio inicou em 1782, quando se usou pontos como símbolos para demonstrar a distribuição geográfica de 56 commodities na Europa.

A este primeiro estágio dá-se o nome de Estágio de Busca (The Searching Stage), caracterizado pela manipulação de conjuntos de dados relativamente pequenos e a geração de gráficos para visualização em papel e lápis.

O segundo estágio, denominado Estágio do Despertar, foi iniciado em 1977, pela inovadora proposta da análise exploratória de dados, onde se procurava descobrir hipóteses sobre a situação analisada através da aplicação de métodos estatísticos e gráficos.

A análise exploratória de dados despertou e ensinou as pessoas a decodificar visualmente informações a partir de dados.

Estudos sobre técnicas espaciais em duas e três dimensões também foram o grande foco nesta época.

Neste tempo, já estavam disponíveis no universo científico computadores pessoais e grandes bases de dados oriundas de satélites tais como dados meteorológicos e geográficos.

O terceiro estágio, o Estágio da Descoberta, iniciou-se em 1986.

Várias técnicas de visualização foram propostas no período de 1986 a 1991, onde a característica comum entre elas era apresentar várias dimensões de dados e várias variáveis em um mesmo display e manipular gráficos diretamente através da escolha de subconjuntos dos dados, filtros e associações entre variáveis.

O quarto estágio iniciou-se em 199e se prolonga até os dias de hoje.

É denominado como Estágio de Elaboração e Avaliação e é caracterizado pelo foco em avaliar a precisão, eficiência e utilidade das técnicas existentes e pela geração de software de visualização de informações que combinam várias técnicas gráficas em um só ambiente.

A intersecção das áreas de visualização de informações com áreas de Mineração de Dados, Estatística e Design continua sendo pesquisada e desenvolvida nos dias de hoje.

Os objetivos da área de Visualização de Informações, segundo, dividem-se em função de três atividades de análise, a análise exploratória, a análise confirmativa e análise de produção.

A análise exploratória de dados é caracterizada pela ausência inicial de hipóteses a serem avaliadas.

A análise dos dados é iniciada sem nenhuma hipótese pré-concebida a respeito dos dados a serem avaliados.

Não há necessariamente nenhuma orientação ou direção a ser seguida, caracterizando um cenário dinâmico, onde as possibilidades de interações nas visualizações são fatores críticos.

O processo segue interativamente procurando por estruturas e tendências.

O resultado obtido é a visualização dos dados, buscando mostrar hipóteses sobre os mesmos.

A análise confirmativa inicia a partir de uma hipótese formulada que precisa ser averiguada.

Este cenário é mais estável que o anterior e a utilização de ferramentas analíticas é bastante necessária para que a análise da hipótese inicial seja eficiente.

O processo é guiado pelo objetivo de examinar a hipótese inicial e o resultado obtido é a visualização dos dados, permitindo a confirmação ou rejeição desta hipótese inicial.

A análise de produção tem o objetivo de, a partir de fatos conhecidos a priori, exibir eficientemente os dados que apresentam estes fatos, através de técnicas de visualização.

Existe uma hipótese validada e o foco do trabalho está no refinamento da visualização para que a apresentação dos dados que validam esta hipótese seja otimizada.

Este é o processo mais estável da visualização de informações.

Inicialmente, a utilização de meios visuais para a analise de informações se beneficiava da representação geométrica inerente aos dados científicos.

No entanto, hoje em dia a maior parte das informações geradas pelos sistemas não possuem estas características, pois são inerentemente multidimensionais e complexos, sendo caracterizados como dados abstratos.

Devido à diferença na natureza dos dados a serem manipulados, a área de Visualização é subdividida em duas subáreas.

Uma das subdivisões é a área de Visualização Científica, na qual os conjuntos de dados são geralmente derivados de medidas ou simulações produzidas por fenômenos naturais do mundo físico e a informação é inerentemente geométrica.

Como exemplo de dados manipulados sob a visualização científica podem ser citados dados oriundos das áreas de meteorologia e ultra-som.

A segunda área é a área de Visualização de Informações, onde os conjuntos de dados são definidos em espaços abstratos.

Dados abstratos podem ser definidos como relacionamentos ou informações inferidas a partir dos dados mensurados.

Exemplo de dados manipulados na área de Visualização de Informações são dados oriundos das áreas de negócios empresariais e financeiras, tais como vendas, produção e investimentos.

As etapas essenciais a serem consideradas no uso de determinada técnica de visualização, ou no desenvolvimento de novas técnicas, podem ser identificadas por meio de Modelos de Referência de Visualização.

Dentre os modelos propostos, destaca-se o proposto por Card que descreve a visualização como uma seqüência de mapeamentos ajustáveis de dados para uma representação visual de modo a possibilitar interação do usuário com o espaço da informação, objetivando o que foi chamado de cristalização do conhecimento.

Modelo de Referencia de Visualização.

Neste modelo, dados brutos, isto é, os dados coletados ou gerados por algum processo, são transformados em tabelas e descrições relacionais que incluem metadados.

Outras novas transformações podem ser aplicadas neste momento, como por exemplo, calcular grandezas estatísticas ou converter dados originais em outros tipos.

As tabelas relacionais são, então, mapeadas para estruturas visuais ou representações visuais.

Este é o elemento central no Modelo de Referência de Visualização, servindo como intermediário entre as etapas que envolvem tratamento de dados e as etapas que lidam com a forma visual.

As estruturas visuais são exibidas como imagens ou visões.

Seguindo o modelo, operações sobre as estruturas visuais são executadas, produzindo visões.

Estas operações têm o objetivo de mostrar informações adicionais sobre elementos do conjunto de dados através de mudanças do ponto de observação, manipulação geométrica ou indicação de região ou subconjunto de interesse.

Uma das principais considerações a ser realizada no processo de visualização é a determinação de qual técnica deve ser empregada em uma determinada aplicação ou situação.

A caracterização dos dados a serem trabalhados bem como a natureza das tarefas que precisam ser realizadas pelo usuário são atributos determinantes na escolha da técnica a ser utilizada.

Devido a isto, vários autores propõem classificações procurando auxiliar um projetista a enquadrar sua aplicação em alguma técnica.

Entre os principais autores, destacam-se Schneiderman, que propôs uma classificação das técnicas de visualização de informações por tipo de dados e por tarefas.

Segundo Schneiderman, as técnicas podem ser temporais, unidimensionais (1, bidimensionais (, tridimensionais (, multidimensionais (n, dirigidas à visualização de hierarquias e relacionamentos (grafos), e podem suportar tarefas como a obtenção de uma visão geral, obtenção de visão detalhada, zooming, filtragem, identificação de relacionamentos, manutenção de histórico de ações e extração de informações diversas.

Outra contribuição relevante à classificação das técnicas de visualização de informações surgiu em, onde é proposta a classificação das técnicas de visualização baseada em três aspectos, a natureza dos dados a serem visualizados, a técnica de visualização e as técnicas de interação e distorção aplicadas.

Estes três critérios serão detalhados a seguir.

Os dados a serem manipulados pelas técnicas de visualização de informações podem ser classificados segundo vários critérios, a dimensionalidade, a classe de informação, os tipos de valores e a natureza e dimensão do domínio.

Os dados geralmente são compostos por grandes números de registros que podem apresentar um número de variáveis.

Cada um destes registros corresponde a uma observação, medida ou transação armazenada.

O número de atributos nas bases de dados pode variar, por exemplo, uma base de dados pode conter cinco variáveis, enquanto outra pode apresentar centenas deles.

Esta característica é denominada dimensionalidade do conjunto de dados.

Os conjuntos de dados podem ser temporais ou unidimensionais, bidimensionais, multidimensionais e podem conter tipos de dados complexos tais como textos ou hipertextos ou grafos e hierarquias.

Há distinção entre dimensões densas e dimensões que possuem valores arbitrários.

Dependendo do número de dimensões arbitrárias, os dados podem ser classificados como univariado, bivariado, etc.

A classe ou tipo de informação que os atributos representam é um fator a ser levado em consideração ao se caracterizar atributos.

Os atributos podem representar uma característica da entidade a que pertencem, sendo neste caso classificados como categoria.
Podem representar uma propriedade, com valores escalares, vetoriais ou tensoriais, que assumem valores inteiros ou reais, dentro de um intervalo e podem indicar a existência de um relacionamento, hierarquia ou ligação.

Os dados ou atributos das bases de dados podem assumir vários tipos.

Os dados podem ser alfanuméricos, inteiros, reais ou simbólicos, representando este último a identificação de entidade ou fenômeno relacionado.

Os domínios assumidos pelos atributos podem ser contínuo, contínuo-discretizado ou discreto.

Como exemplo de cada uma destas categorias podem ser citadas medidas da superfície de um terreno como atributo contínuo, o conjunto de marcas de automóveis como atributo discreto e a referência de ano como atributo contínuo-discretizado.

Todas estas características da natureza dos dados devem ser levadas em consideração na escolha da melhor técnica de visualização a ser utilizada na análise de uma base de dados.

Nesta seção, serão abordados conceitos de classificação das técnicas de visualização de informações, bem como o detalhamento das técnicas mais relevantes.

Devido a grande quantidade de técnicas de visualização propostas, a necessidade de organizar e classificar estas técnicas de forma a trazer um melhor entendimento à sua aplicação é inegável.

O agrupamento de técnicas semelhantes de acordo com suas características, vantagens e desvantagens proporciona um entendimento sistemático sobre as mesmas.

Dentre as classificações propostas, podemos ressaltar a de autoria de Keim e Kriegel e a de Grinstein e Ward.

A classificação proposta por Keim e Kriegel é organizar as técnicas de visualização em seis subdivisões, a saber, projeções D e D convencionais, técnicas baseadas em grafos, técnicas baseadas em projeções geométricas, técnicas baseadas em ícones, técnicas orientadas a pixels e técnicas hierárquicas, além da possibilidade de combinação destas técnicas (técnicas híbridas).

Grinstein e Ward apresentam outra proposta de classificação de técnicas de visualização.

Em, a proposta de classificação das técnicas de visualização segue a orientação de três critérios, características geométricas ou simbólicas, estímulos D ou D e visualizações estáticas ou dinâmicas.

Para este autor, as técnicas geométricas baseiam-se em mapeamento de dados com vários atributos a eixos, demonstrando a sua visualização através de linhas, superfícies e volumes.

Esta classificação engloba as técnicas Coordenadas Paralelas e Scatter Plots.

As técnicas simbólicas buscam representar os dados não numéricos utilizando pixels, ícones, arrays ou grafos.

O objetivo é representar os relacionamentos entre os elementos de dados.

As representações visuais das técnicas geométricas e simbólicas podem ser em duas dimensões, três dimensões ou estereoscópica, onde os estímulos D e D estão presentes simultaneamente.

As técnicas de visualização de informações são ainda classificadas segundo o grau de interação que apresentam, sendo classificadas como estáticas, manipulando dados estáticos, ou dinâmicas, manipulando dados dinâmicos, conforme as possibilidades de interação da cena.

Nem todas as técnicas são classificadas precisamente em uma das classificações descritas.

As linhas de separação entre as classificações não são precisas e muitas técnicas podem ser consideradas híbridas, sendo derivadas de várias outras técnicas.

Nesta dissertação, estará sendo adotada a classificação proposta por Keim e Kriegel, devido a esta ser aplicada mais amplamente no universo científico.

Esta classificação e as técnicas que as representam serão detalhadas a seguir.

Esta classificação abrange técnicas mais simples e amplamente utilizadas como plotagem em planos e espaços, gráficos de barra, histogramas, gráficos de pizza e gráficos lineares.

Exemplo de Histograma.

Exemplo de Gráfico de Pizza.

Esta classificação utiliza técnicas para apresentar grafos de maneira clara e rápida.

Estas técnicas mapeiam as características de um dado grafo, tais como direção (direcionado/Não direcionado) e orientação (cíclico/não cíclico) e podem ser subdivididas pela sua dimensionalidade, D ou D.

Os desenhos D possuem propriedades tais como a planaridade (não cruzamento de linhas), a ortogonalidade e a propriedade de distribuição em grade (coordenadas dos vértices são números inteiros).

Estas características devem ser bem utilizadas de forma a produzir grafos que consigam expressar toda a informação sobre os dados que estão sendo apresentados.

Para tanto, deve-se procurar desenhar grafos com um mínimo de cruzamento de linhas, com exibição ótima de simetria, exibição ótima de agrupamentos, número mínino de curvas em grafos com polilinhas, distribuição uniforme dos vértices e comprimento uniforme das arestas.

Exemplos de grafos são apresentados abaixo.

Grafo Direto Acíclico D.

Grafo Otimizado para Agrupamentos em D.

Grafo Otimizado para Agrupamentos em D.

As técnicas de projeção geométricas buscam descobrir projeções relevantes em conjuntos de dados multidimensionais.

Estas técnicas têm como princípio o mapeamento de dados multidimensionais para padrões bidimensionais, através da utilização dos valores presentes nas bases de dados como parâmetros para a geração de formas geométricas.

Estas formas devem ser tais que o conteúdo da informação representado possa ser percebido e analisado visualmente em suas propriedades gráficas, sendo que, quanto mais propriedades puderem ser percebidas individualmente, mais atributos dos dados serão discriminados.

Como exemplo temos as Coordenadas Paralelas, as Star Coordinates e os Scatter Plots e Web Diagram.

Uma das técnicas relevantes na área de Visualização de Informações é a Técnica de Visualização por Coordenadas Paralelas Parallel Coordinates.

Inicialmente apresentada por Inselberg como uma técnica de Geometria Computacional, e posteriormente contextualizada na área de Visualização de Informação, esta técnica destaca-se pela perspectiva multidimensional conferida à representação visual.

Esta técnica mapeia o espaço k-dimensional em duas dimensões visuais usando k eixos eqüidistantes que são paralelos a um dos eixos visuais.

Os eixos correspondem às dimensões e são mapeados linearmente entre os valores mínimos e máximos da dimensão correspondente.

Cada item de dado é apresentado como uma linha poligonal, fazendo intersecção com cada um dos eixos no ponto que corresponde ao valor da dimensão considerada.

Representação de Coordenada Paralela.

Observando um item de dado com os atributos, sua representação de coordenada paralela é m-1 segmentos de linha, conectando os pontos.

A forma dos segmentos de linha mostra informação sobre os níveis das m variáveis.

Está demonstrado um registro de dado que possui os atributos Crowo, Density, LLT1, SC1, SPF, UNEMF.

O valor máximo dos atributos é seis e o valor mínimo do atributo é 2.

Através da análise do gráfico, os valores assumidos por cada um dos atributos deste registro de dado podem ser observados.

Para visualizar um conjunto de dados inteiro, deve-se representar todos os itens de dados deste conjunto no mesmo gráfico.

Representação de Coordenadas Paralelas para Vários Casos.

Para grandes conjuntos de dados, a aparência deste gráfico é confusa.

No entanto, a identificação de dados ruidosos é bastante facilitada, sendo esta uma das grandes utilidades desta técnica.

A técnica de coordenadas paralelas também é bastante utilizada quando queremos observar o comportamento de um subconjunto de dados.

O exemplo a seguir mostra um subconjunto dos dados, onde estão destacados com a cor preta os dados que assumem o valor mínimo do atributo LLT1.

Os outros registros de dados aparecem representados na cor cinza.

Coordenadas Paralelas com Subconjunto de Dados.

Apesar do princípio utilizado pela técnica Coordenadas Paralelas ser simples, ela se mostra poderosa em revelar uma grande quantidade de características dos dados tais como distribuição dos dados e dependências funcionais.

A técnica Star Coordinates apresenta a visualização de dados multidimensionais.

Para cada dimensão dos dados são utilizados eixos organizados em um círculo, todos tendo a origem em um mesmo ponto.

Cada Star Coordinates consiste de um número de eixos com escalas e rótulos, representando os atributos dos dados.

Cada eixo possui um vetor bidimensional associado a ele.

Estrutura Star Coordinates.

Os valores dos dados são apresentados como pontos sobre a escala.

A posição do ponto representando o valor do dado é definida pela soma dos produtos dos vetores unitários dos eixos pelos correspondentes valores dos atributos.

Exemplo de Gráfico Star Coordinates.

A apresentação da técnica Star Coordinates mostra-se pouco esclarecedora e ambígua, uma vez que um mesmo ponto na visualização pode corresponder a registros de dados.

A solução proposta pelo autor é utilizar técnicas de interação sobre a visualização de forma a esclarecer estas ambigüidades.

A técnica Scatter-Plots é uma das técnicas mais antigas e mais usadas para projetar dados multidimensionais em duas dimensões visuais.

Nesta técnica, pares de projeções paralelas são geradas, cada uma proporcionando ao usuário uma visão geral em relação a relacionamentos entre os atributos do dado.

As projeções são geralmente dispostas numa estrutura de grade, para ajudar o usuário a lembrar das dimensões associadas com cada projeção.

Muitas variações desta técnica foram desenvolvidas para aumentar o conteúdo de informações das imagens geradas bem como para prover ferramentas para o auxílio da exploração de dados.

As vantagens desta técnica são a facilidade de interpretação de seus resultados e a relativa independência ao tamanho do conjunto de dados a ser manipulado.

Scatter Plots, Gerados com a Ferramenta Xmdv Tool.

Uma limitação desta técnica é o fato da sua eficiência diminuir ao trabalhar dados de alta dimensionalidade.

Com a geração de muitas projeções, devido ao grande número de atributos sendo trabalhados, o espaço reservado a cada projeção diminui, uma vez que o espaço total a ser utilizado é limitado ao tamanho da tela, prejudicando a análise de cada projeção.

Algumas estratégias foram desenvolvidas para melhorar esta limitação, incluindo o uso de três dimensões por projeção ou o desenvolvimento de mecanismos de ampliação de imagens (zoom).

A técnica Web Diagram é um grafo de associação que relaciona os atributos de uma base de dados.

Os atributos são listados e linhas conectam os atributos que tem algum relacionamento.

O grau de relacionamento entre os atributos é dado pela largura da linha que os conecta, onde quanto mais densa for esta linha, mais forte é a sua associação.

Esta técnica é utilizada na ferramenta Clementine, da SPSS, na análise de relacionamento de atributos de uma base.

Esta técnica é amplamente utilizada na análise de bases de dados de telecomunicações, onde se procura descobrir os motivos pelos quais os clientes cancelam suas linhas telefônicas.

Utilização do Web Diagram na análise deste contexto.

Gráfico Web Diagram, Demonstrando o Relacionamento entre os Atributos de uma Base de Dados, para Análise do Perfil de Clientes de Telefonia.

Técnicas baseadas em ícones As técnicas baseadas em ícones apresentam como principal característica o uso de ícones como forma de mapear os valores dos atributos de um item de dado multidimensional.

Cada característica visual do ícone corresponde a um atributo.

Algumas das técnicas propostas segundo esta abordagem são a técnica Chernoff-faces e a técnica Stick-Figures.

Chernoff faces.

A técnica Chernoff faces foi desenvolvida por Herman Chernoff em 1973, para representar dados multivariados de uma forma que seja facilmente discernível para os seres humanos.

São utilizados desenhos de faces para representar os atributos de uma base de dados.

As faces consistem em desenhos lineares em duas dimensões que contém uma variedade de características faciais.

Tipos de Expressões Utilizadas na Técnica Chernoff Faces.

Outros Tipos de Expressões Utilizadas na Técnica Chernoff Faces.

A razão para justificar o uso deste método baseia-se na noção que as pessoas são adeptas ao reconhecimento de faces e são capazes de notar mudanças nas características faciais.

Neste método, cada atributo facial representa um atributo do dado.

Os atributos faciais mais utilizados e citados por ordem de importância e representatividade são a área da face, a forma da face, o tamanho do nariz, o localização da boca, o sorriso, a largura da boca, a localização, separação, forma, ângulo e largura dos olhos, localização da pupila dos olhos, localização, largura e ângulo das sobrancelhas.

A variação do comportamento destes atributos comunica a variação do comportamento do dado, facilitando a interpretação do dado.

A técnica Stick Figures utiliza ícones como forma de mapear os valores dos atributos de um item de dado multidimensional.

São utilizadas as duas dimensões da tela para mapear duas dimensões dos dados e as demais dimensões são mapeadas para os ângulos e/ou segmentos de um ícone formado por múltiplos segmentos de reta.

Esquerda, Ícone Stick Figure com orientação demonstrada com 5 variáveis.

Direita, Família de ícones Stick Figure.

Cada um deles tem um corpo e quatro segmentos.

Outras características que podem ser utilizadas para demonstrar o valor de outros atributos de dados nestes ícones são a utilização de cores e espessuras das linhas do corpo e segmentos utilizados.

Os autores reportam que foram realizados experimentos com representação de até 30 variáveis.

Esta técnica é altamente dependente da seleção do mapeamento apropriado dos dados para as estruturas visuais, uma vez que o número de mapeamentos em potencial cresce como fatorial do número de variáveis a serem exibidas.

A qualidade da percepção visual deve ser uma preocupação, e se não for bem representada, pode se constituir em uma fraqueza desta técnica.

Técnicas orientadas por pixels.

As técnicas de visualização orientadas por pixels foram pioneiramente descritas por Daniel Keim, no desenvolvimento do sistema VISDB.

A idéia básica destas técnicas é representar cada valor do atributo como um único pixel colorido, atribuindo o domínio dos possíveis valores dos atributos a um mapa de cores fixas e apresentar cada um dos atributos em uma subjanela distinta.

Para um conjunto de dados com atributos, a tela é dividida em m janelas.

Com essa apresentação, a correlação e a dependência funcional entre os atributos podem ser detectadas pela análise de regiões correspondentes nas múltiplas janelas.

Esta técnica visa a maximizar a quantidade de informação representada ao mesmo tempo, sem nenhuma sobreposição, conseguindo preservar a percepção de pequenas regiões de interesse e ainda manter a visão global.

Se um único atributo for apresentado em uma janela com resolução de 1280 X1024, um milhão de valores podem ser apresentados simultaneamente.

O grande desafio desta técnica é a organização dos pixels na tela, de forma a retratar o conhecimento a ser expresso graficamente, explicitando sua semântica.

Dependendo do objetivo da visualização dos dados, existem duas formas distintas de dispor os pixels, por visualização independente de pesquisa e por visualização dependente de pesquisa.

A visualização independente de pesquisa é utilizada quando se quer visualizar um grande conjunto de dados.

Nesta técnica, ordena-se todo o conjunto de dados de acordo com alguns atributos e adota-se um preenchimento de tela padrão para o arranjo dos valores de dados.

Este padrão pode ser de diversas formas tais como linha por linha, da esquerda para direita e coluna por coluna, de cima para baixo.

Estas técnicas são úteis especialmente para dados com uma ordenação natural inerente, tais como séries temporais, podendo ser visualizados diretamente de uma base de dados.

A técnica de padrões recursivos é um dos exemplos de visualização independente de pesquisa.

No caso de inexistência de uma ordem pré-definida nos dados e o objetivo principal seja a exploração interativa da base de dados, pode-se utilizar a técnica de visualização dependente de pesquisa.

Nesta técnica, busca-se visualizar a relevância dos itens de dados em relação a uma pesquisa.

Neste caso, ao invés de mapear os valores dos atributos diretamente às cores, a técnica baseia-se no cálculo das distâncias entre os valores dos atributos e os valores trabalhados nas pesquisas.

Combinam-se, então, as distâncias de cada item de dados originando uma distância geral e se visualizam as distâncias dos atributos e a distância geral, ordenadas de acordo com a distância geral.

A organização dos dados centraliza os itens de dados mais relevantes no centro da janela e os dados menos relevantes são dispostos em forma espiral em direção à parte de fora da janela.

Uma das técnicas construídas a partir desta abordagem é a técnica de segmentos circulares.

A técnica de Padrões Recursivos é um dos exemplos de técnica orientada por pixels através da visualização independente de pesquisa, segundo a classificação.

Esta técnica é baseada num esquema recursivo genérico que permite ao usuário influenciar a disposição dos itens de dados.

Foi particularmente planejada para representar conjuntos de dados com uma ordem pré-definida por um único atributo, como séries temporais.

Nesta técnica, cada atributo é visualizado em uma subjanela.

Em cada subjanela, cada valor do atributo é representado por um pixel colorido, onde a cor reflete o valor do atributo.

Para que o usuário seja capaz de comparar ou relacionar os valores dos atributos que estão na mesma posição apresentados nas diversas janelas, os objetos são apresentados com a disposição dos pixels na mesma ordem, em cada subjanela.

Com parâmetros para cada esquema recursivo, é possível controlar as subestruturas de significado semântico que determinam a disposição dos valores dos atributos.

A técnica de padrões recursivos é baseada numa disposição "back and forth".

O elemento base da recursão é um padrão de altura h1 e largura w1, especificadas pelo usuário.

Os elementos do padrão correspondem a pixels que são dispostos dentro de um retângulo de altura h1 e largura w1, da direita para a esquerda e da esquerda para a direita.

Esta mesma disposição é praticada em todos os níveis de recursão.

Técnica de Padrões Recursivos.

Exemplo de Comparação de Preços de Ações, durante sete anos.

Estão demonstrados as variações dos valores das ações da DOW JONES, do valor do ouro, do valor das ações da IBM e do dólar durante sete anos.

A posição demonstrada em cada quadro se refere ao mesmo período de tempo.

A cor utilizada em cada uma destas posições demonstra se houve "alta" ou "queda" no valor daquele ativo, podendo estes quatro ativos ser comparados durante sete anos.

A técnica de Segmentos Circulares é uma das técnicas orientadas por pixels, construídas a partir da abordagem de visualização dependente de pesquisa segundo a classificação.

Esta técnica foi proposta por e foi desenvolvida para visualizar conjunto de dados multidimensionais.

Ao invés de representar diferentes atributos em subjanelas, todo o conjunto de dados é representado por um círculo que é dividido em segmentos, um para cada atributo.

Dentro dos segmentos, cada valor de atributo é atribuído a um pixel colorido.

A disposição dos pixels começa no centro do círculo para fora, sendo dispostos numa linha ortogonal à linha de divisão do segmento.

A idéia desta abordagem é que perto do centro todos os atributos estão dispostos perto um do outro, facilitando a comparação visual de seus valores.

Técnica de Segmentos Circulares para Dados com Oito Dimensões.

Comparação de Preços de Ações.

Na figura acima, estão representados cinqüenta tipos diferentes de ações na Bolsa de valores.

Cada subdivisão é uma ação e cada cor representa um valor da ação em um espaço de tempo.

As cores claras representam preços altos e as cores escuras representam preços baixos.

Com esta técnica, pode-se perceber o desempenho dos diversos tipos de ações simultaneamente.

As técnicas baseadas em hierarquias subdividem em k vezes o espaço dimensional e apresentam os subespaços de forma hierárquica, projetando ou embutindo esses espaços uns dentro dos outros.

Uma das técnicas hierárquicas mais conhecidas é a Dimensional Stacking.

A técnica Dimensional Stacking foi proposta por LeBlanc.

Nesta técnica são mapeados dados de um espaço discreto n-dimensional para uma imagem bidimensional.

Utilizam-se os eixos perpendiculares nos quais as dimensões são mapeadas uma dentro da outra.

Ilustrando o exemplo abaixo, seja uma base de dados com as dimensões.

Primeiramente, mapeia-se as dimensões numa visualização tradicional.

Após escolha de uma par, mapeia-se as dimensões da mesma maneira e assim recursivamente até abranger todas as dimensões da base de dados.

Desta maneira, a oclusão dos dados é minimizada e a informação espacial preservada.

Uma das maiores vantagens desta técnica em comparação às outras técnicas hierárquicas é que ela não precisa de funções ou regras extras para que se possa plotar os dados na representação.

Técnica Dimensional Stacking.

Esta técnica foi desenvolvida com o objetivo de ser aplicada sobre os resultados da etapa de mineração de dados.

Derivada da técnica Web Diagram, o Net Diagram foi elaborado com o objetivo de proporcionar o entendimento e a análise de um conjunto de regras de classificação em relação ao processo classificatório, retratando o relacionamento entre o conjunto de regras geradas na etapa de mineração de dados e sua classificação.

Nesta técnica, a identificação das regras, seus índices de interesse e as classificações do problema são apresentadas.

As classificações e a regras geradas são relacionadas através de linhas retas.

Com a utilização desta técnica, vários índices de interesse podem ser utilizados na análise de regras de indução.

Podem ser citados como exemplo, os índices de confiança, índice de suporte ou qualquer outro índice que seja pertinente à análise.

Como relatado anteriormente, o índice de confiança retrata o percentual de ocorrência de uma classificação em um determinado perfil.

Como cada regra possui um índice de confiança que expressa o grau de incidência da regra sobre a base de dados, é realizada uma comparação entre este índice e a média de ocorrência das classes na base de dados, buscando ressaltar as regras que apresentam um índice de confiança superior ao da média de ocorrência das classes.

Para tanto, são utilizadas cores na visualização das retas que conectam cada relacionamento.

Caso o índice de confiança da regra seja superior à média de ocorrência das classes, a linha que as conecta assume a cor verde.

Caso contrário, a linha assume a cor vermelha.

Isto possibilita a indicação do possível grau de interesse na avaliação da regra.

A figura a seguir ilustra a visualização de um conjunto de regras de classificação em um problema de análise de risco de crédito.

Estas regras procuram definir o perfil de clientes adimplentes ou inadimplentes em determinada base de dados.

O gráfico mostra, para cada classe estudada, as regras que as definem, o índice de confiança da regra e como este índice se comporta em relação à média da carteira de clientes desta base de dados.

Desta forma, a análise do comportamento de cada regra em relação à sua classificação e do comportamento geral do conjunto de regras é facilitado.

Técnica Net Diagram, Gráfico Gerado pela Ferramenta Visual.

A técnica Rule Relationship foi criada com o objetivo de demonstrar o relacionamento de similaridade entre um conjunto de regras geradas na etapa de mineração de dados.

A similaridade entre regras pode ser analisada sob vários contextos.

Uma regra pode ser similar à outra num contexto sintático, onde são analisadas as partes antecedentes das regras, ou semântico, onde é levado em consideração não só a sintaxe da regra, mas sua classificação e o grau de incidência da regra sobre a base de dados.

Para tanto, são calculados índices de similaridade sintático e semântico para cada par de regras relacionadas a ser utilizada na técnica Rule Relationship.

A análise sintática de similaridade leva em consideração a similaridade entre os atributos da parte antecedente da regra.

A função utilizada na técnica Rule Relationship para calcular o índice de similaridade sintático leva em consideração a técnica da Medida de Similaridade de Jaccard, também conhecida como uma razão de similaridade, que aborda a presença de partes antecedentes iguais e distintas entre cada duas regras.

Esta técnica está detalhada.

Esta fórmula foi escolhida por representar uma boa análise da parte sintática de ambas as regras, uma vez que considera os atributos similares e dissimilares das duas regras analisadas.

A aplicação desta fórmula produz um índice de similaridade entre elas.

Este índice varia entre 00 e 10, sendo a maior similaridade retratada pelo valor 10.

Este índice de similaridade calculado é retratado através do desenho de linhas retas que ligam as duas regras que possuem similaridade entre si.

A largura e a cor da reta expressa o grau de similaridade entre as regras conectadas.

A análise de similaridade entre regras também é realizada levando-se em consideração a visão semântica.

Nesta abordagem, visa-se a analisar não só a similaridade entre os atributos da parte antecedente, mas também a similaridade entre as partes conseqüentes e a representatividade de cada regra sobre a base de dados em questão.

Para tanto, empregou-se uma técnica que avalia estes três fatores.

Fórmula para Análise de Similaridade Semântica entre Regras.

Fórmula para Similaridade de Estrutura na Análise de Similaridade Semântica entre Regras.

Fórmula para Similaridade de Abrangência na Análise de Similaridade Semântica entre Regras.

A aplicação desta fórmula produz um índice de similaridade semântico entre duas regras.

Este índice varia entre 00 e 10, sendo a maior similaridade retratada pelo valor 10.

Da mesma forma que o índice sintático, o índice de similaridade semântico calculado é retratado através do desenho de linhas retas que ligam as duas regras que possuem similaridade entre si.

Nesta técnica, as regras são selecionadas primeiramente por um critério de limiar de índice de confiança aceitável e desejável pelo usuário.

Este critério pode ser informado pelo usuário.

As regras que estão acima do limiar informado são dispostas em forma circular e tem seus nomes listados na cor vermelha.

As demais regras, aquelas que não atingem o limiar informado, são listadas na cor azul, em forma circular, distantes das regras anteriores.

O percentual de similaridade entre elas é apresentado através da largura e cor das retas traçadas entre as regras, obedecendo a uma escala, conforme apresentada na tela.

A análise do diagrama Rule Relationship permite ao usuário ter uma visão geral da similaridade sintática e semântica do conjunto de regras mineradas e o grau de relacionamento entre cada duas regras similares.

Apresentando os Relacionamentos de Similaridade Sintática entre Regras.

Com o desenvolvimento das técnicas de visualização de informações e a vasta utilização destas técnicas na análise de dados, a necessidade de manipular os conjuntos de dados e as visualizações produzidas a partir destes dados durante a fase de análise é significativa.

As técnicas estáticas não permitem a flexibilidade necessária à exploração dos dados e para se obter maior poder de exploração e análise do contexto a ser estudado, o desenvolvimento de métodos que permitam a filtragem, seleção e manipulação dos dados e imagens é muito importante.

Outra necessidade que impacta a análise das visualizações é a limitação da área de exibição das visualizações.

A pequena área da tela impede a visualização dos dados ao nível de detalhe necessário para que conclusões e hipóteses sejam validadas.

Surge, então, a necessidade da criação de métodos que proporcionem condições de interação do usuário com as técnicas de visualização aplicadas e condições de visualização de uma grande quantidade de dados em níveis de detalhe satisfatórios para que a análise seja mais eficiente.

São criadas as técnicas de interação e técnicas de distorção, que são métodos de manipulação de dados e imagens gráficas.

As técnicas de interação possibilitam a interação direta com as visualizações, mudando-as de acordo com os objetivos da exploração dos dados.

Estas técnicas também possibilitam a relação e combinação de visualizações independentes.

As técnicas de distorção têm a característica de permitir que o usuário examine uma pequena porção dos dados com um maior nível de detalhe em uma parte da tela, e ao mesmo tempo, visualizar o restante do conjunto dados sem detalhamento.

Desta forma, tem-se uma visão ampla e geral de todo o contexto.

A essência das técnicas de distorção é a apresentação concorrente de partes dos dados em detalhe, ao mesmo tempo em que o contexto global é apresentado com a magnitude reduzida.

O formato desta apresentação permite o posicionamento interativo edinâmico com a parte detalhada sem comprometimento grave dos relacionamentos espaciais.

Estes métodos têm o objetivo de facilitar e viabilizar visões dos dados para que a análise a ser realizada seja feita eficientemente, possibilitando o usuário ter a visão da situação das formas mais variadas possíveis.

As técnicas de interação e distorção mais conhecidas são Projeções Dinâmicas, Filtragem Interativa, Zoom interativo, Distorção Interativa e Link and Brush.

As Projeções Dinâmicas tem o objetivo de redefinir dinamicamente projeções geradas a partir de conjunto de dados multidimensionais, proporcionando uma exploração mais abrangente dos dados.

Um exemplo clássico de utilização desta técnica é o sistema Grand Tour.

É utilizada a técnica Scatter-Plotts para mostrar todas as projeções interessantes de um conjunto de dados multidimensional.

Neste caso, foi utilizada uma série de Scatter-Plots, uma para cada relacionamento entre dois atributos.

Os Filtros Interativos foram criados com o objetivo de particionar interativamente os conjuntos de dados, possibilitando a análise de partes do conjunto separadamente.

Essas partições podem ser criadas a partir de uma seleção direta do conjunto de dados ou através de consultas que estipulariam domínios para atributos determinados.

Outra vantagem da utilização desta técnica de interação é a possibilidade de comparar o comportamento de visualizações geradas a partir de diferentes conjuntos de dados e elucidar hipóteses quanto ao comportamento dos dados nas diferentes perspectivas.

Algumas soluções propostas são Magic Lenses, Polaris e InfoCrystal.

O Zoom Interativo é uma técnica muito conhecida e aplicada em situações onde há a necessidade de visualizar grandes quantidades de dados.

Neste contexto, normalmente, as visualizações são apresentadas em forma compacta para que se consiga ter uma visão geral de todo o conteúdo.

Para se obter uma visão detalhada dos dados, é aplicada a técnica zoom, onde não somente são apresentados os objetos num tamanho maior, mas também acontecem transformações nos dados para que eles sejam apresentados em mais detalhes.

O usuário tem a opção de escolher o grau de zoom que é aplicado sobre a visualização, obtendo graus de detalhamento sobre o segmento escolhido.

Exemplos desta técnica são PADD++ e Data Space.

Distorção Interativa é uma das técnicas de distorção que possibilita visualizar detalhes de uma porção dos dados, enquanto simultaneamente se tem uma visão geral de todo o conjunto.

O usuário escolhe a área a ser detalhada e consegue ao mesmo tempo ter a visualização das áreas detalhadas e das áreas que permanecem com sua visualização original.

Esta técnica é executada através da aplicação de funções matemáticas, chamadas funções de transformação, sobre a visualização original.

Esta função de transformação define como a imagem original é mapeada para a nova imagem.

Exemplos desta técnica são Bifocal Display e Perspective Wall.

Devido à constatação de que todas as técnicas de visualização propostas possuem pontos fortes e pontos fracos, optou-se por combinar métodos de visualização diferentes de forma a resolver as dificuldades que existiam ao se trabalhar com determinada técnica isoladamente.

Desta forma, um mesmo conjunto de dados poderia ser analisado por diversas técnicas de visualização simultaneamente, propagando as ações do usuário em determinada técnica para todas as outras.

As mudanças interativas operadas sobre uma visualização são automaticamente refletidas nas outras visualizações.

Um exemplo de aplicação deste método é a identificação de subconjunto de dados selecionado em uma das técnicas sendo também identificado nas outras técnicas, tornando possível a detecção de dependências e correlações.

Chamou-se a esta estratégia de Linking and Brushing Interativo.

Xmdv e Polaris são exemplos de sistemas que implementaram esta técnica.

Se propõe uma classificação distinta à classificação, considerada relevante devido à sua abrangência sobre os recursos de interação possíveis.

Subdivide-se as técnicas de interação como interação através de recursos de navegação, interação por amostragem de dados, interação direta, interação associativa e interação sobre o sistema que suporta a visualização.

A interação através de recursos de navegação possibilita o usuário alterar parâmetros gráficos de forma que se visualize a imagem por diversos ângulos.

Para tanto, podem ser usadas scroll-bars, rotação e translação das imagens, entre outros métodos.

A interação por amostragem de dados permite ao usuário reduzir a quantidade de dados a ser manipulada em cada momento e selecionar diferentes subconjuntos de dados por algum critério especifico que o ajude a analisar o contexto.

Esta funcionalidade permite a análise de grupos específicos de dados, ajudando no direcionamento para avaliação de hipóteses.

A interação direta é utilizada quando há a necessidade de se fazer consultas sobre os dados de uma forma direta, isto é, executar pesquisas (queries) diretamente sobre os dados, para elucidar questões que surjam durante o processo de análise.

A interação associativa permite o acesso de dados relacionados em diferentes técnicas de visualização.

A interação sobre o sistema que suporta a visualização engloba as funcionalidades para abrir, editar e manipular arquivos externos, apresentar campos de dados específicos, estipular parâmetros para as visualizações, criar ou manipular as saídas do sistema e apresentar informações extras sobre os dados.

A classificação apresentada por Grinstein e Ward tem um enfoque pouco diferente da classificação.

A proposta é especialmente relevante devido à atenção demonstrada sobre a potencialidade de interação fora do contexto da manipulação da visualização propriamente dita.

As técnicas de distorção e interação potencializam o poder de análise e exibição de dados das técnicas de visualização propostas.

Com o desenvolvimento e aplicação das técnicas do processo de KDD, a necessidade de envolver o conhecimento de um especialista em cada uma das fases deste processo, para que este fosse eficiente, se tornou cada vez mais premente.

A associação do conhecimento do especialista do universo a ser trabalhado durante as fases de desenvolvimento do processo de KDD minimiza a quantidade de tempo e dinheiro investida na descoberta de regras de negócios irrelevantes e guia os objetivos da mineração de dados a caminharem em consonância com a necessidade exposta.

Combinar o conhecimento, flexibilidade, criatividade e percepção do especialista com as técnicas de mineração de dados potencializa a eficiência do processo de KDD.

No entanto, a tentativa de realizar esta integração encontra algumas dificuldades.

Entre elas está a necessidade do especialista possuir conhecimento técnico específico para que ele possa entender e participar de algumas fases do processo de KDD, fato que dificulta a integração do conhecimento do especialista humano e a equipe especializada nas técnicas do processo de KDD.

Devido ao grande avanço da capacidade de processamento dos computadores e também ao desenvolvimento das técnicas gráficas, surgiu a proposta de utilizar técnicas de visualização gráfica de dados como forma de facilitar o entendimento das diversas fases do processo de descoberta de conhecimento.

Esta proposta é chamada de Visual Data Mining.

Visual Data Mining é a junção de técnicas de visualização gráficas de dados e informações com as técnicas tradicionais de mineração de dados, com o objetivo de facilitar o entendimento do processo de descoberta de conhecimento em bases de dados.

Esta abordagem integra a habilidade de exploração e entendimento da mente humana com o enorme poder de processamento dos computadores para criar um ambiente de descoberta de conhecimento que capitaliza o melhor dos dois mundos.

A integração de especialidades entre duas disciplinas diferentes é um processo difícil de comunicação e reeducação.

Integrar KDD e Visualização de Informações é particularmente complexo pela vasta necessidade de pesquisa em cada uma destas áreas.

O Visual Data Mining possui três abordagens principais de integração.

São elas a visualização precedente, a visualização subseqüente e a visualização fortemente acoplada.

Chama-se visualização precedente (Precedent Visualization PV) ao emprego de técnicas de visualização no contexto pré-mineração.

Nesta fase, a utilização de técnicas de visualização visa a identificar um espaço de busca mais adequado à investigação, entendimento do domínio e distribuição de cada atributo e visualização de dados com estruturas complexas.

Já a aplicação de técnicas de visualização na fase posterior à mineração de dados é utilizada como ferramenta de apresentação de resultados de análises e é chamada de visualização subseqüente.

Nesta fase procura-se mostrar explicitamente os fatos que foram encontrados e fornecer ao especialista uma visão mais clara e abrangente dos resultados obtidos.

Regras de associação, regras de classificação, árvores de decisão, agrupamentos, mineração de textos podem ser expressas graficamente, facilitando o seu entendimento.

Visualização Precedente, Visualização Subseqüente, Visualização Altamente Integrada.

A área de Mineração de Dados Visual utiliza também as técnicas de visualização na etapa de mineração de dados.

Durante esta etapa, vários algoritmos podem ser utilizados para minerar o conhecimento.

O casamento das técnicas de visualização e das técnicas analíticas de mineração de dados promoveria um melhor entendimento dos resultados dos diversos algoritmos de mineração de dados e facilitaria a escolha dos algoritmos de mineração de dados a serem empregados a seguir.

É o que chamamos de visualização altamente integrada (Tightly Integrated Visualization TIV).

Esta integração também possibilitaria ao usuário entender dados e padrões, tomar decisões baseadas nas suas percepções dos resultados e tomar decisões baseadas no conhecimento de domínio.

Várias pesquisas e ferramentas têm sido propostas com o objetivo de trabalhar nestas três abordagens apresentadas.

Um grande número delas se enquadra nas áreas de visualização precedente e subseqüente e apresenta funcionalidades de interação e distorção das visualizações, possibilitando maiores chances de análise dos dados.

Há também inúmeras pesquisas sendo realizadas e ferramentas sendo desenvolvidas na abordagem da visualização altamente acoplada.

Entre elas pode ser citada a ferramenta Clementine, da SPSS.

Esta pesquisa tem a proposta de desenvolver e apresentar a ferramenta Visual DATAMINER, que implementa técnicas de visualização de informações para apresentação e análise dos resultados de regras de classificação, regras de associação e regras fuzzy.

Esta ferramenta foi desenvolvida com o objetivo de ser integrada à ferramentas de mineração de dados que extraiam regras de classificação clássicas, regras de classificação fuzzy e regras de associação.

A proposta de integração da ferramenta Visual DATAMINER à ferramentas de mineração de dados é baseada na idéia de unir o melhor dos dois mundos, mineração de dados e visualização de informações, na busca de oferecer alternativas de mineração de dados e de interpretação e análise dos produtos extraídos em um só ambiente.

O processo de KDD tem sido amplamente utilizado na análise de dados e descoberta de informações relevantes ao desenvolvimento de áreas empresariais, financeiras e científicas.

As técnicas de mineração de dados estão se tornando cada vez mais críticas na manipulação de grandes conjuntos de dados e os resultados obtidos tem se mostrado cada vez mais significativos.

Grandes esforços têm sido empreendidos de forma a desenvolver este processo, principalmente na etapa de mineração de dados.

O processo de descoberta de conhecimento em bases de dados tem sido enriquecido pela participação de especialistas das áreas trabalhadas.

Com esta participação, tenta-se unir o conhecimento de domínio da área analisada com o conhecimento descoberto através das técnicas de mineração de dados.

A participação do especialista muitas vezes é prejudicada devido à apresentação dos resultados obtidos na etapa de mineração de dados ser realizada em notação científica ou formas pouco amigáveis ou pouco conhecidas do seu cotidiano.

A necessidade de aculturação do especialista ao mundo da mineração de dados cria barreiras, gasto de esforço e tempo e muitas vezes desgaste entre as equipes.

Por este motivo, observa-se hoje a necessidade de desenvolver pesquisas com o objetivo de otimizar a apresentação e o entendimento dos resultados obtidos na etapa de mineração de dados.

A etapa de interpretação dos resultados, onde esta apresentação acontece, é essencial ao processo de KDD.

Esta etapa visa a apresentação, interpretação, entendimento e aprendizado do conhecimento descoberto.

A dificuldade de entender notações ou expressões geradas na etapa de mineração de dados é um dos desafios a ser encarado nesta etapa.

Outra dificuldade é a impossibilidade de fazer análises diversas, para verificar relacionamentos entre os diversos resultados obtidos ou implicações destes resultados sobre a base de dados.

Esta restrição pode impedir que se tire todo o proveito do conhecimento minerado e pode diminuir a eficiência do processo de KDD como um todo.

Para que o conhecimento descoberto e o entendimento do processo sejam claramente conhecidos de todos os interessados, várias pesquisas têm sido empreendidas na busca das melhores formas de resolver estas dificuldades.

A aplicação de técnicas de visualização de informações nesta etapa tem sido uma das formas utilizadas para minimizar esta dificuldade e este esforço tem produzido grandes avanços e facilidades ao processo, facilitando o entendimento e o acesso dos usuários ao conhecimento descoberto na etapa de mineração de dados.

Com este objetivo em mente, foi desenvolvida a ferramenta Visual DATAMINER, que tem como proposta aplicar técnicas usadas na área de Visualização de Informações na etapa de Interpretação dos Resultados do processo de KDD.

A partir dos resultados obtidos em um processo de mineração de dados, aplica-se técnicas de visualização de informações sobre estes com o objetivo de apresentar o conhecimento minerado e facilitar a análise exploratória de conhecimentos implícitos do processo de mineração de dados.

A ferramenta Visual DATAMINER trata da análise de regras clássicas, fuzzy e regras de associação, demonstrando graficamente o comportamento das regras, o relacionamento das regras com sua classificação/associação, proporcionando a análise do comportamento de outros atributos da base de dados no universo da regra e a análise de similaridade entre as regras mineradas.

A ferramenta Visual DATAMINER foi desenvolvida em Java e a interface foi implementada utilizando o pacote Swing.

Devido às facilidades oferecidas pela linguagem Java, foram aplicados fundamentos da programação orientada a objetos, o que possibilitou a utilização dos conceitos de herança, encapsulamento e reusabilidade de código.

Este capítulo descreve a ferramenta Visual DATAMINER, apresentando sua estrutura modular, as técnicas implementadas e sua interface gráfica.

Os formatos dos arquivos utilizados para armazenar as bases de dados estão descritos.

As principais vantagens da ferramenta Visual DATAMINER são a aplicação e integração de várias técnicas de visualização de informações na interpretação de regras de classificação e associação em um único ambiente.

A possibilidade de exploração e análise dos dados originais em relação às regras geradas também é um dos pontos a ser ressaltado.

Foram abordadas regras de classificação clássicas, regras de classificação fuzzy e regras de associação.

As técnicas de visualização de informações empregadas nesta ferramenta são as Coordenadas Paralelas, o Net Diagram, o Rule Relationship e Histograma.

A motivação para investigar as técnicas de Coordenadas Paralelas e Histogramas e para propor as técnicas Net Diagram e Rule Relationship foi o potencial ilustrativo, a clareza e a simplicidade de interpretação que elas proporcionam à análise e observação do conjunto a ser avaliado.

A técnica de Coordenadas Paralelas foi escolhida por ser uma técnica gráfica clássica e simples que demonstra todo o corpo da regra, partes antecedentes, conseqüentes e valores assumidos por estas partes, de forma gráfica.

A técnica Net Diagram foi utilizada com o objetivo de se obter uma visão geral do comportamento das regras em relação ao processo classificatório, bem como da regra em relação à sua classificação.

O Rule Relationship é utilizado para mostrar graficamente o relacionamento entre as regras e o grau de similaridade entre elas, sob análises sintáticas e semânticas.

Por fim, a técnica Histograma foi utilizada visando a mostrar a representatividade e comportamento dos atributos dos dados objeto de análise.

Ainda em busca de oferecer maior facilidade e flexibilidade ao processo de análise das regras geradas, técnicas de interação sobre os dados e sobre as visualizações estão disponíveis.

As técnicas de interação implementadas são o uso de scroll-bars, filtros interativos de seleção de dados e possibilidade de aplicar técnicas de visualização de informações conjuntamente e simultaneamente, proporcionando a associação de técnicas para potencializar a capacidade de análise.

Essas técnicas e métodos gráficos implementados são detalhados durante este capítulo.

A ferramenta Visual DATAMINER foi organizada em módulos visando a oferecer uma fácil e rápida navegação entre eles.

A divisão foi baseada nos tipos de insumos a serem trabalhados e está representada.

O primeiro nível contém o módulo central, que é a tela principal da ferramenta.

Esta tela permite o acesso aos outros dois módulos da aplicação.

O segundo nível apresenta os módulos que trabalharão com cada tipo de insumo, regras de classificação clássicas, regras de classificação fuzzy e regras de associação.

Neste nível, os módulos servem apenas como menu de opções para as operações inerentes a cada tipo de regra a ser trabalhada.

Estas operações estão localizadas no terceiro nível.

O terceiro nível apresenta as opções de entrada de parâmetros a serem utilizados na parte de análise das regras, integração de arquivos e a parte de análise propriamente dita.

Estes três módulos serão detalhados nas próximas seções.

A tela principal da ferramenta é apresentada.

Através da barra de menus presente nesta tela, o usuário tem acesso às operações específicas dos módulos implementados da ferramenta.

Tela Principal da Ferramenta VisualDATAMINER.

Para a análise de regras de classificação clássicas, existem três funcionalidades que foram desenvolvidas, a funcionalidade para que o usuário informe os parâmetros necessários à análise da regras, a funcionalidade que prepara os dados advindos de ferramentas de mineração para serem utilizados no Visual DATAMINER e a funcionalidade que aplica as técnicas de visualização sobre as regras mineradas, baseadas nos parâmetros informados.

Estas funcionalidades estão descritas nas próximas seções.

Para a análise de regras de classificação, existe a necessidade de estipular uma série de parâmetros que auxiliarão a análise do estudo de caso e a aplicação das técnicas de visualização propostas nesta pesquisa.

Para tanto, foi desenvolvida uma funcionalidade para a informação dos parâmetros necessários tais como as classes objeto de estudo, a média de previsão de ocorrência de cada classe, o limiar de índice de confiança para a análise de similaridade e a importância de cada atributo da base de dados no processo classificatório.

Estes parâmetros são informados na tela apresentada.

Tela Parâmetros para Análise de Regras Clássicas.

O motivo de permitir a definição destas informações via parâmetros informados pelos usuários é tornar a ferramenta genérica para qualquer tipo de problema a ser analisado e também permitir variações nas técnicas de visualização implementadas.

Os parâmetros que indicam a importância de cada atributo para o processo classificatório podem ser informados diretamente pelo usuário ou importado de um arquivo texto gerado por uma técnica de mineração de dados como redes neurais.

Esta funcionalidade foi criada para preparar os dados advindos da ferramenta de mineração de dados para utilização na ferramenta Visual DATAMINER.

A partir de dois padrões de arquivos determinados, gerados pela ferramenta de mineração de dados, esta funcionalidade gera 5 padrões de arquivos necessários para o funcionamento da ferramenta Visual DATAMINER.

O layout dos arquivos utilizados por esta funcionalidade encontram-se.

Esta funcionalidade é a parte central da ferramenta Visual DATAMINER.

Ela possibilita a aplicação das técnicas de visualização sobre o conjunto de regras, seleção e manipulação das regras a serem avaliadas e integração de mais de uma técnica de visualização em um mesmo display.

Layout da Tela de Análise de Regras de Classificação Clássicas.

O layout da tela é ilustrado e implementado da seguinte forma, a tela de análise de regras de classificação clássicas é dividida em duas partes verticalmente.

A parte da esquerda recebe as escolhas e parâmetros de análise do usuário e está dividida em três painéis.

O primeiro painel apresenta as regras clássicas a serem analisadas.

Dependendo da técnica escolhida, estas regras são trabalhadas individualmente, podendo ser escolhidas uma a uma, ou em grupo.

O segundo painel apresenta as técnicas escolhidas e desenvolvidas na ferramenta, sendo estas a Coordenadas Paralelas, o Net Diagram e o Rule Relationship.

O terceiro painel apresenta a possibilidade de selecionar as regras a serem trabalhadas segundo critérios mutuamente exclusivos (filtros) tais como limiares de confiança, limiares de suporte ou limiares de importância das regras.

Esta seleção possibilita ao usuário fazer análises em subconjuntos de regras distintos, potencializando e possibilitando uma maior autonomia do usuário na manipulação das regras.

A parte direita da tela apresenta a visualização das regras segundo os parâmetros escolhidos.

Esta é dividida em duas partes horizontalmente.

Na parte superior, apresenta-se a visualização da(s) regra(s) segundo a técnica gráfica escolhida, enquanto a parte inferior é destinada à utilização de métodos auxiliares de interação e distorção para ajuda na exploração das informações.

A análise de regras fuzzy segue os mesmos padrões utilizados para a análise das regras clássicas, com apenas algumas modificações para atender a especificidades deste tipo de regra.

Essas modificações estão descritas abaixo.

Para a análise de regras de classificação fuzzy, também existe a necessidade de estipular uma série de parâmetros que auxiliarão a análise do estudo de caso e a aplicação das técnicas de visualização propostas nesta pesquisa.

Para tanto, foi desenvolvida uma funcionalidade para a informação dos parâmetros necessários tais como as classes a serem objeto de estudo e a média de previsão de ocorrência de cada classe e o limiar de índice de confiança utilizado na análise de similaridade das regras.

Nesta parte não será necessária a informação do grau de importância de cada atributo na classificação, pois as regras fuzzy possuem inerentemente este conceito na sua formação.

Parâmetros para Análise de Regras Fuzzy.

A mesma funcionalidade desenvolvida para a criação dos arquivos para manipulação de regras clássicas também foi desenvolvida para a manipulação de regras fuzzy.

Esta funcionalidade gera os arquivos necessários para que as regras fuzzy sejam representadas através das técnicas de visualização, promovendo a integração entre ferramentas de mineração de dados e a ferramenta Visual DATAMINER.

A tela de análise de regras de classificação fuzzy segue o mesmo padrão utilizado na análise de regras clássicas, apresentando apenas pequenas diferenças inerentes às regras em questão.

A tela principal também é dividida em duas partes verticalmente.

A primeira parte recebe as escolhas e parâmetros de análise do usuário e está dividida em três painéis.

O primeiro painel apresenta as regras de classificação fuzzy a serem analisadas.

Dependendo da técnica escolhida, estas regras são trabalhadas individualmente, podendo ser escolhidas uma a uma, ou como um grupo.

O segundo painel apresenta as técnicas escolhidas e desenvolvidas nesta ferramenta.

O terceiro painel apresenta a possibilidade de selecionar as regras a serem trabalhadas segundo critérios de seleção de limiares de grau de certeza, característica inerente às regras fuzzy, e limiar do grau de importância, que será detalhado na seção 444.

A segunda parte da tela apresenta a visualização das regras segundo os parâmetros escolhidos.

Esta é dividida em duas partes.

Na primeira, apresenta-se a visualização da(s) regra(s) segundo a técnica gráfica escolhida, enquanto a segunda parte é destinada à utilização de métodos auxiliares para ajuda na exploração das informações.

As técnicas gráficas utilizadas na análise das regras fuzzy são praticamente as mesmas já citadas anteriormente na análise de regras clássicas.

A única alteração realizada foi a adequação da técnica Net Diagram para demonstrar os índices de importância das partes antecedentes da regra fuzzy na definição da classificação obtida.

Esta modificação será explicada, onde é apresentada a técnica Net Diagram.

Tela de Análise de Regras Fuzzy.

Outra adaptação realizada para o tratamento de regras fuzzy foi a utilização de limiares de certeza e importância para seleção das regras a serem analisadas, ao invés de índices de confiança e suporte.

Com a aplicação das técnicas propostas nesta pesquisa, o processo de análise de regras fuzzy foi beneficiado devido às possibilidades de análise das regras, de visualização gráfica dos textos das regras, análise da similaridade entre o grupo de regras geradas e de associação de técnicas gráficas para melhor entendimento do problema.

O tratamento a ser dado às regras de associação conta com três funcionalidades, a informação dos parâmetros a serem utilizados, a integração dos arquivos e a análise das regras.

O único parâmetro que é informado para a análise das regras de associação é o limiar de confiança para análise de similaridade, que é utilizado pela técnica Rule Relationship.

Os demais parâmetros utilizados para as regras de classificação não são informados por que são inerentes à problemas de classificação.

Tela de Parâmetros para a Análise de Regras de Associação.

A funcionalidade de integração de arquivos funciona da mesma forma que para os outros tipos de regras.

Arquivos que possibilitam a manipulação das regras de associação são gerados, a partir de arquivos com layouts padrões, gerados pelas ferramentas de mineração de dados.

A tela de análise de regras de associação segue o mesmo padrão utilizado na análise de regras clássicas, já detalhada anteriormente, excluindo-se apenas a técnica Net Diagram, uma vez esta técnica trata inerentemente de problemas de classificação.

Na aplicação da técnica Rule Relationship foram retiradas as opções para escolha das classes, uma vez que este conceito não se aplica às regras de associação.

Tratamento das Regras de Associação.

O benefício observado na análise de regras de associação se dá especialmente na análise de similaridade entre as regras, onde se pode observar o grau de similaridade sintático e semântico entre as regras através da técnica Rule Relationship, bem como na análise do comportamento de atributos no universo da regra, através da técnica Histograma.

As técnicas implementadas na ferramenta Visual DATAMINER serão descritas nas seções seguintes.

Técnica Coordenada Paralelas e Dados Sob a Abrangência da Regra Escolhida.

Técnica geométrica clássica criada em 1985, a técnica de Coordenadas Paralelas é classificada como uma das técnicas pioneiras da área de Visualização de Informações.

Esta técnica é apresentada detalhadamente.

Ela foi escolhida com o objetivo de proporcionar a visualização gráfica das regras individualmente.

Cada regra selecionada pelo usuário é apresentada graficamente, sendo cada parte antecedente e conseqüente representada por um eixo vertical.

Os valores assumidos pelas partes antecedentes, verdadeiro ou falso, são mostrados na lateral esquerda do gráfico, sendo apresentados também, à direita do gráfico, as classificações possíveis para a regra no contexto analisado e o seu índice de confiança.

Uma reta horizontal traçada cruzando os eixos verticais faz a relação das partes antecedentes com o valor assumido por ela e também da classificação com o valor relacionado.

O objetivo da utilização desta técnica é facilitar o entendimento de regras que apresentam muitas condições antecedentes e o entendimento dos valores que estas partes assumem.

De uma só vez, você vê todos os atributos que a regra utiliza, os valores que estes atributos assumem, a classificação obtida na utilização deste conjunto e o índice de confiança desta regra em relação à base original.

Como forma de interação, há a possibilidade de usar scroll-bars para percorrer toda a extensão do gráfico gerado, uma vez que a área da tela é limitada.

Junto com a visualização das regras, são apresentados os registros da base de dados original que são "atingidos" pela regra escolhida, complementando a análise das regras com os dados que refletem a sua sintaxe, conjugando o potencial de duas técnicas de análise.

Comportamento do Atributo SEXO, no Universo Atendido pela Regra.

Com o objetivo de analisar detalhadamente os dados que são retratados por uma regra, a ferramenta Visual DATAMINER proporciona a funcionalidade de observar o comportamento de qualquer um dos atributos dos registros de dados atendidos pela regra escolhida.

A técnica histograma permite que seja analisada a distribuição dos valores dos atributos em determinado universo, mostrando graficamente o percentual que cada valor do domínio do atributo representa neste universo.

A partir da seleção de uma regra e da apresentação gráfica desta regra através da técnica de Coordenadas Paralelas, os dados atendidos pela mesma são apresentados.

Ao se escolher um atributo qualquer da base de dados, a distribuição deste atributo é demonstrada através da técnica Histograma.

Esta distribuição é apresentada em três abordagens, o comportamento do atributo no universo de dados selecionado pela regra, o comportamento do atributo no universo original da base de dados e o comportamento do atributo no universo original da base de dados quanto ao processo classificatório.

O objetivo desta análise é avaliar, em relação a uma regra, a mudança de comportamento de determinados atributos em relação à base original de dados.

Se a mudança de comportamento do atributo é significativa, pode-se concluir que este atributo deve ser considerado para o perfil apresentado pela regra.

Comportamento do atributo sexo no universo da regra 1.

Comportamento do Atributo SEXO, no Universo da Base Original.

O comportamento deste mesmo atributo pode ser analisado em relação ao universo total da base de dados original e também em relação ao universo total da base de dados original com o detalhamento em relação ao processo classificatório.

Desta forma, tem-se a visão dos atributos de uma base de dados em relação à regra analisada, em relação ao dado original e em relação ao processo classificatório como um todo.

Comportamento do Atributo SEXO, no Universo da Base Original, detalhado pela classificação.

A técnica Histograma não se apresenta muito eficiente quando se trata de atributos com muitos domínios.

A análise e comparação dos diversos valores de domínios ficam prejudicadas uma vez que não se consegue ter a visão gráfica de todos eles em uma mesma tela.

Portanto, para atender a atributos com muitos valores em seu domínio, foi projetada a alternativa de avaliar este atributo na forma tabular, conforme demonstrado.

Esta funcionalidade está disponível nas três formas de análise de distribuição.

Comportamento do Atributo SEXO, no Universo Atendido pela Regra, em Forma Tabular.

Baseada na técnica Web Diagram, esta técnica foi elaborada com o objetivo de mostrar o comportamento das regras em relação ao processo classificatório.

Nesta técnica, as regras são trabalhadas em grupo e se tem a visão do comportamento das mesmas em relação às classificações possíveis do problema analisado.

Para tanto, são apresentados os nomes das regras, seu índice de confiança e as classificações possíveis para o problema a ser analisado.

Por linhas retas, liga-se esta regra à sua classificação.

Para a análise da regra em relação ao processo classificatório, é utilizado um índice que representa a média de ocorrência da classificação, que indica o padrão de comportamento da base em relação à determinada classificação.

Este índice é informado pelo usuário, na tela de Parâmetros.

Tendo como base esta média, compara-se esta com o índice de confiança obtido pela regra, avaliando se a representatividade da regra sobre o processo de classificação é mais significativo do que a média conhecida da base.

Caso esta regra obtenha um índice de confiança abaixo da média de sua ocorrência, o que significa que a regra conseguiu uma abrangência inferior ao que já se pratica na base de dados, este fato é indicado pela reta de ligação entre a regra e a classe assumir a cor vermelha.

Caso contrário, a reta assume a cor verde.

Se o índice de confiança está acima desta média, isto indica que esta regra consegue determinar um perfil de comportamento de sua classe de forma mais precisa do que hoje se conhece, mostrando que esta regra deverá ser investigada e possivelmente algum conhecimento relevante para o contexto analisado poderá ser aprendido.

O conhecimento deste tipo de informação é relevante na avaliação da importância e representatividade do conhecimento minerado.

Tela com o emprego da técnica Net Diagram com um exemplo de visualização da regra.

Para melhor avaliação da regra, ao se clicar com o mouse sobre o nome da regra que está sendo observada, é apresentada o corpo da regra em forma gráfica.

Para o caso de regras de classificação clássicas e regras de associação é utilizada a técnica Coordenadas Paralelas, para que o usuário possa avaliar o corpo da regra.

Net Diagram, Aplicado a Regras Clássicas.

Para o caso de regras fuzzy, é utilizada uma implementação de forma que se possa detalhar o grau de importância de cada parte antecedente da regra escolhida.

Ao escolher com o mouse uma das regras apresentadas pela técnica Net Diagram, esta regra é apresentada graficamente na parte inferior direita da tela, onde é mostrado o índice de importância de cada parte antecedente da mesma.

As partes antecedentes das regras encontram-se ordenadas decrescentemente pelo grau de importância que elas apresentam, para que se tenha a visão do grau de importância destas partes para o processo classificatório.

Esta é mais uma forma de associar a potencialidade de técnicas de visualização com o objetivo de facilitar a análise e o entendimento do conhecimento descoberto, caracterizando a implementação de uma técnica de distorção.

Net Diagram, Aplicado a Regras Fuzzy.

Devido à necessidade de selecionar subconjuntos de dados, foi implementada uma técnica de interação de filtro de dados sobre o conjunto original de regras.

Estes filtros são implementados sobre alguns índices das regras tais como o índice de confiança, o índice de suporte, o índice de Gini e o índice de importância, este último gerado a partir desta pesquisa.

O índice de suporte é o percentual de transações no conjunto de dados que contém as partes antecedentes da regra e o índice de importância representa uma medida de representatividade da regra no processo de classificação.

O índice de Gini é calculado a partir da razão entre o índice de confiança da regra e a freqüência das classes do problema.

Conforme o filtro de limiares realizada na parte inferior esquerda da tela, são também apresentados na técnica Net Diagram os índices de suporte e importância de cada regra.

Esta dissertação propõe uma técnica de atribuição de importância aos atributos que formam uma regra(peso).

Conforme a importância atribuída aos atributos que compõe uma regra, a representatividade desta regra para o processo de classificação pode ser avaliada.

O grau de importância dos atributos pode ser conhecido pela experiência dos especialistas da área ou pode ser fruto de informações obtidas na etapa de mineração de dados, tais como os pesos obtidos através do uso de uma rede neural.

Estes dados são informados para cada atributo da base na tela de Parâmetros das regras clássicas.

O índice é calculado baseado no grau de importância de cada atributo da regra para o processo de classificação, pela seguinte fórmula.

A criação deste índice é mais uma contribuição desta pesquisa para a área de Visual Data Mining.

O índice de importância pretende informar ao usuário uma medida de relevância da regra, para que o conjunto de regras possa ser analisado.

A análise da similaridade entre regras é um ponto importante a ser explorado na interpretação dos resultados do conhecimento minerado.

É comum a situação de obter inúmeras regras extraídas a partir de um processo de mineração de bases de dados, entretanto, também é comum o fato da maioria delas serem muito similares entre si.

A seleção de regras representativas e distintas entre si é um trabalho demorado e repleto de variações.

Para facilitar esta análise, criou-se a técnica Rule Relationship, que tem o objetivo de mostrar a similaridade sintática e semântica entre as regras extraídas.

Nesta técnica, as regras são selecionadas primeiramente por um critério de limiar de índice de confiança aceitável pelo usuário.

Este critério é informado pelo usuário na tela de Parâmetros.

Esta pré-seleção foi implementada para que se tenha a visão das regras com maior abrangência sobre os dados analisados.

As regras que estão acima do limiar informado são dispostas em forma circular e tem seus nomes listados na cor vermelha.

As demais regras, aquelas que não atingem o limiar informado, são listadas na cor azul, em forma circular, distantes das regras anteriores.

Estas regras são analisadas sintaticamente e semanticamente.

A análise sintática de similaridade leva em consideração a similaridade entre os atributos da parte antecedente da regra.

A função de análise sintática implementada leva em consideração a técnica da Medida de Similaridade de Jaccard, também conhecida como uma razão de similaridade, que aborda a presença de atributos similares entre cada duas regras e os atributos distintos de ambos os lados.

Esta técnica foi detalhada.

Técnica Rule Relationship, com Análise Sintática de Regras de uma Mesma.

Esta fórmula foi escolhida por representar uma boa análise da parte sintática de ambas as regras.

A aplicação desta fórmula produz um índice de similaridade entre elas.

Este índice varia entre 00 e 10, sendo a maior similaridade retratada pelo valor 10.

Este índice de similaridade calculado é retratado através do desenho de linhas retas que ligam as duas regras que possuem similaridade entre si.

O percentual de similaridade entre elas é apresentado através da largura e cor das retas traçadas entre as regras, obedecendo a uma escala, conforme apresentada na tela.

Para se ter uma melhor visualização da similaridade entre as regras, esta técnica provê a opção de ser aplicada entre regras de mesma classificação ou entre todas as regras do conjunto.

A análise de similaridade entre regras também é analisada levando-se em consideração a visão semântica.

Nesta abordagem, visa-se analisar não só a similaridade entre os atributos da parte antecedente, mas também a similaridade entre as partes conseqüentes e a representatividade de cada regra sobre a base de dados em questão.

Para tanto, empregou-se uma técnica denominada função de agrupamento que avalia estes três fatores e que foi proposta em.

Desta forma os três critérios são avaliados gerando um índice de similaridade semântico.

Esta técnica também está detalhada.

Técnica Rule Relationship, com Análise Semântica de Regras.

De posse do índice de similaridade semântico entre duas regras, a ferramenta Visual DATAMINER utiliza a mesma técnica de visualização usada para análise de similaridade sintática apresentada anteriormente.
Demonstrando-a através da largura e cor utilizada na reta que as conecta Para melhor entendimento da similaridade entre as regras, a ferramenta Visual DATAMINER proporciona a funcionalidade de, ao clicar na reta que demonstra a similaridade entre duas regras, o texto de cada uma das regras é apresentada na parte inferior da tela.

Esta é mais uma das técnicas de distorção implementadas nesta pesquisa.

Corpo das Regras com Similaridade Sintática entre si.

Estas técnicas de análise de similaridade proporcionam a visão do grau de similaridade sintático e semântico das regras extraídas, podendo ser analisadas entre as classes ou no contexto geral.

Os resultados obtidos através da aplicação das técnicas de Visualização de Informações no processo de descoberta de conhecimento em bases de dados, especificamente na etapa de Interpretação dos Resultados, podem ser observados no próximo capitulo, onde é apresentado um estudo de caso analisado através da ferramenta Visual DATAMINER.

A ferramenta Visual DATAMINER foi desenvolvida para facilitar e enriquecer a interpretação dos resultados de um processo de mineração de dados.

Através da aplicação de técnicas de visualização de informações sobre regras de associação e regras de classificação clássicas ou fuzzy busca-se descobrir conhecimento sobre o problema analisado.

Com o objetivo de mostrar a funcionalidade da ferramenta Visual DATAMINER, a realização de um estudo de caso é apresentada neste capítulo.

Neste estudo de caso, a ferramenta VisualDATAMINER foi utilizada em uma simulação de situação real, como se esta estivesse sendo utilizada por um usuário.

Com a realização deste estudo de caso busca-se ressaltar os benefícios, facilidades e o aproveitamento do uso da ferramenta VisualDATAMINER na interpretação dos resultados minerados.

O problema escolhido para realização deste estudo é o de análise de risco de crédito ao consumidor.

Este domínio foi escolhido por se tratar de um problema de larga escala, por envolver dados reais com múltiplos atributos relacionados e por sua relevância para o meio empresarial.

As seções a seguir detalham as etapas necessárias à realização deste estudo de caso.

Análise de crédito é uma tarefa desempenhada pela grande maioria das instituições financeiras e comerciais, quando se há a necessidade de concessão de crédito ou empréstimo de dinheiro a pessoas físicas ou jurídicas.

Para que o crédito seja concedido, uma avaliação do perfil do cliente que se interessa por receber o crédito é realizada, baseada em suas características sócio-econômicas.

Para tanto, são utilizadas regras, definição de perfis, técnicas matemáticas e/ou a experiência do especialista que avalia solicitações de crédito realizada por consumidores.

Por serem realizadas em um cenário de incertezas e constantes mudanças, onde as decisões precisam ser tomadas rapidamente, o risco associado às concessões de crédito pode ser elevado.

Portanto, as análises precisam ser executadas de forma correta e precisa, uma vez que a inadimplência no pagamento dos empréstimos pode vir a comprometer a saúde financeira das empresas de crédito.

Muitas técnicas matemáticas vêm sendo utilizadas com êxito no apoio à decisão de alto risco, onde se destacam a análise de componentes principais, a classificação de dados, a análise canônica, a análise fatorial das correspondências e a análise discriminante.

Outras estratégias têm sido utilizadas na definição do perfil do cliente bom/mau pagador tais como a utilização de sistemas especialistas e sistemas de mineração de dados.

Apesar de todas as técnicas utilizadas na definição do perfil do cliente, o conhecimento especialista humano jamais pode ser desprezado.

As técnicas e sistemas utilizados na definição da concessão de crédito se apresentam apenas como ferramentas de apoio à decisão.

Na verdade, o especialista se utiliza destas técnicas para realizar a decisão final, sendo estas estratégias direcionamentos para suas conclusões.

Este problema possui características inerentes à um problema de classificação, onde serão trabalhadas regras de classificação clássicas e regras de classificação fuzzy, em sua análise.

Antes de executar as funcionalidades de cada técnica desenvolvida nesta pesquisa é necessário definir os valores de alguns parâmetros a serem usados no problema a ser analisado.

Para aplicação das técnicas de visualização de informações sobre as regras clássicas, os seguintes parâmetros foram empregados, nome das classes do problema.

Código de identificação da classe.

Média de ocorrência da classe.

Valor de importância dos atributos da base de dados sobre o processo de classificação.

Limiar de seleção das regras na análise de similaridade.

Estes parâmetros são informados para caracterização do problema a ser analisado.

Desta forma, a ferramenta Visual DATAMINER pode ser aplicada sobre qualquer domínio de problema que necessite de análise.

Os valores praticados neste estudo de caso estão demonstrados.

Parâmetros Utilizados na Análise de Regras Clássicas, Informados pelo Usuário, na Tela de Parâmetros para Regras Clássicas da Ferramenta Visual DATAMINER.

O problema de análise de risco de crédito representa as possíveis classes de perfil como a dos clientes adimplentes e a dos clientes inadimplentes.

A cada uma destas classes é atribuída uma identificação, 0(zero) para a classe adimplente e 1 para a classe adimplente.

A média de ocorrência da classe adimplente na base de dados original para o problema investigado é de 80% e a da classe inadimplente é de 20%, percentuais extraídos a partir do percentual de ocorrência das classes na base de dados original.

Como a principio não há conhecimento a priori sobre a importância de cada atributo sobre a classificação, um peso determinado empiricamente foi atribuído igualitariamente a todos os atributos da base.

Para a análise de similaridade entre as regras foi gerado um coeficiente de 80% a ser aplicado na técnica Rule Relationship, descrita nos Capítulos e 4.

Para as regras fuzzy, foram utilizados os mesmos valores empregados nas regras clássicas, com exceção dos índices de importância que são inerentes às regras.

Após a definição destes parâmetros, o módulo de integração, que prepara os dados a serem usados pela ferramenta Visual DATAMINER, pode ser executado.

A ferramenta Visual DATAMINER foi desenvolvida para trabalhar com regras de associação e regras de classificação clássicas ou fuzzy produzidas por uma ferramenta de mineração de dados.

A ferramenta Visual DATAMINER pode ser integrada com qualquer ferramenta de mineração de dados que gere regras deste tipo.

Esta integração é realizada através dos dados originais trabalhados e pelos resultados gerados pela ferramenta de mineração de dados.

Para que esta integração seja feita da forma mais transparente possível, alguns padrões de arquivos foram determinados para que a comunicação entre as ferramentas fosse eficiente.

Um módulo de integração entre as ferramentas foi desenvolvido para ler os arquivos padrões gerados pela ferramenta de mineração de dados e gerar os dados de entrada para a ferramenta Visual DATAMINER no padrão necessário.

Foram determinados dois padrões de arquivos a serem gerados pela ferramenta de mineração de dados.

O primeiro padrão de arquivo possui o texto completo das regras a serem trabalhadas, em formato texto.

Cada tipo de regra gera um arquivo de texto distinto.

O segundo padrão possui os índices de representatividade da regra, a saber, o índice de confiança e índice de suporte para as regras de classificação clássicas e regras de associação e os índices de certeza e importância de cada atributo para o caso de regras fuzzy.

O módulo de integração lê estes arquivos e gera outros quatro arquivos preparados para as necessidades da ferramenta Visual DATAMINER.

Para utilização na representação gráfica das regras, o primeiro padrão de arquivo possui uma série de parâmetros necessários às técnicas de visualização.

Cada regra gera um registro neste arquivo.

Este arquivo possui definições distintas para cada tipo de regra a ser trabalhada.

Para o caso de regras clássicas e regras de associação, o registro de dado é formado pela identificação da regra, a classificação codificada (à classe 1 é atribuído o valor 0, enquanto à classe é atribuído o valor 1, e assim por diante), o grau de confiança, o grau de suporte, cada parte condicional da regra, o valor assumido pela parte condicional e a parte conclusiva da regra.

Já para as regras fuzzy, o layout do arquivo é definido pela identificação da regra, a classificação codificada da mesma forma acima especificada, o grau de certeza da regra, cada parte condicional da regra, o valor assumido pela parte condicional e a parte conclusiva da regra.

O segundo arquivo padrão é utilizado no acesso aos dados originais da base de dados.

Para as regras clássicas e de associação, o arquivo apresenta todo o corpo da regra separado pelas suas condições e sua conclusão, com os atributos da regra referenciando os nomes dos atributos carregados na base de dados relacional.

Já para as regras fuzzy, o arquivo apresenta todo o corpo da regra dividido pelas suas condições e sua conclusão e o índice de importância de cada parte antecedente da regra.

O terceiro arquivo padrão é utilizado na técnica que trabalha com o relacionamento entre o conjunto de regras.

Para os três tipos de regras, o arquivo possui a seguinte definição, a quantidade de condições da regra, a identificação da regra, o código da classificação da regra, o grau de confiança da regra e a codificação de identificação de cada parte antecedente da regra.

O quarto arquivo apresenta os atributos de dados utilizados da base de dados original.

Além dos arquivos padrões para manipulação das regras, a ferramenta Visual DATAMINER precisa ter acesso aos dados originais, sobre os quais as regras de classificação clássicas e fuzzy foram geradas.

Os dados originais manipulados na geração das regras possibilitam a análise de outros atributos no contexto da regra e no contexto da base em geral.

Estes dados originais precisam estar em uma base de dados relacional para que possam ser manipulados com eficiência.

No caso deste estudo de caso, o banco de dados relacional utilizado foi o Microsoft Access, da Microsoft.

A Neural Mining foi a ferramenta escolhida para gerar as regras a serem interpretadas na realização deste estudo de caso.

A Neural Mining é uma ferramenta de mineração de dados desenvolvida por Bruno Amorim em sua dissertação de mestrado.

Esta ferramenta é responsável por capturar dados já consolidados, provenientes de uma base de dados bruta.

Aplicar algoritmos de mineração baseados na abordagem neural híbrida para geração de respostas ou decisões e, apresentar o conhecimento descoberto na forma de regras.

Para este problema, não foram geradas regras de associação, somente regras de classificação clássicas e fuzzy, devido à natureza do problema de análise de risco de crédito ser um problema de classificação.

Os dados de entrada utilizados na ferramenta Visual DATAMINER são as regras geradas na ferramenta de mineração de dados e os dados originais, sobre os quais as regras a serem avaliadas foram geradas.

A seguir apresentaremos detalhadamente cada uma destas entradas, para o estudo de caso de análise de risco de crédito.

Para esta análise, é utilizado o conjunto de dados original de treinamento utilizado na geração das regras pela ferramenta Neural Mining, contendo 30071 registros.

Os dados encontram-se normalizados do mesmo jeito quando foram submetidos aos algoritmos de mineração.

Os critérios de normalização usados estão descritos em.

Esta base de dados possui os dados fornecidos pelos solicitantes no momento da solicitação de crédito a uma operadora de cartões de crédito no Brasil.

Essas informações são utilizadas pela empresa para decidir pela concessão ou não de crédito ao solicitante.

Os dados de todos os clientes que obtiveram a aprovação do crédito foram armazenados nesta base.

Com o passar do tempo, alguns desses clientes, que foram considerados bons pagadores pelo sistema decisório da operadora, se tornaram maus pagadores.

Portanto, o problema aplicado ao estudo de caso contém informações parciais, pois a base de dados possui apenas informações a respeito dos proponentes aceitos e que vieram a se tornar adimplentes ou inadimplentes na carteira de clientes da empresa.

Processo decisório de análise de crédito.

A base de dados original possui os seguintes atributos listados abaixo.

Atributos da base original de dados.

A distribuição amostral encontrada na base total original foi a mesma encontrada no conjunto de treinamento que está sendo utilizado na análise deste estudo de caso.

Isto se deve ao fato da extração do conjunto de treinamento ter sido realizada através de uma amostragem estratificada da base original, mantendo na amostra a mesma representatividade da base original.

Esta característica nos permite deduzir que a distribuição dos valores assumidos pelos atributos é a mesma da base original, refletindo na base parcial o comportamento do total de clientes da base.

A base original total está denormalizada, isto é, não apresenta nenhum tipo de tratamento de ajuste de seus valores a alguma escala pré-definida, enquanto a base de treinamento encontra-se normalizada.

As regras geradas pela ferramenta Neural Mining foram geradas sobre os valores normalizados.

Para melhor explanação dos resultados obtidos com a ferramenta Visual DATAMINER, é realizada uma denormalização da base.

Entretanto, como a base normalizada foi gerada por uma ferramenta e os critérios de normalização são desconhecidos, alguns atributos não puderam ser denormalizados e os valores normalizados atribuídos serão usados.

Para denormalização dos atributos, estas são as técnicas utilizadas, Atributo Idade, Utilizou-se a forma inversa à utilizada na normalização.

Para a normalização deste atributo foi empregada a fórmula de normalização min-max, que por ter relacionamento com os dados originais, pode ser revertida.

Como exemplo, o valor 009 utilizado para o atributo idade nas regras clássicas foi transformado em 22, após a denormalização.

Para os valores utilizados nas regras, foram encontrados os seguintes valores denormalizados, Valores utilizados nas regras denormalizados pela técnica min-max.

Atributos Sexo, flag_tel_residencia, flag_mesma_cidade_residencia_comercial, estado-civil, O domínio destes atributos foi analisado comparando aos valores da base de dados original e os valores da base normalizada e uma correlação entre os valores de domínio normalizados e originais conseguiu ser estabelecida.

Valores denormalizados utilizados nas regras.

Atributos que não conseguiram ser denormalizados.

Os dados de entrada utilizados no estudo de caso de análise de risco de crédito são regras de classificação clássicas e fuzzy.

Estas regras são expressas utilizando os nomes dos atributos e valores normalizados apresentados na base de dados normalizada.

As regras de classificação clássicas foram obtidas a partir de uma árvore de decisão gerada pela técnica TREPAN.

Cada uma destas regras está associada a dois valores, um índice de suporte e um índice de confiança.

O índice de suporte se refere ao número de registros da base de dados que satisfazem às condições da regra e pertencem à mesma conclusão da regra.

O índice de confiança representa o número de registros cobertos pelas condições das regras que pertencem à mesma conclusão da regra.

Fórmula de Cálculo do Índice de confiança.

As regras são divididas em dois grupos conforme sua conclusão.

Do conjunto total de regras, sete regras são associadas aos clientes adimplentes e seis regras são associadas aos clientes inadimplentes.

As regras fuzzy foram geradas a partir da técnica REFuNN.

O conjunto de entrada de regras fuzzy são cinco regras ao todo e, destas cinco regras, três são associadas à classe adimplente e duas associadas à classe inadimplente.

As regras fuzzy também têm dois valores associados, o índice de certeza e o índice de importância.

O índice de certeza representa o fator de confiança da regra quando inferindo a saída fuzzy e se apresenta ao lado da parte conclusiva da regra.

O índice de importância representa o grau de importância da condição para ativar a regra, isto é, o grau de importância daquela condição na determinação da classificação.

Este índice está localizado após cada parte antecedente da regra.

Para a análise deste estudo de caso, a técnica estatística denominada inferência estatística é utilizada.

Inferência estatística é o uso da teoria da probabilidade para fazer inferências sobre uma população a partir de uma amostra desta população.

População é um conjunto de elementos para os quais se deseja estudar algumas características.

Ao se extrair um subconjunto desta população, a saber, uma amostra, sendo esta representativa, as análises feitas sobre a amostra podem ser inferidas sobre a população.

A definição de amostra é todo subconjunto não vazio de uma população.

O termo amostragem refere-se ao processo pelo qual se obtém uma amostra.

Sua idéia básica refere-se "à coleta de dados relativos a alguns elementos da população e a sua análise, que pode proporcionar informações relevantes sobre toda a população".

A amostragem deve ser realizada com técnicas adequadas para garantir a representatividade da população em estudo.

O procedimento de amostragem pode ser realizado por meio de uma amostra probabilística ou não probabilística.

A amostragem probabilística é aquela em que cada elemento da população tem uma chance conhecida e diferente de zero de ser selecionado para compor a amostra.

Os tipos de amostragem probabilística são a amostra randômica simples, a amostra estratificada randômica e a amostra de agrupamento.

A amostragem não probabilística é aquela em que a seleção dos elementos da população para compor a amostra depende ao menos em parte do julgamento do pesquisador ou do especialista.

Os tipos de amostragem não probabilística são a amostra por conveniência, amostra por julgamento e amostra por quota.

Esta pesquisa se detém ao detalhamento de amostras não probabilísticas, que mais se adequam à esta proposta.

A amostra por conveniência é aquela onde o pesquisador seleciona membros da população mais acessíveis.

Este tipo de análise é adequada e freqüentemente utilizada para geração de idéias em pesquisas exploratórias, onde é facilmente justificada como uma base para geração de hipóteses.

A amostra por julgamento é realizada através da seleção de amostras intencionais ou por julgamento de acordo com a opinião do pesquisador.

Pesquisadores argumentam que a escolha de especialistas para esta seleção é uma forma de amostragem por julgamento ou intencional usada devido à possibilidade de usar o conhecimento do especialista para escolher elementos típicos e representativos para uma amostra.

A amostra por quotas constitui um tipo especial de amostra intencional e é considerada a forma mais usual de amostragem não probabilística.

Nesta amostragem, o pesquisador procura obter uma amostra que seja similar à população sob algum aspecto, utilizando para isto características da população como sexo, idade ou nível social.

Nesta pesquisa é utilizada a técnica não probabilística de amostragem intencional, podendo esta ser classificada como sendo por julgamento ou por quotas, onde o especialista utilizado é a própria ferramenta de mineração de dados.

Os critérios de amostragem utilizados são as condições encontradas nas regras de classificação.

Uma pesquisa com amostragem por julgamento poderá trazer bons resultados quando as características relevantes para controle e delineamento da amostra forem conhecidas, estiverem disponíveis ao pesquisador, estiverem relacionadas ao objeto de estudo e se constituírem em poucas categorias.

Ao ser gerada esta amostra de acordo com os critérios das regras, a aplicação de técnicas de visualização de informações permite a análise do comportamento dos atributos desta amostra em comparação ao comportamento destes mesmos atributos no universo da população.

Desta forma, é esperado que inferências sobre as regras de classificação possam ser realizadas, gerando conhecimento através da aplicação de técnicas de visualização.

Os resultados que são extraídos desta análise buscam gerar um maior entendimento sobre o conhecimento minerado, confirmar hipóteses e agregar mais informações sobre o problema que está sendo analisado.

Para tanto, as regras são analisadas individualmente e em grupo.

Individualmente, cada regra é analisada quanto à sua sintaxe, quanto ao universo da base de dados original que é atendida pela regra e a diferença de comportamento de atributos deste universo em relação ao conjunto total de dados.

A análise das regras em grupo busca obter uma visão da representatividade de cada regra, do problema analisado e da similaridade entre elas.

São apresentados nesta seção os resultados obtidos com a análise das regras de classificação clássicas individualmente.

Esta análise é realizada e apresentada por grupo de regras de mesma classe.

As regras são avaliadas buscando-se complementar o perfil por elas apresentado ou segmentar com mais precisão o perfil de cada classe de cliente.

Os resultados apresentados a seguir são encontrados através da utilização da técnica Coordenadas Paralelas, da apresentação das Tabelas de Dados com os dados originais atingidos pela regra analisada, pela utilização de Filtros de Seleção de regras por classe, pela utilização de métodos de distorção e pela aplicação da técnica Histograma na análise do comportamento de atributos no universo da regra e na base total de registros.

Classe Adimplente Estes são os resultados obtidos na análise das regras adimplentes.

Analisando as regras clássicas individualmente, as seguintes conclusões podem ser extraídas, Técnica de Coordenada Paralelas, ilustrando graficamente a regra 1.

Aplicação de Filtro de Seleção sobre as regras adimplentes.

Esta regra indica que se o cliente não possui telefone residencial e sua idade é superior a 50 anos, este cliente é adimplente em 80% dos casos deste perfil na base de dados.

Analisando os universo dos dados originais desta regra com a técnica histograma, observa-se as seguintes conclusões, Há uma maior concentração deste perfil de clientes na loja 18, onde 21,38% dos clientes atingidos pela regra são oriundos dela.

A proporção de clientes oriundos da loja 18 em toda a base é de 10% Esta regra é mais significativa para o sexo feminino, pois no universo da regra, os clientes do sexo feminino representam 77,92% dos clientes.

Este número supera em 8% o percentual de clientes do sexo feminino da base.

O estado-civil "casado" representa 6% a mais de clientes no universo da regra do que a sua participação no universo da base.

O percentual deste valor de domínio no universo da regra é de 40,75% contra 34,33% do observado na base.

Histograma que mostra a distribuição do atributo sexo na base amostral da regra Histogramas que mostram a distribuição do atributo sexo na população total.

Os clientes que possuem residência tipo = superam em 8,15% o percentual de clientes da base que possuem este tipo de residência.

A característica de morar e trabalhar em cidades distintas é presente no perfil da regra, superando em 12% o perfil da base.

Esta regra afirma que clientes com idade superior a 36 anos de idade e que possuem telefone residencial apresentam a característica de serem adimplentes, em 87% dos casos da base de dados analisada.

Além disso, a presença de clientes com este perfil na base de dados representa aproximadamente 40% dos clientes da base, indicado pelo índice de suporte da regra.

Analisando o universo dos dados originais desta regra com a técnica histograma, observa-se as seguintes conclusões, O sexo feminino também é apresentado como predominante nesta segmentação.

O percentual de clientes do sexo feminino no universo da regra é de 75,99% enquanto o percentual de clientes do sexo feminino da base é de 69,64%.

A porcentagem de clientes casados no universo da regra é de 49,56% em comparação à 33,33% observados na base original.

Os clientes que possuem residência tipo = superam em 17% o percentual de clientes da base que possuem este tipo de residência.

Esta regra trata de clientes com idades entre 36 e 50 anos, sem telefone residencial e que moram e trabalham na mesma cidade.

Estas condições determinam o perfil de um cliente adimplente e é encontrado em 1,32% da base.

Apesar do percentual baixo de ocorrência deste perfil na base analisada, 76,89% dos clientes que possuem as características descritas pela regra, também possuem a classificação atribuída.

Na análise dos atributos da base, algumas outras observações são importantes, Foi observada uma maior concentração deste tipo de perfil de cliente na loja 28.

A ocorrência deste perfil de clientes na loja 28 em todo o universo da base é de 13,37%.

No universo da regra, encontra-se um percentual de 20,76%.

Clientes do sexo masculino demonstram ter uma certa predominância neste perfil.

Há um percentual de 38,73% de ocorrência de clientes do sexo masculino, contra uma proporção de 30,36% existente na base.

A ocorrência de 51,65% dos clientes que residem na cidade 2supera em aproximadamente 10% o percentual de clientes que moram nesta cidade no universo da base.

Esta regra afirma que os clientes com idade igual ou inferior a 36, do sexo feminino, que possuem telefone residencial e que trabalham e moram na mesma cidade são adimplentes em aproximadamente 80% dos casos.

Este perfil é encontrado em aproximadamente 10% dos clientes da base em questão.

Além dos atributos apresentados pela regra, algumas outras características podem ser observadas em relação a este perfil, Há uma grande predominância do estado-civil "solteiro" para este perfil.

No universo da regra, 78,96% dos clientes com este perfil são solteiros enquanto na base somente 50,99% apresentam esta característica.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Distribuição do Atributo Estado Civil na Regra 7.

Distribuição do Atributo Estado Civil na População Total.

Esta regra identifica clientes com idades entre 2e 36 anos, do sexo feminino, que possuem telefone residencial, que trabalham e moram em cidades distintas e são casadas.

Este perfil é encontrado em 1,58% da base e das pessoas que tem estas características, 81,79% são adimplentes.

Para este perfil, outras características também são relevantes, Este perfil é mais freqüente em clientes da loja 2, que representam 13,02% dos clientes atingidos pela regra.

O percentual dos clientes da loja na base total é de 8,51%, gerando uma diferença de 4,51% entre as bases.

A incidência de tipo de residência = neste perfil é de 81,26% contra 74,35% dos clientes da base total.

A diferença observada é de 6,91% a mais para o perfil da regra, podendo esta característica ajudar na especialização do perfil de clientes adimplentes.

O percentual de clientes com renda cônjuge = 1 no universo da regra é de 8%, enquanto na base total este percentual é de 1,73%.

Este percentual talvez seja explicado devido ao fato da regra abordar clientes que sejam casados.

De toda forma, esta é uma característica que pode se usada na seleção dos clientes.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Esta regra define o perfil adimplente para clientes com idades entre 2e 36 anos, do sexo feminino, que possuem telefone residencial, que trabalham em moram em cidades distintas, que não são casadas, que usam a loja 28.

Este perfil é encontrado em 0,44% da base, entretanto, 81,99% das pessoas da base que possuem estas características são adimplentes, o que caracteriza uma abrangência alta.

Alem das características apresentadas pela regra, outras informações podem ser relevantes para este perfil, a saber, O perfil apresentado pela regra é predominantemente de pessoas com estado civil solteiro, sendo esta característica encontrada em 84,73% das pessoas.

O percentual de pessoas solteiras da base é de 50,99%, representando uma diferença de 33,74% em relação à distribuição da base.

Há uma diferença de aproximadamente 4% entre o universo da regra e o universo da base que informaram referência pessoal.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Esta regra trata da classificação de clientes entre 2e 36 anos de idade, do sexo feminino, que possuem telefone residencial, que trabalham e moram em cidades distintas, que não são casadas, que não usam a loja 28, que não moram na cidade 16 e possuem ddd residencial = 3.

A classe adimplente é encontrada em 77,27% dos clientes com estas características, embora somente 0,11% dos clientes da base atendam a este perfil.

Outras características deste perfil foram observadas, Aproximadamente noventa por cento dos clientes atingidos pela regra são solteiros.

A distribuição deste perfil na base é de 50,99, apresentando uma alteração de aproximadamente 40%.

Não há cliente tipo = para este perfil.

As lojas que apresentam este perfil de cliente são as lojas 14, 15, 20, 22, 25, muito embora haja uma concentração dos clientes na loja 25, com um percentual de 72,73%.

A incidência de clientes da base na loja 25 é de 2,63%.

As cidades residências declaradas por este perfil de clientes são as cidades 1, 13, 14, 19, 21, 3e 35.

Destas, as que têm maior representatividade no universo da regra são as cidades 14, 19 e 21, com os percentuais de ocorrência de 3333%, 2121 % e 36,36% respectivamente.

A ocorrência destas cidades na base total é de 043%, 427% e 083%.

A predominância de clientes com tipo residência pode ser observada pelo percentual de 3939% de ocorrência no universo da regra.

Este percentual é reduzido a 8,78 no universo da base.

Há uma diferença de aproximadamente 10% entre o universo da regra e o universo da base que informaram referencia pessoal.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Classe Inadimplente São apresentados a seguir os resultados obtidos com a aplicação da ferramenta Visual DATAMINER nas regras de perfil inadimplente.

Esta regra trata de clientes com idades entre 36 e 50 anos, sem telefone residencial, que trabalham e moram em cidades distintas.

Este perfil é encontrado como inadimplente em 28,59% dos casos da base, que representam 0,64% do total da mesma.

Outras considerações são pertinentes, Há uma maior concentração deste perfil na loja 18, onde 35,26% dos clientes deste perfil são encontrados.

A distribuição dos clientes da base total na loja 18 é de 10%.

Os clientes do sexo masculino são a maioria dos clientes deste perfil, com 4368% de ocorrência na regra.

O percentual de ocorrência de clientes do sexo masculino na base total é de 30,36%.

As cidades residência deste perfil concentram-se entre as cidades 5, 8 e 30, sendo a maior concentração observada na cidade 8, que apresenta 21,05 dos clientes deste perfil.

Na base total, somente 747% dos clientes são da cidade-residencia 8.

O ddd residência é utilizado por 68,95% dos clientes do universo da regra.

O percentual observado deste ddd da residência dos clientes na base total é de 22,59%.

Esta regra afirma que clientes com idade igual ou inferior a 36 anos, do sexo masculino são inadimplentes em 27,09% dos casos observados na base.

O perfil deste cliente representa apenas 4,3% dos clientes da base.

Outras características deste perfil podem ser observadas com a ferramenta Visual DATAMINER, a saber, São solteiros em 79,28% dos casos, superando em 29% a porcentagem de ocorrência desta característica na base.

31,28% não possuem telefone residencial, superando em 13% a porcentagem de ocorrência desta característica na base.

Histogramas que Mostram a Distribuição do Atributo Estado Civil na Base Amostral da Regra 5.
Histogramas que Mostram a Distribuição do Atributo Estado Civil na População Total.

Esta regra afirma que 38,09% dos clientes que tem idade igual ou inferior a 36 anos, do sexo feminino e que não possuem telefone residencial são inadimplentes.

Outras características, Há predominância deste perfil na loja 18, de onde 29,98% dos casos são oriundos contra 10% de ocorrência da base.

A maioria das clientes com este perfil é solteira, representando 76,14% do perfil da regra, contra 50,99% de ocorrência na base.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Esta regra trata de clientes com idade igual ou inferior a 2anos, do sexo feminino, que possuem telefone residencial e trabalham e moram na mesma cidade.

A ocorrência destes clientes na base é 1,61%, embora dos clientes que apresentem estas características, 30,65% são inadimplentes.

Outras características observadas utilizando a ferramenta Visual DATAMINER, A maioria das clientes deste perfil são solteiras, apresentando um percentual de 83,4% de ocorrência no universo da base em comparação ao um percentual de 50,99% observado na base total.

Há uma alteração de 5% no percentual de clientes que não possuem cartões de crédito adicionais para este perfil em relação ao perfil da base.

Esta regra trata de clientes entre 2e 36 anos, do sexo feminino, que possuem telefone residencial, trabalham e moram em cidades distintas, não são casadas, não usam a loja 28 e a cidade-residencia = 16.

Devido à baixa abrangência deste perfil na base, onde somente 003% dos clientes se encontram nestas condições, não se pode realizar nenhuma conclusão que seja representativa sobre os outros atributos da base.

Esta regra trata de clientes com idades entre 2e 36 anos, do sexo feminino, que possuem telefone residencial, que trabalham e moram em cidades distintas, não são casadas, não usam a loja 18, não moram na cidade 16 e seu código de área telefônico residencial não é o 3.

Clientes com estas características são inadimplentes em 26,26% dos casos encontrados na base, que representam 0,78% dos clientes totais.

Outras características, A predominância dos clientes da loja neste perfil é observada através do percentual de ocorrência de 14,96 contra 8,51% da base total.

Há uma maior concentração destes clientes com tipo-residência = 2, superando em 7% o percentual dos clientes tipo-residência = da base.

Dez por cento a mais dos clientes do universo da regra não possuem cartões adicionais do que a base.

Com a aplicação da técnica Net Diagram, os seguintes resultados são encontrados, Das treze regras analisadas, 7 são relacionadas à classe adimplente e 6 são relacionadas à classe inadimplente.

As regras descobertas pela ferramenta de mineração de dados apresentam índices de confiança compatíveis com a média de ocorrência das classes da base de dados, demonstrando qualidade no trabalho de classificação realizado.

Nas regras relacionadas à classe adimplente, somente duas delas estão abaixo da média de ocorrência da classificação adimplente informada nos parâmetros, as regras e 12.

As duas regras relacionadas à classe adimplente que estão abaixo da média de ocorrência da classificação apresentam um índice de confiança de 77%, próximo à média de ocorrência informada, de onde pode ser deduzido que essas regras podem trazer conhecimento ao problema analisado.

Entretanto, estas mesmas regras apresentam um índice de suporte baixo em torno de 01%, não demonstrando representatividade sobre a base.

Aplicação da Técnica Net Diagram Sobre as Regras Adimplentes.
Das regras adimplentes que apresentam índice de confiança igual ou superior à média de ocorrência da classificação, somente duas delas apresentam índice de suporte representativo, que são a regra com 40% e a regra 7 com 10% de ocorrência na base.

A regra trata de clientes com idades acima de 36 e que não possuem telefone residencial.

Com este perfil, 87% das pessoas são adimplentes e representam 40% dos clientes da empresa.

A regra 7 trata dos clientes com idade inferior a 36 anos, do sexo feminino, que possuem telefone residencial e que trabalham e moram na mesma cidade.

Este perfil é adimplente em 80% dos casos e representa 10% da base analisada.

Como os pesos usados para os atributos foram igualitários, o índice de importância não contribuiu para nenhuma descoberta de conhecimento.

Entretanto, caso haja o conhecimento de um especialista que possa determinar a grau de influência dos atributos sobre a classificação, este recurso é considerado importante.

Em relação à classe inadimplente, das seis regras analisadas, somente a regra 11 apresenta o índice de confiança abaixo da média de ocorrência da classe, com o valor de 12,31%.

Esta regra também apresenta um baixo nível de suporte, demonstrando que este perfil não é comum na base de dados.

Das regras inadimplentes que tem índice de confiança acima da média de ocorrência da classe, as regras 6 e 9 são aquelas que apresentam os maiores índices de confiança.

Estas regras apresentam índices de suporte em torno de 2%, não demonstrando representatividade sobre a base de dados.

As regras inadimplentes geradas apresentam um índice de suporte baixo devido ao número de registros da base que pertencem a esta classe ser inferior, conforme explicado em.

Apresentação do Relacionamento de Similaridade Sintática das Regras Adimplentes, com a Técnica Rule Relationship.

A análise realizada com a utilização da técnica Rule Relationship tem o objetivo de verificar a similaridade sintática e semântica das regras.

As regras são analisadas por classe (adimplente ou inadimplente) e como um todo.

Classe Adimplente Na análise sintática das regras adimplentes, os seguintes resultados são encontrados, As regras que atendem ao limiar de similaridade informado são as regras 1, 2, 7, 8 e 10.

As regras e 1estão abaixo do limiar de similaridade.

As regras que apresentam o maior grau de similaridade entre si são as regras 7 e 10, regras 7 e 8, regras 10 e 8, 10 e 1e regras 1e 8, que possuem um grau de similaridade sintática entre 0e 06.

As regras que apresentam o grau de similaridade sintática entre 0e 0são as regras 1 e 4, regras 1 e 2, regras 7 e 1e regras 7 e 2.

As regras 7 e 4, 10 e 2, e 1e e 8 apresentam baixo grau de similaridade, sendo este entre 00 e 02.

As regras similares entre si devem ser avaliadas em conjunto para que o perfil da classe seja melhor definido e entendido.

As regras que apresentam similaridade entre 0e 06 apresentam pontos em comum bastante significativos.

Todas elas tratam de clientes com idade inferiores a 36 anos, do sexo feminino, que possuem telefone residencial.

As diferenças entre elas são as especializações em relação a morar e trabalhar na mesma cidade ou não, ser ou não casada, usar ou não determinada loja e morar em determinada cidade.

As regras que apresentam similaridade entre 0e 0se dividem em dois grupos, separados pelos domínios do atributo idade.

As regras 1, e tratam de clientes com idades iguais ou superiores a 36 anos, enquanto as regras 7 e 1tratam de clientes com idade inferiores a 36 anos.

Entre as regras 1, e 4, dentro do domínio do atributo idade igual ou superior a 36 anos, a distinção dos perfis é feita pela característica de possuir ou não telefone residencial e morar e trabalhar na mesma cidade ou não.

Entre as regras 7 e 12, além da característica da idade, os outros atributos determinantes do perfil adimplente são sexo feminino e possuir telefone residencial.

A distinção dos perfis para esta faixa etária leva em consideração se o cliente trabalha e mora na mesma cidade, seu estado civil, local de sua residência, código de área de sua residência e loja que solicita o seu crédito.

Das regras que possuem baixo relacionamento de similaridade, todos as relações apresentam subconjuntos disjuntos, separados pelo critério do atributo idade, de onde se presume que este seja um atributo determinante na classificação.

Na análise semântica das regras adimplentes, os seguintes resultados são encontrados, Como a fórmula de mensuração de similaridade semântica leva em consideração a classificação da regra, e esta avaliação está sendo realizada com regras de mesma classificação, todas as regras do conjunto obtiveram um grau de similaridade elevado.

Em comparação com os relacionamentos sintáticos, os relacionamentos que existiam sintaticamente apresentam um grau de similaridade maior.

Há o aparecimento de relacionamentos novos, com grau de similaridade entre 0e 06, que ao serem analisados não apresentaram muita consistência.

Estes novos relacionamentos, em alguns dos casos, nem mesmo apresentam relacionamento sintático, como o observado entre as regras 1 e 7.

Apresentação do Relacionamento de Similaridade Semântica das Regras Adimplentes, com a Técnica Rule Relationship.

O relacionamento com índices de similaridade entre 06 e 08 são os das regras 1 e 4, 1 e 2, 7 e 10, 10 e 12, 7 e 12, 7 e 8 e 8 e 12.

Classe inadimplente Na análise sintática das regras inadimplentes, as seguintes observações são apresentadas, Todas as regras inadimplentes estão abaixo do limiar de confiança para análise de similaridade.

A relação entre as regras 11 e 1é a que demonstra maior grau de similaridade, apresentando a faixa de similaridade entre 06 e 08.

As regras 1e 6, 6 e 11, 6 e 5, 6 e 3, 1e 9 e 11 e 9 se relacionam com um grau de similaridade entre 0e 04.

Apresentação das Regras Inadimplentes para Análise de Similaridade Sintática com a Técnica Rule Relationship.

As regras 11 e 5, 1e 5, 11 e 3, e 13, e 9 e 9 e 6 se relacionam com um grau de similaridade baixo, entre 00 e 02.

As regras 11 e 1que se relacionam com um grau alto de similaridade apresentam o domínio do atributo idade entre 2e 36 anos, sexo feminino, que possuem telefone residencial, trabalham e moram na mesma cidade, não são casados e não usam a loja 28.

A distinção entre os perfis destas duas regras diz respeito à cidade em que os clientes moram e ao código de área a que eles pertencem.

As regras com grau de similaridade entre 0e 0apresentam-se divididas em dois grupos.

Há um grupo de similaridade entre as regras 13, 6, 11 e 5 que se relacionam pelo critério de idade e sexo.

Os clientes destas regras têm idade igual ou inferior a 36 anos e, com exceção da regra 5, são do sexo feminino.

As outras regras que apresentam este grau de similaridade regras e se relacionam sem envolver o critério idade, que neste estudo mostra-se como determinante.

Por este motivo, as relações que envolvem estas duas regras, apesar de similares por outros atributos, formam conjuntos disjuntos na relação.

Entre as regras que apresentam baixo índice de similaridade nas suas relações estão as regras 11 e 5, 1e 5, 11 e 3, e 13, e 9 e 9 e 6.

Todas estas relações apresentam conjuntos disjuntos entre si.

Apresentação das Regras Inadimplentes para Análise de Similaridade.

Semântica com a Técnica Rule Relationship.

Na análise semântica das regras inadimplentes, a seguinte observação foi realizada, Como as regras inadimplentes apresentam baixo índice de confiança, não foi percebida alteração no comportamento do gráfico em relação à formula de similaridade sintática.

Análise do Conjunto Ao analisar o relacionamento de similaridade sintática entre todo o conjunto de regras gerado, somente as relações de similaridade entre as classes distintas com grau de similaridade alto são investigadas.

Relacionamento Sintático do Conjunto Total de Regras.

Técnica Rule Relationship.

O maior grau de similaridade no relacionamento entre as regras de todas as classes é de 06 a 08, e baseado neste critério, as relações observadas são entre as regras 1e 12, 1e 11 e 11 e 10.

Nestas relações, o perfil que se apresenta nestas regras diz respeito a clientes com idades entre 2e 36 anos, do sexo feminino, que não trabalham e moram na mesma cidade e são casadas.

A distinção dentro deste perfil, para que se determine se um cliente com estas características seria adimplente ou inadimplente, leva em consideração a loja em que o cliente solicitou o crédito, a cidade de sua residência e o código de área ao qual o cliente pertence.

Relacionamento Semântico do Conjunto Total de Regras, Apresentado pela Técnica Rule Relationship.

O que se observa da análise semântica do grupo total de regras é o reforço da similaridade entre os relacionamentos que já possuem similaridade semântica intra-classes.

Entre as classes, nenhum conhecimento significativo foi adicional ao obtido anteriormente.

As regras fuzzy são analisadas de forma diferenciada em relação às regras clássicas.

Elas são avaliadas em conjunto, através da utilização de um mecanismo de inferência.

Portanto, a análise das regras fuzzy não pode ser realizada a nível individual.

De toda forma, com a utilização da técnica de Coordenadas Paralelas, a formação das regras fuzzy pode ser avaliada.

Outra dificuldade existe na interpretação das regras fuzzy que apresentam características não definidas, sem que se tenha noção do que este conceito representa.

Para que se soubesse os que os termos lingüísticos significam, a função de transformação dos valores originais em valores lingüísticos precisava ser conhecida.

Por este motivo, as regras fuzzy nesta pesquisa somente são analisadas como grupo.

Técnica Net Diagram, na Análise de Regras Fuzzy.

Na análise das regras fuzzy com a técnica Net Diagram, as seguintes observações são pertinentes, Das cinco regras fuzzy geradas pela ferramenta Neural Mining, três são associadas à classe adimplente e duas são associadas à classe inadimplente.

Das três regras associadas à classe adimplente, duas delas não atingem a média de ocorrência da classe, apresentando índices de certeza abaixo deste limiar.

Alguns atributos apresentam índices altos de importância na definição da classe adimplentes.

O conceito de similaridade entre regras fuzzy também não é aplicado, uma vez que as regras não são analisadas individualmente, mas em grupo.

Por este motivo, a análise de similaridade através da utilização da técnica Rule Relationship não será realizada.

Através dos resultados obtidos na análise das regras clássicas e fuzzy do problema de análise de crédito, com a ferramenta Visual DATAMINER, as seguintes conclusões podem ser obtidas, Os atributos determinantes na análise de concessão de crédito desta empresa são idade, posse ou não de telefone residencial, o fato de trabalhar e morar na mesma cidade ou não, sexo, estado civil, filial do pedido de crédito, cidade em que reside e código de área telefônico.

Existe uma divisão clara do perfil de bom ou mau pagador pelo critério idade.

Para a classificação adimplente, o seguinte perfil foi encontrado, Clientes com idade acima de 50 anos, predominantemente do sexo feminino, casados, que possuem tipo-residencia e moram e trabalham em cidades distintas.

Este perfil se mostra mais representativo para os clientes da loja 18.

Esta conclusão não pode ser encarada como definitiva, devido ao baixo índice de suporte da regra.

Clientes com idades entre 36 e 50 anos, do sexo feminino, casados, com posse de telefone residencial e que possuem tipo-residencia 2.

Este perfil encontra um alto índice de suporte, o que atesta a sua credibilidade.

Clientes entre 2e 36 anos de idade, sexo feminino, que possuem telefone residencial e trabalham e moram na mesma cidade.

Um outro critério complementar a esta faixa de idade é o fato da maioria destes clientes ser solteira.

Para o perfil inadimplente, as seguintes conclusões foram obtidas, Predomina a ocorrência desta classificação entre clientes com idades entre 2e 36 anos de idade, que não possuem telefone residencial, eminentemente do sexo masculino e solteiro.

Há algumas regras que tratam do perfil inadimplente para o sexo feminino, mas estas demonstram baixa representatividade na base, não devendo suas conclusões serem adotadas.

As conclusões relatadas mostram a contribuição da ferramenta Visual DATAMINER na análise de problemas de larga escala.

As regras analisadas foram detalhadas e enriquecidas e perfis de classificação foram aperfeiçoados.

O detalhamento das conclusões, contribuições e sugestões de trabalhos futuros para a pesquisa desenvolvida nesta dissertação são apresentadas no próximo capítulo.

Investigar, implementar, experimentar e validar técnicas de visualização de informações na descoberta de conhecimento em bases de dados foi o objetivo maior traçado para esta pesquisa.

Para que este objetivo maior fosse alcançado, várias etapas foram realizadas, norteadas pelos objetivos específicos estabelecidos no início deste trabalho.

Serão expostos abaixo os resultados obtidos.

Realização de um estudo sobre os paradigmas do processo de descoberta de conhecimento em bases de dados.

Nesta etapa do trabalho, foi realizada uma investigação teórica sobre a origem do processo de descoberta de conhecimento em bases de dados, sobre a caracterização, o objetivo e as técnicas utilizadas em cada uma de suas etapas, e sobre as questões mais relevantes relativas à utilização do processo de KDD hoje em dia.

Alguns aspectos de análise de regras foram estudados tais como a importância e similaridade entre as regras, relatando algumas técnicas que tentam mensurar estes fenômenos.

Esta investigação foi realizada com o objetivo de prover um embasamento teórico sobre o processo de KDD e identificar o foco a orientar o desenvolvimento da ferramenta Visual DATAMINER.

Esta investigação está relatada nesta dissertação.

Realização de um estudo sobre a área de Visualização de Informações, seus objetivos, técnicas existentes e métodos de interação.

Uma investigação sobre a área de visualização de informações foi realizada com o objetivo de conhecer o desenvolvimento desta área, os objetivos que a norteiam, suas sub-divisões, as diversas técnicas e métodos de interação existentes e suas aplicações.

Um levantamento sobre a nova área de Visual Data Mining e as formas de integração das áreas de KDD e Visualização de Informações foi realizada.

Esta investigação foi conduzida com o objetivo de, mais uma vez, prover embasamento teórico e direcionar a escolha das técnicas e métodos de interação que mais se adequassem à proposta desta dissertação.

Estas informações estão relatadas.

Após a escolha da área de atuação em KDD e das técnicas de visualização de informações que mais se adequaram à aplicação nesta área, o desenvolvimento da ferramenta Visual DATAMINER foi realizado.

O objetivo deste desenvolvimento foi experimentar a aplicação de técnicas de visualização de informações na etapa de interpretação dos resultados, tentando explicitar, concluir e complementar o conhecimento descoberto através das regras mineradas na etapa de mineração de dados.

Como a ferramenta Visual DATAMINERse concentra na parte de interpretação do conhecimento minerado, há a possibilidade de integração da ferramenta Visual DATAMINER com ferramentas de mineração de dados.

As técnicas de visualização de informações implementadas no Visual DATAMINER foram as Coordenadas Paralelas, os Histogramas, a Net Diagram e a Rule Relationship, estas últimas propostas nesta pesquisa.

Estas técnicas são técnicas geométricas, que facilitam o entendimento de textos, expressão de distribuições, comparações e relacionamentos.

Foram implementados também Filtros de Seleção de Conjuntos para manipular os dados analisados e o uso de tabelas de dados para facilitar a análise, bem como métodos de distorção.

A ferramenta Visual DATAMINERfoi desenvolvida em Java e a interface foi implementada utilizando o pacote Swing.

Devido às facilidades oferecidas pela linguagem Java, foram aplicados fundamentos de programação orientada a objetos, o que possibilitou a utilização dos conceitos de herança, encapsulamento e reusabilidade de código.

Descreve-se a ferramenta Visual DATAMINER, apresentando sua estrutura modular, as técnicas implementadas e sua interface gráfica.

O formato dos arquivos exigidos para integração com as ferramentas de mineração de dados e os arquivos gerados para a utilização da ferramenta Visual DATAMINER são apresentados.

Aplicar um problema de larga escala no domínio de análise de crédito ao consumidor, como ambiente de teste para a ferramenta Visual DATAMINER e identificar suas vantagens e deficiências quando aplicada na solução de um problema real de reconhecida relevância.

O estudo de caso desta dissertação está descrito e foi realizado como uma simulação real de um caso de interpretação de resultados gerados na etapa de mineração de dados.

O problema escolhido foi a análise de risco de crédito, por ser um problema de larga escala e de amplo interesse de pesquisadores, desenvolvedores de soluções, gestores e empresários.

O objetivo desta etapa é aplicar as técnicas de visualizações de informações escolhidas em um problema real como ambiente de teste para a ferramenta desenvolvida, testando suas funcionalidades e aplicabilidade para situações de operação real.

A ferramenta utilizada para a geração dos dados de entrada foi a Neural Mining, que gera regras de classificação clássicas e regras de classificação clássicas fuzzy.

Os resultados obtidos com a análise do problema de risco de crédito mostraram o refinamento e o conhecimento descoberto através da aplicação das técnicas de visualização de informações e os métodos de interação propostos sobre as regras mineradas, atestando a eficácia e eficiência da ferramenta desenvolvida.

A interpretação das regras clássicas não demonstrou ter dificuldades na sua realização.

A única consideração é o tempo gasto na análise individual das regras, onde cada um dos atributos da base de dados é analisado, fato que é inevitável, caso a análise que se queria desenvolver seja minuciosa.

Em relação à análise utilizando as técnicas Net Diagram e Rule Relationship, ambas se mostraram amigáveis e de fácil entendimento de suas expressões.

A análise de regras fuzzy é mais complexa devido à utilização de termos lingüísticos na sua formação.

A falta de conhecimento sobre o domínio que determina os valores cobertos por cada termo lingüístico dificultou o entendimento das mesmas.

Entretanto, a avaliação destas pela técnica Net Diagram foi satisfatória.

O desempenho da ferramenta Visual DATAMINER não foi testado na interpretação de regras de associação devido ao problema utilizado ser inerentemente de classificação.

As principais contribuições desta dissertação são, Investigação do processo de descoberta de conhecimento em bases de dados, seus objetivos, suas etapas e técnicas utilizadas em cada uma das etapas.

Durante a execução desta fase, informações detalhadas sobre os objetivos, as etapas e técnicas utilizadas em cada uma dessas etapas foram relatadas.

Um estudo sobre as formas de análise de regras de indução foi realizado, levando em consideração os critérios de relevância existentes tais como a importância da regra para o problema e a similaridade do conjunto de regras geradas.

O resultado deste estudo está relatado nesta dissertação.

Investigação detalhada da área de Visualização de Informações, com seus objetivos, subdivisões, classificações e técnicas utilizadas de apresentação de informações e métodos de interação e proposição de novos algoritmos como resultado da análise.

Levantamento detalhado sobre a área de Visualização de Informações, com o relato do seu desenvolvimento, seus objetivos, modelos de referência de aplicação, tipos de dados a serem manipulados com as técnicas de visualização, critérios para classificação das técnicas existentes e explanação das principais técnicas de cada classificação.

Os métodos de interação existentes também foram apresentados segundo a classificação praticada nesta área.

Nesta etapa, técnicas de visualização de informações foram adaptadas para aplicação em KDD e novas técnicas foram propostas e apresentadas.

O resultado deste estudo está relatado no Capítulo desta dissertação.

Investigação sobre Visual Data Mining e as formas de interação das áreas de KDD e Visualização de Informações.

A motivação para a criação da especialidade Visual Data Mining, a caracterização desta especialidade e as formas de interação das áreas de KDD e Visualização de Informações estão relatadas no Capítulo desta dissertação.

Proposição de duas técnicas de visualização de informações novas, Net Diagram e Rule Relationship.

Para atender às necessidades de análise de regras de indução, duas novas técnicas foram criadas, o Net Diagram, derivada da técnica Web Diagram, e o Rule Relationship.

O Net Diagram tem o objetivo de permitir a análise de todo o conjunto de regras geradas quanto a diversos índices de relevância tais como o índice de suporte e o índice de confiança ou qualquer índice que se queira utilizar.

Nesta técnica, também há a possibilidade de ter acesso à visualização do corpo da regra que se queira investigar.

A técnica Rule Relationship tem o objetivo de apresentar o relacionamento de similaridade entre as regras geradas.

Esta similaridade pode ser analisada no contexto sintático e semântico, de acordo com fórmulas de mensuração destas duas visões.

Ao ser escolhido um dos relacionamentos apresentado, esta técnica possibilita a visualização dos textos das regras envolvidas naquele relacionamento, para que estes possam ser analisados.

A descrição detalhada destas técnicas encontra-se no Capítulo e sua utilização e resultados obtidos com sua aplicação estão descritos no Capítulo 5.

Proposta de um índice de mensuração da importância das regras geradas, auxiliando o processo de análise de relevância das regras.

Para auxiliar o processo de análise de regras, um índice que mede a importância da regra segundo os atributos que a formam, foi proposto.

Este índice foi criado com o objetivo de utilizar o conhecimento do especialista da área ou informações advindas das técnicas de mineração de dados para atribuir importância a alguns atributos mais relevantes à determinação da classificação.

Desta forma, regras que possuem estes atributos na sua formação seriam mais relevantes ao problema.

Este índice pode ser visualizado através da técnica Net Diagram.

A descrição detalhada da aplicação e formas de cálculo deste índice encontra-se no Capitulo 4.

Desenvolvimento da ferramenta Visual DATAMINER, que possibilita a utilização de várias técnicas de visualização de informações em um mesmo ambiente, a serem aplicadas sobre regras geradas a partir de uma ferramenta de mineração de dados.

Possibilita também a interação e refinamento das regras geradas com os dados originais.

Embora KDD seja uma área de recente desenvolvimento, uma grande quantidade de ferramentas de software de KDD já foi desenvolvida.

A maioria destas ferramentas tem seu principal foco na etapa de mineração de dados, onde inúmeras técnicas de mineração de dados são empregadas.

Embora várias técnicas de Visualização de Informações já tenham sido usadas nestas ferramentas, somente poucas delas possibilitam a exploração e interpretação do conhecimento minerado de forma adequada.

Uma das principais contribuições desta dissertação é a ferramenta Visual DATAMINER, uma ferramenta que se concentra na aplicação de técnicas de visualização de informações na etapa de interpretação do conhecimento minerado.

Esta ferramenta se destina à análise de regras de indução mineradas.

As principais vantagens desta ferramenta são a capacidade de integrar várias técnicas de visualização de informações, métodos de interação e métodos de distorção em um mesmo ambiente, a possibilidade de integração com ferramentas de mineração de dados e a característica de ter sido projetada para problemas de larga escala.

Desenvolvida em linguagem Java, o Visual DATAMINER apresenta uma interface amigável e todos os benefícios de ter sido desenvolvida sob o paradigma da orientação a objetos como compatibilidade, reusabilidade e manutenibilidade.

Demonstração da aplicabilidade da ferramenta Visual DATAMINER e validação da utilização da ferramenta em um problema de larga escala, realizada como uma simulação real da ferramenta.

A validação da ferramenta com um problema real, complexo e de larga escala conferiu credibilidade à aplicação e utilização da ferramenta e permitiu a experiência de usar a ferramenta da forma como esta seria usada em uma situação real.

Esta situação permitiu que algumas melhorias fossem implementadas durante o estudo de caso e outras fossem sugeridas para a continuação deste trabalho.

O problema escolhido, a análise de risco de crédito, apresenta uma base de dados extensa e com alta dimensionalidade.

A base de regras utilizada era satisfatória em número e em qualidade, permitindo que a utilidade da ferramenta Visual DATAMINER fosse analisada.

Considerando os resultados alcançados nesta dissertação, os seguintes trabalhos futuros podem ser sugeridos, Incorporar outras técnicas de visualização de informações à ferramenta Visual DATAMINER.

Muitas outras técnicas de visualização de informações podem ser implementadas para auxiliar a análise de regras de indução.

Entre elas, técnicas orientadas por pixels, por ícones, entre outras.

Abranger outras formas de representação de conhecimento minerado.

A ferramenta Visual DATAMINER concentra seu foco de atuação sobre a análise de regras de indução.

Para maior abrangência de seu foco, outras formas de representação do conhecimento podem ser trabalhadas tais como árvores de decisão.

Investigar técnicas de visualização de informações em três dimensões.

Todas as técnicas implementadas pertencem à classificação de duas dimensões e uma investigação das técnicas D poderia enriquecer este trabalho.

Possibilitar à técnica Net Diagram a visualizar regras por classes, visto que hoje esta técnica trabalha com todo o conjunto de regras gerado.

Esta foi uma das necessidades percebidas durante a análise do estudo de caso.

Embora seja uma necessidade visualizar todo o conjunto de regras e seu relacionamento com as classes, ter a possibilidade de visualizar um conjunto de regras por classe seria interessante.

A restrição do foco de visualização ajuda a concentração na área de análise e diminui a possibilidade de erros.

Implementar o limiar de índice de confiança para análise de similaridade utilizado na técnica Rule Relationship por cada classificação do problema a ser analisado.

Esta necessidade foi sentida ao se analisar a similaridade entre as regras de classificação adimplente e inadimplente, que apresentaram índices de confiança muito distintos, no problema analisado.

O limiar de índice de similaridade informado foi interessante para a classe adimplente.

Já para a classe inadimplente, que apresentou índices de confiança mais baixos, não pôde ser usado, uma vez que todas as regras estavam abaixo deste limiar.

Apresentar junto com a técnica Rule Relationship, os índices de confiança e suporte das regras detalhadas.

Esta sugestão também foi percebida durante a análise do estudo de caso.

Durante a análise de similaridade das regras, a informação dos índices de confiança e suporte das regras em análise é necessária para que se tenha a noção da representatividade das regras que são analisadas.

Possibilitar a extensão da análise de atributos da base original, a fim de permitir a análise de um perfil em relação a mais de um atributo ao mesmo tempo.

A possibilidade de analisar o comportamento de um atributo a partir do comportamento de um outro atributo responderia algumas perguntas que podem surgir, ao se analisar um problema complexo.

Um exemplo a ser dado é, a partir da análise do comportamento do atributo "estado civil", o acesso ao comportamento dos atributos "possui telefone residencial" de pessoas "solteiras" seria interessante.

Adicionar a possibilidade de acessar dados armazenados em um SGBD, Sistema de Gerenciamento de Bases de Dados, ao invés da limitação de acessar os dados das regras em arquivos texto.

A ferramenta Visual DATAMINER realiza a integração com ferramentas de mineração de dados através de arquivos texto.

A possibilidade de esta integração ser realizada através de dados armazenados em um SGBD amplia as facilidades de utilização da ferramenta.

Realizar estudo de caso que contemple regras de associação.

No estudo de caso escolhido, não houve a possibilidade de testar as técnicas implementadas na análise de regras de associação, em um problema de larga escala.

Isto se deve ao fato do problema escolhido para o estudo de caso ser um problema de natureza de classificação.

A realização de um novo estudo de caso com um problema onde regras de associação possam ser analisadas enriqueceria este trabalho.

Incorporar à ferramenta Visual DATAMINER um mecanismo de inferência para análise detalhada de regras fuzzy.

Para que a análise de regras fuzzy possa ser realizada detalhadamente, mecanismos de inferência devem ser implementados na ferramenta.

A ferramenta Visual DATAMINERutiliza três tipos de arquivos distintos, arquivos de entrada, gerados pela ferramenta de mineração de dados, arquivos de parâmetros, criados a partir dos parâmetros informados pelos usuários, e arquivos de apoio às técnicas de visualização de informações.

Cada tipo de arquivo está descrito nas próximas seções.

Há três padrões de arquivos a serem fornecidos pela ferramenta de mineração de dados, que proverá as regra a serem manipuladas pela ferramenta Visual DATAMINER.

Os padrões são os seguintes, O primeiro padrão de arquivo possui o texto completo das regras a serem trabalhadas, em formato texto.

O segundo padrão possui os índices de representatividade da regra, a saber, o índice de confiança e índice de suporte para as regras de classificação clássicas e regras de associação e os índices de certeza e importância de cada atributo para o caso de regras fuzzy.

O terceiro padrão possui os dados originais com todos os seus atributos e registros utilizados na geração das regras.

Este arquivo será importado para um banco de dados relacional.

Há três padrões de arquivos gerados a partir das informações fornecidas na tela de Parâmetros, detalhados a seguir, Informações sobre as classes, Nome da Classe, Identificação da Classe Média de Ocorrência da Classe.

Informações sobre a importância dos atributos sobre o processo classificatório, Atributo Importância sobre o processo classificatório.
Limite de índice de confiança/certeza para a análise de similaridade, Limite de índice de confiança/certeza.

Estes três tipos de arquivos são gerados para as regras de classificação clássicas.

Para as regras de classificação fuzzy, somente são gerados o primeiro e terceiro arquivo, devido à importância das partes antecedentes serem um conceito inerente à regras fuzzy.

Para as regras de associação, somente o terceiro arquivo é gerado, devido a este tipo de regra não possuir o conceito de classificação.

A partir dos arquivos de entrada e dos arquivos de parâmetros, quatro padrões de arquivos são gerados, para apoiar o funcionamento das técnicas de visualização de informações.

Os arquivos são descritos abaixo, O primeiro arquivo padrão apóia a utilização da técnica de Coordenadas Paralelas.

Para o caso de regras clássicas e regras de associação, o registro de dado é formado pela identificação da regra, a classificação codificada (à classe 1 é atribuído o valor 0, enquanto à classe é atribuído o valor 1, e assim por diante), o grau de confiança, o grau de suporte, cada parte condicional da regra, o valor assumido pela parte condicional e a parte conclusiva da regra.

Já para as regras fuzzy, o layout do arquivo é definido pela identificação da regra, a classificação codificada da mesma forma acima especificada, o grau de certeza da regra, cada parte condicional da regra, o valor assumido pela parte condicional e a parte conclusiva da regra.

O segundo arquivo padrão é utilizado no acesso aos dados originais da base de dados.

Para as regras clássicas e de associação, o arquivo apresenta todo o corpo da regra separado pelas suas condições e sua conclusão, com os atributos da regra referenciando os nomes dos atributos carregados na base de dados relacional.

Já para as regras fuzzy, o arquivo apresenta todo o corpo da regra dividido pelas suas condições e sua conclusão e o índice de importância de cada parte antecedente da regra.

O terceiro arquivo padrão é utilizado na técnica que trabalha com o relacionamento entre o conjunto de regras.

Para os três tipos de regras, o arquivo possui a seguinte definição, a quantidade de condições da regra, a identificação da regra, o código da classificação da regra, o grau de confiança da regra e a codificação de identificação de cada parte antecedente da regra.

O quarto arquivo padrão apóia o cálculo de índice de importância de cada regra.

Cada registro de dado possui todos os atributos utilizados em determinada regra.

