Este trabalho apresenta uma análise comparativa de técnicas para a extração de regras de bancos de dados através da Análise Formal de Conceitos (AF).

As regras consideradas aqui são conjuntos de dependências entre atributos de bancos de dados.

Especificamente, as dependências são, implicações, dependências funcionais, regras de associação e regras de classificação.

Essas regras são originárias, principalmente, da teoria dos bancos de dados, na qual desempenham papel fundamental para auxiliar processos de tomada de decisão, caso das implicações, regras de associação e classificação, e na normalização de modelos lógicos, caso das dependências funcionais.

A AFC, por sua vez, possui uma estrutura matemática especialmente adequada para auxiliar na análise de dados.

Essa análise é feita através de reticulados conceituais que representam dados de forma hierárquica.

Sendo assim, o objetivo do trabalho é analisar e comparar métodos que utilizem a AFC para a descoberta de dependências entre atributos em bancos de dados.

São analisados dez algoritmos representativos para extração dos quatro tipos de regras mencionados.

Desses algoritmos, quatro são usados na identificação de dependências funcionais e implicações.

São eles, Next Closure, Find Implications, Impec e Aprem-IR.

Os seis algoritmos restantes são úteis na identificação de regras de associação e de classificação.

Foram analisados quatro algoritmos para extrair regras de associação, AClose, Frequent Next Neighbours, Titanic e Galicia.

Finalmente, foram analisados dois algoritmos para identificar regras de classificação, GRAND e Rulearner.

Os algoritmos foram implementados e submetidos a bancos de dados reais e sintéticos.

Os bancos de dados foram escolhidos e gerados segundo dois critérios, tamanho da base de dados (número de entradas) e densidade.

Esses dois critérios tentam suprir a deficiência constatada na literatura no que diz respeito à escolha de bancos de dados para avaliação de algoritmos.

Constatou-se que os algoritmos apresentam comportamentos característicos para diferentes bancos de dados.

Neste trabalho, é sugerida a adequação de cada algoritmo aos bancos de dados com diferentes densidades e tamanhos.

Neste trabalho, é feita uma análise e comparação de técnicas para a extração de regras de bancos de dados através da Análise Formal de Conceitos (AF).

Entende-se como regras conjuntos de dependências entre atributos de bancos de dados.

Dessa forma, como será visto à frente, pretende-se analisar técnicas baseadas na AFC para a identificação de implicações, dependências funcionais, regras de associação e regras de classificação.

Essas regras são originárias, principalmente, da teoria dos bancos de dados, na qual desempenham papel fundamental para auxiliar processos de tomada de decisão, caso das implicações, regras de associação e classificação, e na normalização de modelos lógicos, caso das dependências funcionais.

A AFC, por sua vez, possui uma estrutura matemática especialmente adequada para auxiliar na análise de dados.

Essa análise é feita através de reticulados conceituais que representam dados de forma hierárquica.

Sendo assim, o objetivo do trabalho é analisar e comparar métodos que utilizem a AFC para a descoberta de dependências entre atributos em bancos de dados.

A AFC é um ramo da matemática aplicada que utiliza o arcaboução da teoria de reticulados na concepção e análise de hierarquias de conceitos.

O marco inicial da AFC data do início da década de 1980.

Mais precisamente, a AFC tem seu início com o trabalho publicado em 1982, que propõe um arcabouço formal para a utilização da teoria de reticulados.

Nos últimos anos, a AFC tem "migrado" da Matemática para a Ciência da Computação.

Inicialmente, os trabalhos relacionados à AFC eram apresentados em conferências da área de Matemática, enquanto atualmente, os trabalhos têm sido, majoritariamente, publicados em conferências na área de Ciência da Computação.

Essa característica mostra uma transformação na visão sobre a AFC, enquanto, no passado, a AFC era estudada apenas do ponto de vista teórico, nos últimos anos, ela tem sido explorada sob o ponto de vista prático, isto é, além do desenvolvimento de teorias relacionadas a AFC, os trabalhos atuais investigam sua utilização nas diversas áreas do conhecimento humano, principalmente na Ciência da Computação.

Entre as aplicações da AFC na Ciência da Computação destacam-se aquelas para a recuperação de informação e para mineração e descoberta de conhecimento em bancos.

Em inglês Formal Concept Analysis.

Uma visão detalhada sobre teoria de reticulados pode ser obtida.

O livro sobre Análise Conceitual de Dados confirma a aproximação da AFC à Ciência da Computação, apresentando a técnica de forma mais algorítmica que teórica em contraste com o livro que apresenta a AFC sob o ponto de vista teórico e matemático.

Foram dedicados capítulos exclusivos para mostrar aplicações da AFC na Ciência da Computação.

Os capítulos apresentam métodos baseados na AFC para auxiliar no processo de recuperação de informação.

Já o capítulo 5, é dedicado a apresentação de métodos para extração de regras de bancos de dados através da AFC.

Apesar do livro sobre análise conceitual de dados mostrar métodos para extração de regras, os algoritmos e métodos foram apresentados de forma breve.

Este trabalho tenta suprir as deficiências constatadas no livro de Carpineto e Romano no que diz respeito à cobertura do número de algoritmos, assim como a análise prática de suas complexidades, além de demonstrações de aplicações, uma vez que as apresentadas na referência têm como objetivo ilustrar o funcionamento dos algoritmos e não o de fornecer exemplos de aplicações.

A sociedade atual encontra-se em um período cuja maior riqueza é o conhecimento.

Informações transformaram-se em ativos nas organizações.

Assim, a capacidade de gerir informações adequadamente tornou-se um diferencial para as empresas.

Remetendo-se à Ciência da Informação, informações são dados estruturados de maneira a produzir sentido.

Esta mesma ciência define o seguinte ciclo, Dados Informação Conhecimento.

Sob esta ótica, dados geram informações que, por sua vez, geram conhecimento o qual completa o ciclo gerando mais dados.

O avanço da tecnologia facilitou o processo de coleta e armazenagem de dados o que resultou no aumento da quantidade armazenada.

Esta quantidade de dados provenientes de diversas fontes torna inexeqüível a análise manual destes para a geração de informações.

Então, tornam-se necessárias técnicas para automatizar ou auxiliar o processo de obtenção de informações a partir de dados.

Neste sentido, diversas técnicas têm sido propostas na área de mineração de dados.

Essas técnicas baseiam-se, em sua maioria, em conceitos estatísticos.

Entretanto, há também aquelas baseadas em computação natural ou bio-inspiradas.

No geral, essas correntes, apesar de distintas, possuem pontos em comum, como o fato de utilizarem a distribuição de probabilidades para analisarem os dados.

Alternativamente, a AFC utiliza semelhanças estruturais dos dados, como a relação de inclusão em conjuntos, para a análise.

Dessa forma, acredita-se que os métodos baseados na AFC podem propiciar melhores resultados na análise dos dados por realçar as relações estruturais.

Este argumento justifica a análise e comparação de métodos baseados na AFC.

Como supracitado, uma das aplicações da AFC dá-se no campo da mineração de dados.

Esta técnica basicamente tenta descobrir relacionamento entre dados.

Descoberto um relacionamento, este pode ser expressado através de regras.

Os reticulados conceituais permitem a extração de regras determinísticas e probabilísticas.

O conjunto de regras determinísticas é formado por regras de implicações (freqüentemente chamadas apenas de implicações) e dependências funcionais.

As regras probabilísticas dividem-se em regras de associação e de classificação.

Para se ter uma idéia do que são as implicações, considere o universo dos animais.

Se o animal é terrestre, então ele possui pulmões, já que todo animal terrestre possui respiração pulmonar.

Este tipo de relacionamento entre atributos, aqui características dos animais, podem ser descritos por implicações.

Assim, a implicação terrestre implica pulmões é uma forma de reescrever que todo animal possuindo a característica (atributo) terrestre, também possui a característica pulmões.

Generalizando, uma implicação X implica Y revela que, no universo considerado, todo elemento possuindo o atributo X, também possui o atributo Y.

Os trabalhos relativos ao uso da AFC para extração de implicações sugerem diversos algoritmos para a extração de diversas bases de implicações.

O algoritmo Next-Closure é utilizado para encontrar uma base de implicações mínima (Stem-Base ou, também, Duquenne-Guigues base) sugerida por Duquenne e Guigues.

A base de implicações mínima fornece um conjunto de implicações completo, no sentido de que qualquer implicação válida para o banco de dados pode ser obtida através da combinação das regras da base de implicações, e não-redundante, ou seja, a remoção de uma regra da base mínima faz com que a base deixe de ser completa.

Apesar de fornecer um número mínimo de regras, o problema do uso da base mínima para fins práticos está na dificuldade da derivação de todas as regras válidas para os dados através da combinação das regras da base de implicações.

Carpineto e Romano discutem este problema e apresentam um algoritmo para encontrar uma base de implicações mais legível mais facilmente compreensível para usuários finais leigos.

O algoritmo proposto encontra bases de implicações com um número reduzido de atributos tanto no antecedente quanto no conseqüente de uma implicação.

Taouil e Bastide apresentam outra proposta para a extração de bases de implicações mais legíveis.

O algoritmo proposto por eles encontra um conjunto de regras com um antecedente de tamanho mínimo e um conseqüente contendo apenas um elemento.

Entende-se como base de implicações o conjunto de implicações extraídas de um banco de dados.

O antecedente representa o lado esquerdo, enquanto o conseqüente representa o lado direito de uma implicação.

Seja A B uma implicação

A representa o antecedente e B o conseqüente.

Dependências funcionais possuem estreita relação com as implicações.

Elas, como as implicações, também descrevem relacionamentos entre atributos que são válidos para todos os elementos do universo.

Sendo assim, qual a diferença entre implicações e dependências funcionais?

As dependências funcionais são implicações envolvendo atributos multivalorados, atributos que podem assumir diversos valores, como, por exemplo, a cor de um carro e o sexo de uma pessoa.

As dependências funcionais são, então, implicações que independem dos valores dos atributos.

O exemplo a seguir ilustra o que são as dependência funcionais.

Sabe-se que, no Brasil, uma pessoa possui um único cadastro de pessoa física (CPF) e, também, que toda pessoa tem um nome.

Uma dependência funcional entre estes atributos dos brasileiros é, CPF determina funcionalmente NOME, já que sabido o número do CPF de uma pessoa, sabe-se o nome dela, independentemente dos valores do CPF e do nome.

Logo, para quaisquer duas pessoas, se elas possuem o mesmo número de CPF, então elas possuem o mesmo nome.

As dependências funcionais estão, fundamentalmente, relacionadas à área de modelagem de bancos de dados.

Diversos trabalhos dedicam-se ao estudo do tópico desde sua origem.

Entre os trabalhos que se destacam no tratamento de dependência funcional sob a ótica da modelagem de bancos de dados, encontram-se os livros de Elmasri e Navathe, de Ullman.

O problema da identificação de dependências funcionais através da AFC pode ser reduzido ao problema da identificação de implicações.

Pode-se transformar um conjunto de atributos multivalorados em um conjunto de atributos univalorados de forma que as implicações válidas para os atributos univalorados sejam as dependências funcionais válidas para os atributos multivalorados.

Esta abordagem foi inicialmente apresentada por Wille.

Alternativamente, pode-se utilizar o arcaboução proposto por Jaume Baixeries.

Este método propõe novos operadores de fecho para a construção do reticulado conceitual.

Construído o reticulado, este é utilizado para a descoberta de implicações equivalentes às dependências funcionais do reticulado conceitual construído com os operadores tradicionais.

As regras probabilísticas descrevem fatos que ocorrem parcialmente, por exemplo, se uma pessoa tem mais que dezesseis anos então ela possui título de eleitor.

Observe que existem pessoas com mais de dezesseis anos que não possuem título de eleitor, assim, a regra é válida parcialmente.

Esse tipo de relacionamento pode ser descrito através de regras de associação.

Elas descrevem comportamentos dos dados que são válidos para grupos específicos (no exemplo, para o grupo das pessoas com mais de dezesseis anos e título de eleitor) e não obrigatoriamente para todos os elementos como as implicações.

Dessa forma, regras de associação podem ser consideradas generalizações de implicações.

Vários trabalhos discutem a utilização da AFC para extração de regras de associação.

Além da demonstração da possibilidade da utilização de conjuntos fechados para encontrar atributos freqüentes em bancos de dados, Pasquier e outros apresentam o algoritmo AClose.

O algoritmo foi comparado com o tradicional e reconhecido algoritmo Apriori e, em geral, apresentou melhores resultados.

De acordo com Pasquier e seus co-autores, o espaço de soluções é reduzido ao utilizar-se reticulados conceituais para a extração de regras de associação o que permitiu o melhor desempenho do AClose em relação ao Apriori.

Além do AClose, pode-se citar, entre os principais algoritmos baseados em AFC, o Pascal.

As regras de classificação são úteis para descrever características de classes de objetos.

Descrevendo de uma forma simplista, uma regra de classificação indica atributos que caracterizam uma classe.

Considerando novamente o universo dos animais, uma regra de classificação seria, glândulas mamárias determina a classe mamífero.

Obviamente, a presença de glândulas mamárias é uma característica comum a todos os animais pertencentes à classe dos mamíferos.

A utilização da AFC para a extração de regras de classificação ainda é pouco explorada, embora tenha sido uma das primeiras aplicações de reticulados conceituais em mineração de dados.

Mesmo pouco explorada, pode-se apontar alguns trabalhos com tal abordagem.

Um dos primeiros trabalhos com a utilização de reticulados para classificação é o trabalho de Ganascia.

Além deste, entre os principais trabalhos estão as referências.

O trabalho de Sahami propõe um algoritmo que utiliza o reticulado conceitual como um mapa no espaço de regras possíveis.

O algoritmo apresentado em, denominado Rulearner, foi comparado com outros métodos de classificação, como árvores de decisão, e apresentou melhores resultados.

A identificação de comportamentos de dados e sua expressão através de regras tem sido um tópico de interesse em várias pesquisas ao redor do mundo.

A AFC forneceu um arcaboução formal para a representação de dados.

Os dados, na AFC, são representados através de diagramas o que facilita a análise de dependências entre atributos.

Sendo assim, o problema considerado neste trabalho é resumido na seguinte frase, Dado um reticulado conceitual, quais são as regras que podem ser extraídas deste reticulado?

Os objetivos deste trabalho são avaliar e comparar os métodos propostos na literatura para a extração de regras de reticulados conceituais.

O processo de avaliação consiste em verificar a aplicabilidade, as possíveis utilizações dos algoritmos e das regras geradas por eles, além de analisar o comportamento assintótico (tempo de execução e espaço necessários) e o desempenho dos principais métodos identificados na literatura.

A comparação é feita somente entre os métodos baseados na AFC, não foram comparados os métodos baseados na AFC com outros métodos propostos na literatura.

A comparação é realizada entre métodos de uma mesma classe (métodos que produzem o mesmo tipo de regr).

O objetivo desta comparação é verificar a adequação dos métodos sob os pontos de vista da eficiência no tempo de execução e da utilidade das regras produzidas.

Os demais capítulos estão estruturados da seguinte forma, No capítulo seguinte, uma breve introdução à AFC é realizada.

São apresentados conceitos básicos como o de contextos formais, conceitos formais e reticulados conceituais.

No Capítulo 3, os algoritmos para a identificação de regras determinísticas são apresentados.

Além dos algoritmos, são apresentadas definições formais de implicações e dependências funcionais.

Como as dependências funcionais estão relacionadas à área de modelagem de banco de dados, no Capítulo 3, é, também, discutida a representação de bancos de dados através de contextos formais.

O Capítulo apresenta as definições e algoritmos relacionados às regras de associação e de classificação.

Finalmente, o Capítulo 5 apresenta a conclusão do trabalho.

Neste capítulo, serão introduzidos conceitos básicos sobre AFC.

A AFC é um método proposto para a análise de dados estruturados como conceitos formais, entidades matemáticas que formalizam, simplificadamente, a concepção abstrata de conceitos como manifestações do pensamento humano.

A AFC baseia-se na teoria de reticulados para estruturar conceitos formais e permitir a análise de dados.

Como reticulados são tipos de ordens parciais que possuem certas propriedades, então, antes de apresentar definições relativas à AFC, serão apresentados conceitos básicos sobre ordens parciais e reticulados.

Primeiro, segundo, terceiro Avô, pai, filho Mais alto, mais largo.

Enfim, a idéia de ordem surge freqüentemente no cotidiano das pessoas.

Mas qual o conceito de ordem?

Ordem, de acordo com a definição do dicionário Aurélio, é, Ordem, então, é uma relação estabelecida entre objetos através da qual pode-se inferir a posição de um objeto dentro de uma seqüência.

Uma relação de ordem, é uma relação binária definida sobre um conjunto P que respeita as três propriedades a seguir, As relações de ordem dividem-se em dois tipos, totais e parciais.

Considere o conjunto dos números reais e a relação de ordem 'maior ou igual'.

Nesta relação, para quaisquer dois números reais x e y, ou x y, ou y x, ou ambas as situações são verdadeiras (quando x e y são iguais).

Esta relação é chamada de ordem total, já que todos os elementos do conjunto (números reais) são comparáveis entre si de acordo com a relação.

Entretanto, essa situação não ocorre universalmente.

Se considerada a relação de descendência estabelecida entre as pessoas, obviamente, existem duas pessoas A e B em que nem A é descendente de B, nem B é descendente de A.

Relações com essa característica são chamadas de ordens parciais, pois, nem todos elementos de um conjunto são comparáveis.

Uma relação de ordem estabelecida sobre um conjunto P recebe a notação hP,i.

O conjunto P associado a uma relação de ordem é, em geral, chamado de conjunto ordenado.

Normalmente, quando se tem uma ordem definida sobre objetos, deseja-se saber quais objetos são os primeiros e quais são os últimos.

Em uma ordem parcial, as idéias de primeiros e últimos são substituídas pelos conceitos de máximos e mínimos.

Um elemento é máximo em uma relação de ordem, se não existem elementos maiores que ele.

Analogamente, um elemento é mínimo, se não existem elementos menores que ele.

Em geral, ordens parciais podem apresentar vários elementos máximos e mínimos ao invés de um único elemento.

No entanto, existem situações em que se pode falar sobre o elemento máximo (mínimo) em uma ordem parcial.

Quando um conjunto ordenado possui um único elemento máximo (mínimo), ele é chamado de maior (menor).

Um elemento é o maior de um conjunto ordenado, se esse elemento for maior ou igual a todos os elementos do conjunto ordenado.

Assim, o maior de um conjunto ordenado é sempre comparável a todos os outros elementos do conjunto.

Similarmente, define-se o menor elemento.

Essas definições são formalmente apresentadas na definição 1.

Definição 1.

Seja hP,i uma relação de ordem definida sobre o conjunto P.

Sejam Max P, Min P, x P e y P.

O conjunto Max é um conjunto de máximos de P se, e somente se, Max = {a P|b O conjunto Min é um conjunto de mínimo de P se, e somente se, Min = {a P|b O elemento x é o maior de P se, e somente se, b P b x.

O elemento y é o menor de P se, e somente se, b P y b.

Todo conjunto ordenado possui duas importantes famílias de conjuntos associados.

Essas famílias são a seção inferior e a seção superior.

Seja P um conjunto ordenado.

Um conjunto X P é uma seção inferior de P, se, para quaisquer elementos a X e b P tais que b a, b X.

Analogamente, X P é uma seção superior de P se, para quaisquer elementos a X e b P tais que a b, b X.

O conjunto das seções inferiores (superiores) de P é de denotado por I(P) (S(P)).

Os conjuntos I(P) e S(P) ordenados pela inclusão de conjuntos também formam ordens parciais.

Sejam Q P e x P.

As seções inferiores (superiores) induzidas por Q e x são, Q, Q, x e x são, respectivamente, as menores seções inferiores e superiores contendo Q e x.

Esses conceitos são úteis em algumas ocasiões e, em especial, o conceito de seções inferiores será utilizado na seção 41por algoritmos de extração de regras de associação.

Definição 2.

Seja P um conjunto ordenado.

Sejam Q P e x P.

O elemento x é um limite inferior de Q, se todo elemento de Q é maior ou igual a x, ou seja, y Q x y.

Da mesma forma, x é um limite superior de Q, se y Q y x.

Os conjuntos dos limites inferiores e superiores de Q é dado por, Como as relações de ordem são transitivas, os conjuntos Q e Q são seções inferiores e superiores.

Se o conjunto Q possui um maior, então este elemento é chamado de maior limite inferior de Q ou, simplesmente, ínfimo.

Como o elemento maior é único, se ele existir, então se Q possui um ínfimo, ele é único.

Se o conjunto Q possui um menor, então ele é chamado de menor limite superior ou supremo de Q.

Igualmente ao ínfimo, se o supremo existir, então ele é único.

Em geral, adota-se a notação Q e Q para falar do ínfimo e supremo de Q se eles existirem.

Se Q = {x,y}, então adota-se a notação xy e xy para ínfimo e supremo.

Definição 3.

Seja P um conjunto ordenado.

Se xy e xy existem para quaisquer x,y P, então P é chamado de reticulado.

Respectivamente, em inglês order ideal e order filter.

A prova dessa afirmação pode ser encontrada.

Em um conjunto ordenado, quando existe apenas o supremo (ínfimo) para quaisquer elementos, esse conjunto ordenado é chamado de supremo(ínfimo)-semi-reticulado.

Além disso, todo reticulado finito é um reticulado completo.

Na seção 23, reticulados completos de estruturas denominadas conceitos formais são utilizados para a análise de dados.

Conceitos formais são pares de conjuntos fechados de objetos e atributos.

Conjuntos fechados são um dos temas da próxima seção.

Outro importante conceito para a compreensão da AFC é o de sistemas de fechos.

A idéia de sistemas de fechos é bastante simples apesar de não ser, em princípio, intuitiva.

Isso deve-se ao fato do nível de abstração.

Sistemas de fechos são famílias de conjuntos com algumas propriedades.

Seja P um conjunto.

Uma família C (P) é um sistema de fecho se, A família C é chamada de sistema de fecho já que ela é fechada sob interseções.

Essas famílias são, também, conhecidas como estruturas de interseção com topo.

Um sistema de fecho ordenado pela inclusão de conjuntos, hC,i, é um reticulado completo em que o supremo e o ínfimo são determinados por, Note que o próprio conjunto potência, (P), é um sistema de fecho.

Portanto, (P) ordenado pela inclusão de conjuntos é um reticulado completo.

Associado ao conceito de sistemas de fechos, encontra-se o conceito de operadores de fecho.

Um operador de fecho é uma função que mapeia um elemento de um conjunto ao seu fecho.

Definição 4.

Seja P um conjunto ordenado e x,y P, uma função c, P P é um operador de fecho se, As propriedades da definição são chamadas, respectivamente, de monotonicidade, extensibilidade e idempotência.

Essas são propriedades necessárias e suficientes para que uma função seja um operador de fecho.

Um elemento x P é chamado de fecho (ou fechado) se x = c(x).

O conjunto dos fechos de P é denotado por P.

Esse conjunto revela a estreita relação entre operadores de fecho, sistemas de fechos e reticulados.

Especificamente, o conjunto dos fechos é um sistema de fecho e, ordenado pela relação de inclusão, é um reticulado completo.

Davey e Priestley mostraram em que de um operador de fecho obtém-se um reticulado e de um reticulado obtém-se um operador de fecho.

Seja c um operador de fecho definido sobre um conjunto X.

O conjunto L = {A X|A = c(} ordenado pela inclusão de conjuntos é um reticulado completo cujo ínfimo e supremo são obtidos por, Similarmente, para um reticulado completo L sobre X, a função c,(X) (X), tal que para é um operador de fecho.

Definição 5.

Sejam P e Q dois conjuntos ordenados.

Sejam B, P Q eC, Q P duas funções entre os conjuntos ordenados.

O par (,) é uma conexão de Galois se, para todo p P e Proposição 1 (Lema 726).

Seja (,) uma conexão de Galois entre conjuntos ordenados 5 Leia-se direita e esquerda respectivamente paraB C A AFC baseia-se em três entidades, contextos formais, conceitos formais e reticulados conceituais.

Um conceito é determinado por sua extensão e intensão.

A extensão refere-se ao conjunto de objetos que são instâncias do conceito, enquanto a intensão, ao conjunto de características comuns a todos os objetos.

Dada a dificuldade (ou, até mesmo, a impossibilidade) de listar-se todos os objetos e atributos de um conceito, é comum considerar a extensão e a intensão em um contexto específico, limitando-se o número de objetos e atributos.

Aproveitando a descrição informal de contextos, pode-se definir contextos formais, os quais, intuitivamente, delimitam o conjunto de objetos, o conjunto de atributos e as relações entre atributos e objetos para conceitos formais.

Sendo assim, um contexto formal é matematicamente definido como uma tripla (G,M,I) na qual G é o conjunto de objetos considerados, M é o conjunto de atributos e I G×M uma relação binária entre objetos e atributos, a relação de incidência, que determina se um objeto g G possui um atributo m M.

Um contexto formal é, geralmente, representado por uma tabela cujas linhas representam objetos, colunas representam atributos, e interseções entre linhas e colunas representam a relação de incidência.

A xemplifica um contexto formal.

Neste exemplo, objetos são animais, os atributos são suas características e a relação de incidência informa se um animal possui, ou não, determinada característica.

Um animal possui uma característica se, e somente se, existe um "×" na interseção entre a linha e a coluna respectiva.

Embora a tabela cruzada represente adequadamente a relação entre objetos e atributos, em situações reais, objetos são caracterizados por atributos que podem assumir diversos valores, por exemplo, uma pessoa pode ser caracterizada por, entre outros caracteres, sua idade e um carro por sua cor.

Nestes casos, a simples presença ou ausência de atributos não caracteriza Exemplo de contexto formal.

Adequadamente os objetos, uma vez que cada um pode possuir determinado atributo com determinado valor.

Estes atributos são chamados de atributos multivalorados.

Um contexto formal cujos objetos possuem atributos multivalorados é chamado de contexto formal multivalorado.

Formalmente, ele é definido por (G,M,V,I), em que G é um conjunto de objetos, M é um conjunto de atributos multivalorados, V é o conjunto de valores dos atributos e I é uma relação ternária, I G×M ×V, que obedece a seguinte restrição, Logo, ao se escrever (g,m,v) I, diz-se que o objeto g possui o atributo m com valor v.

Assim, atributos multivalorados podem ser entendidos como funções parciais de G para V, o que permite reescrever (g,m,v) I em notação funcional, m= v.

Apesar da formalização de contextos formais multivalorados, eles não podem ser usados diretamente na AFC.

Os contextos formais multivalorados devem ser transformados em contextos univalorados através do processo chamado de escala conceitual.

A escala conceitual consiste em definir valores de atributos multivalorados a partir da combinação de valores univalorados.

O exemplo 1 ilustra a definição de escala conceitual e o processo de transformação de contextos multivalorados em contextos univalorados.

A escala adotada no exemplo é a escala plana.

Naturalmente, ela não é a única, porém, é a mais simples delas.

Ganter e Wille apresentam um estudo mais detalhado sobre os tipos de escalas e suas interpretações.

Exemplo 1.

Seja a m contexto multivalorado referente aos oito planetas do sistema solar mais Plutão e suas características.

As características são o tamanho dos planetas, a distância do sol e a ocorrência de lua.

Como contextos multivalorados não podem ser usados diretamente na AFC, deve-se transformar o contexto dos planetas em um contexto univalorado.

A transformação de contextos multivalorados em univalorados é feita através da escala conceitual.

Como dito, a escala conceitual especifica a correspondência entre valores de atributos multivalorados e atributos univalorados.

A presenta as escalas dos atributos.

As escalas são representadas como Contexto Multivalorado de Planetas, contextos formais.

Os objetos, nas escalas, são os valores dos atributos multivalorados e os atributos são os atributos a serem considerados no contexto univalorado.

Então, por exemplo, o atributo tamanho no contexto multivalorado corresponderá aos atributos pequeno, médio e grande no contexto multivalorado.

Ainda considerando o exemplo, quando um objeto possui o valor pequeno para o atributo tamanho no contexto multivalorado, ele possuirá o atributo pequeno no contexto univalorado.

Aplicando-se o mesmo raciocínio para os outros atributos, obtém-se o contexto formal da tabela.

Contexto Formal de Planetas.

Seja (G,M,I) um contexto formal, A G um subconjunto de objetos e B M um subconjunto de atributos.

Pode ser necessário conhecer o conjunto de atributos comuns aos objetos de A, ou, de forma análoga, pode ser necessário conhecer o conjunto de objetos que possuem os atributos de B em comum.

Estas necessidades são atendidas através dos operadores de derivação definidos por, Os operadores de derivação (·) e (·), em geral, recebem a mesma notação (·) como forma de simplificação.

Se A G, então A é um conjunto de atributos comuns aos objetos de A.

A A pode-se aplicar, novamente, o operador de derivação e obter A resultando, novamente, em um conjunto de objetos.

Intuitivamente, para A G, A retorna o conjunto de todos os objetos que possuem, em comum, os atributos dos objetos de A, note que A A.

O operador é definido de forma similar para o conjunto de atributos.

Se B M, então B retorna o conjunto de objetos que possuem os atributos de B em comum.

Assim, B retorna o conjunto de atributos comuns a todos objetos que possuem os atributos de B em comum, conseqüentemente, B B.

Formalmente, o operador refere-se à composição dos operadores de derivação, ou seja, para A G, ou para B M.

Com isso, pode-se constatar as seguintes propriedades para os operadores de derivação e suas respectivas composições, Proposição 2.

Sejam A,A,A G, os operadores de derivação satisfazem as seguintes propriedades. De forma similar, para B,B,B M tem-se, Observe que o par (,) de operadores de derivação constitue uma conexão de Galois entre os conjuntos potência e (M).

No entanto, assume-se para (M) a ordem inversa à de inclusão.

Dessa forma, a composição dos operadores de derivação são operadores de fecho e, assim, dão origem a um reticulado.

Supõe-se que f g(x) = f(g(x)).

A partir de um contexto formal, algoritmos podem ser aplicados a fim de obter-se um conjunto de conceitos formais, os quais são simplificações daquilo que se entende como conceitos.

Com esta finalidade, um conceito formal busca identificar o conjunto de atributos que delimitam e caracterizam um objeto (intensão do conceito), assim como os objetos que compartilham tais atributos (extensão do conceito).

Matematicamente, estas entidades são pares ordenados (A, em que A e B são respectivamente subconjuntos do conjunto de objetos e do conjunto de atributos).

O par (A, é um conceito formal se, e somente se, A = B e B = A).

Assim, por exemplo, o par ({homem, macaco}, {terrestre, pulmões, pêlo, mamífero}) é um conceito formal derivado do contexto da tabela.

O conjunto de conceitos formais de um contexto (G,M,I) é denotado por B(G,M,I).

Conceitos formais podem ser ordenados por uma hierarquia criando-se a relação subconceito-superconceito.

Essa hierarquia determina que para dois conceitos formais, (A,B) e (A,B), (A,B) (A,B), ou seja, (A,B) é um subconceito de (A,B), ou (A,B) é um superconceito de (A,B) se, e somente se, A A (ou B B).

Como já dito, os ope-radores de derivação formam uma conexão de Galois e suas composições formam um operador de fecho.

Assim, o conjunto das intensões e extensões são sistemas de fechos e, ordenados pela ordem direta e inversa da inclusão de conjuntos, formam reticulados completos.

Sendo assim, o conjunto de conceitos formais associados à ordem estabelecida acima é um reticulado completo chamado de reticulado conceitual.

Como os conceitos formais são obtidos dos operadores de derivação que formam uma conexão de Galois, os reticulados conceituais também são conhecidos como reticulados de Galois.

O ínfimo e o supremo do reticulado conceitual são obtidos tal como apresentado pelo teorema 1.

Teorema 1.

Seja (G,M,I) um contexto formal.

Então, hB(G,M,I),i é um reticulado completo tal que, para todo C B(G,M,I), supremos e ínfimos são obtidos por O Teorema 1 é conhecido como o teorema fundamental da AFC e sua prova pode ser obtida em.

O reticulado conceitual pode ser representado através de um diagrama de linha.

Para entender como a hierarquia é representada pelo diagrama, é necessário compreender o conceito de relação de cobertura.

Um conceito (A,B) é coberto por outro conceito (A,B), escreve-se Exemplo de Diagrama de Linha ou seja, (A,B) é o vizinho imediatamente inferior ao conceito (A,B) na hierarquia de conceitos.

O diagrama de linha exprime a hierarquia de conceitos representando-os como círculos cuja extensão encontra-se desenhada abaixo e cuja intensão encontra-se desenhada acima, e a relação de cobertura como linhas interconectando conceitos.

A extensão e a intensão dos conceitos formais são exibidas de forma reduzida.

Neste caso, um objeto g somente aparece no rótulo da extensão do menor conceito (X,Y) tal que g X.

O conceito formal (X,Y) é chamado de conceito-objeto do objeto g.

Ele é obtido através da equação (2descrita a seguir).

De forma semelhante, um atributo m somente é expresso no rótulo da intensão do maior conceito (X,Y) tal que m Y.

O conceito formal (X,Y) é chamado de conceito-atributo do atributo m e ele é obtido pela equação (2descrita a seguir).

Exibe o diagrama de linha dos conceitos obtidos do contexto da tabela.

O diagrama de linha permite a visualização dos relacionamentos entre conceitos.

Observando o diagrama, percebe-se que todo animal possuindo razão, também possui pêlo e é mamífero.

Estas relações podem ser expressas através de regras do tipo se-então para o exemplo anterior, a relação é expressa por, se razão então pêlo e mamífero.

Nota-se, ainda, que se terrestre então pulmões e se pulmões então terrestre, isto é, terrestre se, e somente se, pulmões.

Observe ainda que o único animal terrestre e aquático é o sapo.

Atribuindo-se o nome "anfíbio" ao terreste e aquático com rótulo sapo, pode-se ainda enunciar a regra, se terrestre e aquático então anfíbio.

A extração de regras é o tema dos dois próximos capítulos.

Este capítulo é dedicado à apresentação e comparação de métodos para a extração de regras determinísticas de bancos de dados.

O capítulo introdutório apresentou, brevemente, o conceito de regras determinísticas.

Essas são regras válidas para todos elementos em um banco de dados.

Elas estão divididas em dois grupos, o das dependências funcionais e o das implicações.

Os tópicos relacionados às dependências funcionais são discutidos na seção 31.

Inicialmente, é apresentada a relação de contextos formais multivalorados e bancos de dados relacionais.

Em seguida, são definidas, formalmente, dependências funcionais e, depois, apresentada sua utilidade.

Os algoritmos baseados na AFC para a identificação de dependências funcionais foram propostos para a identificação de implicações.

Entretanto, eles podem ser utilizados com a finalidade de identificação de dependências funcionais, transformando-se o contexto formal multivalorado em um contexto formal univalorado.

Após tal transformação, as implicações identificadas do contexto transformado são, justamente, as dependências funcionais do contexto original.

Esta transformação é apresentada na seção 314.

Alternativamente, pode-se utilizar novos operadores de derivação, denominados operadores de particionamento, que são definidos na seção 315.

As dependências funcionais, assim como as implicações, permitem inferência sobre conjuntos de regras.

Dado um conjunto de regras determinísticas, pode-se utilizar certos axiomas para inferir se uma outra regra é dedutível, ou não, daquele conjunto.

Estes axiomas estão apresentados na seção 32.

Outros tópicos abordados na seção 3são a completeza, a equivalência e redundância em conjuntos de regras.

Estes tópicos são importantes já que permitem que conjuntos de regras com determinadas características, denominados coberturas, possam ser encontrados.

O conceito de cobertura de regras é o assunto da seção 33.

Na seção 34, as implicações são apresentadas formalmente.

Naquela seção, além da definição formal de implicações, é, também, discutida a praticidade dessas regras.

Apresentadas as definições acerca das regras determinísticas, os algoritmos para identificá-las são apresentados.

São apresentados quatro algoritmos, Next Closure, Find Implications, Impec, ApremIR.

Cada um destes algoritmos produz uma cobertura de regras diferente.

Ao apresentá-los, além de descrevê-los e mencionar o tipo de cobertura que eles geram, serão discutidas as complexidades de tempo e espaço de cada um.

Finalmente, na seção 36, o desempenho dos algoritmos com bancos de dados reais e sintéticos é discutido.

Entre os objetivos principais para a utilização de bancos de dados, destacam-se dois, o fato destes evitarem redundância de dados e o de aumentarem a confiança nos dados armazenados através da aplicação de diversos tipos de restrições.

Dependências de dados desempenham um papel importante para se atingir um dos objetivos citados acima, aquele relativo à redundância de dados.

Nesta seção, será explorado um tipo específico de dependência de dados, dependências funcionais.

Estas são extremamente úteis para a normalização de modelos lógicos.

Inicialmente, será discutido o conceito de relações em bancos de dados e mostrada a equivalência destas com contextos formais multivalorados.

Apresentado o conceito de relações, será definido o conceito de dependência funcional, o qual foi, informalmente, definido no capítulo introdutório.

O modelo de bancos de dados relacionais proposto por Codd utilizava relações sobre conjuntos de atributos para descrever conexões entre dados.

Esta representação pode ser vista como uma tabela cujas linhas representam as instâncias e as colunas os atributos.

Cada atributo possui um conjunto de possíveis valores relacionados aos quais se dá o nome de domínio do atributo.

Assim, para cada par (instância,atributo), existe um valor correspondente no domínio do atributo.

Portanto, cada linha da tabela corresponde ao conjunto dos pares (instância,valor do atributo) para cada atributo da relação.

Esse conjunto, em geral, é chamado de tupla.

Embora o banco de dados seja constantemente modificado com a inclusão, remoção e modificação de dados, a estrutura do mesmo é dificilmente alterada.

Em outras palavras, o nome da relação assim como o conjunto de atributos quase nunca é alterado.

Assim, confunde-se, muitas vezes na literatura, o conceito da relação em si como conjunto de tuplas e atributos com o de sua estrutura.

Neste trabalho, no entanto, assume-se a notação adotada por Beeri, ou seja, o termo relação é utilizado para designar o conjunto de tuplas e esquema relacional é utilizado para referir-se à estrutura da relação (conjunto de atributos da relação).

Normalmente, um banco de dados é composto por diversos esquemas relacionais, cada um com sua própria relação.

Entretanto, para verificar os relacionamentos existente entre atributos do banco de dados, é conveniente imaginar o banco de dados com um único esquema relacional "universal" com uma única relação "universal".

Seja A o conjunto de atributos de todos os esquemas relacionais do banco de dados D e Vo domínio do atributo a A.

A relação "universal" R para o banco de dados D é definida por, Pode-se interpretar um banco de dados como um contexto formal multivalorado.

Retomando a definição de contexto formal multivalorado apresentada no capítulo 23, um contexto multivalorado C é uma quadrupla (G,M,V,I).

Seja D um banco de dados cujo esquema relacional "universal" seja A e cuja relação "universal" seja R.

Seja O um conjunto de identificadores para as instâncias de D.

Seja V = {V(|a A} a união dos domínios dos atributos de A).

Definindo-se a relação ternária I O×A×V tal que toda tripla (o,a,v) pertence a I se, e somente se, o valor do atributo a para a tupla o em R é v.

Seja f, O × A V uma função que dada uma tupla da relação e um atributo retorna o valor do atributo para a tupla.

A relação I é formalmente definida como I O×A×V tal que (o,a,v) I f(o, = v).

Desta forma, C = (O,A,V,I) é um contexto formal multivalorado equivalente ao banco de dados D.

Apresentada a equivalência entre os conceitos de relações, esquemas relacionais e contextos formais multivalorados, a seção seguinte apresentará a definição de dependências funcionais.

Embora elas sejam definidas sob o ponto de vista da Teoria de Bancos de Dados Relacionais, esta definição pode ser adequada à AFC.

Uma dependência funcional entre atributos de um esquema relacional A é um par (X,Y), X,Y A, em que os atributos de X determinam funcionalmente os atributos de Y, ou seja, para quaisquer duas tuplas, t e t, de uma instância do esquema, se t possui os mesmos valores que t para os atributos de X, então, elas terão os mesmos valores para os atributos de Y.

Usualmente, assume-se a notação X Y para a dependência funcional (X,Y).

Seja D um banco de dados com relação universal R, esquema universal A cujo domínio dos atributos é V e sejam O um conjunto de identificadores das tuplas de R e f, O×(V) uma função que, dados uma tupla t e um subconjunto de atributos X, retorna o conjunto de valores dos atributos de X para a tupla t.

Uma dependência funcional X Y é dita ser válida para D se, e somente se, para toda instância r de D, para toda tupla de r, t e t, f(t,X) = 1 Repare que f é uma generalização da função apresentada na seção 311.

A definição de dependência funcional pode ser reformulada para o contexto da AFC bastando, para tal, considerar um contexto formal multivalorado como um banco de dados na Teoria de Bancos de Dados Relacionais tal como mostrado na seção 311.

Assim, seja C = (G,M,V,I) um contexto formal multivalorado.

Uma dependência funcional é um par (X,Y) em que X,Y M e ela é dita ser válida para C se, e somente se, g,g G f(g,X) = f(g,X) f(g,Y) = Anteriormente, foi mencionada a utilidade das dependências funcionais sob o ponto de vista prático.

A importância fundamental da descoberta de dependência de dados está na normalização de modelos lógicos.

De maneira geral, a normalização consiste na adequação de modelos lógicos de forma a evitar redundância na armazenagem dos dados.

Outro papel de destaque sobre a utilidade de dependências funcionas é a descoberta de atributos chave, atributos que identifiquem tuplas de uma relação.

O primeiro ponto a ser explorado será a utilização de dependências funcionais para descoberta de atributos chave.

Seja A = {a,a,a } um conjunto com n atributos.

Um conjunto de atributos X é uma chave para A, se, o conjunto de dependências {X a,X a,X a } é válido.

O leitor poderá perceber, ao ler as seções 3e 33, que o conjunto de atribu-tos chave pode ser obtido diretamente de uma cobertura canônica completa, utilizando-se os axiomas de Armstrong (seção 32).

Em relação à normalização de modelos lógicos, as dependências funcionais são utilizadas para a adequação de tais modelos às formas normais de Boyce-Codd(BCNF) e Terceira Forma Normal(NF).

Um esquema encontra-se na BCNF se toda dependência válida é da forma X a, a X e X contém apenas atributos chave.

Informalmente, as únicas dependências funcionais válidas nas instâncias de um esquema na BCNF são aquelas referentes aos atributos chave, ou seja, aquelas em que os atributos chave determinam funcionalmente os demais atributos.

A NF é menos restritiva que BCNF.

Neste caso, uma relação R está na NF, se, para toda dependência X Y válida em R, X K 6= 0/ ou Y K, sendo K o conjunto de atributos chave de R.

Observe que a diferença entre a NF e a BCNF está no fato da primeira permitir dependências Neste ponto, tem maior significado explorar atributos chave sob o ponto de vista da Teoria de Bancos de Dados Relacionais, porém, como já mencionado, tal relação pode ser vista como um contexto multivalorado.

A dependência X a é uma abreviação para X {a } do tipo X Y,Y K.

Este tipo de dependência pode permitir que ocorra redundância no armazenamento.

Entretanto, em certas ocasiões não é possível adequar uma relação à BCNF este caso, a NF torna-se bastante útil.

De maneira geral, essas três situações indicam que a qualidade de um projeto de banco de dados está ligada à análise de coberturas de dependências funcionais válidas.

A escolha do conjunto de regras a ser identificado em uma base de dados depende do processo de análise que será aplicado.

Certamente, para os casos em que a análise será feita por um projetista, a escolha da cobertura canônica (ver seção 33 pode ser mais adequada, pois a análise da mesma, para verificação das situações apontadas anteriormente, pode se dar, até mesmo, de forma direta, sem a necessidade da aplicação dos axiomas de Armstrong).

Entretanto, como o número de dependências na cobertura canônica não é o menor possível, a escolha da cobertura mínima (ver seção 33pode tornar-se mais interessante, caso a análise seja automatizada).

Como a escolha da cobertura mais adequada é dependente do uso destinado à tal, serão apresentados algoritmos para a identificação de diversos tipos de coberturas.

A definição de cobertura, os principais tipos de coberturas e os algoritmos para identificá-las são temas das seções 3e 35.

Um dos caminhos para a identificação de dependências funcionais em bases de dados através da AFC é a transformação de contextos formais multivalorados em contextos univalorados.

Através desta transformação, o problema de identificar dependências é mapeado para o de identificar implicações.

Seja C = (G,M,V,I) um contexto multivalorado.

Existe um contexto C = (G,M,I) tal que o conjunto de implicações válidas em C é o conjunto de dependências funcionais válidas em C.

O conjunto de objetos G é o conjunto de todos os pares não ordenados de objetos em G, assim, |G | = o conjunto de atributos é o mesmo deC e (g,g)I m m(g)= m(g), para todo (g,g) G e m M.

Observe que, pela definição de dependência funcional, uma dependência X Y válida em C implica que g,g G(x X(x(g) = x(g)) y Y(y(g) = y(g))).

Portanto, uma implicação válida em C é equivalente a uma dependência funcional em C, pois se a implicação w z, para w,z M, é válida em C então (g,g) G, o que é equivalente a definição de dependências funcionais em C.

Assim, foi mostrada a equivalência entre o problema da identificação de dependências funcionais em contextos multivalorados e o da identificação de implicações em contextos univalorados.

Entretanto, esta não é a única forma de transformar o problema de descoberta de dependências funcionais no problema da descoberta de implicações.

A próxima seção apresenta uma alternativa para a transformação de contextos.

Os algoritmos baseados em AFC para a extração de regras, em geral, utilizam o operador de fecho definido sobre o conjunto de atributos M obtido através da composição dos operadores de derivação.

Além disso, para a identificação de dependências funcionais, é necessária a transformação do contexto multivalorado em um contexto univalorado.

Esta transformação, apesar de possível, pode ser ineficiente.

Tal ineficiência encontra-se no fato que o espaço necessário para o armazenamento do contexto transformado é proporcional a O(|G|).

Assim, para situações em que o número de tuplas (|G|) é grande, torna-se necessário o armazenamento do contexto transformado em memória secundária para a execução dos algoritmos.

Esta situação acarreta o aumento do tempo necessário para a identificação das regras.

Alternativamente, pode-se definir novos operadores de derivação que possibilitem a identificação de dependências funcionais de contextos multivalorados sem a necessidade de sua transformação.

A esses operadores dá-se o nome de operadores de particionamento.

Os operadores de particionamento foram apresentados por Jaume Baixeries.

Como supramencionado, estes operadores foram propostos como alternativa à transformação de contextos multivalorados em contextos univalorados.

Ao observar a definição de dependência funcional apresentada na seção 312, constata-se que subconjuntos de atributos de M induzem partições dos objetos de G em classes de equivalência.

Para A M, os objetos de cada classe induzida por A possuem os mesmos valores para os atributos de A.

Formalmente, seja P a partição dos objetos de G induzida pelos atributos de A, C P.

Similarmente, dada uma partição P, existe um conjunto máximo de atributos que leva a tal partição.

Assim, os operadores de particionamento são definidos da seguinte forma, O conjunto, A M, refere-se à partição dos objetos induzida pelos atributos de A.

Mais precisamente, = {C G|g,g C,a A}.

Deve-se ressaltar que C,C Definindo-se uma ordem parcial para os conjuntos (M) e, o par de operadores (,) forma uma conexão de Galois.

Para (M), define-se h(M),i.

Para o conjunto das partições, define-se h,i, tal que, para, | | | |.

Então, como demonstrado por Baixeries, (,) e (,) formam uma conexão de Galois.

Portanto, e, definem operadores de fecho sobre M, podendo, assim, ser utilizados nos algoritmos para extração de dependências funcionais.

Note que uma dependência A B é válida para o contexto (G,M,V,I) se, e somente se, Huhtala e outros provaram esta afirmação.

Entretanto, intuitivamente, pela definição de dependências funcionais, A B é válida se, e somente se, g,g G B, e assim, percebe-se que A B é válida se g e g perten-cem à mesma classe nas partições induzidas por A e por AB.

Embora seja possível utilizar o operador de fecho como alternativa para a transformação de contextos na descoberta de dependências, esta possibilidade não é tão direta.

Em geral, os algoritmos para a obtenção de reticulados conceituais são baseados nos operadores de derivação definidos no capítulo 2.

Assim, para a construção do reticulado com base nos operadores e, é necessária a adaptação dos algoritmos já existentes ou a elaboração de novos.

Outro fator que aproxima as dependências funcionais das implicações é o fato de ambas permitirem inferência sobre conjuntos de regras.

Seja C = (G,M,I) um contexto formal, e F um conjunto de regras válidas em C, tal que F = {A B, B C} pode-se indagar se a regra A C também é válida em C.

Como será visto, de acordo com os axiomas de Armstrong, pela regra da transitividade, se F é válido em C, então, A C também é válida.

Neste caso, se diz que F implica logicamente A C, ou F |= A C.

Dado um contexto C, torna-se impraticável, embora possível, a descoberta do conjunto de todas as regras válidas em C.

Todavia, pode-se obter um conjunto menor a partir do qual toda regra válida seja implicada logicamente.

Assim, um conjunto F de regras é dito ser completo, se toda regra válida em um contexto for implicada logicamente de F.

Seja F = {X Y|F |= X Y} o conjunto das regras que sejam implicadas logicamente de F e seja T o conjunto de todas as regras válidas.

F é completo se, e somente se, F = T.

De forma semelhante, pode-se afirmar que dois conjuntos de regras F e G são equivalentes, se, e somente se, F = G.

A partir da definição de equivalência de conjuntos de regras, pode-se definir o conceito de redundância de um conjunto, um conjunto F é dito ser redundante, se existe uma regra X Y F, tal que o conjunto de regras F pode ser inferido através da aplicação de um conjunto de axiomas. Neste contexto, o termo regra está sendo usado tanto no sentido de implicação quanto de dependência funcional.

Axiomas de Armstrong.

Este conjunto de axiomas foi inicialmente proposto por Armstrong.

É importante ressaltar que esse conjunto de axiomas é completo, no sentido que toda regra X Y F + pode ser obtida aplicando-se os axiomas e correto, no sentido que toda regra obtida através da aplicação dos axiomas é válida.

Provas e maiores detalhes desses axiomas podem ser obtidos.

O conceito de equivalência entre conjuntos de regras foi apresentado na seção anterior.

A partir deste conceito, pode-se definir o conceito de cobertura.

Sejam F e G dois conjuntos de regras válidas em um contexto.

Se F é equivalente a G, então F é dita ser uma cobertura de G.

Neste ponto, não mencionou-se a relação entre o número de regras em F e em G, porém, mais à frente, serão apresentadas coberturas para as quais impõe-se restrições sobre o número de regras em F e em G.

Especificamente, serão apresentadas coberturas para as quais o número de regras em F não é maior que o número de regras em G.

O conceito de redundância foi apresentado na seção 32.

Relembrando a definição apresentada em tal seção, um conjunto F é dito ser redundante, se existe X Y tal que (F{X 5 Em inglês, sound).

Y} = F.

Assim, uma cobertura F é dita ser não-redundante, se F é uma cobertura e não existe X Y tal que (F{X Y}) = F.

A definição de não-redundância sugere que não existem regras "descartáveis" em uma cobertura F.

Entretanto, pode-se simplificar F, removendo-se atributos de suas regras.

Este tipo de cobertura será apresentado na próxima seção.

Antes de apresentar o conceito de coberturas reduzidas, faz-se necessária a definição de redução.

Uma dependência X Y F é dita ser reduzida à esquerda, se, para X = aZ, tal 6 que a M e Z X, (F{X Y}){Z Y} não é equivalente a F.

De maneira análoga, uma regra X Y F é reduzida à direita, se, para Y = aW, tal que a M e W Y, (F{X Y}){X W} não é equivalente a F.

Apresentado os conceitos de redução à esquerda e à direita, pode-se definir coberturas reduzidas.

Uma cobertura F é uma cobertura reduzida, se F é uma cobertura e toda regra X Y F é reduzida à esquerda, à direita e Y 6= 0.

Caso toda regra X Y F seja reduzida/ apenas à esquerda, então F é dita ser uma cobertura reduzida à esquerda, o mesmo ocorre caso toda regra X Y F seja reduzida apenas à direita, e Y 6= 0, assim F é dita ser uma/ cobertura reduzida à direita.

O conceito de coberturas canônicas surge imediatamente dos conceitos de coberturas não-redundantes e coberturas reduzidas.

Uma cobertura F é dita ser canônica, se toda regra de F é da forma X a, a M, e F é não-redundante.

Observe que a regra X a é, também, reduzida à direita.

Portanto, se uma cobertura é canônica, ela é reduzida e não-redundante.

O leitor mais atento pode observar que, para toda cobertura F reduzida e não-redundante, existe uma cobertura canônica G tal que F = G.

Como F é reduzida e não-redundante, ela pode ser transformada em uma cobertura canônica substituindo toda regra do tipo X Y, Y = a a a pelo conjunto de regras {X a, X a,X a } utilizando-se o axioma da decomposição.

Observe que F é reduzida, portanto, a substituição de X Y por {X a, X a,X a } não torna a cobertura redundante.

Inversamente, a partir de uma cobertura canônica G, pode-se obter uma cobertura F não-redundante e reduzida, utilizando-se o axioma da união.

Pode-se relaxar as restrições impostas às coberturas canônicas de forma a obter o conceito de coberturas próprias.

Este assunto será tratado na próxima seção.

Taouil e Bastide propuseram esta cobertura e definiram-na como, uma cobertura é própria, se toda regra é reduzida à esquerda e o lado direito possui apenas um atributo.

Mais precisamente, uma cobertura F é própria, se toda regra pertencente a F é da seguinte forma, X a é reduzida à esquerda e a M.

Observe que esse tipo de cobertura possui uma estreita relação com as coberturas canônicas e, conseqüentemente, com coberturas reduzidas.

De fato, uma cobertura canônica é uma cobertura própria não redundante.

A relação com coberturas reduzidas é, ainda, de forma mais direta.

Neste caso, uma cobertura própria G pode ser obtida diretamente de uma cobertura reduzida F, utilizando-se o axioma da decomposição, tal como demonstrado na seção 333.

Note que não se impôs a condição que F seja não-redundante, assim, o resultado da decomposição das dependências de F é uma cobertura própria, e não necessariamente uma cobertura canônica.

As coberturas não-redundantes, apesar de não apresentarem regras "descartáveis", não apresentam necessariamente o menor número de regras possíveis.

Uma cobertura F é mínima, se não existe cobertura com número de regras menor.

Obviamente, F é não-redundante, pois, caso contrário, existiria cobertura menor que F.

Implicações são dependências entre elementos de um conjunto.

Seja (G,M,I) um contexto formal.

Uma implicação entre os atributos de M é um par (X,Y), X,Y M, que, tal como as dependências funcionais, recebe a notação X Y.

Uma implicação X Y é dita ser válida para o contexto (G,M,I) se, e somente se, todo objeto que possui os atributos de X também possui os atributos de Y.

Formalmente, X Yé válida g G, ou seja, X Y.

Caso nenhum objeto 0 0 possua os atributos de X, a implicação X Y é dita ser válida por vacuidade.

Graficamente, a validade de uma implicação pode ser verificada através do diagrama de linha da seguinte forma, se o maior conceito contendo os atributos de X for menor ou igual ao maior conceito contendo os atributos de Y, então X Y é uma implicação válida.

Retomando o exemplo apresentado no capítulo 1, a implicação razão pêlo e mamífero é válida para o reticulado, pois o maior conceito contendo razão, ({homem},{pêlo,mamífero,razão}), é menor do que o maior conceito contendo pêlo e mamífero, ({macaco,homem},{pêlo,mamífero}).

Apesar de menos utilizadas para análise de dados, as implicações também são de interesse prático.

Em algumas ocasiões, existe o interesse em se encontrar regras de associações entre dados cuja confiança seja alta e cujo suporte não tenha restrição.

Por exemplo, considerando dados de mercado, pode-se encontrar a seguinte regra de associação cerveja batata frita indicando relacionamento entre itens bastante vendidos, porém, regras do tipo uísque 18 anos salmão defumado não serão abrangidas já que poucos clientes compram uísque e salmão.

Em outras palavras, as regras de associação exprimem padrões freqüentes de dados, enquanto as implicações revelam dados correlacionados.

Cohen discutem o problema da identificação de regras de associação com alta confiança, mas, sem restrições ao suporte.

Eles indicam que a motivação para a identificação deste tipo de regra encontra-se no fato de que elas revelam mais novidades acerca dos dados que as regras de associação tradicionais.

De acordo com Cohen, este tipo de regra possui aplicações em diversas áreas como na detecção de cópias, para verificar documentos ou páginas web similares ou idênticas e clustering.

Estes argumentos demonstram a importância dos algoritmos apresentados na seção 35.

Os algoritmos apresentados aqui foram, inicialmente, propostos para a identificação de implicações.

Entretanto, como o problema da extração de dependências funcionais é equivalente ao da identificação de implicações, os algoritmos podem ser utilizados com o intuito de extrair dependências funcionais através da transformação de contextos formais multivalorados em contextos formais univalorados.

Além destes algoritmos, foram apresentados, na seção 315, novos operadores de fecho que podem ser utilizados como alternativa à transformação de contextos.

As seções seguintes apresentam os algoritmos que são comparados neste trabalho.

O algoritmo Next Closure foi proposto por B.

Ganter com o objetivo inicial de gerar conjuntos fechados, ele pode ser usado para a identificação de implicações em contextos formais.

A descrição do algoritmo é bastante simples e seu funcionamento é baseado na utilização de operadores de fecho para encontrar conjuntos fechados na ordem lexicográfica.

Seja (G,M,I) um contexto formal.

O algoritmo pressupõe a definição de uma ordem total entre os elementos de M.

Assim, para um conjunto M com n elementos, pode-se, por exemplo, definir a seguinte ordem, M = {m < m < < m }.

O algoritmo é dependente da ordem lexicográfica "" entre subconjuntos de M.

Assim, para A,B M, A 6= B, A < B m B(m A m < m (m A m).

Informal-/ mente, A é lexicograficamente menor que B se o menor elemento, para o qual os dois conjuntos diferem-se, pertence a B.

O algoritmo pressupõe, também, a existência de um procedimento para encontrar o próximo conjunto fechado dados um conjunto A M, um atributo m M e um operador de fecho.

O operador de fecho a ser considerado será a composição dos operadores de derivação ((·)), que receberá a notação (·).

Assim, o próximo conjunto fechado Am, para A e o atributo m segundo o operador (·), é encontrado da seguinte forma, Note que para outros operadores de fecho, o algoritmo mantem-se inalterado, exceto para a linha 3, na qual deve-se utilizar o operador escolhido.

Exemplo 2.

Considere o conjunto de atributos M = {1,2,3,4,5,6,7,8} referentes aos atributos do contexto apresentado na tabela, em que 1 refere-se a aquático, a terrestre, e assim por diante e a ordem linear < usual imposta a M.

Seja A = {2,7} um conjunto de atributos, m = 5 um atributo, o próximo conjunto fechado A5 será encontrado da seguinte forma, Assim, pode-se enunciar o algoritmo Next Closure tal como apresentado pelo Algoritmo 1.

O conjunto das implicações válidas em um contexto (G,M,I) é formado por implicações do tipo A B em que B A.

Isto denota a importância do algoritmo Next Closure para Algoritmo Next Closure Entrada, Um conjunto A M e um operador de fecho, Algoritmo Next Closure.

A identificação de implicações.

Obviamente, se A = A então A A é sempre válida.

O leitor mais atento pode observar que computando todas implicações A B A, na verdade, computa-se a cobertura completa de implicações válidas.

Note que para uma cobertura F, computar F + é caro de fato, a computação é proporcional ao total de subconjuntos de cada conjunto B tal que A B F, ou seja, é proporcional a |B|para cada implicação A B.

Felizmente, pode-se definir o operador de fecho,(M) (M) de um conjunto de atributos A com relação a uma cobertura F, de forma que A B F B.

O operador pode ser computado da seguinte forma, seja L a cobertura de implicações mínima, X M, (X) = X S{P |P P L,P X}.

Computa-se (X) = (((X)))), até que (X) = ((X)).

Informalmente, aplica-se recursivamente o operador de fecho em X até o momento em que (X) = ((X)).

Como o objetivo do trabalho é o de mostrar a utilidade do operador para obtenção de regras, os detalhes foram omitidos.

Entretanto, a descrição detalhada do operador e a prova da correção do algoritmo para computá-lo pode ser encontrada.

A definição do operador permite a descrição do algoritmo para encontrar a cobertura de implicações mínima.

Como as implicações A B = A são sempre válidas, o interesse está em regras para as quais B 6= A.

Segue, então, a descrição do algoritmo apresentado em Algoritmo 2, inicia-se o processo considerando A = 0/ e o conjunto de implicações F = 0.

A cada passo atualiza-se o conjunto A/ com o próximo fecho em relação a ele utilizando o Next Closure com o operador de fecho se o próximo conjunto fechado (em relação a) não é fechado em relação ao operador de fecho obtido da composição dos operadores de derivação A 6= A), então F = F {A A } opcionalmente, pode-se optar por F = F {A } uma vez que A A e A A é sempre válida.

Repete-se o processo até que A = M.

Mais uma vez a prova de correção do algoritmo foi omitida, porém ela pode ser obtida em.

Algoritmo Cobertura Mínima.

Apresentado o algoritmo, pode-se analisar o comportamento assintótico de tempo e espaço em relação ao contexto (G,M,I) recebido como parâmetro.

A análise do comportamento assintótico do tempo de execução envolve a análise do custo de cada operador de fecho e do número de conjunto fechados de M.

O custo do operador de fecho relativo ao operador de derivação é O(|G||M|).

O custo do operador é definido em função do número de implicações no conjunto mínimo de implicações, assim, pode-se enunciá-lo como O(|F|).

São gerados C conjuntos fechados.

A cada conjunto fechado encontrado, são aplicados os dois operadores de fecho ((·) 00 e).

Portanto, o custo geral do algoritmo é O(C(|G||M|+|F|)).

Entretanto, deve-se constatar que a análise foi feita para identificação de implicações.

Então, para a análise de dependências, deve-se considerar o contexto indicado como parâmetro um contexto transformado.

Assim, para o contexto transformado tem-se que o comportamento assintótico em relação ao tempo é |G| O(C |M|+|F|)), F é a cobertura mínima das dependências funcionais válidas.

É importante ressaltar que o pior caso do algoritmo ocorre com contextos do tipo (G,G,6=), os quais representam situações extremas que dificilmente acontecem em bases de dados reais.

Em relação ao comportamento do espaço utilizado, este é constante, pois a cada passo é necessário armazenar apenas o conjunto fechado corrente.

O algoritmo Find Implications é apresentado nesta seção.

Este algoritmo tem como objetivo encontrar uma cobertura de implicações de um contexto formal (G,M,I) através de seu reticulado conceitual.

O algoritmo foi proposto por C Carpineto e seus co-autores.

Carpineto e Romano apresentam, em seu livro sobre Análise Conceitual de Dados, o mesmo algoritmo de forma mais clara.

A cobertura encontrada por este algoritmo, apesar de reduzida, pode ser redundante.

Entretanto, pode-se utilizar algoritmos, tal como o apresentado por Maier, para remover a redundância de forma a obter uma cobertura equivalente à cobertura canônica apresentada na seção 333.

O reticulado conceitual é útil para a identificação de implicações porque uma implicação A B é válida para um contexto (G,M,I), se o maior conceito de cuja intensão A é subconjunto, é menor que o maior conceito de cuja intensão B é subconjunto.

Formalmente, A B é válida se (A,A) (B,B).

Exemplo 3.

Retomando o exemplo dado no capítulo 1 para o reticulado, a implicação razão pêlo,mamífero é válida uma vez que ({macaco, homem}, {terrestre, pulmonar, pêlo, mamífero}) ({homem}, {terrestre, pulmonar, pêlo, mamífero, razão}).

Considerando-se cada conceito (X,Y), o interesse nas implicações obteníveis de (X,Y), são nas implicações A B, A Y, B = YA, tal que A B não possa ser obtida a partir de um conceito (W,Z) superconceito de (X,Y), ou seja, a partir de um conceito (W,Z) (X,Y).

O conjunto de todas as implicações referentes a um conceito é redundante, mesmo respeitando as restrições impostas acima.

Assim, acrescenta-se mais uma restrição fazendo com que o interesse seja em implicações reduzidas.

Para obedecer tal restrição, considere o conjunto parcialmente ordenado h(Y),i.

Sejam dois conjuntos de atributos A e B (A,B (Y)), A é denominado mais geral que B (mais específico que, se A B).

Então, para encontrar implicações reduzidas à esquerda, pode-se iniciar a busca utilizando-se conjuntos de atributos mais gerais, especializando-os até que as restrições mencionadas no parágrafo anterior sejam satisfeitas.

Este é o processo de funcionamento do algoritmo Find Implications.

O algoritmo repete tal processo para cada um dos conceitos do reticulado, adicionando as regras obtidas a cada iteração à cobertura que será retornada.

O Algoritmo apresenta o Find Implications.

O Algoritmo refere-se ao comportamento do algoritmo descrito acima, e tem como objetivo encontrar as implicações de cada conceito, enquanto o Algoritmo 5 apresenta a parte referente à conferência de consistência da implicação em relação aos superconceitos de (X,Y).

Em relação à análise de complexidade do método, verifica-se que o ciclo do algoritmo principal (Algoritmo é executado para todos os conceitos do contexto).

Logo, é executado O(|L|), sendo L o reticulado conceitual.

Repare que a cada conceito, a função Concept Implications (Algoritmo é acionada).

A função atualiza o conjunto de antecedentes baseado em cada superconceito do conceito recebido como parâmetro.

A atualização dos antecedentes refere-se ao ciclo das linhas 18 do Algoritmo 4.

Observe que este é executado |lhsSet| vezes, e que os ciclos das linhas 11 e 116 são executados em tempo proporcional a O(|lhsSet||M|).

Como Algoritmo para encontrar implicações de um conceito.

Verifica a consistência da implicação em relação ao pai do conceito.

O ciclo das linhas 18 é executado q vezes, em que q é a maior quantidade de pais de um conceito, o tempo de execução da função é proporcional a O(|L||max {lhsSet }| |M|q), em que max {lhsSet } indica o maior conjunto de antecedentes.

Em relação à complexidade de espaço, é necessário O(|(M)|) para armazenar o conjunto dos antecedentes.

A explicação é encontrada no fato que os antecedentes são subconjuntos das intensões dos atributos, e a maior intensão refere-se à intensão do conceito ínfimo do reticulado que é M.

Vale lembrar que esta é uma situação avaliada para casos extremos, na prática, isto pode não ocorrer, uma vez que as implicações referentes ao ínfimo são válidas por vacuidade na maioria dos casos.

Portanto, pode-se desconsiderar o conceito ínfimo durante a extração de regras.

Entretanto, deve-se considerar o espaço necessário para a armazenagem do reticulado.

Então, o espaço necessário pelo algoritmo é O(|L||(M)|).

O terceiro algoritmo a ser abordado neste trabalho é o Impec.

Este algoritmo foi apresentado por R Taouil e Y Bastide e tem como objetivo encontrar a base de implicações própria (seção 334).

O objetivo é encontrar implicações cujo lado esquerdo é mínimo e o lado direito possui apenas um atributo.

O algoritmo baseia-se em conjuntos e operadores de fecho definidos sobre estes conjuntos.

Assim, para a descoberta das implicações (ou dependências funcionais) são necessários o conjunto de atributos M e um operador de fecho definido sobre M.

Enunciando o problema em outras palavras, o objetivo do algoritmo é encontrar implicações do tipo A b, em que A M,b M e não existe implicação válida da forma D b em que D A.

O algoritmo encontra o lado direito de uma implicação através da proposição 4.

Proposição 4.

Seja A M e (·) um operador de fecho definido sobre M.

O conjunto dos atributos ld= {b M|A b} é obtido da seguinte forma, Demonstração.

Esta proposição foi retirada.

Assim, detalhes e a prova da proposição podem ser obtidas na referência citada.

As implicações a serem descobertas pelo algoritmo devem ser reduzidas à esquerda.

Seja F a cobertura descoberta pelo algoritmo, se A ldpertence a F, então, B ld F / para B A.

Logo, para garantir que as implicações sejam reduzidas à esquerda, utiliza-se a proposição 5 apresentada em.

Proposição 5 (Proposição 1,).

Seja A,B M, B A e (·) um operador de fecho definido sobre M, tem-se, Apresentadas as proposições que servem de base para o algoritmo, pode-se descrevê-lo.

O Algoritmo 6 mostra, em pseudo-código, o Impec.

O algoritmo verifica as implicações envolvendo cada atributo de M utilizando a própria cobertura computada.

O algoritmo inicia a cobertura com a implicação 0/ 0/.

Em seguida, entra em um ciclo para verificar as implicações de cada atributo de M (linhas 15).

A obtenção de novas implicações é feita especializando-se as implicações já pertencentes à cobertura, linhas 14.

Para cada especialização a ser considerada, aplica-se as proposições 5 (linhas 7 e 8), e (linhas respectivamente).

Por fim, adiciona-se a nova implicação encontrada (linha 1).

Algoritmo Impec Entrada, Um conjunto de atributos M, e um operador de fecho (·) definido sobre M Encontra cobertura própria de um conjunto M.

Analisa-se o custo computacional envolvido com o algoritmo.

A iniciar a análise pela complexidade de tempo, considera-se como operador de fecho parâmetro do algoritmo a composição dos operadores de derivação.

Dessa forma, o custo relacionado à linha 5 é proporcional a |G||M| por se tratar da extração de regras através da AFC, considera-se que M seja o conjunto de atributos de um contexto (G,M,I).

O custo relacionado a aplicação das proposições e 5 é, no pior caso, O(|F ||M|).

O ciclo das linhas 1é executado O(|F|) vezes.

Logo, o tempo necessário para a computação das implicações de cada atributo é O(|F|(|G||M|+|F||M|)).

Assim, tem-se que o tempo para todos atributos é O(|M||F|(|G||M|+|F||M|)).

O algoritmo utiliza a cobertura sendo computada para encontrar a cobertura a ser dada como resposta.

Logo, deve-se levar em conta o espaço necessário para o armazenamento das regras já computadas.

Deve-se, também, levar em conta os conjuntos necessários para a computação das novas regras.

Tais conjuntos são Z na linha 5, R na linha 7 e F na linha 3.

O conjunto Z é limitado pelo tamanho de M, ou seja, é O(|M|).

Os conjuntos F e R são limitados pelo tamanho de F F, por sua vez, é limitado por |M|.

Portanto, pode-se definir um limite fraco para o espaço necessário como O|M|.

O último algoritmo a ser apresentado é o Aprem-IR.

Este algoritmo foi apresentado por Taouil e outros, também, por R Taouil e seus co-autores.

O algoritmo recebe, como parâmetro de entrada, uma lista de conceitos e retorna uma cobertura própria de implicações.

Seja (G,M,I) um contexto formal e B(G,M,I) a lista de conceitos associados ao contexto.

O conjunto dos antecedentes, para um atributo m M, é o conjunto De acordo com Taouil e seus co-autores, este conjunto pode ser encontrado através de conjuntos ínfimo-irredutíveis.

Esta afirmação é baseada em um teorema retirado do livro de Mannila e Raiha o qual demonstra que o conjunto dos antecedentes de um atributo m é o conjunto de minimal transversals do hipergrafo formado com o complemento dos conjuntos máximos ínfimo-irredutíveis não contendo m (CIRR(m¯)).

Como o algoritmo Aprem-IR baseia-se nestes conceitos, é importante descrevê-los com um pouco mais de detalhes.

Um conjunto C é ínfimo-irredutível, se ele não pode ser descrito como o ínfimo de elementos estritamente maiores que ele.

Estes conjuntos podem ser, facilmente, detectados em um diagrama de linha observando-se os elementos com apenas um vizinho inferior.

Para computar o conjunto CIRR(m¯), é necessária a computação do conjunto IRR(m¯) dos conjuntos máximos ínfimo-irredutíveis não contendo m.

Como demonstrado por Mannila e Raiha, o conjunto IRR(m¯) é idêntico ao conjunto das máximas intensões não contendo m.

Assim, a família dos conjuntos máximos ínfimo-irredutíveis não contendo m é Obtido o conjunto IRR(m¯), pode-se obter o conjunto CIRR(m¯) já que O conjunto CIRR(m¯) forma um hipergrafo cujos vértices são os atributos de M e as arestas os elementos de CIRR(m¯).

Um transversal em um hipergrafo é um subconjunto de vértices que possui interseção com todas as arestas do hipergrafo.

Seja H = (M,E) um hipergrafo cujo conjunto de arestas seja E e cujo conjunto de vértices seja M e seja T M.

T é um transversal se, para toda aresta E E, T E 6= 0.

T é um minimal transversal se não existe subconjunto próprio de T que seja também um transversal.

Assim, o conjunto dos minimal transversals associados a CIRR(m¯) é Como o algoritmo Aprem-IR baseia-se no fato de que o conjunto dos antecedentes de um atributo m, lhs(m), é igual ao conjunto MTR(CIRR(m¯)), ele possui basicamente duas etapas.

Primeiro, ele determina, para cada atributo m do contexto, uma lista com o complemento dos conjuntos ínfimo-irredutíveis máximos não contendo m.

Depois, determina o conjunto dos minimal transversals e, com isso, o conjunto de implicações para cada atributo m M.

O pseudocódigo do algoritmo é apresentado pelo Algoritmo 7.

As funções obterCIRR e obterRegras referem-se às duas etapas mencionadas anteriormente as quais são apresentadas em Algoritmo 8 e 9.

Encontra cobertura canônica dada uma lista de conceitos B(G,M,I).

O Algoritmo 8, para obter os conjuntos CIRR(m¯) de cada atributo m, inicialmente, insere o conjunto vazio na lista dos conjuntos ínfimo-irredutíveis não contendo m (linhas 1 a).

Em seguida, utilizando-se do fato de que IRR(m¯) é igual ao conjunto das máximas intensões não contento m, o algoritmo verifica cada conceito (linhas a 15).

Para cada atributo m não pertencente à intensão do conceito, remove-se de IRR(m¯) todo subconjunto da intensão do conceito (linhas 6 a 10).

Nas linhas 11 a 13, o algoritmo verifica se a intensão do conceito C não é subconjunto de nenhum conjunto pertencente a IRR(m¯) verifica se a intensão do conceito é máxim antes de inserí-la no conjunto.

Finalmente, nas linhas 16 a 21, o algoritmo obtém, para cada atributo m, o complemento dos conjuntos máximos ínfimo-irredutíveis não contendo m.

A segunda etapa do algoritmo, a obtenção dos minimal transversals e das implicações, é apresentada em pseudo-código pelo Algoritmo 9.

O algoritmo inicia com a base de implicações Encontra o complemento dos conjuntos máximos ínfimo-irredutíveis.

Em seguida, entra em uma repetição para encontrar os antecedentes de cada atributo de M (linhas a 28).

Para determinar o conjunto MTR(m¯), o algoritmo inicia com o conjunto vazio (linha e obtém os minimal transversals gradualmente).

Isto é, na i-ésima iteração, o algoritmo obtém os minimal transversals de tamanho i e determina os potenciais minimal transversals de tamanho i + 1.

Os candidatos iniciais a minimal transversals são obtidos na linha 4.

Seguindo, nas linhas 5 a 24, todos os candidatos a minimal transversal são considerados.

O ciclo das linhas 6 a 11 verifica quais candidatos do conjunto GEN são minimal transversals (linha 7), insere aqueles que, realmente, são (linha 8), removendo-os do conjunto de candidatos (linha 9).

Nas linhas 1a 23, o algoritmo determina os novos candidatos a minimal transversals.

Inicialmente, ao conjunto dos novos candidatos, é atribuído o conjunto vazio (linha 12).

Depois, cada candidato G remanescente em GEN (linhas 1a 22) é combinado com os demais candidatos G tais que |G| < |G | e |GG | = |G|-1 (linhas 1a 21).

Assim, o pontencial novo candidato G = GG é gerado (linha 16).

Se todos os subconjuntos de G com tamanho |G |-1 elementos já foram avaliados como candidatos a minimal transversals (linha 17), então, G é inserido no conjunto dos novos candidatos (linha 18).

Após todos minimal transversals terem sido encontrados, as implicações do tipo T m são encontradas para cada antecedente (minimal transversal) T MTR(m¯)-{m} no ciclo das linhas 25 a 27.

Enfim, após todos os atributos terem sido considerados, a base de implicações F é retornada.

Encontra o conjunto de implicações próprias a partir dos CIRR.

Os algoritmos são comparados sob o ponto de vista prático nesta seção.

Eles foram implementados em Java.

Os testes foram realizados em um computador Pentium 3, 850 MHz com 440 MB de memória principal em sistema operacional Windows XP.

O leitor pode questionar a escolha da linguagem de programação, uma vez que ela não oferece a melhor utilização dos recursos computacionais.

Entretanto, a implementação dos algoritmos tem como objetivo compará-los, e não o objetivo de fornecer implementações eficazes dos mesmos.

Vale ressaltar que, pela implementação ser em Java, também, não é garantido que recursos da linguagem como o coletor de lixo não tem impacto nos testes.

A intensão dos testes é verificar o comportamento dos algoritmos diante de situações reais.

Gerou-se bases de dados sintéticas para a realização dos testes, expondo os algoritmos a situações extremas.

Além de bases sintéticas, foram utilizadas algumas bases reais retiradas do repositório de bases de dados da Universidade da Califórnia em Irvine.

As bases sintéticas utilizadas para avaliar o comportamento dos algoritmos para extrair implicações foram geradas através da metodologia proposta por Agrawal com o auxílio do programa implementado por ele e adaptado por M Zaki que o dispôs em sua página da web.

As bases foram geradas para verificar o desempenho dos algoritmos diante de contextos formais com diferentes densidades e com diferentes quantidades de objetos.

A variação da densidade foi feita mantendo-se as dimensões dos contextos (número de atributos e objetos) e variando-se a média de atributos por objeto.

Variou-se a média de atributos por objeto de forma a obter densidades dos contextos entre 20 e 70%.

O número de objetos foi variado entre 100 e 10000 objetos.

O objetivo de avaliar os algoritmos sob os pontos de vista da densidade dos contextos e do número de objetos encontra-se no fato de que a construção do reticulado conceitual é sensível à relação I e a variação dos objetos foi feita para avaliar o desempenho dos algoritmos com bases de dados de tamanhos diferentes.

Esses testes tentam cobrir uma deficiência constatada na literatura, pois os testes já apresentados não comparam os algoritmos sob os dois pontos de vista mencionados e, em geral, utilizam bases de dados reais porém sem a definição de critérios bem estabelecidos.

As bases de dados retiradas do repositório foram tratadas e transformadas em contextos formais antes de serem utilizadas para avaliar os algoritmos.

O tratamento inclui a remoção de tuplas com valores desconhecidos, a discretização de atributos contínuos através da adoção de intervalos e a transformação de atributos multivalorados em atributos univalorados.

A avaliação do desempenho dos algoritmos para a extração de dependências funcionais também foi feita utilizando-se bases sintéticas.

A geração desses contextos multivalorados deu-se através da escolha aleatória dos valores de cada atributo de cada objeto.

Exemplificando, escolhendo-se |W| = 10, um valor no intervalo de 1 a 10 é atribuído a cada atributo de cada objeto.

Gerou-se contextos formais com 50, 100, 150 e 200 objetos (|G| = 50, |G| = 100) e com 7 e 10 atributos com a mesma quantidade de valores por atributo, ou seja, para |M| = 7 o número de valores por atributo também é 7.

A variação do número de atributos e do número de objetos foi relativamente pequena pois, ao se considerar o problema da descoberta de dependências funcionais, deve-se considerar a transformação de contextos descrita na seção 31que transforma um problema de ordem n em um de ordem n.

Assim, contextos formais relativamente pequenos quando multivalorados, ao serem transformados em univalorados, tornam-se contextos com tamanho significativo.

A presenta as bases de dados utilizadas para a avaliação dos algoritmos.

A tabela apresenta informações sobre as bases já transformadas em contextos formais tais como o número de atributos, o número de objetos, o tamanho da relação I, a média de atributos por objetos e a densidade do contexto formal.

Os contextos formais de a 1na oram utilizados para avaliar os algoritmos na extração de implicações, enquanto os contextos de 15 a 2foram utilizados para a extração de dependências funcionais.

O contexto formal 1 foi utilizado tanto para extração de implicações quanto para dependências funcionais.

Outra ressalva em relação à que os contextos de 15 a 2são contextos formais multivalorados, porém, a densidade deles foi calculada baseando-se no contexto formal univalorado obtido através da escala plana.

Os algoritmos foram inicialmente comparados para a extração de implicações.

A tabela apresenta o desempenho dos algoritmos.

Nela são apresentados o tempo de execução e o número de regras extraídas com os algoritmos.

Os contextos 1 e na tabela, respectivamente a base de dados referente a recomendações sobre o uso de lentes de contato e a base de dados sobre doenças hepáticas, representam duas situações distintas.

O contexto 1 é bastante pequeno, contendo apenas 2objetos e 11 atributos.

Ele foi escolhido com o intuito de avaliar o comportamento dos algoritmos diante de problemas simples.

Já o contexto 2, apesar de ser também um contexto pequeno, foi escolhido porque os dados são bastante correlacionados e o contexto apresenta alta densidade.

Logo, esse contexto foi escolhido para avaliar o comportamento dos algoritmos frente a bases de dados complexas com dados com alto índice de correlação.

O 10 Em geral, as bases do repositório apresentam tuplas cujos valores para alguns atributos são, às vezes, desconhecidos.

Nesses casos, a tupla foi removida da base.

Contextos formais para avaliação dos algoritmos desempenho dos algoritmos para esses contextos foi satisfatório.

Tanto o Next Closure quanto o Find Implications gastaram cerca de 1 segundo para extrair implicações do contexto 1 e gastaram, respectivamente, cerca de minutos e pouco mais de 1 minuto para o contexto 2.

Na tabela, os tempos de execução do Impec e do Aprem-IR não foram apresentados para o contexto pois os testes foram abortados devido a falta de memória.

O desempenho dos algoritmos para a variação da densidade e da quantidade de objetos são apresentados graficamente nas Figuras e 3.

Observa-se que o desempenho dos algoritmos Find Implications e Aprem-IR está relacionado à densidade dos contextos.

À medida que a densidade dos contextos aumenta, observa-se o aumento no tempo de execução.

Entretanto, esse comportamento não foi observado para o Impec e o Next Closure.

O Impec teve o tempo de execução reduzido ao aumentar a densidade do contexto.

Já o Next Closure não sofreu aumento no tempo de execução com o aumento da densidade dos contextos, mantendo tempo de execução praticamente constante.

Os algoritmos Aprem-IR e Find Implications sofreram maior influência no tempo de execução com o aumento da densidade porque utilizam diretamente reticulados conceituais para a extração de implicações.

Como mostrado por Carpineto e Romano, o desempenho dos algoritmos para a construção de reticulados conceituais depende diretamente da relação I.

O pior caso para esses algoritmos ocorre quando os objetos do contexto formal possuem todos os atributos exceto aqueles na diagonal principal.

Desempenho dos algoritmos para implicações.

NC = Next Closure e FI = Find Implications.

Como, ao aumentar a densidade dos contextos, eles aproximam-se do pior caso, os desempenhos do Aprem-IR e do Find Implications são degradados ao aumentar a densidade.

No entanto, entre o Aprem-IR e o Find Implications, observa-se que o desempenho do Find Implications é superior.

O motivo dessa superioridade está na escolha do algoritmo para a construção do reticulado conceitual.

A proposta original do Aprem-IR apresentada por Taouil e co-autores supõe a utilização do algoritmo Aprem para a construção do reticulado, já o Find Implications utiliza o algoritmo Concept Covers de L Nourine e O Raynaud para a construção do reticulado.

Esse algoritmo é apontado, na literatura, como o mais eficiente para a construção do reticulado.

Em relação ao aumento na quantidade de objetos, a Figura revela que, em geral, os algoritmos mostraram-se pouco sensíveis.

O tempo de execução dos algoritmos permaneceu praticamente constante ao se variar a quantidade de objetos.

Uma situação inesperada é observada quando o número de objetos é 2000.

Nesse ponto, o algoritmo Impec obteve o menor tempo de execução.

Contudo, observando-se a descrição dos contextos apresentada na tabela, constata-se que a densidade do contexto 6 referente aos 2000 objetos é inferior em relação aos contextos 3, 4, 5 e 7 referentes ao número de objetos 100, 500, 1000 e 10000 (cerca de 47% contra 53% dos demais) e, observando-se novamente, constata-se que, por volta de 47%, o tempo de execução esperado é, realmente, inferior em relação ao de 53%.

Os algoritmos Next Closure e Aprem-IR apresentaram os melhores resultados com relação ao aumento da quantidade de objetos no contexto.

Eles sofreram pequeno acréscimo no tempo de execução com o aumento do número de objetos.

O algoritmo que mostrou-se mais sensível em relação ao aumento do número de objetos foi o Find Implications que apresentou Desempenho dos algoritmos para extração de implicações variando-se a densidade.

Crescimento exponencial do tempo de execução.

Mais uma vez o fato relaciona-se à construção do reticulado.

Parte do algoritmo Concept Covers, utilizado na construção do reticulado pelo Find Implications, computa diversas vezes fechos de atributos, aplica os operadores de derivação com freqüência e realiza interseções entre as extensões de conceitos e conjuntos de objetos resultados da aplicação do operador de derivação de atributos para a descoberta da relação de cobertura entre conceitos.

O tempo para a computação dos operadores de fecho e de derivação de atributos e a computação das interseções é desprezível quando o número de objetos é pequeno, porém, quando o número de objetos cresce (como ocorreu no experimento) o tempo torna-se significativo e, com isso, o desempenho do algoritmo cai.

Analisando-se os algoritmos sob o ponto de vista do número de regras, observa-se que o aumento da densidade provoca uma queda no número de regras produzidas pelo Next Closure, Impec e Aprem-IR.

Já para o Find Implications ocorre um leve aumento no número de regras.

A queda do número de regras com o aumento da densidade é esperada uma vez que, com o aumento da densidade, os objetos do contexto tornam-se mais correlacionados possuindo mais atributos em comum e, assim, são necessárias menos regras para descrevê-los.

O aumento do número de regras geradas pelo Find Implications também é esperado.

O aumento da densidade do contexto acarreta o aumento no número de conceitos formais no reticulado e, assim, mais regras são geradas já que o algoritmo percorre o reticulado para gerá-las.

Contudo, a maioria dessas regras são válidas por vacuidade e são obtidas pelo ínfimo do reticulado.

Logo, pode-se modificar o algoritmo para desconsiderar essas regras.

Desempenho dos algoritmos para extração de implicações variando-se o número de objetos.

O mesmo comportamento é constatado com o aumento do número de objetos.

No entanto, houve também queda no número de regras para o Find Implications.

A queda do número de regras com relação ao aumento do número de objetos é normal já que, com o aumento do número de objetos, eles tornam-se mais diferentes, com menos atributos em comum e menos regras podem ser extraídas.

A comparação dos métodos para a extração de dependências funcionais foi realizada com os contextos 15 a 2da tabela.

Os algoritmos foram comparados tanto utilizando a transformação de contextos quanto utilizando os operadores de particionamento.

No entanto, o Find Implications não pôde ser comparado com os demais utilizando-se os operadores de particionamento, pois a construção do reticulado através do Concepts Cover baseia-se nos operadores de derivação.

Esse fato não ocorre com o Aprem-IR que também utiliza reticulados diretamente, pois o Aprem-IR utiliza o algoritmo Aprem para construir o reticulado e ele baseia-se em operadores de fecho gerais 11 tal como o Next Closure.

A presenta o tempo de execução e o número de regras para os quatro algoritmos através da transformação de contextos.

A tabela apresenta o tempo de execução e o número de regras para os algoritmos utilizando os operadores de particionamento.

Os tempos de execução apresentados nas Tabelas 7 e 8 são apresentados graficamente na 11 Quaisquer operadores que satisfaçam as propriedades dos operadores de fecho.

Desempenho dos algoritmos para extrair dependências funcionais transformando contextos.

Desempenho dos algoritmos para extrair dependências funcionais com operadores de particionamento.

As duas primeiras comparam o desempenho dos algoritmos para contextos com 10 atributos e 10 valores por atributo com variações no número de objetos.

A Figura utilizando a transformação de contextos e a Figura utilizando os operadores de particionamento.

As duas últimas apresentam as mesmas comparações porém para contextos com 7 atributos e 7 valores por atributo.

Analisando as figuras de uma forma geral, percebe-se que os operadores de particionamento mostraram-se menos eficazes que a transformação de contextos.

O tempo gasto para a computação do fecho utilizando os operadores de particionamento é maior que o tempo para a computação do fecho com os operadores de derivação.

Isso faz com que o desempenho de todos os algoritmos seja comprometido principalmente daqueles que foram desenvolvidos com base no operador de derivação como o Impec.

Obviamente, todos os algoritmos mostraram-se sensíveis ao aumento do número de objetos tanto utilizando a transformação de contexto quanto utilizando os operadores de particionamento.

Todavia, se comparados os tempos de cada algoritmo entre si, observa-se que a diferença entre o tempo utilizando operadores de particionamento e utilizando a transformação de contexto é menor quando o número de atributos é menor.

Exemplificando, tomando-se os contextos 15 e 16 (ambos com 50 objetos, porém o primeiro com 10 atributos e o segundo com nas Tabelas 7 e 8 para o Next Closure, a diferença entre o tempo de execução com o uso de operadores de particionamento e transformação de contexto para o contexto 15 é de 235,15 segundos).

Já a diferença entre o tempo com o uso dos operadores de particionamento e transformação de contexto para o contexto 16 é de 2,1 segundos.

Além disso, em geral, o desempenho do Next Closure utilizando operadores de particionamento é melhor que utilizando a transformação de contexto quando o número de atributos é menor.

Esse mesmo fato não ocorre para o Aprem-IR e para o Impec.

Isso já era esperado para o Impec pois, como dito anteriormente, o Impec foi criado com base nos operadores de derivação.

Contudo, o mesmo não se esperava para o Aprem-IR pois ele foi criado para ser utilizado com quaisquer operadores de fecho.

Assim, esperava-se que o desempenho do Aprem-IR também fosse melhor com operadores de particionamento quando o número de atributos fosse menor.

Isso revela um forte indício que o Aprem-IR foi criado com base nos operadores de derivação, mesmo sendo afirmado por seus autores que ele fora criado com base em operadores de fecho gerais.

Analisando-se as Figuras e 6 observa-se que o desempenho dos algoritmos baseados em reticulado é melhor que os demais para a extração de dependências funcionais, ou seja, o desempenho do Find Implications e do Aprem-IR é melhor que o desempenho do Next Closure e do Impec.

Observa-se também que o bom desempenho do Next Closure para a extração de implicações não foi mantido para a extração de dependências funcionais.

Enquanto o Next Desempenho dos algoritmos para extrair dependências funcionais variando número de objetos e transformando contextos.

Número de atributos 10 e 10 valores por atributo.

Closure obteve o melhor resultado com relação ao tempo de execução para a extração de implicações, para a extração de dependências funcionais, ele obteve o pior resultado entre os quatro algoritmos.

Pode-se ainda observar nas figuras que a variação do número de atributos no contexto influencia menos o desempenho dos algoritmos que utilizam diretamente o reticulado conceitual para a extração de dependências funcionais, ou seja, tanto o Aprem-IR quanto o Find Implications obtiveram tempos de execução semelhantes para 7 e 10 atributos.

Desempenho dos algoritmos para extrair dependências funcionais variando número de objetos e utilizando op de particionamento.

Número de atributos 10 e 10 valores por atributo.

Desempenho dos algoritmos para extrair dependências funcionais variando número de objetos e transformando contextos.

Número de atributos 7 e 7 valores por atributo.

Desempenho dos algoritmos para extrair dependências funcionais variando número de objetos e utilizando op de particionamento.

Número de atributos 7 e 7 valores por atributo.

O avanço da tecnologia facilitou o processo de coleta e armazenagem de dados provenientes de diversas fontes.

Esta facilidade no tratamento dos dados acarretou o aumento da quantidade armazenada, tornando inexeqüível a análise manual destes para a geração de informações que auxiliem o processo de tomada de decisões dentro das organizações.

Então, tornam-se necessários mecanismos automáticos que auxiliem o processo de tomada de decisões dentro das organizações.

A mineração de dados possui papel importante para auxiliar naquele processo já que fornece mecanismos para analisar os dados dentro de uma empresa.

Ela fornece técnicas que, por exemplo, permitem a uma loja analisar seu histórico de vendas, verificando produtos que, normalmente, são vendidos juntos e, assim, criar promoções para o aumento de vendas futuras.

Este foi o problema apontado por Agrawal e seus co-autores ao introduzirem a idéia de regras de associação.

As regras de associação são implicações entre atributos de um banco de dados que são válidas apenas para grupos de elementos.

Uma regra de associação descreve, por exemplo, o relacionamento, 80% dos clientes que compraram um televisor e um aparelho de DVD, também compraram um home theater.

Neste caso, a regra de associação é televisor e aparelho de DVD implica home theater com 80% de confiança.

Televisor e aparelho de DVD são o antecedente da regra, enquanto, home theater é o conseqüente.

A confiança indica o quanto a regra é válida para o banco de dados do qual ela foi obtida, ou seja, no histórico de vendas, 80% dos clientes que compraram televisores e aparelhos de DVD, também compraram home theaters.

Observe que as regras de associação assemelham-se às implicações, pois revelam relacionamentos entre atributos do banco de dados.

Porém, estes dois tipos de regras diferem na questão da confiança.

As implicações são válidas, sempre, para 100% dos dados.

Já as regras de associação admitem exceções para a regra, o que levou-se a introduzir o parâmetro confiança à regra.

Ao se obter as regras de associação de um banco de dados, pode-se desejar apenas aquelas com uma predeterminada confiança mínima.

Assim, em geral, os algoritmos para extração de regras de associação admitem que o usuário informe qual a confiança mínima das regras que ele deseja obter.

De acordo com a definição de confiança no exemplo anterior, ela indica que 80% dos clientes que compraram televisores e aparelhos de DVD, também compraram home theaters.

Ela relaciona-se diretamente com os clientes que compraram televisores, aparelhos de DVD e home theaters.

Porém, o quanto esta regra é representativa para o banco de dados?

O quanto a venda de televisores, aparelhos de DVD e home theaters é representativa em todo o histórico de vendas?

A esta questão, respondeu-se introduzindo mais um parâmetro às regras de associação.

Foi introduzido o denominado suporte que revela, para o exemplo, quantos clientes compraram televisores, aparelhos de DVD ou home theaters do total de clientes.

Generalizando, o suporte revela quantos elementos do banco de dados atendem àquela regra, ou seja, quantos possuem os atributos relacionadas à regra.

Da mesma forma que com a confiança, pode-se informar o suporte mínimo para a extração das regras de associação.

Formalmente, as regras de associação são quádruplas ordenadas (X,Y,suporte,confiança em que X e Y são subconjuntos disjuntos de atributos e o suporte e a confiança são números reais entre 0 e 1 que indicam, respectivamente, a probabilidade dos elementos de um banco de dados possuírem os atributos de X e de Y e a probabilidade condicional dos elementos do banco de dados possuírem os atributos de Y, dado que eles possuem os atributos de X).

Normalmente, utiliza-se a notação X Y para representar a regra de associação (X,Y,suporte,confiança ).

Seja (G,M,I) um contexto formal.

No contexto da AFC, a definição de regra de associação descrita acima pode ser reescrita da seguinte forma, (X, Y, suporte, confiança é uma regra de associação tal que X,Y M, X Y = 0, o suporte e a confiança são/ obtidos como abaixo).

O problema da extração de regras de associação de bancos de dados é, em geral, dividido em dois subproblemas, 1 Encontrar combinações (subconjuntos) de atributos com suporte mínimo, chamados de conjuntos de itens freqüentes 1.

O segundo subproblema consiste em extrair, dos conjuntos de itens freqüentes, as regras de associação.

Em inglês, frequent itemsets.

Exemplo de contexto para regras de associação.

O primeiro problema consiste em encontrar o conjuntoCIF = {X M|sup(X) min_sup}, sendo que M é o conjunto de atributos do contexto (G,M,I), sup(X) = |X |/|G| é o suporte de um conjunto de atributos e min_sup é o suporte mínimo definido pelo usuário.

Este problema é computacionalmente caro, pois, o espaço de soluções é equivalente ao conjunto potência dos atributos, considerando-se que toda combinação de atributos é, potencialmente, um conjunto de itens freqüentes.

A solução consiste em percorrer o reticulado de subconjuntos de M verificando os conjuntos com suporte acima do requerido.

Exemplificando, seja (G,M,I) o contexto da tabela.

O reticulado apresentado ilustra o espaço de soluções do problema de busca por conjuntos de itens freqüentes no contexto da tabela.

Os conjuntos freqüentes (com suporte mínimo igual a 40%) estão destacados.

Nesse reticulado apenas os conjuntos contendo apenas um atributo foram rotulados, assim, para saber o conjunto de atributos representado em um nó, basta unir os atributos dos nós superiores.

Observe que para confirmar se um subconjunto de atributos é freqüente, deve-se verificar todos os objetos que possuam os atributos do conjunto, o que torna o processo de descoberta de conjuntos de itens freqüentes ainda mais caro.

Felizmente, os algoritmos podem, com certa eficiência, computar os conjuntos de itens freqüentes utilizando-se as proposições 6 e 7 apresentadas a seguir, Proposição 6 (Propriedade 1 em).

Todo subconjunto de um conjunto de itens freqüentes é freqüente.

Demonstração.

Seja X CIF.

Seja Y X.

Pela propriedade 1 dos operadores de derivação, Y X.

Como Y X, |Y | |X |.

Logo sup(Y) sup(X).

Como sup(X) min_sup e sup(Y) sup(X), sup(Y) min_sup.

Portanto, Y é freqüente.

Proposição 7 (Propriedade em).

Todo superconjunto de um conjunto de itens não-freqüentes é não-freqüente.

Demonstração.

Seja X CIF.

Seja Y X.

Pela propriedade 1 dos operadores de derivação,/ Y X.

Como Y X, |Y | |X |.

Logo sup(Y) sup(X).

Como sup(X) min_sup e sup(Y) sup(X), sup(Y) min_sup.

Portanto, Y é não-freqüente.

Estas proposições podem ser visualmente constatadas observando-se o reticulado.

Nele todo superconjunto do conjunto de itens não-freqüentes {D} também é não-freqüente.

Já os subconjuntos do conjunto de itens freqüentes {A,B,C,E}(rótulo do nó marcado mais inferior na Figura são todos freqüentes).

Proposição 8.

Seja CIFM = {X CIF|Y CIF} a família dos conjuntos de itens freqüentes máximos.

A família dos conjuntos de itens freqüentes é igual à família de subconjuntos dos elementos de CIFM ({(X)|X CIFM} = CIF).

Os algoritmos podem, ainda, utilizar a proposição 8 para reduzir o espaço de busca de soluções, uma vez que encontrado o conjunto dos itens freqüentes máximos pode-se derivar o conjunto dos itens freqüentes.

Até agora, foram apresentadas propriedades que possibilitam aos algoritmos encontrar com maior eficiência conjuntos de itens freqüentes.

No entanto, não se mostrou como o uso de reticulados conceituais pode contribuir para a solução do problema.

À frente, será demonstrado que o conjunto dos itens freqüentes máximos é igual ao conjunto das máximas intensões dos conceitos freqüentes.

Porém, anteriormente, algumas definições serão feitas.

Um conceito formal (X,Y) é freqüente, se o número de objetos em sua extensão é maior ou igual ao mínimo requerido.

Assim, (X,Y) é freqüente, se |X|/|G| min_sup.

A família dos conceitos formais freqüentes é definida como a seguir, Sendo B(G,M,I) o reticulado conceitual do contexto formal (G,M,I).

Pode-se definir o conjunto dos conceitos freqüentes mínimos.

Note que o conjunto do conceitos freqüentes mínimos dá origem ao conjunto das máximas intensões devido à relação de ordem definida sobre os conceitos.

Antes de demonstrar que conjunto dos itens freqüentes máximos é igual ao conjunto das máximas intensões dos conceitos freqüentes, é necessária a apresentação do lema 2.

Lema (Propriedade 5 em).

O suporte de um conjunto de atributos C é igual ao suporte do maior conceito (X,Y) tal que C Y.

Teorema (Propriedade 6 em).

Seja ICFM = {Y|(X,Y) CFM} o conjunto das intensões dos conceitos freqüentes mínimos.

O conjunto ICFM é igual ao conjunto dos itens freqüentes máximos.

Demonstração.

Para demonstrar que ICFM =CIFM, basta mostrar que os elementos deCIFM são intensões de conceitos no reticulado conceitual.

Seja Y CIFM.

Pela propriedade dos operadores de derivação, Y Y.

Pelo lema 2, sup(Y) = sup(Y) min_sup.

Como Y é máximo, conclui-se que Y = Y.

Portanto, Y ICFM.

O suporte de (X,Y) é igual a |X|/|G| pois, pela definição de conceito formal, X = Y.

O teorema demonstra que o conjunto das intensões dos conceitos freqüentes mínimos é igual ao conjunto dos itens freqüentes máximos.

Dessa forma, o reticulado conceitual pode ser usado para encontrar o conjunto de itens freqüentes com maior eficiência, já que o conjunto das intensões dos conceitos é subconjunto do conjunto de itens ((M)).

O segundo problema consiste em, a partir de cada conjunto de itens freqüentesW, encontrar regras do tipo X WX em que X W.

Obviamente, a regra possui suporte acima do requerido já que W é freqüente.

Para verificar a validade da regra, basta verificar se a confiança está acima do mínimo requerido.

A confiança da regra pode ser obtida através da razão entre o suporte de W e o suporte de X.

Este também é um problema computacionalmente custoso pois, para cada conjunto de itens freqüentes, existem 2potenciais regras válidas (k é o número de atributos no conjunto) uma vez que devem ser verificadas todas as combinações de atributos no antecedente, exceto os conjuntos vazio e o próprio conjunto de itens freqüentes.

Mostrado como reticulados conceituais podem ser úteis para a identificação de regras de associação, nas seções subseqüentes serão apresentados os algoritmos baseados em AFC para tal propósito.

O algoritmo AClose foi proposto por Nicolas Pasquier e seus co-autores.

Os autores propuseram um algoritmo para a identificação dos conjuntos de itens freqüentes através do uso de conceitos freqüentes.

Para a geração efetiva das regras através dos itens freqüentes, o algoritmo utiliza uma versão modificada do algoritmo utilizado no Apriori.

O pseudo-código do algoritmo para a identificação dos conceitos freqüentes é apresentado em Algoritmo 10.

O algoritmo encontra as intensões dos conceitos freqüentes gradualmente, utilizando a idéia de geradores.

Os geradores são os menores conjuntos de atributos que dão origem a uma intensão através da aplicação do operador de fecho (·).

Considere o 00 reticulado do contexto da tabela, o conjunto {B} é um gerador para a intensão do conceito ({2,3,4,5},{B,E}) pois B = BE.

Na i-ésima etapa são avaliados os geradores de tamanho i.

O algoritmo inicia com o conjunto dos elementos de M (linha 1).

Em seguida, enquanto existirem geradores a serem avaliados o algoritmo mantém-se no ciclo das linhas à 13.

O algoritmo atribui, inicialmente, o conjunto vazio a cada conjunto fechado referente a um gerador, assim como atribui zero ao suporte de cada conjunto fechado (linhas e 4).

A seguir, o algoritmo determina o conjunto fechado e seu respectivo suporte para cada gerador com o auxílio da função determinar Conjuntos Fechados (linha 5).

Depois, o algoritmo avalia cada candidato a intensão freqüente armazenando aqueles com suporte acima do suporte mínimo (linhas 6 à 10).

O próximo passo do algoritmo é encontrar os geradores de tamanho i+1.

Na linha 11, com o auxílio da função encontrarGeradores, o algoritmo determina os novos geradores.

Repare que são utilizados apenas os conjuntos freqüentes para determinar os novos candidatos, pois, como demonstrado na proposição 7, os conjuntos originários de conjuntos não-freqüentes também são não-freqüentes.

Por fim, o algoritmo retorna as intensões dos conceitos freqüentes encontradas.

Algoritmo AClose.

Para calcular com eficiência os conjuntos fechados dos geradores de FCC, a função determinar Conjuntos Fechados utiliza a seguinte propriedade do operador de fecho (·).

Seja Y M.

Então, Y = {g |g GY g }.

Dessa forma, percorrendo o contexto formal uma única vez é possível calcular não só o conjunto fechado de cada gerador mas também seu suporte.

O Algoritmo 11 apresenta o pseudo-código da função para calcular os conjuntos fechados.

O Algoritmo 11 funciona da seguinte forma, para cada objeto g do contexto formal (linhas 1 à 11), apenas os geradores subconjuntos do conjunto de atributos do objeto g são atualizados (linha).

Caso os fechos desses geradores sejam o conjunto vazio, então a eles é atribuído o conjunto de atributos de g (linha 5).

Caso contrário, o fecho é atualizado fazendo a interseção do seu estado atual com o conjunto de atributos do objeto (linha 7).

Em seguida, o suporte do gerador é incrementado.

Finalmente, apenas os geradores com fecho não-vazio são retornados.

Geradores com fecho vazio podem ser descartados pois não serão utilizados na geração de regras.

Função para determinar conjuntos fechados.

A função encontrarGeradores é responsável por determinar os geradores de tamanho i+1.

Esta função recebe como parâmetro o conjunto dos geradores freqüentes de tamanho i.

Para determinar os geradores de tamanho i+1, a função encontrarGeradores utiliza as proposições 9 e 10 retiradas de.

Além disso, considera-se uma ordem total sobre os atributos de M.

O Algoritmo 1apresenta o pseudo-código da função encontrarGeradores.

O algoritmo inicia determinando os novos geradores (linha 1).

Os geradores de tamanho i+1 são encontrados unindo-se dois geradores freqüentes p e q de tamanho i com os mesmos i-1 elementos.

Em seguida, remove-se os geradores originários de conjuntos não-freqüentes (linhas à 8).

Geradores originários de conjuntos não-freqüentes são geradores que possuem subconjunto não pertencente a ICF.

A segunda poda no conjunto FCC é feita aplicando-se a proposição 10 (linhas 9 à 16).

Cada novo gerador é verificado e aqueles a que a proposição 10 aplica-se, isto é, aqueles que existe algum subconjunto cujo fecho é superconjunto do gerador, são removidos do conjunto de novos geradores.

Após as duas podas, o conjunto de novos geradores é retornado.

Proposição 9 lema.

Seja Y M e Y Y tal que Y Y.

Então, Y = Y e, para Proposição 10 corolário.

Seja ger um gerador de tamanho i e S = {s ger||s| = (i-1)} tal que ger = S.

Se existe s S tal que ger s, então ger = s.

Encontrada as intensões dos conceitos freqüentes, elas podem ser utilizadas para encontrar o conjunto dos itens freqüentes e, em seguida, as regras de associação válidas para o contexto formal.

Função para determinar geradores de tamanho i+1.

Para encontrar o conjunto dos itens freqüentes a partir das intensões freqüentes, basta, para cada intensão freqüente, encontrar todos os seus subconjuntos.

Esse é, basicamente, o funcionamento do Algoritmo 13.

O algoritmo inicialmente particiona o conjunto das intensões freqüentes em função do tamanho e descobre qual o tamanho da maior intensão freqüente (linhas 1 a 5).

Em seguida, cada conjunto de itens freqüentes de tamanho i-1 é completado com os subconjuntos dos itens freqüentes de tamanho i iniciando-se o processo pelos conjuntos de tamanho k (linhas 6 à 15).

Sobre cada conjunto de itens freqüentes CIF, mantém-se uma ordem sobre seus elementos, as intensões freqüentes vêm sempre à frente dos conjuntos derivados.

Essa ordem é mantida para que o suporte de um conjunto de itens seja sempre atribuído de forma correta, ou seja, o suporte de um conjunto de itens seja igual ao da menor intensão de que ele é subconjunto.

Assim, avalia-se cada conjunto de itens freqüentes (linhas 7 à 14).

Para cada conjunto freqüente I, verificam-se todos os seus subconjuntos de tamanho i-1, adicionando-os ao CIF i-1.

O suporte de cada conjunto adicionado é igual ao suporte do conjunto que lhe originou que, por sua vez, é igual ao suporte da menor intensão que ele é subconjunto.

Finalmente, o algoritmo retorna todos os conjuntos de itens freqüentes derivados.

Deriva os conjuntos de itens freqüentes.

Obtido o conjunto dos itens freqüentes, pode-se encontrar o conjunto de regras de associação válidas para o contexto formal.

O algoritmo para geração das regras de associação usado por Pasquier e seus co-autores é uma adaptação do algoritmo Apriori apresentado por Agrawal.

Seu princípio é bastante simples, para cada conjunto I CIF, derivam-se todos os subconjuntos I I e verifica-se a confiança da regra I I-I é maior que a confiança mínima, ou seja, verifica-se se sup(I)/sup(I) min_conf.

Em caso positivo, a regra é armazenada.

Observe que se a regra I não possui confiança acima do mínimo, então todo subconjunto I I também produzirá regra com confiança abaixo do mínimo já que sup(I) sup(I).

Utilizando essa propriedade, o algoritmo primeiro encontra as regras cujo conseqüente tem tamanho um.

Em seguida, para os antecedentes que originaram regras com confiança acima do mínima, explora as regras com conseqüente de tamanho e assim por diante.

O Algoritmo 1apresenta o pseudo-código do algoritmo para encontrar as regras válidas.

O algoritmo verifica todos os conjuntos de itens freqüentes para a geração de regras válidas (linhas 1 à 12).

Para cada conjunto de itens freqüente, gera-se, inicialmente, o conjunto dos conseqüentes contendo apenas um atributo (linha).

Avalia-se, então, a confiança da regra a ser obtida com cada um desses conseqüentes gerados (linha e 5).

Se a confiança está acima do mínimo requerido, então uma nova regra é gerada (linha 6).

Caso contrário, o conseqüente é excluído do conjunto de conseqüentes uma vez que de todos os superconjuntos deste conseqüente serão obtidas regras com confiança abaixo do mínimo (linha 8).

O procedimento gerar Regras é, então, chamado para gerar as regras com conseqüentes de tamanho i+1.

O funcionamento do procedimento gerar Regras é semelhante ao funcionamento do algoritmo principal.

No entanto, os conseqüentes de tamanho i+1 são obtidos através da função Apriori_Gen (linha 2procedimento gerar Regras).

O pseudo-código dessa função é apresentado em Algoritmo 15.

A função recebe como parâmetro um conjunto de conseqüentes de tamanho m e retorna o conjunto de conseqüentes de tamanho m+1.

Inicialmente, o conjunto de conseqüentes de tamanho m+1, H, é gerado combinando-se dois conseqüentes com os mesmos m-1 elementos (linha 1).

Em seguida, removem-se os conseqüentes de tamanho m+1 superconjuntos de conseqüentes de tamanho m que produziram regras com confiança abaixo do mínimo (linhas à 6).

Após essa poda, os novos conseqüentes são retornados.

Procedimento gerar Regras Entrada, um conjunto I de itens freqüentes de tamanho k e um conjunto H de conseqüentes Gera regras de associação válidas.

Gera novos conseqüentes.

O algoritmo Titanic foi proposto por Gerd Stumme e seus co-autores.

O algoritmo baseia-se no algoritmo Apriori para a construção do reticulado dos conceitos freqüentes chamado de reticulado iceberg.

O reticulado iceberg é um supremo-semi-reticulado dos conceitos freqüentes de um contexto formal.

Este reticulado foi inicialmente proposto como um método para agrupamento de dados.

No entanto, como do conjunto das intensões freqüentes pode-se derivar o conjunto de itens freqüentes, ele foi utilizado para a extração de regras de associação.

Para obter o reticulado iceberg, o algoritmo Titanic evita ao máximo o uso do operador de fecho (·), utilizando uma função de peso compatível com ele.

Antes de apresentar o algoritmo, algumas definições são necessárias.

Uma função de peso p é uma função definida sobre o conjunto potência do conjunto de atributos para uma ordem total hP,i, ou seja, p,(M) P.

O resultado da aplicação de p a um conjunto X M (p(X)) é chamado de peso de X.

A função p é compatível com o operador de fecho (·) se ela respeitar as propriedades discutidas na proposição 11.

Proposição 11.

Seja p,(M) P uma função de peso e seja (·),(M) (M) um operador de fecho.

A função p é compatível com o operador (·) se ela respeitar as seguintes propriedades, Pode-se determinar o fecho de um conjunto X M comparando-se seu peso com o peso dos seus superconjuntos próprios imediatos conforme apresentado na proposição 12.

Proposição 12.

Seja X M.

Então, X = X {m MX|p(X) = p(X {m})}.

A prova será por contradição.

Suponha que exista m XX tal que p(X) 6= p(X {m}).

Então, X 6= (X {m}) pela propriedade (11).

Logo, m X Contradição!

Seja m X {m MX|p(X) = p(X {m})}.

Se m X, então não resta nada a demonstrar.

Assim, suponha m {m MX|p(X) = p(X {m})}.

Como X X {m} e p(X) = p(X {m}), conclui-se, pela propriedade (11, que X = (X {m}).

O conjunto dos fechos de M pode ser obtido através da proposição 1aplicando-se a proposição a cada conjunto X (M).

Entretanto, tal abordagem mostra-se bastante ineficiente uma vez que dois conjuntos X,X M podem gerar o mesmo fecho e, assim, um fecho pode ser obtido diversas vezes.

Alternativamente, pode-se particionar (M) em conjuntos cujos elementos possuam o mesmo fecho e, assim, computar o fecho de apenas um conjunto de cada partição.

Essas partições são as classes de equivalência da relação de equivalência Como essas classes de equivalência só são conhecidas durante a computação dos fechos, eventualmente, pode-se computar o fecho de mais de um conjunto de cada classe.

Assim, é necessária uma estratégia para que o fecho de cada classe seja computado o menor número de vezes possível.

A estratégia adotada pelo algoritmo Titanic é a gradual.

Os conjuntos de (M) são avaliados gradualmente.

Os menores (ou mínimos caso não exista o menor em uma classe de equivalênci conjuntos (com relação à ordem) de cada classe de equivalência são avaliados primeiro.

Os menores (mínimos) conjuntos de cada classe são chamados de conjuntos chave.

Como cada classe de equivalência possui pelo menos um mínimo, para computar o conjunto dos fechos, é suficiente computar apenas o fecho do conjunto dos mínimos.

Formalmente, sendo K o conjunto de todos conjuntos chave, a afirmativa é expressa pela equação (41).

A estratégia gradual adotada pelo algoritmo consiste em descobrir os conjuntos chave durante cada iteração.

Durante a i-ésima iteração, os candidatos à chave de tamanho i são avaliados e somente aqueles cuja classe de equivalência não tenha sido verificada são considerados conjuntos chave e são mantidos para a geração de candidatos para a etapa i+1.

Um conjunto X M com tamanho |X| = i é um conjunto chave para a classe de equivalência = {Y M|(X,Y) E}, se o peso de X é diferente de todos os seus subconjuntos de tamanho i-1, ou seja, se p(X) 6= p(Y) para todo Y X tal que |Y| = i1.

Esta afirmativa é verificada pela propriedade (11).

Já se sabe que a computação dos fechos dos conjuntos chave é suficiente para encontrar o conjunto de todos os fechos.

No entanto, até agora, não foi mostrado como são gerados os candidatos de tamanho i+1 a partir dos de tamanho i.

Antes de elucidar como os candidatos de uma etapa seguinte são gerados, é necessária a apresentação da proposição 1que será útil para compreender como os candidatos são gerados.

Proposição 13.

O conjunto K de conjuntos chave é uma seção inferior para h(M),i.

Se Y K e X Y, então X K para todo X,Y M.

Conclui-se da proposição 1que um conjunto X M é um candidato a conjunto chave se todo Y X for um conjunto chave (Y K).

Como o conjunto K é uma seção inferior, por definição, se X K, então todo subconjunto Y X também pertence a K.

Em outras palavras, todo superconjunto imediato de um conjunto chave é um candidato a conjunto chave.

Formalmente, seja Y K e X Y tal que Z M, então X é um candidato a conjunto chave.

Logo, o conjunto de candidatos a conjunto chave de tamanho i+1 é em que K é o conjunto dos conjuntos chave de tamanho i.

O algoritmo inicia com o conjunto de candidatos de tamanho 0 (conjunto vazio).

Como já se sabe que o peso do conjunto vazio é sempre 1, evita-se a chamada da função pesar na linha 1.

Em seguida, o conjunto vazio é incluído como primeiro conjunto chave (linha pois, pela propriedade 13, o conjunto vazio sempre pertence ao conjunto dos conjuntos chave).

Seguindo, ao indicador de iteração, é atribuído o valor zero marcando o início da computação e os candidatos a conjunto chave de tamanho 1 são gerados (linha 4).

Na linha 5, para cada candidato a conjunto chave c C, armazena-se o menor peso dos subconjuntos de tamanho |c|-1 em cs_p.

Este parâmetro é utilizado para eliminar candidatos que não sejam conjuntos chave (linha 9).

O algoritmo entra em um ciclo (linhas 6 à avaliando todos os candidatos a conjuntos chave).

O algoritmo desconsidera conjuntos chave com fecho não-freqüente para a geração de novos candidatos já que, deles, originar-se-ão conjuntos não-freqüentes.

Durante a i-ésima iteração, o algoritmo calcula os fechos dos conjuntos chave de tamanho i (linha 7), calcula o peso(suporte) dos candidatos a conjunto chave para (i+1)-ésima iteração (linha 8), obtem os conjuntos chave de tamanho i+1 (linha e calcula os novos candidatos a conjunto chave (linha 11).

Os candidatos a conjunto chave de tamanho i+1 são gerados através da função Titanic_Gen.

Ela utiliza a equação (42) para gerar os novos candidatos.

No entanto, aplica-se um filtro ao conjunto dos novos candidatos, excluindo-se aqueles que são superconjuntos de conjuntos chave cujos fechos são não-freqüentes.

O algoritmo é apresentado em pseudo-código pelo Algoritmo 17.

O algoritmo começa obtendo o conjunto dos possíveis candidatos a conjunto chave (linha 1).

Os possíveis candidatos são encontrados combinando-se conjuntos chave de tamanho i Titanic de forma a gerar candidatos de tamanho i+1.

Em seguida, verifica-se cada candidato (linhas à 11).

Para cada candidato k, constata-se se todos os seus subconjuntos de tamanho i pertecem ao conjunto dos conjuntos chave K e se nenhum de seus subconjuntos possui fecho não-freqüente (linhas à 10).

Caso um dos subconjuntos do candidato não seja um conjunto chave, ou possui fecho não-freqüente, o candidato é excluído já que ele não será um conjunto chave ou possui fecho não-freqüente.

Após todos candidatos terem sido avaliados, aqueles que passaram pelo filtro são retornados.

Algoritmo Titanic_Gen Entrada, Um conjunto K de conjuntos chave i Saída, O conjunto C de candidatos a conjunto chave de tamanho i+1 início Gera candidatos a conjunto chave.

O cálculo dos fechos dos conjuntos chave é feito através da proposição 12.

No entanto, utilizam-se as propriedades de monotonicidade crescente e extensividade (respectivamente i e da definiçãodos operadores de fecho para melhorar a eficiência do algoritmo).

Antes de aplicar a proposição 12, o algoritmo faz a união do próprio conjunto chave com o fecho dos seus subconjuntos imediatos.

O pseudo-código do algoritmo é apresentado em Algoritmo 18.

O Algoritmo 18 inicia com um ciclo para calcular o fecho dos conjuntos chave (linhas 1 à 19).

No entanto, como o interesse é apenas nos candidatos com fechos freqüentes, os candidatos com peso igual a1 (candidatos com suporte abaixo do mínimo) não têm seu fecho calculado.

O cálculo do fecho de um candidato inicia-se com o conjunto de atributos do próprio candidato aplicando-se a propriedade dos operadores de fecho (linha).

Em seguida, pela propriedade i dos operadores de fecho, faz-se a união dos fechos dos subconjuntos com o fecho atual do candidato (linhas à 6).

Por fim, aplica-se a proposição 1para os atributos que ainda não foram incluídos no fecho do candidato (linhas 7 à 16).

Calcula os fechos de um conjunto de conjuntos chave.

O peso de um conjunto de candidatos a conjunto chave C é calculado verificando-se o número de objetos do contexto formal que possuem os atributos do candidato.

Para cada objeto g G do contexto formal, verificam-se os candidatos cujos atributos são atributos de g e incrementa-se o peso do candidato indicando que mais um objeto possui aqueles atributos.

Se o número de objetos que possuem os atributos de um candidato é maior ou igual ao mínimo suporte, então o peso do candidato é igual ao suporte do candidato, caso contrário o peso é igual a1.

O algoritmo é apresentado em pseudo-código pelo Algoritmo 19.

Calcula os pesos de um conjunto de candidatos a conjunto chave.

Finalmente, encontrados os conceitos freqüentes, eles são utilizados para a geração dos itens freqüentes e geração das regras da mesma forma que o AClose.

O algoritmo Frequent Next Neighbours (FNN) foi proposto por Carpineto e Romano.

O algoritmo baseia-se nos trabalhos de Zaki.

O algoritmo constrói o reticulado conceitual com os conceitos freqüentes e usa-o como um guia para extração de regras de associação.

O FNN adapta o algoritmo Next Neighbours proposto por Bordat.

O algoritmo proposto por Bordat constrói o reticulado conceitual utilizando a relação de cobertura apresentada no capítulo 2.

O algoritmo descobre, a cada iteração, os próximos conceitos (de acordo com a relação de cobertur de cada um dos conceitos descobertos anteriormente os conceitos descobertos são usados na próxima iteração para descobrir novos conceitos).

O algoritmo inicia com o conceito (G,G), descobre seus sucessores no reticulado e, em seguida, repete-se o processo para os sucessores e, assim, sucessivamente até que todos os conceitos tenham sido encontrados.

A adaptação do FNN em relação ao Next Neighbours é que o FNN, a cada iteração, considera apenas os conceitos freqüentes para encontrar novos conceitos.

Essa adaptação leva em conta a proposição 7.

Os sucessores de conceitos não-freqüentes também são não-freqüentes, portanto, os conceitos não-freqüentes são desconsiderados.

O problema da descoberta de conjuntos de itens freqüentes é solucionado com o algoritmo FNN apresentado em Algoritmo 20.

Utilizou-se o mesmo nome para designar a metodologia para encontrar regras de associação proposta por Carpineto e Romano e para o algoritmo que constrói o reticulado conceitual dos conceitos freqüentes.

No entanto, pelo contexto em que o termo FNN é usado, ficará claro se se refere à metodologia ou ao algoritmo para encontrar o reticulado.

O algoritmo inicia adicionando o conceito (G,G) ao reticulado (linha 1).

Na linha 2, nível atual (conceitos que serão usados para descobrir novos conceitos) também é iniciado com o conceito (G,G).

Em seguida, o algoritmo entra em um ciclo (linhas à para que todos os conceitos freqüentes sejam encontrados).

Para cada conceito no nível atual, o algoritmo encontra seus sucessores freqüentes com a função encontrarProximosConceitos (linha 6), atualiza o conjunto dos próximos conceitos a serem avaliados (linha e adiciona os novos conceitos ao reticulado atualizando a relação entre os conceitos (linhas 8 e 9).

Em seguida, atualiza o nível atual com os novos conceitos encontrados (linha 11).

Encontra conceitos freqüentes.

A função para encontrar os próximos conceitos é apresentada no Algoritmo 21.

O algoritmo recebe como parâmetro um conceito (X,Y) e combina a intensão do conceito com cada atributo almejando encontrar novas intensões.

No entanto, leva-se em conta o suporte.

São considerados candidatos a sucessores do conceito (X,Y) somente os conceitos cujo número de objetos na extensão é pelo menos o mínimo necessário (linhas e 5).

Depois, o novo conceito é gerado (linha 6).

Se o conceito não foi inserido no conjunto dos candidatos, então ele é inserido e atribui-se o contador 1 a ele indicando que foi gerado pela primeira vez.

Caso contrário o contador é incrementado.

Esse contador é utilizado para certificar que um conceito é realmente o sucessor de (X,Y) na relação.

Um conceito (X,Y) é considerado sucessor de (X,Y), se, para todo conjunto Y {m} tal que m YY, o fecho Y {m} é Y, ou seja, m YY (Y Encontra próximos conceitos freqüentes).

Encontrado o reticulado dos conceitos freqüentes, pode-se encontrar as regras de associação.

O problema da descoberta de regras de associação é dividido pelo método FNN, como dividido por Zaki, em dois subproblemas, o problema de encontrar regras de associação com confiança de 100% e o problema de encontrar regras com confiança abaixo de 100%.

O problema de encontrar regras de associação com confiança igual a 100% é tratado da mesma forma que o problema da descoberta de implicações.

Logo, Carpineto e Romano solucionaram o problema aplicando o algoritmo Find Implications apresentado na seção 352.

Para solucionar o problema da descoberta de regras de associação com confiança abaixo de 100%, o algoritmo investiga cada conceito atributo µ(m) e gera regras cujos antecedentes são os subconjuntos contendo m na intensão de µ(m) e cujos conseqüentes são todos os subconjuntos dos sucessores de µ(m) que contêm atributos não pertencentes à intensão de µ(m).

O algoritmo é apresentado em Algoritmo 22.

Encontra regras de associação com confiança abaixo de 100%.

O algoritmo verifica cada conceito do reticulado de conceitos freqüentes para constatar se ele é um conceito atributo (linhas 1 à 17).

Um conceito (X,Y) é um conceito atributo de todos os atributos não pertencentes à intensão dos conceitos maiores que ele.

O algoritmo verifica para quais atributos o conceito é um conceito atributo (linhas e).

Geram-se regras para todo atributo m tal que (X,Y) é um conceito atributo (linhas à 16).

As regras são geradas observando-se os sucessores de (X,Y).

A confiança de cada regra é dada pela razão entre o número de objetos na extensão de um sucessor de (X,Y) e o tamanho de X.

O suporte das regras é igual ao suporte de Y.

Os antecedentes são os subconjuntos de Y que contêm m.

Os conseqüentes são todos os subconjuntos das intensões de sucessores de (X,Y) que contenham algum atributo não pertencente aY.

O algoritmo primeiro gera o conjunto de antecedentes (linhae calcula o suporte das regras).

Depois, avalia os sucessores de (X,Y) e produz regras apenas com aqueles que a confiança está acima do mínimo (linha 6 à 15).

O conjunto de conseqüentes é gerado na linha 11.

As novas regras são produzidas na linha 12.

Elas representam todas as combinações entre os antecedentes encontrados na linha e os conseqüentes encontrados na linha 11 o suporte é inerente ao conceito (X,Y) e a confiança inerente à relação (X,Y) e cada um de seus sucessores.

Os sucessores de (X,Y) são todos os conceitos (X,Y) cobertos por (X,Y).

As regras entre os conceitos (X,Y) e (X,Y) tal que (X,Y) (X,Y) mas (X,Y) 6 (X,Y) não são geradas diretamente.

Elas podem ser geradas por transitividade.

As regras de associação com confiança abaixo de 100% não respeitam o axioma da transitividade para todas as ocasiões, mas segundo Zaki, sendo P,Q,R M conjuntos de atributos tais O nome Galicia é uma abreviação para GAlois Lattice-based Incremental Closed Itemset Approach termo em inglês que designa a idéia de método incremental baseado em reticulados de Galois (reticulados conceituais) para a descoberta de conjunto de itens fechados.

O algoritmo Galicia constrói o reticulado de conceitos freqüentes de maneira incremental.

Ele foi porposto por Valtchev e seus co-autores.

O algoritmo atualiza o reticulado com a adição de um novo objeto sem a necessidade de reconstruir todo o reticulado.

Este método torna-se interessante pelo fato das constantes atualizações nos bancos de dados.

Assim, ao incluir um novo registro em um banco de dados, não existe a necessidade de se reconstruir todo o reticulado.

A intensão de um novo objeto é por si só um conjunto fechado.

Como o reticulado con-ceitual é fechado sob interseções, ou seja, como o ínfimo e o supremo de qualquer conjunto de conceitos sempre existem no reticulado conceitual (de acordo com o teorema fundamental da AFC 1), a inclusão do novo objeto envolve a computação das interseções de sua intensão com as intensões dos conceitos do reticulado.

Ao atualizar o reticulado com a inclusão de um objeto g, divide-se os conceitos em três grupos distintos, conceitos geradores (CG(g)) que dão origem a novos conceitos conceitos modificados (CM(g)) cujas intensões são subconjuntos da intensão de g e conceitos imutáveis (IM(g)) que permanecem inalterados com a inclusão de g.

Os conceitos geradores são aqueles que as interseções de suas intensões com a intensão do novo objeto não pertencem ao reticulado.

Eles são utilizados para gerar os novos conceitos auxiliando na computação da intensão e extensão dos novos conceitos.

Os conceitos modificados são aqueles que apenas suas extensões são modificadas incluindo-se o novo objeto.

A intensão desses conceitos é sempre subconjunto dos atributos do novo objeto.

Já os conceitos imutáveis são aqueles que não sofrem modificações com a inclusão do novo objeto.

O algoritmo para atualizar o reticulado consiste em encontrar esses três grupos e executar as modificações necessárias.

Para construir o reticulado de um contexto formal (G,M,I) incrementalmente, o algoritmo inicia com o reticulado h{(M,M)},0/i e considera a inclusão de cada objeto g G.

O Algoritmo 2apresenta o pseudo-código do algoritmo para a construção incremental do reticulado.

Como o objetivo do algoritmo é encontrar o conjunto das intensões freqüentes, ao invés de armazenar a extensão e a intensão de cada conceito, armazena-se a intensão do conceito e o número de objetos na extensão como forma de melhorar o desempenho do algoritmo.

Encontra o conjunto das intensões freqüentes.

A função atualizarReticulado, apresentada em Algoritmo 24, é responsável por incluir um novo objeto ao reticulado.

O algoritmo encontra as três classes de conceitos mencionadas anteriormente, verifica se as interseções dos atributos do novo objeto estão presentes no reticulado e cria novos conceitos.

Não é necessário atualizar as listas de sucessores dos conceitos pois o interesse é nas intensões freqüentes.

Porém, caso algoritmo fosse utilizado com o objetivo de construir um reticulado conceitual, então deve-se considerar a atualização das listas de sucessores entre os conceitos.

O algoritmo verifica cada conceito (sup,Y) presente no reticulado comparando sua intensão com a intensão do novo objeto (linhas à 15).

Se a intensão do conceito Y é subconjunto da intensão do novo objeto (linha, então o conceito (sup,Y) é um conceito modificado e basta adicionar o objeto à extensão do conceito, logo, o suporte do conceito é incrementado (linha 4).

Se o conceito não é um conceito modificado, então ele é um conceito imutável ou um conceito gerador.

Para descobrir a qual dos dois conjuntos, CGou IM(g), pertence o conceito (sup,Y), verifica-se a interseção Y g (linha 6).

Se existe um conceito em L cuja intensão é Y g, então (sup,Y) é um conceito imutável.

Caso contrário o conceito é um potencial gerador.

O conceito (sup,Y) é um conceito gerador se o conceito cuja intensão é Y g não fora Como foi mencionado anteriormente, um conceito formal (X,Y) é substituído pelo par (|X |,Y) por motivos gerado em outra iteração (linha 8).

Se o conceito (sup,Y) é um conceito gerador, então um novo conceito é gerado.

A intensão do novo conceito é a interseção entre a intensão do objeto e Y e o suporte é o suporte de (sup,Y) mais um (linha 9).

Se o conceito (sup,Y) não é um conceito gerador, ou seja, o conceito (x,Y = Y g) já foi gerado, então o suporte de (x,Y) é atualizado (linha 11).

Após todos os conceitos terem sido verificados, os novos conceitos são incluídos no reticulado (linha 16).

Atualiza o reticulado com um novo objeto.

O algoritmo Galicia é adequado para sitações em que se deseja atualizar um reticulado.

Seu desempenho na construção do reticulado, segundo Valtchev, é inferior em relação a outros algoritmos que constróem o reticulado inteiro.

O algoritmo também apresenta outra deficiência.

Durante a computação dos conceitos freqüentes é necessário que todos os conceitos sejam mantidos, mesmo aqueles não-freqüentes.

Isto deve-se ao fato de que após sucessivas inclusões de objetos, conceitos freqüentes podem se tornar conceitos não-freqüentes, assim como conceitos não-freqüentes podem se tornar conceitos freqüentes.

A váriavel x no conceito (x,Y) foi usada para indicar que o interesse é na intensão do conceito, assim, o suporte é desprezado.

Finalmente, encontrado o reticulado conceitual com o algoritmo Galicia, pode-se selecionar o conjunto das intensões freqüentes, derivar o conjunto de itens freqüentes e extrair o conjunto de regras válidas utilizando os algoritmos 1e 1empregados no algoritmo AClose (seção 411).

Outro processo importante para a mineração de dados é o processo de classificação.

O processo de classificação consiste em descobrir características de classes de objetos e, depois, utilizá-las para classificar objetos com classes desconhecidas.

O processo de classificação é dividido em duas etapas, utilizar um conjunto de objetos com classes conhecidas para determinar um modelo descrevendo as características de cada classe e utilizar o modelo obtido na etapa anterior para classificar objetos que não se conhece a classe.

A primeira etapa é conhecida como fase de aprendizagem, enquanto a segunda como fase de classificação.

Durante a fase de aprendizagem, o conjunto de objetos com a classe conhecida é dividido em dois subconjuntos.

Um subconjunto de objetos é chamado de conjunto de treinamento e o outro é chamado de conjunto de validação.

O conjunto de treinamento é utilizado diretamente na construção do modelo.

É desse conjunto que se extraem as características de cada classe, ou seja, o conjunto de atributos que, geralmente, um conjunto de objetos de uma classe possui.

Já o conjunto de validação é utilizado na avaliação do modelo.

O modelo é aplicado aos objetos do conjunto de validação e verifica-se a precisão do modelo, quantos objetos do conjunto de validação foram classificados corretamente.

A fase de classificação consiste em utilizar um modelo construído e já validado durante a etapa anterior na predição de classes a objetos com a classe desconhecida.

Nesta etapa, o modelo obtido do conjunto de treinamento é efetivamente usado.

Os modelos podem ser obtidos a partir do conjunto de treinamento, por exemplo, através de redes neurais artificiais, árvores de decisão e regras de classificação.

Regras de classificação são uma das técnicas mais populares para expressar um modelo.

As regras de classificação tal como as regras de associação relatam correlações entre conjuntos de atributos.

Contudo, as regras de classificação mostram correlações entre atributos e classes.

No antecedente de cada regra de classificação podem aparecer quaisquer atributos, enquanto no conseqüente restringe-se que seja um atributo-classe.

Seja M um conjunto de atributos e C M um conjunto de atributos-classe.

Formalmente, as regras de classificação são pares (X,Y) tais que X MC e Y C.

Uma regra (X,Y) também recebe a notação X Y.

Os conjuntos de regras de classificação são divididos em dois grupos, listas de decisão e regras não-ordenadas.

Nas listas de decisão, existe uma ordem nas regras.

Ao tentar classificar um novo objeto, inicia-se o processo verificando a primeira regra, caso ela não sirva para classificar o objeto, investiga-se a próxima regra seqüencialmente.

Já as regras de um conjunto de regras não-ordenadas, como o próprio nome revela, não existe ordem em relação às regras.

Logo, considera-se, em geral, todas as regras que servem para classificar um objeto.

Este processo é ineficaz quando existem regras conflitantes no conjunto de regras, ou seja, quando diferentes regras atribuem classes diferentes a um mesmo objeto.

Esse problema é geralmente resolvido utilizando-se uma estratégia de voto majoritário, atribui-se a classe a um objeto de acordo com a maioria das regras.

O uso de reticulados conceituais para classificação ainda foi pouco explorado apesar de ser uma das primeiras aplicações da AFC.

Os reticulados explicitam as dependências entre atributos e as relações entre atributos e objetos.

Com isso, eles podem ser utilizados como guia no espaço de busca de regras tal como na busca por regras de associação.

Além disso, a estrutura dos reticulados também pode ser usada na classificação.

Nesse caso, não são geradas regras, mas, usa-se o reticulado para inferir a classe de um novo objeto.

Como as regras de classificação são relações entre atributos, os algoritmos estudados nas seções 35 e 41 para extração de implicações e regras de associação podem ser usados na identificação de regras de classificação.

Além daqueles, os algoritmos GRAND e Rulearner foram propostos especificamente para a geração de regras de classificação.

O GRAND permite apenas a extração de regras não-ordenadas.

Já o Rulearner pode também ser utilizado para a extração de listas de decisão.

Esses serão os dois algoritmos avaliados neste trabalho para extração de regras de classificação.

Existem outros métodos para classificação através de reticulados como os métodos discutidos por Carpineto e Romano e por Nguifo.

No entanto, eles utilizam a estrutura do reticulado para classificar e, com isso, não serão avaliados neste trabalho.

O algoritmo Graph-based induction (GRAN foi proposto por Oosthuizen para a indução de regras de classificação).

O algoritmo utiliza um pseudo-reticulado na geração das regras.

A estrutura do pseudo-reticulado assemelha-se ao reticulado conceitual e consideram-se três tipos de elementos no reticulado, nós-atributos, nós-objetos e nós-intermediários.

Para um contexto formal (G,M,I), nós-atributos representam cada um dos atributos de M, os nós-objetos representam os objetos do contexto formal e os nós-intermediários servem para manter a estrutura do reticulado, ou seja, para garantir que os supremos de quaisquer nós pertençam ao reticulado.

O pseudo-reticulado é construído incrementalmente.

Inicialmente, cada atributo é representado por um nó-atributo no pseudo-reticulado.

Em seguida, inclui-se um novo objeto representado por um nó-objeto.

Se já existe no reticulado um nó-objeto com os mesmos atributos que o novo objeto, então a atualização do pseudo-reticulado é feita incluindo-se o novo objeto à extensão do nó-objeto correspondente.

Caso contrário, cria-se um nó-objeto para o novo objeto.

Depois, associa-se o novo nó-objeto a cada um dos nós-atributos representando a intensão do objeto.

Em seguida, verifica-se a estrutura do reticulado se o supremo (interseção) do novo nó e cada um dos nós presentes no pseudo-reticulado está presente no pseudo-reticulado.

Então, atualizam-se as listas de sucessores dos conceitos envolvidos na atualização do pseudo-reticulado e remove-se as associações redundantes.

O algoritmo para a construção do pseudo-reticulado é apresentado em pseudo-código.

Inicialmente, criam-se os nós-atributos (linha 1).

Depois, o algoritmo entra em um ciclo para incluir todos os objetos no reticulado (linhas a 29).

Se existe um nó-objeto pertencente ao pseudo-reticulado com a mesma intensão do objeto a ser incluído, então a atualização do reticulado consiste em incluir o novo objeto à extensão do nó-objeto correspondente (linha e 5).

Caso contrário, cria-se um nó-objeto para o novo objeto (linha 7).

O novo nó-objeto é associado a todos os nós-atributos que estão na intensão do objeto (linhas 8 a 10).

Depois, verifica-se se a estrutura do pseudo-reticulado foi preservada após a inclusão do nó-objeto (linhas 11 a 27), ou seja, se os supremos do nó-objeto e os demais nós do pseudo-reticulado também pertencem ao pseudo-reticulado.

Para verificar a estrutura do reticulado, todos os nós já pertencentes ao reticulado, exceto os nós atributos, são avaliados (linha).

Os nós são avaliados em ordem de tamanho da interseção entre a intensão do nó e a do novo objeto.

A cada iteração, escolhe-se o nó (X,Y) que possui o maior número de atributos em comum com o novo objeto (linha 12).

Então, cria-se um novo nó cuja intensão é a interseção da intensão do objeto e a do nó que está sendo avaliado e cuja extensão é a extensão do nó avaliado acrescida do novo objeto (linha 1).

Este novo nó é incluído no pseudo-reticulado caso ele já não pertença ao mesmo (linha 14).

Depois, o novo nó é associado aos demais nós do pseudo-reticulado (linhas 15 a 2).

Para evitar que associações redundantes sejam mantidas, ou seja, para garantir que apenas a relação de cobertura seja mantida pelas listas de sucessores, utilizaram-se os conjuntos conjA e conjB.

O conjA contém os antecessores do novo nó cujas listas de sucessores devem ser atualizadas removendo-se os sucessores do novo nó (linha 25).

O conjB contém os sucessores do novo nó e serve para atualizar a lista de sucessores desse novo nó.

Os sucessores de nós que estão em conjB são removidos da lista de sucessores do novo nó (linha 26).

Após avaliar o nó (X,Y), ele é retirado da lista de nós para que os supremos sejam avaliados (linha 24).

O processo é repetido até que não existam mais nós a serem avaliados.

Constrói pseudo-reticulados.

Após a construção do pseudo-reticulado, as regras de classificação podem ser extraídas.

As regras são geradas para cada um dos atributos-classe.

Ao gerar as regras para um atributo-classe, o algoritmo faz uma busca pelo pseudo-reticulado tentando identificar o nó mais geral (com menos atributos) a partir do qual pode-se deduzir a classe.

A busca inicia-se pelo nó-atributo corresponde à classe que se está extraindo as regras.

Visitam-se os sucessores desse nó até encontrar um nó (X,Y) que atenda às seguintes propriedades, Quando as propriedades forem atendidas, gera-se a regra Y{c} c.

Os sucessores de (X,Y) não são avaliados pois eles não serão mínimos.

O algoritmo para encontrar as regras é apresentado.

É executada uma busca em largura nos sucessores do nó-atributo da classe cujas regras serão extraídas (linha 1).

A função gerarRegrasNos executa efetivamente a geração de regras a partir dos nós do pseudo-reticulado.

O algoritmo verifica, para cada nó (X,Y) que ainda deve ser avaliado (nosVisitar), seus sucessores na tentativa de extrair as regras (linhas 1 a 9).

Se um nó (X,Y) sucessor de (X,Y) atende às propriedades descritas acima, então uma nova regra é criada (linhas e 4).

Caso contrário, os sucessores do nó (X,Y) deverão ser avaliados na próxima iteração e, com isso, o nó é inserido no conjunto dos próximos nós a serem avaliados (linha 6).

Após avaliar todos os nós de um nível, a função gerarRegrasNos é chamada recursivamente para os nós do próximo nível (linhas 10 a 12).

O algoritmo termina quando não existem mais nós a serem avaliados, ou seja, quando proximosNos está vazio.

Gera regras de classificação para nós sucessores.

O algoritmo Rulearner foi proposto por Sahami.

Assim como o GRAND, o algoritmo Rulearner utiliza pseudo-reticulados na extração das regras.

No entanto, o Rulearner apresenta algumas diferenças em relação ao GRAND.

A primeira diferença está na construção do pseudo-reticulado.

Enquanto o GRAND considera os atributos-classe durante a construção do pseudo-reticulado, o Rulearner desconsidera esses atributos.

Ao invés dos atributos-classe, o Rulearner considera uma função que, dado um conjunto de objetos, retorna a classe desses objetos.

A função, ao ser aplicada sobre um único objeto, retorna o atributo-classe desse objeto.

Quando aplicada a um conjunto com mais de um atributo, a função retorna a classe da maioria dos objetos.

A maioria é determinada por um parâmetro de exceção informado pelo usuário.

Esse parâmetro de exceção, como a confiança nas regras de associação, informa qual o percentual de objetos é necessário para que uma classe seja considerada para um conjunto de objetos.

Se não existir essa maioria para uma determinada classe, então a função retorna o valor classe mista, indicando que não se pode determinar qual a classe daquele conjunto.

Outra diferença está no tipo de regras que o Rulearner pode gerar.

O Rulearner pode ser usado tanto para gerar regras não-ordenadas (como o GRAN quanto para gerar listas de decisão).

O algoritmo é apresentado em pseudo-código.

O algoritmo recebe como parâmetros o contexto formal de que se deseja extrair as regras, o conjunto de atributos-classe C e uma função de classificação c, C com um parâmetro de exceção associado.

Como o algoritmo constrói o pseudo-reticulado desconsiderando os atributos-classe, inicialmente, um novo contexto formal sem os atributos-classe é obtido (linha 1).

Em seguida, constrói-se o pseudo-reticulado (linha e geram-se as regras (linha).

O algoritmo para construção do pseudo-reticulado foi apresentado.

A função para extração das regras é apresentada.

A função extrairRegras empregada no algoritmo Rulearner extrai regras de classificação que atendem a todos os objetos do contexto formal utilizado como conjunto de treinamento.

O algoritmo marca inicialmente todos os nós do pseudo-reticulado como ativos, indicando que seus objetos ainda não foram classificados (linha 1).

Então, o algoritmo entra em um ciclo até que todos os objetos tenham sido classificados, ou seja, enquanto existirem nós ativos (linhas a 16).

Os nós são avaliados em ordem decrescente em relação a ordem de inclusão entre as extensões (linha).

Com isso, as regras geradas classificam o maior número de objetos por vez.

São considerados, para a extração de regras, apenas os nós cuja classe é conhecida.

Escolhido o nó com a maior extensão, uma nova regra é gerada em que o antecedente é a intensão do nó e o conseqüente é a classe dos objetos da extensão (linha 4).

O próximo passo é a atualização da atividade dos nós.

Um nó é considerado inativo se não existem mais objetos em sua extensão para serem classificados.

Para atualizar a atividade de um nó, o algoritmo decrementa a cobertura (número de objetos na extensão que ainda não foram classificados) de todos os nós que possuam algum objeto em comum com o nó utilizado para a extração da regra.

O algoritmo considera cada um dos objetos na extensão do nó utilizado para gerar a regra (linhas 5 a 12).

Para cada um desses objetos, o algoritmo encontra o nó-objeto correspondente e decrementa a cobertura de todos os nós maiores ou iguais a ele (linhas 6 a 11).

Se a cobertura de um nó for menor ou igual a zero, ele é considerado inativo (linhas 8 e 9).

Caso o algoritmo seja usado para a extração de listas de decisão, os nós ativos devem ser re-rotulados.

Nesse caso, a função de classificação não deve levar em conta apenas os objetos de um conjunto, a classe deve ser atribuída com base nos nós inferiores, assim, a classificação deixa de ser com base na maioria das classes dos objetos e passa a ser com base na maioria das classes dos nós.

Extrai regras de classificação de um pseudo-reticulado.

Geradas as regras, a classificação de novos objetos é feita de duas formas.

Se uma lista de decisão for obtida, então o objeto é classificado com base na primeira regra aplicável.

Se for obtido um conjunto não-ordenado de regras, então um objeto é classificado de acordo com a maioria das regras.

Observam-se todas as regras aplicáveis e atribui-se a classe obtida pela maioria das regras.

Mais uma vez, essa maioria é determinada pelo usuário.

Esta seção é dedicada à comparação dos algoritmos para extração de regras de associação e de regras de classificação.

Como na seção 36, os métodos foram comparados sob o ponto de vista prático.

Eles foram implementados em Java e foram executados em um computador com processador Pentium 3, 850 MHz e com 440 MB de memória principal em ambiente operacional Windows XP.

Contextos formais para avaliação dos algoritmos.

Os algoritmos foram comparados utilizando-se bases de dados reais e sintéticas.

As bases de dados reais, assim como na seção 36, foram retiradas do repositório de bases de dados da Universidade da Califórnia em Irvine.

As bases de dados sintéticas foram geradas conforme descrito na seção 36, utilizando-se a metodologia sugerida por Agrawal.

A presenta as bases de dados utilizadas para a comparação dos métodos, porém, já transformadas em contextos formais.

A tabela apresenta o número de objetos, o número de atributos, o tamanho da relação de incidência, a média de atributos por objetos e a densidade dos contextos.

A densidade dos contextos refere-se à razão entre o número de elementos da relação de incidência e o máximo de elementos que ela poderia ter (em um contexto (G,M,I), o máximo de elementos que a relação de incidência pode ter é dado por |G|×|M|).

Os contextos de 1 a 8 na oram utilizados na comparação dos métodos para a extração de regras de associação.

Enquanto os demais contextos, além do 1 e 2, foram utilizados para a extração de regras de classificação.

O leitor pode observar que, para a comparação dos métodos de extração de regras de classificação, foram utilizadas apenas bases de dados reais.

Esse fato deve-se à ausência de técnicas comprovadamente eficientes para a geração de bases de dados sintéticas para a avaliação de métodos de classificação.

Logo, optou-se em utilizar, neste trabalho, bases de dados amplamente utilizadas por diversos autores para a comparação de métodos de classificação.

O contexto 1, tal como descrito na seção 36, refere-se a dados relativos a doenças hepáticas.

O contexto, apesar de aparentemente ser um contexto pequeno, foi escolhido devido ao alto índice de correlação entre seus dados.

Os objetos, em geral, apresentam muitos atributos e muitos atributos em comum com outros objetos.

Logo, ele foi escolhido para verificar o comportamento dos algoritmos com bases de dados reais e densas.

O contexto refere-se à base de dados sobre a escolha de métodos contraceptivos entre mulheres casadas na Indonésia de acordo com suas condições sócio-econômicas.

Essa base é um exemplo de base com baixa densidade (aproximadamente 29%).

Assim, ela foi escolhida para avaliar o desempenho dos algoritmos com bases de dados reais com baixa densidade.

Os contextos de a 8 referem-se a bases de dados sintéticas.

Os contextos e 8 foram gerados para verificar o comportamento dos algoritmos com bases de dados em que o número de objetos é significativo.

Já as bases de a 7 foram geradas para verificar o desempenho dos algoritmos com a variação da densidade dos contextos.

Em princípio, foram gerados contextos com densidades variando entre 20 e 70% para contextos com 1000 objetos e 15 atributos.

No entanto, quando a média de atributos por objeto chegou a 9, os algoritmos, em sua maioria, não terminaram o teste devido a falta de memória.

Com isso, foram utilizados apenas os contextos entre e 7.

Uma ressalva em relação à comparação dos métodos de extração de regras de associação é que o algoritmo Galicia, em geral, não foi comparado para os mesmos contextos que os demais.

Isso porque o método é incremental, assim, o tempo gasto para a extração de regras é muito maior que dos demais quando o número de objetos no contexto é grande.

Conforme sugestão de seus próprios autores em, o algoritmo é adequado para ocasiões em que a base de dados é alterada com a adição de novas tuplas (objetos).

Logo, o algoritmo foi analisado separadamente para avaliar seu comportamento simulando situações em que diferentes quantidades de objetos são adicionadas à base.

Os tempos de execução dos métodos para contextos com diferentes densidades são apresentados.

Nesse teste, manteve-se suporte de 2% e confiança de 50%.

Os algoritmos Titanic e AClose mostraram-se extremamente sensíveis ao aumento na densidade dos contextos, apresentando crescimento exponencial do tempo de execução.

Já o FNN não se mostrou muito sensível à variação na densidade dos contextos e apresentou crescimento linear em seu tempo de execução.

Isso retrata características importantes dos algoritmos.

Tanto o Titanic quanto o AClose foram criados com base no algoritmo Apriori.

O algoritmo Apriori não é eficiente para bases de dados com alto índice de correlação entre os dados.

O Titanic e o AClose seguiram o mesmo comportamento do Apriori para bases correlacionadas e não se mostraram eficientes nessas situações.

À medida em que a densidade do contexto aumenta, os objetos tornam-se mais correlacionados, com mais atributos em comum.

Foi exatamente nessas situações que o desempenho dos algoritmos caiu.

Já o FNN utiliza diretamente reticulados conceituais para a extração das regras e, com isso, a influência do aumento da densidade em seu tempo de execução é retardada, pois o pior caso ocorre quando a densidade do contexto chega a 75%.

A densidade do contexto foi aumentada fixando-se o número de objetos e de atributos e aumentando-se a média de atributos por objeto.

Desempenho dos algoritmos para extração de regras de associação variando-se a densidade e fixando-se o suporte em 2% e confiança em 50%.

Desempenho dos algoritmos para extração de regras de associação variando-se o suporte e fixando-se a confiança em 50% utilizando o contexto 1.

Desempenho dos algoritmos para extração de regras de associação variando-se o suporte e fixando-se a confiança em 50% utilizando o contexto 2.

Apresenta os tempos de execução dos quatro algoritmos para o contexto 1.

Nesse teste, fixou-se a confiança em 50% e variou-se o suporte para verificar o desempenho dos algoritmos.

O teste foi feito com variação no suporte já que o número de itens freqüentes em um contexto é determinado pelo suporte.

Objetivou-se verificar o comportamento dos algoritmos com aumento do número de itens freqüentes.

O suporte foi variado entre 75% e 35%.

Contudo, apenas o FNN conseguiu executar o teste com suporte de 35%.

Os demais executaram apenas com suporte de 75% e 50%.

Ao diminuir o suporte, o Titanic, o AClose e o Galicia tiveram o teste abortado devido a falta de memória.

Entre os algoritmos, o que se mostrou mais sensível à diminuição do suporte foi o Titanic.

A redução de 25% no suporte proporcionou um aumento de quase 150 vezes no tempo de execução.

A mesma redução proporcionou aumento de aproximadamente 100 vezes no tempo de execução do AClose, cerca de 9 vezes no tempo de execução do FNN e vezes no tempo do Galicia.

Isso aponta uma certa superioridade dos métodos que utilizam diretamente reticulados conceituais na extração de regras para contextos densos.

Os tempos de execução dos algoritmos para o contexto são apresentados.

Mais uma vez, fixou-se a confiança das regras em 50%, mas, variou-se o suporte entre 75 e 0,25%.

Apenas o FNN conseguiu terminar o teste com 0,25%.

O AClose teve o teste abortado com 0,25% e o Titanic com 1% ambos devido a falta de memória.

Os algoritmos que tiveram os testes abortados possuem crescimento exponencial no tempo de execução com a diminuição do suporte.

Observe que o FNN manteve o tempo de execução praticamente constante.

Isso, novamente, devido à utilização do reticulado conceitual na extração de regras.

Desempenho dos algoritmos para extração de regras de associação variando-se o suporte e fixando-se a confiança em 50% utilizando o contexto 3.

Como mencionado na seção 413, o algoritmo FNN cria um supremo-semi-reticulado com os conceitos freqüentes.

Esse supremo-semi-reticulado de conceitos freqüentes é parte do reticulado conceitual e, quando o suporte tende a zero, o supremo-semi-reticulado aproxima-se do reticulado conceitual.

Logo, o tempo de execução do FNN limita-se ao tempo de construção do reticulado conceitual que relaciona-se diretamente com a densidade do contexto.

Por isso, a variação no suporte tem menos influência para o FNN do que para os demais.

Apresenta o tempo de execução dos algoritmos para o contexto 3.

Como nos dois últimos testes, fixou-se a confiança em 50% e variou-se o suporte entre 75% e 0,25%.

Esse teste, ao contrário dos dois últimos que avaliavam o comportamento dos algoritmos para contextos com densidades diferentes, avalia os algoritmos com contextos com grande quantidade de objetos.

Com o teste, constatou-se que o FNN apresenta o pior desempenho comparado com o AClose e Titanic.

Apesar da densidade do contexto ser de 53% (caso em que o AClose e Titanic não obtiveram resultados satisfatórios), o desempenho deles foi satisfatório mesmo com baixos suportes.

Por outro lado, o desempenho do FNN degrada-se de forma quadrática com a redução do suporte.

O contexto 8 foi também utilizado para avaliar os algoritmos com contexto com muitos objetos.

Os tempos de execução são apresentados.

Nesse teste, a confiança foi mantida em 50% e variou-se o suporte entre 75% e 2%.

Desempenho dos algoritmos para extração de regras de associação variando-se o suporte e fixando-se a confiança em 50% utilizando o contexto 8.

Como acontecido com o contexto 3, o desempenho do FNN foi muito inferior quando comparado com o AClose e o Titanic.

Tanto o AClose quanto o Titanic mantiveram o tempo de execução praticamente constante com a variação do suporte.

O FNN mostrou crescimento gradual com a redução do suporte.

A redução de 75% para 10% no suporte proporcionou um aumento de cerca vezes no tempo de execução do FNN.

Isso reflete novamente que o algoritmo não sofre grande influência com a redução do suporte, porém, sofre grande influência com o aumento do número de objetos no contexto.

Como foi dito, o algoritmo Galicia foi analisado separadamente por ser um algoritmo incremental.

Ele foi analisado sob duas situações distintas.

A primeira sob uma suposta utilização direta do algoritmo para a extração de regras de associação (como já dito, essa não é a situação mais adequada para a utilização do algoritmo).

A segunda situação tem como objetivo analisar o algoritmo simulando a inclusão de diferentes quantidades de objetos no contexto.

Para o segundo teste, foram utilizados os contextos 3, e 5 da tabela, simulando a inclusão de 100, 500 e 1000 objetos no contexto sempre partindo do contexto vazio, sem objetos.

Apresenta-se o teste em que o Galicia foi utilizado para extrair regras de contextos com diferentes densidades.

O algoritmo apresentou tempo de execução elevado mesmo para baixas densidades.

Constatou-se também que o aumento do tempo de execução não acompanha o aumento na densidade do contexto.

Desempenho do Galicia para extração de regras de associação variando-se a densidade e fixando-se o suporte em 2% e confiança em 50%.

O tempo de execução aumentou até a densidade de cerca de 57% quando atingiu seu máximo e começãou a cair logo em seguida.

Esse fato, aparentemente estranho, era esperado devido ao processo de construção do reticulado conceitual do Galicia.

Como explicado na seção 414, o algoritmo atualiza o reticulado conceitual adicionando um objeto por vez, criando conceitos e adaptando a estrutura do reticulado de forma a preservá-la.

Mesmo sendo repetitivo, vale a pena lembrar que o aumento na densidade dos contextos torna os objetos mais similares ou, até mesmo, idênticos.

Objetos similares no contexto exigem menos operações para a atualização do reticulado conceitual e, conseqüentemente, o tempo de execução é reduzido.

Os tempos de execução do Galicia sob a variação do número de objetos no contexto são apresentados.

A figura confirma as expectativas quanto ao tempo de execução do algoritmo em relação a variação do número de objetos e mostra que o aumento do tempo de execução é linear com o aumento do número de objetos.

Isso corrobora a sugestão dos autores do Galicia, mostrando que o algoritmo é mais adequado quando ocorrem pequenas atualizações no contexto.

Os algoritmos para extração de regras de classificação foram avaliados sob o ponto de vista de tempo de execução e de precisão de classificação.

Foram utilizados contextos com densidades diferentes, com números de objetos diferentes e contextos que são amplamente utilizados na comparação de métodos de classificação, como os contextos 11 a 1referentes ao problema dos monges.

Desempenho do Galicia para extração de regras de associação variando-se o número de objetos e fixando-se o suporte em 2% e confiança em 50%.

Os contextos, em geral, foram divididos em conjunto de treinamento, utilizado para a obtenção das regras, e conjunto de validação, utilizado na avaliação da precisão dos métodos para classificar novos objetos.

Os contextos foram divididos de forma que 40% dos objetos fazem parte do conjunto de treinamento e 60% parte do conjunto de validação.

Uma única exceção para essa divisão entre conjunto de treinamento e conjunto de validção refere-se aos contextos 11 a 13, pois eles apresentam-se já divididos nos dois conjuntos no repositório.

A presenta os resultados para o GRAND e Rulearner.

A tabela apresenta as precisões dos métodos, o número de regras extraídas para cada contexto e os tempos de execução.

Em média, os algoritmos apresentaram a mesma precisão, 72,4% para o GRAND contra 73,52% para o Rulearner e, também, os mesmos tempos de execução, 11596,95 segundos para o GRAND contra 10939,19 segundos para o Rulearner.

O desempenho dos algoritmos foi satisfatório na maioria dos casos.

No entanto, para o contexto 2, o resultado obtido foi ruim.

Além do alto tempo para a obtenção das regras, a precisão na classificação de novos objetos ficou abaixo de 50%.

Durante a transformação da base de métodos contraceptivos em contexto formal, os atributos numéricos foram desprezados.

Esses atributos podem ter grande influência para a classificação dos objetos.

Entretanto, como, em contextos formais, só podem haver atributos binários, eles foram descartados.

Esse pode ser um dos motivos que levaram os algoritmos a obter tão baixa precisão.

Outra importante observação acerca da efere-se ao tempo de execução dos algoritmos.

Observe que os algoritmos são extremamente sensíveis ao número de objetos no contexto.

Os piores resultados de ambos em relação ao tempo de execução foram obtidos com os contextos e 1com, respectivamente, 147e 1728 objetos.

Esses algoritmos são incrementais assim como o Galicia e, portanto, são ineficientes para a computação de regras com bases de dados com muitos objetos.

Já quanto à relação tempo de execução e densidade, o GRAND mostrou-se mais sensível ao aumento da densidade.

Tomando-se como exemplo o contexto 1, o GRAND foi cerca de 8 vezes mais lento que o Rulearner.

Neste trabalho, foram comparadas técnicas baseadas em Análise Formal de Conceitos para extração de regras de bases de dados.

Foram comparadas técnicas para extrair implicações, dependências funcionais, regras de associação e regras de classificação.

Os reticulados conceituais permitem a extração de regras determinísticas e probabilísticas.

As regras determinísticas são implicações e dependências funcionais.

Já as regras probabilísticas são regras de associação e regras de classificação.

O capítulo dedicou-se à apresentação das regras determinísticas.

Nesse capítulo, foram formalizadas as definições de implicações e dependências funcionais.

Mostrou-se também que bases de dados relacionais podem ser transformadas em contextos formais, assim, os métodos baseados em AFC podem ser utilizados para a extração de dependências funcionais.

No capítulo 3, foi demonstrado também que o problema da descoberta de dependências funcionais em contextos multivalorados é equivalente ao problema da descoberta de implicações em contextos univalorados.

Foram apresentadas duas técnicas com as quais os algoritmos baseados em AFC, inicialmente projetados para descoberta de implicações, podem ser utilizados para a extração de dependências funcionais.

A primeira técnica consiste na transformação de contextos.

A segunda sugere a utilização de novos operadores de fecho obtidos com a composição dos chamados operadores de particionamento.

No capítulo 3, ainda são apresentadas algumas coberturas de regras determinísticas.

Coberturas de regras são conjuntos de regras com propriedades específicas.

Foram apresentados diversos tipos de coberturas cujas regras possuíam características distintas.

Seguindo, foram apresentados, no capítulo 3, quatro algoritmos para a extração de regras determinísticas.

Os algoritmos que foram, inicialmente, desenvolvidos para a extração de implicações, puderam, também, ser usados para a extração de dependências funcionais utilizando-se as técnicas comentadas anteriormente.

Os algoritmos apresentados foram o Next Closure, o Find Implications, o Impec e o Aprem-IR.

O Next Closure extrai uma cobertura mínima de regras.

O algoritmo foi proposto inicialmente para a descoberta de conceitos formais de contextos formais e foi, em seguida, adaptado para a extração de implicações.

O Find Implications extrai uma cobertura reduzida e redundante de regras.

Ele baseia-se na construção de reticulados conceituais.

Como apresentado, ele executa uma busca em largura no reticulado conceitual para a extração das implicações.

Já o Aprem-IR e o Impec extraem uma cobertura própria.

Esses algoritmos foram propostos com o intuito de extrair regras que fossem mais legíveis para usuários finais leigos.

A base mínima, apesar de fornecer o menor número de regras possível, faz com que a análise das regras seja difícil, necessitando a aplicação de axiomas para a descoberta das relações entre os atributos.

A cobertura própria torna os relacionamento mais explicítos.

Por outro lado, ela aumenta o número de regras na base.

Apesar de tanto o Aprem-IR quanto o Impec fornecerem o mesmo tipo de base, eles baseiam-se em conceitos diferentes.

O Aprem-IR utiliza reticulados conceituais para a extração das regras e o Impec o faz diretamente de contextos formais.

Finalizando o capítulo 3, os algoritmos foram comparados sob o ponto de vista prático.

O desempenho dos algoritmos foi investigado variando-se a densidade de contextos e o número de objetos.

Além disso, o desempenho dos algoritmos para a extração de dependências funcionais foi avaliado tanto com a utilização dos operadores de particionamento quanto com a transformação de contextos.

Observou-se que os algoritmos são sensíveis ao aumento da densidade dos contextos, principalmente os métodos baseados em reticulados conceituais como o Aprem-IR e o Find Implications.

O desempenho desses algoritmos foi afetado pelo aumento da densidade pois, ao aumentar a densidade do contexto, ele se aproxima do pior caso quando, do contexto, são obtidos 2|G|conceitos.

Um contexto gera um número exponencial de conceitos se ele for equivalente ao contexto (G,G,6=).

A densidade do contexto (G,G,6=) é de 75%.

Logo, ao aumentar a densidade dos contextos, eles se aproximam do contexto (G,G,6=) o número de conceitos aumenta, conseqüentemente, aumenta o tempo de execução dos algoritmos baseados em reticulados conceituais.

Os algoritmos, entretanto, mostraram-se menos sensíveis ao aumento do número de objetos.

Os melhores desempenhos foram alcançados com o Aprem-IR e o Next Closure que tiveram tempo de execução abaixo de 1 minuto mesmo para contextos com grandes quantidades de objetos.

O pior desempenho obtido foi com o Find Implications.

O algoritmo mostrou-se muito sensível ao aumento de objetos no contexto.

Analisando-se os resultados dos algoritmos tanto para a variação da densidade dos contextos quanto para a variação da quantidade de objetos, constatou-se que, em geral, os métodos que utilizam reticulados conceituais diretamente na extração de implicações são melhores para contextos densos.

Observou-se também que o Next Closure obteve o melhor desempenho tanto para variação da densidade quanto para a variação do número de objetos, embora não utilize reticulados conceituais diretamente.

O desempenho do Next Closure foi surpreendente já que o algoritmo é o mais simples entre os apresentados.

Os mesmos algoritmos foram comparados para a extração de dependências funcionais utilizando-se tanto a transformação de contextos quanto os operadores de particionamento.

Contudo, foi constatado que o Find Implications não poderia ser usado para a extração de dependências funcionais utilizando os operadores de particionamento já que o algoritmo utilizado para a construção do reticulado conceitual baseia-se nos operadores de derivação.

Com isso, o algoritmo deveria ser modificado para que os novos operadores de fecho pudessem ser utilizados.

Entretanto, optou-se por não modificar o algoritmo e, assim, o Find Implications não pôde ser testado com os operadores de particionamento.

Os algoritmos foram avaliados com contextos com diferentes quantidades de objetos e atributos.

Observou-se que, em geral, o desempenho dos algoritmos com os operadores de particionamento foi inferior em relação à transformação de contextos.

Esse fato deve-se ao alto custo computacional dos operadores de particionamento quando comparados ao custo dos operadores de derivação.

Além do custo dos operadores, outro fator que influenciou o desempenho dos algoritmos utilizando os operadores de particionamento foi o fato deles terem sido projetados com base nos operadores de derivação.

Isso pode ser observado quando compara-se os tempos do Next Closure utilizando os operadores de particionamento e a transformação de contextos.

Ele foi o algoritmo que obteve as menores diferenças entre tempo de execução com a transformação de contextos e com os operadores de particionamento, seguido do Aprem-IR e do Impec.

A comparação entre o tempo de execução com a transformação e com os operadores de particionamento também revelou que o Aprem-IR, embora tenha sido projetado para funcionar com qualquer operador de fecho, obteve melhores resultados quando utilizado com os operadores de fechos obtidos pela composição dos operadores de derivação.

Ao investigar o impacto do aumento do número de atributos no tempo de execução dos algoritmos, constatou-se que o Impec foi o algoritmo mais sensível a tal mudança tanto com a utilização da transformação de contextos quanto com os operadores de particionamento.

Em média, o aumento do número de atributos de 7 para 10 causou um aumento de 60 vezes no tempo de execução quando utilizou-se a transformação de contextos e 5vezes quando utilizouse os operadores de particionamento.

Os piores resultados são obtidos quando o número de objetos é pequeno o pior resultado para a transformação de contexto ocorreu com 50 objetos e para os operadores de particionamento ocorreu com 100 objetos.

O algoritmo que menos sofreu com o aumento do número de atributos utilizando a transformação de contexto foi o Find Implications que, em média, teve aumento no tempo de execução de vezes.

Já com a utilização dos operadores de particionamento ocorreu empate entre os algoritmos que obtiveram o melhor resultado o Next Closure e o Aprem-IR tiveram aumento de 17 vezes em média.

Isso mostra a superioridade dos métodos que utilizam reticulado conceituais diretamente para a extração de dependências funcionais.

Analisando-se o desempenho dos algoritmos tanto para a extração de implicações quanto para a extração de dependências funcionais percebe-se que, embora seja possível, o uso dos algoritmos para a extração dependências funcionais não é adequado.

Mesmo em testes com poucos atributos e objetos, o desempenho dos algoritmos para a extração de dependências funcionais não é satisfatório.

Isso revela a tendência constatada na literatura da falta de pesquisa de métodos baseados em AFC dedicados à extração de dependências funcionais.

Em geral, os autores em suas pesquisas desenvolvem métodos para a extração de implicações e sugerem a possibilidade de sua utilização para a extração de dependências funcionais através das duas técnicas apresentadas aqui.

Contudo, como verificado nesse trabalho, essas técnicas não mostraram-se eficazes.

Logo, verifica-se a necessidade de pesquisas para o desenvolvimento de novas técnicas que permitam mapear o problema da extração de dependências funcionais no problema da extração de implicações ou para o desenvolvimento de algoritmos para a extração direta de dependências funcionais a partir de contextos formais.

Após apresentar tópicos relacionados às regras determinísticas, no capítulo 4, foram apresentadas as regras probabilísticas.

Quatro algoritmos para a extração de regras de associação foram apresentados, o AClose, o Titanic, o Frequent Next Neighbours e o Galicia.

Os dois primeiros foram desenvolvidos com base no algoritmo Apriori.

O Frequent Next Neighbours assemelha-se ao algoritmo Find Implications e utiliza reticulados conceituais para a extração das regras.

O algoritmo Galicia é um algoritmo incremental e também utiliza reticulados conceituais para extração das regras.

No entanto, ele difere-se do Frequent Next Neighbours por utilizar o reticulado conceitual apenas para encontrar o conjunto de itens freqüentes, ao contrário do FNN que utiliza a estrutura do reticulado para encontrar também as regras.

Após a apresentação dos algoritmos para a extração de regras de associação, foram apresentados os algoritmos GRAND e Rulearner para a extração de regras de classificação.

Existem outros algoritmos baseados em AFC para classificação além dos dois apresentados.

Todavia, a maioria deles utiliza a estrutura do reticulado para classificar.

Como o interesse era em algoritmos que produzissem regras de classificação, optou-se por não utilizá-los.

Além desses métodos para classificação, os algoritmos para a descoberta de regras de associação também podem ser usados para a extração de regras de classificação.

Mais uma vez, como eles não foram desenvolvidos com o intuito de classificar, eles não foram comparados com tal propósito.

Os algoritmos para a extração de regras de associação foram comparados, assim como os de implicações, sob a variação da densidade de contextos e, também, sob a variação no suporte mínimo das regras.

Foram utilizadas bases de dados sintéticas e reais.

As sintéticas foram geradas conforme metodologia proposta por Agrawal e as reais foram retiradas do repositório de dados da Universidade da Califórnia em Irvine.

O algoritmo Galicia não foi, em geral, comparado com os demais por ser um algoritmo incremental e, com isso, demandar muito tempo de execução com contextos maiores.

O Galicia, como recomendado por seus próprios autores, é mais adequado quando a base de dados é alterada com a inclusão de novas transações (objetos).

A variação da densidade dos contextos, em geral, influenciou negativamente no desempenho dos algoritmos.

À medida que a densidade dos contextos aumenta, o tempo de execução também aumenta.

O algoritmo que obteve o pior desempenho com o aumento da densidade foi o Titanic, seguido do AClose e FNN.

Os dois primeiros, como já mencionado, foram desenvolvidos com base no Apriori.

Agrawal, ao apresentar o algoritmo, afirmou que o Apriori era indicado para bases de dados esparsas.

Com isso, naturalmente, tanto o Titanic quanto o AClose têm o tempo de execução aumentado com o aumento da densidade dos contextos.

Obviamente, o FNN também sofre com o aumento da densidade dos contextos.

Como já mencionado, o maior número de conceitos é obtido quando a densidade do contexto chega a 75% e o contexto assemelha-se ao (G,G,6=).

Logo, o aumento no tempo de execução do FNN com o aumento da densidade do contexto é retardado para contextos com altas densidades.

Esse fato foi confirmado quando se comparou os algoritmos com outro contexto denso como o das doenças hepáticas.

Entretanto, o bom desempenho do FNN não se manteve quando ele foi submetido a contextos com muitos objetos.

Ao comparar os algoritmos submetendo-os a contextos com muitos objetos, o desempenho do FNN foi o pior entre ele, o Titanic e o AClose.

O AClose obteve o melhor desempenho com os contextos com 10000 e 50000 objetos.

Porém, o Titanic também obteve desempenho satisfatório com esses contextos.

Ao se diminuir o suporte das regras, o tempo de execução do AClose e do Titanic mantiveram-se praticamente constantes, com pequeno aumento quando o suporte aproximou-se de zero.

Já o FNN apresentou crescimento quadrático no tempo de execução com a variação do suporte.

Constatou-se que ao diminuir o suporte o supremo-semi-reticulado construído pelo FNN aproxima-se do reticulado conceitual referente ao contexto.

Assim, o tempo de execução do FNN é afetado pela construção do reticulado.

Os testes revelaram a adequação de cada algoritmo a situações específicas.

Enquanto o Titanic e o AClose adequam-se melhor a bases esparsas, os algoritmos baseados em reticulados conceituais, especificamente, o FNN é mais adequado a bases de dados densas sendo menos sensíveis à variação do suporte das regras para essas bases.

O Galicia foi analisado submetendo-o a contextos com diferentes densidades e quantidades de objetos.

Em relação à variação de densidade, o tempo de execução teve o pior caso quando a densidade aproximou-se de 55%.

Isso retrata que o tempo de execução do algoritmo para a extração de regras de associação é afetado principalmente pela construção do reticulado conceitual como já era esperado.

Como o algoritmo é incremental, os conceitos formais são criados à medida que novos objetos são adicionados.

Em contextos densos, a similaridade dos objetos aumenta.

Logo, são necessárias menos operações para atualizar o contexto.

Em relação à variação na quantidade de objetos no contexto, o tempo de execução do Galicia aumentou linearmente com o aumento do número de objetos.

Mesmo para pequenas quantidades de objetos, o tempo de execução do algoritmo foi alto.

Isso confirma a adequação do algoritmo para ser utilizado apenas quando ocorrem pequenas variações na quantidade de objetos do contexto.

Os algoritmos para extração de regras de classificação foram comparados utilizando-se bases de dados reais.

As bases escolhidas são amplamente utilizadas na comparação de métodos para classificação.

As bases, quando não estavam divididas em conjunto de treinamento e validação, foram divididas de forma que 40% dos objetos, escolhidos aleatoriamente, pertenciam ao conjunto de treinamento e o restante ao conjunto de validação.

Os algoritmos foram avaliados não só sob o ponto de vista do tempo de execução mas também sob o ponto de vista da precisão das regras produzidas.

Os algoritmos apresentaram precisão média de 73% e tempo de execução médio de cerca de 11000 segundos.

O desempenho dos algoritmos foi, em geral, satisfatório tanto do ponto de vista do tempo de execução quanto da precisão.

Contudo, eles não obtiveram bom desempenho para a base de dados de métodos contraceptivos quando apresentaram o maior tempo de execução e a pior precisão na classificação de novos objetos.

Essa base apresenta atributos não-binários como, por exemplo, a idade das pessoas.

Esses atributos foram removidos da base quando ela foi transformada em contexto formal.

Essa pode ter sido a causa do desempenho ruim dos algoritmos.

Esse fato revela também uma deficiência não só dos métodos de extração de regras de classificação baseados em AFC mas também de toda a técnica, o fato dela trabalhar apenas com atributos binários.

Isso restringe a utilização dos métodos a situações em que os atributos são binários ou forçam a discretização de atributos com valores contínuos o que nem sempre conduz a resultados satisfatórios.

Sob o ponto de vista do tempo de execução, o desempenho dos algoritmos não foi satisfatório.

Os algoritmos são incrementais e, com isso, são extremamente sensíveis ao aumento do número de objetos no contexto.

Isso demonstra certa falta de pesquisa de métodos baseados em AFC para a classificação.

Em geral, os algoritmos são antigos, propostos na década de 80 e 90 e por isso obtiveram tempo de execução tão altos.

Contudo, a precisão deles foi satisfatória e comprovou que reticulados conceituais podem ser utilizados para classificação.

Finalmente, apresenta um resumo das características do dez algoritmos retromencionados.

Construiu-se um reticulado conceitual a partir de um contexto formal em que os objetos são os algoritmos e os atributos suas características.

As características consideradas são a adequação do algoritmo a contextos formais densos e esparsos ou com muitos ou poucos objetos, se ele usa o contexto formal ou o reticulado conceitual para extrair as regras, o tipo de regra extraído (implicações, dependências funcionais, regras de associação ou classificação), o tipo de cobertura extraída no caso de implicações e dependências funcionais.

Se é possível usar transformação de contextos ou operadores de particionamento para extrair dependências funcionais, e se o algoritmo usa os atributos-classe ou funções de classificação para gerar regras de classificação.

