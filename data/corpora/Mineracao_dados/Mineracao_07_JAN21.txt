A gestão eficiente de qualquer corporação, seja ela do setor público ou privado, industrial ou varejista, requer planejamento.

Para um planejamento efetivo é necessário que se tenha uma expectativa precisa das condições futuras em que a corporação irá operar, e de como se relacionam os elementos condicionantes desta expectativa.

O governo de uma cidade deve saber qual a expectativa de crescimento populacional e qual o relacionamento deste com o aumento na demanda por serviços, para ampliar, entre outras coisas, sua infra-estrutura básica, como redes de água, luz e esgoto.

Uma indústria deve antecipar qual a demanda de seus produtos para agendar sua produção, por exemplo.

E deve saber também os principais fatores que afetam esta demanda, para tomar as decisões corretas quando necessário.

Tanto decisões estratégicas como operacionais de uma corporação requerem, portanto, explorar o relacionamento presente entre os elementos que compõem a realidade em que a corporação está inserida.

Para apoiar decisões corporativas como as citadas acima, as empresas procuram criar sistemas e procedimentos a fim de explorar cenários, com base em informações quantitativas e/ou qualitativas.

Com o avanço da tecnologia e da capacidade de armazenagem e processamento dos sistemas computacionais, diversos modelos e técnicas quantitativos de previsão têm sido pesquisados, complementando e aprimorando as análises qualitativas por uma série de fatores, incluindo maior precisão.

Modelos de previsão quantitativos utilizam-se basicamente de dados históricos para detectar padrões de comportamento e estimá-los no futuro.

Tais modelos empregam ferramental matemático-estatístico para representar a realidade para a qual foram criados.

Diversas técnicas estatísticas têm sido usadas na criação dos modelos, baseadas em diferentes pressupostos assumidos.

Este trabalho visa explorar o uso de técnicas relativamente novas para criação de modelos voltados à detecção de padrões e previsão de demanda no varejo.

Estas técnicas avançadas, comumente chamadas de data mining ou mineração de dados, incluem métodos de inteligência artificial, árvores de decisão, métodos de indução de regras, redes neurais, entre outras.

Essas técnicas consistem na descoberta de novas e importantes informações, procurando em certos casos "aprender" a partir de relacionamentos escondidos, padrões, correlações e interdependências existentes em grandes bases de dados.

A evolução do desempenho computacional para exploração e análise de dados através de pacotes estatísticos permitiu o desenvolvimento e uso da mineração de dados de maneira crescente, com sucesso em muitas empresas.

Toda previsão é uma tentativa de prognosticar o futuro através do exame do passado.

Consiste em gerar previsões não enviesadas da magnitude de alguma variável, como vendas, com base no conhecimento presente e passado acumulado em bases de dados e na experiência dos gestores e outros profissionais envolvidos.

Muitas variáveis são comumente previstas por gestores, analistas, acadêmicos, instituições governamentais e outros, como renda, nível de emprego, inflação, produção, criminalidade, custo de vida, gastos, vendas, e muitas outras.

Segundo os autores, dois tipos de reação às previsões podem ser identificados, Preparar-se para as conseqüências advindas das mudanças previstas.

Por exemplo, se o gestor de uma empresa prevê queda nas vendas devido a problemas macroeconômicos, ele deve preparar a organização para enfrentar a contração.

Tomar medidas visando modificar as previsões.

Se o cenário previsto é de queda, o gestor pode decidir enfrentá-la, incrementando ações de venda ou marketing, por exemplo.

O planejamento de ações é comumente realizado de uma forma interativa, planejamentos são utilizados como base para previsões, que por sua vez mostram os resultados prováveis se as ações planejadas forem tomadas.

Com base nos resultados previstos, o gestor pode refazer seu planejamento, considerando novas decisões, que por sua vez geram novos resultados, e assim sucessivamente, até estabelecer uma decisão final ideal.

Ainda segundo os autores, a visão oposta, planejar as ações após as previsões, como, por exemplo, planejar ações de marketing para enfrentar adversidades, não é realista, uma vez que os resultados dependem das ações tomadas.

Assim, o planejamento das ações como o nível de preços ou ações de marketing planejadas devem ser levadas em consideração nas previsões.

As previsões variam quanto à sua aplicação em níveis agregados (como na economi ou para um componente específico (como para uma companhia individual).

Também diferem quanto ao horizonte de previsão curto, médio ou longo prazo são conceitos flexíveis, que variam conforme a aplicação.

Previsões de vendas podem ser construídas para toda a companhia, para produtos ou grupo de produtos particulares, para regiões geográficas ou territórios de vendas específicos, para clientes particulares, ou para combinações dos elementos citados.

Este trabalho está focado no processo acima descrito, de geração de previsões de demanda para tomada de decisões no varejo.

A preocupação central desta pesquisa é na exploração de uma metodologia baseada em técnicas de mineração de dados para identificação de relacionamentos entre a demanda de diversos produtos e os fatores de maior influência destas.

Vislumbrando estes relacionamentos com maior clareza, o gestor fica mais habilitado à tomada de decisões acertadas no gerenciamento de vendas e outras atividades conseqüentes, como gerenciamento de estoques, de fluxo de caixa e outros.

Para que o gestor possa tomar decisões bastante específicas, os dados utilizados devem ser o mais atomizados (individuais) possível, gerando previsões com grande nível de detalhamento, porém pequeno horizonte futuro.

A metodologia explorada busca ser útil, portanto, para tomada de decisões operacionais no varejo e não estratégicas, de longo prazo.

Situações típicas do setor varejista como, o que acontece com a demanda e o lucro de minha loja se aumentar o preço da linha de produtos X, ou meu concorrente cortou preços da ordem de 30%.

Qual a melhor medida a ser tomada podem ser vislumbradas e as possíveis soluções endereçadas com o uso de uma metodologia de prospecção de cenários como a aqui proposta.

Previsões de curto prazo são empregadas para auxiliar decisões que devem ser feitas num futuro próximo, como a determinação do nível de inventário, número de funcionários e produção necessários, capital de giro, suprimentos a serem adquiridos, entre outras.

Por outro lado, em situações onde o impacto das decisões tomadas pode influenciar a posição da companhia nos próximos anos, previsões de longo prazo são requeridas.

Tais situações podem envolver decisões a respeito da construção de uma nova fábrica, a entrada em novos mercados, exportar ou não seus produtos, modernizar as operações, entre outras.

Inúmeros autores já pesquisaram sobre a adoção de modelos quantitativos de previsão de vendas, utilizando-se de diferentes técnicas e abordagens.

Alguns autores pesquisaram a adoção de modelos de previsão de vendas agregadas para toda uma corporação ou loja, por exemplo.

Estes modelos foram desenvolvidos a partir de duas abordagens clássicas, através da modelagem de séries temporais, onde se considera como variável de entrada os valores históricos da variável a ser prevista (a demanda agregada, no caso), ou com o uso de variáveis explicativas daquela a ser prevista como entrada de dados, os prováveis causadores da demanda.

Esta abordagem é conhecida como modelagem causal.

Abordagens de previsão de vendas com uso de dados agregados.

Outros autores exploraram a previsão da demanda de produtos de maneira individualizada, a partir de séries históricas das vendas de cada produto.

Esses estudos utilizaram técnicas de modelagem de séries temporais, tomando como entrada dos modelos os valores históricos das demandas de cada produto no tempo, construindo assim um modelo distinto para cada produto.

Abordagens de previsão de vendas com uso de dados individuais e modelagem de séries temporais.

Ainda outros autores exploraram o impacto de diversas variáveis na previsão de demanda de um produto isolado, realizando, portanto, o que se costuma referir como previsão causal, ou seja, a previsão realizada tendo-se como base as variáveis que possam explicar a previsão.

Esses autores buscaram relacionar as variáveis que possam estar relacionadas com a demanda individual de cada produto, desenvolvendo modelos de previsão que tomam como entrada os valores destas variáveis explicativas da demanda no tempo.

Abordagens de previsão de vendas com uso de dados individuais e modelagem causal.

Poucos autores, contudo, realizaram estudos a fim de desenvolver modelos que incorporassem o relacionamento existente entre os produtos.

Mostra-se a importância do conceito de centros de interesse grupo de produtos que têm alguma relação, seja concorrendo entre si ou complementando-se mutuamente e da sua utilização na gestão da loja.

A gestão por categorias de produtos (que representam centros de interesse dos consumidores) é fundamental para o sucesso de um empreendimento de varejo, e costuma influenciar diretamente a disposição dos produtos na loja.

Alguns autores investigaram sobre o relacionamento entre produtos na previsão de vendas, mas nestes estudos pouco se utilizaram dados históricos sobre os produtos, baseando-se mais em análises exploratórias e pesquisas de grupo.

Vê-se então uma lacuna na bibliografia sobre previsão existente, que é o estudo do relacionamento entre os produtos na previsão de vendas, com a construção de modelos quantitativos que possam incorporar e utilizar-se de informações acerca de mais de um produto simultaneamente.

Mostrou-se com o uso de redes neurais artificiais, ser possível melhorar a previsão de séries temporais de variáveis afins ao incorporá-las em um único modelo, e não realizar as previsões isoladamente.

A exemplo da pesquisa, pretende-se neste estudo explorar uma metodologia de previsão de vendas que, a partir de dados históricos da demanda individual de cada produto pertencente a um centro de interesse, levar em consideração o impacto da demanda ou das ações sobre a demand de cada produto sobre a demanda dos demais.

O modelo deve, portanto, incorporar os relacionamentos existentes entre os produtos de um centro de interesse, utilizando para tanto a técnica de redes neurais artificiais.

Como mostrado, este trabalho visa investigar o relacionamento entre produtos afins na previsão de vendas individual de produtos, e, através da adoção de um modelo causal, averiguar como esta previsão pode ser utilizada para auxílio à tomada de decisões no varejo.

O problema de investigação pode ser centrado na seguinte questão, "Como utilizar o processo de mineração de dados para a descoberta de relacionamentos entre produtos e previsão de vendas individual de produtos para apoio à tomada de decisões operacionais no varejo " O objetivo geral desta pesquisa é explorar a possibilidade de usar uma metodologia capaz de identificar padrões de relacionamento úteis na previsão de vendas individual no varejo, com o uso do processo de mineração de dados.

Essas previsões devem abordar grande parte das decisões de curto prazo enfrentadas no cotidiano do gestor da loja, num nível aprofundado, detalhado quanto a produtos, de decisões.

O objetivo é explorar um modelo de previsão de demanda para os produtos visando identificar um composto de marketing adequado (preços, produtos e promoções).

Os seguintes objetivos específicos podem ser destacados, Explorar a técnica de redes neurais artificiais para a criação de modelos de previsão de vendas individualizadas no varejo.

Desenvolver uma rede neural artificial para previsão de vendas capaz de detectar relacionamentos entre variáveis que impactam no volume de vendas de uma loja ou cadeia de lojas, de forma detalhada (por produto).

Durante o desenvolvimento da metodologia de criação de modelos para previsão de demanda, pretende-se explorar as seguintes questões, Como tratar os dados de vendas de maneira atomizada, sem agregações, e de forma sistemática para serem utilizados em um processo de mineração de dados.

Em outras palavras, como realizar a coleta, filtragem, limpeza e transformações necessárias a um adequado pré-processamento de dados para criação de modelos, em se tratando de dados não agregados.

É possível utilizar as informações a respeito dos centros de interesse de um grupo de produtos em modelos de previsão Em caso afirmativo, esse procedimento, incorporando os relacionamentos entre as demandas de produtos afins melhora os resultados dos modelos de previsão construídos.

A técnica de redes neurais artificiais é adequada para geração de modelos de previsão no varejo.

Esta pesquisa está focada em análises de dados de vendas no varejo, e, portanto, não trata de problemas referentes à demanda em serviços, na indústria primária ou mesmo outros elementos da cadeia de distribuição que não seja aquele de distribuição ao consumidor final.

Outra delimitação importante diz respeito ao horizonte de previsão, como os modelos gerados tratam de dados extremamente atomizados, sua utilização em horizontes amplos torna-se inviável.

A princípio, apenas decisões operacionais do composto de marketing num horizonte de curto prazo são consideradas.

Não é preocupação deste estudo, também, a comparação direta entre modelos de previsão de vendas.

Os modelos gerados foram confrontados com outras modelagens apenas para efeito de confirmação de sua capacidade preditiva mínima.

A preocupação central deste trabalho é com a metodologia utilizada para detecção de relacionamentos entre as variáveis que afetam a demanda no varejo, a fim de dar uma melhor compreensão da realidade em que a loja está sujeita por meio de uma previsão de vendas causal mais acertada.

Esta revisão bibliográfica está dividida em três partes.

Na primeira parte, o problema de previsão de vendas é abordado. São mostradas as diversas iniciativas tomadas ao longo da evolução do tema na tentativa de solucionar o problema, e a abordagem desta pesquisa é situada perante as demais abordagens encontradas na literatura.

Na segunda parte, o processo de mineração de dados é abordado, mostrando algumas das técnicas desenvolvidas até o presente momento e a metodologia de análise envolvida em projetos de mineração de dados.

Finalmente, a terceira parte traz uma revisão da literatura sobre a técnica de redes neurais artificiais e sua aplicação para problemas de previsão.

Para aumentar a vantagem competitiva comercial em um ambiente de constantes mudanças, os gestores de uma organização devem tomar a decisão correta no momento certo, com as informações que tiverem em mãos.

Uma previsão acertada tem papel fundamental nesse processo.

Compreendendo melhor o comportamento do consumidor e sua resposta às alterações nos fatores controláveis que o varejista articula (o composto de marketing), os gestores podem prever os resultados de suas ações e obter dados de diagnóstico para se suprir de informações que permitam ações acertadas no futuro.

Previsão, ou em inglês forecasting, pode ser definida como uma seqüência de passos que o tomador de decisões realiza, seja implícita ou explicitamente, para antever satisfatoriamente um valor futuro.

Entretanto, devido às complexidades do processo decisório em ambientes não estruturados, não é uma tarefa fácil saber quais passos tomar.

Fatores que fizeram com que a importância da previsão aumentasse, Aumento na complexidade das organizações (como o aumento do número de nichos de mercado e de produtos oferecidos) e de seu ambiente mudanças tecnológicas e na estrutura da demand, tornando mais difícil levar em consideração todos os fatores relacionados ao desenvolvimento futuro da organização.

As organizações passaram a adotar procedimentos decisórios mais sistemáticos, que envolvem justificativas explícitas para cada ação tomada.

Ter uma previsão de vendas formal é uma forma de suportar tais procedimentos.

O desenvolvimento contínuo das técnicas de previsão e suas aplicações, permitindo que não só analistas especializados como também gerentes e outros tomadores de decisão entendessem e utilizassem tais técnicas.

Apesar do reconhecimento da importância das previsões entre executivos de todos os setores, sua implementação não é universal existe uma lacuna entre o desenvolvimento teórico das técnicas de previsão e sua aplicação prática nas organizações.

De acordo com o que foi levantado nesta revisão bibliográfica, o uso prático das técnicas de previsão ainda está muito distante dos avanços teóricos, fato comprovado pelo uso maciço de técnicas subjetivas ou de técnicas quantitativas rudimentares.

Se propõe um framework para organizar os estudos empíricos relativos à previsão, que é apresentado abaixo.

O framework distingue três diferentes grupos de problemas, Questões metodológicas (design issues) compreendem o propósito e o tipo de previsão, os recursos envolvidos, as características dos analistas e usuários envolvidos e as fontes de dados utilizadas.

Questões de seleção e especificação dizem respeito às técnicas de previsão e respondem as questões de familiaridade, seleção e uso de métodos de previsão alternativos.

Questões de avaliação focam nos resultados das atividades de previsão, como a apresentação e revisão das previsões, medição da performance da previsão e as forças que afetam sua acurácia.

Os três grupos são interligados de tal forma que cada um deles tem implicações nos demais.

As seções seguintes abordam o problema de previsão segundo o framework proposto.

Para analisar o nível de agregação dos dados, as técnicas de previsão quantitativas existentes podem ser classificadas dentro de quatro categorias básicas, que são, Classificação das técnicas de previsão quantitativas segundo o nível de agregação dos dados.

As técnicas de modelagem voltadas para previsão em marketing podem ser divididas, de acordo com o autor, segundo duas dimensões principais,granularidade individual ou agregad ese estão dirigidos ao estudo de novos produtos ou a sensibilidades a produtos já existentes.

As divisões entre as categorias não são perfeitamente claras e precisas, mas elas de fato formam grupos bem distintos de modelos de previsão.

A seguir são apontadas as principais características dos modelos de previsão de cada um dos quadrantes apresentados, e é destacada a abordagem foco desta pesquisa (tipo I).

Modelagens Tipo I Modelos de Varredura de Dados Há muito tempo os pesquisadores estudam séries históricas de dados de produtos existentes para compreender melhor as respostas às variáveis do composto de marketing, particularmente preço e propaganda.

Para realizar estudos sobre dados atomizados (individuais), faz-se necessária uma análise longitudinal de uma grande quantidade de dados capazes de representar o comportamento dos consumidores, caracterizando o tipo de estudo que costuma ser referido como varredura de dados, ou scanner data analysis.

Este tipo de análise pode ser utilizado a fim de obter o composto de marketing otimizado (que resulta em maior faturamento ou lucro, por exemplo) em ambientes altamente competitivos, entender fontes de heterogeneidade de consumidores, e para estudar as relações entre o ponto de venda e a indústria em diferentes segmentos.

Diversas técnicas têm sido utilizadas neste tipo de modelagem, entre elas modelos de escolha discreta, árvore de decisões, técnicas de regressão linear múltipla e análises de séries temporais.

Todas elas demandam um grande esforço do analista para que possam ser adaptadas para trabalharem com dados individualizados, e os resultados nem sempre são satisfatórios.

Uma das técnicas para previsão desagregada que vem sendo utilizada com sucesso é a modelagem com uso de redes neurais artificiais, tendo mostrado superioridade sobre várias outras modelagens, em diversas áreas do conhecimento.

Dentro da classificação proposta, este trabalho localiza-se nesta categoria de previsão, modelagens tipo I, pois está focado no desenvolvimento e avaliação de modelos para o estudo individual das sensibilidades de mercado às variáveis do composto de marketing para cada produto.

Ou seja, dentro de um ambiente de varejo, este trabalho propõe uma análise longitudinal de grande quantidade de dados históricos, a fim de desenvolver um modelo quantitativo capaz de prever a demanda futura de um grupo de produtos de forma individualizada não agregad.

A fim de melhor localizar o presente projeto perante as demais abordagens de previsão segundo o nível de agregação dos dados, as demais tipologias são brevemente apresentadas.

Modelos Econométricos de Comportamento do Mercado Modelos econométricos e de análise de séries de tempo são enquadrados nesta categoria.

Apesar de os modelos de comportamento individual serem considerados modelos econométricos, e os modelos de difusão de inovações serem baseados em séries de tempo, esta categoria é voltada àqueles modelos mais genéricos, desenvolvidos principalmente no campo da econometria e estatística econômica, e então utilizados na área de marketing.

Os modelos econométricos surgiram efetivamente para prever índices econômicos, como o próprio nome diz, e foram posteriormente adaptados para previsões microeconômicas, e baseiam-se principalmente em técnicas estatísticas de regressão.

O uso deste tipo de modelo supõe a construção das equações matemáticas que o regem, o que envolve a seleção a priori de todas as variáveis a ser consideradas e do tipo de relacionamento existente entre elas.

Já a análise de séries temporais é o processo de avaliar os relacionamentos históricos entre uma variável e o tempo.

O propósito básico é identificar comportamentos periódicos e utilizá-los para prever séries futuras.

Exemplos típicos da utilização destes modelos são a previsão de tendências de mercado como a taxa de retração de uma indústria ou setor de índices macroeconômicos como inflação e crescimento do PIB de vendas agregadas em uma empresa, como o total de demanda anual ou mensal dos efeitos de variáveis do composto de marketing na demanda total, exame de canais de distribuição, entre outros.

São modelos calibrados com dados de pré-lançamento, utilizando para tanto dados individualizados de respondentes selecionados.

Os modelos assim desenvolvidos são geralmente voltados ao estudo da participação e penetração no mercado para novos produtos, e se utilizam basicamente da análise de escolhas discretas.

Outras possibilidades são a estimação do tamanho de um novo mercado ou de sua transformação com a entrada de um novo produto ou marca.

O uso típico destes modelos é a aplicação em experimentos com a finalidade de criar uma função-utilidade para situações de escolhas baseadas em menus, ou seja, em seleções pré-definidas de itens ou benefícios em um produto.

Podem ser utilizadas técnicas baseadas em julgamento, em técnicas estatísticas como análise conjoint ou modelos de escolha discreta econométricos tipo multinomial logit ou probit, desde que utilizem dados desagregados.

Observam-se aplicações destes modelos em diversas áreas além da análise de penetração de mercado de novos produtos, como na seleção de cesta de produtos para venda casada, determinação de visitação a um evento, e ainda na previsão de demanda de transporte, para, por exemplo, selecionar qual infra-estrutura viária deve ser construída ou qual meio de transporte coletivo implantar, entre outros.

Os modelos para previsão de difusão de inovações surgiram na década de 60.

Esses modelos foram criados com base na analogia da idéia de propagação de doenças aplicada ao lançamento de novos produtos ou tecnologias, a taxa de adoção inicialmente cresce rapidamente com o efeito de contágio provocado por aqueles que já adotaram a inovação, e posteriormente decresce com o efeito de saturação e diminuição do mercado disponível.

O processo de difusão consiste de quatro elementos básicos, inovação, canais de comunicação, tempo e sistema social.

Como uma teoria de comunicação, a teoria de difusão foca nos canais de comunicação.

Estes consistem em dois meios fundamentais e distintos,mídia de massa ecomunicações pessoais, ou "boca-a-boca".

O modelo de Bass assume então que os adotantes estão subdivididos em dois grupos, aqueles influenciados apenas pela mídia e aqueles influenciados apenas pelo boca-a-boca, os "inovadores" e os "imitadores", respectivamente.

O modelo de Bass e suas formas revisadas têm sido usados para prever a difusão de inovações no varejo, tecnologia industrial, agricultura, educação, farmácia e mercados de bens duráveis.

O objetivo de um modelo de difusão é prever o nível de uso de uma inovação ao longo do tempo em uma população de possíveis adotantes, e serve basicamente para prever as vendas de inovações consideradas "primeira compra".

A principal equação do modelo de Bass pode ser escrita como, onde, n(t) é o número de novos adotantes no período.

Onde, p é o coeficiente inovação ou coeficiente de influência externa.

Onde, q é o coeficiente de imitação, ou coeficiente de influência interna.

Onde, m é o potencial de adotantes totais.

Na equação, nota-se que um dos termos representa a adoção por consumidores não influenciados pela imitação-os inovadores-e um segundo termo representando a adoção dos imitadores.

Além de permitir previsões de venda antes do lançamento para os primeiros estágios do ciclo de vida-quando as compras de reposição ainda não ocorreram-estes modelos também são usados após o lançamento da inovação, sendo este o maior desafio identificado na literatura.

Quando calibrados com base em dados históricos, os modelos de difusão só são estáveis após o ponto de máximo absoluto da taxa de vendas.

Entretanto, quando ajustados com dados externos, a calibração pré-lançamento pode ser alcançada.

É interessante notar que, mesmo depois de mais de trinta anos de pesquisas e desenvolvimento dos modelos de inovação, um dos primeiros modelos-aquele proposto por BASS -continua a ser extensamente utilizado como referencial teórico, e os resultados práticos de sua aplicação em previsões têm-se mostrado com desempenho igual ou superior aos modelos mais recentes e sofisticados.

Revisão completa dos modelos de inovação, seus avanços e conclusões.

Propósito, uso, freqüência e horizonte de tempo das previsões Diversos estudos analisam o porquê de se realizar previsões e seu uso.

64% dos respondentes afirmaram que o principal propósito das previsões é formular um objetivo, uma performance a ser alcançada, enquanto apenas 30% gostariam de obter uma medida realista do potencial do mercado.

Pesquisou-se sobre as principais áreas de aplicação das previsões, e os resultados indicam planejamento de produção, inventários e orçamento como as áreas decisórias mais influenciadas pelas previsões.

Em muitos casos, os gestores confundem previsão com metas a serem alcançadas, ou seja, previsão de vendas com planejamento de vendas, o que gera freqüentemente mudanças nos resultados das previsões por motivos externos, como motivações políticas e estratégicas da organização.

Grandes firmas de varejo utilizam técnicas de previsão de vendas com maior freqüência que os pequenos varejistas, entre as empresas grandes, 100% afirma preparar previsões ao menos uma vez por ano, número este que cai para 93,2% entre as empresas pequenas.

As freqüências de preparação das previsões são segundo os autores, nesta ordem, anual (a mais comum), quadrimestral, semestral e mensal (a menos comum).

Quanto ao horizonte de tempo utilizado nas previsões, há pouco consenso entre os autores, tanto no que se refere ao desempenho das técnicas quanto às próprias definições de curto, médio e longo prazos.

Estudos já foram conduzidos para previsões com horizonte de tempo de um dia até 25 anos.

Os estudos relacionados ao horizonte de tempo e freqüência das previsões confirmam, no entanto, a proposição de WHITE, de que as "companhias utilizam a freqüência que melhor se adapta aos seus produtos, mercado e método de operação.

Não há uma freqüência de previsão ideal".

Parece não haver um consenso sobre a responsabilidade na preparação das previsões, isto é, de maneira geral não há nas organizações um cargo de analista de previsões fixo.

Apenas uma em cada cinco empresas possui equipe específica para planejamento e previsão.

Em metade das empresas essa função está a cargo da alta administração, e em cerca de 15% das companhias a responsabilidade não é definida.

Essa função de analista, ou seja, quem prepara as previsões combina as disciplinas de economia, matemática, estatística, contabilidade e, mais recentemente, computação.

Seu objetivo é tentar explicitar através de equações ou modelos matemático-computacionais seus pressupostos (teorias) e processos lógicos sobre a realidade, tendo a vista a obtenção de previsões sobre determinadas grandezas.

Há observações que indicam falta de comunicação entre usuários e analistas, e uma falta de preparo na elaboração de previsões efetivas, mesmo com a crescente facilidade na obtenção de análises oferecida por pacotes estatísticos comerciais à disposição atualmente.

Há ainda grande disparidade na percepção do status e necessidades das previsões entre os analistas e os usuários, o que causa análises desconexas da necessidade real do gestor e, portanto, freqüentemente ignoradas.

Os gestores responsáveis pelas previsões em empresas de serviço e varejo tendem a ter formação educacional mais baixa que aqueles de empresas industriais.

O autor afirma ainda que os gestores de empresas industriais estão mais familiarizados com técnicas quantitativas relativamente complexas que os gestores de empresas de serviço e varejo.

Outra conclusão relacionada diz respeito à correlação positiva encontrada entre o nível de escolaridade dos analistas e o uso de técnicas de previsão mais sofisticadas, o que ajuda a explicar a maior importância e aplicação dada pelas empresas industriais à previsão de vendas, em comparação às empresas de varejo.

Grandes empresas industriais reconhecem a importância da previsão de vendas e empregam recursos na elaboração de previsões formais e regulares.

As pequenas empresas também costumam utilizar com maior freqüência técnicas subjetivas, em oposição às grandes companhias, que utilizam técnicas quantitativas mais sofisticadas com maior freqüência.

Os responsáveis pelas previsões tendem a ser analistas de negócios, ou seja, analistas com uma visão muito mais ampla do que simplesmente prover os gestores com números relativos a previsões acuradas.

A combinação de apreciação estatística e conhecimento do mercado é necessária para tal análise, o que resulta em uma previsão de vendas que pode ser utilizada na confecção de planos de vendas, de marketing, de produção, de compras e finalmente de negócios.

Essas conclusões fortalecem a carência latente dos segmentos ligados ao varejo em conhecimento sobre as técnicas de planejamento e previsão.

Também corroboram para a idéia de que o analista tem a função de definir e medir os elementos do mercado que direcionam o negócio e não de simplesmente tentar adivinhar o futuro provendo a alta administração com análises de suporte à decisão acionáveis, capazes de influenciar os objetivos de vendas.

Uma expressão comum entre os analistas e estatísticos é, em inglês, Garbage In, Garbage Out, ou "entra lixo, sai lixo".

Ela expressa bem o sentido de que nenhum modelo é capaz de gerar boas previsões sem a presença de dados confiáveis e, principalmente, que representem o problema em questão.

O problema reside no fato de que os sistemas voltados à operação da empresa nem sempre enfatizam a presença de dados "limpos" e confiáveis.

Algumas vezes, os dados disponíveis não são adequados ao que se quer prever.

Por exemplo, na previsão de demanda, muitas vezes só se dispõe de dados de entregas, e estas não são uma boa representação do fenômeno de interesse a demanda.

A presença de dados confiáveis e que representem o fenômeno estudado é um dos fatores de maior ganho em acurácia.

Normalmente, demanda-se um grande esforço de modelagem para um pequeno ganho de acurácia, porém dados que melhor representem o problema geram grandes ganhos, muitas vezes com pouco esforço.

Para a preparação de previsões tanto agregadas como individuais diversas fontes de dados podem ser utilizadas.

Normalmente, elas são divididas em dois grandes grupos de fontes, internas e externas.

Fontes internas de dados são aquelas, como o próprio nome diz, internas à companhia, e muitas vezes estão armazenadas em diversas bases de dados nos departamentos ou setores da empresa.

Para ter uma fonte interna de dados maciça, é necessário manter um registro de todas as atividades do composto de marketing, incluindo modificações realizadas nos produtos, promoções, políticas internas, mudanças ambientais e da concorrência, já que modelos quantitativos podem utilizar todos esses dados, muitas vezes com ganhos de acurácia aliados a pouco esforço adicional para coleta de tais informações.

Os dados internos podem ter origem nos canais de venda da companhia, como dados históricos de vendas em informações coletadas com a força de vendas (quantitativas e qualitativas), em dados dos centros de distribuição e dos pontos de venda final.

Naturalmente, estes últimos são os mais indicados para previsões de venda, já que refletem a demanda real dos produtos, sem distorções devido a reposições de estoque, devoluções, diferenças de tempo entre distribuição e demanda final, entre outras.

As fontes externas de dados mais utilizadas são aquelas que dizem respeito aos fatores macroeconômicos que possam influenciar nas previsões, como índices governamentais, tendências político-econômicas, estatísticas financeiras e sócio-econômicas.

Também é comum a compra de bases de dados externas para se obter dados da concorrência, dados econômicos e do setor, promoções, tendências de preços, de lançamento de produtos e tecnológicas, entre outros.

É freqüente a presença de distorções nos dados disponíveis, que podem ter várias origens, entre elas, Presença de outliers.

Podem ser resultado de ações de marketing extraordinárias, ou fenômenos externos com efeito apenas sobre alguns pontos da série (de curta duração).

Normalmente, a ação de outliers é de difícil detecção pelos modelos.

Presença de cotas de venda.

Os vendedores manipulam suas vendas para atingir as cotas, e muitas vezes prorrogam as vendas para ficar mais fácil atingir as cotas do próximo período, ou realizam promoções agressivas para atingir as metas do período.

Este fenômeno é conhecido como game playing.

Séries de dados contaminadas com dados de fenômenos concorrentes, como o caso da demanda representada pelas entregas ou produção, ao invés de vendas propriamente ditas.

Nestes casos as previsões terão acurácia invariavelmente baixa, e a única solução é tentar aproximar os dados do que se quer estudar de fato.

Por exemplo, pode-se inserir o valor de pedidos atrasados ou adiantados a fim de aproximar os dados de entregas da demanda verdadeira.

Vendas gerenciadas por "contas".

As contas podem não representar a demanda real, uma vez que também estão sujeitas ao game playing.

Estratégias para fugir de taxas e impostos, como a produção acelerada no último mês do ano.

Estas devem ser detectadas e amenizadas.

Políticas empresariais diversas, por questões legais, de marketing ou outras.

Inúmeras técnicas de previsão já foram desenvolvidas na tentativa de melhor elucidar o problema de previsão de vendas.

Tais técnicas podem ser divididas em dois grupos principais de abordagem, quantitativo e qualitativo.

Dentro de cada grupo, várias subdivisões distintas já foram propostas na literatura para classificar as técnicas de previsão.

Propoem-se uma nova taxonomia das técnicas de previsão, buscando classificá-las primeiramente de acordo com sua abordagem conceitual, em seguida por sua metodologia de coleta e tratamento dos dados, e finalmente distinguindo as diferentes técnicas de análise de dados dentro de cada abordagem.

Taxonomia das técnicas de previsão proposta a caixa em destaque sinaliza a categoria de modelos foco desta pesquis, os métodos quantitativos podem ser divididos em análise de séries temporais, baseada na descoberta de padrões nos dados históricos de uma variável.

Métodos causais, baseados em relacionamentos de causa e efeito, tentando descrever matematicamente o relacionamento de uma variável dependente com uma ou mais variáveis independentes e métodos de escolha discreta, utilizados essencialmente em problemas de classificação.

Já os métodos qualitativos podem ser divididos em exploratórios, como as pesquisas de mercado e teoria de utilidade e técnicas de grupo, como as técnicas Delphi e composição da força de vendas.

O processo de previsão pode ser decomposto em três fases principais, Identificação do problema.

Desenvolvimento de alternativas.

Seleção da melhor alternativa baseada em um critério definido.

A partir do problema identificado, como previsão de vendas no varejo com uso de dados atomizados, as alternativas de técnicas são consideradas, como aquelas mostradas nas seções anteriores.

A fase final do desenvolvimento de um modelo de previsão passa pela avaliação dos modelos e seleção daquele mais apropriado ao problema, a partir de critérios bem definidos.

Os critérios utilizados para seleção da melhor técnica de previsão são muito variados, e passam por, Acurácia.

É o critério mais utilizado para seleção da melhor técnica de previsão, e muitas vezes o único.

Sem dúvida é um critério primordial, dado que previsões com grandes erros não suportam o gestor na tomada de decisões,pelo contrário, levam a decisões danosas à corporação.

Custo.

Envolve o custo de obtenção, tratamento e preparação dos dados, o custo no desenvolvimento e avaliação dos modelos, e o custo de implantação do modelo na operação da empresa.

Relaciona-se sempre aos elementos humano, de tempo e computacional.

Habilidade do analista.

Muitas vezes a falta de know-how sobre uma determinada técnica de previsão impede que ela seja sequer considerada.

Uma familiaridade maior na aplicação de alguma técnica influencia decisivamente a escolha da mesma, até porque o analista tende a gerar modelos melhores do que ao utilizar técnicas sobre as quais detém pouco conhecimento.

Características desejadas na previsão, como explicabilidade, incorporação de determinadas variáveis, capacidade de realizar simulações, entre outras.

Características específicas do problema.

Envolvem todo o sistema investigado, número e características das variáveis relevantes, como elas se comportam alguns métodos só podem ser aplicados a dados com determinados padrões de comportamento, horizonte de tempo considerado, número de itens previstos, periodicidade das previsões, entre outras.

Facilidade de uso.

Refere-se à facilidade de uso da técnica em si, na geração do modelo, e da utilização posterior deste modelo no contexto da organização.

Envolve, complexidade, tempo de desenvolvimento, nível de conhecimento exigido, e base conceitual aplicada.

Requerimento de dados.

A disponibilidade de dados dentro dos padrões esperados por determinadas técnicas pode pressionar a escolha das mesmas.

Disponibilidade de softwares.

A exploração das técnicas de previsão depende diretamente da disponibilidade de pacotes estatísticos que as suportem, sobretudo nas previsões quantitativas.

Velocidade.

Refere-se à agilidade e flexibilidade no desenvolvimento e aplicação de novos modelos.

Facilidade de interpretação.

Esta é uma característica fundamental de qualquer previsão, uma vez que ela deve ser bem compreendida pelo gestor.

Está relacionada com o poder explicativo dado pela técnica, e além de influenciar o modo como o administrador irá tomar decisões a partir da previsão, afeta também sua credibilidade.

Eficiência.

Capacidade de melhorar a performance com o passar do tempo, conforme assimila maior quantidade de dados.

Também conhecida como taxa de "aprendizado" do modelo de previsão.

Enviesamento.

Ausência de respostas enviesadas que tendem sempre para mais ou para menos.

Capacidade de incorporar a experiência do gestor.

Este tem sido o critério mais usado para justificar o uso de métodos qualitativos de previsão.

Não há consenso na literatura sobre qual o critério ou conjunto de critérios mais importante na seleção das técnicas de previsão.

Custo, habilidade do analista, características do problema e desejadas no método são os critérios mais relevantes.

Se obteve como respostas de gestores de empresas de manufatura, acurácia, requerimento de dados e facilidade de uso.

Facilidade de uso também foi destacada e a (falta de) habilidade do analista como o fator predominante para o não uso de técnicas quantitativas mais formais.

Os maiores obstáculos para uso de técnicas quantitativas foram enviesamento, falta de know-how (habilidade do analist e tempo (velocidade) para construção dos modelos.

Os resultados, entretanto, mostram que há uma associação positiva entre a importância percebida dos métodos de previsão e sua acurácia.

Isto significa que quanto maior a acurácia de um modelo, maior a importância percebida dele.

De fato, acurácia tem sido relatada como o critério mais importante na seleção das técnicas de previsão.

É importante distinguir os critérios de seleção de técnicas com aqueles de avaliação das previsões.

Durante a seleção, a preocupação é com a escolha da técnica mais adequada ao problema, aquela capaz de modelar o fenômeno com maior precisão ou atingir outro critério de seleção especificado.

Já durante o processo de avaliação, houve preocupação com a performance do modelo criado a partir da técnica selecionada, ou seja, com a medida da qualidade das previsões efetuadas.

Naturalmente, uma escolha acertada da técnica de previsão deve levar a uma melhor capacidade preditiva do modelo gerado.

Acurácia é, também no caso de avaliação de performance, o critério mais utilizado.

Tal resultado é natural que seja esperado, uma vez que a acurácia das previsões de vendas tem um papel fundamental na tomada de decisões, especialmente no que diz respeito ao planejamento financeiro e reposição de inventários.

Previsões mal feitas podem levar a excesso de inventário ou perda de vendas, problemas no agendamento da produção, decisões de preço incorretas, falhas no atendimento ao cliente, entre outros todas situações de perda financeira, ocasionando diminuição da lucratividade.

Quanto aos resultados das previsões, as empresas de bens de consumo (varejo) atingem previsões otimistas com maior freqüência que as empresas industriais, e também consideram a previsão de vendas não importante com maior freqüência.

Uma vez que acurácia é o critério mais importante na seleção e avaliação das técnicas de previsão, ela deve ser definida matematicamente.

A fim de avaliar o desempenho acuráci absoluto e relativo dos diversos modelos de previsão, diversas ferramentas matemáticas foram desenvolvidas com o tempo.

Os principais mecanismos são, ME.

Erro Médio é a medida mais simples de erro das previsões, definida como o somatório dos erros dividido pelo número de observações realizadas.

MAE.

Erro Absoluto Médio é o erro médio tomado em termos absolutos, para que um erro positivo não seja anulado por outro negativo.

MSE.

Erro Quadrado Médio foi definido com a mesma finalidade de não anular os erros durante sua somatória.

MAPE.

Erro Percentual Absoluto Médio é uma medida do erro absoluto médio em termos percentuais, para que se tenha uma visão do erro comparado com o valor previsto, e também para permitir comparações entre modelos que utilizam dados diferentes.

Outras medidas de acurácia podem ainda ser definidas, apesar de não serem tão comuns na prática, como, AE.

Erro Absoluto é o erro total acumulado, somado de forma absoluta (com o uso do módulo do erro).

SSE.

Soma Quadrada dos Erros é o total quadrado dos erros verificados.

Ou seja, representa o MSE sem a medida da média.

PE.

Erro Percentual é o erro total medido em porcentagem.

MPE.

Erro Percentual Médio, medida percentual do erro médio, ou seja, o PE medido em termo médio.

APE.

Erro Percentual Absoluto é o erro absoluto total medido em porcentagem.

Em outras palavras, representa o MAPE sem a medida da média.

RMSE.

Raiz do Erro Quadrado Médio, tomado a partir da raiz do MSE, para que a medida volte à unidade original.

SMAPE.

Erro Percentual Absoluto Médio Simétrico é uma adaptação do MAPE, evitando que grandes erros pontuais tenham peso exagerado na medida da acurácia.

É recomendado quando existirem falhas ou picos repentinos na demanda.

YTD MAPE.

É a medida do MAPE para o último ano corrido, isto é, para as últimas 5semanas ou 1meses até o momento, dependendo da freqüência tomada.

CumRAE.

Erro Absoluto Relativo Acumulado, mede a capacidade preditiva de dois modelos, comparativamente.

É definido como a relação entre os erros totais verificados por dois modelos.

MdCumRAE.

Erro Absoluto Relativo Acumulado Mediano, é uma forma de usar o CumRAE na comparação de mais de dois modelos.

A escolha do critério a ser utilizado para medir a acurácia dos modelos não é arbitrária ela deve ser feita baseada nas características do problema e das medidas citadas.

Um dos métodos mais utilizados é o MSE, por ser um estimador não enviesado.

Porém este e também sua raiz, RMSE são largamente afetados por outliers, grandes e possivelmente raros erros.

Caso ocorram picos de erros, e o objetivo seja ter um modelo buscando acurácia ao longo do tempo, este critério deve ser desconsiderado.

A medida do MAPE pode ser a melhor medida nestes casos, por ser computada por medidas absolutas e em porcentagem do valor previsto.

Outra vantagem da MAPE é ter uma visão da amplitude do erro, por exemplo, dizer que um modelo teve erro médio de 8% (MAPE) fornece mais informações do que dizer que seu MSE foi de 361, ou que seu MAE foi 147.

No caso de se ter como objetivo um modelo de previsão que respeite limites de erro máximo, a medida do MAE pode ser a mais indicada, assim como as medidas de AE e APE.

Não há, de maneira geral, uma conclusão uniforme na literatura sobre o desempenho comparativo das técnicas de previsão.

Geralmente as técnicas são comparadas apenas com relação à sua capacidade preditiva, e as conclusões são díspares não há consenso sobre qual técnica é a mais apropriada para cada tipo de problema.

Porém, é fato que nenhuma técnica supera as demais em todas as aplicações.

Algumas, entretanto, têm baixa performance sempre, como os métodos de suavização através de médias estes são citados mais por questões históricas, e raramente empregados na prática, a não ser para comparar o ganho de acurácia na adoção de uma nova técnica de previsão.

Outra conclusão que parece ser consenso na literatura é quanto à baixa acurácia das técnicas qualitativas.

Tais técnicas só apresentam resultados equivalentes às quantitativas no longo prazo (mais que dois anos), e são freqüentemente mais caras dado que demandam muito tempo dos analistas e da alta administração.

Já as técnicas quantitativas podem ser aplicadas a qualquer horizonte de tempo, desde que os padrões não se modifiquem.

Uma importante ressalva deve ser feita quanto ao padrão dos dados, muitas técnicas, principalmente as de séries temporais, são aplicáveis apenas a alguns padrões, mostrando grandes erros se aplicadas em outros padrões.

De fato, a maior parte das técnicas quantitativas, incluindo as técnicas causais, requer uma análise prévia do padrão geral dos dados, a fim de formular um modelo adequado.

Além das diferenças estruturais nos inúmeros problemas já estudados, outro fator que pode ter influenciado na disparidade das conclusões acerca do desempenho das técnicas é a qualidade (representatividade e quantidade suficiente) e natureza dos dados apresentados, independentemente do modelo utilizado.

Os resultados dos estudos confirmam que a qualidade dos dados é fator preponderante na acurácia das previsões.

A tabela a seguir demonstra a disparidade nos resultados alcançados por diversos autores a respeito da acurácia das diferentes técnicas de previsão, quando comparadas entre si para um mesmo conjunto de dados, Exemplos de comparação de desempenho das técnicas de previsão na literatura.

As técnicas de previsão qualitativas, também chamadas de técnicas subjetivas ou baseadas em critérios de juízo, são aquelas que utilizam primordialmente a capacidade humana de estabelecer generalizações e extrapolações.

Estas técnicas pouco se utilizam, ou simplesmente não se utilizam, de séries de dados históricos, quantitativos.

Uma conclusão presente em vários estudos indica que, em geral, as empresas parecem estar mais à vontade com essas técnicas que com métodos quantitativos.

Contudo, um estudo recente sinaliza que a familiaridade com métodos quantitativos é crescente, em sua pesquisa 76% dos respondentes afirmaram possuir familiaridade com pelo menos uma técnica quantitativa para previsão de vendas, contra 61% obtidos no estudo.

Talvez essa familiaridade com métodos qualitativos seja devido ao fato de eles serem mais simples, e algumas vezes até intuitivos.

fazendo com que sejam os métodos mais utilizados nas empresas.

De fato, facilidade de uso e capacidade de incorporar a experiência do gestor são os principais argumentos utilizados para justificar o uso intensivo de técnicas de julgamento subjetivo nas previsões, em detrimento de técnicas quantitativas.

Entretanto, diversos estudos mostraram que os métodos qualitativos oferecem baixa acurácia, e não raro, as previsões subjetivas geram grandes erros, ocasionando distúrbios no planejamento e resultado final das operações corporativas.

Motivos pelos quais as previsões subjetivas, apesar de serem muitas vezes realizadas por analistas experientes e com informações contextuais do mercado, não possuem boa performance, As informações contextuais podem não ter valor preditivo.

Enviesamento e ineficiência na interpretação dos relacionamentos podem mascarar as informações contextuais levadas em consideração.

Excesso de dados a serem consideradas pelo analista, fazendo com que ele ignore ou dê pesos errados às informações contextuais recebidas.

Perturbações recentes na série temporal podem ser entendidas pelo analista como um sinal de tendência, confundindo a interpretação das informações contextuais.

Acurácia não é o critério mais importante nas previsões das empresas.

Dada a baixa capacidade preditiva das técnicas qualitativas, esta pesquisa está focada em métodos quantitativos de previsão, e, portanto, apenas apresenta brevemente algumas das técnicas qualitativas mais utilizadas, sem detalhar seu funcionamento.

Aqui estão classificadas todas as técnicas qualitativas de exploração de dados e desenvolvimento de cenários.

Não há grande formalidade na aplicação destes métodos, por isso as variações são incontáveis.

Algumas das técnicas mais comuns e sistematizadas são, Pesquisas de mercado.

Pesquisas e testes de mercado são instrumentos de previsão úteis especialmente quando dados históricos não estiverem disponíveis ou não forem confiáveis.

O lançamento de um novo produto é o exemplo clássico.

As desvantagens incluem baixa acurácia, alto custo e tempo dispensado.

Método Pert-modificado.

Método baseado em estimativas otimista, pessimista e realista dos acontecimentos futuros.

Às estimativas são associadas probabilidades de acontecimento, que são posteriormente avaliadas.

Fundamentado na Teoria das Decisões (teoria estatística para avaliação de probabilidades consecutivas), porém simplificado quanto à parte quantitativa.

Teoria da utilidade.

Refere-se à otimização de um parâmetro ou função utilidade, com base em probabilidades previstas por especialistas.

Métodos de extrapolação de gráficos.

A partir de gráficos relacionados à variável de interesse, são traçadas extrapolações à mão livre, com base no comportamento esperado da curva mais informações externas, como crescimentos ou retrações esperados no setor, informações da forca de vendas, entre outras.

Constituem técnicas de previsão onde as conclusões são essencialmente provindas de análises de mais de um especialista ou analista.

As contribuições podem provir de, executivos da empresa, equipe de vendas, clientes e outros especialistas da área.

A maior parte desses métodos utiliza técnicas estatísticas, com maior ou menor intensidade, para combinar as informações provindas dos diversos especialistas.

As técnicas mais utilizadas são, Técnica Delphi.

Baseada em questionários enviados para especialistas, que são revisados até que se chegue em consensos.

Diagnóstico de expectativas.

Baseado em pesquisas de opiniões de um júri executivo utilizado principalmente para detecção de novas tendências, e para combinar opiniões sobre previsões quantitativas previamente calculadas.

Composição da força de vendas.

Uma fonte importante de dados qualitativos é a força de vendas de uma empresa.

A percepção de futuro da equipe de vendas é de particular importância quando a demanda tiver um padrão com muitos picos e vales.

Existem duas condições primárias onde o uso da força de vendas faz sentido na previsão de vendas,os vendedores têm bom discernimento das mudanças no padrão da demanda e conhecimento profundo de seus clientes e os vendedores conhecem bem as probabilidades de garantir grandes vendas.

A demanda de armamentos de guerra é um bom exemplo de situação onde a modelagem matemática é virtualmente impossível, mas a equipe de vendas tem boa expectativa do comportamento futuro da demanda.

São modelos baseados na Teoria Bayesiana, combinando opiniões subjetivas com procedimentos quantitativos para revisar as probabilidades previstas e por isso foram colocados em uma categoria própria, dentro da classificação aqui proposta.

O Teorema de Bayes deriva da Lei de Probabilidade Total, que afirma que dados dois eventos A e B, pode-se sempre dizer que a probabilidade de A é igual à probabilidade da intersecção de A e B mais a probabilidade da intersecção de A e o complemento de B.

Ou, expressando em termos de probabilidade condicional, O Teorema de Bayes permite reverter a condicionalidade dos eventos, ou, em outras palavras, obter a probabilidade de B dado A a partir da probabilidade de A dado B.

Definição de probabilidade condicional.

Lei de Probabilidade Total.

Teorema de Bayes.

Ele pode ser entendido como uma forma de transformar uma probabilidade a priori em uma probabilidade a posteriori.

Convencionou-se, então, em denominar as abordagens que utilizam informação a priori sobre um problema particular, combinando-a com resultados estatísticos, de bayesianas.

A informação a priori a ser utilizada pode ser um resultado estatístico anterior, ou, mais freqüentemente, reflexo do conhecimento do especialista ou analista.

Uma das grandes vantagens de se utilizar a abordagem bayesiana, além da possibilidade de melhorar a acurácia com o uso de informação a priori, é a análise de forma seqüencial que ela permite.

Ou seja, informação obtida em uma análise pode ser utilizada como informação a priori em uma nova análise, quando novos dados estiverem disponíveis.

Assim, a segunda amostragem é utilizada como dados, enquanto os resultados da primeira análise são utilizados como informação a priori.

Esta metodologia é bastante utilizada em problemas de previsão, os dados atuais são combinados com a previsão anterior (esta última como informação a priori) para obter novas previsões.

Também o conhecimento de especialistas pode ser utilizado como informação a priori nas previsões.

Técnicas de previsão quantitativas são aquelas que usam dados históricos para calcular matematicamente extrapolações dos dados no futuro.

A previsão com uso de técnicas quantitativas pode ser aplicada quando, Informação sobre o passado esteja disponível.

Esta informação possa ser quantificada em termos matemáticos.

Seja possível assumir que alguns aspectos do padrão verificado no passado continuarão no futuro.

Esta colocação é também chamada de pressuposto da continuidade.

O procedimento geral para estimar um padrão de relacionamento, seja causal ou de série temporal, é através da aderência a uma forma funcional matemática qualquer, de forma a minimizar o componente de erro.

Exceção deve ser feita à modelagem baseada em redes neurais artificiais, que pode ser usada tanto para modelos causais como de séries temporais, porém não buscam a aderência a um modelo matemático explícito.

A previsão causal, também chamada de explanatória, assume um relacionamento de causa e efeito entre as entradas e saídas de um sistema.

O sistema pode ser visto como a economia nacional, o mercado de uma empresa, e assim por diante.

Já a previsão com uso de séries temporais trata o sistema como uma "caixa preta", sem tentar descobrir os fatores que causam os comportamentos observados.

A seguir as principais técnicas quantitativas de previsão são abordadas.

Uma das técnicas possíveis para previsões quantitativas é a utilização de modelos de escolha discreta, os quais têm sido aplicados principalmente na análise de participação de mercado e também na área de transportes, para planejamento viário e de operações de transporte.

Entretanto, estes são, a rigor, modelos de classificação, sendo que sua aplicabilidade para previsão de vendas é restrita.

Por este motivo, esta modelagem não será abordada neste trabalho.

Uma série temporal é uma seqüência de valores, ordenados no tempo, de uma variável de interesse particular.

Modelos de séries temporais realizam previsões baseadas em uma série de dados observados em intervalos de tempo regulares, buscando padrões no passado para prever o futuro.

Este tipo de modelagem é especialmente útil quando há pouco conhecimento da base teórica sobre o processo em que os dados foram gerados.

As previsões são muitas vezes confiáveis no curto prazo, sendo a classe de técnicas quantitativas há mais tempo em desenvolvimento.

Existem duas razões básicas para tratar um sistema como uma caixa preta.
O sistema não pode ser compreendido, ou, mesmo que possa, é extremamente difícil medir o relacionamento entre as variáveis que governam seu comportamento.

A preocupação pode ser simplesmente prever com algum grau de precisão o que vai acontecer, e não por que.

É importante observar que as técnicas de análise de séries temporais desenvolveram-se de forma paralela à estatística convencional.

Isto porque praticamente todas as técnicas estatísticas são baseadas no pressuposto de amostragem aleatória, ou seja, no pressuposto de que os dados disponíveis são observações independentes do fenômeno de interesse.

Este pressuposto raramente é verdadeiro para séries temporais, demandando portanto o desenvolvimento de novas e específicas técnicas estatísticas.

Existem duas razoes básicas para desconfiar do pressuposto de observações independentes em séries temporais, Os fatores econômicos que contribuíram para a geração de um valor não mudam repentinamente, tendendo-se a manter um nível próximo em períodos adjacentes.

Por exemplo, se as vendas de um mês foram altas, muito provavelmente as do mês subseqüente também o serão, estando portanto os dados de vendas relacionados de alguma forma.

Há geralmente a presença de sazonalidade, ou seja, de padrões de longo prazo (usualmente anuais) constantes, repetitivos.

Assim, o pressuposto de amostragem aleatória não é válido para séries temporais, e o uso de técnicas convencionais leva a grandes erros neste tipo de análise.

Pelo contrário, as técnicas de previsão a partir de séries temporais valem-se justamente do fato de as observações serem bastante dependentes, possibilitando a inferência de valores futuros a partir de dados históricos.

Apesar de sua grande aceitação, as técnicas de séries temporais possuem sérias limitações.

A mais visível delas é o fato de as causas que agem sobre as variáveis previstas serem completamente ignoradas.

Todas as forcas externas, como fatores econômicos, esforços de marketing, ações dos competidores, e assim por diante, são desprezadas.

Outra deficiência é que os padrões históricos que geraram as séries mudam com o tempo, e as técnicas podem não detectar tais mudanças.

Suas deficiências resultam em previsões com baixa acurácia, especialmente no longo prazo.

Como nessa classe de modelagem a preocupação é apenas com o comportamento da variável de interesse no tempo desprezando todos os fatores exógenos condicionantes de seu comportamento as técnicas quantitativas de previsão mais simples, e que demandam menor esforço matemático-computacional, inserem-se nessa categoria.

No entanto, dado o desenvolvimento constante das técnicas, algumas delas são de fato bastante complexas, e de implementação difícil.

Apesar de todos os problemas verificados com as técnicas de análise de séries temporais, elas continuam a ser extensivamente usadas e estudadas, e portanto são brevemente abordadas neste trabalho.

O nome desta classe de métodos smooth em inglês significa homogeneizar, tornar uniforme, aperfeiçoar, suavizar.

E é justamente o que propiciam, uma suavização no gráfico da variável estudada, através de processos matemáticos relativamente simples.

Talvez o mais simples método de suavização e também de previsão seja o chamado método naive ("ingênuo") não ajustado, que consiste simplesmente em considerar a última observação conhecida da série como sua melhor expectativa para o futuro.

Ou seja, parte da idéia de que amanhã será, com grande chance, como hoje.

Por mais simplória que pareça, esta metodologia é freqüentemente usada para comparar o ganho em capacidade preditiva ao se adotar uma técnica mais elaborada e já foi mostrado que sua aplicação oferece acurácia igual ou significativamente melhor que técnicas qualitativas.

Uma forma ligeiramente melhorada de técnica naive é a chamada naive ajustada, onde também é utilizada a última observação para efeito de previsão, porém ela é antes desazonalizada, o efeito da sazonalidade é minimizado através de técnicas de decomposição mostradas adiante melhorando assim sua acurácia.

Este método também é usado para fins de comparação entre capacidades preditivas.

São as técnicas usualmente referenciadas como de suavização através de médias ou averaging propriamente ditas, aplicando procedimentos matemáticos especialmente o cálculo sucessivo de médias para extrapolar a série temporal no futuro.

O mais simples método dessa classe, conhecido como simple average é o cálculo da média das últimas observações como uma aproximação de seu valor no futuro.

De fato, se considerarmos um fenômeno que gere saídas constantes estacionárias, flutuando ao redor de um ponto, a média é uma boa previsão.

Entretanto, se a série contiver tendências ou sazonalidades, a média deixará de ser efetiva.

Mas outras técnicas utilizam mecanismos de suavização dos dados através de médias buscando incorporar tais características dos dados em seus resultados.

A mais conhecida delas está presente em praticamente todos os livros de estatística aplicada é a de médias flutuantes, ou em inglês moving averages.

A técnica consiste em tomar uma média de um número fixo de observações que se move conforme se progride na série de dados.

Assim, para ter a primeira observação em um moving average que utiliza a média de k observações é necessário passar pelos k primeiros números, realizando sua média.

A segunda observação seria composta também pela média de k valores da série, mas iniciando pelo segundo número, e assim por diante.

Quanto maior o número de pontos tomados, mais estáveis as séries obtidas.

Maior estabilidade também pode ser conseguida através da aplicação do algoritmo de averaging mais de uma vez, fazendo a média das médias.

Surgiram assim as chamadas double moving averaging e triple moving averaging, com a aplicação das médias duas, três ou mais vezes.

As técnicas averaging não são muito utilizadas para previsão dada sua baixa acurácia.

De fato, seu uso mais freqüente é o utilizado na desazonalização dos dados, ou seja, na tentativa de minimizar os efeitos da sazonalidade em dados que servirão de entrada a outras técnicas.

As técnicas de moving averages possuem alta estabilidade nas respostas fornecidas, característica esta que é bastante desejável num sistema de previsão, para superar o problema das grandes oscilações devido a alterações puramente aleatórias.

Porém a taxa de resposta às variações é de difícil modificação-baseia-se inerentemente no número de observações tomadas.

Outro problema grave dessas técnicas é sua baixa eficiência, isto é, baixo nível de "aprendizado", ou correção dos erros com o tempo.

Na tentativa de sanar os problemas citados, foram desenvolvidas técnicas de suavização exponencial, que envolvem a aplicação de pesos distintos aos dados históricos, para dar maior importância às observações mais recentes.

Nestas técnicas a taxa de resposta pode ser facilmente corrigida, dando maior flexibilidade aos modelos produzidos.

Outra vantagem é que elas utilizam menor número de dados da série, e portanto a necessidade de registro histórico é menor.

A regra geral utilizada no desenvolvimento das técnicas de suavização exponencial é, para ter uma estimativa da demanda, adicione à estimativa do mês anterior uma fração do erro verificado.

Quanto maior o fator de peso, mais rápida a resposta das previsões às mudanças observadas ao contrário, quanto menor o parâmetro, mais estáveis serão as previsões.

As principais técnicas desenvolvidas com base na idéia de suavização exponencial.

Aplicação da Equação V uma única vez para cada período previsto.

Aplicação da suavização exponencial duas vezes sucessivas, utilizando o resultado da primeira iteração como entrada na segunda.

Pode utilizar o mesmo parâmetro (Método de Brown), usando, portanto, a mesma equação duas vezes ou utilizar dois parâmetros distintos nas iterações (método de Holt).

Aplicação da suavização exponencial por três vezes sucessivas.

De forma semelhante ao double exponential smooting, pode-se utilizar um (método de Brown quadrático) ou três parâmetros distintos (método de Winter).

É na verdade um caso típico de single exponential smoothing, porém o fator de peso é definido matematicamente a partir dos valores de P e Q, e desta forma passa a ter certa liberdade para variar conforme se progride na série de dados.

Esta característica aumenta a eficiência dos modelos, fazendo com que possam adaptar-se melhor aos dados e diminuir os erros com o passar do tempo.

Outras equações podem ser geradas a partir da equação geral de suavização exponencial, como aquelas escritas a partir do modelo de classificação de Pegel.

Tais extrapolações não são, entretanto, muito utilizadas na prática.

As maiores vantagens das técnicas de suavização exponencial são sua simplicidade e baixo custo de desenvolvimento-elas podem facilmente ser aplicados através de planilhas de cálculo.

Com dados estacionários, pode-se utilizar com sucesso as técnicas simples de suavização exponencial.

O método de Brown é indicado para dados não-estacionários sem sazonalidade, principalmente por requerer apenas um parâmetro.

O método de Brown quadrático também é bastante utilizado por sua simplicidade e boa capacidade de detectar pontos de inflexão na tendência.

Seu ponto falho é sua rápida reação (baixa estabilidade), podendo indicar pontos de inflexão onde na verdade há apenas pequenas variações aleatórias.

Para dados com sazonalidade, a única técnica largamente utilizada é a de Winter.

Estas técnicas são normalmente preferidas em detrimento das técnicas averaging para previsão, por oferecerem maior acurácia e necessitarem de menor número de dados históricos armazenados e computados.

Ao contrário das técnicas de suavização, que tentam distinguir entre os padrões e os erros nos dados através de uma medida média de valores anteriores, as técnicas de decomposição buscam separar os componentes que possam estar presentes nos dados.

Os componentes usualmente citados na literatura são, Tendência (T).

Envolve a expectativa de crescimento ou decaimento em um período longo de tempo.

Por exemplo, apesar de apresentar altos e baixos, o índice do Produto Interno Bruto PI de um país desenvolvido tende a aumentar com o tempo.

Sazonalidade (I).

Flutuações periódicas de comprimento constante.

Muitas variáveis econômicas possuem forte padrão anual, semestral ou trimestral, e outras variáveis podem ter sazonalidades quaisquer.

Pode-se verificar facilmente um aquecimento da economia próximo ao fim do ano, por exemplo.

Componente cíclico.

Padrões oscilatórios, desconectados de comportamentos sazonais repetitivos.

Não são necessariamente regulares, mas seguem um certo padrão ao longo do tempo, geralmente relacionado ao comportamento macroeconômico.

Componente irregular.

Agrega a multitude de fatores que influenciam o comportamento de uma série real, e cujo padrão parece ser imprevisível.

Os modelos de decomposição supõem que se pode escrever o padrão dos dados. Todas as técnicas deste gênero, como a Decomposição Clássica, ou a Decomposição Censo, envolvem a decomposição dos dados nos fatores acima.

Métodos Auto-Regressivos (ARIM Os métodos Auto-Regressivos (Autoregressive Integrated Moving Average ARIM constituem uma classe genérica de métodos capazes de gerar modelos através da combinação de três técnicas matemático-estatísticas, auto-regressão (regressão com base no tempo), moving averages (para suavizar e dasazonalizar), e diferenciação (para incluir processos não estacionários).

Uniu-se à teoria de modelagem auto-regressiva a capacidade de tratar dados não-estacionários, através de um processo de diferenciação, criando a classe genérica de modelos ARIMA, que também passaram a ser conhecidos como modelos Box-Jenkins.

Desta forma, são possíveis inúmeras combinações, gerando um conjunto possível de equações que são escolhidas conforme as características dos dados.

A metodologia proposta define três passos para a construção do modelo de previsão, Identificação de um ou mais modelos que descrevam a série temporal adequadamente.

É feita utilizando ferramental estatístico, testando a estrutura de correlação da série, a partir da classe genérica ARIMA.

Estimação dos parâmetros dos modelos.

É similar à estimação de parâmetros de modelos de regressão, porem com uso de técnicas de estimação não-lineares.

Condução do diagnóstico dos modelos, através da medição dos erros provocados por cada modelo, e seleção daquele com maior acurácia.

Apesar de seu apelo estatístico e teórico, estes modelos nunca foram muito utilizados na prática principalmente devido à sua complexidade matemática, que nem sempre se traduz em melhoria na acurácia.

Auto-regressão refere-se à utilização da técnica estatística de regressão sem o uso de variáveis exógenas, mas sim da própria variável dependente em diferentes períodos de tempo.

Assim, o pressuposto de independência dos termos de erro formulado na teoria estatística é imediatamente violado, por isso a obtenção de resultados satisfatórios não é garantida, e só é feita em termos empíricos.

Há uma certa confusão ocasionada pela nomenclatura utilizada no método ARIMA.

Duas são as possíveis ambigüidades, o termo Integrated no nome da técnica na verdade refere-se ao processo de diferenciação da série de dados.

O termo moving averages tem neste método um significado diferente daquele usado nos métodos de suavização aqui ele refere-se à série de termos de erro em diferentes períodos de tempo considerados.

As técnicas de previsão baseadas em análises de séries temporais são especialmente úteis quando há pouco conhecimento sobre a teoria envolvida no processo investigado.

Por exemplo, se houver total desconhecimento dos fatores que influenciam a demanda de um determinado produto, ou se estes fatores não puderem ser mensurados, então o uso de uma técnica de análise de séries temporais pode ser a mais adequada.

Porém o uso dessa abordagem resulta em previsões não explicativas, e conseqüentemente o gestor não é capaz de realizar simulações sobre o comportamento da variável de interesse com mudanças em outros fatores, especialmente sobre aqueles aos quais ele tem controle.

Por exemplo, a demanda de um produto é claramente influenciada pelo preço praticado, e este preço é um fator que está sob controle do gestor.

Habilitar o gestor a simular o comportamento da demanda com mudanças no preço é fornecer a ele uma ferramenta de apoio à decisão, o que é um passo além de uma simples ferramenta de previsão de vendas.

As técnicas de previsão causais buscam descrever matematicamente as relações de causa e efeito entre a variável que está sendo medida e seus fatores constituintes.

Um modelo causal, como o descrito por uma equação de regressão, tem a vantagem de permitir ao tomador de decisões explorar como mudanças nas variáveis explicativas alteram a saída prevista.

Esta pesquisa está focada na geração de uma ferramenta de apoio à decisão no varejo, e, portanto, as técnicas de previsão causais que habilitam a geração de cenários de vendas a partir de mudanças no composto de marketing são aquelas de maior preocupação neste trabalho.

Freqüentemente, o relacionamento entre duas ou mais variáveis de interesse pode ser bem modelado através de uma equação matemática.

O processo de adaptação de uma equação matemática a um fenômeno envolvendo duas ou mais variáveis, de forma que uma delas seja dependente das demais, é chamado de regressão.

Ele gera modelos matemáticos que buscam representar a relação de causa e efeito entre as variáveis consideradas.

Se existir uma associação entre duas variáveis quaisquer, diz-se que há covariância positiva ou negativ.

Se não há associação entre as variáveis, a covariância é nula.

Entretanto, este valor depende da unidade de medida das variáveis.

Para superar esta restrição, foi definida a correlação, que é uma medida pura, independente de escala.

O coeficiente de correlação é definido como a covariância dividida pelo produto dos desvios padrões individuais, Quanto maior o coeficiente de correlação, mais forte a associação entre as variáveis.

Todas as técnicas de regressão são baseadas na idéia de associação entre as variáveis, demonstrada estatisticamente através da correlação.

A técnica mais simples de regressão é a Regressão Linear, que modela o relacionamento entre uma variável dependente Y e uma variável independente X de forma linear.

Assim, onde é uma variável aleatória com média zero.

Por exemplo, se demanda for a variável dependente, e ela tiver uma correlação linear com o preço, escreve-se, Na verdade, a regressão linear é um caso específico da técnica geral de análise de Regressão Múltipla que estabelece matematicamente a relação entre uma variável dependente e uma ou mais variáveis independentes.

Assim, No caso de previsão de vendas, por exemplo, pode-se relacionar a demanda do produto (variável dependente) com a renda dos consumidores, o tamanho da população, o preço do produto, o preço de seus substitutos e complementares, o nível de promoção, o crescimento macroeconômico, entre outras.

Para que as equações possam ser efetivamente empregadas como modelos de previsão, é necessária a determinação dos parâmetros e k.

A principal técnica para determinação dos parâmetros de um modelo de regressão é a técnica de mínimos quadrados, que se baseia no teorema de Gauss-Markov.

A técnica de mínimos quadrados é não-enviesada e ótima para modelos lineares, e muito bem aplicada para modelos de regressão múltipla.

A qualidade de aderência do modelo aos dados é calculada em termo do erro verificado, que é minimizado através da técnica de mínimos quadrados.

Um termo relevante nas análises de regressão é o coeficiente de determinação que denota a proporção de variabilidade da variável dependente explicada pela relação com as variáveis independentes.

Em outras palavras, traduz o poder explanatório da regressão.

Quanto maior o R, diz-se que a curva está mais ajustada aos dados, e portanto o modelo representa bem os dados utilizados para construí-lo.

Comparativamente, os modelos de regressão requerem um esforço maior para construção do que as técnicas de séries temporais excetuando-se possivelmente os modelos ARIM.

Além disso, eles requerem uma série de testes estatísticos para a seleção das variáveis de entrada mais relevantes, e exigem um conhecimento prévio da forma funcional do relacionamento entre as variáveis.

Em condições normais, tal determinação a priori é difícil de ser obtida.

Uma técnica usual é tentar diversas formais funcionais, acarretando em uma família de equações, optando-se pela equação com maior aderência aos dados.

Mesmo neste caso, deve-se decidir as formas funcionais a serem consideradas.

Freqüentemente, os analistas assumem hipóteses simplificadoras de linearidade na estrutura dos dados, para que os modelos sejam construídos mais facilmente.

Modelos lineares, entretanto, têm performance ruim na determinação de pontos de inflexão.

Como os problemas em marketing sempre lidam com dados como vendas e preços, as séries de dados estão inseparavelmente ligadas a pontos de inflexão, tendências e não-linearidades, e muitas vezes podem ser até mesmo caóticas.

Metodologia para a escolha das variáveis a serem utilizadas nos modelos de regressão múltipla.

Ela envolve, Determinação de uma "lista longa" de variáveis.

Uma lista com todas as variáveis potenciais que possam ter algum efeito sobre a variável dependente analisada.

Baseada na opinião de especialistas, na disponibilidade dos dados e no esforço e custo associado à aquisição dos dados.

Redução para uma "lista curta".

Vários métodos podem ser usados na eliminação de algumas das possíveis variáveis da lista longa.

Os mais simples (e menos recomendados) são, plotar cada variável contra Y, observando se há alguma relação visível.

Observar as correlações entre todas as variáveis independentes potenciais, eliminando as de grande correlação para evitar multicolineariedade e efetuar uma regressão múltipla com todas as variáveis, e eliminar aquelas com baixo t.

Os métodos mais sofisticados são, regressão stepwise método iterativo de retirada de variáveis e testes de significância.

Análise de componentes principais e fazer uma análise horizontal no tempo (lag analysis).

Já sugere as seguintes abordagens, Testar todas as regressões possíveis.

Assim, tendo k variáveis independentes, tem-se possibilidades, já que cada variável pode estar ou não incluída no modelo.

Aplica-se então um critério de performance para seleção da melhor opção, que pode ser maior R ou menor MSE.

Seleção "para frente".

Começa com um modelo sem variáveis, testando uma de cada vez e selecionando aquele com maior F estatístico (da análise de componentes principais).

O processo recomeça, adicionando mais uma variável, e mantendo aquela pré-selecionada.

Eliminação "para trás".

Funciona de maneira oposta à seleção para frente, começa com um modelo com todas as possíveis k variáveis e é retirada aquela com menor F, recomeçando o processo.

Regressão stepwise.

É na verdade uma mistura da seleção para frente e eliminação para trás, que reavalia o nível de significância de cada variável a cada passada-ao contrário dos demais que, uma vez tomada a decisão de incluir ou excluir uma determinada variável, ela não mais é revista.

Apesar das dificuldades na seleção das variáveis e construção dos modelos, a análise de regressão é a técnica estatística mais utilizada nas empresas européias, sendo utilizada por cerca de 18% dos respondentes.

A popularidade das técnicas de regressão deve-se a pelo menos dois fatores, São de fácil compreensão, mesmo para aqueles sem profundos conhecimentos matemáticos ou estatísticos.

Resultam em boa acurácia para a maior parte das aplicações.

Certamente, os modelos de regressão são os que oferecem maior capacidade explicativa para as variações nos dados, já que é possível enxergar a relação matemática entre as variáveis explicitamente.

A maior parte das limitações das regressões está relacionada aos pressupostos que foram assumidos para a formulação das equações e determinação dos parâmetros ideais.

Os pressupostos assumidos ao se desenvolver um modelo de regressão qualquer, com n pares de observações disponíveis.

Os valores de xi são números fixos (determinados, por exemplo, por um experimento) ou realizações de variáveis aleatórias Xi, que são independentes do erro.

Os termos de erro são variáveis aleatórias com média zero.

As variáveis têm a mesma variância.

As variáveis não estão correlacionadas uma com a outra.

Não é possível determinar um grupo de números de forma que, (ou seja, as variáveis independentes não estão correlacionadas).

As principais limitações da análise de regressão são decorrentes dos pressupostos acima, e podem ser listadas como sendo.
Relacionamento constante.

As técnicas de regressão assumem um relacionamento constante entre as variáveis, durante o período histórico compreendido pelos dados utilizados na criação do modelo.

Ou seja, todos as observações possuem o mesmo peso durante a regressão.

Isto pode ser um problema sério ao se utilizar grande horizonte de dados históricos, ou ao analisar mercados altamente dinâmicos.

Autocorrelação.

Refere-se à condição onde o valor da variável prevista está relacionado ao seu valor no período anterior.

A análise de regressão assume valores aleatórios, e não ordenados (correlacionados) das variáveis previstas.

Este problema gera previsões enviesadas sub ou sobre estimadas constantemente.

Multicolinearidade.

Presente quando uma ou mais, e até mesmo todas as variáveis independentes estão relacionadas entre si.

A existência desta condição teoricamente invalida as medidas de correlação e o procedimento de stepwise.

Se existir multicolineariedade perfeita numa regressão, a solução ótima baseada no método dos Mínimos Quadrados não pode ser encontrada.

No caso de multicolineariedades quase perfeitas, esta solução será afetada por problemas de arredondamento nos computadores.

Causalidade.

A regressão assume uma relação de causalidade entre a variável dependente e as variáveis independentes.

Porém esta relação não pode ser tida como certa.

Pode-se afirmar que duas variáveis estão estatisticamente correlacionadas a mudança em uma é acompanhada de uma mudança proporcional na segund, porém nada se pode afirmar em respeito à relação de causalidade entre as duas.

Linearidade.

Refere-se à linearidade dos coeficientes, e relata diretamente ao desenvolvimento dos testes F e T.

Em outras palavras, se for violada os testes estatísticos não são mais válidos.

Independência dos residuais.

Também se reflete nos testes F e T, tornando os resultados destes testes não válidos.

O teste Durbin-Watson é uma forma de examinar se este pressuposto está sendo respeitado.

Homocedasticidade.

Refere-se à variância dos erros, que deve ser constante.

Mais uma vez, o impacto está na validade dos testes F e T.

Normalidade dos residuais.

Se há forte violação da normalidade nos erros, os testes estatísticos não são recomendados.

Intervalo de confiança.

Há um intervalo de confiança para o qual o modelo é aplicável, que pode ser determinado matematicamente.

O desrespeito ao intervalo de confiança gera previsões errôneas.

Assim como a regressão linear simples é um caso especial da regressão múltipla, esta última pode ser vista como um caso especial da modelagem econométrica.

Os modelos econométricos envolvem várias equações de regressão, com mais de uma variável dependente.

Vale ressaltar que alguns autores usam o termo "econométrico" para qualquer modelo de regressão, seja simples, múltiplo ou de várias equações.

Na verdade, as técnicas econométricas surgiram efetivamente para prever índices econômicos, como o próprio nome diz, e foram posteriormente adaptadas para previsões microeconômicas.

A análise de regressão assume que cada uma das variáveis independentes seja determinada por fatores externos, exógenos ao sistema.

Este pressuposto nem sempre é verdadeiro na análise macroeconômica, onde as variáveis exibem comportamentos interdependentes.

A maior vantagem dos modelos econométricos é sua habilidade em lidar com interdependências, ou seja, em levar em consideração variáveis com efeito mútuo, onde não é possível determinar uma relação de causa e efeito clara.

Sua principal desvantagem é a inexistência de um conjunto de regras que possam ser aplicadas em situações distintas, o que torna o desenvolvimento dos modelos altamente dependente do problema específico, e requer sempre a presença de um analista experiente.

Por exemplo, se um governo quiser saber qual o impacto de uma redução de impostos em uma economia em recessão, uma modelagem econométrica seria bem apropriada.

Tal redução afetaria a renda das pessoas e empresas, e influenciaria toda a economia, incluindo nível de preços, desemprego, capital circulante aplicado, e assim por diante.

Todos estes fatores são inter-relacionados, e uma equação de regressão múltipla não seria capaz de modelar esse fenômeno eficazmente, tal problema seria mais bem expresso com o uso de um conjunto de equações econométricas.

Varejo consiste em todas as atividades que englobam o processo de venda de produtos e serviços para atender a uma necessidade pessoal do consumidor final.

O varejista desempenha o elo de ligação entre o consumidor e o produtor ou atacadista.

O setor varejista vem atravessando intensas transformações ao longo das últimas décadas, assumindo importância relativa crescente no Brasil e no Mundo.

Uma das transformações mais visíveis é o aumento do poder do varejo observado no Mundo Ocidental.

Viu-se surgirem grandes conglomerados varejistas, muitas vezes com atuação multinacional.

Prova disso é o número crescente de varejistas que figuram na relação de maiores empresas do Brasil.

Tal fenômeno mudou a relação de poder fornecedor-varejista, fazendo com que o varejista ganhe poder de barganha.

Grandes cadeias de varejo têm hoje força suficiente para impor condições de fornecimento (quantidades e freqüências), forma de abastecimento e pagamento, e até mesmo procedimentos de gestão logística.

O surgimento e fortalecimento das marcas próprias produtos comercializados com a marca do varejista, exclusivamente em suas lojas é outro exemplo da mudança no relacionamento varejo-indústria.

Além disso, é crescente a polarização entre grandes e pequenos varejistas.

Os grandes varejistas assumiram uma posição de varejo de massa, que operam em grande parte do território nacional, desenvolvem sofisticados sistemas logísticos e de informações de marketing, e exercem forte poder de barganha junto aos fornecedores.

Já os pequenos varejistas, que não detêm o poder de barganha dos grandes, não são capazes de oferecer preços tão competitivos, e para sobreviver atuam em mercados restritos ou optam por uma estratégia de especialização, aprofundando-se no atendimento de necessidades específicas em certos segmentos de mercado.

O aumento da concorrência no varejo acompanhou aquele verificado em outros setores, porém com uma característica peculiar, ele agora se dá não somente entre empresas que operam no mesmo formato, mas também entre diferentes tipos de varejistas.

Ou seja, diferentes tipos de varejo vendem as mesmas categorias de produto, aumentando as opções de compra do consumidor final.

As padarias, por exemplo, hoje têm como concorrentes não só outras padarias, mas também hipermercados, supermercados e lojas de conveniência.

Finalmente, o surgimento e ampliação das vendas realizadas sem o uso de lojas mudaram a realidade do varejo no mundo todo.

Vendas através da televisão, por catálogo, através de máquinas automáticas e principalmente pela Internet registram contínua expansão, e oferecem desafios crescentes de planejamento e execução aos gestores do varejo.

Em qualquer mercado, a população desenvolve padrões de consumo que se refletem em gastos realizados nos diversos setores varejistas.

Estes gastos representam a demanda de mercado, que é dividida entre todos os varejistas que atuam no setor.

Conseqüentemente, cada varejista é detentor de uma parcela da demanda total do mercado, esta parcela é comumente referida como sendo sua fatia de mercado.

Composto de marketing é "o conjunto de variáveis que compõem o esforço de marketing do varejista, e engloba todos os fatores controláveis que o varejista articula para conquistar as preferências dos consumidores".

Ainda de acordo com o autor, a fatia de mercado de uma loja depende principalmente das características de seu composto de marketing, frente ao composto de marketing de seus concorrentes.

O composto de marketing do varejo pode ser classificado a partir de seis macro-fatores (chamados "seis P's"), Conjunto (mix) de Produtos, linhas de produtos comercializados, caracterizados por sua variedade, qualidade e quantidade.

Preços, preços de venda e créditos oferecidos.

Promoção, nível de propaganda, ofertas, sinalização da loja, programas de fidelização entre outros.

Apresentação, Layout da loja, disposição dos produtos, espaço físico, limpeza da loja, conforto e decoração.

Pessoal, qualidade do atendimento, rapidez, cortesia, serviços.

Ponto, localização da loja, facilidade de acesso, visibilidade, estacionamento, complementaridade com outras lojas.

Ainda segundo o autor, o comportamento de compra do consumidor perante uma loja é influenciado basicamente por três conjuntos de variáveis, variáveis ambientais (economia, clima, demografia, época do ano, tendências de mercado), o composto de marketing da loja e o composto de marketing da concorrência.

Influenciado por estes três grupos de forças, o consumidor decide o quanto gastar no setor varejista e em cada loja especificamente.

A decisão final do consumidor é tomada com base na relação custo-benefício percebida para cada loja, definida como valor, "Os clientes são maximizadores de valor, e agem dentro de seu repertório de alternativas, limitados pela sua mobilidade, conhecimento, custos e renda.

Eles formam uma expectativa de valor que determina seu comportamento.

O grau em que essas expectativas são atendidas define tanto sua satisfação como sua possibilidade de recompra".

Decisão de compra baseada nos conceitos de valor e compostos de marketing.

Forças e variáveis que influenciam a decisão de compra do consumidor, enquanto maximizador de valor.

Alguns modelos de comportamento de compra foram desenvolvidos para tentar explicar de forma simplificada o processo de aquisição de uma mercadoria e os fatores que o influenciam.

O modelo mais citado tem sido aquele que identifica cinco estágios no processo de compra, Reconhecimento do problema.

O consumidor identifica uma necessidade não atendida, e reconhece que poderá atendê-la através da compra de um produto ou serviço.

Busca de informação.

Pesquisa de alternativas de compra, pesquisas de preço e busca de informações sobre os produtos e serviços ofertados.

Este processo varia conforme o consumidor e o tipo de produto adquirido (conveniência, compra comparada ou especialidade).

Análise das alternativas.

Os consumidores comparam e avaliam as alternativas para tomar sua decisão de compra.

Os atributos utilizados para a tomada de decisão também variam conforme o consumidor e o tipo de produto, mas podem ser relacionados ao composto de marketing.

Decisão de compra.

Finalmente, após analisar as alternativas consideradas para compra, o consumidor decide o que e onde comprar.

Além das variáveis do composto de marketing e ambientais, outros fatores podem influenciar o processo de compra, como a fidelidade a determinadas marcas, tempo e recursos disponíveis para compra, e os pesos dados aos atributos dos produtos e das lojas consideradas.

Avaliação pós-compra.

Após a compra, o consumidor reavalia o acerto de sua decisão, e desenvolve sentimentos de satisfação ou insatisfação.

Dentro da classificação, este trabalho está focado na exploração do impacto na demanda proporcionado por alterações nos três primeiros componentes do composto de marketing, produto, preço e promoção.

Esta escolha, de restringir as análises a variáveis pertencentes apenas a estes componentes, deveu-se basicamente à dificuldade observada ao se tentar quantificar e comparar as variáveis dos demais componentes, apresentação, pessoal e ponto.

Tal tentativa de quantificação necessária para fins de modelagem para mineração de dados fugiria do escopo deste trabalho.

A seguir são tecidos alguns comentários a respeito dos três primeiros componentes de marketing citados acima, já que são os componentes mais facilmente computados de forma quantitativa, e que podem, portanto, ser incorporados nos modelos de previsão de vendas baseados em técnicas quantitativas.

A composição do conjunto de produtos oferecido, o mix de produtos, é uma das tarefas mais importantes de uma empresa de varejo, já que determina o ramo ou setor de atividade da empresa, e quais necessidades de seus consumidores estará atendendo.

Na composição do conjunto de produtos, o varejista enfrenta dois objetivos que podem ser conflitantes.

Por um lado, procura atender ao máximo as diferentes necessidades de seus (vários) segmentos de consumidores, ampliando assim seu leque de produtos.

Por outro lado, dispor de uma variedade muito ampla de linhas de produtos acarreta alguns problemas como, maior investimento em estoques, incorporação de produtos com baixo giro de estoque, maior custo operacional com atividades como armazenagem, controle de estoque e compras, enfim, menor retorno sobre o investimento.

O número de itens (produtos distintos) comercializados por lojas de diferentes ramos pode ser muito variado.

Normalmente varia entre aproximadamente mil, lojas de conveniência, por exemplo, a 50 mil itens ou mais, grandes hipermercados.

Para entender e gerenciar melhor tal universo de produtos, os profissionais do varejo geralmente utilizam a subdivisão dos produtos em classificações hierárquicas.

Não existe, entretanto, consenso sobre o número de níveis a serem utilizados e nem mesmo sobre a estrutura de classificação de produtos.

Cada empresa busca classificar seus produtos conforme mais lhe convém, dependendo do foco de mercado, produtos oferecidos e grau de especialização.

Vários autores sugerem níveis hierárquicos semelhantes, como, departamento, seção, categoria, subcategoria, segmento, subsegmento, linhas de produtos entre outros.

A subdivisão dos produtos em níveis hierárquicos reflete, em última análise, as diferentes necessidades dos consumidores, ou seja, seus diversos interesses.

Por este motivo, os profissionais do setor costumam referir-se a seus principais grupos de produtos pela expressão "centro de interesse".

Pode-se definir um centro de interesse como sendo um grupo de produtos que mantém uma sinergia entre si, do ponto de vista do consumidor.

Em outras palavras, são produtos que orbitam ao redor de algum interesse específico do consumidor.

Por definição, todo produto está necessariamente dentro de um ou mais centros de interesse, e, existindo um mínimo de dois produtos em um centro de interesse, eles são necessariamente relacionados de alguma forma.

A problemática existente na determinação do conjunto de produtos a ser oferecido reflete as duas dimensões fundamentais que as norteiam, a amplitude ou extensão, que se refere à quantidade de centros de interesse distintos atendidos pela loja, e a profundidade, que se refere ao número de produtos diferentes existentes dentro de cada centro de interesse, quanto maior a profundidade, maior a sortidade de produtos disponíveis.

Tomando por exemplo o caso do centro de interesse que inclui queijos e derivados, uma loja especializada em queijos possui conjunto de produtos pouco extenso (pode atender somente a este centro de interesse), porém aprofundado, já que dispõe de tipos variados de queijos.

Já um supermercado tem a característica de ser generalista, ou seja, com conjunto de produtos extenso, mas pouco profundo-sua seção de queijos será provavelmente menor que a loja citada, porém ele atende a muitos outros centros de interesse.

Finalmente, um hipermercado pode ter conjunto de produtos ao mesmo tempo extenso e profundo, e no exemplo citado oferecer grande variedade de queijos, além de muitos outros grupos de produtos.

Tipos de varejo de acordo com a amplitude e profundidade do conjunto de produtos usualmente ofertado.

Classificação dos tipos de varejo segundo sua amplitude e profundidade.

A literatura sobre varejo no Brasil costuma referir-se ao conceito de centro de interesse como sendo um dos níveis hierárquicos da classificação de produtos, a categoria.

Gerenciamento de categoria é "o processo de administrar categorias como unidades estratégicas de negócio, visando obter não só uma melhor satisfação do consumidor, mas também melhores indicadores de desempenho e lucratividade".

Para um gerenciamento por categorias eficiente, uma hierarquia adequada deve ser criada para que cada produto seja devidamente classificado.

Em geral, existe uma relação direta entre a estrutura de classificação dos produtos e sua apresentação na loja, o layout da loja geralmente reflete perfeitamente a classificação dos produtos, agrupando-os sempre por categorias com o objetivo de estimular a compra de produtos complementares e facilitar a escolha dos substitutos.

As categorias exercem diferentes papéis para os varejistas.

Fatores como localização, segmentos de mercado atingidos, perfis dos consumidores, concorrência, política de preços, amplitude e profundidade do conjunto de produtos, entre outros, determinam a importância relativa de cada categoria para uma determinada loja.

A importância de cada categoria deve ser definida de acordo com sua capacidade de atrair clientes para a loja e de definir a imagem do varejista.

Uma classificação usual dos papéis de cada categoria segundo esta definição de importância é, Categorias de destino.

São os centros de interesse que definem o foco da loja, contendo os grupos de produtos de maior atração de clientes para a loja.

Categorias de rotina.

Apesar de não serem o foco principal da loja, possuem grande poder de atração de consumidores, que desenvolvem a rotina de comprar tais produtos na loja.

Categorias ocasionais.

Possuem baixo poder de atração de clientes, que compram os produtos destas categorias apenas ocasionalmente.

Categorias de conveniência.

São as categorias de menor poder de atração de clientes.

Normalmente contém produtos que os consumidores sequer esperavam encontrar na loja, e que podem eventualmente adquiri-los apenas por conveniência.

Naturalmente, o gestor deve tratar os produtos de categorias diferentes de forma distinta, dado que sua importância relativa para a loja varia.

A amplitude e a profundidade a serem adotadas para cada categoria dentro de uma loja específica varie conforme sua classificação, categorias de destino, por terem uma atratividade de clientes para a loja maior, devem ter maior amplitude e profundidade, e assim sucessivamente.

Amplitude, profundidade e atratividade das diferentes categorias.

De todas as variáveis do composto de marketing, a decisão de preço é aquela que mais rapidamente afeta a competitividade, o volume de vendas, as margens e a lucratividade das empresas varejistas.

As políticas de preço podem ser alteradas em curtíssimo prazo, e são imediatamente percebidas pelos consumidores, isto é, os consumidores demonstram ter bastante sensibilidade às mudanças de preço.

O varejista normalmente segue uma política de preço mais ou menos uniforme para todos os produtos da loja.

Esta política pode ser de preços acima do mercado, preços médios de mercado e preços abaixo do mercado.

Na política de preços acima do mercado, o varejista declara explicitamente sua intenção de não concorrer com base em preço, procurando maximizar os lucros não através de alto volume de vendas, mas sim por meio da elevada margem bruta por produto que pratica.

Esta política é normalmente adotada quando, A concorrência é pouco intensa.

Os consumidores são menos sensíveis a preço, dando maior valor aos benefícios e serviços prestados.

O varejista consegue diferenciar-se no mercado através de um ou mais componentes de marketing ponto, serviços, produtos, et superior.

A política de preços médios de mercado é a adotada pela maioria dos varejistas, fazendo com que o preço não constitua vantagem nem desvantagem competitiva.

Já com a política de preços abaixo do mercado os varejistas pretendem ter no preço sua principal arma competitiva.

Em geral, esta política está associada a uma forte pressão para baixar os custos, que inclui a oferta de menos benefícios aos clientes.

Para ter sucesso adotando-se esta política de preços, o varejista precisa ter alto volume de vendas e giro rápido dos estoques.

Um dos conceitos mais importantes com relação à definição dos preços de venda dos produtos é o de elasticidade a preços.

A elasticidade a preços, ou simplesmente elasticidade, reflete a sensibilidade dos clientes às alterações de preço.

Ou seja, é a relação entre a variação da quantidade demandada e a variação do preço, Situações de alta elasticidade normalmente ocorrem quando há forte substitutibilidade, ou seja, quando existem no mercado muitos produtos substitutos para atender à mesma necessidade.

Assim por exemplo se existirem muitas marcas de cerveja, o aumento de preço de apenas uma delas deve ocasionar queda aguda em sua demanda.

O mesmo ocorre no caso de alta substitutibilidade de lojas, se muitos estabelecimentos concorrem nas mesmas categorias de produtos, a variação de preço de apenas um deles deve ocasionar forte variação em sua demanda.

Produtos que não são vistos como sendo de primeira necessidade (considerados supérfluos) também têm forte elasticidade.

A situação oposta de baixa elasticidade ou inelasticidade ocorre basicamente em duas situações, quando há urgência em adquirir o produto (bens de primeira necessidade, por exemplo), ou quando há baixo grau de substitutibilidade.

Em situações de monopólio, ou quando todos as alternativas de compra sofrem reajustes simultâneos (como no caso de aumentos no preço de combustíveis, por exemplo), verifica-se pouca alteração na demanda.

A elasticidade cruzada reflete como a variação do preço de um produto afeta o comportamento de outros produtos.

É portanto a relação entre a variação da quantidade demandada do produto A e a variação do preço do produto B correlação preço-demand, Este conceito reflete como variações no preço de um produto afetam a demanda de outros produtos.

Dependendo do resultado da elasticidade cruzada, pode-se identificar os seguintes tipos de relação entre dois produtos ou categorias, Produtos complementares.

Apresentam elasticidade cruzada negativa, isto é, quando um aumento no preço do produto A afeta negativamente as vendas do produto B, e vice-versa.

Por exemplo, aumentos no preço de bebidas refrigerantes e/ou cerveja acarretam uma diminuição na demanda de aperitivos.

Produtos substitutos.

Apresentam elasticidade cruzada positiva, ou seja, uma variação no preço do produto A acarreta variação no mesmo sentido na demanda do produto B.

Utilizando exemplo semelhante, aumento no preço de Coca-cola causa aumento na demanda por Guaraná.

Os conceitos de elasticidade cruzada, produtos substitutos e produtos complementares estão intimamente ligados com aquele de centros de interesse, ou categorias de produtos.

De fato, um centro de interesse é, de acordo com a definição adotada neste texto, um grupo de produtos que estão de alguma forma relacionados.

Este relacionamento nada mais é do que uma elasticidade cruzada.

Portanto, havendo relacionamento na forma de elasticidade cruzada entre um grupo de produtos, conclui-se que tais produtos formam um centro de interesse, e que dois produtos quaisquer desse grupo devem ser, necessariamente, complementares ou substitutos.

Em outras palavras, deve existir sempre uma correlação entre o preço e a demanda de dois produtos pertencentes à mesma categoria.

Se esta correlação for a correlação positiva, os produtos são substitutos.

Exemplo de relacionamento entre produtos substitutos e complementares de dois centros de interesse.

Os produtos substitutos são considerados pelo consumidor como sendo altamente semelhantes, já que atendem a uma mesma necessidade.

Por isso, podem ser vistos como constituintes de grupos uniformes de produtos.

Cada centro de interesse possui um ou mais grupos de substitutos.

Produtos que pertencem a um mesmo centro de interesse mas não fazem parte do mesmo grupo de substitutos são, necessariamente, complementares.

Relacionamentos entre produtos substitutos e complementares de um ou mais centros de interesse.

Alguns produtos podem fazer parte de mais de um centro de interesse simultaneamente, já que atendem a necessidades distintas para o consumidor.

Entretanto, tais produtos continuam a formar um grupo coeso de substitutos.

Uma cadeira de pano, por exemplo, pode servir como cadeira de praia ou cadeira de pesca, dois interesses bem distintos.

Para cada um desses centros de interesse, os produtos complementares são diferentes, por exemplo guarda-sol no primeiro e vara de pescar no segundo.

Todas as cadeiras de pano vistas como equivalentes para o consumidor, contudo, formam um mesmo grupo de produtos substitutos.

O composto promocional é o elemento que o varejista utiliza para atrair os clientes para a loja e motivá-los para a compra.

O composto promocional deve estar integrado às demais decisões acerca do composto de marketing, tendo como objetivos básicos informar (fornecendo dados sobre a loja, produtos e serviços oferecidos), persuadir (influenciando o comportamento de compra do consumidor) e relembrar a audiência-alvo, firmando marcas e qualidades.

A literatura existente sobre este tema é vasta, sendo usual encontrar diversas subdivisões para o composto promocional.

Uma das divisões mais aceitas é aquela que divide as ações possíveis em propaganda, promoção de vendas e relações públicas.

Propaganda é "qualquer forma paga de apresentação e promoção de idéias, bens ou serviços por um patrocinador identificado".

A propaganda pode ser uma forma efetiva de transmitir mensagens, e para tanto é necessário que se siga uma metodologia coerente na aplicação dos recursos em propaganda, que deve passar por, Definição dos objetivos da propaganda.

Decisão sobre o orçamento a ser empregado.

Definição da mensagem e do(s) meio(s) a ser utilizado(s).

Medição do impacto da propaganda.

Já a promoção de vendas pode ser definida como uma "ferramenta de comunicação que oferece um valor extra ao consumidor, que pode ou não envolver o uso de mídia".

Em outras palavras, enquanto a propaganda justifica porque comprar, a promoção de vendas incentiva a compra.

A promoção de vendas pode consistir em, promoção ao consumidor (amostras, descontos, prêmios, garantias, entre outros).

Promoção mercadológica (distribuição de bens, combinação com propaganda, entre outros).

Promoção de negócios e força de vendas (feiras e eventos, premiações a representantes de vendas, entre outros).

Dentre os três componentes da propaganda e promoção, a promoção de vendas vem crescendo em participação relativa dos recursos empregados representando cerca de 675% do orçamento médio nas empresas americanas em detrimento da propaganda, cuja participação no orçamento do composto promocional vem caindo constantemente.

O terceiro item do composto promocional é representado pelas relações públicas, ou simplesmente publicidade, e envolve os programas corporativos designados para promover ou proteger a imagem de uma companhia ou de seus produtos.

Muitas empresas, incluindo as de varejo, operam ou subcontratam departamentos de relações públicas, a fim de monitorar as atitudes públicas de uma organização e distribuir informações à imprensa, seus clientes e comunidade em geral.

Mineração de Dados, ou Data Mining em inglês, consiste na geração de conhecimento a partir de dados acumulados.

Ela permite estruturar o conhecimento que está escondido nos bancos de dados corporativos, utilizando para tanto de tecnologias de banco de dados, reconhecimento de padrões, aprendizado automático, estatística entre outras.

Entre as técnicas de mineração de dados hoje empregadas, citam-se, entre outras, redes neurais, árvores de decisão, regras de associação, métodos de indução de regras, técnicas de segmentação, métodos para análise de cesta de compras, e dedução baseada em memória.

O termo data mining é definido em um escopo estreito por alguns autores, e mais amplo por outros.

Na sua definição estreita, mineração de dados é a descoberta automatizada de padrões interessantes escondidos nas bases de dados, e agrega métodos de aprendizado automático, muitos com origem na inteligência artificial.

Uma definição mais ampla abrange também a confirmação dos relacionamentos descobertos, ou seja, os métodos de teste de hipóteses incluindo análises estatísticas tradicionais, também são considerados mineração de dados.

Esta definição também é conhecida como o processo de Knowledge Discovery in Databases (KDD descoberta de conhecimento em bases de dados).

Finalmente, no escopo mais abrangente, a mineração de dados é vista como um processo, e não um método ou conjunto de técnicas, de descoberta de informações valiosas e não-óbvias a partir de grandes quantidades de dados.

Incorpora, assim, atividades de aquisição de dados, limpeza, formatação e pré-processamento dos mesmos, análise e validação dos dados, construção e implementação de ferramentas de apoio à decisão, e acompanhamento (recalibração e manutenção dos modelos) dos sistemas de apoio à decisão.

As raízes da mineração de dados estão na metodologia criada por J W Tukey nos anos 70 e 80, referenciada como Análise Exploratória de Dados.

As forças que pressionaram o avanço desta tecnologia podem ser divididas em, Supply-Side Factors.

São os efeitos do avanço tecnológico.

Inclui avanços no armazenamento de dados, na tecnologia de processamento, redução de custos de comunicação eletrônica, nas técnicas de análise (como redes neurais, algoritmos genéticos, árvores de decisão, indução de regras), na arquitetura cliente-servidor dos computadores e no advento dos repositórios de dados.

Aumento nas necessidades de análises rápidas em ambientes cada vez mais competitivos.

Colabora também a mudança na hierarquia organizacional, onde geralmente a figura do analista já não existe, e o gerente de marketing é o responsável pelas análises.

Por fim, a atenção dada pela mídia e livros sobre mineração de dados e relacionamento com o cliente pressionam a "corrida tecnológica".

O processo de mineração de dados só pode amadurecer durante os anos 90 devido aos seguintes fatores, Os dados estão sendo produzidos.

Mineração de dados só faz sentido quando existir grandes volumes de dados.

Os dados estão sendo guardados, e freqüentemente em bases separadas dos dados operacionais, permitindo análises para suporte à decisão.

O poder computacional necessário é acessível.

A pressão competitiva é forte.

Pacotes comerciais dedicados a análises de dados estão disponíveis.

As principais aplicações de mineração de dados estão nas áreas de, aquisição de novos clientes, manutenção dos clientes atuais, abandono de clientes não rentáveis, priorização de clientes (para mala direta ou outros fins), análise de cesto de compras, previsão de vendas, segmentação de mercado, risco de crédito, previsão de insolvência, detecção de fraude, entre outras.

Essas aplicações raramente podem ser automatizadas, apesar da crescente disponibilidade de pacotes comerciais que implementem o processo de mineração de dados.

Estatística convencional X Mineração de Dados.

Com relação às técnicas estatísticas, a mineração de dados distancia-se delas na medida em que, A quantidade de dados é muito maior.

Os dados estão "contaminados" com freqüência, isto é, não estão limpos livres de erros, de lacunas, inconsistências etc.

Os dados costumam estar fora da distribuição esperada (normal).

Há observações não-estacionárias, enviesadas e dependentes.

Ou seja, a amostragem não é aleatória.

Envolve a "descoberta" de padrões interessantes, muitas vezes de maneira indireta e com pouca interferência humana.

Isto significa que o processo pode ser aplicado mesmo sem ter um modelo imposto a priori para o fenômeno estudado.

Pode envolver dados não numéricos, qualitativos.

Para dar início ao processo de mineração de dados, faz-se necessária a constituição de um banco de dados a ser explorado, o que levanta algumas questões como, Quais as variáveis relevantes Qual o volume de dados necessário Onde estão os dados Qual o custo para obtê-los De maneira geral, os dados podem ser coletados interna ou externamente à organização.

Dados externos são de difícil obtenção e de qualidade suspeita, uma vez que não podem ser auditados pela própria organização.

Portanto, uma origem importante dos dados, senão a única, é a coleta feita pelos sistemas operacionais da empresa-aqueles que controlam sua operação diária, como sistemas de ponto de venda, sistemas transacionais, e geradores de relatórios operacionais.

Tais sistemas podem acumular dados diversos como informações dos clientes, dos produtos, de vendas, e mantê-los em formato altamente atomizado, guardando todos os detalhes sobre as operações realizadas.

Formam-se assim grandes bases de dados históricas, que geralmente têm de ser trabalhadas a fim de que possam ser exploradas convenientemente.

Normalmente, os dados existentes não foram coletados com o propósito específico de realizar análises exploratórias e de mineração, mas apenas para suportar os processos de negócio operacionais.

Isto pode levar a dados de baixa qualidade, valores ausentes e dados enviesados, que reduzem a aplicabilidade dos resultados encontrados.

Um dos objetivos da aplicação de técnicas avançadas de mineração de dados é justamente desenvolver um modelo válido mesmo se utilizando dados "sujos".

Uma questão comum diz respeito à quantidade de dados necessária para a realização de mineração de dados.

Uma resposta freqüente para esta questão é a de que as atividades de mineração de dados requerem enormes quantidades de dados.

Uma vez que o problema foi adequadamente especificado, apenas uma pequena parte de uma base de dados altamente heterogênea é necessária.

Mesmo se a quantidade de dados resultante for grande, normalmente apenas uma amostra é suficiente para a construção de modelos suficientemente precisos.

Quanto à seleção de dados, a abordagem da mineração de dados difere consideravelmente daquela usual na estatística.

Ao contrário de técnicas de seleção de variáveis, como a setpwise, a experiência até o momento sugere que esta deve ser feita da maneira mais ampla possível, já que o ponto forte das técnicas de mineração de dados é justamente sua capacidade de fazer com que os dados falem por si mesmo, sem restringir a análise a hipóteses previamente formuladas.

As técnicas devem, idealmente, ser capazes de identificar as variáveis relevantes e desprezar as irrelevantes.

É normal que os sistemas operacionais de uma empresa sejam fruto de vários esforços no seu processo de informatização.

Sendo assim, é freqüente a existência de sistemas operando em ambientes arquiteturas de hardware, versões de software et distintos.

Para que a mineração de dados possa ocorrer, estes dados devem ser consolidados em um mesmo ambiente, o Data Warehouse (DW).

Consolidação de dados em um Data Warehouse.

Processo de consolidação de dados em um Data Warehouse para fins analíticos.

Pode-se descrever um Data Warehouse como sendo um banco de dados especial, contendo as seguintes características básicas, Armazena os dados separadamente daqueles utilizados em aplicações operacionais.

Seu foco está no apoio à tomada de decisão.

Cobre um horizonte de tempo muito maior que sistemas transacionais, guardando séries temporais dos dados.

Inclui dados provindos de várias bases de dados operacionais, unificando-os e consolidando-os em um único local.

Os dados são processados e validados antes de serem inseridos no Data Warehouse, para que, uma vez no DW, passam a representar uma fonte segura, "oficial" dos dados.

Isso implica também que as definições dos atributos existentes no DW sejam únicos-por exemplo, o atributo lucro pode ter diferentes definições, conforme o propósito da base de dados.

Esta definição têm de ser única no ambiente DW.

É otimizado para chamadas complexas e com grande volume de dados, vindas de analistas e administradores.

Isso quer dizer que antigos paradigmas não são mais válidos neste ambiente, e normas como a simplificação de tabelas complexas em várias tabelas e relacionamentos, e a regra máxima de se evitar a todo custo qualquer tipo de redundância ficam enfraquecidas, sendo utilizadas apenas seletivamente.

Da mesma forma, é projetado para poucas inclusões ou alterações, a grande maioria das chamadas são para recuperação de dados ou resumindo, seus dados são não voláteis.

É integrado, une vários pontos de vista, não sendo específico a uma dada aplicação ou comunidade de usuários.

É orientado por assunto, os dados são organizados de forma que as análises sejam feitas por um determinado assunto do negócio.

Possuem, além de dados transacionais (atomizados), dados agregados, para agilizar consultas de agregação freqüentes.

Esta propriedade é chamada de granularidade do DW.

Data Warehouse como repositório para análises de dados.

Existem na literatura diversas taxonomias para classificar os usos e objetivos genéricos dos processos de mineração de dados.

Os objetivos mais citados são, sumarização (descrição e visualização), previsão, classificação, segmentação e análise de ligações ou associações.

Algumas destas tarefas são melhor abordadas de maneira top-down, através de teste de hipóteses.

Outras são melhor abordadas de maneira botton-up, através de técnicas de descoberta de conhecimento KD.

Os principais objetivos visados com a mineração de dados são, Previsão.

Lida com comportamentos futuros, conforme detalhado na seção "O Problema de Previsão de Vendas".

Todas as técnicas utilizadas para classificação podem ser usadas com sucesso para previsões, formulando-se os modelos com base em dados históricos, onde a variável a ser prevista é conhecida.

Alguns autores separam esta tarefa daquela de "estimação", onde o elemento tempo não está necessariamente presente pode-se estimar variáveis como renda, altura, número de filhos, balanço da conta corrente ou a probabilidade de responder a uma campanha publicitária.

Sumarização e Descrição.

Visa aumentar o grau de compreensão sobre um fenômeno complexo, representado por grande quantidade de dados de difícil compreensão, através da sumarização destes dados.

Utiliza basicamente técnicas estatísticas descritivas, e ferramentas de visualização gráficas.

Estas últimas de visualização gráfica automática a partir de dados e análises simples são consideradas essenciais, dadas as características inerentes ao ser humano de dificuldade em interpretar dados numéricos aliada à enorme facilidade em interpretar informações visuais.

Classificação.

Consiste em examinar as características de um objeto e tentar colocá-lo dentro de uma série de classes pré-definidas.

A construção de um modelo de classificação consiste na análise das características de elementos previamente classificados, para que se possa "aprender" a classificar novas observações.

Segmentação.

Segmentação é a atividade de separar em grupos homogêneos uma população heterogênea.

Os grupos de registros semelhantes são chamados de clusters, por isto estas técnicas também são conhecidas por técnicas de clustering.

A diferença entre segmentação e classificação é que, nesta última, as classes são pré-definidas, enquanto durante uma segmentação elas são dinamicamente criadas com base na similaridade entre os elementos.

O objetivo é identificar grupos semelhantes de registros, para tratar um grupo populacional como grupos com qualidades distintas.

Assim, as ações sobre o mercado podem ser realizadas de maneira dirigida, focando esforços em pontos onde a resposta é mais favorável.

Agrupamento por Afinidade.

É usado para determinar afinidades ou ligações entre objetos.

A aplicação clássica é a determinação de quais produtos vendem em conjunto em um supermercado também chamada de análise de cesto de compras para dispor ou promover itens de maneira a incentivar a compra de produtos afins.

Consiste basicamente na geração de probabilidades de compra conjunta, quem compra o produto A tem X% de chances de também comprar o produto B.

A simples aplicação de uma técnica de análise de dados avançada a fim de encontrar algum padrão nos dados não é suficiente para caracterizar um processo de mineração de dados.

Para tanto deve-se estar apto a responder aos padrões, a agir com base neles, transformando dados em informação, e informação em ação, e conseqüentemente em valor para a empresa.

SPSS frisa a importância de se ter uma estratégia para completar um processo de mineração de dados com sucesso.

A estratégia proposta pelo autor foi chamada de 5 A's, e corresponde às seguintes etapas.

Avaliar.

Avaliação da situação do negócio, e definição do problema a ser resolvido, dos objetivos da análise.

Acessar.

Acesso aos dados disponíveis e sua organização física.

Algum tempo deve ser gasto conhecendo e preparando os dados, o significado de cada campo e como eles são medidos.

Inclui também o pré-processamento dos dados, etapa fundamental para a construção de modelos representativos do problema, e que envolve tratamentos limpeza, agrupamentos, agregações et e transformações normalização, incremento ou redução de grandezas et no dados.

Muitas vezes, o pré-processamento dos dados geralmente é uma tarefa mais demorada que a construção do modelo em si.

Analisar.

Uso das técnicas de mineração de dados.

O desenvolvimento do modelo é um processo cíclico de ajuste e ação recíproca entre a definição do problema, definição / representação dos dados, definição do modelo e da acurácia desejada dos resultados.

Atuar.

Planejamento e execução de mudanças a partir do conhecimento gerado.

Automatizar.

Incorporar o processo às práticas usuais, através da criação de aplicações específicas.

Metodologias semelhantes são propostas na literatura, mantendo-se sempre a estrutura Identificação do problema Análise / construção do modelo Ação sobre o problema.

A chave para o sucesso de um projeto de mineração de dados é incorporá-lo aos processos de negócio corporativos.

Os estágios são interativos, e sempre dirigidos a resultados.

Os autores definem o processo como o ciclo virtuoso de data mining.

Esse processo é altamente interativo e exploratório.

Segundo os autores, conforme novos conhecimentos são gerados, novas hipóteses podem ser formuladas e testadas, fazendo com que se passe pelas diferentes etapas de maneira não linear.

O ciclo virtuoso de data mining.

Defini-se análise de dados como o processo de construção de um modelo apropriado aos dados.

De maneira concreta, as técnicas de mineração de dados são algoritmos (procedimentos matemáticos repetitivos, programáveis em computadores) que modelam relações ou padrões não-aleatórios em grandes bases de dados históricas.

Cada técnica de mineração de dados tem sua própria abordagem, porém elas compartilham algumas características em comum.

Dentre elas destacam-se, Elas melhoram sua performance gradativamente, conforme "aprendem" a partir dos dados de treinamento coletados.

Há sempre uma fase de treinamento, quando o modelo "aprende" os padrões e relacionamentos, seguida de uma fase de implementação, quando o modelo é posto à prova.

De maneira geral, os dados disponíveis afetam a escolha da modelagem mais apropriada.

Por exemplo, quando há muitas variáveis categóricas, as árvores de decisão são bem apropriadas.

Quando o relacionamento entre as variáveis de entrada e saída é difícil de ser decifrada, as redes neurais são a melhor escolha.

A seguir, uma breve descrição de algumas das técnicas de mineração de dados mais utilizadas é exposta.

A técnica de redes neurais é explanada em capítulo separado, dada sua importância para esta pesquisa.

As técnicas de indução de regras consistem no uso de ferramental matemático e estatístico-como freqüência de correlação, taxa de acurácia, e acurácia das previsões para desenvolver relacionamentos a partir dos dados apresentados.

Tipicamente, são criadas correspondências do tipo "se-então", baseadas em relações causais detectadas nas variáveis.

Cada relacionamento "se-então" extraído é chamado de "regra".

A metodologia para a geração de regras é não-supervisionada, e tende a consumir grande capacidade de processamento.

Um efeito negativo muito comum é a geração de padrões em demasia-muitos deles são triviais, contraditórios ou inúteis.

Ela também requer validação das regras por parte de um especialista, já que ela obtém apenas correlações, e não relações de causa e efeito propriamente ditas.

Outro problema visível desta técnica é que o número de possibilidades cresce exponencialmente com o aumento no número de itens.

Um dos principais usos das técnicas de indução de regras é o agrupamento de produtos por afinidade, ou, em sua denominação mais comum, a análise do cesto de compras.

A análise do cesto de compras é utilizada para encontrar grupos de itens que ocorrem simultaneamente em transações.

As informações coletadas são usualmente utilizadas no planejamento do layout de lojas, na elaboração de promoções de produtos conjuntos, agrupamento de produtos, comparação de vendas entre lojas ou entre períodos com promoções, detectar sazonalidades etc.

Quando as transações não são anônimas, as análises podem utilizar o fator tempo como um elemento, para agrupar produtos adquiridos por um mesmo consumidor ao longo do tempo.

O processo de análise do cesto de compras consiste em, Escolher os itens.

Para tanto é comum realizar uma taxonomia dos itens a fim de generalizá-los propriamente.

Usualmente, são analisados inicialmente os itens genéricos-em hierarquia superior, como segmentos ou linhas de produtos.

Posteriormente, podem ser analisados os itens mais específicos (atomizados), geralmente com o uso de apenas uma amostra dos dados.

O nível de generalização entre os produtos, dentro da taxonomia existente, pode variar.

A freqüência com que os itens aparecem também é indicativa da generalização necessária-produtos que raramente vendem devem ser agrupados para que sua freqüência aumente, e vice-versa.

Por exemplo, se há muitos registros de "coca-cola", este produto pode ser analisado de forma atomizada, enquanto a baixa demanda por outras marcas concorrentes pode forçar seu agrupamento na categoria "refrigerantes de cola".

Gerar as regras através da contagem de ocorrência simultânea dos produtos.

Analisar as regras interessantes, uma vez que podem ser geradas milhares de combinações diferentes, muitas delas inexplicáveis ou óbvias.

Algumas vezes faz-se necessária a geração de itens "virtuais", ou seja, que não aparecem na taxonomia original.

Por exemplo, uma marca específica pode aparecer em produtos bem distintos, e um item virtual indicando sua presença pode ser útil para detectar relacionamentos nas vendas dos produtos dessa marca.

Outros exemplos típicos são informações sobre as transações-se foi paga em dinheiro, cheque ou cartão, qual era o dia da semana, entre outros.

A inclusão de itens "virtuais" deve ser limitada, contudo, a itens acionáveis-aqueles que geram regras úteis à corporação.

Outro tipo de regra que pode ser gerada são as regras de dissociação, que funcionam como as regras de associação, porém possuem o conector "não", além do conector "e", como por exemplo a regra "se A e não B então C".

Corre-se, contudo, o risco de gerar inúmeras regras do tipo "se não A e não B então não C", que não são regras acionáveis.

Assim, é comum testar regras dissociativas apenas com os itens mais freqüentes.

Durante o processo de análise, os fatores mais importantes para monitorar a qualidade das regras criadas são, Suporte à regra.

Número de observações da regra em relação ao número de observações total.

Por exemplo, se tivermos 5 transações e delas incluírem "soda" e "suco", então o suporte para a regra "se soda então suco" é / 5 ou 40%.

confiança.

Razão entre o número de transações que suportam a regra e o número de transações onde o elemento condicional da regra ocorre.

Para o mesmo exemplo acima, se em todos os casos onde ocorre soda também ocorrer suco, a confiança é de 100%.

Entretanto se transações contiverem suco, com apenas delas contendo também soda, a regra "se suco então soda" possui confiança de 50%.

Melhoria, também chamada de lift ou improvement.

Mede o quanto uma regra melhora a previsão de um resultado do que simplesmente assumindo o resultado.

Por exemplo, se um produto ocorre 45% das vezes, uma regra com confiança igual a 45% tem lift igual a 1.

A melhoria é definida matematicamente como a freqüência observada para uma regra dividida pela freqüência esperada, dadas as freqüências de cada um dos itens.

A análise do cesto de compras é de particular interesse nesta pesquisa, pois se trata da técnica utilizada para a detecção do grupo de produtos com maior grau de inter-relacionamento dentro do conjunto de dados coletados, definindo assim um centro de interesse.

A aglutinação dos produtos relacionados serviu para a separação dos produtos a serem analisados cuja demanda foi previst e também como ferramental de auxílio na qualidade das previsões, conforme será mostrado adiante.

Um sistema especialista consiste de uma base de conhecimentos composta por regras extraídas de especialistas, aliada a um sistema de inferência lógica.

Um sistema especialista é capaz de simular o comportamento de especialistas com conhecimentos em um domínio muito estreito e profundo.

O objetivo é chegar nas mesmas conclusões que um especialista chegaria, passadas as informações a respeito do problema.

Sistemas especialistas são altamente supervisionados, ou seja, o treinamento não é automático e exige grande esforço do analista.

Eles possuem performance comparável a de humanos especializados em um domínio de problema, consistindo de conhecimento cognitivo específico.

Os domínios de conhecimento ideais são os de escopo bastante estreito, que permitam que o especialista resolva o problema num curto espaço de tempo.

O conhecimento deve ser facilmente capturável, consistente, de explanação simples, de representação honesta (geralmente através de afirmações do tipo "se-então") e que não dependa de bom senso.

Árvores de Regressão são modelos simples baseados na anatomia de uma árvore, onde cada galho particiona estrategicamente os dados em classes e subclasses sucessivamente.

A cada divisão, é escolhida a melhor forma de separar e classificar os dados, utilizando-se da característica que mais os distingue (maior poder de separação), através de medidas estatísticas.

Em cada separação (ou nó), pode-se medir, o número de registros presentes no nó.

A forma como eles foram separados.

O número de registros em cada galho e a porcentagem de registros classificados corretamente (de acordo com a variável dependente).

As árvores de regressão possuem algoritmos não-supervisionados, ou seja, são capazes de processar automaticamente os dados.

Com o modelo criado, cada partição (ou galho) representa uma regra clara, constituída durante o treinamento.

Estas regras são então usadas para classificar o comportamento de novas situações.

A grande atratividade das árvores de regressão está no fato de que suas respostas representam regras facilmente acionáveis e com grande poder explicativo.

Outra característica positiva é o fato de que fornecem indicações claras dos campos mais importantes para previsão ou classificação.

Memórias associativas, também conhecidas por dedução baseada em memória ou Memory Based Reasoning (MBR), são pares ou grupos de itens associados através de uma memória de longo prazo, usando instâncias conhecidas como um modelo para fazer previsões sobre novas instâncias.

Consiste na procura de vizinhanças nas instâncias conhecidas e combinação de seus valores para prever novas instâncias.

Os dois elementos chaves são a função distância utilizada para encontrar os vizinhos mais próximos e a função de combinação para calcular os valores das previsões.

O processo segue os seguintes passos.

Escolher os dados históricos apropriados.

Determinar a forma mais eficiente de representar os dados históricos.

Determinar as funções distância e combinação, e o número de vizinhos.

Ou seja, são identificados os casos semelhantes nos dados históricos, e então a informação associada a estes casos é aplicada através de equações matemáticas convenientemente selecionadas.

A técnica é apropriada para tarefas de classificação e previsão.

Técnicas estatísticas podem ser combinadas às técnicas de memórias associativas para auxiliar na construção da função combinação.

Não há consenso na literatura sobre a técnica ideal para cada aplicação, nem sequer de quais são os critérios para avaliação das diferentes técnicas de mineração de dados.

Critérios para avaliação das técnicas, robustez, grau de automação, velocidade, poder explanatório, acurácia, quantidade de pré-processamento necessário, escalabilidade, facilidade de integração e habilidade para lidar com muitos atributos (variáveis), facilidade de compreensão do modelo, facilidade de treinamento, facilidade de aplicação, capacidade de generalização, utilidade e disponibilidade.

Ainda de acordo com os autores, os principais fatores que determinam a escolha da técnica a ser utilizada para determinado fim são, preponderância de variáveis categóricas ou numéricas, número de campos, número de variáveis dependentes, orientação no tempo, e presença de dados textuais.

Conclui-se portanto que não há critérios universais, aplicáveis a todas as técnicas de mineração de dados, cada técnica requer critérios específicos de medição.

É extremamente difícil comparar os modelos entre si, já que operam de maneira distinta.

Então, a única forma de avaliá-los é através da medição de sua habilidade em desempenhar a tarefa para a qual foram construídos, ou seja, classificação, previsão, etc.

Neste trabalho, são de interesse as técnicas voltadas para previsão, e, portanto, o principal critério de avaliação deve ser a habilidade do modelo em desempenhar sua função, prever a demanda futura.

Assim sendo, a medida de maior interesse a acurácia do modelo.

Redes neurais artificiais são sistemas de processamento de informações distribuídos, compostas por muitos elementos computacionais simples que interagem através de conexões com pesos distintos.

Inspiradas na arquitetura do cérebro humano, elas exibem características como a habilidade de aprender padrões complexos de informação e generalizar a informação aprendida.

Cada elemento computacional não-linear é chamado nó, e é densamente interconectado através de conexões diretas.

Os nós operam em passos discretos, de forma análoga a uma função de dois estágios, o primeiro estágio calcula a soma dos sinais de entrada, atribuindo pesos aos sinais.

O segundo estágio consiste da aplicação de uma função de saída, chamada de ativação.

Esquema de funcionamento de cada nó em uma rede neural artificial.

Cada nó recebe um ou mais valores de entrada, que são combinados em um único valor com o uso de diferentes pesos para cada entrada e transformam-no em um valor de saída (S) através de uma função de ativação.

Uma das funções de ativação mais usadas é a função logística, que pode ser escrita como, Este funcionamento aparentemente simples de cada neurônio resulta, após o processamento coletivo de todos os nós, em uma capacidade de execução de diversas tarefas com eficiência.

As redes neurais podem ser caracterizadas pelas seguintes propriedades, a estrutura da rede (topologi, como e o que a rede processa (propriedade computacional), e como a rede aprende (propriedade de treinamento).

A topologia da rede refere-se ao número de camadas e nós utilizados.

Uma rede neural artificial deve ser composta por pelo menos duas camadas, uma contendo as entradas e uma contendo as saídas da rede.

Usualmente, são utilizadas também uma ou mais camadas intermediárias, caracterizando assim as chamadas redes neurais multicamadas, compostas por.
Camada de entrada, consiste das variáveis de entrada do modelo, devem ser sempre conhecidas.

Camada de saída, contém um ou mais nós representando os resultados finais do processamento para uma dada entrada, a rede fornece uma saída correspondente.

Camadas intermediárias ou camadas ocultas, podem existir uma ou mais camadas de processamento intermediário, que tornam o modelo mais refinado e não-linear, com a capacidade de aprender padrões mais complexos.

A propriedade computacional de uma rede neural artificial refere-se ao modo pelo qual os elementos computacionais (nós) são ativados e processados.

Diversas funções de ativação podem ser empregadas, de forma linear e não-linear.

Já o aprendizado é o processo no qual uma série de valores de entrada é apresentada seqüencialmente, e os pesos da rede são ajustados até que ela reflita a saída desejada.

As estratégias de treinamento são divididas em treinamento supervisionado e não-supervisionado.

O treinamento supervisionado requer a presença de valores de entrada e a correspondente saída, a partir da qual é calculado o erro, utilizado para corrigir o peso das conexões da rede.

No treinamento não-supervisionado, são apresentados à rede apenas vetores de entrada.

A saída é construída automaticamente pela rede, sendo capaz de capturar regularidades nos valores de entrada, sem receber nenhuma informação adicional.

O processo de montagem e treinamento de uma rede neural pode ser descrito pelos seguintes passos, É definida a topologia da rede.

A rede sofre um processo de aprendizado, através de informações de entrada e saída conhecidas.

As conexões entre os nós são modificadas num processo iterativo, até que a rede forneça saída compatível com as informações apresentadas.

Por isso diz-se que as redes neurais adquirem conhecimento através da experiência.

A rede está pronta para fornecer respostas para novos problemas.

O aprendizado pode continuar indefinidamente.

O conhecimento reside na "força" entre as ligações entre neurônios, ou seja, no peso dado a cada conexão.

A modelagem através de redes neurais artificiais é uma das técnicas de mineração de dados mais utilizadas, largamente empregada em tarefas de classificação e previsão, além de poderem ser aplicadas à geração de conhecimento indireto (na forma de mapas auto-organizáveis).

Utilizaram-se as redes neurais para previsão de vendas. Sua escolha como técnica de modelagem baseou-se em algumas características decisivas reportadas na literatura, mostradas a seguir.

As redes neurais são robustas e tolerantes a falhas, graças ao seu paralelismo (a contribuição dada por um único elemento de processamento não é tão importante).

Isto a torna uma técnica bastante indicada para uso na mineração de dados, onde nem sempre os dados disponíveis são confiáveis.

Diversos autores mostraram a capacidade das redes neurais em lidar com estruturas de dados não-lineares.

Esta é uma característica primordial, já que os problemas em marketing sempre lidam com dados como vendas e preços, onde as séries de dados estão inseparavelmente ligadas a pontos de inflexão, tendências e não-linearidades.

Muitas vezes os dados podem ser até mesmo caóticos e mesmo nesta condição podem ser previstos com precisão por uma rede neural.

Com o uso de redes neurais, um conhecimento a priori da relação entre as variáveis não é requerido, já que elas são auto-adaptativas.

Ou seja, elas são capazes de gerar modelos não lineares sem um conhecimento prévio sobre os relacionamentos entre as variáveis de entrada e saída, podendo ser portanto mais genéricas e flexíveis que metodologias estatísticas tradicionais.

Tome-se como exemplo uma modelagem feita através de regressão, a estipulação da estrutura dos dados (de segundo ou terceiro grau, por exemplo), assim como a determinação de uma variável dependente e de uma ou mais variáveis independentes, são necessários a priori, para que o modelo matemático possa ser gerado.

Já com o uso de redes neurais, os relacionamentos relação de dependênci entre as variáveis são detectados automaticamente.

Podem até mesmo ignorar variáveis que não contribuem nas previsões, o que permite que algumas experimentações possam ser feitas sem prejuízo para o modelo resultante.

Como conseqüência, um melhor resultado é esperado quando o relacionamento entre as variáveis não é aderente ao modelo assumido, e quando há forte interdependência entre os fatores.

Pode-se provar também que uma rede neural com uma camada oculta é capaz de aproximar qualquer função contínua, com a precisão que se desejar.

Uma rede neural pode também mimificar uma série temporal qualquer.

Além disso, relações de multicolinearidade são automaticamente descobertas e assimiladas pela rede de relacionamentos.

A performance das redes neurais não é depreciada por relações de multicolinearidade, como acontece com a regressão.

Condições fora da normalidade, violação de hipóteses, pontos de alta influência e transformações também são bem assimiladas pelas redes neurais.

As vantagens das redes neurais que levaram à sua escolha como método de modelagem para previsão de vendas, conforme reportadas na literatura citada acima, podem ser resumidas da seguinte forma, São robustas e tolerantes a falhas.

Dados imprecisos ou faltantes, relações de multicolinearidade e violações de hipóteses são automaticamente descobertas e assimiladas, não interferindo fortemente no resultado final das previsões.

São inerentemente não-lineares.

O conhecimento a priori da relação entre as variáveis não é requerido.

Portanto, elas comportam-se bem em problemas onde o conhecimento a respeito é de difícil estruturação.

São capazes de aproximar qualquer função contínua ou série temporal, inclusive séries caóticas.

Possuem boa capacidade de generalização.

Após a fase de aprendizado, onde a rede é submetida a dados históricos conhecidos, ela é capaz de prever novos registros com considerável sucesso, mesmo na presença de ruído e variáveis expúrias ao problema.

Vários autores buscaram comparar o desempenho de modelos baseados em redes neurais com metodologias estatísticas tradicionais, porém os resultados ainda não são conclusivos.

A principal explicação para o fato parece ser a ausência de uma metodologia sistemática para o desenvolvimento de modelos baseados em redes neurais, consistindo basicamente de tentativa e erro, o que favorece o surgimento de resultados inconsistentes.

O número de fatores envolvidos na criação de modelos neurais, entre eles sua topologia, método de treinamento, e dados fornecidos para o aprendizado, torna o processo de desenvolvimento de tais modelos ainda mais complexo e sujeito a grandes variações no resultado final.

Os modelos construídos com o uso de redes neurais artificiais já mostraram ter boa aplicabilidade em problemas de previsão, tendo superado significativamente várias outras modelagens em diversas áreas do conhecimento.

Em alguns casos, entretanto, as redes neurais podem mesmo ter poder preditivo menor que metodologias tradicionais, seja simplesmente porque os dados analisados são lineares ou porque o desenvolvimento do modelo baseado em redes neurais não foi adequado.

Vários autores compararam as capacidades preditivas de redes neurais com técnicas estatísticas tradicionais, muitos deles concluindo que as redes neurais possuem performance superior.

Quatro modelos econométricos entre si, e então utilizam os mesmos dados na geração de modelos baseados em redes neurais, para comparação de suas previsões.

Os resultados sugeriram pouca ou nenhuma melhoria nas previsões quando da utilização de redes neurais.

Um ponto crítico durante a modelagem de redes neurais foi a seleção de camadas ocultas (hidden layers).

Quanto mais complexa a inter-relação entre as variáveis, maior o número de camadas ocultas necessárias.

Camadas a menos fazem com que o modelo perca a riqueza das relações entre as variáveis, perdendo em qualidade nas previsões.

Camadas em demasia provocam o efeito de overfitting, que é o excessivo ajuste aos dados de treinamento, perdendo sua capacidade de generalização.

Segundo os autores, apenas uma camada oculta é geralmente suficiente para problemas de previsão, o número ideal, entretanto, deve ser encontrado por tentativa e erro.

As maiores dificuldades relacionadas à modelagem com uso de redes neurais são, a dificuldade em compreender os modelos produzidos (baixa explicabilidade).

A sensibilidade ao formato dos dados de entrada, diferentes representações para os mesmos dados podem resultar em modelos distintos.

Além disso, requerem entrada entre 0 e 1, o que significa que necessitam muitas vezes de grande pré-processamento.

Podem convergir prematuramente, resultando em soluções não ideais.

Exemplos na literatura onde o desempenho das redes neurais foi melhor e/ou pior que outras técnicas analisadas, Comparação de desempenho da técnica de redes neurais artificiais frente a outras técnicas estatísticas.

Este trabalho está fundamentalmente calcado em uma abordagem quantitativa de pesquisa, a fim de atender ao objetivo básico de criação de uma metodologia capaz de identificar padrões de relacionamento úteis na previsão de vendas individual no varejo, com o uso do processo de mineração de dados.

Os estudos quantitativos são aqueles onde o pesquisador estabelece hipóteses e as testa por meio da mensuração de variáveis operacionalmente definidas, quantificando o resultado com o uso de variados métodos quantitativos e estatísticos.

É um tipo de pesquisa tradicionalmente aceita como tendo confiabilidade (fidedignidade) e validade, desde que realizada seguindo uma metodologia rigorosa.

Normalmente, utilizam-se testes estatísticos e análise de variância para verificação das hipóteses estatísticas.

A validação dá-se pelo nível de significância adotado nos testes estatísticos.

As pesquisas quantitativas podem ser orientadas por delineamentos experimentais ou quase-experimentais, ou serem estudos descritivos-exploratórios-isto é, buscarem a descrição da população ou fenômeno e o estabelecimento de relações entre variáveis e fatos.

Esta pesquisa baseou-se numa metodologia orientada por um estudo exploratório dos dados disponíveis, isto é, buscou a descrição do fenômeno-demanda no varejo-e o estabelecimento de relações entre as variáveis explicativas e as vendas.

Apesar de tratar-se de uma pesquisa eminentemente quantitativa, não foram estabelecidas hipóteses a priori, mas sim questões de pesquisa que foram respondidas por meio de um processo efetivo de mineração dos dados, onde o pesquisador realiza levantamentos e análises iterativas buscando informações "escondidas" nos dados.

Construiu-se, através do uso da técnica de redes neurais artificiais, um modelo de previsão de vendas a fim de prever a demanda individualizada de produtos em uma loja de varejo, levando em consideração fatores explicativos da demanda (modelagem causal) e também o relacionamento entre diversos produtos.

A metodologia de pesquisa seguiu os seguintes passos para a elaboração do modelo de previsão de vendas, Obtenção dos dados de vendas atomizadas no varejo.

Seleção e limpeza dos dados.

Análise do cesto de compras para seleção de um grupo de produtos relacionados entre si.

A determinação de um grupo de produtos estreitamente relacionados também serviu para responder a uma das questões de pesquisa levantadas, esta informação pode auxiliar na capacidade preditiva dos modelos de previsão baseados em redes neurais.

Tratamento e preparação dos dados para modelagem através de redes neurais.

Geração dos modelos baseados em redes neurais, testando-se diferentes combinações de topologia e variáveis de entrada.

É selecionada a rede neural com maior acurácia (menor erro nas previsões).

Comparação do desempenho medindo-se a acurácia alcançada dentro e fora dos dados de treinamento.

Para comparação relativa dos seus resultados, a rede com maior acurácia foi confrontada com as modelagens naive e de regressão linear.

Tais comparações foram escolhidas por serem técnicas bastante utilizadas para determinação da capacidade preditiva mínima de modelos de previsão, e amplamente utilizadas na prática pelas empresas.

Processo de análise de dados executado.

Conforme mostrado no modelo acima, a pesquisa foi conduzida por meio da geração de redes neurais artificiais que pudessem prever o comportamento das vendas no nível individualizado, tomando como entrada variáveis explicativas da demanda, a própria demanda histórica, e as variáveis do composto de marketing, agrupadas de acordo com um conjunto de produtos inter-relacionados, pertencentes a um mesmo centro de interesse, selecionados através da técnica de análise do cesto de compras.

A pesquisa realizada foi desenvolvida com base nas vendas de uma grande empresa varejista brasileira, doravante denominada simplesmente EMPRESA.

Sendo uma rede varejista de grande porte, os produtos comercializados pela EMPRESA são bastante variados, passando por eletro-eletrônicos, utilidades domésticas, móveis, vestuário, brinquedos, entre outros.

A rede de pontos de venda é distribuída em todo o território nacional, totalizando mais de 88000 m de área de venda distribuídas em mais de 100 pontos de venda.

Além da venda nos pontos de venda, a EMPRESA realiza também a venda através da Internet e por pedidos.

A fim de obter uma visão geral do processo de previsão e da capacidade preditiva atual da EMPRESA, foi aplicado um questionário aos diretores geral, de vendas e de logística, e aos responsáveis pelas áreas de compras e vendas.

O questionário visou essencialmente descobrir como é executada a função de previsão dentro da EMPRESA, e se esta função é vista com uniformidade dentro da empresa.

De acordo com as respostas dadas, a previsão é realizada pela EMPRESA de duas formas distintas, e com dois objetivos, por departamentos diferentes.

Existe uma previsão de vendas agregada, formulada pela diretoria da empresa anualmente, com base nos resultados anteriores e expectativas para os próximos meses, especialmente macroeconômicas e de desempenho da empresa.

Essa previsão serve de base para tomada de decisões estratégicas, como a abertura ou fechamentos de lojas, ou incremento em linhas de produtos.

É aferida e recalculada mensalmente.

Existe, entretanto, outra previsão de vendas, num nível operacional, que é utilizada basicamente no planejamento de compras, e é de responsabilidade dos gerentes de produto, que são alimentados por dados dos compradores.

Os compradores respondem pela previsão, sendo subordinados pela área de Logística.

A previsão agregada é feita anualmente de forma totalmente qualitativa, tomando-se como base números do ano anterior e a percepção dos gestores.

Já a previsão detalhada é realizada por produto, utilizando-se a técnica naive simples com a média das vendas dos últimos 7 dias.

Em outras palavras, a previsão de vendas operacional é exatamente igual às vendas dos últimos 7 dias.

Como era de se esperar, este modelo gera graves distorções, principalmente porque a EMPRESA externou que suas vendas são caracterizadas por picos acentuados, seja devido a datas comemorativas ou devido a campanhas de marketing muito agressivas e com duração muito curta.

A previsão naive é então modificada qualitativamente pelos gestores de compras da EMPRESA, tendo em vista seu sentimento com relação ao mercado, feedbacks dados pelos fornecedores, pelos vendedores e gerentes, e principalmente tendo em vista desempenhos em anos anteriores.

As previsões não são aferidas, ou seja, a EMPRESA não sabe exatamente quanto está errando nas previsões.

Há o sentimento geral de que o erro é grande, principalmente evidenciado pelo excesso de alguns produtos em estoque, enquanto os produtos mais vendidos freqüentemente faltam nas prateleiras.

Em resumo, a EMPRESA vê a função de previsão como sendo uma questão muito relevante, mas não vê possibilidade de melhoria imediata.

A criação de um modelo causal de previsão de vendas quantitativo dá-se a partir da coleta de seqüências temporais de dados sobre as vendas efetuadas, conjugados com informações que possam explicar as vendas.

Ou seja, dado um determinado período de tempo, a quantidade de produtos vendidos é sumarizada em totais por período, semanal, por exemplo, e tabulada junto com as demais variáveis explicativas, como o preço praticado no período, a divulgação realizada, dados macroeconômicos, entre outros.

Neste trabalho, o objetivo visado foi a criação de modelos de previsão não-agregados, e, portanto, os dados utilizados foram dispostos no nível de produto.

Ou seja, as vendas foram tabuladas por SKU, que é o identificador único de um produto.

Adicionalmente, buscou-se por dados obtidos no varejo, onde o gestor tem maior área de manobra sobre as variáveis independentes e a reação às mudanças sobre o composto de marketing é mais rápida, já que se lida diretamente com o cliente final.

Foram coletados dados relativos às vendas da EMPRESA em nível atomizado-venda a venda-no período de Novembro de 1998 a Junho de 2002, totalizando 4meses, sendo coletados dados referentes a todos os produtos vendidos em cinco pontos de venda selecionados.

O dicionário dos dados coletados encontra-se no ANEXO B.

Os dados coletados estavam distribuídos em múltiplos bancos de dados, incluindo diferentes tabelas constando, dados sobre os clientes, dados relativos às vendas (transação por transação, numa tabela de volume expressivo), dados sobre as transações a prazo, dados sobre as lojas, tabelas de produtos, tabelas de fornecedores, dados dispersos sobre as promoções realizadas, entre outros.

Todos estes bancos de dados foram unificados e convertidos.

Por exemplo, tabelas existentes em bases de dados no formato IBM DBe no formato Excel foram todas convertidas para o formato Microsoft SQL Server, num único banco de dados consolidado.

Buscando detectar comportamentos de venda regionais, optou-se por realizar a análise de todos os produtos comercializados em apenas um ponto de venda.

A loja escolhida foi aquela com maior número de registros, e seus dados foram separados dos demais.

Os dados foram, num primeiro momento, mantidos com suas escalas originais, já que a primeira análise realizada foi a análise do cesto de compras, que visa apenas o estudo das freqüências dos registros para detectar os produtos inter-relacionados, sinalizando os centros de interesse do consumidor.

Obteve-se primeiramente uma visão geral dos dados, incluindo a avaliação de possíveis outliers por meio de estudos de freqüência e plotagem de gráficos temporais e boxplot.

A primeira observação relevante diz respeito à forte curva ABC apresentada, ou seja, apesar de existirem muitos produtos distintos são exatos 48287 SKU s em 430 linhas de produtos apenas uma fração deles possui movimentação freqüente.

Outra característica importante são as vendas realizadas a prazo, responsáveis por 61,8% do total.

As vendas à prazo são especialmente interessantes por tratar-se de transações identificadas, onde há informações precisas sobre o consumidor, que permitem análises de perfis e padrões de consumo.

Em muitos registros foram encontradas inconsistências que forçaram seu descarte do banco de dados, como inconsistências de datas entre a transação de venda a prazo e seu registro na tabela correspondente, juros negativos, estoque sem registro ou negativo, ausência de classificação de produto, entre outros.

Ao final desta etapa, obteve-se um banco de dados unificado e livre de inconsistências, contendo informações sobre todas as vendas realizadas em apenas uma loja, as quais totalizaram 279149 transações, constituídas por vendas de 8106 produtos de 558 marcas distintas.

A análise do cesto de compras é uma técnica de mineração de dados utilizada para encontrar grupos de itens que ocorrem simultaneamente em transações, conforme mostrado na seção "Indução de Regras / Análise do Cesto de Compras".

Com ela são geradas regras do tipo "se o consumidor compra o produto A, há uma probabilidade P de que ele também compre o produto B".

A análise do cesto de compras foi utilizada objetivando formar os centros de interesse de compras, ou seja, para encontrar grupos de produtos inter-relacionados.

A utilização da técnica de análise do cesto de compras teve como propósitos, separar um grupo coeso de produtos, minimizando o total de produtos a serem previstos pelo modelo, uma vez que o objetivo da pesquisa foi testar a aplicabilidade das redes neurais e da metodologia de modelagem empregada, e não construir um modelo de previsão para todos os produtos da EMPRESA.

Determinar quais produtos fazem parte de um mesmo centro de interesse, e assim investigar se essa informação traz alguma melhoria na capacidade preditiva de modelos de previsão baseados em redes.

Duas estratégias de abordagem dos dados foram tomadas,análise através das transações, buscando quais itens são vendidos em conjunto numa mesma transação.

E análise horizontal por cliente, procurando quais produtos um mesmo cliente compra ao longo do tempo.

As análises realizadas e os resultados obtidos seguem abaixo.

Todas as análises foram realizadas com o uso do sistema de mineração de dados IBM Intelligent Miner 61.

A análise do cesto de compras por transação foi executada tomando-se os campos de "série da nota fiscal" e "nota fiscal" concatenados, a fim de se obter um identificador único para cada transação, um identificador do ticket de venda.

Uma mesma transação, que tem conseqüentemente a mesma nota fiscal, pode conter mais de um produto, aqueles produtos que são vendidos juntos com maior freqüência devem, naturalmente, ter maior relação entre si.

As seguintes dimensões foram tomadas na análise do cesto de compras por transação, Análise por produto, ou seja, atomizada ao nível de SKUs.

Análise por linha de produto, que representa os produtos substitutos, de acordo com a classificação adotada pela EMPRESA.

Análise por setor da loja, de acordo com a classificação adotada pela EMPRESA.

Como mostrado anteriormente, duas medidas são essenciais na análise do cesto de compras, o suporte e a confiança.

O suporte representa a porcentagem de vezes em que a regra aparece, ou seja, sua freqüência.

A confiança representa, numa regra do tipo "se A então B", a porcentagem P de transações onde, dado que ocorre A, também ocorre B, a grosso modo, a probabilidade de que quem compra o produto A também irá comprar o produto B.

O pacote de mineração de dados utilizado infere todas as regras existentes automaticamente, tomando sempre como entrada o campo que identifica a transação, e quais os parâmetros de suporte e confiança mínimos desejados.

Se estes parâmetros forem muito altos, o número de regras gerado é pequeno, não trazendo resultados interessantes e por outro lado, se o suporte e/ou confiança exigidos forem muito baixos, são geradas regras em demasia, dificultado sua interpretação e análise.

Por isso, em todas as análises efetuadas os parâmetros de suporte mínimo e confiança mínima foram selecionados iterativamente, de acordo com o número de regras gerado.

Os resultados foram sempre ordenados segundo o multiplicador Support * Confidence, que balanceia modelos com maior suporte e confiança.

A melhoria encontrada com cada regra, lift, também foi medida, representando o quanto a freqüência medida é maior que o esperado.

Na modelagem buscou-se por regras com associação positiva-onde a compra do item A influencia positivamente a compra do item B-ou negativa-onde quem compra o item A não compra o item B.

Resultados da análise do cesto de compras por produto, abordados por transação.

Como se pode observar, aparentemente há forte relacionamento entre os produtos "ventiladores" e "panelas".

De cada 100 compradores do ventilador, 60 também compraram a panela, já que a confiança é de 60%.

De acordo com a melhoria (lift) alcançada, isso representa, uma ocorrência 11,vezes maior do que a freqüência esperada estatisticamente, dadas as freqüências isoladas dos produtos.

Os resultados mostrados foram obtidos adotando-se suporte mínimo de 0,05%-um limite inferior consideravelmente baixo.

Mesmo assim, o número de regras obtidas foi pequeno-apenas fato este que deve ser atribuído à grande quantidade de produtos presentes no universo de dados, e conseqüentemente a freqüência de cada produto, independente de estar associado ou não, é sempre baixa.

Mesmo assim, nas regras obtidas há boa confiança e grande melhoria.

Resultados da análise do cesto de compras por linha de produto, abordados por transação.

O mesmo procedimento foi efetuado em análises semelhantes.

As linhas de produtos são agrupamentos de produtos definidos pela EMPRESA, e foram utilizadas como forma de aglutinar os produtos semelhantes.

A agregação de produtos é um artifício usual na análise do cesto de compras, e normalmente faz com que o número de regras distintas encontradas aumente, já que as ocorrências de transações de suporte às regras aumenta com o uso de registros semelhantes agregados.

Os resultados obtidos na análise por linha de produto confirmam as análises por produto, uma vez que de cada 100 compradores de ventiladores (de qualquer marca e modelo), cerca de 26 também compraram panelas.

Os resultados também mostram relacionamentos importantes entre acessórios para videogames e videogames e entre bolas de jogar e brinquedos educativos.

Na análise por setor da loja, o número de regras encontradas foi muito baixo, forçando a diminuição dos limites mínimos de suporte e confiança.

Adotando-se suporte mínimo de 0,01% e confiança mínima de 5%, obteve-se, Resultados da análise do cesto de compras por setor, abordados por transação Os resultados indicam que a análise por setor não parece ser ideal, dado o grande número de produtos (aparentemente não relacionados) dentro de um mesmo setor, fazendo com que as regras obtidas não sejam fortes.

Possivelmente, uma reorganização dos setores, visando a colocação de produtos relacionados dentro de um mesmo setor, traria resultados consistentes com as análises por produto e por linha de produto.

Uma taxonomia de produtos representa a divisão hierárquica subentendida entre eles ou, em outras palavras, os níveis de agrupamentos existentes entre os produtos.

De acordo com a taxonomia adotada pela EMPRESA, os produtos são separados por setor, e cada setor é então dividido em várias linhas de produtos.

Cada linha de produto é constituída por produtos substitutivos, cada setor, idealmente, deve ser constituído por vários grupos de produtos substitutos, que são complementares entre si, conforme mostrado na seção "Elasticidades cruzadas".

O software de mineração de dados utilizado permite que seja informada a taxonomia existente entre os produtos, para que ela seja considerada na análise do cesto de compras.

Desta forma, as freqüências de ocorrência são pesquisadas não só num nível hierárquico específico, mas também de maneira cruzada, buscando relacionamentos entre produtos e linhas de produtos entre setores e linhas de produtos, e assim por diante.

Além das análises mostradas, outras análises foram efetuadas levando-se em consideração a taxonomia existente entre produtos, linhas de produto e setores.

Foram conduzidas análises considerando as taxonomias, Linhas de Produto Produto. Setor Linha de Produto e finalmente Setor Linha de Produto Produto.

Todos os resultados confirmaram e reforçaram as análises anteriores (por produto, linha de produto e setor, sem considerar a taxonomia entre eles), conforme pode ser observado em algumas das regras evidenciadas.

Exemplos de regras da análise do cesto de compras abordado por transação, com uso de taxonomias entre os produtos.

A partir de todas as análises efetuadas por transação, onde as regras são geradas a partir de uma compra casada de produtos, os seguintes grupos de produtos puderam ser considerados inter-relacionados.

Ventiladores X Panelas.

Aparentemente o relacionamento mais forte, já que pôde ser observado até no nível individual de produtos, e repetiu-se na análise por linha de produtos com confiança de 26%, e também na análise com uso das taxonomias, em diversas regras.

Acessórios para videogames X Videogames.

Observada inicialmente na análise por linha de produto com confiança de quase 43% e uma melhoria de mais de 47 vezes, repetiu-se na análise com uso das taxonomias.

Bolas de jogar X Brinquedos educativos.

Também com forte confiança (37,5%) e melhoria (25,4), segundo a análise por linha de produto.

A análise do cesto de compras por cliente visa estabelecer quais produtos um mesmo cliente compra ao longo do tempo.

Como muitas das transações da EMPRESA são identificadas, a análise do cesto de compras pode ser executada horizontalmente, estabelecendo freqüências de compra ao longo do tempo.

Ou seja, ao invés de registrar as regras observadas através de compras casadas (numa mesma transação), a análise por cliente busca por regras observadas ao longo do tempo quando um mesmo cliente, após adquirir o produto A, retorna à loja para adquirir o produto B.

Para executar esta análise foram levantados os identificadores de clientes, o documento apresentado (RG) ou o número do cartão do cliente na rede da EMPRESA.

Uma vez que nem todos os registros dispunham de número do cartão do cliente apenas os clientes que já realizaram alguma compra à prazo na EMPRES, os registros foram cruzados e comparados, obtendo-se um identificador único para cada cliente.

Mesmo assim, muitos registros (cerca de 40%) não puderam ser identificados, tratam-se de transações anônimas, e que, portanto, não puderam ser analisadas.

Por este motivo, e também porque o comportamento de compra dos consumidores pode ser muito diferente quando realiza compras casadas e quando as faz ao longo do tempo, já era esperado que a análise do cesto de compras por clientes não trouxesse os mesmos resultados que a análise por transações.

Com a criação dos identificadores únicos dos clientes, as compras realizadas por um mesmo cliente ao longo do tempo foram computadas pelo sistema de mineração de dados como pertencentes a uma mesma transação, e assim as análises do cesto de compras por clientes puderam ser conduzidas, levando-se em consideração as mesmas dimensões das análises anteriores.

Resultados da análise do cesto de compras por produto, abordados por cliente.

De acordo com o conjunto de regras formadas, percebe-se que de maneira geral o mesmo cliente retorna à loja para comprar o mesmo produto, ou um produto muito semelhante, ou um produto complementar.

Os relacionamentos encontrados na análise por transação a princípio não se repetiram.

Tomando por exemplo a última regra vemos que um mesmo cliente compra o produto "processador Walita", e também compra o produto "liquidificador Walita", sendo que esta regra tem suporte de 0,0564%, total de observações da regra, e confiança de 25%, de cada clientes que compraram o processador, 1 também comprou o liquidificador.

A melhoria foi de 27,7, o que significa que a regra ocorreu cerca de 27 vezes mais do que o esperado estatisticamente, de acordo com a freqüência de observações dos produtos.

Os resultados da análise horizontal por linha de produtos também diferiram bastante daqueles obtidos na análise por transação.
Resultados da análise do cesto de compras por linha de produto, abordados por cliente.
Repetindo os resultados na análise por transações, esta análise não trouxe regras válidas ou significativas, comprovando que a divisão de setores adotada pela EMPRESA parece não corresponder aos relacionamentos existentes entre os produtos, do ponto de vista do comportamento de compra dos consumidores.

Repetindo o procedimento adotado na análise por transação, foram conduzidas análises considerando as taxonomias Linhas de Produto Produto, Setor Linha de Produto e finalmente Setor Linha de Produto.

Os resultados das análises por produto e por linha de produto foram confirmados.

Durante as análises por cliente efetuadas, as regras obtidas nas análises por transação não foram confirmados, indicando que o comportamento de compra do consumidor é diferente quando ele compra ao longo do tempo e quando compra de imediato (compra casada ou compra por impulso).

Tendo em vista o objetivo final de previsão de vendas com dados atomizados de produtos relacionados, os resultados da análise de cesto de compras indicaram que as seguintes linhas de produtos merecem ser estudados em conjunto, Lista de Linhas de Produtos relacionadas resultados preliminares.

Uma nova fase de análise foi conduzida buscando confirmar e refinar os resultados.

Para esta nova fase de testes, apenas os produtos pertencentes às linhas de produtos aparentemente relacionadas foram considerados.

Todos os registros dos demais produtos foram descartados.

A idéia central por trás desta confirmação de resultados é que separando-se os produtos com alguma afinidade previamente selecionados, e conduzindo-se uma análise do cesto de compras apenas com tais produtos, a freqüência de observação das regras deve ser aumentada, já que os demais produtos foram descartados do universo analisado.

Assim, aumenta-se o suporte das regras, e é possível notar com mais atenção os relacionamentos mais recorrentes.

Todas as análises anteriores foram repetidas, análises orientadas por transação e por cliente, com base nos produtos, linhas de produtos e setores, e nas taxonomias correspondentes.

Os resultados confirmaram a maior parte das regras levantadas anteriormente, sendo que as seguintes linhas de produtos apresentam os relacionamentos mais fortes, Lista de Linhas de Produtos relacionadas.

Exemplo de gráfico esquemático dos produtos e relacionamentos encontrados durante as análises do cesto de compras, a título de ilustração dos relacionamentos obtidos.

Ela é gerada pelo software de mineração de dados, e mostra através da espessura das setas a força dos relacionamentos obtidos, e através de cores as diferentes análises processadas.

Como os resultados incluem análises processadas com uso das taxonomias de produtos, além da sinalização dos produtos, há também em alguns casos a indicação da linha de produto ou setor da loja.

Para a obtenção da figura, segue-se um processo interativo, onde os relacionamentos são plotados pelo sistema de mineração de dados, e são manipulados através de arraste na tela pelo usuário, separando e agrupando os relacionamentos mais importantes.

Relacionamentos entre produtos detectados nas análises do cesto de compras.

Tendo em vista o objetivo final de previsão de vendas com dados atomizados de produtos relacionados, e buscando reduzir o número de produtos a serem considerados nas previsões, foi realizada uma inspeção visual, buscando por produtos que, apesar de relacionados entre si, não formam um grupo coeso com a maioria dos produtos relacionados.

A inspeção visual mostrou que as seguintes linhas de produtos estão conectadas, formando um grupo coeso e conseqüentemente um centro de interesse aparente, Centro de interesse detectado.

Os demais produtos, como por exemplo videogames e acessórios para videogames, apesar de possuírem forte relação entre si, não formam um grupo coeso com os produtos acima, fazendo parte portanto de outro centro de interesse.

A fim de minimizar o número de produtos a ser modelado para previsão de vendas, e analisar apenas produtos que estejam relacionados entre si, apenas um centro de interesse foi considerado.

Todos os registros dos demais produtos foram descartados.

Dessa forma, concluiu-se a análise do cesto de compras, que teve como conseqüência direta a diminuição no volume de dados a serem previstos, apenas os produtos com grande afinidade foram preservados para as fases de tratamento e previsão seguintes.

Além da diminuição na quantidade de produtos analisados, a separação do grupo de produtos pertencentes a um centro de interesse foi utilizada posteriormente como uma informação adicional, na tentativa de melhorar a capacidade preditiva dos modelos de previsão baseados em redes neurais.

O primeiro passo na preparação de dados foi agregar as vendas atomizadas semanalmente, para dessa forma trabalhar com a variação semanal das variáveis independentes.

Algumas variáveis tiveram de ser transformadas para representar as peculiaridades do composto de marketing e demais características das vendas ao longo da semana considerada.

Por exemplo, foram tomadas as médias dos preços praticados por produto ao longo da semana, os juros médios por produto foram calculados de acordo com as prestações da venda, e assim por diante.

Um novo banco de dados foi então formado, e seu dicionário consta do ANEXO C.

Em resumo, as variáveis independentes disponíveis pela EMPRESA eram, Principais variáveis independentes usadas nos modelos de previsão semanal Com a sumarização dos produtos pertencentes ao centro de interesse evidenciado, e agregação dos dados em semanas, restaram 12560 registros representando as vendas semanais de 881 produtos distintos.

Foram realizadas investigações exploratórias nos dados, onde se detectou que as linhas de produtos estão relativamente equilibradas quanto ao número de registros, com certa predominância de transações nas linhas de brinquedos.

Foram traçados boxplots das vendas por produto, linha de produto e setor.

Todos mostraram muitos outliers, inviabilizando a retirada manual dos outliers.

Todos os desvios-padrão mostraram-se altos, o que indica potencialmente uma baixa previsibilidade nos dados.

Boxplot do total de vendas semanal por setor, indicando a presença de muitos outliers.

Total de produtos vendidos numa mesma semana, separados por setor.

No eixo horizontal, vê-se os quatro setores selecionados, com o total de registros em cada setor.

Como pode ser observado, não são evidenciados os quartis usuais do gráfico boxplot, ao invés disso, o que se vê são muitos outliers (mostrados como números correspondentes às semanas em que ocorreram), e uma média de vendas semanal por produto próxima de zero.

Também foram traçados gráficos lineares de vendas semanais e mensais por linha e setor.

Os gráficos mostraram que há forte sazonalidade, e os picos de vendas são muito expressivos.

Vendas médias semanais por setor da EMPRESA, cada setor representado por uma cor diferente.

Como pode ser visto em algumas semanas (como no fim do ano, nas semanas 49 a 52) ocorrem picos de vendas acentuados em todos os setores, porém os picos diferem de um setor para o outro, o mesmo se repetindo para as linhas de produtos analisadas.

Médias de vendas semanais por setor, indicando forte sazonalidade e picos de vendas.

Vários autores mostraram que os padrões nos dados cuja distribuição difere muito da normal são de difícil assimilação por modelos quantitativos, incluindo-se aqueles gerados com uso de técnicas de mineração de dados.

A fim de contornar esta limitação, é comum a aplicação de transformações nos dados, para que estes tenham uma distribuição mais próxima da normal, e assim possam gerar modelos mais precisos (mais aderentes à realidade).

A fim de avaliar a necessidade de transformações nos dados, todas as variáveis numéricas foram apreciadas, sendo realizadas as seguintes operações, Plotagem no tempo.

Construção de boxplot total, separado por setor e por linha.

Construção de histogramas total, separado por setor e por linha.

Todas as variáveis, com exceção de "JurosMedioSemanal", apresentaram comportamentos não-desejados, com distribuições muito distantes da normal.

Partiu-se então para uma transformação nas variáveis.

As transformações necessárias foram obtidas através do gráfico dispersão versus nível.

Transformações aplicadas aos dados.

Foram realizados testes de correlação entre todas as variáveis independentes a serem utilizadas, a fim de visualizar aquelas com maior impacto para a modelagem e estabelecer uma linha mestra para a seleção das variáveis a serem testadas nos modelos baseados em redes neurais.

A matriz de correlação entre as variáveis independentes mostrou forte relacionamento.

Correlações significativas detectadas entre as variáveis independentes.

Como as correlações Pearson são altas (maiores que 0,em módulo), nos modelos criados foi tentada a retirada de uma das variáveis relacionadas.

Isso torna a modelagem mais simples e com maior capacidade de extrapolação, e supõe-se que não ocasione perda de capacidade preditiva.

Já as correlações entre as variáveis independentes e a variável dependente total de vendas na seman resultaram em apenas duas correlações estatisticamente significativas, Correlações significativas entre as variáveis independentes e a variável dependente Visando confirmar visualmente os resultados das análises de correlação, foram desenhados gráficos do tipo scatterplot para todas as variáveis correlacionadas.

Os gráficos mostraram claramente que as correlações são verdadeiras, com exceção possivelmente da correlação, onde não é possível identificar padrões no gráfico.

Os gráficos scatterplot das correlações estão mostrados no ANEXO E.

Quanto maior o coeficiente de correlação, mais forte a associação entre duas variáveis.

Entretanto, não basta analisar apenas as correlações das variáveis num dado instante, numa determinada semana, como é o caso desta pesquisa.

Uma variável pode ter impacto em outra apenas depois de determinado período de tempo, ou seja, sua mudança ou ocorrência pode afetar as vendas após determinado período (lag) de tempo.

A fim de detectar possíveis associações com lag de tempo, faz-se necessário o estudo das autocorrelações-correlação da variável dependente com ela mesma-e das correlações com lag de tempo-correlações das variáveis independentes com as vendas, sendo as primeiras tomadas com lag de tempo.

As correlações parciais com o total de vendas na semana foram computadas para todas as variáveis, sendo obtidos os seguintes resultados estatisticamente significativos, Autocorrelações e correlações parciais verificadas.

Os resultados indicaram a necessidade de modelagem com a utilização das variáveis acima introduzidas com os lags de tempo respectivos, uma vez que elas podem ampliar a capacidade preditiva dos modelos.

Dessa forma, serão testadas, por exemplo, modelagens onde a variável "Grade" é introduzida com lags de tempo de 1 e semanas, indicando que, possivelmente, uma alteração no seu valor afeta as vendas após 1 semana, e também após semanas.

Somente através de um processo de tentativa e erro, introduzindo as variáveis acima com os lags respectivos nos modelos de previsão, é que se poderá verificar se há melhoria na capacidade preditiva.

As autocorrelações são apenas um indicativo dos lags a serem testados.

Uma vez preparados os dados e selecionadas as variáveis a serem utilizadas, deu-se início ao processo de criação dos modelos de previsão baseados em redes neurais artificiais.

A criação da rede neural propriamente dita é feita através de um processo totalmente automático nos pacotes de mineração de dados, como foi o caso do software utilizado.

Uma vez fornecidas as entradas (normalmente importadas através de Bancos de Dados ou mesmo arquivos texto), e saídas a serem previstas no caso, as vendas de cada produto, semana a seman, o software de geração de redes neurais inicia um processo de treinamento automático, onde são testadas inúmeras topologias de rede e parâmetros de treinamento.

Após o treinamento (que para o conjunto de dados utilizado levou de a 1horas, por rede neural) o sistema fornece qual a melhor rede obtida, número de camadas intermediárias, número de nós nestas camadas, quais parâmetros de treinamento foram utilizados, e quais os resultados que ela gera para o conjunto de dados de treinamento, coeficiente de ajuste, erro quadrado médio, desvio-padrão dos erros, entre outros.

Todos os modelos foram gerados com o uso do software de criação de redes neurais artificiais Megaputer PolyAnalyst 45, que gera redes Perceptron multicamadas.

Com a rede já treinada, pode-se utilizá-la para prever outros conjuntos de dados, verificando sua real performance.

A primeira modelagem realizada foi dirigida somente à avaliação das transformações utilizadas.

Duas redes foram construídas, tomando como entrada as mesmas variáveis, porém, na primeira rede as entradas foram tomadas como variáveis literais, enquanto na segunda, elas foram transformadas.

As redes foram construídas utilizando-se todos os dados disponíveis, os softwares modernos de redes neurais já separam os dados em grupos de treinamento e validação-para previsão com prazo de uma semana.

Os resultados mostraram claramente os benefícios advindos das transformações aplicadas aos dados.

A rede 1, sem transformações, obteve Erro Quadrado Médio (MSE) superior, e ajuste aos dados de treinamento sensivelmente inferior à rede 2, com todas as entradas numéricas transformadas.

Todos os modelos gerados a partir de então utilizaram como entradas as variáveis transformadas.

Resultados das redes 1 e 2, comparando os efeitos das transformações nos dados.

Para as variáveis cuja transformação não se mostrou suficiente para aproximar os dados da curva Normal-"Estoque_dias" e "Mostruario"-foram gerados modelos com todas as variáveis transformadas, com exceção delas, uma a uma e em conjunto, Rede Neural variável "Estoque_dias" sem transformações.

O erro MSE teve foi ajustado à variável original, para fins de comparação na mesma unidade de medida.

Os resultados provaram que, mesmo quando a transformação parece não ser adequada, há melhoria no desempenho dos modelos de previsão baseados em redes neurais, uma vez que todos os modelos tiveram desempenho inferior àquele obtido com a rede com todas as entradas transformadas.

Também foi testada a retirada das variáveis com forte correlação entre si-"Estoque" X "Grade" e "Mostruario" X "Preço".

Os resultados mostraram melhorias quando se retira uma das variáveis correlacionadas.

As melhorias foram mais profundas na retirada isolada da variável "Grade", o que de certa forma era esperado uma vez que a quantidade em estoque tem forte correlação com as vendas.

Os novos modelos construídos foram, Resultados da rede neural 6, sem a presença da variável "Grade" Como dito anteriormente, os softwares de geração de redes neurais modernos têm a capacidade de testar inúmeras topologias de rede e configurações de treinamento automaticamente, fornecendo aquela com melhor ajuste (R) possível.

Como se pode observar nos resultados da Rede 6, a melhor topologia obtida foi aquela com camadas-1 camada intermediária-sendo que a camada intermediária foi composta de apenas nós.

O erro nas previsões é dado pelo valor do MSE.

Nos modelos criados a partir de variáveis transformadas o erro não pode ser lido literalmente (em termos de unidades de produtos), já que a diferença entre o previsto e o real foi tomada numa variável transformada (logarítmica, no caso).

Além das transformações visando à melhoria no comportamento das variáveis de entrada, alguns autores sugerem também o procedimento de padronização das entradas, como forma de deixar todas as variáveis numa mesma ordem de grandeza, o que, dadas as características inerentes à forma de treinamento e operação das redes neurais, tende a melhorar seu desempenho.

Outros dois modelos foram então gerados, para, Entradas numéricas padronizadas entre 0 e 1 (rede 10).

Todas as entradas padronizadas entre 0 e 1, inclusive as categóricas (rede 11).

Os resultados mostraram que os procedimentos de normalização e padronização são benéficos para o desempenho das redes neurais.

Entretanto, os modelos não conseguiram assimilar as variáveis categóricas como numéricas padronizadas, piorando seu desempenho.

Isso indica um bom trabalho dos softwares em lidar com as variáveis categóricas, mesmo que estas estejam representadas por números com ordem de grandeza muito superior às demais variáveis.

A rede neural com melhor desempenho foi aquela com todas as variáveis numéricas padronizadas entre 0 e 1, Resultados da rede neural 10, com as entradas numéricas normalizadas e padronizadas. Finalmente, buscou-se o tratamento dos registros contendo valores faltantes (missing) de todas as variáveis, com base nos demais registros existentes.

Vários métodos de preenchimento de valores faltantes, como tendência linear e média foram tentados, mas em nenhum deles obteve-se modelos com capacidade preditiva superior àquela da rede 10.

Após a determinação do conjunto mínimo de variáveis independentes e da melhor forma de tratamento das entradas, deu-se início ao processo de incorporação de novas variáveis que pudessem explicar as vendas, na tentativa de melhorar a capacidade preditiva dos modelos através do fornecimento de informações adicionais, aumentando a complexidade dos modelos.

Foram geradas redes neurais incorporando-se as informações complementares a seguir.

Dados sobre a sazonalidade e passagem do tempo, "Semana_ano", representando a semana do ano, variando de 1 a 52.

"Mes_ano", representando o mês do ano, variando de 1 a 12.

entradas binárias (0 ou 1), representando os meses do ano.

entradas binárias (0 ou 1), representando as semanas do ano.

Não houve melhorias com o acréscimo destas variáveis.

A rede com melhor resultado foi aquela com a medida original e isolada da variável "Semana", mostrando que o modelo original foi capaz de assimilar a sazonalidade.

Análise de lags de tempo das vendas, Vendas da semana anterior.

Vendas de semana anterior.

Vendas de 10 semana anterior.

Vendas da 4semana anterior.

Vendas da 5semana anterior.

Vendas da 52, 42, 10, e semana imediatamente anterior.

Todos os casos anteriores conjugados.

Todos os casos anteriores, preenchendo-se valores missing.

Os resultados mostraram que houve melhoria com a adoção das variáveis de vendas com lag de tempo, desde que os missing values fossem preenchidos.

Neste caso o valor adotado nos registros faltantes foi igual ao último (próximo no tempo) valor conhecido, ou igual a 0 se não houver nenhum outro valor disponível.

Ressalta-se que a melhoria de desempenho foi visível apenas nos lags de 1 e semanas, sendo muito mais pronunciado com o uso apenas do lag de 1 semana (Rede 26), e sendo indiferente a adoção de ambos.

Análise de lags de tempo das variáveis independentes, Grade do produto quantidade especificada a ser mantida por loj da semana anterior.

Grade do produto na semana anterior.

Estoque na semana anterior.

Preço na semana anterior.

Preço na semana anterior.

Juros médio na semana anterior.

Todos os casos anteriores, preenchendo-se valores missing.

Não houve resultados positivos nos modelos gerados com o uso dos lags cruzados.

Eles foram então descartados.

Incorporação de dados de promoção, Dados de campanhas promocionais institucionais, atribuídos com valores 0 ou 1 para todos os produtos.

Dados de campanhas de marketing agressivas, direcionadas somente a alguns produtos.

Número de veiculações na mídia.

Todos os casos anteriores conjugados.

As variáveis promocionais não trouxeram melhora na capacidade preditiva das redes.

A explicação mais provável deve-se à dificuldade em atribuir a um produto a campanha realizada, já que os registros de campanhas realizadas não eram completos nem precisos.

Isso obrigou a atribuir a campanha a toda uma linha ou mesmo setor de produtos na maior parte dos casos, "confundindo" a rede.

No total, foram geradas 66 redes neurais, com a incorporação das variáveis acima citadas.

Todas utilizaram o mesmo conjunto de dados, sempre prevendo para uma semana à frente, e utilizando entradas numéricas normalizadas e padronizadas.

Em resumo, de todas as variáveis incorporadas ao modelo visando ampliar as informações disponíveis sobre o composto de marketing e sobre a sazonalidade, apenas a informação sobre o total do produto vendido na semana anterior interferiu significativamente na capacidade preditiva da rede neural.

Esta variável passou a ser incorporada em todas as análises posteriores.

Resultados da rede neural 26, com incorporação das vendas na semana anterior e missing values preenchidos.

Um dos objetivos da presente pesquisa é avaliar se um modelo de previsão de vendas baseado em redes neurais artificiais é capaz de detectar os relacionamentos presentes entre os produtos que fazem parte de um centro de interesse do consumidor-grupo de produtos inter-relacionados, como aqueles selecionados durante o processo de análise do cesto de compras (veja "Análise do Cesto de Compras", pág 87).

Apesar dos modelos anteriormente construídos levarem em consideração apenas os produtos do centro de interesse detectado, nenhuma informação foi apreciada para incorporar as informações sobre os possíveis relacionamentos existentes entre os produtos, uma vez que as redes foram treinadas com registros individuais dos produtos.

Para que o conhecimento acerca do centro de interesse seja considerado, faz-se necessária a utilização de novas entradas, que representem as informações agregadas sobre o grupo de produtos considerado.

Os seguintes dados foram aglutinados na tentativa de representar o conhecimento sobre as elasticidades cruzadas entre os produtos, Vendas totais da linha de produtos correspondente (rede 79).

Vendas totais do setor da loja correspondente (rede 80).

Vendas totais de todo o centro de interesse (rede 81).

Preço médio da linha de produtos correspondente (rede 82).

Preço médio do setor da loja correspondente rede 8.

Preço médio de todo o centro de interesse (rede 84).

Diferença de preço entre o produto e a média da linha de produto (rede 85).

Diferença de preço entre o produto e a média do setor da loja (rede 86).

Diferença de preço entre o produto e a média de todo o centro de interesse (rede 87).

Idem, para entradas padronizadas (redes 88 a 105).

Idem, com preenchimento de missing values assumindo o último valor existente (redes 106 a 141).

Os resultados mostraram uma grande melhora das redes ao se utilizar a informação de total de vendas dos produtos substitutos (pertencentes a uma mesma linha de produtos, e, portanto, produtos concorrentes diretos), ao mesmo tempo em que não houve melhoria quando da utilização dos preços médios nem das diferenças de preços.

A rede neural 108, cujos resultados são mostrados a seguir, comprova que a incorporação de informações a respeito de produtos inter-relacionados pode trazer benefícios na previsão de vendas com uso de redes neurais artificiais.

Resultados da rede neural 108, com incorporação do total de vendas de produtos substitutos (missing values preenchidos).

Após o processo de modelagem, foi observado que o melhor desempenho das redes neurais artificiais até o momento, no conjunto de dados disponível, foi aquele obtido com, Dados transformados, normalizados e padronizados entre 0 e 1.

Lag de tempo das vendas, com 1 semana de diferença.

Média de vendas da linha de produtos (produtos substitutos).

Processo de geração de redes neurais para previsão executado.

Resumo das redes neurais geradas e performance obtidas.

O Modelo 108 foi tomado então como referência, para medição de capacidade preditiva frente a dados desconhecidos e frente a outras modelagens.

A fim de se avaliar a capacidade preditiva do modelo baseado em redes neurais, o conjunto de dados foi dividido em dois grupos.
Grupo de criação do modelo, constituído internamente por registros de treinamento e validação (automaticamente separados pelo software de mineração de dados).

Para criação do modelo, foram utilizados os registros de Novembro de 1998 a Maio de 2001, num total de 135 semanas consecutivas, sempre prevendo para uma semana à frente.

Grupo de avaliação externo, simulando a utilização real do modelo de previsão em dados "desconhecidos", nunca apresentados à rede neural.

Os registros subseqüentes à semana 135, totalizando 3140 casos, foram usados para pôr à prova a capacidade preditiva do modelo gerado.

Após a criação da rede com o conjunto de dados reduzidos, a mesma foi testada frente aos registros desconhecidos, e os resultados obtidos foram transformados de volta à unidade de medida original, com a retirada da padronização, normalização e função logarítma, alcançando assim a previsão de quantos produtos serão vendidos, semana a semana.

Os resultados geraram uma nova medida de erro, uma vez que agora estão tomados por unidades de produtos vendidos.

Capacidade preditiva do Modelo Baseado em Redes Neurais.

O modelo baseado em redes neurais erra em sua previsão de vendas, em média, um total de 10,unidades de produto para previsão de uma semana à frente, o que corresponde a um erro médio percentual de 41,21%.

Se a modelagem for eficiente (capaz de "aprender" com os erros do passado), não deve haver correlação em série entre os erros de um período para outro.

Os testes de autocorrelação e autocorrelação parcial não mostraram autocorrelação nos erros, indicando que o modelo é eficiente.

Se a modelagem for não-enviesada, haverá um padrão consistente de erros para mais e para menos, e conseqüentemente a distribuição dos erros deve ter uma forma próxima da normal.

O modelo gerado parece também respeitar esta premissa.

Performance da Rede 108 ao tentar prever as vendas das semanas posteriores à 135, que não foram utilizadas para treinamento da rede.

Histograma da dispersão dos erros, na escala da variável prevista.

Gráfico de vendas previstas X vendas reais.

Com o intuito de estabelecer uma referência para a capacidade preditiva do modelo baseado em redes neurais, duas comparações foram realizadas, com a modelagem naive não ajustada e com a regressão linear usando como entradas as mesmas variáveis da rede neural 108.

Um modelo de previsão naive não ajustado é aquele onde se toma o último registro conhecido como sendo a previsão para o próximo período, sem considerar a sazonalidade da variável dependente.

A comparação com a previsão naive é uma referência comum encontrada na literatura a fim de medir o ganho de performance ao se adotar modelagens mais sofisticadas.

É também importante já que é o método de previsão atualmente utilizado na EMPRESA onde os dados foram tomados.

Observou-se que a modelagem baseada em redes neurais artificiais obteve desempenho cerca de 54% superior à previsão naive, tomando-se por base a raiz do erro quadrado médio (RMSE), e 64% superior se for considerado o erro médio percentual (MAPE), ponderando os resultados para os dados de validação.

Capacidade preditiva da previsão naive não ajustada.

A regressão linear talvez seja a técnica mais utilizada para previsão quantitativa, por isso também é uma boa referência comparativa para avaliação do desempenho das redes neurais na previsão de vendas atomizada.

Para esta regressão, foram utilizadas as mesmas variáveis do Modelo 108, que obteve a melhor capacidade preditiva dentre aqueles testados.

O procedimento de tratamento e separação também seguiu aquele do Modelo 108, com a utilização de dados normalizados e padronizados, e separação em dois grupos de dados, dados de criação até a semana 135 e avaliação do desempenho com dados da semana 136 à semana 187.

O software utilizado para a regressão gera, a partir da matriz de correlações, várias equações de regressão, utilizando como variáveis independentes apenas aquelas com maior impacto no modelo, selecionadas através do método stepwise.

Foi gerado um total de 5 equações de regressão válidas, mostradas abaixo.

Equações de regressão linear geradas.

Levando em consideração o grupo de dados de validação, todos eles desempenharam pior que o modelo baseado em redes neurais.
Capacidade preditiva das equações de regressão linear.

As análises de regressão foram realizadas utilizando-se o pacote estatístico SPSS 110.

Avaliando-se a equação de regressão com maior capacidade preditiva, observa-se que ela possui erro de ordem 32% maior que o modelo baseado em redes neurais, comparando-se o erro quadrado médio, e 19,1% maior em termos percentuais.

Desempenho da técnica de redes neurais artificiais frente às técnicas de previsão naive e regressão linear, mostrando que as redes neurais obtiveram erros menores que as demais modelagens, seja qual for a medida de erro adotada.

Performance comparativa das redes neurais.

Este trabalho investigou acerca da possibilidade de utilização de redes neurais artificiais para previsão de vendas de produtos no nível atomizado.

O objetivo geral foi o estabelecimento de uma metodologia capaz de identificar padrões de relacionamento úteis na previsão de vendas individual no varejo.

Tais relacionamentos, uma vez assimilados por um modelo de previsão causal, permitem ao gestor a identificação do composto de marketing adequado (preço, produtos e promoção), através da visualização do impacto nas vendas a partir de simulações de mudanças no composto de marketing.

Durante o trabalho de revisão bibliográfica, observou-se que muitos autores já estudaram a previsão de demanda agregada, para toda corporação, utilizando diversas técnicas de previsão e modelagem de dados, e prevendo em diferentes horizontes de tempo.

Tais estudos mostraram que a previsão quantitativa pode ser bem sucedida tanto numa abordagem de séries temporais, quando a única entrada é a própria informação da venda, em diferentes lags de tempo, quanto numa abordagem causal, quando variáveis explicativas da demanda são usadas como entrada.

As previsões são usualmente consideradas como bem sucedidas quando oferecem melhoria considerável se comparadas com a previsão naive, que considera que as vendas no próximo período serão iguais àquelas verificadas no período anterior.

Alguns autores também já investigaram a previsão de demanda no nível individual de produtos, porém tais estudos voltaram-se à modelagem das vendas de apenas um produto, seja numa abordagem de séries de tempo ou causal.

Sendo assim, tais modelos são incapazes de detectar os relacionamentos entre os produtos, os quais mostraram-se importantes nos estudos (não voltados à previsão).

Os trabalhos citados ajudaram no delineamento da presente pesquisa, mostrando que havia uma lacuna para o desenvolvimento de uma previsão de vendas no nível individual que levasse em conta o relacionamento entre diversos produtos.

Duas técnicas foram selecionadas para o desenvolvimento dos modelos, a análise de cestos de compra, capaz de identificar quais produtos estão inter-relacionados, e a geração de redes neurais artificiais para previsão da demanda, dadas suas qualidades de autoadaptação a dados não-lineares e boa capacidade de generalização.

Durante a realização das análises, obteve-se uma metodologia que pode ser replicada para outro conjunto de dados sem grandes alterações.

A metodologia consiste basicamente em, Selecionar o conjunto de dados em registros agrupados com certa freqüência, por exemplo semanal ou mensal, aglutinando o maior número de variáveis explicativas possível.

Executar a análise do cesto de compras, usando as abordagens por transação (produtos com venda casad e por cliente (análise horizontal no tempo).

A inspeção visual dos relacionamentos ajuda a separar apenas produtos que façam parte de um mesmo centro de interesse.

Tratar os dados de entrada, estabelecendo a necessidade de transformações nos dados.

As entradas numéricas, quando normalizadas e padronizadas, tendem a ocasionar uma melhor performance das redes neurais.

Gerar redes neurais testando a retirada de variáveis correlacionadas.

Testar a incorporação do maior número possível de variáveis explicativas e também de variáveis que possam conter informações sobre o grupo de produtos considerado (centro de interesse).

De acordo com os resultados alcançados nesta pesquisa, a metodologia aplicada parece ser bem sucedida na previsão de vendas de produtos no nível individual, superando as modelagens naive e de regressão linear.

A topologia de rede que obteve o melhor desempenho quando avaliada em dados externos aos de criação, na previsão com horizonte de 1 semana à frente, foi composta por 1 camada intermediária contendo 7 nós.

Sua performance resultou em boa aderência aos dados de criação e erro médio RMSE de 10,erro de cerca de 10 unidades de produto para mais ou para menos, equivalentes a 41,2% do total de vendas.

Também de acordo com os resultados obtidos, algumas informações a respeito do centro de interesse podem ser utilizadas com sucesso na previsão de vendas, uma vez que, no conjunto de dados usado, a informação sobre as vendas de todos os produtos substitutos colaborou para o desempenho da rede neural.

A modelagem através de redes neurais artificiais pode ser considerada adequada para a previsão de demanda de produtos no nível individual.

Com o uso de dados separados por produto, pode-se treinar uma rede neural para simular entradas nas variáveis explicativas, e assim planejar ações e antecipar seus resultados com alguma precisão.

Tomando como exemplo os dados disponíveis nesta pesquisa, o gestor pode simular mudanças no preço, no nível de estoque, na exposição dos produtos na loja (mostruário), na freqüência de reposição, na taxa de juros e incentivo ao pagamento parcelado, no comissionamento dos vendedores, entre outras, e prever as alterações na demanda por produto.

Os resultados sugerem que as redes neurais podem prever as vendas de produtos atomizados no curto prazo com maior precisão do que as técnicas naive não-ajustada e de regressão linear.

A equação de regressão que mais se aproximou das redes neurais obteve desvios da ordem de 19% maiores, o que pode impactar de maneira significativa o planejamento logístico, de suprimentos e finalmente o faturamento e lucro de uma corporação.

Outras conclusões importantes das análises efetuadas foram que as redes neurais podem tratar facilmente os dados de diferentes produtos, e que informações agregadas sobre um centro de interesse podem auxiliar na previsão, comprovando a existência de relacionamentos entre os produtos, e que tais relacionamentos podem ser captados pelas redes neurais.

Entretanto, os erros verificados ainda são altos, a rede neural errou, em termos percentuais, cerca de 41% nas previsões de uma semana à frente-certamente os desvios seriam ainda maiores para horizontes de tempo mais amplos.

Isso pode significar que ainda há espaço para melhorias significativas na metodologia e técnica de previsão.

Vários são os fatores que podem explicar o aind alto erro nas previsões obtidas com a metodologia adotada.

O primeiro ponto a ser levantado é a qualidade dos dados de entrada disponíveis.

O conjunto de dados utilizado dispunha de grande quantidade de registros com valores faltantes (missing)-algumas variáveis chegaram a apresentar até 81% de valores missing-e um número grande de inconsistências também foi detectado, indicando uma baixa confiabilidade nos dados.

A presença de dados confiáveis é um dos fatores de maior ganho em acurácia.

Outro ponto importante foi o (relativamente) baixo número de registros por produto.

O tempo de ciclo de vida dos produtos, normalmente menos de 1 ano, é baixo para que se tenha registros suficientes para um treinamento adequado das redes neurais.

Esta característica negativa foi ainda mais salientada na seleção dos dados, onde se optou pela utilização exclusiva dos registros de apenas uma loja, talvez a inclusão dos registros de todas as lojas aumentasse o corpo de dados e minimizasse o problema.

As informações sobre promoções realizadas também não puderam ser aproveitadas nos modelos, principalmente pelo fato de que os registros disponíveis das promoções não estavam vinculados precisamente aos produtos anunciados.

O histórico de vendas da EMPRESA mostra a presença de picos acentuados nas vendas, picos estes que segundo a própria EMPRESA devem-se reconhecidamente a promoções agressivas, direcionadas a produtos específicos.

A presença de outliers é de difícil assimilação pelos modelos de previsão, e conseqüentemente os picos de vendas devido a tais promoções geraram picos de erros nas previsões, que forçaram a um aumento no erro médio.

Outros dados que sabidamente têm papel importante no desempenho do varejo são aqueles referentes às movimentações macroeconômicas.

Sua incorporação talvez pudesse melhorar a performance das redes neurais.

Dados sobre a concorrência, como preço médio, promoções realizadas, entre outros, também poderiam melhorar a capacidade preditiva das redes neurais, uma vez que o varejo é altamente sensível às ações da concorrência.

Tais dados não foram coletados sistematicamente pela EMPRESA, o que impediu que eles fossem considerados.

Neste trabalho ficou claro que o processo de mineração de dados, através do uso de técnicas de análise de cesto de compras e previsão com uso de redes neurais artificiais, pode ser usado com sucesso com objetivo de previsão de demanda no nível individual.

Esta pesquisa, por ser de certa forma inédita, requer confirmação com outro grupo de dados.

Há indícios de que uma maior quantidade de dados (quantidade de registros de um mesmo produto), presença de informações mais ricas (maior número de variáveis explicativas, como dados sobre a concorrência, e dados mais confiáveis (com menor quantidade de dados faltantes e/ou inconsistentes) podem trazer grandes benefícios na capacidade preditiva dos modelos, tais possibilidades merecem estudo mais aprofundado.

O benefício trazido pela análise conjunta de produtos relacionados também pode ser investigado com maior profundidade.

Uma possibilidade é a inclusão de entradas individuais sobre os demais produtos de um mesmo centro de interesse, tentando captar, por exemplo, o preço, a demanda e demais características dos produtos relacionados, em dados individualizados.

A análise de lags de tempo neste caso também pode ser benéfica, uma vez que as alterações em um produto podem trazer efeitos nos produtos complementares e substitutos apenas depois de determinado período de tempo.

A análise de lags de tempo também pode ser vastamente investigada na incorporação de dados macroeconômicos, já que estes podem ter efeito retardado sobre a demanda no varejo.

Finalmente, a abordagem tradicional de modelagem individual de produtos-gerando um modelo ou rede neural para cada produto-também pode ser averiguada quanto à adição de informações sobre o centro de interesses.

Histograma da variável "Estoque" sem transformações.

Tal distribuição não é adequada para modelagem com uso de técnicas quantitativas, e em especial das técnicas de mineração de dados.

Ela provoca um comportamento excessivamente patológico da variável, como demonstrado pela seu gráfico boxplot.

Este tipo de comportamento é de difícil assimilação pelas técnicas de mineração de dados-dentre elas a técnica de redes neurais artificiais-e acarreta na geração de modelos pouco prováveis de serem aderentes aos dados, e conseqüentemente com baixa performance, seja qual for sua finalidade.

Boxplot da variável "Estoque" sem transformações.

A fim de melhorar o comportamento da variável, são aplicadas transformações nos dados.

A transformação sugerida é aquela dada pelo resultado do gráfico dispersão versus nível, Apesar do gráfico dispersão versus nível mostrar a transformação raiz como ideal, outras transformações foram tentadas e a que melhor ajustou os dados foi a transformação log x.

A transformação aproxima a distribuição da variável de uma distribuição normal, melhorando o comportamento dos dados, como pode ser visto nas figuras a seguir.

Histograma da variável "Estoque" após a transformação.

Boxplot da variável "Estoque" após a transformação.

O mesmo procedimento foi adotada para todas as variáveis numéricas, como mostrado a seguir.

Neste caso a transformação resultou em melhoria aparente dos dados, porém esta melhoria não foi profunda.

Outras transformações foram tentadas, sem sucesso.

Uma nova variável foi criada com a transformação sugerida, porém ambas foram testadas nos modelos criados.

Como o gráfico dispersão versus nível não trouxe resultados satisfatórios, várias transformações foram tentadas, sem sucesso.

A variável permaneceu sem transformações.

Neste caso a transformação resultou em melhoria aparente dos dados, porém esta melhoria não foi profunda.

Outras transformações foram tentadas, sendo a que melhor adaptou os dados a transformação logarítma.

A transformação sugerida mostrou ser a que melhor ajusta os dados.

Resultado do gráfico dispersão versus nível, A transformação sugerida mostrou ser a que melhor ajusta os dados.

A transformação sugerida mostrou ser a que melhor ajusta os dados, mesmo tentando-se outras transformações, como a logarítma.

De fato, mesmo tentando várias transformações, nenhuma mostrou ajustar os dados.

Gráfico de autocorrelação da variável "Vendas_log".

