O objetivo geral desta pesquisa é analisar técnicas para aumentar a acurácia de classificadores construídos a partir de bases de dados desbalanceadas.

Uma base de dados é desbalanceada quando possui muito mais casos de uma classe do que das outras, portanto possui classes raras.

O desbalanceamento também pode ser em uma mesma classe se a distribuição dos valores dos atributos for muito assimétrica, levando à ocorrência de casos raros.

Algoritmos classificadores são muito sensíveis a estes tipos de desbalanceamentos e tendem a valorizar as classes (ou casos) predominantes e a ignorar as classes (ou casos) de menor freqüência.

Modelos gerados para bases de dados com classes raras apresentam baixa acurácia para estas classes, o que é problemático quando elas são classes de interesse (ou quando uma delas é a classe de interesse).

Já os casos raros podem ser ignorados pelos algoritmos classificadores, o que é problemático quando tais casos pertencem à classe (ou às classes) de interesse.

Uma nova proposição de algoritmo é o Cluster-based Smote, que se baseia na combinação dos métodos de Cluster-based Oversampling (oversampling por replicação de casos guiada por clusters) e no SMOTE (oversampling por geração de casos sintéticos).

O método Cluster-based Oversampling visa melhorar a aprendizagem de pequenos disjuntos, geralmente relacionados a casos raros, mas causa overfitting do modelo ao conjunto de treinamento.

O método SMOTE gera novos casos sintéticos ao invés de replicar casos existentes, mas não enfatiza casos raros.

A combinação desses algoritmos, chamada de Cluster-based Smote, apresentou resultados melhores do que a aplicação deles em separado em oito das nove bases de dados utilizadas proposta nesta pesquisa.

A outra abordagem proposta nesta pesquisa visa a diminuir a sobreposição de classes possivelmente provocada pela aplicação do método SMOTE.

Intuitivamente, esta abordagem consiste em guiar a aplicação do SMOTE com a aprendizagem não supervisionada proporcionada pela clusterização.

O método implementado sob esta abordagem, denominado de C-clear, resultou em melhora significativa em relação ao SMOTE em três das nove bases testadas e empatou nas demais.

Foi também proposta uma nova abordagem para limpeza de dados baseada na aprendizagem não supervisionada, a qual foi incorporada ao C-clear.

Esta limpeza somente surtiu melhora em uma base de dados, sendo este baixo desempenho oriundo possivelmente da escolha não adequada de seus parâmetros de limpeza.

A aprendizagem destes parâmetros a partir dos dados ficou como trabalho futuro.

Uma base de dados é dita desbalanceada, no domínio de classificação, quando existem muito menos casos de algumas classes.

Estas classes com pouca representação são chamadas de classes raras.

Outro tipo de desbalanceamento que tem despertado o interesse da comunidade é o desbalanceamento dentro de uma classe (intraclasses), na qual a distribuição dos valores dos atributos é muito assimétrica, levando à ocorrência de casos raros.

A título de ilustração, sejam as classes fraude e não-fraude no domínio de transações de cartões de crédito.

Um exemplo de desbalanceamento de classes é o número de fraudes ser muito menor do que o de não-fraudes, e um exemplo de casos raros é a ocorrência de fraudes milionárias.

Algoritmos classificadores são muito sensíveis a estes dois tipos de desbalanceamento e tendem a valorizar as classes (casos) predominantes e a ignorar as classes (casos) de menor representação.

Os classificadores resultantes de dados com classes raras apresentam altas taxas de falsos negativos para as classes raras, o que é problemático quando a classe de interesse é justamente uma das classes raras.

Quando existem casos raros, estes casos não são aprendidos, o que é indesejável quando eles pertencem à classe de interesse.

A premissa de se estudar estes dois tipos de desbalanceamento vem do fato de que o problema de classes raras freqüentemente acompanha o problema de casos raros.

O problema de classes desbalanceadas é um problema relativamente recente que surgiu quando o aprendizado de máquina evoluiu de seu estado embrionário, puramente científico, para uma tecnologia aplicada, muito usada no mundo de negócios, indústrias e pesquisas científicas.

Apesar deste problema já ter sido identificado há mais tempo, ele somente se fez presente no círculo de pesquisas há cerca de 10 anos.

Sua importância cresceu quando foi percebido que bases de dados desbalanceadas prejudicavam a acurácia de modelos gerados por algoritmos classificadores.

Em aplicações práticas, a relação entre a classe de menor e de maior representatividade pode ser de 1,100, 1,1000, 1,10000 ou ainda maior.

Esta desproporção pode ser decorrente de um problema de amostragem, que poderia ser resolvida fazendo-se uma nova amostragem, mas em certos domínios este desbalanceamento é uma propriedade intrínseca do domínio do problema.

Nestes casos, faz-se necessário o uso de técnicas específicas para classificação em bases de dados desbalanceadas.

O objetivo geral desta dissertação é explorar técnicas para aumentar a acurácia de algoritmos classificadores aplicados a bases de dados desbalanceadas.

A literatura apresenta uma ampla gama de soluções propostas para o problema de desbalanceamento de classes e, durante o desenvolvimento desta pesquisa, foi dado foco apenas aos métodos de amostragem.

Esta escolha foi motivada pela atenção dada pela comunidade a este tipo de abordagem, o que, de forma alguma, desmerece as outras abordagens existentes.

O objetivo específico é propor novas técnicas de balanceamento e/ou limpeza de dados, baseadas na combinação de características dos algoritmos SMOTE e Cluster-based Oversampling.

A mineração de dados constitui uma área de pesquisa que utiliza técnicas de aprendizagem de máquina aplicadas a banco de dados para a construção de modelos e diversas técnicas da estatística.

Ela também é vista como um dos passos do processo de extração de conhecimento em bases de dados (KDD Knowledge Discovery in Databases).

A área da aprendizagem de máquina estuda como construir programas de computador que melhoram seu desempenho com a experiência.

Neste contexto, e para situações em que o desempenho em alguma tarefa possa ser medido, aprender pode ser definido como se segue, um programa computacional aprende a partir da experiência em relação a uma classe de tarefas, com medida de desempenho, se seu desempenho nas tarefas melhora com a experiência.

Os métodos de aprendizagem de máquina têm sido utilizados em diversas aplicações como veículos autônomos que aprendem a dirigir em vias expressas, reconhecimento da fala, detecção de fraudes em cartões de crédito, estratégias para a construção de jogos, programas de mineração de dados que descobrem regras gerais em grandes bases de dados, etc.


Alguns temas relevantes a esta dissertação, mas que não constitui o foco de pesquisa, são sucintamente abordados nos apêndices.

Fundamentação teórica para os classificadores e os métodos de avaliação utilizados nesta pesquisa.

Métodos de clusterização k-means, Squeezer e CEBMDC.

Métricas de distância euclidiana, VDM e HVDM.

Neste capítulo, é apresentada uma revisão da literatura em técnicas de pré-processamento de dados para a mineração de dados em bases desbalanceadas.

Este capítulo está organizado da seguinte maneira, taxonomia para métodos de balanceamento de classes, métodos de amostragem e os métodos de limpeza estudados nesta pesquisa.

Se dividem as soluções propostas para o caso de desbalanceamento de classes em três subáreas, métodos de amostragem (englobam os métodos de classificadores híbridos, também chamados de múltiplos), métodos de aprendizagem de apenas uma classe, por exemplo, a classe de interesse, e métodos de seleção de atributos (do inglês, feature selection).

Esta separação é apenas uma forma didática de se apresentar os diversos métodos existentes, visto que uma solução específica pode vir a se beneficiar da utilização conjunta de dois ou mais tipos de abordagens.

Nesta pesquisa as abordagens a este problema são expostas de forma diferente, de forma a refletir o foco que foi dado à mesma, isto é, Métodos de amostragem de dados, que visam diminuir o desbalanceamento das bases de dados.

Métodos de limpeza de dados, que atuam de forma a melhor definir o espaço de decisão aprendido pelos classificadores.

Outros métodos que são importantes para a solução do problema de classes desbalanceadas, mas que somente são apresentados a título de informação e não serão detalhados por fugirem do foco desta pesquisa.

Os métodos de amostragem visam mudar a distribuição dos dados de treinamento, de modo a aumentar a acurácia de seus modelos.

Isto é alcançado com a eliminação de casos da classe majoritária (denominado na literatura como undersampling) ou replicação de casos da classe minoritária (denominado oversampling).

Não há garantia de que a distribuição original dos dados de treinamento seja a mais adequada para a construção de classificadores.

Os métodos de amostragem servem para alterar a distribuição dos dados de tal forma que seja possível gerar classificadores melhores para eles.

Subdivisão em duas vertentes, métodos básicos e métodos avançados de amostragem.

Os métodos básicos de amostragem são métodos que não utilizam heurística na eliminação e na replicação de casos, ou seja, são métodos que visam balancear a distribuição de classes de forma aleatória.

São eles o undersampling aleatório e o oversampling aleatório.

Diversos autores concordam que os métodos de amostragem que não utilizam heurísticas podem provocar distúrbios indesejados nos modelos gerados.

A simples replicação de casos da classe minoritária pode causar overfit do classificador aos dados de treinamento, ao passo que a eliminação aleatória de casos da classe majoritária pode remover casos importantes para o processo de aprendizagem.

Já os métodos de amostragem avançados utilizam heurística na eliminação de casos da classe majoritária e na replicação de casos da classe minoritária.

Os seguintes métodos de amostragem avançados são discutidos a seguir, segmentação dos dados de uma variante do Condensed Nearest Neighbor Rule CNN, One-sided Selection OSS, SMOTE, Cluster-based Oversampling.

O método de segmentação dos dados consiste em reduzir o número de casos da base de dados com a utilização de informação a priori.

Suponha que se saiba que a classe de interesse está fortemente ligada a um determinado estado de alguma variável, por exemplo, a uma determinada hora do dia.

Pode-se, então, filtrar a base de dados de forma a somente manter os casos que são pertinentes ao problema e, dessa forma, reduzir o desbalanceamento de classes.

O método CNN (do inglês, Condensed Nearest Neighbor Rule), consiste em criar um subconjunto consistente a partir de um conjunto de dados.

O subconjunto é designado consistente quando um classificador consegue classificar de forma correta todo o conjunto, utilizando o subconjunto como conjunto de treinamento.

Utilizou-se uma variante desse método para fazerem undersampling apenas dos casos da classe majoritária.

A idéia é retirar os exemplos redundantes da classe majoritária.

Para isso, o subconjunto é criado com todos os casos da classe interesse e apenas os casos mais relevantes da classe majoritária do conjunto.

Entende-se por casos relevantes da classe majoritária aqueles que, com o classificador, conseguem classificar corretamente todos os casos desta classe.

A seguir, apresenta-se o algoritmo desta variação do CNN.

Um classificador é dito overfitted quando ele é muito específico para o conjunto de treinamento.

Modelos muito específicos para o conjunto de treinamento possuem acurácia elevada para os casos conhecidos, mas não é geral o suficiente para a predição de novos casos.

Observa-se que o subconjunto criado com a variante do CNN não é necessariamente o menor subconjunto consistente do conjunto.

De acordo com os autores, "é satisfatório se o conjunto de casos negativos (da classe majoritári for reduzido de forma razoável").

Basicamente, o método OSS (do inglês, One-sided Selection), separa o conjunto original de dados em dois subconjuntos, o subconjunto, que agrupa os casos da classe de interesse, e o subconjunto que agrupa o restante dos casos.

O subconjunto é reduzido (undersample enquanto o conjunto é mantido intacto).

Este método utiliza uma variante do CNN para eliminar casos do subconjunto que estão muito distantes do limiar de decisão e, logo após, aplica o método Tomek links para retirar os casos que são ruídos ou limítrofes.

Exemplos limítrofes são considerados "instáveis", pois uma pequena quantidade de ruído pode fazê-los cair no lado errado do limiar de decisão.

As simples técnicas de oversampling são amplamente atacadas pela comunidade científica, pois muitas delas apenas replicam casos positivos existentes.

Meramente replicar casos já existentes da classe minoritária, realmente aumenta o víeis do classificador para esta classe.

Entretanto, ocorre o efeito indesejado de modelos overfitted, ou seja, modelos muito específicos para estes casos replicados, prejudicando, dessa forma, seu poder de generalização para a classe de interesse.

Interpreta-se este problema em termos de regiões de decisão no espaço de atributos.

Geralmente, classes raras são acompanhadas de casos raros e estes casos raros são circundados de casos negativos.

O simples ato de replicar um caso positivo propicia que classificadores reconheçam esta região, mas esta região será tão pequena que não conseguirá classificar corretamente novos casos da classe de interesse que venham a cair nas vizinhanças desta região.

Diante desta problemática do oversampling, desenvolveu-se um método diferente de se fazer oversampling da classe minoritária, que consiste na geração de casos sintéticos (casos artificiais) para a classe de interesse a partir dos casos já existentes.

Estes novos casos serão gerados na vizinhança de cada caso da classe minoritária, de forma a fazer crescer a região de decisão e, assim, aumentar o poder de generalização dos classificadores gerados para estes dados.

Chama-se este novo método de SMOTE (do inglês, Synthetic Minority Oversampling Technique).

Visualmente, no espaço amostral do conjunto de dados, estes novos casos sintéticos serão interpolados aleatoriamente ao longo do segmento de reta que liga cada caso da classe minoritária a um de seus k vizinhos mais próximos, escolhidos de forma aleatória.

A seguir, o Algoritmo apresenta o pseudocódigo do método SMOTE.

Nenhum dos métodos apresentados até então enfocam o problema de desbalanceamento interno, que freqüentemente acompanha o problema de classes desbalanceadas.

Um conjunto de dados se apresenta internamente desbalanceado quando ocorrem casos raros em comparação com o restante dos dados.

Casos raros não são facilmente identificáveis, mas métodos de aprendizagem não supervisionada, como a clusterização, ajudam na identificação desses casos, sendo que eles podem se apresentar como pequenos clusters dentro de uma classe.

Em classificadores, casos raros potencialmente se apresentam como pequenos disjuntos, que são conceitos aprendidos que cobrem poucos casos do conjunto de treinamento.

Em outras palavras, casos raros ocorrem quando alguns subconjuntos de uma classe são muito menores do que os outros.

Ocorre que certos algoritmos classificadores, como o C45 Backpropagation, tendem a ignorar tais casos por eles não terem representatividade no conjunto de treinamento, tornando-se problemático quando casos raros ocorrem na classe minoritária, e esta é a classe interesse.

Abordagem para minimizar este problema de casos raros, juntamente com a minimização do problema de classes raras.

Eles partem do pressuposto de que um conjunto de dados retém algumas características originais do problema em questão, e usam estas características para fazer oversampling não somente da classe minoritária, mas também dos casos raros.

A idéia deles é agrupar os dados de treinamento em clusters e, então, balancear a distribuição de seus casos.

Este método é chamado Cluster-based Oversampling.

Primeiramente, o método utiliza a técnica PDDP (do inglês, Principal Direction Divisive Partitioning), que é uma técnica não-supervisionada de clusterização para formar os clusters de cada classe, e depois faz oversampling de cada um destes clusters.

Este método é aplicável a bases de dados multiclasses.

O pseudocódigo de Cluster-based Oversampling é apresentado no Algoritmo 3, a seguir.

Algoritmo O algoritmo Cluster-based Oversampling faz oversampling de todos os clusters de todas as classes do conjunto de entrada.

Primeiramente, os clusters da classe majoritária são balanceados, isto é, cada cluster tem seus casos replicados até que se atinja o tamanho do maior cluster.

Em seguida, as outras classes são sobreamostradas (oversample até atingir o tamanho da classe majoritária, seguindo o mesmo princípio utilizado para a classe majoritária).

Notar que o algoritmo proposto pelos autores finaliza com o conjunto desbalanceado de entrada totalmente equilibrado, ou seja, com todas as classes e seus clusters possuindo a mesma quantidade de casos.

Se propõe que os clusters utilizados no método Cluster-based Oversampling sejam computados com o método de clusterização k-means.

Casos reais podem apresentar ruídos nas suas variáveis (atributos e classe).

Estes ruídos são, geralmente, provocados por erros aleatórios (incerteza na medição, por exemplo) ou por problemas na coleta dos dados.

Os casos de uma base de dados podem ser divididos em quatro categorias, casos ruidosos quanto à classe (class-label noise), limítrofes (borderlines), redundantes e seguros.

Essas quatro categorias são relativas ao espaço de decisão, no qual cada caso é visto como um ponto no espaço.

Quantidade de atributos da base de dados.

Visto desta forma, o objetivo do classificador é aprender a delimitar as regiões de casos semelhantes, separando-as com superfícies de decisão.

Um caso é considerado ruidoso quanto à classe quando ele se situa no lado errado do limiar de decisão, ou seja, ele diverge da classe de seus vizinhos mais próximos.

Casos limítrofes são aqueles que se situam perto da borda de decisão.

Os casos limítrofes não são confiáveis, pois mesmo uma quantidade pequena de ruído em seus atributos pode jogá-los para o lado errado de uma superfície de decisão.

Casos redundantes são aqueles que podem ser representados por seus vizinhos sem perda da expressividade do classificador resultante.

Casos seguros são aqueles que devem ser mantidos para a geração do classificador.

Os dados originais são rotulados de acordo com as categorias.

Os rótulos são, casos ruidosos quanto à classe casos limítrofes casos redundantes.

Os casos não rotulados são os casos considerados seguros para a classificação.

São apresentados os dados limpos com a retirada dos casos rotulados de 1 a 3.

A presença de dados ruidosos quanto à classe, limítrofes e redundantes pode vir a degradar o desempenho dos classificadores gerados para as bases que os contém.

As técnicas de limpeza de dados existentes na literatura visam à eliminação destes tipos de dados.

As seções seguintes apresentam algumas destas técnicas de limpeza de dados.

Para melhor clareza, foi adotado o termo ruído, ou caso ruidoso, para casos que apresentam ruído quanto à classe.

Intuitivamente, em um Tomek link, ou um dos casos é ruído, ou os dois são casos limítrofes.

Os pares Tomek links podem ser utilizados tanto para se fazer undersampling da classe majoritária (remover ruído) quanto para se fazer limpeza em torno do limiar de decisão.

No caso de undersampling, apenas o caso da classe majoritária de um Tomek link é eliminado.

No caso de limpeza de dados, ambos são excluídos.

O Algoritmo 4, a seguir, apresenta o pseudocódigo de Tomek links.

O método ENN (do inglês, Edited Nearest Neighbor Rule) faz undersampling dos dados de forma contrária à maneira em que é feito o undersampling no método CNN.

O método ENN faz undersampling do conjunto de entrada, retirando todo caso cuja classe divergir da classe predita pelos seus vizinhos mais próximos.

Se nem todos os vizinhos são da mesma classe, a determinação da classe predita pode ser feita pela classe mais freqüente ou por uma ponderação das classes em função da importância relativa de cada uma delas.

Esse processo remove tanto os casos ruidosos quanto os casos limítrofes, de forma a propiciar um limiar de decisão mais suave.

Assim, os dados são reduzidos com a eliminação de casos ruidosos ao mesmo tempo em que a vizinhança da classe de interesse é limpa.

A seguir, o Algoritmo 5 apresenta o método ENN.

Esta sensibilidade se dá porque casos ruidosos geralmente são classificados de forma errônea e, assim, são adicionados ao conjunto consistente retornado pelo algoritmo.

Mesmo removendo os casos ruidosos com Tomek links, os resultados não são satisfatórios possivelmente por causa do CNN.

Método multiclasse que enfatiza mais a limpeza de dados do que a sua redução.

O autor justifica esta posição pelo fato de que a qualidade de classificadores não reside necessariamente só na quantidade de casos que cada classe do conjunto de treinamento contém, pois a ocorrência de dados de má qualidade afeta de forma negativa o desempenho de classificadores.

O método NCL, em seu primeiro passo, utiliza o método ENN para fazer undersampling dos casos que não fazem parte da classe de interesse.

A seguir, será sumariamente apresentado o método ENN e, logo após, o método NCL.

O método NCL utiliza a idéia do método ENN (Edited Nearest Neighbor Rule) para encontrar os dados ruidosos que não pertencem à classe de interesse e, logo em seguida, faz limpeza do limiar da vizinhança da classe de interesse.

Primeiramente, o conjunto de entrada é organizado em dois subconjuntos, o subconjunto, contendo os casos da classe de interesse, e o subconjunto, contendo o restante dos dados.

Para identificar os casos ruidosos utilizamos o método ENN para formar o subconjunto.

Então, é feita a limpeza da vizinhança (casos da classe de interesse) da seguinte forma, a partir de cada caso, encontrar seus três vizinhos mais próximos que pertencem (casos que não pertencem à classe de interesse) e movê-los.

A fim de se evitar uma excessiva redução de casos de classes com baixa freqüência, somente serão considerados os casos que pertencerem às classes com freqüências maiores ou iguais à metade da freqüência da classe de interesse na construção do subconjunto L.

A métrica HVDM (do inglês, Heterogeneous Value Difference Metri é utilizada para o cálculo da distância entre casos).

O método NCL de Laurikkala faz undersampling com foco na limpeza de dados.

Primeiramente, são selecionados os casos ruidosos e, logo depois, são selecionados os casos limítrofes que não forem da classe de interesse.

Do total dos casos do conjunto de dados de entrada, são eliminados todos os casos ruidosos e limítrofes que forem de classes cujo tamanho seja maior ou igual à metade do tamanho da classe de interesse.

Esta é uma idéia que tem por objetivo fazer primeiramente oversampling com SMOTE dos casos positivos (casos da classe de interesse) e, logo após, fazer limpeza de dados.

A motivação para limpeza de dados advém do fato de que, geralmente, os casos da classe positiva não estão bem agrupados e pode haver sobreposição entre as classes, isto é, podem existir casos negativos (positivos) que invadem a região de decisão de clusters de casos positivos (negativos).

A aplicação do SMOTE, como método de oversampling da classe minoritária, fará crescer a região de decisão dos casos positivos, que é desejado, mas poderá gerar, ou mesmo aumentar, a ocorrência de casos ruidosos.

A aplicação de Tomek links logo após a aplicação do SMOTE visa remover esses casos ruidosos e, portanto, melhorar os agrupamentos de cada classe.

Para este método de limpeza, o método Tomek links é aplicado tanto para os casos positivos quanto para os negativos.

Os casos rotulados são exemplos de casos removidos pela aplicação de Tomek links.

Combinação de SMOTE e ENN.

O funcionamento deste método é análogo ao SMOTE mais Tomek links, sendo que a única diferença é que ENN promove uma limpeza mais profunda.

Assim como na abordagem SMOTE mais Tomek links, a limpeza é aplicada aos casos de ambas as classes.

O método ENN é utilizado.

Durante o início desta pesquisa, foram estudados vários métodos da literatura antes de se focar nos métodos de amostragem.

Esta seção apresenta alguns destes métodos estudados.

Cabe lembrar que estes métodos são aqui apresentados apenas a título de informação.

Propuseram um esquema de composição de classificadores, denominado ExperText, e aplicam-no no problema de classificação de textos, ambiente em que os dados são altamente desbalanceados.

A idéia básica é utilizar amostragens distintas do conjunto de treinamento e classificá-las com diferentes técnicas de classificação de forma distribuída e incremental.

Os modelos produzidos por cada um destes conjuntos são, então, combinados em um único modelo de classificação.

O classificador combinado produz resultados superiores quando comparados a classificadores individuais.

Esquema de classificador híbrido que mistura as qualidades do Stacking e Bagging.

Os autores utilizam os algoritmos Backpropagation, Naive Bayes e C45 em partições de dados derivados de oversampling com reposição.

Eles utilizam um meta-classificador para escolher os melhores modelos base e, então, combinam estes modelos base com votação (bagging).

Uma outra abordagem para minimizar os efeitos do desbalanceamento de classes é aprender apenas a classe de interesse, sendo o classificador construído somente com os exemplos da classe minoritária.

A idéia é utilizar a aprendizagem baseada em reconhecimento.

Assim, para classificar um novo caso, basta calcular o nível de similaridade entre ele e a classe alvo, e depois compará-lo com o limiar de similaridade adotado.

De acordo com os autores, dentre os métodos que aprendem apenas a classe de interesse, um que tem apresentado bom desempenho é o SVM (do inglês, Support Vector Machine) apud.

Outra abordagem é a de seleção de atributos.

A seleção de atributos é uma etapa que pode ter grande impacto na acurácia do classificador apud.

O processo de aprendizagem em base de dados com muitos atributos necessita de um alto poder computacional e, geralmente, resulta em baixa acurácia.

A existência de muitos atributos favorece a ocorrência de desbalanceamento de classes.

Este capítulo apresenta as duas abordagens propostas para o problema de base de dados desbalanceada.

Na Seção 31 é apresentado o Cluster-based Smote e, na Seção 32, é apresentado o C-clear.

Após a revisão do estado da arte do problema de bases de dados desbalanceadas, observou-se uma maior atividade de pesquisa vinculada ao desbalanceamento de classes e a não existência de métodos específicos que tratam de casos raros ao mesmo tempo em que evitam a simples replicação de casos existentes.

Geralmente, os pequenos disjuntos acabam por serem negligenciadas por algoritmos classificadores e a mera replicação de casos existentes pode causar overfitting.

Embora o método SMOTE seja utilizado para criar novos casos sintéticos, o seu foco não é o tratamento do problema de casos raros.

Uma alteração potencialmente promissora desse método seria povoar as regiões menos densas com maior intensidade do que as regiões mais densas.

Desta forma, seria criado um maior número de casos sintéticos para os casos de pequenos disjuntos, reduzindo a possibilidade de ocorrência de overfitting.

Já o método Cluster-based Oversampling permite descobrir e povoar pequenos disjuntos, mas se limita a apenas replicar casos já existentes que, como anteriormente citado, pode causar overfitting.

Nesta seção, é proposta uma nova abordagem de oversampling que combina os métodos SMOTE e Cluster-based Oversampling.

A idéia consiste em alterar o método Cluster-based Oversampling de forma que ele gere casos sintéticos com o SMOTE, ao invés de simplesmente replicá-los.

Com esta nova abordagem, potencialmente, pode-se obter melhores resultados no balanceamento de base de dados do que apenas utilizar o SMOTE ou o Cluster-based Oversampling.

Este novo método foi nomeado Cluster-based Smote.

Diferentemente do Cluster-based Oversampling, o Cluster-based Smote foi implementado com a opção de se escolher a distribuição de classe resultante.

Esta opção de projeto foi motivada pelo fato de que a distribuição de classe ótima dos dados de treinamento deve ser determinada de forma empírica.

Para melhor compreensão do funcionamento do Cluster-based Smote, esta opção não aparece no Algoritmo 7.

Por opção de projeto, o Cluster-based Smote funciona com qualquer método de clusterização desejado, pois ele utiliza um vetor de inteiros como parâmetro de entrada que indica a qual cluster cada caso pertence, ao invés de chamar um método de clusterização, como indicado no Algoritmo 7, a seguir.

O método SMOTE somente é válido para atributos contínuos.

Para o Cluster-based Smote funcionar com bases que possuem atributos mistos, o SMOTE foi alterado para utilizar a métrica HVDM no cálculo das distâncias entre casos.

Na geração de um caso sintético, os valores contínuos são interpolados e os valores categóricos são replicados.

Originalmente, o método SMOTE é aplicável a todos os casos da classe positiva.

Para ele se integrar no Cluster-based Smote, o SMOTE foi implementado para gerar casos sintéticos a partir dos casos indicados em um vetor de inteiros.

Algoritmo 7 O algoritmo Cluster-based Smote faz oversampling de todos os clusters das classes do conjunto de entrada.

Primeiramente, os clusters da classe majoritária são balanceados, isto é, são criados casos sintéticos para seus clusters até que todos tenham o mesmo tamanho.

Em seguida, cada classe restante é igualmente balanceada até que se atinja o tamanho da classe majoritária.

Apresentou-se um studo sobre o quanto que a sobreposição de classes pode afetar o desempenho de classificadores se propôs a aplicação de métodos de limpeza logo após a aplicação do método SMOTE.

A motivação dessas abordagens consiste em remover casos gerados pelo SMOTE que possivelmente sejam ruídos, os quais degradam o desempenho de classificadores.

A nova abordagem proposta nesta seção visa guiar a aplicação do método SMOTE, de modo a diminuir a quantidade de ruído por ele gerado e, logo após, fazer limpeza de dados.

Intuitivamente, esta abordagem consiste em aplicar o método SMOTE aos casos positivos que residem em regiões onde a freqüência da classe positiva seja maior do que certo limiar e, logo após, remover os casos negativos (positivos) das regiões onde a freqüência da classe positiva (negativ seja maior do que certo limiar).

Tais regiões são obtidas com aprendizagem não supervisionada.

O método desenvolvido sob esta abordagem foi nomeado C-clear.

O funcionamento dessa abordagem se baseia em duas premissas, primeiro, os dados tenderem a se agrupar em clusters, ao invés de se distribuirem uniformemente no espaço amostral e, segundo, a variável classe ser um atributo relevante a este agrupamento.

Sumariamente, o C-clear opera em três passos.

No primeiro, os dados de entrada são agrupados.

No segundo, os clusters são rotulados como positivo e negativo, de acordo com o limiar adotado.

No terceiro, é aplicado SMOTE aos clusters positivos e, opcionalmente, pode ser feita uma limpeza de dados com a remoção dos casos ruidosos casos com classes dissonantes do rótulo do cluster ao qual pertencem.

O parâmetro limiarSmote indica qual a freqüência mínima de casos positivos para que um cluster seja considerado positivo.

O limiarSmote varia no intervalo, sendo 0 para considerar todos os clusters como positivo e, portanto, aplicar SMOTE em todos os casos positivos, e 1 para não aplicar SMOTE.

A intensidade da limpeza depende do parâmetro, o qual indica qual a freqüência positiva (negativ mínima admitida para que um cluster seja considerado positivo (negativo).

Os parâmetros utilizados no C-clear variam no intervalo real, embora na prática poucos valores sejam de interesse.

De fato, o número máximo de valores de interesse é igual à quantidade de clusters que possuem freqüências de classes positivas distintas.

O menor valor de interesse para os parâmetros, depois do 0, é a menor freqüência relativa de casos positivos (negativos) dos clusters.

Por analogia, o maior valor de interesse, antes do 1, para os parâmetros é a maior freqüência relativa de casos positivos (negativos) dos clusters.

O pseudocódigo do C-clear apresentado no Algoritmo 8, a seguir.

Funcionamento do método C-clear.

Algoritmo 8 O algoritmo C-clear aplica SMOTE somente aos clusters que possuem freqüência positiva maior que certo limiar e, opcionalmente, faz limpeza de dados.

Da mesma forma que o método Cluster-based Smote, o C-clear foi implementado de forma a funcionar com qualquer método de clusterização desejado.

Maiores detalhes de implementação, a seguir.

Os métodos propostos Cluster-based Smote e C-clear e os métodos Oversampling aleatório, Cluster-based Oversampling e SMOTE foram implementados em Java 5 no módulo de pré-processamento do UnBMiner.

O UnBMiner é composto de uma plataforma e API abertas e foi desenvolvido para facilitar a implementação, avaliação de técnicas de mineração de dados e construção de modelos.

Esse software suporta parcialmente o modelo de referência CRISP-DM, abrangendo parte da fase de preparação dos dados (módulo pré-processamento do UnBMiner), fase de modelagem (criação de modelos baseados em Naive Bayes, CNM, árvore de decisão algoritmos IDe C45 e rede MLP backpropagation) e fase de avaliação (módulo de avaliação do UnBMiner).


Este capítulo apresenta os resultados obtidos com a aplicação dos métodos propostos Cluster-based Smote e C-clear a bases de dados disponíveis na literatura e geralmente utilizadas para avaliação de desempenho de algoritmos de classificação em mineração de dados.

Foram utilizadas bases de dados disponíveis no repositório da Universidade da Califórnia em Irvine (UCI).

A Seção 41 apresenta a metodologia utilizada para a avaliação dos algoritmos propostos, a Seção 4apresenta a análise dos resultados obtidos e a Seção 4apresenta a conclusão das avaliações realizadas neste capítulo.

Os métodos propostos Cluster-based Smote e C-clear foram avaliados e comparados com o oversampling aleatório, Cluster-based Oversampling e SMOTE.

Sumariamente, o processo de avaliação consiste em três passos, A base de dados de entrada é divida em dois conjuntos, um de treinamento e outro de avaliação

O conjunto de treinamento é pré-processado com os métodos propostos e é gerado um modelo para cada um dos conjuntos de treinamento pré-processados

Os modelos são avaliados com a AUC e análise ROC (Apêndice B2).

Esses mesmos passos são feitos com os métodos oversampling aleatório, Cluster-based Oversampling e SMOTE.

O melhor modelo e, portanto, o melhor método de pré-processamento é aquele que obtiver maior medida AUC e sua curva ROC dominar as outras em todo espectro no gráfico ROC.

Esta metodologia de avaliação está detalhada nas subseções seguintes.

As bases de dados utilizadas para avaliação foram obtidas no repositório da UCI.

Principais características dessas bases.

A segunda coluna apresenta a freqüência de casos positivos, a terceira apresenta a quantidade de atributos contínuos e discretos, a quarta contém as classes utilizadas e, finalmente, a última mostra o número de rodadas necessárias para a estabilidade numérica na validação cruzada.

Bases de Dados Utilizadas.

A escolha destas bases de dados foi feita de forma a garantir que os métodos propostos pudessem ser avaliados em problemas reais e com diferentes tamanhos e distribuições de classe.

De forma análoga a e, as bases de dados com mais de duas classes tiveram as classes não minoritárias colapsadas em uma classe, representadas na coluna Classes, pelo vocábulo outras.

Da base Yeast foram utilizadas apenas as classes POX e CYT, respectivamente tomadas como classe positiva e classe negativa.

A base Letter-vogal foi construída a partir da base original Letter, utilizando as vogais como classe positiva e as restantes como classe negativa.

Essa mesma estrutura foi aplicada à base Letter.

Da base Forest foram utilizadas apenas as classes e 4.

Originalmente, a base Forest possui 7 classes e 5atributos, sendo 4atributos binários.

Com a remoção dos casos das outras classes, 27 atributos binários ficaram com apenas um estado possível e, portanto, foram removidos.

Foram utilizados os métodos Oversampling aleatório, Cluster-based Oversampling e SMOTE e os métodos propostos Cluster-based Smote e C-clear.

De forma a simplificar a comparação dos métodos de oversampling, a distribuição de classe deles resultante foi fixada em 1,1, isto é, totalmente balanceada.

Uma base de dados com distribuição de classe balanceada tende a apresentar bom desempenho quando a medida de qualidade AUC é utilizada.

A clusterização dos dados utilizada nesta pesquisa foi feita da seguinte maneira, para a obtenção dos clusters do conjunto de treinamento com atributos contínuos, foi utilizado o algoritmo k-means.

Optou-se pelo k-means pela simplicidade de seu funcionamento e implementação, embora esse método requeira o informe do número de clusters.

Nesse caso, foi feita uma análise de sensibilidade para a aplicação do método C-clear, resultando no uso para o número de clusters.

A métrica de distância utilizada para o k-means foi a distância euclidiana normalizada pela variância.

Para clusterização de atributos categóricos, optou-se pelo método Squeezer por ele sugerir uma heurística para cálculo do número de clusters, seu único parâmetro de entrada.

A métrica de distância utilizada no Squeezer foi a VDM.

Como freqüentemente bases de dados reais possuem tanto atributos contínuos quanto categóricos, foi implementado o framework de clusterização de atributos mistos CEBMDC.

O k-means foi utilizado para os atributos contínuos e o Squeezer foi utilizado para os atributos categóricos e, também, como metaclusterizador.

Esses métodos de clusterização foram implementados em Java 5 no módulo de aprendizagem não supervisionada do UnBMiner.

Para o Cluster-based Oversampling, os clusters das bases de dados com atributos contínuos foram construídos com o k-means.

Já para as bases de dados com atributos categóricos foi utilizado o Squeezer, sendo que a quantidade de clusters foi determinada pela heurística.

As bases com atributos mistos foram clusterizadas com o CEBMDC, para os atributos contínuos.

Esse mesmo procedimento foi também aplicado para a obtenção dos clusters para o método proposto Cluster-based Smote, de forma a propiciar uma comparação justa entre os dois métodos.

O C-clear foi utilizado com limiarSmote fixado na freqüência de casos positivos, limiar Limpeza Positivo e limiar Limpeza Negativo fixados em 0,7 (quantidade de clusters resultantes do método k-means).

A quantidade de clusters obtidos com o método k-means foi fixada.

Um estudo sobre o impacto dos valores desses parâmetros e uma forma de aprendê-los a partir dos dados foram deixados como trabalho futuro.

A intuição quanto ao elevado número de clusters utilizados foi amenizar a chance de se excluir clusters com alta representatividade e, desta forma, prevenir uma possível degradação do classificador resultante.

A mesma intuição foi utilizada para os parâmetros limiar Limpeza Positivo e limiar Limpeza Negativo.

Um valor demasiadamente baixo para limiar Limpeza Positivo removeria muitos casos da classe negativa, enquanto que um valor muito alto para o parâmetro limiar Limpeza Negativo removeria muitos casos positivos considerados como ruído.

O C-clear foi avaliado de duas formas, com e sem limpeza.

Desta seção em diante, o método C-clear grafado com o símbolo (como em C-clear) refere-se à limpeza de dados habilitada e, quando grafado com o símbolo, refere-se à limpeza de dados desabilitada.

Os classificadores foram induzidos com o C45, utilizando a correção de Laplace.

Esta correção objetiva melhorar a precisão na estimação de probabilidades com base na freqüência observada dos casos positivos sobre o número total de casos.

No estudo em questão, as probabilidades a serem utilizadas para a construção da curva ROC foram inferidas a partir do número de casos reportados pelo C45 em cada classe representada nas folhas da árvore.

Se uma folha apresenta apenas 1 caso positivo e 0 caso negativo, e outra folha apresenta 50 casos positivos e 0 caso negativo, a probabilidade estimada por ambas as folhas é 1.

No entanto, a probabilidade estimada a partir da folha que apresenta maior freqüência é mais precisa.

O C45 foi utilizado sem poda, pois a aplicação de poda raramente resulta em melhora na medida AUC.

Os métodos propostos Cluster-Based Smote e C-clear foram aplicados, durante a fase de pré-processamento, às bases de dados da UCI com o objetivo de reduzir o desbalanceamento de classes e de melhorar a aprendizagem de casos raros.

As bases de dados resultantes foram utilizadas para a aprendizagem de classificadores com a aplicação dos formalismos de árvore de decisão, C45.

Para efeito de comparação de desempenho dos classificadores obtidos, as mesmas bases de dados desbalanceadas iniciais foram pré-processadas com a aplicação dos métodos Oversampling aleatório, Cluster-Based Oversampling e SMOTE, considerados para efeito de controle.

Similarmente, as bases de dados resultantes foram utilizadas para a aprendizagem de classificadores com base nos algoritmos C45.

O processo de avaliação da performance dos classificadores obtidos utilizou a validação cruzada com dez dobras (do inglês, fold cross validation), indicada como a forma de avaliação mais eficiente para a seleção de modelos.

Resumidamente, a metodologia empregada consistiu nos seguintes passos.
Cada base de dados desbalanceada inicial é divida em dez subconjuntos com a utilização de amostragem estratificada, visando conservar a distribuição de classes para cada um dos dez subconjuntos

O método fold cross validation é aplicado, consistindo nos seguintes passos, A cada uma das dez dobras da validação cruzada, a base de dados em questão é dividida em um subconjunto de treinamento, composto de nove dos dez subconjuntos descritos no passo 1, e um subconjunto de avaliação, composto do subconjunto restante

O conjunto de treinamento é balanceado com a aplicação dos métodos de pré-processamento propostos (Cluster-based Smote e C-clear) e dos métodos de pré-processamentos de controle (oversampling aleatório, Cluster-based Oversampling e SMOTE) sendo gerados dois modelos para cada um dos conjuntos de treinamento balanceados, com a aplicação do C45

Cada modelo gerado (correspondendo a um classificador) é avaliado quanto ao sucesso na classificação, sendo para isso calculada a matriz de confusão e, a partir dela, a curva ROC e a AUC, área sob essa curva, conforme descrito

Ao final das dez dobras, são calculadas a média e o desvio padrão das curvas ROC e das AUC obtidas em cada uma das dobras

Os passos 1 a 2podem ser repetidos até que haja uma estabilidade numérica da média AUC computada no passo 24.

Cada ciclo composto dos passos 1 a 2é denominado uma rodada.

Se houver mais de uma rodada, a média e o desvio padrão da AUC e dos pontos que constituem a curva ROC, feitos no passo 24, são calculados ao final de todas as rodadas.

O melhor modelo e, conseqüentemente, o melhor método de pré-processamento, é aquele que obtiver maior AUC e a sua curva ROC dominar as outras em todo espectro no gráfico ROC.

Esta forma de avaliação feita em lote permite que cada método de pré-processamento receba os mesmos dados de entrada e, portanto, garante uma comparação mais justa entre eles.

A estabilidade numérica adotada no passo foi a precisão na unidade da métrica AUC.

Os valores AUC obtidos na avaliação dos classificadores, e seus respectivos desvios padrão, expressos entre parênteses, são apresentados a seguir.

Nessa tabela e nas demais a seguir, os métodos são representados pelas seguintes siglas, Oversampling Over

Cluster-based Oversampling CBO

Cluster-based Smote CBS.

AUC das Bases Sonar, Pima, Vehicle, Letter-vogal e Vowel.

Plotam as AUC obtidas a partir das bases sem pré-processamento versus a proporção entre casos positivos e negativos de cada uma, concluindo que o desbalanceamento de classes não é necessariamente a causa da degradação do desempenho de classificadores.

Esse mesmo procedimento foi realizado nesta pesquisa e os resultados são apresentados a seguir.

Também é observada a mesma conclusão desses autores e exemplos como as bases Forest e Vowel corroboram esta conclusão, pois resultam em altos valores AUC mesmo contendo apenas 7% e 9% de exemplos da classe positiva, respectivamente.

Distribuição de Classe Positiva/Negativa versus AUC.

Ranking dos métodos de pré-processamento para cada base de dados utilizada nesta pesquisa.

Os métodos propostos nesta pesquisa estão em negrito.

Assim os melhores métodos foram comparados com o restante dos métodos com o intuito de se verificar se eles são significativamente melhores.

Para tanto, foi utilizada a análise de variância (ANOV com nível de significância).

Os campos em cor cinza da epresentam os métodos que são significativamente piores que os melhores métodos, estes apresentados na coluna.

Ranking dos Métodos de Pré-processamento.

O C-clear (com limpez apresentou significativamente o melhor resultado para a base Pima e significativamente os piores resultados para três das demais bases).

É provável que esses últimos resultados sejam efeitos da escolha não adequada para os parâmetros de limpeza limiar Limpeza Positivo e limiar Limpeza Negativo, produzindo uma remoção excessiva de casos de ambas as classes.

O maior impacto produzido por essa limpeza foi observado na aplicação do C-clear à base de dados Yeast, na qual a limpeza de dados resultou na diminuição de 1pontos percentuais em relação à não aplicação da limpeza.

Por outro lado, o método apresentou bons resultados, sendo ele o melhor em duas bases e o segundo melhor em outras cinco.

O método Cluster-based Smote também se mostrou competitivo, sendo o melhor em quatro bases, o terceiro melhor em três, e melhor que o Cluster-based Oversampling em oito das nove bases de dados utilizadas nesta pesquisa.

O desempenho foi significativamente superior em três bases em relação ao desempenho do SMOTE e o desempenho do Cluster-based Smote foi significativamente superior ao desempenho do Cluster-based Oversampling em cinco bases.

Bases em que o C+clear é melhor que o SMOTE.

Para cada curva nesses gráficos, são apresentados o nome do método utilizado e a respectiva AUC obtida com eles.

Para maior clareza, os métodos serão referenciados pelo nome da curva do modelo gerado com ele.



No ráfico ROC da base Sonar, observa-se que a curva do método Cluster-based Smote domina as demais curva até próximo ao ponto e, cruza com as outras curvas a partir deste ponto até o ponto onde passa novamente a dominar as outras curvas.

A curva do método Cluster-based Oversampling foi dominada por todas as demais curvas do ponto em diante.

Gráfico ROC para a base de dados Pima.

Diferentemente do gráfico da base Sonar, as curvas se apresentam um pouco mais afastadas umas das outras.

Observa-se que a curva C-clear domina as demais curvas a partir do ponto.

O gráfico ROC da base Vehicle é apresentada.

Nele, as curvas Original e Cluster-based Smote compartilham vários pontos da convex hull.

Observa-se a curva C-clear sendo dominada desde.

Gráfico ROC para a base letter-vogal.

Nele, observa-se a dominância dos métodos baseados no SMOTE em relação aos métodos Oversampling aleatório e Cluster-based Oversampling.

Observa-se também o efeito negativo proporcionado pela limpeza feita com o C-clear e a dominância do método Cluster-based Smote em boa parte do espectro.

No gráfico ROC da base Vowel, a curva C-clear é dominada por todas as outras até pouco antes do ponto e, deste ponto em diante, passa a compartilhar pontos com a curva do método, que foi o melhor método para essa base.

Gráfico ROC para a base Glass.

O método Cluster-based Smote domina as demais curvas até o ponto e passa a cruzar com as demais curvas no restante do espectro.

O gráfico ROC da base Forest é apresentada.

Nele, os métodos Cluster-based Smote e C+clear dominam os demais métodos praticamente em todo espectro do gráfico.

Observa-se com maior detalhe o efeito negativo da limpeza feita pelo C-clear, cuja curva é visivelmente dominada pelas demais curvas.

Gráfico ROC para a base Yeast.

A dominância do método C+clear é mais perceptível.

Esta curva domina as demais até próximo do ponto e, a partir desse ponto, compartilha a dominância com o método SMOTE.

Observa-se o baixo desempenho do método Oversampling aleatório, sendo a curva deste dominada por todas as outras.

O gráfico ROC da base é apresentado.

Assim como no gráfico ROC da base Letter-vogal, os métodos baseados no SMOTE dominam os métodos baseados no Oversampling aleatório.

Novamente, a limpeza feita pelo método C-clear degrada o desempenho do classificador resultante, como se pode observar pela dominância dos outros métodos baseados no SMOTE em relação a ele.

A aplicação do método SMOTE guiada pela aprendizagem não supervisionada feita pelo C+clear mostrou-se competitiva em relação à aplicação do SMOTE de per si.

Em três bases o C+clear foi significativamente superior que o SMOTE.

Nas duas bases em que o SMOTE foi superior ao C+clear, Letter-a e Letter-vogal, o ganho não foi significativo.

A limpeza feita com o método C-clear somente obteve bom desempenho na base Pima.

É provável que este comportamento do C-clear seja resultado da escolha inadequada dos parâmetros de limpeza limiar Limpeza Positivo e limiar Limpeza Negativo, e do número de clusters para as demais bases.

Possivelmente, aprender esses parâmetros a partir dos próprios dados se mostre eficiente.

Esse estudo fica como trabalho futuro.

A proposta de se substituir a simples replicação do método Cluster-based Oversampling pela geração de casos sintéticos com o SMOTE resultou em melhora significativa na métrica AUC.

O método Cluster-based Smote, que une essas duas abordagens, superou significativamente o método Cluster-based Oversampling em oito das nove bases utilizadas nesta pesquisa, e foi significativamente o melhor método em quatro das nove bases.

A análise de curvas ROC permitiu a verificação do comportamento dos métodos de pré-processamento em todo espectro do limiar de decisão.

Porém, os melhores métodos dominaram os demais somente em determinadas faixas do espectro do gráfico ROC.

Entretanto, mesmo que esta dominância do melhor método não seja estatisticamente significativa, a utilização de uma distribuição de classe ou custo de classificação baseada nesta faixa do espectro, onde o melhor método domina os demais, pode resultar na escolha de um modelo significativamente melhor do que os proporcionados pelos demais métodos.


Para melhor contextualização, é apresentada uma breve recapitulação do objetivo geral, da motivação, dos resultados esperados e das linhas de ação adotadas nesta pesquisa.

Logo após, são apresentadas as condições de término desta pesquisa, e explicitadas as restrições admissíveis para a solução proposta.

O objetivo geral é explorar técnicas para aumentar a acurácia de algoritmos classificadores aplicados a bases de dados desbalanceadas com foco nos métodos de amostragem.

O objetivo específico é propor novas técnicas de balanceamento e/ou limpeza de dados, baseadas na combinação de características dos algoritmos SMOTE e Cluster-based Oversampling.

Foram propostas duas abordagens baseadas no funcionamento dos métodos de pré-processamento SMOTE e Cluster-based Oversampling.

A primeira abordagem, Cluster-based Smote, utiliza a geração de casos sintéticos feita pelo método SMOTE para povoar os pequenos disjuntos encontrados com o método Cluster-based Oversampling, ao invés de simplesmente replicá-los.

A motivação foi melhorar a aprendizagem de pequenos disjuntos e, ao mesmo tempo, evitar o overfitting do modelo a eles.

Os resultados apresentados indicam que, realmente, o método Cluster-based Oversampling gera modelos muito ajustados (overfitte para os dados de treinamento e, por conseguinte, para os pequenos disjuntos).

A aplicação do SMOTE realizada pelo Cluster-based Smote, proporcionou resultados superiores ao do método Cluster-based Oversampling em oito das nove bases utilizadas nesta pesquisa, e foi o método que apresentou o melhor desempenho em quatro bases.

A segunda abordagem foi proposta para amenizar a sobreposição de classes que possivelmente venha a ser causada pela aplicação do SMOTE.

Essa abordagem consiste em guiar o método SMOTE pela aprendizagem não supervisionada, aplicando-o somente aos clusters onde a freqüência de casos positivos seja maior que certo limiar e, logo após, aplicar ou não a limpeza de dados.

Essa limpeza consiste na eliminação dos casos negativos (positivos) de todo cluster cuja freqüência positiva (negativ seja maior que certo limiar).

O C-clear, método desenvolvido sob essa abordagem, foi testado com e sem a aplicação de limpeza.

Os resultados obtidos com o C+clear indicam que a aplicação do SMOTE orientada pela aprendizagem não supervisionada atenua o problema de sobreposição de classes gerada, ou mesmo, intensificada pelo SMOTE.

O C+clear apresentou bons resultados, sendo ele o melhor em duas bases e o segundo melhor em outras cinco.

Além disso, o desempenho do C+clear foi significativamente superior ao desempenho do SMOTE em três bases, como apresentado na o SMOTE não apresentou nenhum resultado significativamente melhor que o C+clear.

Já a limpeza efetuada pelo C-clear somente surtiu melhora na base Pima e rendeu os piores resultados para três das demais bases.

Possivelmente, a causa desta degradação esteja vinculada a uma escolha não adequada dos parâmetros de limpeza limiar Limpeza Positivo e limiar Limpeza Negativo, que produziu uma remoção excessiva de casos de ambas as classes.

Para esta pesquisa, foram utilizadas algumas heurísticas, tais como o número de clusters do algoritmo k-means fixo para o C-clear, o parâmetro limiarSmote do método fixo no valor da freqüência da classe positiva, e os limiares de limpeza fixados nos valores 0,7.

Entretanto, os valores estipulados para esses parâmetros possivelmente não são ótimos para todas as bases.

Os resultados obtidos com a limpeza promovida pelo C-clear, por exemplo, corroboram essa hipótese, pois somente a base Pima apresentou ganho de desempenho com este método, e as três demais resultaram em degradação significativa no desempenho.

Portanto, um trabalho futuro seria estender o C-clear para aprender seus parâmetros a partir dos próprios dados de treinamento.

Para tanto, a estrutura de validação implementada no UnBMiner foi projetada de forma que essa extensão possa ser facilmente implementada.

Para efeito de construção de classificadores, um conjunto de variáveis pode ser dividido em variáveis atributos e uma variável classe.

Por existir apenas uma variável classe, é usual se referir aos estados desta variável como as classes possíveis.

Um caso é uma instância deste conjunto de variáveis atributos, isto é, um caso e sua classe formam um registro numa base de dados.

Um classificador é um modelo construído a partir de um conjunto de instâncias de atributos-classe (casos e suas respectivas classes), ou seja, é uma função que mapeia os valores das variáveis atributos nos possíveis estados da variável classe.

A aprendizagem por árvores de decisão é um dos métodos mais usados e práticos para inferência indutiva.

É um método para obter funções que retornam valores discretos (nominais) a partir dos dados de entrada, na qual a função aprendida é representada por uma árvore de decisão.

Cada folha representa uma classe (isto é, um valor possível para os estados da variável de classe), cada nó de decisão especifica um teste a ser feito sobre um atributo que pode levar a uma folha ou alguma outra sub-árvore.

O nó raiz contém a variável de atributo mais informativo para a definição do estado da classe da variável de classe.

Árvores de decisão podem ser representadas como conjuntos de regras de classificação para melhorar o entendimento de uma pessoa que esteja avaliando os dados.

Cada caminho iniciando no nó raiz e finalizando em um nó folha, representa uma regra de classificação.

Muitos algoritmos foram desenvolvidos para a construção de árvores de decisão a partir de um algoritmo base que emprega uma construção top-down, com procura gulosa pelo espaço das possíveis árvores de decisão.

Os algoritmos implementados no UnBMiner foram o ID e o seu sucessor C45.

Notar que esses algoritmos não são os originais de Quinlan, e sim uma reimplementação em Java dos originais.

O usuário do sistema pode realizar inferências com a árvore de decisão, entrando com um valor para cada um dos nós não folha e percorrendo um caminho do atributo raiz até uma folha.

Cada valor induz caminhos a percorrer até se atingir uma folha na qual está representada a classe correspondente.

Basicamente algoritmo IDconstrói uma árvore recursivamente, como se segue, Um atributo é selecionado como nó raiz e arcos são criados para cada valor de atributo possível.

As instâncias de dados são divididas em subconjuntos (um para cada arco que se estende do nó).

Pare se todas as instâncias associadas a um arco possuem a mesma classe Repita os passos 1 a para cada arco que possua mais de uma classe, usando apenas as instâncias a ele associadas.

A questão a ser analisada é qual atributo deve ser selecionado no passo 1.

Deve ser selecionado aquele capaz de produzir a menor árvore de decisão.

Uma boa heurística é escolher o atributo que produz os nós mais puros.

Um nó é puro se as instâncias associadas a ele possuem apenas uma classe.

O grau de impureza de um nó é máximo quando todas as classes são igualmente distribuídas.

Frequentemente, entropia é utilizada como medida de pureza e o maior ganho de informação como critério para selecionar o atributo a ser utilizado.

A entropia, medida da Teoria da Informação, mede o grau de homogeneidade de um conjunto de instâncias de dados do atributo de interesse (no domínio da classificação, o atributo de classe), variando de 0 (todas as instâncias possuem o mesmo valor) a 1 (todos os valores são igualmente distribuídos).

O ganho de informação mede o quanto um atributo divide um conjunto de instâncias S para produzir subconjuntos mais puros em relação a uma variável de interesse, sendo sua fórmula dada pela equação.

Assim, o ganho de informação é a diferença entre a entropia do conjunto e o valor esperado da entropia depois que é particionado o conjunto segundo o atributo.

O algoritmo C45 estende o algoritmo IDcom a consideração de taxa de ganho de informação, dados faltantes, atributos numéricos e poda da árvore.

Suas vantagens são, representação compacta e de fácil visualização, que permite a decomposição de decisões complexas em decisões elementares, e a fácil detecção de atributos redundantes ou irrelevantes.

A árvore de decisão pode vir a ser complexa e a classificação correta de novas instâncias depende da representatividade dos dados utilizados no treinamento.

Usualmente, um algoritmo classificador tem como entrada um conjunto de treinamento e, a partir deste, gera um modelo que é utilizado para classificar casos novos.

O classificador k-NN (do inglês, Nearest Neighbor vizinho mais próximo) é um tipo diferente de classificador que não necessita gerar modelo, pois a classificação de novos casos é feita de forma direta.

Os casos de um conjunto de dados são interpretados como pontos distribuídos no espaço euclidiano.

Quando um novo caso é apresentado ao classificador k-NN, ele o classifica (rotula a sua variável classe) com a classe de maior freqüência entre seus k vizinhos mais próximos.

O algoritmo a seguir mostra os passos deste classificador, Algoritmo 9 O algoritmo classificador k-NN rotula novos casos com a classe de maior freqüência entre seus k vizinhos mais próximos.

Este algoritmo não cria um classificador para os dados de entrada, ele classifica um novo caso diretamente com os dados já conhecidos.

Existem outras variantes mais interessantes do passo 3, como, por exemplo, utilizar pesos distintos para o voto de cada vizinho, conforme a distância entre ele e o caso sendo classificado.

Utilizou-se esta idéia como forma de penalizar a contribuição de vizinhos mais longínquos, multiplicando seus votos pelo inverso do quadrado de suas respectivas distâncias em relação ao caso que se quer classificar.

A equação, a seguir, expressa esta variante do k-NN.

Avaliação de classificadores e o problema de desbalanceamento de classes

Análise ROC

Convex hull

Métrica escalar AUC.

Comparar o desempenho de diferentes algoritmos de mineração de dados para um problema requer a realização de testes e validação (para inferir parâmetros de validade e confiabilidade nos modelos gerados), e o cálculo de estatísticas (para auxiliar a análise dos resultados).

Considere o problema de se obter uma amostra representativa a partir de um conjunto de dados.

Um método de divisão de dados reservaria certa quantidade para avaliação e usaria o restante para treinamento.

Em termos práticos, se o conjunto de dados é representativo do domínio, é comum reservar um dos dados para avaliação e os restantes para treinamento.

Deve-se garantir que a amostragem aleatória para obtenção do conjunto de treinamento e de avaliação seja estratificada.

Uma maneira mais geral de avaliação consiste em repetir o processo completo de divisão várias vezes, nos conjuntos de treinamento e de avaliação, cada vez com diferentes amostras aleatórias estratificadas.

Em cada iteração, uma proporção dos dados, é selecionada para treinamento e o restante, é usado para avaliação.

Os índices de desempenho do modelo são obtidos como a média dos índices de desempenho nas diferentes iterações.

Este processo é conhecido como validação cruzada de dez dobras.

No caso de classificadores, é natural medir o desempenho em termos de erro e de sucesso na classificação.

Em algumas situações práticas, o custo de uma classificação errada pode ser muito elevado e deve ser levado em consideração.

Para classificação multiclasses, as classificações no conjunto de avaliação são apresentadas na forma de uma matriz bidimensional com uma linha e uma coluna para cada classe.

Cada elemento desta matriz, chamada matriz de confusão, apresenta o número de casos avaliados, na qual a classe real é a linha e a classe predita é a coluna.

Bons resultados são caracterizados por valores altos na diagonal principal e valores nulos para elementos fora da diagonal principal.

Suponha que um classificador está sendo utilizado para classificar uma instância que possui apenas duas classes, positivo e negativo.

Nomenclatura utilizada para as quatro situações possíveis.

Matriz de Confusão com duas Classes.

A partir da matriz de confusão, podem ser calculados índices de desempenho.

Todos eles assumem valores no intervalo.

A sensibilidade é a taxa de verdadeiro positivo (tpr), a especificidade é a taxa de verdadeiro negativo (tnr) e a acurácia é a probabilidade de acerto do classificador.

Índices para Discriminação entre Classificadores Dicotômicos.

As métricas apresentadas somente são interessantes quando o custo de erro de classificação das duas classes é igual.

Considere, por exemplo, o erro de diagnóstico de se classificar um paciente doente como sadio.

Esse erro pode custar a vida do paciente.

Nesse domínio, o custo de erro de classificação da classe doente é muito superior à da classe sadio.

A idéia é similar no domínio de desbalanceamento de classes, quando a classe de interesse é a classe minoritária.

Um classificador enviesado para a classe majoritária poderá errar para todos os casos da classe minoritária e, mesmo assim, obter baixo índice de erro global.

As métricas F-measure e G-mean, por exemplo, tentam capturar esses problemas, penalizando os erros relativos a cada classe de forma separada.

Entretanto, elas não são aplicáveis a problemas onde não se conhece os custos de erros para cada classe, ou quando não se conhece a melhor distribuição de classes a priori para a aprendizagem.

A análise ROC (do inglês, Receiver Operating Characteristics), também conhecida como análise de curvas ROC, é uma técnica para a visualização, organização e seleção de classificadores baseadas em seus desempenhos.

Esta análise vem sendo utilizada por longo tempo na teoria de detecção de sinais e pela comunidade médica para representar ganhos e perdas na escolha entre a taxa de detecção de acertos e a taxa de alarmes falsos providas por classificadores.

Esta análise é também utilizada nas áreas de psicologia, economia e previsão do tempo e, recentemente (final dos anos 90), tem recebido maior atenção da comunidade de aprendizagem de máquina.

Um gráfico ROC é um gráfico bidimensional, no qual a taxa de detecção é plotada no eixo das ordenadas e a taxa de falso alarme é plotada no eixo das abscissas.

Nesse gráfico, cada classificador avaliado é representado por um ponto e o ponto representa o classificador perfeito e o ponto representa o pior classificador possível.

Classificadores próximos à diagonal principal são considerados estocásticos por possuírem taxas de acertos e de erros semelhantes.

Gráfico ROC para cinco classificadores.

Gráfico ROC com cinco classificadores binários.

Os classificadores apresentados são classificadores discretos, pois predizem apenas um resultado binário para a classe.

Outros classificadores, como o Naive Bayes e Redes Neurais, produzem, para cada classe, uma probabilidade ou escore, que indica o grau de pertinência de um caso à classe positiva e negativa.

Se o valor predito para a classe positiva for maior que um determinado limiar (geralmente 05), a classe predita pelo classificador é positiva (negativa, caso contrário).

A avaliação destes classificadores com o limiar em 05 equivale a considerar um custo de erro de classificação uniforme, ou mesmo, considerar que a distribuição real de classes é balanceada.

Uma forma de se obter diferentes pontos no gráfico ROC, e com eles construir uma curva, é variar o limiar entre (-,+) e, a cada passo, avaliar o modelo e plotar os pontos resultantes.

A curva resultante da união desses pontos pode ser vista como várias avaliações do mesmo classificador utilizando-se diferentes custos ou diferentes distribuições de classe a priori.

Essa é uma característica importante, quando se necessita comparar dois ou mais classificadores em todo o espectro de custos de erro de classificação ou de distribuição de classe.

Caso onde um classificador é melhor que outro independente do limiar utilizado.

Gráfico ROC com Dominância de Curva.

Convex hull (casca convex é a envoltória dos pontos das curvas dos classificadores em um gráfico ROC).

Exemplo de uma convex hull.

Um classificador é potencialmente ótimo se, e somente se, sua curva ROC se sobrepõe à convex hull apud.

O gráfico ROC apresentado não possui classificadores potencialmente ótimos.

Já no gráfico ROC, a curva do classificador A se sobrepõe à convex hull e, portanto, ele é um classificador potencialmente ótimo.

A existência de um classificador potencialmente ótimo garante que os demais classificadores presentes no mesmo gráfico ROC possam ser seguramente descartados.

A área sob a curva ROC (do inglês, Area Under the ROC Curve) poder ser utilizada como um estimador adimensional de desempenho de classificadores.

Na prática apenas se considera o intervalo.

AUC facilita a comparação de classificadores, mas não substitui a análise de curvas ROC.

Entretanto, analisando a curva do classificador, percebe-se que ele tem desempenho melhor somente enquanto a taxa de falso positivo é menor do que 04.

Para valores fpr maiores do que 04, o classificador tem desempenho melhor.

Gráfico ROC com Intersecção de duas Curvas.

Neste método, cada caso é visto como um ponto no plano cartesiano.

Visto desta forma, o método k-means consiste em dividir a base de dados em k subconjuntos de casos mais próximos.

Inicialmente, k casos da base de dados são eleitos, de forma aleatória, para serem os centróides de cada cluster respectivamente.

Um centróide é o ponto central de um cluster.

Depois de agrupado todos os casos em seus respectivos clusters, o centróide de cada cluster é recalculado.

Para cada cluster, a média aritmética de seus casos é atribuída ao seu centróide.

Desta forma, cada cluster tem seu centróide ci deslocado para o centro geométrico de seus casos.

Novamente, todos os casos da base de dados são reagrupados ao cluster com centróide mais perto.

Este processo é repetido até que não haja mais mudanças em nenhum valor dos centróides.

O método Squeezer é utilizado para a clusterização de dados categóricos.

Intuitivamente, Squeezer lê um caso da base de dados por vez e, ao ler o primeiro caso, este é utilizado para formar o primeiro cluster.

Os demais casos lidos, ou são colocados nos clusters existentes, ou são utilizados para formar um novo cluster.

Essa decisão é tomada com base na similaridade entre o caso lido e os clusters existentes.

Se a maior similaridade entre o caso e os clusters existentes for menor do que certo limiar, o caso lido será colocado no cluster de maior similaridade a ele.

Se, por outro lado, essa maior similaridade for menor do que o limiar adotado, um novo cluster é formado com esse caso.

O processo se repete para todos os casos da base de dados.

O framework CEBMDC (Cluster Ensemble Based Mixed Data Clustering) pode ser caracterizado como uma metaclusterização dos clusters para os atributos categóricos e contínuos feitos de forma separada.

Primeiramente, a base de dados de entrada é separada em dois subconjuntos, um contendo os atributos categóricos e o outro os atributos contínuos.

Em seguida, esses dois subconjuntos são clusterizados com qualquer método de clusterização desejado que retorne um vetor de valores indicando a qual cluster cada caso pertence.

Finalmente, esses dois vetores são utilizados para construir uma nova base de dados, cada vetor representando um atributo, sendo esta base clusterizada com algum método de clusterização para atributos categóricos.

Essa última clusterização é chamada de metaclusterização e o vetor de valores resultante indica a qual cluster cada caso pertence.

Funcionamento do CEBMDC.

Funcionamento do framework de clusterização CEBMDC.

Como a distância euclidiana é aplicável apenas a variáveis numéricas e/ou ordinais, foram propostas outras métricas derivadas para o caso de atributos nominais.

Uma primeira tentativa para aplicação da distância euclidiana a atributos nominais consiste na aplicação da métrica de sobreposição, onde se atribui o valor 0 quando os valores comparados são iguais, e 1 em caso contrário.

Para atributos nominais, utilizou-se uma modificação da distância euclidiana, denominada VDM (do inglês, Value Difference Metri, na qual se leva em consideração a similaridade entre os casos).

Para bases de dados com atributos nominais e ordinais, utilizou-se uma métrica mais elaborada, chamada de HVDM (do inglês, Heterogeneous Value Difference Metri).

A distância entre dois casos é calculada da seguinte forma, para atributos ordinais, é função da soma das distâncias euclidiana normalizadas por um fator da variância entre os atributos (os autores utilizam o valor para esse fator), e, para atributos nominais, é função da distribuição de freqüência relativa por classe dos atributos.



