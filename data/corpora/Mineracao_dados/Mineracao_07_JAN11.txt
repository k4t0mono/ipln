Análise de cluster é uma técnica aplicada a diversas áreas como mineração de dados, reconhecimento de padrões, processamento de imagens.

Algoritmos de clusterização têm por objetivo particionar um conjunto de dados em clusters de tal forma que indivíduos dentro de um mesmo cluster tenham um alto grau de similaridade, enquanto indivíduos pertencentes a diferentes clusters tenham alto grau de dissimilaridade.

Uma importante divisão dos algoritmos de clusterização é entre algoritmos hard e fuzzy.

Algoritmos hard associam um indivíduo a somente um cluster.

Ao contrário, algoritmos fuzzy associam um indivíduo a todos os clusters através da variação do grau de pertinência do indivíduo em cada cluster.

A vantagem de um algoritmo clusterização fuzzy é que este pode representar melhor incerteza e este fato é importante, por exemplo, para mostrar que um indivíduo não é um típico indivíduo de nenhuma das classes, mas tem similaridade em maior ou menor grau com mais de uma classe.

Uma forma intuitiva de medir similaridade entre indivíduos é usar medidas de distância tais como a distância euclidiana.

Existem muitas medidas de distância disponíveis na literatura.

Muitos dos algoritmos de clusterização populares geralmente buscam minimizar um critério baseados numa medida de distância.

Através de um processo iterativo estes algoritmos calculam parâmetros de modo a diminuir o valor do critério iteração a iteração até um estado de convergência ser atingido.

O problema com muitas das distâncias encontradas na literatura é que elas são estáticas.

Para o caso de algoritmos de clusterização iterativos, parece razoável ter distâncias que mudem ou atualizem seus valores de acordo com o que for ocorrendo com os dados e as estruturas de dado do algoritmo.

Esta dissertação apresenta duas distâncias adaptativas aplicadas ao algoritmo fuzzy c-means pelo Prof Francisco de Carvalho.

Este algoritmo foi escolhido pelo fato de ser amplamente utilizado.

Para avaliar as proposições de distância, experimentos foram feitos utilizando-se conjunto de dados de referência e conjuntos de dados artificiais (para ter resultados mais precisos experimentos do tipo Monte Carlo foram realizados neste caso).

Até o momento, comparações das versões do fuzzy c-means, obtidas através da utilização de distâncias adaptativas, com algoritmos similares da literatura permitem concluir que em geral as novas versões têm melhor performance que outros disponíveis na literatura.

A informatização possibilitou que grandes quantidades de dados fossem armazenadas.

Esses dados estão relacionados a diversas atividades e áreas, como vendas de produtos, operações financeiras, consultas médicas, catálogos de bibliotecas e mais recentemente a própria WorldWide Web, We, que pode, sob certos aspectos, ser vista como um banco de dados distribuído.

Nesse sentido, ao realizar as mais simples atividades cotidianas tem-se algum dado sendo armazenado em algum banco de dados B.

Empresas públicas e privadas de médio a grande porte mantêm seus dados mais importantes armazenadas em sistemas de gerenciamento de banco de dados (SGBDs).

Some-se a isso tudo o fato de que com a popularização de dispositivos móveis e portáteis, como celulares e sensores, a capacidade de gerar dados tem aumentado consideravelmente.

Todas essas enormes quantidades de dados armazenados podem revelar várias informações a respeito do domínio a que se relacionam.

No entanto, essas valorosas informações estão na maior parte das vezes camufladas ou escondidas.

Recuperar essa informação camuflada é um objetivo da técnica de mineração de dados ou data mining.

O processo de Data mining como um todo e também etapas específicas desse processo têm recebido grande interesse como área de pesquisa, devido aos benefícios sociais e econômicos que avanços nessa área podem representar.

Para ilustrar alguns avanços potenciais, pode-se citar, prevenir catástrofes naturais como terremotos, secas e inundações através da análise de dados históricos relacionados ao meio ambiente de um certo local, detectar potenciais fraudadores em concessão de financiamentos públicos, prevenir falências detectando uma potencial situação de insolvência de uma empresa com antecedência, entre outros.

O processo de obter informação a partir de bancos de dados não é uma tarefa trivial, pois envolve alguns problemas como, processar grandes massas de dados acumuladas ao longo de um extenso período, entender o significado que os dados a serem utilizados trazem, selecionar os dados relevantes ao que se deseja analisar.

Detectar possíveis anomalias nesses dados, criar modelos que representem suficientemente bem os dados, ser capaz de tratar diversos tipos de dados como imagens, sons, vídeos, textos, arquivos XML e os SGBDs onde estes estão armazenados quais sejam bancos de dados relacionais, data warehouses, bancos de dados de transações, banco de dados orientados a objetos, objeto-relacionais, sistemas baseados em arquivos, Web.

Em a situação descrita no início deste capítulo é bem sumarizada como, "Situação de riqueza de dados e pobreza de informação".

Chegar à informação através da enorme quantidade de dados que é possível ter armazenada atualmente é uma tarefa de muita relevância.

Tem-se observado, por exemplo, que devido à falta de ferramentas adequadas, dados em grandes bancos de dados transformam-se em "túmulos de informação", isto é, uma vez coletados jamais são revistos ou analisados.

Este fato faz com que tomadores de decisão tenham de confiar apenas em suas intuições ao longo do processo decisório simplesmente porque estes não têm as ferramentas adequadas para extrair o valoroso conhecimento incrustado em meio à grande quantidade de dados disponível.

Pode-se generalizar as técnicas relacionadas a analisar banco de dados e trazer informações novas a partir dos dados como Knowledge Discovery in Databases, KD ou Data mining.

O termo Data mining tem obtido preferência para fazer referência ao processo descrito, brevemente, a seguir.

Data mining visa desenvolver meios automáticos de prospecção de conhecimento em grandes bases de dados.

O processo de Data mining inicia com uma etapa de pré-processamento objetivando assegurar a qualidade dos dados a analisar.

A próxima etapa é a descoberta de conhecimento propriamente dita.

Esta etapa compreende a aplicação de métodos inteligentes para extrair padrões, não trivialmente perceptíveis, de uma coleção de dados.

Para finalizar o processo de Data mining, uma terceira etapa é empregada com a finalidade de avaliar e utilizar o conhecimento extraído visando apoiar algum processo de tomada de decisão.

A técnica de clustering, dentre outras, pode ser utilizada para descobrir conhecimento em banco de dados no processo de Data Mining.

Clustering consiste em formar grupos conforme algum critério como a similaridade dos dados, assim os dados mais similares ficariam num mesmo grupo.

O objetivo dessa técnica é pois, dividir o conjunto de dados em subconjuntos homogêneos A maioria dos algoritmos de clusterização são de alguma forma dependentes de uma distância a qual é utilizada para definir que indivíduos são similares e quais não.

Algoritmos clássicos como K-means e fuzzy c-means são exemplos de algoritmos em que a distância utilizada tem um papel crucial no resultado final.

Em suas versões originais os dois algoritmos citados utilizam a distância L Minkowsky ou Euclidiana.

Sabe-se que esta distância tem limitações como não ser eficiente para tratar anomalias e induzir nos clusters detectados a forma de uma hiperesfera.

Por estes motivos, este trabalho análise a introdução de distâncias adaptativas para o fuzzy c-means, a idéia no uso de distâncias adaptativas é que elas permitam detectar clusters não homogêneos e de forma diferente da hiperesfera.

Tal trabalho já foi feito para o K-means, a extensão do mesmo para o fuzzy c-means é importante para aliar o poder de expressividade da associação de um indivíduo a vários clusters (clusterização fuzzy) através da função de pertinência com uma distância que supera algumas limitações da clássica L ou Euclidiana.

Postas as informações anteriores, os objetivos principais desta dissertação.
Descrever e analisar duas novas versões do algoritmo fuzzy c-means baseadas em distâncias adaptativas.

Comparar empiricamente o desempenho das novas versões do fuzzy c-means com o algoritmo original e com o K-means.
Contribuir com um framework para automatizar a realização de testes com diferentes algoritmos de clusterização e combinação exaustiva de parâmetros.

O restante desta dissertação está organizado da seguinte forma, Vivemos em um mundo cheio de dados.

Todos os dias, pessoas encontram grandes quantidades de informação e armazenam-nas como dados para análise e manipulação posterior.

Uma das principais formas de lidar com esses dados é classificá-los ou agrupá-los (cluster) em categorias ou grupos.

Classificação é uma das atividades mais primitivas dos seres humanos e tem desempenhado um papel importante na história do desenvolvimento humano.

Desde a infância para aprender alguma coisa, as pessoas sempre tentam achar as características que descrevem o objeto sendo aprendido e num passo seguinte comparam-nas com as características de objetos já conhecidos baseado em similaridades ou dissimilaridades.

Descrita informalmente no parágrafo anterior está a atividade de análise de dados.

Análise de dados pode ser dividida em exploratória ou confirmatória baseado na existência de modelos apropriados para a fonte de dados, mas um elemento chave em ambas formas de análise é o agrupamento (clustering) ou classificação de dados feito a partir de uma das seguintes formas, i Boa adequação a um modelo postulado.

Grupos naturais (clustering) descobertos através de análise.

Análise de agrupamento ou análise de cluster é a organização de uma coleção de padrões (usualmente representado como vetores de medidas ou um ponto em um espaço multidimensional) em clusters tomando como critério a similaridade.

Embora não haja uma concordância universal quanto a definição de clusterização, muitos pesquisadores,descrevem clusterização como a tarefa de formar clusters (grupos) considerando a similaridade interna e a separação externa, isto é, padrões em um mesmo cluster devem ser similares uns aos outros enquanto padrões em diferentes clusters devem ser diferentes.

Um exemplo de clusterização (clustering) é mostrado.

Os padrões de entrada e os clusters desejados são mostrados.

Neste caso, pontos pertencendo ao mesmo cluster receberam o mesmo rótulo.

Exemplo de clusterização de dados.

É importante entender a diferença entre análise de cluster (classificação não supervisionad e análise discriminante (classificação supervisionada).

Em classificação supervisionada, há o fornecimento de uma coleção de padrões e os seus rótulos, o problema é então rotular um novo padrão para o qual o rótulo não foi informado.

Tipicamente, os padrões rotulados são utilizados para aprender a descrição da classe (treinamento) e esta informação aprendida, por sua vez, é usado para rotular um novo padrão.

No caso da clusterização, o problema é colocar em grupos um conjunto de padrões não rotulados de forma que os grupos tenham um significado relevante.

Sob certo ponto de vista, rótulos estão presentes na atividade de clusterização, cada grupo formado poderia ser entendido como um rótulo, mas estes rótulos são obtidos a partir dos próprios dados.

Análise de clusters é amplamente utilizada.

Para citar apenas alguns dos vários exemplos encontrados, aprendizado de máquina incluindo mineração de dados, recuperação de documentos e segmentação de imagens, várias situações de análise exploratória de dados e tomada de decisão.

Geralmente, nestes tipos de problema, há pouca informação prévia sobre os dados, como modelos estatísticos, disponível e os tomadores de decisão devem fazer a menor quantidade possível de conjecturas sobre os dados.

É sob estas condições que a análise de cluster é particularmente útil, servindo para explorar inter-relações entre os dados para avaliar ainda que preliminarmente sua estrutura.

Clusterização é também conhecida como taxonomia numérica, aprendizado sem um professor (aprendizado não supervisionado), análise tipológica e partição, esta diversidade reflete a importante posição da análise de cluster na pesquisa científica, mas por outro lado causa confusão pelas diferentes terminologias e objetivos.

Algoritmos de clusterização feitos para resolver problemas numa área específica geralmente fazem conjecturas em favor daquela área específica.

Esta limitação afeta inevitavelmente a performance quando se aplica estes algoritmos em áreas cujos problemas não satisfazem essas premissas.

Muitos livros e artigos foram publicados sobre análise de cluster e suas aplicações a algum problema ou área específica.

Isto torna uma tarefa impossível fazer uma compilação que revise toda a abundante literatura disponível sobre o tópico.

O objetivo deste capítulo é, pois, trazer temas clássicos em qualquer revisão sobre clustering e direcionar a discussão para o propósito geral da dissertação.

Realizou uma descrição sistemática e compreensiva dos algoritmos de clusterização mais importantes e influentes com ênfase nos avanços dos últimos anos, voltando o trabalho para as áreas de estatística, computação e aprendizado de máquina.

Embora o livro seja voltado para mineração de dados, há um capítulo sobre análise de clusters excelente para uma introdução geral aos temas clássicos e avanços recentes.

Estão reunidos artigos clássicos sobre teoria de conjuntos fuzzy e suas diversas aplicações em reconhecimento de padrões, incluindo um capítulo somente para análise de clusters.

O restante deste capítulo está organizado como segue, na seção 2são apresentadas as principais fases envolvidas num processo de análise de clusters assim como são discutidas as questões relevantes de cada fase, apresenta a notação e a nomenclatura mais usual na área de clustering e que será usada ao longo de toda a dissertação, são apresentados os principais tipos de dados nos quais os dados para análise podem ser representados, ainda nesta seção apresenta-se como lidar com cada tipo de dado.

Apresenta uma categorização dos algoritmos de clusterização, os algoritmos relacionados a esta dissertação são vistos em maiores detalhes.

Procedimento de clusterização.

A típica análise de cluster consiste em quatro passos com retroalimentação.

Esses passos são intimamente relacionados e afetam os clusters resultantes.

Procedimento de análise de cluster com quatro passos básicos.

São eles, Seleção ou extração de atributos.

Seleção de atributos escolhe atributos distintivos de um conjunto de atributos candidatos, enquanto extração de atributos utiliza transformações para gerar atributos úteis e novos a partir dos originais.

Ambos são cruciais para a efetividade de aplicações envolvendo clusterização.

Uma boa seleção de atributos pode diminuir a carga de trabalho do e simplificar o passo seguinte.

Geralmente, atributos ideais devem servir para distinguir padrões pertencentes a diferentes clusters, ser imunes a ruído, fáceis de extrair e interpretar.

Desenvolvimento ou seleção de algoritmo de clustering.

Este passo é usualmente combinado com a seleção de uma medida de proximidade e a definição de uma função de critério.

Padrões são agrupados de acordo com a semelhança mútua.

Obviamente, a medida de proximidade afeta diretamente a formação dos clusters resultantes.

Quase todos os algoritmos de clusterização são explícita ou implicitamente dependentes de alguma definição de medida de proximidade.

Uma vez uma medida de proximidade foi escolhida, a construção de uma função de critério de clusterização torna a partição dos padrões em clusters um problema de otimização, o qual é bem definido matematicamente e tem várias soluções na literatura.

Clustering é um problema ubíquo e uma grande variedade de algoritmos têm sido desenvolvidos para resolver diferentes problemas em áreas específicas.

Contudo, não há um algoritmo de clusterização que possa ser usado para resolver todo e qualquer problema.

Têm sido difícil desenvolver um framework unificado para pensar a respeito de clustering em um nível técnico que abranja todas as diversas abordagens de clustering.

Desta forma é importante investigar cuidadosamente o problema em questão para selecionar ou desenvolver um algoritmo de clusterização adequado.

Validação dos clusters.

Dado um conjunto de dados, todo algoritmo de clusterização pode gerar uma partição, não importando se existe uma estrutura ou não que justifique isso.

Ainda mais, algoritmos diferentes levam normalmente a partições diferentes, mesmo para um determinado algoritmo parâmetros ou ordem de apresentação dos padrões de entrada podem afetar o resultado final.

Desta forma, é preciso haver padrões e critérios de avaliação eficientes para fornecer ao usuário confiabilidade em relação ao resultado obtido a partir do algoritmo utilizado.

Esta avaliação deve ser útil para responder questões como quantos clusters estão escondidos dentre os dados?

Mesmo que os clusters obtidos sejam significativos ou apenas artefatos do algoritmo, outra questão é por que se escolhe um algoritmo ao invés de outro qualquer?

Geralmente há três categorias de critérios, índices externos, índices internos, índices relativos.

Estes são definidos nos seguintes tipos de estruturas de clustering, hierárquica, particional e individual.

Há testes para a situação onde não existem grupos nos dados, mas estes são raramente usados, uma vez que o usuário acredita na presença de clusters.

Índices externos são baseados em alguma estrutura pré-especificada, a qual é a reflexão de informação prévia sobre os dados e usada como padrão para validar o resultado encontrado pelo algoritmo de clusterização.

Índices internos, ao contrário, examinam diretamente as estruturas criadas pelos algoritmos de clusterização para os dados de entrada.

Índices relativos enfatizam a comparação entre diferentes resultados obtidos por algoritmos de clusterização diferentes para que seja possível decidir qual deles pode revelar as características dos dados.

Interpretação dos resultados.

O objetivo final da análise de cluster é prover aos usuários impressões significativas a partir dos dados originais de forma que estes possam resolver efetivamente os problemas encontrados.

Especialistas nos campos relacionados interpretam a partição dos dados podendo ser necessário maiores análises e experimentos para garantir a confiabilidade do conhecimento extraído.

Fluxo para realimentação.

Análise de Cluster não é um processo realizado em apenas uma execução.

Em muitas circunstâncias, é necessário uma série de tentativas e repetições.

Ainda, não há universal e efetivo critério para guiar a seleção de atributos e de algoritmos de clusterização.

Critérios de validação provêm impressões sobre a qualidade dos clusters, mas como escolher este mesmo critério é ainda um problema que requer mais esforços.

Os seguintes termos e notações são usados ao longo deste trabalho, Um padrão ou vetor de atributos ou ponto em um espaço multidimensional ou indivíduo ou objeto x é um simples item de dados utilizado pelo algoritmo de clusterização.

Ele tipicamente consiste em um vetor de medições.
Os componentes escalares individuais de um padrão são chamados atributos ou variáveis ou características, p é a dimensionalidade do indivíduo ou padrão, ou ainda, a dimensionalidade do espaço dos padrões.
Um conjunto de indivíduos ou padrões é denotado.

O enésimo padrão é denominado.

Em muitos casos, um conjunto de dados a ser clusterizado é visto como uma matriz de padrões, uma classe, em abstrato, refere-se a um estado da natureza que governa o processo de geração de padrões em alguns casos.

Mais concretamente, uma classe pode ser vista como uma fonte de padrões cuja distribuição no espaço de atributos é governada por uma função densidade de probabilidade específica para a classe.

Técnicas de clusterização têm como objetivo agrupar os padrões de forma que as classes obtidas no processo reflitam os diferentes processos de geração de padrões presentes no conjunto de dados.

Nesta seção, serão estudados os tipos de dados que freqüentemente ocorrem em análise de cluster e como pré-processá-los para análise.

Supondo que um conjunto de dados para ser clusterizado contenha n indivíduos, os quais podem representar pessoas, casas, documentos, países e outros.

Algoritmos de clusterização tipicamente operam em alguma das estruturas de dados a seguir, Matriz de dados (ou estrutura de indivíduo por atributo), Esta estrutura representa n indivíduos, tais como pessoas, com p atributos, tais como idade, altura, peso, sexo, raça e outros.

A estrutura está na forma de uma tabela relacional.
Matriz de dissimilaridade (ou estrutura de indivíduo por indivíduo), Esta estrutura armazena uma coleção de proximidades que estão disponíveis para todos os pares de n indivíduos.

Ela é representada por uma tabela e a diferença ou dissimilaridade medida entre os indivíduos.

Em geral, é um número não negativo que é perto de zero quando os indivíduos i e j são muito similares e torna-se maior quanto maior for a diferença entre os indivíduos.

Conforme pode ser visto pela matriz.

Medidas de dissimilaridade serão abordadas adiante neste capítulo.

A seguir serão vistos os tipos de dados que normalmente são utilizados para descrever os atributos dos indivíduos e como medir a proximidade ou similaridade entre indivíduos.

Em outros trabalhos não se distingue entre atributos em escala linear e escala não linear sendo ambos tratados como quantitativos.

Uma vantagem em se fazer tal distinção é a de que o valor expresso pelo atributo está associado à escala ou sistema de medida utilizado como referência e esta informação pode ser utilizada, por exemplo, por alguma medida de distância.

Esta seção discute atributos em escala linear (ou quantitativos contínuos) e como normalizá-los.

São descritas a seguir medidas de distância que são comumente utilizadas para computar dissimilaridade entre indivíduos descritos por este tipo de atributo.

Entre tipos de medidas utilizadas estão as distâncias Euclidiana, Manhatan ou city-block e Minkowski.

Atributos em escala linear são medições contínuas de uma escala aproximadamente linear.

Exemplos típicos incluem peso e altura, coordenadas de latitude e longitude e temperatura ambiente.

A unidade de medida utilizada pode afetar a análise de cluster.

Por exemplo, mudança de unidade de medida de metros para polegadas para medir altura ou de quilograma para libras para medir peso pode levar a uma estrutura de cluster bem diferente.

Para ajudar a evitar dependência na escolha da unidade de medida, os dados devem ser normalizados.

Normalizar medidas tenta dar a todas os atributos igual peso.

Isto é particularmente útil quando não há conhecimento prévio dos dados.

Contudo, em algumas aplicações, o usuário pode querer dar mais importância a um certo conjunto de atributos do que ao resto.

Por exemplo, quando clusterizando candidatos a jogadores de basquete pode ser preferível dar mais peso importância à variável altura.

Para normalizar medidas, uma escolha é converter as medidas originais em atributos sem unidade.

Dados os valores medidos para um atributo f, isto pode ser feito da seguinte forma, O desvio absoluto médio, s, é mais robusto a anomalias do que o desvio padrão.

Quando se calcula o desvio absoluto médio, os desvios da média não são elevados ao quadrado, desta forma, o efeito de anomalias é de certa forma reduzido.

Há outras medidas de dispersão robustas como desvio absoluto mediano.

A vantagem de utilizar o desvio absoluto médio é que os escores-z de anomalias não se tornam muito pequenos, portanto as anomalias podem ser detectadas.

Uma vez normalizados os dados, ou sem normalizá-los já que esta decisão depende do problema em questão.

A dissimilaridade ou similaridade entre os indivíduos descritos por atributos em escala linear é calculada baseada na distância entre cada par de indivíduos.

Uma das distâncias mais populares é a Euclidiana.

Outra bastante conhecida é a Manhattan ou city block.

Uma distância ou função de dissimilaridade sobre um conjunto de dados X é definido para satisfazer as seguintes condições, também são observadas a distância ou função de dissimilaridade é chamada de métrica.

Medidas de similaridade podem ser obtidas a partir de medidas de dissimilaridade pela fórmula.

Todas as distâncias mostradas na mpõem restrições nos dados devido a sua geometria.

A geometria de uma distância pode ser vista facilmente considerando-se um indivíduo.

Medidas de dissimilaridade e similaridade para dados quantitativos.

Os contornos para distâncias constantes mostram que tipo de forma geométrica são enfatizadas na busca por estruturas nos dados por alguns exemplos de distância.

Desta forma, percebe-se que a distância Euclidiana favorece clusters de forma circular.

Exemplo de funções de distância três dimensões e contornos, Euclidiana, City-block, Sup ou Tchebyschev.

Um atributo binário tem apenas dois estados, 0 ou 1, onde 0 significa o atributo está ausente e 1 significa o atributo está presente.

Dado, por exemplo, um atributo fumante descrevendo um paciente, 1 indica que este paciente fuma enquanto 0 indica que o paciente não fuma.

Tratar atributos binários como se eles fossem atributos quantitativos normais pode levar a resultados errôneos pelo algoritmo de clusterização.

Desta forma, são necessários métodos específicos para calcular a dissimilaridade entre atributos binários.

Para calcular a dissimilaridade entre dois atributos binários uma abordagem envolve calcular a matriz de dissimilaridade dos dados.

Se considera-se que todos os atributos binários têm o mesmo peso, tem-se a e contingência, onde n é o número de atributos que são iguais a 1 para os indivíduos i e j, n é o número de atributos que são iguais a 1 para o indivíduo i mas que são iguais a 0 para o indivíduo j, n é o número de atributos que são iguais a 0 para o indivíduo i mas iguais a 1 para o indivíduo j e n é o número de atributos que são iguais a 0 para tanto o indivíduo i quanto o indivíduo j.

Uma tabela de contingência para atributos binários.

Um atributo binário pode ser simétrico ou assimétrico.

É simétrico quando os dois estados têm o mesmo valor e peso, não há preferência entre atribuir 0 ou 1 ao atributo.

Um exemplo é o atributo sexo.

Similaridade baseada em atributos binários simétricos é chamada similaridade invariante, no sentido que o resultado não muda quando alguns ou todos os atributos binários têm valores diferentes.

Para similaridades invariantes, o mais conhecido coeficiente para avaliar a dissimilaridade entre os indivíduos i e j é o coeficiente de casamento simples (simple matching coefficient), definido como, Definidos como medida de similaridade há outras coeficientes mostrados a seguir (lembrando que pela fórmula 26 é possível obter uma medida de dissimilaridade de uma medida de similaridade).

Pares de não casamento são ponderados com base em sua contribuição para a similaridade.

Um atributo binário é assimétrico se os estados não são igualmente importantes, tal como o resultado positivo ou negativo de um teste para a presença de uma doença.

Por convenção, deve-se codificar o resultado mais importante, o qual usualmente é mais raro, por 1 e tratar o outro por 0.

Dados dois atributos binários assimétricos, a ocorrência de dois 1 s (um casamento positivo) é considerado mais importante que dois 0 s (um casamento negativo).

Desta forma, este tipo de atributo é considerado muitas vezes como sendo unário, ou seja, tendo apenas um estado.

A similaridade baseada nesse tipo de atributo é chamada similaridade não invariante.

Para similaridades não invariantes, o mais conhecido coeficiente é Coeficiente de Jaccard, onde o número de casamentos negativos, n, é considerado não importante, portanto ignorado no cálculo do coeficiente.

A fórmula do Coeficiente de Jaccard.

Há outras medidas de similaridade focando apenas a co-ocorrência de um estado entre os atributos.

A seguir alguns exemplos.

Quando ocorrem num conjunto de dados atributos binários simétricos e não simétricos a abordagem para dados formados por atributos mistos descrita na seção 24pode ser aplicada.

Exemplo de dissimilaridade entre atributos binários, Suponha que uma tabela de registros de pacientes contém os atributos nome, sexo, febre, tosse, teste-1, teste-2, teste-e teste-4, onde nome é um atributo identificador do paciente, sexo é um atributo binário simétrico e o restante dos atributos são binários assimétricos.

Para atributos assimétricos, seja o valor para S(sim) e P(positivo) definido como 1 e o valor para N (não ou negativo) definido como 0.

Supondo que a distância entre pacientes é computada apenas tomando como base os atributos assimétricos.

De acordo com a fórmula do Coeficiente de Jaccard, a distância entre os pares de pacientes João, Maria e José é, Uma tabela de dados relacional contendo principalmente atributos binários.

As distâncias calculadas sugerem que provavelmente José e Maria não tem uma doença similar uma vez que o valor da dissimilaridade entre eles é o maior calculado.

João e Maria são o par com maior probabilidade de ter uma doença similar.

Esta seção descreve como calcular a dissimilaridade entre indivíduos descritos através de atributos nominais, ordinais e em escala não linear.

Atributos Nominais.

Um atributo nominal é a generalização de um atributo binário no sentido que ele pode assumir mais que dois estados.

Por exemplo, o atributo cormapa é um atributo nominal que pode assumir, por exemplo, os estados, vermelho, amarelo, verde, rosa e azul.

Seja o número de estados de um atributo nominal M.

Os estados podem ser denotados por letras, símbolos, ou um conjunto de números inteiros como {1,2,M}.

Note que os inteiros são utilizados apenas para manipular os dados e não representam, portanto, nenhum tipo de ordem.

Assim no caso do atributo nominal cormapa se o estado azul for mapeado para o valor inteiro 1 e o estado amarelo for mapeado para o valor isto não significa que amarelo é maior que azul.

Pesos podem ser atribuídos para aumentar o efeito de m ou para atribuir maior peso a atributos que tenham um grande número de estados.

Atributos nominais podem ser codificados como atributos binários assimétricos através da criação de um atributo binário para cada um dos M estados que o atributo nominal possua.

Para um indivíduo cujo atributo nominal esteja num certo estado, o atributo binário correspondente a este estado é atribuído 1 enquanto o restante dos atributos binários recebem valor 0.

Por exemplo, para codificar o atributo nominal cormapa, um atributo binário deve ser criado para cada um dos cinco estados mencionados anteriormente.

Para um indivíduo tendo o valor de cormapa definido como amarelo, o atributo binário correspondente, amarelo, recebe o valor 1 e todos os demais atributos recebem 0.

Desta forma os coeficientes de cálculo de dissimilaridade visto na seção 24podem ser utilizados.

Atributos ordinais.

Um atributo ordinal discreto lembra um atributo nominal exceto pelo fato que os M estados de um atributo ordinal estão ordenados numa seqüência significativa.

Atributos ordinais são muitos utilizados para registrar informações de julgamento subjetivo.

Se pedirmos a uma pessoa para dar sua opinião a respeito de uma obra de arte utilizando um dos seguintes valores, gosta muito, gosta, gosta pouco e detesta, a resposta para cada obra de arte será armazenada como um atributo ordinal e nesse caso a diferença entre estados é óbvia.

Outro exemplo é em escalas profissionais onde as profissões são normalmente enumeradas em ordem ascendente como assistente, associado, sênior.

Um atributo ordinal contínuo parece um conjunto de valores contínuos numa escala desconhecida, isto é, a ordem relativa é essencial, mas a magnitude não é.

Por exemplo, a ordem relativa em um esporte (ouro, prata, bronze) é normalmente mais importante do que o exato valor de um estado.

Atributos ordinais podem ser obtidos pela discretização de valores de atributos quantitativos através da divisão da faixa de valores em um número finito de classes.

O tratamento de atributos ordinais é bem similar ao tratamento de atributos quantitativos (em escala linear ou não) ao calcular o valor da dissimilaridade entre dois indivíduos.

Suponha que f é um atributo de um conjunto de atributos ordinais descrevendo n indivíduos.

O cálculo da dissimilaridade em relação a f ocorre da seguinte forma, Como cada atributo ordinal pode ter um número diferente de estados, é necessário mapear a faixa de cada variável para o intervalo peso.

Isto pode ser feito substituindo o valor r do i-ésimo indivíduo no f-ésimo atributo.

A dissimilaridade pode ser calculada usando qualquer uma das formas apresentadas na seção 241 utilizando z para representar o valor do atributo f para o indivíduo i.

Atributos em escala não linear.

Um atributo em escala não linear expressa uma medição em um escala não linear, como uma escala exponencial, seguindo aproximadamente uma fórmula como, onde A e B são constantes positivas.

Típicos exemplos incluem o crescimento de uma população de bactérias e o decaimento de um elemento químico radioativo.

Há três modos de calcular dissimilaridade entre indivíduos descritos por atributos em escala exponencial.

Tratar atributos em escala não linear como atributos em escala linear.

Isto pode não ser uma boa escolha já que a escala pode estar distorcida.

Os valores de y podem ser tratados como atributos em escala linear como descrito na seção 241 Tratar x como ordinal e sua posição no ranking como atributo de escala linear.

Os dois últimos métodos são os mais eficientes, embora a escolha seja sempre dependente de onde será aplicado.

Em muitos conjuntos de dados reais, os indivíduos são descritos por atributos de todos os tipos discutidos até agora como atributos em escala linear, em escala não linear, binários, ordinais, nominais e outros.

Deste fato surge o problema de como calcular a dissimilaridade entre indivíduos descritos através de atributos de diferentes tipos.

Uma solução é juntar todos os atributos de um certo tipo e realizar uma análise de cluster para cada tipo de atributo.

Um problema desta abordagem é que análises feitas desta forma não levarão, provavelmente, a resultados compatíveis.

Outra abordagem para o problema é mapear todos os atributos para o intervalo usar medidas como a distância euclidiana.

Ou ainda, transformar todos os atributos em binários e utilizar medidas de dissimilaridade para atributos binários.

Uma importante desvantagens dessas abordagens é a perda de informação.

Uma abordagem mais poderosa foi proposta na forma de uma medida de similaridade.

Outras abordagens através de medidas (funções de similaridade) para o cálculo da similaridade de indivíduos descritos por diferentes tipos de atributos podem ser encontradas.

Existem muitos algoritmos de clusterização disponíveis na literatura.

A escolha de um algoritmo depende tanto dos tipos de dados disponíveis quanto da aplicação desejada.

Se a análise de clusters for usada como uma ferramenta para exploração dos dados, vários algoritmos podem ser executadas sobre o mesmo conjunto de dados a fim de avaliar os diferentes resultados de cada algoritmo e desta forma, comparando os resultados, descobrir que informações os dados trazem ocultas.

A classificação de algoritmos de clusterização não é uma tarefa direta ou canônica.

A categorização fornecida a seguir é baseada em categorizações que vêm sendo feitas na literatura.

A divisão mais unanimemente aceita é classificar os algoritmos em de métodos hierárquicos ou de partição.

Neste caso conforme se explicará a seguir o critério de distinção é a saída produzida.

Na classificação mostrada a seguir, note que o principal critério para definir a categoria é a técnica, método ou teoria utilizada no algoritmo.

Em alguns casos, um mesmo algoritmo pode apresentar características de mais de uma categoria.

Baseados em restrições, Clustering com distâncias obstruídas, CO.

Baseados em computação evolucionária, Algoritmo KM genético, GK, algoritmo guiado geneticamente GG.

Fuzzy, Fuzzy c-means (FCM), Fuzzy c-shells (FCS).

Além dos critérios já citados para categorizar algoritmos de clusterização dois novos critérios têm ganhado importância nos últimos anos.

Escalabilidade para grandes conjuntos de dados, são exemplos de algoritmos nesta categoria, CURE, BIRCH, DBSCAN, WaveCluster e ART.

A outra categoria é a capacidade de lidar com dados descritos por muitas dimensões, um exemplo dessa categoria é o algoritmo CLIQUE.

Algoritmos de clustering baseados no método hierárquico, H organizam um conjunto de dados em uma estrutura hierárquica de acordo com a proximidade entre os indivíduos.

Os resultados de um algoritmo HC são normalmente mostrados como uma árvore binária ou um dendograma.

A raiz do dendograma representa o conjunto de dados inteiro e os nós folha representam os indivíduos.

Os nós intermediários representam a magnitude da proximidade entre os indivíduos.

A altura do dendograma expressa a distância entre um par de indivíduos ou entre um par de clusters ou ainda entre um indivíduo e um cluster.

O resultado da clusterização pode ser obtido cortando-se o dendograma em diferentes níveis.

Esta forma de representação fornece descrições informativas e visualização para as estruturas de grupos em potencial, especialmente quando há realmente relações hierárquicas nos dados como, por exemplo, dados de pesquisa sobre evolução de espécies.

Exemplo de algoritmo de clusterização de dados hierárquico aglomerativo e divisivo.

Algoritmos HC são divididos em aglomerativos ou divisivos.

Algoritmos aglomerativos começam com cada indivíduo no conjunto de dados sendo representado por um único cluster.

Uma série de combinações entre os clusters iniciais ocorrem resultando depois de um número de passos em todos os indivíduos estando em um mesmo cluster.

Algoritmos divisivos operam na direção oposta.

No começão, o conjunto de dados inteiro está em um só cluster e um procedimento que divide os clusters vai ocorrendo sucessivamente até que cada indivíduo seja um único cluster.

Para um conjunto com N indivíduos há diferentes possíveis divisões do conjunto em dois subconjuntos, o que é computacionalmente ineficiente de avaliar a medida que N se torna um número grande.

Por esta razão, algoritmos divisivos não são utilizados normalmente na prática.

São ilustrados os modos de funcionamento de um algoritmo aglomerativo e um divisivo, note que k representam o número do passo atual do algoritmo, para o algoritmo aglomerativo a medida que k aumenta um único cluster vai sendo formado e o contrário é verdade para o algoritmo divisivo.

Os passos genéricos de um algoritmo hierárquico aglomerativo são mostrados no algoritmo 1.

Algoritmo 1 Algoritmo hierárquico aglomerativo 1, Comece com N clusters cada um com um único indivíduo.

Calcula a matriz de proximidade para os N clusters.

Procure a distância mínima, onde é calculada a partir da matriz de proximidade.

Esta distância é usada para formar um novo clusters a partir dos clusters C e C.

Atualize a matriz de proximidade, calculando a distância entre os novos clusters e os outros.

Repita os passos e até que todos os indivíduos estejam no mesmo cluster.

Baseados nas diferentes definições de distância entre dois clusters, há muitos algoritmos aglomerativos.

Entre os mais simples e populares métodos estão ligação simples e ligação completa.

Na ligação simples, a distância entre dois clusters é determinada pelos dois indivíduos, cada um de um cluster diferente, mais próximos um do outro.

Ao contrário, a ligação completa usa como distância entre dois clusters aquela entre os indivíduos nos diferentes clusters que estão mais afastados um do outro.

Está ilustrada o cálculo da distância entre dois clusters, no caso da ligação simples e no caso da ligação completa.

Exemplo de calculo da distância entre grupos, ligação simples, ligação completa.

Clusterização de um conjunto de dados em grupos baseada no algoritmo KM.

Ao contrário de métodos hierárquicos, métodos de partição associam um conjunto de indivíduos a K grupos sem criar uma estrutura hierárquica.

Como pode ser visto o conjunto de dados está sendo dividido em grupos, ainda são mostradas diferentes etapas de execução do algoritmo, note como elementos vão mudando de um cluster para outro e como a forma destes vai se modificando.

Em princípio, a partição ótima, baseada em algum critério específico, pode ser encontrada enumerando-se todas as possibilidades.

Esta abordagem exaustiva é inviável pelo tempo computacional que seria necessário para fazer uma busca por todas as combinações possíveis.

Por tudo isso, algoritmos baseados em heurísticas para encontrar uma solução aproximada (geralmente estes são baseados em algoritmos gulosos, do inglês greedy, cujo princípio é tomar a melhor decisão no momento esperando que isto conduza a melhor decisão global) têm sido desenvolvidos.

Um dos fatores importantes em métodos de partição é a função objetivo ou critério.

A soma de erros ao quadrado é uma das mais amplamente utilizadas como critério.

Supondo que tem-se um conjunto de indivíduos e deseja-se organizá-los.

O critério baseado no erro quadrado é definido da seguinte forma.
Sejam os protótipos do cluster, eles minimizam o critério de clustering e são atualizados de acordo com a expressão a seguir, O resultado até agora mostra que atualizando-se pela equação obtém-se um ponto extremo.

Para concluir se este é um ponto de mínimo usaremos o teste da segunda derivada.

A segunda derivada é positiva, portando pode-se concluir que calculando-se conforme a equação, minimiza.

O resultado do somatório no denominador da fração acima é o total de indivíduos no cluster.

O algoritmo K-means (KM) é o algoritmo mais conhecido baseado em erro quadrado.

Os principais passos do KM estão descritos no algoritmo 2.

Uma vez que K e p são geralmente menores que n, KM pode ser utilizado para clusterizar grandes conjuntos de dados.

As desvantagens do KM são também bem estudadas e como resultado disto muitas variações do KM têm sido propostas para superar estes obstáculos.

A seguir um sumário das principais desvantagens e de propostas para solucioná-las.

Não há nenhum método eficiente e universal para identificar o número de clusters e a partição inicial.

Os protótipos encontrados no estado de convergência variam com a partição inicial.

Uma estratégia geral para este problema é executar o algoritmo várias vezes com diferentes partições aleatórias iniciais.

Apresentaram um algoritmo de refinamento que utiliza o KM M vezes em M subconjuntos aleatórios do conjunto de dados original.

O conjunto formado pela união das soluções (protótipos dos K clusters) dos M subconjuntos é clusterizado M vezes novamente, em cada uma dessas vezes uma das diferentes soluções, protótipos, para um dos subconjuntos será utilizada como solução inicial.

A partição inicial para o conjunto de dados inteiro é obtida escolhendo-se a solução com soma de distâncias ao quadrado mínima dentre as soluções obtidas nas execuções anteriores.

O algoritmo ISODATA lida com estimar o número K.

ISO-DATA ajusta automaticamente o número de clusters unificando e dividindo clusters de acordo com um limiar pré-especificado (desta forma o problema de identificar o número inicial de clusters se torna o de ajustar o valor do limiar).

O novo K é utilizado como o número esperado de clusters na próxima iteração.

O procedimento iterativo do KM não garante convergência a um ótimo global.

Técnicas baseadas em pesquisa combinatória como SA (Simulated Annealing) Clustering encontram o ótimo global a custa de alto tempo computacional para produzir uma resposta.

GKA balanceia busca global e convergência rápida.

São analisados diversos aspectos relacionados à convergência do KM.

K-means é sensitivo a ruído e anomalias.

Mesmo se um indivíduo está bem afastado dos protótipos dos clusters ele é obrigado a ser incluído em algum cluster desta forma distorcendo a forma daquele cluster.

ISODATA e PAM consideram os efeitos de anomalias.

ISODATA livra-se das anomalias eliminando clusters com poucos indivíduos.

A operação de divisão executada pelo ISODATA evita a ocorrência do efeito de clusters alongados típico do KM.

PAM utiliza indivíduos do conjunto de dados como protótipo dos clusters e assim evita o efeito de anomalias.

A definição de "means"(médias) limita a aplicação a somente dados descritos por atributos quantitativos.

O algoritmo K-medianas é uma escolha natural quando o cálculo de médias não é possível já que achar as medianas não necessita que cálculos sejam feitos e sempre existem medianas.

Exemplo da clusterização fuzzy x hard.

Abordagens de clustering tradicionais geram partições, em uma partição, cada indivíduo pertence a um e somente um cluster isto é conhecido como clusterização hard.

Assim os clusters nesses tipos de abordagens são disjuntos.

Fuzzy clustering estende essa noção para permitir associar um indivíduo com todos os clusters usando uma função de pertinência.

Na prática a separação entre clusters é uma noção fuzzy, nebulos, por exemplo, um anfíbio é um animal que tem características de animais aquáticos e terrestres, portanto, vai apresentar características semelhantes a ambos grupos.

O conceito de conjuntos fuzzy oferece a vantagem de expressar este tipo de situação em que um indivíduo compartilha similaridade com vários grupos através da possibilidade de um algoritmo associar cada indivíduo parcialmente a todos os grupos.

Em um algoritmo de clusterização fuzzy, cada cluster é um conjunto fuzzy de todos os indivíduos.

Os retângulos dividem o conjunto de dados em dois hard clusters.

Um algoritmo de clusterização fuzzy poderia produzir os dois clusters fuzzy F e F representados por elipses.

Os indivíduos vão ter pertinência definida no intervalo.

Os pares ordenados em cada cluster representam respectivamente o indivíduo n e sua pertinência ao cluster i, u.

Valores de pertinência altos indicam alta confiança na associação de um indivíduo a um cluster.

Uma partição hard pode ser obtida a partir de uma partição fuzzy aplicando um limiar nos valores de pertinência.

A seguir uma breve exposição sobre espaço de partição fuzzy.

Qualquer conjunto de valores satisfazendo às condições 21219 pode formar uma matriz degenerado de c partições fuzzy de X, No caso de todos os u serem 0 ou 1, tem-se o subconjunto de c partições hard de X, A razão para estas matrizes serem chamadas partições é que u pode ser interpretado como a pertinência de um indivíduo x em um cluster i.

Usar M como modelo é mais realístico uma vez que sabe-se pela experiência que as fronteiras entre as classes de objetos reais é muito mal delineada.

O algoritmo de clusterização fuzzy mais popular é o fuzzy c-means (FCM).

Ele é melhor que o KM que é um algoritmo har para evitar mínimo local, mas o FCM pode ainda convergir para um mínimo local do critério baseado em soma de erros ao quadrado.

O desenvolvimento de funções de pertinência é o problema mais importante em clusterização fuzzy.

Diferentes escolhas incluem aquelas baseadas em decomposição de similaridade e centróides de clusters.

Uma generalização do FCM foi proposta através de uma família de funções objetivo (critério).

FCM pode ser tomado como uma generalização do algoritmo ISO-DATA.

O algoritmo fuzzy c-means (FCM) é um algoritmo não hierárquico de clusterização cujo objetivo é fornecer uma partição fuzzy de um conjunto de indivíduos em c clusters.

Para tanto o FCM define e minimiza uma função objetivo, a qual mede a adequação entre os indivíduos e os clusters.

A função objetivo é definida.

Semelhante ao KM, FCM tem problemas para lidar com ruídos e anomalias nos dados e além disso, há a dificuldade de definir a partição inicial.

Outro problema relevante do FCM é a sua complexidade computacional.

FCM alterna o cálculo da matriz de pertinência (passo de alocação) e o cálculo da matriz de protótipos (passo de representação).

Por sua complexidade FCM não é considerado escalável para grandes conjuntos de dados.

Foi proposta uma forma de acelerar o cálculo através de uma atualização combinada da matriz de pertinência e dos protótipos, o que torna o algoritmo linear em relação ao número de clusters.

O algoritmo começa a partir de uma matriz de pertinência inicial e a daí alterna um passo de representação e um passo de alocação até a convergência quando o critério atinge um valor estacionário que corresponde a um mínimo (geralmente local).

Sejam os protótipos, eles minimizam o critério de clustering e são atualizados de acordo com a expressão a seguir, Uma vez que, no passo de representação, a pertinência de cada indivíduo no cluster, o parâmetro está fixo, pode-se reescrever o critério.

O resultado até agora mostra que atualizando-se g pela equação 22obtém-se um ponto extremo de J(g).

Para concluir se este é um ponto de mínimo usaremos o teste da segunda derivada.

A segunda derivada de J(g) é positiva portando pode-se concluir que calculando-se g conforme a equação 22minimiza J(g).

Seja a pertinência u de cada indivíduo ao cluster, ela minimiza o critério de clustering sob as seguintes restrições e é atualizada de acordo com a expressão a seguir, Considere que no passo de alocação, os protótipos do cluster e o parâmetro estão fixos.

Uma vez que é uma matriz degenerada, suas colunas são independentes e a minimização pode ser obtida aplicando-se o método dos multiplicadores de Lagrange a cada termo.

Será verificado agora pelo teste da segunda derivada se este valor é um mínimo.

A matriz é diagonal e todos os seus termos são positivos.

Isto permite concluir que ela é definida positivamente e portanto isto implica que atualizar pela expressão leva a um valor mínimo.

O sumário dos passos do algoritmo Fuzzy c-means (FCM), distância L Minkowsky, é dado no algoritmo.

Neste capítulo, foi apresentada brevemente a análise de cluster ou agrupamento.

Foram apresentadas as principais fases num processo de análise de agrupamento e as questões abertas relacionadas às mesmas.

Um categorização dos métodos de clusterização foi feita, dando-se maior destaque a divisão dos algoritmos entre hierárquicos e particionais.

Os algoritmos K-means e fuzzy c-means foram apresentados com maiores detalhes, pois estes serão utilizados adiante neste trabalho.

Este capítulo apresenta duas versões do algoritmo Fuzzy c-means para distâncias adaptativas.

Na primeira versão, Fuzzy c-means com Distância Adaptativa por Atributo (FCMA I), é introduzido o cálculo de pesos para cada atributo como principal alteração do algoritmo original.

Na segunda versão, Fuzzy c-means com Distância Adaptativa por Classe e por Atributo (FCMA), é introduzido o cálculo de pesos para cada classe e para cada atributo da classe como principal alteração do algoritmo original.

A idéia de distâncias adaptativas vem de um trabalho com sucesso nesse sentido para o algoritmo nuvem dinâmica.

Nesta seção, apresenta-se um algoritmo fuzzy c-means para dados quantitativos baseado em uma distância L Minkowsky adaptativa por atributo, ou única.

A principal idéia contida nesta abordagem é que há uma distância para comparar indivíduos e os seus respectivos protótipos que muda a cada iteração, mas que é a mesma para todas as classes (aqui o conceito do termo classe é considerado sinônimo de cluster).

A vantagem desse tipo de distância adaptativa é que o algoritmo de clustering torna-se capaz de achar clusters de diferentes formas e tamanhos.

Este algoritmo adaptativo busca por uma partição fuzzy de um conjunto de indivíduos em clusters, os protótipos correspondentes e o quadrado de um distância L Minkowsky adaptativa de tal maneira que um critério medindo a adequação entre os clusters e seus protótipos, ou representativos, é localmente minimizado.

Para o presente algoritmo o critério Jé definido como segue, agora o quadrado de uma distância LMinkowsky adaptativa, parametrizado por um vetor de pesos, o qual muda a cada iteração mas é o mesmo para todas as classes.

O algoritmo inicia a partir de uma matriz de pertinência inicial para cada indivíduo k em cada cluster C e alterna entre uma passo de representação e outro de alocação até o estado de convergência do algoritmo, no qual o critério Jatingi um valor estacionário representado por um mínimo, geralmente, local.

O passo de representação tem agora duas etapas.

Na primeira etapa, a pertinência u de cada indivíduo k em cada cluster C e o vetor de pesos estão fixos, atualiza-se então os protótipos.

Na segunda etapa, a pertinência u de cada indivíduo k em cada cluster C e os protótipos g correspondentes aos clusters estão fixos, atualiza-se então o vetor de pesos.

No passo de alocação, os protótipos das classes e o vetor de pesos, estão fixos, atualiza-se neste momento a pertinência de cada indivíduo em cada cluster.

Sejam os protótipos do cluster, eles minimizam o critério de clustering Je são atualizados de acordo com a expressão a seguir, Nesta seção, apresenta-se um algoritmo fuzzy c-means para dados quantitativos baseado em uma distância L Minkowsky adaptativa por classe e por atributo.

A principal idéia contida nesta abordagem é que há uma distância para comparar clusters e os seus respectivos protótipos que muda a cada iteração, isto é, a distância não é definida uma vez e permanece fixa, como também varia de um atributo de uma classe para o mesmo atributo em outra classe (o termo classe é utilizado aqui como sinônimo para cluster).

Novamente, a vantagem desse tipo de distância adaptativa é que o algoritmo de clustering torna-se capaz de achar clusters de diferente formas e tamanhos.

Este algoritmo adaptativo busca por uma partição fuzzy de um conjunto de indivíduos em c clusters, os protótipos correspondentes e o quadrado de um distância L Minkowsky adaptativa por atributo e por classe de tal maneira que um critério Jmedindo a adequação entre os clusters e seus protótipos, ou representativos, é minimizado, geralmente, localmente.

O algoritmo inicia a partir de uma matriz de pertinência inicial para cada indivíduo k em cada cluster C e alterna entre uma passo de representação e outro de alocação até o estado de convergência do algoritmo, no qual o critério Jatingi um valor estacionário representado por um mínimo, geralmente, local.

O passo de representação tem agora duas etapas.

Na primeira etapa, a pertinência de cada indivíduo em cada cluster e o vetor de pesos estão fixos, atualiza-se então os protótipos.

Na segunda etapa, a pertinência de cada indivíduo em cada cluster e os protótipos correspondentes às classes estão fixos, atualiza-se então o vetor de pesos.

No passo de alocação, os protótipos das classes e o vetor de pesos estão fixos, atualiza-se neste passo a pertinência de cada indivíduo em cada cluster.

Sejam os protótipos das classes, eles minimizam o critério de clustering Je são atualizados de acordo com a equação a seguir, Este capítulo descreve como os algoritmos foram avaliados.

Primeiramente, são descritos os dados artificiais ou sintéticos gerados para aplicar aos algoritmos e em seguida descreve-se os conjuntos de dados reais ou bench-mark utilizados.

Considerou-se um conjunto de dados como real aquele conjunto de dados cujo valor dos atributos não foi processado durante os experimentos que serão descritos adiante, ao contrário, os dados foram apresentados aos algoritmos de clusterização tal como obtidos.

Os dados artificiais foram gerados de forma a oferecer diferentes graus de dificuldade para os algoritmos.

A avaliação dos algoritmos para este caso foi feita tomando por base um experimento do tipo Monte Carlo.

Os dados reais escolhidos são exemplos clássicos da literatura relacionada a este trabalho.

A avaliação do desempenho dos algoritmos nos conjuntos de dados selecionados foi feita através de um índice de validação externa, Índice de Rand Corrigido (CR) e através do Coeficiente de Partição Normalizado de Dunn D.

Para realizar a avaliação experimental que se segue foi desenvolvido um conjunto de ferramentas de software utilizando-se a linguagem de programação C++.

O desenvolvimento foi feito tomando por base princípios da engenharia de software como reuso, orientação a objetos, padrões de projeto.

O resultado é que o framework mostra-se ser muito extensível e de fácil manutenção.

Estendê-lo para suportar o presente trabalhou comprovou às características mencionadas.

O framework é dividido em vários módulos que compartilham uma estrutura de classes em comum.

Este trabalhou criou mais dois módulo.

Um chamado módulo de simulação que pode ser usado por qualquer dos módulos existentes, através deste é possível, por exemplo, automaticamente simular cinco diferentes algoritmos de clusterização existentes no framework, para 20 conjuntos de dados diferentes e ainda para cada algoritmo e conjunto de dados especificar várias combinações de parâmetros a serem testadas.

Antes deste módulo as diferentes execuções eram feitas manualmente o que dificultava executar uma combinação exaustiva de parâmetros.

O outro módulo implementado para esta dissertação é composto dos algoritmos apresentados aqui, KM, FCM, FCMA I e FCMA.

Este capítulo é dividido em duas seções, seção 4 descreve os conjuntos de dados utilizados nos experimentos, foram utilizados tanto dados gerados artificialmente quanto dados reais, seção 4 descreve os índices aplicados para avaliar o desempenho de um algoritmo quando aplicado a um conjunto de dados.

Nesta seção, os parâmetros utilizados nos algoritmos KM, FCM, FCMA I e FCMA para tanto conjuntos de dados artificiais quanto reais são descritos.

Os algoritmos anteriores que têm o parâmetro m, FCM, FCMA I e FCMA, o valor deste foi sempre especificado como que é o valor usualmente fornecido para este parâmetro na literatura.

O número máximo de iterações para os experimentos com conjuntos de dados artificiais foi especificado como 150 e para os conjuntos de dados reais foi especificado como 300.

O parâmetro utilizado pelos algoritmos para verificar se o estado de convergência foi atingido foi definido em todos os experimentos como 10 8.

Todos os algoritmos utilizados têm a dependência de uma etapa de inicialização.

Esta etapa consiste para o KM em definir os protótipos inicias e para o FCM, FCMA I e FCMA em definir uma matriz de pertinência inicial.

Esta etapa de inicialização tem recebido atenção como área de pesquisa.

Três grandes famílias de métodos de inicialização podem ser citadas amostragem randômica (random sampling methods), otimização de distância (distance optimization methods) e estimativa de densidade (density estimation methods).

Para todos os algoritmos utilizados neste trabalho a implementação da etapa de inicialização foi feita à semelhança de uma amostragem randômica.

Para o KM a inicialização dos protótipos é feita escolhendo-se pelo menos um indivíduo do conjunto de dados para ser usado no cálculo dos protótipos dos clusters, após esta fase que garante que cada cluster não começará vazio, os indivíduos restantes no conjunto de dados são divididos aleatoriamente para serem usados no cálculo dos protótipos dos clusters.

Para o FCM, FCMA I e FCMA o método de inicialização utilizado foi o mesmo.

Busca-se inicialmente garantir que nem um cluster fique vazio.

Isto é feito escolhendo aleatoriamente no conjunto de dados um indivíduo para pertencer com mais alto grau de pertinência a um cluster específico.

Garantida esta configuração para todos os clusters o algoritmo de inicialização prossegue, aleatoriamente alocando os demais indivíduos do conjunto de dados nos clusters, neste caso alocando significa dado um indivíduo aleatoriamente definir pertinência para este em todos os clusters existentes.

Nas subseções que se seguem são apresentados os conjuntos de dados artificiais e reais utilizados para avaliar os algoritmos em situações com diferentes graus de dificuldade.

Todos os algoritmos foram aplicados nesses conjuntos de dados, medidas foram colhidas para que uma avaliação comparativa posterior fosse feita.

Nesta etapa, foram simulados conjuntos de dados quantitativos no com diferentes propriedades estatísticas.

Cada classe do conjunto de dados, foi gerada de acordo com uma distribuição normal bi-variada usando um algoritmo que permite definir para cada classe o valor de correlação entre as duas variáveis, o vetor de médias e a matriz de covariâncias da seguinte forma, Nas subseções 4311 e 431os dados artificiais descritos são gerados conforme descrito acima.

Para avaliar cada algoritmo em relação aos dados artificiais que se seguem e ainda comparar os resultados obtidos pelos diferentes algoritmos uma experiência Monte Carlo foi executada da maneira especificada pelo algoritmo 6, descrito a seguir.

Nesta seção são descritos quatro conjuntos de dados artificiais todos contendo quatro classes.

Estas quatro configurações apresentam dificuldades que vêm sendo utilizadas na literatura para avaliar algoritmos de clusterização como presença menos ou mais intensa de sobreposição entre as classes e classes de diferentes tamanhos.

São definidos códigos para fazer referência aos conjuntos de dados descritos na presente seção assim como é dada a descrição de cada conjunto de dados.

São dados os valores assumidos pelos parâmetros necessários para gerar os conjuntos de dados descritos nesta seção.

Note que para gerar conjuntos de dados cujas classes têm matriz de covariância diagonal, o parâmetro foi definido como zero.

Para gerar conjuntos de dados cujas classes têm matriz de covariância diferentes, os parâmetros e de cada classe foram manipulados de forma a garantir que o valor de de uma dada classe não é igual ou próximo do de nenhuma outra classe, o mesmo foi garantido para o valor de de cada classe em relação às demais.

Descrição sumária dos conjuntos de dados artificiais contendo quatro classes Código do conjunto de dados.

Valores dos Parâmetros utilizados para gerar os quatro conjuntos de dados artificiais contendo quatro classes.

Para cada um dos conjuntos de dados descritos anteriormente o algoritmo 6 é executado, devendo gerar 60 diferentes replicações dos conjuntos de dados.

Em cada replicação o algoritmo de clusterização escolhido é executado 60 vezes sobre o conjunto de dados replicado até a convergência ou o limite máximo de iterações ser atingido.

Ao final das 60 execuções do algoritmo de clusterização, a partição obtida para a qual o valor da função objetivo foi o menor é utilizada para calcular os índices que se deseja coletar.

Conjunto de dados quantitativos 1 mostrando classes que possuem matrizes de covariância diagonal e similares.

Conjunto de dados quantitativos mostrando classes que possuem matrizes de covariância diagonal e diferentes.

Conjunto de dados quantitativos mostrando classes que possuem matrizes de covariância não diagonal e similares.

Conjunto de dados quantitativos mostrando classes que possuem matrizes de covariância não diagonal e diferentes.

O objetivo dos experimentos que se seguem foi analisar a influência dos parâmetros, forma, separação entre as classes, distribuição espacial e tamanho.

Tentou-se fazer o estudo o mais exaustivo possível para ser possível analisar a sensibilidade dos algoritmos a variações nos parâmetros mencionados.

Ao contrário dos experimentos artificiais anteriores, ao longo dos experimentos não foram manipulados os parâmetros similaridade entre as matrizes de covariância e matriz diagonal ou não.

No entanto, para permitir estabelecer uma comparação entre estes experimentos é mostrada a relação entre as matrizes de covariância das duas classes em cada configuração.

Note que todas as matrizes de covariância nos casos que seguem são diagonais.

Os parâmetros citados assumem os seguintes valores, forma elíptica, circular, separação entre as classes bem separada, sobreposta, distribuição espacial esparsa, compacta, tamanho diferente.

Para manipular o parâmetro forma define-se o valor de igual ao valor de caso se deseje uma classe de forma circular, para obter uma classe elíptica basta definir diferente de O parâmetro separação entre as classes pode ser manipulado definindo-se valores para µ1 e µ de uma classe próximos dos mesmos valores de outra classe.

Neste caso quanto mais próximos esses valores das duas classes mais estas serão sobrepostas.

Quanto mais distantes os valores, mais as classes serão bem separadas.

A distribuição espacial pode ser manipulada usando a fórmula densidade.

Nas configurações apresentadas, calculou-se a densidade de cada classe, conforme mostrado no início do parágrafo, procurou-se nas configurações esparsas garantir que a razão da densidade da classe 1, DC, sobre a densidade da classe 2, DC, fosse de no máximo ordem 2, 1 2.

Nas configurações compactas, a densidade das classes foi definida. São definidos códigos para fazer referência aos conjuntos de dados descritos na presente seção assim como é dada a descrição de cada conjunto de dados.

Descrição sumária dos conjuntos de dados artificiais contendo duas classes.

Para cada um dos conjuntos de dados descritos na algoritmo 6 é executado, devendo gerar 60 diferentes replicações dos conjuntos de dados.

Em cada replicação o algoritmo de clusterização escolhido é executado 60 vezes sobre o conjunto de dados replicado até a convergência ou o limite máximo de iterações ser atingido.

Ao final das 60 execuções do algoritmo de clusterização, a partição obtida para a qual o valor da função objetivo foi o menor é utilizada para calcular os índices que se deseja coletar.

Nos oito conjuntos de dados que se seguem o parâmetro forma foi fixado como sendo elíptica e os demais parâmetros foram variados.

São dados os valores assumidos pelos parâmetros para gerar os oito conjuntos de dados elípticos descritos nesta seção.

Valores dos Parâmetros utilizados para gerar os oito conjuntos de dados artificiais contendo duas classes de forma elíptica fixado como sendo circular e os demais parâmetros foram variados.

São dados os valores assumidos pelos parâmetros para gerar os oito conjuntos de dados esféricos descritos nesta seção.

Valores dos Parâmetros utilizados para gerar os oito conjuntos de dados artificiais contendo duas classes de forma circular fixado para uma das classes como sendo elíptica e para outra como sendo circular e os demais parâmetros foram variados.

São dados os valores assumidos pelos parâmetros para gerar os oito conjuntos de dados contendo cada uma classe de forma circular e outra classe de forma elíptica descritos nesta seção.

Valores dos Parâmetros utilizados para gerar os oito conjuntos de dados artificiais contendo uma classe de forma circular e outra de forma elíptica.

Oito diferentes conjuntos de dados quantitativos contento cada uma classe de forma elíptica e uma classe de forma circular.

Para fazer uma avaliação da aplicabilidade prática dos algoritmos, escolheu-se seis conjuntos de dados quantitativos que vêm sendo utilizados na literatura para aplicar aos mesmos.

Foram eles, Hipotireóide, Imagens Segmentadas, Íris, Pima Diabetes, Vidro, Vinho.

Todos os conjuntos de dados mencionados estão disponíveis no repositório UCI.

Todos esses conjuntos de dados possuem um atributo que identifica a que classe cada indivíduo pertence, um rótulo, mas este atributo não foi utilizado pelos algoritmos de clusterização.

Para cada um dos conjuntos de dados reais selecionados os algoritmos de clusterização foram executados 60 vezes até a convergência ou o limite máximo de iterações ser atingido.

Ao final das 60 execuções de cada algoritmo de clusterização, a partição obtida para a qual o valor da função objetivo foi o menor é utilizada para calcular os índices que se deseja coletar.

Este conjunto de dados consiste no resultado de cinco exames laboratoriais para cada indivíduo do conjunto.

Há ainda um atributo que corresponde a classe de cada indivíduo, este atributo assume um dos seguintes valores, euthyroidism, hypothyroidism ou hyperthyroidism.

A definição do valor do atributo de classe, isto é, o diagnóstico de cada indivíduo foi baseada em registros médicos.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 215.

Distribuição dos indivíduos nas classes, Classe 1, (normal) 150.

Classe 2, (hiper) 35.

Classe 3, (hipo) 30.

Número de atributos, 5 atributos quantitativos e mais um atributo que informa a classe.

Informação sobre os atributos, Tresin uptake test, em percentual.

Total Serum thyroxin, conforme medido pelo "isotopic displacement method".

Total serum trodothyronine, conforme medido pelo "radioimmuno assay".

Basal thyroid-stimulating hormone (TSH), conforme medido pelo "radioimmuno assay".

Diferença absoluta máxima de valor de TSH após injeção de 200 micro gramas de "thyrotropin-releasing hormone"quando comparada ao valor base.

O conjunto de dados de imagens segmentadas foi formado a partir de imagens que foram selecionadas randomicamente de um banco de dados de sete imagens ao ar livre.

As imagens foram segmentadas a mão para criar as sete classes, céu, semente, janela, tijolo, grama, folhagem e estrada.

Cada classe tem 330 indivíduos e cada indivíduo é caracterizado por 17 atributos de valor real.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 2310, sendo 330 em cada classe.

Número de classes, 7.

Número de atributos, 17 atributos quantitativos mais um atributo que informa a classe do indivíduo.

Informação sobre os atributos, region-centroid-col, a coluna do pixel central da região, region-centroid-row, a linha do pixel central da região, region-pixel-count, número de pixels numa região, vedge-mean, Média da medida do contraste de pixels horizontalmente adjacentes na região.

Este atributo é usado como um detector de borda vertical, vegde-sd, desvio padrão da medida explicada para o atributo anterior, hedge-mean, Média da medida do contraste de pixels verticalmente adjacentes na região.

Este atributo é usado como um detector de linha horizontal, hedge-sd, Desvio padrão da medida explicada para o atributo anterior, intensity-mean, Média do valor de cada pixel da região, rawred-mean, Média do valor da banda R de cada pixel da região, rawblue-mean.

Média do valor da banda B de cada pixel da região, rawgreen-mean, Média do valor da banda G de cada pixel da região, 1exred-mean, Média da medida do excesso de vermelho para cada pixel da região, O excesso de vermelho para um pixel é calculado, 1exblue-mean.

Média da medida do excesso de azul para cada pixel da região, O excesso de azul para um pixel é calculado, 1exgreen-mean, Média da medida do excesso de verde para cada pixel da região, O excesso de verde para um pixel é calculado, 1value-mean, transformação não linear d dos pixels RGB.

A partir dos novos valores calculados para cada pixel computa-se o valor médio para a região da intensidade dos pixels, 1saturatoin-mean, Média da saturação dos pixels da região após a transformação não linear d, 1hue-mean, Média do parâmetro "hue"dos pixels da região após a transformação não linear d.

Este é talvez o mais conhecido conjunto de dados encontrado na literatura de reconhecimento de padrões.

O conjunto de dados contém classes cada uma com 50 indivíduos, cada classe refere-se a um tipo de planta íris, são elas Íris Setosa, Íris Versicolour e Íris Virginica.

Uma classe é linearmente separável das outras duas.

Essas duas outras classes restantes não são linearmente separáveis uma da outra, isto é, elas têm sobreposição entre si.

Os atributos sepal length e sepal width contém ruído.

Este é considerado um conjunto de dados fácil.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 150, sendo 50 em cada classe.

Número de atributos, atributos quantitativos e mais um atributo que indica a classe do indivíduo.

Informação sobre os atributos, sepal length em centímetros, sepal width em centímetros, petal length em centímetros, petal width em centímetros.

Este conjunto de dados foi construído a partir da aplicação de várias restrições de seleção de indivíduos em um grande banco de dados.

Todos os indivíduos que constituem este conjunto de dados são pacientes do sexo feminino, com no mínimo 21 anos de idade e de origem étnica Pima-Indiana.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 768 Distribuição dos indivíduos nas classes, Classe 1, 500, indivíduo não diabético.

Classe 2, 268, indivíduo diabético.

Número de atributos, 8 atributos quantitativos mais atributo de classe.

Informação sobre os atributos, 1.

Número de vezes grávida.

Concentração de glucose no plasma.


Pressão sanguínea diastólica medido em mm Hg.

Triceps skin fold thickness medido em mm.

Hour serum insulin medido em mu U/ml.

Índice de massa corporal, (peso em Kg) / (altura em m) ao quadrado.

Diabetes pedigree function.Idade.

Este conjunto de dados foi criado no Central Research Establishment, Home Office Forensic Science Service Reading, Berkshire.

O conjunto de dados é formado por 21indivíduos divididos em 6 classes.

Cada indivíduo é caracterizado por um número id, nove medidas de propriedade/composição química e um número de classe que varia de 1 até 7.

Número de classe é reservado não aparecendo neste conjunto de dados.

O atributo id não foi utilizado pelos algoritmos de clusterização.

Este é um conjunto de dados considerado difícil para classificação, pois algumas classes têm poucos indivíduos, como a classe que tem apenas 1indivíduos e a classe 5 que tem apenas 9 indivíduos sendo assim eventos raros.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 214.

Distribuição dos indivíduos nas classes.
Classe 1, 70,janela de prédio processad.
Classe 2, 17,janela de veículo processad.
Classe 3, 76,janela de prédio não processad.
Classe 4, 13,(container).
Classe 5, 9,mes.
Classe 6, 29,farol de carro.

Número de atributos, 9 atributos quantitativos mais um atributo que informa a classe do indivíduo.

Informação sobre os atributos, RI, índice de refração.

Na, Sódio, medido como percentual de massa no correspondente óxido.

Mg, Magnésio, medido como atributo 3.

Al, Alumínio, medido como atributo 3.

Si, Silício, medido como atributo 3.

K, Potássio, medido como atributo 3.

Ca, Cálcio, medido como atributo 3.

Ba, Bário, medido como atributo 3.

Fe, Ferro, medido como atributo 3.

Este conjunto de dados consiste de três tipos de vinhos cultivados na mesma região da Itália, mas vindos de diferentes cultivares.

Cada indivíduo, vinho, é caracterizado por 1atributos representando a quantidade de constituintes encontrados em cada um dos três tipos de vinhos.

A seguir um resumo das principais informações sobre este conjunto de dados, Número de indivíduos, 178 Distribuição dos indivíduos nas classes, Classe 1, 59.

Classe 2, 71.

Classe 3, 48.

Número de atributos, 1atributos quantitativos mais atributo que informa a classe.

Informação sobre os atributos, Clustering é um processo não supervisionado, isto é, não há classes predefinidas e nem exemplos que poderiam mostrar que tipo de relação se desejaria para ser válida entre os indivíduos do conjunto de dados.

Como conseqüência, a partição final de um conjunto de dados deve ser avaliada em muitas aplicações.

Por exemplo, questões como, "Quantos clusters existem em um conjunto de dados?

A partição resultante é adequada ao conjunto de dados?

Há uma partição melhor para o conjunto de dados?

Demandam por métodos de validação para clustering.

O objetivo de vários métodos presentes na literatura é fornecer uma avaliação quantitativa do resultado de um algoritmo de clusterização, estes métodos são conhecidos sobre o termo geral de métodos de validação de clustering.

Apesar da importância da validação de clustering, esta é raramente utilizada em aplicações de análise de cluster.

As razões para isto incluem a falta de instruções sobre como validação de clustering deve ser feita e a necessidade de muitos recursos computacionais.

Nesta seção dois índices são descritos, um baseado em índice externo e outro em índice interno.

A partir desses os resultados obtidos por cada algoritmo poderão ser comparados.

Índices externos são usados para avaliar o grau de compatibilidade entre duas partições U e V, onde a partição U é o resultado de um algoritmo de clusterização e a partição V é resultado de informação a priori sendo independente da partição U, a partição V pode ser algo como um rótulo de categoria ou uma informação de classe.

Há muitos índices externos definidos na literatura, tal como Hubbert, Jaccard, Rand e Corrected Rand.

Uma característica de muitos desses índices é que eles são sensíveis tanto ao número de classes na partição ou a distribuição dos elementos nos clusters.

Por exemplo, alguns índices tem a tendência de apresentar maiores valores para partições que possuem um número grande de classes, enquanto outros apresentam maiores valores para partições que possuem número pequeno de classes.

O índice de Rand corrigido (CR), que têm seus valores corrigidos contra acertos aleatórios, não tem nenhuma dessas indesejáveis características.

Assim, CR é por esta razão o índice de validação externa utilizado no presente trabalho.

No trabalho Hubert, 1985, os autores definiram índices através da comparação de duas partições.

Quando duas partições são independentes, no sentido que uma depende dos valores no conjunto de dados e a outra não, a distribuição de alguns desses índices pode ser estabelecida a partir de uma perspectiva teórica.

Todos esses tipos de índices são derivados a partir de uma tabela construída a partir das duas partições em questão.

As duas partições de n indivíduos ou objetos são denotadas U (obtida pelo algoritmo de clusterização) e V (obtida a partir de conhecimento a priori).

Comparação entre duas partições.

O índice CR foi utilizado neste trabalho para medir a similaridade entre uma partição a priori do tipo hard e uma partição do tipo hard fornecida pelos algoritmos em análise.

A partir de uma partição fuzzy fornecida pelos algoritmos analisados FCM, FCMA I e FCMA, uma partição do tipo hard pode ser obtida.

Tal partição do tipo hard foi obtida no presente caso tomando as classes como sendo duas partições do tipo hard do mesmo conjunto de dados tendo respectivamente.

O índice de Rand corrigido (CR) é definido como segue, Uma partição obtida a partir de um algoritmo de clusterização fuzzy apresenta uma propriedade chamada fuzziness.

Tal propriedade consiste no grau de pertinência de cada indivíduo aos clusters.

Quando cada indivíduo tem igual pertinência em todos os clusters, isto é, quando a pertinência desses indivíduos em cada cluster é igual a, onde c é o número total de clusters, tem-se o que se chama de completa fuzziness.

Em outra direção, quando ocorre que cada indivíduo tem pertinência igual a 1 em algum cluster, tem-se o que se chama de partição completamente hard.

O Coeficiente de Partição de Dunn mede o quão hard é uma partição fornecida por um algoritmo de clusterização fuzzy.

Ele é definido da seguinte forma, DC varia de 1 (partição har até 0 (partição totalmente fuzzy), independente do número de clusters.

Neste capítulo, foram apresentados os dados utilizados para avaliar os algoritmos KM, FCM, FCMA I e FCMA.

Para comparar os resultados de cada algoritmo nos conjuntos de dados foram escolhidos dois índices.

Um índice externo, o CR, Índice de Han corrigido e um índice interno, o coeficiente de partição normalizado de dunn D.

Este capítulo apresenta os resultados da execução dos algoritmos, K-means (KM), para detalhes ver seção 2521, Fuzzy c-means (FCM), para detalhes ver seção 2531, Fuzzy c-means com Distância Adaptativa por Atributo (FCMA I), para detalhes ver seção 3e Fuzzy c-means com Distância Adaptativa por Classe e por Atributo (FCMA).

Os índices CR e DC foram utilizados para avaliar o desempenho desses algoritmos, como DC é calculado a partir de uma partição fuzzy para o algoritmo KM não é possível calculá-lo.

Nas subseções que seguem os resultados de acordo com os índices CR e DC para os quatro algoritmos estudados são exibidos.

Testes estatísticos foram utilizados para permitir uma interpretação acurada dos resultados.

Nove testes independentes t-Student com nível de significância de 5% foram realizados para cada configuração de dados artificiais.

Em cada teste, considerou-se que as observações são independentes entre si, já que uma base de dados artificiais para cada execução de cada algoritmo foi gerada, desta forma se utilizou o cálculo dos graus de liberdade e os procedimentos de teste de hipótese para diferença de médias para observações independentes do processo ou população.

Para fazer referência aos conjuntos de dados são utilizados os códigos.

Apresenta os valores médios e o desvio padrão para o índice CR para os quatro algoritmos em estudo.

Apresenta os valores médios e o desvio padrão para o índice DC para os algoritmos em estudo com exceção do KM.

Aparece aqui para ilustrar a diferença de complexidade computacional dos quatro algoritmos.

Pode ser observado que o KM é o que possui menor custo computacional de todos, enquanto os três algoritmos de clusterização fuzzy apresentaram em média praticamente seis vezes mais iterações que o KM.

De acordo com os testes que comparam as médias do CR o algoritmo FCMA se sobressaiu em relação aos demais nos conjuntos de dados que possuem matrizes de covariância diferentes para cada classe.

Isto corrobora com o modelo de distância utilizado por este algoritmo, isto é, a presunção de uma distância para cada classe e cada atributo desta.

De acordo com o CR, também pode-se observar que o FCMA I tem um melhor desempenho nas configurações 1 e que possuem classes com matriz de covariância semelhantes, que é o modelo de distância adotado pelo FCMA I.

Embora para as configurações 1 e o valor médio do CR para FCMA I tenha sido maior que o do FCMA, estatisticamente os desempenhos dos dois algoritmos foi igual.

Os algoritmos que não utilizam distâncias adaptativas em todos os casos apresentaram desempenhos inferiores aos do FCMA I e FCMA.

O índice DC, cujos valores médios são mostrados e comparados, permite concluir que o algoritmo FCMA é o que faz alocações mais precisas pois apresenta partições mais próximas de uma partição hard e como visto pelos valores de CR apresentados por este algoritmo as alocações feitas são também em médias mais corretas do que as alocações dos outros algoritmos.

O FCMA I destaca-se em relação ao FCM somente nas configurações 1 e pois como visto esta configuração é favorável ao modelo de distância que o FCMA I utiliza.

Valores de CR para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes.

Valores de DC para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes.

Número médio de iterações para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes.

Resultados para os testes de 1 a.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes.

Resultados para os testes de a.

Valores de teste t-Student para comparar os valores médio de DC para os algoritmos analisados para os conjunto de dados artificiais contendo quatro classes.

A seguir são apresentados os resultados para as configurações de dados artificiais com apenas duas classes e variação de parâmetros como forma, diferença do número de indivíduos entre as classes, distribuição espacial, separação linear.

Podem ser visto os valores médios e os desvios padrões para o CR e DC obtidos pelos quatro algoritmos em estudo nas oito configurações de dados artificiais contendo duas classes de forma elíptica.

Pode-se concluir que na maior parte dos conjuntos de dados os algoritmos FCM e KM foram superados pelos algoritmos com distâncias adaptativas.

Os algoritmos FCMA I e FCMA tiveram desempenho similar na maior parte dos casos, a hipótese alternativa de que FCMA I teria uma média de CR maior que a média de CR de FCMA foi aceita apenas em dois dos casos.

Importante notar que para os conjuntos de dados 11 e 1os testes mostraram que os algoritmos baseados em distâncias adaptativas não se sobressaíram e o algoritmo com melhor desempenho foi o KM, o que serve como indício de que para conjuntos sobrepostos com uma classe compacta o KM possa ter um desempenho superior.

O índice DC permite concluir que o algoritmo FCM para os conjuntos com sobreposição não tem um bom desempenho pois para este tipo de configuração este apresenta partições mais hard que FCMA I e FCMA mas no entanto como revela os valores de CR obtidos pelo FCM para as mesmas configurações esta alocação feita com maior tendência a um certo cluster não corresponde a uma alocação correta.

Valores de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma elíptica.

Valores de DC para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma elíptica.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma elíptica.

Resultado para os testes de 1 a 3.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma elíptica.

Resultado para os testes de a 6.

Valores de teste t-Student para comparar os valores médio de DC para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma elíptica.

Podem ser visto os valores médios e os desvios padrões para o CR e DC obtidos pelos quatro algoritmos em estudo nas oito configurações de dados artificiais contendo duas classes de forma circular.

Os conjuntos de dados 1e 1não apresentaram dificuldade alguma para todos os algoritmos, a utilidade desta conclusão é que tais configurações servem para testar um algoritmo novo num caso que se espera dele não menos que total sucesso.

Testes t-Student para os valores médios dos índices CR e DC respectivamente.

Para os conjuntos de dados 1e 1os testes para a média do CR não foram possíveis de serem feitos devido não haver diferença, as médias dos quatro algoritmos foram iguais a 1 e os desvios padrões 0.

Os testes sobre os valores médios do CR permitem concluir que os quatro algoritmos não apresentaram diferença significativa de desempenho para este conjunto todo de configurações, dentre todos os testes para a igualdade mostrados apenas duas vezes a hipótese nula foi rejeitada.

Os mesmos testes ainda revelam que, entretanto, para os casos de conjunto de dados com sobreposição os algoritmos sem distância adaptativa se saem melhor que o FCMA I e FCMA.

Estas configurações são o caso mais favorável ao FCM e KM o que explica a ligeira melhor performance destes, porém os algoritmos FCMA I e FCMA que não favorecem clusters com forma circular não apresentaram um resultado relevantemente inferior.

Esta conclusão é relevante pois as versões adaptativas são por hipótese melhores em configurações que possuem formas diversas, os resultados vistos nesta seção mostram que mesmo para conjuntos de dados com classes circulares os algoritmos FCMA I e FCMA não têm desempenho médio muito inferior aos algoritmos que favorecem explicitamente formas circulares.

Valores de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma circular.

Valores de DC para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma circular.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma circular.

Resultado para os testes de 1 a 3.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma circular.

Resultado para os testes de a 6.

Valores de teste t-Student para comparar os valores médio de DC para os algoritmos analisados para os conjunto de dados artificiais contendo duas classes de forma circular.

Podem ser visto os valores médios e os desvios padrões para o CR e DC obtidos pelos quatro algoritmos em estudo nas oito configurações de dados artificiais contendo uma classe de forma elíptica e outra circular.

Testes t-Student para os valores médios dos índices CR e DC respectivamente.

Os testes para a igualdade de média feitos para o CR permitem concluir que para muitas das configurações mostradas os algoritmos tiveram resultados semelhantes.

Em algumas configurações como as 21 e 26 os algoritmos adaptativos FCMA I e FCMA se destacam e em outras como as 25 e 28 um algoritmo não adaptativo se destaca.

A conclusão é que configurações com uma classe circular favorecem os algoritmos não adaptativos cujas distâncias favorecem a detecção de cluster de forma circular.

Como nas configurações onde todas as classes eram circulares, vale destacar que os algoritmos adaptativos apesar de não favorecem a forma circular quando apresentam um desempenho inferior aos algoritmos não adaptativos a diferença de desempenho não é de ordem de grandeza tão grande quanto a diferença entre os algoritmos adaptativos e os não adaptativos para configurações com somente classes elípticas.

Valores de CR para os algoritmos analisados para os conjunto de dados artificiais contendo uma classe de forma elíptica e outra de forma circular.

Valores de DC para os algoritmos analisados para os conjunto de dados artificiais contendo uma classe de forma elíptica e outra de forma circular.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo uma classe de forma elíptica e outra de forma circular.

Resultado para os testes de 1 a 3.

Valores de teste t-Student para comparar os valores médio de CR para os algoritmos analisados para os conjunto de dados artificiais contendo uma classe de forma elíptica e outra de forma circular.

Resultado para os testes de a 6.

Valores de teste t-Student para comparar os valores médio de DC para os algoritmos analisados para os conjunto de dados artificiais contendo uma classe de forma elíptica e outra de forma circular.

Nas subseções a seguir os resultados obtidos de acordo com os índices CR e DC pelos algoritmos estudados neste trabalho são exibidos.

Em cada caso uma breve análise dos resultados é feita.

Este tipo de tabela tem a função de fornecer uma comparação entre a partição obtida pelo algoritmo e a partição a priori.

A principal informação que pode ser obtida por esse tipo de tabela é quantos indivíduos foram alocados indevidamente pelo algoritmo.

Para cada conjunto de dados reais um gráfico como o gráfico 51 é mostrado.

Este tipo de gráfico mostra como cada algoritmo utilizado distribuiu os indivíduos de uma classe ao longo dos clusters formados.

Espera-se que um algoritmo seja capaz de identificar todos os indivíduos que pertencem a mesma classe e os aloquem ao mesmo cluster, tal comportamento pode ser observado no gráfico 5para o conjunto de dados íris.

Neste gráfico, todos os quatro algoritmos conseguiram isolar todos os indivíduos da classe Setosa em um único cluster.

Esta tabela mostra a partição da base de dados Hipotireóide obtida pelo algoritmo e a partição a priori.

Em cada uma das tabelas desta forma a partição obtida pelo algoritmo corresponde às linhas e é denota P01 a P0 K, onde K é o número de classes.

A partição a priori corresponde às colunas sendo denotada C01 a C0 K.

Em cada linha destacou-se o elemento que foi considerado como o componente "dominante"na linha em questão.

A idéia é destacar a classe que está mais presente em cada uma das partições obtidas pelo algoritmo.

Este elemento em destaque é composto de dois números separados por uma barra.

No exemplo, o elemento em destaque para a linha da partição P01 é o 150/1, o número à esquerda da barra, 150, é o número de indivíduos da partição a priori C01 que estão em P01, o número à direita da barra, 1, representa a fração da partição a priori C01 contida na partição P01, neste caso C01 está totalmente contida na partição P01.

Pode-se analisar a distribuição dos indivíduos de uma classe a priori nas partições formadas pelos algoritmos.

Além disso para cada classe é mostrado lado a lado como cada algoritmo efetuou a distribuição dos indivíduos desta ao longo das partições que ele formou.

Valores do CR e do DC obtidos pelos quatro algoritmos sendo analisados.

Os algoritmos com distâncias adaptativos FCMA I e FCMA se sobressaíram neste caso tanto de acordo com o índice CR pois os valores maiores obtidos neste índice indicam que estes algoritmos foram capazes de identificar mais indivíduos corretamente.

Por outro lado, o índice DC também pode ser interpretado favoravelmente aos algoritmos com distâncias adaptativas pois como estes obtiveram valores nesse índice mais próximos de 1 que os algoritmos FCM e KM isto significa que a partição gerada pelos algoritmos FCMA I e FCMA foi mais próxima de uma partição hard do que a identificada pelo FCM.

Ainda outra forma de interpretar este resultado é que os algoritmos com distâncias adaptativas identificam corretamente os indivíduos e quando o fazem têm "certeza"da identificação.

Constituição de cada uma das três partições formadas pelos algoritmos FCMA I, FCMA, FCM e KM respectivamente.

Valores de CR e DC para os algoritmos analisados para o conjunto de dados Hipotireóide.

Hipotireóide,Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Hipotireóide,Comparação entre a partição obtida pelo algoritmo FCMA e a partição a priori.

Hipotireóide,Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Valores do CR e do DC obtidos pelos algoritmos sendo analisados.

Hipotireóide, Comparação entre a partição obtida pelo algoritmo KM e a partição a priori.

Hipotireóide, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos.

Os algoritmos FCMA e KM apresentaram erros numéricos ao serem aplicados a este conjunto de dados.

Constituição de cada uma das sete partições formadas por cada algoritmo.

Valores de CR e DC para os algoritmos analisados para o conjunto de dados Imagens Segmentadas.

Valores do CR e do DC obtidos pelos quatro algoritmos sendo analisados.

O resultado para este conjunto de dados lembra ainda o resultado mostrado nas seções 531 e 53mas com a ressalva de que no caso presente todos os algoritmos tiveram um índice elevado de acertos.

Este fato pode ter como razão o conjunto de dados Íris ser um conjunto considerado fácil, uma classe separável linearmente das demais, todas as classes têm o mesmo número de indivíduos.

Imagens Segmentadas, Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Imagens Segmentadas,Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Constituição de cada uma das três partições formadas pelos algoritmos FCMA I, FCMA, FCM e KM respectivamente.

Valores de CR e DC para os algoritmos analisados para o conjunto de dados Íris.

Íris,Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Imagens Segmentadas, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos.

Íris, Comparação entre a partição obtida pelo algoritmo FCMA e a partição a priori.

Íris, Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Íris, Comparação entre a partição obtida pelo algoritmo KM e a partição a priori.

Valores do CR e do DC obtidos pelos quatro algoritmos sendo analisados.

Ao contrário do que ocorreu nos Íris, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos resultados reportados nas seções 531, 53e 533, não há evidência suficiente para dizer que qualquer dos quatro algoritmos obteve de fato um resultado superior em relação aos demais.

Ainda que o algoritmo FCMA I tenha obtido o maior valor de CR, todos os valores de CR obtidos são tão próximos de zero que poderia atribuir-se as diferenças ao acaso na fase de inicialização, por exemplo.

Constituição de cada uma das duas partições formadas pelos algoritmos FCMA I, FCMA, FCM e KM respectivamente.

Valores de CR e DC para os algoritmos analisados para o conjunto de dados Pima Diabetes.

Pima Diabetes, Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Valores do CR e do DC obtidos pelos quatro algoritmos sendo analisados.

Para este conjunto de dados, os algoritmos de clusterização com distância adaptativa obtiveram um resultado pior que os Pima Diabetes, Tabela de Comparação entre a partição obtida pelo algoritmo FCMA e a partição a priori.

Pima Diabetes, Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Pima Diabetes, Comparação entre a partição obtida pelo algoritmo KM e a partição a priori.

Para o algoritmo FCMA o problema foi mais grave pois este falhou em chegar a uma clusterização e assim não foi possível calcular CR ou DC.

A razão é que a forma que FCMA calcula os protótipos, os pesos e a pertinência o torna muito sensível a situações em que um atributo do conjunto de dados apresenta valor constante ou os valores que o atributo assume para todos os indivíduos é muito próximo (varia numa escala pequena).
Em situações como essa o somatório presente no denominador do cálculo dos pesos tende a zero pois o protótipo para esse tipo de atributo assume um valor muito próximo ao valor que o atributo assume para cada indivíduo.

Ocorrendo este fato, há um erro de divisão por zero e o algoritmo falha em obter uma partição do conjunto de dados.

Valores de CR e DC para os algoritmos analisados para o conjunto de dados Vidro.

Pima Diabetes, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos.

Vidro, Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Vidro, Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Para este conjunto de dados os algoritmos que utilizam distância adaptativa claramente tiveram performance superior ao FCM e KM.

Note-se que FCM e KM tiveram resultados praticamente idênticos e ambos muito baixos configurando o que na literatura se considera mero acaso.

Ainda digno de menção é a diferença de valores para o índice DC obtidos pelos algoritmos.

Vidro, Comparação entre a partição obtida pelo algoritmo KM e a partição a priori.

Vidro, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos.

FCMA I e FCMA e FCM.

O valor de DC obtido pelo FCM foi quase o dobro dos valores obtidos pelos outros dois, o que de acordo com a semântica do índice DC significa que ele obteve uma partição mais próxima da partição hard que os outros algoritmos.

Na prática isto significa que o FCM atribui a cada indivíduo altos valores de pertinência desse indivíduo num só cluster, no entanto o valor obtido pelo mesmo FCM para o índice CR mostra que esta decisão foi de longe correta dado o baixo valor que CR assumiu.

Na direção contrária, os algoritmos FCMA I e FCMA tiveram valores de DC que indicam que a partição que eles obtiveram foi próxima de uma partição totalmente fuzzy, ou seja, embora não tenham tendido para alocar os indivíduos em nenhum cluster em particular esses algoritmos foram capazes de atribuir maiores valores de pertinência dos indivíduos aos clusters adequados como se comprova pelos acertos obtidos conforme acenado pelo CR.

As tabelas 545, 546, 547 e 548 mostram cada uma a constituição de cada uma das duas partições formadas pelos algoritmos FCMA I, FCM e KM respectivamente.



Valores de CR e DC para os algoritmos analisados para o conjunto de dados Vinho.

Vinho, Comparação entre a partição obtida pelo algoritmo FCMA I e a partição a priori.

Vinho, Comparação entre a partição obtida pelo algoritmo FCMA e a partição a priori.

Vinho, Comparação entre a partição obtida pelo algoritmo FCM e a partição a priori.

Vinho, Comparação entre a partição obtida pelo algoritmo KM e a partição a priori.

Neste capítulo, foram apresentados e discutidos os resultados para os conjuntos de dados artificiais e reais descritos no capítulo de experimentos desta dissertação.

Uma conclusão quanto aos dados artificiais é que os algoritmos com distâncias adaptativas se destacam em configurações com classes cujas formas não sejam dominantemente circulares.

Vinho, distribuição dos indivíduos representantes de cada classe nos clusters formados pelos algoritmos.

Distâncias adaptativas se saem melhor em casos de configuração com classes de forma circulares mas os algoritmos com distâncias adaptativas têm desempenho próximo nessas mesmas configurações.

Quanto aos conjuntos de dados reais para a maioria dos conjuntos analisados neste trabalho os algoritmos com distâncias adaptativas se sobressaíram.

Neste capítulo, são apresentadas as considerações finais relacionadas com esta dissertação, assim como algumas extensões que possam existir originadas do trabalho aqui realizado.

O contexto deste trabalho é o de análise de dados.

Todos os dias pessoas armazenam ou representam grandes quantidades de dados com o propósito de análise e manipulação posterior.

Dois meios importantes de lidar com esta massa de dados são através das atividades de classificação ou de clustering, isto é, classificar os dados em um conjunto finito de categorias ou formar clusters.

Para aprender algo novo ou entender um novo fenômeno, as pessoas sempre tentam procurar as características que descrevem este objeto ou fenômeno e em seguida compará-lo com outros objetos ou fenômenos conhecidos, baseado na similaridade ou dissimilaridade.

Deste fato decorre o papel de sistemas de classificação ou de clustering para a aquisição de conhecimento.

Foram analisados ao longo da presente dissertação algoritmos de classificação não supervisionada ou clustering.

Clustering é uma metodologia com aplicação em várias áreas como em análise de dados inteligente, mineração de dados, reconhecimento de padrões, processamento e segmentação de imagens, agentes inteligentes, recuperação de informação, classificação de documentos, ferramentas para auxílio de diagnóstico médico, mineração de regras de associação, análise de transações.

Os algoritmos KM e FCM são importantes representantes de duas famílias de algoritmos de clustering, hard e fuzzy clustering, respectivamente.

Este trabalho analisou a proposição de dois algoritmos do tipo fuzzy c-means baseado em distâncias adaptativas do tipo L Minkowsky.

Concluiu que o algoritmo analisado utilizando distâncias adaptativas supera o mesmo algoritmo com distâncias estáticas.

Vários conjuntos de dados artificiais com diferentes graus de dificuldade (num experimento Monte Carlo) e mais conjunto de dados reais significativos da literatura foram aplicados aos quatro algoritmos analisados KM, FCM, FCMA I e FCMA.

Para comparar os diferentes resultados obtidos dois índices foram aplicados, CR e DC.

Com base nesses resultados e mais o suporte estatístico de testes de hipótese chegou-se à conclusão que em média aos algoritmos baseados em distâncias adaptativas têm desempenho superior ao KM e FCM.

A seguir citam-se as contribuições deste trabalho, Fazer um estudo da inserção de dois tipos diferentes de distâncias adaptativas no algoritmo de clusterização FCM.

Comparar dois algoritmos de clustering amplamente utilizados KM e FCM com as versões do FCM com distâncias adaptativas FCMA I e FCMA.

Fazer um estudo do desempenho dos quatro algoritmos em diversas configurações artificiais revelando que algumas dessas configurações podem ser constituídas como benchmark para novos algoritmos de clusterização e/ou modificações propostas para algoritmos existentes.

Implementar um framework de software, o qual foi desenvolvido na linguagem de programação C++, primando pelo uso dos princípios da orientação a objetos, padrões de projeto e outras técnicas de engenharia de software.

Como conseqüência dessa iniciativa o framework mostra-se ser muito extensível e manutenível, como exemplo o desenvolvimento iniciou-se nos trabalhos e, a extensão para aplicar o framework às necessidades do trabalho atual foram feitas de maneira simples, em alguns casos a solução foi escrever uma nova classe apenas.

A estrutura do framework é bastante propícia ao reuso de código, isto torna a implementação de novas funcionalidades mais rápidas do que se uma aplicação tivesse de ser implementada desde o início.

Uma das características implementadas para este trabalho com possibilidade de reuso em trabalhos futuros foi o módulo de simulação, o qual permite executar vários algoritmos gerando automaticamente várias combinações de parâmetros necessários ao algoritmo.

O framework pode ser acessado.

A seguir são enumerados alguns dos trabalhos a serem feitos como forma de dar continuidade e/ou estender o trabalho relatado nesta dissertação, Os algoritmos FCMA I e FCMA estão restritos a dados quantitativos, muitas aplicações precisam de algoritmos que sejam capazes de manipular dados de natureza mais complexa como intervalos, texto, categorias, imagens.

Uma direção de trabalho futuro é estender esses algoritmos para outros tipos de dados.

Outra melhoria para aumentar a robustez do algoritmo seria dar um tratamento à conjuntos de dados que apresentam atributos ausentes para alguns dos indivíduos.

Foi estudado que para o KM a etapa de inicialização é muito importante para o resultado final.

Diferentes formas de inicialização têm sido estudadas para o KM com algum sucesso.

Assim é interessante estender o presente trabalho analisando o papel dessas formas de inicialização para o FCMA I e FCMA.

Algoritmos como KM e FCM que tentam minimizar um critério para conseguir clusterizar um conjunto de dados são muito dependentes da distância sendo utilizada.

No presente trabalho, utilizou-se apenas a distância Minkowski L, é interessante investigar versões do FCM, semelhantes às mostradas aqui, com outras distâncias da literatura como Minkowsky L e L tornando-as adaptativas.

A complexidade computacional dos algoritmos FCMA I e FCMA deve ser reduzida.

Em conjuntos de dados reais como abalone que possui cerca de 4177 indivíduos, descritos por 8 atributos não foi possível fazer os mesmos tipos de experimentos feitos para os conjuntos reais mostrados na seção 5devido a grande quantidade de tempo que os algoritmos necessitaram para executar um conjunto de execuções neste conjunto de dados.

Já existem trabalhos para reduzir a complexidade em termos de tempo do algoritmo FCM, tentar reproduzir estes esforços para FCMA I e FCMA é o caminho natural para torná-los mais rápidos.

Um framework do tipo validação cruzada (cross validation CV) é necessário para clustering para que se tenha idéia de desempenho médio, bias e um formato uniforme de comparar diferentes algoritmos.

Uma tentativa foi proposta em, mas de acordo com o trabalho pode se concluir que na verdade, no caso citado, foi feita a avaliação do quão acurado um classificador do tipo 1 neighborhood classifier foi obtido.

Avaliar o quão acurado é um classificador é conceitualmente diferente de avaliar o quão acurado é um algoritmo de clustering, muitos dos trabalho sobre CV dão bom suporte estatístico a conclusões baseados em classificadores.

Há trabalhos devotados a clustering mas que tentam avaliar a influência do parâmetro número de clusters que muitos dos algoritmos necessitam.

Desta forma é importante algoritmos e estudos que permitam conhecer a precisão média de algoritmos de clustering tomando como base não o classificador que pode ser induzido a partir dos protótipos, por exemplo, mas tomando como base o funcionamento geral de um algoritmo de clustering.

A partir de uma solução inicial o algoritmo KM busca por soluções que vão decrescendo iteração a iteração do algoritmo.

Para tanto a cada iteração é executado um passo de alocação e em seguida um de representação.

A função é decrescente.

A partir de uma solução inicial o algoritmo FCM com distâncias adaptativas (ou seja tanto FCMA I quanto FCMA) busca por soluções que vão decrescendo iteração a iteração do algoritmo.

Para tanto a cada iteração é executado um passo de representação e em seguida um de alocação.

O passo de representação é dividido agora em duas etapas, na primeira, atualiza-se os protótipos e na segunda, atualiza-se os pesos utilizados no cálculo das distâncias.

A função é decrescente.

