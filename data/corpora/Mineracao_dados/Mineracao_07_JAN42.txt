As bases de dados do mundo real contêm grandes volumes de dados, e entre eles escondem-se diversas relações difíceis de descobrir através de métodos tradicionais como planilhas de cálculo e relatórios informativos operacionais.

Desta forma, os sistemas de descoberta de conhecimento (Knowledge Discovery in Data Bases, KD surgem como uma possível solução para dessas relações extrair conhecimento que possa ser aplicado na tomada de decisão em organizações).

Mesmo utilizando um KDD, tal atividade pode continuar sendo extremamente difícil devido à grande quantidade de dados que deve ser processada.

Assim, nem todos os dados que compõem essas bases servem para um sistema descobrir conhecimento.

Em geral, costuma-se pré-processar os dados antes de serem apresentados ao KDD, buscando reduzir a sua quantidade e também selecionar os dados mais relevantes que serão utilizados pelo sistema.

Este trabalho propõe o desenvolvimento, aplicação e análise de uma Arquitetura Híbrida formada pela combinação da Teoria dos Rough Sets (Teoria dos Conjuntos Aproximados) com uma arquitetura de rede neural artificial denominada Mapas Auto-Organizáveis ou Self-Organizing Maps para descoberta de conhecimento.

O objetivo é verificar o desempenho da Arquitetura Híbrida proposta na geração de clusters (agrupamentos) em bases de dados.

Em particular, alguns dos experimentos significativos foram feitos para apoiar a tomada de decisão em organizações.

Os avanços na área da Tecnologia da Informação têm possibilitado o armazenamento de grandes e múltiplas bases de dados.

Esses dados produzidos e armazenados em larga escala são difíceis de serem analisados, interpretados e relacionados pelos métodos tradicionais, como planilhas de cálculo e relatórios informativos operacionais.

Nesses grandes volumes de dados escondem-se diversas relações interessantes que, ao serem descobertas, acrescem conhecimento à tomada de decisão.

Por essa razão faz-se necessário o uso de sistemas que possam extrair o conhecimento dessas bases, viabilizando a análise dos dados, denominados de KDD (Knowledge Discovery in Databases) ou Descoberta de Conhecimento em Bases de Dados.

O KDD pode ser definido como um processo de extração de conhecimentos válidos, novos, potencialmente úteis e compreensíveis para apoiar a tomada de decisão.

Para tanto, o KDD se utiliza das seguintes áreas para realizar os seus processos, métodos estatísticos, reconhecimento de padrões, visualização, banco de dados, aprendizado de máquina, Inteligência Artificial, Data Warehouse, dentre outras.

O KDD é um processo constituído de fases que possuem inúmeros passos, os quais envolvem um número elevado de decisões a serem tomadas pelo usuário, ou seja, é um processo interativo.

É também um processo iterativo, pois ao longo do processo de KDD, um passo será repetido tantas vezes quantas se fizerem necessárias para que se chegue a um resultado satisfatório.

Entretanto, nem todos os dados que compõem as bases servem para um sistema descobrir conhecimento.

Assim, é necessário pré-processar os dados antes de seguir com o processo de descoberta de conhecimento.

A fase de pré-processamento dos dados é importante, pois o sucesso dos resultados obtidos no processo de KDD depende diretamente da qualidade dos dados de entrada.

A redução de dados é uma forma de pré-processamento que visa obter uma representação reduzida dos dados, mas que produz os mesmos (ou quase os mesmos) resultados analíticos.

Boas razões para a redução de dados são a ultrapassagem da capacidade de processamento dos programas de aprendizagem e o tempo muito longo para obter uma solução.

Pode-se citar como vantagens da redução de dados a redução do tempo de aprendizagem dos algoritmos e a interpretação mais fácil dos conceitos aprendidos.

A redução de dados pode ser realizada por, agregação via cubo, redução de atributos (dimensão), compressão de dados, redução de casos, discretização e construção de hierarquias.

Neste trabalho a redução de atributos será mais explorada no capítulo 4, em função de a Arquitetura Híbrida proposta utilizar Rough Sets para este fim.

As técnicas de redução de atributos procuram selecionar o menor conjunto de atributos para dividir o espaço de instâncias de tal maneira que a distribuição das classes no novo espaço é tão próxima quanto possível daquela do espaço original.

Considera-se que a permanência de atributos irrelevantes pode levar à descoberta de padrões de baixa qualidade pelo algoritmo responsável pela descoberta de conhecimento (Data Mining).

Podem-se dividir em três grupos as abordagens propostas para selecionar um subconjunto de atributos, Embutida, a seleção de atributos é realizada internamente pelo próprio algoritmo responsável pela descoberta de conhecimento (Data Mining) como parte do processo de criação do modelo.

Filtro, a seleção de atributos é realizada em um processo anterior separado da aplicação do algoritmo de descoberta de conhecimento.

Neste trabalho foi utilizada a abordagem Filtro.

Wrappers, consiste em selecionar um subconjunto de atributos e medir a precisão do classificador sobre esse conjunto de atributos.

É realizada uma busca pelo subconjunto que gera o classificador com menor erro.

Essa busca avalia cada subconjunto candidato, até que o critério de parada, relacionado com a precisão do classificador, seja satisfeito.

Nas próximas seções deste capítulo serão abordadas as técnicas que compõem a Arquitetura Híbrida proposta.

A Teoria dos Rough Sets (RS), ou Teoria dos Conjuntos Aproximados, (TC foi proposta em 198como um novo modelo matemático para representação do conhecimento e tratamento de incerteza, tendo sido usada, posteriormente, para o desenvolvimento de técnicas para classificação aproximada em aprendizado de máquina).

Devido a estas características, tem-se utilizado esta teoria em Inteligência Artificial, especialmente nas áreas de aquisição de conhecimento, raciocínio indutivo e descoberta de conhecimento em base de dados.

Conjuntos aproximados podem ser considerados conjuntos com fronteiras nebulosas, ou seja, conjuntos que não podem ser caracterizados precisamente, utilizando-se dos atributos disponíveis.

Entre as abordagens mais tradicionais existentes para a modelagem e tratamento de incertezas, encontram-se, Regra de Bayes

Fator de Certeza

Teoria de Dempster-Shafer

Teoria dos Conjuntos Fuzzy (Fuzzy Sets)

Raciocínio Default e Teoria de Endorsements.

A incerteza pode se manifestar de diversas formas, como, imprecisão, incompletude, inconsistência, etc.

RS trata de um tipo fundamental de incerteza, a indiscernibilidade.

A indiscernibilidade surge quando não é possível distinguir elementos de um mesmo conjunto e representa a situação em que esses elementos parecem todos ser um único elemento.

Uma das principais vantagens da Teoria dos RS é que ela não necessita de informações preliminares ou adicionais sobre os dados, tais como a distribuição de probabilidade em estatística, atribuição de probabilidades básicas na Teoria de Dempster-Shafer, ou mesmo os graus de pertinência na Teoria dos Conjuntos Fuzzy.

Isto pode ser verificado em Ziarko.

O conceito dos RS relaciona-se, de alguma maneira, com outras teorias matemáticas desenvolvidas para manipular incerteza, particularmente com a Teoria da Evidência de Dempster-Shafer.

A principal diferença é que a teoria proposta por Dempster-Shafer utiliza a função de crença como ferramenta principal, enquanto a Teoria dos RS faz uso de Aproximações Inferior e Superior.

Existe também uma relação entre a Teoria dos RS e a Teoria dos Fuzzy Sets, as quais são freqüentemente comparadas e até mesmo confundidas, pois tratam da incerteza.

Um estudo comparativo mais aprofundado entre RS e Fuzzy Sets pode ser encontrado em Yao.

Szladow, utiliza um exemplo da área de processamento de imagens para explicar a diferença entre RS e Fuzzy Sets.

Enquanto Fuzzy Sets trata da existência de mais de um nível de cinza nos pixels, RS trata do tamanho desses pixels.

Assim, Fuzzy Sets trata da relação entre intensidades de elementos dentro da mesma classe, enquanto RS trata da relação entre grupos de elementos em diferentes classes.

Entretanto, a Teoria dos RS não compete com a Teoria dos Fuzzy Sets, mas a complementa.

Adjei apresenta um trabalho que combina as duas teorias.

Na realidade, a Teoria dos RS e a Teoria dos Fuzzy Sets são duas abordagens independentes para o tratamento de conhecimento impreciso.

Os conceitos dos RS têm se mostrado muito úteis quando aplicados a problemas dos seguintes tipos, redução de atributos, descoberta de dependência entre atributos e na descoberta de padrões entre os dados.

A redução de atributos realizada por RS é feita através dos chamados redutos, que são subconjuntos de atributos capazes de representar o conhecimento da base de dados com todos os seus atributos iniciais.

Devido às características comentadas, a crescente utilização de RS pode ser comprovada pelo número de aplicações e publicações científicas nas seguintes áreas, KDD e Data Mining, medicina TSU98,00,01 a,01, negócios, engenharia, reconhecimento de padrões, mineração de textos, dentre outros.

A Arquitetura Híbrida proposta neste trabalho utiliza além de RS as redes neurais artificiais (RNAs).

As redes neurais artificiais são redes inspiradas na estrutura do cérebro, com o objetivo de apresentar características similares ao comportamento humano, tais como, aprendizado, associação, generalização e abstração.

Uma rede neural artificial é um processador maciçamente paralelo, distribuído, constituído de unidades de processamento simples, que têm capacidade para armazenar conhecimento experimental e torná-lo disponível para uso.

Vários trabalhos científicos descrevem as características vantajosas em utilizar as RNAs.

Pinheiro relata a alta precisão dos modelos de predição, os quais podem ser aplicados a uma extensa gama de problemas.

Baets relata que são robustas e tolerantes a falhas, graças ao seu paralelismo.

As RNAS podem generalizar os resultados obtidos para dados previamente desconhecidos, ou seja, produzir respostas coerentes e apropriadas para padrões ou exemplos que não foram utilizados no seu treinamento.

Zhang mostra a capacidade das RNAs em lidar com estruturas de dados não lineares, o que é uma característica interessante para aplicações em marketing que lidam com dados como vendas e preços.

Muitos trabalhos mostram o melhor desempenho das RNAs em comparação com outra técnicas.

Ressalta-se que as RNAs podem ter várias entradas e várias saídas, sendo aplicáveis a sistemas com muitas variáveis.

A RNA é não-paramétrica e faz suposições menos restritivas a respeito da distribuição dos dados de entrada do que métodos estatísticos tradicionais.

Os trabalhos de Almeida, Arraes, Freitas, e Portugal, voltados para a área de negócios, retratam o melhor desempenho das RNAs com relação aos métodos estatísticos.

As RNAs separam grupos de maneira não linear, levando a uma separação mais refinada, o que é uma vantagem com relação às técnicas estatísticas discriminatórias e classificatórias lineares.

As técnicas estatísticas requerem inferências iniciais que nem sempre são fáceis de aplicar e interpretar e são difíceis de ser aplicadas a um número muito grande de variáveis.

As redes neurais artificiais automatizam os processos estatísticos, aliviando o usuário final desse grande trabalho.

Em Ebecken, os resultados também foram melhores com RNAs quando aplicadas na área de energia, e em Zaremba, na área médica.

Pode-se verificar aplicações de redes neurais artificiais nas organizações no capítulo 5, tabela.

Ainda em Almeida é feita uma comparação entre RNAs e Sistemas Especialistas.

As redes neurais artificiais não precisam de um especialista para a criação de sua base de conhecimentos.

Elas não trabalham com regras.

Sua aquisição de conhecimento é feita automaticamente a partir de exemplos coletados em bases de dados.

Arraes e Rodrigues citam em seus trabalhos vantagens das RNAs sobre as Árvores de Decisão.

Em Tápia, os experimentos mostram melhores resultados obtidos com as RNAs para previsão do que com a metodologia Box-Jenkins.

Como toda técnica, as RNAs apresentam algumas desvantagens, como, utilizar predominantemente dados numéricos, em certos casos o tempo de treinamento é longo, requerer muitos parâmetros que influenciarão fortemente os resultados e a obtenção do melhor resultado depende do ajuste dos parâmetros baseados na tentativa e erro.

Berry discute as dificuldades relacionadas ao uso das RNAs e Passari analisa vantagens e desvantagens do uso das RNAs, desenvolvida pelo finlandês Teuvo Kohonen em 1984.

A rede SOM é um tipo de rede neural artificial baseada em aprendizado não supervisionado, sendo capaz de mapear um conjunto de dados, de um espaço de entrada multidimensional, em um conjunto finito de neurônios organizados em um arranjo normalmente unidimensional ou bidimensional.

Além das características vantajosas das redes neurais artificiais citadas na seção 13, a rede SOM possui algumas outras vantagens, como ajuste local do mapa e visualização, o que faz com que seja utilizada nas mais diversas áreas, que vão da medicina ao marketing.

No caso de KDD, a rede SOM é muito utilizada na tarefa de clusterização, por possibilitar em um mapa uni ou bidimensional a formação e a visualização simples dos clusters (grupos) e da correlação dos dados, preservando a posição relativa dos clusters no hiperespaço original.

Ebecken mostra alguns trabalhos onde a avaliação dos resultados foi realizada através da visualização do mapa gerado.

Além das vantagens sobre as técnicas estatísticas tradicionais, um estudo comparativo e detalhado entre técnicas de clusterização em mineração de dados realizado por Ultsh e Vesanto mostrou que a rede SOM apresenta melhores resultados.

Uma das desvantagens da rede SOM é a imprecisão na definição de fronteira entre os clusters.

Esta desvantagem será analisada com mais detalhes no capítulo 6.

Segundo Goldschmidt, técnicas podem ser combinadas para gerar as chamadas Arquiteturas Híbridas.

A grande vantagem desse tipo de sistema deve-se ao sinergismo obtido pela combinação de duas ou mais técnicas.

Este sinergismo reflete na obtenção de um sistema mais poderoso (em termos de interpretação, de aprendizado, de estimativa de parâmetros, de generalização, dentre outros) e com menos deficiências.

Existem três formas básicas de se associarem duas técnicas para a construção de uma Arquitetura Híbrida, Híbrida Seqüencial, nesta forma, uma técnica atua como entrada de outra técnica.

A Arquitetura Híbrida proposta neste trabalho é seqüencial, pois usa o RS como pré-processador da rede SOM.

Híbrida Auxiliar, esta forma poderia ser exemplificada do seguinte modo, uma RNA invoca um Algoritmo Genético para a otimização de seus pesos ou de sua estrutura.

Neste caso, tem-se um maior grau de hibridização em comparação com o híbrido seqüencial.

Híbrida Incorporada, nesta forma praticamente não há separação entre as duas técnicas.

Pode-se dizer que a primeira técnica possui a segunda técnica e vice-versa.

Poderia ser exemplificado por um sistema neuro-fuzzy híbrido em que um sistema de inferência fuzzy é implementado segundo a estrutura de uma rede neural artificial.

Aqui a hibridização é a maior possível.

Existem vários trabalhos que combinam RS com outras técnicas formando uma Arquitetura Híbrida Com a rede SOM, pode-se destacar o trabalho de Lingras, no qual o algoritmo da rede SOM foi modificado com base nos conceitos de Aproximação Inferior e Superior dos RS buscando um melhor intervalo entre os clusters.

Neste trabalho a Arquitetura Híbrida proposta emprega RS pré-processando a rede SOM.

O presente trabalho tem como seu pilar fundamental desenvolver, aplicar, analisar e avaliar o desempenho de uma Arquitetura Híbrida proposta (RS com a rede SOM) na descoberta de conhecimento em bases de dados.

Por possuírem grandes quantidades de informação, as organizações cada vez mais necessitam do auxílio de técnicas computacionais apropriadas para auxiliar o homem na análise, na interpretação e no relacionamento dessas informações em busca de conhecimento.

Em geral, as técnicas apresentam ao homem vantagens na sua utilização, mas também apresentam desvantagens, levando à conclusão de que em muitos casos existe a necessidade de combinar duas ou mais técnicas a fim de eliminar ou reduzir as deficiências.

Surgiu, assim, neste trabalho a idéia de juntar RS com a rede SOM para formar uma Arquitetura Híbrida.

RS, pelas vantagens apresentadas na seção 1e também por ser uma teoria relativamente nova, cujo campo de pesquisa ainda está aberto à exploração e necessita de contribuição.

No caso da rede SOM, pelas vantagens descritas na seção 14.

A combinação RS com a rede SOM pela potencial melhoria sobre a baixa definição de fronteira dos clusters gerados pela rede SOM.

A Arquitetura Híbrida proposta será avaliada pelo seu desempenho na definição de fronteira entre os clusters.

Na Arquitetura Híbrida desenvolvida neste trabalho, as duas técnicas têm as seguintes funções, a função de RS é de redução de atributos (dimensão) e a função da rede SOM é a de gerar os clusters.

A base de dados, com todos os seus atributos iniciais, é apresentada ao RS a fim de que ele reduza o número de atributos.

Em seguida, a base de dados reduzida é apresentada à rede SOM para a geração do mapa auto-organizável.

A análise dos resultados não busca apenas verificar a redução do tempo de treinamento da rede SOM, mas analisar principalmente se a deficiência na definição de borda (fronteir dos clusters obtidos foi reduzida ou até eliminada).

O objetivo principal deste trabalho é, através da análise dos resultados obtidos com a Arquitetura Híbrida proposta, procurar responder a uma pergunta fundamental, "Existe melhoria nos resultados gerados pela rede SOM quando pré-processada por RS ".

Além do objetivo principal, vislumbra-se um outro objetivo, que é contribuir para o entendimento mais aprofundado dos RS como uma técnica de redução de atributos, analisando para tanto os conceitos de dependência e significância de atributos.

Este trabalho busca contribuir com uma Arquitetura Híbrida que combina RS com rede SOM na descoberta de conhecimento em bases de dados, eliminando ou reduzindo deficiências da rede SOM para melhorar a tomada de decisão.

Ainda não está claro como se deve escolher os parâmetros do algoritmo da rede SOM de maneira a se obter um "bom mapeamento".

Normalmente a rede SOM é avaliada com base na resolução do mapa e em relação à preservação da topologia dos dados de entrada.

A escolha do "melhor mapeamento" deveria ser, portanto, por aquele que "melhor representa os dados de entrada".

Este critério normalmente é traduzido por duas medidas, o erro de quantização e o erro topográfico.

Em Vesanto, Villmann e Pölzlbauer pode-se encontrar experimentos utilizando outras medidas de qualidade, mas o erro de quantização e o erro topográfico ainda são as medidas mais utilizadas.

Além das medidas de qualidade, serão adotados também como critérios de avaliação o tempo de treinamento da rede SOM, o número de clusters gerados e a visualização do mapa através do Mapa por Similaridade de Cor.

Como os clusters são obtidos por intermédio da aplicação dos conceitos de similaridade e distância, um número maior ou menor de clusters pode indicar o que ocorreu com as similaridades entre os dados e as diferenças de separação dos clusters.

Comark afirma que a tarefa de clusterização está pautada em duas idéias básicas, a coesão interna dos elementos e o isolamento externo entre os clusters.

Dessa forma, um bom cluster é aquele que apresenta alta similaridade interna (intra-cluster) e baixa similaridade externa (extra-cluster).

Em Halkidi pode-se encontrar um estudo detalhado de validação das técnicas de clusterização.

Um dos motivos que fazem com que a rede SOM seja muito utilizada para o KDD é a visualização.

As formas de visualização mais utilizadas para detectar clusters gerados pela rede SOM são, a chamada Matriz-U ou matriz de distâncias unificadas e o Mapa por Similaridade de Cor, que permite a análise da estrutura dos clusters.

Vários trabalhos se utilizaram da visualização dos mapas gerados pela rede SOM para chegar à conclusão nos seus experimentos.

Os resultados dos mapas obtidos nos experimentos realizados no sexto capítulo desta tese serão analisados através do Mapa por Similaridade de Cor.

A realização da pesquisa deste trabalho está embasada em consultas a fontes bibliográficas e de referencial teórico como, artigos, livros, teses, dissertações e relatórios técnicos nos campos de redes neurais artificiais, RS, KDD e de negócios.

Foi desenvolvida uma parte experimental (capítulo para avaliar os resultados obtidos com a aplicação da Arquitetura Híbrida proposta).

Foram realizados cinco experimentos com bases de dados distintas comparando a Arquitetura Híbrida proposta (RS com a rede SOM) com uma rede SOM.

Cada experimento foi dividido em duas fases, Na primeira fase, denominada rede SOM sem redutos, apresentou-se à rede SOM cada uma das cinco bases de dados dos experimentos com todos os atributos e avaliaram-se os resultados.

Na segunda fase, denominada Arquitetura Híbrida (rede SOM com redutos), apresentou-se primeiro RS a cada uma das cinco bases de dados para a geração dos redutos e, em seguida, as bases de dados reduzidas foram apresentadas à rede SOM.

Os detalhes dos experimentos estão relatados no capítulo 6 deste trabalho.

Este trabalho foi estruturado em sete capítulos.

Além desta Introdução (capítulo 1), o trabalho compõe-se das seguintes partes, Capítulo 2A Organização na Era do Conhecimento, o capítulo trata das constantes mudanças organizacionais decorrentes da pressão exercida pelo meio ambiente.

Para mudar corretamente é necessário conhecer este meio ambiente.

É neste cenário que entram os sistemas de descoberta de conhecimento como elemento importante no processo de transformação da informação em conhecimento.

A contribuição do capítulo para este trabalho reside em mostrar a necessidade das organizações em extrair conhecimento de suas bases de dados, justificando a utilização dos sistemas de descoberta de conhecimento.

Capítulo 3Descoberta de Conhecimento em Bases de Dados (KD, este capítulo descreve todo o processo dos sistemas de descoberta de conhecimento, os chamados KDD, constituído de fases, tarefas e técnicas).

A contribuição do capítulo está em apresentar as características dos sistemas de descoberta de conhecimento, servindo de apoio para entender o objetivo da Arquitetura Híbrida proposta.

Capítulo 4Teoria dos Rough Sets, este capítulo descreve os conceitos gerais e aplicações da Teoria dos RS, também chamada de Teoria dos Conjuntos Aproximados.

O capítulo contribui principalmente com o entendimento da Teoria dos RS nos conceitos referentes à incerteza e à redução de atributos que são relevantes para este trabalho, pois a Arquitetura Híbrida proposta combina RS com a rede SOM.

Capítulo 5Rede SOM, neste capítulo descreve-se os conceitos gerais das redes neurais artificiais e suas aplicações.

É realizado um estudo mais aprofundado da Rede SOM, pois é a rede neural artificial utilizada na Arquitetura Híbrida proposta.

O capítulo contribui para o entendimento de como a rede SOM realiza a tarefa de clusterização e também discute quais são as medidas de qualidade e as formas de visualização do mapa que serão utilizadas na avaliação do desempenho da Arquitetura Híbrida.

Capítulo 6 Experimentos Comparativos com a Arquitetura Híbrida, neste capítulo são realizados os experimentos para avaliar o desempenho da Arquitetura Híbrida proposta.

Ao final de cada experimento é realizada uma avaliação dos resultados obtidos, e no final do capítulo analisam-se os resultados através de um balanço geral dos experimentos.

Capítulo 7Conclusão, neste capítulo conclui-se o trabalho com a discussão dos resultados obtidos e propõem-se direções para futuras investigações.

Este capítulo descreve a importância da informação na relação das organizações com o meio ambiente e a necessidade de transformação dessas informações em conhecimento para apoiar a tomada de decisão.

Atualmente as organizações precisam reagir de modo rápido às ameaças e às oportunidades que surgem no ambiente empresarial cada vez mais competitivo e mutável.

Este ambiente está intimamente relacionado à combinação de fatores sociais, jurídicos, econômicos, físicos e políticos, que afetam as atividades da organização.

Zuboff vê a informação como um elemento tão importante para organizações, que convencionou, chamar a era atual de Era da Informação, e a economia de Economia da Informação.

Na Era da Informação a riqueza nasce de idéias inovadoras e do uso inteligente da informação.

As organizações da Era da Informação concorrem em um mercado repleto de desafios, que muda rapidamente, é complexo, globalizado, supercompetitivo e voltado para o cliente.

A boa informação deve, no mínimo, ser, correta, destinada à pessoa certa e em tempo hábil.

Deve também ser, precisa, completa, econômica, flexível, confiável, relevante, simples e verificável.

O conhecimento consiste em informações organizadas e processadas para transmitir discernimento, experiências, aprendizagem acumulada ou habilidade, ser aplicável a um problema ou processo empresarial.

As informações que são processadas para extrair implicações críticas e refletir experiências e habilidades anteriores fornecem a quem as recebe o conhecimento organizacional, que é altamente valorizado.

Uma organização forma a sua base de conhecimento através da reunião das experiências individuais, parciais, dispersas e voláteis, ou seja, a base de conhecimento é a reunião de informações transformadas em conhecimento coletivo, coerente e memorizado.

Organização formando sua Base de Conhecimento.

A Organização formando a sua Base de Conhecimento.

Pode-se visualizar o processo de troca de informações entre a organização (Matriz) e suas várias partes, possibilitando a geração e armazenando do conhecimento.

As informações e o conhecimento compõem recursos estratégicos essenciais para o sucesso de uma organização.

Portanto, o correto tratamento das informações pode ser uma vantagem competitiva muito importante.

A organização que trata corretamente as informações apresenta as seguintes características, Atuante, competitividade comprovada pela rentabilidade em curto prazo e sobrevida no longo prazo

Identidade definida, sabe quem é, o que deseja ser e têm conhecimento sobre como agir para tal

Aberta em seu interior (comunicante e participativ e em seu exterior (proativa e antecipativa).

Adaptável às mudanças de ambiente com aprendizagem inovadora

Aperfeiçoa os seus membros, motivando e melhorando suas competências

Mobiliza as energias e inteligências para atingir seus objetivos.

Através da informação a organização pode rever alguns critérios determinantes para o seu sucesso, capacidade de obter a qualidade total do serviço prestado ao cliente, rapidez de reação, capacidade de evolução, e capacidade de inovação.

A organização deve sempre recolher informações visando melhorar seu poder de adaptação nos mercados em que atua.

A informação agrega ganho de capital ao produto ou serviços oferecidos.

A organização necessita do meio ambiente (todos os elementos que, atuando fora de uma organização são relevantes para as suas operações) para retirar insumos (mão-de-obra, matéria-prima, capital, et e devolve ao meio ambiente bens ou serviços).

Este mesmo meio ambiente exerce pressão sobre a organização, fazendo com que ela tenha de se modificar, inovar para poder sobreviver.

Relação da organização com o meio ambiente.

A organização se relacionando com o Meio Ambiente.

Pode-se verificar a relação da organização com alguns dos elementos que constituem o meio ambiente onde ela está inserida.

Existe uma relação de troca, ou seja, há uma dependência mútua entre a organização e os elementos que compõem este meio, como por exemplo, a organização necessita dos clientes para adquirir seus bens ou serviços e o cliente necessita da organização para que as suas necessidades sejam atendidas.

O retorno (feedback) da informação sobre desempenho da organização com relação ao meio ambiente é essencial para a criação de uma organização flexível onde existe um constante "aprendizado", que imediatamente implementa a estratégia de seus objetivos e reconhece a necessidade de modificar esses objetivos quando estes se tornam ineficazes.

A relação da organização com o meio ambiente pode significar tanto uma oportunidade como uma ameaça.

Assim, as organizações podem responder de forma reativa a uma pressão já existente, ou de modo pró-ativo a uma pressão esperada através de estratégias, do foco no cliente e no serviço, de esforços contínuos de melhoria e da reengenharia de processo de negócio.

Essas reações são facilitadas pelo uso da Tecnologia da Informação (TI), a qual pode ser definida como toda forma de gerar, armazenar, veicular, processar e reproduzir a informação.

Fundamenta-se a TI nos seguintes componentes, hardware e seus dispositivos e periféricos, software e seus recursos, sistemas de telecomunicações, gestão de dados e informações, e gestão do conhecimento.

A TI é vital para todas as áreas funcionais de uma organização, e suas ferramentas fazem parte integral de todas essas áreas (finanças, contabilidade, marketing, vendas, produção, recursos humanos, etc).

Suas ferramentas são poderosos agentes de mudanças dentro da organização, pois, através da aplicação da ferramenta correta para o problema em questão, podem-se alcançar melhorias nos processos deficitários da organização, bem como diferenciais competitivos no ambiente de negócios.

Os objetivos da TI são introduzir mudanças fundamentais no trabalho, integrar funções do negócio da organização, aumentar a competitividade, criar ou manter oportunidades estratégicas de diferenciação da concorrência, possibilitar o enxugamento da estrutura organizacional.

As necessidades a serem atendidas pela TI são, necessidades organizacionais, necessidades de informação e de evolução tecnológica.

Os fatores críticos de sucesso apoiados pela TI são, o negócio da organização e seu grau de sucesso, o negócio da área de Informática e seu próprio sucesso, a percepção dos executivos da organização.

A TI permite alterar o modo como se estruturam e se administram as organizações e os processos.

Torres enumera diversos efeitos estratégicos ocorridos com o uso da TI.

Aqui estão destacados os principais.

Mudanças radicais em processos operacionais com grandes ganhos de tempo e custos.

Melhoria da imagem da organização, informações rápidas aos clientes, agilidade nas transações, rapidez de resposta a novas demandas do mercado, criação de laços fortes com fornecedores, criação de laços fortes com agentes comerciais ou clientes, conhecimento da concorrência.

Maior poder de análise para situações de risco.

Mudanças na estrutura e nos custos de um produto, uso de sistemas em situações impossíveis ou de grande dificuldade pelo ser humano.

Balanceamento de disponibilidades de recursos com demanda, rastreamento do produto desde sua fabricação até o término de sua vida útil, aumento de confiabilidade em diagnósticos e prescrições de solução de problemas.

Integração e aumento do potencial de vendas dos agentes de vendas.

Turban enumera as tendências tecnológicas da TI em função do sucesso obtido pelas organizações na sua utilização, melhoria constante na relação custo/desempenho, armazenamento e memória maiores, interfaces gráficas e amigáveis com o usuário, Data Warehouse, Data Mining.

Ambiente orientado a objetos.

Gerenciamento de documentos eletrônicos.

Realidade virtual.

Sistemas inteligentes.

Computação portátil.

Fusão de eletro-eletrônicos.

Comércio eletrônico e computação doméstica integrada.

O uso da TI acabou por criar uma economia inteiramente nova, uma Economia de Informação, tão diferente da Economia Industrial quanto a Industrial era da Agrícola.

Atualmente vivemos na Economia da Informação e quando a fonte da riqueza das organizações se modifica, todo o meio ambiente em que ela está inserida também se modifica.

Zuboff mostra as diferenças básicas entre a Economia Industrial e a Economia da Informação, onde as organizações voltadas para dentro, ou seja, apenas para os seus processos, são tidas como participantes da Economia Industrial, e as organizações voltadas para o meio ambiente, como participantes da Economia da Informação.

Nas últimas décadas, a Revolução da Informação que gerou a Economia da Informação está mudando a fonte de riqueza.

A nova fonte de riqueza não é material, é informação, conhecimento aplicado ao trabalho para criar valor.

A busca de riqueza agora é, em grande medida, a busca de informação e a aplicação de capital intelectual aos meios de produção.

Como fonte de riqueza, a informação vem de várias formas, desde os dados eletrônicos de curta validade a anos de pesquisa acumulada incorporada em memórias de computadores que operam fábricas automatizadas ao capital intelectual carregado no cérebro das pessoas.

As organizações da Economia Industrial são ineficazes na Economia da Informação, pois fortalecem a profunda especialização da mão-de-obra, a produção em massa e uma estrutura organizacional hierárquica extremamente rígida.

A TI é fundamentalmente diferente da Tecnologia Industrial no sentido de que ela pode ser programada para fazer a tarefa requerida e, se necessário, pode ser continuamente ajustada.

A maneira pela como as roupas são cortadas pode ser uma operação de elevado valor adicionado ou de perda de dinheiro.

Hoje, com os cortadores controlados por computador, a máquina pode ser programada para produzir qualquer número de tamanhos diferentes em qualquer volume de peças, tornando assim economicamente viável atender a qualquer pedido do cliente.

Os seres humanos descobriram uma maneira de aplicar ao trabalho o conhecimento rapidamente crescente que possuem para criar valor de maneiras até há bem pouco tempo desconhecidas.

Quando a TI transformou a informação no mais importante fator de produção, tornou a oportuna aquisição da melhor informação a meta número um da administração comercial.

Dentro de dados demográficos, sociológicos, econômicos e de marketing estão ocultas infindáveis idéias sobre produtos, serviços e estratégias de marketing que devem ser aproveitadas pela organização.

Hoje, mais do que nunca, ter uma estratégia comercial significa ter uma estratégia de informação, uma estratégia para reconhecer as oportunidades nas mudanças, uma estratégia para garantir que a organização tire total valor do conhecimento acumulado por seus trabalhadores em vez de permitir que o conhecimento desapareça ou se escoe para outros.

Torres diz que a essência da Estratégia da Era da Informação é transformar a carga de dados comerciais numa recompensa de oportunidades.

A concorrência sob a Era da Informação obriga as organizações a desenvolver qualidades como, serem globalizadas, reagirem rapidamente às novas oportunidades, serem bem informadas, mais especializadas em suas áreas do que seus concorrentes ou seus reguladores e serem capazes de explorar novas capacidades produtivas.

Uma vez que a maior parte de seu capital será intelectual, elas tenderão a ser altamente flexíveis, sensíveis a seus ambientes políticos e sociais e sempre prontas a desviar as operações para países com climas mais favoráveis.

A transformação da informação em conhecimento também é possível através da utilização da Inteligência Artificial (I, que é o estudo de como fazer os computadores realizarem coisas que, no momento, as pessoas fazem melhor).

A utilização da IA nesta transformação embasa o desenvolvimento deste trabalho na criação de uma Arquitetura Híbrida para extrair conhecimento de bases de dados presentes nas organizações.

Para ilustrar a utilização da IA, mostramos uma breve cronologia da TI.

Breve cronologia da TI.

Pode-se observar a evolução da TI, caracterizada inicialmente por aplicações militares e comerciais rotineiras até os dias atuais com a presença do Datawarehouse e do Data Mining utilizando IA para descobrir conhecimento.

Nas duas últimas décadas, pelo menos no mundo industrializado, assistiu-se à morte dos mercados de massa para mercadorias padronizadas.

Os clientes desejam essencialmente bens e serviços personalizados a preços de produção em massa.

A noção de produção em massa significa essencialmente que uma organização fabrica o mesmo produto para um grande número de clientes, enquanto os produtos personalizados fornecem conjuntos de benefícios especiais que satisfazem determinadas necessidades particulares do cliente.

Torna-se cada vez mais necessário fabricar produtos altamente personalizados sem abrir mão dos benefícios de custo das técnicas de produção de massa.

Essencialmente, o mercado está chegando ao ponto em que cada cliente é um nicho.

A informação está no âmago dessas tentativas de se chegar a um pluralismo de mercado.

Por exemplo, esforços para atingir a personalização em massa no processo de fabricação, através da implantação de estratégias de fabricação flexíveis, tornam-se possíveis através do uso de tecnologias onde a informação representa papel preponderante.

A informação e o conhecimento embutidos em tecnologias de fabricação assistidas por computador (CAM) criam economias comparáveis às economias de escala que acionaram as estratégias de produção de massa durante a Revolução Industrial.

As tecnologias de fabricação flexíveis utilizam a TI para embutir essas informações e esse conhecimento nas próprias máquinas, reduzindo a importância das economias de escala.

As administradoras de cartões de crédito oferecem um bom exemplo de estratégia de produto/serviço baseado em informação.

Elas coletam e mantêm enormes quantidades de dados como um subproduto das suas operações de processamento de cartões de crédito.

Uma vez que esses dados foram capturados, os mesmos poderão ser elaborados e revendidos como outras formas de produtos de informação.

Para contas empresariais, por exemplo, os encargos podem ser resumidos e relatados por área geográfica ou tipo de gasto.

Isso representa uma oportunidade adicional de extrair valor da informação que foi coletada como parte das atividades rotineiras de processamento de transações da organização.

Os requisitos estabelecidos pelo conceito de produto ótimo (quantidade maximizada, tempo de entrega minimizado e custos minimizados) somente se viabilizam com o uso intensivo da TI.

Hoje a estratégia baseia-se no domínio por meio de recursos estratégicos como o conhecimento e a informação e não mais somente no domínio dos recursos naturais.

Essa estratégia afeta a organização como um todo e define as atividades comerciais, o modo de operar essas atividades e, particularmente, a forma de diferenciar seus produtos e serviços daqueles oferecidos pelos concorrentes.

Assim, uma estratégia abrangente deverá abordar duas questões, em primeiro lugar, as estratégias devem considerar os clientes e os segmentos de mercado aos quais a organização deseja servir e em segundo lugar, as estratégias devem considerar habilidades e recursos que a organização deverá reunir para fornecer produtos e serviços a esses mercados.

A estratégia depende da informação, e a correta utilização da informação depende da TI.

A definição de clientes e mercados potenciais depende de informação externa sobre as necessidades e de interpretação inteligente dessas necessidades, de forma a possibilitar o bom uso dos pontos fortes da organização.

Segundo McGee, a tradução das escolhas em relação a clientes e mercados deve levar em consideração cinco pontos, A definição e o projeto de produtos e serviços a serem oferecidos.

O estabelecimento para a organização de objetivos de desempenho, financeiros e não financeiros.

A definição de processos organizacionais e operacionais que possam atender aos objetivos de desempenho, diferenciando os produtos e serviços da organização dos produtos e serviços de seus concorrentes.

O desenvolvimento de recursos de tal forma que maiores probabilidades sejam criadas para que os objetivos de desempenho sejam alcançados.

O monitoramento do desempenho organizacional e redirecionamento de recursos conforme necessário.

A questão da diferenciação é fundamental para a compreensão da estratégia, pois uma estratégia efetiva deve definir as formas pelas quais os produtos e serviços de uma organização serão superiores aos de seus concorrentes (e concorrentes em potencial) aos olhos dos clientes.

Se uma estratégia não permitir que uma organização ofereça (ou pelo menos convença os consumidores de que oferece) melhor qualidade, menor custo, melhor serviço, ou alguma outra característica desejável, não será estratégia.

Sem essa capacidade de claramente diferenciar seus produtos e serviços dos oferecidos por seus concorrentes, uma organização não poderá almejar alcançar um desempenho superior.

Qual a informação que a organização precisa ter e dela se utilizar de maneira mais eficiente do que qualquer outra com relação a clientes, concorrentes e ambiente competitivo Que informação as organizações precisam fornecer a seus clientes e fornecedores, e como estes a utilizarão para fornecer subsídios relevantes para a organização Essas são perguntas a que uma estratégia competitiva com uso de informação deve procurar responder.

A informação desempenha papel especialmente importante no desenvolvimento de esforços para criar e manter a diferenciação.

No momento da interação entre a organização e o cliente, a informação torna-se um instrumento essencial para a individualização do serviço ao cliente num mundo anônimo.

Essa personalização do serviço ao cliente evolui para a criação de um maior número de nichos de mercado, possível pela obtenção de informações mais precisas sobre esses grupos de clientes ou nichos, o limite dessa tendência é tratamento tão personalizado a ponto de cada cliente ser considerado um nicho de mercado.

A idéia de serviço individualizado ao cliente está embutida no conceito de que os consumidores cada vez mais exigem ser tratados como indivíduos e não apenas como membros de um grande grupo.

Em suma, busca-se a experiência do comerciante do século XIX, que conhecia cada cliente pessoalmente, mas na escala global do século XXI.

Este cenário de plena utilização da informação e a necessidade de obtenção de conhecimento fizeram surgir o conceito de Economia do Conhecimento, e o principal veículo desta nova economia é a Internet.

Não basta possuir informação (as organizações, em geral, possuem muita informação), é necessário transformá-la em conhecimento.

A Gestão do Conhecimento é um dos pilares associados às novas Tecnologias da Informação como o KDD (Knowledge Discovery in Database) ou Descoberta de Conhecimento em Bases de Dados (capítulo, que transforma informação em conhecimento em benefício das organizações).

Cornella chama a Economia do Conhecimento de Infonomia, ou seja, é a "Ciência para o ótimo aproveitamento da informação e a obtenção de conhecimento dentro das organizações".

A chave está em aliar os negócios a TI para prever e analisar as mudanças no comportamento do cliente, as necessidades atuais e futuras deste cliente e como conseguir satisfazê-lo.

O objetivo deste capítulo foi mostrar a importância da informação e de sua transformação em conhecimento para apoiar a tomada de decisão nas organizações.

O conhecimento possibilita a uma organização definir os segmentos de mercado para os quais deseja atuar e definir as estratégias necessárias para este fim.

Conhecer o meio ambiente e definir estratégias são atividades mais bem realizadas quando apoiadas na TI.

Desta forma, a Arquitetura Híbrida proposta neste trabalho foi desenvolvida para descobrir conhecimento em bases de dados apoiando a tomada de decisão organizacional.

O capítulo descreve todo o processo de Descoberta de Conhecimento em Bases de Dados (Knowledge Discovery in Databases KD).

Este capítulo trata da descoberta de conhecimento em bases de dados (KD através da análise das suas fases, das formas de interpretação dos dados (tarefas) e das aplicações nas diversas áreas do conhecimento.

Existem várias definições para KDD.

Segundo Fayyad, o KDD pode ser definido como um processo de extração de conhecimentos válidos, novos, potencialmente úteis e compreensíveis, visando melhorar o entendimento de um problema.

Brachman define KDD como um processo cujo uso de conhecimento é intensivo e estruturado em fases, consistindo de interações entre uma pessoa e um banco de dados, tendo por suporte um conjunto heterogêneo de ferramentas.

Em Cabena, descobrir conhecimento significa extrair, de grandes bases de dados, sem nenhuma formulação prévia de hipóteses, informações relevantes e desconhecidas, que podem ser utilizadas para a tomada de decisão.

Para tanto, o KDD se utiliza de diversas áreas para realizar os seus processos como, métodos estatísticos, reconhecimento de padrões, visualização, banco de dados, aprendizado de máquina, Inteligência Artificial, Data Warehouse, dentre outras.

KDD é o termo criado pelo Gartner Group na década de oitenta para descrever todo o processo de extração de conhecimento dos dados devido ao crescimento vertiginoso das bases de dados das organizações.

Com toda essa informação armazenada, as organizações perceberam que não bastava apenas disponibilizá-la, era necessário interpretá-la, analisá-la e relacioná-la a fim de extrair conhecimento para apoiar a decisão.

O KDD é um processo composto por fases que devem ser desenvolvidas para atingir o objetivo final, que é a extração de conhecimento.

As fases do KDD possuem numerosos passos, que envolvem um número elevado de decisões a serem tomadas, ou seja, é um processo interativo.

É também um processo iterativo, pois ao longo do processo KDD, um passo será repetido tantas vezes quantas se fizerem necessárias para que se chegue a um resultado satisfatório.

No processo de KDD cada fase possui uma intersecção com as demais.

Desse modo, os resultados produzidos numa fase são utilizados para melhorar os resultados das próximas fases.

O KDD é composto das seguintes fases, seleção dos dados, pré-processamento dos dados, transformação dos dados, mineração de dados (Data Mining) e interpretação/avaliação do conhecimento.

Fases do processo do KDD, com início na apresentação dos dados até chegar à obtenção do conhecimento.

A iteração entre as fases pode ser observada pelas setas tracejadas.

Fases do processo de KDD.

As três fases iniciais do KDD, que envolvem a seleção, o pré-processamento e a transformação, também chamadas de preparação dos dados, exigem bastante tempo, aproximadamente entre 60 e 80% do tempo utilizado em todo o processo, sendo que a maior parte desse tempo é consumida com a limpeza dos dados.

Grande esforço requerido na preparação dos dados.

Esforço requerido para cada fase do processo de KDD.

Considerando em 100% o esforço total necessário para realizar todo o processo de KDD, pode-se verificar que somente na fase de preparação de dados gasta-se 60% de todo o esforço.

A seleção de dados consiste na criação de um conjunto de dados-alvo, ou dados selecionados.

Nesta fase, seleciona-se um conjunto de dados ou focaliza-se um subconjunto de atributos (variáveis) em que a descoberta deverá ser efetuada.

A seleção de dados vai variar de acordo com os objetivos da organização.

Na seleção dos dados escolhem-se apenas atributos relevantes do conjunto de atributos da base de dados.

Em suma, a seleção de atributos consiste na escolha de um subconjunto de atributos relevantes para o objetivo da tarefa.

O subconjunto selecionado é então fornecido para o algoritmo de mineração dos dados.

Uma motivação para essa seleção é otimizar o tempo de processamento do algoritmo minerador, visto que ele apenas trabalhará com um subconjunto de atributos, desse modo diminuindo o seu espaço de busca.

As grandes bases de dados são altamente susceptíveis a ruídos, valores faltantes e inconsistentes.

Dados limpos e compreensíveis são requisitos básicos para o sucesso da mineração dos dados.

O pré-processamento dos dados tem por objetivo assegurar a qualidade dos dados selecionados.

A limpeza dos dados envolve a verificação da consistência das informações, a correção de possíveis erros e o preenchimento ou a eliminação de valores nulos e redundantes.

Nessa fase são identificados e removidos os dados, duplicados e/ou corrompidos.

A execução dessa fase corrige a base de dados eliminando consultas desnecessárias que seriam executadas pelo algoritmo minerador e que afetariam o seu processamento.

Os métodos de limpeza dos dados são herdados e dependentes do domínio da aplicação, assim, torna-se essencial a participação do analista de dados.

Um exemplo simples de limpeza de dados seria a definição de um intervalo de possíveis valores para um determinado atributo, {010}.

Caso surgisse qualquer valor diferente dos definidos no intervalo, esse dado seria retirado.

A limpeza dos dados consiste em resolver problemas com, dados com erros (valores discrepantes), registros repetidos e valores faltantes.

Os valores que são significativamente fora do esperado são denominados valores discrepantes ou outliers.

Os outliers podem indicar uma notícia boa ou má.

Uma boa notícia se indicarem uma nova tendência de resultados para as variáveis em questão, e uma má notícia se realmente forem dados inválidos.

Um tipo comum de outlier é devido a erro humano, como por exemplo, um registro de compra da ordem de milhões de reais.

Esses registros devem ser corrigidos se valores razoáveis ou válidos estiverem disponíveis, caso contrário, devem ser excluídos da análise.

Como valores faltantes incluem-se aqueles que simplesmente não estão presentes no conjunto selecionado e os valores inválidos que foram eliminados durante a detecção de outliers.

Os valores faltantes podem ocorrer devido a erros humanos, ou porque a informação não está disponível no momento do levantamento dos dados, ou quando os dados são selecionados considerando-se diferentes origens, gerando informações contraditórias.

Xiaohui apresenta um estudo detalhado sobre outliers.

A redução de dados é considerada uma técnica de pré-processamento dos dados e seu estudo tem a maior importância neste trabalho, porque a Arquitetura Híbrida proposta combina o RS como técnica de redução de atributos com as redes neurais artificiais.

A redução da quantidade de dados pode ser utilizada para reduzir o tempo de aprendizagem do algoritmo de mineração, quando o tempo para obter uma solução é muito longo, e para tornar mais fácil a interpretação dos conceitos aprendidos.

Observa-se que a redução de dados pode ocorrer de duas formas, redução de atributos (colunas) e redução de casos (linhas).

Formas de pré-processar dados.

As linhas (reg) se referem a casos e as colunas (atr) se referem a atributos.

No caso de RS a redução é realizada nos atributos, também chamada de redução de dimensão.

A fase de Transformação de Dados ou Codificação dos Dados tem como objetivo principal converter o conjunto bruto de dados em uma forma padrão de uso.

Os dados de um atributo podem ser padronizados (normalizados) para cair dentro de uma faixa de valores, como por exemplo,1,0 a 1,0 ou 0,0 a 1,0.

A transformação de dados para o atributo idade poderia ocorrer da seguinte forma, {018} Faixa 1, {1925} Faixa 2, {2630} Faixa e assim por diante.

Nesse exemplo, os valores contínuos das idades foram discretizados em faixas.

Em resumo, essa fase converte os dados para a forma mais adequada para a construção e interpretação do modelo.

A Transformação de Dados é potencialmente a tarefa que requer grande habilidade no processo de KDD.

Tipicamente essa etapa exige a experiência do analista de dados e seu conhecimento nos dados em questão.

Embora o processo de KDD possa ser executado sem essa fase, nota-se que quando efetivada os resultados obtidos são mais intuitivos e valiosos, além de, na maioria das vezes, facilitarem a construção do modelo.

As vantagens de se codificar um atributo são melhorar a compreensão do conhecimento descoberto, reduzir o tempo de processamento para o algoritmo minerador, facilitar o algoritmo a tomar decisões globais, já que os valores dos atributos foram englobados em faixas.

Como desvantagem cita-se a redução da medida de qualidade de um conhecimento descoberto, perdendo-se assim detalhes relevantes sobre as informações extraídas.

O termo Data Mining surgiu devido às semelhanças entre a procura de informação importante para o mundo dos negócios (numa base de dados) e o ato de minerar a montanha para encontrar um veio de ouro.

Ambos os processos requerem ou a seleção de um imenso amontoado de material, ou um sondar inteligente desse material, para encontrar o sítio onde está o valor desejado.

A mineração de dados é considerada a etapa mais importante do processo de KDD.

Caracteriza-se pela existência do algoritmo minerador (Data Mining), que diante da tarefa especificada será capaz de extrair de modo eficiente conhecimento implícito e útil de um banco de dados.

Segundo Berry, Data Mining é a exploração e análise, por meios automáticos ou semi-automáticos, de grandes quantidades de dados para descobrir modelos e regras significativas, permitindo a uma empresa aumentar, por exemplo, suas operações de marketing, vendas e apoio aos clientes pela melhor compreensão da clientela.

Essas técnicas e ferramentas são igualmente aplicáveis em outras áreas do conhecimento.

De uma forma mais simples, Data Mining é produzir conhecimento novo escondido em grandes bases de dados.

O Data Mining usa técnicas baseadas em descobertas por meio de procura de padrões dos dados, o que é feito com o emprego de uma série de algoritmos inteligentes para encontrar relações fundamentais entre os dados.

Tais técnicas e algoritmos caracterizam-se por serem "automáticos", isto é, são de baixa interação com o ser humano, procurando restringir a participação humana aos processos de ajuste necessários aos mecanismos de busca.

De certo modo, enquanto as técnicas de busca tradicionais procuram respostas para perguntas realizadas, as técnicas de Data Mining permitem avaliar como as perguntas se relacionam com as respostas (padrões e relações) encontradas.

Esquema simplificado de Data Mining.

Esquema simplificado de Data Mining.

Achadas essas relações e padrões, é fornecida uma base de regras que servem de apoio aos processos de tomada de decisão.

Para tal, utilizam-se técnicas de procura baseadas em Inteligência Artificial, como as Redes Neurais Artificiais, as Árvores de Decisões, a Teoria dos Conjuntos Fuzzy, os Algoritmos Genéticos ou, ainda, combinações entre essas técnicas gerando os chamados sistemas híbridos (Arquiteturas Híbridas).

Pode-se encontrar um estudo sobre as técnicas de Data Mining.

O resultado obtido pela aplicação do Data Mining deve ser compacto, legível (apresentado de alguma forma simbólic, interpretável, e deve representar fielmente os dados que lhe deram origem).

Uma questão fundamental do Data Mining é quanto à qualidade do conhecimento extraído, levando em consideração, a precisão, a compreensibilidade, a surpresa e ou potencial interesse do conhecimento obtido.

É necessário verificar o que foi aprendido, o que há de novo, eliminando o conhecimento "inútil" e muito óbvio.

Exemplifica, se uma cadeia de lojas resolve buscar associações entre os itens que ela vende, certamente haverá associações entre itens que farão sentido, e outras que não farão.

Faz-se necessário, então, que um especialista dos negócios da empresa avalie quais são as associações realmente relevantes para a empresa.

Por último, as associações descobertas precisam ser acionáveis, isto é, é necessário que possam ser realizadas ações simples para que o conhecimento gerado seja traduzido em vantagem aos negócios da empresa.

O Data Mining serve para prever tendências e comportamentos futuros, permitindo a tomada de decisão baseada em fatos e não em suposições.

O Data Mining pode responder a questões de negócio que tradicionalmente demandariam muito tempo para resolver.

Ele explora as bases de dados à procura de padrões escondidos, encontrando dados que permitem prever tendências e comportamentos futuros, que os especialistas podem não descobrir devido ao fato de essa informação sair do limite de suas expectativas e possibilitando a tomada de decisão.

Esta fase também é conhecida como pós-processamento.

Após a fase de mineração de dados é necessária a interpretação do conhecimento descoberto, ou algum processamento desse conhecimento.

Essa interpretação deve ser incluída no algoritmo minerador, porém algumas vezes é vantajosa a implementação separadamente.

Em geral, a principal meta dessa fase é melhorar a compreensão do conhecimento descoberto pelo algoritmo minerador, validando-o através de medidas da qualidade da solução e da percepção de um analista de dados.

Esses conhecimentos serão consolidados em forma de relatórios demonstrativos com a documentação e explicação das informações relevantes ocorridas em cada etapa do processo de KDD.

Uma maneira genérica de obter a compreensão e interpretação dos resultados é utilizar técnicas de visualização.

Segundo Goldschmidt, as técnicas de visualização estimulam a percepção e a inteligência humana, aumentando a capacidade de entendimento e a associação de novos padrões.

Existem várias formas de interpretação dos dados pelo KDD denominadas tarefas.

As tarefas mais comuns são, associação, classificação, clusterização (agrupamento) e visualização.

Também chamada de Análise de Cestas de Mercado (Market Basket Analysis).

A idéia principal da associação é identificar grupos de itens tipicamente associados (o que vai com o quê).

Dada uma coleção de itens e um conjunto de registros, cada qual contendo um número de itens da coleção, através de técnicas de associação realizam-se operações sobre o conjunto de registros retornando afinidades entre a coleção de itens.

O objetivo da técnica é encontrar tendências, a partir de grande número de transações que possam ser usadas para entender e explorar padrões de compra.

Regras facilmente encontradas assumiriam a forma "80% das vendas de cerveja também correspondem à venda de batatas fritas", ou "30% das vendas de fornos elétricos também correspondem à venda de luvas térmicas" ou, ainda, quando se compram batatas fritas, 65% das vezes também é adquirida coca-cola, a não ser que haja uma promoção, e nesse caso a coca-cola é campeã em 85% das vezes.

Uma vez encontradas as regras, estas poderiam ser usadas de maneiras diversas, de acordo com o tipo de atividade analisada.

Poderiam ser realizadas mudanças na disposição de produtos, na determinação de políticas de formação de estoques ou de pedidos de compra, na promoção de campanhas direcionadas de vendas, no estabelecimento de práticas comuns (ou incomuns) de vendas.

Na área financeira poderiam ser avaliados serviços que freqüentemente são adquiridos em conjunto.

As técnicas de classificação criam automaticamente um modelo a partir de um conjunto inicial de registros, que serve de exemplo e é chamado de conjunto de treinamento.

Os registros do conjunto de treinamento devem pertencer a um pequeno grupo de classes pré-definidas (pelo analist).

O modelo é composto de padrões, essencialmente generalizações em relação aos registros, os quais são usados para diferenciar as classes.

Uma vez obtido o modelo, este é usado para classificar automaticamente os demais registros.

A classificação pode ser utilizada com êxito, por exemplo, para aplicações típicas de cartões de crédito.

Dada uma base de dados dos usuários de cartões e tomando-se como exemplo aqueles cujo histórico de crédito é conhecido, atribui-se a estes um grau de risco de acordo com seu histórico (alto, médio ou baixo).

Desse modo, por meio das técnicas de classificação, podem ser obtidas regras para a caracterização do grau de risco de um usuário tendo em vista dados como a faixa de renda familiar, a idade, a área de moradia (por exemplo, usuários de 30 a 50 anos, com renda familiar superior a 20000 reais anuais, moradores da região X apresentam baixo risco).

De maneira análoga, numa aplicação de detecção de fraudes seriam obtidos registros de operações fraudulentas e não-fraudulentas.

Posteriormente seriam estabelecidos os atributos desses registros, que tipificariam uma determinada operação como fraudulenta ou não, permitindo a classificação dos demais registros.

O método se ajusta a uma vasta gama de situações, tais como concessão de empréstimos, detecção de sonegação fiscal etc.

A clusterização transforma registros com grande número de atributos em conjuntos relativamente menores.

Essa transformação é realizada, automaticamente, por meio de identificação das características que distinguem o conjunto de dados e pelo seu posterior particionamento.

Não é necessário identificar os agrupamentos desejados nem os atributos que devem ser utilizados para a produção dos segmentos.

O objetivo nessa tarefa é maximizar similaridade intra-cluster e minimizar similaridade inter-cluster.

Os resultados de uma operação de clusterização podem ser usados de duas diferentes maneiras.

Ora para produzir um sumário da base de dados por meio das características de cada cluster, ora como dados de entrada para outras técnicas, por exemplo, classificação.

A clusterização pode ser usada em casos que façam uso de modelos de segmentação de população, tais como segmentação demográfica de mercados de consumidores (identificar grupos homogêneos de elementos, identificar elementos dentro do mesmo grupo maximamente semelhantes) implicando possível comparação dos hábitos de consumo de múltiplos segmentos de população, visando determinar campanhas de vendas.

A clusterização será tratada com mais profundidade no capítulo 5.

As ferramentas de visualização não são propriamente tarefas de Data Mining, mas sim meios de analisar e observar os dados de uma determinada base de dados.

A visualização fornece meios de obter sumários visuais dos dados de uma base de dados.

No caso de técnicas de clusterização, podem ser usadas ferramentas de visualização para determinar qual ou quais clusters criados são úteis ou interessantes para os métodos de Data Mining.

No caso específico da rede SOM, as formas de visualização mais utilizadas são a Matriz-U e o Mapa por Similaridade de Cor, conforme detalhadas no capítulo 5, seção 59.

As ferramentas de visualização podem ainda ser usadas como um mecanismo de compreensão da informação extraída por meio das técnicas de Data Mining.

Características difíceis de detectar pela simples observação de linhas e colunas com valores numéricos podem se tornar óbvias se forem observadas graficamente.

Por meio dessas ferramentas podem ser encontrados características ou fenômenos pouco comuns ou interessantes sem que se esteja diretamente procurando por eles.

No capítulo 5 serão aprofundados estudos sobre as formas de visualização da rede SOM.

Pode-se aplicar o Data Mining em diversas áreas, como, É crença corrente no comércio em geral que a maior arma mercadológica é compreender as necessidades individuais de cada cliente e atender a essas necessidades.

Com essa finalidade, um número sempre crescente de companhias procura aplicar técnicas de Data Mining que identifiquem as preferências do consumidor e seus padrões de compra.

Tais informações podem ser usadas para direcionar campanhas de vendas, promover oferta combinada de serviços ou produtos, avaliar o comportamento do mercado e para detectar novas tendências mercadológicas ou necessidades de consumo também podem auxiliar na formação de estoques, estabelecer disposições mais eficientes de produtos nos balcões ou setores de um ponto de venda (visando estimular o consumo), dentre uma vasta gama de outras possibilidades.

Em Ebecken pode-se verificar diversos trabalhos na área de tomada de decisões empresariais utilizando KDD.

Podem ser criados mecanismos de detecção de fraudes, descoberta de sonegação de impostos, avaliação de atividades censitárias, acompanhamento de oscilações em atividades econômicas sob a ótica de variações de políticas cambiais, de incentivo fiscal, modificação de taxas e índices e aplicações na área policial, dentre outras.

Áreas com possíveis aplicações seriam pesquisa médica, teste de novos medicamentos ou associação de medicamentos diversos, pesquisa subatômica, descoberta de galáxias e outros corpos celestes a partir de imagens capturadas por telescópios.

Há aplicações em áreas como prevenção e detecção de fraudes, possíveis aplicações em concessão de crédito, descoberta de áreas seguras para investimentos, serviços adquiridos em diferentes áreas geográficas ou por diferentes segmentos populacionais.

Este capítulo procurou descrever todo o processo de KDD através da análise de suas fases, tarefas, técnicas e aplicações.

Essa descrição contribuiu para justificar o desenvolvimento deste trabalho e a finalidade da Arquitetura Híbrida proposta, que é a descoberta de conhecimento.

Os experimentos realizados no capítulo 6 descrevem em contextos concretos todo o processo de KDD, com especial referência ao experimento de número (seção 634).

Nos próximos dois capítulos deste trabalho inicia-se o estudo das técnicas escolhidas para compor a Arquitetura Híbrida.

No capítulo será estudado o RS e no capítulo 5 a rede SOM.

Neste capítulo será feita uma discussão da Teoria dos Rough Sets (RS), também denominada Teoria dos Conjuntos Aproximados (TC, com o objetivo de apresentar os principais conceitos, a motivação do estudo, bem como exemplos de aplicação da Teoria).

A Teoria dos Rough Sets (RS) foi proposta por Zdzislaw Pawlak em 198como um novo modelo matemático para representação do conhecimento e tratamento de incerteza, tendo sido usada, posteriormente, para o desenvolvimento de técnicas para classificação aproximada em aprendizado de máquina.

Devido a essas características, tem-se utilizado essa teoria em Inteligência Artificial, especialmente nas áreas de aquisição de conhecimento, raciocínio indutivo e descoberta de conhecimento em base de dados.

Conjuntos aproximados podem ser considerados conjuntos com fronteiras nebulosas, ou seja, conjuntos que não podem ser caracterizados precisamente, utilizando-se dos atributos disponíveis.

A incerteza pode se manifestar de diversas formas, como, imprecisão, incompletude, inconsistência, etc.

RS trata de um tipo fundamental de incerteza, a indiscernibilidade.

A indiscernibilidade surge quando não é possível distinguir elementos de um mesmo conjunto, e representa a situação em que esses elementos parecem todos ser um único elemento.

Os conceitos de RS têm se mostrado muito úteis quando aplicados a problemas do tipo, redução de atributos, descoberta de dependência entre atributos e na descoberta de padrões entre os dados.

A redução de atributos realizada pelos RS é feita através dos chamados redutos, que são subconjuntos de atributos capazes de representar o conhecimento da base de dados com todos os seus atributos iniciais.

Este procedimento de eliminação de atributos irrelevantes é uma das características da Teoria.

Devido às características comentadas, a crescente utilização de RS pode ser comprovada pelo número de aplicações e publicações científicas nas seguintes áreas, KDD e Data Mining, medicina, engenharia, reconhecimento de padrões, Text Mining, dentre outros.

Os principais conceitos dos RS são Espaços Aproximados, Aproximação Inferior (AI), Aproximação Superior (AS), Sistema de Informação (S), Sistema de Decisão e Indiscernibilidade (IN).

Este trabalho não tem como finalidade o aprofundamento no formalismo matemático dos RS, que é grande e considerado um aspecto importante da Teoria.

Para isto recomenda-se os trabalhos de Uchôa.

Este formalismo matemático não é sistematicamente abordado, não é padronizado e tampouco muito explorado.

Uma das razões para isso se deve ao fato dos RS ser uma teoria razoavelmente recente.

Ainda não existe uma padronização da notação matemática, assim, em alguns casos foi necessário adotar uma notação própria, com o objetivo de torná-la mais clara, como, Aproximação Inferior (AI) B, Aproximação Superior (AS) B, Sistema de Informação (S), atributos condicionais (, atributo de decisão (, Sistema de Decisão, Região de Fronteira RF(X), Região Negativa RN(X) e Reduto (RE).

Deve-se entender também que a palavra elementos é tratada como sinônimo de casos, exemplos ou registros que compõem uma base de dados.

Um espaço aproximado é um par ordenado A = (U,R), onde, U é um conjunto não vazio, denominado conjunto universo, e R é uma relação de equivalência sobre U, denominada Relação de Indiscernibilidade.

Uma relação binária R X × X, a qual é reflexiva (um elemento está relacionado com ele próprio xRx), simétrica (se xRy então yRx) e transitiva, é chamada de relação de equivalência.

Dados os elementos x, y U, se xRy então x e y são indiscerníveis em A, ou seja, a classe de equivalência definida por x é a mesma que a definida por y, R = R.

A classe de equivalência de um elemento x X consiste de todos os elementos y X para os quais xRy.

Os elementos que são indiscerníveis formam conjuntos chamados de conjuntos elementares.

Dessa forma, pode-se dizer que as classes de equivalência de R são os conjuntos elementares de A.

Pode-se visualizar o espaço aproximado.

Espaço Aproximado.

A forma mais comum para representação dos dados em RS é através de um sistema de informação (S) que contém um conjunto de elementos, sendo que cada elemento tem uma quantidade de atributos condicionais.

Esses atributos são os mesmos para cada um dos elementos, mas os seus valores nominais podem diferir.

Dessa forma, um sistema de informação é um par ordenado, onde U é um conjunto finito e não-vazio de elementos chamado de universo, e C é um conjunto finito e não-vazio formado pelos atributos.

Cada atributo a C é uma função a, U Va, onde Va é o conjunto dos valores permitidos para o atributo a.

É apresentado o sistema de informação, pode-se observar os principais conceitos de RS, o espaço aproximado, o universo U formado pelos elementos e1

Experiência do Vendedor, Qualidade do Produto e Boa Localização e R a relação de equivalência sobre U.

Exemplo de um Sistema de Informação (S).

O principal conceito envolvido em RS é a Relação de Indiscernibilidade, a qual normalmente está associada a um conjunto de atributos.

Se tal relação existe entre dois elementos, isso significa que todos os valores nominais dos seus atributos são idênticos com respeito aos atributos considerados, portanto não podem ser discernidos (distinguidos) entre si.

Para cada subconjunto de atributos B C no sistema de informação, uma relação de equivalência INDsé associada, chamada de Relação de Indiscernibilidade definida como, O conjunto de todas as classes de equivalência na relação INDsé representado por U/INDs(, denominado quociente de U pela relação INDs().

Em muitos casos é importante a classificação dos elementos considerando um atributo de decisão que informa a decisão a ser tomada.

Dessa forma, um SI que apresenta um atributo de decisão é denominado Sistema de Decisão.

SD obtido a partir do sistema de informação S da tabela, destacando os atributos condicionais (Experiência do Vendedor, Qualidade do Produto e Boa Localização) e o atributo de decisão (Retorno).

Sistema de Decisão (Sistema de Informação com o atributo de decisão Retorno) Os valores dos atributos são chamados de valores nominais e estão expressos como, Experiência do Vendedor {Alta, Média, Baixa}

Qualidade do Produto {Boa, Média}

Boa Localização {Não, Sim} e Retorno {Lucro, Prejuízo}.

Considerando cada atributo condicional de forma independente, a relação de equivalência do sistema de informação S forma os seguintes conjuntos elementares, experiência do vendedor Alta {e1, e6}

Média {e2, e3, e5}

Baixa {e4}

Qualidade do Produto, Boa {e1, e2, e3}

Média {e4, e5, e6} e Boa Localização, Não {e1, e2, e3, e4}

Sim {e5, e6}.

Ao utilizar todos os atributos condicionais do sistema de informação S da btêm-se os seguintes conjuntos elementares, {e1}, {e2, e3}, {e4}, {e5} e {e6}.

Observando a tabela, pode-se perceber que existem elementos (casos) {e2} e {e3} iguais (destacados em negrito), no que se refere a valores de atributos condicionais.

Sistema de Decisão com os elementos ee eindiscerníveis, com relação aos atributos condicionais.

Existindo a Relação de Indiscernibilidade entre os elementos {e2} e {e3} como mostrado na significa que todos os valores nominais de seus atributos são idênticos com relação ao subconjunto de atributos B (B S) considerado, ou seja, não podem ser diferenciados entre si.

A redução de atributos será tratada com maior profundidade na seção 45 deste capítulo.

A presenta os elementos do sistema de informação S segundo as características do atributo de decisão.

Pode-se então, fazer a seguinte pergunta, quais características dos atributos condicionais definem o retorno da loja como lucro ou prejuízo Note-se que não há uma resposta única para esta pergunta, pois as lojas {e2} e {e3} apresentam as mesmas características dos atributos condicionais, mas se diferenciam no atributo de decisão.

Pode-se dizer com certeza, conforme a tabela, que qualquer loja com características iguais às das lojas {e1} e {e6} terão lucro, assim como qualquer loja com características iguais às das lojas {e4} ou {e5} terá prejuízo, porém, nada se pode afirmar para lojas com características iguais às das lojas {e2} e {e3}, pois, apesar de apresentarem atributos condicionais com as mesmas características, possuem atributos de decisão diferentes.

São nesses casos que RS pode ser aplicado.

Um conjunto definível em S é qualquer união finita de conjuntos elementares.

Para cada conceito X que é o conjunto de elementos com respeito a B, ou seja, X é obtido através das informações dos atributos de B, são computados o maior conjunto definível contido em X e o menor conjunto definível que contém X.

O primeiro conjunto é chamado de Aproximação Inferior (AI) de X, enquanto o segundo conjunto é chamado de Aproximação Superior (AS) de X.

A Aproximação Inferior B (X) e a Aproximação Superior B (X) de um conjunto de elementoss X U com respeito a um conjunto de atributos B S pode ser definida em termos das classes na relação de equivalência, da seguinte forma, Os elementos da Aproximação Inferior B (X) são classificados com certeza como membros de X, utilizando o conjunto de atributos B, enquanto os elementos da Aproximação Superior B (X) podem ser classificados como possíveis membros de X, utilizando o mesmo conjunto de B.

Os elementos que correspondem a lucro podem ser tomados como exemplos do conceito X.

Com base na ode-se observar que existem três elementos que possuem como atributo de decisão lucro {e1, e3, e6}, porém, existe Relação de Indiscernibilidade entre os elementos {e2} e {e3}, impedindo que {e3} seja considerado com certeza como membro de X.

Assim, somente os elementos {e1, e6} podem ser classificados como membros de X e elementos da Aproximação Inferior B (X).

Aproximação Inferior B (X) destacada na cor cinza.

Aproximação Inferior de X.

Nos quadrados de cor cinza, estão contidos os elementos que correspondem a Aproximação Inferior, os quadrados em branco tocados pela _ elipse (X) estão os elementos que correspondem a Aproximação Superior B (X) e os quadrados em branco que não são tocados pela elipse correspondem com certeza ao elementos que não pertencem a B (X) (Região Negativ).

Como já foi dito, na Aproximação Superior B (X) são classificados os elementos que são possíveis membros de X.

Dessa forma, a Aproximação Superior reúne os elementos com atributo de decisão igual a lucro {e1, e3, e6} e também os elementos que possuem atributo de decisão igual a prejuízo, desde que exista uma Relação de Indiscernibilidade entre os elementos, como ocorre entre o elemento {e2} e o elemento {e3}.

Aproximação Superior B (X).

Aproximação Superior de X.

A Região de Fronteira, também chamada de Duvidosa, possui somente os elementos que não podem ser classificados com certeza como pertencentes em X, utilizando o conjunto de atributos B.

É a região formada pelos elementos de U que pertencem à Aproximação Superior, mas que não pertencem à Aproximação Inferior.

Um conjunto X é definido como rough (impreciso) se a sua Região de Fronteira é diferente do conjunto vazio (RF(X)0), e é definido como crisp (preciso) se o conjunto for vazio (RF(X)=0).

Pode-se observar que os elementos {e2, e3} fazem parte da Região de Fronteira.

A Região Negativa é dada pela diferença dos elementos.

A Região Negativa possui somente os elementos que com certeza não podem ser classificados como pertencentes à ilustra todas as regiões de X em A.

Todas as regiões de X em A.

Através dos conceitos de AS e AI, podem-se definir as quatro classes básicas de incerteza em RS.

Um conjunto definível em S é qualquer união finita de conjuntos elementares.

O significado dessas classes, ondeX denota U X (complemento) é o seguinte, X é rough B-definível, se e somente se B (X) e B (X) U.

Se X é rough B-definível, isso significa que é possível decidir para alguns elementos de U quando eles pertencem a X e para alguns elementos de U quando eles pertencem aX, utilizando B

X é internamente B-indefinível, se e somente se B (X) = e B (X) U.

Se X é internamente B-indefinível, isso significa que é possível decidir para alguns elementos de U quando eles pertencem aX, mas não é possível decidir para nenhum elemento de U quando ele pertence a X, utilizando B

X é externamente B-indefinível, se e somente se B (X) e B (X) = U.

Se X é externamente B-indefinível, isso significa que é possível decidir para alguns elementos de U quando eles pertencem a X, mas não é possível decidir para qualquer elemento de U quando ele pertence a X, utilizando B

X é totalmente B-indefinível, se e somente se B (X) = e B (X) = U.

Se X é totalmente B-indefinível, isso significa que não é possível decidir para qualquer elemento de U quando ele pertence a X ou aX, utilizando B.

A Qualidade das Aproximações obtidas pelas definições dadas previamente pode ser caracterizada numericamente a partir dos próprios elementos que as definem.

O Coeficiente para medir essas Qualidades é representado por B (X), sendo X o conjunto de elementos com respeito a B, e podem ser realizadas de três formas, Coeficiente de Incerteza

Coeficiente de Qualidade da Aproximação Superior

Coeficiente de Qualidade da Aproximação Inferior.

As três medidas foram calculadas nos exemplos a seguir, O Coeficiente de Incerteza B(X), pode ser entendido como a Qualidade da Aproximação de X, dado por, em que em que | B (X)| e | B (X)| denotam a cardinalidade das Aproximações Inferior e Superior, respectivamente.

Obviamente 0 B 1.

Se B (X) = 1, X é crisp (preciso) em relação ao conjunto de atributos B.

Se B (X) < 1, X é rough (impreciso), em relação ao conjunto de atributos B.

O coeficiente B(X) pode ser interpretado como o percentual de todos os elementos possivelmente classificados como pertencentes a X, dado por, ou seja, com base nos elementos da tabela, 50% de X é preciso com respeito a B.

Coeficiente de Qualidade da Aproximação Superior.

B B (X)), que pode ser interpretado como sendo o percentual de todos os elementos possivelmente classificados como perte ou seja, com base nos elementos da tabela, 66% de U possivelmente pertence.

Coeficiente de Qualidade da Aproximação Inferior B(B(X)), que pode ser interpretado como sendo o percentual de todos os elementos certamente classificados como pertencentes a X, dado por, RS pode ser utilizado para a análise de dependência entre atributos, visando principalmente a identificação e eliminação de atributos redundantes ou desnecessários.

A eliminação de atributos redundantes permite que se encontre um subconjunto mínimo de atributos que possui o mesmo valor discriminatório do conjunto de atributos original.

Um conjunto de atributos D (decisão) depende totalmente de C (condicionais), se todos os valores nominais de D forem univocamente determinados por valores nominais dos atributos de C.

Em outras palavras, D depende totalmente de C se existir uma dependência funcional entre valores nominais de C e D.

Se (C, = 1, diz-se que D depende totalmente de C, se (C, = 0, D não depende de C e se 0 < (C, < 1, D depende parcialmente de C).

Formalmente, a dependência pode ser assim definida, sejam C e D subconjuntos de S.

O Grau de Dependência de D em relação a C é dado por, Quando D depende parcialmente de C indica que não existe a necessidade da presença de todos os atributos condicionais de C para gerar os valores nominais do atributo de decisão D, abrindo espaço para uma redução de atributos.

Do resultado de 0,3(33%) deduz-se que dos atributos condicionais pertencentes ao sistema de informação S, um deles (33%) pode ser reduzido sem que a base de dados perca a sua representatividade original e os outros dois (66%) apresentam dependência, e por isso não podem ser reduzidos.

A redução da base de dados 41 será apresentada na seção Redução de Atributos (45).

Como foi visto na seção 43, alguns atributos são mais significativos (relevantes) que outros impossibilitando a sua redução.

Sejam C e D conjuntos de atributos condicionais e de decisão, respectivamente, e seja a um atributo condicional pertencente a C.

A Significância de a será calculada em função da mudança do Grau de Dependência de D em relação a C (0,6com a remoção de a, segundo a fórmula abaixo, Este Coeficiente pode ser visto como o erro que ocorre na definição de D por C quando a é removido).

Com base na utilizando os elementos com atributo de decisão igual a Lucro, temos, para X = {e1, e3, e6}, D = {Retorno}, C = {Experiência do Vendedor, Qualidade do Produto, Boa Localização}, o Grau de Significância do atributo a é dado por 1 (({Experiência do Vendedor, Qualidade do Produto})/({Experiência do Vendedor, Qualidade do Produto, Boa Localização})) = 1 (2(2)) = 0, significando que remover o atributo Boa Localização não afetará os resultados.

O mesmo resultado seria encontrado ao se remover Qualidade do Produto de {Experiência do Vendedor, Qualidade do Produto, Boa Localização}.

A redução de atributos em RS é feita através dos chamados Redutos (RE, que são subconjuntos de atributos capazes de representar o conhecimento da base de dados com todos os seus atributos iniciais).

Um Reduto de B sobre um sistema de informação S é um conjunto de atributos B' B tal que todos os atributos a (B B') são dispensáveis.

Com isso, U/INDs(B') = U/INDs().

O termo REDé utilizado para denotar a família de redutos de B.

O procedimento para a redução de atributos é o seguinte, comparam-se os conjuntos elementares de cada atributo individualmente e depois dois a dois, três a três, assim sucessivamente com os conjuntos elementares formados com todos os atributos.

Nesta comparação, quando um atributo apresentar os seus conjuntos elementares iguais aos conjuntos elementares formados com todos os atributos, este atributo pode ser reduzido.

O cálculo de reduções para gerar os redutos é um problema n-p completo, e seu processamento em grandes bases de dados exige grande esforço computacional.

Desta forma, comparando cada atributo individualmente (os conjuntos elementares são formados pelo valor nominal do atributo de decisão) obtêm-se, Experiência do Vendedor, Lucro {e1, e3, e6}

Prejuízo {e2, e4, e5}

Qualidade do Produto, Lucro {e1, e3, e6}

Prejuízo {e2, e4, e5}

Boa Localização, Lucro {e1, e3, e6}

Prejuízo {e2, e4, e5}.

Nenhum atributo teve os seus conjuntos elementares iguais aos conjuntos elementares formados com todos os atributos, portanto, neste momento não houve redução.

Comparando os atributos, Experiência do Vendedor e Qualidade do Produto = Alta e Boa {e1}

Média e Boa {e2, e3}

Baixa e Média {e4}

Média e Média {e5}

Alta e Média {e6}.

O resultado foi {e1} {e2, e3} {e4} {e5} {e6}.

Os conjuntos elementares obtidos são iguais aos conjuntos elementares obtidos com todos os atributos, portanto, é possível reduzir um atributo.

Comparando os atributos, Experiência do Vendedor e Boa Localização = Alta e Não {e1}

Média e Não {e2, e3}

Baixa e Não {e4}

Média e Sim {e5}

Alta e Sim {e6}.

O resultado dos conjuntos elementares foi, {e1} {e2, e3} {e4} {e5} {e6}.

Os conjuntos elementares obtidos são iguais aos conjuntos elementares obtidos com todos os atributos, portanto, existe mais de uma redução de atributo.

Comparando os atributos, Qualidade Produzida e Boa Localização = Boa e Não {e1, e2, e3}

Média e Não {e4}

Média e Sim {e5, e6} O Resultado dos conjuntos elementares foi, {e1, e2, e3}

Os atributos {e4} e {e5, e6}, mostrando-se diferente dos conjuntos elementares obtidos com todos os atributos e, portanto, não sendo possível nenhuma redução.

Chega-se à conclusão de que existem dois redutos RED(, {Experiência do Vendedor, Qualidade do Produto} e {Experiência do Vendedor, Boa Localização}, ou seja, utilizando qualquer um dos dois redutos consegue-se representar a base de dados com todos os seus atributos originais).

Pode-se visualizar na tabela, o sistema de informação S sem a presença do atributo reduzido Boa Localização.

Sistema de Informação S sem o atributo Boa Localização.

Pode-se visualizar na tabela, o sistema de informação S sem a presença do atributo reduzido Qualidade do Produto.

Sistema de Informação S sem o atributo Qualidade do Produto.

Na seção 45 foi mostrado como se realiza o procedimento de redução de atributos.

Essa redução é feita pela função de discernibilidade, a partir da Matriz de Discernibilidade.

Considerando o conjunto de atributos B = {Experiência do Vendedor, Qualidade do Produto e Boa Localização} para o sistema de informação S, o conjunto de todas as classes de equivalência determinadas por B sobre S é dado por U/INDs= {{e1} {e2, e3} {e4} {e5} {e6}}, que estão representadas na tabela.

A Matriz de Discernibilidade do sistema de informação S, denotada por MD(, é uma matriz simétrica n x n com, mD = {a B a a} para i,j = 1, 2,n sendo 1 i, jn e n=U / INDs().

Logo, os elementos da matriz de discernibilidade mD é o conjunto de atributos condicionais de B que diferenciam os elementos das classes com relação aos seus valores nominais.

Considerando Experiência do Vendedor, Qualidade do Produto (QP) e Boa Localização (BL), com a finalidade de construir a Matriz de Discernibilidade MD(, tem-se na sua representação, Matriz de Discernibilidade).

A função de discernibilidade Fsé uma função booleana com m variáveis, que determina o conjunto mínimo de atributos necessários para diferenciar qualquer classe de equivalência das demais, definida como, Utilizando o método de simplificação de expressões booleanas na função Fs(, obtém-se o conjunto de todos os implicantes primos dessa função, o qual determina os redutos de S).

A simplificação é um processo de manipulação algébrica das funções lógicas com a finalidade de reduzir o número de variáveis e de operações necessárias para a sua realização.

A função de discernibilidade Fsé obtida da seguinte forma, para os atributos contidos dentro de cada célula da Matriz de Discernibilidade MD(, aplica-se o operador "soma", "or" ou "" e, entre as células dessa matriz, utiliza-se o operador "produto", "and" ou "", resultando em uma expressão booleana de "Produto da Soma").

A Fsda representada por, Simplificando esta expressão, utilizando teoremas, propriedades e postulados da Álgebra Booleana, obtém-se a seguinte expressão minimizada, Fs= (EV (QP BL)), que ainda pode ser escrita na forma de "Soma do Produto", ou seja Fs= (EV (QP BL)).

Confirmando o que já foi verificado na seção 45, os redutos são RED= {Experiência do Vendedor, Qualidade do Produto } e {Experiência do Vendedor, Boa Localização}.

A função de discernibilidade determinou o termo mínimo da função, ou seja, determinou o conjunto mínimo de atributos necessários para discernir as classes formadas por todas as classes de equivalência da relação INDs.

Estudos mais aprofundados sobre Matriz de Discernibilidade podem ser encontrados em Bozi, Felix, Fujimori e Slowinski.

Reduzir atributos é a função de RS na Arquitetura Híbrida proposta.

Entretanto, RS pode gerar regras de decisão para serem usadas em classificação.

A seguir estão descritas, em caráter ilustrativo, algumas regras geradas pelo RS a partir dos conceitos apresentados neste capítulo, Se Experiência do Vendedor = Alta e Qualidade do Produto = Boa então Retorno = Lucro.

Se Experiência do Vendedor = Média e Qualidade do Produto = Boa então Retorno = Lucro.

Se Experiência do Vendedor = Baixa e Qualidade do Produto = Média então Retorno = Prejuízo.

Se Experiência do Vendedor = Alta e Qualidade do Produto = Média então Retorno = Lucro.

Em RS, a credibilidade de cada regra gerada é associada a um Fator de Credibilidade (F através de uma função de pertinência).

Com isso, uma regra consistente receberá um Fator de Credibilidade igual a 1, enquanto regras não tão consistentes receberão um Fator de Credibilidade menor que 1.

Quanto mais próximo de 1 é o Fator de Credibilidade, maior é a credibilidade da regra.

Maior aprofundamento no assunto regras de decisão pode ser obtido em Pawlak.

Neste capítulo foram abordados os principais conceitos dos RS e suas aplicações no tratamento de um tipo fundamental de incerteza, a Indiscernibilidade.

O estudo da redução de dados contribui para a fundamentação e o entendimento dos RS na Arquitetura Híbrida proposta.

Desde o aparecimento de RS, muitos trabalhos vêm sendo gerados para conhecer melhor a Teoria e suas aplicações.
Como, utilização dos conceitos de Aproximações Superior e de Aproximação Inferior para aproximar atributos desconhecidos com base em atributos conhecidos, elaboração de relações e operações sobre conjuntos rough (imprecisos), similares àquelas utilizadas em conjuntos clássicos, medidas de incerteza baseadas em teoria da informação e aplicação de RS em modelos de bancos de dados relacionais.

O próximo capítulo trata das redes neurais artificiais e, mais especificamente, da rede SOM, que é a outra técnica combinada ao RS para formar a Arquitetura Híbrida proposta.

Este capítulo apresenta o histórico e o conceitual básico de redes neurais artificiais, enfatizando a rede SOM, que é a arquitetura de rede neural artificial utilizada neste trabalho.

Os modelos neurais artificiais procuram aproximar o processamento dos computadores ao cérebro humano.

A partir do momento em que as máquinas começaram a evoluir, um grande desejo do homem tem sido a criação de uma máquina que possa operar independentemente do controle humano, cuja independência seja desenvolvida de acordo com seu próprio aprendizado e que tenha a capacidade de interagir com ambientes incertos (desconhecidos por el).

Uma máquina que possa ser chamada de autônoma e inteligente ou cognitiva.

O sucesso dessa máquina autônoma e inteligente dependeria única e exclusivamente da sua capacidade de lidar com uma variedade de eventos inesperados no ambiente em que opera.

Essas máquinas teriam maior capacidade de aprender tarefas de alto nível cognitivo que não são facilmente manipuladas por máquinas atuais e continuariam a se adaptar e realizar tais tarefas gradativamente com maior eficiência, mesmo em condições de ambiente imprevisíveis.

Então, seriam muito úteis naqueles casos em que a interação humana é perigosa, tediosa ou impossível, como em reatores nucleares, combate ao fogo, operações militares, exploração do espaço a distâncias em que a nave espacial estaria fora do alcance do controle na terra, porém enviando informações.

O funcionamento dos computadores é feito de modo seqüencial, proporcionando maior eficiência na resolução de tarefas que devem ser executadas seguindo determinadas etapas.

Já o cérebro humano funciona de modo paralelo, e sendo extremamente conectado é mais eficiente na resolução de tarefas que exigem a utilização de diversas variáveis.

Assim, podemos dizer que as redes neurais artificiais são redes inspiradas na estrutura do cérebro humano, com o objetivo de apresentar características similares ao comportamento humano, tais como, aprendizado, associação, generalização e abstração.

Uma rede neural artificial é um processador maciçamente paralelo, distribuído, constituído de unidades de processamento simples que têm capacidade para armazenar conhecimento experimental e torná-lo disponível para uso.

As pesquisas sobre redes neurais artificiais tiveram início em 194com Warren Mcculloch e Walter Pitts.

A estrutura proposta abstraía a complexidade da atividade neural em sistemas neurais reais e estabeleceu a base da neurocomputação, concebendo procedimentos matemáticos análogos ao funcionamento dos neurônios biológicos.

A contribuição de Warren Mcculloch e Walter Pitts foi puramente conceitual, buscando mais descrever um modelo artificial de um neurônio e apresentar suas capacidades computacionais do que apresentar técnicas de aprendizado, sendo tomada como base para a maioria dos modelos conexionistas desenvolvidos posteriormente.

Em 1949 Donald Hebb publica The Organization of Behavior e dá um passo muito importante na história das redes neurais artificiais ao propor um modelo segundo o qual se assumiu que a aprendizagem do conhecimento representado em uma rede neural seja alcançada pelo fortalecimento das conexões entre os neurônios adjacentes (vizinhos, próximos), sempre que estes estiverem excitados.

Em 1958, Frank Rosenblatt criou o perceptron, um modelo cognitivo que consistia de unidades sensoriais conectadas a uma única camada de neurônios de Mcculloch e Pitts.

Assim, Rosenblatt demonstrou que, se acrescidas sinapses ajustáveis às redes neurais artificiais de Mccullogh e Pitts, estas poderiam ser treinadas para classificar padrões de classes linearmente separáveis.

Com base no modelo de Mcculloch e Pitts, no início da década de 1960 Widrow e Hoff publicaram um artigo sobre o neurônio artificial, denominado Adaline (ADAptive LINear Element).

A contribuição desse trabalho está associada à regra de aprendizagem proposta, a regra Delta.

Em 1969 Minsky e Papert, na publicação Perceptrons, expuseram as limitações do modelo de Rosenblatt, provando que tais redes não são capazes de resolver uma ampla gama de problemas devido às restrições de representação.

Nessa época, pensava-se que para reproduzir o comportamento do cérebro humano bastaria construir uma rede neural suficientemente grande.

Uma rígida análise matemática, no entanto, comprovou o pouco poder computacional dos modelos de redes neurais artificiais utilizados na época, levando as pesquisas neste campo a ficarem esquecidas de meados dos anos 60 até o início da década de 80 do século passado.

Nos início dos anos 80, o interesse pela área retornou devido, em grande parte, ao surgimento de novos modelos neurais, como os propostos por Hopfield e por Teuvo Kohonen.

Além disso, nessa mesma época, ocorreu o desenvolvimento de computadores mais rápidos e poderosos, facilitando a implementação das redes neurais artificiais.

O modelo conexionista (baseado em redes neurais) proposto por John Hopfield em 198permitiu, pelas suas características computacionais e estabilidade, esclarecer boa parte das dúvidas até então existentes em relação ao processo dinâmico executado por certas redes neurais artificiais.

Em 198Teuvo Kohonen publica um artigo descrevendo a rede neural artificial baseada em auto-organização e nas características de aprendizado adaptativo do cérebro humano.

Cerca de dois anos mais tarde, Rumelhart, Hinton e Williams aperfeiçoaram a idéia de perceptron, criando o algoritmo de retropropagação do erro (error back-propagation), levando a uma explosão de interesse em redes neurais artificiais.

O sucesso deste algoritmo estimulou o desenvolvimento de muitas pesquisas em redes neurais artificiais e de uma ampla variedade de modelos cognitivos.

Comparação entre o neurônio biológico e o neurônio artificial.

Pode-se perceber a semelhança entre as duas estruturas.

Analogamente, as entradas de um neurônio artificial representam os dendritos do neurônio biológico, a junção aditiva no neurônio artificial representa o corpo celular no neurônio biológico e a saída do neurônio artificial representa o axônio no neurônio biológico.

Comparação entre o neurônio biológico e o neurônio artificial Em suma, uma rede neural artificial é uma tradução simplificada do funcionamento do cérebro humano e do sistema nervoso, a partir de algoritmos.

As redes neurais artificiais caracterizam-se por possuírem, elevado número de elementos de processamento simples inspirados no funcionamento do neurônio biológico, conexões entre os elementos de processamento em cada conexão um peso associado, que representa quão forte é a interação ou acoplamento entre os elementos de processamento e se a sua natureza é excitatória ou inibitória.

Por apresentar certas características vantajosas (descritas no capítulo 1), as redes neurais artificiais vêm sendo utilizadas em diversas aplicações, como, reconhecimento de padrões, reconhecimento e classificação de imagens (textos, assinaturas, impressões digitais e objetos), previsão de séries temporais, monitoramento e controle, diagnóstico médico em áreas como Economia, Finanças, Marketing e como técnicas do processo de KDD.

As RNAs são muito utilizadas na área de negócios por organizações que buscam cada vez mais conhecimento para poderem sobreviver em um ambiente extremamente complexo, onde a palavra de ordem é mudança (capítulo).

Essas organizações, além de relacionarem-se internamente com agentes instáveis, como funcionários e a sua própria estrutura organizacional, devem relacionar-se com consumidores, concorrentes, sistema financeiro, comunidade e sindicatos, entre outros.

É grande a necessidade de buscar informações além de suas fronteiras, para poder de certa forma reagir com mais rapidez e segurança às mudanças ambientais.

O uso da Tecnologia da Informação (capítulo auxilia em muito a organização na busca dessas informações competitivas, e a aplicação de redes neurais artificiais vem ajudar na análise da grande quantidade de informações que a organização possui).

A ostra a relação entre o tipo de organização, a área de aplicação e a tarefa executada pela RNA.

O estudo de clusterização é alvo deste trabalho, por isso, as aplicações nas organizações que utilizam esta tarefa estão destacadas em negrito.

Aplicações de RNAs nas Organizações.

Em negrito as aplicações que utilizam clusterização (agrupamento).

Adaptado de Bigus As redes neurais artificiais são utilizadas nos Negócios como uma técnica para extrair conhecimento das bases de dados.

As RNAs podem realizar muitas tarefas relacionadas ao KDD como, classificação, clusterização (agrupamento) e associação, dentre outras.

Por isso, as RNA são muito utilizadas quando o objetivo é a descoberta de conhecimento em dados.

A seguir, serão apresentadas brevemente algumas das tarefas em que os algoritmos baseados em RNA podem ser utilizados, pois o assunto tarefas do KDD foi aprofundado no capítulo 3.

Uma RNA pode ser treinada para, a partir de exemplos de uma mesma classe ou de um conjunto de classes com características em comum, separar os elementos determinando se algum registro pertence a uma classe ou não.

A RNA pode, por exemplo, classificar os itens comprados pelas pessoas.

Essa classificação pode ser utilizada para verificar a relação entre itens de compra e fornecer para a empresa o perfil dos seus clientes, além de definir novas estratégias de negócios, realizar promoções, determinar a necessidade de aquisição de novos produtos para vender ou até trocar de atividade comercial.

Enfim, a utilização de RNAs para fazer classificações pode ser uma ferramenta muito útil para uma empresa gerenciar o seu relacionamento com os clientes.

Uma RNA pode ser treinada para agrupar diversos objetos similares, por exemplo, itens de compras de clientes, categorias de clientes, faixa etária de clientes, entre outros.

É muito útil para uma empresa obter informações sobre a categoria de clientes que ela possui.

Uma categoria pode ser entendida como classes sociais, faixas etárias e períodos de compra dos clientes.

Uma empresa pode obter informações de seu interesse, como, por exemplo, que noventa por cento dos seus clientes fazem parte da classe média, ou que setenta e cinco por cento dos seus clientes são mulheres com idade entre vinte e cinco e trinta e oito anos, ou que oitenta por cento das vendas feitas pela empresa ocorrem entre o dia cinco e o dia quinze de cada mês e aos sábados à tarde.

Todos estes exemplos de informações podem determinar novas estratégias de vendas de uma empresa.

Neste trabalho, a tarefa de clusterização é de especial importância, pois, tanto a rede SOM (primeira fase dos experimentos) como a Arquitetura Híbrida proposta (segunda fase dos experimentos) têm como função a geração de clusters.

O cérebro humano é como um dispositivo de armazenamento e, ao longo de toda a sua existência, a memória humana armazena informações de forma similar a um banco de dados.

As pessoas armazenam informações e fazem associações de fatos, idéias e de dados com outras memórias relacionadas.

Uma RNA pode ser treinada, por exemplo para descobrir associações entre itens de compras feitas pelos consumidores de um supermercado e, a partir desta informação, definir novas estratégias de negócios.

Uma RNA pode também realizar outras tarefas, como, Regressão, Previsão de Séries Temporais e Predição.

Em uma rede neural artificial os neurônios estão dispostos em uma ou mais camadas e interligados por grande número de conexões, geralmente unidirecionais.

Na maioria dos modelos, essas conexões estão associadas a pesos, os quais armazenam o conhecimento representado no modelo e servem para ponderar a entrada recebida por cada neurônio da rede.

As redes neurais artificiais possuem a capacidade de aprender por exemplos e fazer interpolações e extrapolações do que aprenderam.

Um conjunto de procedimentos bem definidos para adaptar os pesos de uma rede neural artificial para que ela possa aprender uma determinada função é chamado algoritmo de aprendizado.

Tipo de arquitetura de rede neural artificial muito utilizada chamada MultiLayer Perceptron (MLP).

Arquitetura de uma rede neural artificial MultiLayer Perceptron (MLP).

Pode-se observar os dados (vetores de dados) de entrada da rede (X1, XN), os neurônios da camada de entrada da rede (Ne1, Nem) com seus respectivos pesos, os neurônios que formam a camada intermediária da rede (No1, Non) e a camada de saída (Ns1), formada por um neurônio.

Uma definição geral do que vem a ser aprendizado em uma RNA pode ser expressa da seguinte maneira, "Aprendizado é o processo pelo qual os parâmetros de uma rede neural artificial são ajustados através de uma forma continuada de estímulo pelo ambiente no qual a rede está operando, sendo o tipo específico de aprendizagem realizada definido pela maneira particular como ocorrem os ajustes realizados nos parâmetros".

Diversos métodos para aprendizado foram desenvolvidos, podendo ser agrupados em dois paradigmas principais, aprendizado supervisionado e aprendizado não-supervisionado.

O treinamento supervisionado necessita de um par de vetores, composto do vetor de entrada e do vetor alvo que se deseja como saída.

Juntos, estes vetores são chamados de par de treinamento ou vetor de treinamento, sendo interessante ressaltar que geralmente a rede é treinada com vários pares de treinamento.

O procedimento de treinamento funciona da seguinte forma, o vetor de entrada é aplicado, a saída da rede é calculada e comparada com o correspondente vetor alvo.

O erro encontrado é então realimentado através da rede, e os pesos são atualizados de acordo com um algoritmo determinado a fim de minimizar este erro.

Este processo de treinamento é repetido até que o erro para os vetores de treinamento tenha alcançado níveis bem baixos.

O algoritmo mais difundido é o de retropropagação do erro utilizado pela rede MultiLayer Perceptron.

O treinamento não-supervisionado, por sua vez, não requer vetor alvo para as saídas e, obviamente, não faz comparações para determinar a resposta ideal.

O conjunto de treinamento modifica os pesos da rede de forma a produzir saídas que sejam consistentes, isto é, tanto a apresentação de um dos vetores de treinamento como a apresentação de um vetor que é suficientemente similar irão produzir o mesmo padrão nas saídas.

No caso da rede SOM, o processo de treinamento extrai as propriedades estatísticas do conjunto de treinamento e agrupa os vetores similares em classes.

A aplicação de um vetor de uma determinada classe à entrada da rede irá produzir um vetor de saída específico, mas não existe maneira de se determinar, antes do treinamento, qual o padrão que será produzido na saída para um vetor de entrada de uma determinada classe.

Este aprendizado não possui tempo determinado de execução.

Algumas redes neurais artificiais requerem apenas uma passagem através dos dados, enquanto outras podem requerer centenas ou milhares de execuções sobre a mesma base de dados.

O aprendizado de uma rede neural artificial pode ser controlado por um conjunto de parâmetros ou estes mesmos parâmetros podem servir para o ajuste dos pesos da RNA.

O aprendizado de uma RNA, na maioria dos casos, acontece com um subconjunto de exemplos (vetores de dados) que definem o chamado conjunto de treinamento, e o teste da RNA é realizado com um outro subconjunto de exemplos (vetores de dados) que definem o chamado conjunto de teste.

A presenta alguns parâmetros adotados no aprendizado das RNAs.

Parâmetros de Aprendizado (treinamento) para arquiteturas de rede neural artificial.

As RNAs podem ser treinadas utilizando-se valores iniciais aleatórios para as conexões de pesos.

Os parâmetros de aprendizado são inicializados e os padrões de treinamento dos vetores de dados são apresentados para a RNA.

Ao longo do progresso do treinamento são ajustadas as conexões de pesos e é possível monitorar a performance da RNA.

A rede SOM, ou Mapas Auto-Organizáveis, foi desenvolvida pelo finlandês Teuvo Kohonen em 1984.

Em vários trabalhos, esta rede é chamada de rede de Kohonen, o que pode provocar certa confusão, uma vez que Kohonen também propôs outros modelos de redes neurais artificiais.

Neste trabalho serão chamadas simplesmente de rede SOM.

Em contraste com outros modelos, estas redes possuem uma forte inspiração neurofisiológica.

Elas são baseadas no mapa topológico presente no córtex cerebral.

O desenvolvimento da rede SOM como mapa neural foi idealizado a partir de uma característica do cérebro humano, que é organizado em muitas regiões, de tal forma que entradas sensoriais distintas são representadas por mapas computacionais topologicamente ordenados.

Por exemplo, entradas sensoriais tácteis, visuais (o mapeamento é definido pelas características visuais primitivas, como intensidade de luz, orientação e curvatura de linhas) e acústicas (o mapeamento reflete as diferentes freqüências sonoras) são mapeadas em diferentes áreas do córtex cerebral, de uma forma topologicamente ordenada.

Esses neurônios estão espacialmente ordenados dentro destas áreas e, assim, neurônios topologicamente próximos tendem a responder a padrões ou estímulos semelhantes.

A rede SOM é um tipo de rede neural artificial baseada em aprendizado não-supervisionado, sendo capaz de mapear um conjunto de dados, de um espaço de entrada multidimensional, num conjunto finito de neurônios organizados em um arranjo normalmente unidimensional ou bidimensional.

Os nós da rede SOM estão localmente interconectados por uma relação de vizinhança, determinando a topologia do mapa.

Num mapa bidimensional, a vizinhança pode ser hexagonal ou retangular.

Arranjo hexagonal.

Arranjo hexagonal para uma rede SOM com grade bidimensional.

Pode-se observar no centro do arranjo o neurônio vencedor (de cor pret e sua vizinhança imediata com seis vizinhos).

O formato do arranjo influencia diretamente a adaptação da rede SOM, sendo que o modelo hexagonal oferece tradicionalmente resultados melhores que o retangular.

Os conceitos de neurônio vencedor (competição) e adaptação serão tratados na seção Esta estrutura possibilita à rede SOM algumas vantagens como robustez, ajuste local do mapa e visualização, o que faz com que seja utilizada nas mais diversas áreas, que vão da medicina ao marketing.

No caso de KDD, a rede SOM é muito utilizada por possibilitar em um mapa de uma ou duas dimensões uma visualização simples dos clusters e da correlação dos dados, preservando a posição relativa dos clusters no hiperespaço original.

A idéia básica das técnicas de agrupamento é buscar regiões de alta densidade de pontos no espaço de entrada, os agrupamentos (clusters) de dados, e escolher os seus centros para representar os pontos das regiões.

No caso da rede SOM, a definição de relação de vizinhança não é simétrica nos extremos do mapa, fato que implica uma estimativa de densidade diferente nas regiões da borda comparadas com as unidades do centro do mapa, prejudicando a definição de borda (fronteir entre os clusters, o que é uma desvantagem da rede SOM).

Essa desvantagem será analisada com mais detalhes no capítulo 6.

O algoritmo de aprendizado da rede SOM para gerar os clusters é composto por três fases, competição, adaptação e cooperação.

Para cada dado apresentado à rede haverá uma competição entre todos os neurônios pelo direito de representá-lo.

Vence a competição o neurônio que tiver o vetor de pesos mais próximo do vetor de dados.

O algoritmo da rede SOM utiliza o chamado aprendizado competitivo.

Uma rede neural com aprendizado competitivo é uma rede de uma única ou duas camadas (uni ou bidimensional) em que todos os neurônios recebem a mesma entrada.

Cada neurônio computa o seu nível de ativação multiplicando o seu vetor de pesos pelo vetor de dados da entrada.

O neurônio que tiver o maior nível de ativação (equação 51) é chamado de neurônio vencedor ou de BMU (Best Match Unit), ou seja, o padrão de entrada que estiver sendo apresentado à rede provocará a ativação de apenas um neurônio de saída ou um neurônio por grupo.

Aprendizado competitivo em uma rede SOM bidimensional com 16 neurônios.

Rede SOM com aprendizado competitivo.

Na rede SOM, pode-se visualizar que a rede possui M neurônios organizados em uma grade bidimensional.

Os padrões de entrada são vetores.

Quando um dos P padrões de entrada é apresentado à rede SOM, cada um dos M neurônios recebe esse padrão e calcula o seu nível de ativação através da equação abaixo, onde x é o vetor de entrada, i é o índice que indica o neurônio e w é o vetor de i pesos entre o padrão de entrada e o neurônio i (os pesos não estão mostrados na equação para não congestioná-l).

O neurônio é vencedor quando o padrão x que estiver sendo apresentado é aquele que tiver o maior valor de u, neste trabalho denominado i (i asterisco).

O único neurônio ativo (o BMU) em resposta ao padrão x representa o padrão de entrada.

É como se os M neurônios da rede competissem entre si para determinar qual vai ficar mais ativo em resposta ao padrão de entrada e apenas o vencedor permanecesse ativo, deixando todos os perdedores inativos.

Por causa disso, este tipo de rede é também chamado de rede do tipo "o vencedor fica com tudo" (winner-takes-all) Uma rede do tipo o-vencedor-fica-com-tudo implementa um mapa entre um espaço N-dimensional contínuo de vetores x e um espaço discreto de M neurônios.

Note que podemos ter mais de um vetor x sendo representado pelo mesmo neurônio vencedor.

Neste caso, este neurônio é o representante do grupo de padrões x que o fazem ser vencedor.

O neurônio vencedor é então adaptado (winner-takes-all).

O seu vetor de pesos é alterado no sentido de representar ainda mais o dado apresentado, aumentando a probabilidade de que este mesmo neurônio volte a vencer numa próxima apresentação do mesmo dado, ou dado semelhante.

Chamando o vetor de pesos do neurônio vencedor i de w, a regra do aprendizado competitivo utilizada para a adaptação do BMU é a seguinte, onde é a constante de taxa de aprendizagem (um valor entre 0 e 1) que controla a rapidez com que são feitas as mudanças nos pesos.

Para entender melhor a regra do aprendizado competitivo analisa-se a mesma vetorialmente.

O neurônio vencedor é aquele que tiver o maior nível de ativação u.

Como o nível de ativação de um neurônio i é o produto entre o vetor de entrada x e o vetor de pesos do neurônio i, como demonstrado na equação 53, o critério para a escolha do neurônio vencedor é o de similaridade entre x e w.

Este produto pode ser escrito como na equação 54, onde é o ângulo entre os vetores x e w.

Se os vetores de entrada e de pesos estiverem normalizados, um valor grande de u indica que o vetor de entrada x está próximo do vetor de pesos w, ou seja, que x está nas vizinhanças de w.

Já um valor pequeno de u indica que o vetor de entrada x é quase perpendicular ao vetor de pesos w, como pode ser visualizado.

Processo de escolha do neurônio vencedor.

Processo de escolha do neurônio vencedor em um caso em que os padrões de entrada são bidimensionais, com uma rede neural com três neurônios e os vetores de peso e de entrada normalizados.

O neurônio vencedor i é o que tiver o valor máximo de w x.

A normalização dos padrões de entrada provoca alterações, o que nem sempre é desejável.

Para evitar isto, costuma-se usar como critério de definição do neurônio vencedor o cálculo da distância euclidiana entre o vetor x e o vetor de pesos w.

Conclui-se que maximizar o produto w x é matematicamente equivalente a minimizar a distância euclidiana entre x e w.

Portanto, o neurônio vencedor i é aquele cujo vetor de pesos tiver a menor distância euclidiana com relação ao padrão de entrada.

Uma vez determinado o neurônio vencedor, o seu vetor de pesos é alterado pela regra do aprendizado competitivo, w = (x w).

Ela nos diz que o vetor de pesos do neurônio vencedor deve ser alterado por um fator na direção.

Geometricamente, em duas dimensões.

Alteração do vetor de pesos do neurônio vencedor.

Esta regra de mudança de pesos faz com que o vetor de pesos do neurônio vencedor, que já era o mais próximo do padrão de entrada, seja arrastado na direção do padrão de entrada, ficando ainda mais próximo dele.

A partir de uma população de padrões de entrada escolhe-se, a cada passo, um dos padrões, aplicando-o à entrada da rede.

O neurônio vencedor é determinado e o seu peso alterado conforme a regra do aprendizado competitivo.

Repetindo-se este procedimento várias vezes, os pesos acabam convergindo para uma situação de relativa estabilidade em que eles ficam nos centros de massa de agrupamentos de padrões de entrada.

Processo de adaptação em um exemplo com 1padrões de entrada e uma rede SOM com neurônios.

Convergência dos pesos da rede SOM para os centros de massa dos agrupamentos de padrões de entrada.

No Início, os pesos da rede SOM estão distantes dos padrões de entrada (bolas pretas).

No fim, a atualização dos pesos da rede SOM provoca o deslocamento do BMU para o centro de massa dos agrupamentos.

A convergência dos vetores de peso é controlada pelo valor.

Se for muito grande as mudanças nos pesos serão grandes e a convergência será rápida, mas os vetores de peso podem não se estabilizar.

Se for muito pequeno, a convergência pode ser muito vagarosa.

Uma técnica muito adotada é fazer variar com o passo de iteração t, começando com um valor próximo de 1 e diminuindo à medida que o aprendizado progride.

A regra de aprendizado competitivo faz com que a rede SOM represente grupos de padrões que estejam próximos entre si por um vetor protótipo.

O espaço de vetores de entrada fica dividido em regiões, ou vizinhanças, tais que cada uma delas está associada a um vetor protótipo, que é o vetor de pesos de um dos neurônios da rede.

Os vetores protótipos são representantes de todos os possíveis vetores que estiverem na sua vizinhança.

Como já foi dito, a idéia básica das técnicas de agrupamento é buscar regiões de alta densidade de pontos no espaço de entrada, os agrupamentos (clusters) de dados, e escolher os seus centros para representar os pontos das regiões.

Portanto, o centro de cada uma dessas regiões é o protótipo que representa todos os pontos da região.

No caso da rede neural treinada por aprendizado competitivo, os protótipos são os vetores de pesos dos M neurônios da rede.

Quando um padrão x é apresentado na entrada da rede, a unidade vencedora, i, é aquela cuja distância euclidiana entre o seu vetor de pesos w e o padrão x for a menor de todas (como no caso do aprendizado competitivo), Na fase de cooperação não é só o neurônio vencedor que tem o seu vetor de pesos atualizado, mas os seus neurônios vizinhos.

A regra de mudança de pesos é, onde (t) é a chamada função de vizinhança centrada no neurônio vencedor i e (t) é a taxa de aprendizagem.

Em geral, tanto (t) como (t) variam com o passo de aprendizagem t.

O efeito da introdução da função de vizinhança (t) é fazer com que o vetor de pesos não apenas do neurônio vencedor seja alterado na direção do padrão atual, mas que os vetores de pesos de todos os neurônios vizinhos ao neurônio vencedor também sejam alterados na direção do padrão atual, porém, a alteração do vetor de pesos vai diminuindo à medida que o neurônio correspondente vai ficando mais distante do vencedor.

Fazendo uma analogia, seria como se todos os vetores de pesos dos neurônios da rede SOM estivessem unidos por elásticos, sendo que os elásticos mais fortes uniriam os vetores de pesos dos neurônios que fossem primeiros vizinhos, elásticos um pouco mais fracos uniriam os vetores de pesos dos neurônios que fossem segundos vizinhos, elásticos mais fracos uniriam ainda os vetores de pesos dos neurônios que fossem terceiros vizinhos e assim sucessivamente.

Quando o vetor de pesos de um neurônio vencedor em um dado passo for alterado, ele irá arrastar consigo os demais vetores de peso, e mais fortemente aqueles dos neurônios mais próximos.

Em geral, usa-se uma função gaussiana para implementar a função de vizinhança como a descrita na equação 58, onde d é a distância entre um neurônio i e o neurônio vencedor i.

Se a rede neural for unidimensional, essa distância é simplesmente o módulo da diferença entre os índices de i e i, d = |índice_de_iíndice_de_i | se a rede for bidimensional, essa distância é dada pela distância euclidiana entre os seus vetores de posição mostrada na equação 59, onde r é o vetor posição do neurônio i e r é o vetor posição do neurônio vencedor, os dois sendo medidos no espaço discreto definido pelos nós da rede neural.

O desvio padrão da função de vizinhança, (t) diminui com o número de passos t.

Uma maneira comum de implementar essa diminuição é por um decaimento exponencial descrito na equação 510, onde é uma constante temporal (determinada empiricamente).

Em geral, também se faz a taxa de aprendizagem (t) diminuir com o passo de iteração de uma maneira exponencial descrita na equação 511, onde é outra constante temporal (também determinada empiricamente).

Os padrões vizinhos no espaço de entrada são representados por neurônios vizinhos na rede neural artificial.

As regiões do espaço de entrada cujos padrões x tenham maior probabilidade de ocorrer são representadas por um número maior de neurônios da rede neural artificial, isto é, elas são representadas com uma resolução maior do que as regiões cujos padrões ocorrem menos freqüentemente.

Portanto, um mapa entre o espaço contínuo de entrada e o espaço discreto da rede neural implementado pelo algoritmo SOM tende a preservar tanto a métrica como a distribuição do espaço de entrada.

É importante lembrar que isto ocorre a partir apenas da informação contida nos padrões de entrada, o mapa criado pelo SOM não é supervisionado.

O treinamento da rede SOM ocorre em duas fases, Fase de Auto-organização ou de Ordenamento, É nesta fase que ocorre o ordenamento topográfico dos vetores de pesos.

Inicialmente, os vetores de pesos têm valores aleatórios e não possuem qualquer tipo de ordenamento.

À medida que a rede vai sendo treinada, vetores de neurônios vizinhos entre si no espaço da rede neural começam a se aproximar uns dos outros, de maneira que neurônios de uma mesma área da rede neural acabam representando padrões vindos de uma mesma região do espaço de entrada.

Esta fase pode levar muitas iterações para terminar, em geral mais do que 1000.

Fase de Convergência, Nesta fase ocorre o refinamento do mapa, levando a uma representação mais acurada do espaço de entrada por parte da rede neural.

Como regra geral, o número de iterações nesta fase deve ser de pelo menos 500 vezes o número de neurônios na rede neural.

Logo, o número de iterações pode atingir dezenas de milhares de passos.

Nesta fase, a taxa de aprendizagem deve permanecer pequena, da ordem de 0,05 e a função de vizinhança deve englobar apenas o próprio neurônio vencedor e, no máximo, os seus primeiros vizinhos.

Pode-se sintetizar o algoritmo da rede SOM da seguinte forma, Inicialização.

Escolha valores aleatórios para as componentes iniciais dos vetores de pesos.

Escolha do padrão de entrada.

Escolha um padrão x da população para ser colocado na camada de entrada da rede.

Determinação do neurônio vencedor.

Use o critério de similaridade baseado na distância euclidiana entre o vetor de entrada e os vetores de peso para * determinar o neurônio vencedor i para o passo atual.

Atualização dos pesos.

Modifique os vetores de pesos dos neurônios da rede.

Continuação.

Volte para o passo e continue até que não sejam observadas mudanças significativas no mapa formado.

Como discutido no capítulo 1, ainda não é claro como se devem escolher os parâmetros do algoritmo da rede SOM de forma a obter um "bom mapeamento".

Normalmente a rede SOM é avaliada com base na resolução do mapa e em relação à preservação da topologia.

A escolha do "melhor mapeamento" deveria ser, portanto, por aquele que "melhor representa os dados de entrada".

Este critério normalmente é traduzido por duas medidas, o erro de quantização e o erro topográfico.

O erro de quantização corresponde à média das distâncias entre cada vetor de dados x e o correspondente vetor de pesos do neurônio BMU.

A medida corresponde à resolução do mapa e está descrita na equação 512.

O erro topográfico quantifica a capacidade do mapa em representar a topologia dos dados de entrada.

Para cada vetor de dados de entrada x são calculados seu primeiro BMU e o seu segundo BMU e toda vez que eles não forem adjacentes (vizinhos, próximos), aumenta-se o erro em uma unidade, tirando-se depois a média pelo número total de vetores, onde u(xk) igual a 1 se o primeiro e o segundo BMU não são adjacentes, caso contrário igual a 0.

A medida EQ corresponde à acuidade, ou resolução do mapa, que é inversamente proporcional ao número de neurônios, ou seja, o erro de quantização diminui com o aumento do número de neurônios no arranjo (resolução aument).

Se o número de neurônios for muito grande e (ou) se sofreu um processo de treinamento onde o raio de vizinhança (define a largura da região da vizinhanç torna-se menor ou igual a 1 durante muito tempo, pode ocorrer de os neurônios se posicionarem sobre os objetos a serem representados).

Neste caso, EQ = 0, mas o arranjo pode estar tão retorcido que a capacidade de representar a topologia dos dados é perdida (ET aument).

O comportamento de ET nesta situação dependerá também do número de neurônios disponíveis no arranjo

ET aumenta se há poucos neurônios e diminui se há muitos neurônios.

Quando ambos os valores de EQ e ET são muito baixos, suspeita-se de sobreajuste (overfitting) A rede SOM, na tentativa de representar o mais fielmente possível os dados, "dobra-se" de tal forma que acaba representando exatamente os dados, podendo perder a sua capacidade de generalização.

O fenômeno inverso, o subajuste (underfitting), ocorre quando um mapa é "rígido" demais.

Isso pode acontecer quando há poucos neurônios para representar um número proporcionalmente grande de dados ou o raio de vizinhança final da função gaussiana for maior que 1 durante o treinamento.

Neste caso, os valores de EQ podem ser mais altos (os vetores de pesos dos neurônios encontram-se, em média, menos próximos dos vetores de dados).

Em geral, valores muito baixos de ET, associados a valores mais altos de EQ, podem sugerir subajuste.

Na seção 58 foram descritos alguns dos motivos que fazem com que a rede SOM seja muito utilizada para o KDD, sendo que um deles é a visualização.

Nesta seção, serão apresentadas algumas formas de visualização dos clusters gerados pela rede SOM.

A Matriz-U, ou matriz de distâncias unificadas, é a matriz de distâncias entre cada neurônio do mapa e seus vizinhos.

É a técnica de visualização mais utilizada para detectar clusters da rede SOM.

Exemplo de Matriz-U.

Matriz-U (matriz de distâncias unificadas).

A Matriz-U mostra a topologia do mapa e conseqüentemente a topologia dos dados de entrada.

A análise revela que os valores altos presentes na barra de cores, representados por tons escuros, correspondem a grandes distâncias no espaço de entrada, e valores baixos, representados por tons claros, indicam similaridades no espaço de entrada, caracterizando clusters.

Assim, um dos usos mais comuns da Matriz-U é servir como ponto de partida para a clusterização.

Esta outra forma de representação é útil tanto para a visualização da distribuição dos clusters no mapa como para a criação de rótulos, permitindo a análise da estrutura dos clusters.

Pode-se observar que foi utilizado o contraste de cores para representar os 10 clusters rotulados de acordo com as classes contidas na base de dados.

Mapa por Similaridade de Cor evidenciando a localização de agrupamentos.

A barra ao lado do mapa representa a relação entre o número do cluster e a sua respectiva cor, como por exemplo, o cluster de número 8 corresponde no mapa à cor amarela.

Estas informações são relevantes na escolha do "melhor" mapa com base nas similaridades entre os dados e nas diferenças de separação dos clusters e, por isso, fazem parte da análise dos experimentos realizados.

Mapa por similaridade de cor onde cada neurônio foi rotulado com todos os tipos de dados para os quais foi vencedor (BMU).

Mapa por Similaridade de Cor onde cada grupo de neurônios foi rotulado com todos os tipos de dados para os quais foi vencedor (BMU).

Os resultados da qualidade visual dos mapas obtidos nos experimentos realizados no sexto capítulo serão analisados através do Mapa por Similaridade de Cor.

Sempre que se desejar descobrir conhecimento novo em uma massa de dados, deve-se pensar na possibilidade de apresentar essa massa a uma rede SOM.

Os mapas oferecem, através das ferramentas de visualização descritas neste capítulo, uma forma geométrica simples de verificar se há algo de interessante ou organizado na massa de dados.

Muitos programas comerciais de mineração de dados possuem rotinas prontas de redes SOM.

Este capítulo apresentou de forma introdutória um panorama geral do histórico, dos principais conceitos e aplicações das redes neurais artificiais.

O enfoque principal foi dado na rede SOM, onde foram abordadas as características e a estrutura da rede.

O capítulo se encerra com duas análises, das medidas de qualidade mais utilizadas para avaliar a resolução do mapa e a preservação da topologia da rede SOM, o erro de quantização e o erro topográfico e do Mapa por Similaridade de Cor, que possibilita a escolha do "melhor" mapa com base nas similaridades entre os dados e nas diferenças de separação dos clusters.

As análises são importantes porque a avaliação do desempenho da Arquitetura Híbrida proposta neste trabalho está embasada nos resultados do erro de quantização, do erro topográfico, no número de clusters gerados, no Mapa por Similaridade de Cor e no tempo de treinamento da rede SOM.

O sexto capítulo trata dos experimentos realizados.

Neste capítulo, realizou-se uma comparação entre a Arquitetura Híbrida proposta (RS com a rede SOM) e apenas a rede SOM analisando-se os resultados obtidos para a descoberta de conhecimento em bases de dados (KD).

A comparação teve como objetivo principal verificar se a Arquitetura Híbrida apresentou um desempenho superior ao da rede SOM.

Foram utilizadas nos experimentos cinco bases de dados diferentes, nas quais há variação de dimensionalidade, quantidade e tipos de dados disponíveis (discretos, contínuos, binários, et buscando uma melhor validação dos experimentos).

Neste capítulo ainda estão descritos, o motivo de escolha das bases de dados, a metodologia utilizada na realização dos experimentos, os resultados experimentais e a avaliação destes resultados com base no conhecimento descoberto em cada experimento.

Como dito anteriormente, foram realizadas no total cinco séries de experimentos com bases de dados distintas, comparando a Arquitetura Híbrida proposta com uma rede SOM, divididos da seguinte forma, Experimento 1, Realizado com uma base de dados gerada artificialmente, o que possibilitou também a verificação do RS com relação aos conceitos de dependência e significância de atributos (realizado somente neste experimento).

Experimento 2, Realizado com a base de dados pública Íris.

Experimento 3, Realizado com a base de dados pública ZOO.

Experimento 4, Realizado com a base de dados do mundo real Consumidor.

Experimento 5, Realizado com a base de dados do mundo real The Insurance Company.

A área de marketing tradicionalmente tem muito interesse em KDD como por exemplo, conhecer o perfil do consumidor.

Assim, as bases de dados utilizadas nos experimentos e 5 são da área de marketing.

Vale ressaltar que o principal critério de escolha das bases de dados foi a quantidade de atributos (dimensionalidade), o que justifica a escolha de cinco bases de dados com variação no número de atributos, pois procurou-se avaliar dentro da Arquitetura Híbrida o comportamento do RS, que na geração dos redutos (capítulo 4, seção 4reduz a dimensionalidade da base de dados).

Os experimentos foram realizados em duas fases distintas descritas a seguir, Na primeira fase de cada experimento, denominada rede SOM sem redutos, apresentou-se à rede SOM cada uma das cinco bases de dados dos experimentos com todos os atributos e avaliaram-se os resultados.

Primeira fase dos experimentos.

Primeira fase dos experimentos em que as bases de dados são apresentadas à rede SOM.

Na segunda fase, denominada Arquitetura Híbrida (rede SOM com redutos), apresentou-se primeiro ao RS cada uma das cinco bases de dados para a geração dos redutos e, em seguida, as bases de dados reduzidas foram apresentadas à rede SOM.

Segunda fase dos experimentos.

Segunda fase dos experimentos em que as bases de dados são apresentadas à Arquitetura Híbrida (rede SOM com redutos).

Nos experimentos 1 e 2, realizados com bases de dados mais simples, foram observados, os conceitos de dependência e significância de atributos na geração de redutos pelo RS, o número de clusters e a visualização do mapa gerado pela rede SOM utilizando o Mapa por Similaridade de Cor (capítulo 5).

Após análise dos resultados obtidos nos experimentos 1 e foram realizados os experimentos 3, e 5, utilizando bases de dados mais complexas, onde se observou a comparação entre a Arquitetura Híbrida proposta e a rede SOM com mais detalhes, através, do valor do Erro de Quantização, do valor do Erro Topográfico, do Tempo de Treinamento da rede SOM, do número de clusters gerados, da visualização do mapa gerado utilizando o Mapa por Similaridade de Cor.

Conclui-se que o melhor desempenho é aquele que apresenta os melhores resultados para os itens descritos acima.

Note-se que nos experimentos e 5, foram abordadas bases de dados que representam problemas concretos de KDD no contexto da área de aplicação discutida no capítulo 2, ou seja, organizações e o uso de mineração de dados para apoio à decisão.

Para realização dos experimentos com a rede SOM foi utilizada a ferramenta SOM Toolbox, uma implementação do Mapa Auto-Organizável de Kohonen em Matlab.

A escolha desta ferramenta baseou-se no grande número de trabalhos publicados que relatam a sua utilização.

De certa forma, esta ferramenta pode ser considerada como uma plataforma padrão que tem sido adotada em grande parte das pesquisas atuais com Mapas Auto-Organizáveis.

Essa Toolbox foi originalmente escrita para Matlab 50, porém funciona também nas versões mais atuais.

A toolbox pode ser utilizada para pré-processamento dos dados, inicialização e treinamento do Mapa Auto-Organizável em várias topologias e para visualização dos dados e do mapa após treinamento.

Possui também funções de análise das propriedades dos mapas e dos dados, correlação entre variáveis e funções de clusterização.

A ferramenta foi desenvolvida para conjuntos pequenos de dados (até 10000 registros), mas é capaz de trabalhar também com conjuntos de tamanho médio (até 1000000 de registros).

É de domínio público e pode ser encontrada para instalação.

Para a realização dos experimentos com RS foi utilizada a ferramenta chamada Rosetta (A Rough Sets Toolkit for Analisys of Dat, de domínio público, encontrada para instalação).

Como acontece com a SOM Toolbox, a escolha da ferramenta Rosetta foi embasada no grande número de publicações que relatam a sua utilização.

A ferramenta vem sendo continuamente aperfeiçoada desde a sua primeira versão em 1997, estando atualmente na versão 1440.

Foi desenvolvida numa cooperação entre o Grupo de Sistemas de Conhecimento do Departamento de Computação e Ciência da Informação da Universidade Norueguesa de Ciência e Tecnologia, Noruega e o Grupo de Lógica do Instituto de Matemática da Universidade de Warsaw, Polônia.

Informações mais detalhadas sobre a ferramenta Rosetta podem ser encontradas em Pila.

A plataforma de hardware utilizada nos experimentos foi um Pentium IV com 2MHZ, 51MB de memória RAM e 40 GB de disco rígido.

Como já foi citado no capítulo 5, a escolha dos parâmetros, tanto de inicialização quanto de treinamento do mapa, ainda não segue regras bem definidas.

Mesmo lançando mão das heurísticas existentes para definição dos parâmetros, é consenso entre os pesquisadores que se devem efetuar alguns testes com diferentes configurações de mapas antes de decidir-se qual representa melhor o conjunto de dados em questão.

Em geral, são propostas heurísticas baseadas no comportamento do mapa e em medidas de qualidade como EQ e ET.

Os parâmetros que regulam a rede SOM podem ser agrupados em dois conjuntos, Parâmetros de estrutura e Parâmetros de treinamento.

Os Parâmetros de estrutura são os seguintes, Dimensões, tamanho do mapa, número de neurônios.

Vizinhança, hexagonal ou retangular.

Formato do Arranjo, Folha, Cilindro ou Toróide.

Os Parâmetros de treinamento correspondem ao número de épocas de treinamento.

Nos experimentos realizados variaram-se alguns dos parâmetros de estrutura para conhecer a sua influência nos resultados.

Desta forma, todos os parâmetros descritos nesta seção foram utilizados nos experimentos tanto na primeira fase como na segunda fase, e são os seguintes, Parâmetros de estrutura, Dimensões, número de neurônios para os experimentos 1 e (que possuem um número menor de registros e de atributos) 5 X5 = 25 neurônios.

Número de neurônios para os experimentos 3, e 5 (que possuem um número maior de registros e de atributos) 15 X15 = 225 neurônios Vizinhança, hexagonal Formato do Arranjo, plano hexagonal.

Parâmetros de treinamento, O treinamento de um mapa na SOM Toolbox é dividido em etapas conforme descrito na seção 612, "rough phase" e "fine tune".

Para cada uma destas etapas são definidos diferentes números de épocas.

Os valores default são, para a "rough phase" = 10 x mpd épocas, e para a "fine tune" = 40 x mpd épocas, onde mpd = neurônios/dados.

A taxa de aprendizado foi de 0,5 para a fase inicial e 0,05 para a fase de convergência.

As medidas de qualidade utilizadas são o Erro Médio de Quantização e o Erro Topográfico.

O objetivo principal das séries de experimentos documentadas neste capítulo é verificar qual das duas alternativas tem melhor desempenho na descoberta de conhecimento em bases de dados, a rede SOM ou a Arquitetura Híbrida A escolha do "melhor" mapa deve se basear em algumas características, como a representação da topologia da estrutura dos dados, as similaridades entre os dados e as diferenças de separação dos agrupamentos.

No caso da rede SOM, o critério adotado para escolher o "melhor" resultado do mapa gerado utilizando-se inclusive dos rótulos de dados é aquele que apresenta o menor erro de quantização e o menor erro topográfico.

Estes indicadores expressam a capacidade do mapa em representar a topologia dos dados de entrada.

Os menores valores para esses erros, em geral, indicam uma melhor adaptação do mapa à topologia dos dados no espaço de entrada.

Foram descartados os resultados que apresentaram ET igual a 0 devido à possibilidade de sobreajuste (overfitting) ou subajuste (underfitting).

Neste trabalho, além do critério descrito acima, serão adotados também como critério o tempo de treinamento da rede SOM, o número de clusters gerados, o que indica as similaridades entre os dados e as diferenças de separação dos clusters e a visualização do mapa gerado através do Mapa por Similaridade de Cor.

Para verificar o desempenho da Arquitetura Híbrida proposta neste trabalho, compararam-se os resultados obtidos na primeira fase dos experimentos com os resultados obtidos na segunda fase dos experimentos.

Nos experimentos 1 e avaliaram-se os conceitos de dependência e significância de atributos na geração dos redutos, o número de clusters gerados e a visualização do mapa através do Mapa por Similaridade de Cor.

Nos experimentos 3, e 5 avaliaram-se o EQ, o ET, o tempo de treinamento da rede SOM, o número de clusters gerados e a visualização do mapa através do Mapa por Similaridade de Cor.

O procedimento básico de realização dos experimentos utilizando as ferramentas foi o seguinte, na primeira fase com a utilização da ferramenta SOM Toolbox, apresentou-se à rede SOM a base de dados com todos os atributos.

Na segunda fase (Arquitetura Híbrid submeteu-se primeiro a base de dados com todos os atributos à ferramenta Rosetta para que com base em RS reduzisse o número de atributos).

O reduto com o menor número de atributos foi o escolhido por apresentar o menor esforço computacional.

Finalmente, a base de dados reduzida pelo RS foi apresentada à rede SOM.

Foram geradas artificialmente três classes com diferentes distribuições normais.

O primeiro conjunto, considerou para a geração dos dados os seguintes valores de média com dispersão de 0,2.

No segundo conjunto, consideraram-se os seguintes valores de média e dispersão 0,e o terceiro conjunto com valores de média e dispersão 0,1.

Os valores médios foram escolhidos tendenciosamente para que cada classe tivesse dispersões diferentes e para que pelo menos um atributo fosse o responsável em forçar alguma sobreposição entre as classes.

O objetivo é utilizar o RS para investigar o atributo responsável pela sobreposição, e conseqüentemente sugerir um conjunto reduzido de atributos para reduzir essa sobreposição melhorando as fronteiras entre os clusters.

Considera-se como atributo cada eixo do gráfico (X1, X2, X).

A primeira com a distribuição inicial dos dados, a segunda sem o atributo X1 a terceira sem o atributo X e a quarta sem o atributo X(somente X1, X2).

O objetivo destes gráficos é visualizar a nova distribuição dos dados desprezando um atributo por vez.

Ilustrações do Experimento 1 apresentando a Distribuição Inicial dos dados e as Distribuições sem um dos atributos.

Em sentido horário, na ilustração acima, no alto, à esquerda, visualiza-se a distribuição inicial dos dados com os três atributos representados, no alto, à direita, visualiza-se a distribuição dos dados sem o atributo X1, embaixo, à esquerda, visualiza-se a distribuição dos dados sem o atributo X2, e embaixo, à direita, visualiza-se a distribuição dos dados sem o atributo X3.

Analisando, pode-se perceber que quando considera-se a presença do atributo X1 a sobreposição dos dados é maior.

Espera-se então que RS gere redutos sem a presença do atributo X1.

A redução com RS sugeriu utilizar o atributo Xou o atributo X2, reduzindo a dimensionalidade dos dados de para apenas 1 dimensão, porém sem perder a representatividade dos dados.

Redutos gerados pelo RS.

Redutos gerados pelo RS no Experimento 1 utilizando uma base de dados artificial.

O suporte igual a 100 indica que os redutos representam a base de dados na sua totalidade e portanto podem gerar regras precisas no caso de uma classificação.

O comprimento indica o número de atributos presentes no reduto, no caso 1 atributo.

Com o mesmo objetivo da figura anterior, apresentam-se os gráficos de distribuição das classes com apenas um atributo cada e assim avalia-se a separabilidade entre as classes.

Ilustrações do Experimento 1 apresentando a Distribuição das classes com apenas um atributo cada.

Observa-se que o atributo X1 causa grande sobreposição entre as classes, o contrário do que ocorre quando analisamos os gráficos com o atributo Xou com o atributo X3.

O atributo Xé o que fornece maior separação linear entre as classes.

Concluiu-se então que o reduto com apenas o atributo Xé o que apresenta melhor resultado.

Foi utilizada a rede SOM para validar os experimentos anteriores, em que se procurou explorar no mapa gerado os resultados da redução bem como procurou-se analisar a melhora entre agrupamentos.

Para isso foram gerados três mapas, um com a distribuição inicial, outro utilizando o reduto com o atributo Xe o terceiro utilizando o atributo X3, estes últimos gerados pelo RS.

Visualização dos Mapas gerados no Experimento 1 pela rede SOM sem redutos (Distribuição inicial) e pela rede SOM com o reduto Xe com o reduto X(Arquitetura Híbrid).

A primeira fase do experimento (rede SOM sem redutos) é mostrada através do mapa que apresenta a Distribuição inicial e a segunda fase do experimento (Arquitetura Híbrid através dos dois mapas (ao centro e à direit que apresentam apenas o atributo Xe apenas o atributo X3).

Ao sugerir os redutos, RS mostrou que, além de reduzir informação não necessária à rede SOM, também utilizou-se dos conceitos de dependência e significância de atributos para gerar os redutos, mantendo a representatividade inicial da base de dados.

Observou-se então que, quando todos os atributos foram utilizados, a rede SOM apresentou um número maior de clusters (em função da sobreposição causada pelo atributo X1).

Por outro lado, quando usada a Arquitetura Híbrida os resultados foram melhores, pois o RS gerou redutos sem o atributo X1, que causava a sobreposição das classes.

Assim, com apenas o atributo Xa rede SOM gerou clusters, porém ainda com alguns exemplos confusos entre uma classe e outra.

Finalmente, quando se usou apenas o atributo X3, este mostrou ser o que melhor representou as classes, formando três classes diferentes e sem nenhuma sobreposição.

Isto ocorreu porque RS reduziu informação desnecessária da base de dados que era apresentada à rede SOM, melhorando a similaridade entre os dados e reduzindo a diferença de separação dos agrupamentos (de 5 para clusters).

Neste experimento simples, conclui-se que o desempenho da Arquitetura Híbrida pode ser considerado melhor.

A base de dados Íris talvez seja uma das bases mais conhecidas na literatura de reconhecimento de padrões.

O conjunto de dados contém classes de pétalas de 50 exemplos cada, onde cada classe refere-se a um tipo de pétala da planta Íris.

Sabe-se que uma das classes é linearmente separável das outras duas e estas não são linearmente separáveis entre si.

A base de dados Íris contem atributos numéricos mais a classe, descritos a seguir, 1 sepal length em cm, sepal width em cm, petal length em cm e petal width em cm.

As classes são Íris-setosa, Íris-versicolour e Íris-virgínica.

Sabe-se que a correlação entre os atributos petal length e petal width é alta e a Distribuição das classes é de 33,3% para cada uma das classes.

O número de épocas foi calculado da seguinte forma, rough phase = quantidade de neurônios (22/ tamanho de dados (150) x 10 = 16,0 e a fine tune = quantidade de neurônios (22/ tamanho de dados (150) x 40 = 67,0.

Somando as duas fases (16,0 + 67,0) obtêm-se 8épocas.

Primeira fase do experimento, rede SOM sem redutos Apresentada a base de dados Íris com todos os seus atributos à rede SOM, obteve-se o mapa que pode ser visualizado.

Mapa gerado pela rede SOM sem redutos após a apresentação da base de dados Íris.

Do mapa, observou-se que a rede SOM formou clusters com base nas três classes, sendo que duas classes apresentaram problemas de sobreposição (cluster na cor vermelha e cluster na cor azul-clar e a terceira classe foi separada em dois clusters (nas cores amarela e azul-escur).

Segunda fase do experimento, rede SOM com redutos (Arquitetura Híbrid).

Ao aplicar o RS para reduzir os atributos, foram sugeridos redutos, capazes de representar a base de dados Íris com todos os seus atributos.

A ostra os redutos gerados pelo RS.

Os redutos gerados pelo RS utilizando a base de dados Íris Em seguida, a base de dados Íris foi reduzida obedecendo aos redutos gerados pelo RS e apresentada à rede SOM.

Os mapas gerados estão apresentados.

Os quatro Mapas gerados no Experimento pela rede SOM com redutos (Arquitetura Híbrid).

Como no experimento 1, ao sugerir os redutos, RS mostrou que não apenas reduziu informação não necessária à rede SOM como também utilizou dos conceitos de dependência e significância de atributos na geração dos redutos (capítulo 4, seções 4e 44).

Observou-se que nos quatro mapas, foi possível defini-los com apenas clusters, assim, a classe Íris-setosa, que foi dividida em duas na primeira fase do experimento, agora aparece unificada, e fica evidente que é ela a classe linearmente separável das outras duas.

Por outro lado, as outras duas classes, Íris-versicolour e Íris-virgínica ainda, em alguns poucos casos, apresentam uma pequena sobreposição, o que pode ser avaliado como uma boa melhora, já que estas duas classes são consideradas não separáveis linearmente.

Isto ocorreu porque RS reduziu informação desnecessária da base de dados que era apresentada à rede SOM melhorando a similaridade entre os dados e reduzindo a diferença de separação dos agrupamentos (de para clusters).

Novamente o desempenho da Arquitetura Híbrida foi melhor.

A base de dados ZOO possui um total de 101 amostras de animais com 16 atributos, sendo 1 atributo numérico (número de pernas) e 15 atributos binários (tem penas, voa, et).

O Conjunto é dividido em 7 classes, conforme a tabela.

Características da base de dados ZOO.

A escolha da base de dados ZOO foi feita com base em três motivos descritos a seguir, é considerada uma base de dados "bem comportada" por possuir uma boa separação entre as classes, o que a torna diferente das bases de dados utilizadas nos experimentos 1 e 2.

O número de classes (possibilita uma boa visualização e comparação de como as amostras foram alocadas no mapa na primeira fase do experimento e depois na segunda fase do experimento e, por último, com estas características, é provável que os clusters gerados pela rede SOM sejam de boa qualidade, surgindo então uma pergunta, "O mapa pode ser melhorado utilizando-se a Arquitetura Híbrida ").

O número de épocas foi calculado da seguinte forma, rough phase = quantidade de neurônios (22/ tamanho de dados (101) x 10 = 23,0 e a fine tune = quantidade de neurônios (22/ tamanho de dados (101) x 40 = 90,0.

Somando as duas fases (23,0 + 90,0) temos 11épocas.

Primeira fase dos experimentos, rede SOM sem redutos Na primeira fase do experimento foi apresentada à rede SOM a base de dados ZOO com todos os atributos (1e verificaram-se os resultados através do EQ, do ET, do tempo de treinamento da rede e da visualização do mapa).

A ostra os resultados obtidos após a apresentação da base de dados ZOO à rede SOM sem redutos.

Resultados da rede SOM sem redutos.

Podem-se visualizar os 1clusters gerados pela rede SOM sem redutos rotulados de acordo com as classes da base de dados ZOO.

Mapa gerado pela primeira fase do Experimento onde os clusters podem ser identificados através de cores e as classes identificadas através das amostras rotuladas.

Segunda fase dos experimentos, rede SOM com redutos (Arquitetura Híbrid Na segunda fase do experimento submeteu-se a base de dados ZOO ao RS, gerando 5 redutos com 11 atributos cada e 1 reduto com 10 atributos).

Cada reduto foi de forma independente apresentado à rede SOM e verificou-se o seu desempenho através do EQ, do ET, do tempo de treinamento da rede e o número de clusters gerados.

Este procedimento foi necessário para verificar se os 6 redutos apresentavam desempenho aproximado.

A ostra os resultados obtidos de cada reduto apresentado à rede SOM.

Resultados da rede SOM com redutos (Arquitetura Híbrid referentes ao Experimento 3).

Observando os resultados da tabela, pode-se concluir que os seis redutos são iguais em número de clusters e se equivalem em tempo de treinamento da rede, no EQ e no ET.

O reduto de número foi desconsiderado em função de o erro topográfico ser zero, o que sugere overfitting/underfitting.

Por apresentar o menor número de atributos o sexto reduto foi o escolhido.

Pode-se visualizar os 10 clusters rotulados de acordo com as classes da base de dados ZOO gerados pela rede SOM com redutos (Arquitetura Híbrid utilizando o sexto reduto).

Pode-se visualizar os 10 clusters rotulados de acordo com as classes da base de dados ZOO gerados pela rede SOM com redutos (Arquitetura Híbrid utilizando o sexto reduto).

Mapa gerado pela segunda fase do Experimento 3, onde os clusters podem ser identificados através de cores e as classes identificadas através das amostras rotuladas.

Na feita a comparação dos resultados entre a rede SOM sem redutos com a Arquitetura Híbrida.

Comparação entre a rede SOM e a Arquitetura Híbrida proposta.

Analisando os números da ica evidente o melhor desempenho da Arquitetura Híbrida sobre a rede SOM sem redutos.

Em resposta à pergunta, "O Mapa Auto-Organizável pode ser melhorado utilizando a Arquitetura Híbrida ", pode-se afirmar que sim, apesar de ser uma base de dados "bem comportada".

Os motivos estão relacionados a seguir, Os valores de EQ e ET foram menores em todos os redutos se comparados aos resultados da rede SOM sem redutos, o que indica uma melhor representação da topologia da estrutura dos dados.

O tempo de treinamento foi ligeiramente menor.

O número de clusters gerados em todos os redutos foi menor, se comparado aos resultados da rede SOM sem redutos, indicando uma melhor similaridade tanto entre os dados quanto entre as diferenças de separação dos agrupamentos.

Desta forma, os clusters apresentaram uma fronteira mais bem definida, melhorando a visualização.

Isto ocorreu devido ao fato de a Arquitetura Híbrida (através do RS) ter reduzido informação não relevante que antes era apresentada à rede SOM (primeira fase do experimento) dificultando a formação das fronteiras entre os clusters.

Dentre vários exemplos pode-se verificar que a Arquitetura Híbrida realizou uma melhor separação entre aves e mamíferos utilizando cores diferentes.

Isto pode ser observado nas amostras swan (cisne) e fruitbat (morcego).

Pode-se visualizar também que a rede SOM sem redutos teve dificuldades em separar os peixes colorindo os clusters com azul muito próximo de aves e mamíferos.

Isto pode ser observado na amostra carp (carp).

No caso da Arquitetura Híbrida foi utilizada outra cor (marrom) para colorir e diferenciar o cluster dos peixes.

Como nos experimentos anteriores, a Arquitetura Híbrida proposta apresentou melhores resultados.

Nos experimentos anteriores, as bases de dados eram conhecidas em termos do número de classes e do número de amostras por classe.

No caso da base de dados Consumidor disponível em SAS, não se conhece o número de amostras por classe e devido ao número de atributos (48), fica difícil reconhecer o perfil dos consumidores através de métodos tradicionais, o que justifica a utilização do KDD.

Assim, a escolha da base de dados Consumidor foi oportuna, pois todas as fases do KDD puderam ser realizadas para a descoberta de conhecimento.

A base de dados Consumidor é uma base de dados que contém informações do consumo de 1968 consumidores (registros) com 48 atributos, sendo 47 atributos condicionais e um atributo de decisão dividido em duas classes.

No KDD é a chamada fase de seleção de dados.

Foi necessário pré-processar a base de dados eliminando o atributo que identificava o número da conta do cliente, o que poderia prejudicar a formação dos clusters pela rede SOM e a geração de redutos pelo RS.

No KDD é fase de pré-processamento da base de dados.

O atributo prefixo do nome é nominal e portanto foi transformado em numérico para poder ser processado pela rede SOM.

A transformação foi realizada da seguinte forma, Ms = 0

Mr = 1

Mrs = e None = 3.

No KDD é a fase de transformação dos dados.

Na primeira fase do experimento apresentou-se à rede SOM a base de dados com todos os atributos condicionais (4e os resultados foram avaliados através do EQ, do ET, do tempo de treinamento da rede SOM e da visualização do mapa).

Na segunda fase do experimento submeteu-se primeiramente a base de dados com todos os atributos condicionais (4ao RS para reduzir o número de atributos gerando os redutos e em seguida a base de dados Consumidor reduzida foi submetida à rede SOM).

Nesta parte do experimento fica claro que o processo de KDD é interativo e iterativo (capítulo, pois, apesar de a fase de transformação já ter sido realizada, retornou-se à fase anterior de pré-processamento para o RS reduzir os atributos da base de dados antes de apresentá-los à rede SOM).

Foram gerados 87 redutos, sendo 16 redutos com atributos, 6redutos com atributos e 8 redutos com 5 atributos.

A escolha do reduto baseou-se no menor esforço computacional.

Escolheram-se então, os 16 redutos que apresentaram um número menor de atributos (, em detrimento dos outros redutos que apresentaram ou 5 atributos).

Dentre os 16 menores redutos a escolha foi aleatória já que apresentaram resultados semelhantes.

O reduto escolhido foi o formado pelos seguintes atributos, valor da casa do consumidor, freqüência de pedidos e idade do consumidor.

O número de épocas foi calculado da seguinte forma, rough phase = quantidade de neurônios (22/ tamanho de dados x 10 = 12,0 e a fine tune = quantidade de neurônios (22/ tamanho de dados x 40 = 58,0).

Somando as duas fases (12,0 + 46,0) temos 58 épocas.

Primeira fase dos experimentos, rede SOM sem redutos A primeira fase do experimento foi realizada com a apresentação da base de dados à rede SOM sem redutos.

No processo de KDD é a fase chamada de Data Mining.

Podem-se visualizar os 1clusters gerados pela primeira fase do experimento.

Visualização dos 1clusters gerados pela rede SOM sem redutos, rotulados de acordo com as classes da base de dados Consumidor.

Segunda fase dos experimentos, rede SOM com redutos (Arquitetura Híbrid).

Na segunda fase do experimento a base de dados Consumidor foi reduzida pelo RS e depois apresentada à rede SOM (Arquitetura Híbrid).

No processo de KDD é a fase chamada de Data Mining.

Pode-se visualizar os 9 clusters gerados pela segunda fase do experimento.

Visualização dos 9 clusters gerados pela rede SOM com redutos (Arquitetura Híbrid rotulados de acordo com as classe da base de dados Consumidor (segunda fase do Experimento 4).

Uma forma de interpretar e avaliar os resultados extraídos pelo Data Mining é utilizar técnicas de visualização.

Mapas que mostram as características dos clusters gerados pela rede SOM, auxiliando na fase de interpretação e avaliação dos resultados desta fase do KDD.

Comparação entre as duas fases do experimento 4.

Números do Experimento comparando a rede SOM sem redutos com a rede SOM com redutos (Arquitetura Híbrid).

Não é a finalidade deste trabalho avaliar o conteúdo dos clusters gerados pela rede SOM e sim a qualidade de formação desses clusters pela Arquitetura Híbrida comparada com a rede SOM.

Essa investigação poderia ficar a cargo de um classificador como por exemplo, uma outra arquitetura de rede neural artificial, o próprio RS, Algoritmos Genéticos ou utilizar-se de ferramentas para descobrir associações.

Entretanto, em caráter didático procurou-se mostrar que a Arquitetura Híbrida por intermédio da rede SOM agrupou os registros da base de dados Consumidor obedecendo aos critérios de igualdade ou de semelhança entre os registros.

A rede SOM utilizada no experimento é uma rede neural artificial 15 X 15, ou seja, ela possui 225 neurônios, agrupados em nove clusters contendo os registros dos consumidores (F ou M), por igualdade ou semelhança.

Foram destacados aleatoriamente neurônios para verificar se realmente a rede SOM agrupou segundo esses critérios.

Visualização dos neurônios localizados no mapa gerado pela Arquitetura Híbrida.

RS com a rede SOM (Arquitetura Híbrid agrupou os registros da base de dados Consumidor de acordo com os valores de cada atributo, valor da casa do consumidor, a freqüência de pedidos e a idade do consumidor (reduto utilizado).

Informações dos atributos (reduto) consideradas pela rede SOM para agrupar os neurônios.

Pode-se verificar na tabela que o neurônio 15 e o neurônio 90 (destacados em negrito) têm atributos com valores semelhantes e por isso pertencem ao mesmo cluster.

Pode-se concluir que a rede SOM agrupou os registros obedecendo aos critérios de igualdade e semelhança de atributos.

O neurônio 1 pertencente ao cluster de cor amarela mostra que este é um cluster de consumidores que possuem residência com valor intermediário, é um cliente com um número mais alto de pedidos e de meia-idade.

Os neurônios 15 e 90 pertencentes ao mesmo cluster de cor azul mostram que este é um cluster de consumidores que possuem um valor de residência mais baixo, freqüência de pedidos num intervalo médio e de meia-idade.

O neurônio 225 pertencente ao cluster de cor vermelha mostra que o cluster é de consumidores que possuem casa com valor mais alto, freqüência de pedidos média e são mais jovens.

As conclusões a respeito de alto, médio e baixo valor da casa, freqüência de pedidos alta, média e baixa, e meia-idade ou jovem foram feitas com base na tabela.

Percebe-se que a base de dados Consumidor possui um número muito maior de casos do sexo feminino do que do sexo masculino.

Isto poderia ser facilmente percebido em bases de dados pequenas, porém, a dificuldade seria maior em relação a bases de dados com milhões de registros.

Este fato justifica os clusters apresentarem um número maior de letras F (feminino) do que M (masculino) e a colocação no mesmo cluster dos dois sexos sugere perfis de consumo iguais ou muito semelhantes, que podem ser verificados com uma maior profundidade utilizando um classificador ou ferramentas de associação, como já foi comentado.

No processo de KDD esta fase também é conhecida como pós-processamento.

Após a fase de mineração de dados é necessária a interpretação do conhecimento descoberto, ou algum processamento desse conhecimento.

Pode-se verificar neste experimento uma típica aplicação do KDD em marketing buscando descobrir segmentos de clientes na base de dados (capítulo2, seção 24).

Neste experimento todas as fases do processo de KDD foram realizadas.

A aplicação da Arquitetura Híbrida na base de dados Consumidor reduziu os valores de EQ e ET, indicando uma melhor representação da topologia da estrutura dos dados.

O tempo de treinamento da rede SOM também foi menor.

Houve também uma redução no número de clusters, indicando uma melhor similaridade tanto entre os dados quanto entre as diferenças de separação dos agrupamentos.

Os resultados levaram a concluir que o RS reduziu informação que era apresentada à rede SOM melhorando a formação dos clusters.

Assim, houve melhora na visualização do mapa em função de melhor definição das fronteiras.

A melhora na formação dos clusters possibilita um maior entendimento da base de dados da organização a fim de realizar atividades como, direcionamento nas campanhas de vendas, promoção de ofertas combinadas de serviços ou produtos, avaliação do comportamento do mercado e detectação de novas tendências mercadológicas ou necessidades de consumo.

O experimento possibilitou também maior conhecimento da base de dados, sabe-se que o número de registros do sexo feminino é muito maior do que do sexo masculino e existem perfis de consumo semelhantes entre os dois sexos.

Pode-se concluir novamente que a Arquitetura Híbrida apresentou melhores resultados do que a rede SOM sem redutos.

Esta base de dados foi usada para a Competição de Data Mining ocorrida no ano de 2000 denominada "The COIL 2000 Data Mining Competition".

O "Desafio COIL 2000" foi organizado por um conjunto de redes de excelência custeadas pela União Européia, denominada COIL (Computational Intelligence and Learning), congregando esforços nas áreas de Redes Neurais Artificiais, Sistemas Fuzzy, Computação Evolucionária e Aprendizado de Máquina para extrair conhecimento da base de dados The Insurance Company.

O conhecimento extraído possibilitaria entender melhor as características da base de dados auxiliando no desenvolvimento de estratégias de marketing mais eficazes.

O objetivo proposto na competição foi predizer quem seriam os potenciais consumidores interessados em seguros para trailers.

Foram gerados 29 artigos utilizando a base e que podem ser acessados no seguinte endereço.

É uma base de dados real da área de marketing composta por dados de consumo e dados sócio-demográficos de 582consumidores com 86 atributos numéricos.

A empresa Dutch Data Mining Company Sentient Machine Research, proprietária da base de dados, solicita o uso apenas para pesquisas não comerciais e para propósitos educacionais.

Mais informações podem ser obtidas no seguinte endereço.

A base de dados The Insurance Company foi incorporada em UCI (Repository of Machine Learning Databases).

Irvine, CA, University of California, Department of information and Computer Science.

Os 582registros contêm informações sobre 10 tipos de consumidores descritos a seguir, consumidores bem-sucedidos, produtores de hortifrutigranjeiros, consumidores de classe média, consumidores solitários, consumidores felizes com a sua condição social, consumidores da terceira idade que gostam de viajar, consumidores aposentados, famílias sem crianças, famílias conservadoras, fazendeiros.

Não se conhece a representatividade de cada tipo de consumidor dentro da base de dados.

Por este motivo e pelo número de atributos (8a base de dados The Insurance Company foi escolhida).

Primeira fase dos experimentos, rede SOM sem redutos.

Na primeira fase do experimento apresentou-se à rede SOM a base de dados com todos os atributos (8e os resultados foram avaliados através do EQ, ET, do tempo de treinamento da rede, do número de clusters gerados e da visualização do mapa).

Clusters gerados pela rede SOM sem redutos.

Visualização dos clusters gerados pela rede SOM sem redutos (primeira fase do Experimento 5).

Vale ressaltar que os clusters estão rotulados de acordo com as classes da base de dados The Insurance Company e os números na barra ao lado do mapa representam apenas a quantidade de clusters presentes no mapa.

Segunda fase dos experimentos, rede SOM com redutos (Arquitetura Híbrid).

Na segunda fase do experimento submeteu-se a base de dados ao RS para reduzir o número de atributos e gerar os redutos, a base de dados reduzida foi submetida à rede SOM.

Visualização dos clusters gerados pela rede SOM com redutos (segunda fase do Experimento 5).

Foram gerados 15redutos, sendo o menor reduto com 51 atributos e o maior com 67 atributos.

Escolheu-se o reduto com menor número de atributos (51) pelo menor esforço computacional requerido.

Pode-se visualizar os 1clusters gerados pela rede SOM com redutos (Arquitetura Híbrid).

Os resultados comparando as duas fases do experimento são apresentados.

Números Comparativos da rede SOM sem redutos com a rede SOM com redutos (Arquitetura Híbrid referentes ao Experimento 5).

O número de épocas foi calculado da seguinte forma, rough phase = quantidade de neurônios (22/ tamanho de dados x 10 = 4,0 e a fine tune = quantidade de neurônios (22/ tamanho de dados x 40 = 16,0).

Somando as duas fases (16,0 + 4,0) temos 20 épocas.

Como nos experimentos anteriores, a aplicação da Arquitetura Híbrida na base de dados The Insurance Company reduziu os valores de EQ, de ET, o tempo de treinamento da rede SOM, o número de clusters gerados e a visualização do mapa, caracterizando novamente uma melhoria na representação da topologia da estrutura dos dados, na similaridade entre os dados e na diferença de separação dos agrupamentos.

Os resultados levaram a concluir que o RS reduziu informação que era apresentada à rede SOM melhorando a formação dos clusters.

Assim, houve melhora na visualização do mapa em função de melhor definição das fronteiras.

No caso da rede SOM sem redutos, os clusters de maneira geral, ficaram espalhados no mapa, indicando dificuldades na separação dos agrupamentos e na similaridade entre os dados.

Com a Arquitetura Híbrida, o número de clusters é menor, o que indica um bom resultado.

A realização do experimento possibilitou também um melhor conhecimento da base de dados.

Sabe-se que o número de famílias sem crianças (tipo de consumidor número é muito alto e provavelmente com características muito diversas fazendo-se presente em todos os clusters).

Este fato pode ser justificado, pois entendem-se como famílias sem crianças as famílias sem filhos, as famílias formadas somente por adultos (filhos crescidos) e aquelas com filhos adolescentes.

Como no experimento 4, pode-se dizer que a melhora na formação dos clusters possibilita um maior entendimento da base de dados da organização a fim de realizar atividades como direcionamento nas campanhas de vendas, promoção de ofertas combinadas de serviços ou produtos, avaliação do comportamento do mercado e detectação de novas tendências mercadológicas ou necessidades de consumo.

Pode-se concluir que pré-processar a rede SOM com RS gera clusters mais definidos e de melhor visualização, o que possibilitou um melhor desempenho da Arquitetura Híbrida.

Nos experimentos 1 e foi visto que RS leva em consideração os conceitos de dependência e significância de atributos discutidos no capítulo para gerar os redutos.

Isto leva a concluir que, apesar da redução de informação, os atributos selecionados que compõem os redutos conseguem representar a base de dados original (com todos os atributos).

A continuação dos experimentos 1 e compara o desempenho da rede SOM sem redutos com a Arquitetura Híbrida proposta (RS com a rede SOM).

A avaliação visual dos mapas gerados mostrou um melhor desempenho da Arquitetura Híbrida, que reduziu a sobreposição das classes gerando fronteiras mais bem definidas entre os clusters.

Estes dois experimentos iniciais foram realizados com bases de dados simples e tiveram uma importância vital para o desenvolvimento deste trabalho, pois, se RS não gerasse redutos preservando a representatividade de todos os atributos da base de dados inicial ou o mapa gerado pela Arquitetura Híbrida fosse inferior ao mapa gerado pela rede SOM sem redutos, este trabalho não teria continuidade.

No experimento a base ZOO foi escolhida por possuir todos os atributos numéricos, não ter dados faltantes e ser composta por várias classes.

Com estas características provavelmente a rede SOM sem redutos deveria gerar um mapa com clusters de boa qualidade.

Assim, abordou-se a questão de se a Arquitetura Híbrida conseguiria melhorar o mapa.

Comparadas as duas fases do experimento 3, a Arquitetura Híbrida apresentou os melhores resultados de EQ, ET, do tempo de treinamento da rede SOM e da visualização do mapa.

O experimento teve como característica o não-conhecimento do número de amostras por classe e um número maior de atributos com relação aos experimentos 1, e 3, portanto, sendo difícil conhecer o perfil dos consumidores.

As duas fases foram comparadas analisando-se os resultados do EQ, do ET, do tempo de treinamento da rede SOM e da visualização do mapa.

Novamente a Arquitetura Híbrida apresentou os melhores resultados.

No experimento 5 não se conhece a representatividade de cada tipo de consumidor (dentro da base de dados e o número de atributos é alto (86).

Também neste experimento os resultados da Arquitetura Híbrida foram melhores, comparados aos da rede SOM sem redutos.

Em geral quando se fala de redução de dados sempre vem à mente um ganho em tempo de processamento, o que nos experimentos realizados neste trabalho ocorreu sensivelmente.

Entretanto, a redução realizada pelo RS fez com que informação considerada desnecessária não fosse apresentada à rede SOM, melhorando as fronteiras entre os clusters gerados pela rede SOM através de uma melhor separação dos agrupamentos, uma melhor similaridade entre os dados (número menor de clusters) e uma melhor representação da topologia da estrutura dos dados.

Deve-se considerar que uma melhor definição das fronteiras dos clusters após a redução realizada pelo RS reduz a incerteza sobre as informações, tornando os conceitos de Aproximação Inferior, Aproximação Superior, Região de Borda e Região Negativa mais precisos melhorando o desempenho de um classificador.

A Arquitetura Híbrida proposta teve um desempenho superior ao apresentado pela rede SOM sem redutos em todos os cinco experimentos.

Os avanços da Tecnologia da Informação têm possibilitado o armazenamento da ordem de terabytes (T de informação).

Sem o auxílio de ferramentas computacionais apropriadas, a análise destas grandes quantidades de informação é inviável.

Surge então uma questão, "Como aproveitar essa grande quantidade de informações armazenadas em benefício das organizações e no apoio à decisão de uma forma geral ".

Por essa razão, faz-se necessário o uso de sistemas que possam extrair o conhecimento dessas bases, viabilizem a análise dessas informações, denominados KDD (Knowledge Discovery Databases) ou Descoberta de Conhecimento em Bases de Dados.

Motivado por este cenário, desenvolveu-se neste trabalho uma Arquitetura Híbrida que combina duas técnicas, a Teoria dos Rough Sets (Teoria dos Conjuntos Aproximados) e as redes SOM, a fim de descobrir conhecimento em bases de dados.

Na Arquitetura Híbrida, a função dos RS é a de redução de atributos (dimensão) e a função da rede SOM é a de gerar clusters.

Assim, no primeiro capítulo deste trabalho realizou-se uma introdução conceitual dos principais assuntos abordados, além da justificativa e do objetivo que levaram ao seu desenvolvimento.

No segundo capítulo foi abordada a importância da informação para as organizações e a necessidade de descobrir conhecimento para apoiar a tomada de decisão.

No terceiro capítulo, estudou-se o processo de descoberta de conhecimento em bases de dados, denominado KDD, a fim de embasar o desenvolvimento da Arquitetura Híbrida proposta.

No quarto capítulo foi apresentado o conceitual e o embasamento matemático que envolvem a Teoria dos Rough Sets, e no quinto capítulo estudaram-se os principais conceitos das redes neurais artificiais com ênfase na rede SOM.

No sexto capítulo, a Arquitetura Híbrida proposta foi aplicada em cinco bases de dados diferentes, e o seu desempenho avaliado através do erro de quantização, do erro topográfico, do tempo de treinamento da rede SOM, do número de clusters e do mapa gerado utilizando o Mapa por Similaridade de Cor.

Inicialmente, por se tratar de uma temática emergente, esforços foram feitos nas revisões de literatura e em particular na formulação do texto do capítulo que aborda RS, sendo necessária a padronização da notação matemática objetivando maior clareza.

Em um segundo momento, o trabalho foi desenvolvido no sentido de conseguir combinar RS com a rede SOM, numa abordagem voltada à mineração de dados em que uma das formas de visualização do mapa gerado, denominada Mapa por Similaridade de Cor, foi também utilizada como um dos indicadores do desempenho da Arquitetura Híbrida na descoberta de conhecimento.

Como citado em Goldschmidt, técnicas podem ser combinadas para gerar as chamadas Arquiteturas Híbridas.

A grande vantagem desse tipo de sistema deve-se ao sinergismo obtido pela combinação de duas ou mais técnicas.

Este sinergismo resulta na obtenção de um sistema mais poderoso (em termos de interpretação, de aprendizado, de estimativa de parâmetros, de generalização, dentre outros) e com menos deficiências.

Foi o que ocorreu na comparação do desempenho entre a rede SOM sem redutos (primeira fase de cada experimento) com a Arquitetura Híbrida proposta (rede SOM com redutos, segunda fase de cada experimento).

O desempenho foi maior na Arquitetura Híbrida e pode ser verificado no capítulo 6 na avaliação dos resultados e na análise final dos resultados, onde a Arquitetura Híbrida apresentou, em todos os cinco experimentos, menor erro de quantização, menor erro topográfico, menor tempo de treinamento da rede SOM, menor número de clusters e melhor visualização do mapa gerado.

Os menores valores de EQ e ET indicam que a Arquitetura Híbrida conseguiu representar a topologia dos dados de entrada melhor do que a rede SOM sem redutos.

Como os clusters são obtidos por intermédio da aplicação dos conceitos de similaridade e distância, um número menor de clusters indica melhor similaridade entre os dados e maior diferença de separação dos clusters, ou seja, alta similaridade intraclasse e baixa similaridade interclasses.

A análise visual do Mapa por Similaridade de Cor também indicou melhor visualização dos clusters gerados pela Arquitetura Híbrida.

Em geral quando se fala de redução de dados sempre vem à mente um ganho em tempo de processamento, o que nos experimentos realizados neste trabalho ocorreu de maneira sensível (menor tempo de treinamento da rede SOM).

Entretanto, a redução de atributos realizada pelo RS ao pré-processar a rede SOM fez com que informação considerada desnecessária não fosse apresentada à rede, melhorando as fronteiras entre os clusters gerados.

Esta informação desnecessária, quando submetida à rede SOM sem redutos, gerou incerteza, ocasionando em certos clusters uma definição de fronteira ruim, prejudicando a separação dos agrupamentos.

Isto pode ser observado nos experimentos realizados com a rede SOM sem redutos no capítulo 6, seção 631, seção 632, seção 633, seção 63 e seção 614.

A combinação de RS com a rede SOM em uma Arquitetura Híbrida fez com que uma das principais deficiências da rede SOM (a definição de fronteira entre os clusters) fosse reduzida, levando à conclusão de que em muitos casos é necessária a combinação de duas ou mais técnicas a fim de eliminar ou reduzir certas deficiências individuais de cada técnica.

O objetivo principal deste trabalho foi, através da análise dos resultados obtidos com a Arquitetura Híbrida proposta, procurar responder a uma questão fundamental, "Existe melhoria nos resultados gerados pela rede SOM quando pré-processada por RS ".

Assim, com base nos experimentos realizados e nos resultados apresentados, pode-se concluir que a Arquitetura Híbrida teve um desempenho superior ao da rede SOM sem redutos e responder sim à questão fundamental do trabalho, ou seja, existe melhoria nos resultados gerados pela rede SOM quando pré-processada por RS.

Acredita-se que, além do objetivo principal, o trabalho conseguiu atingir um outro objetivo, que foi o de possibilitar um maior conhecimento sobre RS como técnica de redução de atributos, analisando para tanto os conceitos principais e o formalismo matemático apresentados no capítulo 4.

A redução da incerteza e a conseqüente melhora na geração dos clusters obtida com a Arquitetura Híbrida possibilitam a formação de clusters mais confiáveis, pois os elementos da base de dados que estavam na região de fronteira dos clusters agora são agrupados melhor.

Isto pode resultar numa geração de regras mais confiáveis por parte de um classificador ao ser utilizado após a submissão da base de dados à Arquitetura Híbrida.

Resumindo, as contribuições do presente trabalho foram as seguintes, Desenvolvimento de uma Arquitetura Híbrida (RS com rede SOM) que pode ser utilizada com mais vantagem em relação à rede Som sem redutos (capítulo na descoberta de conhecimento em bases de dados).

Maior conhecimento da Teoria dos Rough Sets (capítulo 4).

A verificação do desenvolvimento de todo o processo de KDD realizado no Experimento com a Base de Dados Consumidor (capítulo 6, seção634).

A verificação da fase de mineração de dados, onde a rede SOM se utiliza de certos critérios para agrupar os elementos e formar os clusters.

A ilustração detalhada de como a Teoria dos Rough Sets gera os redutos (capítulo 4, seção 45).

Finalmente, com base nos resultados obtidos, pode-se considerar que a aplicação da Arquitetura Híbrida pode ser vantajosa em diversas áreas alvo de KDD, como marketing (detecção de perfil do consumidor, de necessidades de consumo, de clientes que podem abandonar a fidelidade aos produtos ou serviços de uma organização, de tendências mercadológicas).

Medicina (imagens, diagnósticos), governamental (detecção de fraudes, perfil do sonegador de impostos), imagens espaciais (imagens de satélite, sistemas de informação geográfica, posicionamento por satélites), financeiras (concessão de crédito, descoberta de áreas seguras para investimentos), dentre outras.

A aplicação da Arquitetura Híbrida pode ser realizada nos estudos citados no capítulo 4, bem como pode vir a gerar interessantes frutos no contexto dos seguintes trabalhos, Lingras, em que o algoritmo da rede SOM foi modificado com base nos conceitos de Aproximação Inferior e Superior dos RS buscando um melhor intervalo entre os clusters.

Neste caso, a Arquitetura Híbrida reduziria informação desnecessária, a fim de melhorar a definição das regiões de borda dos clusters.

Sankar, que combina Rough-Fuzzy MLP em uma Arquitetura Híbrida.

Aqui, poderia ser desenvolvida uma Arquitetura Híbrida Rough SOM-Fuzzy MLP.

Lingras, que utiliza RS com Algoritmos Genéticos para gerar clusters em mineração de dados na Internet.

Neste caso, poderíamos substituir os Algoritmos Genéticos pela rede SOM.

Pawlak, aplicação da Arquitetura Híbrida em computação granular.

Skowron, aplicação da Arquitetura Híbrida em mereologia.

Dar continuidade aos estudos da Arquitetura Híbrida proposta neste trabalho, incluindo RS como um classificador dos clusters gerados pela rede SOM, formando uma nova Arquitetura Híbrida RS-SOM-RS.

O objetivo é verificar se haverá melhora na definição dos conceitos de Aproximação Inferior, Aproximação Superior, Região de Borda e Região Negativa.

Verificar o desempenho da Arquitetura Híbrida proposta utilizando bases de dados que apresentem um número muito maior de registros do que as utilizadas nos experimentos deste trabalho.

Otimização do número de neurônios da rede SOM.

Os estudos aqui realizados não têm a pretensão de esgotar o assunto, pelo contrário, buscou-se realizar uma contribuição com o desenvolvimento da Arquitetura Híbrida na descoberta de conhecimento em bases de dados.

Sabe-se que existe uma clara demanda por estudos sistematizados que possam estabelecer outros domínios de aplicação ainda mais adequados para a Arquitetura Híbrida proposta.

Este cenário oferece portanto amplo espaço para trabalhos de continuidade.

Este anexo tem como objetivo ilustrar os passos necessários para a reprodução dos experimentos utilizando as ferramentas de ensaio.

Para maiores detalhes sobre as bases de dados e parâmetros utilizados, veja o Capítulo 6.

Para a redução de dados com Rough Sets usou-se a ferramenta chamada Rosetta.

O processo de redução pode ser dividido em duas etapas principais.

A primeira é a escolha da base de dados e abertura do arquivo pela ferramenta.

Ilustração da tela do Rosetta com a base de dados carregada.

A segunda etapa é a redução dos atributos da base de dados.

Para isto deve-se escolher o algoritmo de redução, que no exemplo é o algoritmo genético (SAVGeneticReducer (Genetic Algorithm)).

Ilustração da tela do Rosetta na escolha do algoritmo e dos parâmetros de redução.

A seguir, os redutos gerados pelo Rough Sets.

Anexo A, Roteiro de Execução dos Experimentos Comparativos com a Arquitetura Híbrida.

Ilustração da tela do Rosetta com os redutos gerados pelo Rough Sets utilizando a base de dados Iris.

Informações mais detalhadas sobre a ferramenta Rosetta podem ser encontradas em Pila.

O experimento tem seqüência apresentando a base de dados reduzida à rede SOM.

Para a realização dos experimentos com a rede SOM, usou-se o SOMToolbox descrito na Seção 611.

O código fonte de chamada das funções do SOMToolbox está descrito a seguir,