Muitos dados são coletados e armazenados, Web data, e-commerce.

Compras em departamentos/ supermercados.

Bancos / Transações com cartão de crédito.

Computadores se tornaram baratos e mais potentes.

Pressão competitiva é forte, Fornecer serviços melhores e personalizados como um diferencial (em CRM).

Dados coletados a enormes velocidades GB/hor, Sensores remotos em satélites.

Telescópios sondando o céu.

Micro-arranjos gerando dados de expressão gênica.

Simulações científicas.

Gerando terabytes de dados Técnicas tradicionais inviáveis para dados brutos.

Mineração de dados pode ajudar cientistas, classificando e segmentando dados na Formulação de Hipóteses.

Freqüentemente há informação "escondida" nos dados que não estão prontamente evidente.

Analistas humanos podem levar semanas para descobrir informação útil.

Muito dos dados não é analisada nunca.

Muitas definições, Extração não-trivial de informação implícita, previamente desconhecida e potencialmente útil a partir dos dados.

Exploração & análise, por meios semiautomáticos ou automáticos, de grandes quantidades de dados a fim de descobrir padrões significativos.

O que não é Mineração de Dados.

Buscar número de telefone em catálogo.

Perguntar a um motor de busca por informação sobre "Amazon".

O que é Mineração de Dados.

Certos sobrenomes são mais comuns em certas regiões dos EUA.

Agrupar documentos similares retornados por um motor de busca de acordo com o contexto.

Usa idéias de aprendizagem de máquina / IA, reconhecimento de padrões, estatística, e sistemas de bases de dados.

Técnicas tradicionais não são adequadas, Quantidade de dados.

Alta dimensionalidade dos dados.

Natureza distribuída e heterogênea dos dados.

Métodos Preditivos, Usa algumas variáveis para prever valores desconhecidos ou futuros de outras variáveis.

Métodos Descritivos, Encontra padrões compreensíveis por humanos para descrever os dados.

Dado um conjunto de registros (conjunto de treinamento), Cada registro contém um conjunto de atributos, um dos atributos é a classe.

Encontrar um modelo para o atributo classe como uma função dos valores dos outros atributos.

Objetivo, a registros previamente não-usados deve ser assinalada uma classe tão precisamente quanto possível.

Um conjunto de testes é usado para determinar a precisão do modelo.

Usualmente, o conjunto de dados é dividido em conjunto de treinamento e conjunto de testes, sendo o conjunto de treinamento usado para construir o modelo e o conjunto de testes usado para validá-lo.

Marketing Direto Objetivo, Reduzir custo de propaganda escolhendo um conjunto de clientes que provavelmente comprarão um novo produto celular.

Abordagem, Usar os dados de maneira similar ao exemplo anterior.

Sabe-se quais clientes decidiram comprar o produto e quais não.

Esta decisão {comprar, não-comprar} forma o atributo classe.

Coletar várias informações demográficas, de estilo de vida, e de interação com a empresa relacionadas a todos os clientes.

Tipo de negócio, onde eles ficam, quanto recebem, etc.

Usar esta informação como atributos de entrada para treinar um modelo de um classificador.

Detecção de Fraude Objetivo, Prever casos fraudulentos em transações de cartão de crédito.

Abordagem, Usar transações de cartão de crédito e a informação sobre os clientes como atributos.

Quando um cliente compra, o que ele compra, quão freqüentemente ele paga em dia, etc Rotular as transações passadas como transação do tipo fraude ou honesta.

Isto forma o atributo classe.

Treinar um modelo para a classe das transações.

Usar este modelo para detectar fraude observando transações de cartão de crédito sobre uma conta.

Insatisfação de clientes, Objetivo, Prever se um cliente tem propensão a migrar para um competidor.

Abordagem, Usar registros detalhados de transações de cada um dos clientes passados e atuais, para montar atributos.

Quão freqüentemente o cliente liga, para que setor ele liga, em que horário do dia ele liga mais, seu estado financeiro, estado civil, etc.

Rotular o cliente como leal ou não-leal.

Encontrar um modelo para a lealdade.

Catálogo de Pesquisa do Firmamento Objetivo, Prever a classe (estrela ou galáxi de objetos celestes, especialmente os visualmente muito fracos, baseado em imagens de pesquisa de telescópios (do Observatório Palomar).

São 3000 imagens com 23,040 x 23,040 pixels por imagem.

Abordagem, Segmentar a imagem.

Medir atributos da imagem (características) 40 delas por objeto.

Modelar a classe baseado nestas características.

História de Sucesso, Encontrou 16 novos high red-shift quasars, alguns dos objetos mais distantes que são difíceis de encontrar!.

Dado um conjunto de pontos de dados, cada um tendo um conjunto de atributos, e uma medida de similaridade entre eles, encontrar agrupamentos tais que Pontos de dados em um grupo são mais similares entre si.

Pontos de dados em grupos diferentes são menos similares entre si.

Medidas de Similaridade, Distância Euclidiana se os atributos são contínuos.

Outras medidas dependentes do problema.

Segmentação de Mercado, Objetivo, subdividir um mercado em distintos subconjuntos de clientes em que cada subconjunto pode ser visto como um mercado-alvo a ser atingido com uma mistura de marketing distintos.

Abordagem, Coletar diferentes atributos de clientes baseado em informação relacionada ao seu estilo e posição geográfica.

Encontrar grupos de clientes similares.

Medir a qualidade dos grupos observando padrões de compra dos clientes no mesmo grupo versus aqueles de diferentes grupos.

Agrupamento de Documentos, Objetivo, Encontrar grupos de documentos que são similares entre si baseado nos termos importantes que aparecem neles.

Abordagem, Identificar termos que ocorrem com freqüência em cada documento.

Formar uma medida de similaridade baseada na freqüência dos diferentes termos.

Usá-la para agrupar.

Ganho, Recuperação de Informações pode utilizar os grupos para relacionar um novo documento ou termo de pesquisa aos documentos agrupados.

Dado um conjunto de registros, cada um dos quais contém certo número de itens de uma coleção.

Produzir regras de dependência que predirão a ocorrência de um item baseado nas ocorrências de outros itens.

Gerenciamento de prateleira de Supermercado.

Objetivo, Identificar itens que são comprados juntos por um número suficiente de clientes.

Abordagem, Processar os dados coletados no ponto-de-venda com scanners de código de barras para encontrar dependências entre itens.

Um regra clássica-Se um cliente compra fraldas e leite, então ele provavelmente comprará cerveja.

Portanto, não se surpreenda se você encontrar engradados de cerveja próximos às fraldas!.

Gerenciamento de Inventário, Objetivo, Uma companhia de reparos de aparelhos domésticos quer antecipar a natureza dos reparos nos produtos de seus clientes e manter os veículos de serviço equipados com as partes certas para reduzir o número de visitas às casas dos clientes.

Abordagem, Processar os dados sobre ferramentas e partes necessárias em reparos prévios em diferentes localizações de clientes e descobrir os padrões de coocorrência.

Dado um conjunto de objetos, cada objeto associado com sua própria linha do tempo de eventos, encontrar regras que predigam fortes dependências seqüenciais entre diferentes eventos.

Regras são formadas descobrindo inicialmente padrões.

As ocorrências de eventos nos padrões são governadas pelas restrições temporais.

Prevê um valor de uma dada variável continuamente valorada baseada nos valores de outras variáveis, assumindo um modelo de dependência linear ou nãolinear.

Muito estudado nos campos da estatística e redes neurais.

Exemplos, Prever quantidade de vendas de um novo produto baseado nos gastos de propaganda.

Prever velocidade dos ventos como uma função da temperatura, umidade, pressão do ar, etc.

Previsão de séries temporais de índices do mercado financeiro.

Detectar desvios significantes do comportamento normal Aplicações, Detecção de Fraudes em Cartões de Créditos.

Detecção de Intrusão em Redes.

Coleção de objetos de dados e seus atributos.

Um atributo é uma propriedade ou característica de um objeto, Exemplos, cor dos olhos de uma pessoa, temperatura, etc.

Atributo também é conhecido como variável, campo ou característica.

Uma coleção de atributos descreve um objeto, Objeto também é conhecido como registro, ponto, caso, amostra, entidade, ou instância.

Valores de atributos são números ou símbolos assinalados a um atributo.

Distinção entre atributos e valores de atributos Um mesmo atributo pode ser mapeado em diferentes valores de atributo.

Exemplo, altura pode ser medida em pés ou metros Diferentes atributos podem ser mapeados no mesmo conjunto de valores.

Exemplo, Valores de atributo para ID e idade são inteiros.

Mas propriedades dos valores dos atributos podem ser diferentes.

ID não tem limite mas idade tem um valor máximo e um mínimo.

A forma com que se mede um atributo pode, às vezes, não estar de acordo com as propriedades dos atributos.

Há diferentes tipos de atributos, Nominal.

Exemplos, números de ID, cor dos olhos, códigos de CEP.

Exemplos, datas de calendário, temperaturas em Celsius ou Fahrenheit.

Razão.

Exemplos, temperatura em Kelvin, comprimento, tempo, contagem.

O tipo de um atributo depende de quais das seguintes propriedades ele possui, Distinção.

Ordem.

Adição.

Multiplicação.

Atributo Nominal, distinção.

Atributo Ordinal, distinção e ordem.

Atributo Intervalar, distinção, ordem e adição.

Atributo Razão, todas as quatro propriedades.

Atributo Discreto, Tem um conjunto de valores finito ou contavelmente infinito.

Exemplos, código CEP, contagens, ou o conjunto de palavras em uma coleção de documentos.

Freqüentemente representados como variáveis inteiras.

Nota, atributos binários são um caso especial de atributos discretos.

Atributos Contínuos, Tem números reais como atributos de valores.

Exemplos, temperatura, altura, ou peso.

Na prática, valores reais somente podem ser medidos e representados usando um número finito de dígitos.

Atributos Contínuos são representados tipicamente como variáveis de ponto flutuante.

Registro, Matriz de dados.

Dados de documentos.

Dados de transações.

Grafo, World Wide Web.

Estruturas Moleculares.

Ordenados, Dados espaciais.

Dados temporais.

Dados seqüenciais.

Dados de seqüências genéticas.

Dimensionalidade.

Maldição da Dimensionalidade, esparsidade.

Somente a presença importa, resolução.

Padrões dependem da escala.

Dados que consistem de um coleção de registros, cada um dos quais consiste de um conjunto fixo de atributos.

Se os objetos de dados tem o mesmo conjunto fixo de atributos numéricos, então os objetos de dados podem ser vistos como pontos em um espaço multidimensional, em que cada dimensão representa um atributo distinto.

Tal conjunto de dados pode ser representado por uma matriz m por n, em que há m linhas, uma para cada objeto, e n colunas, uma para cada atributo.

Cada documento torna-se um vetor de 'termos', cada termo é um componente (atributo) do vetor, O valor de cada componente é o número de vezes que o termo correspondente ocorre no documento.

São dados de registro de um tipo especial, em que cada registro (transação) envolve um conjunto de itens.

Por exemplo, considere um supermercado.

O conjunto de produtos comprados por um cliente durante constitui uma transação, enquanto os produtos individuais comprados são os itens.

Exemplos de problemas de qualidade nos dados, Ruídos e outliers.

Dados faltantes.

Dados duplicados.

Ruído refere-se à modificação de valores originais, Exemplos, distorção da voz de uma pessoa falando.

Outliers são objetos de dados com características que são consideravelmente diferentes da maioria dos outros objetos de dados no conjunto de dados.

Razões para valores faltantes, Informação não foi coletada (pessoas não fornecem sua idade e peso).

Atributos podem não ser aplicáveis a todos os casos (salário anual não é aplicável a crianças).

Manipulando valores faltantes, Eliminar objetos de dados.

Estimar valores faltantes.

Ignorar valores faltantes durante análise.

Substituir por todos os valores possíveis (ponderados por suas probabilidades).

Conjunto de dados pode incluir objetos de dados que são duplicatas, ou quase duplicadas de outros, Grande problema quando unindo dados de fontes heterogêneas Exemplos, Mesma pessoa com múltiplos endereços de email Limpeza dos dados, Processo de trabalho com dados duplicados.

Combinar dois ou mais atributos (ou objetos) em um único atributo (ou objeto) Finalidade, Redução de dados.

Reduzir o número de atributos ou objetos, Alteração de escala.

Cidades agregadas em regiões, estados, países, etc, Dados mais "estáveis".

Dados agregados tendem a ter menor variabilidade.

Amostragem é a principal técnica empregada na seleção de dados, Usada freqüentemente tanto para investigação preliminar dos dados quanto para análise final dos dados.

Estatísticos amostram porque obter o conjunto completo dos dados de interesse é muito caro ou consome tempo demais.

Amostragem é usada em mineração de dados porque o processamento do conjunto inteiro dos dados de interesse é muito caro ou consome tempo demasiado.

O princípio básico para amostragem efetiva é o seguinte, usando uma amostra funcionará tão bem quanto usando o conjunto completo de dados se a amostra é representativa.

Uma amostra é representativa se ela tem aproximadamente as mesmas propriedades (de interesse) quanto o conjunto original de dados.

Amostragem simples aleatória, Há uma probabilidade igual de selecionar qualquer item particular.

Amostragem sem reposição, À medida que cada item é selecionado, ele é removido da população.

Amostragem com reposição, Objetos não são removidos da população quando são selecionados para compor a amostra.

Na amostragem com reposição, o mesmo objeto pode ser escolhido mais de uma vez.

Amostragem estratificada, divide os dados em várias partições e retira então amostras aleatórias de cada uma das partições.

Quando a dimensionalidade aumenta, os dados tornam-se muito esparsos no espaço que ocupam.

Definições de densidade e distância entre pontos, que são críticas para agrupamento e detecção de outliers, passam a ter menos significado.

Finalidade, Reduzir a maldição da dimensionalidade.

Reduzir a quantidade de tempo e memória necessárias pelos.

Algoritmos de mineração de dados.

Permitir que os dados sejam mais facilmente visualizados.

Ajudar a eliminar características irrelevantes ou a reduzir o ruído.

Técnicas, Análise de Componentes Principais PCA.

Singular Value Decomposition SVD.

Outros, técnicas supervisionadas e não-lineares.

O objetivo é encontrar a projeção que captura a maior quantidade de variação nos dados.

Achar os autovetores da matriz de covariância.

Os autovetores definem o novo espaço.

Construir um grafo de vizinhança.

Para cada par de pontos no grafo, calcular as distâncias de menor caminho distâncias geodésicas.

Características redundantes, Duplicam muita ou toda a informação contida em um ou mais atributos.

Exemplo, preço de venda de um produto e a quantidade de taxas de venda pagas.

Características irrelevantes, Não contém informação que seja útil para a tarefa de mineração de dados sendo executada.

Exemplo, ID do estudante é freqüentemente irrelevante na tarefa de prever o seu desempenho.

Técnicas, Abordagem de força bruta, Tenta todos os subconjuntos possíveis de características como entrada para o algoritmo de mineração de dados.

Abordagem embutidas, Seleção de características ocorre naturalmente como parte do algoritmo de mineração de dados.

Abordagem filtro, Características são selecionadas antes que o algoritmo de mineração de dados seja executado.

Abordagem wrapper, Uso o algoritmo de mineração de dados como uma caixa preta para encontrar o melhor subconjunto de atributos.

Cria novos atributos que podem capturar informação importante em um conjunto de dados muito mais eficientemente que os atributos originais.

Três metodologias gerais, extração de características específicas do domínio.

Mapeamento de dados para novo espaço.

Construção de características.

Combinando características.

Uma função que mapeia o conjunto inteiro de valores de um dado atributo para um novo conjunto de valores de substituição tal que cada valor antigo pode ser identificado com um dos novos valores, Funções simples.

Padronização e Normalização.

Similaridade, Medida numérica de quão parecidos dois objetos.

É maior quando objetos são mais parecidos.

Freqüentemente está na faixa [0,1].

Dissimilaridade, medida numérica de quão diferentes dois objetos são menor quando dois objetos são mais parecidos.

Dissimilaridade mínima é freqüentemente zero.

Limite superior varia.

Proximidade refere-se à similaridade ou dissimilaridade.

Distância de Minkowski é uma generalização da distância Euclidiana, em que r é um parâmetro, n é o número de dimensões (atributos) e pk e qk são, respectivamente, os k-ésimos atributos (componentes) dos objetos de dados p e q.

Distâncias, tais como a Euclidiana, tem algumas propriedades bem conhecidas.

Uma distância que satisfas estas propriedades é uma métrica.

Similaridades também tem algumas propriedades bem conhecidas.

Uma situação comum é que objetos, p e q, tem somente atributos binários.

Calcular similaridades usando as seguintes quantidades, M01 = número de atributos em que p é 0 e q é 1.

M10 = número de atributos em que p é 1 e q é 0.

M00 = número de atributos em que p é 0 e q é 0.

M11 = número de atributos em que p é 1 e q é 1.

Coeficientes Simple Matching e Jaccard, SMC = número de coincidências / número de atributos.

J = número de coincidências 1 / número de valores de atributos não ambos 0.

Variação de Jaccard para atributos contínuos ou contáveis, Reduz-se a Jaccard para atributos binários.

Correlação mede o relacionamento linear entre objetos.

Para calcular a correlação, padronizam-se os objetos de dados, p e q, e faz-se seu produto interno.

Às vezes atributos são de muitos tipos diferentes, mas uma similaridade geral é necessária.

Pode não ser desejado tratar todos os atributos igualmente.

Usar pesos wk que estão entre 0 e 1 com soma 1.

Agrupamento baseado em densidade requer a noção de densidade.

Abordagem mais simples é dividir a região em um número de células retangulares de igual área e definir densidade como número de pontos que a célula contém.

Densidade euclidiana é o número de pontos dentro de um raio específico a partir do ponto.

Em EDA, como originalmente definido por Tukey, O foco está na visualização.

Agrupamento e detecção de anomalias eram vistos como técnicas exploratórias.

Em mineração de dados, agrupamento e detecção de anomalias são grandes áreas de interesse, e não são vistas apenas como exploração.

Nesta discussão de exploração de dados, o foco está em, Estatística Sumária.

Visualização.

Online Analytical Processing (OLAP).

Estatística Sumária são números que resumem as propriedades dos dados, Propriedades sumarizadas incluem freqüência, posição e dispersão.

A maioria das estatísticas sumárias pode ser calculada em um único passo através dos dados.

A freqüência do valor de um atributo é a percentagem do tempo em que o valor ocorre no conjunto de dados, Por exemplo, dado o atributo 'gênero' e uma população representativa de pessoas, o gênero 'feminino' ocorre cerca de 50% do tempo.

A moda de um atributo é o valor mais freqüente do atributo.

As noções de freqüência e moda são usadas tipicamente com dados categóricos.

Para dados contínuos, a noção de percentil é mais útil.

Dados um atributo contínuo ou ordinal x e um número p entre 0 e 100, o p-ésimo percentil é um valor xp de x tal que p% dos valores observados de x são menores que xp.

Por exemplo, o percentil 50 é o valor x50% tal que 50% de todos os valores de x são menores que x50%.

A média é a medida mais comum de posição de um conjunto de pontos.

Entretanto, a média é muito sensitiva a outliers.

Então, a mediana ou uma mediana ajustada é usada comumente.

Faixa é a diferença entre o máximo e mínimo.

A variância ou desvio padrão é a medida mais comum de desvio de um conjunto de pontos.

Entretanto, elas são sensitivas a outliers, e outras medidas são freqüentemente utilizadas.

Visualização é a conversão dos dados para um formato visual ou tabular de tal forma que as características dos dados e os relacionamentos entre itens de dados ou atributos possa ser analisada ou reportada.

Visualização de dados é uma das técnicas de maior apelo e poder para exploração de dados.

Os seres humanos tem uma habilidade bem desenvolvida de analisar grandes quantidades de informação que seja apresentada visualmente.

Pode detectar padrões gerais e tendências.

Pode detectar outliers e padrões não usuais.

É o mapeamento da informação em um formato visual.

Objetos de dados, seus atributos e as relações entre objetos de dados são traduzidos em elementos gráficos tais como pontos, linhas, formatos e cores.

Exemplo, Objetos são representados freqüentemente como pontos.

Seus valores de atributo podem ser representados como a posição dos pontos ou as características dos pontos, cor, tamanho e formato.

Se a posição é usada, então os relacionamentos entre os pontos, se eles formam grupos ou um ponto é um outlier, são facilmente percebidos.

É a colocação de elementos visuais dentro de um display.

Pode fazer uma grande diferença no quão fácil é para compreender os dados.

É a eliminação ou a tirada de ênfase de certos objetos e atributos.

Seleção pode envolver a escolha de um subconjunto de atributos, Redução de dimensionalidade é muito usada para reduzir o número de dimensões para duas ou três.

De outra forma, pares de atributos podem ser usados.

Seleção também pode envolver escolher um subconjunto de objetos, Uma região da tela mostra um número fixo de pontos.

Pode-se amostrar, mas querer preservar pontos em áreas esparsas.

Histograma, Usualmente mostra distribuição de valores de uma variável.

Divide os valores em faixas e mostra um gráfico de barras do número de objetos em cada faixa.

A altura de cada barra indica o número de objetos.

Formato do histograma depende do número de faixas.

Mostra a distribuição conjunta dos valores de dois atributos.

Gráficos de Caixa, Outra forma de indicar a distribuição dos dados.

Figura mostra a parte básica de um gráfico de caixa.

Gráficos de Caixa podem ser usados para comparar atributos.

Gráficos de Dispersão, Valores dos atributos determinam a posição.

Gráficos de dispersão bidimensionais são mais comuns, mas também há gráficos tridimensionais.

Freqüentemente atributos adicionais podem ser mostrados usando tamanho, forma e cor dos marcadores que representam os objetos.

É útil ter arranjos de gráficos de dispersão para sumarizar de maneira compacta os relacionamentos de vários pares de atributos.

Gráficos de Contorno, Útil quando um atributo contínuo é medido em uma grade espacial.

Particionam o plano em regiões de valores similares.

Linhas de contorno que formam os limites destas regiões conectam pontos com valores iguais.

O exemplo mais comum são os mapas de contorno de elevação.

Também pode indicar temperatura, precipitação, pressão do ar, etc.

Gráficos Matriciais, Podem plotar a matriz de dados.

Pode ser útil quando os objetos são ordenados de acordo com a classe.

Normalmente os atributos são normalizados para evitar que um atributo domine o gráfico.

Gráficos de similaridade ou matrizes de distância também podem ser úteis para visualizar os relacionamentos entre objetos.

Coordenadas Paralelas, Usadas para plotar os valores dos atributos de dados de alta dimensionalidade.

Em lugar de eixos perpendiculares, usa-se um conjunto de eixos paralelos.

Valores dos atributos de cada objeto são plotados como um ponto em cada um dos eixos coordenados correspondentes e os pontos são ligados por linhas.

Então, cada objeto é representado como uma linha.

Freqüentemente linhas representam uma classe distinta de objetos agrupados, ao menos para alguns atributos.

Ordenar atributos é importante para ver os grupos.

Gráficos Estrela, Abordagem similar a coordenadas paralelas, mas eixos irradiam a partir de um ponto central.

A linha conectando os valores de um objeto é um polígono.

Faces de Chernoff, Esta abordagem associa cada atributo com a característica de uma face.

Valores de cada atributo determinam a aparência da característica facial correspondente.

Cada objeto torna-se uma face separada.

Baseia-se na habilidade humana de distinguir faces.

Bases de Dados Relacionais colocam os dados em tabelas, enquanto OLAP usa uma representação de arranjos multidimensional.

Tais representações de dados existiam previamente em estatística e outras áreas.

Várias operações de análises de dados e de exploração de dados são mais fáceis com tal representação de dados.

Dois passos básicos na conversão de dados tabulares em um arranjo multidimensional.

Identificar quais atributos serão as dimensões e qual será o objetivo cujos valores aparecem como entradas no arranjo multidimensional.

Atributos usados como dimensões devem ter valores discretos.

O valor alvo geralmente é contável ou contínuo, o custo de um item.

Pode não ter variável alvo exceto a contagem de objetos que tem o mesmo conjunto de valores atributo.

Encontrar o valor de cada entrada no arranjo multidimensional somando os valores (do atributo alvo) ou contando todos os objetos que tem os valores dos atributos correspondendo àquela entrada.

Mostra-se como os atributos petal length, petal width e specie type podem ser convertidos para um arranjo multidimensional, Primeiro, discretizam-se petal width e length para os valores categóricos, low, medium, e high.

Obtém-se a seguinte tabela notar o atributo contagem.

Cada t-upla única de petal width, petal length e specie type identifica um elemento do arranjo.

A este elemento é assinalada o correspondente valor da contagem.

A figura ilustra o resultado.

Todas as t-uplas nãoespecificadas são 0.

Fatias dos arranjos multidimensionais são mostradas pelas seguintes tabulações cruzadas.

A operação-chave de uma OLAP é a formação de um cubo de dados ("data cube").

Um cubo de dados é uma representação multidimensional dos dados, junto com todos os possíveis agregados.

Significa os agregados que resultam pela seleção de um subconjunto próprio das dimensões, e soma sobre todas as dimensões remanescentes.

Por exemplo, escolhendo a dimensão tipo de espécie da IRIS e somando sobre todas as outras dimensões, o resultado será uma entrada de uma dimensão com três entradas, cada uma dando o número de flores de cada tipo.

Considere um conjunto de dados que registra as vendas de produtos dos depósitos de uma companhia em várias datas.

Estes dados podem ser representados com um arranjo de dimensões.

Ha agregados de dimensões (escolhem), agregados de 1 dimensão, e 1 agregado com 0 dimensões (total geral).

Slicing é selecionar um grupo de células do arranjo multidimensional inteiro especificando um valor específico para uma ou mais dimensões.

Dicing envolve selecionar um subconjunto de células especificando a faixa de valores do atributo.

Isto é equivalente a definir um sub-arranjo do arranjo completo.

Na prática, ambas operações podem ser acompanhadas de agregação sobre algumas dimensões.

Valores de atributos em geral tem uma estrutura hierárquica.

Cada data é associada com um ano, mês e semana.

Um local é associado com um continente, país, estado província, et e cidade.

Produtos podem ser divididos em várias categorias, tais como vestuário, eletrônica e mobília.

Note que estas categorias freqüentemente se aninham e formam uma árvore ou um reticulado, Um ano contém meses que contém dias.

Um país contém um estado que contém uma cidade.

Esta estrutura hierárquica dá origem às operações roll-up e drill-down.

Para dados de venda pode-se agregar (roll up) as vendas ao longo de datas em um mês.

Inversamente, dada uma visão de um dado onde a dimensão tempo é quebrada em meses, pode-se dividir as vendas mensais totais (drill down) em vendas diárias totais.

Da mesma foram pode-se fazer o drill down ou roll up sobre os atributos de localização ou de ID.

Dada uma coleção de registros (conjunto de treinamento) Cada registro contém um conjunto de atributos e um dos atributos é a classe.

Encontrar um modelo para o atributo classe como uma função dos valores dos outros atributos.

Objetivo, aos registros previamente desconhecidos deve ser assinalada uma classe tão precisamente quanto possível.

Um conjunto de teste é usado para determinar a precisão do modelo.

Geralmente o conjunto de dados fornecido é dividido em conjuntos de treinamento e testes, com o conjunto de treinamento sendo usado para construir o modelo e o conjunto de testes sendo usado para validá-lo.

Prever se tumor em células é benigno ou maligno.

Classificar transações de cartão de crédito como legítimas ou fraude.

Classificar estruturas secundárias de proteínas como alpha-helix, beta-sheet, ou random coil.

Categorizar textos novos como finanças, tempo, lazer, esportes, etc.

Seja Dt o conjunto de registros de treinamento que chegam ao nó t.

Procedimento Geral, Se Dt contém registros que pertencem à mesma classe yt, então t é um nó folha com nome yt.

Se Dt é um conjunto vazio, então t é um nó folha com o nome da classe padrão yd Se Dt contém registros que pertencem a mais de uma classe, usar um teste em atributo para dividir os dados em sub-conjuntos menores.

Aplicar recursivamente o procedimento para cada subconjunto.

Estratégia gulosa.

Dividir os registros baseado sobre o teste do atributo que otimiza determinado critério.

Pontos a considerar, Determinar como dividir os registros.

Como especificar a condição de teste do atributo.

Como determinar a melhor divisão.

Determinar quando parar de dividir.

Depende dos tipos de atributos, Nominal.

Ordinal.

Contínuo.

Depende do número de formas de dividir, way split.

Multi-way split.

Multi-way split, Usar tantas partições quantos forem os valores distintos.

Binary split, Divide os valores em dois subconjuntos.

Necessita de encontrar o particionamento ótimo.

Diferentes formas de manipular, Discretização para formar um atributo categórico ordinal.

Estático discretizar uma vez no início.

Dinâmico pode-se encontrar faixas que tem largura igual, freqüência igual (percentis), ou agrupamento.

Decisão Binária, considerar todas as possíveis divisões e encontrar o melhor corte pode ser mais intensivo computacionalmente.

Índice Gini para um dado nó t.
Máximo quando registros são igualmente distribuídos entre todas classes, implicando informação menos interessante.

Mínimo quando todos os registros pertencem a uma classe, implicando informação mais interessante.

Usar Decisões Binárias baseadas sobre um valor.

Várias escolhas para o valor divisório, número de possíveis valores divisórios = Número de valores distintos.
Cada valor divisório tem uma matriz de contagem associada a si.
Classe conta em cada uma das partições, A < v e A v Método simples para escolher melhor v.Para cada v, percorrer base para obter matriz de contagem e calcular seu índice Gini.

Computacionalmente ineficiente.

Repetição do trabalho.

Para cálculo eficiente, para cada atributo, Ordenar os atributos baseado nos valores.

Linearmente pesquisar estes valores, cada vez atualizando a matriz de contagem e calculando o índice gini.

Usar a posição de corte que tem o menor índice gini.

Entropia em um dado nó t, Mede a homogeneidade de um nó.

Máximo quando registros são igualmente distribuídos entre todas classes => informação menor.

Mínimo quando todos registros pertencem à mesma classe, implicando informação maior.

Cálculo baseados em entropia são similares ao cálculo do índice GINI.

Ganho de Informação, Mede redução na Entropia obtida pela divisão.

Escolhe divisão de máxima redução (maximiza GAIN) Desvantagem, Tendência para divisões que resultam em um número maior de partições, cada uma sendo pequena porém pura.

Razão de Ganho, Ajusta Ganho de Informação pela entropia do particionamento (SplitINFO).

Maior entropia de particionamento (grande número de pequenas partições) é penalizado!.

Criado para superar a desvantagem do Ganho de Informação.

Parar de expandir um nó quando todos os registros pertencem à mesma classe.

Parar de expandir um nó quando todos os registros tem valores de atributo similar.

Vantagens, Sem dificuldades de construção.

Extremamente rápida para classificar registros desconhecidos.

Fácil de interpretar para árvores pequenas.

Precisão comparável a outras técnicas de classificação para muitos conjuntos com dados simples.

Sobre-ajuste resulta em árvores de decisão que são mais complexas que o necessário.

Erro de treinamento não é mais uma boa estimativa de quão bem a árvore desempenhará em registros previamente desconhecidos.

Necessita de novas formas para estimar erros.

Erro de Re-substituição, erro sobre treinamento (S e(t)).

Erro de Generalização, erro sobre teste (S e'(t)).

Métodos para estimar erro de generalização, Abordagem Otimista, e'(t) = e(t).

Abordagem Pessimista, Para cada nó folha, e'(t) = (e(t)+0,5).

Erro total, e'(T) = e(T) + N × 0,5 N, número de nós folh.

Para uma árvore com 30 nós folha e 10 erros no treinamento (de um total de 1000 instâncias), Erro de treinamento = 10/1000 = 1%.

Erro de Generalização = (10 + 30×0,5)/1000 = 2,5%.

Poda do Erro Reduzido (Reduced error pruning REP), usa dados de conjunto de validação para estimar erro de generalização.

Dados dois modelos com erro de generalização similares, deve-se preferir o modelo mais simples em relação ao modelo mais complexo.

Para modelos complexos, há uma maior chance que tenha sido ajustado acidentalmente pelos erros nos dados.

Portanto, deve-se incluir a complexidade do modelo durante a avaliação do modelo.

Custo(Modelo,Dados) = Custo(Dados|Modelo) + Custo(Modelo), Custo é o número de bits para codificar.

Busca o modelo de menor custo.

Custo(Dados | Modelo) codifica o erro de classificação.

Custo(Modelo) usa codificação do nó (número de filhos) mais codificação da condição de divisão.

Pré-Poda (Early Stopping Rule), Pára algoritmo antes de ter árvore completa.

Condições típicas de parada para um nó, Pare se todas instâncias pertencem à mesma classe.

Pare se todos os valores de atributo são iguais.

Condições mais restritivas, Pare se o número de instâncias é menor que algum limiar especificado pelo usuário.

Para se a distribuição de classes das instâncias é independente das características disponíveis usando teste c.

Pare se a expansão do nó atual não melhora a medida de impureza (Gini ou ganho de informação).

Pós-poda, Cria a árvore de decisão completa.

Ajusta os nós da árvore de decisão de maneira bottom-up.

Se erro de generalização melhora após ajuste, substituir sub-árvore pelo nó folha.

Rótulo da classe do nó folha é determinado pela classe da maioria das instâncias na subárvore.

Pode usar MDL para pós-poda.

Valores faltantes afetam a construção de árvores de decisão de três maneiras diferentes, Afetam como medidas de impureza são calculadas.

Afetam como distribuir instâncias com valores faltantes para os nós filhos.

Afetam como uma instância de teste com valor faltante é classificada.

Número de instâncias torna-se menor à medida que se caminha na árvore.

Número de instâncias nos nós folha pode ser muito pequeno para tomar qualquer decisão estatisticamente significante.

Encontrar uma árvore de decisão ótima é NP hard, O algoritmo apresentado até aqui usa uma estratégia de particionamento recursiva, gulosa, top-down para induzir uma solução razoável.

Outras estratégias.

Bottom-up.

Bi-direcional.

Árvore de decisão fornece representação expressiva para aprendizagem de função com valores discretos.

Apesar disto, não generalizam bem para certos tipos de funções Booleanas.

Exemplo, função paridade, Classe = 1 se há um número par de atributos Booleanos com valor verdade = True.

Classe = 0 se há um número ímpar de atributos Booleanos com valor verdade = True.

Para modelagem precisa deve ter uma árvore completa Não é suficientemente expressiva para modelar variáveis contínuas.

Particularmente quando a condição de teste envolve somente um único atributo por vez.

Foco na capacidade preditiva de um modelo Em lugar de quão rápido ele classifica ou constrói modelos, escalabilidade, etc.

Considere um problema com classes, Número de exemplos da Classe 0 = 9990.

Número de exemplos da Classe 1 = 10.

Se o modelo prevê tudo como sendo classe 0, precisão é 9990/10000 = 99,9 %.

Precisão pode enganar porque o modelo não detecta qualquer exemplo da classe 1.

Como obter uma estimativa confiável de desempenho.

Desempenho de um modelo pode depender de outros fatores além do algoritmo de aprendizagem, Distribuição de classes.

Custo de classificação incorreta.

Tamanho dos conjuntos de treinamento e teste.

Curva de Aprendizagem mostra como precisão muda com a variação do tamanho da amostra.

Requer um escalonamento de amostragem para criar a curva de aprendizagem, Amostragem aritmética.

Amostragem Geométrica.

Efeitos de amostra de pequeno tamanho, Bias na estimativa.

Variância da estimativa.

Holdout, Reserva 2/para treinamento e 1/para teste.

Sub-amostragem aleatória, Holdout repetido.

Validação Cruzada, Particionar dados em k subconjuntos disjuntos.

K-fold, treinar em k-1 partições, testar no restante.

Leave-one-out, k = n.

Amostragem Estratificada, Sobre-amostragem versus sub-amostragem.

Bootstrap, Amostragem com reposição.

Desenvolvido nos anos 1950 para teoria de detecção de sinais para analisar sinais ruidosos, Caracteriza o compromisso entre acertos positivos e falsos alarmes.

Curva ROC plota TP (no eixo y) contra FP (no eixo x).

Desempenho de cada classificador representado como um ponto na curva ROC, Alterando o limiar do algoritmo, a distribuição a distribuição da amostra ou a matriz de custo altera a posição do ponto.

Classificar registros usando uma coleção de regras "if then ".

Regra, (Condição) em que, Condição é uma conjunção de atributos.

Y é o rótulo da classe.

LHS, antecedente da regra ou condição.

RHS, conseqüente da regra.

Cobertura de uma regra, Fração de registros que satisfazem o antecedente da regra.

Precisão de uma regra, Fração dos registros que satisfazem tanto o antecedente quanto o conseqüente da regra.

Regras mutuamente exclusivas, Classificador contém regras mutuamente exclusivas se as regras são independentes entre si.

Cada registro é coberto por no máximo uma regra Regras exaustivas.

Classificador tem cobertura exaustiva se ele leva em conta toda possível combinação dos valores dos atributos.

Cada registro é coberto por pelo menos uma regra.

Regras deixam de ser mutuamente exclusivas, Um registro pode disparar mais de uma regra.

Solução.

Conjunto de regras ordenadas.

Conjunto não ordenado de regras usar esquema de votação.

Regras deixam de ser exaustivas, Um registro pode não disparar nenhuma regra.

Solução.

Usar uma classe default.

Regras são ordenadas de acordo com sua prioridade, chamada lista de decisão, Quando um registro de teste é apresentado, Ele é assinalado ao rótulo de classe da regra de mais alta ordem que tenha sido disparada.

Se nenhuma das regras dispara, é assinalado à classe default.

Ordenação baseada em Regras, Regras individuais são ordenadas baseado em sua qualidade.

Ordenação baseada na Classe, Regras que pertencem à mesma classe aparecem juntas.

Método Direto, Extrair regras diretamente dos dados.

Critério de Parada, Cálculo do ganho.

Se o ganho não é significante, descartar a nova regra.

Poda de Regras, Similar à pós-poda de árvores de decisão.

Reduced Error Pruning, Remover uma das conjunções da regra.

Comparar a taxa de erro no conjunto de validação antes e após a poda.

Se o erro aumenta, podar o conjunto.

Para um problema de classes, escolher uma delas como positiva e a outra como classe negativa, Aprender regras para a classe positiva.

Classe negativa será a classe default.

Para problema de várias classes, Ordenar as classes de acordo com a prevalência crescente da classe (fração de instâncias que pertencem a uma classe particular).

Aprender o conjunto de regras para a menor classe primeiro, tratar o restante como classe negativa.

Repetir com a menor classe seguinte como classe positiva.

Crescendo uma regra, Iniciar com a regra vazia.

Adicionar conjunções enquanto elas melhorarem o ganho de informação FOIL.

Parar quando regra não cobrir mais exemplos negativos.

Podar a regra imediatamente usando reduced error pruning incremental.

Medida para poda, v = (p-n)/(p+n) p, número de exemplos positivos cobertos pela regra no conjunto de validação.

Número de exemplos negativos cobertos pela regra no conjunto de validação.

Método de Poda, retirar qualquer seqüência final de condições que maximiza v.

Construindo um conjunto de regras, Usar o algoritmo de cobertura seqüencial.

Encontrar a melhor regra que cobre o conjunto atual de exemplos positivos.

Eliminar tanto os exemplos positivos quanto negativos cobertos pela regra.

Cada vez que uma regra é colocada no conjunto de regras, calcular o novo comprimento da descrição Parar de adicionar novas regras quando o novo comprimento da descrição for d bits maior que o menor comprimento de descrição encontrado até então.

Otimizar o conjunto de regras, Para cada regra r no conjunto de regras R.

Considerar regras alternativas, Regra de substituição (r*), crescer nova regra a partir do zero.

Regra de revisão (r'), adicionar conjunções para estender r.

Comparar o conjunto de regras para r com o conjunto de regras para r* e r'.

Escolher conjunto de regras que minimiza o princípio MDL.

Repetir geração e otimização de regras para o restante dos exemplos positivos.

Extrair regras de uma árvore de decisão nãopodada.

Para cada regra, r, A y, Considerar uma regra alternativa r', A' y em que A' é obtida removendo uma das conjunção em A.

Comparar a taxa de erro pessimista para r com todos os r's.

Podar se um dos r's tem taxa de erro pessimista inferior.

Repetir até que não se possa melhorar o erro de generalização.

Em lugar de ordenar as regras, ordenar subconjuntos de regras (ordenação de classes).

Cada subconjunto é uma coleção de regras com o mesmo conseqüente (classe).

Calcular comprimento da descrição de cada subconjunto.

Comprimento da descrição = L(erro) + g L(modelo).

Onde g é um parâmetro que leva em conta a presença de atributos redundantes em um conjunto de regras (valor default = 0,5).

Exemplos, Rote-learner, Memoriza todos os dados de treinamento e realiza classificação somente se atributos do registro correspondem exatamente a um dos exemplos de treinamento.

Vizinho mais próximo, Usa k pontos "mais próximos" (vizinhos mais próximos) para realizar classificação.

Requer três coisas Conjunto de registros armazenados Métrica de distância para calcular distância entre registros Valor de k, o número de vizinhos mais próximos a recuperar Para classificar registro desconhecido, Calcular distância para outros registros de treinamento Identificar k vizinhos mais próximos.

Usar rótulos da classe dos vizinhos mais próximos para determinar o rótulo da classe do registro desconhecido pelo voto da maiori.

Calcular distância entre dois pontos, Distância Euclidiana, Determinar a classe a partir da lista dos vizinhos mais próximos, Usar voto da maioria dos rótulos de classe entre os k-vizinhos mais próximos.

Ponderar o voto de acordo com a distância.

Escolhendo o valor de k, Se k é muito pequeno, há sensibilidade ao ruído.

Se k é muito grande, a vizinhança pode incluir pontos de outras classes.

Questões de escalamento, Atributos podem ter de ser escalados para evitar medidas de distâncias serem dominadas por um dos atributos.

Exemplo, altura de uma pessoa pode variar de 1,5m a 1,8m.

O peso de uma pessoa pode variar de 45kg a 150kg.

Classificador k-NN é lazy learner ("aprendizagem preguiçosa"), Não constrói explicitamente modelos.

Diferente da "aprendizagem ávida" ("eager learner") como árvore de decisão e sistemas baseados em regras.

Classificação de registros desconhecidos é relativamente cara computacionalmente.

PEBLS, Parallel Exemplar-Based Learning System (Cost & Salzberg), Funciona tanto com atributos contínuos quanto nominais.

Para características nominais, distância entre dois valores é calculada usando modified value difference metric (MVDM).

A cada registro é assinalado um fator de ponderação.

Número de vizinhos mais próximos, k = 1.

Para atributos contínuos.
Discretizar a faixa em regiões, um atributo ordinal por região viola o princípio da independência.

Two-way split, (A < v) ou (A > v), escolher só uma das divisões como novo atributo.

Estimação da densidade de probabilidade, Assume que atributo segue distribuição normal.

Usa dados para estimar parâmetros da distribuição (média e desvio padrão).

Sendo conhecida distribuição de probabilidades, pode-se usá-la para estimar a probabilidade condicional PAi|.

Robusto a pontos isolados de ruído, Suporta valores faltantes ignorando a instância durante cálculos da estimativa de probabilidade.

Robusto a atributos irrelevantes.

Pressuposição de Independência pode não se sustentar para alguns atributos.

Usar outros técnicas tais como Bayesian Belief Networks (BBN).

Modelo é um conjunto de nós interconectados e ligações com pesos.

Nó de saída soma cada valor de entrada de acordo com o peso de sua ligação.

Compara nó de saída com algum limiar t.

Inicializar os pesos.

Ajustar os pesos de tal forma que a saída da RNA seja consistente com os rótulos das classes dos exemplos de treinamento.

Função Objetivo, encontrar os pesos wi que minimizam a função objetivo anterior.

Algoritmo backpropagation.

Encontrar um hiperplano linear (superfície de decisão) que irá separar os dados.

Achar hiperplano que maximiza a margem => B1 é melhor que B2.

Construir um conjunto de classificadores a partir dos dados de treinamento.

Prever o rótulo da classe de registros previamente desconhecidos através da agregação de previsões feitas por múltiplos classificadores.

Procedimento iterativo para alterar de maneira adaptativa a distribuição dos dados de treinamento centrando mais em registros previamente classificados incorretamente, Inicialmente, a todos os N registros assinalam-se pesos iguais.

Ao contrário de bagging, pesos podem mudar no final da rodada de boosting.

Registros que são classificados incorretamente terão seus pesos aumentados.

Registros que são classificados corretamente terão seus pesos diminuídos.

Se qualquer rodada intermediária produz taxa de erro maior que 50%, os pesos retornam para 1/n e o procedimento de re-amostragem é repetido.

Dado um conjunto de transações T, o objetivo da mineração de regras de associação é encontrar todas as regras que tenham, suporte maior igual à liminar minsup.

 confiança maior igual à limiar minconf.

Abordagem de força bruta, Listar todas as possíveis regras de associação.

Calcular o suporte e confiança para cada regra.

Podar regras que não atendem aos limiares de minsup e minconf.

Computacionalmente proibitivo!.

Abordagem em dois passos, 1 Geração de Conjuntos de Itens Freqüentes, Gerar todos os conjuntos de itens com suporte maior igual à minsup.

Geração de Regras, Gerar regras com alta confiança a partir de cada conjunto de itens freqüente, em que cada regra é uma partição binária de um conjunto de itens freqüente.

Geração de conjuntos de itens freqüentes ainda é computacionalmente cara.

Abordagem por força bruta, Cada conjunto de itens no reticulado é um candidato a conjunto de itens freqüentes.

Contar o suporte de cada candidato baseado nos dados.

Avaliar cada transição em relação a cada candidato Complexidade ~ O(NMw) => Caro pois M = 2d!.

Dados d itens únicos, Número total conjuntos de itens = 2d.

Número total de possíveis regras de associação.

Reduzir o número de candidatos (M), Busca Completa, M = 2d.

Usar técnicas de poda para reduzir M.

Reduzir o número de transações (N), Reduzir tamanho de N conforme crescem o conjuntos de itens.

Usado por DHP e algoritmos de mineração vertical-based.

Reduzir o número de comparações (NM), Usar estruturas de dados eficientes para armazenar os candidatos ou transações.

Sem necessidade de comparar todos os candidatos com todas as transações.

Princípio Apriori, Se um conjunto de itens é freqüente, então todos os seus subconjuntos também devem ser freqüentes.

Princípio Apriori se mantém devido à seguinte propriedade das medidas de suporte, Suporte de um conjunto de itens nunca excede o suporte de seus subconjuntos.

Conhecido como propriedade anti-monótona de suporte.

Método, Faça k = 1.

Gerar conjuntos de itens freqüentes de tamanho 1.

Repetir até não haver novos conjuntos de itens freqüentes.

Gerar conjuntos de itens candidatos de tamanho (k +1) a partir de conjuntos de itens freqüentes de tamanho k.

Podar conjuntos de itens candidatos contendo subconjuntos de tamanho k que não são freqüentes.

Contar o suporte de cada candidato na base de dados Eliminar candidatos que não sejam freqüentes, deixando somente aqueles que são freqüentes.

Contagem de candidatos, Verificar a base de transações para determinar o suporte de cada conjunto de itens candidato.

Para reduzir o número de comparações, armazenar os candidatos em uma estrutura hash.

Em vez de testar cada transação com cada candidato, testá-lo com candidatos contido nos hashed buckets.

Escolha do limiar do suporte mínimo, Baixando limiar do suporte resulta em mais conjuntos de itens freqüentes.

Isto pode aumentar o número de candidatos e o comprimento máximo dos conjuntos de itens freqüentes.

Dimensionalidade (número de itens) do conjunto de dados, É preciso mais espaço para guardar contagem de suporte de cada item.

Se número de itens freqüentes também aumenta, tanto os custos de cálculo quanto de I/O também podem aumentar.

Alguns conjuntos de itens são redundantes pois tem suporte idêntico ao de seus super-conjuntos.

Número de conjuntos de itens freqüentes.

É preciso uma representação compacta.

Um conjunto de itens é freqüente máximo se nenhum seu superconjunto imediato é freqüente.

Um conjunto de itens é fechado se nenhum de seus super-conjuntos imediatos tem o mesmo suporte que o conjunto de itens.

Cruzamento do Reticulado do Conjunto de Itens, Geral para específico versus Específico para geral.

Classes Equivalentes.

Largura Primeiro versus Profundidade Primeiro.

Representação da Base de Dados, Leiaute horizontal versus vertical dos dados.

Algoritmos de regras de associação tendem a produzir regras em demasia, muitas delas são redundantes ou não interessantes.

Medidas de "interessabilidade" podem ser usadas para podar/ordenar os padrões derivados.

Na formulação original de regras de associação, suporte e confiança são únicas medidas usadas.

Dada uma regra, informação necessária para calcular medida de interessabilidade pode ser obtida de tabela de contingência.

Piatetsky-Shapiro, propriedades que uma boa medida M deve satisfazer, MA, = 0 se A e B são estatisticamente independentes.

MA, aumenta monotonicamente com PA, quando Pe Ppermanecem inalterados.

MA, diminui monotonicamente com P[ou P] quando PA, e P[ou P] permanecem inalterados.

Maioria dos algoritmos de mineração de regras de associação usa medida de suporte para podar regras e conjuntos de itens.

Estudar efeito de poda com suporte sobre a correlação de conjuntos de itens, Gerar 10000 tabelas de contingência aleatórias.

Calcular suporte e correlação aos pares para cada tabela.

Aplicar poda baseada em suporte e examinar as tabelas que são removidas.

Investigar como poda baseada em suporte afeta outras medidas.

Passos, Gerar 10000 tabelas de contingência.

Ordenar cada tabela de acordo com as diferentes medidas.

Calcular a correlação par-a-par entre as medidas.

Medida Objetiva, Ordenar padrões baseado em estatísticas calculadas a partir dos dados medidas de associação suporte, confiança, Laplace, Gini, informação mútua, Jaccard.

Medida Subjetiva, Ordenar padrões de acordo com interpretação do usuário.

Um padrão é subjetivamente interessante se ele contradiz a expectativa de um usuário.

Um padrão é subjetivamente interessante se ela é utilizável.

Como aplicar a formulação de análise de associação a variáveis binárias não-assimétricas.

Transformar atributos categóricos em variáveis binárias assimétricas.

Introduzir um novo "item" para cada par atributo valor distinto.

Exemplo, substituir o atributo Browser Type por, Browser Type = Internet Explorer.

Browser Type = Mozilla.

Browser Type = Mozilla.

Questões potenciais, Se o atributo tem muitos valores possíveis.

Exemplo, atributo país tem mais de 200 valores possíveis.

Muitos dos valores do atributo pode ter suporte muito baixo, Solução potencial, Agregar valores de atributo de baixo suporte.

Se a distribuição dos valores do atributo é muito irregular.

Exemplo, 95% dos visitantes tem Buy = No.

Maioria dos itens será associada com o item (Buy=No), Solução potencial, abandonar os itens altamente freqüentes.

Diferentes tipos de regras, Age.

Salary.

Diferentes métodos, Baseados em discretização.

Baseados em estatística.

Baseados em não-discretização, minApriori.

Usar discretização.

Não supervisionada, Faixa de igual largura.

Faixa de igual profundidade.

Agrupamento.

Tamanho dos intervalos discretizados afeta suporte e confiança, Intervalos muito pequenos, pode não ter suporte suficiente.

Intervalos muito grandes, pode não ter confiança suficiente.

Solução potencial, usar todos intervalos possíveis.

Tempo de execução, Se intervalo contém n valores, há em média O(n2) faixas possíveis.

Conseqüente da regra consiste de uma variável contínua, caracterizada pelas suas estatísticas, média, mediana, desvio padrão, etc.

Abordagem, Separa a variável meta do resto dos dados.

Aplicar geração de conjuntos de itens freqüentes sobre o resto dos dados.

Para cada conjunto de itens freqüentes, calcular a estatística descritiva para a correspondente variável meta.

Conjunto de itens freqüentes torna-se uma regra pela introdução da variável meta como conseqüente da regra, Aplicar testes estatísticos para determinar a interessabilidade da regra.

Min-Apriori, Dados contém somente atributos contínuos do mesmo "tipo", freqüência de palavras em um documento Solução potencial, Converter em matriz 0/1 e aplicar então algoritmos existentes, perde informação de freqüência de palavras.

Discretização não se aplica, pois usuários querem associações entre palavras não faixas de palavras.

Como determinar o suporte de uma palavra.

Se usa-se simplesmente sua freqüência, contagem do suporte será maior que o número total de documentos!.

Normalizar os vetores das palavras usando norma L1.

Cada palavra tem um suporte igual a 1.

Regras de Associação Multi-nível, Por quê incorporar o conceito de hierarquia.

Regras de níveis inferiores podem não ter suporte suficiente para aparecer em conjuntos de itens freqüentes.

Regras em níveis inferiores da hierarquia são específicas em demasia.

Abordagem 1, Estender a formulação de regras de associação atuais pelo aumento de cada transição com itens de nível superior.

Transação Original, {skim milk, wheat bread}.

Transação Aumentada, {skim milk, wheat bread, milk, bread, food}.

Questões, Itens que residem em níveis superiores tem contagem de suporte muito mais elevados, se o limiar do suporte é alto, padrões freqüentes somente envolvem itens de níveis superiores.

Dimensionalidade dos dados aumentada.

Abordagem 2, Gerar padrões freqüentes no nível mais superior primeiramente.

Então, gerar padrões freqüentes no próximo nível mais superior e assim por diante.

Questões, Requisitos de I/O aumentam dramaticamente pois é preciso realizar mais passagens sobre os dados.

Pode perder alguns padrões de associação entre níveis potencialmente interessantes.

Definição Formal de uma Seqüência, Uma seqüência é uma lista ordenada de elementos (transações), Cada elemento contém uma coleção de eventos (itens).

Cada elemento é atribuído a um tempo ou a um local específico.

Comprimento de uma seqüência, |s|, é dado pelo número de elementos da seqüência.

Uma k-seqüência é uma seqüência que contém k eventos (itens).

Mineração de Padrões Seqüenciais, Definição.

Dados, Uma base de dados de seqüências.

Um limiar de suporte mínimo especificado pelo usuário, minsup.

Tarefa, Encontrar todas as subseqüências com suporte maior igual à minsup.

Generalized Sequential Pattern (GSP), Passo 1, Realizar primeira passagem sobre a base de dados de seqüências D para obter todas as seqüências freqüentes de elemento.

Passo 2, Repetir até não encontrar novas seqüências freqüentes.

Geração de Candidatos, Unir pares de subseqüências freqüentes encontradas no (k-1)-ésimo passo para gerar seqüências candidatas que contém k itens.

Poda de Candidatos, Podar k-seqüências de candidatos que contém (k-1)-subseqüências não freqüentes.

Contagem de Suporte, Fazer nova passagem sobre a base de dados de seqüência D para encontrar o suporte para estas seqüências candidatas.

Eliminação de Candidatos, Eliminar k-seqüências candidatas cujo suporte atual é menor que minsup.

Geração de candidatos, Caso base (k=2), Unir duas seqüência freqüentes <{i1}> e <{i2}> produzirá duas seqüências candidatas, <{i1} {i2}> e <{i1 i2}>.

Caso geral (k>2), Uma (k-1)-seqüência freqüente w1 é unida com outra (k-1)-seqüência freqüente wpara produzir uma k-seqüência candidata se a subseqüência obtida pela remoção do primeiro evento em w1 é a mesma que a subseqüência obtida pela remoção do último evento em w2.

O candidato resultante após união é dado pela seqüência w1 estendida com o último evento de w2, Se os dois últimos eventos em wpertencem ao mesmo elemento, então o último evento em wtorna-se parte do último elemento em w1.

Caso contrário, o último evento em wtorna-se um elemento separado adicionado ao final de w1.

Unindo as seqüências w1=<{1} {3} {4}> e w=<{3} {5}> produzirá a seqüência candidata < {1} {3} {5}> pois os dois últimos eventos em we pertencem ao mesmo elemento.

Unindo as seqüências, w1=<{1} {3} {4}> e w=<{3} {4} {5}> produzirá a seqüência candidata < {1} {3} {4} {5}> pois os dois últimos eventos em we não pertencem ao mesmo elemento.

Não é necessário unir as seqüências, w1 =<{1} {6} {4}> e w=<{1} {2} {5}> para produzir o candidato < {1} {6} {5}> pois se este último é um candidato viável, então ele pode ser obtido unindo w1 com < {1} {6} {5}>.

Minerando Padrões Seqüenciais com Restrições Temporais, Abordagem 1, Minerar padrões seqüenciais sem restrições temporais.

Pós-processar os padrões descobertos.

Abordagem 2, Modificar GSP para podar diretamente candidatos que violam restrições temporais.

Sem restrição maxgap, Uma k-seqüência candidata é podada se pelo menos uma de suas (k-1)-subseqüências não é freqüente.

Com restrição maxgap, Uma k-seqüencia candidata é podada se pelo menos uma de suas (k-1)-subseqüências contíguas não é freqüente.

