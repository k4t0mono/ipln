A demanda por métodos de análise e descoberta de conhecimento em grandes bases de dados tem fortalecido a pesquisa em Mineração de Dados.

Dentre as tarefas associadas a essa área, tem-se Regras de Associação.

Vários algoritmos foram propostos para tratamento de Regras de Associação, que geralmente têm como resultado um elevado número de regras, tornando o Pós-processamento do conhecimento uma etapa bastante complexa e desafiadora.

Existem medidas para auxiliar essa etapa de avaliação de regras, porém existem lacunas referentes a inexistência de um método intuitivo para priorizar e selecionar regras.

Além disso, não é possível encontrar metodologias específicas para seleção de regras considerando mais de uma medida simultaneamente.

Esta tese tem como objetivo a proposição, desenvolvimento e implementação de uma metodologia para o Pós-processamento de Regras de Associação.

Na metodologia proposta, pequenos grupos de regras identificados como potencialmente interessantes são apresentados ao usuário especialista para avaliação.

Para tanto, foram analisados métodos e técnicas utilizadas em Pós-processamento de conhecimento, medidas objetivas para avaliação de Regras de Associação e algoritmos que geram regras.

Dessa perspectiva foram realizados experimentos para identificar o potencial das medidas a serem empregadas como filtros de Regras de Associação.

Uma avaliação gráfica apoiou o estudo das medidas e a especificação da metodologia proposta.

Aspecto inovador da metodologia proposta é a utilização do método de Pareto e a combinação de medidas para selecionar as Regras de Associação.

Por fim foi implementado um ambiente para avaliação de Regras de Associação, denominado ARInE, viabilizando o uso da metodologia proposta.

A popularização da Internet, maior e principal rede de computadores do planeta e os avanços tecnológicos em Banco de Dados, sistemas de comunicação, mecanismos de coleta e armazenamento de dados, têm proporcionado às organizações a capacidade de armazenar informações detalhadas sobre cada transação efetuada, gerando grandes bases de dados que, em alguns casos, chegam a dezenas de gigabytes ou até dezenas de terabytes de informações coletadas por ano.

Paralelamente a esta evolução tecnológica, as organizações passaram a reconhecer o valor das informações e do conhecimento contidos em suas bases de dados.

A crescente demanda por informações precisas, detalhadas e atualizadas, necessárias ao gerenciamento das organizações, levou à busca por meios mais rápidos e eficientes de recuperação da informação e do conhecimento, incentivando a realização de pesquisas voltadas para o processo de transformação de dados em conhecimento.

Frente às demandas apresentadas, métodos predominantemente manuais utilizados para a transformação de dados e informações em conhecimento, tornam o processo de extração de conhecimento dispendioso, subjetivo e, geralmente, inviável quando aplicado a grandes bases de dados.

Nesse sentido, buscando automatizar o processo de análise, várias pesquisas têm sido desenvolvidas na área de O processo de extração de conhecimento de grandes bases de dados não pode ser entendido como um sistema de análise automática, mas sim, como um processo interativo e iterativo, centrado na interação entre engenheiros de conhecimento, especialistas do domínio e usuários.

Não se pode esperar a obtenção de conhecimento util simplesmente submetendo um conjunto de dados a uma "caixa preta".

Cabe destacar que sendo a Mineração de Dados um processo iterativo, suas etapas, Compreensão do Domínio, Pré-processamento de Dados, Extração de Padrões, Pós-processamento do Conhecimento e Utilização do Conhecimento, são interconectadas de tal modo que alterações, mesmo sutis, em uma determinada etapa podem afetar significativamente quaisquer das etapas seguintes ou ainda determinar o reinício de todo o processo.

As tarefas de Mineração de Dados utilizadas para identificação de padrões podem ser agrupadas nas categorias, preditivas e descritivas.

As tarefas preditivas constroem uma hipótese, a partir da generalização de exemplos com classes previamente definidas, na tentativa de prever um tipo de comportamento para novos casos (atributo met).

Os principais tipos de problemas de predição são a Classificação e a Regressão.

As tarefas descritivas consistem na identificação de comportamentos intrínsecos do conjunto de dados.

Das tarefas descritivas destacam-se Clustering, Regras de Associação e Sumarização.

Dentre as tarefas de Mineração de Dados, Regras de Associação é uma das técnicas que tem despertado grande interesse.

Na área acadêmica, pesquisas vêm sendo desenvolvidas e organizações têm utilizado seus resultados em aplicações relacionadas a comércio, contratos de seguro, seguridade social, geoprocessamento, serviços bancários e outras áreas.

Exemplo do emprego de Regras de Associação na área comercial pode ser observado em páginas na Internet, que apresentam sugestões do tipo " quem comprou o produto que você procura também comprou estes outros produtos ".

Em termos gerais, a tarefa Regras de Associação permite identificar padrões intratransações em uma base de dados.

Especificamente, uma Regra de Associação caracteriza o quanto a presença de um conjunto de atributos nos registros de uma base de dados implica na presença de algum outro conjunto distinto de atributos nos mesmos registros.

Para identificação de Regras de Associação diversos algoritmos foram propostos, dentre os quais pode-se citar, AIS, Apriori e AprioriTid, Opus, Closet e FP-Growth.

As regras geradas pela tarefa Regras de Associação não possuem como característica o atributo meta, podendo qualquer atributo estar presente na regra.

Outro aspecto a ser considerado é que as Regras de Associação são empregadas de maneira individual e não de maneira conjunta como no caso das Regras de Classificação cada regra expressa distintas regularidades existentes nos dados e, geralmente, retratam situações diferentes umas das outras.

Essa condição determina que a avaliação e interpretação do conhecimento extraído por meio do processo de Mineração de Dados seja tratada de modo diferenciado.

No cenário presente com tendências como marketing individualizado, gerenciamento de relações com clientes (CRM), conteúdo de Web personalizado, dentre outras, vem ocorrendo a disseminação, intensificação e popularização do uso de técnicas de Extração de Conhecimento na resolução de problemas concretos.

Essa demanda tem aumentado a exigência de confiabilidade no conhecimento extraído e reforça a necessidade de maior ênfase na análise e avaliação do conhecimento adquirido.

Considerando-se que os usuários de Sistemas Inteligentes normalmente têm conhecimento prévio sobre o domínio para o qual a aplicação está sendo desenvolvida, o Pós-processamento irá contribuir para ratificar/retificar o conhecimento existente.

A realização de pesquisas em Pós-processamento do conhecimento (principalmente em tópicos como avaliação da qualidade, grau de interesse, compreensibilidade e visualização do conhecimento extraído) tem obtido grande destaque e vem ao encontro das necessidades atuais à medida que busca desenvolver meios para identificar o conhecimento interessante e incorporar o conhecimento prévio do domínio.

Apesar das vantagens da utilização do processo de Mineração de Dados, sua aplicação pode levar, em alguns casos, à descoberta de conhecimento muito óbvio ou irrelevante que pouco acrescenta ao conhecimento já existente sobre o domínio.

No caso particular de Regras de Associação, a possibilidade de descoberta de todas as associações existentes nas transações de uma base de dados é o aspecto de maior relevância da tarefa.

Entretanto, esta característica determina a geração, usualmente, de um número surpreendente de regras, dificultando sobremaneira a interpretação do conjunto de regras pelo usuário.

Para superar esta problemática, técnicas de Pós-processamento do conhecimento devem ser aplicadas.

Em Regras de Associação, a maioria das abordagens para a avaliação do conhecimento extraído consiste na utilização de uma ou mais métricas para ordenar regras (ou realizar algum tipo de pod e, assim, apresentá-las para a avaliação do usuário final).

Se propõe uma metodologia para seleção de regras em duas etapas.

Na primeira etapa a medida qui-quadrado é utilizada para estabelecer o grau de significância para as regras, que são ordenadas por medidas objetivas na segunda etapa.

Se propõe um framework que combina técnicas estatísticas e gráficas para a poda de conjuntos de regras.

Se propõe uma metodologia e ferramenta para navegação e visualização de regras baseada em um conjunto de operadores que permitem focar características particulares de um conjunto de regras considerando uma variedade de aspectos.

Utilizou-se recursos de realidade virtual e medidas objetivas para Pós-processamento visual de Regras de Associação.

As regras são representadas visualmente como árvores com diferentes valores de tamanho, cor e distância do observador associados às medidas objetivas.

Lacunas podem ser identificadas nos trabalhos de Pós-processamento de Regras de Associação.

A primeira delas refere-se à inexistência de um método intuitivo para priorizar e selecionar regras estabelecendo intervalos de valores de medidas para sua análise.

Outra lacuna está na ausência de metodologias específicas para seleção de Regras de Associação considerando mais de uma medida simultaneamente.

Trabalhos vêm sendo desenvolvidos no Laboratório de Inteligência Computacional (LABI do Instituto de Ciências Matemáticas e de Computação (ICM da USP São Carlos, abordando temas relacionados ao processo de Mineração de Dados e Textos).

Está em desenvolvimento o ambiente computacional Discover para a extração de conhecimento e o ambiente RulEE para exploração de regras.

O objetivo no desenvolvimento do Ambiente Discover é disponibilizar, aos pesquisadores, um ambiente integrado de suporte às etapas do processo de aquisição de conhecimento com funcionalidades para o Aprendizado de Máquina e a Mineração de Dados e Textos.

A compatibilidade entre os diferentes módulos do ambiente é baseada em sintaxes padrões utilizadas para representar dados e conhecimento.

O Ambiente RulEE oferece mecanismos para exploração de regras de Classificação, Regressão e Associação, utilizando medidas objetivas e subjetivas.

Por tratar-se de ambiente genérico, não possui os mecanismos específicos e o desempenho necessário para auxiliar nos processos de filtragem e seleção de Regras de Associação.

Considerando que na área de Mineração de Dados grande parte das pesquisas realizadas focalizam as etapas de Pré-processamento e Extração de Padrões (especialmente à tarefa Classificação), é importante desenvolver trabalhos centrados na etapa de Pós-processamento de conhecimento.

No Ambiente Discover, a situação é semelhante.

Grande parte dos trabalhos desenvolvidos estão associados a estas etapas, existindo a necessidade de trabalhos que abordem outras questões relacionadas a estas áreas.

No contexto apresentado, este trabalho teve como objetivo investigar o processo de Mineração de Dados utilizando a tarefa Regras de Associação, com enfoque em aspectos relacionados ao Pós-processamento do conhecimento adquirido.

Dessa perspectiva, busca-se solucionar a problemática da análise do grande número de Regras de Associação geradas, uma vez que é contraproducente o fornecimento de grande quantidade de padrões ao usuário.

O objetivo principal desta tese é propor uma metodologia para o Pós-processamento de Regras de Associação.

Nessa metodologia, pequenos grupos de regras potencialmente interessantes são apresentados ao usuário para sua avaliação.

A definição destes grupos de regras deve ser baseada no uso de medidas objetivas.

A hipótese adotada neste trabalho é, em um conjunto de Regras de Associação existem poucas regras que apresentam valores elevados em diversas medidas objetivas, a maioria das regras apresenta valores elevados em apenas uma medida.

Essa condição permite obter um conjunto de regras reduzido para ser analisado por um especialista.

O problema foi abordado sistematicamente, de maneira iterativa, de acordo com o processo de Mineração de Dados, consistindo especialmente nas fases de Pré-processamento, Extração de Padrões e Pós-processamento.

Para tanto, foi necessário analisar métodos e técnicas utilizadas em Pós-processamento de conhecimento e propor uma metodologia para explorar Regras de Associação, por meio de medidas objetivas.

O ponto inicial da pesquisa consistiu no levantamento dos algoritmos que geram Regras de Associação.

A intenção foi analisar o conteúdo do arquivo de saída dos algoritmos para identificar informações fornecidas com as regras e a forma pela qual o algoritmo representa a regra, no sentido de definir um formato padronizado de representação.

Paralelamente, procedeu-se a identificação das medidas objetivas empregadas na avaliação de regras visando conhecer as características das medidas e definir como poderiam ser calculadas de maneira padronizada.

Todas as medidas consideradas podem ser determinadas a partir da tabela de contingência.

A medida que estes estudos foram concluídos, foi possível especificar uma sintaxe padrão para representar as Regras de Associação.

Nessa sintaxe, além dos elementos constituintes da regra, é representada a tabela de contingência associada à regra.

A intenção de estabelecer uma sintaxe padrão é facilitar a manipulação dos conjuntos de regras nas etapas posteriores de execução do projeto e evitar a necessidade de re-implementação de procedimentos de acesso aos conjuntos de regras e para cálculo das medidas identificadas.

Também foi necessário definir procedimento para calcular a tabela de contingência a partir dos resultados dos algoritmos que geram Regras de Associação.

Especificada a sintaxe padrão para Regras de Associação e os procedimentos para cálculo da tabela de contingência, foi implementada biblioteca em PERL para converter arquivos de regras para o formato especificado na sintaxe padrão.

Atualmente está implementada a conversão a partir do algoritmo Apriori (versão desenvolvida por Borgelt) e dos aplicativos MineSet TM, MagnumOpus e WEKA.

Vale ressaltar que a inclusão de novos algoritmos e aplicativos na biblioteca de conversão é tarefa de fácil realização.

A medida que se sucederam as etapas do trabalho e considerando os objetivos estabelecidos, foi verificada a necessidade de analisar o potencial das medidas identificadas inicialmente, para emprego como filtros para regras.

Assim, foi realizado conjunto de experimentos com esse objetivo.

Nesses experimentos foi avaliado o potencial de utilização de cada medida utilizando-se como referência o princípio de Pareto.

Esse princípio estabelece, em linhas gerais, que grande parte das consequências de um fenômeno provêm de um pequeno número de causas.

No presente contexto, a medida é o fenômeno, os valores da medida são as consequências e as regras são as causas.

Desse modo, para que uma medida seja adequada como filtro, um número reduzido de regras deve possuir valores elevados para a medida.

O resultado das análises realizadas nos experimentos foi sintetizado e, com o apoio de técnicas de Clustering e Classificação, análises complementares foram realizadas no intuito de identificar padrões gerais de comportamento para as medidas.

Como resultado geral destes experimentos, constatou-se que algumas medidas têm comportamento semelhante e, principalmente, que o desempenho das medidas varia em relação às bases de dados e aos parâmetros empregados nos algoritmos de extração de regras.

Essa conclusão demanda a criação de um método para agrupar medidas semelhantes.

Possível abordagem para agrupar as medidas é o uso da técnica estatística de análise fatorial, que permite agrupar atributos para um conjunto de exemplos, no caso, as medidas são os atributos e os exemplos são os valores das medidas para cada regra.

A partir dos resultados e conclusões obtidos, foi possível especificar uma metodologia de Pós-processamento de Regras de Associação, fundamentada na hipótese inicial da tese, e que atende aos objetivos aqui estabelecidos e também às necessidades identificadas durante a realização dos experimentos.

A metodologia proposta é disponibilizada com a implementação de um ambiente para avaliação de Regras de Associação, ARInE, implementado utilizando a linguagem de programação PERL e interface Web, sendo a análise de regras realizada com o uso de um navegador.

Uma das contribuições desta tese é a proposta e implementação de método para calcular os valores da tabela de contingência a partir de valores de medidas presentes nos resultados de cada algoritmo de extração de regras, o que agiliza sobremaneira o cálculo destes valores.

Além disso, foram catalogadas e disponibilizadas diversas medidas objetivas, bem como gerada um caracterização dessas medidas em relação à capacidade para filtrar regras e à manutenção desta capacidade para diferentes bases de dados e parâmetros para algoritmos de Mineração de Dados.

Destaca-se ainda, como principal contribuição desta tese, a metodologia de Pós-processamento proposta para avaliação exploratória e interativa de conhecimento representado por Regras de Associação.

Essa metodologia permite ao usuário "navegar" em um conjunto de regras utilizando técnicas baseadas em medidas objetivas para obter pequenos grupos de regras interessantes.

Tais contribuições foram disponibilizadas no Ambiente ARInE, proposto, desenvolvido e implementado durante este trabalho.

Esta tese está dividida em 7 capítulos.

Neste capítulo foi contextualizada a problemática abordada na tese, além de apresentados os objetivos, a metodologia utilizada e as principais contribuições deste trabalho.

É descrito o processo de Mineração de Dados com o detalhamento das principais etapas do referido processo.

É apresentada a tarefa de Regras de Associação incluindo as definições, formalizações e os principais algoritmos.

Também são descritas variantes da tarefa e aplicativos e implementações para extração de regras.

A questão da avaliação do conhecimento representado por Regras de Associação é abordada.

São apresentadas as principais técnicas utilizadas nas referências consultadas.

Entre elas destacam-se as técnicas que empregam medidas na avaliação de regras, abordagem adotada neste trabalho.

Nesse sentido, são descritas diversas medidas (objetivas e subjetivas) utilizadas para avaliar regras e duas técnicas de apoio à análise com medidas objetivas.

Adicionalmente, são descritos alguns trabalhos relacionados ao Pós-processamento de Regras de Associação.

São apresentados os experimentos realizados para verificação da potencialidade do uso de medidas objetivas como "filtros" para Regras de Associação.

Foram realizados experimentos preliminares para verificação do comportamento gráfico de algumas das medidas descritas, a partir dos quais foi elaborada a hipótese de comportamento para as medidas.

Foi realizado conjunto de experimentos utilizando 1bases de dados e 1medidas objetivas, a fim de verificar a hipótese elaborada e analisar a estabilidade do comportamento das medidas em relação a diferentes bases de dados e diferentes parâmetros para extração de regras.

Os resultados obtidos foram analisados e interpretados utilizando-se três técnicas distintas.

O capítulo inclui a síntese dessa investigação e, adicionalmente, estudo experimental sobre a capacidade restritiva de algumas das medidas avaliadas com filtros para regras.

No Capítulo 6 é proposta uma metodologia de Pós-processamento de Regras de Associação desenvolvida a partir da hipótese e dos objetivos desta tese, assim como a partir dos resultados experimentais obtidos.

Essa metodologia utiliza medidas objetivas para avaliar a qualidade das regras, a análise de Pareto como suporte ao usuário na definição de valores de"corte"das medidas e, todo o processo de análise é auxiliado por gráficos das referidas medidas.

E ainda apresentado o Ambiente ARInE que implementa a metodologia proposta, com a apresentação de suas funcionalidades e sua implementação do Ambiente, bem como descrição do processo de análise de Regras de Associação no ARInE.

Além disso, são apresentadas as sintaxes padrão definidas e utilizadas no ARInE que permitem sua integração com os ambientes Discover e RulEE e destes com diferentes algoritmos para extração de Regras de Associação.

As conclusões e principais contribuições desta tese, assim como as propostas de trabalhos futuros, são apresentados.

Por fim, são apresentadas as bibliografias referenciadas nesta tese.

A constante evolução da computação, relacionada especialmente às tecnologias de coleta, armazenamento e transmissão de dados, ao aumento da velocidade de comunicação e à redução dos custos associados a estas tecnologias, tem proporcionado às organizações a capacidade de armazenar e processar suas operações de maneira detalhada.

Isso pode gerar, em pouco tempo, gigabytes, terabytes ou até pentabytes de dados, o que excede em muito a capacidade humana de fazer análises e identificar informações proveitosas.

Tradicionalmente a transformação de dados em informações uteis, em conhecimento, tem ocorrido por meio de análises e interpretações realizadas a partir de procedimentos fundamentalmente manuais, apesar de muitas vezes auxiliados por aplicativos, tais como planilhas eletrônicas e gerenciadores de Banco de Dados.

Em geral os dados são analisados por um ou mais especialistas familiarizados com a natureza dos dados, que comparam gráficos e relatórios, cruzam informações, utilizando-se do conhecimento prévio que possuem sobre o domínio da aplicação.

Assim, os especialistas atuam como uma espécie de interface entre os dados e os possíveis usuários das informações.

Esse procedimento usualmente é lento, subjetivo e custoso, tornando-se impraticável quando se trabalha com grande volume de dados.

Diante das dificuldades para analisar grandes volumes de dados e do reconhecimento por parte das organizações do potencial das vantagens competitivas que podem ser obtidos a partir de suas bases de dados, estudos têm sido direcionados ao desenvolvimento de tecnologias de extração automática de conhecimento de bases de dados.

Esse campo de pesquisa é chamado Extração de Conhecimento de Base de Dados, geralmente referenciado na literatura Knowledge Discovery in Databases (KD ou Mineração de Dados (M).

Alguns autores consideram os termos KDD e MD referentes a processos distintos.

Neste trabalho, os termos KDD e MD serão empregados indistintamente como referência ao processo de extração de conhecimento a partir de dados.

Essa abordagem é adotada em e justifica-se pelo agrupamento de algumas etapas e pela dimensão das bases de dados usualmente utilizadas.

A definição do processo KDD aceita por diversos pesquisadores em Mineração de Dados foi elaborada, "Extração de Conhecimento de bases de dados é o processo de identificação de padrões válidos, novos, potencialmente uteis e compreensíveis embutidos nos dados".

Para melhor compreensão do conteúdo dessa definição, deve-se considerar individualmente cada componente, Dados, Conjunto de fatos ou casos em um repositório de dados.

Por exemplo, dados correspondem aos valores dos campos de um registro de vendas em uma base de dados qualquer

Padrões, Denota alguma abstração de um subconjunto dos dados em alguma linguagem descritiva de conceitos

Processo, A Extração de Conhecimento de bases de dados envolve diferentes etapas a preparação dos dados, a busca por padrões, a avaliação do conhecimento

Válidos, Os padrões descobertos devem satisfazer funções ou limiares que garantam que os exemplos cobertos e os casos relacionados ao padrão encontrado sejam aceitáveis

Novos, Padrões identificados devem fornecer novas informações sobre os dados.

O grau de novidade é um indicador da originalidade desses padrões

Uteis, Os padrões descobertos devem ser incorporados e utilizados

Compreensíveis, Um dos objetivos da realização do processo de Mineração de Dados é identificar padrões descritos em alguma linguagem que possa ser compreendida pelos usuários permitindo uma análise mais aprofundada da base de dados

Conhecimento, O conhecimento é dependente do domínio da aplicação e está relacionado com a capacidade de inovação de um conceito, sua utilidade e inovação para o usuário.

Diferentes abordagens para a divisão do processo de Mineração de Dados em etapas são encontradas na literatura.

Se propõem a divisão do processo em nove etapas.

Divide-se o processo em quatro etapas.

Neste trabalho é considerada a divisão do processo que agrupa o processo em um ciclo com três grandes etapas, Pré-processamento, Extração de Padrões e Pós-processamento.

Adicionalmente, os autores propõem a inclusão de uma fase anterior ao processo de Mineração de Dados, que se refere à identificação do problema a ser abordado, e uma fase posterior ao processo, que se refere à utilização do conhecimento obtido.

Etapas do processo de Mineração de Dados.

O processo de Mineração de Dados é baseado na interação entre as diferentes categorias de usuários e o sucesso de sua aplicação depende, em grande parte, da qualidade dessa interação.

Os usuários do processo podem ser agrupados em classes, a saber, Especialista do domínio, Usuário que possui conhecimento amplo do domínio da aplicação e oferece suporte ao analista na execução do processo.

Analista, Usuário especialista no processo de Extração de Conhecimento e responsável pela sua execução conhece com profundidade cada etapa constituinte do processo.

Usuário final, Representa o grupo de usuários que utiliza o conhecimento extraído no processo de Mineração de Dados como instrumento de apoio nos processos de tomada de decisão.

Não devem possuir necessariamente conhecimento aprofundado do domínio da aplicação.

Nesta classificação cabe mencionar as situações em que um indivíduo é agente em mais de uma classe, como no caso do especialista do domínio que também é usuário final e/ou auxilia e executa funções do analista.

A descoberta de conhecimento util pelo analista é pouco provável sem o suporte do especialista que pode identificar o conhecimento considerado interessante em um domínio particular.

O estudo do domínio da aplicação e a definição de objetivos e metas a serem alcançados no processo de Mineração de Dados são identificados nessa fase.

Também é realizada a identificação e seleção dos conjuntos de dados a serem utilizados durante o processo.

A participação de especialistas do domínio da aplicação fornecendo conhecimento prévio sobre o domínio e dando suporte aos analistas na tarefa de identificação de padrões é aspecto determinante no sucesso do processo de Extração de Conhecimento.

E imprescindível adquirir conhecimentos iniciais sobre o domínio da aplicação, antes de se proceder a execução das tarefas de Mineração de Dados.

Nesta fase devem ser definidas as metas do processo e identificados os critérios de desempenho, o tipo de conhecimento a ser extraído (tipo caixa-preta ou simbólico) e a relação entre simplicidade e precisão do conhecimento extraído.

O conhecimento sobre o domínio subsidia tomadas de decisão nas etapas subsequentes do processo de Mineração de Dados, Pré-processamento, Extração de Padrões e Pós-processamento.

Na etapa de Pré-processamento, este conhecimento contribui para a escolha do conjunto de dados mais adequado à extração de padrões, para a identificação dos valores válidos para os atributos, para a definição de critérios de preferência entre os possíveis atributos, para o estabelecimento de restrições de relacionamentos e para a geração de novos atributos.

Na etapa de Extração de Padrões, o conhecimento sobre o domínio contribui para a seleção de um critério de preferência entre os modelos gerados e ainda no ajuste dos parâmetros do processo de mineração.

Também pode auxiliar a produção de um conhecimento inicial a ser utilizado como entrada do algoritmo de extração visando aumentar a precisão ou melhorar a compreensibilidade do modelo final.

Na etapa de Pós-processamento, esse conhecimento pode contribuir para a avaliação dos padrões extraídos, determinando se os mesmos são interessantes para o usuário.

Entender o domínio dos dados é naturalmente um pré-requisito para se extrair conhecimento util, o usuário final do sistema deve ter algum grau de entendimento sobre a área de aplicação para que quaisquer informações significantes sejam obtidas.

Por outro lado, se existem especialistas com profundo conhecimento sobre o domínio, obter novas informações com o uso de ferramentas semi-automáticas pode ser pouco provável, como no caso de domínios estáveis, no qual os seres humanos tiveram tempo para adquirir o conhecimento especializado em detalhes (exemplo, áreas de comércio em que os produtos e clientes permanecem os mesmos por um longo período).

Usualmente os dados selecionados para o processo de Mineração de Dados não estão em formato adequado para a Extração de Conhecimento.

Problemas gerados no processo de coleta de dados, tais como erros de digitação ou de leitura de dados por sensores, podem originar dados incorretos ou inconsistentes que necessitam de tratamento.

Limitações na capacidade de memória ou no tempo de processamento podem impossibilitar a aplicação direta dos algoritmos de Extração de Padrões a todo o conjunto de dados.

Nesse sentido, torna-se necessária a aplicação de métodos para tratamento, limpeza, redução do volume de dados, antes de se iniciar a etapa de Extração de Padrões.

E importante destacar que os objetivos do processo de extração definidos na fase de Identificação do Problema, devem sempre nortear a execução das operações de Pré-processamento, de tal maneira que o conjunto de dados resultante apresente as características necessárias para que os objetivos estabelecidos sejam atingidos.

Na etapa de Pré-processamento podem ser executadas diversas operações no conjunto de dados, a saber, Extração e integração As fontes de dados disponíveis para Mineração de Dados podem ser encontradas nos mais diversos formatos arquivos texto, arquivos no formato de planilhas, Banco de Dados, Data Warehouse, dentre outros.

Nesse sentido, é necessária a obtenção desses dados e sua unificação, constituindo-se assim uma unica fonte de dados no formato atributo-valor.

Conjunto de dados com exemplos de atributos.

Uma linha t representa o i-ésima transação ou atributo meta.

Em Mineração de Dados preditivo, o valor desse atributo é o valor a ser predito pelo modelo encontrado para novos exemplos.

Quando a função de Mineração de Dados é descritiva, como no caso de Regras de Associação ou clustering, o atributo meta não é definido.

Conjunto de exemplos no formato atributo-valor.

Transformação, Após a extração e unificação os dados devem ser adequados para utilização em algoritmos de Extração de Padrões.

Algumas transformações podem ser aplicadas aos dados.

Resumo, quando dados de transações são agrupados para constituir resumos diários, semanais, mensais.

Transformação de tipo, quando um atributo do tipo data é transformado em outro tipo para que o algoritmo de Extração de Padrões possa utilizá-lo de maneira mais adequada.

Normalização de atributos contínuos quando os valores são colocados em intervalos definidos (por exemplo, entre 0 e 1).

Limpeza, a qualidade dos dados é um fator de relevância no processo de Extração de Conhecimento considerando-se que os resultados gerados usualmente são utilizados em um processo de tomada de decisão.

E possível que a qualidade dos dados esteja comprometida em função de erros (de digitação ou na leitura dos dados por sensores, por exemplo) gerados no processo de coleta.

Nesse sentido, pode ser necessária a aplicação de técnicas de limpeza no conjunto de dados a fim de garantir a qualidade dos mesmos.

A limpeza pode ser realizada utilizando o conhecimento do domínio através da identificação de registros com valor inválido de algum atributo, de granularidade incorreta ou de exemplos errôneos, por exemplo.

A decisão das estratégias de tratamento de atributos incompletos, de remoção de ruído e de tratamento de conjunto de exemplos não balanceados são maneiras de efetuar limpeza dos dados independentemente do domínio da aplicação.

Redução do volume de dados, Em alguns casos, devido a restrições no espaço de memória ou no tempo de processamento, pode ser necessária a aplicação de métodos para redução do volume de dados, para viabilizar a utilização de alguns algoritmos de Extração de Padrões.

Sugere-se três abordagens distintas na redução do volume de dados, a saber, Redução do número de exemplos, deve ser feita mantendo-se as características do conjunto de dados original, isto é, através da geração de amostras representativas do conjunto de dados.

A abordagem mais utilizada para redução do número de exemplos é a amostragem aleatória, considerando-se que tende a produzir amostras representativas.

Cabe destacar que se a amostra não for representativa, ou se a quantidade de exemplos for insuficiente para caracterizar os padrões embutidos nos dados, os modelos encontrados podem não ser representativos da realidade, não tendo portanto qualquer valor.

Outro aspecto a ser considerado é que a utilização de uma quantidade relativamente pequena de exemplos pode dar origem ao fenômeno overfitting, o modelo gerado é representativo do conjunto de treinamento, mas não é adequando para a representação de novos exemplos.

Redução do número de atributos, pode ser uma maneira de redução do espaço de busca pela solução.

Ocorre por meio da seleção de um subconjunto dos atributos existentes, de tal modo que não haja impacto na qualidade da solução final.

A aplicação desse procedimento deve, preferencialmente, contar com o apoio do especialista do domínio, uma vez que a remoção de um atributo potencialmente util para o modelo pode diminuir consideravelmente a qualidade do conhecimento extraído.

A seleção de atributos extras deve ser considerada porque não são conhecidos, a priori, os atributos importantes para atingir os objetivos estabelecidos para o processo de Mineração de Dados.

Uma maneira alternativa de redução do número de atributos é a indução construtiva, na qual um novo atributo é criado a partir do valor de outros, descartando-se os atributos originais.

A indução construtiva visa aumentar a qualidade do conhecimento extraído.

Vale dizer que, em alguns casos, pode ocorrer aumento no número de atributos com o uso da indução construtiva.

Redução do número de valores de um atributo, os processos de discretização ou de suavização de valores de um atributo são geralmente aplicados nessa tarefa.

No processo de discretização de um atributo ocorre a substituição de um atributo contínuo (inteiro ou real) por um atributo discreto, por meio do agrupamento de seus valores.

Em termos gerais, um algoritmo de discretização tem como entrada os valores de um atributo contínuo e produz como saída uma pequena lista de intervalos ordenados.

Cada intervalo é representado na forma, respectivamente, os limites inferior e superior do intervalo.

Os métodos de discretização podem ser classificados em supervisionados ou não supervisionados, locais ou globais e parametrizados ou não parametrizados.

No processo de suavização de valores, o número de valores distintos de um atributo é reduzido, sem que haja discretização do atributo.

Durante este processo os valores são agrupados, de maneira que cada grupo de valores é substituído por um valor numérico representativo, podendo ser a média, a mediana ou mesmo valores de borda de cada grupo.

As operações de Pré-processamento descritas devem ser realizadas de maneira criteriosa e com o devido cuidado, para que as informações presentes nos dados originais continuem presentes nas amostras geradas e os modelos finais sejam representativos da realidade expressa nos dados originais.

A etapa de Extração de Padrões é direcionada ao cumprimento dos objetivos definidos na fase de Identificação do Problema.

Nessa etapa é realizada a escolha, a configuração e a execução de um ou mais algoritmos para extração de conhecimento.

Por tratar-se de processo iterativo, pode ser necessário que essa etapa seja executada repetidas vezes para ajuste do conjunto de parâmetros visando a obtenção de resultados mais adequados aos objetivos pré estabelecidos.

Ajustes podem ser necessários, por exemplo, para a melhoria do grau de precisão ou do nível de compreensibilidade do conhecimento extraído.

A etapa Extração de Padrões inclui as atividades escolha da tarefa de Mineração de Dados e escolha do algoritmo a ser empregado, além da extração dos padrões propriamente dita, Escolha da Tarefa, A escolha da tarefa é realizada em função dos objetivos desejáveis para a solução a ser encontrada.

As funções possíveis na Extração de Padrões podem ser agrupadas em atividades preditivas e descritivas.

Tarefas de Mineração de Dados.

Atividades relacionadas à predição, ou Mineração de Dados preditiva, consistem na generalização de exemplos ou experiências passadas com respostas conhecidas em um modelo capaz de identificar a classe (atributo met de um novo exemplo).

Os dois principais tipos de problemas de predição são Classificação e Regressão.

Problemas de Classificação consistem na predição de um valor categórico, por exemplo, predizer se o cliente é do tipo bom ou mau pagador.

Nos problemas de regressão, o atributo a ser predito consiste num valor contínuo, por exemplo, predizer o lucro ou a perda em um empréstimo financeiro.

Atividades relacionadas à descrição, ou Mineração de Dados descritiva, consistem na identificação de comportamentos intrínsecos do conjunto de dados, sendo que esses dados não possuem um atributo-meta especificado.

Algumas das tarefas descritivas são Clustering, Regras de Associação e Sumarização.

A tarefa Regras de Associação, foco deste trabalho, é descrita detalhadamente.

Escolha do Algoritmo, Uma vez definida a tarefa de Mineração de Dados a ser empregada, existe uma variedade de algoritmos para sua execução.

A definição do algoritmo de extração e a configuração de seus parâmetros são atividades incluídas nesta etapa.

A escolha do algoritmo é subordinada à linguagem de representação dos padrões a serem encontrados.

Pode-se utilizar algoritmos indutores de árvores de decisão ou regra de produção, por exemplo, se o objetivo é realizar uma classificação.

Dos tipos mais frequentes de representação de padrões, destacam-se, Arvores de Decisão, regras de produção, modelos lineares, modelos não lineares (Redes Neurais Artificiais), modelos baseados em exemplos (KNN, K-Nearest Neighbor, Raciocínio Baseado em Casos) e modelos de dependência probabilística (Redes Bayesianas).

A maioria dos algoritmos utilizados em Mineração de Dados podem ser vistos como composição de princípios e técnicas básicas.

A especificação de um algoritmo para Mineração de Dados para uma determinada tarefa inclui a definição dos componentes de algoritmos, a tarefa de Mineração de Dados que o algoritmo deve realizar, Classificação, regressão ou Regras de Associação, por exemplo.

Diferentes tipos de algoritmos são necessários para diferentes tipos de tarefas.

A estrutura (funcional) do padrão que se está ajustando aos dados (por exemplo, um modelo de regressão linear).

A estrutura define os limites do que pode ser aproximado ou aprendido.

Dentro desses limites, os dados levam a um padrão particular.

Em Regras de Associação, a estrutura é definida por meio das regras obtidas.

A função é selecionada para determinar a qualidade dos modelos filtrados a partir dos dados observados (por exemplo, erro quadrático).

Essa função é crítica para o aprendizado.

Em Regras de Associação, as principais funções utilizadas são o suporte e a confiança o método de busca utilizado para busca dos padrões sobre os parâmetros e estruturas.

Em Regras de Associação, um dos métodos mais utilizados é a busca em largura com poda a técnica de gerenciamento de dados utilizada para armazenar, indexar e recuperar os dados durante o processo de extração de dados.

Demonstra-se experimentalmente que não existe um unico algoritmo adequado para todas as tarefas de Mineração de Dados.

Sendo assim, para realizar a tarefa desejada podem ser utilizados uma variedade de algoritmos diferentes.

Isto leva à obtenção de diferentes modelos que, na etapa de Pós-processamento, serão tratados para fornecer o conjunto de padrões mais adequado para o usuário final.

Especificamente no caso de Extração de Padrões por Regras de Associação, mostra-se experimentalmente a independência dos padrões extraídos em relação ao algoritmo de Regras de Associação utilizado.

Extração de Padrões, A etapa de Extração de Padrões propriamente dita no processo de Mineração de Dados consiste na aplicação dos algoritmos selecionados para realizar a extração dos padrões embutidos nos dados.

Cabe destacar que dependendo da função adotada (predição ou descrição) podem ser necessárias diversas execuções dos algoritmos de Extração de Padrões.

Por exemplo, com o objetivo de se obter uma avaliação mais precisa da taxa de erro de um classificador, tem-se utilizado métodos de re-amostragem (resampling).

No método k-fold cross-validation, o conjunto de exemplos é sub-dividido em k partições mutuamente exclusivas e, em cada iteração do método, k-1 partições são fornecidas ao algoritmo de aprendizado.

A partição restante é utilizada para calcular a taxa de erro da iteração.

A média das taxas de erro de cada uma das k iterações é usada como a taxa de erro final, estimada pelo método.

Outro exemplo é o uso de técnicas de combinação de preditores que têm sido pesquisadas com o objetivo de construir um preditor mais preciso pela combinação de vários outros.

O resultado dessa combinação é chamado de ensemble.

A utilização de ensembles tem obtido melhores resultados que a utilização de um unico preditor.

No caso das Regras de Associação o algoritmo pode ser executado uma unica vez sem que haja comprometimento da qualidade do conhecimento extraído, pois os diversos algoritmos de associação devem apresentar a mesma solução.

Adicionalmente, considerando-se que os parâmetros de entrada para avaliação dos resultados parciais (durante a geração das regras) são geralmente o suporte e a confiança, alterações nesse conjunto de valores significa apenas incluir ou excluir novas regras do conjunto de regras extraídas.

A disponibilização do conjunto de padrões extraídos nesta etapa para o usuário ou a sua incorporação a um Sistema Inteligente é realizada tendo-se concluída a análise e/ou processamento dos padrões na etapa de Pós-processamento.

O Pós-processamento é uma etapa importante do processo de Mineração de Dados, na qual os padrões extraídos podem ser simplificados, avaliados, visualizados ou simplesmente documentados para o usuário final.

Considerando-se que o conhecimento extraído possa ser empregado na resolução de problemas do mundo real, seja por meio de um Sistema Inteligente ou por um ser humano fornecendo apoio em um processo de tomada de decisão, devem ser verificadas, nesta etapa, questões do tipo, se o conhecimento extraído representa o conhecimento do especialista, a maneira que o conhecimento extraído difere do conhecimento do especialista, em que parte o conhecimento do especialista está correto.

A resposta a essas questões geralmente não é trivial, considerando-se que os algoritmos de extração de padrões podem gerar uma quantidade elevada de padrões, muitos dos quais podem não ser importantes, relevantes ou interessantes para o usuário.

Sabe-se também que não é produtivo fornecer uma grande quantidade de padrões ao usuário que, normalmente, procura uma pequena lista de padrões interessantes.

Sendo assim, é de grande importância o desenvolvimento de técnicas de suporte para que seja possível fornecer aos usuários exclusivamente os padrões mais interessantes.

Esta etapa inclui conjunto de métodos e procedimentos que podem ser agrupados nas seguintes categorias, Filtragem do Conhecimento, Pode ser realizada por meio de mecanismos de pós-poda, para o caso de Arvores de Decisão, ou de truncagem, no caso de regras de decisão.

E aplicável principalmente nos casos em que os algoritmos de indução produzem arvores de decisão com muitas folhas ou regras de decisão muito específicas, cobrindo poucos exemplos (overfitting).

O emprego de restrição de atributos ou ordenação de regras por meio métricas (medidas de avaliação) é uma maneira alternativa de filtragem do conhecimento, especialmente util para Regras de Associação.

Interpretação e Explanação, E usualmente aplicada quando o conhecimento obtido é utilizado por um usuário final ou num Sistema Inteligente.

O conhecimento pode ser documentado, visualizado ou modificado de modo a torná-lo compreensível ao usuário.

O conhecimento extraído pode ser comparado ao conhecimento preexistente para a verificação de conflitos ou de conformidade e pode ser sumarizado e/ou combinado com o conhecimento prévio do domínio.

Avaliação, Pode ser realizada por meio dos critérios precisão, compreensibilidade, complexidade computacional, grau de interesse, dentre outros.

São apresentadas medidas e critérios que podem ser empregados nesta avaliação.

Integração do Conhecimento, Os sistemas tradicionais de suporte à decisão são dependentes de uma unica técnica, estratégia e modelo.

Os novos e sofisticados sistemas possibilitam combinar ou refinar os resultados de vários modelos de tal modo que se pode obter maior precisão e melhor desempenho do sistema na sua totalidade.

Uma das técnicas que pode ser empregada, como mencionado anteriormente, é o uso da combinação de classificadores.

No desenvolvimento de um ambiente para avaliação de conhecimento, Freitas sugere que sejam considerados, inicialmente, aspectos objetivos de avaliação do conhecimento para a seleção de regras potencialmente interessantes ao usuário e, num segundo momento, aspectos subjetivos como um filtro final para identificação das regras de fato interessantes.

Dentro dessa abordagem, um exemplo de ferramenta de Pós-processamento é o aplicativo AIAS.

A análise do conhecimento extraído poderá determinar se o processo de extração de padrões deve ser ou não repetido.

Caso o conhecimento extraído não seja de interesse para o usuário ou não esteja de acordo como objetivos estabelecidos na fase de Identificação do Problema, pode ser necessária a realização de etapas específicas do processo ou de todo o processo, ajustando-se os parâmetros utilizados ou realizando-se melhorias na seleção de dados.

A investigação do conhecimento extraído também pode determinar a iteração do processo de Extração de Conhecimento, uma vez que podem ser especificados novos objetivos a serem atingidos.

O objetivo maior do processo de Extração de Conhecimento é o uso do conhecimento obtido.

Assim, após ser avaliado e validado na etapa de Pós-processamento, o conhecimento extraído é consolidado na fase de Utilização do Conhecimento.

Após a sua consolidação, o conhecimento pode ser utilizado para resolver eventuais conflitos entre o conhecimento pré-existente (fornecido pelo especialist e o conhecimento obtido com o processo de Mineração de Dados, incorporado a um Sistema Inteligente, ou ainda utilizado diretamente pelo usuário final para apoio a algum processo de tomada de decisão).

O processo de Mineração de Dados tem como objetivo automatizar a tarefa de extrair conhecimento util a partir de grandes volumes de dados.

Não se trata de um sistema de análise automática, mas de um processo de natureza iterativa e interativa, baseado na interação entre especialistas do domínio, usuários e responsáveis pela execução do processo.

Nesse sentido, não é possível presumir que a mera submissão de um conjunto de dados a uma "caixa preta" proporcione a extração de algum conhecimento util.

Neste capítulo, o processo de Mineração de Dados e as etapas que o constituem foram descritos em linhas gerais.

A predição e a descrição são os dois principais tipos de atividade do processo de Mineração de Dados.

Na atividade preditiva, os exemplos fornecidos possuem uma classe associada, e o objetivo é predizer o valor da classe de novos exemplos não rotulados.

Na atividade descritiva, o objetivo é identificar comportamentos intrínsecos existentes no conjunto de dados.

No próximo capítulo é abordada a tarefa de Regras de Associação, uma tarefa descritiva que pode ser empregada no processo de Mineração de Dados e que é foco deste trabalho.

A tarefa é descrita em linhas gerais com a apresentação dos principais conceitos, notações e algoritmos.

Também são apresentados aspectos complementares e uma síntese das ferramentas disponíveis para geração de Regras de Associação.

Na classificação usualmente empregada em Mineração de Dados, a tarefa Regras de Associação é classificada como atividade de Mineração de Dados descritiva.

Apresentada inicialmente, a tarefa Regras de Associação tem tido destaque nos ultimos anos, seja em aplicações práticas ou na área acadêmica.

Dada sua aplicabilidade a problemas de negócio, e elevada compreensibilidade da tarefa mesmo por aqueles não especialistas em Mineração de Dados.

A tarefa de Regras de Associação foi desenvolvida inicialmente a partir da análise dos itens presentes em compras de supermercados, com o objetivo de identificar relações do tipo "um cliente que compra os produtos também irá comprar os produtos com probabilidade".

As Regras de Associação não estão restritas a análises de dependência no contexto de aplicações de varejo, podendo ser aplicadas a uma ampla gama de problemas relacionados a contratos de seguros, saúde, bio-informática, geoprocessamento.

Em termos gerais, uma Regra de Associação determina o quanto a presença de um conjunto de atributos nos registros de uma base de dados implica na presença de algum outro conjunto distinto de atributos u nos mesmos registros.

Conforme a definição original de Agrawal, uma Regra de Associação é representada como uma implicação na forma LHS RHS, em que LHS e RHS são, respectivamente, o lado esquerdo (Left Hand Side) e o lado direito (Right Hand Side) da regra, definidos por conjuntos disjuntos de atributos.

Nesta Seção são apresentadas notações e definições baseadas nos trabalhos, necessárias à compreensão do processo de Mineração de Dados por Regras de Associação, conforme sua definição inicial.

Cabe lembrar que outras abordagens, como as apresentadas na Seção 34, modificam algumas definições de maneira a considerar aspectos não abrangidos pela definição inicial de Regras de Associação.

Diferentemente da base de dados apresentada na tabela, página 16, a base utilizada pelos algoritmos de Regras de Associação é uma tabela booleana de itens-transações.

Tabela booleana de itens-transações.

A base de dados utilizada pelos algoritmos de Regras de Associação pode também ser uma tabela de itens-transações.

Essa tabela é uma simplificação da tabela booleana de itens-transações, na qual itens com valor 1 são mantidos e itens com valor 0 são removidos das transações da tabela de itens-transações.

Tabela de itens-transações.

Haverá necessidade de Pré-processamento dos dados quando a base de dados estiver em formato distinto dos apresentados.

As Regras de Associação são definidas, Seja uma base de dados composta por um conjunto de itens ordenados lexicograficamente e por um conjunto de transações na qual cada transação é composta por um conjunto de itens tal que a regra de associação é uma implicação na forma.

Vale dizer que nem todas as regras descobertas a partir de um conjunto de transações são interessantes ou uteis.

Para prevenir esta ocorrência restrições devem ser estabelecidas.

As mais comumente empregadas são suporte mínimo e confiança mínima (conf-min).

Neste caso, são consideradas exclusivamente regras com valores de suporte e confiança superiores aos valores mínimos definidos.

O emprego destas restrições leva à redução do número de regras geradas e também do esforço (tempo) de processamento.

As medidas suporte e confiança são formalmente definidas.

Anteriormente à descrição dos procedimentos para geração de Regras de Associação, é necessário definir os termos itemset e k-itemset.

É empregado o termo CAS, Canonical Attribute Sequences (Sequência Canônica de Atributos) ao invés do termo itemset.

Conjunto de itens ordenados lexicograficamente.

Já um k-itemset é um conjunto de k itens ordenados lexicograficamente.

Um k-itemset frequente é definido como um conjunto de k itens ordenados lexicograficamente que possui valor de suporte maior que o suporte mínimo definido pelo usuário.

Nas etapas I e relacionadas, são definidos os procedimentos para geração de Regras de Associação, I Encontrar os k-itemsets com suporte maior ou igual que suporte mínimo especificado pelo usuário.

Os itemsets com suporte igual ou superior a sup-min são definidos itemsets frequentes, os demais conjuntos são denominados itemsets não-frequentes.

É apresentado detalhadamente o procedimento para geração dos itemsets.

É apresentado algoritmo para identificação do conjunto de k-itemsets frequentes.

Utilizar os k-itemsets frequentes para gerar as Regras de Associação.

Para cada itemset frequente, encontrar os subconjuntos a de itens de l, não vazios e diferentes de l.

Para cada subconjunto, gerar uma regra na forma se a razão é maior ou igual à confiança mínima especificada pelo usuário (conf-min).

Com um conjunto de itemsets frequentes e um subconjunto de itemsets frequentes, por exemplo, pode-se gerar uma regra.

É apresentado um algoritmo para gerar regras a partir de itemsets frequentes.

As medidas suporte e confiança são as mais empregadas em Regras de Associação, tanto na etapa de Pós-processamento do conhecimento adquirido, quanto na etapa de seleção dos subconjuntos de atributos durante o processo de geração de regras.

Para facilitar a compreensão das medidas, estas são definidas, suporte, quantifica a incidência de um itemset X ou de uma da regra no conjunto de dados, ou seja, indica a frequência com que X ou LHS RHS ocorrem no conjunto de dados.

O suporte de uma regra LHS RHS é representado na equação relacionada, sendo n(LHS RHS) o número de transações nas quais LHS e RHS ocorrem juntos e N o número total de transações consideradas.

Assim, o suporte é equivalente à frequência, ou seja, determina a frequência com que LHS e RHS ocorrem simultaneamente no conjunto de dados.

 confiança indica a frequência com que LHS e RHS ocorrem juntos em relação ao número total de transações em que LHS ocorre.

Assim, a confiança de uma regra LHS RHS pode ser representada por, sendo o número de transações nas quais LHS ocorre.

A confiança é equivalente à frequência condicional, ou seja, representa a frequência de ocorrência de RHS, dado que LHS ocorre.

Tradicionalmente no estudo de Regras de Associação tem-se utilizado como medidas básicas para a avaliação das regras o suporte e a confiança.

Contudo, o uso de apenas estas duas medidas não oferece recursos discriminatórios suficientes.

A sua utilização tem sido objeto de diversos estudos críticos.

Apresenta-se um conjunto de observações a partir da síntese desses estudos.

Observação, Pode-se observar que para valores as regras a b e a c são válidas, possuindo a ultima os maiores valores de suporte e confiança.

Esta condição pode ser considerada um paradoxo dado que, significando que o atributo b é positivamente correlacionado com a, enquanto c é negativamente correlacionado com a, ou seja, a probabilidade de b ocorrer aumenta quando o atributo a é considerado (de 0,25 para 0,50).

Por outro lado, a probabilidade de c ocorrer diminui quando a é considerado (de 0,875 para 0,750).

Pode-se observar que a predominância (em termos de suporte e confiança da regra a c sobre a b deve-se à ocorrência frequente do atributo c).

Apenas com o uso do suporte e da confiança não é possível identificar a ocorrência desta situação, que pode conduzir à geração de regras espúrias, como neste caso.

Observação 2, Considerando ainda os mesmos valores de sup-min e conf-min, para comparar as regras a b e a b.

Observa-se que os valores de suporte e confiança são os mesmos e que b é negativamente correlacionado com a, o que torna a regra a b espúria.

Novamente os valores das medidas foram influenciados pela ocorrência frequente de um atributo.

Adicionalmente, não há meios de prevenir a geração de ambas as regras, ou a regra a b não é gerada, ou é gerada conjuntamente com a regra contraditória, o que não é aceitável.

Observação 3, Considerando uma base de dados que possua vários atributos com elevado valor de suporte, como o caso do atributo sexo em uma base de dados censitários, ou atributos provenientes de processo de discretização ou ainda atributos negados.

Considerando-se também que o conjunto de medidas suporte/ confiança favorece a geração de regras cujo consequente ocorra com grande frequência, pode-se esperar a geração de muitas regras com elevado valor de suporte.

No caso particular de regras envolvendo a negação de atributos, é possível a geração de muitas regras espúrias.

Observação 4, Os atributos a e d não são correlacionados, significando que não existe relação estatística significante entre os atributos.

Contudo, a regra a d tem os mesmos valores de suporte e confiança que a regra a b.

As observações anteriores confirmam a necessidade de considerar outras medidas para a avaliação de regras, especialmente no caso das Regras de Associação.

Contudo, apesar das deficiências apresentadas, o uso do suporte é importante por representar a significância estatística da regra.

Exemplo de base de dados binária.

Regras de associação geradas a partir da base de dados.

Considerando-se uma base de dados D no formato de uma tabela booleana itenstransações ou de uma tabela itens-transações é possível gerar todos os k-itemsets frequentes e, a partir destes, produzir as Regras de Associação.

Exemplo do processo de geração de itemsets frequentes é relacionado a seguir.

No início do processo de mineração de Regras de Associação cada itemset é considerado potencialmente frequente.

Ou seja, o espaço inicial de busca de itemsets frequentes é composto de todos os subconjuntos com exceção do conjunto vazio.

Assim, mesmo os conjuntos com poucos itens tendem a possuir um espaço de busca grande.

O espaço de busca para um conjunto de itens é ilustrado.

Para que não haja necessidade de se percorrer todo o espaço de busca à procura dos itemsets de fato frequentes, algoritmos modernos para minerar Regras de Associação utilizam um método que gera e testa itemsets candidatos.

Esses algoritmos geram conjuntos de itemsets potencialmente frequentes chamados conjuntos de itemsets candidatos.

Utilizam-se da propriedade de linha de fronteira (downward closure) do suporte de um itemset, que determina que todo subconjunto de um itemset frequente deve ser frequente (para remover os itemsets com pelo menos um subconjunto de itens não-frequente).

Espaço de busca para o conjunto de itens.

Então, calcula-se o valor de suporte para cada itemset candidato (não removido) utilizando a base de dados, removendo-se, na sequência, os itemsets candidatos com suporte inferior ao suporte mínimo definido pelo usuário.

O algoritmo inicia uma nova iteração utilizando os itemsets frequentes gerados na ultima iteração e tem encerrada sua execução quando não houver nenhum itemset potencialmente frequente podendo ser considerado um itemset candidato.

A linha em negrito (representa o suporte mínimo definido pelo usuário) separa os itemsets frequentes (parte superior da linh dos itemsets não-frequentes (parte inferior da linh).

A existência dessa linha é garantida pela propriedade da linha de fronteira do suporte de um itemset.

Assim, ao invés de se percorrer todo o espaço de busca à procura de itemsets frequentes, percorre-se apenas o espaço de busca acima da linha em negrito, que representa os itemsets frequentes.

Os itemsets frequentes para gerar Regras de Associação podem ser obtidos utilizando-se diferentes algoritmos, como, AIS, SETM APUD Adamo, Closet, FP-Growth, Direct Hashing and Pruning (DHP) APUD Adamo, Charm, Opus, Dynamic Set Couting (DI APUD Adamo, Apriori e AprioriTid).

Separação de itemsets frequentes e não-frequentes no espaço de busca.

Ainda que diferentes, esses algoritmos (teoricamente) devem gerar sempre um mesmo resultado para um mesmo valor de suporte e confiança mínimos e para um mesmo conjunto de dados.

Por ser considerado, historicamente, um dos mais importantes algoritmos para gerar itemsets frequentes, o algoritmo Apriori é descrito.

Para uma maior compreensão dos procedimentos para identificação das Regra de Associação é apresentado o Exemplo 1, que utiliza o roteiro descrito.

Exemplo 1, Seja uma base de dados que contém um conjunto de itens e um conjunto de transações, no qual a relação de itens comprados por cada transação é apresentada.

Considerando o valor, é possível obter as Regras de Associação contidas seguindo os passos, I Encontrar todos os k-itemsets contidos na que possuam suporte maior ou igual a sup-min (itemsets frequentes).

Não são apresentados os k-itemsets frequentes.

Com os k-itemsets frequentes, gerar todas as Regras de Associação, na próxima página, da seguinte maneira, regra 1, tênis camiseta, regra 2, camiseta tênis.

Com base nos itemsets obtidos no passo I, no passo são geradas as Regras de Associação.

E importante destacar que a complexidade de um sistema de mineração de Regras de Associação é dependente do algoritmo utilizado para gerar os itemsets frequentes.

Na próxima seção são apresentados os primeiros algoritmos para Regras de Associação.

O Apriori extrai todos os itemsets frequentes que estão contidos em uma base de dados e o algoritmo simples proposto, utiliza os itemsets extraídos para gerar Regras de Associação.

São apresentados, adicionalmente, outros exemplos de algoritmos de Regras de Associação.

Nesta seção são apresentados alguns dos algoritmos propostos para extração de Regras de Associação.

Na seleção dos algoritmos apresentados foi considerada sua importância pela frequência com que são apresentados e utilizados pela comunidade.

Os algoritmos AIS e SetM Agrawal foram os primeiros desenvolvidos para aplicação em Regras de Associação.

No AIS, os itemsets candidatos são gerados e contados on-the-fly, conforme a base de dados é percorrida.

Após a leitura de uma transação, é determinado quais dos itemsets previstos para serem frequentes no passo anterior (k-1) estão presentes na transação corrente.

Novos itemsets candidatos são gerados pela extensão destes itemsets frequentes por outros atributos presentes nesta transação.

Um itemset frequente l é estendido apenas por aqueles atributos que são considerados frequentes e ocorrem em ordem lexicográfica após qualquer outro item de l.

Os candidatos gerados a partir de uma transação ou são adicionados ao conjunto C de itemsets candidatos mantido pelo passo atual (k), ou os contadores das entradas correspondentes que foram criados em uma etapa anterior são incrementados.

Dessa maneira, o processo iterativo continua até a geração de todos os itemsets frequentes.

O algoritmo SetM foi motivado pelo desejo de usar SQL para computar os itemsets frequentes.

Como AIS, o algoritmo SetM também gera os itemsets candidatos on-the-fly, baseando-se nas transações lidas da base de dados.

Dessa maneira, o SetM gera cada itemset candidato como o AIS gera.

Contudo, para usar a operação join padrão do SQL na geração das candidatas, SetM separa a etapa de geração das candidatas da contagem dos itemsets.

Uma cópia dos itemsets candidatos é salva junto com um identificador da transação geradora em uma estrutura sequencial.

No final da etapa, o valor do suporte é determinado por meio da ordenação e agregação desta estrutura sequencial.

Este algoritmo não é objeto de detalhamento porque, em termos gerais, apresenta grande semelhança com o algoritmo AIS.

Os algoritmos AIS e SetM possuem pouco interesse prático na descoberta de Regras de Associação, à medida que possuem um desempenho operacional muito inferior aos algoritmos Apriori e AprioriTid.

Esta condição foi mostrada nos experimentos realizados, nos quais as diferenças de desempenho (em termos de tempo) variam, em geral, de a mais de 10 vezes quando reduz-se o valor do suporte mínimo.

Os algoritmos Apriori e ArioriTid diferem fundamentalmente dos algoritmos AIS e SetM na maneira pela qual cada itemset candidato é gerado e contado.

Tanto no Apriori como no AprioriTid, os itemsets candidatos a serem contados em uma dada iteração são gerados a partir dos itemsets frequentes gerados na iteração anterior, sem que se considerem as transações na base de dados.

Aspecto fundamental nesta consideração é que qualquer subconjunto de um itemset frequente deve também ser frequente.

Dessa maneira, pode-se gerar o conjunto de k-itemsets através da combinação dos itemsets e da posterior eliminação dos itemsets não frequentes.

Este procedimento resulta na geração de um número menor de itemsets candidatos, reduzindo significativamente o esforço necessário para identificação dos itemsets frequentes.

O algoritmo Apriori é apresentado no Algoritmo 1.

O algoritmo é utilizado para encontrar todos os k-itemsets frequentes contidos em uma base de dados.

Esse algoritmo gera um conjunto de k-itemsets candidatos e então percorre a base de dados para determinar se os mesmos são frequentes, identificando desse modo todos os k-itemsets frequentes.

Utiliza-se a notação L para representar o conjunto de k-itemsets frequentes e C para representar o conjunto de k-itemsets candidatos.

Inicialmente o algoritmo conta a ocorrência de itens, determinando os itemsets frequentes que são armazenados em L.

O passo seguinte é dividido em duas etapas.

Na primeira (linha do Algoritmo 1) o conjunto de itemsets frequentes L obtido no passo é utilizado para gerar o conjunto de k-itemsets candidatos C usando a função apriori-gen (descrita no Algoritmo).

A seguir (linhas a 9 do Algoritmo 1), a base de dados é percorrida para determinar o valor do suporte dos k-itemsets candidatos em C.

Finalmente, são identificados os k-itemsets frequentes em cada passo (linha 10).

A solução final é dada pela união dos conjuntos L de k-itemsets frequentes.

Essa solução é utilizada como entrada para algum algoritmo que gera Regras de Associação, tal como o Algoritmo 4.

A seguir são apresentadas as funções apriori-gen e subset que fazem parte do algoritmo Apriori.

A função apriori-gen é apresentada no Algoritmo 2.

Essa função usa como argumento L e retorna um superconjunto do conjunto de todos os (k-1)-itemsets.

A função executa inicialmente um join dos elementos dos itemsets em L com o ultimo elemento de outros itemsets, diferentes do primeiro, em L (linhas 1 a do Algoritmo).

Em seguida são podados os k-itemsets que possuem algum subconjunto de tamanho (k1) não pertencente a L (linhas 5 a 11 do Algoritmo).

Essa poda é orientada conforme a propriedade downward closure de suporte de um itemset.

No Exemplo é mostrado como a função apriori-gen pode ser utilizada para gerar os itemsets candidatos.

A função subset retorna os k-itemsets candidatos que estão contidos em uma dada transação t.

Para isso os itemsets candidatos são armazenados em uma árvore-hash.

Cada nó da árvore pode conter uma lista de itemsets ou uma tabela hash (nó folha ou nó intermediário, respectivamente).

Partindo do nó raíz, a função encontra todos os itemsets candidatos presentes na transação t.

Se um nó folha é atingido e o itemset encontrado está contido na transação t, uma referência é adicionada ao conjunto de resposta.

Se um nó intermediário é atingido a partir de um item a, cada item é pesquisado (hash) após a em t.

Isso é possível porque os itens estão em ordem lexicográfica.

No nó raíz, todos os itens a em t são pesquisados.

No Exemplo é ilustrado como a função subset é utilizada.

Itemsets candidatos armazenados em uma árvore-hash.

Na página seguinte é apresentado o funcionamento geral do algoritmo Apriori aplicado a um pequeno conjunto de transações.

Exemplo 4, Seja D uma base de dados que contém um conjunto de itens e um conjunto de transações, no qual a relação de itens comprados por cada transação é apresentada.

O valor do suporte mínimo é igual a 50%.

Ao aplicar o Apriori na base de dados D da tabela, o algoritmo irá percorrer a base de dados gerando um conjunto C de itemsets candidatos e em seguida produzirá um conjunto L de itemsets frequentes (linha 1 do Algoritmo 1), como apresentado no esquema abaixo relacionado.

Na sequência é executada a função apriori-gen (linha do Algoritmo 1) gerando um conjunto C de itemsets candidatos.

O próximo passo, realizado nas linhas a 11 do Algoritmo 1, é verificar quais dos itemsets candidatos em C são frequentes, gerando um conjunto L de itemsets frequentes, como é apresentado a seguir.

Vários passos são realizados pelo algoritmo Apriori até que não seja mais possível gerar um conjunto de itemsets frequentes.

Ao término da execução do algoritmo é gerado o conjunto Resposta pela união de todos os conjuntos L que contêm os k-itemsets frequentes.

Conjunto Resposta contendo os itemsets frequentes.

O conjunto Resposta é utilizado como entrada para algum algoritmo que gera Regras de Associação, como o algoritmo apresentado.

O algoritmo AprioriTid, desenvolvido por é descrito no Algoritmo 3, e assim como o Apriori, utiliza a função apriori-gen para determinação dos itemsets candidatos.

O AprioriTid difere basicamente do Apriori por não utilizar a base de dados para a determinação do suporte dos itemsets candidatos após a primeira iteração.

Para tanto, estabelece uma codificação baseada nos itemsets candidatos do passo anterior.

Esta codificação é armazenada no conjunto C no formato, sendo cada X um potencial k-itemset presente em uma transação com identificador.

Entre os algoritmos que geram Regras de Associação para uma base de dados a partir dos itemsets frequentes, é apresentado, no Algoritmo 4, na página oposta, o algoritmo proposto, um dos mais simples para realização dessa tarefa.

O algoritmo é executado para os k-itemsets frequentes, com k 2.

Inicialmente são gerados os subconjuntos não vazios de um itemset frequente.

Em seguida são geradas regras do tipo LHS RHS utilizando os subconjuntos definidos, que satisfazem a condição, confiança da regra maior ou igual a confiança mínima especificada pelo usuário (conf-min).

O procedimento para gerar uma Regra de Associação a partir de um conjunto de itemsets frequentes é apresentado no Exemplo 5 (será considerado um dos conjuntos de itemsets frequentes contidos na o Exemplo 4).

Exemplo 5 Seja resposta, o itemset frequente contido no conjunto Resposta apresentado na confiança mínima (conf-min) igual a 50%, no procedimento genrules são gerados todos os subconjuntos a a partir de resposta (linha 5 do Algoritmo 4), Como mostrado a seguir, o passo seguinte na execução do algoritmo consiste no cálculo da confiança desses subconjuntos e na geração de uma Regra de Associação para cada subconjunto com confiança maior ou igual a conf-min (linhas 6 a 9 do Algoritmo 4).

Até o presente momento foram considerados nas descrições e exemplos apresentados atributos do tipo binário, seguindo a abordagem.

Entretanto, cabe dizer que as bases de dados de interesse podem ser compostas por outros tipos de atributos (categóricos e numéricos).

Nesta seção serão apresentadas questões complementares relacionadas aos atributos, o uso de taxonomia de atributos, de restrições de atributos e de atributos categóricos e numéricos.

Também serão apresentados estudos relacionados ao uso de Regras de Associação na identificação de padrões sequenciais, de ocorrências periódicas, de associações negativas e uso de pesos em atributos.

As taxonomias refletem uma visão coletiva ou individual, sempre arbitrária, de como os atributos podem ser hierarquicamente classificados.

Eventualmente, múltiplas taxonomias podem estar presentes simultaneamente refletindo a existência de diferentes pontos de vista ou a possibilidade de classificações distintas para o mesmo conjunto de atributos.

É apresentado um exemplo do uso de taxonomia para alguns itens de vestuário.

As principais motivações apresentadas para o emprego de taxonomias em Regras de Associação são apresentadas a seguir, regras simples (aquelas cujos elementos são compostos exclusivamente por itens terminais na taxonomia, atributos) podem não ter suporte suficiente para ser incluídas na solução, mas podem produzir conhecimento interessante quando agrupadas segundo uma dada taxonomia generalização de regras muito especializadas.

Como no caso anterior, mesmo considerando-se regras simples com elevados níveis de suporte e confiança, estas ainda podem ser agrupadas em regras gerais, melhorando-se a compreensibilidade por parte do usuário (também com maiores valores de suporte e confiança, identificação de regras interessantes com uso de informações da taxonomia).

Utiliza-se esta tarefa no aplicativo AIAS.

Do ponto de vista da lógica proposicional, a introdução de taxonomias é uma maneira de se complementar a capacidade descritiva das Regras de Associação com disjunções.

Por exemplo, "roupas leves calçados" pode ser lido como (camisetas ou bermudas, tênis ou sandáli).

Pode-se observar que esta é uma maneira restritiva de introdução de disjunções, uma vez que os argumentos de disjunção não são automaticamente descobertos, como o são os argumentos de conjunção.

As disjunções devem ser introduzidas nas regras, explícita e arbitrariamente, por meio de taxonomias especificadas pelo usuário.

As regras com taxonomias são denominadas Regras de Associação generalizadas e buscam identificar associação entre os itens que estejam no mesmo nível de taxonomia ou em níveis diferentes.

Um exemplo de Regra de Associação generalizada é "camiseta calçado".

Uma maneira direta, mas pouco eficiente, de se obter esse tipo de regra seria a geração de um atributo novo para cada nível hierárquico (geralmente os níveis da hierarquia excluem o nível de fundo, isto é, os itemset).

Algoritmos que incorporam o uso de informação hierárquica no processo de extração de padrões foram propostos.

Uma abordagem orientada a objetos é apresentada.

Se propõe o emprego de consultas do tipo SQL para identificar Regras de Associação em múltiplos níveis.

As conjunções estão presentes no antecedente e no consequente de uma Regra de Associação de maneira implícita, conforme a definição desta.

Finalmente, Regras de Associação flexíveis em múltiplos níveis são apresentadas.

Como já mencionado, os usuários geralmente não estão interessados em todo o conjunto de regras, mas sim num conjunto particular que contenha alguns atributos.

Por exemplo, regras que contenham os atributos A e B como consequentes ou regras que não contenham o atributo C como antecedente.

O uso de restrição sobre atributos é uma maneira alternativa para reduzir o escopo do conhecimento extraído.

A aplicação de restrições sobre atributos na etapa de Pós-processamento é trivial, podendo ser realizada por meio da aplicação de filtros.

Porém pode-se obter vantagens significativas em se tratando da velocidade de processamento considerando-se estas restrições durante o processo de geração de regras.

Em seu trabalho definem as Regras de Associação com restrições e propõem os algoritmos, Reorder, MultipleJoin e Direct, derivados do algoritmo Apriori, que consideram a inclusão de restrição sobre atributos.

São adotadas funções do tipo booleana para indicar a presença ou ausência dos atributos de interesse.

Também são adotadas restrições sobre a taxonomia dos atributos, podendo-se considerar ou desconsiderar regras com presença ou ausência de ancestrais ou descendentes de determinados atributos.

Dentre os resultados apresentados pelos autores, destaca-se a melhoria de 5 a 20 vezes no tempo de processamento em função do nível de seletividade (restrições) adotado para os atributos.

Os resultados foram obtidos a partir da comparação com o desempenho do algoritmo Apriori.

Um exemplo do uso de restrições que podem ser empregadas é que expressa a restrição nas regras contêm camisetas e calçados ou os descendentes de roupas leves e não contêm os ancestrais de sandálias.

Ao invés de descobrir todas as regras e depois realizar a poda utilizando as restrições desejadas, os autores incorporam o uso de restrições no algoritmo de geração das Regras de Associação.

A abordagem denominada Consultas de associações com restrição permite lidar com restrições mais complexas no processo de geração de Regras de Associação.

Como exemplo, a expressão, que encontra os lanches mais baratos que as cervejas.

Outra abordagem é proposta, que impõe, além das restrições relacionadas ao suporte mínimo e à confiança mínima, uma nova restrição para garantir que toda regra possua "vantagem preditiva" em relação às suas simplificações.

Como apresentado, é considerado que a base de dados sobre a qual são aplicados os algoritmos de Regras de Associação está representada numa tabela booleana, onde a presença de um atributo em uma transação é representada por "1" e sua ausência por "0".

Contudo, a maioria das bases de dados possui atributos de maior complexidade, atributos categóricos, podendo assumir vários valores, e atributos numéricos.

Para considerar esses tipos de atributos foi introduzido o conceito Regras de Associação quantitativas.

Um exemplo de Regra de Associação quantitativa é que indica que em 40% das transações totais, 90% das pessoas cuja idade está entre 30 e 40 anos e são casadas têm dois carros.

O algoritmo proposto pelos autores busca dividir os valores de atributos quantitativos e categóricos em intervalos, de tal modo a maximizar a associação entre os itens da regra.

Uma maneira de considerar a complexidade desses tipos de atributos é realizar o mapeamento dos atributos para a forma binária.

No caso dos atributos discretos, pode-se fazer o simples mapeamento criando-se um novo atributo para cada valor do atributo em questão.

No caso dos atributos contínuos, uma possível solução é discretizar os atributos e então realizar o mapeamento para a tabela binária.

Os autores apresentam as dificuldades relacionadas a esta abordagem considerando-se o suporte mínimo e a confiança mínima, no caso do suporte mínimo, se o número de intervalos para um atributo numérico (ou valores, para atributos categóricos) é elevado, o valor de suporte para cada intervalo definido pode ser baixo.

Sendo assim, sem o uso de intervalos amplos, algumas regras envolvendo este atributo podem não ser encontradas por não possuírem suporte suficiente, no caso da confiança mínima (conf-min), há alguma perda de informação quando particionam-se valores em intervalos.

Algumas regras podem ter confiança mínima apenas quando um item no antecedente consiste em um unico valor (ou num pequeno intervalo).

A perda de informação é ampliada à medida que se aumenta o tamanho do intervalo.

Os problemas apresentados conduzem a uma situação dúbia, se os intervalos são amplos a regra pode não ter confiança suficiente se os intervalos são pequenos a regra pode não apresentar suporte suficiente.

Contorna-se esse problema considerando todos os intervalos contíguos aos intervalos adotados (para o caso do supmin) e aumentando o número de intervalos considerados para o caso da conf-min.

Abordagem alternativa para essa problemática é apresentada, que introduziu o conceito de Regras de Associação otimizadas, no qual a discretização dos valores dos atributos numéricos é realizada de maneira a maximizar os valores de suporte e confiança das regras.

Idéia semelhante foi investigada, considerando, além dos atributos numéricos, atributos do tipo categórico.

Se propõe a fusão de intervalos baseada no grau de interesse para combinar diferentes intervalos em um unico intervalo, de maneira a maximizar o grau de interesse de uma regra.

Em estudo relacionado a atributos numéricos, propõem Regras de Associação do tipo fuzzy.

Regra de Associação fuzzy.

E assumido que conjuntos fuzzy para cada atributo são fornecidos como entrada.

Emprega-se esquema de agrupamento (clustering) para extrair os conjuntos fuzzy.

Regras de Associação visam a descoberta de co-ocorrências em um dado intervalo de tempo.

Com o armazenamento de dados por um longo período e o desenvolvimento de Bancos de Dados Temporais, a descoberta de padrões sequenciais se tornou assunto importante.

Exemplo de padrão sequencial, clientes tipicamente alugam "Guerra nas Estrelas", "O império contra ataca" e "O Retorno do Jedi".

Os itens em um padrão sequencial precisam ser consecutivos e estar numa ordem determinada.

Três algoritmos foram propostos para extrair padrões sequenciais a partir de uma base de dados de transações.

Amplia-se essa abordagem, permitindo o uso de janelas corrediças e de informações hierárquicas sobre os atributos.

Define-se os termos episódios frequentes, uma coleção de eventos em um certo padrão, e episódios generalizados, episódios que satisfazem determinadas condições, em uma sequencia de dados.

Algoritmos para a descoberta de episódios frequentes.

Em uma sequência de dados, Regras de Associação podem identificar propriedades periódicas.

Algumas regras podem ter valor de suporte superior ao suporte mínimo para um pequeno período de tempo, mesmo não tendo suporte tão elevado quando computado no Banco de Dados completo.

Se introduz o conceito de Regras de Associação cíclicas, que são regras com confiança e suporte acima dos valores especificados em intervalos de tempo regulares.

Esse tipo de regra, por exemplo, pode conter a afirmação "Pessoas compram jornais junto com leite todos os domingos".

Ao invés de identificar conjuntos de regras a cada instante de tempo e então produzir as regras periódicas a partir destes conjuntos, os algoritmos incorporam heurísticas para otimizar o processo.

Estes algoritmos podem ser aplicados apenas em casos nos quais as regras são repetidas a todo instante de tempo.

Estende-se o estudo de Regras de Associação periódicas para encontrar Regras de Associação em calendários.

O termo calendário pode ser entendido como um período de tempo recorrente.

Nessa abordagem, busca-se identificar regras que sigam padrões em um calendário especificado pelo usuário.

Além disso, os algoritmos foram propostos para extrair regras considerando quaisquer calendários.

Estes estudos estão baseados no conceito de periodicidade completa, a regra deve ser válida em todos os instantes de tempo definidos por um dado padrão (calendário).

Se propõe flexibilizar essa restrição buscando identificar padrões periódicos parciais, um tipo de periodicidade menos restritiva.

Nos algoritmos até agora apresentados, os atributos presentes na base de dados são considerados como tendo o mesmo grau de importância.

Entretanto, em determinadas situações, alguns desses atributos podem possuir importância diferenciada, como no caso de fatores condicionantes de uma doença.

Se intriduz o conceito de Regras de Associação ponderadas, que considera atributos com diferentes graus de importância na geração de regras a partir da atribuição de pesos.

Os pesos geralmente estão relacionados às promoções de determinados produtos, ao preço destes ou a outra qualquer característica diferenciadora.

Considerando que a abordagem tradicional de identificação dos itemsets (bottom-up) não é válida neste caso, os autores propõem um novo algoritmo e definem o suporte ponderado de um itemset e de uma Regra de Associação.

Em Regras de Associação negativas considera-se que se um determinado item ocorre em uma transação outro não deverá ocorrer.

Exemplo desse tipo de regra é "a maioria das pessoas que compra comida congelada não compra legumes".

Uma solução possível para essa abordagem é definir os valores de suporte mínimo e confiança mínima tão baixos quanto possível.

Esta solução entretanto gera muitas Regras de Associação negativas desinteressantes.

Uma abordagem mais eficiente é proposta, na qual são extraídas regras para as quais se espera um grau alto de associação positiva e o suporte é significativamente menor que o esperado.

Em Regras de Associação de proporções, o objetivo é identificar correlações existentes entre quantidades ou preços de itens diferentes.

Exemplo desse tipo de regra "clientes gastam tipicamente 1,2,5 reais em pão, leite, manteiga".

No algoritmo é definido método para identificar a qualidade das regras denominado de "erro de aproximação".

As Regras de Associação de proporções podem ser empregadas, por exemplo, para calcular valores ausentes, ainda que vários valores estejam ausentes simultaneamente.

As ferramentas para geração de Regras de Associação, em geral, tratam-se de aplicativos que auxiliam, com maior ou menor grau de facilidade para o usuário, a realização do processo de Mineração de Dados como um todo.

Geralmente dispõem de facilidades que permitem a unificação, transformação, limpeza e redução dos dados, a realização de análises estatísticas, a escolha entre diferentes funções de Extração de Conhecimento, a visualização e avaliação do conhecimento extraído.

A análise destas ferramentas objetiva, principalmente, a coleta de subsídios para estabelecer um formato padrão de representação de regras, atributos e dados, a identificação dos parâmetros a serem armazenados para avaliação das regras na etapa de Pós-processamento e a definição das maneiras de apresentação dos padrões extraídos.

Características das ferramentas de Mineração de Dados que possuem capacidade para gerar Regras de Associação.

A lista de ferramentas analisadas foi obtida no site e complementada com os aplicativos Oracle e Microsoft Data Analyzer.

Neste capítulo foi realizada descrição pormenorizada da tarefa para Extração de Conhecimento denominada Regras de Associação.

Foram apresentadas sua definição formal e a notação empregada, descritos os algoritmos utilizados e analisados aspectos complementares sobre atributos a serem considerados.

O capítulo inclui, adicionalmente, algumas ferramentas para Mineração de Dados que disponibilizam esta tarefa.

Em termos gerais, as Regras de Associação permitem identificar associações entre atributos ou conjuntos de atributos de uma base de dados.

A despeito de sua utilidade em aplicações práticas, os algoritmos desta tarefa tendem a produzir grande quantidade de regras, muitas das quais sem qualquer interesse para o usuário.

Analisando manualmente um grande volume de regras, dificilmente serão identificadas aquelas realmente interessantes.

Ferramentas para Regras de Associação.

A solução dessa questão passa necessariamente pela etapa de Pós-processamento do conhecimento extraído, especialmente, por meio do emprego de medidas que permitem analisar e avaliar as regras sob variados aspectos.

No capítulo a seguir é apresentado um elenco de medidas que podem ser utilizadas para avaliação de regras, são mostrados princípios de interessabilidade de regras e o aplicativo AIAS (que considera aspectos subjetivos na análise do interesse de Regras de Associação).

O capítulo inclui um experimento para verificação do potencial de uso das medidas objetivas descritas como auxiliares na etapa de Pós-processamento do conhecimento.

A partir da proposição da tarefa Regras de Associação, a maioria dos esforços em pesquisas sobre a mesma tem sido direcionada ao aprimoramento do desempenho dos algoritmos de extração de regras, em especial no sentido de propor soluções mais eficientes para o problema da identificação e contagem dos itemsets.

Em paralelo, diversos aplicativos foram desenvolvidos incorporando os avanços propiciados por essas pesquisas.

Existe outra problemática associada às Regras de Associação que está relacionada à quantidade de regras geradas, mesmo quando a tarefa é aplicada a bases de dados relativamente pequenas.

São apresentados os resultados obtidos por meio da aplicação de algoritmos de Regras de Associação a algumas bases de dados.

Pode-se notar a grande quantidade de regras geradas quando se compara às quantidades usuais obtidas por outras tarefas de Mineração de Dados (classificação e regressão, por exemplo).

Esta quantidade geralmente excede a dimensão da base de dados minerada e, principalmente, vai além da capacidade de interpretação do ser humano.

Adicionalmente, o especialista do domínio ou o usuário final do sistema está interessado em uma pequena fração das regras, aquelas que apresentam algum conhecimento util, interessante ou inovador.

Assim, é necessário desenvolver metodologias e ferramentas eficientes que apóiem o usuário na identificação de regras de seu interesse durante a etapa de Pós-processamento do conhecimento.

Problemática de Regras de Associação.

Desse modo, são apresentados neste capítulo princípios de interessabilidade de regras, para se definir as principais características que regras interessantes devem possuir.

São apresentadas, adicionalmente, algumas técnicas de Pós-processamento de conhecimento propostas por diversos pesquisadores.

Na sequência são descritas diferentes medidas (objetivas e subjetivas) para a avaliação de regras e também duas técnicas utilizadas no apoio à análise de regras utilizando medidas objetivas.

No final do capítulo são descritos, de forma sucinta, o aplicativo AIAS, os ambientes Discover e RulEE e o módulo computacional RulEE GAR, instrumentos que podem ser empregados na avaliação de conhecimento.

Medidas do grau de interesse de uma regra são baseadas em conceitos tais como os de utilidade e inesperabilidade.

A possibilidade do usuário em obter proveito da sua utilização é um indicador da interessabilidade da mesma.

A inesperabilidade indica que as regras que representam um conhecimento "inesperado" para o usuário podem ser consideradas interessantes, uma vez que irão ampliar ou contestar os conhecimentos que o usuário possui sobre o domínio.

Vale dizer que a inesperabilidade e a utilidade são conceitos não excludentes entre si, podendo ser combinados nas formas apresentadas, regras inesperadas e utilizáveis, regras inesperadas e não utilizáveis, regras esperadas e utilizáveis.

As regras pertencentes às duas primeiras categorias são identificadas pela aplicação de medidas de inesperabilidade, enquanto as regras pertencentes à primeira ou terceira categoria são identificadas com o uso de medidas de utilidade.

Se propõe princípios intuitivos que uma medida deve satisfazer para avaliar o grau de interesse (IR) de uma regra.

Esse tipo de medida deve associar valores elevados a regras "fortes" e valores baixos a regras "fracas".

Uma "regra forte" pode ser definida como uma descrição de regularidade, com alta confiança para uma grande quantidade de exemplos, uma"regra fraca" representa uma regularidade com alta confiança, para um número reduzido de exemplos.

Dada a regra defini-se os princípios, A aplicação destes princípios pode ser estendida a outras medidas.

Por exemplo, no caso das medidas apresentadas, IR aumenta com a confiança e IR diminui com a confiança esperada.

Ou ainda IR aumenta com a complexidade da regra e IR diminui com o tamanho do conjunto de exemplos.

Diversos trabalhos avaliam o conhecimento obtido pelo processo de Mineração de Dados.

Nessa seção são descritos alguns desses trabalhos, categorizados e agrupados conforme a predominância da técnica empregada.

Essa abordagem permite que o usuário explore o conjunto de regras por meio do uso de uma linguagem de consulta (tipicamente inspirada no SQL, Structured Query Language).

Usando esse tipo de facilidade é possível obter subconjuntos de regras contendo determinados elementos e/ou satisfazendo alguma restrição definida pelo usuário, que envolvam, por exemplo, o suporte e a confiança.

Uma variedade de medidas para avaliação de conhecimento tem sido pesquisada com a finalidade de fornecer subsídios ao usuário no entendimento e utilização do conhecimento adquirido.

Essas medidas podem ser categorizadas quanto à forma de avaliação e quanto ao objeto de avaliação.

Quanto ao modo de avaliação, as medidas podem ser objetivas ou subjetivas.

As medidas objetivas dependem exclusivamente da estrutura dos padrões (regras) e dos dados utilizados no processo de extração de conhecimento.

As medidas subjetivas dependem fundamentalmente dos usuários que irão interpretar o conhecimento.

Com relação ao objeto de avaliação, as medidas permitem avaliar o desempenho ou a qualidade de uma regra.

O desempenho de uma regra está associado à fidelidade com que representa os dados.

Para a avaliação da qualidade dos padrões gerados são utilizados os conceitos de compreensibilidade e grau de interesse.

A facilidade de um ser humano interpretar um dado conjunto de regras está relacionada à compreensibilidade deste conjunto, podendo ser estimada, por exemplo, pelo número de regras presentes no conjunto e pelo número de condições em cada regra.

O grau de interesse, por sua vez, é uma avaliação de natureza qualitativa realizada a partir de estimativas da quantidade de conhecimento interessante (inovador, inesperado) presente nas regras.

As medidas do grau de interesse são representadas, por exemplo, pelo grau de utilidade das regras para o usuário final do processo de Extração de Conhecimento.

Uma das abordagens utilizadas para reduzir o número de regras geradas é a poda (pruning), que tem o objetivo de eliminar regras redundantes ou que não são interessantes para o usuário.

A técnica de sumarização objetiva resumir o conhecimento extraído por meio da criação de regras gerais ou conceitos abstratos que sejam facilmente entendidos pelo usuário.

As regras gerais retratam conceitos amplos do conhecimento descoberto, enquanto as regras específicas podem ser exploradas posteriormente, em uma análise pormenorizada.

A técnica de generalização utiliza taxonomias para transformar regras específicas em conceitos gerais, produzindo conjuntos de regras mais compactos e geralmente mais compreensíveis aos usuários.

O objetivo da técnica de agrupamento (clustering) é fornecer ao usuário grupos de regras que correspondam a diferentes temas do domínio em questão.

Essa abordagem procura identificar agrupamentos que representam características específicas de determinados grupos de regras contidas no conjunto original.

Com base nesse agrupamento, técnicas de sumarização/generalização podem ser utilizadas para resumir e/ou compactar as regras de cada agrupamento encontrado.

Cada um desses agrupamentos pode representar particularidades da base de dados.

Na etapa de Pós-processamento do conhecimento, uma das abordagens possíveis é o emprego de medidas para avaliação de regras.

A organização do trabalho, neste tópico, prevê a divisão das medidas nas categorias-medidas de caráter objetivo e medidas subjetivas.

Adicionalmente é realizado um exemplo de cálculo para as medidas objetivas.

Visando padronizar e sintetizar a representação das medidas objetivas, é feita sua descrição com base nas frequências associadas a cada termo da regra.

As frequências são obtidas a partir da tabela de contingência descrita na sequência.

A tabela de contingência para uma regra representa sua cobertura.

LHS denota o antecedente da regra, e LHS o complemento de LHS de forma análoga RHS denota o consequente e RHS seu complemento.

N(LHS RHS) denota o número de transações nas quais LHS e RHS são verdadeiros.

N(LHS RHS) denota o número de transações nas quais LHS é falso e RHS é verdadeiro, os demais elementos são representados de modo análogo.

N denota o número total de transações.

Tabela de contingência.

Em termos genéricos a frequência relativa associada ao conjunto pode ser calculada e denotada.

Assim definido, a frequência relativa pode ser utilizada como estimativa de probabilidade.

A notação segue a definição usual de probabilidade.

Com base na tabela de contingência são calculadas as frequências relativas referentes a cada elemento.

As frequências a serem utilizadas nas medidas de desempenho são apresentadas.

Frequências obtidas a partir da tabela de contingência.

Medidas objetivas são aquelas que dependem, para sua determinação, exclusivamente da estrutura dos padrões (regras) extraídos e dos dados utilizados no seu processo de extração.

Utilizadas pelo aplicativo MineSet TM, em conjunto com as medidas suporte e confiança, a confiança esperada e o Lift são definidos, confiança esperada, quantifica a incidência de RHS no conjunto de dados, independente da ocorrência de LHS.

A diferença entre confiança e confiança esperada indica o ganho de poder descritivo ao considerar LHS.

Em um estudo sobre Regras de Associação para atributos numéricos, apresentam as medidas J, J e J, que, de maneira geral, expressam a relação entre a generalidade e a capacidade discriminatória de uma regra.

Medida J, o primeiro termo da Equação 4epresenta a generalidade da regra.

O termo entre colchetes avalia o "poder discriminatório" de LHS em relação a RHS, isto é, o ganho de discriminação obtido pela consideração de LHS, quando comparado à regra true RHS.

A medida é dada por, Medida J, derivada da medida J, quando considera-se a capacidade de discriminação.

A medida J representa o "excedente" de associação da regra LHS RHS em relação ao nível de associação especificado pela confiança mínima (conf-min).

Apresenta-se um conjunto de dezesseis medidas, dentre as quais suporte e confiança (já definidas anteriormente).

As demais medidas são apresentadas nas Equações 47 à 420.

 confiança negativa, representa a confiança na não ocorrência de associações, ou seja, dado que o antecedente é falso, o consequente também é falso.

Sensibilidade, é definida como a frequência condicional de que LHS é verdadeiro dado que RHS é verdadeiro.

Pode ser interpretada também como sendo a confiança da regra RHS LHS.

Especificidade, é dada pela frequência condicional de que LHS seja falso dado que RHS é falso.

Cobertura, representa a fração das instâncias cobertas pelo antecedente da regra (LHS), podendo ser considerada como medida de generalidade da regra.

Novidade, objetiva identificar o quão inovadora, interessante ou não-usual é uma dada regra.

E definida pela comparação entre o valor observado da ocorrência de LHS e RHS e o valor esperado de ocorrência se LHS e RHS fossem independentes.

Cabe destacar que o primeiro termo da medida é o suporte da regra, indicando que valores elevados de suporte apenas são interessantes quando os valores são relativamente pequenos.

Compara o suporte da regra ao suporte de cada termo, no caso de independência estatística.

De maneira semelhante à medida suporte, a medida novidade, tal como definida, é simétrica em relação aos lados da regra, embora essa característica, intuitivamente, não seja uma condição sempre verdadeira.

A medida satisfação, definida na sequência, se utilizada conjuntamente com a medida novidade permite diferenciar as regras na referida simetria.

Satisfação, representa a queda relativa do erro entre true RHS e LHS RHS.

 confiança relativa, representa o ganho de confiança em relação à regra.

De maneira similar à definição da confiança relativa, são definidas as medidas confiança negativa relativa, sensibilidade relativa e especificidade relativa.

 confiança negativa relativa, representa, de modo semelhante à confiança relativa, a utilidade de considerar LHS como antecedente de RHS.

Sensibilidade relativa, representa o ganho de sensibilidade em relação a LHS true.

Especificidade relativa, de forma análoga à sensibilidade relativa, representa o ganho de especificidade em relação a LHS true.

Considerando que as medidas relativas fornecem mais informações sobre a utilidade de uma regra que as medidas absolutas, é comum obter-se valores elevados de confiança relativa para regras altamente específicas.

Para contornar esse inconveniente, as medidas relativas são ponderadas, definindo, desse modo, o novo conjunto de medidas apresentado na sequência.

 confiança relativa ponderada, definida pela ponderação da confiança relativa, a medida avalia de maneira balanceada a generalidade e a confiança relativa, Precisão Relativa, uma vez que a definição do termo precisão adotada pelos autores é coincidente com a definição de confiança, quando trabalha-se com Regras de Associação.

 confiança negativa relativa ponderada, de maneira análoga à medida anterior, avalia balanceadamente a generalidade e a confiança negativa relativa de uma regra.

Sensibilidade relativa ponderada, definida pela ponderação da sensibilidade relativa, estabelece um balanceamento entre a confiança negativa relativa e a generalidade de uma regra.

Especificidade relativa ponderada, de maneira similar à medida anterior, estabelece um balanceamento entre a especificidade relativa e a generalidade de uma regra.

Vale dizer que as medidas relativas ponderadas são equivalentes entre si e numericamente iguais à medida novidade.

Sugere-se o uso da convicção como medida complementar ao suporte e à confiança, para utilização durante o processo de geração de regras.

Convicção, de modo similar à medida confiança, a convicção representa o "poder" associativo entre o antecedente e o consequente de uma regra.

A convicção, entretanto, apresenta maior capacidade distintiva e a capacidade de diferenciar a regra LHS RHS da regra RHS LHS.

Com a finalidade de ilustrar a aplicação das medidas de avaliação descritas, é apresentado na seção seguinte exemplo de cálculo dessas medidas.

Na realização do exemplo foi considerada amostra de cerca de 25% da base de dados QVU que contém 5 atributos e 30 exemplos.

Com o auxílio do aplicativo MineSet TM foram geradas as Regras de Associação, considerando-se os valores default e no máximo três atributos por regra.

Das 1081 regras geradas pelo aplicativo, duas foram selecionadas para exemplificar o cálculo das medidas.

Para a Regra 1, a tabela de contingência é apresentada.

As frequências utilizadas no cálculo das medidas são apresentadas.

Para a Regra 2, a tabela de contingência e as frequências utilizadas no cálculo das medidas são apresentadas.

Interpretação, A regra significa que, para um percentual de até 92,8372% de domicílios atendidos por rede de esgotamento sanitário e com índice de qualidade de vida superior a 0,42407, o percentual de domicílios ocupados pelo proprietário é inferior a 51,945%.

Interpretação, A Regra significa que para o percentual de domicílios atendidos pela rede de esgotamento sanitário entre 97,68871% e 98,05199%, a renda dos chefes de família está entre 152,17 e 182,25.

Da análise nota-se que, de modo geral, os valores das medidas para a Regra 1 são ligeiramente melhores em relação aos valores para a Regra 2.

Optou-se pela utilização deste aplicativo para a geração das Regras de Associação, pelos recursos que oferece (discretização automática de atributos, por exemplo) e pela sua disponibilidade no Laboratório de Inteligência Computacional (LABI, onde foram executados os experimentos).

Amostra da base de dados QVU.

Descrição dos atributos.

ACESSIBI, Nível de acessibilidade as atividades urbanas.

RENDCHEF, Renda dos chefes de família.

PerOcpPr, Percentual de domicílios ocupados pelos proprietários.

ESGOTO P, Percentual de domicílios atendidos pela rede de coleta de esgoto sanitário.

QV, Nível de qualidade de vida urbana.

No caso dos valores de Lift e confiança esperada a Regra apresenta valores melhores, indicando um ganho de informação pela consideração do LHS.

Analisando-se intuitivamente as regras, esta semelhança de representatividade de informações pode ser confirmada, uma vez que ambas as regras, em linhas gerais, relacionam nível de atendimento por infra-estrutura urbana e nível de renda (RENDCHEF e PerOcpPr) de forma positiva, isto é, para baixos valores de oferta de infra-estrutura, o nível de renda tende a ser baixo, com a situação inversa também sendo verdadeira.

Frequências obtidas a partir tabela de contingência para a Regra 1.

Os valores das medidas confiança relativa ponderada, confiança negativa relativa ponderada, sensibilidade relativa ponderada e especificidade relativa ponderada são idênticos para cada regra individualmente.

Esta situação ocorre sempre, atingindo o objetivo estabelecido em definir uma medida unificada para avaliação de regras.

Nesse sentido, no experimento descrito nesta seção, foi considerada apenas a medida confiança relativa ponderada.

Tabela de contingência para a Regra 2.

Frequências obtidas a partir tabela de contingência para a Regra 2.

Síntese do exemplo de cálculo das medidas objetivas.

Uma maneira de incluir a subjetividade no processo de análise de regras é considerar o conhecimento prévio do domínio.

Assim, se propôs as seguintes medidas, Identificação de conformidade, permite identificar e classificar regras em conformidade com um conhecimento impreciso ou uma impressão geral fornecidos pelo usuário do domínio.

A conformidade é avaliada nos dois lados da regra pela expressão.

Número total de itens no antecedente e no consequente da regra descoberta

Número total de itens no antecedente e no consequente da regra descoberta que "casam" com o conjunto de itens especificado pelo usuário

Número de itens do conjunto especificado pelo usuário que "casam" com itens do conjunto de regras descobertas

Número de itens do conjunto especificado pelo usuário.

Total de elementos no antecedente e no consequente do conhecimento impreciso

Número de elementos no antecedente e no consequente da regra que "casam" com o conhecimento impreciso.

Nesta seção foram apresentadas medidas objetivas e subjetivas que avaliam o conhecimento extraído, sob diversos aspectos, permitindo identificar regras interessantes para o usuário.

Nesta seção são apresentadas a Análise de Pareto, utilizada para priorização de análises e de controle em diversas áreas (controle de estoque, gerenciamento de qualidade, controle de perdas), e a análise fatorial, técnica estatística de análise multivariada empregada para redução de variáveis.

Essas técnicas fundamentaram a metodologia para avaliação de regras.

Em todos os países e em todas as épocas, a distribuição de renda e bens segue o padrão logarítmico regular expresso na Equação 432.

Desde sua proposição, essa relação tem se mostrado consideravelmente estável (válid em estudos empíricos).

Dos trabalhos (que representou a distribuição de bens por meio de uma curva cumulativ e também a partir de estudos empíricos, se propôs o Princípio de Pareto (também conhecido como regra 80-20 ou lei dos poucos vitais e muitos triviais), generalizando para diversos domínios do conhecimento as descobertas.

Tratando de fenômenos naturais o princípio de Pareto expressa que, em várias situações, a maior parte das consequências do fenômeno (80%) é determinado por um pequeno número de causas (20%).

Adicionalmente, para os 20% de fatores causa, o princípio também se aplica, ou seja, nesse pequeno universo, 80% das consequências também derivam de 20% das causas e assim por diante.

Esse princípio é caso especial de um fenômeno mais amplo, a distribuição de Pareto.

A distribuição de Pareto (também conhecida como distribuição de Bradfor é uma distribuição estatística do tipo potência).

As distribuições desse tipo significam que a probabilidade de se atingir um certo tamanho é proporcional.

Os fenômenos que seguem a distribuição de Pareto, possuem a distribuição de probabilidades caracterizada pela Equação 433.

Seu comportamento gráfico é representado.

Formatos característicos da distribuição de Pareto.

Diferentemente das distribuições gaussianas, as distribuições do tipo potência não possuem uma escala"típica"e são usualmente referenciadas "livres de escala".

Por exemplo, a distribuição do tamanho de cidades (tipo potênci, inclui poucas metrópoles com tamanho muito superior ao tamanho médio das cidades).

Por outro lado a distribuição gaussiana, que rege, por exemplo, a distribuição de altura entre seres humanos, não permite que uma pessoa tenha altura muito superior à altura média.

Dos fenômenos que seguem esse tipo de distribuição, podem ser citados, a distribuição de bens individuais antes da criação da classe média, a distribuição de bens individuais após a criação da classe média, o consumo de água e a geração de esgoto, a distribuição de pessoas em cidades, os tamanhos de cidades, os defeitos em produtos, o tamanho de arquivos que trafegam pela internet e usam protocolo TCP, a distribuição do número de páginas em sites na web.

Baseando-se no Princípio de Pareto pode ser realizada a Análise de Pareto (também conhecida como Método AB, na qual os casos representativos do fenômeno em estudo são colocados em ordem crescente e é realizada a soma cumulativa de cada contribuição, gerando uma curva).

Dessa curva é possível dividir os casos em três grupos.

No primeiro estão presentes poucos casos (20%) mas que representam grande parte do fenômeno (80%).

No segundo grupo existe uma quantidade maior de casos que representam uma contribuição mediana para o fenômeno.

Finalmente no terceiro grupo, uma grande quantidade de casos é pouco representativa do fenômeno.

A partir do Princípio de Pareto foi elaborada a hipótese para filtragem de regras adotada neste trabalho.

Nesta seção é apresentada, em linhas gerais, a técnica de análise fatorial, uma abordagem estatística multivariada, que apóia a identificação de relações estruturais gerais no conjunto de dados.

Detalhes sobre a aplicação dessa técnica e interpretação dos resultados são apresentados.

Se mostra, detalhadamente, o conjunto de operações estatísticas e matemáticas relacionadas à técnica.

A principal vantagem do uso de técnicas de análise multivariada é a possibilidade de se considerar múltiplas variáveis na busca do entendimento de relações em algum fenômeno, o que não é possível com a utilização de métodos uni-variados e bi-variados.

O aumento do número de variáveis implica também em aumentar a possibilidade de que as variáveis não sejam todas não-correlacionadas e representantes de conceitos distintos.

Por outro lado, podem ser identificados grupos de variáveis inter-relacionadas que representam um conceito mais geral contido nos dados.

A análise fatorial é utilizada para analisar estas inter-relações e explicar as variáveis em termos de suas características em comum.

O objetivo geral desse tipo de análise é determinar uma maneira de condensar a informação representada pelas variáveis originais utilizando um conjunto menor de variáveis com mínimo de perda de informação.

A análise fatorial difere das técnicas de dependência (regressão múltipla, análise discriminante, análise multivariada de variância, ou correlação canônic, nas quais uma ou mais variáveis são consideradas explicitamente dependentes e as demais variáveis são consideradas independentes).

Na análise fatorial, os fatores são formados para maximizar a descrição de todo o conjunto de variáveis e não para predizer uma variável (ou variáveis) dependente.

Considerando uma analogia com as técnicas de dependência, cada variável observada (original) é uma variável dependente que é função de algum conjunto de fatores implícito e latente, que por sua vez, são constituídos por todas as outras variáveis.

Desse modo, cada variável é predita por todas as outras.

Em outra perspectiva, cada fator pode ser visto como uma variável dependente que é função de todo o conjunto de variáveis observadas.

Cabe diferenciar a análise fatorial de outro método estatístico multivariado, a clusterização.

Enquanto a análise fatorial permite criar grupos de variáveis a partir de casos existentes em um conjunto de dados, a clusterização permite criar grupos de casos.

Da análise fatorial, primeiramente são identificados aspectos implícitos representativos das relações estruturais (denominadas fatores) e depois é determinado o grau com que cada variável é descrita por cada fator.

Uma vez determinados os fatores e a descrição de cada variável os objetivos do uso da análise fatorial (sumarização e redução de dados) podem ser atingidos.

Na sumarização de dados, a análise fatorial permite identificar aspectos implícitos, mas não facilmente observados, que, quando interpretados e entendidos, descrevem os dados utilizando um número menor de conceitos em comparação com o número de variáveis originais.

A redução de dados pode ser alcançada por meio da identificação das variáveis mais representativas ou por meio da criação de um conjunto novo de variáveis para substituir parcial ou completamente o conjunto de variáveis originais.

Um exemplo de aplicação da análise fatorial é apresentado e representa a matriz de correlação para nove indicadores relacionados com elementos de avaliação em uma loja, dentre os quais, número de atendentes, preço e oferta de produtos.

Uma pergunta que poderia ser realizada é, "Há necessidade de medir e analisar esses indicadores separadamente ou é possível definir grupos mais abrangentes de indicadores ".

Analisando, apesar de ser observada a ocorrência de alta correlação entre algumas variáveis, visualmente é difícil identificar algum padrão de agrupamento.

A partir dos resultados da análise fatorial é possível agrupar as variáveis, viabilizando a identificação de alguns padrões.

Primeiro, quatro variáveis relacionadas às experiências dos compradores dentro da loja foram agrupadas.

Depois, três variáveis relacionadas à organização e à disponibilidade de produtos e por fim, duas variáveis relacionadas a qualidade e preço dos produtos foram agrupadas.

Exemplo do uso de análise fatorial para identificar uma estrutura em um grupo de variáveis.

No contexto deste trabalho, a análise fatorial é utilizada para agrupar medidas objetivas empregadas na avaliação de regras permitindo verificações adicionais na etapa de Pós-processamento de Regras de Associação.

Nesta seção são apresentados alguns ambientes e aplicativos que apóiam o processo de avaliação do conhecimento.

Inicialmente são apresentados alguns requisitos desejáveis em um ambiente de avaliação de regras.

Em seguida é descrito brevemente o aplicativo AIAS, que por meio do conhecimento prévio do domínio de aplicação e do uso de medidas subjetivas para avaliação de regras, busca identificar as regras mais interessantes a partir de uma coleção de Regras de Associação.

Na sequência são descritos os ambientes Discover e RulEE e o módulo computacional RulEE GAR, desenvolvidos por pesquisadores do LABIC para apoiar as etapas do processo de Extração de Conhecimento.

Os ambientes são bastante relacionados com o ambiente ARInE que é proposto neste trabalho e descrito.

O desenvolvimento de um Ambiente para Exploração de Regras, que auxilie os especialistas do domínio e usuários finais na compreensão e identificação do conhecimento interessante, é de grande relevância para o sucesso do processo de Extração de Conhecimento.

Além disso, ambiente desse tipo pode possibilitar que novas pesquisas relacionadas aos métodos para avaliação de regras e ao processo de Mineração de Dados como um todo, possam ser realizadas.

O desenvolvimento de um Ambiente para Exploração de Regras vem suprir essas deficiências por disponibilizar o conhecimento e facilitar o acesso aos usuários, além de agregar métodos de avaliação das regras em diferentes aspectos como precisão, compreensibilidade e interessabilidade.

Aspecto importante do processo de Extração de Conhecimento é a dificuldade de análise das regras extraídas já que se encontram em arquivos texto.

Pela utilização de ferramentas comerciais mais completas, algumas funcionalidades adicionais podem estar disponíveis, como as ferramentas para visualização dos modelos.

Entretanto, existem poucos métodos implementados para auxiliar a análise, identificação e utilização do conhecimento interessante.

A implementação de um Ambiente para Exploração de Regras implica no desenvolvimento de uma arquitetura que facilite o cálculo e armazenamento do valor das medidas de avaliação das regras, e a incorporação de novos métodos para cálculo de novas medidas.

E interessante que cada conjunto de regras disponibilizado apresente um conjunto de medidas tanto objetivas quanto subjetivas, de apoio à análise do usuário.

Vemos também que com o acúmulo desses dados sobre a avaliação fica disponível uma nova base de dados valiosa para os pesquisadores do processo de Extração de Conhecimento.

A partir desses dados, análises podem ser feitas sobre as medidas ou métodos de avaliação visando, dentre outras possibilidades, identificar medidas mais efetivas para avaliação e estabelecer relacionamentos entre as mesmas.

Somando-se a esses dados o feedback fornecido pelos usuários durante a consulta às regras disponibilizadas, algumas análises bastante interessantes podem ser feitas.

Dentre essas análises utilizando o feedback do usuário, destaca-se a possibilidade da análise das funcionalidades mais utilizadas com o objetivo de encontrar medidas de avaliação mais efetivas e verificar a existência de possíveis relações entre medidas objetivas e subjetivas.

Para auxiliar o usuário na compreensão e identificação do conhecimento interessante, algumas características desejáveis para um Ambiente para Exploração de Regras são, Interatividade e iteratividade. Como citado anteriormente, muitos pesquisadores do processo de Extração de Conhecimento de Bases de Dados têm concentrado suas pesquisas na fase de descoberta do conhecimento.

Entretanto, a inferência de padrões é somente parte do problema em se obter conhecimento util a partir de dados.

Além disso, o processo de Extração de Conhecimento de Bases de Dados é descrito de modo a enfatizar a importância da interação com o usuário e da iteração entre suas fases, sendo que a principal iteração do processo é a repetição das fases de Pré-processamento e a extração de padrões após a análise do conhecimento na etapa de Pós-processamento.

A qualidade do conhecimento extraído pode ser aprimorada em cada iteração ao reutilizar informações coletadas nas iterações anteriores.

No entanto, a iteratividade e interatividade do processo de Extração de Conhecimento também podem ser realizadas na fase de disponibilização do conhecimento.

A iteratividade dessa fase pode ser feita por meio da visualização do conjunto de regras, permitindo ao usuário escolher a cada momento um tipo de filtro ou medida para ordenação das regras e assim identificar as mais interessantes.

Caso não seja obtido sucesso, outros filtros podem ser escolhidos.

Nesse sentido, uma condição importante para um Ambiente para Exploração de Regras é permitir que o usuário possa explorar iterativamente e interativamente o conhecimento, especificando a cada instante o foco de interesse desejado.

Medidas de avaliação, Outro requisito importante para um Ambiente para Exploração de Regras é a disponibilização ao usuário de diversos métodos para poda e ordenação dos padrões.

Além disso, para seleção de regras, tanto as medidas objetivas quanto as subjetivas devem estar disponíveis.

As medidas objetivas podem ser consideradas como filtro inicial para seleção de regras, enquanto que medidas subjetivas podem ser utilizadas para focar o interesse do usuário em um conjunto de regras realmente interessante.

Na disponibilização, as medidas de avaliação podem ser utilizadas para, Poda do conjunto de regras, o usuário pode escolher alguma medida de avaliação para selecionar um subconjunto das regras apresentadas.

Para isso, define um limite de valor para a medida, apresentando somente as regras nas quais o valor da medida é superior ou inferior ao limite estipulado, ou utiliza uma medida que selecione somente regras interessantes segundo critério definido.

Ordenação, o usuário pode escolher medida para ordenar o conjunto de regras apresentado.

Visualização de modelos ou regras, Como em Mineração de Dados busca-se por padrões de comportamento contidos nos dados que não são previamente conhecidos pelos usuários, a compreensão dos modelos encontrados pode ser difícil.

Dessa maneira, a compreensão dos modelos é um aspecto que precisa ser bastante pesquisado.

Uma das maneiras utilizadas para auxiliar os usuários na compreensão e interpretação do conhecimento é a utilização de ambientes em que os modelos são visualizados pelo usuário de maneira adequada.

Diversas ferramentas de Mineração de Dados possuem módulos de visualização de modelos e de dados bastante completos e uteis.

Entretanto, nessas ferramentas a visualização é realizada somente nas estações em que a ferramenta está instalada.

Um aspecto adicional de um Ambiente para Exploração de Regras é a possibilidade de visualização do conhecimento por meio de uma interface baseada na Web.

Algumas vantagens obtidas pela visualização do conhecimento pela Web são a independência de plataforma, facilidade de uso e, especialmente, a acessibilidade, que permite a visualização do conhecimento pelos usuários do processo, independente de sua localização física.

Durante a visualização de um modelo, é importante que o usuário possa interagir com este.

Isso significa permitir que o usuário possa selecionar diferentes valores de atributos e verificar os resultados produzidos.

Esta interação possibilita a compreensão pelo usuário de situações de causa-efeito, ou seja, o usuário pode entender quanto cada atributo do conjunto de dados afeta o modelo final.

A visualização interativa dos modelos deve permitir que usuários com pouco conhecimento em Aprendizado de Máquina e Estatística sejam capazes de compreender fatores que influenciam as soluções e que também possam visualizar o modelo completo para compreenderem como os exemplos são tratados.

Integração, Outro modo de auxiliar o usuário na compreensão do conhecimento é fornecer-lhe um contexto, como quando se mostra a relação existente entre o conhecimento extraído e o conjunto de dados a partir do qual foi gerado.

Isto é chamado integração.

Um tipo de integração é a possibilidade de se selecionar parte de um modelo e obter acesso aos dados originais a partir dos quais essa parte do modelo foi gerada.

Ambientes colaborativos, Outro requisito importante em um Ambiente para Exploração de Regras é que este seja um ambiente do tipo colaborativo, permitindo a interação e integração entre os diversos usuários do ambiente.

No caso de um Ambiente para Exploração de Regras, o primeiro passo para fornecer um suporte eficiente a múltiplos usuários é armazenar as seções dos usuários em um sistema de logs.

O sistema de logs deve armazenar os trabalhos dos usuários, permitindo a repetição e análise por esses ou outros usuários dos trabalhos realizados.

Isto permite ganho geral de produtividade entre os usuários do ambiente, além de possibilitar que as informações contidas nos logs sejam utilizadas pelos analistas do processo de Extração de Conhecimento para auxiliar na obtenção de melhores resultados em uma próxima iteração do processo.

A utilização da Web como interface para disponibilização de regras facilita também a implementação de ambiente para trabalho cooperativo dos usuários do processo de Extração de Conhecimento.

Dessa maneira, os diversos usuários do processo podem analisar e compreender o conhecimento individualmente e fornecer e compartilhar feedback para o ambiente por meio de comentários, idéias, dicas ou sugestões.

Além disso, com esse tipo de interface, os diversos usuários finais e especialistas do domínio podem analisar os conjuntos de regras em suas estações de trabalho sem a necessidade de instalar ferramentas específicas de Mineração de Dados.

A partir das informações contidas nos logs de utilização dos usuários e de seu feed-back, análises podem ser feitas, dentre as quais, os especialistas do processo de Extração de Conhecimento podem verificar o que é de interesse do usuário, fornecendo novos subsídios para apoiar uma próxima iteração de todo o processo e trazer resultados mais interessantes para o usuário final.

As funcionalidades mais utilizadas podem ser analisadas com o objetivo de se encontrar medidas de avaliação mais efetivas.

A existência de alguma relação entre medidas objetivas e subjetivas pode ser verificada a partir das consultas realizadas pelos usuários.

O aplicativo Association Interestingness Analysis System (AIAS), permite identificar regras interessantes, em especial regras inesperadas, a partir de um conjunto de Regras de Associação.

Para tanto, o AIAS emprega o conhecimento existente do domínio para ordenar as regras segundo diferentes critérios de interessabilidade e diversos tipos de inesperabilidade.

Para análise das regras é iniciado o procedimento a partir da definição de um arquivo de análise de interessabilidade contendo, arquivo de regras, especificação da taxonomia do conhecimento, definição do conhecimento impreciso, definição de impressões gerais.

Para a elaboração dos três ultimos itens apresentados é necessário conhecimento do especialista.

Os itens são detalhados na sequência, Taxonomia do conhecimento, O conceito de taxonomia empregado é similar ao apresentado.

Aqui são especificadas categorias e níveis de abstração para os atributos.

Adicionalmente, pode-se criar uma hierarquia de classes para os mesmos.

Conhecimento impreciso (CI), O conhecimento impreciso possibilita informar um conhecimento que o especialista supõe ser verdadeiro.

E expresso na forma, Impressões gerais (IG), As impressões gerais possibilitam informar uma relação que o especialista acredita existir entre os atributos especificados.

IG é expresso na forma.

Durante o processo de avaliação, o aplicativo realiza quatro tipos distintos de análises para um dado conhecimento impreciso ou uma impressão geral, a saber, Identificação de Conformidade, Consequente Inesperado, Antecedente Inesperado, Antecedente e Consequente Inesperados.

Após a análise das regras utilizando-se as medidas propostas, a ferramenta possibilita a visualização de diferentes aspectos das regras potencialmente interessantes, Tipos de interesse, as regras são categorizadas conforme o tipo de interesse, propiciando ao usuário a análise do conhecimento sob diferentes aspectos de interessabilidade.

Graus de interesse, as regras são separadas pelo grau de interesse (regras com maior grau de conformidade ou as mais inesperadas, por exemplo).

Atributos interessantes, a apresentação dos atributos interessantes pode ser considerada mais produtiva que a apresentação da regra como um todo.

É ilustrada a combinação de aspectos visuais com o uso de medidas.

No círculo central é informado o tipo de conhecimento prévio considerado (Conhecimento Impreciso ou Impressões Gerais).

Nos quadrantes ao redor do círculo são apresentados os tipos de análise, Identificação de Conformidade no quadrante inferior

Antecedente Inesperado no quadrante esquerdo

Consequente Inesperado no quadrante direito e Antecedente e Consequente Inesperado no quadrante superior.

Cada quadrante é subdividido em faixas de valores das respectivas medidas subjetivas.

O número de regras que satisfaz cada faixa de valor é indicado sobre a linha que liga o círculo central aos retângulos laterais.

Adicionalmente são apresentados os atributos que estão ou não em conformidade com o tipo de conhecimento em análise.

Tela do aplicativo AIAS.

Selecionando-se os elementos na tela, informações detalhadas são apresentadas.

Por exemplo, clicando-se sobre o elemento correspondente à conformidade na faixa de 05 são apresentados o conjunto de regras que satisfazem esta condição e os valores associados de conformidade, suporte e confiança.

O aplicativo possibilita a identificação de subconjuntos de regras com características específicas, permite maior facilidade na exploração interativa do conhecimento e, dentre outras facilidades, proporciona uma visão global do conhecimento extraído.

A presença do especialista do domínio no processo de análise é fundamental, tanto no fornecimento do conhecimento prévio (taxonomia, conhecimento impreciso e impressões gerais) como na análise dos resultados apresentados pelo aplicativo.

O Ambiente Discover foi proposto com o objetivo de fornecer um ambiente integrado para apoiar as etapas do processo de Extração de Conhecimento de Dados e Textos.

Nesse ambiente são utilizados algoritmos de Aprendizado de Máquina implementados pela comunidade científica, bem como módulos com finalidades específicas desenvolvidos pelos pesquisadores do LABIC.

Entre as funcionalidades desses módulos estão, pré-processamento de dados, pré-processamento de textos e pós-processamento de conhecimento.

As principais funcionalidades do Ambiente Discover são ilustradas.

Funcionalidades do Ambiente Discover.

O Ambiente Discover é capaz de acessar os dados na forma atributo-valor ou no formato texto, por meio de métodos para Pré-processamento de dados e de textos, desenvolvidos por pesquisadores do LABIC.

Caso a quantidade de dados exceda a capacidade dos algoritmos de extração de padrões, amostras representativas devem ser geradas para serem fornecidas aos algoritmos.

Os resultados dos algoritmos de extração de padrões ou modelos podem ser avaliados por meio de diversas medidas, entre elas precisão, compreensibilidade e grau de interesse.

No início, o projeto Discover consistia apenas em um repositório de scripts a serem implementados na linguagem PERL, Practical Extraction and Report Language, e posteriormente reutilizados de modo a facilitar a execução de experimentos.

A opção por essa linguagem foi determinada em função das vantagens e fatores relacionados, permite desenvolver códigos de maneira rápida, fornece um mecanismo poderoso para trabalhar com expressões regulares, muito utilizadas nessas implementações, permite a manipulação de grande quantidade de dados de maneira eficiente e é bastante flexível com relação aos tipos de dados, facilita a escrita de protótipos e scripts, bem como funções genéricas e pacotes.

O CPAN fornece um excelente repositório de código reutilizável dessa linguagem.

O projeto Discover, a priori, consistia em um conjunto de ferramentas independentes que poderiam ser desenvolvidas conforme as necessidades dos usuários, e reunidas em um repositório compartilhado de tal forma que os usuários pudessem combinar ferramentas para executar experimentos de maior complexidade.

Posteriormente, foi proposto criar um ambiente integrado, no qual os scripts seriam substituídos por bibliotecas de classes empacotadas como componentes.

A partir daí, os programas têm sido desenvolvidos como componentes.

O Ambiente Discover possibilita a realização de experimentos por meio da composição desses componentes.

No entanto, para tornar o Discover um ambiente integrado, sua implementação tornou-se mais complexa.

Surgiu a necessidade de maior comunicação entre os participantes e de interação entre os diferentes componentes.

Com relação à arquitetura do ambiente, é proposto framework para a integração dos componentes do Ambiente Discover baseado em software patterns, no qual os componentes são integrados por meio de linguagem baseada em XML.

Além disso, a interface gráfica responsável pela composição dos componentes deveria ser implementada de acordo com a arquitetura proposta para o ambiente.

Scripts são pequenos programas que realizam tarefas atômicas.

Por meio de interfaces bem definidas entre os módulos e procedimentos para conversão de dados e de conhecimento para as sintaxes padrão (principal forma de comunicação entre as funcionalidades) são utilizados no Discover algoritmos já desenvolvidos, testados e consolidados pela comunidade científica, evitando dessa forma sua reimplementação e garantindo maior confiabilidade dos experimentos realizados.

Define-se a sintaxe padrão para representação de dados como uma extensão do formato utilizado como entrada do algoritmo C45.

No Ambiente Discover, a extensão foi realizada para adequação do formato dos arquivos de dados.

A biblioteca, de nome Dataset foi implementada para manipulação do conjunto de dados na sintaxe padrão.

Esta biblioteca apresenta métodos para conversão de dados na sintaxe padrão para os formatos de entrada dos algoritmos C45, C45 rules, C50, Cubist, CN2, Newid, RT, RETIS, CART, WEKA, SNNS e SVMTorch.

No Ambiente Discover foi desenvolvida sintaxe padrão para representação de Regras de Classificação, bem como scripts para conversão de regras ou árvores de decisão extraídas por algoritmos de Aprendizado de Máquina para esse formato, além de scripts para cálculo da tabela de contingência para cada regra.

Há scripts implementados que convertem a saída dos algoritmos ID3, C45, C45 rules, C50/See5, CN2, OC1, Ripper, Te MCpara o formato padrão de Regras de Classificação.

Para o cálculo dos valores da tabela de contingência e geração dos arquivos no formato padrão estendido foram desenvolvidos scripts.

Foi desenvolvida também sintaxe padrão para representação de Regras de Regressão adequada para englobar o tipo de representação da hipótese induzida por vários algoritmos de regressão.

Além da definição da sintaxe para Regras de Regressão, foram desenvolvidos scripts para conversão dos conjuntos de exemplos da sintaxe padrão do Discover para a sintaxe dos algoritmos M5, RT, Cubist, CART e RETIS e scripts para conversão da saída desses algoritmos para a sintaxe padrão de regressão.

De modo geral, o projeto Discover pode ser entendido como um conjunto de métodos aplicados sobre os dados ou sobre o conhecimento extraído a partir dos dados.

Dessa maneira, é importante que o projeto Discover ofereça uma base sólida para manipulação de dados e conhecimento.

Essa base é composta por sintaxes padrões utilizadas para representação de dados e conhecimento, e ainda por bibliotecas que oferecem um conjunto de funcionalidades básicas.

Dentre as diversas ferramentas e sistemas implementados para o projeto Discover podem ser citados, o sistema XRULER, no qual um conjunto de exemplos é dividido em amostras para treinamento e teste.

As amostras de treinamento são submetidas a diferentes indutores e os classificadores obtidos são convertidos para o formato padrão de regras de decisão.

Daí, a partir do subconjunto de regras que cobrem os exemplos obtém-se o classificador simbólico final, a biblioteca de classes Discover Object Library, DOL, que implementa a manipulação de dados na sintaxe padrão.

Essa biblioteca apresenta métodos para conversão de dados na sintaxe padrão para os formatos de entrada de diversos sistemas de Aprendizado de Máquina.

Além disso, a biblioteca fornece vários métodos para pré-processar os dados, entre os quais podem ser citados, cálculo de estatísticas descritivas básicas, embaralhamento, amostragem aleatória e complementar.

Uma biblioteca que transforma os Classificadores Simbólicos (induzidos pelos algoritmos ID3, C45, C45 rules, C50/See5, CN2, OC1, Ripper, Te MC4) para o formato padrão de regras de decisão do Discover.

Uma biblioteca que transforma os Regressores Simbólicos (induzidos pelos algoritmos M5, RT, Cubist, CART e RETIS) para o formato padrão de Regras de Regressão do Discover.

O Ambiente Computacional SNIFFER, que gerencia avaliações e comparações experimentais de algoritmos de Aprendizado de Máquina.

Entre suas principais funcionalidades podem ser citadas, gerenciamento de sintaxes dos sistemas de aprendizado, aplicação de métodos de resampling para estimar a taxa de erro verdadeira, cálculo e comparação de medidas de desempenho, a ferramenta PreTexT realiza o Pré-processamento de uma coleção de documentos utilizando a abordagem bag-of-words.

Entre as principais funcionalidades da ferramenta, podem ser citadas, transformação de palavras em stems, criação da tabela atributo-valor no formato padrão do Discover, redução da dimensionalidade do conjunto de atributos, utilização de indução construtiva na criação de novos atributos na tabela atributo-valor.

O Ambiente Computacional DiPER auxilia na avaliação de um modelo de regressão simbólico.

Entre as suas principais funcionalidades, podem ser citadas, conversão dos regressores gerados por diversos algoritmos de regressão para o formato padrão de regras de regressão do Discover, tratamento semântico do conjunto de regras no formato padrão, cálculo de medidas de precisão e compreensibilidade para as regras de regressão.

Construção de uma tabela de contingência para regras de regressão, realização de teste de hipóteses, comparando regressores induzidos por diferentes algoritmos de regressão sobre um mesmo conjunto de dados.

A partir dos diversos trabalhos e parcerias realizadas pelo grupo de pesquisadores do LABIC, surgiu a necessidade de se desenvolver um ambiente para o Pós-processamento de Conhecimento, que auxilie os usuários do processo de Mineração de Dados na compreensão e identificação de conhecimento interessante, de maneira colaborativa, além de possibilitar o acesso remoto a esse conhecimento.

Assim, para atender essas necessidades, foi projetado e implementado o Ambiente RulEE descrito na próxima seção.

Um Ambiente para Exploração de Regras que viabilize tanto a análise quanto a disponibilização de regras é essencial para a realização do Pós-processamento do conhecimento obtido no processo de Mineração de Dados.

Com o intuito de mostrar a viabilidade desse tipo de ambiente, foi desenvolvido o Ambiente RulEE (Rule Exploration Environment).

O RulEE atende alguns dos requisitos apresentados na Seção 461 e fornece as funcionalidades necessárias para mostrar a vialibilidade de um ambiente de Pós-processamento como o desejado.

O Ambiente RulEE suporta análise de Regras de Classificação, Regressão e Associação.

O ambiente tem como entrada arquivos com conjuntos de regras representados utilizando, para cada uma das tarefas de Mineração de Dados mencionadas, a sintaxe padrão definida no Projeto Discover.

Por utilizar esses formatos de regras e pela utilização dos scripts de conversão dos diversos tipos de regras para a sintaxe padrão, também implementados no Discover, podem ser inseridas no ambiente as saídas de uma grande variedade de algoritmos para Extração de Conhecimento de Dados.

Após a execução dos algoritmos, seus resultados são convertidos para a sintaxe padrão do tipo de problema tratado (classificação, regressão ou associação).

A partir das regras expressas na sintaxe padrão e de um conjunto de dados para teste, devem ser utilizados os scripts para cálculo dos valores da tabela de contingência elaborados.

Os resultados desses scripts são utilizados como entrada no RulEE, não sendo portanto necessário o cálculo dessas informações internamente pelo ambiente.

O RulEE disponibiliza um conjunto de medidas para avaliação de regras para classificação, regressão e associação, permitindo que diversos tipos de análise sejam realizadas.

A partir de um conjunto de regras inserido no ambiente, os valores das diversas medidas são calculados e ficam disponíveis para os usuários.

O ambiente provê fácil acesso às regras e aos valores dessas medidas e permite que as medidas sejam utilizadas para a ordenação e seleção de conjuntos de regras.

O Ambiente RulEE apresenta arquitetura para facilitar o armazenamento e a disponibilização dos valores das medidas desenvolvidas permitindo a fácil utilização do conjunto de medidas existentes.

O cálculo do valor de novas medidas deve ser realizado com procedimentos desenvolvidos pelos próprios pesquisadores.

O RulEE disponibiliza dois modos diferentes para utilização.

O primeiro modo apresenta uma biblioteca de classes que permite acesso às diversas funções implementadas, como inserção, recuperação de regras e de valores de medidas, bem como acesso aos dados disponíveis na base de dados do ambiente.

A utilização da biblioteca é realizada por meio de programas escritos na linguagem PERL, sendo voltada para usuários analistas do processo de Mineração de Dados que necessitam recuperar informações em seus scripts para Pós-processamento ou mesmo para definir alguma função desejada que não esteja implementada no ambiente.

O segundo modo de interação utiliza a interface Web.

Essa interface possui a função principal de facilitar o acesso às informações armazenadas no ambiente e apresenta diversas funções para acesso às regras e aos valores de medidas, além de facilitar a inserção de novos conjuntos de dados.

A interface é voltada para usuários não especialistas ou usuários que não necessitam da flexibilidade da utilização da biblioteca de classes.

O objetivo principal no uso dessa interface é facilitar o acesso e a apresentação das regras descobertas a partir de uma interface mais intuitiva, na qual os usuários finais ou especialistas do domínio possam acessá-la sem a necessidade da leitura de arquivos texto, facilitando a interação destes usuários com o conhecimento extraído.

Como mencionado anteriormente, o Ambiente RulEE foi implementado buscando atender as especificações de um Ambiente para Exploração de Regras genérico.

Sendo um primeiro trabalho desenvolvido, foi implementada a arquitetura básica do ambiente possibilitando, assim, que características complementares pudessem ser implementadas em trabalhos futuros.

Dessa maneira, as principais características ou requisitos implementados no Ambiente RulEE são disponibilização das medidas objetivas para avaliação de regras descritas no framework e a medida RI, implementação de consultas para recuperação dos conjuntos de regras e dos respectivos valores de medidas.

Implementação de uma interface simples baseada na Web, armazenamento de valores inseridos por usuários como indicativos da interessabilidade, compreensibilidade, utilidade e novidade das regras, além de comentários anexos as regras, conjuntos de regras e consultas.

Considerando que o Ambiente ARInE proposto nesta tese, utiliza-se de componentes do Ambiente RulEE são apresentados, na sequência, alguns detalhes da implementação desse ambiente.

A implementação do Ambiente RulEE é baseada em três elementos, Biblioteca de classes, Na qual são implementadas as funções disponíveis no ambiente.

Base de dados, Na qual são armazenados o conhecimento no formato de regras, as medidas de avaliação de regras, os dados referentes aos usuários e os dados utilizados na extração e avaliação de conhecimento.

Interface, Baseada na Web visando facilitar o acesso aos conjuntos de regras.

A arquitetura geral do Ambiente RulEE é apresentada.

No contexto do RulEE, o processo de análise e disponibilização de regras está baseado na idéia de se utilizar um Repositório de Regras e Medidas, entidade principal do ambiente.

No Repositório são armazenados os conjuntos de regras, os valores das medidas e as consultas realizadas pelos usuários sobre o repositório de regras e medidas.

Arquitetura geral do Ambiente RulEE.

A disponibilização das regras deve ser iniciada pela inserção no Repositório de Regras dos conjuntos de regras representados na sintaxe padrão.

Esta atividade é realizada a partir dos métodos existentes no Módulo de Gerenciamento.

Quando disponíveis no Repositório, as regras podem ser acessadas pela interface Web ou pela biblioteca de classes, podendo ser analisadas pelos usuários e utilizadas em algum processo de tomada de decisão.

Os elementos da figura que estão no retângulo pontilhado correspondem ao Ambiente RulEE.

A descrição detalhada dos elementos representados é relacionada na sequência, Módulos de processamento, São os programas escritos para processamento de dados, regras e informações dos usuários.

Seguindo o padrão de desenvolvimento do Ambiente Discover, esses programas foram implementados na linguagem PERL.

Os módulos de processamento encontrados no ambiente são, Módulo de gerenciamento, responsável pelo gerenciamento das regras disponibilizadas no ambiente.

Este módulo recebe requisições da Interface do ambiente, busca as regras correspondentes no Repositório de Regras, ativa os módulos de Pós-processamento ou recupera algum conjunto de dados no Repositório de Dados para então retornar o resultado da requisição para a Interface.

Este módulo permite obter ou calcular os valores de medidas para as regras e está implementado como uma biblioteca de classes.

Módulos de pós-processamento, são responsáveis pelo cálculo das medidas de avaliação das regras disponibilizadas.

Esses módulos contêm procedimentos para cálculo das medidas e apresentam métodos distintos para tratamento de Regras de Classificação, Associação e Regressão.

E importante destacar que no Ambiente RulEE são utilizados os módulos de Pós-processamento implementados no Projeto Discover, por exemplo, o módulo computacional RulEE GAR proposto e o ambiente ARInE proposto neste trabalho.

Interface, Por meio da Interface o usuário pode acessar todas as informações disponibilizadas no Ambiente RulEE.

A Interface é baseada na Web a fim de aproveitar algumas das suas vantagens, como a independência de plataforma e, especialmente, a facilidade de utilização pelos usuários do processo, independente de sua localização física.

Repositórios, Correspondem à base de dados desenvolvida utilizando o gerenciador MySql, ou aos arquivos nos quais são armazenadas as informações disponibilizadas.

No ambiente são encontrados três tipos de repositórios, Repositório de Informações de Usuário, conjunto de tabelas MySql nas quais são armazenadas as informações relacionadas aos usuários do ambiente.

Algumas informações armazenadas são, cadastro de usuários, comentários e votações em regras.

Repositório de Regras e Medidas, conjunto de tabelas para armazenamento das regras e dos valores das medidas a serem disponibilizadas no ambiente.

Um aspecto importante foi a definição do modelo deste repositório, para facilitar o armazenamento e a recuperação das regras, o cadastro e inserção de novas medidas para avaliação e o cálculo e recuperação dos valores das medidas para as regras.

Repositório de Dados, arquivos nos quais são armazenados os dados referentes aos conjuntos de regras disponibilizados.

Estes dados podem ser utilizados para auxiliar o usuário na compreensão das regras, assim como podem ser utilizados por algum módulo de Pós-processamento.

Para facilitar a utilização dos métodos de Pós-processamento implementados no Ambiente Discover, estes arquivos de dados devem seguir a sintaxe padrão para representação de dados.

Estes arquivos são armazenados em um subdiretório do ambiente, sendo referenciados em sua base de dados.

Conjuntos de Regras, Representam as regras extraídas e padronizadas para suas respectivas sintaxes padrão.

Como citado anteriormente, o Ambiente RulEE trata regras expressas no formato padrão, permitindo que o resultado de uma variedade de algoritmos possa ser manipulado pela utilização das bibliotecas de conversão.

Nesta seção é apresentado o módulo computacional RulEE GAR, que objetiva fornecer funcionalidades para generalizar Regras de Associação e também para analisar as regras generalizadas.

A generalização é realizada utilizando o algoritmo IGART (Interactive Generalization of Association Rules using Taxonomy, Generalização Interativa de Regras de Associação usando Taxonomi).

Para prover as funcionalidades de análise de Regras de Associação generalizadas, o módulo utiliza a base de dados e a biblioteca de classes desenvolvidas para o Ambiente de Exploração de Regras RulEE.

O módulo computacional RulEE GAR foi desenvolvido como Módulo de Pós-processamento do Ambiente RulEE.

O processo de generalização de Regras de Associação de acordo com o algoritmo IGART é apresentado.

Processo para generalização de Regras de Associação no RulEE GAR.

O processo generaliza apenas um dos lados (LHS ou RHS) das Regras de Associação.

Inicialmente as regras (representadas na sintaxe padrão são agrupadas em subconjuntos que apresentam antecedente ou consequente semelhante).

Se o processo for utilizado para generalizar o lado esquerdo das regras (LHS), os subconjuntos são gerados utilizando consequentes (RHS) semelhantes.

Se o processo for utilizado para generalizar o lado direito das regras (RHS), os subconjuntos são gerados utilizando antecedentes (LHS) semelhantes.

Processo de generalização do lado esquerdo das regras, por consequência, os subconjuntos são agrupados utilizando semelhanças no lado direito das regras.

Em seguida são utilizadas as taxonomias para generalizar cada subconjunto.

Ao final da generalização, os itens de cada regra são ordenados lexicograficamente e a regra é armazenada em um conjunto de Regras de Associação generalizadas no repositório de regras do RulEE Concluído esse processo é calculada a tabela de contingência para cada Regra de Associação generalizada, uma vez que a sintaxe padrão inclui a regra acompanhada por sua tabela de contingência.

O processo de generalização é finalizado com o cálculo da tabela de contingência.

Neste capítulo foi apresentada a variedade de medidas que podem ser empregadas para avaliação do conhecimento extraído por Regras de Associação.

Foram apresentados princípios de interessabilidade de regras, que estabelecem diretrizes para identificar se uma regra é interessante ou não.

Foram descritas medidas (objetivas e subjetivas) para avaliação de regras e apresentadas as técnicas de Análise de Pareto e análise fatorial, que constituem a base para a metodologia de análise de conhecimento proposta neste trabalho.

Também foi apresentado, resumidamente, o aplicativo AIAS que incorpora considerações de caráter subjetivo no processo de análise e avaliação do conhecimento extraído.

Foi realizada descrição do projeto Discover e do ambiente RulEE, desenvolvidos no LABIC e com os quais o ambiente ARInE está integrado.

A abordagem utilizada neste trabalho prevê a utilização de medidas objetivas não apenas para ordenar, mas para filtrar regras.

Nesse caso a principal dificuldade é estabelecer um valor "corte" para cada medida.

Visando estabelecer um método para definir o valor de "corte", foram realizados experimentos nos quais se calculou para um conjunto de regras, o valor de algumas das medidas objetivas.

Então foi elaborado um gráfico com os valores destas medidas para cada regra, ordenado respectivamente pelas medidas confiança e suporte.

O gráfico obtido é apresentado.

Observa-se que algumas medidas, como confiança negativa, confiança relativa, sensibilidade, cobertura, apresentam comportamento semelhante e outras possuem valores alternados em relação às demais confiança esperada e confiança relativa.

Algumas medidas ( confiança e satisfação, por exemplo) apresentam valor máximo em cerca de 50% das regras, outras apresentam valores reduzidos na grande maioria das regras e valores elevados em pequeno número de regras (suporte e lift, dentre outras).

Gráfico que apresenta relação entre medidas para um conjunto genérico de regras.

Da análise realizada estabeleceu-se a hipótese, medidas com valores semelhantes em muitas regras e valores diferenciados em poucas regras podem seguir o Princípio de Pareto.

Deste princípio é definido método para estabelecer o ponto de "corte" de uma medida.

As medidas que não satisfazem essa condição podem ser empregadas como "pré-filtros" ou como medidas complementares.

Nesta seção é realizada análise detalhada do comportamento gráfico de medidas, página 62, e é verificado seu desempenho como filtro para seleção de regras.

Inicialmente é definida hipótese para o comportamento gráfico esperado.

São realizados experimentos para analisar a variabilidade do comportamento gráfico em relação a diferentes bases de dados e parâmetros do processo de Mineração de Dados.

Finalmente é verificado o desempenho das medidas como filtros para regras, utilizadas de forma isolada ou combinadas a outras medidas.

A partir do gráfico apresentado e com o auxílio de uma planilha eletrônica, foram isolados os gráficos de cada medida e ordenados os valores das medidas de maneira decrescente.

A partir dos gráficos resultantes foi possível identificar quatro formatos característicos de curvas, decréscimo inicial acentuado, decaimento linear, decaimento após patamar e sigmoidal.

Esses formatos característicos são ilustrados.

O formato de curva com decréscimo inicial acentuado, permite identificar que para um pequeno número de regras ocorre uma grande variação no intervalo de valores da medida.

Os demais formatos característicos não permitem essa identificação.

O formato sigmoidal possui viabilidade de uso como filtro quando considerado em suas regiões extremas.

Vale lembrar que medidas com comportamento gráfico do tipo decréscimo inicial acentuado seguem o Princípio de Pareto, e dessa maneira têm maior capacidade restritiva quando utilizadas como filtros para conjuntos de regras.

Nesta seção são descritos os experimentos realizados para verificação do comportamento gráfico das medidas objetivas.

A análise é realizada considerando-se a intensidade restritiva da medida para uso como filtro e, também, a manutenção associada a seu comportamento gráfico para diferentes bases de dados e parâmetros dos algoritmos de Mineração de Dados.

Destaca-se que o objetivo principal destes experimentos é identificar o padrão geral de comportamento gráfico para cada medida objetiva analisada, considerando-se conjuntos de regras com características diversas.

Outro aspecto a ser destacado é que a classificação dos gráficos, em conformidade com os quatro formatos característicos, tem forte componente subjetivo, uma vez que os gráficos são produzidos a partir de dados experimentais e não de uma expressão matemática.

A metodologia utilizada para realização dos experimentos seguiu as etapas do processo de Mineração de Dados e é ilustrada.

Foram realizados experimentos com dados artificiais e reais.

Na etapa de obtenção dos dados foram selecionadas 10 bases de dados (8 artificiais e reais).

Utilizou-se o aplicativo Gen para gerar as bases de dados artificiais.

As bases de dados reais BMS-POS e QVU são descritas na Seção 533.

O uso de bases de dados artificiais é justificado pela possibilidade de garantir a existência diversidade de características entre as bases de dados.

Na etapa de Pré-processamento de dados foi utilizado exclusivamente o script de conversão (TidSet) para disponibilizar os dados no formato do aplicativo Apriori.

Na etapa de Extração de Padrões foram utilizados os aplicativos Apriori e MineSet TM para geração das Regras de Associação para cada base de dados.

No Pós-processamento de dados os conjuntos de regras gerados foram convertidos para a sintaxe padrão de Regras de Associação.

Para cada arquivo de regras no formato padrão foram calculados os valores das medidas utilizando-se o script PERL TcMedidas.

Metodologia utilizada nos experimentos.

Utilizando software específico para criação de gráficos de funções, são criados os gráficos para cada medida objetiva e cada base de dados.

Uma descrição dos aplicativos, das bases de dados e os resultados serão apresentados nas próximas seções.

Para a realização desses experimentos foram utilizados o aplicativo Gen, na geração dos dados artificiais, e os aplicativos Apriori e MineSetTM para gerar as regras.

O aplicativo Gen produz bases artificiais de dados sintéticos, de acordo com parâmetros definidos pelo usuário.

O aplicativo está disponível e tem sido amplamente utilizado em testes de algoritmos de Regras de Associação.

As funcionalidades do aplicativo Gen são agrupadas nas categorias abaixo relacionadas, Tipos de bases, bases de transações, bases de transações com taxonomias e bases de padrões sequenciais.

Formato do arquivo de saída, formato binário ou ASC.

O formato binário.

O formato ASC.

Parâmetros de geração das bases, o aplicativo permite definir características com as quais a base será gerada.

Para as bases de transações estão disponíveis as opções, número de transações, número médio de itens por transação, número de itens distintos, número de padrões, tamanho médio máximo dos padrões, correlação entre os padrões, confiança média de uma regra, nome dos arquivos de saída, somente utilizada na geração de números aleatórios.

Além do arquivo de dados, é gerado automaticamente um arquivo de descrição contendo a descrição dos dados, uma lista de itens com alta probabilidade de estarem presentes em um itemset e uma lista com itemsets com grande probabilidade de estar em uma transação.

O aplicativo Apriori é uma implementação, do tradicional algoritmo de Regras de Associação.

O aplicativo foi desenvolvido na filosofia GNU, sendo utilizado nos experimentos a versão 408.

Como características principais desta implementação do Apriori, podem ser destacadas, Suporte, utiliza definição diferente da original, contudo é possível a utilização da definição original de Suporte, adicionando-se a opção na linha de comando

Métricas, permite utilizar métricas complementares na geração de regras.

As métricas são ativadas pela opção, e da linha de comando, seguida pelo número da métrica.

Adicionalmente é necessário especificar o valor mínimo da métrica, dado pela opção, seguido do referido valor.

As métricas disponíveis são, Diferença absoluta de confiança à priori, diferença entre a confiança da regra TRUE X e a confiança da regra Y X

Diferença do quociente de confiança em relação à 1, diferença entre o valor 1 e a razão entre a confiança da regra TRUE X e a confiança da regra Y X

Diferença absoluta do ganho de valor em relação à 1, semelhante à métrica anterior quando a confiança da regra Y X é menor que a da regra TRUE X.

Diferença de informação à priori, representa o ganho de informação.

Medida Chi normalizada, bastante conhecida na estatística, é frequentemente empregada para mensurar a diferença entre uma distribuição supostamente independente entre duas variáveis discretas e a distribuição conjunta real, para determinar a intensidade de interdependência das variáveis entre si.

Tipos de saída, permite gerar além das regras apenas os itemsets e Hyperedges (conjuntos de itens que são fortemente preditivos uns em relação aos outros).

Restrição de atributos, é possível especificar, por meio de um arquivo, uma lista dos itens que estarão presentes nas regras, seja no antecedente, no consequente ou em ambos.

Tipos de atributos, trata somente atributos discretos

TidSet, utilitário para conversão de formatos de dados.

Transforma o formato por linha, em uma transação por linha no formato item1, item2, itemN.

O MineSet TM da Silicon Graphics é um aplicativo que disponibiliza várias ferramentas para Mineração de Dados, dentre as quais uma específica para Regras de Associação.

As principais características deste aplicativo, referentes a Regras de Associação, são, Métricas, além das métricas Suporte e confiança, gera também as métricas confiança Esperada e Lift.

Geração de regras, permite controlar o número de atributos por regra, sem limites de atributos, um atributo no antecedente e outro no consequente (default), outro número de atributos especificado pelo usuário.

Formato de saída, salva as regras geradas em diversos formatos, incluindo MineSet.

Visualização de regras, possui módulo específico para visualização de regras, no qual é possível aplicar uma série de filtros sobre os valores das métricas de avaliação e sobre os atributos presentes.

Para as regras geradas considerando-se apenas um atributo no antecedente e outro no consequente, existe uma ferramenta de visualização gráfica.

Atributos contínuos, discretiza automaticamente atributos contínuos.

Neste experimento foram utilizadas dez bases de dados sobre domínios variados e com características distintas.

Dessas bases, duas incluem dados reais sobre Qualidade de Vida Urbana (QVU) e comércio eletrônico (BMS-POS) e oito são bases de dados geradas artificialmente.

A base sobre Qualidade de Vida Urbana é composta pelo conjunto de variáveis de qualidade de vida consideradas na formulação do Indicador da Qualidade de Vida Urbana (IQVU) e pelo próprio índice.

Os dados são referentes aos setores urbanos da cidade de São Carlos, SP, Brasil.

Após realizadas as atividade de Pré-processamento, a base passou a ter um total de 120 exemplos, 19 atributos contínuos e sem valores ausentes.

A base BMS-POS descreve o conjunto de transações em um site de comércio eletrônico num período de dois meses.

Foi utilizada no KDD-cup 2000.

A base pré-processada, inclui 1657 atributos e 515597 exemplos.

Os atributos estão relacionados a três categorias de informação, clickstreams, order information e registration form.

As bases de dados artificiais foram geradas com o auxílio do aplicativo Gen descrito anteriormente.

Para identificar possíveis variações no comportamento das medidas, as bases artificiais (denominadas de ART) foram geradas com as mais variadas características com relação aos números de atributos, de exemplos, de atributos por exemplo e de atributos por itemset.

Os atributos utilizados são do tipo discreto, dadas as características específicas do problema de Regras de Associação.

As características dessas bases são descritas, lembrando-se que para as características não especificadas foram utilizados valores default do aplicativo Gen.

Características das bases de dados artificiais.

Com o auxílio do aplicativo MineSet TM, foram geradas Regras de Associação, considerando-se os valores default e no máximo três atributos por regra, para a base QVU.

A base BMS-POS e as bases artificiais foram mineradas com o auxílio do aplicativo Apriori.

Foram utilizadas as definições originais de Suporte e confiança, variando-se os valores mínimos dessas medidas.

Para os demais parâmetros foram utilizados valores default.

Para verificar a influência de outro parâmetro, a base ART D foi minerada variado-se o número máximo de atributos por regra.

Foram considerados os limites de 2, e atributos (o valor default para esse parâmetro é 5).

Uma síntese dos parâmetros utilizados para a geração de regras é apresentada.

Os aplicativos MineSet TM e Apriori foram utilizados em bases distintas pela maior facilidade no formato de entrada de dados.

Cabe lembrar que por tratar-se de uma tarefa descritiva de Mineração de Dados, o conjunto de regras gerado é independente dos algoritmos e dos aplicativos empregados.

Parâmetros utilizados nos algoritmos para Regras de Associação.

O número de regras obtidos a partir dos algoritmos de extração de regras utilizando os parâmetros é apresentado.

Número de regras geradas com a aplicação dos algoritmos de Regras de Associação.

Para cada regra gerada foram calculados os valores das medidas descritas na Seção 44, página 60.

A seguir, para cada base de dados, cada medida foi ordenada individualmente em sentido decrescente e os valores plotados em gráficos, com os valores das medidas nas ordenadas e o número sequencial das regras nas abscissas.

Desse procedimento foi gerado um total de 156 gráficos, cada qual para cada medida e cada execução do algoritmo para Mineração de Dados em um conjunto de dados (1medidas e 1execuções dos algoritmos para Mineração de Dados).

Alguns desses gráficos serão apresentados e comentados na próxima seção e uma síntese dos resultados obtidos será apresentada na Seção 54.

Em Melanda & Rezende é realizada a descrição pormenorizada do conjunto de gráficos.

Observando-se as Figuras de 5a 516 é possível identificar classes de comportamento gráfico para as funções decréscimo após um patamar constante, decréscimo linear, decréscimo inicial acentuado.

Como já mencionado, para uso como filtro são mais adequadas as medidas que a função apresenta decréscimo inicial acentuado.

Analisando os gráficos da medida confiança, pode-se observar que, de modo geral, a função da medida decai mais acentuadamente quando da ocorrência de um patamar relativamente constante (Figuras 54a 54).

Essa condição não é verificada unicamente nas Figuras 54e 54(f), que apresentam decaimento próximo do linear.

Observa-se a ocorrência de um patamar inicial constante com valor máximo da medida até cerca de 50% do número total de regras e decréscimo mais acentuado após este valor (ou seja, 50% das regras possuem o valor máximo da medida nesse conjunto de dados).

Medidas que têm funções com tal comportamento, em geral são pouco uteis para filtrar regras, contudo, podem ser empregadas em outros tipos de análises ou como um "pré-filtro".

Pode-se observar que a forma dos gráficos distingue-se consideravelmente das demais.

A instabilidade na forma gráfica da medida dificulta sua utilização em um sistema automatizado e portanto requer a supervisão do usuário.

No que se refere à variação de parâmetros para uma mesma base de dados, a medida não apresentou variações, como pode-se observar.

Medida confiança.

Nos gráficos da medida convicção, é possível identificar um padrão com decréscimo inicial acentuado dos valores da medida.

Medida confiança (continuação).

Para as 2500 primeiras regras (cerca de 10% do total de regras), o valor da medida convicção varia em cerca de 70% (de 1,00 para aproximadamente de 0,30) e, para as demais regras, o valor de convicção varia de 0,30 para 0,10.

Essa condição possibilita a identificação de um número reduzido de regras e portanto a medida é considerada adequada para utilização como filtro.

Com relação à estabilidade na forma das medidas, pode-se observar que o conjunto de bases apresenta, em termos gerais, comportamento semelhante.

Contudo, as bases se diferenciam das demais.

ART D apresenta decréscimo após patamar relativamente constante e ART D apresenta decaimento linear.

Considerando a relativa estabilidade da medida, pode-se utilizá-la como filtro, desde que haja monitoramento do usuário.

Quanto à variabilidade da medida em relação aos parâmetros de Mineração de Dados, na tabela, para uma mesma base de dados.

São apresentados os gráficos da medida confiança negativa.

Pode-se visualizar uma variedade de formas, que, no caso das curvas, podem ser considerados como apresentado decaimento do tipo linear.

O comportamento da medida para as bases de dados representadas mostra decaimento inicial acentuado.

Comportamento peculiar pode ser observado, e apresenta curva com pequeno decaimento inicial acentuado, com eventuais inflexões, seguido por decaimento suave.

Medida confiança Negativa.

Pode-se observar decréscimo no valor da confiança negativa com variação de 1,00 para 0,70 (39%) para as primeiras 11200 regras (44,8%).

Na sequência ocorre variação de 76,1% para 55,2% de regras, indicando decaimento aproximadamente linear.

Considerando a variedade de formas para as diferentes bases de dados, e que a capacidade restritiva que se aproxima do tipo da linear, é aconselhável que a medida confiança negativa seja empregada como um filtro complementar e desde que haja supervisão do usuário.

Os gráficos plotados para análise da medida novidade estão relacionados.

As curvas representadas apresentam em conjunto decaimento inicial acentuado, comportamento desejado para utilização da medida como filtro, que tem sua forma mais acentuada.

A curva apresenta decréscimo acentuado a partir de um patamar relativamente constante.

A variação inicial ocorre apenas para as duas primeiras regras.

Medida Novidade.

Medida Novidade (continuação).

O gráfico apresenta decréscimo inicial acentuado e subsequente decaimento linear, aqui descrito como do tipo sigmoidal.

Também nesse caso, considerando-se a variabilidade no comportamento gráfico da medida para as diferentes bases de dados utilizadas, é aconselhável que a medida novidade seja empregada com a supervisão do usuário.

Analisando-se os gráficos da medida confiança relativa, em termos gerais, há três tipos de comportamento gráficos distintos.

Apresenta-se curva com decaimento próximo do tipo linear.

Esse comportamento qualifica a aplicação da medida como um filtro complementar.

O gráfico tem formato do tipo sigmoidal.

Funções dessa forma são interessantes para realização de análises preliminares considerando a variação que ocorre nas extremidades da curva.

Medida confiança Relativa.

Contudo, a identificação das regiões de análise deve ser supervisionada pelo usuário.

Os gráficos apresentam em conjunto decaimento inicial acentuado, podendo ser empregados como filtro.

Da análise pode-se observar que, com a elevação do valor limite inferior de 0 para 0,25 (variação de 25%) ocorre a redução do número de regras para aproximadamente 16000 (há 36% de redução).

Esse comportamento indica baixa capacidade restritiva da medida para a base QVU, quando comparada com o grau de restrição da medida para a base ART D.

Nessa base, para o mesmo percentual de alteração no limite inferior (25%), ocorre redução de 77% no número de regras.

Como nos casos anteriores, considerando-se a variabilidade de formas obtidas para as diferentes bases de dados utilizadas, é aconselhável que o emprego da medida confiança relativa seja acompanhado por um usuário.

Nos gráficos plotados para medida confiança negativa relativa, de modo geral, a função apresenta decaimento inicial acentuado.

Essa condição não se verifica unicamente no caso da curva, que apresenta formato próximo do tipo sigmoidal.

Medida confiança Negativa Relativa.

Para as primeiras 1250 regras ocorre variação no valor da medida confiança negativa relativa de 0,78 para 0,0(94,9%) e variação de 0,0para 0,000 (5,1%) para as demais regras, indicando a potencialidade restritiva da medida, para uso como filtro de regras.

Com relação à estabilidade no comportamento gráfico, apenas a curva apresenta características ligeiramente distintas (possivelmente em função da quantidade reduzida de regras, apenas 44).

Desse modo a medida confiança negativa relativa pode ser adequada para uso como filtro sem a supervisão do usuário nas bases de dados estudadas.

Medida confiança Negativa Relativa (continuação).

São apresentados os gráficos da medida sensibilidade relativa.

Ocorre decréscimo aproximadamente linear nos valores da medida.

Ocorre decaimento inicial acentuado nos valores da medida.

A análise permite identificar o decaimento nos valores da medida da ordem de 75% (de 0,98 para 0,2para as primeiras 2500 regras, e de cerca de 25% para os 90% restantes das regras, indicando a capacidade restritiva do filtro).

Com relação ao formato gráfico e à sua estabilidade, a medida sensibilidade relativa pode ser considerada adequada para uso como um filtro para regras sem a supervisão de usuários.

Medida Sensibilidade Relativa.

Nos gráficos da medida especificidade relativa o decréscimo inicial acentuado é identificado como padrão.

Apesar do formato sigmoidal da curva, há uma queda inicial bastante acentuada, que indica a adequação da medida para utilização como filtro.

Medida Especificidade Relativa.

Da análise verifica-se que para as primeiras 1250 regras (5,0% do total), a medida especificidade relativa varia de 0,77 para 0,05 (variação de 93,5%) e de 0,05 para 0,00 para os outros 96% das regras, refletindo a capacidade de utilização como filtro da medida.

O comportamento gráfico dessa medida pode ser tido como estável considerando-se que apenas para a base de dados ART A ocorre variação no formato (mais uma vez, essa variação pode estar associada ao pequeno número de regras existentes).

Nesse sentido a medida pode ser considerada adequada para uso como filtro sem supervisão do usuário.

Observando-se os gráficos da medida satisfação identifica-se que ocorre decaimento inicial acentuado da curva.

Decaimento aproximadamente linear e decaimento nos valores após um patamar relativamente constante.

O comportamento da função se aproxima do tipo sigmoidal, com decréscimo inicial mais acentuado.

Medida Satisfação.

A análise comparativa mostra a grande variação da capacidade de restrição da medida como filtro.

O valor da satisfação decresce de 80% (1,0 para 0,2), para as primeiras 2500 regras, enquanto que na Medida Satisfação o valor permanece máximo e constante para as primeiras 12500 regras (cerca de 50% do total de regras).

A medida satisfação, considerando-se a grande variabilidade apresentada pode ser empregada como pré-filtro ou filtro principal, sob a supervisão do usuário.

A análise dos gráficos da medida sensibilidade mostra variabilidade significativa no comportamento gráfico da função.

Ocorre a situação de decréscimo inicial acentuado.

Para a base de dados o formato da função mostra decaimento após patamar constante.

O formato da função é aproximadamente linear.

Medida Sensibilidade.

Medida Sensibilidade.

No gráfico é observado que para o intervalo de 1,00 a 0,26 do valor da medida sensibilidade (variação de cerca de 74%), aproximadamente 2500 regras estão contidas no intervalo.

Enquanto que para os demais 90% das regras, os valores da medida variam de 0,26 a 0,00, o que representa cerca de 26% do intervalo de valores.

Considerando o comportamento gráfico obtido para as diferentes bases de dados, a medida satisfação pode ser empregada como um filtro secundário ou um pré-filtro conforme orientação do usuário.

Os gráficos da medida especificidade são apresentados.

De modo geral a curva apresenta decréscimo acentuado após patamar relativamente constante.

Esse comportamento ocorre nos gráficos.

O formato da função é aproximadamente linear.

Medida Especificidade.

A avaliação dos gráficos apresentados indica a grande variação da capacidade de restrição da medida como filtro.

O valor da medida varia de 77% para 50% das regras, enquanto o valor da especificidade permanece máximo e constante para cerca de 50% das regras.

Considerando a predominância do formato com decaimento acentuado após patamar, a inexistência do formato com decréscimo inicial acentuado e a ocorrência de apenas dois gráficos com decaimento próximo do modo linear, a medida especificidade pode ser empregada como um "pré-filtro" automatizado, sem a supervisão de um usuário.

Nos gráficos da medida suporte é possível identificar o padrão com decaimento inicial acentuado.

A oscilação que ocorre na curva é irrelevante para a classificação realizada, uma vez que se está buscando um comportamento geral para a medida.

O decaimento da função é próximo do modo linear, condição que determina uma redução menor no número de regras.

Medida Suporte.

A análise, mostra que para as primeiras 1250 regras (valor de cerca de 5% do total de regras), o valor da medida suporte varia de 0,76 para 0,06, variação de 92%.

Para as demais 23750 regras o valor do suporte varia de 0,06 para 0,02.

Medida Suporte.

A partir dos resultados obtidos, a medida suporte pode ser considerada para uso como filtro principal sem supervisão do usuário.

Encontram-se os gráficos da medida J para as diferentes bases de dados.

Para as bases o comportamento da função se aproxima do modelo sigmoidal, com a curva tendendo para o modo linear.

As curvas apresentam decaimento inicial acentuado, formato que é predominante para esta medida.

Medida J.

Ao observar detalhadamente, identifica-se variação de 69% (de 0,para 0,0para as primeiras 1250 regras, o que demonstra a capacidade de restrição que a medida J oferece e a adequação desta para uso como filtro principal ou secundário, com a supervisão do usuário).

Nesta seção são realizadas duas análises complementares dos gráficos.

A partir dessas análises buscou-se identificar de modo automatizado o formato dos gráficos ou reconhecer algum padrão característico de comportamento, para validar as análises realizadas na seção anterior.

Objetivo secundário dessas análises é verificar a possibilidade de automatização do processo de categorização das medidas.

Uma abordagem alternativa adotada na análise do comportamento gráfico das funções utiliza a técnica de clustering sobre as imagens dos gráficos.

Esse procedimento foi adotado para validar a análise realizada de forma manual.

Foram utilizados os conceitos de similaridade de imagens e de cluster.

Optou-se por analisar os gráficos por meio de similaridade de imagens, ao invés da análise por semelhança de equações, uma vez que se deseja obter uma similaridade genérica, relacionada principalmente ao formato de decaimento da curva.

Os arquivos de imagens dos gráficos foram convertidos para arquivos de imagens no formato PBM ASC.

A partir destes arquivos, foram calculados os descritores, área sob a curva (A, comprimento (C e momento da curva (M).

A seguir, esses dados foram processados utilizando o algoritmo k-means, variando-se o número de clusters de a 5.

Na apresentada a síntese com os resultados das execuções do algoritmo que descreve o número de clusters especificados, os atributos considerados (AC, CC e M e o número de gráficos em cada cluster).

Para cada execução do algoritmo de clustering os arquivos de imagem foram agrupados e verificados visualmente com relação à similaridade das curvas.

A divisão do conjunto de imagens em três clusters e considerando as variáveis AC e MC foi a solução que apresentou melhores resultados, conforme avaliação visual realizada.

Distribuição do número de gráficos segundo os clusters.

Para identificar a existência de alguma relação entre a forma gráfica da medida, as características da base de dados, os parâmetros do processo de Mineração de Dados e os descritores gráficos, foi realizada verificação da possibilidade de classificação automática dos gráficos utilizando o algoritmo C45.

Os dados utilizados para execução do algoritmo de classificação foram obtidos a partir das Tabelas de 51 a 5e da análise dos gráficos das medidas realizadas.

Os atributos considerados são apresentados, e representam o arquivo names utilizado na execução do algoritmo de classificação.

Descrição dos atributos considerados para execução do algoritmo de classificação.

A árvore de decisão resultante do processo de classificação é apresentada.

Observa-se que o principal atributo discriminador para o formato gráfico é o momento da curva, para valores de momento acima de 19e+7 a curva é do tipo P (decréscimo após patamar constante) e para valores desse atributo inferiores a curva é do tipo I (decréscimo inicial acentuado).

Para valores de momento intermediários a esses limites é necessário considerar outros atributos, em especial a medida.

Nesse sentido, pode ser possível estabelecer um método automatizado para identificar a forma do gráfico a partir de descritores de imagens e características do processo de Mineração de Dados.

Esta seção resume as análises dos gráficos das medidas objetivas realizadas.

Para cada medida e cada base de dados é indicada a classificação atribuída na interpretação dos gráficos e também a classe mais frequente (mod para cada medida).

Aspecto que pode ser verificado neste experimento é a frequência de ocorrência para cada formato gráfico.

Arvore de decisão simplificada para os gráficos das medidas.

Os formatos com decréscimo linear e decréscimo após patamar são os menos frequentes com ocorrência em 12,8% e 14,1%, respectivamente.

Os formatos com decréscimo inicial acentuado e do tipo sigmoidal, que têm maior restritividade como filtro, representam em conjunto a maioria das ocorrências, 73,1% dos casos (55,2% e 17,9%, respectivamente).

Nesse sentido, pode-se considerar que as medidas objetivas representam um bom recurso para utilização como filtros.

Cabe lembrar que o uso dessas medidas deve ser acompanhado pelo usuário, porque as medidas avaliam aspectos diferentes de qualidade da regra e principalmente devido à variabilidade existente nos formatos gráficos.

Distribuição do tipo de gráfico por base de dados.

De modo geral, há predominância do tipo Decréscimo Inicial, com os demais formatos distribuídos entre as diferentes bases.

Há variabilidade na distribuição de formatos para a base ART D, para a qual foram alternados os parâmetros utilizados no Apriori.

Distribuição dos tipos de gráficos por base de dados.

Distribuição do tipo de gráfico por medida avaliada.

Mais uma vez verifica-se a predominância do tipo Decréscimo Inicial.

A variabilidade do tipo de gráfico em relação às medidas neste caso é menor, com várias medidas apresentando apenas dois tipos de comportamento gráfico diferentes ( confiança relativa negativa, especificidade, suporte, entre outras).

Distribuição dos tipos de gráficos por medida.

Analisando, observa-se que os diferentes tipos de formatos gráficos estão distribuídos com maior uniformidade entre as bases de dados que em relação às medidas.

Esse é um resultado desejável, considerando que o ideal seria um comportamento gráfico característico da medida e independente da base de dados.

A partir das análises realizadas, para o conjunto de bases de dados.

Nessa tabela é apresentado o formato gráfico das medidas analisadas para cada base de dados considerada e o valor da moda associada a cada medida.

As cores associadas aos diferentes formatos são utilizadas para facilitar a comparação dos dados contidos nesta tabela com aqueles outros.

As medidas confiança e especificidade apresentaram, em geral, curva com decréscimo após patamar relativamente constante.

A medida confiança negativa apresentou, em geral, curvas com decréscimo sigmoidal.

Para a medida J as curvas com decréscimo inicial acentuado e decréscimo sigmoidal ocorreram com igual frequência.

Para as demais medidas o comportamento gráfico predominante foi aquele com decréscimo inicial acentuado.

Dessa análise pode-se inferir que a maioria das medidas são adequadas para uso como filtro.

Para a divisão em três clusters, a categoria em que cada gráfico de medida foi classificado e a moda associada a cada medida.

O cluster de número contêm o grupo de gráficos com funções que apresentam decréscimo inicial acentuado (formato mais interessante para uso como filtro).

Resumo da categorização dos gráficos realizada na Seção 5351

Os clusters de números e 3, contêm os grupos de gráficos com funções que apresentam decréscimo linear e decréscimo após patamar constante, respectivamente.

Foi incluída a classificação constante da tabela por meio das cores de fundo de cada célula, para maior facilidade na análise comparativa das tabelas.

Da análise dos dados contidos na tabela, pode-se observar que as medidas confiança negativa relativa e sensibilidade relativa são aquelas que apresentam a menor variabilidade no formato gráfico em apenas 1 das 1bases de dados analisadas a forma foi diferente.

O resultado do agrupamento realizado para a medida foi semelhante ao da análise do tipo manual.

As medidas especificidade, especificidade relativa e suporte também apresentaram menor variação no formato gráfico.

Esse aspecto indica que tais medidas poderiam ser empregadas como filtros em um sistema pouco dependente da supervisão do usuário.

Para as demais medidas seria necessário supervisão do usuário, como o caso da medida confiança, que apresenta grande variabilidade nas diferentes bases de dados (1 gráfico com Decréscimo Inicial Acentuado, gráficos com Decréscimo Linear e 9 gráficos com Decréscimo após Patamar).

Resumo da categorização dos gráficos utilizando a técnica de cluster.

Comparando a coluna moda das Tabelas 55 e 56 é possível constatar que, em grande parte das medidas, o valor da moda foi coincidente.

Apenas no caso das medidas confiança negativa e satisfação o resultado foi divergente.

A medida confiança negativa, foi inicialmente classificada como tendo o formato sigmoidal.

A classificação por cluster, identificou a medida como do tipo linear (deve-se observar que o formato sigmoidal é bastante parecido com o linear, diferenciando-se apenas nas extremidades da curv).

Os gráficos com comportamento sigmoidal ficaram restritos aos clusters 1 e 2, com predominância no cluster 2.

Esse comportamento pode ser explicado pela semelhança entre os formatos linear e sigmoidal.

Os gráficos com o formato de decaimento inicial foram predominantes nas duas análises realizadas, por cluster e manual.

Nesta seção é avaliado o uso de medidas objetivas como filtros individuais e como filtros compostos, sendo considerada fundamentalmente a redução do número de regras.

Para esta avaliação a base de dados QVU foi selecionada pela possibilidade de interpretação das regras geradas.

As medidas confiança, J, especificidade relativa e suporte foram selecionadas para ilustrar medidas de alta e baixa capacidade restritiva.

A análise da capacidade restritiva de cada medida é realizada aumentando-se o valor mínimo de cada medida.

Na ão apresentados os resultados do uso de cada medida como filtro individual.

O valor mínimo de cada medida foi aumentado em 5%, 10%, 30%, 50% e 80% e foram selecionadas as regras com valor da medida superior ao novo valor mínimo.

Assim, para cada percentual de incremento e para cada medida são apresentados os números de regras que satisfazem essa condição e o percentual dessas regras em relação ao total.

Em termos gerais, as medidas suporte e especificidade relativa são as mais restritivas.

A elevação de apenas 5% no valor mínimo de cada medida conduz a uma redução de aproximadamente 90% no número de regras.

Para uma elevação de 10% no valor mínimo da medida, o número de regras atinge cerca de 99% de redução.

Assim, essas medidas podem ser consideradas filtros altamente restritivos.

Em termos do princípio de Pareto, tem-se que para 90% das consequências (5% de elevação) ocorrem 10% das causas, 99% das consequências (10% de elevação) são produzidos por apenas 1% das causas.

Número de regras por percentual de elevação no valor mínimo da medida.

A medida J pode ser considerada um filtro medianamente restritivo, porque apenas com elevação acima de 50% no valor mínimo da medida, ocorre uma redução expressiva no número de regras.

Para a medida confiança, o valor máximo da medida ocorre em mais de 50% das regras, portanto essa medida não é adequada para uso como um filtro isolado.

O uso combinado de medidas incrementa a capacidade restritiva dos filtros.

Na tabela são apresentados os resultados para 5% de incremento no valor mínimo de cada medida o percentual menos restritivo representando cerca de 95% do fenômeno (medid).

Cabe lembrar que os valores presentes na diagonal da tabela são referentes ao uso da medida de maneira individual.

Os demais valores correspondem à combinação das medidas duas-a-duas.

Dentre as medidas analisadas, o suporte apresenta a redução mais expressiva no número de regras, quando combinado com as demais medidas e a confiança apresenta a redução menos expressiva.

O menor número de regras ocorre com a combinação das medidas especificidade relativa e suporte 1585 regras selecionadas das 2471regras, representando redução de aproximadamente 93% no número total de regras.

Número de regras selecionadas para 5% de elevação.

Além da combinação das medidas duas-a-duas, o uso das quatro medidas em conjunto como um filtro unico também foi explorado.

Os resultados são descritos na tabela.

Ocorre redução expressiva no número de regras em todas as situações e, a partir de 10% de elevação do valor mínimo de cada medida (90% de representatividade do fenômeno), o número de regras é reduzido o suficiente para ser apresentado a um especialista do domínio, para análise individualizada das regras.

Para percentuais de elevação superiores a 30% a combinação de medidas não é eficiente, uma vez que não ocorre redução significativa no número de regras.

Número de regras selecionadas, combinando as quatro medidas.

Na tabela são apresentadas as 11 (onze) regras selecionadas aplicando o percentual de 50% de incremento e combinando quatro medidas.

Estas regras são as mesmas 11 regras selecionadas quando aplicando-se o percentual de 50% de aumento para os valores da medida especificidade relativa.

Para avaliar a qualidade do processo de filtragem, o conjunto de regras foi apresentado a um especialista.

As regras de número 1 a 4, 9 e 10 podem ser consideradas "conhecimento óbvio" por estarem representando os atributos altamente correlacionados, dpi p, porpdimp e pdp.

As regras de número 5 a 8 são consideradas interessantes pelo especialista, principalmente as regras 7 e 8, que contradizem as expectativas, para valores reduzidos de dpi p e porpdimp, são esperados altos valores de Pess Dom.

Regras selecionadas para avaliação do especialista do domínio.

O emprego de medidas objetivas de avaliação de regras na etapa de Pós-processamento permite o estabelecimento de filtros para seleção de regras, uma vez que estas medidas fornecem uma indicação da força de associação (hipotétic entre o LHS e o RHS).

Nesse sentido, o experimento realizado permite constatar que algumas medidas são mais restritivas que outras e portanto mais adequadas à constituição de filtros.

Outras medidas apresentam menor capacidade de restrição e podem ser empregadas como "préfiltros" ou como medidas complementares.

Também é observado que algumas medidas, aquelas com formato do tipo sigmoidal, são interessantes porque produzem pequenos conjuntos de regras em seus valores extremos.

Com relação à estabilidade do formato gráfico, e a consequente estabilidade da capacidade restritiva, ocorreu significativa variação apesar de, na maioria dos casos, o comportamento gráfico ter sido do tipo com decaimento inicial acentuado.

O experimento comparando a restritividade adotada para os filtros e a qualidade das regras geradas, avaliadas por usuário especialista, permite concluir que o uso de medidas isoladas e/ou combinadas proporciona resultados interessantes.

Com relação à restritividade dos filtros combinados é constatado que níveis elevados de restritividade apresentam resultados semelhantes aos dos filtros isolados, não sendo portanto uma opção adequada nos estudos realizados.

Sendo assim, a supervisão do usuário é requerida durante um processo de análise utilizando tais medidas.

Os resultados obtidos subsidiam a elaboração de um sistema que apóia o usuário na identificação de regras potencialmente interessantes.

Essa identificação pode ser realizada utilizando uma ou mais medidas para filtrar regras.

O controle de utilização das medidas deve ser realizado interativamente, pelo usuário, de acordo com a intensidade restritiva de cada medida.

Nesse sentido é proposto o ambiente computacional ARInE, que é apresentado no capítulo a seguir.

O emprego de métricas na avaliação de conhecimento extraído com a tarefa de Regras de Associação é um procedimento bastante usual.

A maioria das abordagens utiliza uma ou mais métricas para ordenar as regras e então apresentá-las para avaliação do usuário final.

A partir das demandas para análise de Regras de Associação identificadas durante os experimentos apresentados no Capítulo 5 e dos resultados dos referidos experimentos, foi proposta uma metodologia para o pós-processamento de Regras de Associação fundamentada em análises gráficas de medidas objetivas.

O diferencial dessa metodologia é o uso do Método de Pareto para definir um valor de "corte" nos valores das medidas objetivas durante a seleção de regras.

A metodologia proposta é viabilizada com o auxílio do ambiente computacional ARInE-Environment for Association Rules Interactive Exploration e definiu o fluxo de análise de regras disponibilizado no ambiente, bem como suas principais funcionalidades.

Neste capítulo inicialmente é apresentada a metodologia proposta para Pós-processamento de Regras de Associação.

A seguir, é apresentada definição de representação padronizada de Regras de Associação, visando incorporar a metodologia proposta no Projeto Discover.

Também é descrito neste capítulo o Ambiente ARInE que implementa a metodologia de Pós-processamento de Regras de Associação proposta nesta tese.

A metodologia para Pós-processamento de Regras de Associação proposta nesta tese foi elaborada a partir dos objetivos iniciais da tese, de necessidades identificadas durante a realização dos experimentos apresentados no Capítulo 5 e dos resultados obtidos nesses experimentos.

Em linhas gerais, na metodologia proposta busca-se identificar pequenos grupos de regras para serem avaliadas por um especialista.

A identificação destes grupos de regras inicia-se a partir de um conjunto de regras resultante da etapa de Extração de Conhecimento.

Por meio de consultas simples pode-se selecionar regras com atributos ou itens que se deseja analisar constituindo, assim, um novo conjunto de regras (focus-ruleset).

A partir da definição do focus-ruleset é iniciado o processo de análise das regras utilizando-se medidas objetivas.

Para cada medida é possível analisar graficamente a distribuição de seus valores para o universo de regras do focus-ruleset utilizando os gráficos de dispersão e de barras e o gráfico de Pareto.

Os gráficos de dispersão e de barras podem ser utilizados para a analisar a distribuição de valores em termos gerais e, também, para identificar algum comportamento diferenciado bastante evidente.

Com o gráfico de Pareto é possível identificar o percentual do valor acumulado da medida, para o qual existe relação de poucas regras e elevado valor acumulado.

Assim, define-se o conjunto de regras potencialmente interessantes para uma medida.

Esse processo pode ser repetido para outras medidas, criando-se novos conjuntos potencialmente interessantes.

E realizada a intersecção destes conjuntos e as regras resultantes são analisadas e interpretadas por especialista que irá identificar aquelas de seu interesse, ou seja, o conjunto de regras interessantes.

Cabe dizer que o especialista também pode analisar os conjuntos de regras potencialmente interessantes individualmente.

O fluxo de análise adotado na metodologia proposta é apresentado, consistindo das seguintes etapas, escolha do conjunto de regras.

Este conjunto contém as regras geradas por um algoritmo para gerar Regras de Associação, definição do focus-ruleset por meio consultas simples ao conjunto de regras, análise de similaridade entre medidas, utilizando análise fatorial ou outros métodos de análise de similaridade, seleção das medidas a serem utilizadas como filtros.

É determinada pelos interesses do especialista e subsidiada pela análise de similaridade entre medidas, aplicação dos filtros apoiado por análises gráficas, apresentação das regras selecionadas ao especialista para avaliação final.

Processo de identificação de regras interessantes.

Na primeira etapa do processo de identificação das regras interessantes, o usuário, a partir de um conjunto de Regras de Associação define seu conjunto de regras de foco utilizando métodos simples de consulta à base de regras.

Nas consultas podem ser especificados os termos e definidos se ocorrem no antecedente, no consequente ou em qualquer parte da regra.

Os termos podem ser nomes ou valores de atributos, ou ainda sua combinação.

O resultado da consulta é apresentado ao usuário, que pode optar por realizar uma nova consulta, salvar ou excluir todas as regras resultantes, ou ainda realizar estas duas operações individualmente para cada regra.

As regras salvas serão analisadas em etapas posteriores.

As regras excluídas têm duas funções, as consultas subsequentes não retornam regras que já foram excluídas, o usuário posteriormente pode analisar as regras excluídas.

Esta etapa de definição do conjunto de regras foco é opcional, contudo é bastante util quando o usuário está interessado em descobrir regras relacionadas a apenas alguns atributos.

Definido o focus-ruleset é necessário selecionar as medidas objetivas que serão utilizadas para avaliar as regras.

Para isso é proposto o uso da técnica de análise fatorial para apoiar ao usuário na escolha das medidas.

A análise fatorial irá agrupar medidas correlacionadas.

Isso permite adotar duas linhas de análise, medidas correlacionadas, a análise usando medidas correlacionadas permite identificar regras que "fogem" do padrão de correlação apresentado pelas medidas.

Por apresentarem um comportamento diferenciado estas regras podem ser interessantes, devendo então serem verificadas por um especialista.

Medidas não correlacionadas, a análise usando medidas não correlacionadas, de maneira semelhante, permite identificar que não seguem um padrão de comportamento esperado por exemplo valores semelhantes em medidas não correlacionadas

Cabe lembrar que o processo proposto é iterativo e interativo, podendo o usuário selecionar uma medida, utilizá-la como filtro, avaliar regras e escolher outra medida e repetir o processo.

Na próxima seção é descrito como as medidas objetivas selecionadas podem ser utilizadas como filtros.

Definido o conjunto de regras a ser a analisado e selecionadas as medidas objetivas, a próxima fase consiste na aplicação de filtros utilizando estas medidas.

Cada filtro aplicado gera um subconjunto de regras que pode ser interpretado pelo especialista ou ser combinado com o resultado dos demais filtros gerando um outro conjunto de regras potencialmente interessantes.

Esses conjuntos são apresentados a um especialista para validação, identificando assim regras interessantes.

Pode-se observar que o processo de análise proposto é interativo, com diversas etapas necessitando de intervenção do usuário e iterativo com alguns ciclos, se necessário.

A aplicação de filtros representa o conceito central da metodologia proposta, que é identificar pequenos conjuntos de regras de maneira que o usuário/especialista possa interpretá-los e identificar as regras que considere interessantes.

Estes pequenos conjuntos podem ser, por exemplo, identificados visualmente observando-se as áreas em destaque no gráfico de dispersão.

Contudo, nem sempre a identificação destes conjuntos é trivial,na qual os pontos estão dispersos.

Nesse caso é necessário o uso de outros métodos de análise.

Para tanto, são propostas outras duas maneiras de análise também baseadas no estudo de gráficos, a distribuição de frequência e a análise de Pareto.

Gráficos de dispersão de valores de medidas para Regras de Associação.

Na análise da distribuição de frequência, os valores de uma dada medida são discretizados e é verificado o número de regras para cada intervalo.

São exemplificados os resultados obtidos para esse tipo de análise.

Esse procedimento visa analisar conjunta e comparativamente regras com características semelhantes (valor da medid).

Distribuição irregular de regras por valores Distribuição regular de regras por valores de medidas de medidas.

Gráficos de distribuição de frequência de valores de medidas para Regras de Associação.

A análise de Pareto é baseada no Princípio de Pareto, que afirma"para a maioria dos fenômenos naturais, 80% das consequências são originadas por apenas 20% das causas".

No âmbito da avaliação de regras de associação, o Princípio de Pareto pode ser assim interpretado, "apenas 20% das regras são responsáveis por 80% do valor acumulado de uma medida".

Desse modo é possível priorizar a análise de regras considerando-se apenas as regras que mais contribuem para uma dada medida.

Na análise de Pareto, é gerado o gráfico da soma cumulativa dos valores da medida para cada regra considerada.

Então o gráfico é apresentado ao usuário que o interpreta e define o percentual limite de corte para seleção das regras.

Como referência, é destacado o percentual de 80% no gráfico.

No caso apresentado,a relação 80/20 não é válida, mas pode ser obtida a relação 60/30 que indica que para 60% de representatividade da medida são selecionadas apenas 30% das regras.

É obtida a relação 80/50 (para 80% de representatividade da medida, são selecionados 50% das regras).

A relação dessa proporção indica que a medida pode ser mais util como um pré-filtro para combinação com outra medida.

Independente do modo de seleção utilizado (dispersão, análise de distribuição ou análise de Pareto), cabe lembrar que é possível reduzir ainda mais o número de regras a ser apresentado ao usuário especialista, por meio da união ou da intersecção dos conjuntos de regras de maior contribuição para cada medida, de maneira semelhante à apresentada nas Tabelas 58 e 59, página 135.

Curva com inclinação inicial mais acentuada. Curva com inclinação inicial menos acentuada Gráficos para análise de Pareto.

Para facilitar e flexibilizar as análises propostas, os conjuntos de regras são inicialmente armazenados no banco de dados do Ambiente RulEE.

Para integração ao RulEE e consequentemente ao Projeto Discover, foi necessário definir uma representação padronizada para as Regras de Associação.

Detalhes da relação ARInE RulEE são apresentados na Seção 64.

Em Regras de Associação, apesar da semelhança na forma de representação das hipóteses, cada algoritmo salva regras geradas com sintaxes diferentes.

Os algoritmos além das regras, fornecem as métricas suporte e confiança, geralmente acompanhadas de métricas complementares lift, confiança esperada, leverage, entre outras variando de algoritmo para algoritmo.

Devido às diferentes sintaxes de representação das regras geradas pelos algoritmos, qualquer medida ou estatística que necessite ser calculada sobre as regras geradas por diferentes algoritmos deveria ser implementada várias vezes.

Esse fato também dificulta a interpretação das regras pelo usuário, que deverá lidar com símbolos e formatos diferentes para cada conjunto de regras e para cada algoritmo utilizado.

Para propiciar ao usuário um conjunto de informações padronizadas sobre cada regra e evitar retrabalho nas implementações das métricas de avaliação, foi definida uma sintaxe padrão para representação de Regras de Associação.

Com base nessa sintaxe, são padronizadas as saídas dos algoritmos de associação e convertida para um formato unico no qual as regras não são ordenadas.

Considerando que a maioria das medidas objetivas para avaliação de regras encontradas na literatura podem ser derivadas da tabela de contingência, o formato padrão para Regras de Associação contém as frequências relativas f(LHS RHS), f(LHS RHS), f(LHS RHS) e f(LHS RHS), além do número total de exemplos considerados N.

Os valores absolutos das medidas podem ser obtidos pela multiplicação das frequências relativas por N.

As frequências marginais f(LHS), f(RHS), f(LHS) e f(RHS) também podem ser obtidas pela soma das linhas e colunas da tabela de contingência.

Assim, para cada regra são acrescentados os valores de frequência relativa e o número de exemplos, do seguinte modo, Esse conjunto de informações é calculado a partir das métricas fornecidas pelos algoritmos de Regras de Associação, conforme apresentado na Seção 632.

Sendo assim, uma Regra de Associação na sintaxe padrão apresenta a formatação mostrada na Expressão 61.

Cabe observar que a metodologia desenvolvida para análise de Regras de Associação faz parte da proposta do Discover.

Desse modo, foi desenvolvida uma biblioteca com scripts para conversão dos dados na sintaxe padrão de exemplos para o formato de entrada dos algoritmos de extração de Regras de Associação Apriori e dos softwares WEKA, MineSet TM e MagnumOpus, e também, os scripts para conversão da saída desses algoritmos para a sintaxe padrão de Regras de Associação.

Essa abordagem segue o padrão estabelecido para comunicação entre as funcionalidades do Ambiente Discover.

Metodologia dos formatos padrão para comunicação entre as funcionalidades do Ambiente Discover trada dos algoritmos de extração de Regras de Associação Apriori e dos softwares WEKA, MineSet TM e MagnumOpus, e também, os scripts para conversão da saída desses algoritmos para a sintaxe padrão de Regras de Associação.

Essa abordagem segue o padrão estabelecido para comunicação entre as funcionalidades do Ambiente Discover.

Para a definição de uma sintaxe padrão para Regras de Associação, é especificada uma gramática a ser utilizada na conversão das regras geradas pelos algoritmos de associação para essa sintaxe.

A definição dessa gramática levou em consideração a padronização definida pelo grupo do LABIC que trabalha com Extração de Conhecimento de Dados.

Foi considerada também, a necessidade de otimização no processamento de arquivos de regras e a compreensibilidade das regras na sintaxe padrão para os usuários.

Nesse contexto, foi proposta a sintaxe padrão para Regras de Associação, que possui boa compreensibilidade e legibilidade permitindo fácil leitura e interpretação das regras pelo usuário.

Contudo, considerando o elevado número de regras e o tamanho dos arquivos gerados foi necessário desenvolver uma sintaxe padrão compacta permitindo o processamento mais ágil e proporcionando arquivos de regras com tamanho consideravelmente inferior (de 30% a 40% menor, conforme alguns experimentos).

Porém a compreensibilidade desta sintaxe é significativamente menor quando comparada à sintaxe padrão.

Sintaxe Padrão para Regra de Associação A gramática que define a sintaxe padrão para Regras de Associação, é definida como uma quádrupla As regras geradas por um algoritmo são convertidas para a sintaxe padrão de Regras de Associação.

Na sequência são apresentados alguns exemplos dessa conversão.

Exemplo de regra gerada pelo MineSet TM, convertido para a sintaxe padrão de Regras de Associação.

Deve-se lembrar que, no formato original, as regras são armazenadas em arquivo-texto, com uma regra por linha.

Na sintaxe padrão as regras ainda são armazenadas em arquivo-texto só que não mais uma regra por linha.

Produções da gramática para Regras de Associação.

Regra de Associação gerada pelo algoritmo do MineSet TM.

Regra de Associação na sintaxe padrão.

Observe-se também que algumas informações fornecidas pela regra original (por exemplo, 71429 100) não são armazenadas na sintaxe padrão, uma vez que essas medidas normalmente são distintas para cada algoritmo de associação.

As informações necessárias à avaliação de regras tabela de contingência são calculadas, como descrito na Seção 632, e armazenadas de modo padronizado e independe do algoritmo que as produziu.

Sintaxe Padrão Compacta para Regras de Associação.

A gramática que define a sintaxe padrão compacta para Regras de Associação, é definida como uma quádrupla.

Produções da gramática para Regras de Associação no Formato Compacto.

A regra apresentada, convertida para a sintaxe padrão compacta.

Cabe destacar que todas as regras convertidas também são armazenadas em arquivo-texto com uma regra por linha.

Assim como na sintaxe padrão, também são desconsideradas algumas informações presentes no arquivo de regras original.

Regra de Associação na sintaxe padrão compacta.

Diferentemente de outras técnicas de Mineração de Dados, a técnica Regras de Associação gera um número elevado de regras (da ordem de milhares ou centenas de milhares de regras), com cada regra representando um conhecimento unico e independente de outras regras.

O cálculo da tabela de contingência para tal quantidade de regras demanda tempo considerável, já que é necessário avaliar, para cada regra, todos os exemplos existentes na base de dados, ou seja, ocorre uma passagem pela base de dados para cada regra.

Assim, para um conjunto de um milhão de regras seria necessário um milhão de passagens pela base, o que evidentemente tornaria o processo muito lento.

Para contornar essa dificuldade, a alternativa adotada é calcular a tabela de contingência a partir de métricas fornecidas pelos algoritmos, calculadas durante o processo de geração das regras.

Suporte e confiança são duas métricas fornecidas pelos algoritmos de Regras de Associação, contudo são insuficientes para elaboração da tabela de contingência o que determina o emprego de outra métrica.

A confiança esperada, o lift (razão entre confiança e confiança esperad ou a diferença de confiança esperada (diferença entre confiança e confiança esperad, são métricas usualmente fornecidas pelos algoritmos em conjunto com suporte e confiança).

Considerando que a confiança esperada pode ser obtida de maneira simples a partir das outras duas medidas mencionadas, esta foi adotada para ilustrar os cálculos da tabela de contingência.

Na sequência é apresentado como a tabela de contingência é elaborada a partir do suporte, confiança e confiança esperada.

Estas medidas são definidas, respectivamente, pelas Equações 32, 3 e 42, e para facilitar o entendimento são apresentadas novamente como se segue, As Equações 65 e 66 que representam, respectivamente, o número de transações nas quais LHS ocorre e, o número de transações nas quais RHS não ocorre.

Estes valores auxiliarão na elaboração da tabela de contingência para cada regra de associação.

Assim, utilizando-se apenas as medidas suporte, confiança e confiança esperada, é calculada a tabela de contingência para cada regra com as expressões apresentadas nas Equações 67, 68, 69 e 610.

Cabe lembrar que esse recurso é usado para eliminar a necessidade de percorrer a base de dados para construção da tabela de contingência, otimizando o processo de cálculo.

Como já mencionado, as regras geradas por algoritmos diferentes quase sempre são expressas em formatos específicos e no Projeto Discover a comunicação entre as atividades desenvolvidas em cada etapa do processo é realizada utilizando sintaxes padrão.

O processo de conversão das regras para a sintaxe padrão de Regras de Associação é realizado pela classe Rass Conv que contém os métodos que convertem a saída de cada algoritmo de associação para a sintaxe padrão ou para a sintaxe padrão compacta.

A classe também permite a conversão entre as duas sintaxes.

É apresentado o esquema funcional do módulo de conversão de sintaxes.

Conversão das saídas dos algoritmos de Regras de Associação.

No restante da seção são descritos os sistemas e algoritmos de associação para os quais foram implementados os conversores, bem como exemplos de Regras de Associação no formato original de cada sistema e depois de convertidas para o formato padrão de Regras de Associação.

Os aplicativos Apriori e WEKA são freeware.

Para o MineSet TM existe uma versão licenciada para o LABIC e o aplicativo Magnum Opus foi utilizado em sua versão demonstrativa.

Para facilitar a compreensão das regras geradas, os algoritmos foram executados utilizando a base Weather Nominal obtida do conjunto de exemplos do aplicativo WEKA e apresentada na tabela.

Base de dados Weather Nominal.

O aplicativo Apriori, implementado por Christian Borgelt, é descrito na Seção 532.

Para obter todas as informações necessárias para o cálculo da tabela de contingência e para que o módulo conversor atue e converta as regras para o formato padrão, o aplicativo deve ser executado com as opções,oe1d0vap%9 f%%.

A opçãoo indica que devem ser utilizadas as definições originais de suporte e confiança.

A opçãoe1d0v indica que as regras, além de satisfazer a condição de suporte e confiança mínimos, devem satisfazer a condição de "diferença de confiança esperada" superior ao valor especificado na opçãod que, para não afetar os resultados, deve possuir o valor 0 (zero) e a opçãov para apresentar o valor da medida no arquivo de saída.

A opçãoa inclui o valor absoluto do suporte (número de transações) e a opçãop%9 f%% formata os valores de todas as medidas com 9 casas decimais.

Cabe lembrar que a diferença de confiança esperada representa a diferença entre a confiança esperada e a confiança para uma regra.

É apresentado exemplo do arquivo de saída gerado pelo aplicativo Apriori e as regras desse exemplo na sintaxe padrão compacta.

O formato geral da regra para o Apriori é, "RHS LHS (suporte/número de exemplos cobertos pelo suporte, confiança, diferença de confiança esperad").

Considerando a regra outlook=sunny <-temperature=cool (7142857143%/1, 25000000000%, 10714285714%), a tabela de contingência é calculada assim, Algumas regras geradas pelo Apriori utilizando o conjunto de dados Weather Nominal.

Regras convertidas para o formato padrão compacto de Regras de Associação.

O ambiente WEKA (Waikato Environment for Knowledge Analysis) foi desenvolvido na Universidade de Waikato na Nova Zelândia dentro da filosofia GNU de código fonte aberto.

Possui ferramentas para pré-processamento de dados, classificação, regressão, clustering, Regras de Associação e visualização.

Os diversos algoritmos de Aprendizado de Máquina, implementados na linguagem Java, são representados cada qual por uma classe, o que permite que as classes sejam organizadas em pacotes, que são simplesmente diretórios contendo uma coleção de classes relacionadas.

Isso é bastante util porque certos algoritmos compartilham muitas funcionalidades, e desse modo, várias classes em um pacote podem ser utilizadas por mais de um algoritmo.

O pacote weka associations contém a implementação do algoritmo Apriori proposto por Agrawal.

Para obter todas as informações necessárias para o cálculo da tabela de contingência e para que o módulo conversor atue e converta as regras para o formato padrão, é necessário que o módulo de Regras de Associação do WEKA seja executado com a opção metricType diferente de confidence (confiança, assim serão calculadas métricas adicionais que permitirão a elaboração da tabela de contingência sem a necessidade de consultas à base de dados).

Diferentemente do que ocorre com o algoritmo Apriori, na conversão do formato original do WEKA para o formato padrão de Regras de Associação ocorre a perda de detalhes do processo de geração de regras.

Algumas regras geradas pelo WEKA utilizando o conjunto de dados Weather Nominal.

Regras convertidas para o formato padrão compacto de Regras de Associação.

O aplicativo MineSet TM da SGI, é descrito na Seção 532, página 105.

É apresentado exemplo do arquivo de saída, no formato ASC, gerado por este aplicativo.

São apresentadas as regras desse exemplo na sintaxe padrão compacta.

Regras convertidas para o formato padrão compacto de Regras de Associação.

O aplicativo Magnum Opus, comercializado pela Rule Quest, é a implementação do algoritmo Opus proposto por Webb.

Esse aplicativo apresenta como principais características, além do suporte, considera as métricas coverage, lift, leverage e strength, define o número máximo de regras a serem geradas, permite definir o número máximo de atributos em LHS, mas gera regras com apenas um item em RHS, possui filtros para regras consideradas improdutivas, triviais e insignificantes, possui recursos, apesar de limitados, para tratar atributos contínuos.

É apresentado exemplo do arquivo de saída gerado pelo aplicativo Magnum Opus e, são apresentadas as regras desse exemplo na sintaxe padrão compacta.

O formato geral da regra para o Magnum Opus, "LHS, RHS, cobertura, (#cobertur, suporte, (#suporte), strength, lift, leverage, (#levarage)" (a representação (#nome medid indica o número de exemplos cobertos pela medid).

A tabela de contingência é calculada a partir dos valores de cobertura, suporte e lift.

Considerando a regra, humidity=normal, play=yes, 0500, 7, 0429, 6, 0857, 133, 01071, 1 e os valores cobertura=0500, suporte=13e lift=14, tem-se a tabela de contingência.

Algumas regras geradas pelo Magnum Opus utilizando o conjunto de dados Weather Nominal.

Regras convertidas para o formato padrão compacto de Regras de Associação.

Uma sintaxe padrão para representar Regras de Associação é importante pois permite que as regras geradas pelos diferentes algoritmos, em diferentes formatos, sejam analisadas e exploradas utilizando um conjunto padrão de métricas, visto que conversores podem ser implementados para colocar as regras em um formato unico.

Nesse sentido, a definição de uma sintaxe padrão para representar Regras de Associação, e a implementação de uma classe em PERL para converter as saídas dos algoritmos de associação Apriori, MineSet TM, WEKA e Magnum Opus para essa sintaxe, viabilizam a implementação de diversas medidas de pós-processamento uma unica vez e a utilização destas com regras geradas pelos diferentes algoritmos.

Considerando as restrições quanto ao tamanho dos arquivos de regras geradas, os requisitos de compreensibilidade do conhecimento pelo usuário e a eficiência no processamento dos arquivos de regras foi necessário adotar duas sintaxes, uma padrão com elevada compreensibilidade, outra compacta com arquivos cerca de 30% menores.

A sintaxe padrão é utilizada para apresentação direta a um usuário (por meio de arquivos no formato texto) e a sintaxe padrão compacta é utilizada para a inserção dos conjuntos de regras nos ambientes RulEE e ARInE.

Por fim, a partir da definição dessas sintaxes foi possível incrementar a potencialidade do Projeto Discover para trabalhar problemas de associação.

A implementação do Ambiente ARInE (Association Rules Interactive Explorer) objetiva, primeiramente, viabilizar a metodologia de análise de Regras de Associação proposta na Seção 6por meio de métodos automatizados e iterativos.

Foi projetado e implementado para disponibilizar um ambiente interativo com o objetivo de apoiar a avaliação de Regras de Associação.

A criação de um ambiente específico para avaliação de Regras de Associação é motivada pelas características diferenciadoras entre esse tipo de regra e outros tipos de regras, a saber, complexidade da regra, atividade de Mineração de Dados e, principalmente volume de conhecimento obtido.

Adicionalmente, o ambiente provê o Projeto Discover de uma ferramenta computacional específica para análise de Regras de Associação.

Nesse sentido, foram verificados inicialmente os requisitos gerais para um Ambiente para Exploração de Regras que nortearam o projeto do Ambiente ARInE.

Considerando que o Ambiente ARInE está inserido no Projeto Discover (descrito na Seção 463, página 86), utiliza algumas funcionalidades do Ambiente RulEE (descrito na Seção 464, página 90) e, também, as peculiaridades das Regras de Associação, foi necessário definir uma representação padronizada para as Regras de Associação.

Desse modo, seguindo os requisitos, as demandas e padrões definidos na Seção 461, página 79 foi implementado o Ambiente ARInE com funcionalidades para análise interativa, iterativa e gráfica de Regras de Associação, fornecendo assim o suporte necessário à execução do Pós-processamento de Regras de Associação segundo a metodologia proposta nesta tese.

Inicialmente o ambiente ARInE foi concebido como módulo computacional de Pós-pro-cessamento do Ambiente RulEE, apresentado na Seção 464.

Entretanto, por limitações de desempenho de vários métodos disponibilizados pelo RulEE, optou-se por desenvolver um ambiente complementar com características e otimizações especificamente elaboradas para tratar Regras de Associação.

É ilustrada a relação ARInE RulEE, com a região sombreada indicando os elementos do RulEE utilizados pelo ARInE.

Do Módulo de Gerenciamento, são utilizados exclusivamente os procedimentos para controle de seção e o direito de acesso aos repositórios pelos usuários.

Do repositório de regras e medidas são utilizados os métodos para recuperação de regras e para cálculo e recuperação dos valores de medidas objetivas.

Relação entre ARInE e RulEE.

Para prover o Ambiente RulEE das funcionalidades necessárias para tratar Regras de Associação e para viabilizar o funcionamento do Ambiente ARInE foram realizadas otimizações em alguns módulos PERL e alterações na estrutura do banco de dados utilizado.

As principais alterações nos métodos PERL foram realizadas nas classes Ruleset, que provê métodos de manipulação dos conjuntos de regras, e Selection para recuperação de regras e medidas armazenadas no repositório, InsertAssociationRules este método é utilizado para inserir novos conjuntos de Regras de Associação no repositório de regras, a partir de um arquivo de regras na sintaxe padrão compacta de Regras de Associação.

O método insere as regras, os valores da tabela de contingência e calcula os valores das medidas objetivas definidas como "pré-calculadas" (na inclusão de uma medida no ambiente deve ser especificado se a medida será pré-calculada ou não).

Neste método foram realizadas alterações que otimizam o uso de memória, permitindo a inserção de grandes conjuntos de regras.

Com relação ao tempo de processamento também foram obtidas melhorias especialmente para conjuntos de regras de tamanho superior à quantidade de memória disponível no computador que está instalado o sistema

Remove este método é utilizado para excluir um conjunto regras.

Originalmente, devido a limitações do gerenciador de banco de dados, eram selecionadas todas as regras do conjunto a ser excluído.

Então para cada regra era executado o método Rule Remove que excluía os valores de medidas associados à respectiva regra e então excluía a regra.

Com esse procedimento, era executado uma instrução SELECT que retornava todas as regras (que eram mantidas na memória durante o processo de exclusão) e duas instruções DELETE para cada regra (uma apaga os valores das medidas e outra apaga a regra, ou seja, para a exclusão de um conjunto de regras com um milhão e meio de regras seria necessária a execução de mais de três milhões de instruções SQL).

Na alteração implementada, é executado uma unica instrução DELETE que exclui todas as regras e os valores das medidas.

Esta alteração viabilizou a exclusão de conjuntos de regras com mais de 20000 regras.

WhereCondition este método constrói uma cláusula WHERE a partir de uma expressão simples.

As modificações realizadas permitiram incluir a avaliação de expressões que contenham valores para atributos.

Anteriormente, não era possível utilizar a expressão atributo=valor_do_atributo A principal alteração efetuada no banco de dados utilizado pelo Ambiente RulEE foi a inclusão de quatro novas tabelas ARINE, ARINE EXCLUDED RULES, ARINE INTER-SECTION RULES e ARINE SAVED SESSION.

O ambiente ARInE utiliza do ambiente RulEE a estrutura para acesso aos repositórios de regras e de medidas.

Diagrama físico da base de dados utilizada pelos ambientes ARInE e RulEE.

A interface do Ambiente ARInE está disponível no endereço 143107231137/arine/ index html.

Cabe lembrar que para completo acesso às funcionalidades do ambiente é necessário que o navegador esteja com a opção Java Script ativada.

Página inicial do Ambiente ARInE.

Inicialmente, tem-se acesso à descrição geral do Ambiente ARInE, do processo de análise disponibilizado e das medidas objetivas possíveis de serem empregadas na análise, por meio dos links introdução, processo de análise e descrição das medidas, respectivamente.

O acesso à essas funcionalidades é realizado sem a necessidade de login no sistema, para as demais funcionalidades do sistema é requisitada a autenticação do usuário, com login e senha.

Após a autenticação do usuário, por meio do link projeto e ruleset, devem ser escolhidos o projeto e o conjunto de regras a serem analisados.

Existe a opção de recuperar uma seção de análise previamente realizada e salva, isso fará com que seja recuperado o conjunto de regras de foco do usuário focus-rulesetapresentado na metodologia.

Nesta página, também é possível visualizar as descrições de cada base de regra para o projeto atual, desde que tenham sido fornecidas pelo usuário que as inseriu no sistema RulEE.

Seleção de projeto e base de regras.

Escolhidos o projeto e o conjunto de regras ou recuperada uma seção de análise, é informado na parte superior da página o projeto e o conjunto de regras "ativo" para análise e a descrição da base de regra aparece em destaque.

A próxima etapa é a definição do focus-ruleset, ilustrada nas Figuras 625 e 626, que consiste de, uma ou mais consultas à base de regras, excluir regras não desejadas e salvar as regras que serão analisadas (focus-ruleset).

Caso tenha sido recuperada uma seção de análise é possível ir diretamente para análise com as medidas objetivas (link medidas) ou, opcionalmente, visualizar e remover as regras do focus-ruleset (link regras selecionadas) e adicionar novas regras ao focus-ruleset, utilizando os mesmos procedimentos utilizados para a sua definição inicial.

A seleção de regras (acessada por meio do link regras da seção Seleção) é realizada, sendo possível escolher as características desejáveis Projeto e base de regras selecionados para as regras.

Essas características estão relacionadas à presença de atributos, valores de atributos ou itens na regra.

Essa presença pode ser considerada em todo o corpo da regra, apenas no antecedente ou somente no consequente.

O exemplo apresentado seleciona regras que possuam os atributos ddpp550 e ddpp240 no antecedente e, no consequente, valores bom ou ótimo para o atributo mgt.

O resultado da consulta é então apresentado, sendo possível salvar ou excluir as regras do processo de análise.

Isso pode ser realizado para todas as regras de uma vez, ou de maneira individualizada, selecionando-se as caixas de verificação ao lado de cada regra.

Ao salvar uma regra, esta é incluída no focus-ruleset, desde que não tenha sido excluída antes.

O processo de consultar e salvar/excluir é repetido até que o conjunto de regras para análise esteja definido e com pelo menos uma regra salva no focus-ruleset.

Caso deseje-se analisar todas as regras de um conjunto de regras, deve-se utilizar a opção selecionar todas, e depois salvar todas.

Essas restrições são adotadas por questões de otimização necessárias à etapa de avaliação gráfica.

Após definido o focus-ruleset pode-se selecionar as medidas para a análise utilizando os métodos disponibilizados.

Essa operação é acessada por meio do link medidas, que dá Construção de consulta para seleção de regras.

As seguintes funcionalidades estão disponíveis nesta opção, selecionar as medidas para análise, utilizando as caixas de verificação, obter descrição detalhada de cada medida, acessando o link associado ao nome da medida, realizar análises de similaridade entre as medidas, considerando os dados de valores das medidas para o focus-ruleset utilizando análise fatorial, definir os intervalos/percentuais para cada tipo de análise.

O resultado obtido utilizando a análise fatorial é apresentado.

Essa definição pode ser feita especificando-se os intervalos individualmente, como o exemplo na coluna Todos, que irá dividir os valores das medidas nos intervalos percentuais de 10, 25, 45, 80, 90.

Um valor unico de variação também pode ser informado, como o exemplo apresentado na coluna Pareto (canto inferior esquerdo da mesma figur, que irá produzir intervalos de 10 em 10% analisar o conjunto de regras utilizando análise de Pareto, dispersão e frequência, conforme descritas na Seção 62).

Resultado da seleção de regras.

Cabe lembrar que esta página é gerada dinamicamente, o que significa que se uma nova medida for inserida no repositório de medidas, a mesma será disponibilizada automaticamente.

Observa-se o resultado da análise fatorial, com as medidas agrupadas conforme os fatores obtidos (cor de fundo), neste caso fatores.

Para as medidas em que não foi possível estabelecer o fator predominante, a cor de fundo permaneceu a mesma.

Com esse agrupamento são criadas duas situações de análise, medidas semelhantes (pertencentes a um mesmo grupo) e medidas distintas (pertencentes a grupos distintos).

Para as medidas semelhantes, que são altamente correlacionadas, pode-se buscar por regras que não sigam este padrão de correlação.

Uma vez que a regra são segue um padrão, pode ser que represente algum conhecimento inovador.

De maneira semelhante esse tipo de análise pode ser feito para as medidas distintas.

Na sequência são apresentadas as telas com exemplos de resultados do uso de cada tipo de análise, dispersão, frequência e Pareto.

A estrutura de apresentação das informações é semelhante para todas as análises.

Na parte superior da página é apresentada uma tabela contendo, na primeira coluna, os intervalos em que os valores das medidas foram divididos e, nas demais colunas, para cada medida, o número e o percentual de regras para cada intervalo.

Ao número e percentual de regras está associado um link para a apresentação das respectivas regras.

Seleção de medidas tela inicial.

Seleção de medidas após análise fatorial.

É apresentado um exemplo de resultado obtido com análise de dispersão.

Como já mencionado, inicialmente é apresentada uma tabela com o número de regras para cada intervalo e medida.

A seguir, para cada medida são apresentadas as informações, valores limites da medida, mínimo e máximo teórico e no conjunto de regras analisados.

Cabe lembrar que caso na definição da medida não tenha sido especificado seus limites teóricos, são utilizados os limites presentes no conjunto de regras, histograma com o número de regras para cada intervalo especificado, gráfico de dispersão, considerando, como limites, os valores da medida presentes no conjunto de regras, gráfico de dispersão, considerando, como limites, os valores teóricos da medida.

Exemplo de resultado empregando análise de dispersão.

É apresentado um exemplo de resultado para a análise de discretização.

Como já mencionado, inicialmente é apresentada uma tabela com o número de regras para cada intervalo e medida.

A tabela apresentada mostra adicionalmente uma coluna com o número de regras, resultado da combinação das medidas.

A seguir, para cada medida são apresentados os histogramas, que permitem identificar rapidamente os intervalos com mais ou com menos regras.

É apresentado um exemplo de resultado para a análise de Pareto.

Da mesma maneira que para o caso da análise de discretização, é apresentada uma tabela com o número de regras para cada intervalo e medida e, o resultado da combinação das Exemplo de resultado empregando análise de discretização.

Neste caso, os intervalos são definidos com a redução do percentual acumulado do valor da medida e o número de regras também é acumulado.

A seguir, são apresentados os gráficos da análise Pareto para cada medida, que permitem identificar a região na qual a contribuição das regras é mais intensa (porção da curva com inclinação mais acentuad).

No gráfico também é indicada, como referência, a linha correspondente ao percentual de 80% relacionado ao valor teórico do Princípio de Pareto.

Ao lado do gráfico é disponibilizado um recurso adicional para exploração interativa de regras, no qual são especificados os limites percentuais superiores e inferiores do intervalo que se deseja observar as regras.

O recurso para visualização das regras para os três tipos de análises (Pareto, dispersão e frequênci é semelhante).

Por meio de links associados a cada número de regras tem-se acesso à página para visualização de regras e avaliação final pelo especialista.

Caso existam regras interessantes, estas podem ser salvas no conjunto de regras interessantes.

As regras interessantes podem ser acessadas pelo link regras interessantes localizada no menu lateral.

Exemplo de resultado empregando análise de Pareto.

Na seção Utilitários do menu lateral estão presentes as seguintes funcionalidades, seção de análise, permite salvar ou recuperar uma seção de análise.

A seção de análise é constituída pelo focus-ruleset e pelo conjunto de regras interessantes.

Conjuntos de regras permite inserir e excluir conjuntos de regras.

Estas funcionalidades são executadas pelo Ambiente RulEE, RulEE GAR, módulo computacional, associado ao Ambiente RulEE, para generalização de Regras de Associação com o uso de taxonomias, descrito brevemente na Seção 465, página 95.

Cabe lembrar que, a estrutura do repositório de regras utilizada, permite ao Ambiente ARInE tratar Regras de Associação generalizadas de modo idêntico às não-generalizadas.

Por fim, o logout do ambiente é realizado por meio do link Finalizar seção.

Com este procedimento é encerrada a seção atual e são apagados arquivos e tabelas temporárias.

Exemplo de visualização de regras para um intervalo de análise.

Neste capítulo foi apresentado o Ambiente ARInE, um Ambiente para Exploração de Regras desenvolvido especificamente para manipular Regras de Associação com o apoio de medidas objetivas.

Além de implementar otimizações na exploração e visualização de regras, é provido pelo ambiente uma estrutura que viabiliza a metodologia de análise e avaliação para Regras de Associação proposta.

A metodologia proposta para analisar regras, permite identificar pequenos grupos de regras, os quais são apresentados a um especialista para validação.

A análise de Pareto é a principal técnica adotada para a identificação destes pequenos grupos.

No sentido de garantir a independência do Ambiente ARInE dos algoritmos para extração de Regras de Associação e, principalmente, a integração com os demais projetos desenvolvidos no LABIC (Discover e RulEE) foi desenvolvida e apresentada uma representação padronizada para Regras de Associação.

Na seção final do capítulo foi descrita a implementação do ARInE, detalhando sua relação com o ambiente RulEE e o uso da metodologia por meio de sua interface Web.

A etapa de Pós-processamento do conhecimento tem despertado interesse como objeto de pesquisa em anos recentes.

Esse interesse é motivado principalmente pela intensificação do uso do conhecimento extraído em aplicações práticas e pelo grande volume desse conhecimento que, em geral, inviabiliza sua análise manual.

No caso da extração de padrões por Regras de Associação, a questão do grande volume de conhecimento gerado é uma problemática ainda mais expressiva, considerando que esta tarefa é conhecida por gerar um elevado número de regras.

Com o avanço da tecnologia de Mineração de Dados e sua utilização para solucionar problemas reais em processos de tomada de decisão, pode-se notar a importância da etapa de Pós-processamento do conhecimento, que tem papel fundamental no sucesso do processo como um todo.

A utilização efetiva do conhecimento descoberto em um processo de apoio à tomada de decisão depende de fatores como a possibilidade do usuário compreender o conhecimento e conseguir identificar, por exemplo, o conjunto de regras mais interessante sob seu ponto de vista.

Assim, a avaliação dos conjuntos de regras quanto a sua qualidade, compreensibilidade, utilidade e grau de interesse é de grande relevância para auxiliar o especialista na utilização do conhecimento descoberto.

Nesse sentido, o desenvolvimento de uma metodologia para identificar conhecimento interessante por meio do pós-processamento de padrões extraídos adquire significado especial, principalmente no caso do uso de Regras de Associação.

Na etapa de Pós-processamento, o emprego de medidas objetivas para a avaliação de regras é tarefa usual, considerando que estas medidas fornecem uma indicação da força de associação (hipotétic entre os lados da regra).

Contudo, de maneira geral, a maioria dos trabalhos limita-se a ordenar regras por uma ou mais medidas e apresentá-las ao especialista para avaliação.

No contexto apresentado, este trabalho investigou a tarefa Regras de Associação no processo de Mineração de Dados com enfoque nos aspectos relacionados ao Pós-processamento do conhecimento adquirido.

Nessa perspectiva, buscou solucionar a problemática da análise do grande número de Regras de Associação.

O objetivo principal desta tese é desenvolver e implementar uma metodologia para o Pós-processamento de Regras de Associação que permita a um especialista "navegar" por conjuntos de regras, concentrando-se em determinados grupos de regras.

Nessa metodologia, pequenos grupos de regras potencialmente interessantes, devem ser apresentados ao usuário para avaliação.

A definição destes grupos de regras interessantes é baseada no uso de medidas objetivas.

Para auxiliar o especialista no uso da metodologia, foi proposto e desenvolvido um ambiente interativo, que viabiliza a exploração de regras seguindo a metodologia definida.

No decorrer do trabalho foi constatado não ser possível definir uma medida que apresente bom desempenho na análise de regras para todas as bases de dados e parâmetros utilizados nos algoritmos de extração de padrões.

Assim, na metodologia proposta neste trabalho, são fornecidos métodos de apoio para seleção da(s) medida(s) mais adequada(s) para cada caso.

Esses métodos podem ser categorizados em, métodos para análise de similaridade entre medidas e métodos para avaliação do desempenho da medida como filtro para seleção de regras.

Métodos para análise de similaridade entre medidas são necessários considerando a variedade de medidas disponíveis para avaliação de regras.

Seria inviável verificar o desempenho de todas as medidas para cada base de regras analisada.

Assim, foi proposto o uso da técnica de análise fatorial para agrupar medidas altamente correlacionadas.

Com o uso dessa técnica é possível analisar apenas o desempenho de uma medida de cada grupo em vez de se analisar todas as medidas.

Para análise do desempenho da medida como filtro, é utilizada a análise de Pareto que permite avaliar com o auxílio de gráficos, o potencial restritivo da medida e estabelecer um valor de "corte" para a seleção de regras usando esta medida.

Métodos complementares para análise de medidas, também apoiados em gráficos, são providos de, gráfico de dispersão, para conhecimento geral da dispersão dos valores e gráfico de barras, para verificação da concentração de regras nas diferentes faixas de valores da medida.

Assim, destacam-se as principais contribuições desta tese, i proposta de uma metodologia para avaliação exploratória e interativa de conhecimento representado por Regras de Associação.

Essa metodologia permite ao usuário "navegar" num conjunto de regras utilizando técnicas baseadas em medidas objetivas para obter pequenos grupos de regras de interesse, caracterização das métricas analisadas, quanto a capacidade para filtrar regras e quanto a manutenção desta capacidade perante diferentes bases de dados e parâmetros utilizados nos algoritmos de extração de padrões.

Definição de uma sintaxe padrão para Regras de Associação incorporada aos ambientes Discover e RulEE.

Disponibilização de análises de regras generalizadas, utilizando interface com o módulo computacional RulEE GAR.

Os conjuntos de regras generalizadas também podem ser analisados por meio da metodologia proposta

Disponibilização de uma biblioteca que transforma as Regras de Associação (geradas pelo algoritmo Apriori e pelos aplicativos MineSet TM, MagnumOpus e WEK para o formato padrão de Regras de Associação do Discover).

Otimização dos métodos de acesso ao repositório de regras do Ambiente RulEE, associados aos três tipos de regras suportadas pelo ambiente (classificação, regressão e associação)

Viabilização de métodos otimizados para Pós-processamento de Regras de Associação no Ambiente Discover

Proposição, desenvolvimento e implementação do Ambiente ARInE, no qual são disponibilizadas as contribuições descritas anteriormente.

Além das contribuições gerais apresentadas, destacam-se as contribuições específicas desta tese para a área de Mineração de Dados.

Uso da análise de Pareto para definir os intervalos de valores de medidas utilizados na seleção de regras.

Esse tipo de análise permite identificar as regras de maior contribuição para a medida.

A identificação é realizada definindo-se um percentual limite de contribuição e esse percentual limite é identificado a partir dos gráficos gerados.

Combinação de medidas para selecionar regras.

A partir da análise de Pareto, são selecionados conjuntos de regras para cada medida.

A intersecção destes conjuntos produz um conjunto de cardinalidade igual ou menor que os conjuntos iniciais, reduzindo assim a quantidade de regras.

Considerando que as regras dos conjuntos iniciais são as mais representativas para uma dada medida, o conjunto resultante contém as regras mais representativas para as medidas que foram combinadas.

A tese aqui concluída sobre Pós-processamento de Regras de Associação é trabalho pioneiro no LABIC e inicia novo foco de pesquisa, que empresta contribuição aos trabalhos de doutorado, por Carvalho, e de mestrado, desenvolvido por Domingues e em desenvolvimento por Pivato, Sinoara, Martins.

Além dos trabalhos em desenvolvimento relacionados e aqui motivados, são apresentadas algumas propostas de trabalhos futuros que visam complementar, incrementar e dar sequência a esta tese.

Realizar validação da metodologia proposta utilizando bases de dados reais e especialistas do domínio.

Verificar se as medidas com Decréscimo Após Patamar podem ser empregadas como filtro utilizando concepção inversa da adotada para medidas com decréscimo inicial acentuado, ou seja, analisar as regras com os menores valores das medidas.

Analisar detalhadamente a classificação automática de medidas por meio de descritores de forma (momento e área da curva, por exemplo), no sentido de reduzir o grau de dependência da supervisão do usuário.

Realizar estudos empíricos para analisar a semelhança de medidas utilizando análise fatorial, com o objetivo de aumentar o grau de automatização das análises.

Implementar sistema de log detalhado no Ambiente ARInE para estabelecer relações entre características das bases de regras analisadas e os procedimentos adotados pelo usuário durante a análise.

Otimizar procedimentos ainda não alterados no Ambiente RulEE e nos repositórios de regras e medidas para tratar de maneira eficiente os conjuntos de Regras de Associação.

Estudar o significado semântico da combinação de medidas.

Implementar outros métodos de agrupamento de medidas.

Verificar empiricamente o resultado da aplicação da análise de Pareto sucessivamente ao mesmo conjunto de regras.

Implementar a análise das regras excluídas pelo usuário durante o processo de avaliação.

Estender as funcionalidades do módulo conversor de formatos para incorporar outros algoritmos implementados pela comunidade científica.

Incluir funcionalidades no Ambiente ARInE de maneira que seja possível realizar o processo de Mineração de Dados para Regras de Associação como um todo, em uma unica interface.

Adicionalmente, pesquisas comparando a restritividade adotada para os filtros e a qualidade das regras geradas (avaliadas por usuários especialistas) e a avaliação da estabilidade das medidas para outras bases de dados, são etapas a serem desenvolvidas para complementar o estudo realizado.

