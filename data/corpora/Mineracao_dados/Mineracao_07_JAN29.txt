Técnicas de Inteligência Artificial I têm se tornado cada vez mais importantes na solução de problemas biológicos.

Nesta dissertação, utilizamos um Algoritmo Genético (AG) na busca de regras de alto nível do tipo IF-THEN.

Este AG foi aplicado na mineração de regras de classificação em uma base de dados de expressão gênica de células cancerígenas (NCI60), advindas de experimentos de microarray.

O objetivo dessa mineração é descobrir relações entre os níveis de expressões gênicas e os nove tipos de classes de câncer analisados neste trabalho.

Atualmente, a bioinformática é imprescindível para a manipulação dos dados biológicos.

Ela pode ser definida como uma modalidade que abrange todos os aspectos de aquisição, processamento, armazenamento, distribuição, análise e interpretação da informação biológica.

Através da combinação de procedimentos e técnicas advindos da matemática, da estatística e da ciência da computação, são elaboradas várias ferramentas que auxiliam a compreender o signicado biológico representado nos dados genômicos.

Uma das áreas em que a aplicação de técnicas computacionais tem se mostrado mais promissora é a Biologia Molecular.

O termo expressão gênica refere-se ao processo em que a informação codificada por um determinado gene é decodificada em uma proteína, manisfestando assim, características particulares àquele gene.

As células e tecidos têm suas funções normais quando os genes são expressos de forma regulada.

A expressão alterada de um gene pode alterar o equilíbrio do organismo, podendo vir a gerar uma doença.

Assim, a seleção de genes relevantes a uma determinada doença torna-se uma tarefa importantíssima, podendo num futuro próximo, ser aplicada no diagnóstico médico.

Na busca destes pequenos conjuntos de genes preditores, técnicas advindas da Inteligência Articial I, tais como, os algoritmos genéticos e as redes neurais artificiais, são cada vez mais empregados, devido a sua capacidade de aprender automaticamente a partir de grandes volumes de dados e produzir hipóteses úteis.

Diferentes técnicas de IA foram aplicadas na análise de dados de expressão gênica, tais como, as redes neurais artificiais e os algoritmos genéticos.

Em todos os projetos citados anteriormente, o objetivo é encontrar conjuntos de genes (clusters) que possam ser utilizados como classificadores contáveis, com uma elevada taxa de classificação e um bom desempenho de generalização.

Dessa forma, os conjuntos minerados podem auxiliar na classificação de novos casos, facilitando o diagnóstico e o tratamento de doenças.

Entretanto, somente encontramos classificadores baseados em regras de alto nível, por exemplo, regras do tipo IF-THEN.

Nos demais, os classificadores obtidos são do tipo caixa-preta, onde a entrada são os dados de expressão de uma determinada amostra de células e a saída é a classe à qual essa amostra provavelmente pertence, podendo esta saída estar associada, por exemplo, a uma classe de doença.

Assim, a partir de um conjunto de dados de milhares de genes chega-se a um pequeno conjunto de poucas dezenas de genes que sejam discriminantes para o problema.

O enfoque desse trabalho foi na busca (mineração) de regras de alto nível, que não só estivessem associadas a cada classe individualmente, reduzindo o problema a poucos genes por classe, mas também associando o nível de expressão gênica a cada gene que compõe a regra.

Acreditamos que esse tipo de informação possa ser de grande utilidade aos especialistas que buscam entender o mecanismo responsável pelas alterações nos padrões de expressão gênica associadas ao aparecimento de determinadas doenças.

Para tal, elaborou-se um algoritmo genético (AG) para a obtenção de regras do tipo IF-THEN a partir de bases de dados de expressões gênicas.

O AG foi fortemente inspirado no modelo proposto para a mineração de regras de classificação.

O ambiente evolutivo implementado foi aplicado na classificação de uma base de dados de expressões gênicas de células cancerígenas, advindas de experimentos de microarray.

Esta base é de domínio público e é conhecida como NCI60.

O principal objetivo do nosso trabalho é a busca das relações entre os níveis de expressões gênicas de nove classes de câncer, mama, sistema nervoso central, cólon, leucemia, melanoma, pulmão, ovário, renal e células reprodutivas.

Na base NCI60 foram obtidas expressões de mais de 8000 genes para 61 amostras de células.

Diversos trabalhos aplicaram diferentes técnicas na busca de conjuntos de genes preditores para esta base.

Nesse trabalho, como ponto de partida, utilizamos quatro conjuntos reduzidos de genes que foram minerados a partir da NCI60, totalizando 55 genes.

Esta dissertação está dividida em 6 capítulos, sendo o primeiro uma introdução sobre o trabalho e os objetivos propostos pelo mesmo.

O segundo capítulo apresenta alguns conceitos sobre biologia molecular, microarrays de DNA, bioinformática, trazendo também a descrição de alguns trabalhos aplicados à base NCI60.

O terceiro capítulo apresenta informações a respeito dos algoritmos genéticos, tais como, visão geral do método, representacão do indivíduo, operadores genéticos, seleção de pais, crossover, reinserção, dentre outros.

Além destes tópicos relatados anteriormente, o capítulo também aborda aplicações de algoritmos genéticos em tarefas de datamining e também na análise de dados advindos de expressão gênica.

O quarto capítulo descreve o ambiente evolutivo utilizado neste trabalho, ajustes que foram necessários para se chegar neste ambiente e as bases de dados que foram investigadas.

O quinto capítulo contempla os resultados obtidos no ambiente evolutivo proposto.

O sexto capítulo apresenta as conclusões do trabalho e as propostas de trabalhos futuros.

Genética é o nome dado ao estudo da hereditariedade, o processo pelo qual as características são passadas dos genitores para a prole de modo que todos os organismos, inclusive os seres humanos, assemelhem-se a seus ancestrais.

O conceito central da genética é que a hereditariedade é controlada por um grande número de fatores, os genes, que são pequenas partículas físicas presentes em todos os organismos vivos.

Os primeiros geneticistas estavam interessados principalmente em como os genes são transmitidos dos genitores à sua prole durante a reprodução e em características variáveis, tais como altura e cor dos olhos.

Durante a década de 1930, a pesquisa tomou novos rumos ao reconhecer que se os genes são entidades físicas, assim como outros componentes da célula, eles devem ser feitos de moléculas e, portanto, deve ser possível estudá-los diretamente por métodos biofísicos e bioquímicos.

Isso levou a um novo ramo da genética, chamado Biologia Molecular, que tinha como um de seus objetivos iniciais a identificação da natureza química do gene.

Este novo enfoque levou a novos conceitos e os biólogos deixaram de considerar os genes simplesmente como unidades de herança, passando a encará-los como unidades de informação biológica, possuindo a quantidade total de informações necessárias para a construção de um exemplo vivo e funcional daquele organismo.

A compreensão científica nos dias de hoje da complexidade e do dinamismo celular apóia-se nos trabalhos de milhares de cientistas nos últimos 150 anos.

Os pesquisadores modernos fundiram conceitos e técnicas experimentais da bioquímica, da genética e da biologia molecular com aqueles da biologia celular clássica para produzirem uma concepção dinâmica da vida celular.

Os conhecimentos sobre as células progridem paralelamente ao aperfeiçoamento dos métodos de investigação.

Inicialmente, o microscópio óptico possibitou o descobrimento das células e a elaboração da teoria de que todos os seres vivos são constituídos por células.

Posteriormente, foram descobertas técnicas citoquímicas que possibitaram a identificação e localização de diversas moléculas constituintes das células.

Com o advento dos microscópios eletrônicos, que têm grande poder de resolução, foram observados pormenores da estrutura celular que não poderiam sequer ser imaginados pelos estudos feitos com os microscópios ópticos.

Com o uso dos microscópios eletrônicos, foram aperfeiçoados métodos para a separação de organelas celulares e para o estudo in vitro de suas moléculas e respectivas funções.

A análise de organelas isoladas em grande quantidade, a cultura de células, a possibilidade de manipular o genoma através da adição ou supressão de genes e o aparecimento de numerosas técnicas de uso comum aos diversos ramos da pesquisa biológica levaram ao surgimento da biologia celular e molecular, que é o estudo integrado das células, através de todo o arsenal técnico disponível.

Um fragmento de DNA pode conter diversos genes.

A propriedade mais importante dos genes está no fato de que eles contêm o código genético para a expressão do mRNA (RNA mensageiro) que será traduzido em proteínas, componentes estes, essenciais a todo ser vivo.

As proteínas são polipeptídeos compostos por conjuntos de aminoácidos.

Estes aminoácidos são representados por trincas (códons) de nucleotídeos (Adenina A, Uracila U, Citosina C e Guanina G) no DNA.

O processo pelo qual as seqüências de nucleotídeos dos genes são interpretados na produção de proteínas é denominado expressão gênica.

Mensurar e analisar informações de expressão gênica é de grande interesse para as Ciências Biológicas.

Esse tipo de análise pode fornecer informações importantes sobre as funções de uma célula, uma vez que as mudanças na fisiologia de um organismo são geralmente acompanhadas por mudanças nos padrões de expressão dos genes.

Uma das técnicas mais difundidas para esta medição são os Microarrays de DNA.

O microarray de DNA é uma metodologia utilizada para comparar a expressão de um grande número de genes simultaneamente.

Essa técnica emprega arranjos (arrays), que contêm um grande número de genes distribuídos por um braço robótico de forma ordenada (spots) sobre placas de vidro.

A quantificação dos níveis de expressão gênica na tecnologia de microarray é baseada em experimentos onde os milhares de clones de cDNA são hibridizados com duas sondas marcadas com diferentes fluorecências (geralmente uma emite cor vermelha (Cye outra verde (Cy).

As sondas podem ser conjuntos de cDNAs gerados a partir de células ou tecidos em duas situações diferentes, que se deseja comparar.

Os resultados são produzidos sob forma de diferentes intensidades de fluorescência que são captadas por microscopia a laser em função dos diferentes níveis de expressão de cada gene.

A imagem dos pontos fluorescentes é processada por meio de métodos computacionais com o objetivo de calcular a intensidade obtida para cada mRNA.

A tecnologia de microarrays não fornece apenas informações sobre a função de genes anônimos mas também constitui uma ferramenta indispensável para estudos globais de expressão gênica, com grande aplicabilidade nos estudos de biologia molecular e fisiologia vegetal.

Como exemplo do resultado obtido por essa técnica, podemos citar a base NCI60 utilizada em nossa mineração de regras.

Para a construção desta base, foram utilizados microarrays de cDNA na busca de 1 Molécula de DNA produzida a partir de um mRNA e, portanto, sem íntrons.

A hibridização de ácidos nucléicos baseia-se na capacidade destas moléculas, quando em cadeias simples, poderem associar com seqüências complementares formando cadeias duplas mais estáveis.

Esquema de microarray de cDNA expressões gênicas de aproximadamente 8000 genes distintos.

Estes genes, oriundos de 61 linhagens celulares, foram classificados em 9 (nove) classes de câncer,mama,sistema nervoso central, cólon, leucemia, melanoma, pulmão, ovário, renal e células reprodutivas.

Os números entre parênteses referem-se ao número utilizado para representar cada classe na base de dados.

O número de ocorrências de cada classe é dado a seguir, mama (7), sistema nervoso central (6), cólom (7), leucemia (6), melanoma (8), pulmão (9), ovário (6), renal (e células reprodutivas (4), totalizando 61 amostras.

Dentre os 8000 genes, 3700 foram previamente caracterizados em proteínas humanas, 1900 advindos de genes homólogos de outros organismos e os 2400 restantes foram identicados somente por EST's (expressed sequences tags).

No trabalho foi realizado um pré-processamento, no qual foram excluídos genes que estavam em spots inválidos, de controle e vazios, levando a 6176 genes.

Para cada array, a expressão gênica de cada spot foi normalizado, subtraindo a média das razões de Cy5/Cydos spots de controle e dividindo este resultado pelo desvio padrão da razão Cy5/Cydos spots de controle.

Finalmente, partindo dos 6176 genes pré-processados, chegaram a um dataset reduzido contendo 1000 genes, os quais, apresentaram os maiores valores de desvio padrão na base NCI60.

Visão geral da base NCI60 reduzida e utilizada nos experimentos.

Estes genes foram indexados de 1 a 1000.

Apresenta uma visão geral da base NCI60 reduzida, composta pela expressão de 1000 genes (colunas), medida para 61 amostras de células (linhas), sendo que cada amostra é classificada em uma das nove classes de câncer citadas anteriormente última colun.

A utilização de técnicas e ferramentas de computação na resolução de problemas da Biologia é chamada de Bioinformática ou Biologia Computacional.

Essa área de pesquisa vem se tornando cada vez mais importante.

A computação pode ser aplicada na resolução de problemas como comparação de sequências (DNA, RNA e proteínas), montagem de fragmentos, reconhecimento de genes, identificação e análise da expressão de genes e determinação da estrutura de proteínas.

O emprego de métodos computacionais na Biologia iniciou-se na década de 80, quando biólogos experimentais, em conjunto com cientistas da computação, físicos e matemáticos, começaram a aplicar esses métodos na modelagem de sistemas biológicos.

Na segunda metade de década de 90, com o surgimento dos seqüenciadores automáticos de DNA, houve uma explosão na quantidade de seqüências a serem armazenadas, exigindo recursos computacionais cada vez mais eficientes.

Além do armazenamento ocorria, paralelamente, a necessidade da análise desses dados, o que tornava indispensável a utilização de plataformas computacionais eficientes para a interpretação dos resultados obtidos.

Assim, a Bioinformática surgiu para tentar dar significado a essa enorme quantidade de dados.

Durante esse período, ferramentas computacionais foram desenvolvidas para análise dos dados, utilizando algoritmos convencionais da Ciência da Computação.

Devido à grande quantidade e a complexidade da informação, as ferramentas baseadas na computação convencional têm se mostrado limitadas na abordagem de problemas biológicos complexos.

Isto vem ocorrendo, entre outras razões, devido à ausência de uma teoria fundamental em nível molecular.

Outra razão para essa dificuldade é a ineficiência das ferramentas convencionais em lidar com grandes quantidades de dados.

Técnicas de Inteligência Articial I, tais como, Algoritmos Genéticos, Redes Neurais artificiais, dentre outros, são assim cada vez mais empregadas para tratar problemas em Biologia Molecular, por sua capacidade de aprender automaticamente a partir de grandes volumes de dados e produzir hipóteses úteis.

Um dos principais exemplos de aplicação de técnicas de bioinformática reside na análise de dados de expressão gênica.

Devido ao avanço das tecnologias utilizadas na obtenção de dados de expressão gênica, o volume desses dados vem aumentando exponencialmente.

Assim, uma das áreas mais proeminentes da Bioinformática nos dias atuais, reside na aplicação de técnicas computacionais para a análise dos dados gerados em experimentos de microarray.

Diferentes técnicas de Inteligência Artificial foram aplicados na análise de dados de expressão gênica, tais como, Redes Neurais artificiais e Algoritmos Genéticos.

Um exemplo de aplicação de diferentes técnicas de bioinformática na análise de dados de expressão gênica é a diversidade de trabalhos envolvendo a base NCI60, desde a sua publicação em 2000, descrevendo os experimentos de microarray.

Utilizou-se a base NCI60 para a comparação de performance entre diferentes métodos de classificação.

Os métodos avaliados incluem os classificadores baseados no vizinho mais próximo (nearest-neighbor), análise de discriminante linear (linear discriminant) e árvores de decisão.

Neste trabalho, das 9 classes existentes na base NCI60 foram utilizadas 8, não inserindo na análise a classe 9 (células reprodutivas).

Foram obtidos conjuntos preditores formados por 30 genes.

Os resultados encontrados para estes conjuntos foram validados utilizando 1/das amostras, isto é, os classificadores foram treinados utilizando 2/de todas as amostras da base.

Os três ambientes foram executados 200 vezes e os resultados apresentam a média do número de erros encontrados nestas 200 execuções.

Para o método que utilizou análise de discriminante linear foram encontrados 9 erros em 21 amostras, isto é, ele classicou corretamente 1amostras (57,14%).

No método baseado em árvores de decisão, foram encontrados 10 erros em 21 amostras, totalizando 52,38% de acertos e os classificadores baseados no vizinho mais próximo erraram 8 amostras em 21 possíveis, totalizando 61,9% de acerto.

Buscou-se identificar pequenos conjuntos de genes a partir de amostras de câncer que possuem duas ou mais classes.

Na busca destes conjuntos, o método NSGA- (Nondominated Sorting Genetic Algorithm) foi aplicado na otimização de classificadores baseados no método WN/OVA.

Neste trabalho, a base NCI60, composta por 61 amostras, foi dividida em dois conjuntos, treinamento, contendo 41 amostras, e teste, contendo 20 amostras.

Foram encontrados conjuntos formados por 1genes que obtiveram 92,68% de acurácia em treinamento e 90% em teste.

Também identicaram conjuntos de genes preditivos a partir da base NCI60, utilizando para isso um AG e um classicador MLHD.

Na busca deste conjunto, os pesquisadores partiram de um fragmento da NCI60 formada por 61 amostras de 1000 genes.

Estas bases foram divididas em dois conjuntos, treinamento e teste, tendo 2/e 1/das 61 amostras, respectivamente.

Neste trabalho foi obtido um conjunto preditivo com 1genes com taxa de erro de 14,63% (6 erros em treinamento) no método leave-one-out cross validation (LOOCV) e 5% (1 erro em teste) utilizando um conjunto de teste independente.

Uma outra análise foi feita neste trabalho retirando-se da função de avaliação a segunda taxa de erro.

Para este ambiente, foi encontrado um conjunto preditivo com 1genes com taxa de erro de 9,76% (erros em treinamento) no método cross validation e 20% (erros em teste) utilizando um conjunto de teste independente.

Utilizou-se algoritmos genéticos (AG) combinado a support vector machines (SVM) na busca de pequenos conjuntos de genes que fossem classificadores contáveis em bases multiclasses.

O AG foi usado como seletor de genes e a SVM na categorização das classes analisadas.

Foi utilizado o método leave-one-out cross-validations (LOOCV) na validação dos resultados, obtendo 88,52% de acertos, considerando a base completa (61 amostras), com um conjunto preditor composto por 40 genes para a base NCI60.

Conjuntos de genes preditores utilizados na classificação da base NCI60.

Antes de executar a classificação, foi feita uma seleção de genes utilizando o software RankGene, onde foram selecionados os top 100 genes.

O ambiente utilizado na busca destes conjuntos é formado por um AG padrão combinado a um classicador k nearest neighbour (KNN).

Devido ao baixo número de amostras da base NCI60, os autores não consideraram adequado dividí-la em treinamento e teste.

A avaliação deste conjunto preditivo foi feita utilizando LOOCV aplicado à base inteira.

O melhor resultado encontrado para a base NCI60 foi 76,23% de acertos e um conjunto preditor de 30 genes.

No trabalho buscou-se a construção de pequenos conjuntos preditores de genes eficazes na classificação multiclasse.

Para tal, eles buscaram identificar conjuntos com o menor número de genes possível e bons níveis de predição.

Neste trabalho foi investigado o uso de algoritmos de random forest na classificação de dados multiclasse advindos de experimentos de microarray.

Este método é formado por conjuntos de árvores de decisão, que segundo os autores, possuem um bom poder de predição em dados com ruído.

Neste trabalho, a base NCI60 foi dividida em dois conjuntos chamados de treinamento e teste.

As taxas de erros obtidas utilizando o método foi de 25,2%.

Esse valor foi comparado com o obtido por outros métodos, também utilizando o método de avaliação bootstrap, nos quais resultados similares foram obtidos.

No trabalho foi utilizado um algoritmo genético combinado com uma função discriminante silhouete statistics (GASS) para seleção gênica e reconhecimento de padrões.

Este AG é utilizado na identificação de um conjunto de características correlatas e então evolui-se este conjunto utilizando silhouette statistics com distâncias métricas distintas para filtrar as características chaves para a classificação.

Na pré-seleção dos genes que seriam analisados, usou-se o método BSS/WSS para rankear os genes que são fortemente correlacionados à uma determinada classe e que não estão correlacionados às outras analisadas.

Bons resultados foram encontrados.

Para a base NCI60 foram obtidos 87,8% de acertos em treinamento e 85% em teste.

Constantemente, o homem tem se servido das características e princípios existentes na natureza para a criação de máquinas, métodos e técnicas.

Alguns exemplos típicos desta inspiração foram as seguintes invenções, aviões baseados nas características de pássaros, submarinos com sistemas de imersão semelhantes ao dos peixes, sonares baseados nos morcegos, dentre vários outros.

Em meados do século XIX, surgiu um dos mais importantes princípios no campo da evolução da vida, a Teoria da Evolução de Darwin, que defende a idéia de que na natureza, os seres vivos com as melhores características tendem a sobreviver frente aos demais.

Baseada nesta teoria, a medicina e suas ciências buscam mapear toda a informação genética humana, relacionando deste modo, cada gene de cada cromossomo às características que eles representam nos indivíduos, hereditárias, físicas e funcionais.

Busca-se assim, elucidar quais genes e características promovem a disparidade entre os indíviduos.

A ciência da computação inspirou-se também nestes princípios para a resolução de outros problemas.

Surgiu então, a técnica de inteligência artificial conhecida por Algoritmo Genético, que teve seu marco inicial no trabalho de John Holland, na década de 60.

Algoritmos Genéticos são métodos computacionais de busca baseados nos mecanismos da evolução natural e na genética, simulando a teoria da seleção natural de Darwin.

Os AGs fazem parte da Computação Evolutiva, área da Inteligência Artificial proveniente da interseção entre a Biologia Evolutiva e a Ciência da Computação, sendo constituída de procedimentos de busca e otimização, em que o espaço de busca das soluções de um problema é explorado a partir de uma amostragem aleatória de seus pontos, utilizando um mecanismo inspirado na evolução biológica.

Estes pontos sofrem operações, análogas às operações genéticas, de forma a guiar a busca para regiões mais promissoras desse espaço de soluções.

Computação Evolutiva, interseção entre a Inteligência Artificial e a Biologia Evolutiva AGs são métodos computacionais de busca, baseados nos mecanismos da evolução natural e na genética natural.

Eles combinam a sobrevivência do melhor adaptado dentre estruturas formadas por sequências de bits, com uma troca de informação aleatória e estruturada para formar um algoritmo computacional com algum faro inovador da busca humana.

Apesar de não serem determinísticos, os Algoritmos Genéticos não são uma simples caminhada aleatória.

Eles exploram eficientemente informações históricas para especular novos pontos de busca com um aumento esperado de performance.

O AG é um algoritmo que manipula, em paralelo, um conjunto de indivíduos (chamado de população), tipicamente constituído por cadeias de símbolos de tamanho, que representam os cromossomos.

A cada indivíduo é associada uma avaliação.

O AG transforma a população corrente em uma nova população usando operações de reprodução e sobrevivência, segundo critérios baseados em uma determinada função de avaliação.

Em AGs, uma população de possíveis soluções para o problema em questão evolui de acordo com operadores probabilísticos concebidos a partir de metáforas biológicas, de modo que haja uma tendência de que, na média, os indivíduos representem soluções cada vez melhores à medida que o processo evolutivo continua.

O ciclo básico de execução de um AG.

Ciclo de Execução Básico de um AG.

Como é possível perceber, os AGs manipulam uma população de indíviduos, sendo que cada indivíduo na população representa uma possível solução para um dado problema.

A cada indivíduo é associado um valor de adaptabilidade, chamado de aptidão.

A tarefa do AG é procurar uma solução ótima para o problema ou uma solução que satisfaça um determinado critério de qualidade.

A cada iteração do AG uma nova geração de indivíduos é criada, usando os princípios Darwianos de reprodução e sobrevivência dos mais aptos, através da aplicação de operações genéticas tais como recombinação (crossover) e mutação.

Vários aspectos do projeto devem ser cuidadosamente analisados e especificados para que se possa trabalhar com AGs eficientemente.

Dentre esses aspectos podemos citar como principais.
Representação do indivíduo (ou codificação do cromossomo).

Definição de uma estratégica para a geração da população inicial.

Definição da função de avaliação ou aptidão (tness function).

Especificação dos operadores genéticos, Operadores de seleção de indivíduos que serão utilizados na reprodução (pais).

Operadores de cruzamento ou crossover.

Operadores de mutação.

Operadores de reinserção da população ao final de cada geração.

Definição de um critério de parada.

Especificação dos parâmetros genéticos, Tamanho da população (Tp).

Taxa de crossover T.

Taxa de mutação (Tm).

Número de gerações (Nger).

Nas seções a seguir, detalharemos alguns desses aspectos.

Os AGs manipulam simultaneamente um conjunto de soluções chamado de população.

Cada elemento desse conjunto de soluções, ou cada ponto no espaço de busca, é denominado indivíduo ou cromossomo.

Cada indivíduo representa uma possível solução do problema que se deseja resolver.

Um indivíduo é normalmente representado por uma cadeia de símbolos, podendo esta cadeia ser estática ou dinâmica.

As cadeias estáticas podem ser representadas por um vetor (ou por um conjunto de vetores), cujos elementos podem ser binários, inteiros ou reais.

As cadeias dinâmicas são geralmente representadas por vetores dinâmicos ou árvores.

As cadeias dinâmicas podem, ao longo da execução do AG, diminuir ou aumentar de tamanho.

O mesmo não ocorre com as cadeias estáticas, onde o tamanho é fixado no início da execução do AG.

Os AGs iniciam a busca da melhor solução a partir de um conjunto inicial de soluções.

Na maioria das aplicações, a geração da população inicial é feita de forma aleatória.

Entretanto, em problemas de difícil convergência, a geração da população pode ser feita de forma tendenciosa, utilizando-se algum conhecimento prévio do problema nesta escolha.

A Aptidão refere-se ao grau de contribuição de uma determinada solução candidata para a convergência do AG na busca da melhor solução dentro do espaço de busca.

Para mensurar esta grandeza utiliza-se uma Função de Avaliação ou Aptidão (Fitness Function), cujo objetivo é estabelecer uma medida de qualidade para cada indivíduo da população.

Por isso, a definição dessa função decorre diretamente da modelagem do problema onde se deseja utilizar o AG.

Segundo estimativas, o cálculo da função de avaliação consome a maior parte do tempo de processamento de um AG, podendo chegar a até 95% deste tempo de processamento.

Devido a este fato, a definição da função de avaliação torna-se um fator crítico e um dos pontos mais importantes no projeto dos AGs.

O princípio básico dos operadores genéticos é transformar a população (conjunto de soluções candidatas) através de sucessivas gerações, realizando a busca pela melhor solução até que seja alcançado um resultado satisfatório.

Os operadores genéticos são necessários para que a população se diversique mas que também mantenha as boas características de adaptação adquiridas pelas gerações anteriores.

Os principais operadores genéticos são, seleção dos pais para a reprodução, cruzamento ou recombinação (crossover), mutação e reinserção da população.

A seleção seleciona quais serão os pais que passarão seu material genético para a próxima geração.

O cruzamento ou crossover cria novos indivíduos que possuem em sua carga genética genes vindos dos pais selecionados.

A mutação altera um indivíduo para produzir uma nova solução, um pouco diferente de outra já existente na população.

De acordo com a teoria de Darwin, o princípio da seleção natural privilegia os indivíduos mais aptos e com maior longevidade e, portanto, com maior probabilidade de reprodução.

Indivíduos com mais descendentes têm mais chance de perpetuarem seus códigos genéticos nas próximas gerações.

A maioria dos métodos de seleção de pais são projetados para escolher preferencialmente indivíduos com maiores valores de aptidão, embora não exclusivamente, a de manter a diversidade da população.

Com base na teoria Darwiniana, foram construídos vários métodos de seleção, dentre os quais podemos citar, truncamento, ranking, roleta, amostragem estocástica, torneio simples e torneio estocástico.

Detalharemos os métodos conhecidos por roleta e torneio estocástico por serem os métodos investigados neste trabalho.

O método de seleção de pais mais clássico é conhecido por método da roleta, onde os indivíduos de uma geração são escolhidos para fazer parte da próxima geração, através de um sorteio de roleta.

Neste método, cada indivíduo da população é representado na roleta proporcionalmente ao seu índice de aptidão.

Assim, aos indivíduos com alta aptidão é dada uma porção maior da roleta, enquanto aos de aptidão mais baixa é dada uma porção relativamente menor.

Finalmente, a roleta é girada um determinado número de vezes, dependendo do número de pais que serão selecionados para o crossover.

Os indivíduos selecionados pela roleta fornecem material genético para a construção de novos indivíduos, chamados filhos.

Exemplo de construção da roleta para seleção dos pais, indicando a distribuição das aptidões relativas para uma população fictícia de indivíduos.

A aptidão do indíviduo 1 é igual a e representa 18,75% da soma de todas as aptidões da população.

A aptidão do indivíduo é igual a 7 (43,75%) e as aptidões dos indivíduos e iguais a (12,5%) e (25%), respectivamente.

Assim, estes percentuais definem as probabilidades de cada indivíduo da população ser sorteado para a formação dos pais para o crossover.

Por exemplo, ao sortearmos um pai para realizar o crossover, qualquer um dos quatro indivíduos pode ser sorteado, mas o indivíduo 2, que é o melhor da população, tem uma probabilidade acima de 40% de ser sorteado.

Por outro lado, o indivíduo 3, que é o pior, tem uma probabilidade de sorteio abaixo de 15%.

No método de seleção torneio estocástico, n indivíduos que irão participar do torneio são selecionados utilizando uma roleta, elaborada da mesma forma que a explicada anteriormente.

Para que se possa montar um torneio estocástico com tour de tamanho (três), por exemplo, teremos que rodar a roleta três vezes, e o vencedor do torneio é aquele indivíduo que tiver a maior aptidão entre os três competidores.

Por exemplo, suponha a mesma população de indivíduos cujas avaliações são retratadas na roleta.

No primeiro torneio, a roleta foi girada vezes e ocorre a disputa entre os indivíduos 1, e 3.

Ao final, temos a vitória do indivíduo por possuir maior valor de aptidão (igual a 7).

No segundo torneio, concorrem os indivíduos 1 e sendo que o indivíduo 1 foi sorteado duas vezes.

Nesse caso, o indivíduo é o vencedor com uma avaliação de 4.

Torneio Estocástico de tamanho 3, empregando a roleta.

Comparando-se os dois métodos, é possível perceber que o torneio estocástico é bem mais seletivo do que a roleta.

Embora todos os indivíduos possam ser sorteados, como pode ser observado no exemplo da população, a probabilidade do indivíduo (o pior da população) ser sorteado é bem menor no torneio estocástico.

Na roleta, basta um único sorteio com probabilidade de 12,5% (casas em 16 possíveis) para que ele seja sorteado.

No torneio estocástico, a única forma do indivíduo ser vencedor é se a roleta for girada vezes e nas três ele for sorteado.

Caso contrário, qualquer outro indivíduo sorteado será o vencedor em relação ao 3.

Assim, a probabilidade do indivíduo ser sorteado por torneio estocástico cai de 12,5% para 0,195%.

Os indivíduos sorteados pelo método de seleção dos pais são recombinados através do operador genético crossover.

O operador de crossover é considerado a característica fundamental dos AGs, simulando a reprodução sexuada na natureza.

Este operador gera novas soluções (filhos) a partir de soluções escolhidas da lista de soluções já existentes (pais).

O operador de crossover possui diferentes variações, muitas delas específicas a um determinado problema.

Alguns exemplos de métodos de crossover são, o crossover simples, o crossover múltiplo e o crossover uniforme.

No crossover simples, ocorre o sorteio de um único ponto de corte no cromossomo.

Dois filhos são gerados, cada um formado com uma parte do material genético de cada progenitor.

O primeiro filho repete os genes do cromossomo do primeiro pai até o ponto de crossover.

A partir deste ponto, ele repete os genes do segundo pai.

O segundo filho repete os genes do segundo pai até o ponto de crossover e a partir deste ponto, ele repete os genes do primeiro pai.

Troca de carga genética em indivíduos binários, através do crossover simples.

O crossover múltiplo segue a mesma idéia do crossover simples.

A diferença está no número de pontos de crossover sorteados.

Enquanto que no crossover simples há apenas um sorteio, no crossover múltiplo há ao menos dois sorteios.

Troca de carga genética com dois pontos de crossover.

O crossover uniforme é um tipo de crossover múltiplo levado ao extremo, onde ao invés de serem sorteados pontos de crossover, sorteia-se uma máscara que possui o mesmo tamanho do cromossomo, que indica qual cromossomo pai fornecerá cada gene do primeiro filho.

O segundo filho é gerado pelo complemento desta máscara.

O crossover uniforme é exemplificado.

Crossover Simples.

Crossover Múltiplo.

Crossover Uniforme.

O operador genético de mutação é aplicado para que seja feita a manutenção da diversidade genética da população, alterando-se arbitrariamente um ou mais genes do cromossomo.

Dessa forma, a mutação fornece meios para a introdução de novos indivíduos na população assegurando que existe a possibilidade de se chegar a qualquer ponto do espaço de busca.

Além disso, ele pode contornar o problema de ótimos locais, alterando levemente a direção da busca.

A operação de mutação muda aleatoriamente a descendência criada pelo crossover.

Este operador é aplicado aos indivíduos com uma probabilidade dada pela taxa de mutação Tm, fornecida como parâmetro de entrada do AG.

Esta taxa de mutação pode ser dada por indivíduo ou por gene.

Os tipos de mutação são diretamente influenciados pela estrutura do indivíduo.

Os tipos mais comuns de mutações são, mutação binária, mutação real e permutação.

A mutação binária é aplicada a cromossomos binários.

Neste operador troca-se um ou mais bits do cromossomo, modicando-o(s) pelo seu complemento binário.

Mutação Binária.

A mutação real altera o valor original contido no gene através do sorteio de um pequeno valor de incremento ou decremento.

Após este sorteio, este valor é incrementado ou decrementado ao valor original.

Neste exemplo foi sorteado um decremento de 0,7.

Mutação Real.

Na permutação, ocorre a troca de lugar entre dois genes ou mais genes.

Mutação Permutação.

O operador genético de reinserção é responsável pela seleção dos indivíduos que farão parte da população de pais para a próxima geração.

Os principais métodos de reinserção são, reinserção pura, reinserção uniforme, elitismo e melhores pais e filhos.

No método de reinserção pura, ocorre a substituição de toda a população antiga pela nova população gerada (filhos).

Na reinserção uniforme, a seleção dos indivíduos é feita utilizando-se algum método de sorteio, como a roleta e o torneio estocástico, aplicado à união da população de pais e filhos.

No método melhores pais e filhos, todos os pais e filhos são colocados numa mesma população e os Tp melhores indivíduos são selecionados para a próxima geração.

A escolha destes Tp melhores indivíduos é feita exclusivamente baseada nas suas aptidões.

O operador de elitismo garante que os n (fornecido como parâmetro de entrada do AG) melhores indivíduos encontrados na geração são passados para a nova população, de forma que as melhores soluções possam sobreviver às sucessivas gerações.

Dependendo das características de cada projeto, os critérios de parada adotados podem variar.

Eles podem estar correlacionados a um determinado número de gerações, se o AG encontrou ou não a solução ótima isso se a mesma for conhecid, perda de diversidade das soluções ou pode estar correlacionda à convergência nas últimas k gerações, isto é, quando não ocorre melhoria na aptidão média e máxima.

Os parâmetros genéticos influenciam diretamente no comportamento dos AGs.

Devido a este fato, devemos estabelecê-los conforme as necessidades do problema em questão e dos recursos disponíveis.

Os principais parâmetros genéticos que devemos ajustar são, tamanho da população, taxa de crossover, taxa de mutação e o número de gerações.

O tamanho da população afeta diretamente o desempenho global e a eficiência dos AGs.

Com uma população pequena o desempenho pode cair, pois deste modo, a população fornece uma pequena cobertura do espaço de busca do problema.

Por outro lado, uma grande população fornece uma cobertura representativa do domínio do problema, além de prevenir convergências prematuras para soluções locais ao invés de globais.

No entanto, para se trabalhar com grandes populações, são necessários maiores recursos computacionais ou que o AG trabalhe por um período de tempo muito maior.

A taxa de crossover representa o número de pais presentes na população atual que serão selecionados para a geração dos indivíduos que irão compor uma nova população.

Quanto maior for esta taxa, mais rapidamente novos indivíduos são introduzidos na população, mas também maior é o custo computacional.

A taxa de mutação representa a probabilidade de cada gene do indivíduo ter o seu valor alterado por outro valor válido.

A taxa de mutação deve ser o suficiente para assegurar a diversidade dos cromossomos na população.

Uma taxa de mutação baixa previne que uma dada população que estagnada em um valor, além de possibilitar que se chegue a qualquer ponto do espaço de busca.

Por outro lado, com uma taxa muito alta, a busca se torna essencialmente aleatória.

O número de gerações corresponde ao número de iterações completas que o AG deverá executar.

O número de gerações deve ser analisado cuidadosamente para que se tenha um melhor aproveitamento das execuções.

O modelo de AG discutido na seção 31 é conhecido por AG Padrão.

Esse modelo é fortemente baseado no modelo original e foi extensamente difundido e investigado nas décadas de 80 e 90.

Recentemente, novos modelos surgiram incorporando características que melhor se adaptavam a algumas classes de problemas.

Podemos citar como exemplos desses novos modelos os AGs Coevolutivos e os AGs Multi-Objetivos.

Visão geral dos AGs Multi-Objetivos.

Data Mining é um conjunto de técnicas e ferramentas aplicado para a descoberta do conhecimento em bases de dados.

A tarefa de classificação é uma das várias estudadas em data mining.

Em essência, o problema consiste em atribuir valores para os registros pertencentes a um pequeno conjunto de classes, e assim, descobrir algum relacionamento entre estes atributos.

Cada registro é composto de um conjunto de atributos preditivos e um atributo objetivo.

Um algoritmo de data mining é aplicado ao conjunto de treinamento, contrapondo-os a uma classe conhecida, na busca de algumas relações entre os atributos preditivos e o atributo objetivo.

Estes relacionamentos são então usados para predizer a classe (o valor do atributo objetivo) de amostras cuja classe é desconhecida.

O conhecimento descoberto pode ser representado na forma de regras de classificação do tipo IF-THEN.

Este tipo de regra se destaca devido ao seu alto nível de entendimento e pela representação do conhecimento simbólico, contribuindo para compreensibilidade das informações descobertas.

As regras descobertas podem ser construídas de acordo com vários critérios, tais como, grau de confiança da predição, taxa de acerto da classificação para amostras de classes desconhecidas, compreensibilidade, dentre outros.

Como exemplo de aplicação de AGs em data mining, podemos citar o trabalho no qual um modelo de AG foi elaborado para a obtenção de regras de classificação em bases de dados clínicos.

Esse AG foi implementado através do GALOPPS 32, ferramenta esta de domínio público que incorpora várias características propostas pelos AGs.

Deste modelo, várias características são importantes elucidar, tais como, codicação do indivíduo, operadores genéticos e função de avaliação.

O indivíduo é composto por n genes e cada gene é dividido em três partes, peso, operador e valor.

Cada gene corresponde a uma condição da parte IF da regra, e o indivíduo corresponde à toda parte conseqüente da regra.

A parte ENTÃO é omitida no indivíduo.

A cada execução do AG, todas as regras são evoluídas para uma mesma parte conseqüente.

Assim, por exemplo, se o atributo objetivo possui 5 valores, o AG deve ser executado pelo menos 5 vezes, a primeira execução para minerar as regras com conseqüente atributo-objetivo, a segunda para atributo objetivo e assim sucessivamente.

Os operadores genéticos de seleção, crossover e reinserção aplicados foram os tradicionais, torneio estocástico, crossover ponto-simples e elitismo respectivamente.

Foram desenvolvidos três operadores de mutação, específicos para os campos de peso, operador e valor.

A função de avaliação avalia a qualidade de cada regra ou indivíduo.

Foi utilizado neste trabalho a função de avaliação empregada, combinando indicadores comumente utilizados em domínios médicos, chamados de sensibilidade e especificidade.

Dados sobre domínios dermatológicos e de câncer de mama compunham as bases de dados.

Os resultados obtidos neste trabalho foram satisfatórios.

Para a base dermatológica foram encontrados regras simples, variando de duas a seis condições na parte IF, resultando em aptidões de treinamento variando entre 85,5% a 100% e aptidões de teste de 78,3% a 100%.

Os resultados obtidos para a base de câncer de mama foram um pouco piores.

Foram obtidas regras com três condições na parte IF com aptidões de treinamento variando de 49,7% a 56,4% e aptidões de teste de 36,5% a 39,3%.

Além deste trabalho, vários outros foram desenvolvidos utilizando-se AGs na solução de tarefas de classificação.

Os AGs também foram utilizados em outras tarefas de datamining, tais como, associação, dentre outros.

Existem duas abordagens na aplicação de AGs, para a obtenção de regras de classificaço chamadas Michigan e Pittsburgh.

A abordagem Michigan emprega uma forma de representação que ficou conhecida como Michigan em referência ao nome da universidade de origem.

Nessa abordagem, a população como um todo é a solução para o problema, isto é uma parte da solução candidata é composta por todas as regras (população).

A abordagem Pittsburgh é proveniente dos trabalhos desenvolvidos por De Jong e Smith na Universidade de Pittsburgh.

Esta abordagem emprega outra abordagem na representação dos indivíduos.

Diferentemente da abordagem Michigan, nessa abordagem cada indivíduo da população representa uma solução do problema.

Dessa forma, a população contém vários conjuntos de regras, sendo que cada indivíduo (conjunto de regras) representa uma solução homogênea do problema.

Comparativamente com a abordagem Michigan, a abordagem Pittsburgh requer um esforço computacional menor para obter a solução, embora o cálculo da aptidão dos indivíduos seja mais complexa que na outra abordagem.

No nosso trabalho, utilizamos a abordagem Pittsburgh, onde cada indivíduo da população é uma regra de alto nível do tipo IF-THEN e esta regra corresponde a uma solução do problema para uma determinada classe de câncer, contida na base de dados avaliada.

Alguns dos principais projetos desenvolvidos aplicando-se AGs na análise dos dados de expressão gênica são revisados a seguir.

Vários deles utilizaram a base de dados NCI60 investigada também nessa dissertação.

No trabalho, o objetivo foi estabelecer pequenos conjuntos de genes preditores que tiveram suas expressões medidas a partir de amostras de câncer que possuem duas ou mais classes.

Neste trabalho, foi utilizado o AG multi-objetivos conhecido por NSGA- na busca de classificadores.

Foram estudadas bases de dados de classificação binária e multiclasse.

Para as bases de classificação binária, um método de classificação por ranking chamado weigthed voting (WV) foi empregado.

Para a classificação multiclasse, além da abordagem WV, foi utilizado o método de classificação one-versus-all OV binary pair-wise.

Cinco bases de dados de domínio público foram analisadas neste trabalho, leukemia, diuse large B-cell lymphoma, sendo as três primeiras de classificação binária e as duas últimas multiclasses.

A base de dados leukemia foi dividida em dois conjuntos de dados, chamados de treinamento e teste, sendo constituídos por 38 amostras e 3amostras, respectivamente.

Para esta base, foi obtido 91,4% de acerto nas 7amostras.

A base de dados diuse large B-cell lymphoma é constituída de 96 amostras sendo sendo 4amostras de lymphoma e 5de outros câncers.

As amostras desta base foram divididas em quantidades iguais em treinamento e teste (50% para cada conjunto) obtendo conjuntos classificadores compostos por 8 genes que classificaram corretamente 100% das amostras em treinamento e 97,91% das amostras em teste.

A base de dados GCM é composta por 198 amostras que foram divididas em 14amostras para treinamento e 5amostras para teste.

Foi obtido um conjunto de 37 genes que obteve 86% de acerto em treinamento e 80% em teste.

A última base de dados analisada foi a NCI60.

Esta base é composta por 61 amostras divididas em 41 amostras de treinamento e 20 amostras de teste, sendo encontrado 92,68% de acurácia em treinamento e 90% em teste.

O trabalho foi fundamental para o desenvolvimento dessa dissertação.

Os autores buscaram identicar um conjunto de genes preditivos em relação a nove classes de câncer, a partir de uma base reduzida da NCI60, contendo as expressões gênicas de 1000 genes.

Foi utilizado como estratégia de classificação um classicador MLHD e um AG que otimiza a entrada do MLHD.

O AG determina automaticamente os membros do grupo de genes preditivos, assim como o tamanho ótimo deste conjunto, usando para isto, um método de classificação de máxima verossimilhança MLH, utilizado na avaliação da afinidade destes genes selecionados.

Neste trabalho foram investigadas as bases GCM.

A partir da NCI60, conjuntos de genes preditores foram gerados, dois deles utilizando o método AG/MLHD investigado no trabalho e dois deles empregando técnicas de ranking para comparação.

Foram encontrados bons resultados para ambas as bases analisadas.

Estas bases foram divididas em dois conjuntos, treinamento e teste, tendo 2/e 1/de todas as amostras de 1000 genes, respectivamente.

Para a base GCM, formada por 198 amostras divididas em 1classes, foram obtidos conjuntos preditivos formados por 3genes e com taxa de erro de 20,14% no método leave-one-out cross validation (LOOCV) e 14,81% utilizando um conjunto de teste independente.

Para a base NCI60, composta de 61 amostras divididas em 9 classes, foi obtido um conjunto preditivo com 1genes com taxa de erro de 14,63% no método LOOCV e 5% utilizando um conjunto de teste independente.

É importante salientar que para se chegar nos resultados apresentados anteriormente, estas duas taxas de erro foram utilizadas na evolução do AG.

Vários pesquisadores questionaram os resultados obtidos com essa aptidão.

Neste cálculo, foi utilizada uma informação vinculada à base de teste (a taxa de erro de teste independente).

Assim, o AG utiliza, de uma certa forma, a base de teste em sua evolução.

Portanto, a base de teste não pode ser considerada "independente"(blind test).

Uma segunda evolução foi realizada sem a inserção da taxa de erro de teste, encontrando um conjunto preditivo com 1genes com taxa de erro de 9,76% no método LOOCV e 20% utilizando um conjunto de teste independente resultado este, inferior ao encontrado com as duas taxas de erro.

O objetivo foi encontrar pequenos conjuntos de genes preditivos que sejam classificadores conáveis em bases multiclasse.

Neste trabalho, foram combinados algoritmos genéticos, usados como seletores de genes, e support vector machines (SVM), na categorização das classes analisadas.

As SVM's necessitam estar integradas a outros algoritmos para proverem classicações multiclasse, tais como one-vs,all ou all-paired (AP).

Neste trabalho foi utilizado o método AP.

O AG foi utilizado para evoluir o ambiente AP-SVM na busca dos melhores classificadores para as bases NCI60.

Para a validação dos resultados encontrados neste trabalho, também foi utilizado o método leave-one-out cross-validation (LOOCV).

Porém, nesse caso, os autores não dividiram a base em treinamento e teste, realizando a validação LOOCV em 100% das amostras.

Bons resultados foram encontrados para ambas as bases.

Para a base NCI60 foi alcançado 88,52% de acertos com um conjunto preditor composto por 40 genes.

Na base Brown os resultados foram um pouco piores, alcançando 81,23% de acerto.

Utilizou-se AGs multi-objetivos na busca de clusters com altos valores de relação intra-class e baixos valores de relação inter-class.

Altos valores intra-class signica alta afinidade entre os genes de um determinado cluster, enquanto que, baixos valores inter-class denota uma independência (ou especificidade) entre estes clusters.

Neste trabalho foram utilizados bases de dados de leveduras e de humam B-cell lymphoma advindos de experimentos de microarray.

Estas bases podem ser encontradas no endereço.

Também foi utilizada neste trabalho, o AG multi-objetivos NSGA- (Nondominated Sorting Genetic Algorithm), que se mostrou efetivo na construção de clusters com qualidade.

A Biclusterização tem sido aplicada em análises de expresão gênica envolvendo dados cancerígenos, sendo utilizada principalmente na identificação de genes correlatos, anotação de funções gênicas e classificação de amostras.

A validação biológica dos genes selecionados nos biclusters foi realizada pelo GO Consortium.

Na busca de conjuntos de genes preditivos e seus respectivos coeficientes de correlação ao câncer de mama, utilizaram uma pequena variação do algoritmo genético padrão na evolução de classificadores simples.

Neste trabalho, há a criação de uma lista de elite dos genes (top genes) construída utilizando-se uma versão de ranking muito parecida com o método threshold number of misclasication score (TNoM).

Após construída esta lista, o AG é utilizado na evolução de classificadores do tipo linear, single-threshold, que selecionam os genes dentre a elite.

A base de dados utilizada era composta por 97 amostras de 5277 genes com apenas duas classes, divididos em 78 amostras para treinamento e 19 amostras para teste.

Nesta classificação binária, foram formados conjuntos de 7 genes que obtiveram bons resultados de classificação.

Nas bases destinadas ao treinamento e teste, foram obtidas regras com 97,4% e 89,5% de acertos, respectivamente.

Em todos os trabalhos citados anteriormente, os AGs foram empregados com o objetivo de ajustar algum outro modelo de classificador.

Por exemplo, o AG seleciona o conjunto de genes que deve ser utilizado como entrada de um classicador MLHD.

Os AGs são utilizados para otimizar as SVMs, que são os classificadores de fato.

Entretanto, todos esses classificadores são do tipo "caixa-preta"e não explicam o conhecimento utilizado na classificação.

Poucos trabalhos foram encontrados nos quais os AGs são empregados para encontrar regras de classificação de alto nível do tipo IF-THEN.

Esses trabalhos são revisados a seguir.

Utilizou-se uma abordagem de aprendizagem supervisionada na predição de processos biológicos advindos de experimentos de microarray, buscando características ou pers de expressão que possam ser discriminantes na formação de regras de decisão.

Foi utilizado o sistema Rosetta, ambiente este, utilizado para data mining e knowledge discovery.

Este ambiente emprega um AG padrão na construção e adaptação dos modelos preditivos.

Cada regra IF-THEN identifica um conjunto mínimo de características discriminantes de uma determinada classe de doença.

O conjunto de regras de todas as classes analisadas constituem um classificador que pode ser aplicado em novas amostras de genes.

Na avaliação destes classificadores é utilizado uma curva chamada receiver operating characteristics RO, contrapondo sensibilidade e especificidade.

A base utilizada neste trabalho foi extraída do The Gene Ontology Consortium 2000, sendo dividida em dois conjuntos, um de treinamento e um de teste, divididos em 27 classes.

Os classificadores evoluídos no conjunto de treinamento foram avaliados em teste utilizando 50-fold cross validation e obtiveram, em média, no melhor resultado, 65% de acertos.

Investigou-se a performance de classificadores baseados em regras fuzzy em cinco bases de dados distintas.

O objetivo dos autores era a geração de regras pequenas e simples, conseguida através de tipos de algoritmos.

Um algoritmo para fazer a categorização dos valores contínuos dos níveis de expressão, e um segundo algoritmo, responsável pela descoberta das regras.

Estes algoritmos combinam discretização fuzzy, responsável pela discretização de valores contínuos em valores tais como, baixo, médio e alto ou benigno e maligno, e operadores fuzzy responsáveis pela geração das regras.

O ambiente é composto principalmente por quatro partes, pré-seleção dos genes, apredizado fuzzy, construção das regras e filtragem destas regras.

Para cada gene selecionado, ocorre a discretização do seu nível de expressão em um dos três valores possíveis (baixo, médio ou alto).

Após feita a discretização de todos os genes selecionados, este conjunto de dados é utilizado na construção das regras.

O último passo consiste em retirar regras redundantes.

Na filtragem das regras foi utilizado o ambiente Rosetta por ser simples e eficiente.

Assim, este conjunto final de regras pode ser utilizado para determinar a classe de qualquer novo ou desconhecido elemento.

Foram utilizadas cinco bases de dados, quatro delas encontradas e a base NCBI-NLM 2004, sendo pré-selecionados 200 genes de cada base de dados, divididos em treinamento e teste.

Neste trabalho foi utilizado a mesma forma de avaliação, a curva ROC, alcançando 99,81% de acertos em treinamento e 96,62% em teste de média para todas as 5 bases, utilizando na validação dos resultados obtidos em teste.

Construiu-se classificadores interpretáveis baseados em regras IF-THEN fuzzy precisas e compactas formadas por um pequeno número de genes relevantes para dados advindos de análises de microarray.

Neste trabalho foi construído um classificador, chamado de iGEC, que busca otimizar três objetivos, precisão máxima de classificação, número mínimo de regras e número mínimo de genes utilizados.

Um dos módulos deste ambiente é uma variação do AG.

Este método, chamado de IGA, é utlizado para resolver eficientemente o ajuste do AG.

Este ambiente foi aplicado em oito bases de dados.

Os dados extraídos contêm níveis de expressão gênica de tumores cerebrais, agrupados em 5 e classes, respectivamente.

Os dados encontrados possuem informações sobre diuse large b-cell lymphomas and follicular lymphoma, agrupados em duas classes.

Foram obtidos níveis de expressão de leucemia, agrupados em classes, de pulmão agrupados em 5 classes, de small, round blue cell tumors of childhood agrupados em classes.

O ambiente conseguiu uma precisão média de classificação de 87,9%, com média de 3,9 regras para cada base, e cada regra formada por 5 genes em média.

Para validação destes resultados, foi utilizado uma validação cruzada com dobras.

Este ambiente se mostrou mais efetivo na classificação do que os classificadores baseados em regras fuzzy existentes e também a outros classificadores não baseados em regras, considerando todos os três objetivos.

Em nenhum dos trabalhos citados anteriormente, que fizeram a busca de regras de classificação de alto nível (IF-THEN), foi utilizada a base NCI60, investigada na presente dissertação.

É muito difícil propôr regras ou critérios na determinação de um conjunto de genes que seja discriminantes no diagnóstico de doenças, especialmente quando as bases de dados estudadas possuem um elevado número de classes, tais como a complexa NCI60.

A base NCI60 é conisderada um desafio para os algoritmos de classificação por suas características peculiares, um número relativamente alto de classes (para um número relativamente baixo de amostras (61), resultando em número baixo de amostras por classe, variando de a 9 amostras por classe.

O ambiente evolutivo implementado neste trabalho foi baseado, principalmente, no trabalho.

Em relação ao trabalho, adaptamos o modelo do AG existente neste trabalho para minerarmos dados advindos de expressão gênica, além de alterarmos os operadores genéticos de crossover e de reinserção.

Em relação ao trabalho, ao invés de fornecermos um conjunto de genes preditores, que funciona como um classicador caixa-preta, construimos regras de classificação do tipo IF-THEN, representando o conhecimento em alto nível.

O modelo do AG empregado no nosso ambiente evolutivo foi adaptado a partir do modelo proposto por se tratar de um ambiente voltado à mineração de regras do tipo IF-THEN.

O AG foi elaborado com o objetivo de obter regras de classificação em bases de dados clínicos de pacientes e suas principais características foram revisadas.

As bases de dados onde o ambiente foi aplicado eram formadas por registros que se caracterizavam por dados do paciente, no caso, a idade e presença da doença em histórico familiar e por dados relacionados a sintomas do paciente.

As características que se relacionavam aos sintomas, que eram a maioria, foram todas discretizadas em, ausente, ocorrência leve, ocorrência moderada e ocorrência severa.

O ambiente evolutivo proposto nesta dissertação foi implementado na linguagem Delphi e precisou ser adaptado para trabalhar com bases de dados de expressão gênica, onde os registros apresentam os níveis de expressão de dezenas ou centenas de genes, que são valores contínuos e com precisão variável (números reais).

Para se chegar no ambiente evolutivo utilizado neste trabalho, partimos dos parâmetros propostos e fomos, experimentalmente, ajustando-os para a nossa aplicação.

Vários aspectos foram abordados, tais como, melhores métodos de seleção e reinserção, tamanho da população, número de gerações, peso, tamanho do tour e precisão número de casas após a vírgul.

A seguir, as principais características de nosso modelo de AG são detalhadas, codificação do indivíduo, operadores genéticos, função de avaliação e parâmetros genéticos.

Antes de prosseguirmos, estabeleceremos a seguinte convenção, tanto no domínio do problema abordado, expressão gênica, como na descrição da técnica utilizada, algoritmo genético (AG), a palavra gene é utilizada, podendo surgir dúvidas em relação ao termo.

Assim, convencionaremos que gene (em itálico) se refere ao gene do indivíduo do AG e gene (sem itálico) se refere ao gene humano.

O indivíduo ou cromossomo do AG proposto é composto por n genes, onde n corresponde ao número de genes encontrados na base de expressão gênica avaliada.

Cada i-ésima posição do indivíduo é subdividida em quatro campos, I (índice), P (peso), O (operador) e V (valor).

Cada gene corresponde a um termo da condição na parte IF da regra e o indivíduo (cromossomo) representa todo o antecedente da regra.

Cromossomo ou Indivíduo.

O campo I corresponde ao índice do gene correspondente na base de expressão gênica utilizada.

O campo P é uma variável do tipo inteira e o seu valor está compreendido entre os valores 0 (zero) e 10 (dez).

É importante dizer que este campo P é o responsável pela inserção ou exclusão de um termo na condição.

Caso este valor seja menor do que um valor limite para este gene (e o gene correspondente na base) não fará parte da regra, caso contrário, o mesmo fará.

Na maioria das execuções do AG, foi utilizado como limite o valor 8 (oito).

Isso signica que uma condição referente ao Gene só estará presente efetivamente na regra se o valor do campo Pi for 8, 9 ou 10.

Para todos os outros valores (0 a 7), a condição não estará presente na regra, independentemente dos valores dos outros campos.

Exemplo de cromossomo.

Por exemplo, consideremos o indivíduo dado, onde todos os outros genes omitidos têm o campo e o indivíduo representa uma regra que pode ter no máximo 20 condições.

Ou seja, o AG é aplicado sobre uma base com níveis de expressão gênica de 20 genes.

O antecedente da regra equivalente a esse indivíduo é dado. Ou seja, apenas o gene e o gene 15, que se referem às expressões dos genes de índice 11 e 289 da base, respectivamente, estão presentes no antecedente.

O conseqüente não é representado explicitamente na regra.

Ao contrário, a cada execução o AG busca regras de classificação para uma classe pré-especificada.

Assim, suponha que o indivíduo represente uma regra de uma execução do AG especificada para a classe 2.

O campo de V é uma variável do tipo ponto flutuante que pode variar entre o menor e o maior valor encontrados na base de expressão gênica avaliada e a precisão (número de casas decimais) utilizada nesse campo é um parâmetro de execução do AG, que se mostrou bastante importante para a convergência de nossos experimentos.

A Aptidão (ou tness) refere-se ao grau de contribuição de uma determinada solução candidata para a convergência do AG, na pesquisa da melhor solução dentro do espaço de busca, avaliando a qualidade de cada regra (indivíduo).

Para o entendimento da FA aqui aplicada, alguns conceitos precisam ser elucidados.

Quando aplicamos uma regra na classificação sobre os dados de uma amostra um registro da base de expressão gênic, quatro diferentes resultados podem ser observados, dependendo da classe predita pela regra e a da verdadeira classe da amostra.

São eles, True Positive (tp), A regra classifica a amostra em uma determinada classe e a amostra de fato pertence a essa classe.

False Positive (fp), A regra classifica a amostra em uma determinada classe, mas a mesma não pertence a essa classe.

True Negative (tn), A regra classifica a amostra como não pertencente a uma determinada classe e a amostra é de fato de outra classe.

False Negative (fn), A regra classifica a amostra como não pertencente a uma determinada classe, mas a amostra pertence à classe em questão.

A FA utiliza dois indicadores comumente utilizados em domínios médicos, chamados de sensibilidade (Se) e especificidade (Sp).

A FA utilizada é definida como o produto destes dois indicadores, como segue abaixo, O objetivo do AG é maximizar ao mesmo tempo e, conseqüentemente, o valor de Aptidão, utilizando-se para isso, as equações.

Em cada execução, o AG trabalha com um problema de classificação de duas classes, isto é, quando regras de uma dada classe C estão sendo mineradas, todas as outras classes são agrupadas em uma segunda classe.

Na seleção dos pais para o crossover, na maioria das execuções do AG, aplicamos o método do torneio estocástico utilizando tour de tamanho (três).

Este método foi revisado na seção 313.

Sobre os pais selecionados, aplicamos crossover múltiplo com dois pontos de corte, gerando dois novos filhos com taxa de crossover de 100%.

Nestes dois filhos gerados, aplicamos o operador de mutação.

Os operadores de mutação utilizados neste trabalho variam com o tipo do gene avaliado e foram aplicados a uma taxa de mutação por gene no valor de 30%.

Para o campo P do gene o novo valor é dado sorteando o incremento ou o decremento de uma unidade do valor corrente.

Mutação aplicada ao campo P onde foi sorteado o incremento de uma unidade ao valor original.

Mutação no campo trocando-se o operador.

Mutação do campo V do gene é feita sorteando-se um incremento ou um decremento de 0,1 no valor corrente.

Foi sorteado o decremento de 0,1, que foi aplicado ao valor original deste campo.

Na composição dos indivíduos que irão participar da próxima geração do AG, selecionamos os melhores pais e filhos.

Mutação aplicada no campo.

Mutação aplicada no campo.

Mutação aplicada no campo.

Neste trabalho, após os ajustes que serão descritos na seção 42, utilizamos uma população formada por 400 indivíduos, taxa de crossover de 100%, taxa de mutação de 30% por gene e executamos o AG por 100 gerações.

Embora essa taxa de mutação não seja usual em trabalhos que envolvem AGs, esse valor foi originalmente utilizado.

Após avaliações experimentais, constatamos a importância de se usar essa taxa relativamente alta para uma boa convergência do AG.

A base de dados NCI60 descrita foi obtida a partir de experimentos de microarray aplicados sobre 61 amostras de células cancerígenas resultando nos níveis de expressão de mais de 8000 genes.

Essa base foi obtida nos experimentos descritos na referência. Aplicou-se alguns procedimentos simples de filtragem, excluindo os genes mais ruidosos, chegando a uma base com a expressão de 1000 genes.

A partir dessa base, diferentes técnicas foram aplicadas para se chegar a conjuntos reduzidos de genes que fossem bons preditores das nove classes de câncer.

Nessa dissertação, utilizamos quatro conjuntos reduzidos de genes.

A preleção gênica é necessária quando se trabalha com dados advindos de experimentos de microarray e diversos outros trabalhos realizam algum tipo de pré-processamento antes de realizar o data mining propriamente dito.

O primeiro conjunto, chamado de G1, foi minerado utilizando-se um AG e um classicador de máxima verossimilhança MLH.

O conjunto AG/MLHD determina automaticamente quais genes farão parte do conjunto preditor.

O melhor conjunto encontrado é formado por 1genes preditivos.

Estes genes são identificados pela sua posição dentro da base de 1000 genes que foi minerada.

O segundo conjunto, chamado de G2, foi minerado utilizando um método B/W (between-group/within-group), onde os genes são rankeados baseados na soma dos quadrados das relações entre between-groups e within-groups.

Após calcular o valor desta relação para cada gene, os mesmos foram rankeados decrescentemente e selecionados os top 20 genes.

São eles, 2, 17, 18, 19, 28, 75, 97, 141, 224, 231, 235, 246, 280, 292, 302, 409, 499, 526, 637 e 843.

O terceiro conjunto, chamado de G3, foi minerado utilizando-se uma adaptação do método, chamada SN/OVA (signal-to-noise/one-vs.all), podendo assim, ser aplicado em cenários multiclasse.

Na formação deste conjunto, para cada classe, um conjunto de genes positivamente correlacionados (altos valores positivos para SN) e outro, formado por genes negativamente correlacionados (pequenos valores negativos para SN) são formados.

Para cada classe foi selecionado o gene que possui o maior valor de relação SN positivo e o gene que possui o menor valor de relação negativa para SN, totalizando 18 genes.

São eles, 2, 2, 41, 63, 97, 229, 379, 456, 475, 485, 525, 531, 637, 721, 786, 870, 890 e 929.

Uma observação importante a ser colocada com relação ao conjunto Brefere-se à presença em duplicidade do gene 2.

Em nossos experimentos retiramos todas as duplicidades existentes, devido a este fato, o conjunto Gé composto por 17 e não por 18 genes.

São eles, 2, 41, 63, 97, 229, 379, 456, 475, 485, 525, 531, 637, 721, 786, 870, 890 e 929.

O método empregado na construção do quarto conjunto, chamado de G4, é uma variação do método empregado na construção do conjunto G1.

Foi empregado um AG em conjunto com um classicador MLHD, utilizando uma função de aptidão simplicada, ignorando uma das duas taxas de erro que compõem a função de aptidão original, utilizada na obtenção do conjunto G1.

O conjunto Gé composto por 1genes, 11, 46, 177, 289, 306, 336, 380, 499, 661, 783, 865 e 950.

O objetivo da obtenção dos conjuntos Ge Gem, gerados a partir de técnicas de ranking, foi de compará-los com os genes preditivos obtidos pela técnica AG/MLHD.

Uma das conclusões do trabalho é que os conjuntos reduzidos por técnicas diferentes não se sobreporam na maioria dos genes.

Entretanto alguns genes aparecem em dois ou mais conjuntos.

A partir da composição dos quatro grupos G1, G2, Ge G4, excluindo-se os genes duplicados, chegou-se a um total de 55 genes distintos, cujos níveis de expressão estão representados.

Realizamos experimentos de mineração utilizando-se sub-conjuntos obtidos a partir desses 55 genes, que serão discutidos.

O objetivo dessa mineração é partir de um conjunto reduzido de genes, construídos a partir de outras técnicas de data mining, e chegar em regras de alto nível, do tipo IF-THEN que não só sejam associadas a cada classe individualmente, reduzindo o problema a poucos genes por classe, mas também associando o nível de expressão gênica a cada gene que compõe a regra.

O ajuste do ambiente foi realizado em três etapas e partiu da configuração dos parâmetros.

Na primeira etapa foram ajustados os operadores genéticos.

A segunda etapa contemplou o ajuste dos parâmetros genéticos e a terceira analisou a precisão do campo do gene.

Escolha dos métodos de seleção e reinserção.

A primeira etapa consistiu em avaliar os métodos de seleção de pais para o crossover e reinserção.

Dentre os métodos de seleção existentes, analisamos o métodos conhecidos como roleta e torneio estocástico, que foram revisados.

Os métodos de reinserção avaliados foram o elitismo e os melhores pais e filhos (steady-state), que também foram revisados.

Para essa avaliação, os valores do tamanho da população e do número de gerações foram fixados.

Avaliamos as seguintes combinações, roleta e elitismo, roleta e melhores pais e filhos, torneio estocástico e elitismo e torneio estocástico e melhores pais e filhos.

Como principal conclusão dessa etapa, temos que os melhores resultados foram encontrados com a combinação torneio estocástico + melhores pais e lhos.

Ajuste dos parâmetros genéticos. A segunda etapa consistiu em ajustar os parâmetros genéticos do AG.

Para este ajuste, foram avaliados os valores 100, 200 e 400 para o tamanho da população (Tp), 50, 100 e 200 para o número de gerações (Nger) e 2, e para o tamanho do tour do torneio estocástico.

Fixamos o método de seleção (torneio estocástico) e o método de reinserção (melhores pais e filhos), ajustados na etapa anterior, e avaliamos os resultados encontrados com a combinação de três valores para Tp (100, 200 e 400), três valores para Nger (50, 100 e 200) e três valores para o tour do método torneio estocástico (2, e 4).

Os melhores resultados foram encontrados.

É importante salientar que mesmo ao aumentar o Nger para 200 gerações, não houve uma melhoria significativa nos valores encontrados que justificasse a opção por este valor, visto que, a escolha levaria a um aumento significativo no tempo de processamento do AG.

Precisão do campo Operador do gene. Na terceira etapa foi utilizado os valores 1, e para o número de casas decimais após a vírgula para o campo do cromossomo.

Fixando o método de seleção (torneio estocástico), o método de reinserção (melhores pais e filhos), fizemos experimentos utilizando 1, e casas após a vírgula no campo.

A convergência para boas regras de classificação foi signicativamente superior utilizando apenas 1 casa após a vírgula.

Após esclarecimentos junto aos especialistas, que confirmaram ser essa precisão ideal para a interpretação das regras obtidas, resolvemos manter a precisão em apenas uma casa decimal.

Após todas as etapas de ajuste, chegamos a um ambiente cuja especificação foi utilizada em todos os experimentos descritos nas próximas seções.

Neste capítulo, serão apresentados os resultados dos principais experimentos conduzidos na mineração de bases de dados extraídas a partir da base NCI60.

Inicialmente, o ambiente evolutivo construído para a mineração das regras foi aplicado sobre quatro bases de dados criadas a partir dos quatro conjuntos de genes obtidos, chamadas nesta dissertação de B1, B2, Be B4.

Os resultados obtidos em casa base individual foram analisados e comparados.

Numa segunda etapa, na tentativa de obter resultados ainda melhores que os obtidos nas bases individuais, novas bases foram criadas a partir das composição a 2, a e complet das bases B1, B2, Be B4.

Os principais resultados obtidos nesta etapa são discutidos.

Finalmente, a seção 5faz uma análise mais detalhada dos melhores resultados obtidos nas duas etapas de experimentos.

Dessa análise, dois conjuntos de regras denominados K1 e Kforam extraídos dentre as melhores regras.

Análises comparativas entre esses conjuntos e os principais classificadores encontrados na literatura para a base NCI60 são apresentados onde é possível constatar que os resultados obtidos nessa dissertação são bastante competitivos com os publicados por outros autores.

Quatro bases de dados reduzidas foram criadas a partir da base de 1000 genes disponibilizada.

Os genes utilizados nessas bases correspondem aos conjuntos reduzidos, que chamamos de G1, G2, Ge G4, resultando nas bases B1, B2, Be B4.

Inicialmente, o AG foi aplicado em cada uma dessas quatro bases individualmente.

A base NCI60 é composta por 61 amostras categorizadas em 9 classes de câncer.

Portanto, o objetivo da mineração é obter regras de classificação para essas nove classes.

A avaliação da qualidade das regras mineradas foi feita inicialmente por classe.

Na avaliação por classe, cada base composta por 61 amostras foi dividida em partições de tamanhos semelhantes, guardando sempre a proporcionalidade entre o número de amostras de cada classe.

Duas partições foram utilizadas em treinamento e a terceira partição, chamada de teste, foi utilizada para a avaliação do nível de generalização das regras obtidas em treinamento.

Isto é, as regras que foram evoluídas pelo AG, usando a junção das partições 1 e 2, posteriormente foram testadas na partição.

O mesmo procedimento foi realizado para as demais combinações, partições 1 e em treinamento e a partição em teste e as partições e em treinamento e a partição 1 em teste.

Cada um desses experimentos foi composto por 50 execuções para cada uma das nove classes possíveis do atributo objetivo.

A avaliação de cada regra obtida é dada pela AptidaoTrein e AptidaoTeste.

Como cada base é formada por 61 amostras, buscou-se manter a proporcionalidade entre as classes em cada partição.

Assim, cada partição possui aproximadamente 20 amostras da base.

A presenta os resultados de AptidaoTrein e AptidaoTeste das melhores regras obtidas para a base B1, a partir de 50 execuções do AG, para cada uma das 9 classes e para cada experimento de teste.

Para a base B1, foram encontrados ótimos resultados (100% em treinamento e em teste) para 5 das 9 classes existentes (2, 3, 4, 5 e 8).

Resultados razoáveis foram encontrados para a classe 9 (100% em treinamento e 89,5% em teste).

Resultados inferiores foram encontrados para as demais classes.

O melhor resultado para a classe 1 foi 97,1% em treinamento e 31,7% em teste e para as classes 6 e 7 foram encontrados 100% em treinamento e 66,7% e 50% em teste, respectivamente.

Assim, obtivemos bons resultados em 6 das 9 classes mineradas.

Para algumas dessas classes (3, 4, 7 e 8), mais de uma regra foi obtida com o maior valor de aptidão.

O mesmo procedimento foi realizado utilizando-se as bases B2, Be B4.

Melhores regras obtidas (considerando-se os experimentos de teste) e os valores de aptidão associados a elas.

Em relação à base B2, foram encontrados ótimos resultados (100% em treinamento e em teste) para das 9 classes existentes (4, 5 e e bons resultados para outras classes (2, 3, 7 e 9).

Para as demais classes, 1 e 6, não foram encontrados resultados satisfatórios.

Assim, obtivemos bons resultados em 7 das 9 classes mineradas.

Em relação à base B3, foram encontrados ótimos resultados (100% em treinamento e em teste) para das 9 classes existentes 2, 4, 5 e e bons resultados para as classes e 8.

Para as demais classes 1, 6 e não foram encontrados resultados satisfatórios.

Assim, obtivemos bons resultados em 6 das 9 classes mineradas.

Para a base B4, foram encontrados ótimos resultados (100% em treinamento e em teste) para das 9 classes existentes 2, e e bons resultados para as classes 4, 8 e 9.

Para as demais classes (1, 6 e 7), foram encontrados resultados insatisfatórios.

Assim, obtivemos bons resultados em 6 das 9 classes mineradas.

Assim, independentemente da base utilizada na mineração, foi possível encontrar regras perfeitas (100% em treinamento e em teste) ou eficazes (acima de 90% de média entre AptidaoTrein e AptidaoTeste) em 6 das 9 classes analisadas, 2, 3, 4, 5, 8 e 9.

Com relação à classe 7, apenas o experimento com a base Bfoi capaz de encontrar uma regra eficaz.

Para as classes 1 e 6, nenhum experimento conseguiu evoluir regras com eficácia razoável.

Uma análise conjunta desses experimentos com as bases individuais foi feita agrupando as melhores regras obtidas para cada classe, independentemente da base utilizada.

A presenta essas regras assim como a AptidaoTrein e AptidaoTeste, representando assim a qualidade de cada regra separadamente.

Com exceção da classe 9, em todas as classes para as quais foi possível encontrar regras perfeitas (100% em treinamento e teste), também foi possível encontrar mais de uma regra.

Uma outra forma de análise foi feita sobre este conjunto de regras, na qual foi elaborado um classificador composto de uma regra de cada classe, para posteriormente, verificarmos sua taxa de acertos na base completa (61 amostras).

Para realizar esta análise, foi necessário selecionar apenas uma regra de cada classe, sendo que o critério adotado para a seleção destas regras foi pegar a primeira ocorrência para cada classe.

De posse das 9 regras, aplicamos estas regras no conjunto de dados compreendido por 1000 genes e 61 amostras da base NCI60.

O conjunto de regras do classificador avaliado é apresentado.

Denominamos esse procedimento de análise AECD (Acerto | Erro Grave | Confusão | Desconhecimento).

Este método consiste em analisar um registro da base de cada vez, correspondente a uma amostra de célula, e este registro pode ser interpretado como acerto, erro grave, confusão ou um desconhecimento, dependendo do resultado de classificação.

Melhores regras encontradas na base de dados B4.

Um acerto ocorre quando somente a regra que possui a mesma classe do registro é disparada.

Por exemplo, se o registro avaliado é da classe 1 somente a regra da classe 1 dispara na classificação deste registro.

Um erro grave ocorre quando a regra correspondente à sua classe não é disparada na classificação do registro e uma outra regra de classe diferente é disparada.

Por exemplo, o registro é da classe 1 e na classificação a regra da classe 1 não dispara enquanto que a regra da classe dispara.

Uma confusão acontece quando o registro é classificado pela regra da sua classe e por uma outra regra de outra classe.

Por exemplo, o registro é da classe 1 e as regras da classe 1 e da classe disparam.

Um desconhecimento ocorre quando nenhuma regra é disparada na classificação do registro, nem da mesma classe e nem de outras classes.

O resultado da análise AECD utilizando as regras. Um classicador da base NCI60 retornou um percentual de acerto de de 90,16% nos 61 registros da base, sendo 55 acertos, nenhum erro grave, confusões e desconhecimentos.

Melhores regras encontradas para o conjunto de bases B1, B2, Be B4.

Conjunto de regras do classificador.

Conforme apresentado na seção anterior, a mineração de regras realizada pelo AG sobre as bases individuais retornou resultados bons para 7 das 9 classes envolvidas na base NCI60.

Entretanto, para as classes 1 e 6 o resultado foi insatisfatório.

Cabe ressaltar que essa mesma dificuldade nas classes 1 e 6 da base NCI60 foi observada.

Assim, partimos para uma nova etapa de experimentos, na qual bases com um número maior de genes foram utilizadas durante a fase de treinamento realizada pelo AG.

Esperávamos ser possível melhorar os resultados para essas duas classes, sem decair a eficácia das outras sete.

Assim, foram realizadas diferentes composições das quatro bases B1 (1genes), B(20 genes), B(17 genes) e B(1genes), associadas a 2, a e a 4, excluindo-se os genes repetidos, gerando outras 11 bases.

São elas, B1 B(3genes), B1 B(29 genes), B1 B(2genes), BB(3genes), BB(31 genes), BB(29 genes), B1 BB(46 genes), B1 BB(41 genes), B1 BB(39 genes), BBB(45 genes) e B1 BBB(55 genes).

Nos experimentos envolvendo as 11 bases compostas, foi utilizado o mesmo procedimento empregado no caso das bases individuais, a base completa foi dividida em partições contendo aproximadamente 1/das amostras.

Depois o AG foi evoluído em três experimentos diferentes.

Os resultados completos desses experimentos são apresentados por partição.

As melhores regras obtidas em cada experimento são apresentadas.

Apresentamos os valores de aptidão de treinamento e de teste para as melhores regras evoluídas em cada experimento, independentemente do experimento de teste (partições) em que foram mineradas.

Reproduzimos os valores das melhores aptidões obtidas nas bases individuais B1, B2, Be B4, para facilitar a comparação com os novos experimentos.

Resultados encontrados para as bases de dados individuais e para todas as composições. Apresenta os resultados obtidos para cada classe analisada, em todos os experimentos.

Foi considerado um resultado satisfatório se foi encontrada uma regra perfeita (100% em treinamento e teste) ou uma regra com pelo menos 90% de treinamento e 85% de teste.

O valor encontrado entre parênteses, refere-se ao número de genes presente nas bases de dados.

O melhor resultado foi obtido na mineração da base composta BB3, na qual foi obtido um resultado insatisfatório apenas para a classe 1.

Em seguida, podemos destacar os resultados das bases B2, B1 B3, BB4, BB4, B1 BB3, B1 BB4, B1 BBe BBB4 que retornaram resultados insatisfatórios apenas para as classes 1 e 6.

Quando comparamos os resultados obtidos pelas bases individuais e os resultados obtidos pelas composições de bases, percebemos que apenas a base BB conseguiu superar os resultados encontrados para as bases individuais, que retornaram resultados insatisfatórios em duas ou três classes.

Um outro ponto a ser destacado, refere-se aos resultados obtidos pela composição das quatro bases (B1 BBB4) que retornou resultados inferiores aos obtidos pelas bases individuais.

Exceto no experimento com essa base "completa", o nosso AG se manteve robusto, não decaindo o desempenho com o aumento de genes nas bases e até superando os resultados obtidos nas bases individuais em algumas das bases analisadas.

Classes que obtiveram ótimos/bons e ruins resultados para todas as bases.

Por outro lado, essa base contempla todos os 55 genes utilizados nos outros 1experimentos.

Portanto, potencialmente, a base B1 BBBcontém todas as informações utilizadas nos outros experimentos.

Assim, o AG não foi capaz de convergir para regras eficazes.

Esse fato pode sinalizar que o ajuste realizado para nosso AG começou a decair o desempenho com o aumento do número de genes manipulados.

Outro fato que corrobora essa observação é que algumas bases compostas por duas individuais B1 Be BB retornaram melhores resultados do que as bases compostas por três individuais.

Assim, observamos que a convergência do AG para regras eficazes começa a decair quando analisamos conjuntos maiores que aproximadamente 40 genes.

A análise AECD também foi aplicada às regras mineradas a partir de cada composição de base.

O melhor resultado foi encontrado para as regras mineradas a partir da base B1 Batingindo 90,16% de acertos, ou seja, o mesmo resultado alcançado pelo conjunto de regras das melhores regras obtidas nas bases individuais.

O segundo melhor resultado foi obtido pelas regras mineradas a partir da base B1 Be a partir da base B1 BB3, atingindo 86,89% de acertos.

Análise AECD para todas as combinações de bases.

Melhores regras, e seus respectivos valores de aptidão, obtidas em todo o conjunto de bases, independentemente das bases utilizadas na mineração e da partição utilizada como teste.

Foi possível encontrar regras perfeitas (100% em treinamento e em teste) ou eficazes (acima de 90% de média entre AptidaoTrein e AptidaoTeste) em oito das nove classes analisadas, 2, 3, 4, 5, 6, 7, 8 e 9.

Apenas a aptidão da melhor regra encontrada para a classe 1 foi abaixo do desejado, 76,65% em média (treinamento e teste).

Quando comparamos este resultado com o encontrado para as bases individuais, constatamos uma melhoria signicativa para a classe 6 e uma melhoria pouco signifcativa para a classe 1.

Na classe 6, o melhor resultado encontrado, utilizando-se apenas uma base individual, foi igual a 83,35% de aptidão em média, resultado este, minerado na base B1.

Com a composição de bases, conseguimos elevar este valor para 95,3%, valor este encontrado na base BB3.

Para a classe 1, a melhoria foi menos significativa, o melhor resultado em base individual foi igual a 70,8% de média, minerada na base B2, e na base composta, foi igual a 76,65% em média B1 B.

Assim, com a composição das bases individuais conseguimos efetivamente melhorar os valores de aptidão somente para a classe 6.

Entretanto, mesmo nas outras classes onde o desempenho já havia sido satisfatório na mineração das bases individuais, foi possível encontrar um número maior de regras perfeitas.

Todas as regras apresentadas anteriormente foram avaliadas segundo o valor de aptidão que elas retornaram no experimento em que foram evoluídas, tanto em treinamento quanto em teste.

Entretanto, essa avaliação é melhor estimada pelo procedimento de validação cruzada que é realizado através da média das melhores regras obtidas nos experimentos com as três partições de teste distintas.

Para cada classe, apresentamos o valor da melhor aptidão, que reproduz os valores fornecidos anteriormente para as melhores regras e a aptidão média nas três partições.

Assim, embora todas as regras tenham sido apresentadas com suas aptidões reais calculadas nos experimentos em que as mesmas foram evoluídas, os valores apresentados como aptidão média, fazem uma melhor estimativa do desempenho das mesmas.

Média nas nove classes para a melhor aptidão e para a aptidão média.

É importante ressaltar que, em treinamento, os valores médios encontrados são os mesmos, tanto para a aptidão média, quanto para a melhor aptidão, mostrando que em qualquer uma das três partições os valores de treinamento obtidos foram bem próximos.

Quando avaliamos os resultados médios nas nove classes em teste, há uma diferença de aproximadamente de 5% entre a média entre os melhores resultados e a aptidão média.

Isso demostra que existe uma queda na eficácia das regras obtidas, em função da partição escolhida, para algumas classes.

Essa queda pode ser percebida principalmente nas classes 6 e 8.

Entretanto, de uma forma geral, podemos dizer que independentemente da partição escolhida, o resultado médio está muito próximo ao resultado obtido na melhor partição.

Resultado do cross validation.

As aptidões obtidas nas melhores regras apresentadas, seja pela aptidão absoluta obtida no experimento em que as mesmas foram evoluídas, seja pela aptidão média nas três partições, nos fornecem avaliações da eficácia dessas regras em relação a cada classe analisada.

Entretanto, para que pudéssemos ter uma avaliação geral do conjunto de regras como um todo, na classificação de todas as amostras da base NCI60, realizamos novamente a análise AECD, onde um conjunto de 9 regras (uma para cada classe) é empregado como um classificador caixa-preta na avaliação das 61 amostras.

Essa avaliação é importante, sobretudo, para compararmos os resultados de classificação das regras com outros classificadores disponíveis na literatura, que não realizam uma avaliação por classe.

Esse conjunto foi obtido selecionando a primeira regra de cada classe, mas outros conjuntos/classificadores poderiam ser elaborados com as demais regras.

Aplicando a análise AECD neste conjunto, foi obtido 86,88% de classificações corretas, sendo 5acertos, nenhum erro grave, 7 confusões e um desconhecimento.

Resultados melhores que os 86,88%, citados anteriormente, já haviam sido obtidos utilizando-se o conjunto classificador, elaborado a partir das regras obtidas nos experimentos com bases individuais e também pelo classificador construído a partir das regras mineradas da base B1 B2, cujo resultado foi apresentado.

Nos dois casos, a análise AECD retornou uma taxa de 90,16% de classificações corretas.

Esse resultado, a princípio, nos pareceu inconsistente.

Como seria possível obter um valor mais baixo na análise AECD com o conjunto das melhores regras, se na seleção dessas regras, todas as outras são consideradas.

Após uma análise, registro a registro, dos erros de classificação, foi possível esclarecer a situação, conforme a explicação a seguir.

A métrica que utilizamos na avaliação das regras por classe, relaciona-se à sensibilidade e especificidade das regras evoluídas, e não simplesmente ao número de acertos da regra, que é a medida efetivamente utilizada na avaliação AECD.

Assim, na análise simples de acertos, um erro de classificação por falso positivo ou por falso negativo não faz diferença.

Por outro lado, na avaliação efetuada pelas equações 41 e 42, um falso negativo tem um peso muito maior no valor de aptidão do que um falso positivo, pois o denominador da sensibilidade, que contém o número de amostras da classe em questão, é tipicamente menor que o denominador da especificidade, que contém o número de amostras de todas as outras classes.

Dessa forma, não necessariamente o mesmo conjunto que retorna os maiores valores de aptidão, segundo a equação 43, retornarão o maior valor na análise de AECD.

Por outro lado, nos outros trabalhos que fifizeram a mineração da base NCI60, a análise é feita puramente em cima do número de acertos.

Assim, realizamos novamente uma busca, considerando-se todas as melhores regras evoluídas em cada execução do AG (para todas as bases e todas as partições de teste) e selecionamos um segundo conjunto de regras, que retornou a melhor análise AECD.

Chamamos esse conjunto de Ke o conjunto anterior, formado pelas melhores regras segundo a aptidão, de K1.

Dois conjuntos, com suas respectivas avaliações de aptidão, além dos erros de classificação por classe.

Valores totais de erros para treinamento e teste.

Regras com os maiores valores de aptidão segundo a equação.

Regras com o maior número de acertos na análise AECD.

Resultados de sensibilidade e especificidade para os conjuntos K1 e K2.

O que difere o conjunto K1 e Ksão as regras utilizadas como classificadores para as classes 1 e 6.

Para a classe 1, apesar do valor de aptidão encontrado para o conjunto K1 (0,53 ser maior do que o encontrado para o conjunto K(0,33, o número de erros encontrados para o primeiro conjunto é maior do que o encontrado para o segundo (5 e 2, respectivamente).

Na composição do valor de aptidão, uma classificação errada encontrada no cálculo da sensiblidade é mais severa do que uma classificação errada encontrada no cálculo da especificidade.

Isso acontece devido ao tamanho do conjunto de amostras utilizado no cálculo.

Para o cálculo, utilizam-se apenas as amostras de uma determinada classe.

Por outro lado, no cálculo de Sp, utilizam-se as demais amostras da base.

Consideremos, por exemplo, o cálculo para a classe 1 da base NCI60.

Para o cálculo, serão avaliados apenas 7 amostras, ao passo que, ao calcular, serão utilizadas as 5amostras restantes da base.

Um erro encontrado no cálculo diminuirá de 1/7 o valor de aptidão, enquanto que um erro encontrado no cálculo, diminuirá de 1/5essa aptidão.

Para a análise AECD, diferentemente do que acontece para o cálculo da aptidão, erros encontrados (falso negativo ou falso positivo) possuem o mesmo peso.

Assim, a regra da classe 1 encontrada no conjunto possui aptidão maior do que a regra encontrada no conjunto, mas possui uma quantidade de erros maior.

Para a classe 6, o número de erros encontrados do conjunto para o conjunto decaiu de uma unidade.

No conjunto, foram encontrados dois erros, um na base de treinamento e outro na base de teste, ambos no cálculo, causando um pequeno decréscimo ao valor da aptidão.

Para o conjunto, foi encontrado apenas um erro na base de teste, mas este erro aconteceu no cálculo, causando um grande decréscimo no valor da aptidão.

A partir dos resultados dos erros absolutos obtidos pelos conjuntos de regras, é possível comparar o desempenho desses classificadores com outros da literatura, que foram elaborados para a base NCI60 e que tiveram sua taxa de acertos divulgada.

Alguns desses trabalhos também fizeram duas partições, uma contendo 2/das amostras utilizadas no treinamento, e outra, contendo 1/das amostras utilizadas no teste.

Estas partições são apresentadas.

Outros trabalhos divulgaram apenas a taxa de acertos em relação à base total e são apresentados.

Sensibilidade e especificidade das regras dos conjuntos K1 e K2.

O resultado obtido é uma média encontrada em 5 experimentos.

O trabalho possui duas ocorrências devido ao uso das duas abordagens nestes trabalhos.

É importante dizer que os resultados obtidos não utilizam métodos tradicionais de teste.

Assim, estes resultados não foram utilizados na análise de treinamento e teste, apenas na análise com base completa.

O símbolo (*) encontrado refere-se à média do número de erros encontrados em cinco execuções do ambiente.

Comparativo dos erros encontrados e de outros trabalhos, utilizando 2/da base em treinamento e 1/em teste.

Comparativo dos erros encontrados e de outros trabalhos, utilizando todas as amostras da base NCI60.

Na análise comparativa considerando-se o número total de erros do conjunto de regras K1, podemos observar que esse conjunto obteve resultados comparáveis com diversos outros classificadores que também fizeram a partição 2/de treinamento e 1/de teste, sendo superado signicativamente apenas pelo classicador de Deb.

Com relação aos classificadores que foram ajustados utilizando-se a base completa, portanto com maiores chances de encontrar um baixo número de erros, mas com um resultado de generalização questionável, também é possível dizer que o conjunto K1 obteve resultados comparáveis aos classificadores.

Com relação ao desempenho de erros na base de teste, o conjunto K1 superou apenas o classificador de Dudoit, sendo superado pelos demais classificadores.

Entretanto, devemos salientar que o conjunto K1 foi obtido considerando-se os valores de sensibilidade e especificidade das regras em suas respectivas classes, tanto na evolução do AG quanto na seleção das melhores regras, sem ser direcionado diretamente à taxa de acertos.

Dessa forma, consideramos bom o desempenho do conjunto K1 uma vez que esse classicador elaborado para a base NCI60, diferentemente dos demais, possui um conhecimento de alto nível e detalhado por classe, sem apresentar um decaimento signicativo de taxa de acerto, em relação aos classificadores tipo caixa-preta, publicados na literatura.

Na análise comparativa considerando-se o conjunto K2, em relação aos classificadores obtidos por meio de particionamento treinamento/teste, é possível observar que ele só é superado pelo classicador de Deb, na taxa de acertos na base de teste, por um lado, mas Ksupera esse mesmo classicador na taxa de acertos total, assim como os classificadores.

Com relação aos classificadores que usaram a base completa, o conjunto Ksó é superado por Lin2, lembrando que o classicador Linfoi obtido usando a base completa, enquanto que cada regra de Kfoi evoluída utilizando-se apenas 2/da base.

Assim, embora na evolução do AG a taxa de acertos não seja utilizada diretamente, a seleção posterior das melhores regras utilizando-se a análise AECD, resultou em um conjunto/classicador competitivo com os demais do tipo caixa-preta, superando a maioria dos resultados publicados.

Concluindo, o conjunto K1 é o que apresenta os resultados individuais por classe mais expressivos considerando-se a sensibilidade e especificidade, com uma razoável taxa de acertos em relação aos classificadores já publicados.

Entretanto, o AG também foi capaz de evoluir regras eficazes em relação à taxa de acertos, sendo possível construir o conjunto K2, que supera a maioria dos classificadores já publicados, em relação à taxa de acertos.

Uma informação importante, é que na constituição dos conjuntos K1 e Knenhum esforço foi gasto na busca de conjuntos que tivessem a mesma perfomance e um número menor de genes, já que o método de seleção adotado foi a de buscar a primeira regra de cada classe.

Assim, pode-se buscar conjuntos que utilizam um menor número de genes e que tenham o mesmo desempenho dos conjuntos K1 e Kapresentados.

Em nossos experimentos, foi possível observar que embora a obtenção de regras com alto índice de treinamento seja relativamente fácil de se conseguir, a qualidade dessas regras é logo diminuida em algumas classes pelo desempenho das mesmas na base de testes.

Acreditamos que tal comportamento possa ser justicado pelo baixo número de amostras por classe, inerente ao problema.

Para compensar essa diculdade, procuramos efetuar um grande número de execuções do AG, para obtenção de um maior número de regras por classe, com alta taxa de desempenho na base de treinamento.

Dessa forma, conseguimos obter regras ecazes em oito das nove classes.

Embora a base NCI60 tenha sido extensivamente investigada, em nenhum dos trabalhos analisados foram encontrados conjuntos de genes preditivos para cada classe, e sim, um único conjunto preditivo para todas as classes.

Selecionar um conjunto de genes relacionados a uma determinada classe de câncer é relevante para o entendimento das interações moleculares (molecular pathways) e também para encontrar novos alvos que sejam úteis no desenvolvimento de novas drogas.

Outro ponto importante refere-se ao fato de nenhum dos trabalhos analisados apresentarem conhecimento de alto nível para a base NCI60.

O nosso trabalho apresenta um conjunto reduzido de genes por classe (variando de a 5 genes), conjunto este, apresentado na forma de regras do tipo IF-THEN, relacionando genes, intervalos de níveis de expressão e sua classe.

Um outro ponto forte do projeto refere-se à avaliação de sensibilidade e especificidade de cada classe, não realizado em nenhum outro trabalho de que tenhamos notíci na base NCI60.

Para a validação nal do ambiente foi utilizado o método de cross validation 2,1, que obteve em média para as 9 classes avaliadas, 99,38% de acertos em treinamento e 88,9% em teste.

Um conjunto formado por representantes das regras que apresentaram o melhor desempenho treinamento/teste, chamado K1, além de apresentar um conhecimento de alto nível e valores aceitáveis de sensibilidade e especificidade, também apresenta um número de acertos total.

Esse conjunto retornou as aptidões médias de 99,38% em treinamento e 94,06% em teste, medidos pela equação 43, que combina a sensibilidade e a especificidade.

Um segundo conjunto, chamado K2, também foi elaborado a partir das melhores regras evoluídas.

Embora o resultado de sensibilidade e especificidade seja inferior ao K1, o conjunto Kpossui uma taxa de acertos total igual a 93,44%, superando diversos métodos publicados e sendo inferior apenas ao resultado obtido por Lin e colaboradores (95,08%).

Entretanto, os autores usaram a base completa na evolução do AG, enquanto as regras do conjunto Kforam evoluídas usando 2/da base (para cada classe) para encontrar esse valor.

Além dos dois conjuntos citados anteriormente, nos quais realizamos uma análise comparativa com os principais classificadores publicados na base NCI60, apresentamos um número maior de regras por classe.

Todas as regras representam o melhor desempenho obtido, com o menor número de genes possível, para cada classe correspondente.

De posse dessas regras, diversos outros conjuntos/classificadores podem ser elaborados e avaliados.

Além disso, essa pluralidade de regras pode fornecer mais informações aos biólogos sobre as relações entre os genes e a existência de genes homólogos (genes distintos que possuem a mesma função).

Por exemplo, poderíamos construir uma regra mais complexa para a classe 4, da seguinte forma, Este tipo de conhecimento pode ser utilizado pelos biólogos para investigar as relações entre os conjuntos de genes {50, 224, 235} e {485, 843} (homólogos) e a leucemia.

Conseguimos delimitar genes relacionados a cada classe de câncer e seus respectivos níveis de expressão.

Desta forma, obtivemos uma associação gene/câncer e gene/gene que esperamos que possa contribuir para o diagnóstico deste tipo de câncer limitando assim o número de genes a serem analisados na busca de novos tratamentos.

Como trabalho futuro, sugerimos a construção de um AG multi-objetivo, que trabalhe com várias métricas de forma simultânea, porém isoladas.

Um resumo dos AGs multi-objetivos é apresentado.

Diferentes métricas podem ser aplicadas como objetivos, tais como, sensibilidade, especificidade, precisão, cobertura, dentre outros.

Uma outra extensão para este trabalho seria a utilização de bases com um número maior de genes.

Por exemplo, na base NCI60, diversos trabalhos divulgaram conjuntos reduzidos de genes, que aplicados a algum modelo de classificador (RNA, SVM, MLHD, dentre outros), retornaram uma taxa de acertos razoável.

Em nosso trabalho, partimos apenas dos genes extraídos no trabalho, chegando a 55 genes na base completa (B1 BBB4).

Os genes extraídos em outros trabalhos poderiam ser incorporados a essa base, aumentando a disponibilidade de informações para o AG evoluir regras eficazes.

Entretanto, antes de mais nada, será necessário realizar experimentos com o objetivo de ajustar o ambiente evolutivo na manipulação de bases com um número maior de genes.

Conforme ressaltamos no capítulo anterior, um resultado que nos chamou a atenção foi obtido na base completa B1 BBBque, embora use todo o potencial de informação das expressões gênicas, retornou resultados inferiores se comparado às evoluções das bases individuais.

Esse resultado mostra que o AG teve dificuldades de convergência para regras eficazes, com o aumento do número de genes.

Acreditamos que tal ajuste tem forte relação com o valor limite do parâmetro peso o valor que decide se uma condição estará presente ou não em uma regra e o tamanho da população quanto maior o tamanho do cromossomo, maior a necessidade de amostragem do espaço de busc.

Experimentos incluindo ruído na base B1 BBB(genes extraídos aleatoriamente dos 1000 genes da base NCI60) podem auxiliar neste ajuste.

Aplicar o ambiente em outras bases de dados públicas de expressão gênica.

Estas bases de dados podem ser binárias ou multiclasse.

A partir da leitura de trabalhos publicados, foi possível observar uma diversidade de métodos aplicados pelos pesquisadores para validação dos seus resultados.

Diversidade essa que prejudica inclusive a comparação do desempenho entre os diversos classificadores.

Propomos como continuidade a esse trabalho, a aplicação de outras estratégias de validaço, tais como, o leave-one-out cross validation, a técnica mais empregada e o bootstrap, uma técnica que vem sendo aplicada nos trabalhos mais recentes e que nos parece contornar melhor os problemas inerentes aos experimentos de microarrays (baixo número de amostras com um elevado número de genes).

Devido a sua forma de trabalhar com uma gama de soluções a cada geração, os AGs são capazes de encontrar várias soluções não dominadas ao longo do processo de otimização.

Essa propriedade aliada a sua adaptabilidade a diferentes tipos de problemas, tornam os AGs importantes ferramentas de otimização multiobjetivo.

Muitos problemas do mundo real envolvem uma otimização simultânea de múltiplos objetivos, isto é, existem vários critérios que devem ser balanceados.

Na otimização de um único objetivo, tenta-se obter o melhor resultado, ou a melhor decisão, o que usualmente é o mínimo ou o máximo global.

No caso de múltiplos objetivos, pode não haver uma melhor solução (ótimo global) com respeito a todos os objetivos.

Em um problema de otimização multi-objetivos, existe um conjunto de soluções que são superiores às demais dentro de um espaço de busca onde todas as possíveis soluções são consideradas.

Esse conjunto de soluções é conhecido como o Ótimo de Pareto ou soluções não dominadas.

O Ótimo de Pareto foi formulado pelo sociólogo e economista Vilfredo Pareto e tornou-se o princípio de otimização quando há a competição de múltiplos objetivos.

A solução ótima de Pareto não é única, mas sim, um conjunto de pontos os quais são considerados igualmente bons em função do vetor objetivo.

Esse espaço pode ser visto como um espaço de busca de soluções, no qual cada objetivo poderia ser aperfeiçoado, mas seria melhorado às custas de pelo menos outro objetivo.

Não dominância versus dominância.

A busca pelo ótimo de Pareto tem sido conhecida como otimização simultânea de múltiplos objetivos.

Uma conceituação alternativa seria pensar que uma solução é a ótima de Pareto se, para um dado conjunto de objetivos, não exista nenhuma outra solução que seja superior a ela, considerando-se todos os objetivos.

Para elucidar este conceito será utilizado como exemplo a compra de um automóvel, onde várias decisões precisam ser tomadas, priorizando custo ou conforto, fatores estes conflitantes.

Exemplo que ilustra várias opções de compra de automóvel (5), considerando o seu custo e conforto.

Neste exemplo, o objetivo é minimizar o custo e maximizar o conforto.

Neste caso, existem cinco opções de compra.

Intuitivamente, descarta-se a solução 1, já que a solução 5 oferece mais conforto pelo mesmo custo.

A solução também é descartada pela mesma razão.

Restam então três boas alternativas de compras, 3, e 5.

Em termos quantitativos nenhuma solução é melhor que a outra, pois o acréscimo no nível de conforto do automóvel traz consigo um aumento no custo do mesmo.

Em raciocínio análogo, ao diminuir o custo do automóvel, diminui-se também o nível de conforto do mesmo.

Em outras palavras, a solução ótima de Pareto seria aquela para a qual não exista outra no espaço de busca que a domine.

Similarmente, uma solução não é considerada uma solução ótima de Pareto se ela for categoricamente dominada por, pelo menos, uma solução do conjunto de candidatas.

Quando consideramos soluções dominadas, devemos pensar que diferentes níveis de dominância são possíveis.

Uma solução dominada será sempre categoricamente inferior às soluções não dominadas do conjunto de soluções do ótimo de Pareto.

Entretanto, uma solução dominada pode também dominar outra solução.

Por exemplo, a solução 1 é dominada pela solução 5 e domina a solução 2.

Esses níveis de dominância permitem caracterizar totalmente o grupo de soluções, separando-as dentro de fronteiras de não dominância, que inclui a fronteira correspondente aos ótimos de Pareto.

Esse aspecto do paradigma é muito importante no projeto de um Algoritmos Genético para Problemas Multi-objetivos (AGMO).

Usando o conceito de não dominância e dominância, a análise de Pareto pode ser simplicada como a busca da não dominância.

A busca consiste em classificar as soluções candidatas em grupos de soluções não dominadas, pois são elas as favoritas.

Dessa forma, a otimização de Pareto pode ser vista como uma otimização clássica, onde a dominância global é o atributo desejado.

Se os pontos não dominados estão em um espaço contínuo, pode-se desenhar uma curva.

Todos os pontos contidos na curva formam a Frente de Pareto ou Fronteira de Pareto.

O AG requer uma informação de avaliação escalar para poder trabalhar.

Isso signica dizer que, para a solução de problemas envolvendo múltiplos critérios, necessitamos escalonar um vetor de objetivos.

Um dos problemas, é que nem sempre é possível derivar um critério global baseado na formulação do problema.

Na ausência de informação, os objetivos tendem a ter uma importância equivalente.

Por outro lado, quando temos uma certa compreensão do problema, podemos combiná-los de acordo com a informação existente, provavelmente atribuindo maior importância a alguns objetivos.

Otimizar uma combinação de objetivos tem a vantagem de produzir apenas uma solução simples, não exigindo uma iteração posterior para a tomada de decisão.

A utilização de AGs como método de otimização permite que uma abordagem efetivamente multi-objetivos, levando-se em consideração os conceitos de dominância e ótimo de Pareto, seja utilizada sem a necessidade de se combinar os objetivos através de pesos de importância relativa.

Nos últimos anos muitos pesquisadores têm modificado as idéias iniciais propostas para tratamento de problemas multi-objetivos.

Podemos citar alguns desses principais métodos, VEGA (Vector Evaluated Genetic Algorithms).

Agregação dos objetivos por pesos variáveis.

MOGA (Multi-objective Optimization Genetic Algorithm).

NPGA (Niched Pareto Genetic Algorithm).

NSGA (Nondominated Sorting Genetic Algorithm).

SPEA (Strength Pareto Evolutionary Algorithm).

PAES (Pareto Archieved Evolution Strategy).

NSGA- (Nondominated Sorting Genetic Algorithm).

PESA (Pareto Enveloped-based Selection Algorithm).

SPEAStrength Pareto Evolutionary Algorithm.

PMOGA (Pareto Multiobjective Genetic Algorithm).

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B2.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B3.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 B2.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 B3.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 B4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base BB3.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base BB4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base BB4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 BB3.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 BB4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 BB4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base BBB4.

Aptidão de treinamento e aptidão de teste das melhores regras evoluídas na base B1 BBB4.

Melhores regras encontradas na base de dados B1 B2.

Melhores regras encontradas na base de dados BB3.

Melhores regras encontradas na base de dados B1 BB3.

Melhores regras encontradas na base de dados BBB4.

Melhores regras encontradas na base de dados B1 BBB4.

Melhores regras encontradas na base de dados B1 B3.

Melhores regras encontradas na base de dados B1 B4.

Melhores regras encontradas na base de dados BB4.

Melhores regras encontradas na base de dados B1 BB4.

Melhores regras encontradas na base de dados B1 BB4.

