Devido à popularização de recursos computacionais e ao avanço científico na área de Computação de Alto Desempenho (HPC High Performance Computing), hoje é possível construir ambientes de computação eficientes e com grande poder computacional a um custo razoável.

Existem diferentes arquiteturas e ambientes que disponilizam computação de alto desempenho sob diferentes abordagens, o que proporciona um vasto leque de possibilidades para executar-se aplicações de alto desempenho.

Tal cenário coloca a área de HPC em evidência, despertando interesse de diversas áreas de conhecimento que damandam alto desempenho para a resolução de seus problemas.

Tecnologias como a de Grades Computacionais, por exemplo, antes restritas a ambientes acadêmicos, estão expandindo fronteiras de atuação, estando cada vez mais presentes em ambientes corporativos.

Devido à essa expansão e ao interesse pela área de HPC, existe uma demanda por ferramentas e tecnologias que facilitem a adaptação em ambientes de HPC para que esses possam suprir as necessidades de aplicações corporativas.

Essas aplicações apresentam caracterísiticas tais como acesso a bases de dados, implantação em multicamadas, interatividade e garantias de execução que ainda são pouco ou não suportadas em ambientes de HPC, especialmente trantando-se de ambientes distribuídos e heterogêneos, por exemplo em Grades Computacionais.

Uma das áreas que buscam nas tecnologias de HPC uma base para fornecimento de poder computacional para a execução de suas aplicações, é a área de Mineração de Dados (Data Mining).

Tarefas de Mineração de Dados, a grosso modo, consistem na aplicação de algoritmos sobre grandes bases de dados para extrair conhecimento dessas.

A execução de tais tarefas demanda grande poder computacional e um volume considerável de transmissão de dados, demandas essas, especialmente no que tange ao fornecimento de alto poder computacional, que são a principal motivacão para a existência de ambientes de computação de alto desempenho e que o Estado da Arte em HPC aprensenta soluções eficazes para o suprimento dessas.

O problema é que embora existam boas técnicas para oferecer poder computacional, carece-se de ferramentas específicas para auxiliar alguns nichos de aplicações a utilizarem esse poder computacional.

Sendo esse o caso, por exemplo, de aplicações para Mineração de Dados.

Acrescentando-se o papel estratégico que aplicações de Mineração de Dados desempenham dentro de organizações corporativas, governamentais ou acadêmicas.
Percebe-se a importância da investigação e desenvolvimento de técnicas e ferramentas que possibilitem que aplicações de Mineração de Dados possam beneficiar-se das vantagens que ambientes de HPC oferecem, possibilitando a melhora no tempo de execução e acurácia dessas aplicações a um custo financeiro relativamente baixo, se comparados com soluções centralizadas, como supercomputadores por exemplo.

Este documento apresenta uma proposta de estudo e pesquisa para dissertação de mestrado que visa criar uma arquitetura para suportar a execução de aplicações de Mineração de Dados em ambientes paralelos e distribuídos.

Primeiramente é apresentada uma breve introdução sobre os ambientes de computação paralela e distribuída que serão abordados na criação da arquitetura (Capítulo 2) e uma conceitualização de mineração de dados e suas principais técinicas e aplicabilidades (Capítulo 3).

Após, apresenta-se a proposta de estudo e pesquisa de dissertação (Capítulo 4)  Neste capítulo apresenta-se uma descrição dos ambientes de computação paralela e distribuída que servirão como plataformas para o desenvolvimento do estudo.

Dá-se ênfase especial a ambientes de baixo custo e alta popularidade.

Serão apresentados os ambientes de grades e clusters computacionais, além dos ambientes formados por computadores de múltiplos núcleos de processamento (multi-core) que recentemente têm-se destacado pela seu alto poder computacional e baixo custo.

Segundo De Rose e Navaux, máquinas SMP (Symmetric Multiprocessors), em português multiprocessadores simétricos são "sistemas constituídos de processadores comerciais, também denominados de prateleirà conectados a uma memória compartilhada, na maioria das vezes através de um barramento de alta velocidade".

Esses sistemas são ditos simétricos pois não existe hierarquia de acesso à memória entre os processadores e, a priori, todos eles têm prioridade igual no escalonamento sob o ponto de vista do sistema operacional.

Essas máquinas existem a mais de uma década e inicialmente tinham um preço relativamente alto, compondo somente um nicho específico de mercado e geralmente compradas para um fim específico.

A partir do início desta década, surge a tecnologia de processadores de múltiplos núcleos (multi-cores) que consiste na disposição de mais de um núcleo de processamento em um único processador, compartilhando os mesmos barramentos e memória, por isso considerados máquinas SMP.

O único diferencial dessas máquinas em relação às máquinas SMP tradicionais, diz respeito ao fato de que as arquiteturas multi-cores geralmente compartilham a memória cache entre suas unidades de processamento, enquanto que em arquiteturas SMP tradicionais, geralmente cada unidade de processamento tem uma memória cache individual.

Tal avanço resultou numa expansão do poder computacional de computadores de baixo custo, uma vez que, ao contrário de tempos atrás, hoje máquinas SMP estão presentes até mesmo em computadores pessoais.

Isso proporciona um novo cenário para aplicações paralelas, uma vez que recursos antes escassos estão presentes em todas as escalas da organização, despertando novos desafios de como criar ferramentas de desenvolvimento e execução de aplicações para que essas possam aproveitar as caracterísiticas dessas máquinas.

Os principais desafios, no que tange a arquiteturas multi-cores, dizem respeito à criação de aplicações que possam utilizar-se de todos os núcleos presentes na máquina e que levem em conta que esses núcleos possuem memórias e caches compartilhados.
E, quando essas máquinas estão presentes num cluster de computadores, demandam soluções de escalonamento híbridas, que sejam capazes de gerenciar tanto a caracterísitica de memória distribuída (entre os nodos do cluster) e de memória compartilhada (entre os núcleos de processamento de cada nodo do cluster).

Clusters de computadores podem ser definidos como um conjunto de computadores interligados por uma rede de comunicação em um único domínio administrativo.

A principal motivação que levou a criação desses ambientes, foi o alto custo de supercomputadores e máquinas SMP e também a baixa escalabilidade de máquinas SMP.

Para contornar esses obstáculos, cria-se uma rede de interconexão e um único domínio de administração para gerenciar usuários, armazenamento de dados e rede e, conecta-se computadores nessa rede e domínio, de modo que esses possam trabalhar em conjunto para a execução de aplicações especialmente preparadas para esses ambientes.

De acordo com a classificação comercial de máquinas paralelas, pode-se dividir clusters de computadores em três classes, Máquinas Maciçamente Paralelas (MPP Massively Parallel Processors), Redes de Estações de Trabalho (NOW Networks of Workstations) e Máquinas Agregadas (COW Cluster of Workstations).

Segue uma breve descrição de cada um desses ambientes, Clusters MPP São máquinas constituídas por uma grande escala de processadores comerciais conectados por uma rede de alta velocidade e baixa latência, de dedicação exclusiva para a execução de aplicações paralelas e colocados num ambiente especialmente preparado em termos de fornecimento de energia e refrigeração.

São geralmente soluções comerciais vendidas em blocos já preparados em uma arquitetura diferenciada, de modo a melhorar a comunicação e interligação entre os nodos do cluster.

Essas máquinas são utilizadas para aplicações de alto desempenho e que demandam larga escala e intercomunicação de baixa latência e alta disponibilidade.

São máquinas geralmente caras e com mecanismos especiais para armazenamento de dados, segurança e redundância de componentes.

Clusters NOW Este tipo de cluster é composto por máquinas de trabalho já presentes em uma rede comum.

São utilizados mecanismos que detectam a ociosidade dessas máquinas e, quando isso ocorre, assignam trabalho para as mesmas.

As principais caracterísiticas desses ambientes é o baixo custo para construí-los que muitas vezes é nulo, pois utiliza-se estruturas já existentes e, a baixa velocidade e estabilidade da rede de interconexão das máquinas do cluster, já que essas ficam à mercê da qualidade e topologia da rede a qual os nodos do cluster já estão conectados.

O uso mais comum dessas máquinas é em soluções de baixo custo, onde se visa principalmente o aproveitamento de recursos ociosos do que o alto desempenho das aplicações propriamente dito, embora que, dependendo do cenário e aplicação, possa-se conseguir um bom desempenho de aplicações nesse tipo de cluster.

Cluster COW Os clusters COW são uma evolução das máquinas NOW, com a diferença de que se utiliza computadores comuns, ditos "de prateleira"para uso exclusivo de aplicações paralelas.

Difere-se também de NOW no fato de que todo o ambiente de software e infraestrutura de hardware é projetado especialmente para propiciar o bom funcionamento do cluster na execução de aplicações paralelas.

As vantagens desse ambiente é a possibilidade de construção de um ambiente de relativo alto desempenho e alta escalabilidade comparados com a cara solução de clusters MPPs.

Como o ambiente é concebido exclusivamente para aplicações paralelas, pode-se aumentar a eficiência do mesmo através da implantação de uma rede de alto desempenho, facilitando aplicações de troca de mensagens a terem um bom desempenho.

Comumente essa máquinas são utilizadas por aplicações paralelas que demandam alto desempenho e um grau de confiança maior do que o ambiente NOW pode oferecer, mas não têm recursos financeiros suficientes para implementar uma soluções MPP.

São ideais para aplicações de troca de mensagens e com alta escalabilidade.

O crescimento da internet, acompanhado do crescente desenvolvimento tecnológico dos computadores e da disponibilidade de redes de alta velocidade a custos relativamente baixos, possibilitou o uso de recursos computacionais distribuídos como uma única abstração.

Essa abstração, que permite que diversos recursos computacionais sejam acessíveis como se fossem um, é chamada de Grade computacional  ou ainda Computação em Grade, ambos termos derivados de Grid Computing.

O uso do termo Grid vem da área de engenharia elétrica, onde ocorre o uso de recursos distribuídos heterogêneos (energia elétrica proveniente de diversos geradores geograficamente distribuídos e de diferentes capacidades) de forma trasparente, em termos de localização e acesso.

Essa definição, incialmente, dizia respeito ao uso de diversos recursos computacionais de forma cooperativa para prover poder computacional para problemas de larga escala, instrinsicamente ligados à computação de alto desempenho.

Esse conceito foi se refinando, tornando-se cada vez mais genérico, englobando utilizações além do uso para somente processamento de alto desempenho, ficando definida uma grade computacional como uma infraestrutura para compartilhamento de recursos para a solução de problemas de forma colaborativa.

Atualmente, a definição aceita para Grades classifica-as como infraestruturas para encapsulamento e virtualização de recursos, de modo que esses possam ser acessados de maneira transparente por seus usuários.

Com essa definição mais ampla, abra-se o leque de utilização de grades computacionais, estendendo seu uso para recursos que vão além do compartilhamento de ciclos de processamento, como armazenamento e compartilhamento de dados e, mais recentemente, para o provimento de serviços com a arquitetura OGSA (Open Grid Services Architecture).

Segundo Nemeth  uma Grade deve possuir algumas características.

Entre elas destacam-se, escalabilidade, heterogeneidade, compartilhamento, controle distribuído, repositório virtual de recursos, dinamicidade, transparência, segurança, economia, imagem de sistema, tolerância a falhas, aplicações, escalonamento de recursos e heurísticas de escalonamento.

Dentre as características citadas, destacam-se a heterogeneidade e transparência de uma Grade.

A maioria das grades existentes possuem recursos fisicamente distribuídos e de diferentes configurações.

Por exemplo, uma mesma grade pode encapsular computadores com processadores e arquiteturas diferenciados uns dos outros.

Esses podem estar sobre redes diversas e possuírem outras atribuições na organização, ou seja, estarem compartilhados para outras aplicações ou usuários.

Essa característica, que por um lado impulsiona o desenvolvimento de Grades computacionais, pois permite que recursos heterogêneos que são utilizados para outras tarefas possam ser compartilhados numa Grade, agregando recursos a mesma,concomitantemente, gera uma série de fatores e situações que precisam ser tratados.

Por exemplo, como disponibilidade de nodos, instabilidade de recursos e escalonamento de tarefas a serem executadas numa Grade.

O requisito de transparência de uma Grade também deve ser destacado, pois é fundamental para permitir que usuários acessem os recursos da Grade sem precisar conhecer sua estrutura interna e também, para que desenvolvedores possam construir soluções escaláveis que possam ser executadas em diversas implementações de Grades.

Do ponto de vista dos elementos que compõem uma Grade, pode-se dividi-los em três camadas, infraestrutura, middleware da grade e aplicações da grade.

A camada de infraestrutura diz respeito ao conjunto de dispositivos de software e hardware individuais que integram uma grade.

A camada do middleware da grade tem a função de agregar, gerenciar e disponibilizar os recursos da infraestrutura da grade de forma transparente aos seus usuários ou seja, é ponto central de uma Grade, responsável por intermediar as camadas de infraestrutura e aplicação.

A camada de aplicação da grade é composta por aplicações projetadas de forma que possam aproveitar os recursos oferecidos pelos middlewares de grades.

Aplicações de Grades são geralmente formadas por um conjunto de tarefas (tasks), pertencentes a um trabalho (job).

A  Estrutura de uma Grade Computacional.

Existem diversos tipos ou classes de aplicações para ambientes paralelos.

Segue as classes mais comuns.

Parameter Sweep, aplicações denominadas PS (Parameter Sweep) ou de troca de parâmetros são compostas por diversas instâncias de um único programa (algoritmo), onde cada instância deste é executada com diferentes parâmetros.

Geralmente, apresentam-se como uma subclasse de Bag of Tasks.

Bag of Tasks (BoT), aplicações assim denominadas, "saco de tarefas", são aplicações nas quais não existe nenhuma dependência entre as tarefas que a compõe, não havendo relação de precedência entre elas.

Workflow, são aplicações de fluxo de execuções, nas quais existe uma certa dependência na ordem de execução das mesmas, sendo esta relação uma dependência de dados ou de controle entre as tarefas.

Troca de mensagens, são aplicações que utilizam-se de mecanismos de troca de mensagens para trocar dados durante a sua execução.

Embora esse tipo de programação possa ser utilizado para implementar aplicações PS e BoT, geralmente é utilizado para aplicações que apresentam dependência de dados entre as tarefas que estão sendo executadas.

Geralmente são implementadas através das tecnologias de sockets e da biblioteca MPI.

Pode-se definir mineração de dados como sendo "um processo de descoberta de novas correlações, padrões e tendências significativas através da examinação de grandes quantidades de dados armazenados em repositórios através do uso de tecnologias de reconhecimento de padrões, de estatística e de matemática".

As técnicas de descoberta de conhecimento surgiram devido à necessidade que empresas, governos e instituições acadêmicas têm de analisar o comportamento de seus processos de negócios e buscar conhecimentos, padrões e caracterísiticas até então desconhecidos de modo que esse conhecimento possa ser aplicado para melhorar o funcionamento dessas instituições, principalmente no que tange ao auxílio para tomadas de decisões.

Outro fator determinante que possibilitou a criação de mecanismos de mineração de dados, foi a informatização dos processos das organizações.

Essa informatização faz com que todos os processos que ocorrem dentro de uma organização sejam devidamente armazenado numa base de dados.

Sem essa base de dados, o processo de mineração de dados, da forma como conhecemos hoje, seria inviável.

Embora as técnicas para mineração de dados constituem na aplicação de algoritmos sobre bases dados, salienta-se que essas bases devem estar previamente preparadas e corretamente populadas.

Técnicas de mineração de dados só obtém sucesso em suas aplicações quando todo o sistema de informação da organização estiver consolidado de maneira a garantir a integridade e acuracidade dos dados armazenados em suas bases.

Portanto, pode-se considerar as tarefas de mineração de dados como sendo um processo que passa por fases que vão desde a correta estruturação do sistema de informação da organização, alimentação de bases de dados e definição de objetivos de negócios a serem alcançados no processo, até as fases de preparação, transformação e, efetivamente, a mineração dos dados.

O processo termina com a análise dos resultados da mineração e assimilação desses resultados, incorporando-os nos processos de negócios da organização.

Atualmente, a área de mineração de dados tornou-se interdisciplinar, tendo aplicações nas mais diversas áreas de conhecimento e servindo para as mais diversas funções.

Mostra as funções mais comuns que técnicas de mineração de dados podem exercer e alguns exemplos de aplicações dessas técnicas no cumprimento de cada função.

Segundo Larose, existem seis técnicas principais que os processos de mineração de dados podem exercer sobre uma base de dados de modo a atingir seus objetivos.

Segue uma breve descrição dessas técnicas e suas aplicabilidades.

Trata-se da busca de padrões, tendências e relações contidas na base de dados em análise.

Esta busca é realizada através de técnicas de análise de dados.

Essa descrição de padrões serve como parte do processo de mineração de dados e auxilia na elaboração de modelos preditivos genéricos.

Deve-se tomar cuidado em não confundir testes de hipóteses sobre bases de dados com técnicas de análise de dados para buscas de padrões e relacionamentos.

Mineração de dados para descrição de padrões, busca relações não necessariamente óbvias, embora também possa ser usada para verificar hipóteses de padrões e relacionamentos.

Técnicas de análises de dados podem ser classificadas em explanatórias ou gráficas.

Essas técnicas podem ser úteis para se fazer uma análise mais profunda de bases de dados em comparação a uma análise simplemesmente estatística.

Além disso, pode-se utilizá-las para examinar as interrelações entre atributos de uma base dados e desenvolver uma idéia inicial de possíveis associações entre atributos da base e identificar possíveis subconjuntos da base de dados que devem ser melhor analisados por outras técnicas de mineração.

Técnicas estimativas ou de avaliação de dados buscam estimar o valor númerico de uma determinada varíavel através de determinados atributos da base dados e de prognosticadores da variável em análise que devem também estar especificados no conjunto de testes da base em análise.

Assim, quando se deseja realizar uma análise, é feita uma estimativa da variável a ser analisada com base nos prognosticadores fornecidos pela base de dados de teste.

Através do relacionamento dessa estimativa e dos prognosticadores fornecidos, pode-se gerar um modelo que passa a ser aplicado para cada entrada da base de dados.

Essa técnica demanda uma base de dados dita como completa, onde além dos atributos de cada entrada da base, é fornecido um prognosticador da varíavel a ser analisada.

Essas técnicas são obtidas majoritariamente através de métodos de análises estatísticas, tais como regressão linear, correlação e regressão múltipla.

Redes neurais também podem ser utilizadas para realizar-se estimativas.

Como exemplos de aplicações dessa técnica, pode-se citar um caso onde se deseja estimar a pontuação que determinado jogador irá obter em um campeonato de futebol, estimar os gastos que uma determinada família irá ter para determinada atividade, entre outros.

Esta técnica é similar às técnicas estimativas e descritivas.

O que a difere é que esta busca fazer um prognóstico de variáveis, ou seja, busca predizer o valor e/ou comportamento de determinada varíavel em algum ponto no futuro.

Busca encontrar generalizações, baseadas em fatos e comportamentos do passado, que tendem a reincindir num futuro.

Tais generalizações podem ser feitas tanto através dos mesmos métodos das técnicas anteriormente apresentadas quanto através de métodos de mineração de dados e descoberta de conhecimento tais como redes neurais e árvores de decisão, entre outros.

Segundo  essa técnica desenvolve-se em duas fases, de treinamento e de teste.

Na fase de treinamento é construído um modelo baseado em dados históricos.

Na fase de teste esse modelo é aplicado sobre conjuntos de dados desconhecidos com a finalidade de se fazer prognósticos sobre esses conjuntos de dados.

Técnicas preditivas podem ser classificadas em duas especilizações, de classificação e de predição de valores.

Técnicas preditivas de classificação buscam classsificar a varíavel em observação em um conjunto finito pré-definido de classes.

Por exemplo, num modelo que se está fazendo um prognóstico sobre a confiança de um cliente de uma seguradora, pode-se definir que este cliente pode ser da classe 'confiável' ou da classe 'não confiável'.

O resultado da aplicação do modelo irá classificar o cliente em análise em uma das duas classes.

Técnicas de predição de valores buscam estimar um número contínuo que está associado a um registro da base de dados.

Por exemplo, uma loja qualquer deseja fazer um prognóstico do tempo que determinado cliente irá permanecer comprando na loja.

O resultado desse prognóstico é um valor contínuo e não classificatório como na técnica de classificação.

Além dos exemplos já citados, pode-se usar técnicas preditivas para o gerenciamento de fidelização de clientes, aprovação de crédito, oferecimento de vendas casadas, entre outros.

Nas técnicas de classificação existe uma variável a qual se deseja analisar no processo, a qual pode-se ser designada uma classificação.

Por exemplo, pode-se querer predizer o rendimento financeiro de pessoas representadas em um conjunto de dados.

Dessa forma, a variável 'rendimento' a ser analisada pode receber três classificações, alto, médio e baixo.

Para o funcionamento da técnica é apresentado um banco de dados com registros já classificados.

Com base nesses registros, que servem como conjunto de testes, é feito um treinamento do algoritmo para que este possa "aprender"como classificar novos registros ainda não classificados.

Para a execução dessas técnicas, utiliza-se, mais comumente árvores de decisões, redes neurais e o algoritmo k-nearest neighbor.

Pode ser aplicada como auxiliar à tomada de decisões em casos como descoberta de fraude em transações financeiras, determinação de riscos de crédito e diagnóstico de doenças.

Técnicas de agrupamento, buscam dividir o conjunto inteiro de dados em análise em grupos de registros que são relativamente homogêneos entre si e suficientemente diferentes dos registros contidos em outros grupos.

Técnicas de agrupamento são frequentemente utilizadas como tarefas preliminares a outras técnicas de mineração de dados, onde os agrupamentos resultantes são utilizados como entradas para outros algoritmos de mineração.

Para a execução dessa técnica pode-se usar redes neurais, redes de Kohonen e o algoritmo k-means.

Aplicações comuns de clustering concentram-se nas áreas de definição de nichos de mercado para direcionamento de marketing ou auditamento de contas, onde com a agrupamento de contas semelhantes pode-se descobrir grupos de contas fraudulentas, entre outras.

As técnicas de mineração de dados para associação consistem em encontrar relações entre atributos de um conjunto de dados, ou seja, atributos que têm uma tendência em terem valores "casados"em cada registro da base.

Por exemplo, numa base contendo todas as compras de um supermercado, pode-se observar que os produtos A e B são frenquentemente comprados em conjunto num determinado dia da semana, sábado por exemplo.

Dessa forma, tem-se uma associação entre A e B que indica que se o dia for sábado quem compra A irá comprar B.

Para cada associação é calculada uma taxa de suporte e confiança.

A taxa de suporte é calculada com base no total de registros da base que confirmem a regra, neste caso compras que ocorreram no sábado e, no total de registros onde há potencialidade de ocorrer a regra, neste exemplo, nos registros onde ocorre compra de A e B no sábado.

A taxa de confiança é calculada com base nas registros em que podem ocorrer a regra e nos registros onde a regra ocorre de fato.

Por exemplo, se num total de 200 compras que se comprou A, 50 dessas também tinham B na compra, têm-se uma confiança de 25% na associação (50/200).

Essas técnicas também são conhecidas como técnicas de análise de afinidades.

Segundo Larose, são exemplos de algoritmos para técnicas de associação os algoritmos priori e GRI.

Além do exemplo de aplicação já citado, essa técnica é aplicada para planejamento de oferecimento de serviços, previsão de degradação de materiais, prever efeitos colaterias de medicamentos, entre outros.

Mostra-se uma breve visão sobre os trabalhos que já foram realizados com o intuito de facilitar a execução paralela de técnicas de mineração de dados.

Foram muitos os trabalhos encontrados e estudados.

No entanto, ainda não cabe ao escopo desse documento o detalhamento desses trabalhos.

O objetivo desse estudo foi encontrar soluções que possam ser utilizadas no trabalho proposto, além de buscar encontrar lacunas e necessidades ainda não satisfeitas pelo atual Estado da Arte de modo a compôr a motivação do trabalho em proposição.

Nos trabalhos estudados percebeu-se que esses dividem-se basicamente entre dois grupos, trabalhos que apresentam técnicas para paralelização de processos de mineração de dados e trabalhos que apresentam ferramentas ou arquiteturas de serviços para a execução dessas técnicas.

Entre os trabalhos do primeiro grupo, destacam-se os trabalhos de Wolff, Guralnik e Kunath que apresentam soluções paralelas para as técnicas de associação, distribuição e clustering, respectivamente.

Tais trabalhos mostram soluções para diversas variações de algoritmos para cada uma das respectivas técnicas que exploram e usam um tipo de aplicação específica para a validação dos seus métodos.

Nos casos dos trabalhos acima citados, e na grande maioria dos trabalhos estudados, o ambiente de testes utilizado é o ambiente de COW.

Os resultados apresentados por esses trabalhos mostram desempenhos significativos na paralelização de processos de mineração de dados, chegando em alguns casos de aceleração da execução paralela ser superescalar, de acordo com o número de computadores envolvidos.

Por exemplo, em, a solução paralela executou em tempo 17 vezes mais rápido do que a solução seqüencial, quando utilizado um cluster de 15 computadores.

Nos trabalhos classificados no segundo grupo, destacam-se os trabalhos apresentados pelo Projeto Tamanduá.

O projeto Tamanduá cria um Web Service para execução de aplicações de mineração de dados paralela das técnicas de análise associativa e de árvores de decisão.

O servidor Tamanduá tem um perfil para cada tipo de aplicação e, como base nisso, determinda como uma aplicação deve ser escalonada e como os resultados da mesma devem ser exibidos para o usuário.

O Tamanduá trabalha com recursos de Grades computacionais e tem uma interface Web onde os seus usuários podem solicitar serviços de execução e analisar os resultados do processamento de seus serviços.

O projeto Weka4 WS também implementa uma arquitetura de Web Service para execução de aplicações paralelas.

Esse serviço é na realidade uma extensão do software Weka  que é um framework para auxiliar na construção de aplicações de mineração de dados seqüenciais.

O que o Weka4 WS faz é adaptar as técnicas já presentes no Weka de modo que essas possam ser executadas em paralelo.

O ambiente de execução que o Weka4 WS é projetado, de acordo com seus desenvolvedores, é um ambiente de Grade orientado a serviço.

O GridMiner talvés seja o projeto mais completo entre os estudados.

Apresenta uma arquitetura de serviços completos para execução de aplicações de mineração de dados distribuída, envolvendo serviços que vão desde os processos pré-mineração (pré-processamento, pré-seleção, entre outros) até os processos de visualização de informações.

Apresenta também funcionalidades para acesso a dados distribuídos, que é herdado de projetos relacionados à estrutura do Globus Toolkit 30 sob o qual o GridMiner é baseado.

Porém, aponta-se a ausência de ferramentas para criação de aplicações de mineração de dados de maneira paralela, ficando a cargo do usuário essa implementação.

O GridMiner fornece somente um ambiente otimizado para execução dessas aplicações em grades Globus Toolkit 3 0  Este capítulo apresenta a proposta de uma arquitetura para facilitar a execução de tarefas de mineração de dados em ambientes de computação de alto desempenho.

Conforme pode-se constar com base nos estudos apresentados nos Capítulos 2 e 3, atualmente é possível construir ambientes de computação com elevado poder computacional a custos relativamente baixos graças a tecnologias abertas, que permitem serem utilizadas, estudadas e aprimoradas.

Também pode-se perceber a importância que os processos de mineração de dados desempenham nas mais diversas aplicações e quanto uma organização pode se beneficiar com a aplicação desses para auxiliar às suas tomadas de decisões, proporcionando a melhora de seus negócios, redução de custos e direcionamento de seus investimentos.

Embora a aplicação de processos de mineração de dados para descoberta de conhecimentos seja um fator estratégicamente importante para uma organização, devido a caracterísitca instrínsica desses processos de demandar a análise e processamento de grandes quantidades de dados, a execução desses processos, quando feita da forma tradicional, acaba se tornando computacionalmente e, conseqüentemente, financeiramente cara.

Conforme aumenta-se a necessidade por análise de dados e pela acurácia do processo, cresce a demanda de recursos computacionais para prover o ambiente necessário para a execução desses processos, obrigando a organizações fazerem altos investimentos em máquinas capazes de prover tal demanda, o que pode resultar na necessidade de diminuição da acurácia dos resultados do processo e muitas vezes até mesmo no impedimento da realização desse devido ao alto custo.

Diante desse cenário, pode-se apresentar os ambientes vistos no Capítulo 2, excetuando-se ambientes MPP, como soluções baratas e eficazes para prover recursos computacionais.

Porém, esses ambientes, em sua maioria, só respondem com eficiência à execução de aplicações especialmente programadas de modo a aproveitar a natureza paralela inerente desses.

Exige-se que essas aplicações disponham de algoritmos modelados para serem executados de forma paralela.

Algoritmos de mineração de dados foram inicialmente propostos sem a preocupação de paralelização e, além disso, em alguns casos, devido à natureza do processo, esta paralelização torna-se inviável.

Apesar disso, existem vários trabalhos que apresentam paralelizações de técnicas de mineração de dados e até mesmo ambientes de apoio à execução dessas técnicas, que apresentam sucesso em seus objetivos, tornando o processo de mineração mais rápido, o que possibilita uma análise mais profunda de uma base de dados mais extensa no mesmo tempo que se levaria para analisar uma quantidade de dados menor em técnicas não paralelas.

Embora os trabalhos realizados apresentem uma boa base de soluções para mineração de dados paralelas, salienta-se algumas lacunas nas soluções apresentadas por esses trabalhos, sejam na abordagem utilizada ou na ausência de soluções para determinados cenários.

Seguem algumas dessas lacunas apontadas a partir de uma análise preliminar desses trabalhos,  Falta de testes em diversas plataformas diferentes.

A maioria dos trabalhos que apresentam soluções para paralelização de algoritmos de mineração de dados realizam a validação de suas descobertas apenas sobre um tipo de ambiente paralelo.

Tal abordagem dificulta a medição do quão genérica esta solução é, dificultando o entendimento de como essas soluções podem se comportar em outros ambientes.

Essa deficiência implica no desconhecimento de qual plataforma é mais adequada para determinada técnica, em relação a custo/benefício, tempo de execução, entre outros fatores de medição, não explicitando até mesmo se é possível que determinada técnica possa ser executada em determinado ambiente.

Falta de testes de diferentes aplicações para as metodologias propostas.

Muitos das soluções apresentadas mostram seus testes e, em alguns casos, até mesmo a elaboração de suas soluções baseados em aplicações muito específicas, deixando uma incongnita no que diz respeito a generalidade de suas aplicações.

Falta da análise sobre a forma de acesso à base de dados.

Conforme dito anteriormente, processos de mineração de dados necessitam de acesso a base de dados e implicam no processamento desses dados.

As soluções apresentadas pelos processos estudados não levam em conta a forma como esses dados são acessados, adimintindo-se sempre que esses dados serão distribuídos já embarcados nas tarefas da aplicação a ser paralelizada, com exceção do GridMiner que provê acesso a bases distribuídas.

Trabalhos como os apresentados em, mostram que, para determinados ambientes de computação distribuída, podem ser aplicadas diferentes formas de acesso a dados pelas aplicações em execução nesses, por exemplo como replicação de banco de dados para ambientes de grades e acesso direto das tarefas distribuídas a bases de dados para ambientes dispostos em domínios centralizados como cluster ou máquinas multi-núcleos.

Tal deficiência na abordagem dos trabalhos estudados implica no desconhecimento de qual método de acesso é o mais adequado para uma determinada técnica quando deseja-se executá-la num determinado ambiente.

Diante dessas observações, destaca-se a necessidade de um estudo que mostre a relação das técnicas de paralelização de mineração de dados com a ambiente de execução onde deseja-se implantá-las e a forma como essas realizam o acesso a bases de dados a partir desses ambientes.

Além disso, considera-se oportuno, diante da visibilidade e aplicabilidades que técnicas de mineração de dados paralelas têm, a criação de uma arquitetura que seja capaz de auxiliar no planejamento, implementação, escalonamento e execução dessas técnicas em ambientes distribuídos.

Os objetivos da presente proposta concentram-se na criação de um estudo de aplicação de técnicas de mineração de dados paralelas em ambientes distribuídos e na criação de uma arquitetura que apresente ferramentas que auxiliem no desenvolvimento e execução de aplicações de mineração de dados em ambientes de cluster, grade e em computadores multi-núcleos.

De maneira mais especifica, pode-se citar os objetivos iniciais que deseja alcançar com o trabalho em proposição como, Criar um perfil das técnicas de mineração de dados paralelas baseados no ambiente de execução dessas e na forma como essas acessam os dados que demandam durante sua execução.

Analisar e propor, caso possível, novas alternativas ou técnicas para a execução de mineração de dados paralela em ambientes distribuídos quando essas ainda não estiverem apresentadas no Estado da Arte do assunto.

Estudar e propor ferramentas que possibilitem o acesso a dados a partir aplicações que estão sendo executados nos ambientes paralelos.

Através do resultado dos estudos e de possíveis novas soluções alcançadas na execução dos objetivos acima propostos, seja capaz de prover um ambiente de desenvolvimento que auxilie na tarefa de modelar, criar, escalonar e executar aplicações paralelas de técnicas de mineração de dados.
Fornecendo ao desenvolvedor ferramentas que o permitam criar uma solução genérica para sua aplicação, que possa ser executada sobre ambientes diferentes e com diferentes formas de acesso a dados, de acordo com os recursos disponíveis na organização.

Segue na Tabela 41 o cronograma de realização de atividades que visam desenvolver o trabalho proposto.

Abaixo, segue a descrição de cada uma dessas atividades.

Levantamento bibliográfico.

O levantamento bibliográfico ocorrerá durante todo o período da pesquisa, uma vez que constantemente novos artigos científicos sobre os temas abordados são publicados em conferências ou periódicos.

Estudo aprofundado sobre mineração de dados.

Esta atividade faz-se necessária e  será realizada logo no início do trabalho pois, o entendimento do funcionamento dessas  técnicas, além de ser de fundamental importância para o trabalho, não são de domínio do  proponente.

Estudo sobre o Estado da Arte em mineração de dados paralela.

Nesta atividade  estudar-se-á os trabalhos já existentes no assunto, procurando absorver as suas contribuições e identificar deficiências que possam auxiliar no planejamento da arquitetura.

Estudo das técnicas de acesso a bases de dados a partir de ambientes paralelos.

Visa  estudar quais as técnicas existentes para tal e quais as contribuições, aplicabilidades e  resultados de tais técnicas.

Salienta-se que o proponente já tem resultados nessa área, frutos de trabalhos anteriores.

Criação dos perfis das técnicas de mineração de dados distribuídas.

Nesta atividade  serão testadas as diferentes técnicas em relação ao ambiente e tipo de acesso a dados que  elas utilizarão.

Aqui, se aplicável, também serão testadas novas abordagens possíveis  para paralelização de técnicas ainda não exploradas no Estado da Arte.

Elaboração da arquitetura proposta.

Nesta atividade será elaborada a arquitetura que  se propõe com base nas estudos realizados nas atividades anteriores.

Implementação e validação da arquitetura elaborada.

Implementação e validação da  arquitetura desenvolvida ao longo deste trabalho.

Inclui também a definição de casos de  teste para execução na arquitetura proposta.

Seminário de andamento.

Preparação para o seminário de andamento.

Elaboração de Artigo.

Criação de um artigo com os resultados do trabalho para submissão para congressos e/ou revistas científicas.

Escrita da dissertação.

Redação da dissertação de mestrado.

Defesa da dissertação.

Preparação para a defesa do trabalho de mestrado.

Cronograma de atividades.

