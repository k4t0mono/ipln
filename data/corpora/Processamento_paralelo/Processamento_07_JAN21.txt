Este curso objetiva trabalhar os conceitos básicos e as principais tendências na área de arquitetura de máquinas paralelas.

Serão abordados os seguintes temas, Classificações de máquinas paralelas e organização de memória.

Coerência de memórias cache.

Avaliação de diferentes redes de interconexão e políticas de roteamento de mensagens.

Tendências na construção de máquinas paralelas com ênfase para as máquinas agregadas (cluster de estações).

Neste curso serão apresentados os fundamentos da programação concorrente em arquiteturas com múltiplos processadores, dotadas ou não de memória compartilhada.

O enfoque do curso será no estudo de ferramentas de desenvolvimento baseadas em processos leves (threads POSIX) e bibliotecas de comunicação (MPI, Message Passing Interface).

As decisões de projetos relativas às NoCs (Networks-on-Chip) e NUCAs (Non-Uniform Cache Architectures) são fundamentais para obtenção de alto desempenho, vazão de dados e escalabilidade nas futuras gerações de processadores many-core de propósito geral.

As NoCs são as alternativas mais prováveis para suportar computação de alto desempenho em arquiteturas many-core devido aos diversos problemas e limitações físicas associadas aos fios.

Desta forma, as NoCs deverão tomar lugar de interconexões tradicionais como barramentos e chaves crossbar para comunicação entre vários núcleos.

Arquitetura não uniforme de memória cache é outra abordagem que está sendo discutida para os novos processadores many-core.

As arquiteturas NUCA são uma alternativa para aumentar o desempenho dos processadores que possuem uma quantidade muito grande de núcleos de processamento.

Em se tratando de memória compartilhada, é possível apontar a latência de acesso como o principal problema.

As arquiteturas NUCA reduzem as latências de acesso a dados, aumentando a escalabilidade, além de possuírem grande potencial para o aumento de desempenho em relação as tradicionais arquiteturas uniformes de memória cache.

Aumentar o desempenho das aplicações é um dos principais objetivos durante o projeto da arquitetura de um processador.

No entanto, devido a restrições no paralelismo de instruções, consumo de potência, além dos limites impostos pela latência do fio, o ganho de desempenho das gerações de processadores com um único núcleo (single-core) tem crescido a taxas inferiores ao que se esperava pela densidade de integração da Lei de Moore.

A solução para esta desaceleração na evolução e no ganho de desempenho dos processadores single-core está em uma nova geração de processadores com múltiplos núcleos (multi-core).

Atualmente os núcleos de um processador multi-core são mais simples, alcançando uma maior eficiência no consumo de potência e energia.

Porém, múltiplos núcleos aumentam a capacidade de exploração do paralelismo no nível de thread, podendo ocasionar uma intensa comunicação interna.

Como conseqüência do aumento do número de núcleos, há também a necessidade de uma rede de comunicação entre os núcleos de alto desempenho e eficiente.

Soluções tradicionais de interconexão, barramentos e chaves crossbar, possuem problemas de escalabilidade e limites físicos.

Portanto, aumentar o tamanho destas soluções de interconexão implica diretamente nas seguintes conseqüências, aumento da resistência do fio, aumento da latência de comunicação e aumento da complexidade de roteamento do fio.

Neste sentido, a comunidade tem trabalhado com o objetivo de reduzir a influência dos limites físicos dos fios através de uma arquitetura de rede chamada de NoC.

As NoCs são redes de comunicação em chip baseadas em troca de pacotes, que possuem os seguintes componentes básicos, roteadores, adaptadores de rede, e links de comunicação.

A maioria das NoCs é projetada para que um roteador seja responsável pela interconexão de um núcleo à rede de comunicação.

Por exemplo, em uma rede com topologia mesh, um roteador teria entre o núcleo e os demais roteadores adjacentes links curtos e com baixa resistência e latência de comunicação.

Em contrapartida, cada roteador aumenta em complexidade a gerência de comunicação interna, podendo ou não aumentar a latência de comunicação.

Como conseqüência do uso das NoCs, a possibilidade de uma maior quantidade de núcleos de processamento aumenta consideravelmente.

Uma vez que a rede em chip é responsável por interconectar núcleos de processamento e qualquer outro tipo de periférico, os acessos às memórias cache podem não ser mais uniformes.

Isto significa que núcleos de processamento mais distantes levariam mais tempo para acessar uma memória cache compartilhada do que um núcleo mais próximo a esta memória.

Esta arquitetura de memórias cache é chamada de NUCA.

Estudos e projetos relativos às NUCAs e NoCs ainda estão em desenvolvimento pela comunidade científica, mas possuem resultados promissores para as novas gerações de processadores many-core.

Chips com milhares de núcleos ainda é uma projeção, mas os desafios para viabilizar chips com dezenas ou centenas de núcleos já começam a ser enfrentados e resolvidos.

Os processadores com uma quantidade elevada de núcleos são chamados de many-core.

Um exemplo deste tipo de processador de propósito geral em desenvolvimento é da linha de pesquisa Tera-scale da Intel.

Portanto, o objetivo deste curso é apresentar os principais conceitos e tendências relativas às arquiteturas de NoCs e NUCAs para as novas gerações de processadores many-core de propósito geral.

Nesse sentido, o texto está dividido em três partes que focam em, arquiteturas de processadores many-core, arquiteturas de NoC, e arquiteturas NUCA.

Ao final deste curso é apresentada uma conclusão analisando as diversas abordagens de arquiteturas apresentadas.

Apesar de processadores com múltiplos núcleos existirem desde a década passada, em sistemas dedicados, processadores de rede, o surgimento de processadores de propósito geral tem alguns anos.

Os fatores que mais motivaram o surgimento destes processadores são os limites da Lei de Moore e o alto consumo de potência.

A base do desempenho dos processadores se valia pelo paralelismo no nível de instrução, com pipeline superescalar, no máximo duas threads simultâneas (SMT, Simultaneous Multihreading) e a constante elevação da freqüência de operação.

Porém, o consumo de potência já atingia limites impraticáveis, o paralelismo no nível de instrução já havia encontrado limites de desempenho e múltiplas threads simultâneas em um mesmo pipeline muitas vezes degradava o desempenho.

Sendo assim, era necessária uma nova abordagem para a constante evolução no aumento do desempenho, atendendo limites de consumo de potência.

Os processadores com múltiplos núcleos, já usados em outras áreas, se tornaram alternativas para computadores de propósito geral.

Mostra um exemplo de arquitetura quad-core, sendo que cada núcleo possui um pipeline superescalar de duas vias.

Em comparação com outras arquiteturas, cada um dos núcleos pode ser considerado relativamente simples.

Neste exemplo, o objetivo é suportar tanto paralelismo no nível de instrução quanto no nível de thread.

No entanto, as threads são somente suportadas pela existência de mais de um núcleo, já que cada núcleo não é multithreaded, sendo responsável pela execução de apenas uma thread.

Processadores multi-core que suportam múltiplas threads também podem ser chamados de Chip Multithreading.

Execução de múltiplas threads em um processador multi-core.

Existem processadores multi-core de propósito geral que suportam múltiplas threads em cada núcleo, tanto por entrelaçamento de instruções, (IMT, Interleaved Multithreading) quanto por execução simultânea de instruções.

O motivo para escolha de uma técnica ou outra, assim como a escolha entre single-core ou multi-core, se deve muito à característica das cargas de trabalho.

Ilustra um experimento realizado em 1996 que mostra as vantagens de usar uma arquitetura multi-core em relação a uma arquitetura superescalar.

O experimento mostra que a área ocupada pelas duas soluções é a mesma, e as características de cada arquitetura são as seguintes, Ambas são baseadas em arquiteturas do processador MIPS R10000.

A arquitetura é um superescalar de seis vias de execução.

A arquitetura multi-core possui quatro núcleos, sendo que cada um possui pipeline superescalar de duas vias.

Comparação entre arquiteturas.

Superescalar, multi-core Em resumo os resultados mostram que uma arquitetura superescalar possui desempenho melhor se a carga de trabalho possuir alto paralelismo no nível de instrução.

Por outro lado, a arquitetura multi-core possui melhor desempenho se a carga de trabalho tiver um alto paralelismo no nível de thread.

Este experimento foi o início para o surgimento do processador Niagara, que possui oito núcleos escalares com suporte IMT cada um.

Este processador possui alta vazão de threads e baixa vazão de instruções.

Portanto, uma arquitetura como esta não é adequada para um computador pessoal, mas adequada para um sistema de aplicações web e banco de dados, onde a carga de trabalho possui alto paralelismo de threads.

Pode ser visto, que para quatro núcleos uma chave crossbar é suficiente para prover a comunicação interna.

A mesma alternativa foi adotada para o processador Niagara, que possui oito núcleos.

No entanto, para a nova geração de processadores many-core, uma única chave crossbar possui limites físicos que impedem o aumento da quantidade de núcleos interconectados a ela.

Mostra que o projeto Tera-scale da Intel desenvolve um processador many-core com oitenta núcleos e uma network-on-chip.

Em realce, cada núcleo possui um roteador e, portanto, a comunicação interna é por troca de mensagens.

A Seção 33 descreve os principais detalhes das Networks-on-Chip.

Arquitetura many-core de desempenho Teraflop As principais vantagens dos processadores many-core, além do aumento do desempenho. 
Ativação somente dos núcleos necessários.

Redistribuição de carga de trabalho em função de altas temperaturas atingidas pelos núcleos.

Núcleos reservas para substituição de núcleos com problemas.
Expansão da funcionalidade com diversos dispositivos dentro do chip.

Outro detalhe que pode ser percebido é a necessidade de uma rede de comunicação eficiente e que possibilite as vantagens apresentadas.

Na seção seguinte as diversas abordagens são descritas.

Justificativas do projeto Tera-scale, utilização inteligente de núcleos, redistribuição de carga de trabalho, núcleos reservas, integração de dispositivos dedicados.

De acordo com os limites de escalabilidade impostos pelos fios, as tradicionais soluções de interconexão largamente utilizadas em arquiteturas multi-core, tais como barramento e chave crossbar, são impraticáveis para arquiteturas many-core.

A solução que vem sendo estudada e proposta através de várias pesquisas é a Network-on-Chip.

Nesta seção é dada uma ênfase às NoCs apresentando as principais abordagens relativas às arquiteturas da rede ou roteador, topologias e protocolos, além de uma análise da viabilidade em comparação com as alternativas de interconexões tradicionais.

Uma Network-on-Chip é composta por três elementos básicos, roteador, interface e links de comunicação.

O roteador é o elemento principal responsável pela interconexão da rede, pela definição de rotas, pelo controle de fluxo, qualidade de serviço e, portanto, pela garantia de entrega do pacote de dados.

Por se tratar de uma rede de comunicação composta por roteadores, o mecanismo de entrega de dados é através de passagem de mensagem ou pacotes de rede.

Interligando os roteadores existem os links de comunicação.

Estes links são os fios responsáveis pela existência do caminho a ser percorrido pelos pacotes.

A forma como os roteadores estão interconectados pelos links dá origem à topologia da rede.

A Seção 3 3 2 apresenta detalhes e comparações entre topologias propostas para NoCs.

O último elemento que compõe uma NoC é a interface de rede.

Esta interface também é chamada de adaptador ou wrapper, sendo necessária para garantir a correta comunicação entre a rede (roteadores da NoC) e os núcleos ou periféricos que estão interconectados.

Esta interface garante que haja uma correta comunicação entre protocolos diferentes (NoC, núcleo, memória, etc).

Ilustra um exemplo de NoC baseada na topologia mesh.

Neste caso, a NoC é uma mesh 3 x3 que interconecta três núcleos de processamento através de três interfaces de rede.

Estas interfaces estão interconectadas, cada uma, a um roteador diferente da NoC e, portanto, cada roteador é específico para um núcleo de processamento.

Os demais roteadores da NoC nesta figura não estão interconectados a outros componentes do chip, mas poderiam ser memórias, ou qualquer hardware dedicado.

Arquitetura básica de uma NoC com topologia mesh.

Um chip composto por uma NoC, núcleos de processamento, memórias e outros hardwares dedicados representam um sistema de computação.

Portanto, para este caso existe um termo muito comum conhecido como Sistema-em-Chip ou System-on-Chip (SoC).

Os projetos de arquiteturas de processadores many-core caminham para uma heterogeneidade de componentes em chip que os aproximam desta classificação.

Ilustra uma arquitetura típica de roteador para uma NoC mesh apresentada na Estes roteadores trabalham com roteamento XY e, portanto, possuem quatro portas de entrada/saída para norte, sul, leste e oeste.

Além disso, é necessária uma porta para o núcleo de processamento através da interface de rede.

Para a interconexão entre as portas o roteador se baseia em uma arquitetura simples de chave crossbar, neste exemplo 5 x5, normalmente sem nenhum adicional de complexidade, como o suporte a broadcast.

Para definição das conexões que devem ser realizadas, é necessário um Árbitro.

A função principal deste mecanismo de arbitragem está na solução de conflitos na utilização da chave crossbar e, por conseqüência, na liberação de pacotes dos buffers de entrada.

Neste exemplo de roteador, os buffers estão presentes apenas nas entradas.

É importante ressaltar, que a arquitetura deste roteador ilustra um exemplo de circuito dedicado e não programável.

Arquitetura básica de um roteador de NoC para topologia mesh (modelo de filas).

Roteadores programáveis também podem ser utilizados no projeto de uma Network-on-Chip.

Ilustra o projeto de um roteador com um processador de rede gerenciando um aglomerado de núcleos de processamento interconectados a uma chave crossbar e buffers de entrada.

Neste caso o roteador não gerencia apenas um núcleo como ilustrado, mas oito núcleos.

Uma NoC constituída por um roteador deste tipo teria a interconexão entre dois roteadores através de uma das oito portas de entrada.

Projetos de NoCs para clusters ou aglomerados de núcleos também podem ser encontrados na literatura.

O impacto da utilização de buffers pode estar relacionado ao tamanho, quanto maior a profundidade do buffer, maior o tamanho da NoC, ou pelos seguintes motivos relacionados à chave crossbar, Buffers de entrada, As técnicas de arbitragem são relativamente simples, possui uma melhor relação de área e potência, além de proporcionar um melhor desempenho para a chave crossbar.

Buffers de saída, em função de N entradas conectadas a cada um dos buffers de saída, a chave crossbar precisa ser N vezes mais rápida.

A adoção de buffers de saída não é a mais adequada para alto desempenho.

No entanto, existem vantagens em se tratando da eliminação do bloqueio de pacotes que não receberam permissão de envio porque o primeiro pacote da fila ainda não teve liberação de uma determinada saída.

Este problema é conhecido como head of the line blocking e pode acontecer nas soluções com buffers de entrada.

Buffers de crosspoint, cada ponto de conexão da chave crossbar possui um buffer.

É utilizada a técnica de roteamento chamada de self-routing.

Neste caso, em cada crosspoint seria necessário além do buffer um decodificador para decisão de envio ou não do pacote.

Esta solução aumenta o tamanho e a potência consumida da chave crossbar.

Arquitetura de um roteador programável.

A definição de uma topologia de NoC, tal como de um roteador, está relacionada à carga de trabalho que será executada.

Ilustra as principais NoCs e suas respectivas topologias.

Nesta seção é feita uma análise das principais características de cada uma destas topologias.

É possível citar três tipos de topologias, fixas, sem fio e reconfiguráveis.

As topologias fixas são alternativas clássicas de adoção de uma determinada forma de interconexão que privilegie um comportamento específico de uma determinada carga de trabalho.

Topologias sem fio são alternativas recentes para eliminar as limitações do fio no projeto de NoCs através de uma tecnologia chamada de Radio-on-Chip.

Por fim, as topologias reconfiguráveis utilizam plataformas programáveis para que sejam realizadas adaptações na forma de interligações em função de mudanças no padrão de comunicação das cargas de trabalho.

Espera-se através da reconfiguração um aumento na flexibilidade de topologias da NoC.

Exemplos de topologias de NoCs.

Características de algumas topologias fixas.

Árvore Gorda é uma alternativa tolerante a falhas baseada na replicação de conexões ou uso de conexões com maior vazão nas ligações perto da raiz.

Totalmente conectada serve como referência para análise.

H = altura, n = número de nós, r = linhas, g = grau do nó.

A topologia totalmente conectada é apresentada como referência e alternativa para alcançar melhor desempenho de uma rede.

Neste caso, as características desta topologia mostram que ela possui um alto número de ligações, um alto grau do nó e um baixo diâmetro.

Estas são características que descrevem um baixo número de saltos e alta redundância de ligações.

Quanto menor o número de ligações e grau do nó, maior é o diâmetro (maior distância entre dois componentes quaisquer) da rede.

O grande problema de uma topologia totalmente conectada é o custo.

É uma rede grande com problemas físicos para roteamento dos fios.

As topologias mais encontradas na literatura são baseadas em mesh e torus.

A principal característica está relacionada à capacidade de suportar aplicações cujos problemas podem ser particionados (operações com matrizes e processamento de imagens).

As topologias honeycomb mesh ou mesh hexagonal são consideradas da mesma família e possuem desempenho similar.

A vantagem de uma topologia honeycomb em relação à mesh está na diminuição do custo em função de uma menor área ou quantidade de links, além de facilitar o mapeamento de aplicações.

A topologia anel possui o menor custo entre as apresentadas, mas em contrapartida possui um diâmetro que cresce de forma linear em função do número de nós.

Uma alternativa que mantêm um baixo custo é a topologia anel chordal que possui caminhos alternativos, aumentado o grau do nó e diminuindo o diâmetro.

Esta alternativa é mais tolerante a falhas do que a versão original (anel) e, portanto, mais confiável.

A topologia árvore binária é uma solução interessante para aplicações baseadas em algoritmos de divisão e conquista.

O diâmetro cresce de forma linear em relação a altura e a confiabilidade é relativamente baixa, já que a perda de um nó pode separar a topologia em duas partes.

Outro problema está relacionado ao nó raiz que é um gargalo entre as subárvores da esquerda e direita.

A solução para este problema está na árvore gorda.

Esta solução é mais tolerante a falhas, pois possui um maior número de ligações entre os nós próximos da raiz.

Outra solução é aumentar a largura de banda destas ligações, reduzindo as contenções de comunicação do efeito concentrador / gargalo da raiz da árvore.

Encontrar uma topologia, que atenda os melhores requisitos de desempenho, escalabilidade, confiabilidade e custo não é tão trivial.

No projeto Tera-scale da Intel duas topologias estão em teste, anéis interconectados e mesh.

Devido ao grande número de diferentes domínios de aplicação ou padrões de comunicação, algumas propostas apontam para o uso de conceitos de reconfiguração para aumentar a adaptabilidade da rede.

Topologias do projeto Tera-scale, anéis interconectados, mesh.

Políticas e estratégias de transporte de dados em uma NoC é de responsabilidade dos protocolos.

A definição do protocolo descreve as principais características de funcionamento da rede.

Neste sentido, os protocolos são capazes de garantir a entrega dos dados, a confiabilidade da rede, a melhor rota, além do melhor desempenho, entre outras.

Os principais aspectos dos protocolos para NoCs são apresentados a seguir, Chaveamento por circuito, Existe uma rota dedicada para o caminho do dado.

Esta rota (circuito) permanece reservada até que a transmissão do dado acabe.

Chaveamento por pacote, Pacotes trafegam pela rede e o caminho é definido através de roteamento por salto.

Não há reserva de caminho e, portanto, vários pacotes podem compartilhar o mesmo meio.

Orientada a conexão, Existe uma conexão lógica entre origem e destino.

Portanto, a transmissão só inicia após a confirmação de um estado de pronto entre o transmissor e receptor.

Não orientada a conexão, Não existe uma conexão lógica.

A comunicação é estabelecida sem o acordo entre origem e destino.

Roteamento determinístico, A definição da rota é feita pela origem e destino.

Um exemplo utilizado pelas topologias mesh e torus é o roteamento XY.

Primeiro são percorrida as linhas e depois as colunas até o destino.

Roteamento adaptativo, a definição da rota é feita a cada salto de roteamento.

Neste caso, um roteador intermediário pode alterar o caminho que o pacote seguirá.

Roteamento mínimo ou não mínimo, O roteamento é mínimo se sempre é feita a escolha do menor caminho entre origem e destino.

Atraso versus perda, No caso de um protocolo baseado em atraso, o pior caso é um pacote atrasado.

No caso de um protocolo baseado em perda, é possível excluir um pacote da rede, portanto, uma retransmissão seria o pior caso.

Controle central ou distribuído, Pelo controle central, o roteamento é feito de forma global.

No caso do distribuído, cada roteador define a melhor rota para o pacote.

Existem três técnicas de encaminhamento de pacotes conforme descrição a seguir, Store-and-Forward, Todo o pacote é armazenado em buffers do roteador para que o cabeçalho seja analisado.

Em seguida o pacote é encaminhado.

Wormhole, Enquanto o pacote é recebido seu cabeçalho é analisado.

Definida a rota, todo o pacote é encaminhado sem a necessidade de armazenamento temporário em buffers para o nó seguinte.

Virtual cut-through, Segue o mesmo mecanismo do wormhole, mas o nó antes de encaminhar espera uma confirmação do próximo nó destino para envio do pacote.

Os mecanismos para controle de fluxo são responsáveis por garantir o funcionamento da rede.

Entre os principais benefícios estão, Garantir que pacotes não sejam descartados.

Evitar retransmissões de pacotes.

Diminuir o congestionamento da rede ou contenções nos roteadores.

Otimizar o uso de recursos da rede.

O Buffers menores.

O Menor número de transmissão de pacotes.

O Menor uso dos buffers e roteadores.

O Menor consumo de potência e energia.

O Melhor desempenho da rede.

Um dos conceitos básicos do controle de fluxo é ajustar a taxa de saída de dados de um transmissor para um receptor.

Três abordagens clássicas são descritas a seguir, Handshake, também chamado de "aperto de mão", consiste em um acordo entre transmissor e receptor através de linhas adicionais de controle para liberação do envio de pacotes.

Transmissor envia sinal de solicitação e receptor envia sinal de buffer livre.

Créditos, o receptor envia a quantidade de espaço livre no buffer de entrada para que o transmissor saiba quantos créditos estão disponíveis para envio de pacotes.

Canais virtuais, técnica para eliminar o problema head of the line blocking em que o primeiro pacote bloqueia os demais da fila.

Através de canais virtuais é possível criar diversas saídas do buffer de entrada, como se este tivesse várias pequenas filas.

Apresenta as vantagens e desvantagens da adoção de uma NoC em relação às soluções tradicionais (barramento e chave crossbar) mais utilizados atualmente nos processadores multi-core.

Análise comparativa para uso de NoCs (adaptado de relacionadas a escalabilidade e largura de banda apontam para a NoC como a melhor alternativa para futuras gerações de processadores many-core.

A próxima seção descreve quais as propostas de arquiteturas NUCA e como a rede em chip se insere neste projeto de memórias cache.

Nesta seção os principais conceitos e arquiteturas referentes às NUCAs (Non-Uniform Cache Architectures) são descritos.

Porém, é importante fazer uma apresentação dos conceitos UCA (Uniform Cache Architecture) antes de descrever a nova tendência.

Como uma memória cache possui diversos espaços para gravação de dados, e cada local da cache pode armazenar conteúdo de diversos locais da memória, algumas informações adicionais devem ser adicionadas para sabermos, durante uma pesquisa, se um dado requerido encontra-se na memória cache.

A utilização de tags junto ao dado copiado à memória cache é útil uma vez que a tag contêm as informações de endereço necessárias para identificar se a palavra pesquisada corresponde com o dado requisitado.

Assim podemos observar um exemplo de memória cache com dados copiados da memória principal e o tag de identificação.

Podemos ver no caso ilustrado, que o tag de dados precisa conter apenas a parte superior do endereço de memória referente para que seja possível sua identificação.

Diagrama de uma memória cache com mapeamento direto, apresentando a tag e o bit de validade.

Além da tag de dados, é necessária uma maneira para saber se um bloco de dados possui informações válidas.

Um exemplo claro dessa necessidade pode ser ilustrado pela inicialização do sistema, quando a memória cache encontra-se apenas com possíveis sujeiras, sem dados reais copiados.

O método mais comum para saber se a tag deve ser ignorada ou não é a inclusão de um bit de validade para indicar se a entrada contém um bloco válido.

A estratégia adotada para definir o modo em que os blocos de dados deverão ser posicionados na memória cache é chamado de estratégia de mapeamento de dados.

Um modo usual de mapeamento de dados na memória cache é o mapeamento direto.

Nesse modo de mapeamento cada local da memória é mapeado exatamente para um local na cache.

Assim, a tag trará informações adicionais do endereço e o endereço base será diretamente associado ao endereço da cache.

Este modo de mapeamento é atraente, pois é de fácil planejamento, uma vez que, se o número de entrada da cache for uma potência de dois, então o tamanho do endereço base pode ser calculado simplesmente usando os log bits menos significativos, e assim os dados podem ser associados a cache de forma direta.

Outra estratégia de mapeamento de dados na memória cache é o chamado mapeamento totalmente associativo.

Neste mapeamento um bloco pode ser posicionado em qualquer local da cache.

Porém, nesse esquema de mapeamento para um dado ser pesquisado na cache, todas as entradas precisam ser pesquisadas.

Para a implementação desse tipo de mapeamento, é comum a utilização de comparadores paralelos, sendo que, para cada bloco deve existir um comparador de endereços para a pesquisa de blocos.

A inclusão desses comparadores aumenta significantemente o custo de uma memória cache, sendo que esta técnica só é viável em memórias cache com pequeno número de blocos.

O ponto de equilíbrio entre os modos de mapeamento é o mapeamento associativo por conjunto.

Neste modo de mapeamento, existe um número fixo de locais (no mínimo dois) onde cada bloco pode ser colocado.

Uma dada memória cache, se for associativa por n conjuntos, sendo n o número de locais de mesmo endereço base onde cada bloco pode ser alocado, é chamada de cache associativa por conjunto de n vias.

Neste modo de mapeamento cada bloco é mapeado para um conjunto único na cache de acordo com o endereço base, e um bloco pode ser alocado em qualquer posição desse conjunto.

Logo, uma pesquisa de busca deverá acessar o conjunto referente ao endereço e depois pesquisar entre os elementos do conjunto o endereço requisitado.

Ilustra diversos modos de mapeamento de memória cache, onde pode ser visto os blocos utilizando mapeamento direto, ou associativo por conjunto e ainda uma memória cache totalmente associativa.

Alocação de um bloco de memória em uma memória cache com 8 blocos em diferentes tipos de mapeamento, mapeamento direto, mapeamento associativo por conjunto e  totalmente associativo.

O número total de blocos da cache é igual ao número de conjuntos multiplicados pela associatividade, porém, em uma cache associativa por conjunto, o número de comparadores de endereço costuma ser igual ao número de vias de associatividade.

Assim, o aumento do grau de associatividade de uma dada memória tende a reduzir as faltas de conflito de endereços, mas também reduz a quantidade de endereços base, além de aumentar o custo relacionado aos comparadores de endereço.

Durante o processamento, quando ocorrer uma falta de dados, ao buscar o bloco de dados para a memória cache, esta deve escolher qual será a posição onde o bloco será gravado, ou seja, qual bloco de dados será substituído.

Esta escolha é chamada de política de substituição de dados.

Quando se utiliza o modo de mapeamento direto em uma memória cache, ao trazer novos dados não existe escolha a ser feita para a substituição de dados, devendo apenas gravar o novo dado em seu endereço referente.

Porém, quando se utiliza modo de mapeamento completamente associativo ou associativo por conjuntos, existe uma escolha a ser feita sobre qual bloco será substituído para dar lugar ao novo bloco de dados.

Existem quatro estratégias principais para política de substituição de blocos, Aleatória, Nessa política o bloco a ser substituído será encontrado ao acaso, ou seja, o sistema irá escolher o bloco alvo através de uma escolha aleatória.

Menos Recentemente Usado, também conhecida como política LRU (Least Recently Used), esta política baseia-se em reduzir a chance de descarte dos blocos usados mais recentemente.

Desta forma, esta informação sobre quão recentemente cada bloco foi utilizado deverá permanecer junto a cada bloco para a decisão.

Desta maneira pode-se dizer que esta política prevê o futuro baseando-se no passado, e também que esta política baseia-se no princípio de localidade temporal.

Menos Freqüentemente Usado, Conhecida como política LFU (Least Frequently Used), esta política substitui os blocos que foram menos referenciados.

Esta informação sobre quão freqüente cada bloco foi referenciado pode permanecer junto a cada bloco para a decisão.

Primeiro a Entrar, Primeiro a Sair, Como pode ser difícil o cálculo do bloco que não é utilizado por um tempo mais longo, esta política também conhecida como FIFO (First In, First Out) baseia-se apenas nas informações sobre os dados mais antigos para achar o bloco alvo a ser substituído.

Podemos notar que a política LRU tende a ser de melhor desempenho em relação as outras, porém, a abordagem aleatória é a de mais fácil implementação, enquanto a política FIFO apresenta maior simplicidade em relação à política LRU, sendo que essa diferença se acentua na medida em que se aumenta o número de blocos a serem controlados.

O acesso a dados da memória cache é uma das operações mais importantes da memória cache e também a que exige maior velocidade, uma vez que ao pesquisar dados na memória cache, o processador quase sempre ficará em estado latente até que o dado esteja disponível.

A busca por dados na memória cache ocorre em três etapas básicas, O endereço pesquisado é enviado para a memória cache apropriada.

Se a memória cache sinalizar um sucesso na busca de dados, a palavra pesquisada estará disponível nas linhas de comunicação de dados.

Se a memória cache sinalizar uma falta de dados, o endereço é solicitado no nível mais alto da hierarquia de memória cache ou então na memória principal, e só então após localizar a requisição o dado estará disponível.

Quando a solicitação chega até a memória cache, esta deve pesquisar entre os dados para indicar sucesso ou falta de dados e então prosseguir com o procedimento apropriado.

O dispositivo básico de busca de dados em uma memória cache associativa de 4 vias esta ilustrado, a palavra de endereço é decomposta em três blocos principais, de tag, index e offset.

O bloco de index é utilizado para selecionar o endereço de interesse, onde as tags de todos os blocos dos conjuntos associativos devem ser comparadas.

Assim, após acessar as linhas de endereço, o segundo passo é a comparação do tag vindo da pesquisa com os tags dos blocos de memória.

Neste ponto, será identificado se houve um sucesso ou falta de dados, caso haja um acerto, o multiplexador é responsável por selecionar o dado correto e disponibilizar no canal de comunicação de dados, caso contrário a memória cache apenas indicará falta de dados.

Implementação de um dispositivo de busca de dados em uma memória cache associativa em conjunto de 4 vias, utilizando quatro comparadores e um multiplexador.

A estrutura interna de cada vetor de dados é um pouco mais complexa do que apresentada, porém, segue os mesmos princípios de funcionamento.

Apresenta a estrutura de um vetor de dados da memória cache, em um esquema mais próximo da implementação física, onde existem dois blocos de informações, um contendo informações sobre as tags e o segundo contendo os dados propriamente ditos.

Assim, com a entrada do endereço o decodificador acessa a linha referente à palavra pesquisada.

O sense amplifier é o responsável por detectar o que está dentro da célula de memória acessada.

Como cada sense amplifier é compartilhado para diversas linhas de bits, os quais são detectados um de cada vez, um multiplexador deve ser utilizado antes de cada sense amplifier.

Após identificar as informações de tag contidas no vetor de tags, o valor é comparado com o tag provido pelo endereço de entrada, onde o número de comparadores é igual ao número de conjuntos associativos da memória cache.

Assim o resultado da comparação é utilizado para ativar a saída de validade (sucesso ou falta de dados) e também ativar a saída de dados quando o dado tenha sido encontrado.

Estrutura de implementação de memória cache.

Com o aumento do tamanho das memórias cache, os diversos vetores de dados e tags foram divididos em vários bancos.

Um layout utilizado com freqüência para os bancos de memória é o chamado H-tree, assim a área ocupada pela memória cache atualmente é calculada não só considerando o tamanho dos bancos, mas também a área ocupada pelas interconexões.

Apresenta uma ilustração da disposição de 16 bancos de memória cache e as linhas de comunicação ligando todos os bancos.

Estrutura de organização de sub-blocos de memória cache.

Com as novas tecnologias de integração o tamanho das memórias cache tendem a continuar a crescer assim como a demanda por mais vazão de dados.

Mesmo a memória cache sendo dividida em diversos vetores de dados, ela deve se comportar como um grande bloco, onde o tempo de acesso a dados é igual ao tempo de acesso ao vetor de dados mais distante.

Portanto, as memórias cache devem ser projetadas considerando problemas de latência, largura de banda, interconexão, área entre outros.

Entretanto, para futuras tecnologias de integração, as memórias cache que utilizem o modelo atual de arquitetura vão sofrer com problemas de atraso do fio, desta maneira, o tempo de acesso aos dados para os grandes bancos de memória cache serão demasiadamente lentos, gerando grande contenção ao processamento.

As atuais memórias cache de múltiplos níveis são organizadas em alguns poucos níveis discretos.

Tipicamente, cada nível inclui e replica o conteúdo dos níveis menores nos níveis acima, reduzindo assim os acessos aos níveis mais próximos da memória principal.

Ao escolher o tamanho de cada nível de memória cache, o projetista deve balancear entre o tempo de acesso e capacidade, preocupado sempre em manter os custos totais de área e potência.

Para as futuras tecnologias, grandes memórias cache dentro do chip, com um tempo único e discreto de acesso a dados não são apropriadas, uma vez que problemas relacionados ao atraso do fio dentro do chip estarão cada vez maiores.

Este é o principal argumento para a criação de uma nova arquitetura de memórias cache que não seja uniforme.

Além deste, podemos citar os problemas de eventuais falhas de componentes que vão estar presentes nos próximos processadores, onde o isolamento de módulos de memória e interconexão pode garantir que mesmo ocorrendo eventuais falhas de componentes os demais continuarão trabalhando.

O principal conceito envolvendo estas novas arquiteturas não uniformes é que os dados que residam mais próximos do processador podem e devem ter tempos de acesso diferenciados.

Sendo assim, dados mais próximos serão acessados a velocidades superiores que os dados mais distantes do processador.

Além disso, problemas associados a número de portas e eventuais falhas podem ser em parte resolvidos por uma arquitetura não centralizada como veremos adiante.

Pelo funcionamento básico envolvido por trás de memórias NUCA, cada sub-banco de memória cache deve retornar a sua pesquisa o mais rápido possível sem esperar os demais resultados.

Diversos modelos de arquitetura podem ser descritos, a primeira proposta de memórias NUCA levantou a possibilidade de três tipos básicos de arquiteturas não uniformes para memória cache em comparação com dois modelos UCA.

Diversas propostas de memória cache, memórias UCA, NUCAs estáticas, e NUCA dinâmica.

Apresentando o tempo em ciclos de relógio de acesso a dados dentro de cada banco de memória Dois modelos básicos mais utilizados atualmente de memória cache com arquitetura uniforme são de um ou dois níveis.

É possível ver o ganho que se tem em adotar um nível a mais na hierarquia de memória cache considerando o tempo de acesso a dados, dado em ciclos de relógio no centro de cada bloco de memória ilustrado.

Tratando de memórias de arquitetura não uniforme dois modelos estáticos foram propostos.

O primeiro se assemelha bastante com a própria divisão da memória cache em sub-blocos como acontece internamente nas memórias UCA, porém sem os atrasos de espera por resposta de todos os sub-blocos.

O modelo de NUCA apresenta um tipo diferente de interconexão, utilizando uma rede de interconexão em chip (NoC, Network-on-Chip) para interligar os diversos blocos ao processador.

Nesta proposta, o conceito de migração de blocos está presente, visando assim, aumentar a velocidade do acesso a palavras de dados que são utilizadas várias vezes.

Para isso, a política de migração de blocos adotada deverá ser capaz de reconhecer os blocos mais referenciados e assim escalonar estes para blocos de memória mais próximos ao processador.

Baseados nestas três propostas básicas de memórias NUCA serão discutidos nas próximas seções diversos aspectos relacionados a fim de esclarecer detalhes de implementação e problemas em aberto destas arquiteturas.

A forma de mapeamento dos dados na memória NUCA é o que define como os dados serão mapeados nos bancos e em quais bancos um dado pode residir.

Diversas formas de mapeamento podem ser planejadas, uma vez que existe flexibilidade de utilização dos diversos bancos de memória.

Mesmo com a flexibilidade provida pela existência de diversos bancos de memória, as memórias NUCA estáticas (S-NUCA) prevêem apenas um mapeamento estático e único para cada endereço.

Portanto, como em uma memória UCA, cada sub-banco está associado a diversos endereços os quais devem ser respeitados.

Esta estratégia estática perde muito das vantagens de uma memória NUCA, pois dependendo do mapeamento estático adotado, certos dados sempre terão um alto tempo de acesso, podendo criar um gargalo no desempenho.

Já nas memórias NUCA dinâmicas (D-NUCA), uma total flexibilidade é planejada, onde cada dado pode ser mapeado em qualquer banco NUCA.

Entretanto, esta estratégia deve considerar que o sobrecusto de pesquisa por dados será maior, uma vez que todos os bancos devem ser pesquisados, mesmo utilizando políticas de diretório de tags ou então fazendo broadcast do endereço para todos os bancos.

Uma solução intermediária para o mapeamento de dados é chamada spread sets, que consiste em tratar diversos bancos de memória como sendo um conjunto associativo.

Assim, cada conjunto estará espalhado por diversos bancos e cada banco representará uma via do conjunto associativo.

Ainda utilizando esta solução de mapeamento, três políticas básicas de alocação de bancos são apresentadas, ilustrando uma memória NUCA de conjunto associativo de 4 vias, onde cada conjunto associativo está indicado pelas setas, e o bloco externo representa o processador.

Diferentes políticas dinâmicas de alocação de blocos de memória NUCA, mapeamento simples, mapeamento balanceado e mapeamento compartilhado.

No simple mapping (mapeamento simples) apresentado cada coluna de bancos representa um conjunto de bancos e assim, cada coluna compreende as vias de um conjunto associativo.

Para a pesquisa de dados, primeiramente deve-se selecionar a coluna de bancos (conjunto), depois selecionar a via dentro da coluna, e então pesquisar pela tag.

As desvantagens desse modelo é que o número de linhas pode não corresponder ao número de associatividade da memória e também a latência de acesso de um conjunto não é o mesmo para todas as vias de associatividade.

O mapeamento utiliza a política fair mapping (mapeamento equilibrado), esta política ataca os dois problemas apresentados pelo simple mapping, com o sobrecusto de complexidade adicional.

Esta política tenta equalizar o tempo de acesso às vias dentre os diversos conjuntos associativos, sendo este o principal ponto positivo desta política.

Porém, ponto negativo desta equalização de tempos de acesso é o sobrecusto de roteamento de blocos inerente a esta política.

A política de shared mapping (mapeamento compartilhado), tenta prover acesso rápido para todos os conjuntos associativos compartilhando os bancos próximos entre diversos conjuntos.

Para que essa política funcione, os bancos próximos ao processador devem ser compartilhados por todas as vias dos conjuntos associativos o que reduz o número de endereços nestes bancos mais próximos.

Continuando no contexto de associatividade de memória NUCA, a proposta de utiliza técnicas de predição de utilização de vias associativas para desativar vias que tendem a não ser utilizadas durante a execução de determinadas aplicações.

Isto cria uma memória NUCA dinâmica capaz de reduzir o consumo de potência sem grandes conseqüências no desempenho final.

A política de busca de dados define como será pesquisado o conjunto de bancos a fim de encontrar determinado dado.

Existem duas políticas distintas principais que podem ser utilizadas para a busca de dados entre os bancos de memória NUCA.

A primeira política chamada de incremental search (busca incremental), os bancos são pesquisados ordenadamente começando do banco mais próximo ao processador até que o bloco seja encontrado ou uma falta seja retornada pelo último banco do conjunto.

Esta política reduz o número de mensagens nas interconexões da cache e mantém baixo o consumo de potência sob a penalidade de redução do desempenho do sistema.

Na segunda política chamada de multicast search (busca em múltiplos destinos), o endereço pesquisado é enviado a todos os bancos de uma só vez.

Dessa forma, a pesquisa é feita toda em paralelo a não ser pelos diferentes tempos de roteamento da mensagem pela interconexão.

Esta política aumenta o consumo de potência, mas ganha em desempenho.

Além das duas políticas principais, existem ainda algumas alternativas híbridas como a limited multicast (múltiplos destinos limitados), onde os bancos formam conjuntos, os quais são pesquisados seqüencialmente.

Entretanto, os bancos de cada conjunto são pesquisados em paralelo.

Na política híbrida partitioned multicast (múltiplos destinos particionados), os bancos são divididos em conjuntos que são pesquisados em paralelo, sendo que os bancos dentro de cada conjunto são pesquisados seqüencialmente.

Neste caso, o paralelismo entre os bancos é similar ao das memórias UCA de múltiplos níveis.

As políticas de migração de dados apresentam as condições sobre as quais deve haver uma migração de dados de um banco para outro.

O objetivo desta migração de dados que acontece em memórias NUCA dinâmicas é maximizar o número de acertos na busca de dados nos bancos mais próximos ao processador.

Uma política interessante seria a ordenação LRU entre as linhas nos conjuntos de bancos, com os bancos mais próximos carregando as linhas MRU (Most Recently Used).

O problema com esta política é que ela poderia levar a grandes movimentos de dados, desta maneira políticas factíveis de uso devem combinar o consumo de potência e o aumento na contenção sobre a migração de dados com os benefícios esperados de um ordenamento dos dados.

Uma solução dos problemas relacionados ao grande fluxo de migrações é a política conhecida como generational promotion.

Nesta política ao acontecer um acerto na busca de dados, o bloco é enviado ao processador no mesmo tempo em que é migrado para o banco mais próximo ao processador, evitando assim movimentações de dados entre bancos distantes.

Ainda assim, existe outro problema a ser resolvido durante a migração de dados, que se refere ao que se deve fazer com os blocos retirados do banco mais próximo para dar lugar ao novo bloco.

Para solucionar esse problema, existem duas políticas para tratar o chamado bloco vítima.

A primeira política remove o bloco vítima da memória cache.

Já na segunda política o bloco vítima troca de lugar com o bloco que ocupará seu lugar.

Assim, evita-se que o bloco que poderá ser utilizado novamente seja retirado da memória cache, fazendo com que o mesmo se movimente mais lentamente para fora da memória cache de acordo com o tempo em que não esta sendo utilizado.

Diversos tipos de interconexão entre blocos de memória NUCA podem ser planejados.

Nesta seção listamos alguns tipos conhecidos a fim de avaliar os pontos positivos e negativos de cada proposta.

O modelo de memória S-NUCA 1 apresentado em estrutura os blocos de tal forma que cada bloco possui um canal de comunicações privado como pode ser visto na Embora esta proposta de canais privados possa fornecer alto desempenho existem diversos problemas associados a esta abordagem, uma vez que o tamanho desta interconexão tende a ocupar grandes áreas do chip caso haja muitos bancos a serem interligados, impossibilitando o aumento de tamanho da memória cache.

Estrutura de organização de memória S-NUCA1 (KIM 02).

Blocos de memória, compartilhando assim os canais de comunicação e roteadores como pode ser visto na Por utilizar uma Network-on-Chip (NoC) para interligar os blocos, esta abordagem garante maior escalabilidade ao modelo, podendo facilmente aumentar o número de blocos sem maiores problemas.

Além dessa vantagem, o modelo utilizando redes intra-chip pode ser projetado para utilizar mais agressivamente a estrutura de múltiplos bancos, aumentando ainda mais a velocidade de acesso aos bancos próximos ao processador, além de permitir migração de dados, priorizando blocos que são acessados mais freqüentemente.

Estrutura de organização de memória S-NUCA2 e D-NUCA utilizando redes de interconexão intra-chip.

Uma mudança no número de portas no roteador da rede de interconexão foi proposta por, onde foi modificado o esquema de utilização de um roteador por bloco de cache, para a utilização de um roteador para cada 4 blocos de cache.

Esta modificação garante broadcast mais eficiente, além disso, a redução no número de saltos até atingir todos blocos de memória pode representar aumento no desempenho e vazão de dados.

Modelo de NoC e roteador adaptado para interligar blocos de memória NUCA.

A proposta de uso de rede de interconexão híbrida visa diminuir o tempo de roteamento de endereços e dados entre o processador e os bancos de memória.

Para isso, um modelo baseado em roteadores e barramentos compartilhados entre diversos núcleos é utilizado, como pode ser visto na Uma vez que o tempo de roteamento e transmissão de dados entre roteadores costumam corresponder a menos que um ciclo, a cada ciclo parte do tempo o sistema fica desocupado, aguardando novo ciclo de transmissão de dados.

Baseado nessa afirmação a proposta híbrida ataca o problema reduzindo o número de roteadores e utiliza barramentos para aumentar o desempenho durante o broadcast de endereço nas buscas.

Além disso, caso o barramento demore mais de um ciclo para completar a transmissão, os autores propõem o uso de barramentos pipelined como solução.

Topologia de rede híbrida para um sistema mono-processado.

Muitas das propostas de NUCA abordam apenas o caso da memória cache conectada a apenas um processador, uma vez que estas memórias cache no atual contexto já apresentam complexidade suficiente para que mesmo em sistemas monoprocessados existam diversas questões relacionadas ao projeto que ainda necessitem de maior estudo.

Porém, alguns trabalhos começam a avaliar métodos de utilizar as memórias cache NUCA em sistemas multi-core.

Para estes sistemas, as principais questões de projeto são relacionadas ao sistema de migração de dados, organização dos blocos de memória e núcleos de processamento dentro do chip, políticas de controle de coerência de dados, além de controle de dados locais e compartilhados.

Portanto, projetos de processadores utilizando NUCA ainda são raros, porém, já começam a surgir idéias de projetos arquiteturais.

Um dos primeiros estudos sobre o uso de NUCA em CMP (BECKMANN, 04) sugere a utilização de uma organização conhecida como CIM (Cache In the Middle).

Podemos ver 8 núcleos de processamento nas bordas do projeto, enquanto no centro existe uma memória NUCA.

O projeto de NUCA proposto utiliza três regiões de dados, a primeira, chamada de região local é a região mais próxima de cada núcleo, a região compartilhada formada de quatro bancos no centro é chamada de região central, nos meios entre as duas regiões existe ainda a região intermediária.

Projeto de CMP com NUCA estática e dinâmica combinadas.

Mesmo esta primeira abordagem sendo bastante complexa, a idéia principal de utilizar a memória cache no centro de diversos núcleos de processamento é relativamente difundida e utilizada com algumas simplificações em outros trabalhos que visam avaliar aspectos relacionados, como controle de coerência e controle de migração de dados.

Outro projeto utilizando NUCA em CMP chama-se Nahalal, nesse projeto os processadores ficam centralizados em formato de anel enquanto as memórias cache os rodeiam e também ficam no centro.

Neste projeto, as memórias localizadas no centro são compartilhadas entre todos os núcleos formando uma área comum de alta velocidade de acesso, enquanto as áreas fora do círculo são privadas para cada núcleo de processamento.
Como pode ser visto na Ainda com este projeto inicial de processadores organizados em anel, pode-se planejar a utilização de clusters desta organização, como apresentado também na Projeto Nahalal de CMP com NUCA com espaços compartilhados no centro e isolados ao redor dos núcleos, apresentando o modelo com 8 núcleos e um projeto de cluster de Nahalal.

Um terceiro tipo de organização para CMP utilizando NUCA utiliza uma rede de interconexão de topologia mesh interligando os 16 núcleos de processamento e os bancos de memória.

O projeto visa ainda diversos tipos de compartilhamento da memória cache, variando o grau de compartilhamento chamado também de SD (Sharing Degree) de 1 até 16 como ilustrado na Nesta proposta utilizando diversos graus de compartilhamento, o grau de compartilhamento 4 apresenta os melhores resultados de desempenho.

CMP utilizando NUCA em diversos graus de compartilhamento.

Mesmo com todos os esforços, os modelos de uso de NUCA para CMP apresentam problemas parecidos de coerência de dados, o que é acentuado na utilização de memórias NUCA dinâmicas, onde muitas vezes o custo extra de utilização de migração de dados e coerência de dados não é coberto pelo ganho de desempenho apresentado por estas técnicas.

Portanto, podemos avaliar que os principais esforços para os próximos estudos sobre memórias NUCA devem estar direcionados para a redução do custo para o controle de coerência, políticas especiais de controle de coerência para NUCA, além do estudo de uso de NUCA para processadores many-core com dezenas ou centenas de núcleos.

Através dos processadores multi-core, uma ampla oportunidade de pesquisa tem surgido em função do aumento da quantidade de núcleos.

Decorrente desse aumento, a nova geração de processadores many-core também gera uma vasta relação de problemas com foco em interconexões e memórias cache conforme descrito neste curso.

A tendência apresentada pela literatura é que existem sérios problemas de escalabilidade nas soluções tradicionais de interconexões.

Por este motivo, é necessário que as arquiteturas de Networks-on-Chip sejam projetadas para que atendam requisitos de desempenho, aumentando a vazão de pacotes e diminuindo o tempo de transmissão dos dados.

Essas interconexões além de serem responsáveis pela comunicação entre núcleos, também são necessárias para comunicação com periféricos, principalmente as memórias cache.

Como conseqüência, o compartilhamento das memórias cache distribuídas pelo chip do processador implicam em um projeto completamente diferente do que se tem visto nas soluções comerciais de processadores, ou seja, as NUCAs.

O que este curso procurou mostrar é que a área de arquitetura de processadores tem evoluído rapidamente, e que a nova geração de processadores many-core demanda estudo em áreas que antes não estavam diretamente associadas ao núcleo de um processador single-core.

Resultados de pesquisas mostram que a academia e a indústria têm apresentado diversas soluções para os problemas em many-core.

Portanto, o futuro da computação de alto desempenho também está associado a uma boa escolha de arquitetura de rede em chip e memórias NUCA.

Muitos conceitos que ainda estão sendo formulados e validados implicarão diretamente no desenvolvimento e desempenho de diversos tipos de aplicações.

Virtualização é abstração que representa um recurso computacional qualquer, mas sua forma mais conhecida é através de máquinas virtuais.

Uma máquina virtual oferece um ambiente completo, muito similar ao de uma máquina física real, tendo seu próprio sistema operacional, aplicativos e serviços de rede(Internet) de forma totalmente isolada e independente uma das outras.

Devido a essa característica, a virtualização tem recebido uma atenção especial em infraestruturas de TI para efetuar a consolidação de servidores, reduzir custos de gerenciamento e de energia, prover tolerância a falhas e melhorar a segurança.

O objetivo deste trabalho é fornecer os conceitos básicos para compreender no quê consiste a virtualização, suas formas de implementação, compromissos e vantagens de suas aplicações típicas em uma infra-estrutura de TI e de processamento paralelo.

A virtualização e o uso de máquinas virtuais se tornaram um assunto destaque no mundo da tecnologia da informação (TI), apesar de não ser exatamente uma novidade.

A introdução da linguagem de programação Java nos anos 90 trouxe consigo a noção de máquina virtual, mas sua origem é mais antiga.

O termo máquina virtual foi introduzido na década de 60 como um conceito de sistemas operacionais para indicar uma abstração em software de um sistema computacional em hardware.

Já nos anos 70 era comum que cada computador (mainframe), mesmo de um único fabricante, tivesse seu próprio sistema operacional causando problemas de portabilidade e de sistemas legados.

Na época, a solução encontrada foi explorar a risca o conceito de máquina virtual, ou seja, uma camada de software que oferece um ambiente completo similar ao de uma máquina física que executa sobre um sistema computacional nativo.

Uma máquina virtual contém seu próprio sistema operacional, bibliotecas e aplicativos e é totalmente independente e isolada das demais.

Essa abordagem foi usada com sucesso pela IBM que, na linha de mainframes 370 e seus sucessores, oferecia uma máquina virtual portada para cada uma das plataformas de hardware sobre a qual as aplicações executavam.

Dessa forma era possível executar, ou migrar, uma aplicação, de uma plataforma a outra, desde que houvesse uma versão de máquina virtual para a plataforma alvo.

Na década de 80, a medida que os computadores começaram a se tornar mais comuns e terem seu hardware uniformizado, a quantidade de sistemas operacionais convergiu para algumas poucas famílias (Unix, Macintosh e Microsoft), cada uma com um público-alvo e um conjunto de aplicativos.

Nesse contexto, o emprego de máquinas virtuais perdeu importância.

Entretanto, o aumento do poder computacional dos atuais processadores, a disseminação de sistemas distribuídos e a onipresença das redes de computadores causaram, por várias razões, o ressurgimento da virtualização.

Até agora, a virtualização foi apresentada como uma técnica que permite a execução de múltiplos sistemas operacionais e de suas aplicações em máquinas virtuais sobre uma única máquina física.

Entretanto, o conceito de virtualização é mais amplo.

Segundo a EMA (Enterprise Management Association), virtualização é a técnica que "mascara" as características físicas de um recurso computacional dos sistemas, aplicações ou usuários que os utilizam.

Nesse contexto, encontramos a virtualização na implementação de desktops remotos, de discos virtuais, na computação em cluster e mesmo em dados como, por exemplo, através do uso de XML, SQL, JMS, entre outros.

Ainda, a partir do momento que se define máquinas virtuais, surge, quase que imediatamente, a necessidade de conectá-las em rede.

Todas as máquinas virtuais existentes provêm interfaces de redes virtuais idênticas as suas similares reais, isso é, com endereços MAC e podem ser interligadas a equipamentos de interconexão de rede reais, ou virtuais, como switches e roteadores.

Outro ponto interessante de uma máquina virtual é que, por ela ser um ambiente isolado, o comprometimento de sua segurança não afeta as demais.

Inclusive, é possível ter políticas de segurança diferentes para cada uma delas.

Com base no que foi mencionado até o momento, é possível imaginar que a virtualização oferece um potencial muito grande para criar e manter infra-estruturas de rede.

O objetivo deste trabalho é fornecer os principais conceitos da virtualização e de suas formas de implementação.

Atualmente existem várias ferramentas que oferecem suporte a virtualização, tanto soluções proprietárias, quanto em software livre.

As mais conhecidas são Xen.

VMware, Microsoft VirtualPC 2007 e Virtualbox, e serão apresentadas como estudos de caso.

Conclui-se com a discussão de algumas aplicações típicas da virtualização em infra-estruturas de TI e processamento paralelo.

Um computador é um sistema de computação relativamente complexo devido à variedade de componentes de hardware e de software que o constitui e às interações entre eles.

Entretanto, essa complexidade nunca foi um empecilho para o crescimento e a evolução dos computadores em geral devido ao seu projeto de forma hierárquica, com diferentes níveis de abstração e com interfaces bem definidas entre esses níveis.

O uso de níveis de abstração e interfaces, tanto para os componentes de software, como para os de hardware, permitiu que cada componente fosse visto como um subsistema independente oferecendo serviços para os demais.

Os detalhes internos de implementação de cada um deles não precisam ser conhecidos, basta conhecer as interfaces e os serviços oferecidos.

O princípio básico é o de "dividir para conquistar", o que significa dividir o sistema computacional em várias camadas funcionais hierárquicas, cada uma com um nível de abstração apropriado e provendo serviços para a camada superior através de uma interface bem definida.

Uma camada n não enxerga os detalhes internos da camada n-1, mas apenas as suas abstrações e interfaces.

Por exemplo, é possível saber o quê um processador é capaz de executar analisando seu conjunto de instruções assembly sem conhecer como ele é internamente construído.

Isso permite que um compilador gere código compatível para processadores de fabricantes diferentes, desde que eles ofereçam a mesma interface, isso é, o mesmo conjunto de instruções com os mesmos códigos de máquina.

Da mesma forma, programadores C não precisam estar cientes de detalhes do hardware e do sistema operacional quando eles usam recursos padrões da linguagem.

Entretanto, essa organização cria uma dependência entre as camadas em função das interfaces disponibilizadas, ou seja, é obrigatório respeitar uma interface para usar os serviços daquela camada.

Uma solução para eliminar essa dependência seria mapear uma interface para outra com a introdução de uma camada intermediária de adaptação.

Esse mapeamento é a base da virtualização e foi introduzido através do conceito de isomorfismo.

O isomorfismo consiste em transformar o estado de um sistema A em um estado equivalente em sistema B.

Para uma seqüência s de operações que modificam o estado E em um estado E no sistema A, existe uma i j seqüência s' que executam uma transformação equivalente do estado E 'i para o estado E no sistema B.

Apesar do isomorfismo também poder ser usado para explicar o que é abstração, fazem uma diferenciação interessante entre abstração e virtualização, enquanto uma abstração esconde detalhes internos de um componente, a virtualização os apresenta ao exterior de uma forma diferente da real.

Um computador é composto por duas grandes camadas, a de hardware e a de software, que por sua vez são divididas em subcamadas.

Para melhor compreender o princípio de funcionamento da virtualização, e os tipos de máquinas virtuais existentes, é preciso conhecer aspectos básicos dessas duas camadas.

Esses aspectos dizem respeito a alguns conceitos básicos de arquitetura de computadores e de sistemas operacionais, os quais serão apresentados a seguir.

Por arquitetura de computadores entende-se a descrição lógica e funcional dos componentes que formam o hardware de um sistema computacional e suas interações, mas sem entrar em detalhes de como cada um desses componentes é implementado internamente.

Segundo, a área de arquitetura de computadores engloba pelo menos três grandes categorias, Conjunto de instruções de máquina (Instruction Set Architecture ISA), é a abstração do processador através de seu conjunto de instruções de máquina (assembly).

Inclui, além das instruções, os modos de endereçamento possíveis e os registradores de máquina existentes.

Projeto do sistema, envolve os componentes externos ao processador como barramentos, memória, controladores, arbitramento, subsistema de E/S e suas interconexões.

Trata ainda dos mecanismos de suporte necessários a multiprocessadores, por exemplo.

Microarquitetura, descrição de como são constituídas as unidades internas de um processador e como elas são interligadas para implementar o conjunto de instruções.

Diz respeito a como um processador é organizado internamente.

Cada uma dessas categorias possui um nível de abstração que fornece um conjunto de serviços e uma interface.

Apenas as duas primeiras categorias, conjunto de instruções e projeto de sistema, são importantes no contexto deste capítulo.

Fornece uma visão esquemática das diferentes camadas de abstração encontradas em um sistema computacional incluindo o sistema operacional e seus aplicativos.

O conjunto de instruções (ou ISA) é a interface limítrofe entre o nível de abstração de hardware e o de software.

Essa interface é composta por todos os códigos de máquina aceitos pelo processador e que correspondem, cada um, a uma instrução.

Tipicamente, um processador possui pelo menos dois modos de operação, não-privilegiado e privilegiado.

A interface ISA é dividida em dois subconjuntos para refletir cada um desses modos de operação.

O primeiro é formado por todas as instruções de máquina que podem ser diretamente executadas por programas de usuário e, por isso, constitui o que denomina de instruções de usuário ou user ISA.

Os programas de usuário executam em modo não-privilegiado.

O segundo subconjunto, formado pelas instruções de sistema (system ISA), é constituído por aquelas instruções de máquina capazes de configurar o comportamento do próprio processador e de acessar componentes de hardware diretamente, como as instruções de E/S.

Essas instruções são acessíveis unicamente em modo privilegiado e, na prática, são executadas exclusivamente pelo núcleo do sistema operacional.

A user ISA e a system ISA estão assinaladas como interfaces 1 e 2, respectivamente.

Ao iniciar um sistema computacional, os primeiros passos realizados antes de se passar o controle ao sistema operacional estão relacionados com a configuração do processador e de seu hardware.

Isso é feito através da inicialização de estruturas internas e de registradores de controle do processador como, por exemplo, vetor de interrupção e a unidade de gerência de memória (Memory Management Unit, MMU), e das controladoras dos dispositivos de hardware.

Tipicamente, cada controladora possui registradores internos que permitem sua configuração, leitura e escrita de dados e a consulta de seu estado de operação.

Esses registradores são vistos pelo processador como uma série de endereços especiais definidos no momento do projeto e constituem uma interface entre o processador e os dispositivos de hardware.

As instruções de máquina que permitem configurar o processador e acessar endereços especiais de E/S pertencem ao conjunto de instruções de sistema (system ISA).

Note que, após a inicialização, o sistema operacional tem acesso ao hardware, ao sistema de memória, ao processador e aos dispositivos de E/S através dessas instruções.

Arquitetura de computadores como um conjunto de camadas de abstração (adaptada de ).

Qualquer pessoa que atualmente use um computador sabe que existe algo denominado de sistema operacional que, de alguma forma, controla os diversos dispositivos que o compõe.

A definição clássica para sistema operacional, encontrada em vários livros, é a de uma camada de software inserida entre o hardware e as aplicações que executam tarefas para os usuários e cujo objetivo é tornar a utilização do computador, ao mesmo tempo, mais eficiente e conveniente.

A utilização mais eficiente busca um maior retorno no investimento feito no hardware.

Maior eficiência significa mais trabalho obtido pelo mesmo hardware.

Isso é obtido através da distribuição de seus recursos (espaço em memória principal, processador, espaço em disco, etc) entre diferentes programas.

Cada programa tem a ilusão de estar executando sozinho no computador quando na realidade ele está compartilhando com os demais.

Uma utilização mais conveniente do computador é obtida, escondendo-se do usuário detalhes de hardware, em especial, dos periféricos de entrada e saída.

Tipicamente, isso é feito através da criação de recursos de mais alto nível oferecido através de interfaces gráficas.

Por exemplo, os usuários usam espaço em disco através do conceito de arquivos.

Arquivos não existem no hardware.

Eles formam um recurso criado a partir do que o hardware oferece.

Isso é um exemplo de virtualização de recursos.

O conceito fundamental em sistemas operacionais é o de processo.

Um processo é uma abstração que representa um programa em execução.

Um processo é representado por um espaço de endereçamento lógico composto por regiões de texto e de dados, uma pilha e um heap.

A região de texto contém o código a ser executado.

A região de dados mantém todas as variáveis globais, inicializadas ou não.

A pilha serve para armazenar o endereço de retorno de uma chamada de função, para passagem de parâmetros, além de ser também a área de memória onde são criadas as variáveis locais.

Por fim, a região de heap, que serve para a alocação dinâmica de porções de memória.

A abstração de processo em um sistema operacional.

Cada processo é um ambiente de execução isolado dos demais processos que executa sobre um processador lógico, isto é, um processador virtual, vinculado a si no momento da criação do processo.

Cabe ao núcleo do sistema operacional, através de seu escalonador, alternar os diferentes processadores lógicos (virtuais) sobre um processador físico.

A ilusão de paralelismo é criada pelo chaveamento rápido entre os processos.

De uma forma simplificada, a execução de um processo pode ser acompanhada pela abstração de dois registradores lógicos (virtuais), o contador de programa (Program Counter PC) e o apontador de pilha (Stack Pointer, SP).

O contador de programa fornece, em um dado instante de tempo, um endereço da região de texto onde se encontra a instrução a ser executada.

O apontador de pilha informa a região onde devem ser armazenados o endereço de retorno de uma chamada de função, seus parâmetros e suas variáveis locais.

É através do apontador de pilha que se lê e escreve nos parâmetros de função e nas variáveis locais a função.

Cabe ao sistema operacional, através de seu escalonador e do dispatcher, mapear os registradores lógicos PC e SP de um processo para os registradores físicos PC e SP, e únicos, do processador durante o chaveamento de contexto.

A alternância entre os diversos PC e SP lógicos, dos diferentes processos, nos registradores PC e SP físicos fornece a ilusão de que vários processos estão executando simultaneamente.

Portanto, um processo nada mais é que um tipo de máquina virtual que executa um único programa.

Outro princípio importante em sistemas operacionais é a sua estruturação em camadas hierárquicas, com seus diferentes níveis de abstrações e interfaces, de forma similar ao que ocorre em nível de hardware.

Inicialmente, na seção anterior, foi visto que um processo de usuário só pode empregar instruções não-privilegiadas (user ISA), entretanto, em certas situações, como para realizar E/S, os processos de usuário precisam executar instruções do modo privilegiado.

Para permitir isso, sem violar o princípio dos dois modos de operação, o sistema operacional disponibiliza aos programas de usuário um conjunto de chamadas de sistema (system calls).

As chamadas de sistema possibilitam que os programas de usuários acessem de forma indireta e controlada, através do sistema operacional, os recursos de hardware.

Ao fazer isso, cabe ao sistema operacional realizar todos os procedimentos necessários para garantir que o processo de usuário não comprometa o correto funcionamento do sistema.

As chamadas de sistema constituem, portanto, uma interface entre o processo de usuário e o sistema operacional.

Como mencionado anteriormente, um dos objetivos de um sistema operacional é tornar mais conveniente o uso de um sistema computacional.

Dentro desse contexto surgem as bibliotecas de funções, as quais disponibilizam uma série de funcionalidades para simplificar a construção de programas.

Por exemplo, a biblioteca de sockets provê os mecanismos para o desenvolvimento de aplicações em rede ou, ainda, a biblioteca gtk que facilita a construção de interfaces gráficas.

Cada função de biblioteca realiza um determinado procedimento e é identificada por um nome, parâmetros e valor de retorno.

O conjunto de funções de uma biblioteca constitui o que é denominado de interface aplicativa ou apenas API, do inglês, Application Programming Interface.

Algumas APIs de bibliotecas apenas encapsulam chamadas de sistemas para apresentá-las de uma forma mais amigável ao programador, enquanto outras podem ser mais complexas e envolver várias chamadas de sistema para realizar uma determinada funcionalidade.

Uma API é definida para ser usada em conjunto com uma linguagem de programação de alto nível.

Assim, por exemplo, um programador C utiliza em seu código fonte chamadas de funções de uma biblioteca com sintaxe e semântica do C.

A própria biblioteca é um programa escrito em uma linguagem de alto nível que, posteriormente, é compilada e gera um código objeto com a implementação de cada função que disponibiliza.

O código objeto de uma biblioteca, junto com o código objeto resultante da compilação de um programa fonte, forma o código executável.

Para que os programas de usuário e as bibliotecas possam executar sobre um dado sistema operacional e processador (plataforma) é preciso respeitar as chamadas de sistema disponibilizadas e ter um código binário compatível com o do processador.

Isso é obtido através da compilação do programa e das bibliotecas para essa plataforma.

O produto final é um código executável que contém os equivalentes binários das instruções de usuário (user ISA) e das chamadas de sistema (system calls).

A junção das versões binárias dessas duas interfaces é denominada de interface binária da aplicação, ou ABI (Application Binary Interface).

Na prática, o que um programa em execução "enxerga" de uma plataforma é apenas a sua ABI e é isso que será a base para a construção de máquinas virtuais.

Vale ressaltar que todas as funções de uma biblioteca são mapeadas ou para chamadas de sistema ou para instruções não privilegiadas.

Interfaces genéricas de um sistema de computação.

Um sistema operacional nada mais é que um software que executa sobre um determinado hardware com o objetivo de controlar seus recursos e oferecer um ambiente de execução para os programas aplicativos.

Esse ambiente, definido no momento da criação do processo, é composto por um espaço de endereçamento contendo as instruções do programa de usuário a serem executadas e por uma versão lógica (virtual) dos registradores do processador real.

Quando um processo é posto em execução pelo escalonador do sistema operacional, os valores dos registradores lógicos são atribuídos aos registradores reais e assim o programa é executado.

Isso equivale a imaginar que cada vez que um processo é criado, o sistema operacional atribui a ele um processador virtual.

Além de virtualizar o processador, o sistema operacional oferece para os processos de usuário uma abstração dos recursos de hardware através do subsistema E/S e do sistema de arquivos.

Em um sistema Unix, por exemplo, o subsistema de E/S classifica os dispositivos em orientados a bloco, orientados a caractere e rede.

O acesso a cada dispositivo é feito através de chamadas de sistema como put, get, ioctl etc.

O sistema de arquivos, através de sua noção de diretórios e arquivos, é como um usuário enxerga todos os dispositivos de armazenamento de dados (discos, cdrom, pendrive, por exemplo).

O acesso ao sistema de arquivos é feito através de comandos e chamadas de sistema como cd, ls, mv, open, read, write, close, entre tantos outros.

Portanto, o sistema operacional oferece para os processos de usuário um ambiente de execução que é basicamente uma máquina virtual completa (processador e dispositivos de E/S) cujo acesso é feito através das chamadas de sistema (a criação de processos também é uma chamada de sistema).

Na verdade, a máquina virtual disponibilizada para um processo de usuário pelo núcleo do sistema operacional é composta de duas partes.

A primeira é justamente uma versão virtual da máquina real que o núcleo oferece através de chamadas de sistema e das suas abstrações de dispositivos de E/S e do sistema de arquivos.

A segunda é o processador físico (real) visto pelas instruções de máquina que o processo executa (user ISA).

Essas duas partes, como visto na seção anterior, compõem a interface binária de aplicação (ABI).

Analisando sob esse ponto de vista, é possível explicar o por quê, por exemplo, de um aplicativo Windows não executar sobre um sistema operacional GNU/Linux, a máquina virtual oferecida não é a mesma que o aplicativo espera.

Além disso, há o problema de compatibilidade de código binário devido as diferentes ISA.

Por exemplo, um programa compilado para um Intel x86 compatível não tem como executar sobre um powerpc.

Note que tanto o núcleo do sistema operacional como o programa aplicativo contém instruções binárias específicas a um processador.

Em sua essência, a virtualização consiste em estender ou substituir um recurso, ou uma interface, existente por um outro de modo a imitar um comportamento.

Isso é feito através de uma camada de software responsável por transformar ações de um sistema A em ações equivalentes em um sistema B (isomorfismo).

Dependendo de como e onde essa transformação é feita, é possível classificar os softwares de virtualização em três grandes categorias, Nível de hardware, é aquela em que a camada de virtualização é posta diretamente sobre a máquina física e a apresenta às camadas superiores como um hardware abstrato similar ao original.

Corresponde à definição original de máquina virtual dos anos 60.

Nível de sistema operacional, é um mecanismo que permite a criação de partições lógicas em uma plataforma de maneira que cada partição seja vista como uma máquina isolada, mas que compartilham o mesmo sistema operacional.

Nesse caso, a camada de virtualização se insere entre o sistema operacional e as aplicações.

São exemplos desse tipo de abordagem o FreeBSD Jails, Linux Vserver, OpenVZ e as zonas do Solaris.

Nível de linguagens de programação, a camada de virtualização é um programa de aplicação do sistema operacional.

O objetivo é definir uma máquina abstrata sobre a qual executa uma aplicação desenvolvida em uma linguagem de programação de alto nível específica.

Essa é a abordagem da máquina virtual Java e da Microsoft Commom Language Infrastructure, base do Net.

Embora cada um desses tipos de virtualização tenha um objetivo diferente, eles possuem um conjunto comum de características desejáveis.

Primeira, oferecer compatibilidade de software.

Isso significa que todo software desenvolvido para uma máquina virtual deve executar nela independente de onde ela esteja sendo usada.

Para isso acontecer, a máquina virtual deve mascarar as diferenças de software e de hardware existentes.

Esse requisito traduz a filosofia Java de "write once, run anywhere".

Segunda, isolamento, um software em execução em uma máquina virtual não deve ver, afetar ou ser afetado por outro software em execução em outra máquina virtual.

Terceira, encapsulamento, permitir a qualquer momento a captura do estado completo do ambiente virtual parando a sua execução (suspend).

Isso de ser feito de forma a possibilitar que, posteriormente, a execução seja retomada a partir desse estado (resume).

Por fim, a camada de virtualização deve ser projetada e implementada de forma a não impactar em demasia o desempenho das aplicações que executam sobre ela.

Isso representa uma relação custo-benefício que diz respeito a como as máquinas virtuais são implementadas.

Esse assunto será desenvolvido nas próximas seções.

Para melhor compreender o conceito de máquina virtual é importante notar que existem duas perspectivas diferentes a serem consideradas, a de um processo e a do núcleo do sistema operacional.

Sob o ponto de vista de um processo de usuário, uma máquina consiste em um espaço de endereçamento lógico onde as instruções e dados do programa residem.

Os dispositivos de E/S existentes, e o tipo de acesso que se pode fazer a eles, são aqueles definidos pelo núcleo do sistema operacional e disponibilizados através das chamadas de sistema.

A interface entre a aplicação e a máquina virtual é feita através do conjunto de instruções não-privilegiadas (user ISA) e pelas chamadas de sistema (system calls).

Com uma máquina virtual de processo, uma aplicação de usuário tem a ilusão de estar executando sozinha na máquina física.

Já na perspectiva do núcleo do sistema operacional, uma máquina virtual é um ambiente completo que oferece suporte de execução para várias aplicações (processos).

Cabe ao núcleo do sistema operacional gerenciar a utilização dos recursos de hardware como memória, acesso a dispositivos de E/S e mesmo o processador entre os vários processos.

Além disso, como visto anteriormente, o núcleo do sistema operacional ainda oferece uma visão abstrata dos recursos de hardware como, por exemplo, os meios de armazenamento através do sistema de arquivos.

A interface entre o núcleo do sistema operacional e o hardware físico é feita através do conjunto de instruções (ISA) do processador.

Diferentemente de um processo, que existe apenas enquanto está em execução, o núcleo do sistema operacional deve existir enquanto o sistema computacional estiver ativo.

Essa constatação leva a afirmação de que uma máquina virtual de processo tem uma existência temporária enquanto que a máquina virtual de sistema é perene.

Cada uma dessas perspectivas dá origem a um tipo diferente de máquina virtual, as máquinas virtuais de processo e de sistema, que veremos a seguir.

Antes, porém, alguns detalhes quanto à terminologia.

O processo ou sistema operacional que executa sobre uma máquina virtual é denominado de hóspede ou convidado, enquanto que a plataforma subjacente onde a máquina virtual executa é denominada de hospedeiro ou sistema nativo.

A camada de virtualização que implementa a máquina virtual é genericamente denominada de runtime (ou executivo) para as máquinas virtuais de processo, e de monitor de máquina virtual (Virtual Machine Monitor, VMM), ou hipervisor (hypervisor), para as máquinas virtuais de sistema.

Uma máquina virtual de processo é aquela que fornece um ambiente de execução para uma única aplicação de usuário através de uma ABI virtual (chamadas de sistema e user ISA).

Um ponto importante a destacar é que um processo é uma entidade efêmera, ou seja, ele existe apenas quando o programa está em execução.

Portanto, uma máquina virtual de processo é criada sob demanda e só existe enquanto o processo estiver executando.

Uma aplicação executa sobre um programa que implementa a máquina virtual de processo (runtime ou executivo).

A máquina virtual utiliza as funcionalidades providas pelo sistema operacional, como chamadas de sistema e funções de biblioteca, e pelo próprio processador através de instruções não-privilegiadas.

A aplicação emprega apenas a interface (ABI) provida pelo ambiente (máquina) virtual.

Máquina virtual de processo.

Um desafio importante na concepção de máquinas virtuais de processo é quando a aplicação hóspede possui um código binário para um processador diferente daquele sobre o qual a máquina virtual executa, o hospedeiro.

Por exemplo, uma aplicação compilada para Intel 32 que se deseja executar sobre um processador sparc.

Existem várias maneiras de resolver esse problema, a mais direta é através da interpretação.

Um interpretador é um programa que realiza um ciclo de busca de instrução, decodificação e emulação dessa instrução para o sistema hospedeiro.

Um exemplo típico dessa abordagem é a linguagem de programação Java e de sua máquina virtual (Java Virtual Machine JVM).

A JVM é uma máquina virtual de processo que possui um código binário específico, os bytecodes, para os quais uma aplicação Java é compilada.

Ao se lançar uma aplicação Java, na verdade, se está executando a máquina virtual Java passando como parâmetro um arquivo binário bytecode.

A máquina virtual Java então interpreta, bytecode a bytecode, transformado-os em ações e instruções equivalentes da máquina real subjacente (hospedeiro).

A principal vantagem desse método é a portabilidade, mas o processo de interpretação pode ser bastante oneroso em termos de desempenho.

A tradução binária dinâmica surge como uma solução para melhorar o desempenho da interpretação feita nas máquinas virtuais de processo.

Nesse caso, blocos de instruções da aplicação hóspede são traduzidos em blocos de instruções do sistema hospedeiro, o que já pode levar a alguma otimização em relação a se fazer instrução por instrução.

Além disso, os blocos traduzidos podem ser armazenados em uma cache interna e posteriormente reaproveitados amortizando assim o custo da tradução.

Essa técnica é denominada de compilação Just-In-Time (JIT).

Uma abordagem alternativa para a camada de virtualização é a definição de uma máquina virtual de sistema.

Aqui a máquina virtual oferece um ambiente de execução completo onde podem coexistir um sistema operacional e vários processos, possivelmente de diferentes usuários.

Dessa forma, uma única plataforma de hardware pode executar múltiplos sistemas operacionais hóspedes, um em cada máquina virtual, simultaneamente.

É esse tipo de abordagem que permite a consolidação de servidores que será apresentada na Existem duas formas básicas de implementação de máquinas virtuais de sistema ou hipervisores.

Os hipervisores tipo I, ou nativos, são aqueles que executam diretamente sobre o hardware de uma máquina real e as máquinas virtuais são postas sobre ele.

A função básica de um hipervisor nativo é compartilhar os recursos de hardware (processador, memória, meios de armazenamento e dispositivos de E/S) entre as diferentes máquinas virtuais de forma que cada uma delas tenha a ilusão de que esses recursos são privativos a ela.

Esse tipo de hipervisor corresponde ao originalmente implementado nos sistemas IBM no início da década de 70.

Atualmente, o Xen e o VMware ESX Server são exemplos de hipervisores que adotam essa abordagem.

Os hipervisores tipo II, ou hóspedes, são caracterizados por executar sobre um sistema operacional nativo como se fossem um processo deste.

Nesse caso, o que o hipervisor oferece é uma camada de virtualização composta por um sistema operacional hóspede, possivelmente diferente do sistema operacional nativo, e por um hardware virtual criado sobre os recursos de hardware oferecidos pelo sistema operacional nativo.

São exemplos dessa abordagem o Vmware Player, VirtualBox e o MS VirtualPC 2007.

Um outro aspecto a ser considerado nos hipervisores é em relação ao quê é realmente virtualizado.

Uma possibilidade é fornecer uma ABI (chamadas de sistema e user ISA) completamente diferente daquela do sistema nativo.

Nesse caso, o hipervisor hóspede precisa emular todas as instruções executadas o que representa um custo no desempenho.

Entretanto, essa estratégia permite que um sistema operacional, e suas aplicações, executem em plataformas diferentes daquelas para os quais ele foi concebido.

Um exemplo dessa abordagem é o VMware Fusion que oferece um ambiente Windows para o sistema operacional MacOS X.

Máquina virtual de processo, hipervisores tipos I e II.

Diferentes visões da virtualização.

A segunda possibilidade considera que a máquina virtual (hipervisor) forneça um conjunto de instruções não-privilegiadas (user ISA) idêntico ao do sistema nativo sobre o qual executa.

Nesse caso, não é necessário emular um conjunto de instruções a partir de outro.

A virtualização, nesse caso, se reduz às chamadas de sistema e os recursos de hardware (discos, memória, etc) que compõem a máquina virtual.

Esse é o caso, por exemplo, do VMwarePlayer, do VirtualBox e VirtualPC quando executam sobre um sistema operacional Windows e em processadores Intel IA-32 e oferecem à aplicação final um sistema operacional GNU/Linux.

A implementação das máquinas virtuais não é tão simples como possa aparentar em um primeiro momento.

Além da preocupação direta com desempenho, existe o problema de como os recursos físicos da máquina são compartilhados entre o sistema nativo e o sistema hóspede sem que um interfira em outro, a começar pelo próprio processador.

O problema fundamental consiste no que fazer quando o sistema hóspede executa instruções privilegiadas (System ISA), já que essas, por uma questão de proteção do sistema, são exclusivas ao sistema nativo (hospedeiro).

A ação a ser tomada, depende de recursos oferecidos pela arquitetura do processador (hardware).

Em 1974, classificaram o conjunto de instruções de um processador em três grupos, privilegiadas, que se executadas por um programa em modo usuário causam exceções (trap), sensíveis de controle, que permitem a alteração de recursos do sistema, e  sensíveis comportamentais, cujo resultado ou comportamento dependem da configuração de recursos como, por exemplo, conteúdo de registradores internos ou modos de execução do processador.

Com base nessa classificação, Popek e Goldberg, propuseram o seguinte teorema, "um monitor de uma máquina virtual pode ser implementado de forma adequada sempre que as instruções sensíveis de controle e sensíveis comportamentais forem um subconjunto das instruções privilegiadas".

Na prática, isso se traduz no fato que qualquer instrução que possa afetar o comportamento do sistema deve ser monitorada e tratada adequadamente.

Para melhor compreender esse problema que envolve o conjunto de instruções e a virtualização é interessante analisar a arquitetura x86.

A arquitetura x86 provê quatro modos de operação para o processador, identificados de 0 a 3, denominados de anéis de proteção (rings) ou CPL (Current Privilege Level).

Nos sistemas operacionais convencionais (Microsoft Windows e UNIXes), para esse tipo de arquitetura, apenas dois modos são usados.

O ring 0, que detém os maiores privilégios de execução, é usado pelo sistema operacional, e o ring 3, de menor privilégio é empregado pelos processos de usuário.

Se um processo de usuário tentar executar uma instrução privilegiada ocorrerá uma exceção (trap) que deverá ser tratada adequadamente.

Entretanto, a arquitetura x86, em especial o Pentium, possui dezessete instruções sensíveis que são não privilegiadas, ou seja, programas em modo usuário podem afetar o funcionamento do processador sem gerar traps.

Entretanto, a virtualização é possível mesmo em processadores que não seguem a restrição enunciada no teorema de Popek e Goldberg, mas ao custo de um pior desempenho.

A virtualização nessas arquiteturas é feita tratando as instruções sensíveis de acordo com duas estratégias, virtualização total e paravirtualização.

A virtualização total consiste em prover uma réplica (virtual) do hardware subjacente de tal forma que o sistema operacional e as aplicações possam executar como se estivessem diretamente sobre o hardware original.

A grande vantagem é que o sistema operacional hóspede não precisa ser modificado para executar sobre o monitor de máquina virtual (VMM) ou hipervisor.

Virtualização total.

No entanto, a virtualização total tem alguns inconvenientes.

Primeiro, por não ser modificado, todas instruções executadas pelo sistema hóspede devem ser testadas na máquina virtual para saber se elas são sensíveis ou não, o que representa um custo de processamento.

As instruções sensíveis devem ser interceptadas e emuladas no hospedeiro para evitar que a máquina virtual altere o comportamento do sistema nativo.

Essa interceptação e emulação podem ser onerosas se o processador nativo não possuir suporte em hardware para virtualização.

Atento a isso, os fabricantes de processadores, Intel e AMD, desenvolveram extensões para arquitetura x86 para prover mecanismos que facilitassem essa tarefa.

A Intel apresenta suas extensões para as arquiteturas x86 de 32 e 64 bits sob o nome IVT (Intel Virtualization Technology), ao passo que a AMD oferece esse suporte apenas para suas arquiteturas de 64 bits.

A extensão da AMD é denominada de AMD-V, AMD-Virtualization.

As soluções da Intel e da AMD foram desenvolvidas independentemente uma da outra e, embora sirvam para o mesmo propósito, são incompatíveis.

O segundo inconveniente da virtualização total é a dificuldade em se implementar uma máquina virtual que imite o comportamento exato de cada tipo de dispositivo, dada a diversidade de dispositivos existentes que compõem um computador.

A solução consiste em prover a máquina virtual com suporte a um conjunto genérico de dispositivos.

Tipicamente, cada máquina virtual possui um teclado e mouse do tipo PS/2, unidades de disquete, controladores IDE, cdrom ATAPI, portas seriais e paralelas, suporte USB, uma placa gráfica padrão e as placas de redes mais comuns em ambientes PC.

Sendo assim, pode-se ter uma subutilização de um recurso de hardware real.

Por fim, a implementação de uma máquina virtual de processo deve contornar alguns problemas técnicos relativos a implementação da gerência de memória.

Por exemplo, o Linux e o Windows implementam memória virtual através de paginação.

Há toda uma gerência de alocação, liberação e controle de acesso às páginas que devem ser respeitadas.

A questão que se apresenta é, deve a máquina virtual pré-alocar uma quantidade de páginas do sistema nativo e emular sobre elas um espaço de endereçamento físico para o sistema hóspede ou pode-se usar diretamente a gerência do sistema nativo (hospedeiro)?

Grosso modo, é necessário "converter" o espaço de endereçamento do sistema hóspede para o do sistema nativo disputando recursos com outros eventuais sistemas hóspedes.

Tecnicamente não há maiores empecilhos em se fazer isso, porém, esse tratamento também representa uma queda de desempenho.

A estratégia de paravirtualização aparece como uma abordagem alternativa para contornar os problemas de desempenho da virtualização total.

Nessa abordagem, o sistema hóspede deve ser modificado para chamar a máquina virtual sempre que for executar uma instrução ou ação considerada sensível.

Na prática isso traduz pela necessidade de alterar todas as instruções de sistema (system ISA) do hóspede por chamadas a máquina virtual para que ela interprete e emule essas ações de forma adequada.

Isso é a principal desvantagem da paravirtualização.

A vantagem é que as instruções de usuário (user ISA) não precisam ser alteradas e podem ser executadas diretamente sobre o processador nativo.

Ao preservar o user ISA, todas as aplicações que foram desenvolvidas para o sistema hóspede podem ser executadas sem alterações.

Note que, nesse caso, se considera que tanto o sistema hóspede quanto o hospedeiro (sistema nativo) são compilados para a mesma plataforma.

Paravirtualização.

Uma outra vantagem da paravirtualização é que ela provê aos sistemas hóspedes acessos aos recursos de hardware a partir dos drivers instalados no próprio hipervisor.

Dessa forma, os sistemas hóspedes podem usar os recursos reais da máquina e não apenas dispositivos genéricos como era o caso na virtualização total.

Ainda, a máquina virtual apenas gerencia o compartilhamento do hardware e monitora as áreas de memória e de disco alocadas para cada um dos sistemas hóspedes.

Em decorrência dessa gerência, a máquina virtual pode informar ao sistema operacional hóspede qual a área de memória física que está alocada para ele.

Assim, a gerência de memória do hóspede é capaz de traduzir diretamente as páginas virtuais em quadros físicos, sem necessitar de uma etapa adicional de conversão.

A virtualização se tornou a grande revolução da área de TI nesses últimos anos, basta ver o crescimento do volume de investimento das empresas nesse sentido e o crescimento das empresas que oferecem soluções de virtualização.

Atualmente, existem disponíveis várias soluções de virtualização.

Para ter uma idéia dessa quantidade, consulte a wikipedia, ou o google, fazendo uma busca por "virtualization".

Basicamente, existem soluções comerciais, gratuitas, em software livre, integradas a sistemas operacionais, etc.

Seria inviável, e fora do escopo deste capítulo, tecer comentários sobre todas elas, por isso optou-se por apresentar apenas o Xen, VMware, Virtual PC 2007 e VirtualBox, por serem as mais comuns.

O Xen é um monitor de máquina virtual (hipervisor) em software livre, licenciado nos termos da GNU General Public Licence (GPL), para arquiteturas x86, que permite vários sistemas operacionais hóspedes serem executados em um mesmo sistema hospedeiro.

A primeira versão do Xen é de outubro de 2003 e, originalmente empregava a estratégia de paravirtualização, ou seja, era necessário modificar o sistema operacional hóspede para torná-lo consciente da existência do hipervisor.

Essa decisão se justificava por questões de desempenho, mas limitou o emprego do Xen aos sistemas Unix, principalmente aqueles com filosofia de código aberto.

A partir da versão 3, o Xen passou a oferecer virtualização total, ou seja, permite o uso de sistemas operacionais não modificados, como os da família Microsoft Windows.

Entretanto, isso só é possível se o processador oferecer suporte em hardware (Intel VT ou AMD-V) para virtualização.

A arquitetura do Xen é um pouco diferente daquela apresentada na seção 4 4 Os dois principais conceitos do Xen são domínios e hipervisor.

Os domínios são as máquinas virtuais do Xen e são de dois tipos, privilegiada (domínio 0) ou não-privilegiada (domínio U).

O hipervisor tem por função controlar os recursos de comunicação, de memória e de processamento das máquinas virtuais, e não possui drivers de dispositivos.

O hipervisor Xen, considerando suas características, não é capaz de suportar nenhum tipo de interação com sistemas hóspedes.

Por isso, é necessário que exista um sistema inicial para ser invocado pelo hipervisor.

Esse sistema inicial é o domínio 0.

As outras máquinas virtuais só podem ser executadas depois que ele for iniciado.

As máquinas virtuais de domínio U são criadas, iniciadas e terminadas através do domínio 0.

O domínio 0 é uma máquina virtual única que executa um núcleo Linux modificado e que possui privilégios especiais para acessar os recursos físicos de entrada e saída e interagir com as demais máquinas virtuais (domínios U).

O domínio 0, por ser um sistema operacional modificado, possui os drivers de dispositivos da máquina física e dois drivers especiais para tratar as requisições de acesso à rede e ao disco efetuados pelas máquinas virtuais dos domínios U.

Para oferecer suporte tanto para a paravirtualização como para a virtualização total, o Xen distingue os domínios U entre paravirtualizados (domínios U-PV) e virtualizados (domínios U-HVM, de Hosted Virtual Machines).

Os domínios U-PV têm consciência de que não tem acesso direto ao hardware e reconhecem a existência de outras máquinas virtuais.

Os domínios U-HVM não têm essa consciência, nem reconhecem a existência de outras máquinas virtuais.

Na prática, isso se traduz no fato de que os domínios U-PV possuem drivers específicos para acesso à rede e ao disco para interagirem com as suas contra-partidas no domínio 0.

Já as máquinas dos domínios U-HVM não possuem esses drivers (não foram modificados) e iniciam como um sistema convencional procurando executar a BIOS.

O Xen virtual firmware simula a existência da BIOS fazendo todos os procedimentos esperados durante o boot normal de um ambiente PC compatível.

O compartilhamento do disco e as requisições de rede de um domínio U-HVM são feitos através de um daemon Qemu vinculado a cada instância U-HMV (O QEMU é um emulador em software de código livre).

O hardware disponível para as máquinas virtuais do domínio U-HVM são aquelas oferecidas pelo QEMU.

A VMware player é um monitor de máquina virtual que segue a estratégia de virtualização total.

Na realidade, por questões de desempenho, o VMware Player não executa completamente em espaço de usuário, pois é instalado um driver de dispositivo específico, (VMDriver) que permite que as máquinas virtuais acessem os drivers de dispositivo do sistema hóspede.

Por exemplo, o VMDriver põe a placa de rede em modo promíscuo e cria uma bridge ethernet virtual que recebe todos os quadros ethernet e os reencaminha para o sistema hóspede ou para a máquina virtual especificada.

Essa implementação tambémoferece NAT (Network Address Translation), de tal forma que cada interface virtual tem seu próprio endereço IP.

A gerência de memória também é feita de forma híbrida.

Cada instância do VMware recebe uma área de memória do sistema operacional nativo e a gerencia diretamente.

A máquina virtual VMware Player é disponível gratuitamente para os sistemas operacionais nativos Windows e Linux e existe uma versão comercial (VMware Fusion) para o sistema operacional MacOS X.

O objetivo é permitir que usuários tenham contato com a virtualização.

A VMware, a partir de seu site, distribui uma série de imagens de sistemas operacionais, denominadas de appliances, que contemplam diferentes distribuições linux e Windows Server 2003.

A VMware Player não permite que um sistema operacional hóspede seja criado na máquina virtual a partir de zero.

Para que isso seja possível, dentro das opções oferecidas pela VMware, é necessário uma outra versão, VMware Workstation, que é paga.

Na realidade, a VMware oferece uma infra-estrutura de virtualização completa com produtos abrangendo desde desktops a data centers organizados em três categorias, gestão e automatização, infra-estrutura virtual e virtualização de plataformas.

Cada categoria possui um conjunto de produtos específicos.

Os produtos de gestão e automatização têm por objetivo principal, como seu próprio nome induz, permitir de uma forma automatizada e centralizada a gerência de todos os recursos da infra-estrutura virtual permitindo a monitoração do sistema, auxiliando na conversão de sistemas físicos em virtuais, na recuperação de desastres, entre outros.

Os produtos de infra-estrutura virtual auxiliam a monitoração e alocação de recursos entre as máquinas virtuais de forma a atender requisitos e regras de negócios.

Eles fornecem soluções para alta-disponibilidade, backup, migração de máquinas virtuais e atualização de versões de softwares.

Por fim, os produtos de virtualização de plataformas, ou seja, aqueles destinados a criar máquinas virtuais.

Essa categoria é composta vários produtos onde se destacam, além dos produtos mencionados anteriormente, o VMware ESX Server 3, o VMware ESX Server 3 i, o VMware Virtual SMP, o VMware VMFS e VMware Server, que auxiliam a criação de data centers virtuais, exploração de mais de um processador por máquina virtual e a criação de sistemas de arquivos.

Atenta ao movimento da virtualização, a Microsoft oferece uma gama de produtos para esse tipo de tecnologia.

Esses produtos exploram o conceito da virtualização, na sua forma mais ampla, para oferecer soluções que sejam integradas e apropriadas à infra-estrutura de TI que se encontra hoje em dia.

Basicamente, Virtualização de aplicações, também denominada de SoftGrid, cujo objetivo é fornecer aplicações por demanda.

Isso implica que se um determinado desktop necessita executar uma aplicação e a mesma não está disponível nele, o sistema executará automaticamente a busca, instalação e configuração da aplicação.

Virtualização de apresentação, essa ferramenta separa e isola as tarefas de tratamento gráfico (visualização) e de E/S, permitindo que uma determinada aplicação seja executada em uma máquina, mas utilize recursos gráficos e de E/S de outra.

Gerenciamento da virtualização, System Center Virtual Machine Manager é um ambiente de gerenciamento que facilita as tarefas de configuração e de monitoração de um ambiente virtual.

É através dele que se realiza a administração das contas de usuários e seus privilégios.

Virtualização de desktops (Virtual PC), permite a criação de máquinas virtuais em um sistema hospedeiro Microsoft Windows, cada uma com seu próprio sistema operacional.

Basicamente destina-se àquelas aplicações onde é necessário executar software legado, criar ambientes de testes, realizar treinamentos, etc.

Virtualização de servidores, é a solução que permite criar máquinas virtuais em servidores.

Nessas máquinas, questões ligadas à segurança, tolerância a falhas, confiabilidade, disponibilidade se tornam importantes.

Portanto, a solução de virtualização de servidores, denominada de Hyper-V (ou viridian) foi projetada para endereçar esses requisitos.

Dentre esses produtos, o que se enquadra na discussão desta seção é o virtual PC 2007 para virtualização de desktops.

O Virtual PC 2007 é uma máquina virtual para família Windows que pode ser configurada para executar qualquer outro sistema operacional.

A estratégia adotada é a da virtualização total.

Segundo a Microsoft, o principal objetivo do Virtual PC é o desenvolvimento e teste de software para múltiplas plataformas.

Dentro desse princípio, o Virtual PC oferece mecanismos para interconectar logicamente as diferentes máquinas virtuais.

Cada máquina virtual tem seu próprio endereço MAC e endereço IP.

Além disso, o Virtual PC oferece um servidor de DHCP, um servidor NAT e switches virtuais.

Dessa forma, é possível construir cenários de rede usando máquinas virtuais.

O virtual PC 2007 é disponível para download, assim como um white paper que ensina a configurar as máquinas virtuais e um ambiente de rede.

Um ponto interessante a comentar em relação à gratuidade do Virtual PC é que, na FAQ do Virtual PC, a Microsoft alega que o que tem valor agregado não é a máquina virtual em si, mas sim os ambientes de gerenciamento.

Uma diferença que o Virtual PC 2007 tem em relação ao VMware é a possibilidade de se definir máquinas virtuais e instalar o seu próprio sistema operacional.

É importante salientar que mesmo em uma máquina virtual as restrições de licenças são aplicadas.

O VirtualBox é um monitor de máquina virtual que adota a abordagem de virtualização total.

O VirtualBox é parte integrante de sua solução de virtualização da Sun Microsystems, o Sun xVM.

Existem duas versões do VirtualBox, uma versão em software livre, sob os termos da GPL (GNU General Public Licence) denominada de VirtualBox OSE (Open Source Edition) e outra comercial (versão full).

A diferença básica entre elas é o suporte que a versão comercial oferece para RDP (Remot Desktop Protocol), para USB e para i-SCSI (mecanismo de acesso a discos SCSI remotos).

O VirtualBox executa sobre sistemas operacionais Microsoft Windows, Linux, Macintosh e OpenSolaris permitindo vários sistemas hóspedes, entre eles, os mais comuns, Microsoft Windows, GNU/Linux e OpenBSD, entre outros.

A virtualização feita pelo VirtualBox executa todas as instruções de usuário (user ISA) nativamente no processador.

Isso, é claro, se houver compatibilidade binária entre o sistema hóspede e o hospedeiro.

As instruções de sistema (system ISA) são interceptadas e emuladas no hospedeiro.

Essa emulação é feita com o auxílio da técnica de tradução binária dinâmica.

Ainda, o VirtualBox emula um disco virtual com um arquivo em um formato específico, o Virtual Disk Image, mas possui a capacidade de ler e escrever imagens de discos virtuais VMware (Virtual Machine Disk Format, VMDK).

Isso possibilita que o VirtualBox possa ser inicializado a partir de uma imagem definida e criada por ferramentas da VMware.

Outra característica interessante no VirtualBox é o fato da permitir a montagem de imagens ISO.

Assim, é possível, por exemplo, usar uma imagem de uma distribuição linux sem ter que "queimar" um CD/DVD Como já comentado anteriormente, as máquinas virtuais oferecem um ambiente completo, cada uma contendo seu próprio sistema operacional, bibliotecas e aplicativos de uma forma totalmente auto-contida e independente.

Essa característica pode ser explorada de forma a trazer uma série de benefícios para diferentes infra-estruturas de computação.

As aplicações típicas de máquinas virtuais são a consolidação ou virtualização de servidores, virtualização de desktops e segurança (honeypots).

O emprego da virtualização em ambientes de processamento paralelo e distribuído é um exemplo de uso dessas aplicações típicas, mas em um outro contexto.

Iremos elaborar um pouco mais cada uma dessas aplicações.

Hoje em dia é muito difícil imaginar um sistema computacional que não seja conectado em rede.

Na prática, essa conectividade faz com que os administradores de rede sejam responsáveis por manter um conjunto grande e heterogêneo (hardware e/ou softwares distintos) de servidores, cada um executando uma aplicação diferente que pode ser acessada por clientes também heterogêneos.

É comum encontrarmos em infra-estruturas de rede uma filosofia "um servidor por serviço" por motivos que variam desde a heterogeneidade de clientes à segurança.

Normalmente, a carga de processamento de um servidor não explora todo o poder computacional disponibilizado pelo processador.

Há um desperdício de ciclos de processamento e, por conseqüência, de investimento.

Nessa situação, pode-se imaginar cada serviço executando em uma máquina virtual individual, com seu próprio sistema operacional, mas sobre um mesmo hardware físico, proporcionando um uso eficiente do poder de processamento disponível.

Isso é o que se denomina de consolidação de servidores.

O princípio básico é o melhor aproveitamento de recursos, ao invés de haver n servidores com percentual de utilização de x é possível ter um único servidor com um percentual de uso de aproximadamente nx (nx < 100).

A consolidação de servidores contribui para a diminuição de máquinas físicas, o que auxilia na redução de custos de infra-estrutura como espaço, energia elétrica, cabeamento, refrigeração, suporte e manutenção de vários sistemas.

Inicialmente, vamos discutir o caso de pequenas e médias organizações e seus servidores.

Nesse contexto, é comum que os servidores sejam máquinas ultrapassadas e "intocáveis".

Afinal, se os serviços executados não excedem a capacidade dessas máquinas, por que investir em equipamentos novos?

Além disso, atualizar máquinas implica em mexer em sistemas que, bem ou mal, estão funcionando.

Entretanto, essa situação traz um risco inerente, um hardware está sujeito a falhas, principalmente os discos, e, muitas vezes, para equipamentos ultrapassados, os contratos de manutenção não são mais válidos e as peças de reposição simplesmente inexistem.

É uma questão de tempo ter um problema (grave) de indisponibilidade de serviços.

Para esses casos, uma solução possível é adquirir apenas um único equipamento novo e instalar nele tantas máquinas virtuais quanto serviços houver.

É claro que se deve dimensionar a capacidade total de processamento desse novo servidor em relação à carga de serviços que ele receberá.

A vantagem é trocar um parque de máquinas ultrapassadas e, eventualmente, subutilizadas, por uma máquina mais nova e bem utilizada.

Os serviços que executam nas máquinas mais antigas podem ser migrados, um a um, para uma máquina virtual na nova máquina, facilitando o processo de transição.

Dependendo da solução de virtualização escolhida, existem ferramentas que auxiliam nessa migração.

A notar, ainda, que cada máquina virtual tem seu próprio sistema operacional, portanto, é possível pôr em um único equipamento todos os serviços, ou aplicações, que executam em diferentes sistemas operacionais.

Futuramente, se for necessário uma nova atualização de hardware, basta parar as máquinas virtuais, fazer uma imagem delas e as inicializar novamente, sem necessitar reinstalar e configurar softwares específicos.

Um segundo caso a analisar são os data centers, onde as vantagens discutidas anteriormente tomam uma envergadura maior.

Por sua própria natureza, em um data center é comum haver diferentes sistemas operacionais e, eventualmente, mais de uma versão de um mesmo sistema operacional devido a necessidades específicas de produtos que seus clientes executam.

Sem virtualização, cada sistema operacional (e versão) precisaria de um hardware dedicado.

Isso representa um custo de instalação, manutenção, refrigeração, energia e suporte técnico para administrá-los.

Já com a virtualização, um único hardware, dimensionado adequadamente, pode manter vários sistemas operacionais de forma mais econômica.

Os procedimentos de migração e de tolerância a falhas são importantes em data center.

A migração pode ser usada no momento de atualizar parques de máquinas movendo um serviço de uma máquina para outra enquanto a troca é feita.

A migração ainda é útil como mecanismo de balanceamento de carga, pois permite mover serviços de uma máquina a outra até que a carga do sistema seja homogênea.

Isso evita que certos sistemas fiquem sobrecarregados enquanto outros estão ociosos.

A virtualização também traz uma série de benefícios quando empregada em desktops.

Inicialmente, a virtualização oferece uma forma simples para testar novas configurações ou executar programas que foram feitos para sistemas operacionais diferentes do nativo.

Dessa forma, um usuário que deseja testar um software, ou abrir um programa, que não foi desenvolvido para seu sistema operacional pode lançar mão desse recurso.

Há uma série de ferramentas para o uso de virtualização em desktops, onde entre elas, se destacam os produtos da VMmare e o Virtual PC por serem de fácil instalação e uso.

Os projetistas de software são um tipo de usuário que podem se beneficiar ao executar programas de diferentes sistemas operacionais em uma única máquina física.

As máquinas virtuais permitem isso de maneira bastante simples, o que é interessante em fases de desenvolvimento e depuração de software.

Um outro ponto, vinculado à depuração de sistemas, é a facilidade que as máquinas virtuais têm de registrar uma espécie de instantâneo (snapshot) de seu estado quando finalizadas.

Isso permite que uma análise seja parada em qualquer instante e retomada mais tarde a partir do mesmo ponto.

Outra utilização interessante da virtualização de desktops é em laboratórios de treinamento e de ensino, como aqueles encontrados em universidades.

Para algumas disciplinas é necessário ter ambientes com diferentes sistemas operacionais.

Apesar de ser possível, e comum, se configurar um PC compatível com múltiplos boots, essa solução nem sempre é a mais adequada.

Primeiro, dependendo dos sistemas operacionais, questões práticas afetam a ordem de instalação e de configuração dos diversos sistemas.

Segundo, e talvez mais importante, é comum haver cursos e treinamentos onde o participante necessita ter acessos administrativos.

Nesses casos, mesmo mudando as senhas de administrador (root) a cada sessão, há a possibilidade de se corromper o sistema operacional pela instalação de rootkits, backdoors, sniffers de teclado, e todo um conjunto de malwares, além de ser possível, por exemplo, montar a partição dedicada a um outro sistema operacional e provocar danos.

Nesse caso, as máquinas virtuais são bem apropriadas, o participante de uma disciplina pode usar um ambiente virtual para fazer todos os experimentos e, por rodar de forma confinada em um processo, suas ações como administrador não afetam o sistema nativo.

Ao finalizar o processo (máquina virtual) não há nenhum vestígio permanente do que foi feito.

Um detalhe, havendo necessidade de preservar arquivos de uma aula a outra, as máquinas virtuais oferecem formas de comunicação (ftp, telnet, scp, etc) e armazenamento (usb) que possibilitam a cópia e o salvamento de arquivos.

De uma forma simplista, honeypot consiste em se colocar intencionalmente máquinas na Internet de forma que elas sejam atacadas por crackers.

O intuito é monitorar as atividades desses, se precaver de ataques e tornar mais fácil a investigação de incidências de ataques e de sua recuperação.

O problema com honeypots é que, dependendo do tipo de ação sofrida, pode haver o comprometimento da máquina (sistema operacional).

Nesses casos, a solução passa por reinstalar o sistema.

Outra característica do uso de honeypots é que eles normalmente são compostos por máquinas destinadas a essa finalidade e que são postas em segmentos de redes específicos, o que aumenta os investimentos tanto em nível de hardware como de suporte de administração de rede.

A virtualização surge como uma opção interessante para se implementar honeypots por várias razões.

Primeiro, o comprometimento de um sistema operacional é resolvido apenas com a remoção da máquina virtual e a instanciação de uma nova máquina virtual.

Uma segunda razão é que, em um mesmo hardware físico, é possível instalar várias máquinas virtuais, cada uma com um sistema operacional diferente, o que permite aumentar o número de "máquinas iscas".

Por fim, com o uso de softwares de emulação de equipamentos de rede, pode-se definir uma infra-estrutura de rede virtual, com firewalls, proxies, roteadores etc, em uma única máquina.

Note que isso não é imprescindível, pois as ferramentas de virtualização oferecem normalmente suporte a interfaces de redes virtuais, com endereços MAC e IP distintos, e a VLANs, o que permite interconectar as máquinas virtuais em equipamentos de interconexão físicos sem maiores problemas.

Uma outra situação, associada com a idéia de honeypot, é a possibilidade de se executar softwares de origens "não confiáveis" em um ambiente onde os prejuízos são minimizados.

Isso é interessante para abrir arquivos attachment suspeitos ou verificar malwares.

Vale a pena salientar que as ferramentas tradicionais existentes para honeypots podem ser utilizadas sem restrições alguma em ambientes virtualizados, pois elas são aplicações para um sistema operacional.

Uma boa referência para ferramentas de honeypots.

Uma característica importante das máquinas virtuais é a sua capacidade de oferecer um ambiente computacional completo, isso é, sistema operacional, bibliotecas, arquivos de configurações e aplicações em uma imagem (tipo especial de arquivo).

Uma máquina virtual pode então ser vista como um aplicativo que recebe como parâmetro um arquivo de imagem.

Isso se traduz na possibilidade de se definir um ambiente de trabalho e criar uma espécie de "pacote de distribuição" a ser instalado e usado em qualquer máquina.

Outro aspecto muito interessante das máquinas virtuais é a sua capacidade de isolamento, ou seja, o que acontece no interior de uma máquina virtual não afeta o sistema nativo, nem outras máquinas virtuais.

Além disso, uma máquina virtual pode ser parada em qualquer momento preservando seu estado atual o qual pode ser recuperado posteriormente.

Esses aspectos fornecem perspectivas interessantes para o uso da virtualização em ambientes de processamento paralelo e distribuído que serão vistos a seguir.

Infra-estrutura de processamento paralelo, Já há muito tempo clusters e grids se tornaram uma infra-estrutura para processamento paralelo e distribuído por fornecerem um poder computacional semelhante ou superior ao de supercomputadores com um custo significativamente mais baixo.

Grosso modo, ambos podem ser vistos como um conjunto de máquinas independentes interconectadas em rede e destinadas a execução de uma aplicação paralela.

Uma diferença fundamental entre esses dois sistemas é o fato de que os clusters pertencem a um mesmo domínio administrativo ao passo que um grid é formado por vários deles.

Por domínio administrativo entende-se uma organização que é responsável por gerenciar e manter os recursos de um ambiente de rede.

Isso implica em instalação de sistemas operacionais e softwares (bibliotecas, aplicativos etc), configuração, manutenção das máquinas e cadastro de usuários.

Existem dois problemas essenciais com clusters e grids, a grande quantidade de máquinas que podem constituir um ou outro e que devem ser corretamente instaladas, configuradas e mantidas, e a segurança que atinge principalmente os grids já que envolve vários domínios administrativos.

Inicialmente, vamos considerar o caso de clusters.

Um cluster pode ser formado por centenas de máquinas.

É impraticável executar tarefas administrativas nessa quantidade de máquinas de forma manual, portanto muitos esforços foram feitos no sentido de automatizar as tarefas de instalação, configuração e manutenção de sistemas baseados em cluster.

Talvez o mais significativo deles tenha sido o Oscar.

A questão de segurança é de certa forma minimizada já que um cluster é tipicamente formado por uma máquina frontal (front-end) conectada a duas redes, uma rede acessível a partir do exterior e uma rede interna formada pelos nós de processamento.

Para lançar uma aplicação, um usuário se autentica na máquina frontal e a partir dela dispara a execução de sua aplicação.

Assim é relativamente fácil gerenciar a segurança já que basicamente garantir acesso seguro ao front-end aos usuários com conta nesse ambiente.

No entanto, os clusters têm duas limitações inerentes a sua construção.

Primeira, em um ambiente de cluster todas as máquinas possuem um mesmo sistema operacional.

Segunda, eventualmente, devido à velocidade do mercado de computadores, não se consegue aumentar, de forma homogênea, o número de máquinas de um cluster construído no passado por não se encontrar mais exatamente o mesmo tipo de hardware.

Tem-se então um problema de heterogeneidade que pode ter impactos em relação ao conjunto de software instalado.

Um cluster, quanto mais homogêneo for, melhor é sua manutenção.

Nesse ponto, a virtualização surge como uma solução com uma série de vantagens.

A primeira delas é a capacidade de fazer com que o cluster seja visto como um conjunto de máquinas virtuais.

Sendo assim, a homogeneidade do sistema é dada pelo fato de que as máquinas virtuais são instanciadas a partir de uma mesma imagem independente do hardware e do sistema operacional sobre as quais executam.

Segundo, um cluster, como visto anteriormente, é fisicamente formado por um front-end e por um conjunto de nós de cálculo.

Isso pode representar um desperdício de investimento dependendo da taxa de utilização do cluster e de seus nós.

Porque então não criá-los e dimensioná-los a medida do necessário, por demanda?

Por fim, a instalação, configuração e manutenção de um cluster são feitas a partir de uma única imagem, a qual posteriormente é usada em todos os nós.

Considerando essas facilidades foram desenvolvidos alguns projetos voltados a clusters usando a virtualização como, por exemplo, o OSCAR-V e Cluster on Demand (COD).

O primeiro é uma versão estendida de seu predecessor OSCAR que adiciona a capacidade de criar e gerenciar máquinas virtuais sobre um cluster físico.

No segundo, a criação de um cluster é feita por demanda, inicialmente localizando quais máquinas físicas (recursos) estão disponíveis.

Na seqüência, é feita a instanciação de uma máquina virtual sobre cada um deles e a criação de um contexto.

O contexto pode ser visto, por exemplo, como a configuração da quantidade de nós disponíveis para se lançar uma aplicação MPI.

Um grid computacional pode ser visto, de certa forma, como a extensão natural de um cluster.

Os grids estendem a noção de máquina virtual para aquela de uma organização virtual que permite o compartilhamento de recursos (processamento, memória, discos, dados etc) entre sistemas que estão geograficamente dispersos.

Uma organização virtual pode ser vista como uma coleção de usuários e recursos, construída sobre um conjunto de integrantes de diferentes administrativos e controlada por regras de compartilhamento.

Em um grid os problemas de instalação, configuração e manutenção de um grande parque de máquinas apresentam um fator de complexidade adicional por atravessar domínios administrativos e implicar em regras e restrições de segurança diferentes.

Para sintetizar essa questão, imagine a reação de um administrador de redes de uma organização ao receber o pedido, de um usuário que ele nunca ouviu falar, de que é preciso instalar uma biblioteca no sistema para seu software executar?

A idéia fundamental da computação em grid é que para lançar uma aplicação um usuário descreve os recursos necessários.

A partir disso, os recursos são localizados e alocados e, finalmente, a aplicação é lançada.

Entretanto, isso não é simples assim.

Primeiro, nem sempre é fácil se obter um ambiente adequado para se executar uma dada aplicação.

Além de encontrar os recursos mais apropriados é necessário, muitas vezes, negociar o seu uso.

Existem vários fatores que afetam os critérios de escolha de recursos, por exemplo, sites diferentes oferecem diferentes requisitos de qualidade de serviço ou, ainda, necessidade de realizar experimentos sazonais exigindo "picos" de processamento.

Na prática, o problema maior não são os recursos, pois é relativamente fácil uma organização possuir um determinado número de máquinas ou de cores.

As questões principais são, como instalar e manter os pacotes de softwares necessários?

Como conciliar os conflitos entre usuários e as restrições de segurança do site?

Como coordenar o escalonamento de aplicações?

Uma possibilidade de resposta a essas questões é a criação de ambientes virtuais de trabalho, ou virtual workspaces.

Essencialmente, a idéia consiste em alocar dinamicamente recursos e configurá-los de forma a oferecer para o usuário um ambiente computacional completo por demanda.

Por recursos, se entende a necessidade de processamento (cpu), de memória, de disco, de banda passante e que, esses recursos possam ser renegociados mesmo durante a execução da aplicação de forma a refletir mudanças de requisitos e restrições.

Nesse contexto, denomina-se de appliance, o ambiente de software necessário para uma aplicação.

O projeto Virtual Workspace evoluiu de tal forma que hoje ele é apenas um módulo de uma infra-estrutura mais completa, o Nimbus.

O projeto Nimbus é uma iniciativa de empregar a tecnologia de virtualização para computação em Grid.

Basicamente, o Nimbus é um conjunto de ferramentas que permite que se descreva um conjunto de requisitos necessários a uma infra-estrutura e, de forma automatiza, é feita a procura e a configuração de recursos (máquinas) que possam atender esses requisitos.

Os conceitos originais do Nimbus são do projeto Virtual Workspaces.

As principais tecnologias empregadas no Nimbus são o Xen para máquinas virtuais e o uso de web services, através do WSRF (Web Services Resource Framework) para o suporte a descrição e localização de recursos.

Além do Nimbus, existem outros projetos que buscam a definição de uma infra-estrutura baseada em máquinas virtuais para implementar um grid computacional.

Entre outros, é possível destacar o Virtuoso.

Balanceamento de carga e tolerância a falhas, Como mencionado anteriormente um dos aspectos interessantes das máquinas virtuais é que elas encapsulam um ambiente computacional completo em qualquer instante de tempo.

Essa característica permite que se suspenda a execução de uma máquina virtual e que se preserve o seu estado atual de execução criando uma espécie de "fotografia instantânea" do ambiente.

Esse estado pode ser posteriormente restaurado permitindo que a execução da máquina virtual siga a partir do ponto em que ela havia sido suspensa.

Em uma infra-estrutura de rede, essa facilidade permite que uma máquina virtual seja parada, transferida para outra máquina física, e reiniciada, o que é particularmente útil para se fazer a manutenção ou upgrade em uma máquina física sem causar a indisponibilidade de serviços.

Isso é o que se denomina de migração.

A migração pode ser empregada como um mecanismo de base para balanceamento de carga e de tolerância a falhas, que são extremamente úteis em processamento paralelo.

Na prática, o encapsulamento feito por uma máquina virtual guarda muitas similaridades com um mecanismo de checkpointing/restart.

Ele permite que uma máquina virtual seja suspensa por um tempo indeterminado e que se retome a execução exatamente no ponto em que a suspensão foi feita.

Se a suspensão e a retomada forem feitas em máquinas diferentes, tem-se a migração.

Se o checkpoint de uma aplicação executando sobre uma ou mais máquinas virtuais é feito regularmente é possível preservar estados conhecidos e coerentes dessa aplicação.

Isso é particularmente interessante para aplicações que executam durante muito tempo ou em ambientes em que os recursos podem ser tornar indisponíveis, como o caso de grids.

Nessa situação, em caso de pane ou indisponibilidade de recursos, é possível retomar a execução a partir do último checkpoint realizado com sucesso, evitando a perda do processamento executado.

No entanto, apesar de conceitualmente simples, essa abordagem traz consigo alguns desafios.

Primeiro, em cluster e grids, há o problema de que uma aplicação executa em nós distintos que se comunicam, na grande maioria das vezes, através de conexões TCP.

A mudança de um endereço IP e das portas usadas na conexão fazem com que a mesma seja "quebrada".

O segundo desafio é relacionado com o tamanho da imagem.

Dependendo da aplicação que está sendo executada, e do ambiente necessário para tal, é comum uma imagem ocupar centenas de megabytes a alguns gigabytes.

Nesse caso, para migrar uma aplicação de um nó a outro, é necessário realizar uma transferência de imagem via rede onde a latência e a banda passante se tornam fatores importantes.

O problema da troca do endereço IP é relativamente simples de ser contornado através do uso de técnicas como IPMobile, VPN e NAT para definir uma rede virtual com endereços IP independentes do endereço IP da máquina hospedeira.

Portanto, a aplicação executa sempre usando o mesmo IP (virtual).

Já o tamanho da imagem é um pouco mais delicado de ser tratado e duas abordagens são utilizadas, reduzir o tempo de transferência da imagem pela rede ou decidir se a vale a pena ou não realizar uma migração.

Uma primeira abordagem para reduzir o tempo de transferência é a utilização de um sistema de arquivos distribuídos.

Nesse esquema, todas as máquinas compartilham um mesmo sistema de arquivos via rede, mas a máquina alvo faz a leitura do arquivo imagem de forma incremental, por demanda, e em background.

Assim, não é necessário esperar a transferência ser concluída para iniciar a execução da máquina virtual.

Nesse trabalho, há ainda a sugestão de eventualmente reaproveitar porções da imagem que já estejam na máquina alvo.

Isso é válido para qualquer tipo de arquivo no interior da imagem que seja read-only.

Uma variação da abordagem anterior é proposta no projeto Stanford Collective.

Nesse trabalho, foi feita uma série de otimizações para reduzir a quantidade de dados a serem transferidos de uma máquina a outra através de técnicas como compactação, reaproveitamento de blocos de disco etc.

Uma vez que há condições para migrar máquinas virtuais de forma eficaz é possível usar a migração como um mecanismo de base para executar balanceamento de carga.

Assim, um sistema pode ficar monitorando a carga das diversas máquinas e ao detectar a ociosidade de uma em relação à outra é possível deslocar carga de processamento de uma máquina a outra.

Entretanto, é necessário que o custo de migração (checkpointing, transferência e restart) sejam menos onerosos que o que se espera ganhar com a migração.

Também vale a pena ressaltar que a granularidade da migração é de uma máquina virtual completa, portanto, migrar uma máquina virtual de uma máquina física para outra só se justifica se a máquina sobrecarregada estiver executando mais de uma máquina virtual ou se a máquina ociosa tiver uma capacidade de processamento muito superior.

Em é proposto um modelo de custo para dimensionar a viabilidade de uma migração.

Ainda, em é apresentado o custo de migração de máquinas virtuais em um trabalho baseado no Xen.

Até o presente momento a virtualização foi colocada como uma solução para vários problemas, no entanto, ela também apresenta alguns inconvenientes quando a aspectos de segurança, gerenciamento e desempenho.

Inicialmente, o hipervisor é uma camada de software e, como tal, está sujeito a vulnerabilidades.

Segundo Neil MacDonald, especialista de segurança da Gartner, hoje em dia, as máquinas virtuais são menos seguras que as máquinas físicas justamente por causa do hipervisor.

Mas há muita controvérsia nesse campo.

Além disso, há a questão da disponibilidade de serviços que ocorre com o comprometimento, lógico ou fisico, da máquina física que hospeda vários servidores virtuais, já que todos seriam afetados simultaneamente.

É verdade que as soluções corporativas, como o ESX Server e o Citrix Enterprise, permitem toda uma monitoração dos sistemas, replicação, migração e backup dos sistemas virtualizados, que diminuem o tempo de recuperação em caso de problemas, mas é algo a ser avaliado.

Um outro argumento freqüentemente empregado em favor da disponibilidade é que um pool de máquinas virtuais é similar a um rack com vários servidores físicos, com a vantagem de ter uma flexibilidade e uma portabilidade maior que estes últimos.

O segundo aspecto diz respeito ao gerenciamento das máquinas virtuais.

Os ambientes virtuais necessitam serem criados, monitorados, configurados, mantidos e salvos.

Existem produtos, como os brevemente citados na seção, que integram essas soluções, porém ainda há espaço para melhorias.

Em especial, há o aspecto de gerenciamento relacionado com a segurança que ainda é deficiente nas atuais soluções, a correlação de eventos e ações feitas como root em ambientes virtuais.

Por fim, mas não menos importante, a questão desempenho que pode ser formulada de duas formas.

Primeira, qual o custo de processamento introduzido pela camada de virtualização?

Segunda, quantas máquinas virtuais são possíveis sem comprometer uma qualidade de serviço (escalabilidade)?

Para responder esses questionamentos, vários estudos foram feitos e se encontram disponíveis na Internet, porém, muitos deles são específicos a uma determinada situação ou solução.

Atento a essa necessidade, e como forma de sistematizar e fornecer uma metodologia padrão para avaliar o desempenho de máquinas virtuais, foi criado um comitê especial, o SPEC Virtualization Commitee.

O objetivo desse comitê é desenvolver benchmarks para avaliar o desempenho da virtualização em servidores e prover mecanismos para comparar o comportamento desses quando executando muitos hipervisores.

No momento da redação deste trabalho, a liberação para primeira versão de benchmark estava prevista para o primeiro semestre de 2009.

Enquanto não há um benchmark específico para avaliar a virtualização, os estudos feitos até agora costumam empregar benchmarks existentes que simulam cargas de processamento, de entrada e saída, e de tráfego na rede.

A partir de uma avaliação feita pela VMware e posteriormente questionado e reconduzido pela Xen Source, se popularizaram os benchmarks SPECcpu2000, focado em aplicações computações intensivas.

O Passmark, que gera uma carga de trabalho para testar os principais subsistemas que compõem um sistema operacional.
O NetPerf, para avaliar o desempenho no envio e recepção de dados via rede.

O SPECjbb2005 que representa um servidor e sua carga, e a compilação do pacote SPECcpu2000 INT.

Os resultados obtidos pela VMware e XenSource apontaram para uma queda de desempenho, em geral, entre 2% e 10%, com algumas situações impondo perdas maiores.

Cabe ressaltar que esses resultados foram obtidos usando benchmarks genéricos destinados a outras finalidades.

Em é apresentado um outro estudo sobre o desempenho do Xen.

Virtualização é o conceito que fornece a abstração de um recurso computacional qualquer mascarando suas características físicas dos usuários e aplicações que os utilizam.

No entanto, a forma mais conhecida de virtualização são as máquinas virtuais.

Cada máquina virtual, por si só, é um ambiente de execução isolado e independente das demais.

Com isso, cada máquina virtual pode ter seu próprio sistema operacional, aplicativos e serviços de rede (Internet).

O sistema operacional do hóspede pode ser diferente daquele utilizado pelo hospedeiro.

A virtualização não é um conceito recente, remonta a década de 60, e existem várias técnicas para implementá-la, onde se destacam as máquinas virtuais de processo e o monitor de máquina virtual.

Uma máquina virtual é implementada considerando dois pontos de vistas diferentes, a de um processo e a do núcleo do sistema operacional.

Uma máquina virtual de processo fornece um ambiente de execução para uma única aplicação através de uma interface virtual (ABI) a qual é mapeada para uma interface real composta pelas chamadas de sistema e pelo conjunto de instruções não-privilegiadas (user ISA).

Na prática, isso pode ser visto como um processo (a aplicação) executando dentro de outro processo (a máquina virtual).

Uma máquina virtual de processo só existe enquanto a aplicação estiver em execução.

Uma máquina virtual de sistema, também denominada de VMM (Virtual Monitor Machine) ou hipervisor, é aquela que oferece um ambiente de execução completo onde coexiste um sistema operacional e várias aplicações.

Ao contrário das máquinas virtuais de processo, as máquinas virtuais de sistema estão ativas até serem explicitamente paradas.

Há dois tipos de hipervisores, nativos (tipo I) e hóspedes (tipo II).

Os hipervisores nativos são uma camada de software posta entre o hardware físico da máquina e as máquinas virtuais executam sobre eles.

Já os hipervisores hóspedes (tipo II) executam sobre o sistema operacional como se fossem um processo deste.

Há ainda, duas técnicas usadas nos hipervisores, virtualização total e paravirtualização.

A diferença essencial é se o sistema operacional hóspede precisa ser modificado (paravirtualização) ou não (virtualização total) para executar sobre o hipervisor.

Assim como já aconteceu no passado com a multiprogramação, o projeto dos processadores mais recentes tem considerado mecanismos em hardware para dar suporte a virtualização.

É o caso dos fabricantes AMD e Intel que desenvolveram extensões para a arquitetura x86 suportar virtualização, respectivamente, AMD-Virtualization (AMD-V, codinome Pacífica) e Intel Virtualization Technology (IVT, condinome Vanderpool).

As principais vantagens da virtualização são, Isolamento de dados e de processamento permitindo que uma mesma máquina física execute várias máquinas virtuais sem que elas interfiram umas nas outras e no próprio sistema nativo.

Capacidade de instalar de maneira rápida e uniforme, independente de plataforma, um mesmo ambiente computacional (sistema operacional, biblioteca e aplicações).

Isso permite se ter um ambiente capaz de ser executado em qualquer lugar.

As máquinas físicas não precisam ter um mesmo hardware para executar uma mesma máquina virtual.

Essa característica é uma forma de contornar questões de heterogeneidade.

Portanto, as máquinas virtuais são uma forma de se manter ambientes homogêneos sobre hardwares heterogêneos.

Possibilidade de integrar facilmente mecanismos de migração e de balanceamento de carga através da facilidade de se executar a suspensão e a sua retomada (mecanismo suspend/resume).

Essa mesma facilidade pode ser usada como base para tolerância a falhas através de mecanismos de checkpoint/restart.

Essas vantagens fazem da virtualização uma ferramenta interessante para ser usada em infra-estruturas de TI e, em ambientes de processamento paralelo, na implementação de clusters e grids.

O uso típico da virtualização em infra-estruturas de TI é na consolidação de servidores, isso é, permitir que vários servidores executem simultaneamente em um único hardware físico, mas cada um em sua própria máquina virtual.

Entretanto a virtualização é empregada com sucesso em várias outras situações como ambientes de desenvolvimento e teste de produtos, laboratórios de treinamento de cursos de redes e de sistemas operacionais, e servir de base para implantação de mecanismos de segurança (honeypots).

No caso de processamento paralelo e de alto desempenho (PAD) a virtualização surge como uma possibilidade promissora para resolver problemas relacionados com gerenciamento de recursos e migração.

Um exemplo são os clusters que podem ser dimensionados por demanda e não mais vinculados à existência de hardware físico.

Nesse caso, os nós do cluster são máquinas virtuais que podem ser instanciadas em uma ou várias máquinas físicas.

Em ambientes de grids, a virtualização encontra um nicho natural, pois um grid nada mais é que a definição de um grande sistema virtual.

Nesse contexto a virtualização auxilia na resolução de problemas relacionados com a segurança e tolerância a falhas.

Primeiro, a partir do instante que uma máquina virtual é uma imagem que pode ser instanciada e executada sobre um sistema operacional nativo sem interferir em seu funcionamento, muito das restrições e cuidados atuais de compartilhar recursos são minimizados.

A única exigência que se faz é que a máquina alvo tenha instalado a máquina virtual.

Em grid a volatilidade dos recursos é algo problemático por representar a perda de computação feita até um momento e para contornar esse problema normalmente são empregados mecanismos de tolerância a falhas.

As máquinas virtuais, com sua facilidade de suspend/resume auxiliam na implementação de tais mecanismos.

Por fim, a virtualização é uma ferramenta muito poderosa e que oferece uma gama de oportunidades, mas ainda existem vários pontos a serem melhores tratados, como as questões de gerenciamento, segurança e de desempenho.

Para reforçar essa afirmação uma citação interessante da Microsoft (em seu site technet), "o valor de mercado da virtualização não está na pilha necessária para implementá-la, isso é, no conjunto sistema operacional mais o hipervisor, mas sim no desenvolvimento de sistemas de gerenciamento para ambientes virtuais".

A rápida popularização de processadores multicores possibilitou o ingresso do processamento paralelo em praticamente todos os sistemas computacionais.

Como resultado, aumentou consideravelmente a demanda pelo desenvolvimento de aplicativos que explorem efetivamente esta capacidade de processamento.

Neste contexto, o modelo de programação e execução proposto pela multiprogramação leve é o que possibilita obter maiores vantagens no desenvolvimento de software para este tipo de arquitetura.

O presente capítulo caracteriza as arquiteturas de processadores multicores, bem como os recursos oferecidos pela multiprogramação leve.

O texto é ilustrado com a descrição de três das mais populares ferramentas para multiprogramação leve, Pthreads.

NET e OpenMP, visando oferecer um panorama de diferentes domínios de aplicação da multiprogramação leve sobre processadores multicores.

Por fim, são apresentadas técnicas de programação focadas na exploração eficiente das características dos processadores multicores.

Graduação e doutorado em Ciência da Computação na Universidade Federal do Rio Grande do Sul.

Atualmente, é Professor na Universidade Federal de Pelotas.

Seus temas de pesquisa incluem, Arquiteturas de Computadores, Processamento Paralelo e Distribuído, Computação Pervasiva.

Graduado em Informática pela Pontíficia Universidade Católica do Rio Grande do Sul.

Mestrado e doutorado em Ciência da Computação pela Universidade Federal do Rio Grande do Sul.

Atualmente, é Líder de Grupo no CEITEC.

Seus principais temas de pesquisa são, Arquiteturas de Computadores, Processamento Paralelo e Distribuído.

Graduado em Informática pela Pontíficia Universidade Católica do Rio Grande do Sul.

Mestre em Ciência da Computação pela Universidade Federal do Rio Grande do Sul.

Doutor em Informatique, Systèmes et Communications pela Institut National Polythecnique de Grenoble.

Atualmente, é Professor da Universidade Federal de Pelotas.

Atua principalmente nos seguintes temas, Arquiteturas Paralelas e Distribuídas, Programação Paralela, Escalonamento, Regulação de Carga, Fluxo de Dados.

O surgimento da programação concorrente se deu por volta de 1960, com o surgimento de dispositivos dedicados a operações de entrada e saída de dados (E/S).

O emprego da concorrência tinha como objetivo obter melhor aproveitamento do tempo de processamento do processador principal, pois, uma vez que dispositivos especializados responsabilizavam-se por executar as operações de E/S, não havia necessidade do processador permanecer ocioso aguardando o término das requisições.

A estratégia adotada permitia a execução de fluxos de instruções de diferentes aplicações, sobrepondo computação do fluxo de execução de uma aplicação com operações de E/S de outras.

Passados quase 50 anos, o compartilhamento de tempo ainda é uma técnica muito empregada.

Hoje, no entanto, o horizonte de aplicação da programação concorrente é mais amplo, permitindo que seja empregada em praticamente todos os recursos computacionais existentes com apoio de diferentes ferramentas de programação.

Processadores multicores refletem os mais recentes avanços na arquitetura de processadores, trazendo o poder do processamento paralelo aos computadores domésticos e estendendo a barreira da supercomputação.

Basicamente, um processador é dito multicore quando possui dois ou mais núcleos completos de processamento (processadores) no mesmo chip.

A arquitetura básica segue o modelo da arquitetura SMP (Symmetric MultiProcessors).

Como nesta arquitetura, os diferentes processadores (ou cores) são capazes de executar fluxos de instruções de forma independente, e compartilham um espaço de endereçamento provido por uma área de memória comum.

A multiprogramação leve permite que uma aplicação seja descrita em termos de diversos fluxos de execução, ou threads, capazes de executar de forma concorrente.

Cada fluxo possui uma área de endereçamento própria, mas os threads compartilham acesso a um espaço de endereçamento global.

A concorrência, neste caso, expressa não somente a disputa de recursos de processamento, como tempo de processador e espaço de memória, mas também a disputa dos threads de um programa pelo acesso aos dados armazenados em memória compartilhada.

Como consequência natural da popularização dos multicores, o desenvolvimento de novos aplicativos, desde os programas multimídia até elaboradas aplicações de Computação Científica, deve incorporar técnicas de multiprogramação leve.

O restante deste texto introduz conceitos e ferramentas para multiprogramação leve em processadores multicores.

A próxima seção descreve as características principais dos processadores multicores, sendo também apresentados aspectos históricos da evolução tecnológica que resultou nesta arquitetura e produtos de mercado.

Na Seção seguinte, são apresentados conceitos relacionados à multiprogramação leve e questões relacionadas ao desenvolvimento de programas.

A sequência é dada, onde três ferramentas para multiprogramação leve são apresentadas.

Net, POSIX threads e OpenMP.

Discute técnicas e recursos de programação aplicáveis a arquiteturas multicores.

Por fim, tece comentários finais sobre este texto.

Os avanços na área de circuitos integrados permitiram o aumento de desempenho dos microprocessadores nos últimos 25 anos graças às frequências de clock cada vez maiores, e mais oportunidades para inovações microarquiteturais.

Esta evolução, ao longo dos anos, permitiu que o aumento de desempenho fosse observado pelo usuário, sem que este tivesse que se preocupar necessariamente com a forma com que o software era escrito.

As características do software existente são na maioria das vezes projetadas em função da aplicação em si, e não em função do resultado em termos do desempenho de sua execução.

Isto se dá porque historicamente observou-se um aumento de desempenho na ordem de 40% ao ano transparentes para o software.

Ou seja, cada nova geração de microprocessador permitiu que o software se tornasse mais complexo, sem preocupação com o desempenho.

Explorando frequências de clock mais rápidas, os microprocessadores disponíveis comercialmente até poucos anos atrás exploravam essencialmente o paralelismo no nível de instruções, executando de forma paralela instruções definidas por um único fluxo de execução, thread ou processo.

Com esta estratégia, programas sequenciais poderiam se beneficiar das inovações microarquiteturais, sem necessariamente serem reescritos, ou seja, de forma transparente.

Este cenário infelizmente contribuiu para que pouco fosse explorado comercialmente no que diz respeito à paralelização do software convencional, aquele usado no dia-a-dia, incluindo, nesta categoria, sistemas operacionais.

A frequência de clock não é a única medida de desempenho, nem mesmo é necessariamente uma boa medida, mas no entanto é uma medida representativa.

Nas útimas três décadas observou-se um incremento significativo nas frequências de clock atingindo, atualmente, a faixa dos 3 a 4 GHz.

No entanto, pode-se perceber que nos últimos cinco anos houve uma redução nesta escala de incremento.

De forma similar, os mecanismos até então empregados para explorar o paralelismo no nível de instrução também encontraram seus limites, já que o custo para incrementar o desempenho a partir de inovações microarquiteturais não justifica maiores esforços nesta área, pois em geral estão limitados a um único thread.

O que se percebe atualmente é que uma nova abordagem deve ser considerada para que novos índices de desempenho sejam alcançados.

Todas as inovações advindas de melhor e mais eficiente tecnologia e de mecanismos agressivos implementados na microarquitetura resultaram em um aumento significativo da complexidade e custo de projeto, sem que resultados em escalas proporcionais fossem propiciados.

Tornou-se comercialmente inviável e tecnicamente quase impossível aumentar o desempenho das aplicações convencionais apenas com o aumento da frequência de clock, ou com o uso de técnicas que exploram o paralelismo limitado a um único thread.

Nesta Seção, são discutidas as características das arquiteturas multicores.

Os aspectos históricos e a evolução destas arquiteturas são apresentados como forma de contextualizar a sua inserção no mercado como tecnologia de ponta e, principalmente, como forma de enfatizar o impacto desta arquitetura no projeto e desenvolvimento de software.

A arquitetura de von Neumann é um modelo de computação que emprega uma estrutura comum de memória que armazena dados e intruções.

O modelo apresentado por John von Neumann foi considerado uma revolução na época ao especificar o modelo de referência para arquiteturas sequenciais.

Os computadores até então possuíam um programa fixo e não permitiam a execução de outros programas, cujo código já não se encontrava no seu projeto.

A proposta de von Neumann introduziu um novo conceito, permitindo que se modificasse o conteúdo de uma memória introduzindo ou modificando instruções como forma de se reprogramar o computador.

Surge também neste momento a ideia de conjunto de instruções e de detalhamento da computação, a partir de uma sequência de instruções (programas).

Esta sequência de instruções tem como finalidade especificar os passos que devem ser executados pelo processador, para que um determinado algoritmo seja realizado.

Cada instrução ocupa uma ou mais posições de memória e é representada por uma sequência específica de bits (código de máquina).

Para que o programa possa ser executado, o processador realiza uma série de etapas, independentes do algoritmo.

Apesar de ser um modelo simples, porém, revolucionário, o modelo de von Neumann introduziu um problema que permanece até hoje e que John Backus chamou de gargalo de von Neumann, também conhecido na literatura por memory gap.

A comunicação entre processador e memória representa um fator limitante, pois todo o processamento depende de dados e instruções que estão armazenados na memória.

Neste modelo, o acesso à memória determina a velocidade em que o processador pode realizar o processamento propriamente dito.

Outro fato historicamente importante é que a velocidade dos processadores e o tamanho das memórias têm aumentado muito mais rapidamente do que a velocidade de acesso dos processadores a estas memórias.

No que diz respeito ao software, a maioria das linguagens de programação convencionais está baseada no modelo de computação de von Neumann, em particular as linguagens do paradigma imperativo.

Nestas linguagens, variáveis correspondem a células de armazenamento, comandos de atribuição correspondem a leitura, comandos de escrita, operações artiméticas e comandos de controle correspondem a instruções de teste e salto.

Segundo Backus, o símbolo "=" representa linguisticamente o gargalo de von Neumann.

Com este modelo reforça-se a maneira sequencial de pensar e escrever programas.

Gordon Moore, fundador da Intel, previu empiricamente que o avanço na tecnologia seria tal que o número de transistores em um circuito integrado dobraria a cada 24 meses.

Esta previsão passou a ser chamada de Lei de Moore, e os avanços da indústria comprovam esta previsão até os dias atuais.

Porém, o fato mais importante em relação a isso não é a previsão em si, mas sim que os projetistas de microprocessadores se depararam com um problema singular, o que fazer com tantos transistores ?

O número cada vez maior de transistores disponíveis permitiu o incremento das inovações microarquiteturais.

Entende-se por inovação microarquitetural toda técnica nova que melhora ou explora de maneira mais eficiente (ou, pode-se dizer, mais agressiva) a execução de instruções no interior de um microprocessador.

Por outro lado, o aumento do número de transistores é resultado da diminuição do tamanho do transistor em si, que, por sua vez, é resultado das melhorias na tecnologia de fotolitografia capaz de "imprimir" transistores cada vez menores em uma pastilha de silício.

Transistores menores permitem frequências de clock mais elevadas, que, associadas a técnicas mais agressivas de execução de instruções, oferecem melhor desempenho.

No entanto, microprocessadores atuais, e em especial os da família x86, atingiram uma barreira técnica no que diz respeito ao consumo de potência a frequências muito elevadas.

Segundo, em 1993 um microprocessador Intel Pentium tinha por volta de 3 milhões de transistores, enquanto hoje um microprocessador dual core Itanium 2 tem mais de 1 bilhão de transistores.

Por fim, a velocidade das memórias não cresce na mesma proporção que a velocidade da lógica de processamento.

No final da década de 1980, um microprocessador i486 necessitava de 6 a 8 ciclos de clock de processador para acessar a memória.

Já o microprocessador Pentium necessita por volta de 224 ciclos de clock para a mesma tarefa, ou seja, um aumento de quase 20 vezes.

Os fatores apresentados fazem crer que não serão vistos aumentos drásticos de frequência enquanto os limites físicos não forem atingidos, embora ainda exista espaço para aumentar o número e a velocidade dos transistores.

O aumento de desempenho observado a cada nova geração de microprocessador foi obtido principalmente pelas otimizações realizadas a partir das pesquisas em três frentes distintas.
Frequência de clock, o aumento da frequência de clock pode ser resultado da diminuição dos atrasos de propagação dos sinais, da diminuição do tempo de chaveamento dos transistores ou da simplificação e/ou particionamento da lógica.

O aumento de desempenho resultante desse tipo de otimização pode não ser proporcional ao aumento de frequência obtido pela aplicação da otimização, em alguns casos pode até mesmo ocorrer perda de desempenho.

O resultado prático e direto do aumento da frequência é a realização de mais ciclos por segundo.

Fluxo de execução, as otimizações no fluxo de execução trazem como resultado a possibilidade de executar um maior número de operações por ciclo.

Otimizações desse tipo são obtidas pelo emprego de técnicas de pipelining (escalar ou superescalar), previsão de desvios, execução fora de ordem, multithreading em hardware e diversas outras em geral aplicadas no nível microarquitetural.

Emprego de memórias cache, praticamente todo processador apresenta pelo menos um nível de memória cache.

Como são mais rápidas que a memória principal, embora menores, permitem reduzir o impacto do acesso a dados e instruções.

Os primeiros microprocessadores que implementaram o modelo de von Neumann utilizavam um modelo baseado em ciclo único único de execução.

Neste modelo, toda e qualquer instrução deve ser executada em um único ciclo de clock.

Para que isto seja possível, o tempo de ciclo deve ser determinado em função da instrução mais lenta, ou seja, da instrução que requer mais tempo para ser executada.

Como forma de tornar o processamento mais eficiente e aumentar o desempenho, surgiram as arquiteturas baseadas em múltiplos ciclos de execução.

Com este modelo, foram introduzidos registradores intermediários (barreiras) que têm como finalidade separar fisicamente cada etapa do ciclo de instrução.

Dessa forma, com uma unidade de controle mais complexa é possível determinar quais etapas são efetivamente necessárias para a execução de cada instrução.

Cada etapa consome necessariamente um ciclo de clock, porém, cada instrução pode requerer um número diferente de etapas.

Dessa forma, é possível executar somente as etapas necessárias para cada instrução, tornando o processamento mais eficiente.

A inserção de registradores intermediários como barreiras permitiu o isolamento das etapas, e deu abertura para a implementação de uma técnica conhecida como pipelining.

Esta técnica, amplamente empregada nos dias de hoje, tem como objetivo sobrepor a execução de múltiplas instruções.

A implementação de pipelining (de instruções) pode seguir duas abordagens, escalar ou superescalar.

A primeira permite a execução de múltiplas instruções, de forma que cada instrução ativa esteja em uma etapa diferente do ciclo de instrução.

Em outras palavras, pode-se dizer que cada estágio pode receber somente uma instrução por vez.

Como resultado, ao contrário da abordagem multiciclo, onde uma instrução leva necessariamente mais de um ciclo de clock para ser executada, na implementação com pipeline escalar pode-se chegar ao desempenho máximo de um ciclo por instrução, ou uma instrução por ciclo (máximo teórico), em uma implementação multiciclo, o número de instruções por ciclo (IPC) é sempre menor do que um.

Historicamente, o aumento considerável na complexidade dos microprocessadores se deu devido a dois fatores, (i) disponibilidade de um número cada vez maior de transistores, e (ii) forte pressão do mercado para o oferecimento de microprocessadores mais rápidos, rapidez aqui denota desempenho e não somente frequência de clock.

Seguindo essa tendência, surgiram então as arquiteturas superescalares.

Na implementação de um pipeline superescalar, o objetivo é atingir IPCs maiores que um.

A única forma de alcançar esse objetivo é permitir a execução de múltiplas instruções por estágio de pipeline.

Observe que a intenção é executar instruções efetivamente em paralelo.

Nesse nível, o paralelismo é chamado de ILP (Instruction Level Parallelism), e acontece entre instruções de um mesmo thread.

Na prática, é necessária replicação de cada unidade interna do pipeline (estágio) de forma proporcional ao número de instruções que se pretende executar por ciclo.

Em uma arquitetura superescalar, o IPC máximo (ideal) é proporcional ao número de instruções que podem ser executadas de forma paralela.

Duas observações importantes devem ser realizadas neste ponto.

Primeiro, todos os objetivos em termos de desempenho, tanto das arquiteturas com pipeline escalar, como superescalar, estão limitados à resolução de dependências de dados, de controle e contenção de recursos.

A existência de dependências entre instruções faz com que o processador não consiga preencher todas as unidades funcionais internas, resultando em desempenho abaixo do ideal (máximo teórico).

Segundo, os mecanismos para tratamento das dependências entre instruções adicionam complexidade significativa à unidade de controle e, para a exploração mais eficiente do ILP, são necessárias técnicas cada vez mais agressivas.

No entanto, nas últimas gerações de microprocessadores observou-se que o custo destas implementações mais complexas não se justifica, devido ao fato de que o paralelismo existente nas aplicações é limitado.

Portanto, os retornos em termos de desempenho não são proporcionais à complexidade adicionada e, não menos importante, as questões relacionadas ao consumo, custo e tempo de projeto passaram a receber mais atenção dos projetistas.

A idéia de usar múltiplos processadores com a finalidade de melhorar o desempenho e/ou a disponibilidade (redundância, tolerância a falhas) não é recente.

Flynn, propôs uma taxonomia simples para categorização das arquiteturas baseando-se no número de fluxos de instruções e de dados (D) observados.

Os fluxos são caracterizados por serem únicos (Single, ou S) ou múltiplos (Multiple, ou M).

Esta taxonomia, ainda atualmente empregada, é composta por quatro categorias, SISD, SIMD, MISD e MIMD.

As arquiteturas SISD apresentam um fluxo único de instruções operando sobre um fluxo único de dados (uniprocessadores).

As arquiteturas SIMD apresentam fluxos adicionais de dados, sobre os quais as mesmas instruções são executadas em paralelo (processadores vetoriais e matriciais).

As arquiteturas MISD possuem vários fluxos de instruções agindo sobre um mesmo fluxo de dados, não sendo encontrados comercialmente (embora alguns autores caracterizem processadores pipeline como MISD).

Finalmente, arquiteturas MIMD possuem múltiplos fluxos de instruções que executam sobre fluxos de dados.

Nesta última categoria, encontram-se os multicores, os SMPs (Symmetric Multi-Processors), os SMTs (Simultaneous Multi-threaded) e multicomputadores em geral.

Atualmente, o paralelismo é explorado em diferentes níveis em praticamente todas as arquiteturas.

Assim, o paralelismo é explorado deste sua granulosidade mais fina, no nível de instrução em uniprocessadores, até a exploração do paralelismo no nível de threads em máquinas MIMD comunicando-se via troca de mensagens.

Além disso, é importante também ressaltar que as formas de exploração de paralelismo podem ser combinadas.

Por exemplo, um cluster pode ser implementado com base em processadores superescalares com instruções SIMD.

Neste caso, o paralelismo de threads, dados e instruções é explorado em diferentes níveis do sistema.

A exploração do paralelismo no nível de instrução (ILP) em arquiteturas superescalares foi, durante muitos anos, foco das otimizações na maioria dos microprocessadores de propósito geral.

A razão está no fato de que o aumento de desempenho obtido é percebido no software de maneira transparente, ou seja, sem que este precise ser modificado.

No entanto, esta técnica atingiu níveis de complexidade elevados e não apresenta mais os mesmos retornos em termos de desempenho.

Ganhos consideráveis de desempenho foram obtidos pela exploração do paralelismo no nível de instrução em um único thread de controle.

Estes ganhos foram obtidos com a aplicação de técnicas microarquiteturais complexas, como execução fora de ordem, previsão de desvios, renomeação de registradores, caches on-chip, superpipelining, entre outras.

Estudos mais recentes e a própria experiência da indústria apontam que este caminho está saturado.

Além disso, consumo de energia e altas latências de acesso à memória têm representado obstáculos difíceis de serem transpostos, levando em consideração o desempenho de um único thread.

Durante anos foram discutidas alternativas e técnicas para aumento de desempenho dos processadores e, no final da década de 1990, parecia haver um consenso de que o foco das otimizações deveria voltar-se para a exploração do paralelismo de threads.

Naquele momento diversas implementações de processadores superescalares já estavam no mercado, por exemplo, Pentium III, AMD K6-III, UltraSparc I, Alpha 21264 e outros.

Estes processadores implementam as complexas estruturas necessárias para explorar o paralelismo no nível de instrução, mas, em eficiência, estes estavam muito aquém do que poderiam alcançar.

A eficiência com relação ao desempenho ideal é reduzida devido às limitações impostas pelas dependências de dados e de controle que fazem com que os recursos sejam subutilizados.

Uma alternativa para aumentar a eficiência seria inserir uma lógica capaz de gerenciar instruções provindas de múltiplos threads em execução.

Um dos trabalhos pioneiros nesta direção foi realizado por Nemirovsky e depois ampliado por Tullsen, que introduziu o termo SMT (Simultaneous Multi-Threading).

Um processador SMT é capaz de executar instruções em paralelo, tal como um processador superescalar, porém, pode simultaneamente extrair instruções de múltiplos threads de controle.

Este modelo de computação chamou a atenção dos fabricantes, pois estes tinham conhecimento de que muitos dos recursos já existentes não estavam sendo utilizados eficientemente, em vista das limitações da exploração de paralelismo no interior de um único thread.

O fator motivante teve por origem no fato de que estes recursos poderiam ser melhor utilizados, se houvesse outras fontes de onde se pudesse extrair instruções independentes.

Para implementar um processador SMT, no entanto, seria necessário garantir que instruções provenientes de um determinado thread não causariam nenhum efeito negativo para instruções de outros threads.

A solução envolve replicar o banco de registradores e marcar as instruções, de forma que os resultados produzidos por instruções de um thread nunca se misturassem aos resultados de instruções dos demais.

Porém, este não era o único problema.

A maior disponibilidade de instruções causava também uma maior pressão no sistema de memória, uma vez que a frequência de acessos à memória era maior.

O termo SMT é também conhecido como Hyperthreading, e foi empregado pela Intel nos processadores Intel Pentium 4 HT, com duas threads.

Houve poucas implementações de máquinas puramente SMT, mas alguns exemplos podem ser citados, IBM POWER5, MIPS MT e RMI MIPS SOC.

Embora as implementações SMT/HT superassem em parte o problema original (limitação de paralelismo ILP), a complexidade do processador em si continuava crescendo.

Olukotun foi um dos pioneiros nas pesquisas relacionadas às arquiteturas multicores.

Por multicore entende-se todo processador que possui dois ou mais cores, ou seja, processadores completos no mesmo chip que trocam informações via uma memória compartilhada.

Esta forma de TLP é frequentemente chamada também de CMP (Chip-level Multiprocessing).

Uma série de vantagens dos chips multicores, tais como uso eficiente de energia, custo de desenvolvimento e as possibilidades de exploração do paralelismo de threads e processos estão fazendo com que todos os fabricantes de microprocessadores de propósito geral apresentem as suas versões de CMP.

No entanto, nem tudo é perfeito.

O problema de eficiência das arquiteturas superescalares resulta das limitações do paralelismo de instruções, ou seja, da forma com que o software era descrito.

Em alguns casos, o compilador pode ajudar, mas nem sempre é possível mudar drasticamente a natureza sequencial de algumas aplicações.

No caso das arquiteturas multicores, a eficência e melhor desempenho podem ser percebidas ou não dependendo do sistema operacional e, principalmente, da aplicação.

É provável que um usuário comum perceba a diferença ao mudar para um processador de dois cores.

No entanto, à medida em que o número de cores aumenta, fica mais difícil perceber melhorias no desempenho, se as aplicações não forem devidamente adaptadas para explorar e expor mais o paralelismo de threads.

Considerando a evolução na tecnologia de fabricação de circuitos integrados, não demorará muito tempo para que um número bem maior de cores por chip esteja disponível.

Exemplo é o processador UltraSparc T2, que possui oito cores, sendo que cada um é capaz de executar até oito threads.

Outro exemplo é o processador Vega 2 da Azul Systems, que possui 48 cores, sendo que a máquina 3220 apresenta 4 chips ou seja, um total de 192 cores otimizados para aplicações Java.

Com estas perspectivas, pode-se dizer que mais uma vez as plataformas de desenvolvimento de software, bem como a cultura de desenvolvimento de software paralelo, estão bem atrás do potencial do hardware disponível.

Esta Seção introduz a multiprogramação leve.

Inicialmente, são apresentados conceitos de processo leve e programa concorrente.

A sequência se dá introduzindo as principais facilidades de programação oferecidas por ferramentas de programação multithread, e caracteriza os principais modelos para implantação de programas concorrentes.

A seção finaliza com uma discussão sobre erros comuns observados na programação concorrente.

Um programa escrito em uma linguagem imperativa sequencial, ao ser lançado, é manipulado pelo sistema operacional sob a forma de um processo.

Este processo é formado por um fluxo de execução, também chamado thread, uma área de memória e um registro descritor de processo (ou PCB, para o termo em inglês Process Control Block).

O fluxo executa sequencialmente as instruções definidas pelo programa.

Complementa este fluxo uma área de memória própria, manipulada como uma pilha, na qual os dados temporários são amazenados e que contém o registro da invocação de funções e o estado dos registradores.

A memória abriga os dados manipulados pelo programa e o próprio código em execução.

O PCB contém informações relevantes ao processo, como prioridade, ocupação de memória e manipuladores de arquivos em uso.

O grupo de processos em execução simultânea é gerenciado pelo sistema operacional, o qual é responsável pelo escalonamento e pelo gerenciamento dos recursos disponíveis.

O lançamento de um programa multithread também implica na criação de um processo.

A diferença se dá que, ao invés de apresentar um único fluxo de execução, verifica-se a existência de múltiplos threads, cada thread executando uma determinada sequência de instruções, possuindo sua própria pilha e a manutenção do estado dos registradores.

Não raro o número de threads ativos varia durante a execução do programa.

Também de forma semelhante ao thread único do processo sequencial, cada thread necessita de escalonamento para ocupar o processador.

Neste ponto, se apresenta uma característica fundamental de thread, que lhe vale o usual sinônimo de processo leve em muitos textos da literatura.

Uma vez que a quantidade de informação de contexto relativa a um thread é menor que a quantidade de informação associada de um processo, o custo de manipuação de um thread pelas ações do sistema operacional é menor que para um processo convencional.

Um programa é a solução computacional para uma aplicação.

Programas são normalmente vistos como implementações de algoritmos que, ao executar, tendo como entrada um conjunto de dados, os transformam, produzindo como resultado final um novo conjunto de dados.

Embora esta visão também se aplique a programas concorrentes, em uma análise mais detalhada podem ser identificados dois níveis nos algoritmos destes programas.

Em um dos níveis encontra-se a resolução algorítmica da aplicação propriamente dita.

Este nível é composto por tarefas, sendo que cada tarefa tem por objetivo oferecer a solução computacional para parte do problema da aplicação.

De forma análoga a um programa, cada tarefa possui um conjunto de dados de entrada e, no final de sua computação, produz resultados.

Os resultados produzidos por uma tarefa podem servir como entrada para outras tarefas ou, ao final da execução, comporão parte da solução final.

Uma propriedade particular destas tarefas é que elas podem ser submetidas à execução concorrente.

O segundo nível propõe a estratégia de controle de evolução do programa.

Em outras palavras, este nível oferece o algoritmo que coordena a execução das tarefas, tendo como atribuição principal garantir a correta ordem de execução das tarefas e comunicação de dados entre estas, de forma a atingir o resultado final esperado.

Embora em muitas implementações o código para estes dois níveis encontre-se mesclado, o programador precisa ter consciência de que ele possui dois problemas para resolver.

O primeiro diz respeito à implementação da solução do problema definido pela aplicação.

O segundo, ao uso das técnicas de programação concorrente para exploração eficiente de um determinado hardware paralelo.

Outro aspecto a ser considerado é que, embora um programa possa contar com diversas tarefas aptas a ser executadas em um determinado instante de tempo, apenas um subconjunto destas estarão executando simultaneamente.

A concorrência expressa o paralelismo potencial da aplicação, mas a execução paralela, de fato, depende da quantidade de recursos disponíveis.

Por exemplo, em um computador com quatro processadores haverá, no máximo, quatro tarefas executando paralelamente.

No contexto da multiprogramação leve para arquiteturas multiprocessadas, a natureza concorrente do programa é reflexo do número de tarefas que o compõem.

Na maioria dos casos, as aplicações contêm um grau de paralelismo muito maior que aquele suportado por um determinado hardware paralelo.

Assim, as tarefas, quando em execução, competem pelos recursos disponíveis, em particular por tempo de acesso aos processadores.

No entanto, ressalta-se que estas tarefas compõem partes de um algoritmo maior, cujo objetivo é realizar a transformação de dados, de forma a atingir o resultado final.

Nesta perspectiva, as tarefas possuem uma relação de colaboração, na qual dados armazenados em memória devem ser compartilhados, e sobre os quais cada tarefa opera seu algoritmo de transformação próprio.

Assim, a concorrência entre tarefas reflete, tanto a competição pelo acesso aos recursos de hardware, quanto a disputa no acesso aos dados compartilhados.

O controle da primeira forma de concorrência é normalmente delegado ao sistema operacional, que, munido de suas estratégias de escalonamento, garante acesso aos recursos disponíveis pelas partes componentes do programa.

A concorrência no acesso aos dados, por sua vez, é a estratégia empregada no nível de controle do algoritmo.

Esta segunda forma de concorrência deve ainda ser entendida pelo programador, não tanto pela idéia de disputa, mas sim de coordenação, pois as tarefas necessitam acessar dados para colaborar entre si, comunicando resultados de suas computações.

Na multiprogramação leve, os recursos de programação são voltados, basicamente, às operações de controle dos fluxos de execução.

Estas operações envolvem desde a criação de novos fluxos até bloqueio temporário da execução de um fluxo.

De alguma forma, todas estas operações envolvem a sincronização entre diferentes fluxos.

Estas sincronizações têm por objetivo fornecer aos fluxos de execução as informações sobre o estado da execução de outros fluxos.

As operações de sincronização permitem que fluxos de execução publiquem informações sobre seus estados de execução, ou façam leituras sobre os estados de execução de outros fluxos.

Estas sincronizações permitem introduzir a coordenação dos threads no acesso aos dados compartilhados.

A própria operação de criação de thread pode ser considerada uma sincronização, pois ela indica que o thread original produziu e armazenou na memória compartilhada dados que podem ser lidos pelo thread criado.

No entanto, outros operadores garantem outras formas de sincronização.

Talvez o mais elementar dos recursos de sincronização disponíveis seja a barreira.

Uma barreira define um ponto no código de um grupo de threads.

Cada thread deste grupo, ao alcançar este ponto, suspende sua execução, até que todos os threads do grupo tenham alcançado este ponto.

Outro mecanismo similar é o operador join, que suspende a execução do thread que o executou, até que um determinado thread, especificado como parâmetro deste operador, termine sua execução.

Outros recursos populares de programação permitem controle de acesso a seções críticas e regulagem de avanço de fluxos pelo uso de semáforos e variáveis de condição.

Uma seção crítica corresponde a um trecho de código do corpo de um thread que acessa pelo menos um dado na memória compartilhada, dado este cujo acesso também pode ser requisitado por instruções executando em outro thread.

A seção é dita crítica pela necessidade de cuidados para manuntenção da integridade do dado manipulado.

Para tanto, deve ser garantida exclusividade no acesso aos dados ao referido trecho de código, sob pena de ocorrer incoerência no acesso aos dados pela manipulação simultânea deste por dois ou mais threads.

O mecanismo normalmente utilizado para controle de seções críticas é o mutex, acrônimo para a expressão em inglês mutual exclusion, exclusão mútua.

De forma similar ao mutex, também conhecido como semáforo binário, operam os semáforos.

É importante observar que tanto mutex como semáforo oferecem duas operações de sincronização, uma indicando a intenção de um fluxo de execução entrar em um determinado trecho de código, e a segunda informando a sua saída.

É importante que estas duas operações sejam sempre realizadas de forma que as seções de código de fluxo controlado sejam monitoradas, e as execuções dos diferentes threads ocorram de forma coordenada.

Com uma concepção um pouco diferente se apresenta o mecanismo de variável de condição.

Uma variável de condição permite o controle do avanço de threads sobre determinados pontos do programa, considerando o valor de um dado presente na aplicação.

Na prática, a condição é considerada satisfeita ou não satisfeita considerando o valor deste dado no momento em que a condição é avaliada.

Caso a condição seja considerada não satisfeita, o mecanismo de variável de condição é empregado para suspender a execução de um fluxo, para que este aguarde a sinalização da mudança de valor do dado.

Este sinal é enviado quando um outro thread modificar o valor do referido dado.

A onipresença de sistemas computacionais requer agilidade nos processos de desenvolvimento de software.

A comunidade de desenvolvimento de software, em particular sua parcela ligada ao paradigma de orientação a objeto, já há alguns anos emprega técnicas de padrões de projeto (design patterns).

Na computação paralela, padrões de programação também ocorrem.

Embora haja esforço de pesquisa em padrões de projeto para computação paralela, a atenção é aqui dedicada a padrões recorrentes encontrados para representar a decomposição das tarefas em aplicações.

São diversos os padrões encontrados, mas uma discussão sobre as estruturas mais gerais, representadas pelo paralelismo de tarefas, paralelismo de dados, fluxo de dados e tarefas comunicantes, permite oferecer uma visão bastante clara das questões de projeto de um código concorrente.

O paralelismo de tarefas é bastante empregado em aplicações onde a concorrência é definida em termos de atividades que atuam sobre conjuntos disjuntos de dados.

O esforço de programação está relacionado ao controle das trocas de informações entre estas atividades concorrentes.

Aplicações ditas trivialmente paralelizáveis são um dos casos onde o paralelismo de tarefas torna-se interessante.

Estas aplicações são caracterizadas por possuírem reduzidas, ou mesmo inexistentes, necessidades de trocas de dados entre as atividades em execução.

Desta forma, a concepção de algoritmos concorrentes, em geral, é simplificada pela reduzida necessidade de sincronizações entre as atividades.

A principal propriedade que uma aplicação deve apresentar para ser implementada segundo o modelo de paralelismo de dados é possuir uma grande coleção de dados, devendo os elementos deste conjunto sofrer, de forma independente, uma série de manipulações.

Bons exemplos são encontrados em aplicações que manipulam vetores e matrizes de grandes dimensões.

As abstrações de programação permitem gerar e controlar a concorrência na manipulação dos elementos destas estruturas de dados.

O modelo de fluxo de dados representa a execução de um programa sob a forma de um grafo dirigido.

Neste grafo, os vértices correspondem às tarefas, e as arestas às dependências de dados (comunicações) entre estas tarefas.

Nesta representação, as arestas são dirigidas, partindo da tarefa que produz um dado em direção à tarefa que o consome.

A mais conhecida implementação do modelo de fluxo de dados segue o esquema de linha de produção, conhecido como pipeline, embora esteja presente em implementações de algoritmos mais complexos, como aplicações da programação dinâmica.

Finalmente, o modelo de tarefas comunicantes é composto por tarefas e canais de comunicação, sendo também representado por um grafo.

Como no modelo de fluxo de dados, os vértices deste grafo representam tarefas.

As arestas, porém, não são dirigidas e representam canais de comunicação entre tarefas.

Esta forma de comunicação indica dependência mútua das tarefas por alguma informação.

Em uma implementação multithread é comum o uso desta estrutura, particularmente considerando que canais de comunicação podem ser representados por dados armazenados em memória.

Embora os modelos de paralelização tenham sido apresentados de forma independente, é comum que implementações reais sejam compostas por combinações destes, ou mesmo por uma variante de uma estrutura básica.

Além de descrever sua aplicação em termos de um programa concorrente, o programador deve ainda atentar para não cometer erros de programação.

No uso dos recursos de programação multithread, os erros mais comuns são as condições de corrida e situações de impasse.

As condições de corrida (data-races) são situações em que não há a devida sincronização dos threads para executar instruções em uma seção crítica.

Nestes casos, as instruções de duas ou mais seções críticas podem ser executadas de forma intercalada, e não em regime de exclusão mútua.

A falha refletida por tal erro de programação é a inconsistência no valor de um dado compartilhado, pois não há garantias de como os acessos serão realizados.

Uma situação de impasse (deadlock) ocorre quando um thread permanece bloqueado aguardando um sinal, que nunca é enviado, de outro thread.

Um erro de programação típico é um thread informar sua entrada em uma seção crítica e não informar sua saída.

A falha que ocorre é suspensão da execução de parte do programa.

Outro cuidado que o programador deve ter é de não presumir ordem de execução dos threads.

Embora esquemas de prioridade sejam especificados em diferentes interfaces de programação, não é confiável delegar a credibilidade do programa desenvolvido a características de implementação de suportes de execução.

No mínimo, o programador teria o ônus de verificar a compatibilidade de seu programa a cada nova versão do suporte utilizado.

Na prática, seria comum verificar em um mesmo suporte comportamentos anômalos de execução.

Existem diversas ferramentas para programação multithread.

Dentre estas, nesta Seção são apresentadas Pthreads, NET Framework e OpenMP.

Estas ferramentas são hoje bastante populares nos seus próprios nichos de aplicação, Pthreads é bastante popular na construção de sistemas de softwares básicos, em particular em ambientes UNIX-like.

NET, por sua vez, define uma interface de programação portável para diferentes linguagens em diversos ambientes, e OpenMP abre a possibilidade de exploração de processamento de alto desempenho em aplicações com grão fino de concorrência.

Dadas as restrições de espaço e o escopo deste curso, esta Seção se limita a uma visão geral destas ferramentas.

O padrão POSIX (Portable Operating System Interface for uniX), proposto pela IEEE, define um conjunto de interfaces de programação para o sistema UNIX.

O padrão para threads é normalmente referenciado como Pthreads.

Nos sistemas UNIX-like, Pthreads são disponibilizadas na forma de uma biblioteca.

O padrão caracteriza a interface de serviços para uso conjunto com programas C/C++, embora existam esforços de padronização desta interface para a linguagem FORTRAN.

A importância de Pthreads está intimamente associada à abrangência deste padrão.

Como o padrão POSIX encontra-se disponível na maioria dos sistemas operacionais, seu potencial de portabilidade é bastante alto.

Uma das principais características desta interface de programação multithread é que ela oferece grande liberdade ao programador para implementação de seu programa.

Como resultado, diversas estruturas de programa concorrente podem ser empregadas.

Nesta seção considera-se a utilização de Pthreads em ambiente GNU/Linux.

Nestes ambientes, o pacote Pthreads conta com dois arquivos principais, o arquivo de header e a biblioteca de funções.

O arquivo de header, pthread h, deve ser incluido no programa-fonte que utiliza os serviços da biblioteca.

A biblioteca propriamente dita deve ser ligada ao programa do usuário, o que pode ser feito passando o parâmetro lpthread para o compilador GCC.

Em Pthreads, o lançamento de um novo thread é associado à execução da sequência de instruções definidas no corpo de uma função.

Esta função deve receber como parâmetro um endereço para uma área de memória, correspondendo ao local onde encontram-se os parâmetros de entrada da função, retornando outro endereço de memória, onde encontram-se os resultados produzidos pela função.

O retorno deve encontrar-se no heap ou alocados globalmente, porque variáveis alocadas automaticamente pela função na pilha são descartadas no fim da execução da função.

Ambos os endereços são do tipo void.

A criação e término de threads são realizadas de forma explícita.

A invocação da primitiva pthread_create por um thread implica a criação de um novo fluxo.

Um thread termina sua execução quando executar toda a sequência de instruções definida em seu corpo, ou quando executar o comando return convencional de C, alternativamente, invocando o serviço pthread_exit.

A assinatura destas funções é dada por, Em pthread_create, o novo thread a ser criado deve executar o código especificado pela função func.

Este thread poderá ser referenciado futuramente por meio do valor retornado no parâmetro thid.

Pthread permite ainda aplicar atributos de execução à nova thread.

Estes atributos são passados à biblioteca pelo parâmetro atrib.

O inteiro retornado corresponde a um código de erro, com zero normalmente associado ao sucesso na execução.

O padrão não define quando o novo thread será disparado, tão pouco como se dará a sobreposição do tempo de uso do processador por este novo thread com os threads já em curso de execução.

Em contrapartida, há o serviço pthread_join, que sincroniza um thread com o thread identificado por thid, garantindo que um thread continue apenas quando o outro thread tenha terminado sua execução.

Adicionalmente, o parâmetro ret retorna o endereço de memória que contém os dados retornados pelo thread sincronizado quando este terminar.

Caso o thread sincronizado já tenha terminado, ocorre simplesmente a recuperação do retorno.

Caso contrário, o thread que necessita a sincronização permanecerá bloqueado até que esta seja satisfeita.

Cada thread pode sofrer apenas uma operação de join.

Caso o retorno do thread seja útil a dois ou mais pontos do programa, o programador deve usar estratégias de programação próprias.

Outro cuidado a ser tomado é em relação à utilização da primitiva pthread_exit em programas C++, a execução desta primitiva termina o fluxo de execução corrente, mas não invoca os destrutores dos objetos armazenados na área de memória local ao thread.

Um exemplo da utilização deste primeiro conjunto de primitivas é apresentado a seguir.

Neste exemplo, uma função oiMundo é executada por um thread.

O programa principal, função main, cria o thread e aguarda seu término.

Neste exemplo, foi utilizado o valor NULL para os atributos do thread criado, fazendo com que assumam os valores default.

Os atributos desejados são descritos em uma variável do tipo pthread_attr_t.

O padrão POSIX para threads define um conjunto de serviços para manipular variáveis deste tipo, entre eles, Enquanto o serviço init inicializa uma variável com os valores default de atributos, o serviço destroy torna uma variável de atributo inutilizável.

Os demais serviços set* permitem configurar diferentes propriedades para atributos de threads.

O primeiro apresentado setdetachstate pode receber dois valores, PTHREAD_CREATE_JOINABLE, para indicar que o thread deve sofrer uma sincronização por join ou PTHREAD_CREATE_DETACHED, indicando o contrário.

A comunicação entre threads não se limita apenas aos parâmetros de entrada e do retorno da função executada por um thread.

A própria memória do processo serve de base de comunicação, sendo que o acesso à memória se dá pela simples execução de instruções de escrita e leitura.

O problema é garantir o correto acesso às informações pelos threads, evitando condições de corrida e situações de impasse.

A biblioteca Pthreads oferece os recursos de mutex e variável de condição.

Em Pthreads o mutex é oferecido pelo tipo pthread_mutex_t.

Instâncias deste tipo são usualmente chamadas de mutexes e são manipuladas pelos seguintes serviços, De forma análoga ao que ocorre com os atributos de thread, anteriormente descritos, os serviços init e destroy preparam para o uso e inutilizam um mutex.

O atributo default de inicialização é indicado por NULL, fazendo com que inicialmente o mutex esteja disponível.

Os serviços lock e unlock permitem manipular o mutex.

A invocação do serviço lock indica o desejo do thread corrente em executar instruções em uma seção crítica.

A invocação ao unlock indica a saída do thread de uma seção crítica.

Enquanto a operação do serviço unlock é simples, apenas liberando o mutex, a operação de aquisição pode resultar em um thread bloqueado, caso outro thread já tenha adquirido o mutex.

A seguir, um exemplo do uso de mutex para sincronizar dois threads é apresentado.

Observe que o mutex m é uma variável de acesso global, e seu uso depende da lógica adotada no algoritmo implementado.

No exemplo, o algoritmo associa o mutex m à variável x.

Cada seção crítica que necessita acesso a x deve explicitar as operações lock e unlock.

Caso existissem outras variáveis compartilhadas, outros mutex poderiam ser criados e manipulados dentro do programa.

O serviço trylock, por sua vez, não é bloqueante.

Ele é utilizado para adquirir um mutex, caso este esteja disponível.

Outro mecanismo disponível na biblioteca Pthreads é a variável de condição.

Este recurso permite controlar o avanço de threads sobre seções críticas, considerando o valor de algum dos dados manipulados pelo algoritmo da aplicação.

Os serviços disponíveis são os seguintes, Como mostram as assinaturas das funções, variáveis de condição também são manipuladas com apoio de variáveis construídas a partir de um tipo de dado especial, pthread_cond_t.

Os serviços init e destroy permitem inicializar (o valor default de atributo é NULL) e inutilizar uma variável deste tipo.

A invocação do serviço wait por um thread faz com que este suspenda sua execução, até que receba um sinal indicando que a condição foi satisfeita.

Os serviços signal e broadcast podem ser executados por threads para sinalizar que uma condição encontra-se satisfeita, signal envia o sinal de condição satisfeita a apenas um dos threads suspensos na condição indicada, broadcast sinaliza todos os threads suspensos na variável de condição.

Note-se, no entanto, que variáveis de condição são utilizadas para coordenar acesso a uma seção crítica.

Assim, seu uso é associado, necessariamente, a um mutex.

Isto é caracterizado no parâmetro do tipo mutex indicado no serviço wait.

Segue um exemplo de um produtor/consumidor utilizando variáveis de condição para sincronizar a produção e o consumo de itens em um buffer.

É importante observar que uma sinalização em uma variável de condição não é memorizada, ou seja, somente os threads em estado de wait recebem o sinal.

Outro aspecto importante a observar é que tanto o serviço wait como signal (ou broadcast) devem ser executados dentro das seções críticas que manipulam os dados compartilhados.

Não existe possibilidade de ocorrer uma situação de impasse, pois a implementação do wait, na biblioteca, faz com que a primeira ação executada seja a liberação do mutex recebido como segundo parâmetro e, assim que o sinal de condição satisfeita for recebido, faz com que o thread entre novamente na disputa pelo mutex.

Outro mecanismo de sincronização de fluxos de execução são os semáforos.

O padrão Pthreads não define serviços para sua utilização, sendo sua especificação apresentada na Seção IEEE 10031 b.

No entanto é comum que serviços para manipulá-los sejam disponibilizados.

Por exemplo, no GNU/Linux semáforos são disponibilizados pela inclusão de semaphoreh.

Alguns dos serviços disponibilizados são, A manipulação de semáforos se dá com apoio de variáveis do tipo sem_t.

Enquanto o serviço destroy inutiliza o uso de um semáforo, init permite inicializá-lo, val deve informar o valor inicial do semáforo e comp permite indicar se o uso do semáforo será restrito aos threads do processo corrente ou se deverá operar entre processos.

O valor zero para comp indica uso local.

As operações wait e post correspondem, respectivamente, às tradicionais operações P e V de semáforos.

O algoritmo básico para um semáforo pode ser encontrado Há ainda outros mecanismos de sincronização disponíveis, tais como barreiras, os quais não são apresentados neste texto.

Em 2002, foi lançado pela Microsoft o CLR (Common Language Runtime), um ambiente de execução desenvolvido para permitir uma plataforma de execução única para diferentes linguagens.

Similar a Java, este ambiente opera como uma máquina virtual, oferecendo uniformidade de operação.

Junto com CLR, foi também apresentado um conjunto de recursos de programação conhecidos como NET Framework.

Os recursos de NET podem ser utilizados em diversas linguagens, como C#, C++, Visual Basic NET e JScript.

NET propõe uma interface de programação de alto nível, adotando características da programação orientada a objetos.

A popularidade desta interface de programação pode ser associada ao grau de abstração proposto, embora a compatibilidade da interface entre as linguagens suportadas pelo CLR não deva ser desconsiderada.

Outro fator ainda a ser considerado é o grande número de computadores que rodam sob o sistema operacional da Microsoft, e a implementação multiplataforma do ambiente Mono.

Neste texto, os exemplos NET foram apresentados em C#.

Para informar que utilizará os serviços de thread em c#, deve-se incluir o pacote System Threading.

O corpo do código de um thread deve ser especificado em um método estático de uma classe qualquer.

A manipulação de um fluxo de execução se dá com auxílio de um objeto da classe Thread.

Um objeto desta classe é criado explicitamente pela invocação do serviço ThreadStart.

Este serviço necessita como parâmetro um delegate para o método a ser executado pelo thread.

O termo delegate segue a terminologia adotada por NET, podendo ser considerado equivalente ao endereço do método.

Por exemplo, A criação de um objeto da classe Thread não implica na ativação de um novo fluxo de execução.

O lançamento do novo thread deve ser realizado de forma explícita, com invocação do método Start no objeto Thread.

De forma equivalente, o método Join pode ser empregado para sincronizar a execução de um thread com o término de outro.

Com a sobrecarga de método, o método Join também aceita um valor (inteiro) indicando o tempo em milisegundos que o thread corrente pode aguardar a sincronização.

Caso a sincronização seja satisfeita é retornado o valor true, caso contrário false.

Uma outra possibilidade em C#/NET é a de interromper, pela invocação do método Abort, a execução de um thread.

A chamada deste método implica o envio de um sinal que pode ser capturado pelo thread, possibilitando o abandono de sua execução.

Este recurso faz uso do mecanismo de tratamento de exceções, conforme ilustra o seguinte exemplo, Além do método Abort, dois outros métodos podem ter aplicações interessantes na manipulação de threads, Suspend e Resume.

Com estes métodos, é possível interromper e retomar a execução de threads.

Em C#, o recurso mais simples, mas nem por isso pouco eficiente, é a primitiva lock, operando sobre um objeto.

O caso típico de uso é não permitir acesso simultâneo de dois ou mais métodos sobre um atributo de um objeto.

Um exemplo desta situação é dado no trecho de código seguinte, onde a primitiva lock é aplicada sobre um objeto ctrLock para proteger a região crítica onde o saldo é incrementado.

Uma possibilidade seria utilizar a própria referência this já existente para o objeto ao invés de criar um objeto especialmente para este fim.

No entanto, sabendo que o objeto é público, a criação de um atributo privado permite maior robustez no código.

As operações atômicas em C# usam os serviços da classe Interlocked.

Desta classe, Increment e Decrement permitem incrementar e decrementar um valor armazenado em uma variável inteira.

O método Exchange permite a inicialização atômica de uma variável.

O exemplo na sequência manipula o atributo inteiro contador.

Os monitores, por sua vez, são bastante similiares, em sua estrutura, ao mecanismo oferecido por lock, sendo que o programador deve usar os métodos estáticos oferecidos na classe Monitor.

Monitores garantem execução de seções críticas em regime de exclusão mútua com apoio de um objeto válido.

A entrada em uma seção crítica é requisitada pela invocação do método Enter e a saída informada pela invocação do método Exit.

Também é possível utilizar o serviço não-bloqueante TryEnter, que retorna o valor booleano false, caso algum outro thread detenha o direito de uso do monitor.

O exemplo de uso do monitor apresentado manipula um objeto ctrMonitor para controlar o acesso às seções críticas.

O uso das cláusulas try/finally não são obrigatórias, no entanto elas aumentam a robustez do código, garantindo que, na ocorrência de alguma exceção que impossibilite a execução do código na seção crítica, o monitor será liberado.

Este procedimento previne eventuais situações de impasse.

Outro recurso disponível para manipulação de monitores é a liberação temporária do direito de execução sobre a seção crítica.

A invocação ao método Wait, após ter sido adquirido o direito de uso do monitor, faz com que o thread corrente libere o acesso ao monitor em uso e, imediatamente, entre na disputa por ele novamente.

Utilizar este serviço favorece a implementação de algoritmos em que ocorrem constantes trocas de dados entre os threads.

O mecanismo de NET para mutex possui uma estrutura diferente do empregado nos monitores.

Um mutex é manipulado por meio de um objeto mutex, instanciado a partir da classe Mutex.

Os métodos de tentativa de acesso às seções críticas e de saída são, respectivamente, WaitOne e ReleaseMutex.

Um exemplo de uso de mutex é dado como segue.

Como o exemplo caracteriza, o mutex m é um objeto estático, de forma a ser compartilhado por todas as instâncias da classe Contador.

Semáforos em NET são bastante semelhantes ao mutex, sendo disponibilizados pela classe Semaphore.

A diferença básica é que um objeto mutex é binário, e um semáforo pode ter qualquer valor positivo.

Assim, a sincronização garante a passagem de um conjunto de threads, de acordo com o valor inteiro positivo do semáforo.

O valor inicial é dado quando da criação do objeto, e os métodos WaitOne e Release o manipulam.

WaitOne é bloqueante caso o valor associado ao semáforo seja igual a zero, isto significa que já foi atingido o número permitido de passagens de threads especificados pelo semáforo.

Caso contrário, o valor do semáforo é decrementado e o thread prossegue sua execução.

O método Realease incrementa o valor associado ao semáforo, permitindo que um outro thread adquira direito de passagem.

OpenMP é um acrônimo para Open Multi-Processing.

Em 1997,6 OpenMP foi proposto como uma interface de programação multithread para a linguagem FORTRAN.

Atualmente, além de FORTRAN, está também disponível na linguagem C/C++ em diferentes plataformas UNIX-like e Windows.

Esta interface de programação é fortemente apoiada por processos de compilação.

Programas OpenMP são desenvolvidos principalmente por meio de diretivas de compilação, podendo ainda incorporar chamadas a serviços de biblioteca em tempo de execução.

Outra forma de interação possível é via variáveis de ambiente.

O grande apelo desta plataforma é seu alto grau de portabilidade.

Para tanto, colabora o fato de diversos compiladores incluírem mecanismos de apoio a esta interface.

Somado a isto, a facilidade de paralelização de código também tem papel crescente na sua popularidade.

Os recursos de programação disponibilizados permitem incorporar trechos paralelos a programas sequenciais de forma incremental, sem alterar o algoritmo da aplicação.

Dentre as variáveis de ambientes utilizadas para influenciar a execução de programas OpenMP, OMP_NUM_THREADS determina o número de threads default para executar trechos de código em regiões paralelas.

Outras variáveis de ambiente permitem determinar as demais condições de execução, como estratégia de escalonamento de threads e tamanho da pilha de dados dos threads.

Para compilar programas OpenMP nas versões mais atuais de GCC, usa-se a flag fopenmp, enquanto que o compilador Intel necessita do parâmetro openmp.

A estrutura de programas OpenMP segue o modelo fork/join.

Uma operação fork identifica, implícita ou explicitamente, atividades a ser executadas concorrentemente.

A operação join sincroniza o término da execução de todas as atividades disparadas por um fork.

As atividades concorrentes definidas pelo programa não são threads de fato, mas sim unidades de trabalho a serem executadas por threads de serviço, implementados no ambiente de execução.

No entanto, neste texto, o termo thread é utilizado como sinônimo de atividade concorrente, de forma a manter a nomenclatura compatível com a empregada pelas demais ferramentas vistas.

O número de atividades concorrentes efetivamente instanciadas depende, tanto do código a ser executado, quanto de decisões de escalonamento tomadas pelo ambiente de execução.

A estratégia considera, entre outros critérios, o número de threads de serviço disponíveis, identificado na variável de ambiente OMP_NUM_THREADS, e a execução inicia atribuindo a um destes threads a responsabilidade da execução da função main.

Este thread de serviço é identificado como mestre do time de threads de serviço.

A interface de programação de OpenMP emprega diretivas de compilação (pragmas) como abstração para as operações fork/join.

Chamadas a serviços de biblioteca permitem interação do programa em execução com o ambiente de execução.

As diretivas de compilação OpenMP são compostas por sentinelas, diretivas e cláusulas, Nesta sintaxe, #pragma omp é chamado sentinela, sendo dependente da linguagem de programação utilizada.

A diretiva OpenMP é identificada em diretiva, opcionalmente acompanhada de uma ou mais cláusulas.

O corpo para os threads é definido em corpo, composto por um bloco de instruções ou por um comando de iteração por bloco.

Nas instruções do bloco não podem ser incluídas instruções de salto (goto) para qualquer posição do programa, mesmo dentro da própria região paralela.

As operações fork e join encontram-se implícitas nesta estrutura, o fork é especificado na própria linha onde o pragma se encontra e o join no final do bloco que especifica o corpo das atividades concorrentes.

Um primeiro exemplo é dado pela utilização da diretiva sections, Neste exemplo, foram definidas duas seções, cada uma identificada por um bloco específico.

Estas duas seções definem o corpo para a execução de dois threads do programa, o primeiro executa a função identificada por foo e a segunda a função bar.

O conjunto dos dados de entrada dos threads criados é a memória acessível ao thread corrente, no exemplo representado pela variável inteira x.

O comportamento padrão define que o acesso à memória é compartilhado pelos fluxos criados.

Assim, os threads criados acessam a mesma instância de x, podendo ocorrer situações de corrida caso algum dos threads acesse x em escrita.

Outro aspecto interessante é que as execuções de todas as seções paralelas sincronizam no seu final de forma implícita, ou seja, o trecho de código que segue a definição de seções paralelas somente será habilitado a executar quando todos os threads completarem suas respectivas execuções.

Outra diretiva para expressar paralelismo é parallel.

Esta diretiva implica a criação de threads, que deverão ser executados pelos threads de serviço do ambiente de execução.

Como o número de threads de serviço corresponde ao valor associado à variável de ambiente OMP_NUM_THREADS, o número de threads em execução concorrente será dependente deste número.

Assim, se esta variável possuir o valor 4, quatro threads de serviço executarão, de forma paralela, quatro threads do programa de aplicação.

O uso desta diretiva é exemplificado por, Caso OMP_NUM_THREADS = 4, quatro instâncias do código especificado serão executadas.

Tal como ocorre nas seções paralelas, o final da região paralela determina uma barreira para sincronização entre os threads.

No trecho de código apresentado também é exemplificada a comunicação de da dos entre os threads, no caso as variáveis inteiras x e y.

O comportamento-padrão implica que todos os dados locais ao thread original sejam acessíveis aos threads criados.

No exemplo, esta situação é observada pelo uso da variável inteira y.

O comportamento padrão é alterado para uso da variável x, onde a cláusula private indica que cópias locais devem ser instanciadas para cada thread, sendo que o valor inicial de x local é indefinido.

Caso seja necessário, é possível herdar o último valor de uma variável antes do disparo dos threads, fazendo uso da cláusula firstprivate.

O comportamento-padrão, compartilhamento de instância de dados, pode ser identificado pelo uso da cláusula shared.

No sentido inverso, dos threads criados para o thread original, podem ser aplicadas duas outras cláusulas, lastprivate e reduce.

A primeira faz com que o valor computado pelo último thread do programa seja o valor utilizado pelo thread original após a sincronização.

Uma variável pode ser, ao mesmo tempo, first e lastprivate.

A cláusula reduce é mais elaborada, permitindo a combinação de resultados parciais dos threads ao final.

Esta combinação pode ser realizada aplicando um dos operadores disponíveis, aritméticos ou lógicos.

O uso da cláusula reduce é exemplificado na sequência.

Neste exemplo, a variável inteira soma é inicializada com 100 e são criados quatro threads, cada uma produzindo o valor 1.

No final, o valor acumulado corresponde a 104, ou seja, o valor inicial de soma adicionado de uma unidade para cada thread executado.

Note-se o uso de set_num_threads para definir o número de threads a serem criados.

Outro importante recurso disponível em OpenMP é a possibilidade de realizar a paralelização de laços.

A diretiva empregada é parallel for.

O código a seguir apresenta o uso desta primitiva para acumular os valores de um vetor.

Algumas regras devem ser seguidas para construção do laço a ser paralelizado.

A variável de iteração necessita ser do tipo inteiro com sinal (int em C/C++).

Da mesma forma, devem ser valores inteiros com sinal o teste da condição de término do laço e o valor a ser adicionado ou subtraído da variável de iteração a cada iteração realizada.

Estes dois valores não podem variar durante toda execução do laço.

O teste de continuação pode empregar, apenas, os seguintes operadores relacionais, <, <=, > e >=, incrementando ou decrementando, conforme o caso, a variável de iteração a cada iteração.

Outro aspecto importante é que o corpo do laço possui apenas uma entrada e uma saída, o comando break de C/C++ não pode, portanto, ser utilizado.

Embora de aparência e utilização simples, na prática a paralelização de laços não é uma tarefa trivial.

Um conjunto de regras deve ser seguido para eficiência na utilização deste recurso.

O programador deve evitar situações nas quais a passagem por uma iteração qualquer dependa da execução de uma passagem anterior, como no caso onde a computação da passagem i depende dos resultados da computação da passagem i 1.

A discussão de estratégias para detecção e remoção de dependências entre instruções foge ao escopo deste texto, sendo aconselhado ao leitor buscar maiores informações sobre o assunto em literatura especializada.

O substrato para comunicação de dados entre os threads providos por OpenMP é o espaço de endereçamento único provido pela memória compartilhada.

Não são oferecidos mecanismos de troca de dados via parâmetros e retorno de funções, como em Pthreads, nem abstrações como monitores e objetos como em C#/Net.

Mecanismos de controle explícito do fluxo de execução podem ser realizados por meio de diretivas e cláusulas específicas para coordenar a execução dos threads ativos.

O mais básico destes recursos é oferecido pela diretiva barrier.

O uso desta primitiva implica a criação de um ponto de sincronização entre todos os threads criados a partir de uma diretiva parallel comum.

Este mecanismo garante que, no escopo da execução de cada thread, a instrução seguinte à barreira somente será executada após todos os threads participantes terem satisfeito a condição de sincronização, ou seja, terem atingido a barreira.

Um mecanismo mais elaborado que a barreira é possível com o uso da diretiva critical.

O uso desta diretiva permite controlar a execução de blocos de comandos em um regime de exclusão mútua.

Considere o código apresentado na sequência.

Este código encontra-se inserido em uma região paralela, e tem por objetivo manter a variável compartilhada maior atualizada com o maior valor encontrado em um vetor.

A barreira implementada pela diretiva critical tem abrangência global, ou seja, é aplicada a todos os threads executando coletivamente.

Para evitar contenção desnecessária, é possível nomear seções críticas, identificando claramente o recurso a ser utilizado em exclusão mútua.

Assim, no exemplo anterior, poderia ser informado critical(MAXVAL) em ambos os pragmas, identificando, na lógica do programa, que o recurso compartilhado é a variável global maior.

Em algumas situações, dentro de uma região paralela, pode ser necessário executar um trecho de código apenas por um dos threads ativos.

Um exemplo típico é dado pelo envio de saídas para o console (impressão de mensagens na tela).

A diretiva single permite definir um trecho de código que deve ser executado apenas por um dos threads.

O código na sequência exemplifica o uso da diretiva single considerando duas situações, na primeira, todos os threads permanecem bloqueados aguardando que o usuário seja informado que diferentes threads serão criados para execução de trabalho, na segunda, o usuário é informado que a execução de uma destas instâncias foi concluída com auxílio da cláusula nowait.

Esta última relaxa a condição de sincronização dos threads, permitindo que threads avancem sem a necessidade de sincronização no final do bloco single.

O padrão não define, para a diretiva single, qual thread é responsável pela execução do referido trecho de código.

É garantido apenas que o código será executado uma única vez, e que todos os threads devem permanecer bloqueados no final do corpo do código da seção single aguardando seu término, exceto se for utilizada a cláusula nowait.

Opcionalmente a diretiva master pode ser utilizada.

Esta diretiva garante que o código será executado pelo thread mestre de um time.

Nesta Seção, são apresentados alguns aspectos que devem ser considerados quando se usa ferramentas de multiprogramação leve.

Em particular, são destacados aspectos relcionados ao desempenho, embora nem todas as implementações sejam focadas em processamento de alto desempenho, é desejável evitar onerar a execução de aplicações com implementações mal projetadas.

Inicialmente, esta Seção apresenta os principais cuidados a serem tomados no momento do projeto de um programa multithread.

A sequência aborda questões associadas ao desempenho, tais como estratégias para exploração da hierarquia de memória disponível em arquiteturas multicores e associação de afinidade de threads a processadores.

Quando se trata de programação concorrente, a primeira preocupação do programador deve ser identificar, na sua aplicação, quais são as atividades concorrentes que podem ser definidas e quais as dependências de dados existentes entre estas.

Esta etapa visa realizar a decomposição paralela do programa.

De posse desta informação, é possível caracterizar a decomposição como funcional ou de dados.

A decomposição funcional consiste em mapear as atividades concorrentes de um programa em unidades de execução independentes (os threads).

Normalmente estas atividades concorrentes são implementadas como uma função, podendo receber e retornar dados, como parâmetros ou retorno de resultados.

A decomposição de dados, por outro lado, explora a propriedade de fraco acoplamento de dados que certas aplicações possuem, e permite dividir um dado de entrada, o domínio inicial, em um conjunto de domínios independentes, atribuindo cada subdomínio a uma computação passível de execução paralela por um thread.

O resultado final é obtido com a agregação dos resultados parciais obtidos a cada computação realizada.

Esta forma de decomposição é, normalmente, bem representativa em compiladores paralelizantes e em OpenMP.

O programador deve também estar atento para identificar as duas formas de paralelismo em uma determinada aplicação.

Assim, em um primeiro momento, o programa pode exprimir atividades concorrentes caracterizadas pela decomposição funcional da aplicação.

Internamente, cada atividade concorrente pode ainda determinar etapas de sua computação, que podem ser executadas segundo o modelo de decomposição de dados.

Outro aspecto fundamental é a determinação da granularidade da aplicação.

A granularidade expressa a relação entre a quantidade de cálculo e o número de sincronizações efetuadas por um programa paralelo em execução.

O tamanho do grão pode ser facilmente entendido como a quantidade de computação existente entre duas sincronizações (em termos médios).

Por fim, deve-se considerar a portabilidade do programa não apenas em relação ao seu código.

Deve também ser considerada a sua portabilidade de desempenho.

A portabilidade de código diz respeito à possibilidade de migrar código entre duas plataformas distintas.

Já a portabilidade de desempenho está relacionada ao potencial de escalabilidade de execução do programa.

Em outras palavras, é a propriedade que o programa possui de ter seu desempenho de execução melhorado à medida em que aumenta o poder de processamento paralelo de uma determinada arquitetura.

As linguagens e bibliotecas apresentadas neste texto possuem um bom grau de portabilidade, permitindo a execução em ambientes UNIX-like e Windows através de diferentes implementações.

Portabilidade com o sentido de escalabilidade é uma nova questão que tem cada vez mais despertado atenção dos desenvolvedores de programas paralelos.

Atualmente, processadores multicores dispõem de dois ou quatro cores de processamento.

No entanto, o histórico na evolução dos sistemas computacionais permite inferir que este número irá crescer, podendo facilmente atingir dezenas de cores.

A implicação imediata é que diferentes configurações de plataforma estarão disponíveis, com até 80 cores.

Para minimizar o impacto do tempo de acesso à memória principal no tempo total de processamento de programas, as arquiteturas de processadores modernos fazem uso de hierarquias de memória com caches.

A exploração da localidade de referência de dados e instruções permite diminuir o número de acessos à memória principal, concentrando acessos nas caches, mais rápidas e mais próximas do processador.

Estratégias de programação podem ser empregadas para melhorar o aproveitamento das caches.

Em ambientes multiprocessados como os processadores multicores, o impacto do emprego ou não de tais estratégias no desempenho é ainda maior.

As técnicas mais comuns visam aumentar a localidade de referência de dados em instruções próximas.

Uma técnica bastante simples consiste em observar como os elementos de um arranjo multidimensional, como uma matriz, são alocados em memória, e fazer com que laços aninhados percorram este arranjo explorando a vizinhança entre os dados.

Por exemplo, em C/C++ os elementos de uma matriz bidimensional são ordenados por linha.

Assim, em C/C++, o elemento da primeira coluna de uma linha i qualquer ocupa a posição de memória seguinte à ocupada pelo elemento da última coluna da linha i 1.

Dessa forma, sendo necessário percorrer todos os elementos desta matriz, é mais interessante organizar o acesso a cada linha, percorrendo todas as suas colunas, conforme o seguinte código, Outra estratégia para explorar a localidade de dados é menos evidente ao programador, considerado o mecanismo empregado pelo processador para alinhar dados em memória.

Este alinhamento é definido pela arquitetura do processador, de forma a otimizar o desempenho de sua operação.

Um endereço de variável de um tipo que possua n bytes é dito alinhado caso n seja potência de dois e o endereço seja múltiplo de n.

Dessa forma, uma variável do tipo char é considerada alinhada em qualquer posição de memória, e uma variável int somente nas posições de memória múltiplas de quatro.

Apesar do ganho de desempenho na manipulação dos dados, podem ocorrer espaços vazios de memória, o que pode ser ilustrado pela execução do seguinte código, No caso do código apresentado, é pouco provável que a diferença entre os endereços corresponda a um  byte, isto aconteceria em situações onde a variável x pudesse ser alocada em uma posição imediatamente anterior a um endereço alinhado para y.

Como experiência, substitua a linha char x, por int x, e depois por char a, b, c, x. 

Nestes casos, a diferença entre os endereços de x e y são quatro bytes, o tamanho necessário para armazenar o inteiro y em um processador de 32 bits.

Como regra geral, o alinhamento dos dados deve ser observado para não disperdiçar espaços de memória.

Em particular deve ser evitado intercalar a definição de variáveis de tipos de dados diferentes, principalmente quando o tamanho em bytes dos seus respectivos tipos de dados diferir substancialmente.

Em arquiteturas multiprocessadas e multicores, uma outra estratégia para exploração da memória cache é a associação de afinidade de threads a processadores.

Esta estratégia pode ser explorada quando o programador dispõe de informações para realizar o balanceamento de carga de sua aplicação.

Suponha-se um chip com dois cores e um programa onde quatro threads disputam recursos de processamento.

Neste programa, dois threads são dedicados a cálculo intensivo e dois a operações de entrada e saída.

Um escalonamento satisfatório deveria atribuir um thread de cálculo e um thread de entrada e saída a cada um dos cores disponíveis.

Importante observar que, caso um programador incorpore no seu código instruções para definir afinidades de threads a processadores, significa que ele está retirando do mecanismo de escalonamento do sistema operacional a atribuição de definir em qual processador um determinado thread deverá executar.

Isto também caracteriza o uso de recursos de programação de baixo nível, acessando diretamente serviços providos pelo sistema operacional.

Atualmente, OpenMP não incorpora facilidades para definição de afinidades de threads.

Em NET, é possível atribuir afinidade de todos os threads de um processo a um processador pelo método set_ProcessorAffinity, do pacote Process.

Este método recebe como parâmetro um inteiro, cujo valor representa uma máscara com a qual é representada a afinidade estabelecidada entre o thread e processadores.

O valor para a máscara depende do número de processadores reais disponíveis.

Cada bit nesta máscara corresponde a um processador, devendo ser colocado com valor 1 o bit correspondendo ao processador habilitado a executar os threads do processo.

Dessa forma, o valor default é 2 1, onde n é o número de processadores do sistema.

Considerando um processador com quatro cores, o valor 0 x0001 habilita o core 1 a executar os threads, enquanto o valor 0 x0003 habilita os cores 1 e 2.

O método get_ProcessorAffinity permite obter a afinidade determinada para os threads do processo corrente.

Em ambiente GNU/Linux, a chamada de sistema sched_setaffinity permite registrar a afinidade, enquanto que sched_getaffinity consulta a afinidade atualmente associada.

No caso do GNU/Linux, estes serviços são aplicados individualmente a threads, não ao conjunto de threads de um processo.

Processos criados com a chamada de sistema fork herdam do processo-pai sua afinidade.

Durante a execução de um programa multithread, o uso de mecanismos de sincronização como barreiras e seções críticas restringe o paralelismo de execução.

Em outras palavras, a execução de uma primitiva de sincronização implica, de uma forma ou outra, serialização de instruções pertencentes a dois fluxos de execução distintos.

Caso um programa introduza um grande número de sincronizações, o efeito que pode ser observado é a contenção de execução, além de introduzir tempo de processamento extra no processamento da sincronização propriamente dita (não raro envolvendo chamadas de sistema), pode ocorrer o bloqueio de diversos threads pela espera de um mesmo recurso.

Como resultado, observa-se um baixo potencial de portabilidade.

As alternativas que se apresentam tendem a incorporar nas implementações soluções algorítmicas que eliminem a necessidade de sincronizações ou, pelo menos, diminuam o impacto destas no tempo total de processamento.

Uma prática de programação bastante empregada é reduzir ao máximo o número de instruções nas seções críticas, de forma a minimizar o tempo de espera, e, consequentemente, o número de threads na fila de espera, pelo uso de um determinado recurso.

O uso desta técnica, no entanto, pode implicar adição de sobrecustos extras de processamento, não somente em função da frequência de requisição e liberação de recursos, mas também do esforço extra do mecanismo de escalonamento em trocas de contextos a cada requisição bem/mal sucedida de recursos de sincronização.

Como alternativa, aponta-se o uso de spin locks.

Spin locks são mecanismos para controle de acesso a seções críticas no qual o thread invocador da operação lock não libera o processador, caso o recurso solicitado não esteja disponível.

A idéia é evitar a troca de contexto do thread solicitante, apostando que o tempo de espera pelo recurso não seja muito longo.

A biblioteca Pthreads oferece este recurso, A interface de serviços oferecida é bastante semelhante à empregada para uso de mutex convencional.

Destaca-se o parâmetro pshared na inicialização de um spin lock, podendo indicar que o escopo a ser empregado é limitado ao contexto do processo corrente (valor PTHREAD_PROCESS_PRIVATE) ou compartilhado entre processos (PTHREAD_PROCESS_SHARED).

É importante observar que o uso de spin lock pode favorecer a obtenção de desempenho.

No entanto, o tempo de espera no laço deve ser calibrado de acordo com o sistema em que o programa encontra-se executando, uma espera muito curta pode implicar em fracasso na tentativa de obter o recurso e uma consequente troca de contexto torna-se inevitável, já uma espera muito longa pode comprometer a estratégia.

Outro ponto é a relação entre o número de threads em execução e o número de processadores disponíveis.

Se esta relação indicar que o número de processadores é baixo, o uso deste recurso torna-se desinteressante, em um caso extremo, com um único processador, não é interessante usar spin locks.

Outras técnicas podem ser utilizadas, tais como o uso de instruções especiais que executam de forma atômica.

No entanto, a portabilidade dos programas fica comprometida.

A programação concorrente e ferramentas para programação multithread são temas obrigatórios nos cursos de graduação na área de Computação.

Com a recente popularização de processadores multicore, observa-se a ampliação do interesse nestes assuntos, uma vez que o processamento paralelo passa a ser quase uma necessidade para exploração eficiente de tais arquiteturas.

A demanda atual é por utilização consciente dos conceitos associados à multiprogramação leve para obter das arquiteturas multicore o máximo de sua capacidade.

A arquitetura multicore representa um dos mais recentes avanços na tecnologia de implementação de arquiteturas de processadores.

Diferente das técnicas que a precederam, processadores multicore explicitam seu potencial para execução paralela na forma de várias unidades de processamento independentes.

Como consequência, no desenvolvimento de um programa para este tipo de arquitetura, deve ser prevista a execução concorrente de atividades.

Neste contexto, a multiprogramação leve se apresenta como uma importante aliada do programador, oferecendo recursos de programação orientados a arquiteturas multiprocessadas.

Além de apresentar a arquitetura dos processadores multicore, pontuando aspectos históricos que marcaram a evolução tecnológica em direção a esta solução, o texto apresentou uma visão geral da multiprogramação leve, considerando seu modelo de programação e execução.

A maior parte do capítulo foi dedicada à prática da programação, sendo apresentadas três ferramentas de programação multithread, Pthreads, NET Framework e OpenMP.

Estas ferramentas foram selecionadas tendo em vista critérios como popularidade e representação em diferentes domínios de aplicação, considerando diferentes necessidades das aplicações e dos ambientes de desenvolvimento.

Por fim, foram apresentadas questões específicas de desenvolvimento de programas multithread em processadores multicore.

Em particular, foram caracterizados aspectos relacionados à exploração eficiente dos recursos de processamento disponíveis e estratégias de programação para reduzir contenção de execução dos threads.

O Fórum de Pós-Graduação da ERAD é um espaço para trabalhos de alunos de mestrado e doutorado na área de processamento de alto desempenho.

O objetivo do Fórum é divulgar pesquisas em andamento e resultados obtidos, bem como incentivar a troca de experiências entre membros da comunidade.

Para isso, o Fórum propicia aos alunos de pós-graduação uma oportunidade de apresentação de seus trabalhos, possibilitando que recebam sugestões para o aperfeiçoamento dos mesmos.

Além disso, busca-se com o Fórum proporcionar um espaço para discussões sobre a pós-graduação na área e para a integração da comunidade acadêmica que participa da ERAD.

Esta é a sexta edição do Fórum de Pós-Graduação, o qual vem consolidando-se desde sua primeira edição em 2004.

Para esta edição, 19 trabalhos foram submetidos e aceitos para publicação e apresentação no Fórum, representando os programas de pós-graduação do Rio Grande do Sul.

Todos os resumos foram avaliados por no mínimo 3 revisores, que registraram comentários aos autores sugerindo correções e formas de aprimorar o trabalho.

Esperamos que o Fórum seja proveitoso para todos os seus participantes e agradecemos a todas as pessoas envolvidas na realização de mais uma edição da ERAD.

Em especial, registramos aqui nossos agradecimentos aos integrantes da comissão organizadora, aos membros do comitê de programa, aos avaliadores dos artigos e, principalmente, aos autores dos trabalhos.

As modelagens numéricas de previsão do tempo e clima demandam grandes recursos computacionais a fim de resolver os sistemas mais rapidamente ou com maior precisão e refinamento.

O Centro de Previsão de Tempo e Estudos Climáticos (CPTEC) utiliza atualmente o Modelo de Circulação Geral Atmosférico (MCGA), também conhecido por modelo Global para previsão do tempo e clima.

Este modelo utiliza modelagens numéricas descritas por equações espectrais e baseia-se no modelo COLA (Center for Ocean, Land and Atmosphere Studies), desenvolvido no NOAA (National Oceanic and Atmospheric Administration).

Com as atuais inovações tecnológicas, milhões de transistores tornaram-se disponíveis dentro de um único chip, e com as implicações físicas das novas tecnologias de integração o uso de arquiteturas multi-core tem aumentado expressivamente nos últimos anos em processadores de propósito geral.

Dessa maneira, programas antes planejados para executarem em máquinas paralelas se deparam com mais um nível de paralelismo no nível de fluxo de instruções dentro do chip, tornando necessária a utilização de novas técnicas de expressão de paralelismo e de comunicação entre os diversos fluxos paralelos.

Neste contexto, outros trabalhos já avaliaram formas de melhorar o desempenho do MCGA em arquiteturas paralelas vetoriais.

O presente trabalho está inserido e contextualizado em um projeto de pesquisa que tem o objetivo de estudar os gargalos apresentados e melhorar as expressões de paralelismo nas novas arquiteturas multi-core para modelos climáticos.

Este texto se refere a resultados iniciais sobre a escalabilidade do modelo Global do CPTEC com relação ao paralelismo em novas arquiteturas multi-core.

O modelo Global (MCGA), originalmente foi escrito utilizando implementação em MPI (Message Passing Interface) para descrição de paralelismo.

Neste trabalho sobre a escalabilidade do modelo Global em multi-core, foi utilizada metodologia de medição, onde foram executados experimentos em máquinas paralelas e comparado o desempenho de cada execução, a fim de obter maior precisão nas medições o código foi instrumentado para distinguir a inicialização da integração utilizando a ferramenta PAPI (Performance Application Programming Interface).

O experimento consiste na execução paralela do modelo Global com 1 a 8 processos MPI paralelos, utilizando máquinas distintas (nodos distintos) de um mesmo cluster, onde foi utilizado apenas um núcleo de processamento por nó e a comunicação entre nodos distintos é feita através de uma rede Gigabit Ethernet.

Comparando com a execução em apenas um nó de processamento, também variando de 1 a 8 o número de processos MPI (1 a 8 núcleos utilizados).

Cada nó de processamento é composto de dois processadores Xeon Quad-Core E5310 de 16 GHz (total de 8 núcleos de processamento divididos entre dois processadores) com fatias de memória cache L2 de 4 MB compartilhadas para cada 2 núcleos de processamento e um total de 8 GB de memória principal.

O modelo Global foi compilado para o experimento utilizando o compilador Intel versão 101 e o MPI provido pelo Mpich 1 2 7 p1 utilizando driver de passagem de mensagem por socket TCP/IP nos dois experimentos.

Uma vez que o modelo é parametrizável, foi definido um caso de estudos para ser modelado e executado definindo assim condições iguais de comparação para todos os experimentos.

O problema modelado refere-se ao furacão Catarina utilizando uma grade com 210 km de resolução espacial.

Após a execução dos experimentos base, foi observado que a execução da aplicação não apresentou mesma aceleração nos dois graus de paralelismo adotados.

Apresenta os resultados de tempo de execução, aceleração da aplicação, MFLOP/s e porcentagem de MFLOP/s sobre o limite teórico considerado de 4 FLOP/ciclo, nos dois modelos variando o número de processos, onde se pode ver claramente o ganho do uso dos múltiplos nodos sobre o uso de apenas um nodo multiprocessado.

Resultados do experimento de escalabilidade do modelo Global.

Considerando os resultados apresentados, fica clara a importância de estudos sobre a escalabilidade do modelo Global em paralelismo de múltiplos níveis.

Dentre as próximas atividades está prevista a avaliação mais detalhada da queda de desempenho utilizando a ferramenta VTune da Intel, analisando parâmetros como faltas de dados na memória cache, contenções na interconexão, contenção na memória, uso das unidades funcionais dentre outros.

Espera-se encontrar formas de melhorar tanto a escalabilidade do modelo Global quanto as arquiteturas dos futuros processadores multi-core.

Em ambientes de computação ubíqua (UbiComp) o mecanismo de descoberta de recursos é essencial para que eles possam ser encontrados na sua organização complexa e distribuída.

Os recursos podem ser servidores, impressoras, storages, softwares, etc.

Os estudos realizados até agora no desenvolvimento deste trabalho, apontam que a representação desses recursos pelos mecanismos de descoberta se mostra uma questão prioritária, para facilitar a pesquisa e o compartilhamento por diferentes mecanismos de descoberta de recursos.

O estudo de trabalhos relacionados indicou que muitos mecanismos de descoberta de recursos organizam a descrição dos recursos que gerenciam em linguagem XML (eXtensive Markup Language) e realizam pesquisas baseadas em comparações de strings e números.

Dessa forma, esses mecanismos não reconhecem equivalências entre recursos, e nem conseguem inferir relações sobre eles.

Também não possuem uma semântica definida, apenas uma organização sintática e estrutural.

Nessa perspectiva a expressividade na descrição de recursos e critérios de pesquisa dos mecanismos de descoberta de recursos é limitada, não permitindo descrever e pesquisar as características dos recursos adequadamente.

Em vista disso, torna-se necessário uma representação desses recursos de forma concisa e não ambígua para uma maior abrangência nos resultados de pesquisa dos mecanismos de descoberta de recursos.

A utilização de um padrão sintático e semântico decorrente do uso de tecnologias de web semântica, dentre estas, o emprego de ontologias, aumenta a expressividade na representação dos recursos e facilita a interoperabilidade entre os serviços de descoberta de recursos.

Este trabalho se encontra em fase inicial, tendo sido desenvolvido no primeiro semestre do PPGINF/UCPel, tendo como principais objetivos uma revisão estruturada dos trabalhos relacionados à área, bem como a concepção de um mecanismo de descoberta de recursos, do middleware EXEHDA, baseado nas tecnologias de web semântica.

Está sendo construída uma ontologia para o serviço de descoberta de recursos utilizando-se como modelo a especificação da FIPA Device Ontology.

O aumento da expressividade do serviço de descoberta de recursos é alcançado através do processamento da ontologia proposta.

A ontologia proposta contém quatro classes principais que são, Dispositivos, Softwares, Locais e Usuários.

Esta ontologia está sendo construída prevendo também sua utilização pelos mecanismos de sensibilidade de contexto e adaptação de contexto do middleware.

Alguns relacionamentos da ontologia já estão definidos, como por exemplo, um dispositivo pode conter uma ou mais instâncias de softwares.

Um dispositivo pode ser instanciado, por exemplo, por uma impressora que pode possuir um serviço de impressão e um serviço de FAX associado a ela.

Essa ontologia possui a classe "Softwares" com as sub-classes "Aplicaoões" e "Sistemas Operacionais".

Se um cliente solicitar uma pesquisa sobre nodos com sistemas operacionais "Unix", atravps de uma consulta na ontologia será possível verificar se há alguma instância da Classe "Unix" disponível.

Caso não esteja disponível será possível inferir que "Linux" p um sub-conceito de "Unix" e disponibilizar instâncias de "Linux" para o cliente.

Acredita-se que um mecanismo de descoberta de recursos baseado em ontologias para o EXEHDA possibilitará respostas mais completas, potencializando a descoberta de recursos dispersos e heterogêneos do ambiente ubíquo.

Na continuidade deste trabalho está previsto consolidar a arquitetura de software do modelo, bem como concluir os relacionamentos necessários entre as classes de recursos para possibilitar a derivação de conhecimento na ontologia proposta.

Para processamento de ontologias existem testes iniciais que apontam para a utilização de tecnologias como a API Jena para processar a ontologia e integrar com o mecanismo de descoberta.

O mecanismo proposto constituirá uma alternativa para o PerDis, atual mecanismo de descoberta de recursos do EXEHDA, o qual não processa ontologias.

O principal problema na programação paralela é decompor um problema inicial em sub-problemas de tal forma que executem em paralelo.

Problemas estáticos podem prever sua carga de trabalho e utilizam a decomposição por dados.

Por outro lado, alguns problemas desconhecem sua carga antes da execução devido as características do algoritmo ou dà arquitetura.

Uma decomposição recursiva pode solucionar esta questão ao dividir recursivamente um problema inicial em menores até que sua carga seja conhecida.

Esta divisão pode atribuir os subproblemas para tarefas criadas dinamicamente em tempo de execução.

Um fator importante para o desempenho de aplicações paralelas é a granularidade, que representa o grau de agrupamento de tarefas em um determinado processador.

Entretanto, um controle de granularidade em aplicações dinâmicas depende do ambiente de programação devido ao grafo de dependências entre as tarefas ser desconhecido antes da execução.

Alguns ambientes de programação como Cilk e KAAPI propõem um controle de granularidade com tarefas abstratas.

Todavia, estes ambientes possuem limitações de arquitetura e complexidade de programação.

O padrão MPI possibilita dinamismo na criação de processos, mas as implementações não dispõem de um controle de granularidade.

Este trabalho propõe uma biblioteca (libSpawn) que incorpora um controle de granularidade no MPI através de tarefas, que podem ser processos ou threads.

A implementação consiste na alteração da chamada Spawn para a escolha entre processos ou threads.

A granularidade é definida em tempo de execução através de parâmetros que avaliam o contexto de execução, cores da arquitetura, carga de sistema e recursos do sistema operacional disponíveis.

Estes parâmetros são avaliados localmente para cada processo e proporcionam distribuir a carga de trabalho para outros nós disponíveis.

Neste trabalho, nenhum tipo de controle distribúido ou centralizado de tarefas é utilizado.

A libSpawn implementa a troca de mensagens de acordo com o padrão MPI de forma que mensagens entre processos usem chamadas MPI e mensagens entre threads usem transferências em memória compartilhada.

A biblioteca manipula a comunicação entre threads através de procedimentos em memória compartilhada com exclusão mútua.

A descrição de uma mensagem é recriada para incluir além do message envelope três campos de controle.

As mensagens são armazenadas em duas listas para cada Send e Receive onde uma thread busca mensagens correspondentes para depois completa-las.

Dessa forma, esta implementação elimina impasses entre threads e proporciona comunicações eficientes.

O experimento consiste em uma ordenação Mergesort chamado Cilksort que acompanha o Cilk.

Essa ordenação decompõe sua entrada com a técnica de Divisão-e-Conquista (D&C) onde cada divisão resulta em tarefas criadas dinamicamente.

O gráfico mostra os tempos obtidos na execução com processos e com o controle proposto.

A entrada é de 2 milhões de elementos, em ordem aleatória, que geram 10813 tarefas.

As execuções foram realizadas em 30 nós da grade francesa Grid'5000 com 2 Intel Xeon Dual Core 233 GHz, em um total de 120 cores, e rede Gigabit Ethernet.

As médias e desvio padrão foram obtidas a partir de 20 execuções independentes.

As execuções começam com resultados acima de 32 cores devido as limitações na implementação MPICH2 que não permitem execuções com um número menor de cores para a entrada utilizada.

Os resultados obtidos demostram ganhos de 27% (120 cores) até 57% (24 cores) com relação ao uso de processos e sem controle de granularidade.

Este ganho se deve à criação de tarefas como threads e as comunicações em memória compartilhada entrè threads.

Como trabalhos futuros, um mecanismo para a gerência do número de processos a serem criados será avaliado com o intuito de maximar o ganho de desempenho.

Além disso, a inclusão de um escalonador em 2 níveis será estudado para melhorar a distribuição de carga e, por conseguinte, aumentar a eficiência.

MPI (Message Passing Interface) é a especificação padrão da indústria para o desenvolvimento de aplicações de alto-desempenho com comunicação via troca de mensagens prin-cipalmente em ambientes com memória distribúida.

Por se tratar de uma especificação de biblioteca existem diferentes implementações, porém a norma somente especifica interfaces para as linguagens de programação C, Fortran e C++ fazendo com que somente essas linguagens possuam implementações de bibliotecas MPI fortemente suportadas.

Por um lado as linguagens de programação evolúiram para linguagens de altonível orientadas a objetos como Java e C# permitindo a execução de programas em ambientes heterogêneos.

Por outro lado a especificação MPI evoluiu para MPI-2 oferecendo suporte a criação dinâmica de tarefas, E/S paralela e operações remotas de memória.

Tendo em vista que MPI é um padrão muito popular no contexto de HPC (High Performance Computing) logo surgiram projetos visando integrar MPI com as novas linguagens de programação.

Entre esses diversos projetos podemos destacar Java (JavaMPI, mpiJava, PJMPI, MPJava), C# (MPI NET, Pure MPI NET), Python (pympi), Ruby (ruby-mpi).

A falta de um ambiente de programação de alto-nível baseado em MPI que ofereça criação dinâmica de processos e alto desempenho motiva a pesquisa sobre projetos que permitam a utilização de MPI nesses ambientes alto-nível, sendo que o projeto MPI NET demonstrou bons resultados de desempenho, bem como uma boa interface de programação, deixando a desejar no quesito de criação dinâmica de tarefas.

Portanto, a forte motivação deste trabalho está em explorar as possibilidades de criação dinâmica de tarefas MPI dentro do ambiente de programação alto-nível provido pela biblioteca MPI NET.

Durante o estudo da biblioteca MPI NET foi constatado que para implementar as funções de criação dinâmica de tarefas será primeiramente necessário substituir a biblioteca MPI-C a qual a biblioteca MPI NET utiliza em sua camada mais baixa para realizar as chamadas MPI devido a essa biblioteca não oferecer suporte a criação dinâmica de tarefas.

O ciclo de desenvolvimento deste trabalho será baseado primeiramente no estudo das possibilidades de tornar a biblioteca MPI NET capaz de executar em ambientes heterogêneos.

Após a biblioteca permitir execução de processos MPI em ambientes heterogêneos, serão então estudadas as possibilidades de adicionar funções para criação dinâmica de tarefas e funções para gerenciamento desses processos criados dinâmicamente.

A biblioteca será validada através da utilização de aplicações que criem processos dinâmicamente, como por exemplo aplicações baseadas no modelo de Divisão e Conquista.

Por fim a biblioteca terá seu desempenho testado ao lado de uma biblioteca MPI-C para verificar se o overhead de estar utilizando uma linguagem de programação com maiores níveis de abstração não irá ser muito impactante nos resultados de desempenho.

Segundo o desempenho da biblioteca se mantém muito similar ao da linguagem C.

Como resultado final espera-se uma biblioteca que possua uma interface de programação com alto-nível de abstração a qual possua bom desempenho e permita a programação intuitiva de programas MPI baseados em criação dinâmica de tarefas pelos programadores C#, e que esses programas possam ser executados sobre ambientes heterogêneos.

A biblioteca terá seu código livre permitindo que outras funcionalidades possam ser agregadas.

Aglomerados de computadores (clusters) tornaram-se alternativa prática e cada vez mais comum em PAD, visto que podem ser constrúidos através da composição em redes de hardware menos especializado.

Mesmo que avanços na construção de processadores de propósito geral possam trazer melhoria no desempenho de execuções seqüenciais, é necessário suporte da arquitetura de cluster para o uso otimo dessas melhorias.

Graças as tecnologias Simultaneous Multiprocessors (SMP) e Simultaneous Multithread (SMT) tem-se (em conformidade com ) multicomputadores MIMD (clusters) de multiprocessadores MIMD (chip multicore).

A teoria clássica de escalonamento trata da alocação de tarefas em recursos (1 nível).

O cenário atual, todavia, evidencia a necessidade de escalonamento multinível.

As setas significam "é escalonado em".

O tamanho dos blocos é proporcional à granularidade de um componente em relação ao proximo.

Todos os relacionamentos sao do tipo 1,1.

Não há na literatura algum padrão para comunicação e escalonamento em clusters que faça escalonamento dinâmico multinível e seja largamente utilizado, hoje, MPI é o padrão de facto para comunicação em clusters.

MPI-1 centrava-se na comunicação entre processos, que se dá por troca de mensagens.

MPI-2 é uma extensão da primeira norma que oferece uma série de facilidades para o programador, em especial a criação dinamica de tarefas.

No entanto, não há especificação de como o escalonamento destes processos deve ser feito em relação aos recursos disponíveis, tampouco uma correspondência explícita entre a noção de tarefa e a noção de processo.

Não há restrição de dependência de dados.

Embora MPI suporte a utilização de threads pelo usúario, não provê suporte próprio ao seu uso.

No quesito de granularidade da tarefa (threads vs processos), a LibSpawn oferece um mapeamento transparente de processos MPI-2 em threads POSIX, ao invés do tradicional mapeamento em processos do Sistema Operacional, sempre que o número de threads em um dado recurso superar uma constante M (definida em tempo de compilação), devido à criação dinâmica de uma dada tarefa, um novo processo de Sistema Operacional é alocado em um novo recurso.

Apoiando-se em um padrão amplamente difundido, ainda falta à LibSpawn a capacidade de escalonar eficientemente as tarefas de acordo com a carga de trabalho dos processadores participantes da computação.

Não temos conhecimento de alguma implementação do padrão MPI-2 capaz de fornecer um escalonamento multinível, distribúido, adaptável e eficiente de tarefas dinamicamente criadas e de maneira transparente ao programador.

Este trabalho visa preencher essa lacuna, fornencendo um escalonador com balanceamento de carga em dois níveis (threads e processos) para a LibSpawn.

Esse trabalho está em seu estágio inicial e a descrição apresentada é fruto de uma Proposta de Estudos e Pesquisa para Mestrado em Ciência da Computação.

Este trabalho, tem como objetivo central propor um modelo para a coordenação de aplicações na Computação Pervasiva, estando prevista sua implementação como um serviço para o middleware EXEHDA.

O middleware EXEHDA, em desenvolvimento no G3 PD/UCPEL, consiste em uma arquitetura para um ambiente de grade computacional direcionado as aplicações distribúidas, móveis e conscientes do contexto da Computação Pervasiva.

Maiores detalhes sobre sua arquitetura e funcionalidades podem ser obtidas no documento.

As motivações de pesquisa centrais deste trabalho envolvem os diversos desafios de coordenação gerados pela mobilidade lógica e física suportadas pelo EXEHDA.

A coordenação, em computação distribúida, consiste de um sistema que manipula a comunicação e cooperação entre os diferentes processos envolvidos.

Modelos baseados em coordenação caracterizam-se pela separação entre computação e coordenação.

Um dos modelos de coordenação mais difundidos é o de Espaço de Tuplas (TS) que foi introduzido por Linda.

Este modelo consiste de uma memória associativa independente, compartilhada entre todos os nodos do sistema.

Esta memória pode ser considerada como uma espécie de repositório para as estruturas denominadas tuplas.

A comunicação entre processos se dá através da inserção e leitura destas tuplas neste espaço compartilhado.

Os modelos de coordenação tradicionais apresentam limitações de utilização considerando as demandas da Computação Pervasiva.

Por isso, quando da concepção do EXEHDA, foi introduzida a perspectiva de uso de um TS distribúido com atuação proativa, isto e, com a possibilidade de notificação aos componentes de software associados as diferentes tuplas.

O EXEHDA-TS tem por foco tornar substantiva esta premissa, e se propõem a modelar os diversos aspectos inerentes a sua operação de modo a atender as demandas da Computação Pervasiva.

Nos estudos realizados durante a concepção do EXEHDA-TS foram identificados alguns trabalhos relacionados, como Lime e Tota, entre outros.

No EXEHDA-TS, os TS são associados aos Objetos EXEHDA (OX).

Conforme o OX se estabelece em um EXEHDAnodo específico, seu TS pode ser estendido de forma a ser acessível as demais aplicações, formando um TS unico.

Esta estratégia proporcionà um tratamento as desconexões.

Por questões de desempenho, o TS não pode prescindir de uma area de abrangência limitada.

Reforçando a indicação para a pesquisa em andamento que o EXEHDA-TS tenha abrangência em nível celular, considerando o ambiente pervasivo previsto pelo EXEHDA.

Para promover uma melhor escalabilidade, o EXEHDA-TS prevê o suporte a tratamento de eventos, que vem também a contribuir na redução dos esforços computacionais, porque libera as aplicações do onus de monitorarem o TS por conta própria.

Como estratégia para coordenar aplicações com componentes de software localizados em células distintas, o EXEHDA-TS prevê a incorporação sob demanda, de espaços de tuplas de OX externos a célula.

O EXEHDA-TS prevê a possibilidade de organizar tuplas com características semelhantes em espaços de tuplas particulares.

Esta organização é determinada por regras associadas aos espaços de tuplas de cada OX.

Estas regras podem ser utilizadas, por exemplo, para separar tuplas com informações de contexto das demais.

O EXEHDA-TS é um modelo de Espaço de Tuplas focado na coordenação de aplicações da Computação Pervasiva.

O trabalho está em fase de modelagem e pretende ser implementado como um serviço para o EXEHDA.

Está previsto o desenvolvimento de um estudo de caso contemplando aplicações em medicina pervasiva.

Modelos meteorológicos são uma importante ferramenta cuja aplicação se estende a diversas áreas como agricultura, defesa civil, controle de tráfego aéreo e outros setores econômicos e ambientais importantes.

Tipicamente, esses modelos são compostos por um conjunto de equações que representam o estado da atmosfera.

Essas equações são discretizadas e resolvidas em um processo iterativo.

Um aspecto importante desse tipo de aplicação é a resolução.

Resolução corresponde a qualidade do resultado produzido em uma previsão meteorológica.

Muitas aplicações exigem resoluções que normalmente não podem ser resolvidas em computadores seqüenciais em tempo satisfatório.

Portanto, para diminuir o tempo de execução, computadores paralelos são empregados.

Uma alternativa bastante comum de implementação de modelos meteorológicos (e aplicações paralelas em geral) se baseia no padrão MPI (Message Passing Interface).

Na maioria dos casos, é empregada uma decomposição espacial de domínio em que os processadores trocam mensagens contendo as bordas (ghostzones) entre si.

O objetivo deste trabalho é analisar a aplicação de uma otimização que corresponde a seleção de que subdomínios devem ser executados em que processador e em que núcleo dos processadores multicore que compõem os nós do cluster.

Essa abordagem corresponde ao problema de mapeamento de grafos minimizando os custos de comunicação, sendo esse um problema NPcompleto.

Neste trabalho, são mostradas características da técnica de mapeamento de processos aplicada a um modelo meteorológico em clusters com máquinas multicore visando melhorar o desempenho.

Para o desenvolvimento do presente trabalho, utilizamos o modelo regional BRAMS (Brazilian Regional Atmospheric Modeling System), cuja decomposição montada é apresentada.

O padrão de comunicação deste experimento foi extraído através de uma instrumentação da biblioteca MPI.

Através dos dados obtidos, é possível montar um grafo de comunicação a ser utilizado no processo de obtenção de um bom mapeamento de domínios para processadores.

Em clusters onde os nós são computadores SMPs (Symmetric Multiprocessors) ou baseados em processadores multicore, o tempo de comunicação entre processos varia muito de acordo com o processador para onde eles foram mapeados.

Sendo assim, montase um grafo que representa a interconexão dos processadores para realizar o mapeamento entre os grafos.

Tendo obtidos a estrutura de comunicação entre os processos e as características da interconexão entre os processadores sob a forma de grafos, o mapeamento pode ser tratado como o problema de mapear um grafo em outro.

Para obter um mapeamento próximo do ótimo, utilizouse o método Dual Recursive Bipartitioning.

Muitos programas de modelos meteorológicos são rodados várias vezes, com várias condições iniciais e de contorno, entretanto, apresentando o mesmo padrão de comunicação.

A aplicação de técnicas de mapeamento para esse caso se mostra útil pois podese analisar a comunicação em uma rodada e melhorar consideravelmente o desempenho das próximas execuções.

Técnicas de mapeamento de processos são muito importantes em clusters e a construção de modelos para tal utilizando MPI é um processo intuitivo.

Apesar de essas técnicas já terem se mostrado eficientes em outros programas, aqui foi explorado um programa paralelo de modelo meteorológico que apresenta características desejáveis para a aplicação da técnica e a utilização da técnica em clusters de multicores, sendo obtidos ganhos de desempenho.

Migração de processos é um pertinente mecanismo para oferecer balanceamento dinâmico de carga, principalmente em ambientes dinâmicos e heterogêneos.

Nesse contexto, foi desenvolvido um modelo chamado MigBSP que controla o reescalonamento de processos em aplicações BSP (Bulk Synchronous Parallel).

Uma aplicação BSP é dividida em um ou mais superpassos, cada qual contendo ambas fases de computação e comunicação seguidas por uma barreira de sincronização.

Uma vez que a barreira espera pelo processo mais lento, a idéia final de MigBSP é ajustar a localização dos processos com o intuito de reduzir o tempos dos superpassos.

Considerando o escopo de aplicações BSP, as novas idéias de MigBSP são, combinação de três métricas em outra escalar chamada Potencial de Migração.

Emprego de padrões de computação e comunicação para controlar a regularidade de cada processo.

Adaptação eficiente quanto a periodicidade de lançamento do reescalonamento de processos BSP.

O Potencial de Migração (PM) indica quais são os processos candidatos para a migração e pode ser encontrado através da Equação 1.

Cada processo i computa n funções PM(i,j), onde n é o número de Conjuntos e j significa um Conjunto em especial.

Um Conjunto é a abstração de uma rede e pode ser entendido como sendo um cluster, uma rede local ou um supercomputador.

Cada um possui um gerente responsável pelo escalonamento hierárquico.

A estratégia do cálculo do PM é não fazer todos os testes processos recursos possíveis no momento do reescalonamento.

PM(i,j) é encontrado através da combinação das métricas Computação, Comunicação e Memória.

A relação entre elas é baseada na noção de força da física.

As métricas Computação e Comunicação atuam como forças a favor da migração, enquando a Memória trabalha em sentido contrário e adiciona ao modelo uma idéia de custo.

Quanto maior o valor de PM(i,j), maior é a tendência de migração do processo.

Um alto PM(i,j) significa que o processo i possui um alto tempo de computação, bastante comunicação com processos que pertencem ao Conjunto j e um baixo custo de migração para o Conjunto destino.

A métrica Computação Comp(i,j) considera três informações, predição do tempo de computação do processo i.

Padrão de Computação do processo i.

Grau de desempenho do Conjunto j.

A predição é baseada no conceito de Aging e usa dados entre duas chamadas para reescalonamento de processos.

O Padrão de Computação mede a regularidade do processo quanto as instruções que realiza em cada superpasso.

O seu valor é próximo de 1 se o processo é regular (considerando um intervalo de erro) e perto de 0, caso contrário.

O tratamento do escalonamento usando padrões foi empregado pois espera-se que o processo desempenhe um comportamento similar no lugar de destino.

Assim como a métrica Computação, a Comunicação Comm(i,j) computa o Padrão de Comunicação entre processos e Conjuntos.

A cada superpasso, esse padrão considera a quantidade de dados recebidos pelo processo i provenientes de envios de processos do Conjunto j.

Além disso, essa métrica usa predição do tempo de comunicação considerando dados entre duas ativações de balanceamento de carga.

A métrica Memória Mem(i,j) trabalha com os seguintes dados, (i) memória do processo.

Taxa de transferência entre o processo i e o gerente do Conjunto alvo j.

Custos de migração.

A decisão de remapeamento de processos é tomada no final de uma superpasso (depois da barreira de sincronização e antes do próximo superpasso).

Esse ponto de migração foi escolhido porque neste momento é possível analisar dados de todos os processos BSP nas suas fases de computação e comunicação.

Contrário a biblioteca PUBWCL, MigBSP não realiza o reescalonamento a cada superpasso mas sim de acordo com o estado do ambiente.

Para isso, foram aplicadas duas adaptações que gerenciam o valor de é atualizado a cada chamada para reescalonamento e irá indicar o próximo intervalo para tal.

Na Equação 1, a métrica Memória é dividida por pois assume-se que os processos vão executar em média mais superpassos após a 2 2 chamada para reescalonamento.

Para gravar as variações no estado do sistema entre duas chamadas para migração, uma varíavel temporária chamada 0 foi usada.

Ela é atualizada a cada superpasso através do incremento ou decremento de uma unidade e é completada com o valor de 0 no momento da chamada de reescalonamento.

As idéias gerais das adaptações são, atrasar a chamada para rebalanceamento de carga se o sistema estiver estável (processos estão balanceados entre os recursos) ou torná-la mais frequente, caso contrário.

Atrasar a chamada de reescalonamento se o sistema apresentar um padrão sem migrações nas ultimas chamadas de reescalonamento.

A segunda adaptação é importante para cenários onde os processos estão desbalanceados e migrações são invíaveis.

Os resultados do desempenho de MigBSP.

Eles mostraram a viabilidade de uso do modelo, o qual impõe uma baixa sobrecarga na execução normal da aplicação quando migrações não acontecem (custo de 5%).

Além disso, ganhos acima de 20% foram obtidos com migrações no ambiente multi-cluster testado.

MigBSP oferece uma definição e uma relação precisa de informações para rescalonamento de processos.

O trabalho de unificação de métricas de escalonamento para os cenários de sistemas distribúidos complexos é visto com um grande desafio para a area.

A computação pervasiva teve sua primeira definição através de Mark Weiser no final dos anos 80 quando publicou seu artigo sobre "O Computador do Século 21".

Em meados de 1990, a computação começou a introduzir o uso de dispositivos portáteis como (PDAs) e, mais recentemente, os acessos a recursos computacionais via telefones celulares e notebooks, entre outros dispositivos eletrônicos.

Esteambiente computacional pervasivo consiste de uma grande variedade de nodos de diversos tipos, móveis e fixos, aplicações e serviços interconectados.

Neste caso, computação pervasiva também é sinônimo de conectividade pervasiva, e reflete uma computação altamente dinâmica e distribuída.

Hoje, a concentração das pesquisas está em disponibilizar tecnologias chave, para suportar este novo ambiente computacional.

Dentro dessas necessidades este trabalho visa apresentar um modelo gerenciador de descoberta de serviços pervasivos (MgDsP) para utilização em aplicações móveis, sensíveis ao contexto.

Alguns trabalhos foram avaliados para a modelagem desse modelo, SOCAM, AROUND, protocolos foram comparados e os estudos foram baseados nas especificações de.

Na avaliação desses trabalhos, verificou-se que as arquiteturas são voltadas para o ambiente em que ocorre a aplicação e delimitam-se somente a restrições aplicadas ao ambiente e aos usuários específicos que interagem localmente.

Por isso, essas arquiteturas, são restritas na situação do contexto e na funcionalidade do dispositivo que interage na aplicação, ou na descoberta de serviços ou recursos.

O Middleware tem sua construção feita com base em requisitos mínimos, que foram descritos através da arquitetura desenvolvida por, bem como a avaliação e integração do modelo proposto também atenderão os mesmos requisitos.

A proposta desse trabalho visa também estudar o ambiente, os seus dispositivos e suas contribuições que estes podem proporcionar para o ambiente, usuário e situação na aplicação pervasiva.

Seguindo o fato que os dispositivos são voltados para uma determinada funcionalidade, sendo para uso genérico e em diversos tipos de dispositivos, são realizados estudos sobre o tratamento dos dispositivos para a situação das aplicações sensíveis ao contexto no ambiente pervasivo onde ocorre à situação e das necessidades dos usuários.

Por isso, o modelo definido deve ser flexível, tanto para o tratamento do ambiente e do usuário, quanto das funcionalidades dos dispositivos e da situação que ocorre no ambiente.

Os componentes do middleware estão sendo implementados em Java, entre outras características, a principal deve-se a portabilidade.

A principal contribuição do middleware é fornecer as seguintes funcionalidades.
(DI) Dispositivos, é realizada a detecção do dispositivo.
(DS) Descoberta Serviços, onde é realizada a verificação de dispositivos.
(SC) Servidor de Contexto, onde estarão armazenadas informações contextuais do usuário e seus dispositivos.
(TS) Tabela serviços, onde são armazenados localmente os serviços baseados no contexto de cada usuário e seu dispositivo relacionado ao usuário, estas informações estão disponíveis localmente.
(CR) Componente de recursos, gerencia os serviços disponíveis localmente e os transmite.
(WS) Web Services, este componente é responsável por prover comunicação e troca de dados e informações entre diferentes redes e formas de comunicação com a rede pervasiva local.

Algumas dessas tarefas podem ser melhor aproveitadas com uso de serviços de outros usuários, através de seus dispositivos, fazendo surgir assim, a necessidade da descoberta de recursos e serviços no contexto pervasivo.

Através dessa necessidade esse trabalho propõe um modelo gerenciador de serviços para plataformas pervasivas sensíveis ao contexto.

A deduplicação tem por objetivo identificar e combinar registros que representam a mesma entidade no mundo real, melhorando, assim, a qualidade dos dados e a integração.

No entanto, esta tarefa se torna muito difícil quando as bases de dados são extremamente grandes e complexas.

Para reduzir o custo computacional e de recursos humanos diversos trabalhos estão sendo realizados com objetivo de estender técnicas de deduplicação para arquitetura paralela.

Desse modo, tarefas que antes eram realizadas em dias podem ser executas em algumas horas, com alta eficiência.

Entretanto, o tratamento de algoritmos paralelos implica em algumas questões que precisam ser solucionadas.

A perda de desempenho acarretada pela baixa velocidade da comunicação é um dos principais desafios, podendo determinar o bom funcionamento da técnica.

Neste contexto, uma solução para reduzir este problema é a blocagem, na qual registros semelhantes são inseridos em um mesmo grupo, restringindo o número de possíveis réplicas para elementos de um mesmo bloco.

A blocagem possibilita, portanto, que a base de dados seja fragmentada em blocos independentes e distribuída entre processos, sem a necessidade de uma exaustiva troca de mensagens.

Sendo proposto, por este artigo, uma nova técnica de blocagem paralela chamada P-Canopy, como uma extensão da técnica de blocagem Canopy.

O objetivo principal é melhorar a interdependência e, conseqüentemente, aperfeiçoar o desempenho das técnicas de deduplicação paralela.

Inúmeras abordagens são discutidas para implementar técnicas de blocagem, mas poucas são eficientes para grandes bases de dados e, são desconhecidos trabalhos que abordem a blocagem paralela.

Esta seção apresenta a técnica de blocagem Canopy, cujos conceitos foram utilizados para definição da proposta de blocagem paralela, descrita na próxima seção.

A técnica de blocagem Canopy é utilizada para grandes e multidimensionais conjuntos de dados devido à obtenção de bons resultados com custo computacional reduzido.

A melhora de eficiência é obtida através da divisão em duas fases, agrupamento dos conjuntos utilizando um método de medida de similaridade aproximada, com baixo custo computacional, e a utilização de uma técnica precisa com alto custo computacional sobre os subconjuntos gerados.

Os subconjuntos criados na primeira fase do método são chamados de Canopies.

Um Canopy é simplesmente um agrupamento de elementos formado de acordo com uma medida de distância aproximada, dentro de um limiar mínimo estabelecido, como um índice invertido.

Cada elemento pode pertencer a mais de um bloco.

Os Canopies seguem a propriedade de que pontos que não estão em um mesmo Canopy são tão distantes que não podem ser possível estar em um mesmo bloco.

Como a distância usada para criar os grupos é aproximada, não se tem como garantir esta propriedade.

Mas como os Canopies podem se sobrepostos através da escolha de um limiar suficientemente grande, na grande maioria dos casos, esta propriedade pode ser garantida.

A técnica Canopy é utilizada de forma seqüencial.

Este artigo propõe a extensão para o modelo paralelo, chamada de P-Canopy.

Usufruindo das características de baixo custo computacional, independência na geração dos blocos e eficiência, a técnica original é adaptada para obter um bom desempenho com a extensão para o ambiente multiprocessado.

Seguindo o modelo seqüencial a primeira fase é responsável pela divisão da base de dados.

Como o maior esforço computacional está concentrando na segunda fase, com o cálculo da similaridade entre cada par de registros, a primeira etapa será executada de forma centralizada.

A etapa tem como saída blocos independentes com uma precisão aproximada.

Na segunda fase, cada bloco é enviado para um processo para a execução de uma técnica mais precisa, como por exemplo K-means ou Greedy Aglomerative Clustering (GAC), de forma paralela.

Como cada processo que executa a segunda etapa possui uma porção independente de dados não é necessário uma interação entre os blocos para troca de registros, restringindo a comunicação apenas para troca de informações entre a fase centralizada e distribuída.

Este trabalho propõe uma nova abordagem para o algoritmo de blocagem Canopy.

Originalmente tem como característica a divisão da base de dados em duas fases, possibilitando englobar duas características primordiais em técnicas de blocagem que são, eficiência e precisão.

A técnica P-Canopy é proposta visando estender estas características para uma arquitetura paralela.

Objetivando melhorar a independência de dados, minimizando o overhead da comunicação que são fatores de vital influência para degradação de sistemas paralelos.

Como trabalho futuro será analisada em detalhes a proposta de blocagem paralela proposta por este artigo, para uma futura validação.

Além disso, serão implementados alguns algoritmos de precisão, como K-means e GAC, para comparar seus desempenhos.

Posteriormente, pretende-se trabalhar no desenvolvimento de outras abordagens para blocagem paralela.

Neste trabalho, propõe-se o p-iCone, paralelização do software iCone.

A abordagem sequencial do iCone foi desenvolvido na etapa inicial deste projeto e provê uma solução computacional para cálculo do volume de relevos não uniformes, representando produtos em armazéns.

Para esta tarefa não existem equipamentos e softwares específicos satisfazem corretamente as necessidades dos usúarios (cálculo do volume com precisão e baixo tempo de resposta, tendendo a execução em tempo real).

A metodologia considera uma matriz tridimensional, consistindo nos dados de entrada para o algoritmo iterativo que calcula o volume total representado.

Integrado ao Projeto D-GM (Distributed Geometric Machine), a modelagem do p-iCone é graficamente descrita no ambiente de desenvolvimento VPE-GM (Visual Programming Environment for the Geometric Machine), e executada em paralelo sobre o ambiente VirD-GM (Virtual Distributed Geometric Machine).

O Projeto D-GM propõe o desenvolvimento de um ambiente para criação, gerenciamento e processamento de aplicações paralelas e/ou distribúidas da computação científica com base nas abstrações da Máquina Geométrica.

O modelo GM consiste em uma máquina abstrata, com memória compartilhada, possivelmente infinita e tempo de acesso constante, onde processos e estados computacionais são indexados pela discretização de pontos de um espaço geométrico.

O VirD-GM está sendo desenvolvido pela integração de dois grupos de pesquisa, o de Fundamentos Matemáticos da Computação (GFMC) e o de Processamento Paralelo e Distribúido (G3 PD).

No ambiente de execução VirD-GM, utiliza-se o middleware EXEHDA (Execution Environment for High Distributed Applications) para suporte aos mecanismos de distribuição, comunicação e gerência das computações distribúidas.

O VirD-GM se mostra apropriado para a correta execução do protótipo p-iCone, pois apresenta todos os mecanismos de expressão para o paralelismo e sincronização, necessários para a implementação da solução para o problema de cálculo de relevo não uniformes.

O software p-iCone consiste no estudo de caso para validação do projeto D-GM, pela manipulação de estruturas matriciais tridimensionais.

A estratégia de solução aplica o método de divisão da matriz de pontos em camadas, e de subdivisão das camadas em n novas subcamadas, as quais são resolvidas concorrentemente.

A metodologia propõe o cálculo do volume através do somatório dos produtos da profundidade pela area obtida por integração numérica das faces de cada subcamada.

Nó desenvolvimento do protótipo, consideram-se quatro etapas.
Processamento da entrada, efetua a leitura da matriz de entrada, conversão dos dados para o formato de uma matriz tridimensional de pontos e determinação dos polinômios cúbicos de interpolação (Spline).

Segmentação da matriz, realiza a separação em camadas da matriz, considerando dois intervalos de medições e respectivos polinômios de interpolação.

Divisao de camadas, gera n subcamadas pela interpolação por splines, visando a preservação de propriedades matemáticas e geométricas que determinam a fidelidade da modelagem, incluindo o cálculo do respectivo volume.

Geração do resultado final, retorna o cálculo do somatório dos volumes das diversas subcamadas.

O processo de geração da matriz de entrada, passando pela segmentação e posteriormente pela subdivisão, pode ser visualizado.

Processo de tratamento dos pontos da matriz.

O protótipo desenvolvido apresenta mesma precisão para o cálculo do volume de produtos armazenados, não alterando a qualidade dos resultados em relação aos testes efetuados na versão seqüencial apresentada em.

Devido a atual fase de desenvolvimento, ainda não existem comparativos de desempenho.

Os testes foram executados sobre uma matriz de dados fictícios.

Na continuidade do projeto busca-se o desenvolvido o equipamento digitalizador, o qual possibilitará a geração automática de uma matriz real.

O desenvolvimento do protótipo p-iCone viabiliza o processo de automação do cálculo do volume de produtos em armazéns, pela exploração do paralelismo de dados.

A facilidade de desenvolvimento aliada à validação automática dos parâmetros de entrada do algoritmo no ambiente gráfico VPE-GM, integram-se a modelagem, implementação e geração dinâmica de processos no VirD-GM.

Estas facilidades mostram adequação do DGM ao desenvolvimento do p-iCone.

Além disso, considera-se como uma estratégia de desevolvimento, a sua execução em dispositivos multicore de forma embarcada.

O crescente uso de programas de computador para auxílio ao diagnóstico está relacionado ao rápido desenvolvimento de tecnologias computacionais aplicadas à medicina.

Cada modalidade de imagem médica tem peculiaridades que fazem com que esta seja mais ou menos adequada para o diagnóstico de determinadas doenças.

Os diferentes objetivos e os diferentes problemas apresentados pelas modalidades requerem tratamentos diferentes, que podem ser obtidos a partir da aplicação de técnicas de processamentos de imagens.

Acredita-se que um ambiente computacional que integre técnicas tradicionais de segmentação e classificação de imagens médicas com operadores baseados em algoritmos evolutivos (como algoritmos genéticos, autômatos celulares, redes neurais, etc) possa permitir ferramentas mais versáteis e com precisão apropriada ao tipo de aplicação pretendida.

Imagens do setor de Radiologia do Hospital Universitário São Francisco de Paula (HUSFP) são utilizadas como objeto desta pesquisa e define-se como aplicação alvo o cálculo da densidade ossea a partir de imagens captadas por instrumentos de raio-x.

Busca-se viabilizar o diagnóstico e a administração de terapia com baixo custo operacional, valendo-se de computação de alto desempenho.

O projeto M-IPE (Medical Information Processing Environment) propõe a formalização e a sedimentação de um ambiente para o processamento de informações médicas, baseado em algoritmos evolutivos, integrando-se a um repositório de informações.

O middleware proposto para gerenciar o processo de computação paralela e distribúida foi concebido no PPGINF/UCPEL, desenvolvido em Java e de licença livre.

O modelo atual do middleware contempla o paralelismo de aplicações do tipo bag-of-tasks.

Estas são um tipo de aplicação paralela cujo resultado final pode ser obtido particionando o problema em tarefas independentes.

O estudo da viabilidade e da aplicabilidade de algoritmos evolutivos para o problemas de segmentação de imagens médicas foi realizado em.

O esboço da arquitetura proposta pode ser visualizado.

As fontes de imagens podem ser diversas, desde aparatos convencionais gerando imagens em filme ou papel, até equipamentos mais sofisticados cujos resultados são digitais.

No caso de aparatos convencionais, o processo de digitalização exige modelos numéricos para tratamento de erros, como rúidos e diferenças de métodos de captura com Matemática Intervalar e Números/Conjuntos Fuzzy.

Atores da Arquitetura do Ambiente M-IPE.

O acesso as informações disponibilizadas no M-IPE para os usúarios será gerenciado através de um controle de perfil do usúario.

E facultado acesso através de interface web tanto por dispositivos móveis (PDAs e netbooks) quanto por dispositivos convencionais (desktops e workstations).

Esta interface, além de ter a responsabilidade do gerenciamento dos perfis de usúarios, possui o mecanismo de submissão e registro de imagens no banco de dados e o mecanismo de submissão de tarefas, sendo este ultimo o responsável pela parametrização dos algoritmos que serão aplicados quando da segmentação das imagens.

Além disso, contém um sistema de visualização apropriado para a análise de informações fuzzy geradas pelos modelos, uma vez que se pretende fornecer resultados numéricos acompanhados do diagnóstico assistido por informações qualitativas, gerados pelo modelo fuzzy intervalar.

O banco de dados além de catalogar usúarios e informações administrativas, tem o registro temporal das imagens submetidas, as parametrizações default e customizadas para cada tarefa submetida e os resultados numéricos quantitativos e qualitativos dos diagnósticos.

A utilização de recursos computacionais que proporcionem alto desempenho são peças indispensáveis na manipulação de imagens médicas, já que estas, apresentam grande volume de dados para processamento e este processamento pode envolver operações de caráter emergencial.

A continuidade do trabalho irá buscar resultados mais detalhados de casos reais do cálculo da densitometria ossea e o aprofundamento das análises de precisãó dos algoritmos de segmentação e de performance da arquitetura paralela.

Através do monitoramento pode-se obter informações como o estado atual de componentes de um sistema distribúido, dados sobre a execução de programas no mesmo e sobre o funcionamento da rede de interconexão.

Estas informações podem auxiliar na realização de uma grande variedade de tarefas.

Algumas destas são, detecção de falhas, escalonamento, análise de desempenho e adaptação da execução de programas.

Um tipo especial de sistema distribúido que pode também beneficiar-se do monitoramento é o Grid.

No entanto, uma ferramenta que proponha-se a realizar o monitoramento de tal sistema necessita lidar com suas características inerentes.

Sua grande escala, dinamicidade quanto aos recursos disponíveis, heterogeneidade e distribuição geográfica devem ser levados em conta no projeto.

Uma dificuldade essencial do monitoramento de sistemas distribúidos é que as informações também estão distribúidas.

Por isso é necessário um mecanismo para a coleta desses dados.

Uma abordagem para resolver esse problema é a utilização de uma estrutura hierárquica formada por componentes que forneçam acesso a informações obtidas por um conjunto de componentes de nível hierárquico inferior.

A ferramenta Ganglia e o Monitoring and Discovery Service (MDS), que faz parte do Globus Toolkit, utilizam esse tipo de estrutura.

O Ganglia utiliza o modelo pull (sob demanda) de recebimento de dados, utilizando polling para detectar a disponibilidade de novas informações.

No entanto, para o propósito de acompanhar o comportamento de um conjunto de objetos por um período de tempo, é mais interessante utilizar o modelo push.

Dessa forma os dados são enviados para os interessados, assim que tornam-se disponíveis ou que alguma condição for satisfeita, como a acumulação de uma certa quantidade de informação ou a passagem de um determinado intervalo de tempo desde o ultimo envio.

O MDS versão 4 suporta o modelo push, mas é mais otimizado para tarefas como descoberta ou envio de alertas do que para tarefas como o acompanhamento do comportamento de componentes ou da execução de programas.

Tendo em vista o problema abordado na seção anterior, propõe-se um modelo hierárquico de coleta de dados de monitoramento distribúidos, utilizando o modelo push de recebimento de dados.

Esse modelo estende o DIMVisual, que é um modelo de integração de dados de monitoramento para visualização.

O DIMVisual, que possui um protótipo implementado, recebe informações de monitoramento relacionadas a diversos objetos e coletados por diferentes ferramentas.

Esses são unificados e padronizados, gerando dados em um formato de entrada de uma ferrramenta de visualização.

No DIMVisual original, o usúario precisa reunir localmente as informações de monitoramento, que estão espalhadas por todo um sistema distribúido, para que se possa fornecê-las ao modelo.

A extensão aqui proposta visa automatizar esse processo.

Se possível, também pretende-se adicionar ao DIMVisual a capacidade de realização de monitoramento on-line.

O modelo proposto é composto por dois tipos principais de componentes chamados coletores e agregadores e por um cliente.

O cliente passa a ser o responsável por realizar o processo de integração dos dados.

Um coletor é um componente que obtém dados de monitoramento de uma fonte e os envia a um conjunto de agregadores.

Os agregadores formam uma estrutura hierárquica e são responsáveis por receber informações de monitoramento de coletores e de agregadores de nível inferior e repassá-los a agregadores de nível superior ou a clientes.

Para diminuir o envio de dados desnecessários aos clientes, é utilizado um mecanismo de subscrição.

Cada cliente tem acesso a informações sobre os coletores acessíveis através de um agregador.

Esses dados contém uma identificação unica, e uma identificação do componente monitorado, para cada coletor.

Para a o fornecimento desses dados, os coletores devem registrar-se nos agregadores antes de poderem enviar dados a eles.

Essas informações são repassadas pelos agregadores, até chegarem ao topo da hierarquia.

Nesse processo também é registrado qual agregador repassou o registro, a fim de poder-se percorrer o caminho até o coletor.

Utilizando as informações de registro do coletor, o cliente faz solicitações de subscrição aos dados gerados pelos coletores de interesse.

Essas são repassadas através da hierarquia até encontrar-se um agregador que já possua a subscrição ou então o próprio coletor.

Quando possui dados a enviar, um coletor consulta suas subscrições e os envia aos agregadores subscritos.

Um agregador, por sua vez, possui uma lista de subscrições para cada coletor.

Dessa forma ele pode determinar para quem deve repassar os dados recebidos.

Está sendo implementado um protótipo do modelo proposto, utilizando a linguagem Objective-C.

Nesse caso, os componentes são objetos distribúidos, e o envio de informações de monitoramento, solicitações de subscrição e informações registro é feito utilizando o envio de mensagens (chamadas de métodos) a esses objetos.

Pretende-se realizar testes com esse protótipo para validar o modelo.

Tem-se interesse em avaliar sua utilização em Grid.

O escopo deste trabalho é a Computação Ubíqua (UbiComp), na qual os sistemas computacionais interagem com o ser humano a todo o momento, não importando onde ele esteja, constituindo um ambiente altamente distribúido, heterogêneo, dinâmico, móvel, de composição mutável e com forte interação entre homem e máquina.

Para isso, as aplicações precisam "entender" o ambiente, tendo como referência o seu contexto de interesse, e se adaptarem ao mesmo.

A UbiComp é centrada no usúario e é reconhecida como um novo paradigma, decorrente do rápido avanço tecnológico dos dispositivos móveis, redes sem fio, redes de sensores, dispositivos e sistemas inteligentes, sistemas distribúidos, grades computacionais, Web Services e Web Semântica.

Nesta visão da UbiComp, as pesquisas que contemplam adaptação ao contexto tem ganho espaço na literatura especializada.

Como etapa da metodologia de pesquisa utilizada, foram estudados projetos relacionados ao controle da adaptação ao contexto, sob a perspectiva da UbiComp.

Ao todo foram contemplados 20 projetos, sendo a comparação entre os mesmos realizada, considerando os seguintes aspectos, i) monitoração em tempo real.

Monitoração de vários tipos de contexto.

Localização.

Análise do perfil do usúario.

Anúncio de produtos ou serviços.

Orientação a serviço.

Descoberta de recursos ou serviços.

Modelagem do domínio.

Suporte a raciocínio.

Compartilhamento.

Histórico de situações e/ou adaptações ao contexto.

Uso de ontologias.

Estes aspectos foram selecionados, tendo como critérios, demandas entendidas como características da UbiComp.

Na comparação, constatou-se que os projetos considerados, atendem alguns ou grande parte dos requisitos introduzidos pela UbiComp, porém nenhum deles contempla os mesmos na sua totalidade.

Além disso, as soluções, de um modo geral, referem-se a um determinado domínio de aplicação ou à determinada tecnologia de rede.

Deste modo, o controle da adaptação ainda e, normalmente, específico a determinado contexto ou á determinada aplicação.

Os trabalhos existentes ainda não possuem uma solução padronizada para as aplicações ubíquas genéricas.

Este estudo está registrado na publicação.

A proposta do trabalho objetiva criar um modelo de controle da adaptação dinâmica de aplicações em ambiente ubíquo, que considere o contexto, com base em informações monitoradas, informações semânticas e inferências a partir das mesmas.

O modelo proposto pretende possibilitar uma evolução incremental das especificações de políticas, regras e ações de adaptação, permitindo a reutilização e a customização destas para o desenvolvimento de novas aplicações.

Além disso, deverá possuir mecanismos para administrar situações de conflito quando da tomada de decisões do servidor de adaptação, perseguindo a premissa de adaptar os serviços da UbiComp ao ambiente sem, ou com mínima, intervenção explícita do usúario.

Na perspectiva da lógica de descrições de contexto, foi analisado o uso de ontologias para a concepção deste modelo de controle de adaptação.

Esta escolha se deve ao fato de ontologias apresentarem elevada expressividade na definição de contexto e na especificação de regras de adaptações, por possúirem a possibilidade de realizar inferências a partir das informações semânticas, facilidade de reutilização e extensibilidade.

O modelo semântico utilizará ontologias de domínio, ontologia básica com todas as entidades, seus atributos e relacionamentos possíveis do contexto ubíquo.

Ontologia da situação de contexto e seus elementos.

Ontologia de adaptação com regras, perfis e preferências, restrições e ações de adaptação.

Ontologia para histórico das adaptações realizadas.

O modelo semântico também utilizará ontologias como artefatos de software, tanto em tempo de desenvolvimento quanto em tempo de execução.

Por sua vez, na gerência do mecanismo de adaptação, estão sendo revisados os conceitos de controle provenientes da Computação Autônoma.

O modelo proposto será empregado no Controle de Adaptação funcional e não funcional, do EXEHDA e como areas de aplicações pretende-se explorar soluções pará Medicina e Agropecúaria.

O EXEHDA, Execution Environment for Highly Distributed Applications é um middleware direcionado as aplicações distribúidas, móveis e conscientes do contexto da UbiComp.

Este middleware é baseado em serviços, e tem por objetivos criar e gerenciar um ambiente ubíquo, bem como prover a execução de aplicações sobre este ambiente.

A próxima etapa do trabalho contemplará a definição da arquitetura de software para o modelo, considerando as características do EXEHDA.

As arquiteturas multi-core influem diretamente no desenvolvimento de software porque para melhor aproveitar esses processadores os desenvolvedores devem paralelizar seus programas.

Programas concorrentes para máquinas multi-core são geralmente implementados usando threads que se comunicam através de uma memória compartilhada.

Quando múltiplas threads acessam uma mesma faixa de endereços de memória é possível que alguma intercalação de suas instruções resulte em uma execução inválida.

Para evitar essas execuções inválidas, as linguagens fornecem mecanismos de sincronização como por exemplo bloqueios (locks), mas sincronizações baseadas em bloqueios possuem algumas armadilhas que dificultam a programação e são propensas a erros.

Memórias transacionais fornecem um novo modelo de controle de concorrência que não apresenta as mesmas dificuldades encontradas no uso de bloqueios.

Ela traz para a programação concorrente os conceitos de controle de concorrência usados a décadas pela comunidade de banco de dados.

Construções em linguagens transacionais são fáceis de serem usadas e podem gerar programas fortemente escaláveis.

Este trabalho tem como objetivo apresentar a linguagem CMTJava, uma linguagem de domínio específico para programação de memórias transacionais em Java.

Essa linguagem foi criada visando facilitar a programação de máquinas multi-core e sua grande vantagem é que o sistema de tipos da linguagem garante que transações serão executadas apenas através da primitva atomic.

Com o objetivo de apresentar um simples programa usando a linguagem CMTJava, segue abaixo o código de uma classe Conta, a qual possui um atributo saldo.

CMTJava provê uma abstração de objetos transacionais (interface TMObject).

TMObject é usada como um alerta para o compilador, a partir desse alerta são gerados automaticamente métodos especiais de get e set para os atributos da classe.

Para a classe Conta são gerados os seguintes métodos, STM<Void> setSaldo(Double saldo), STM<Double> getSaldo.

Esses métodos retornam ações como resultado.

Uma ação transacional é uma ação que, quando executada, irá produzir um efeito desejado.

No caso do método getSaldo, STM<Double> é uma ação transacional que retorna como resultado um Double.

A primitiva atomic executa usando sincronização otimista, transações são executadas em paralelo sem o uso de bloqueios, mas uma transação só é efetivada se nenhuma outra transação concorrente modificou uma area de memória em comum.

O sistema dé tipos da linguagem CMTJava garante que ações transacionais somente serão executadas através da primitiva atomic.

A linguagem CMTJava também oferece as primitivas retry e orElse para compor transações sequencialmente ou como alternativas.

Ações transacionais podem ser implementadas através do conceito de mônadas.

Uma mônada é usada para descrever computações que podem ser combinadas gerando novas computações.

Por esta razão, mônadas são freqüentemente usadas na implementação de linguagens de domínio específico.

As operações básicas de uma mônada usam funções de alta ordem, dessa maneira, para implementar CMTJava foi usada uma extensão Java que suporta closures.

A linguagem CMTJava define um conjunto de regras de tradução que transforma blocos STM em operações monádicas usando closures.

Todo o suporte a transações é implementado com código Java puro.

Outro detalhe importante é que mesmo o sistema sendo desenvolvido em Java, as idéias apresentadas podem ser implementadas em qualquer outra linguagem orientada a objetos que suporte closures, como por exemplo C#.

A linguagem CMTJava é a primeira extensão Java para transações que suporta a construção OrElse.

A idéia foi adaptada da linguagem STM Haskell para um contexto de orientação a objetos.

Vale ressaltar que este trabalho serve como motivação para a introdução de closures na linguagem Java.

O escopo deste trabalho é a computação ubíqua, a qual é um paradigma de interação usúario-computador em que a tecnologia é integrada de forma transparente a ambientes físicos para auxiliar pessoas na realização de suas tarefas díarias de forma contínua e onipresente.

Para viabilização deste paradigma, a sensibilidade ao contexto se mostra componente central.

Nesse trabalho, entende-se por aplicações sensíveis ao contexto, toda e "qualquer informação que pode ser usada para caracterizar a situação de uma entidade (pessoa, local ou objeto) que é considerada relevante para a interação entre o usúario e a aplicação, incluindo o próprio usúario e a aplicação ".

As aplicações conhecem o ambiente no qual estão sendo utilizadas e tomam decisões de acordo com mudanças no seu próprio ambiente.

Ou seja, reagem a ações executadas por outras entidades, podendo essas ser pessoas, objetos ou até mesmo outros sistemas, que modifiquem o ambiente.

Essas aplicações, de um modo geral, utilizam sensores para tomar ciência de modificações que venham a acontecer no ambiente.

Tais modificações são alterações nas informações de contexto.

Este texto apresenta uma dissertação de mestrado inserida nos esforços de pesquisa do Projeto PERTMED (Sistema de TeleMedicina Móvel), tendo sido identificada a area médica, como alvo de avanços das tecnologias móveis e sem fio, como Bluetooth, WiFi, GPRS, os quais somados a popularização dos dispositivos móveis e sem fio, PDAs, celulares, GPS e pequenos dispositivos médicos, como holters, entre outros, facilitam a tarefa de monitoramento de pacientes.

Infra-estruturas de software para o gerenciamento dessas informações contextuais necessitam, em geral, coletar uma grande quantidade de informações de diferentes naturezas do ambiente, analizando essas informações como varíaveis independentes, ou combiná-las com outras informações do passado ou presente.

Além disso, essas aplicações são caracterizadas por apresentarem contextos altamente dinâmicos e variados, com um grande grau de mobilidade dos seus principais atores (médicos, pacientes, paramédicos, etc).

Este trabalho tem como objetivo principal propor a integração de tecnologias de Web Semântica em mecanismo de sensibilidade ao contexto direcionado a Computação Ubíqua.

O mecanismo proposto deverá ser integrado ao middleware EXEHDA (Execution Environment for Highly Distributed Applications), e, será avaliado com o desenvolvimento de aplicação ubíqua voltada para medicina.

Este trabalhou buscou identificar as possibilidades de uso de ontologias, um dos elementos que constitui a Web Semântica, para descrever características de contexto.

As características ontológicas de formalidade, semântica explícita e abstração de implementação habilitam sistemas de software não apenas a inferir novas informações a partir de informações modeladas por ontologias, mas também a compartilhar entre si essas informações de maneira a integrar de forma transparente os serviços que as manipulam.

Nesta perpectiva, o mecanismo proposto a ser integrado ao EXEHDA, deverá fazer o uso de serviços que possibilitam a tradução dos dados sensorados para contextualizados, independente do tipo de aplicação, através do uso de um interpretador de contexto que realiza consultas e inferências sobre a ontologia.

Para isso, está em desenvolvimento um modelo ontológico, denominado OntContext, o qual visa atender a vários domínios de aplicações sensíveis a contexto, como descoberta de recursos, sensibilidade ao contexto e adaptação ao contexto.

O OntContext, modela informações de contexto, segundo as dimensões semânticas discutidas por Abowd & Mynatt de identificação, localização, tempo, atividade, dispositivos, eventos e modo de captura e acesso.

A utilização de tecnologias de Web Semântica para representação e processamento de informações de contexto traz como vantagens, a descrição formal, padrão e estruturada de cada dimensão semântica de informação de contexto.

O suporte à interoperabilidade sintática, estrutural e, principalmente, semântica entre aplicações sensíveis ao contexto.

A capacidade de interpretar e inferir inter-relacionamentos com base nos contéudos e descrições semânticas das entidades envolvidas.

Até o presente momento, é possível resumir as contribuições desse trabalho na revisão das principais estratégias para sensibilidade ao contexto na computação ubíqua, sistematizando e comparando suas principais caracterísiticas.

Modelagem do OntContext, desenvolvido como base em ontologias e padrões de Web Semântica para modelagem e representação de informação contextual.

Na continuidade será feita a definição da arquitetura de software do serviço a ser modelado para o uso com o middleware EXEHDA.

Uma Proposta de Memoria Distribúida Compartilhada para ó Projeto D-GM.

Este artigo resume o trabalho desenvolvido no primeiro semestre do curso de mestrado no PPGINF/UCPel, com objetivo de propor uma modelagem para a memória distribúida compartilhada do projeto D-GM.

Para tal, considera-se o emprego de sistemas DSM, pela sua elevada abstração em relação aos sistemas baseados em troca de mensagens, gerenciando de modo transparente os problemas de comunicação em rede, permitindo níveis de escalabilidade semelhantes aos dos sistemas multicomputadores.

O projeto D-GM (Distributed-Geometric Machine) tem por foco modelar uma versão distribúida para o modelo GM (Geometric Machine).

A máquina geométrica (GM) é um modelo computacional abstrato utilizado para modelagem de computações paralelas e/ou concorrentes e não determinísticas que contempla dois tipos de concorrência, a espacial que determina a execução de vários processos em múltiplos recursos no mesmo instante de tempo.

A temporal com a execução de processos concorrentes utilizando posições distintas de memória em um mesmo recurso.

Neste modelo, os processos são indutivamente constrúidos a partir de construtores aplicados a processos elementares, os quais são definidos pela ação associada a uma distribuição espacial da memória e cuja execução altera apenas uma posição de memória.

Este modelo abstrato com memória global, compartilhada, possivelmente infinita e com tempo de acesso constante foi concebido para dar semântica aos algoritmos concorrentes da computação científica.

Para melhorar o acesso, a manipulação e a compreensão da programação paralela no modelo GM, o projeto provê a execução paralela e/ou distribúida de aplicações da computação científica, pela integração de dois importantes componentes de software, o ambientededesenvolvimentovisualVPE-GM(VisualProgrammingEnvironmentfor Geometric Machine), disponibilizando a modelagem gráfica das aplicações paralelas e/ou distribúidas, incluindo seleção dos construtores de processos e configuração da memória.

O ambiente de execução VirD-GM (Virtual Distributed Geometric Machine), estendendo o uso do middleware EXEHDA como suporte à execução paralela e/ou distribúida de aplicações concebidas no D-GM.

Nesta proposta, pretende-se conceber um modelo de memória compartilhada distribúida para uso no projeto D-GM, de tal forma a obter uma formalização e definição compatíveis com as abstrações da memória do modelo GM.

Para alcançar esta meta se empregará como base sistemas do tipo DSM (Distributed Shared Memory).

Em um sistema DSM, ocorre o compartilhamento de memória entre processos, mesmo que estejam executando em nodos que não compartilham memória fisicamente.

Consideram-se relevantes os estudos da estrutura de sistemas DSM e dos dados compartilhados, incluindo coerência de memória, modelos de consistência de memória e algoritmos de implementação, assim como implementações em software de DSM.

Pela análise das características de conhecidos sistemas de DSM, como Terra-cotta, ORCA e Treadmarks verificou-se cinco importantes quesitos para as demandas do modelo GM, 

Plataforma de desenvolvimento, que deve ser a mesma do ambiente de execução do modelo D-GM.


Nível de dificuldade de implementação para tornar a modelagem efetiva na prática.

Análise e escolha do modelo de consistencia de memoria, o qual fará uso de varíaveis dé sincronização.

Escalabilidade, permitindo alocar de nodos de acordo com a aplicação.

Confiabilidade, garantindo a não perda de dados em razão de falhas.

Resultados preliminaresdentre as opções estudadas, o sistema DSM Terracotta se mostra o mais adequado para a modelagem prevista no projeto D-GM.

Além de possuir a mesma plataforma de desenvolvimento, apresenta, coerência de dados através de replicação e atualização, modelo de consistência de memória semelhante ao da máquina virtual JAVA, alta escalabilidade, com a criação de instâncias de máquinas virtuais JAVA nos nodos, tantas quantas forem necessárias pelas threads da aplicação, boa confiabilidade através da gravação em disco das varíaveis compartilhadas, e possibilidade de customização.

Assim, na fase atual dos trabalhos, entende-se que o sistema DSM Terracotta proporcionará suporte para a modelagem de um sistema DSM que atenda as necessidades do projetò D-GM.

O uso de bibliotecas de programação numérica têm sido recorrente para o desenvolvimento de aplicações computacionais.

A biblioteca Linpack, desenvolvida no final da década de 1970, foi um dos primeiros esforços no sentido de padronizar o acesso a rotinas FORTRAN que envolvessem operações aritméticas entre escalares, vetores e matrizes, possibilitando, ainda, a execução eficiente em supercomputadores da epoca.

Desde então, inúmeras outras bibliotecas (Eispack, Lapack, ScaLapack,) foram implementadas com o intuito de oferecer desempenho e reutilização de código.

Atualmente, diversas bibliotecas possibilitam, também, o uso de paralelismo implícito para sistemas com memória compartilhada e distribúida, além de oferecer altos níveis de abstração de código.

Neste trabalho, é feita uma revisão de literatura, avaliando os recursos de algumas bibliotecas de programação numérica atuais.

Com isso, além de apresentar o estado da arte da area, o objetivo do trabalho é determinar quais os níveis de abstração que as bibliotecas numéricas proporcionam ao programador.

Nas próximas seções são relacionadas as características de algumas bibliotecas de programação e uma síntese dos resultados.

Neste trabalho foram avaliadas três bibliotecas, Math Kernel Library (MKL) implementa rotinas otimizadas para a computação científica, engenharias e aplicações financeiras, voltadas para as arquiteturas Intel (Itanium 2, Xeon, Pentium 4 e Multicore).

As rotinas desenvolvidas em Fortran incluem operações com vetores e matrizes, Transformada Rápida de Fourier e funções matemáticas vetoriais.

A performance de MKL é obtida através de otimizações realizadas explorando as capacidades do processadores e das características do sistema em si.

O desenvolvimento de aplicações pode ser feito em C ou Fortran explorando arquiteturas SMP.

Engineering and Scientific Subroutine Library (ESSL) é uma biblioteca da IBM para aplicações científicas e de engenharia que executam em arquiteturas IBM POWER e Blade Center.

A biblioteca possui diversos recursos para a algebra linear, além de funções de busca e ordenação, interpolação, geração de números aleatórios, entre outros, tanto para as versão seqüencial quanto paralela da biblioteca.

Além do paralelismo em SMP, ESSL oferece recursos para a programação em clusters, utilizando como mecanismo de comunicação interprocessosMPI ou LAPI (padrão proprietário da IBM).

A programação pode ser feita em Fortran, C e C++.

Portable, Extensible Toolkit for Scientific Computation (PETSc) é uma biblioteca que prove um conjunto de estruturas de dados e rotinas para a implementação de soluções numéricas de equações diferenciais parciais (EDP) em Fortran, C e C++.

Estes recursos permitem a programação de aplicações tanto para a execução serial quanto paralela (utilizando MPI para a comunicação interprocessos) de códigos em larga escala.

A biblioteca é organizada hierarquicamente, possibilitando o usúario escolher o nível de abstração mais apropriado para um determinado problema.

Além disso, PETSc oferece muita flexibilidade em termos de técnicas de programação através do uso de orientação a objetos.

O encapsulamento dos diversos níveis de abstração necessários para o desenvolvimento de uma aplicação são essenciais para simplificar o processo de programação.

Ao avaliar as bibliotecas numéricas vistas neste trabalho observou-se que existem diversos recursos agregados quando comparadas as primeiras bibliotecas que foram desenvolvidas.

Bibliotecas como PETSc, por exemplo, oferecem diversas camadas de abstração, que incluem, nos níveis mais baixos, diversas implementações de bibliotecas mais antigas.

No entanto, para se programar uma aplicação são necessárias diversas outras etapas, que incluem, além da resolução numérica, conhecimentos específicos sobre modelagem, equações matemáticas e programação paralela.

Observou-se que atualmente nenhuma biblioteca numérica oferece suporte completo para os processos de modelagem e discretização em conjunto com as etapas de resolução numérica, sendo esta feita em separado por diversas ferramentas.

Neste sentido, um aspecto que poderia simplificar o desenvolvimento de uma aplicação é fazer uso de uma linguagem de especificação.

Em uma linguagem de especificação o código fonte do programa poderia ser gerado, de maneira que não houvesse uma preocupação com o funcionamento e controle do código em si, sendo necessário apenas descrever alguns parâmetros relacionados a execução.

Através desta abordagem o código poderia ser gerado a partir de um arquivo de configuração, o qual conteria informações a respeito do domínio a ser simulado, como as equações matemáticas que representam o problema, o domínio, a forma de geração das malhas, quais os métodos de precondicionamento e numérico que devem ser utilizados, bem como informações sobre o ambiente e forma de execução paralela.

Tal proposta resume-se, portanto, em oferecer abstrações de alto nível em bibliotecas numéricas paralelas.

Aplicações voltadas para sistemas distribúidos como grids necessitam de uma análise de desempenho.

Essa análise permite ao desenvolvedor melhorar a sua aplicação levando em conta características do ambiente e correlacionando-as com o comportamento da aplicação.

Sendo uma operação complexa, a visualização de dados é uma solução comumentemente utilizada para realizar a análise de aplicações paralelas.

Grande parte das ferramentas utilizam gráficos em duas dimensões, onde uma dimensao é o tempo e a outra dimensão é usada para se listar os componentes da aplicação, como processos e fluxos de execução.

Alguns exemplos dessas ferramentas são Pajé e Vampir.

O uso de duas dimensões para a análise de aplicações paralelas em ambientes como grids pode ser limitante.

O primeiro problema que surge é que com ferramentas 2 D, não é possível analisar as aplicações levando-se em conta a topologia da rede.

Em ambientes como grid, as características de sua interconexão podem ser decisivas na obtenção de alto desempenho.

O impacto da topologia de rede na aplicação grid pode ser ainda mais relevante quando a interconexão é fortemente hierárquica, com diferentes tecnologias de rede em diferentes níveis dessa hierarquia.

Um exemplo disso é a interconexão do Grid'5000, onde os recursos do grid são agrupados por clusters homogêneos internamente organizados com Gigabit Ethernet e globalmente conectados por fibra optica.

O segundo problema é que com ferramentas 2 D não é possível analisar o padrão de comunicação entre os processos, uma vez que são observadas sempre as comunicações propriamente ditas entre eles e não o padrão global da comunicação levando-se em conta todas as comunicações.

Este trabalho consiste em propor novas técnicas de visualização para a análise de aplicações paralelas.

Um primeiro esforço nesse sentido é a visualização em três dimensões dos dados de rastreamento de uma aplicação.

Nessa técnica, duas dimensões são utilizadas para representar a topologia de um grid e os componentes da aplicação paralela, e a terceira dimensão é usada como tempo.

Assim, é possível ver a evolução dos processos de uma aplicação paralela e relacioná-los aos recursos utilizados.

As comunicações são então dispostas nesse ambiente 3 D sendo possível a verificação do padrão de comunicação da aplicação assim como quais foram os canais de comunicação utilizados.

Um protótipo chamado Triva Three Dimensional and Interactive Visualization Analysis foi criado para validar a técnica.

O protótipo utiliza, como entrada, dados gerados pelo DIMVisual, adaptado para a leitura de rastros de aplicações KAAPI.

Internamente, o protótipo contém adaptadores para o simulador Pajé, a biblioteca Graphviz e Ogre3 D.

Para alimentar o protótipo com dados, foi realizado o rastreamento de diferentes execuções de aplicações paralelas KAAPI no Grid'5000.

As aplicações eram compostas por uma quantidade de processos que varia de 30 à 2900.

Mostra alguns exemplos de visualizações 3 D possíveis com o protótipo.

A primeira imagem mostra o padrão de comunicação de uma aplicação e os outros dois, duas aplicações diferentes com a topologia de rede e as comunicações da aplicação.

Visualizações 3 D dos rastros de aplicações paralelas.

Como trabalhos futuros, pretende-se realizar uma análise mais profunda dos resultados já obtidos e trabalhar com intervalos de tempo, agregação e integração de dados para permitir uma sumarização dos rastros.

Bem vindos à Sessão de Iniciação Científica da Nona Escola Regional de Alto Desempenho.

Esse ano, temos o prazer de realizar a ERAD na UCS, Universidade de Caxias do Sul, e trazê-los a Caxias do Sul, berço da colonização italiana no estado do Rio Grande do Sul e localizada a 120 km da nossa capital.

É com grande satisfação que apresentamos a Sessão de Pôsteres de Iniciação Científica da ERAD 2009.

Agradecemos aos alunos submeteram seus trabalhos para ERAD 2009, bem como os professores pesquisadores que incentivaram seus alunos a participarem desse encontro.

Gostaríamos de iniciar essa sessão agradecendo aos jovens pesquisadores e bolsistas que deixaram de lado a diversão das férias e trabalharam arduamente para submeter seus trabalhos a esse fórum.

São 33 trabalhos de grande qualidade que demonstram a força da área de alto desempenho na nossa região, que cresce a cada ano que passa.

Desse modo, queremos igualmente agradecer a todos os professores que orientam e conduzem pesquisas relacionadas, reconhecendo-os como os grandes responsáveis pela formação desses estudantes.

Além disso, gostaríamos de agradecer a todos os membros do comitê de programa que fizeram um ótimo trabalho, revisando e emitindo seus pareceres para todos os artigos submetidos.

Essa etapa é especialmente importante nessa modalidade, já que um dos grandes objetivos desse fórum é fazer com que nossos alunos acostumem-se com o ato de escrever artigos científicos e as revisões são de extrema importância nesse processo de crescimento.

Finalmente, desejamos a todos um ótimo trabalho e uma excelente estada em Caxias do Sul!

Inúmeros problemas encontrados no mundo real exigem elevada capacidade computacional na sua solução.

Com o avanço da tecnologia criaram-se computadores com altíssima velocidade de processamento e grande capacidade de memória, empregados em pesquisas científicas e militares.

Estes equipamentos, porém, possuem custo elevado.

Os avanços na área de sistemas computacionais distribuídos permitiram o surgimento de uma alternativa de baixo custo para estes problemas, os clusters de computadores.

Um cluster é formado por um conjunto de computadores convencionais interconectados por uma rede de alta velocidade, sob ação de um sistema operacional distribuído, de forma a trabalhar como uma única máquina de grande porte.

Para melhorar a velocidade de resposta de um determinado processamento, o paralelismo é amplamente utilizado.

Tarefas grandes e complexas são divididas em sub-tarefas que são distribuídas entre os processadores do cluster, que as executam simultaneamente, com o intuito de obter rapidamente uma solução para o problema global.

Além do baixo custo e do alto poder de processamento, existem outras vantagens no uso de clusters, tais como, alta escalabilidade, pois é possível adicionar máquinas ao cluster.

Tolerância a falhas, pois o sistema não é totalmente comprometido quando algum computador falha.

Heterogeneidade, pois é possível a utilização de diferentes configurações de hardware nos componentes.

Esta alternativa apresenta uma contrapartida, a necessidade de se adaptar os algoritmos, que antes funcionavam em sistemas monoprocessados, para os sistemas distribuídos e com vários processadores que caracterizam o cluster.

Este trabalho apresenta um estudo de avaliação de desempenho em um cluster que emprega ambiente de programação paralela, sobre o problema da multiplicação matricial.

Resultados sobre o desempenho são obtidos a partir do incremento gradual do número de componentes do cluster, a partir de um programa desenvolvido para a distribuição da tarefa entre várias máquinas.

Algumas ferramentas surgiram para auxiliar na implementação de algoritmos e programas em sistemas paralelos.

Dentre elas, podem-se citar as bibliotecas PVM (Parallel Virtual Machine) e MPI (Message Passing Interface).

Ambas possuem rotinas específicas para permitir a troca de mensagens entre os processadores em ambiente de memória distribuída.

A PVM permite a junção de várias estações de trabalho, que podem ser heterogêneas, formando uma única máquina paralela virtual.

Ela trata de forma transparente o roteamento das mensagens, a conversão de dados e o escalonamento das tarefas em uma rede de computadores.

Possui uma interface de programação onde é possível utilizar bibliotecas com rotinas que permitem a inicialização e o término de atividades na rede, além da comunicação e sincronização entre tarefas.

O MPI é uma biblioteca que possui diversas rotinas para a troca de mensagens entre processadores de um cluster.

As funções da biblioteca podem ser utilizadas a partir das linguagens C, C++ e Fortran.

As tarefas não são gerenciadas em relação aos processadores.

Cabe ao programador definir qual tarefa deve ser executada em cada processador.

Neste trabalho utilizou-se um cluster que funciona sob ambiente operacional obtido com o uso da biblioteca MPI.

Esta opção foi adotada pois as funções em MPI para troca de mensagens possuem melhor desempenho que as equivalentes em PVM, quando o cluster possui máquinas semelhantes em hardware e software.

Para a obtenção de um ambiente computacional de alto desempenho neste trabalho empregou-se um cluster do tipo Beowulf.

Clusters Beowulf são amplamente empregados por serem formados a partir de computadores comuns, e muitas vezes obsoletos, interligados por tecnologias de redes simples e instrumentalizados com sistemas operacionais open-sources.

O cluster é composto por onze máquinas com processador Intel Pentium D com 28 GHz de clock e 1 GB de memória RAM, sob sistema operacional Ubuntu Server 804.

Um procedimento mestre, alocado em uma das máquinas, atua como responsável por coordenar todas as funções do cluster tais como a distribuição das tarefas entre o restante dos nós.

Para realizar a análise de desempenho do cluster foi desenvolvido um programa que realiza a multiplicação de duas matrizes e armazena o respectivo tempo de execução.

Neste programa cada posição da matriz resultante é obtida em um nó do cluster, sendo que os resultados são centralizados no mestre para a apresentação do resultado final.

O programa desenvolvido segue os seguintes passos, A decomposição é feita seguindo-se as linhas da primeira matriz e as colunas da segunda matriz.

Cada elemento a calcular da matriz resposta está associado a um nó específico.

Em cada nó do cluster todas as posições da matriz são percorridas.

Caso a posição do elemento a calcular corresponda ao número do nó, o elemento correspondente é calculado pela fórmula usual da multiplicação matricial.

Caso não haja correspondência, o nó simplesmente ignora esta posição.

Realizada a multiplicação, o nó envia o resultado do elemento correspondente ao mestre.

O mestre recebe os resultados dos elementos individuais e os insere na devida posição da matriz resultante, de forma a obter o resultado final.

O programa que multiplica matrizes foi executado utilizando matrizes de diversas dimensões, com elementos de valores aleatórios.

Desempenho obtido ao multiplicar matrizes de pequenas, médias e grande dimensões.

Nota-se que a utilização de mais de uma máquina torna o processamento significativamente mais rápido.

No entanto, com matrizes de dimensões iguais ou menores que 100 x100, não há ganho significativo em relação ao desempenho a partir do uso de 5 nodos, pois o tempo de execução se estabiliza em torno de 0,2 segundos.

Isso ocorre devido à limitação por parte da comunicação de dados, que passa a ser especialmente relevante quando o tempo de processamento se torna muito baixo.

Nos testes realizados com matrizes quadradas de dimensões médias e grandes, houve um ganho considerável de desempenho.

A cada nó adicionado, obteve-se a redução do tempo de processamento.

Nota-se que para processamentos mais pesados, o custo da limitação por parte da comunicação torna-se irrelevante.

Este trabalho teve como principal objetivo avaliar o desempenho de um cluster sobre a tarefa de multiplicação de matrizes.

Utilizou-se um cluster Beowulf formado por onze computadores convencionais.

Para realizar os testes foi desenvolvido um programa em linguagem C que emprega chamadas à biblioteca MPI para efetuar a troca de mensagens entre os nós do cluster.

Este programa retorna o tempo utilizado para sua execução, de forma a se avaliar o tempo necessário para efetuar a multiplicação de matrizes.

Foram efetuados testes com matrizes de diversas dimensões.

Notou-se que com matrizes menores houve um pequeno ganho de desempenho no tempo de processamento para obtenção da resposta global, no entanto, este ganho foi limitado pelo overhead de comunicação.

Para multiplicação entre matrizes com dimensões médias e grandes sempre houve ganho de desempenho, diminuindo o tempo de processamento à medida que se adicionavam nós ao cluster.

Assim, conclui-se que o uso de clusters é uma alternativa interessante e eficaz em aplicações computacionalmente intensivas.

O modelo de Potts Celular pertence ao campo da Biologia e tem por finalidade simular a reorganização celular.

Atualmente, ele vem sendo largamente utilizado para realizar diversos tipos de simulações, dentre os quais a evolução de células cancerígenas.

Com relação a algoritmos para simular este modelo, o de Monte Carlo é um dos mais utilizados e mais aceitos pela comunidade acadêmica e científica, pois seus resultados são bastante próximos daqueles esperados.

Porém, ele é bastante ineficiente em termos computacionais.

O algoritmo de Random Walker apresenta-se como uma alternativa viável e eficiente para solucionar este problema.

O ganho de desempenho obtido com a execução paralela foi bastante acentuado, como pode ser observado nos trabalhos de Cercato e Gusatto.

No entanto, os resultados das simulações geradas pelo algoritmo de Random Walker ainda não foram validados em relação aos resultados oferecidos pela aplicação das técnicas convencionais de simulação, mais especificamente em relação ao algoritmo de Monte Carlo, utilizado como padrão para executar o modelo de Potts.

O objetivo do presente trabalho é apresentar resultados quantitativos de simulações geradas por esses dois algoritmos.

Também procura-se explorar o potencial das arquiteturas multi-core com o auxílio de um ambiente extremamente propício para a escrita de programas paralelos, a ferramenta OpenMP.

O restante deste artigo está organizado como segue.

Modelo de Potts Celular.

Algoritmos de Monte Carlo e Random Walker.

Ambiente para programação paralela em OpenMP e a seção 5 analisa os resultados quantitativos gerados pelas simulações dos dois algoritmos.


O modelo de Potts Celular é um modelo computacional desenvolvido por Glazier e Graner com a finalidade de simular a reorganização celular fruto da evolução das interações entre células no ciclo evolutivo de um sistema.

Este modelo é largamente utilizado em diversos tipos de simulações, dentre eles o crescimento de tumores, desenvolvimento dos membros de aves e a interação entre espumas de sabão.

Entretanto, em sua forma tradicional, o modelo de Potts apresenta um custo computacional bastante elevado.

O algoritmo de Monte Carlo é um método numérico utilizado para encontrar soluções para problemas matemáticos, os quais podem ter muitas variáveis, e que não podem ser facilmente resolvidos através de outros métodos numéricos conhecidos.

Devido ao seu grau probabilístico, serve de auxílio a diversas simulações de fenômenos naturais.

O algoritmo de Monte Carlo, quando aplicado ao modelo de Potts Celular, permite selecionar células de um sistema para fazê-lo evoluir.

Um passo de simulação através deste algoritmo corresponde ao sorteio de tantas células quantas houver no sistema, caracterizando, desta forma, seu custo computacional.

O algoritmo de Random Walker, por sua vez, concentra a seleção de rótulos às áreas de borda das células, rótulos estes sujeitos a trocas de energia.

Tal fato reduz o tempo de execução de simulações e, conseqüentemente, melhora o desempenho.

Esta característica representa uma vantagem significativa sobre o Monte Carlo.

Para explorar o paralelismo das aplicações, foi utilizada a interface de programação multithread OpenMP, juntamente com a linguagem C, em ambiente GNU/Linux.

O hardware utilizado foi um computador Intel Pentium Core 2 Duo 6320, com freqüência de clock 20 GHz e memória RAM de 2 GB.

O compilador icc da Intel, para o suporte a sintaxe OpenMP, serviu de base para a compilação dos códigos-fonte.

Foram utilizados três algoritmos para a simulação do modelo de Potts Celular, Monte Carlo seqüencial, Monte Carlo paralelo e Random Walker paralelo.

Na simulação, os algoritmos paralelos utilizaram duas  threads de serviço.

Resultados mais detalhados encontram-se documentados em Luz.

Os três algoritmos evoluíram dentro de um número total de 1100 passos, utilizando o mesmo tamanho de matriz, 2000 x 2000.

Importante observar que todas as execuções consideraram a mesma matriz inicial.

Os resultados apresentados consideram o número de passos realizados na simulação, representando unidades de tempo.

Tanto no Monte Carlo quanto no Random Walker, um passo corresponde ao número de sorteios necessários para que todos os rótulos passíveis de sorteio possam ser escolhidos.

Os algoritmos visam simular estruturas celulares representadas em um espaço discreto.

Uma célula é constituída por um grupo de rótulos (posições de uma matriz) espacialmente conexos e de mesmo valor numérico.

Como o sistema apresenta muitas células, e todas elas aumentam ou diminuem de tamanho ao longo da simulação, o registro deste tipo de informação torna-se um fator importante para a determinação do resultado de uma simulação.

Os gráficos a seguir ilustram o número de células e a área média das mesmas em uma determinada unidade de tempo.

O primeiro gráfico, mostra a variação do número de células no sistema no decorrer da simulação, de acordo com o número de passos executados.

Inicialmente, o número total de células presentes no sistema é de aproximadamente 10800.

Como pode ser observado, o número de células na simulação decresce conforme aumenta o número de passos ao longo do tempo.

Próximo do fim do experimento, as células começam a atingir um determinado ponto de equilíbrio e o sistema torna-se mais estável.

Gráfico ilustrando a variação do número de células de acordo com o número de passos simulados.

Como pode ser constatado, os três algoritmos apresentaram resultados muito semelhantes em praticamente todos os passos de simulação.

O segundo tipo de resultado, mostra a variação do valor da área média das células presentes no sistema ao longo da simulação, de acordo com o número de passos executados.

Este valor depende do número de células presentes no sistema em determinado passo de execução e é obtido através da razão entre o número total de elementos presentes na matriz pelo número total de células em cada passo da simulação.

Antes da simulação, o valor da área média é de aproximadamente 368 UA (unidades de área).

Conforme pode ser observado, os três algoritmos apresentaram resultados muito semelhantes, como indica o comportamento das curvas mostradas no gráfico.

Tal semelhança entre os resultados apresentados nas duas figuras sugere que o algoritmo de Random Walker é tão confiável quanto o de Monte Carlo para simulações de sistemas complexos.

Gráfico ilustrando a variação do valor numérico da área média das células de acordo com o número de passos simulados.

Este trabalho apresentou um comparativo entre três implementações (duas paralelas e uma seqüencial) dos algoritmos de Monte Carlo e Random Walker, utilizando o ambiente de programação multithreaded OpenMP.

Foi possível observar que os resultados obtidos pelas simulações foram bastante próximos.

Entretanto, faz-se necessário realizar uma análise criteriosa dos resultados quantitativos apresentados, de forma a validar o algoritmo de Random Walker.

Tal avaliação deve ser feita, preferencialmente, por profissionais ligados às áreas de Bioinformática, Física e Biologia, com maior legitimidade em compreender os processos envolvidos na evolução das simulações.

A computação reconfigurável vem sendo utilizada para o aumento de desempenho de aplicações através do uso de Field Gate Programable Arrays (FP-GAs).

Tais dispositivos podem ser configurados para realizar tarefas específicas, permitindo o aumento de desempenho pela ampliação da vazão de dados ou pela paralelização de computações.

Atualmente, existem sistemas que incorporam dispositivos reconfiguráveis em sua arquitetura, possibilitando sua utilização conjunta com processadores de propósito geral.

Esta abordagem, conhecida por computação híbrida, visa a utilização de distintos recursos computacionais para o aumento do desempenho de aplicações.

Uma técnica utilizada em computação híbrida consiste em executar em FP-GAs as operações computacionalmente intensivas de um programa, mantendo as demais operações executando na CPU.

Neste caso, podem ocorrer múltiplas transferências de dados entre os dispositivos para efetuar-se a computação, sendo que o tempo gasto na comunicação pode tornar-se um fator limitante do desempenho.

Neste contexto, este trabalho apresenta uma avaliação das abordagens de comunicação com FPGA no supercomputador Cray XD1, baseada em medições de taxas de transferências de dados em diferentes situações.

No restante deste artigo, expõe-se inicialmente o ambiente híbrido utilizado e suas formas de comunicação com o FPGA.

Em seguida, apresenta-se a metodologia de avaliação e os resultados obtidos.

O Cray XD1 é um sistema híbrido composto por 12 blades.

Cada blade contém dois processadores Opteron de 64 bits, até 8 GB de memória DDR SDRAM e um processador de comunicação RapidArray.

Este processador permite a comunicação com alta largura de banda e baixa latência entre os processadores.

Cada blade pode conter ainda um módulo de expansão que conta com um FPGA, o qual é conectado ao blade através de outro processador RapidArray.

A Cray disponibiliza a API RapidArray Transport Core para a comunicação dos processadores com o FPGA.

Esta API é composta por dois blocos denominados Fabric Request e User Request.

Cada bloco realiza a comunicação utilizando abordagens distintas que serão detalhadas nas subseções seguintes.

Nesta abordagem de comunicação, conhecida como push, o programa executado nos processadores é responsável pela comunicação com o FPGA.

Para isso, a Cray disponibiliza a biblioteca enlib, a qual abstrai ao programa o FPGA como um arquivo.

A transferência de dados entre os processadores e o FPGA é realizada através de leituras e gravações pelo programa em C neste arquivo.

Com a realização de uma leitura ou escrita, o FPGA recebe, através do RapidArray Transport Core, uma requisição que deve ser tratada pela aplicação do FPGA.

Em caso de leitura, deverá ser retornado um valor ao RapidArray Transport Core para que ocorra o retorno da função chamada pelo programa em linguagem de alto nível.

Somente é permitida a manipulação de um quadword (64 bits) por requisição utilizando o bloco Fabric Request.

A abordagem que utiliza o bloco User Request é conhecida como pull e, diferentemente da Fabric Request, mantém os processadores livres durante a transferência de dados entre o programa em C e o FPGA.

Para isso, este bloco permite que o FPGA realize leituras e escritas em um espaço de memória compartilhado do programa.

O endereço para a região de memória, que é compartilhada utilizando a einlib, é enviado ao FPGA através do bloco Fabric Request.

Estando disponível o endereço, a aplicação descrita para o FPGA é capaz de fazer até 32 requisições seqüenciais ao RapidArray Transport Core.

Cada requisição pode solicitar até 8 posições contíguas da memória do programa.

Os retornos das solicitações ao RapidArray Transporte Core não são necessariamente na ordem em que foram realizadas.

O core garante somente a ordem das 8 posições contíguas de cada requisição.

O sistema descrito para o FPGA deve ordenar os dados através do auxílio de tags disponibilizadas durante a requisição e o retorno.

Outra solução é aguardar o retorno de uma solicitação antes de realizar outra.

Para medir a taxa de transferência de dados entre os processadores e o FPGA, implementou-se duas descrições em VHDL e um programa em linguagem C.

Cada descrição em VHDL utiliza uma das duas abordagens de comunicação.

Para a abordagem push, a descrição para FPGA faz uso do bloco Fabric Request e realiza a manipulação das requisições de leitura e escrita através de uma máquina de estados.

Para a abordagem pull, a aplicação descrita utiliza os dois blocos disponíveis no RapidArray Transport Core.

Isto é necessário pois o envio do endereço da região de memória compartilhada é feito através do bloco Fabric Request, e a transferência de dados utiliza o bloco User Request.

Esta implementação utiliza uma máquina de estados para tratar o recebimento do endereço de memória e outra que faz as requisições de dados e manipula as respostas do RapidArray Transport Core.

Esta máquina aguarda o retorno de uma requisição antes de realizar outra.

Este método foi utilizado visando simplificar a descrição do sistema e evitar que uma maior area do FPGA seja utilizada para o tratamentó de comunicação.

A função principal do programa em linguagem de alto nível é realizar a carga dos arquivos de configuração no FPGA e calcular o tempo de execução dos testes de transferência de cada abordagem.

Para cada forma de comunicação implementou-se uma função que realiza a transferência dos dados.

Na abordagem push a função envia os dados para o FPGA através de sucessivas gravações.

Para a abordagem pull o programa compartilha a região de memória, envia o endereço para o FPGA, envia o total de requisições que o FPGA deverá realizar e aguarda o fim da transferência dos dados.

O tempo de compartilhamento de memória foi considerado na obtenção dos tempos.

A avaliação da comunicação entre os processadores do Cray XD1 e os FPGAs é baseada na tomada de tempos de execução das transferências de dados utilizando as duas abordagens.

Nesta avaliação não são considerados os tempos de carga dos binários de descrição de hardware no FPGA.

Para avaliar diferentes situações de transferência de dados aos FPGAs, realizou-se experimentos que variam a quantidade de dados de 8 B até 2 MB.

Em outro estudo que avalia a comunicação com FPGAs no XD1, trabalha-se com quantidades de dados maiores, que variam de 100 MB a 500 MB.

Tal estudo é direcionado a uma aplicação que executa inteiramente em FPGA.

No caso de aplicações híbridas, no entanto, a quantidade de dados transferidos pode ser menor, o que justifica a delimitação dos experimentos no presente trabalho.

O tempo de cada experimento foi obtido pela média de 10 execuções.

A quantidade de dados transferidos, os tempos médios das execuções e os desvios padrão são apresentados.

Podem ser vistas as curvas formadas entre o tempo e o total de dados transferidos das duas abordagens implementadas.

A abordagem push foi mais rápida para transferências com até 1 KB.

Nos demais experimentos a abordagem pull foi mais rápida em relação à push.

Pode-se observar um crescente distanciamento das curvas, fato este que pode estar ligado ao tempo constante no envio do endereço para o FPGA e ao tempo de compartilhamento de memória, que em média foi de 939,1 µs.

Este tempo fixo torna-se cada vez menos significativo com o aumento dos dados transferidos.

A maior taxa de transferência foi encontrada no experimento que utiliza a abordagem pull com 2 MB e é de 87,5 MB/s.

Essa taxa está muito abaixo da máxima téorica especificada pela Cray 1,6 GB/s.

Em experimentos de gravação e leitura realizados em um Cray XD1 utilizando o bloco User Request foi possível transferir 500 MB em 1 s.

Esta maior taxa pode ser justificada pela utilização de 32 requisições seqüenciais de 8 posições contíguas de memória e a realização de leituras e escritas simultaneamente.

Com a comparação da transferência de dados usando as abordagens push e pull, verificouse que o segundo método possui melhor desempenho, em termos de tempo, em relação ao primeiro, para transferências superiores a 16 KB.

A transferência de dados no sentido processador-FPGA chega a ser 7 vezes mais rápida usando a interface User Request em relação à Fabric Request.

Entretanto, para a transferência de poucos dados, é preferível o uso da abordagem push.

Mesmo assim, a taxa de transferência está muito aqúem da máxima téorica, sugerindo que o estudo deva ser continuado, buscando-se formas mais eficientes de implementar a comunicação usando os recursos disponíveis.

Tempos de comunicação.

Tempos de comunicação.

Este trabalho propõe uma avaliação paralela de uma extensão do modelo dinâmico baseado em autômatos celulares para a simulação de incêndios em florestas de Chopard.

Mais especificamente, buscou-se atender a condições mais detalhadas de tipo de vegetação e outros aspectos ambientais, como corpos d'agua, estradas e areas urbanas, que não estavam presentes no modelo de Chopard originalmente.

O modelo de propagação de incêndio florestal em questão é um autômato celular probabilístico tridimensional.

A evolução do modelo é feita de acordo com regras pré-definidas, que são aplicadas paralelamente em toda a malha.

Os autômatos celulares são modelos matemáticos simples de sistemas naturais.

Eles são constitúidos de uma malha, ou reticulado, de células idênticas e discretas, onde cada célula tem seu valor sobre um conjunto finito.

Os valores evoluem, em passos de tempo discretos, de acordo com regras determinísticas que especificam os valores de cada célula em termos dos valores das células vizinhas.

A vizinhança de cada célula pode consistir, de 5 células, ela mesma e as quatro vizinhas acima, abaixo, à esquerda e à direita, esta vizinhança bidimensional é chamada de vizinhança de Von Neumann, e, de 9 células, a vizinhança de Von Neumann mais as células nas diagonais, é chamada de vizinhança de Moore.

Como exemplo de autômato celular, considera-se uma linha (vetor) de células com valores 0 ou 1.

O valor de uma célula na posição i no tempo t é a.

Uma regra muito simples para a evolução no tempo dos valores das células e, onde mod 2 indica que é tomado o resto 0 ou 1 da divisão por 2.

De acordo com esta regra, o valor de uma célula em particular é dada pela soma módulo 2 (ou, equivalentemente ao ou exclusivo da algebra Booleana) dos valores de suas células vizinhas à esquerda e à direita no passo de tempo anterior.

Esquema típico de um Automato Celular.

Mostra o padrão gerado pela evolução após 500 passos de tempo utilizando a regra acima a partir de uma semente consistindo de apenas uma célula com o valor 1 e todas as demais com o valor 0.

Evolução apos 500 passos.

O presente trabalho é uma extensão do modelo dinâmico baseado em autômatos celulares para a simulação de incêndios em florestas (Forest Fire Model), como o modelo de Chopard.

Além disso, propõe-se um autômato celular probabilístico tridimensional, que simula incêndios em areas florestais e urbanas, utilizando-sé a vizinhança de Moore.

Originalmente, o modelo de propagação de incêndio em florestas seguia um autômato celular probabilístico cujas regras simulavam incêndio e crescimento da vegetação.

As regras eram as seguintes, uma arvore em chama, torna-se uma célulá vazia (solo exposto).

Uma arvore torna-se uma arvore em chama se pelo menos umá das suas vizinhas mais próximas está em chama.

Uma arvore cresce com probabilidadé p em uma célula vazia.

Uma arvore sem uma vizinha próxima em chamas torna-sé uma arvore em chamas com probabilidade f (combustão espontânea).

Nesta proposta, os estados que cada célula pode atingir compreendem, i) casa (área urbana).

Arvore (área florestal), agua, estrada, solo exposto e vi) fogó (incêndio).

Para fins de experimentação do modelo, o estado inicial escolhido para o autômato é definido a partir de uma situação sintética.

Definiu-se uma "estrada" fixada na malha do autômato.

Além disso, definiu-se um "lago" em um dos lados do autômato e as demais células são preenchidas aleatoriamente, podendo ser, casa, arvore ou solo.

O autômato evolui, ou seja, os estados das células são alterados conforme o estado de suas células vizinhas, de acordo com as regras determinísticas já definidas no modelo.

A linguagem de programação C++ foi utilizada para a implementação do ambiente de simulação, de modo orientado a objetos.

Juntamente com o ambiente de simulação foi desenvolvido um ambiente de visualização 3 D para uma melhor interpretação dos estados das células.

Esse ambiente foi desenvolvido utilizando a API OpenGL.

As regras que permitem a evolução do modelo de autômato celular são aplicadas paralelamente em toda a malha com a utilização de trocas de mensagens, possibilitando um alto desempenho quando da computação do modelo.

Na implementação deste trabalho, utilizou-se MPI (Message Passing Interface), sendo a plataforma alvo deste trabalho ambientes de memória distribúida do tipo agregado de computadores.

A exploração do paralelismo foi modelada considerando os seguintes componentes, um frontend responsável pela execução do ambiente de visualização 3 D dos resultados obtidos pelo modelo, uma infraestutura do tipo cluster onde ocorre a computação paralela do modelo.

A comunicação entre o frontend e cluster é realizada utilizando como base o modelo de comunicação cliente/servidor.

O frontend faz o papel de um servidor, que a cada novo estado da malha recebido promove a sua exibição, caracterizando para o usúario final a evolução da computação como um todo.

Por sua vez, o cluster faz o papel de um cliente, aplicando paralelamente as regras do autômato no estado atual da malha, gerando um novo estado.

Na computação paralela do modelo proposto é empregado o modelo mestre/escravo onde, uma máquina que funciona como mestre (ou root), divide a malha em pequenas partes e essas partes são distribúidas para que outras máquinas do cluster apliquem as regras do modelo na malha recebida, as malhas atualizadas em cada máquina escrava são enviadas a máquina mestre que as coleta, recompõe e envia ao frontend para que apresente a evolução da computação.

O tamanho das partes em que a malha vai ser dividida depende da quantidade de máquinas que o usúario escolherá para que o sistema seja executado, essa quantidade pode ser alterada a cada execução do sistema.

Como o modelo paralelo funciona como mestre/escravo, essa quantidade nao deve ser inferior a dois, onde uma máquina executará o papel de mestre e a outra ficará responsável por aplicar as regras em toda a malha, não funcionando assim de modo paralelo.

Por essa razão o recomendado é uma execução com no mínimo três máquinas, onde já se pode obter paralelismo na aplicação das regras na malha, havendo assim ganhos computacionais.

Este trabalho é uma proposta de extensão do modelo dinâmico baseado em autômatos celulares para a simulação de incêndios em florestas de Chopard.

Mais especificamente, buscou-se a extensão desde modelo para atender a condições mais detalhadas de tipo de vegetação e outros aspectos ambientais, como corpos d'água, estradas e areas urbanas, que não estavam presentes no modelo de Chopard originalmente.

O modelo de propagação de incêndio florestal está definido por um autômato celular probabilístico tridimensional síncrono, e por essas características, é razóavel imaginar o modelo como um programa paralelo e distribúido.

Testes preliminares apontaram que é possível esperar significativos ganhos de desempenho, a continuidade do trabalho prevê medições de speedup e eficiência considerando diferentes condições operacionais.

Pretende-se futuramente fazer simulações mais realistas partindo de informações geográficas referenciadas e planos de informações obtidos a partir de sistemas de informações geográficas.

Durante os anos 70 até meados dos anos 90 já existiam algumas implementações envolvendo arquitetura a fluxo de dados, cuja principal diferença em relação à tradicional arquitetura de Von Neumann é que esta é baseada no controle seqüencial, enquanto que nas máquinas a fluxo de dados o controle é executado pela presença dos dados.

A representação dessa arquitetura é feita por meio de um grafo a fluxo de dados, que é um conjunto de operadores interligados por arcos, onde a presença de dados em cada arco de um operador irá disparar sua execução.

Existem dois tipos de arquiteturas a fluxo de dados, estática e dinâmica.

No modelo estático apenas um dado está presente em um arco a espera de seus parceiros para completar a execução.

Um protocolo deve garantir o sincronismo entre os operadores para garantir a permanência de apenas um dado por arco, limitando o paralelismo ao conjunto de operadores cujos arcos possuem apenas um dado.

No modelo dinâmico, vários dados podem estar presentes em um mesmo arco.

Nesse caso o protocolo deve garantir que as operações só sejam disparadas quando todos os dados necessários estejam presentes nos arcos.

A metodologia "Tagged token" pode ser utilizada no protocolo para controlar os dados parceiros em cada arco.

Neste modelo, o paralelismo acontece quando da execução dos vários operadores cujos dados estejam todos disponíveis.

A computação reconfigurável, por sua vez, ainda é uma área em desenvolvimento e a validação de novos e diferentes tópicos nessa arquitetura tem sido um desafio dos pesquisadores da área nos últimos 20 anos.

O que tem tornado a computação reconfigurável uma realidade são os FPGAs, já que inúmeros sistemas com aplicações específicas e complexas vêm utilizando essa tecnologia em seus projetos, pois ela reduz os riscos de desenvolvimento e ao mesmo tempo proporciona flexibilidade para correções e atualizações de forma rápida e segura.

A Ferramenta ChipCFlow trata-se de uma ferramenta de compilação que executa algoritmos através do Modelo de Arquitetura a Fluxo de Dados associado ao conceito de arquiteturas reconfiguráveis, com o objetivo de acelerar a execução de programas de aplicação escritos em C, aproveitando ao máximo o paralelismo considerado natural do modelo a Fluxo de Dados e das características do hardware reconfigurável.

Para se obter o máximo da arquitetura a Fluxo de dados dinâmica, é necessário dispor de organização de memória que forneça e receba dados, na velocidade de trabalho da arquitetura a Fluxo de Dados.

A seguir é descrita a proposta do modelo de organização de memória sendo desenvolvido para o projeto.

A arquitetura a fluxo de dados é baseada em operadores que executam paralelamente realizando operações aritméticas, lógicas e de controle.

Como o próprio nome já menciona diferentemente das arquiteturas tradicionais, na arquitetura a fluxo a dados são os dados que definem o ritmo e as características de execução que nas arquiteturas tradicionais são tratadas por diretivas de controle.

Por isso, uma demora no fornecimento de dados afeta drasticamente toda a execução do programa, acarretando uma gigantesca perda de performance.

Em particular, no projeto ChipCFlow, um operador roda a 6 ns, e para que a organização de memória seja transparente para o operador, esta deve fornecer dados em tempos inferiores a 6 ns.

Além disso, como qualquer aplicação da ferramenta é composta por diversos operadores de entrada, devemos supor diversas entradas em paralelo, nas quais os dados devem ser fornecidos na forma de rajada de dados.

Representação da Memória de Entrada abastecendo múltiplos operadores.

Representação da Estrutura de Memória.

Como as entradas são definidas a partir do programa C, a organização de memória também deve ser passível de reestruturação em tempo de execução.

Visando atender primeiramente aos requisitos de tempo e de múltiplas entradas, como proposta inicial optou-se por uma estrutura extremamente simples.

Na intenção de diminuir ao mínimo o tempo gasto no acesso a memória, optou-se por não utilizar nenhuma unidade de controle complexa.

Decidiu-se então definir a seguinte estrutura, n um banco de 2 registradores de 32 bits selecionados através de um contador de n bits e n um multiplexador de 2 entradas de 32 bits.

Dessa forma, o tempo gasto pela memória fica restrito apenas ao tempo de comutação do multiplexador.

Para lidar com múltiplos operadores recria-se a estrutura acima mencionada para cada uma das duas entradas de cada operador.

Porém, como a implementação é baseada em FPGA, dispõem-se apenas da capacidade interna do FPGA para construir os registradores, temos então que n é inversamente proporcional ao número de entradas, por isso o valor de n deve ser definido de acordo com o número de operadores de entrada e de acordo com a capacidade do FPGA utilizado.

Dada a importância da memória para a ferramenta, bem como a grande quantidade de recursos do FPGA por ela ocupada, optou-se por estruturar a memória e a lógica da ferramenta (motor Dataflow) em FPGAs diferentes, a fim de permitir uma maior quantidade de memória e total independência de funcionamento das duas partes.

Isso pode ser melhor observado sendo o FPGA 1 o que contém a síntese de memória e o FPGA 2 o que contém o motor Dataflow da aplicação.

Dessa maneira, a memória inicialmente é carregada a partir de um dispositivo de armazenamento secundário (Hard Disk, Compact Flash) e somente depois inicia o fornecimento de dados para a ferramenta.

Neste artigo foi descrito o projeto ChipCFlow, uma ferramenta para execução de algoritmos escritos originalmente em C e convertido no modelo a Fluxo de Dados a ser executado diretamente em hardware reconfigurável e os requisitos necessários para a implementação de uma memória que seja capaz de responder na velocidade dos operadores em rajadas de dados, à cada 6 ns, que é o tempo de processamento de cada operador no projeto ChipCflow.

O projeto encontra-se em desenvolvimento e novos resultados deverão ser publicados brevemente.

Os aglomerados de computadores tornaram-se arquiteturas populares no desenvolvimento do Processamento de Alto Desempenho (PAD) devido, principalmente, a sua facilidade de aquisição e o poder de processamento proporcionado em relação aos supercomputadores.

Ferramentas clássicas utilizadas para programação de aglomerados empregam troca de mensagens, tanto síncronas quanto assíncronas.

Para obtenção do PAD deve-se, portanto, minimizar os custos com a comunicação entre os nós da arquitetura.

O modelo de Mensagens Ativas propõe a realização da comunicação com pequeno custo.

Isto é feito através da sobreposição de computação e comunicação com o uso de um processador em hardware dedicado à comunicação.

Esta sobreposição pode ser alcançada em software através do uso de multithreading na simulação do processador de comunicação.

Neste contexto, este trabalho apresenta uma biblioteca de comunicação desenvolvida em software, a qual implementa o modelo de Mensagens Ativas e simula o processador dedicado à comunicação através do uso de multithreading.

Na seqüência, a Seção 2 apresenta o modelo de Mensagens Ativas.

Já a Seção 3 apresenta a biblioteca desenvolvida e sua interface de programação aplicativa.

Na Seção 4 são discutidos resultados de desempenho da biblioteca e a Seção 5 conclui o artigo.

Uma aplicação implementada para executar em arquiteturas como aglomerados e que vise alto desempenho, deve buscar minimizar o tempo entre a produção de um dado e seu consumo.

Nesse tipo de arquitetura, produção e consumo podem ocorrer em nós diferentes, o que exige troca de mensagens entre as partes envolvidas.

Uma solução eficiente para implementação de troca de mensagens entre os nós de um aglomerado consiste na utilização do modelo assíncrono denominado Mensagens Ativas que, normalmente, está entre os mecanismos de comunicação mais rápidos disponíveis.

Nesse modelo, a mensagem contém em seu cabeçalho a identificação de um serviço a ser executado pelo nó receptor e em seu corpo os dados que compõem os argumentos.

A idéia é que o transmissor envie uma mensagem para a rede e continue seu processamento.

A recepção de uma mensagem dispara um serviço a ser executado pelo receptor.

O modelo original de Mensagens Ativas foi desenvolvido utilizando um processador, em hardware, dedicado à comunicação e funcionava de duas maneiras, com uma política de interrupção, onde o processo em execução era interrompido com a chegada de uma nova mensagem, e com uma política de polling, onde o processador de comunicação verificava constantemente a interface de rede.

Como este modelo apresentava problemas em relação a operações de sincronização, desenvolveram-se implementações alternativas, UpCall, PopUp e Fila de Execução.

O modelo UpCall caracteriza-se por possuir um thread independente para recebimento e tratamento das mensagens.

Já o modelo PopUp caracteriza-se pela criação de um novo thread a cada recebimento de mensagem.

Ambos os modelos podem funcionar tanto com a política de interrupção quanto com a política de polling.

O modelo de Fila de Execução é um modelo híbrido entre UpCall e PopUp, com o objetivo de minimizar os problemas encontrados nestes.

Caracteriza-se por um daemon de comunicação que é responsável por verificar a interface de rede e no recebimento de uma nova mensagem, inseri-la em uma lista de tarefas.

Esta lista é constantemente verificada pelos threads executando no nó da arquitetura, com o objetivo de executar rapidamente as tarefas descritas pela aplicação.

A biblioteca foi desenvolvida na linguagem de programação C e fez uso do padrão de comunicação MPI para troca de mensagens.

Optou-se pelo uso de MPI, pois este oferece um nível mais alto de abstração em relação a outras ferramentas como o padrão sockets.

O formato das primitivas da interface seguiu o seguinte padrão, os acrônimos act e msg separados pelo caractere _' e a identificação, em inglês, do serviço realizado pela primitiva.

Podem ser conferidas as principais primitivas desenvolvidas.

Além destas ainda existem, av_init, av_finalize, act_msg_realloc e act_msg_getserv.

Uma aplicação que fará uso da biblioteca deverá definir serviços.

Sua implementação se dá na forma de funções sem retorno que recebem como parâmetro um ponteiro para uma área de memória.

Estes serviços não devem realizar operações de sincronização ou serem muito extensos, pois executam no corpo do processador de comunicação.

Além disso, devem ser registrados em cada nó da arquitetura através da primitiva específica da biblioteca para que os endereços dos serviços na memória de cada nó sejam abstraídos em índices de seus endereços em um vetor de serviços.

O processador de comunicação é executado, no nó zero (0) da arquitetura virtual MPI, em um thread criado no processo de inicialização da biblioteca.

Nos demais nós, o processador de comunicação é executado sobre o fluxo de execução principal.

UpCall com a política de polling foi o modelo seguido para implementação deste processador.

A obtenção dos resultados de desempenho foi realizada, localmente, utilizando um computador com processador Intel Core Duo com freqüência de clock 1,66 GHz, memória cache de 2 MB e memória RAM de 2,5 GB.

Para realização dos testes em um ambiente de aglomerado foi conectado, através de uma interface de rede Ethernet de 100 Mbit/s, um computador com processador Intel Pentium 4, com freqüência de clock 1,70 Ghz, memória cache de 256 KB e memória RAM de 512 MB.

O sistema operacional utilizado foi o GNU/Linux, versão do kernel 2 6 24-21 e a versão da biblioteca Open MPI utilizada foi a de número 1 2 6 A metodologia de avaliação de desempenho consistiu em contabilizar o tempo de execução das operações básicas realizadas pela biblioteca.

Entre as operações avaliadas estão as de empacotar e desempacotar um dado do tipo int com o uso das primitivas pack/unpack, a de criação (create) de mensagens de tamanho 1024 e 2048 bytes e a de envio (send) de mensagem com um dado do tipo int empacotado na área de dados e fazendo uso de um serviço vazio.

Além destas, utilizou-se um serviço exemplo tipo ping-pong, onde um nó virtual desempacota um dado, adiciona um valor ao dado, empacota-o e reenvia para o nó transmissor.

Foram obtidas as médias de 100 operações na operação de pack/unpack, create e send.

Para o serviço ping-pong foi obtida a média de 20 pares de operações ping-pong.

As operações de pack/unpack e create foram avaliadas somente localmente, com a utilização de somente um nó virtual MPI, enquanto que a operação de send e o serviço exemplo ping-pong foram avaliados tanto localmente quanto em um ambiente de aglomerado com o uso de dois nós virtuais MPI.

Pode-se conferir o tempo de execução médio das operações de Pack e Unpack.

A análise quantitativa destes resultados permitiu verificar que são satisfatoriamente relevantes aos objetivos procurados.

Tempo de Execução Médio das Operações Pack/Unpack.

Pode-se conferir o tempo de execução médio da operação de criação de mensagens tanto com 1024 bytes quanto com 2048 bytes.

Os resultados mostraram que o tempo de criação de uma nova mensagem varia linearmente de acordo com o tamanho da mensagem criada.

Tempo de Execução Médio da operação de criação de mensagens.

Pode-se conferir o tempo de execução médio da operação de envio de mensagens tanto localmente quanto remotamente.

Novamente, os resultados mostraram-se satisfatórios em relação aos resultados esperados, apesar de mostrar que o desempenho em rede foi menor que o desempenho local.

Tempo de Execução Médio da operação de envio de mensagens.

Pode-se conferir o tempo de execução médio do serviço exemplo ping-pong.

Este serviço permitiu verificar que o desempenho das diversas operações necessárias para o envio de uma mensagem estava, assim como o desempenho da biblioteca desenvolvida, dentro das expectativas.

Tempo de Execução Médio do serviço exemplo ping-pong.

Com o uso da biblioteca, o programador pode desenvolver aplicações voltadas ao processamento de alto desempenho utilizando o modelo de Mensagens Ativas sem depender de hardware específico e, ainda assim, obter desempenho semelhante.

Como trabalhos futuros, tem-se a inclusão da biblioteca no ambiente de programação paralela Anahy, permitindo estender este ambiente para arquiteturas com memória distribuída e permitir que estas façam uso do mecanismo de escalonamento disponível em Anahy.

Cada vez mais as aplicações computacionais precisam de maior poder de processamento para serem executadas.

Uma maneira de conseguir esse grande poder computacional é utilizando o processamento paralelo.

Uma motivação para a utilização dessa técnica é a popularização das máquinas multicore e também a possibilidade de se trabalhar com clusters, que são um conjunto de máquinas interligadas, trabalhando como um computador de grande porte.

Porém, desenvolver aplicativos que explorem o paralelismo é mais complicado que a programação convencional.

Programas paralelos precisam se preocupar com a criação dos processos e também em como esses processos serão manipulados.

Tendo em vista os desafios de desenvolver programas paralelos e visando ajudar o programador, busca-se alternativas para facilitar essa programação.

Uma alternativa é a utilização de Esqueletos de Paralelismo.

Com o objetivo de facilitar o desenvolvimento de aplicações paralelas, este artigo apresenta a implementação de dois esqueletos de paralelismo na linguagem Erlang, o esqueleto Map Paralelo e o esqueleto Divisão e Conquista.

Para testar os esqueletos, foram desenvolvidas duas aplicações simples, o cálculo da aproximação do Pi com o método de Monte Carlo e o cálculo do Fibonacci.

Os resultados da execução dessas aplicações em uma máquina com processador de oito núcleos são apresentados e discutidos.

Esqueletos de paralelismo são funções de alta ordem que encapsulam padrões recorrentes de paralelismo.

Eles são utilizados para resolver problemas rotineiramente encontrados quando se desenvolve programas para execução paralela.

Utilizando esqueletos de paralelismo, o programador pode desenvolver programas paralelos sem se preocupar em como os processos serão criados e escalonados.

Erlang é uma linguagem funcional desenvolvida pela Ericson para ser utilizada em aplicações distribuídas e tolerante a falhas.

A linguagem se destaca pela facilidade de criar processos e fazer a comunicação entre eles utilizando troca de mensagens.

Os processos são fortemente isolados uns do outros, pois não existe compartilhamento de recursos entre eles.

As principais primitvas para criação de processos e comunicação em Erlang são, l spawn, serve para criar processos, recebe como argumentos o módulo onde está definida a função que será executada, a função a ser executada e os argumentos que a função receberá.

Como resposta, retorna o identificador do processo criado, para enviar uma mensagem para um processo utiliza-se o caracter após o identificador do processo, e passa-se a mensagens entre chaves.

Receive, para que um processo possa receber uma mensagem, ele deve conter a primitiva receive.

A mensagem será recebida entre chaves após a primitiva receive.

Após a seta, a mensagem pode ser manipulada.

Por essa facilidade de criar e manipular processos, a linguagem funcional Erlang foi escolhida para desenvolver os Esqueletos de Paralelismo apresentados neste trabalho.

Ao se trabalhar com linguagens funcionais, uma função muito utilizada é a Map.

Seus argumentos são uma função e uma lista de elementos.

A Map aplica a função a cada elemento da lista e devolve uma lista de resultados.

No esqueleto Map Paralelo é criada uma lista de processos, um para cada elemento da lista.

Cada processo receberá a função e um elemento da lista, aplicará a função ao elemento, devolvendo o resultado da avaliação.

Ao final, os resultados das avaliações são combinados em uma lista de respostas.

Divisão em processos e combinação das respostas em uma lista.

Para testes do esqueleto Map Paralelo, foi utilizado o cálculo da aproximação do Pi utilizando o método de Monte Carlo.

Esse método consiste em gerar coordenadas aleatórias dentro de um quadrado com um círculo inscrito dentro dele.

Forma onde são geradas coordenadas aleatórias.

O cálculo do Pi é dado pela fórmula, 4*(pontos que incidem no círculo / total de pontos) O esqueleto Map Paralelo foi testado no método de Monte Carlo para o cálculo da aproximação do Pi rodando em uma máquina com processador de oito núcleos.

Foram feitos testes limitando o número de núcleos do processador, conseguindo ganhos de desempenho para todas as configurações.

Ganho de desempenho utilizando esqueleto Map Paralelo.

A Divisão e Conquista consiste em pegar um grande problema com solução complexa e dividi-lo em vários pequenos problemas de fácil solução.

O esqueleto Divisão e Conquista testa se o problema é divisível.

Se for, ele aplica uma função que cria uma lista de pequenos problemas.

Então, utilizando o esqueleto Map Paralelo, é aplicada a função solução em cada problema que compõe a lista, gerando uma lista de soluções.

Essa lista de soluções é combinada para gerar a solução final.

No algoritmo utilizado para calcular o fibonacci de um número usando o esqueleto Divisão e Conquista, a função que divide o problema "abre" a árvore para o cálculo do fibonacci até que a base tenha o número de elementos igual ao número de máquinas ou processadores em que a aplicação for rodar.

Utilizando o esqueleto Map Paralelo é criado um processo para cada elemento da base e é executada a função fibonacci convencional em cada processo, retornando uma lista de resultados.

Árvore para o cálculo do fibonacci.

Na lista de resultados, é aplicada uma função que une as respostas.

Essa função soma as respostas aos pares recursivamente, até sobrar um elemento, que é a solução final.

O cálculo do fibonacci com o esqueleto Divisão e Conquista conseguiu um ganho de desempenho, mas devido ao desequilíbrio no tamanho dos processos, uns processadores ficaram ociosos enquanto outros ainda estavam trabalhando.

Isso causou um speedup relativamente baixo, 1,18 com 2 processadores.

É seqüência do projeto efetuar mais testes com variações do algoritmo para o cálculo do fibonacci, buscando um melhor equilíbrio no tamanho dos processos.

Com base nos resultados obtidos nos testes com os esqueletos de paralelismo implementados nesse trabalho, observa-se o ganho de desempenho na execução das aplicações do cálculo da aproximação do valor do Pi com o método de Monte Carlo e no cálculo do valor fibonacci em máquinas multicore, além de simplificar o desenvolvimento das aplicações testadas.

A linguagem Erlang, por sua sintaxe simples e facilidade na criação e manipulação de processos, se mostrou muito favorável na implementação de esqueletos de paralelismo para máquinas multicore.

Aseqüência deste trabalho é desenvolver esqueletos de paralelismo direcionados a clusters, fazendo o escalonamento dos processos entre as máquinas que o compõem.

Também, desenvolver os esqueletos de paralelismo em linguagens mais populares e de alto nível, como por exemplo, Java.

Algoritmos de escalonamento de lista são citados na literatura como eficientes para escalonamento de grafos de fluxos de dados em arquiteturas multiprocessadas.

Estes algoritmos procuram otimizar o tempo de execução de uma aplicação partindo do conhecimento do caminho crítico para execução das tarefas deste grafo.

Existem diversas estratégias de escalonamento de lista aplicadas a grafos estáticos, algumas destas documentadas em Adam.

Estas técnicas baseiamse em valorar as tarefas do grafo segundo alguma política de prioridade e no uso de uma lista ordenada (segundo o critério de prioridade adotado) para controlar a ativação da execução destas.

Em ambientes como Cilk, onde o grafo é gerado de forma dinâmica, variantes de algoritmos de lista alcançam bons índices de desempenho.

Em particular, Cilk utiliza com sucesso a técnica de roubo de trabalho (work stealing) em arquiteturas multiprocessadas.

Devese observar que, em Cilk, técnicas de implementação do núcleo de execução foram desenvolvidas para permitir a execução de um subconjunto de grafos de fluxo de dados, uma vez que a interface de programação deste ambiente permite apenas a descrição de paralelismo aninhado.

Este artigo apresenta um estudo sobre algoritmos de escalonamento de lista voltado a identificar estratégias de execução aplicáveis a ambientes multithread dinâmicos.

O objetivo é desenvolver estratégias de escalonamento de threads quando estas reagrupam seqüências de tarefas descritas em termos de fluxos de dados.

Os resultados serão aplicados no núcleo de escalonamento de Athreads.

Programas paralelos podem ser descritos em termos de um grafo de fluxo de dados entre tarefas, ou, simplesmente, DAG (Directed Acyclic task Graph).

Em um DAG, cada nó representa uma tarefa e cada aresta entre dois nós representa dependência de dados entre tarefas.

Escalonar um DAG consiste em, dado um conjunto finito de processadores, determinar a seqüência de tarefas que cada processador deve executar em um determinado instante de tempo.

Grande parte dos algoritmos de escalonamento é baseada no escalonamento de lista.

Genericamente, esta classe de algoritmos consiste de dois passos, no primeiro passo as tarefas do grafo são classificadas de acordo com alguma política de prioridade e as mesmas tarefas são inseridas em uma lista ordenada por ordem decrescente de prioridade das tarefas, no segundo passo, as tarefas são mapeadas para os processadores disponíveis, levando respeitando a prioridade de execução delas.

Os atributos de prioridade permitem identificar o caminho crítico da execução de tarefas da aplicação descrita no DAG.

Dois dos mais importantes atributos para conferir prioridade às tarefas do DAG são contemplados pela ferramenta, o t_level (top level) e o b_level (bottom level).

O t_level de um nó Ni é o tamanho do maior caminho no DAG, partindo de um nó de entrada até Ni (excluso).

Nesta abordagem, o tamanho do caminho entendese pela soma dos custos de todos os nós e arestas ao longo do caminho.

O b_level de um nó Ni é o tamanho do maior caminho no DAG partindo de Ni até um nó de saída.

Para a ferramenta, o b_level é um atributo estático relacionado com o caminho crítico do DAG enquanto o t_level é um atributo dinâmico, pois é atribuído depois das tarefas serem escalonadas nos processadores, o que pode fazer com que custos entre tarefas escalonadas em um mesmo processador sejam ignorados.

O estudo das estratégias de escalonamento foi realizado no contexto do projeto Anahy.

Anahy propõe a utilização de um modelo de programação concorrente que permita descrever uma aplicação em termos de um grafo de fluxo de dados, composto por tarefas e identificação de dependências de dados entre tarefas, associado a um modelo de execução baseado em multithreading habilitado a limitar o número de execuções paralelas de tarefas em função dos recursos da arquitetura.

Athreads é uma implementação de Anahy, na qual tarefas são encapsuladas no contexto de threads e, no suporte a execução, estes threads são executados sobre processadores virtuais.

Representa um DAG gerado pela execução de um programa desenvolvido em Athreads.

Observase que os threads são containers de tarefas, desta forma, o núcleo de execução é capaz de manipular um grafo de threads, reduzindo o sobrecusto de manipulação de um grande número de dependências entre tarefas.

Como conseqüência, as estratégias de escalonamentos de lista devem ser adaptadas para obter o melhor proveito desta representação.

Para compreender e adaptar estes algoritmos para Athreads, foi desenvolvido um simulador de estratégias de escalonamento de lista.

Este simulador é composto de duas partes principais, um gerador de grafos e um aplicador de estratégias de escalonamento.

Importante observar que o grafo gerado representa a estrutura implementada em Athreads, sendo as tarefas encapsuladas em threads.

Neste grafo, o DAG é descrito pelo relacionamento entre tarefas, sendo que dependências entre tarefas de threads distintos representam as criações e sincronizações (por join) entre threads.

A geração do grafo requer parametrização, de forma a caracterizar a estrutura de programa a ser avaliada.

São quatro os parâmetros necessários, profundidade (P), indicando a profundidade, em termos de threads, que o grafo deve possuir, comprimento (K), número de tarefas que cada thread deve conter, custo de computação (w) e (iv) custo de comunicação, indicando, respectivamente, o tempo necessário para processamento de cada tarefa e para comunicação de seus resultados.

DAG para um programa desenvolvido em Athreads Observe que o comprimento mínimo de um thread é um  e que um thread de comprimento informado n terá, de fato, (2 n+1) tarefas, sendo que n tarefas terminam com a construção de um novo thread, outras n terminam sincronizando, individualmente, com os threads criados e a tarefa final representa a computação final do thread.

Um quinto parâmetro permite indicar diversas organizações para os threads, em função do espaço deste artigo, assumese uma organização sérieparalelo (forksjoins aninhados) dos threads.

A etapa de avaliação inicia valorando cada tarefa do DAG em termos dos respectivos t_level e b_level.

Na seqüência são identificados, para cada thread, os valores mínimos e máximos de t_level e b_level obtidos nas tarefas que estas contêm.

O passo seguinte consiste em aplicar uma estratégia de escalonamento de lista sobre o DAG e, por fim, indicar, considerando os t_level e b_level mínimos e máximos de cada thread, o escalonamento desejado para os threads.

Escalonamento do DAG com quatro processadores virtuais Enquanto apresenta o grafo gerado pelo simulador, onde os parâmetros utilizados foram, P = 3, K = 2, w = 1 e c = 0, apresenta o escalonamento obtido para o DAG e para o grafo de threads.

Em cada instante de tempo, o processador "Proci", executa um par "thread,tarefa".

Para realizar o escalonamento do DAG foi aplicado um algoritmo simples, o algoritmo DLS (Dynamic Level Scheduling), adaptado para este caso onde não há custos de comunicação entre tarefas.

Para o escalonamento dos threads foi aplicado o mesmo algoritmo, considerando os valores mínimos para t_level e b_level.

O resultado foi satisfatório visto que cada processador busca, em um determinado instante de tempo, a tarefa que representa menos custo do ponto de vista do escalonamento dos threads.

Por exemplo, nos instantes de tempo 5 e 6, onde há várias tarefas prontas para executar no mesmo instante, o processador Proc1 escolhe executar as tarefas 25 e 27 que possibilitam dar continuidade a execução do thread 10.

Como podese observar os processadores Proc2, Proc3 e Proc4 ficam ociosos em períodos de tempo onde não há tarefa pronta para execução.

A popularização de arquiteturas multiprocessadas, com o barateamento das configurações baseadas em processadores multicore, aumenta a demanda por processamento paralelo.

Neste tipo de arquitetura, dotada de memória compartilhada, o modelo de programação mais adequado é o baseado em multiprogramação leve.

Athreads é uma ferramenta de programação que representa este modelo de programação e ainda permite a descrição da concorrência em termos de um grafo de fluxo de dados.

Resultados de desempenho já documentados deste ambiente mostram sua eficiência, como pode ser visto em Cordeiro.

Com o presente trabalho, pretendese aplicar técnicas de escalonamento de lista dinâmicas no seu núcleo de execução.

O atual estágio de desenvolvimento contempla a introdução de estratégias de escalonamento de lista no simulador.

O trabalho prosseguirá com o estudo dos mapeamentos possíveis dos escalonamentos obtidos com estas técnicas para o nível de threads do grafo e, então, na proposição de estratégias dinâmicas de escalonamento de listas para threads e da incorporação destas no núcleo de execução de Athreads.

Os modelos matemáticos que simulam situações reais do sistema são bastante estudados e aplicados ao processamento de alto desempenho, por demandarem alto poder de processamento e pela possibilidade das equações possuírem um alto grau de paralelismo.

Pela flexibilidade, simplicidade de implementação será paralelizado nesse trabalho um algoritmo genético cuja aplicação está difundida nas mais diversas áreas da ciência.

Função de Avaliação, a função de avaliação, função objetivo ou função custo é quem determina o grau de aptidão (fitness) de um cromossomo (possível solução).

Nos AG todos os cromossomos, em cada geração, são submetidos à função objetivo para determinar o valor de fitness de cada indivíduo que definirá a probabilidade de seleção e reprodução para a geração seguinte.

Operadores Genéticos, São responsáveis pelas mudanças que ocorrem na população, ao passar de uma geração para outra, mudando algumas de suas características, os quais são, Seleção, Cruzamento (aritmético ou aritmético com os extremos) e Mutação (Uniforme ou não-uniforme).

Critério de Parada os AG são métodos iterativos de busca estocástica e, como tal, necessitam de um critério de parada para informar ao programa o momento de parar.

Esse critério pode ser o número máximo de iterações ou gerações da população, ou ainda, verificar a evolução do melhor indivíduo após determinado número de gerações.

Nesse algoritmo, a família inicial de cromossomos é gerada aleatoriamente e em seguida submetida à função de avaliação para determinar a aptidão ou fitness de cada cromossomo.

Depois é verificado se foi atingido o critério de parada, que normalmente está relacionado a precisão da resposta e ao número máximo de gerações.

Caso não seja satisfeito esse critério, entra-se no laço onde ocorrem o incremento das gerações e a aplicação dos operadores genéticos de reprodução, cruzamento e mutação para, em seguida, os cromossomos serem novamente submetidos à função de avaliação.

O ponto de partida nos AG é a representação de cada possível solução no espaço de busca (universo de soluções) como uma seqüência de símbolos pertencentes a um alfabeto finito U = {s, s, s }.

A cada seqüência de s corresponde a um cromossomo e cada elemento de s equivale a um gene.

O cluster computacional foi constituído no padrão Beowulf com computadores do tipo PC interligados por um switch central Fast Ethernet 100 baseT Full Duplex, onde uma das máquinas é o nó mestre e os demais são os nós escravos dedicados a execução das tarefas solicitadas pelo nó mestre.

O cluster é formado por 19 máquinas homogêneas com processadores Intel CoreTM 2 Duo E4600 240 GHz com 2 Mb de memória cache compartilhada, 2 GB de memória principal e adaptador de rede Gigabit Ethernet 1000 baseT Full Duplex.

Utilizou-se o sistema operacional GNU/Linux distribuição OpenSuse 102, com kernel versão 2 6 182-34-default x86, compilador GCC na versão 4 1 2 e o middleware MPICH na versão 1 2 7 p1.

A versão paralela do algoritmo foi desenvolvida na linguagem de programação C com base no algoritmo desenvolvido em MATLAB por e fazendo uso da biblioteca de troca de mensagens MPI.

Basicamente utilizou-se matrizes alocadas em tempo de execução do tipo double ** para armazenar as informações, sendo necessário dessa forma a conversão das informações de double ** para double * para serem enviadas através das funções de troca de mensagens.

A execução do AG no cluster começa pelo nó mestre com a carga dos parâmetros de entrada, a construção da matriz de repressores e a definição do valor de normalização.

Após definido esses parâmetros, as informações são replicadas para cada nó do cluster através de funções de comunicação coletivas.

Na seqüência cada nó do cluster gera aleatoriamente uma população inicial e a avalia, concentrando ao final desse processo as informações no nó mestre, para que nele sejam selecionados somente os indivíduos mais aptos.

A classificação do indivíduo quanto à aptidão é feita através de seu erro com relação aos dados de entrada, então o indivíduo que tiver menor erro esse será mais apto a continuar na evolução.

Essa população selecionada então é dividida para que cada nó do cluster dentro do laço das gerações aplique os operadores genéticos sobre os seus indivíduos.

Cada indivíduo ou cromossomo representa uma linha na matriz da população, dessa forma cada nó será responsável por processar um determinado número de linhas da matriz.

Ao final de cada iteração do laço das gerações, cada processo avalia sua população de cromossomos e envia para o nó mestre seu melhor indivíduo.

O nó mestre novamente avalia dentre todos os resultados recebidos qual é o melhor cromossomo e de posse desse o armazena como valor daquela geração, que mais tarde ao fim do laço das gerações todos os dados armazenados formarão o resultado do algoritmo genético.

Caso o melhor indivíduo dessa geração seja pior que o da anterior, então o cromossomo da geração anterior é copiado para a atual e o pior individuo é eliminada da população sendo substituído pelo da geração anterior.

Dessa forma ao fim do laço das gerações tem-se armazenados os melhores cromossomos evoluídos ao longo do tempo, formando a saída estimada do sistema que dever ser o mais próximo possível da saída real do modelo.

A análise de desempenho da implementação paralela do AG foi realizada através da execução paralela em diferentes números de nodos processadores, buscando obter a melhor relação de divisão de trabalho entre os nodos do cluster.

Os tempos representam uma média de cinco execuções e correspondem a configuração do AG com um população inicial de 2000 indivíduos e 200 gerações.

No Gráfico 1, 2 e 3 são apresentados respectivamente os tempos de execução em segundos, o speed-up e a eficiência obtidos variando-se o número de nodos do cluster.

Analisando os tempos de execução do algoritmo paralelo apresentados nos gráficos, observa-se um ganho significativo de tempo até as execuções com 10 nós.

A partir de 11 nós o tempo começa a se estabilizar havendo pouca diferença com acréscimo de mais nós.

Devido à limitação de 19 máquinas que compunham o cluster e ao tempo de execução do algoritmo ter diminuído até essa quantidade de nodos, não foi possível definir com quantos nós o tempo de execução começaria a aumentar devido ao tempo gasto com a comunicação.

O menor tempo de execução observado é de 17,5 segundos na utilização de 19 nodos.

Tomando como base o tempo do algoritmo seqüencial que é de 253,5 segundo, percebemos um uma redução de tempo muito próximo à décima quarta parte do tempo seqüencial, ou seja, um speed-up de 14,4 e uma eficiência de 76%.

O algoritmo se mostrou mais eficiente nas execuções em 8 e 9 nodos, atingindo um valor ideal de 100%, o que quer dizer que o algoritmo nessas situações utilizou 100% do processamento disponível em cada nó explorando ao máximo o paralelismo do algoritmo.

Speed-up.


Através dos experimentos realizados comprova-se que o uso de cluster de baixo custo pode ser uma alternativa viável para a resolução de problemas que necessitam de uma resposta com maior precisão em menos tempo.

Observou-se também a alta granularidade dos algoritmos genéticos quando paralelizados com uso da biblioteca de troca de mensagem MPI.

A comprovação apresenta-se na nos tempos que em alguns casos o algoritmo chegou a um grau ótimo de paralelização e utilização dos nodos.

Como trabalhos futuros têm-se a possibilidade de ajustar o AG para que possa ser executado em Grids Computacionais, que dão a possibilidade de submeter Jobs do algoritmo genético através da web para serem processados em máquinas geograficamente distribuídas.

O escopo desse trabalho é a Computação Pervasiva, que na visão do grupo G3 PD, que faço parte, se caracteriza por propiciar ao usuário acesso a seu ambiente computacional independente de localização, tempo e equipamento.

Dentro dessa perspectiva, ressaltase que a Computação Pervasiva, constituirá ainda um campo fértil para ofertas de produtos e desenvolvimento de pesquisas nos próximos anos.

Na Computação Pervasiva a aplicação ou o ambiente de execução proativamente monitoram e controlam as condições do contexto.

Esse trabalho considera que a aplicação reage às alterações no contexto através de um processo de adaptação.

A proposta da computação pervasiva pode ser construída pela integração da Computação Móvel, Computação em Grade e Computação Sensível ao Contexto.

Em um ambiente de computação pervasiva, os dispositivos, serviços e agentes devem ser conscientes de seus contextos e automaticamente adaptarse às suas mudanças, isso caracteriza a Sensibilidade ao Contexto.

A motivação para o estudo de ontologias, se deve ao fato de que, o EXEHDA ON foi concebido para agregar a manipulação de ontologias ao mecanismo de sensibilidade ao contexto do middleware EXEHDA, tendo sido proposto em dissertação de mestrado desenvolvida no Programa de PósGraduação e Informática da UCPel.

A aplicação de ontologias para expressar e processar informações de contexto, aplicada a Computação Pervasiva é proposta no EXEHDAON, como alternativa para propiciar uma semântica de maior expressividade que a usualmente praticada na coleta e no processamento dos dados sensorados, qualificando os níveis de descrição nas informações que caracterizam o contexto do ambiente computacional.

As ontologias vem sendo utilizadas por várias áreas da Ciência da Computação, principalmente com o intuito de dotar os sistemas de metaconhecimento.

Os resultados da pesquisa estão sendo canalizados para qualificar o mecanismo de sensibilidade ao contexto do middleware EXEHDA.

Esta contribuição vem indicando ótimos níveis de usabilidade para o EXEHDAON, quando da programação de aplicações sensíveis ao contexto.

O estudo do middleware EXEHDA, que faz parte dos esforços de pesquisa do Projeto ISAM (Infraestrutura de Suporte às Aplicações Móveis Distribuídas), constituem referência à proposta do EXEHDAON.

O EXEHDA é um middleware adaptativo ao contexto e baseado em serviços que visa criar e gerenciar um ambiente pervasivo, bem como promover a execução, sob este ambiente, das aplicações que expressam a semântica sigame.

Estas aplicações são distribuídas, móveis e adaptativas ao contexto em que seu processamento ocorre, estando disponíveis a partir de qualquer lugar, todo o tempo.

Para atender a elevada flutuação na disponibilidade dos recursos, inerente à computação pervasiva, o EXEHDA é estruturado em um núcleo mínimo e em serviços carregados sob demanda.

Os principais serviços fornecidos estão organizados em subsistemas que gerenciam, a execução distribuída, a comunicação, o reconhecimento do contexto, a adaptação, o acesso pervasivo aos recursos e serviços, a descoberta e o gerenciamento de recursos.

O foco dos estudos voltase ao mecanismo de adaptação do EXEHDA, que emprega uma estratégia colaborativa entre aplicação e ambiente de execução, através da qual é facultado ao programador individualizar políticas de adaptação para reger o comportamento de cada um dos componentes que constituem o software da aplicação.

Os principais requisitos, apontados na proposta do EXEHDAON, que o EXEHDA deve atender são, gerenciar tanto aspectos não funcionais como funcionais da aplicação.
E de modo independente, dar suporte à adaptação dinâmica de aplicações, disponibilizar mecanismos para obter e tratar informações de contexto, utilizar informações de contexto na tomada de decisões, decidir as ações adaptativas de forma colaborativa com a aplicação  e disponibilizar a semântica siga me, possibilitando ao usuário o disparo de aplicações e o acesso a dados a partir de qualquer lugar, e a execução contínua da aplicação em face ao seu deslocamento.

Estudos registram que, as ontologias vem sendo utilizadas por várias áreas da Ciência da Computação, principalmente com o intuito de dotar os sistemas de metaconhecimento.

A utilização de ontologias para descrição semântica de um determinado vocabulário proporciona um entendimento amplo das características e propriedades das classes pertencentes a um domínio, assim como seus relacionamentos e restrições.

Na área de Computação, uma das definições mais citadas na literatura é a que define ontologia como uma "especificação explícita de uma conceituação".

Nesta definição, uma ontologia representa a especificação de um vocabulário representativo dentro de um domínio compartilhado, definindo classes, relações, funções e outros objetos.

Tendo o desenvolvimento de uma ontologia o objetivo de compartilhar conhecimento.

Ao longo desse período podese observar que, uma ontologia não se resume somente a um vocabulário, também possui relacionamentos e restrições (axiomas) entre os conceitos definidos pelo vocabulário.

Quando um sistema processa uma ontologia, também é possível inferir novas informações por meio de regras de inferência.

Então, podese considerar que uma ontologia compreende um vocabulário que possui relacionamentos e restrições entre seus termos e, por meio de regras de inferência, é possível derivar novos fatos baseandose em fatos existentes.

A discussão da modelagem do EXEHDAON e a avaliação do mesmo constituíram o foco desse trabalho.

O modelo ontológico para uso no EXEHDAON foi definido considerando aspectos que modelassem o domínio do ambiente pervasivo provido pelo EXEHDA.

A perspectiva era de que este modelo representasse o estado atual do ambiente de execução pervasivo provido pelo EXEHDA, gerando deste modo um conhecimento sobre o mesmo, possibilitando assim sua manipulação pelo servidor de contexto, o qual responde às demandas introduzidas pelas aplicações dos usuários.

No desenvolvimento da ontologia para o ambiente pervasivo tratado pelo EXEHDAON os esforços concentramse no conhecimento do domínio, caracterizando se os tipos de dispositivos, os sensores, os componentes de software e a infraestrutura das redes de interconexão.

No EXEHDAON estão previstos diversos serviços que gerenciam a obtenção, processamento e disseminação das informações de contexto.

Os componentes de software foram definidos levando em consideração, os serviços previstos para o EXEHDAON, arquitetura de software proposta para o EXEHDAON e os requisitos do servidor de contexto.

Assim, foram projetados componentes de software que possuem funcionalidades que envolvem aspectos relacionados com a coleta das informações de contexto nos nodos, o envio das mesmas para atualização da base ontológica junto ao servidor de contexto e a execução de consultas à ontologia.

Um protótipo do servidor de contexto para o ambiente pervasivo foi modelado e desenvolvido com o uso da API Jena e a linguagem de programação Java, durante a concepção do serviço EXEHDA-ON.

Para testar o protótipo foram criados sensores para dados estáticos (arquitetura, número de núcleos, tipo de processador, número de interfaces de rede, memória física, disco instalado) e dinâmicos (processador disponível, memória disponível, disco disponível, tráfego de entrada na rede, tráfego de saída da rede).

As atividades desenvolvidas ao longo deste trabalho, especialmente os estudos realizados e a prototipação e testes do serviço EXEHDAON, permitiram a obtenção das seguintes conclusões.
Pesquisas envolvendo aspectos da sensibilidade ao contexto se mostram presentes nas diferentes propostas para Computação Pervasiva, os projetos de pesquisa em sensibilidade ao contexto mais atuais, discutem o uso de ontologias.

O uso de ontologias se mostra oportuno para sensibilidade ao contexto, pois, permite o compartilhamento e a representação do conhecimento em sistemas distribuídos dinâmicos e abertos.
Provê significados para as informações contextuais, com sua semântica declarativa, potencializa a interoperabilidade das entidades computacionais com o servidor de contexto, possibilita que a interpretação de contexto seja realizada em alto nível.

Ainda, foi possível identificar uma qualificação dos mecanismos usados para obtenção e processamento de informações de contexto a partir dos estudos realizados, bem como do desenvolvimento da proposição do modelo ontológico e da arquitetura de software para o EXEHDAON.

Assim, o EXEHDAON atingiu seu objetivo de prover melhores níveis de descrição nas informações que caracterizam o contexto do ambiente pervasivo provido pelo EXEHDA, ficando indicado que o uso de ontologias e suas tecnologias associadas possibilita o emprego de uma semântica de maior expressividade que a usualmente praticada na coleta e no tratamento dos dados sensorados.

A programação paralela trata-se de um modelo de programação em que tarefas complexas são divididas em tarefas menores a fim de serem executadas paralelamente (ou concorrentemente), realizando-se vários cálculos de forma simultânea em diferentes núcleos de processamento.

Dentro desse contexto, criou-se a norma MPI (Message Passing Interface), a qual é uma especificação padrão para um protocolo de comunicação utilizado para realizar a interação entre diferentes processos em um aplicativo paralelo.

A comunicação realizada entre esses processos ocorre, portanto, através da troca de mensagens.

Diversas implementações para diferentes linguagens de programação foram produzidas visando fornecer as facilidades do MPI, tais como o MPICH, o Open MPI e o LAM/MPI.

Porém, mesmo através da utilização de um protocolo padronizado de comunicação, a produção de aplicações paralelas pode ser considerada como uma tarefa de razoável complexidade, uma vez que o ser humano tende a raciocinar através de uma lógica seqüencial de eventos.

Baseado no princípio de construir uma camada de abstração entre o MPI e o aplicativo, o Open Systems Laboratory da Universidade de Indiana desenvolveu uma interface para uma implementação nativa do MPI, sendo tal interface denominada MPINET e voltada para a linguagem de programação C#.

Essa linguagem orientada a objetos foi projetada para possuir diversas facilidades relativas ao modo de programação e a abstração de conceitos.

Embora a maioria das funcionalidades presentes na especificação MPI estivessem contidas no MPI NET, essa interface não provia meios para que o programador utilizasse um modelo de comunicação de envio de mensagens com o uso de buffers definidos pelo próprio usuário.

Esse modelo de envio de mensagens é especificado pela primitiva de comunicação MPI_Bsend, juntamente com outras duas primitivas de definição e tratamento de buffers, a MPI_Buffer_attach e a MPI_Buffer_detach.

Este trabalho tem a finalidade de expandir a interface MPI NET com a implementação da primitiva Bsend, a fim de que a mesma esteja disponível para a programação paralela com MPI na linguagem de programação C#.

A solução apresentada pelo MPI NET contém uma razoável quantidade de classes e abstrações.

De forma geral, três dessas classes tiveram de ser modificadas para incorporar parte da nova solução, as quais serão descritas abaixo.

Essa classe encapsula o conceito de pertinência de processos a um mesmo grupo, permitindo que mensagens sejam enviadas entre os mesmos através de chamadas a métodos públicos pertencentes a um determinado objeto dessa classe.

Assim sendo, todas as funções MPI relativas à comunicação propriamente dita entre processos encontram-se traduzidas como métodos da classe Communicator.

Essa classe provê a inicialização, finalização e a disponibilidade de operações de consulta ao ambiente MPI.

Da mesma forma que na especificação MPI o usuário deve utilizar a primitiva MPI_Init para inicializar o ambiente, na solução MPI NET, deve-se instanciar um objeto da classe Environment para se obter tal funcionalidade.

Essa classe provê acesso direto a biblioteca MPI, onde as funções efetivamente implementadas são importadas e utilizadas pelo MPINET.

Trata-se de um módulo importante para realizar a conexão da interface com o MPI nativo.

A primitiva de comunicação MPI_Bsend existente na especificação MPI consiste no envio de mensagens através da utilização de um buffer definido pelo usuário.

Assim sendo, no momento em que esta primitiva é executada por um processo, os dados a serem enviados são copiados para o buffer do usuário e a rotina retorna ao ponto seguinte a primitiva.

Trata-se de uma vantagem significativa nos casos em que existe uma grande ocupação do buffer do sistema, uma vez que é permitido um certo controle da quantidade de memória utilizada pelo MPI.

Geralmente, a definição de buffers pelo usuário em linguagens como C se dá através da alocação de memória para um array de uma determinada quantidade de bytes.

Como no C# arrays são objetos, eles são considerados alvos do Garbage Collector, artifício de gerenciamento de memória comumente encontrado em linguagens orientadas a objeto.

Esse fator é um problema em potencial, uma vez que a implementação nativa do MPI exige que a área de memória utilizada pelo buffer do usuário seja fixa.

Isso impede que seja feita a utilização natural de arrays em C# para a definição de buffers, já que os mesmos podem ser deslocados na memória pelo Garbage Collector.

A fim de padronizar o modo de criação de buffers pelo usuário, foi inserida uma nova classe na solução MPINET, a classe BufferMPI.

A instanciação de um objeto dessa classe possui a capacidade de alocar uma determinada quantidade de bytes de memória não-gerenciada pelo Garbage Collector.

Assim sendo, o usuário informa o número de bytes que deseja alocar através do construtor da classe.

Em seguida, a implementação do construtor utiliza-se do método Marshal AllocHGlobal do espaço de nome System Runtime.

InteropServices da plataforma NET para alocar a quantidade de memória não-gerenciada desejada.

A criação de buffers para utilização na primitiva Bsend restringe-se a instanciação de objetos da classe BufferMPI, portanto.

Uma vez que os métodos referentes à comunicação ponto-a-ponto encontram-se descritos na classe Communicator, foi realizada a inserção de um novo método para o envio de mensagens através do modo buffered, o Bsend.

A criação desse método seguiu os mesmos princípios da implementação da primitiva Send.

Dessa forma, diferentes tratamentos são fornecidos conforme o tipo de dado a ser enviado pelo MPI NET.

O tratamento dos dados a serem enviados ocorre de uma maneira particularmente simples.

No caso do dado ser de algum tipo simples da linguagem C# como int, por exemplo, associa-se o dado a um tipo MPI existente, MPI_INT, no caso.

Por outro lado, se o dado não possui um tipo MPI nativo associado como, por exemplo, um objeto ou uma estrutura, é necessária a realização da serialização do dado, a fim de que o mesmo possa ser enviado como um stream de bytes.

É importante notar que, nessa situação, dois envios são feitos, um contendo o tamanho de bytes serializados do stream e outro contendo os dados propriamente ditos.

Em ambos os casos de tratamento, a implementação do Bsend realiza chamadas à função nativa MPI_Bsend importada pela classe Unsafe.

Além das alterações na classe Unsafe para que as primitivas MPI_Bsend, MPI_Buffer_attach e MPI_Buffer_detach fossem importadas da implementação nativa do MPI, tornou-se necessária a definição do modo pelo qual o usuário informaria ao ambiente sua intenção de utilização de um buffer próprio para o envio de mensagens com o Bsend.

Essa questão pôde ser solucionada adicionando-se os métodos BufferAttach e BufferDetach à classe Environment.

O primeiro método recebe como parâmetro um objeto da classe BufferMPI e realiza a chamada nativa ao MPI_Buffer_attach com o vetor alocado por esse objeto.

Já o segundo método não recebe parâmetros e retorna um objeto BufferMPI contendo o buffer que estava sendo utilizado anteriormente pelo ambiente.

Além disso, ele realiza a chamada nativa ao MPI_Buffer_detach, desanexando o buffer antigo do MPI.

Diversos testes foram realizados utilizando-se a primitiva Bsend, incluindo-se neles o envio de uma quantidade razoável de dados de tipos simples e abstratos do C#, como instanciações de objetos e estruturas.

Todas as execuções obtiveram sucesso na sua realização.

Além disso, foram efetuados testes com um programa seqüencial de multiplicação de matrizes para realizar uma comparação entre C e C#, a fim de que se possa ter uma noção do overhead existente com o C# influência direta no desempenho de aplicativos paralelos.

Os testes foram rodados em uma máquina Intel Pentium D (299 Ghz e 300 Ghz) com 1 GB de memória RAM no Windows XP SP3.

Comparação de multiplicação de matrizes entre o C e o C#.

A inclusão de novas primitivas existentes na especificação padrão MPI na interface MPI NET traz como vantagem uma solução mais robusta e possivelmente mais aceita para ser utilizada pela linguagem C# na plataforma NET em aplicações de alto desempenho, associadas a programação paralela.

Dentro desse contexto, o trabalho desenvolvido na implementação da primitiva Bsend na Universidade Federal do Rio Grande do Sul foi parcialmente apoiado pela Microsoft e pôde ser realizado com sucesso.

Este trabalho está inserido nos esforços de pesquisa do G3 PD/UCPEL em Computação Pervasiva, a qual prevê que uma aplicação e/ou o ambiente de execução, proativamente monitoram e controlam as condições do contexto.

O sistema computacional (middleware e aplicação) reage às alterações no contexto através de um processo de adaptação.

A proposta da Computação Pervasiva pode ser construída pela integração da computação móvel, computação em grade e computação sensível ao contexto.

Em um ambiente de computação pervasiva, os dispositivos, serviços e agentes devem ser conscientes de seus contextos e automaticamente adaptarse às suas mudanças, isso caracteriza a sensibilidade ao contexto.

O middleware EXEHDA faz parte dos esforços de pesquisa do Projeto ISAM.

O ISAM vem sendo desenvolvido por um consórcio de universidades gaúchas, e foi iniciado na UFRGS sob a coordenação do Prof Cláudio Geyer.

O EXEHDA é um middleware adaptativo ao contexto e baseado em serviços que visa criar e gerenciar um ambiente pervasivo, bem como promover a execução, sob este ambiente, das aplicações que expressam a semântica sigame.

Estas aplicações são distribuídas, móveis e adaptativas ao contexto em que seu processamento ocorre, estando disponíveis a partir de qualquer lugar, todo o tempo.

A contribuição desse trabalho dáse através do estudo do middleware MoCA (Mobile Collaboration Architecture), onde foram analisadas as suas tecnologias e funcionalidades pertinentes a Sensibilidade ao Contexto.

O MoCA é um middleware baseado em serviços, flexível e extensível, constituído de um conjunto de serviços e APìs, que visam dar suporte ao desenvolvimento de aplicativos distribuídos sensíveis ao contexto.

Seus aplicativos são direcionados a operarem em uma infraestrutura de rede local, com suporte inclusive a conexões sem fio (IEEE 80211 b/g).

Ele fornece meios para recolhimento, armazenamento e tratamento de dados dos diferentes contextos aos quais os dispositivos estão submetidos.

Ao longo do estudo desenvolvido, foi possível identificar que o middleware possui em sua estrutura principal um conjunto de componentes, os quais são descritos a seguir, Context Information Service (CIS), capaz de receber, armazenar e processar informações de contexto enviadas pelos monitores em execução nos dispositivos móveis, que podem ser acessadas de forma síncrona ou assíncrona.

Location Inference Service (LIS), responsável por inferir a localização aproximada de um dispositivo móvel, através da comparação do padrão de radiofrequência atual de um dispositivo, com padrão de sinais mensurados em pontos de referência prédefinidos, sendo possível definir regiões simbólicas, feitas por uma associação de nomes a regiões físicas definidas (ex, prédios, salas).

Symbolic Region Manager (SRM), Permite estabelecer relações entre as regiões atômicas definidas pelo LIS, caracterizando uma hierarquia entre as mesmas.

Configuration Service (CS), é responsável por armazenar e gerenciar informações de configuração de cada dispositivo utilizadas pelo CIS, armazenandoas em tabelas hash, e indexadas pelo endereço MAC (media access control address) do dispositivo, onde cada entrada na tabela guarda os endereços do servidor CIS e de um DS, bem como a periodicidade do envio das informações do dispositivo.

Discovery Service (DS), Armazena informações como nome, propriedade e endereço de qualquer aplicação e seus componentes ou serviço registrado e disponibiliza a descoberta de seu endereço por parte de seus clientes.

Monitor, É executado em cada um dos dispositivos móveis.

É responsável por armazenar dados relativos ao estado e ambiente de execução do dispositivo e enviálos para o CIS.

Dentre os dados coletados estão incluídos dados relativos a qualidade da conexão, energia, uso de CPU, uso de memória, etc.

É o único desenvolvido em C++.

Após a instalação dos componentes básicos do MoCA, que deve ser feita considerando as orientações disponíveis no site do projeto, pode ser realizada a configuração dos serviços do middleware respeitando uma série de dependências funcionais, que são necessárias antes de se executar uma aplicação propriamente dita.

Configuração do serviço CIS, Antes do serviço CIS ser iniciado, é necessário que seja criado um arquivo com o nome de cis properties, salvo no diretório "CIS_HOME\conf", contendo o endereço do servidor externo e as portas de comunicação que serão usadas para atender as requisições dos clientes.

Configuração do LIS, Para a configuração do LIS, devem ser criados três arquivos, salvos no diretório "LIS_HOME\conf".

O primeiro, chamado cis properties, contém informações sobre o servidor externo CIS, como endereço IP e portas de comunicação, de modo que o LIS possa se registrar.

Exemplo de Arquivo cis properties utilizado pelo LIS.

O segundo arquivo, chamado srm properties, descreve os parâmetros do servidor SRM, permitindo que o servidor LIS possa conectarse e obter informações sobre regiões simbólicas e a hierarquia entre as mesmas.

Exemplo de Arquivo srm properties.

O terceiro e último arquivo, chamado lis properties, possui em seu conteúdo informações sobre os parâmetros do servidor LIS, como endereço IP e portas de comunicação que devem ser usadas para atendimento das requisições dos clientes.

Exemplo de Arquivo lis properties.

A partir desse momento, para executar o LIS, basta invocar o comando "lis" através do terminal.

Configuração do CS, O procedimento de configuração do CS se dá através da criação e edição do arquivo cs properties.

Esse arquivo deve ser salvo no diretório indicado pelo variável de ambiente MOCA_HOME e deve conter o endereço IP do servidor onde o CS será executado.

Exemplo de Arquivo lis properties.

O CS será iniciado através do comando "java jar csrc2 jar", executado através do terminal.

Nesse momento, o CS está pronto para adicionar ou remover dispositivos de sua base de dados, bem como requisitar informações sobre um dispositivo.

Configuração do DS, De forma semelhante a configuração do CS, a configuração do DS é feita através da criação de um arquivo chamado server properties contendo o endereço do servidor onde o DS está executando e a porta usada para comunicação.

Exemplo de Arquivo server properties.

Para iniciar a execução do serviço DS, o comando "java cp dsrc1 c jar moca service ds slp Da " deve ser invocado no terminal.

Configuração do Monitor, Para configurar o Monitor, no diretório "C,\Windows\MonitorXP" deve ser criado o arquivo monitor properties, que contém os endereços IP e número das portas dos servidores CS, CIS e DS, além do intervalo de tempo entre as requisições e o número da porta do próprio Monitor.

Apresenta o conteúdo original do arquivo monitor properties.

Exemplo de Arquivo monitor properties.

Os estudos realizados até agora caracterizaram o middleware MoCA como uma estrutura vocacionada para dispositivos móveis, e para o desenvolvimento de aplicações com foco em localização.

Os estudos também mostraram que as tecnologias empregadas no MoCA, particularmente o sensoramento da localização mostramse interessantes para uso no EXEHDA.

Como trabalhos futuros esperase implementar aplicações com a mesma funcionalidade nos middlewares EXEHDA e MoCA, tendo como objetivo uma comparação mais profunda dos diferentes aspectos de programação e execução das soluções em ambas plataformas.

Além disso, buscarseá sistematizar uma comparação entre os middlewares EXEHDA e MoCA, contemplando aspectos de modelagem, implementação e perfil operacional, com o intuito de avaliar os aspectos do MoCA enquanto middleware para suporte a Computação Sensível ao Contexto, passíveis de serem empregados na qualificação das funcionalidades do EXEHDA.


O avanço da computação ubíqua traz a necessidade do projeto de novas e complexas aplicações.

Uma infra-estrutura de software permite a abstração de diversas questões de implementação, simplificando seu desenvolvimento.

A infra-estrutura de software Continuum foi projetada de acordo com as principais questões de pesquisa da computação ubíqua.

Este artigo apresenta a pesquisa desenvolvida durante o projeto dos serviços cell information base e communicator, utilizados pelo Continuum.

O termo computação ubíqua foi utilizado pela primeira vez em.

O autor afirma que é necessária uma maior integração da tecnologia da informação com o cotidiano, de forma que ela se torne transparente, auxiliando os usuários a lidar com a sobrecarga diária de informações.

Pesquisas na computação ubíqua englobam questões de diversas áreas, dentre elas sistemas distribuídos e computação móvel, além de questões próprias da computação pervasiva.

Um estudo sobre os desafios que caracterizam a computação ubíqua é apresentado em.

Este estudo é utilizado na definição de um modelo abrangente de infra-estrutura de software para a computação ubíqua, que tem como elementos principais um framework, para auxiliar no projeto de aplicações para a arquitetura, e um middleware, responsável por tratar diversas questões da execução destas aplicações.

Este modelo é a base da definição do Continuum, uma infra-estrutura de software para aplicações ubíquas, baseada em uma arquitetura orientada a serviços (SOA).

Uma SOA é composta por um conjunto de serviços base, que podem ser utilizados para a criação de novos serviços e aplicações através de composição e gerenciamento.

Assim como no modelo abrangente, o Continuum é composto por um framework para auxílio ao desenvolvimento de aplicações, e um middleware, que tem como núcleo uma série de serviços plugáveis, responsáveis pelas principais funcionalidades do ambiente de execução.

Atualmente, há apenas um modelo da interface dos serviços que compõe o middleware do Continuum, sendo necessária sua modelagem e implementação.

Este artigo apresenta as pesquisas realizadas para determinar os requisitos e possíveis metodologias a serem empregados no projeto de dois serviços utilizados pelo Continuum, o cell information base, responsável pela manutenção do ambiente distribuído formado pelas entidades que executam o Continuum, e, communicator, que provê um mecanismo de comunicação por eventos para a infra-estrutura.

A seção 2 apresenta o modelo de abstração de entidades físicas do Continuum e a pesquisa realizada para a definição das propriedades do serviço cell information base.

A seção 3, por sua vez, apresenta uma análise do estado da arte na área de sistemas de comunicação por eventos, que é a base do serviço communicator.

Finalmente, na seção 4, são apresentas considerações finais relativas ao trabalho desenvolvido e sua continuidade.

Em é proposto um modelo de abstração que permite a representação, no contexto de aplicações ubíquas, de entidades relevantes do mundo real.

No modelo, uma CoDimension engloba todas as entidades que podem estar contidas no ambiente de uma aplicação, sendo composta por um conjunto de células, denominadas CoCells, que representam os lugares do ambiente modelado.

Uma célula pode, ainda, ser composta por outras células, possibilitando a representação de lugares compostos.

O nível de abstração representado por uma célula varia com a aplicação desenvolvida.

CoPersons representam as pessoas presentes no ambiente, que estão localizadas em uma célula, e podem se deslocar para outras.

Os objetos representados pelo modelo, denominados CoNodes, englobam dispositivos eletrônicos presentes nas células, como computadores, sensores, ou dispositivos móveis.

CoNodes representam dispositivos estáticos básicos, tais como computadores de mesa.

Além desses nodos, existem tipos particulares de representações empregadas na infra-estrutura.

Cada célula possui um nodo especial, denominado CoBase, responsável pela execução de serviços básicos para seu gerenciamento.

Dispositivos com características móveis, como um notebook com conexão por rede sem fio, são denominados CoMobis.

Dispositivos móveis com propósitos específicos, como celulares ou PDAs, são tratados de maneira diferenciada pelo ambiente e são denominados CoGadgets.

A manutenção da infra-estrutura modelada é gerenciada pelo serviço cell information base, que também permite a consulta aos recursos presentes no ambiente.

Esta funcionalidade pode ser comparada a de um serviço de gerenciamento de informações de contexto, porém estes serviços têm uma abrangência maior no escopo das informações coletadas, o que não é objetivo do CIB.

Para definir-se os requisitos para a modelagem do CIB, foram consultadas referências a serviços similares, presentes em outras infra-estruturas para a computação ubíqua, dentre elas o Aura, o Gaia, e o EXEHDA.

Os serviços pesquisados atuam como centralizadores, responsáveis pelas informações sobre o contexto das aplicações.

A forma de coleta varia com o tipo dos dados armazenados pelo serviço.

No caso do registro de entidades como dispositivos e usuários presentes no ambiente, os dados são informados diretamente pelas entidades registradas.

Outras informações, por exemplo o ponto geográfico em que a entidade está localizada, são obtidas de outros sensores no ambiente, que operam em conjunto com o serviço.

Os serviços utilizam, para persistir os dados adquiridos, bancos de dados relacionais ou serviços de diretório.

A persistência pode ser realizada por um serviço centralizado, que se torna responsável por todas as resquisições do ambiente.

Outros serviços utilizam persistência distribuída, onde cada instância é responsável por armazenar uma porção dos dados do ambiente, e as consultas são realizadas através da propagação de mensagens na rede, o que pode criar sobrecargas durante as consultas.

Para a realização de pesquisas, alguns sistemas utilizam funções especiais, que recuperam uma entidade e todos seus atributos, a partir de informações como o nome ou identificador de uma entidade.

Outros serviços usam uma linguagem com uma semântica similar à da SQL, que é interpretada pelo serviço, permitindo maior flexibilidade na seleção e filtragem dos atributos retornados.

Outro método é o envio de consultas diretamente na linguagem do sistema de persistência utilizado pelo serviço.

O serviço communicator implementa um sistema de eventos para o Continuum, baseado no modelo de comunicação publicar-assinar (publish-subscribe).

A comunicação orientada a eventos permite que provedores e assinantes sejam desacoplados nas dimensões de espaço, tempo e sincronização.

Provedores e assinantes não mantêm contato direto, utilizando o serviço de notificações para comunicação.

Além disso, não é necessário que provedores e assinantes estejam ativos simultaneamente para que a interação seja realizada.

Por fim, operações de publicação e recepção de eventos são métodos não-bloqueantes, o que permite aos nodos mantêrem suas atividades durante a comunicação, que ocorre de forma assíncrona.

A anonimidade e assincronismo inerentes deste modelo o tornam ideal para sistemas distribuídos.

Para definir os requisitos do serviço communicator, foram estudados outros sistemas publicar-assinar, dentre eles o Hermes, o Echo, e o Java Messaging Services.

Nos sistemas estudados, a separação entre provedores e clientes é realizada por uma entidade centralizadora.

Trabalhos com foco em alto desempenho e adaptabilidade usam como base uma rede de overlay, para tratar o roteamento eficiente das notificações.

Neste caso, a entidade central é implementada de maneira distribuída, e é executada pelos pares que utilizam o sistema de eventos.

Estes sistemas apresentam escalabilidade e adaptabilidade, mas sobrecarregam os clientes que utilizam o sistema.

Alguns dos sistemas permitem que os eventos sejam temporariamente armazenados, de forma que a desconexão de clientes não ocasione a perda de notificações.

Esta funcionalidade é limitada pelo espaço disponível na entidade central que gerencia os eventos, que se for totalmente distribuída, vai depender do armazenamento disponível nos clientes.

O método utilizado na definição das assinaturas pode ser baseado em um sistema de tipos, onde clientes especificam os eventos que desejam receber a partir dos tipos de dados contidos nas notificações.

Este mecanismo reduz as conversões de dados durante o tráfego de notificações.

O sistema, porém, torna-se dependente de uma linguagem de programação ou biblioteca de tipos.

O uso de um sistema de tópicos, onde assinaturas são definidas a partir de tópicos pré-definidos no sistema, elimina esta restrição e mantém o desempenho do sistema, porém apresenta pouca flexibilidade para a especificação de assinaturas.

Assinaturas por conteúdo, definidas a partir dos atributos presentes na própria notificação, através de uma linguagem de pesquisa, apresentam maior flexibilidade para a filtragem de eventos, porém uma maior sobrecarga no tratamento das assinaturas.

A computação ubíqua tem avançado rapidamente, graças à popularização dos dispositivos móveis, aliado a capacidade desses de acesso a redes sem fio.

O desenvolvimento de uma aplicação ubíqua, por sua vez, tem como desafio o tratamento das diversas questões relacionadas com a área, tornando seu projeto uma tarefa complexa.

Tendo como base um modelo atualizado para a computação ubíqua, a infra-estrutura de software Continuum apresenta diversas vantagens para o desenvolvimento destas aplicações.

Este artigo apresentou a pesquisa realizada com o objetivo de definir os requisitos para os serviços cell information base e communicator, presentes no middleware do Continuum.

Dando continuidade ao trabalho, será realizado o detalhamento do projeto destes serviços, e em seguida sua implementação, com o objetivo de integrá-los ao Continuum, que atualmente encontra-se em desenvolvimento.

A utilização de recursos computacionais de alto desempenho, em muitos casos, é realizada por pesquisadores de outras áreas do conhecimento.

A dificuldade encontrada por este tipo de usuário pode ser equiparada à dificuldade encontrada por estudantes da disciplina de Processamento Paralelo e Distribuído.

Desta forma, pode-se recomendar o uso de ferramentas para auxiliar o desenvolvimento de programas paralelos ou distribuídos.

De forma semelhante, a utilização de recursos visuais para representação da estrutura do programa pode facilitar a compreensão, desenvolvimento e depuração de programas paralelos e distribuídos.

Uma das principais formas de representação gráfica de programas paralelos e distribuídos é a utilização de grafos, onde os nodos representam os elementos de processamento e as arestas representam os elementos de comunicação.

Este artigo apresenta a ferramenta GraphProgramming, uma ferramenta opensource para auxílio ao desenvolvimento de programas para ambientes paralelos e distribuídos.

Buscando facilitar o processo de desenvolvimento de aplicações para ambientes paralelos e distribuídos foi desenvolvido um projeto para uma ferramenta de programação visual utilizando, em um primeiro momento, o padrão Message Passing Interface (MPI) para o desenvolvimento.

Apesar de inicialmente suportar apenas o padrão MPI, a ferramenta deve ser capaz de permitir a inclusão futura de outras tecnologias para programação para ambientes paralelos e distribuídos.

O principal objetivo da ferramenta é a geração do código-fonte da aplicação desenvolvida.

Este código deverá ser gerado através de uma estrutura criada pelo usuário, que deverá utilizar recursos visuais para a criação desta estrutura.

Estes recursos gráficos são representados analogamente a grafos, onde os elementos de processamento são representados pelos nodos e os elementos de comunicação são representados pelas arestas.

Cada nodo da estrutura deverá conter dados referentes ao elemento de processamento que representa, como os códigos relativos à computação ou o número de processo.

Os nodos podem ser de três tipos, nodos que representam um único processo (P), nodos que representam todos os processos (N) e nodos que representam todos os processos exceto o processo root (N-1).

Dois nodos, delimitando o início e o término da estrutura da aplicação desenvolvida, devem ser inseridos automaticamente pela ferramenta.

Estes nodos devem possuir as rotinas de inicialização e término do ambiente.

Cada elemento de comunicação deverá conter dados acerca da comunicação que representa, como nome da variável de envio e recebimento, tipo de dado comunicado, entre outros.

As comunicações são divididas em dois grupos, comunicação ponto-a-ponto e coletiva, e os tipos de dados enviados podem ser escalares ou vetoriais.

As comunicações ponto-a-ponto representam comunicações diretamente entre dois processos.

Normalmente este tipo de comunicação envolve dois nodos do tipo P (comunicação P para P) ou o retorno de um dado por todos os processos exceto o processo root (comunicação N-1 para P), mas pode ser utilizado para enviar um grupo de dados distintos de um processo para todos exceto o root (comunicação P para N-1).

Este tipo de comunicação é análoga às funções MPI_Send e MPI_Recv no padrão MPI.

As comunicações coletivas têm como característica o envolvimento de mais de um processo por vez.

As comunicações coletivas são classificadas como, comunicação de um único dado ou de um mesmo grupo de dados do processo root para todos os demais (comunicação de P para N-1).
Sendo este análogo a função MPI_Bcast no padrão MPI, comunicação de um dado, ou de um grupo distinto de dados, de todos os processos para o processo root (comunicação N para P) podendo, ou não, haver processamento destes dados.
Sendo análogos às funções MPI_Gather (sem processamento dos dados) e MPI_Reduce (com processamento dos dados), e a comunicação de um grupo distinto de dados do processo root para todos os processos (comunicação P para N), que é análogo à função MPI_Scatter.

Outro tipo de comunicação tratada é a sincronização explícita dos processos, análogo à função MPI_Barrier no padrão MPI.

GraphProgramming é uma ferramenta opensource (protegida pela licença GNU General Public License (GPL)) para programação visual para ambientes paralelos e distribuídos.

A ferramenta foi desenvolvida na linguagem Java, utilizando o Java Development Kit (JDK) 1 6 0_07, escolhida devido aos benefícios para programação gráfica e portabilidade.

Além das bibliotecas padrões do JDK, foi utilizada a biblioteca NetBeans Visual Library para desenvolvimento da interface gráfica.

A interface de interação com o usuário é composta por quatro partes básicas.

A interface possui uma área de menus (A), botões de acesso aos recursos da área gráfica (B), diferentes interfaces de interação (C) e legenda de cores para distinção dos diferentes tipos de comunicação (D).

Interface de interação com o usuário.

A ferramenta possui três tipos de interface de interação com o usuário.

A interface gráfica (Graph View) possui elementos de representação gráfica da estrutura do programa em desenvolvimento pelo usuário.

É nesta interface que serão inseridos novos nodos, comunicações e onde existe acesso às funções de inserção de código na estrutura.

A segunda interface é a interface de edição de código (Code View), onde o código gerado pela ferramenta será exibido para, caso seja necessário, alterações manuais posteriores.

A última interface é a interface de execução (Execution View), onde trabalhos futuros poderão inserir recursos de auxílio a execução e monitoramento aos programas gerados.

O projeto gerado pela ferramenta pode ser armazenado em um arquivo para utilização futura.

Para o armazenamento do projeto foi criado o formato gp1', que segue o padrão eXtensive Markup Language (XML), que facilmente se adapta ao formato de grafo na qual o projeto é desenvolvido.

Para realizar a validação dos códigos-fontes gerados pela ferramenta foram utilizados programas com pouca necessidade computacional e diferentes tipos de comunicação entre os processos.

Um dos programas utilizados para teste consiste no envio de uma mensagem do processo root para todos os demais processos, que deverão realizar algum processamento com os dados recebidos e retornar o resultado deste processamento ao processo root que, por sua vez, deverá realizar um processamento durante o recebimento dos valores e exibir o resultado obtido.

Ilustra o mapeamento deste programa para a ferramenta GraphProgramming.

Esse programa teve seu código-fonte gerado pela ferramenta e foi compilado e executado com sucesso, obtendo os resultados esperados.

Entre outras ferramentas que possuem similaridade de propósitos com a GraphProgramming, um comparativo foi realizado com as ferramentas HeNCE, P-GRADE.

Entre os quesitos utilizados para comparação entre a ferramenta estão, disponibilidade para uso, sendo irrestrita na GraphProgramming e HeNCE porém restrita nas demais.

Plataforma de execução, sendo a GraphProgramming multiplataforma (utilizando Java), a HeNCE utilizando a plataforma X-Window e as demais não possuindo informações sobre plataforma de execução.

O padrão de programação da ferramenta GraphProgramming é o MPI, enquanto a HeNCE utiliza o PVM e as demais suportam diversos padrões de programação. Por fim, a representação dos elementos, que na GraphProgramming utiliza uma representação através de grafos, semelhante a HeNCE e P-GRADE, enquanto a ASKALON representa seus elementos através de uma estrutura Unified Modeling Language (UML).

Os resultados positivos na execução dos códigos gerados pela ferramenta, assim como a simplicidade da estrutura resultante dos mapeamentos dos problemas apresentados, indicam que a ferramenta pode ser utilizada para facilitar o desenvolvimento de aplicações mais complexas.

Dessa forma, os usuários da GraphProgramming podem se estender de alunos de disciplinas de processamento paralelo e distribuído a pesquisadores de outras áreas do conhecimento, que possuem grande demanda computacional e limitações em programação para ambientes que supram esta demanda.

O desenvolvimento da ferramenta foi feito de forma modular, permitindo o desenvolvimento de melhorias nos padrões de programação já desenvolvidos ou a adição de novos padrões de programação.

Um aprimoramento já em andamento trata do auxílio à execução do programa gerado para ambientes de grids computacionais que utilizem o middleware Globus Toolkit, através do auxílio à criação do arquivo Resource Specification Language (RSL), que pode ser utilizado para execução de programas MPI.

Atualmente há um número cada vez maior de aplicações que necessitem de alto poder de processamento para executarem uma determinada tarefa.

Tais aplicações só são viáveis quando processadas de forma distribuída.

Então, o uso de programas paralelizados rodando em agregados de computadores conhecidos como clusters tornou-se uma ferramenta comum para computação de alto desempenho, não apenas para pesquisas acadêmicas, mas também para aplicações industriais.

Existem, hoje, diversos modelos de clusters, cada um implementando os conceitos de processamento distribuído da sua maneira e usando softwares e plataformas distintas.

Essa diversidade faz com que, às vezes, seja preciso que o usuário de aplicações paralelas aprenda detalhes internos do cluster que pretende usar.

Outro ponto importante neste cenário é a heterogeneidade da utilização dos recursos e a alternância de períodos de alta demanda e ociosidade.

Gerando um desperdício de poder computacional.

O Interop Router foi concebido para integrar diversos clusters de diferentes plataforma, gerando interoperabilidade e aumento da escalabilidade e do poder computacional.

Usando a Internet para interconectar clusters de diferentes arquiteruras e sistemas operacionais, dispersos geograficamente, os usuários podem submeter jobs paralelos e receber seus resultados através de uma interface simples e transparente.

Ao mesmo tempo, ao levar em conta o poder de processamento total da rede de clusters, permite que os recursos presentes nos agregados sejam melhor utilizados.

Visão geral da conexão entre o Interop Router e os clusters.

As aplicações são submetidas ao IR e então distribuídas entre os diferentes clusters.

O Interop Router é organizado em camadas, facilitando a interoperabilidade e a portabilidade.

Ele possui quatro camadas básicas, Guaardian (Interface), Banco de Dados (Armazenamento), Deemon (Processamento) e Aangel (Escalonamento).

Sua conexão com o usuário é feita pelo Guaardian, interface que possibilita o acesso a todas as ferramentas do Interop Router.

Através dessa interface ocorre a submissão de aplicações e arquivos de dados, monitoramento dos estados dos jobs, download dos resultados dos projetos e informações sobre os clusters conectados ao Interop Router.

O Deemon (Processamento), presente em cada cluster, realiza a comunicação do cluster com o Banco de Dados que, por sua vez, comunica-se com o Aangel (Escalonamento).

Este verifica informações de carga dos clusters e dos jobs a serem processados e os distribui de forma a otimizar o processamento.

Os projetos são submetidos pelo usuário e consistem no código fonte da aplicação paralelizada, escrita em C e compatível com MPICH2, que será compilada pelo cluster, e um ou mais arquivos de dados que serão processados pelo programa no cluster.

Os jobs são as partes nucleares de um projeto, formados a partir de cada arquivo de dados.

Cada cluster responsável por no mínimo um job recebe do Banco de Dados o(s) arquivo(s) de dados e o código fonte da aplicação.

No caso de vários jobs, o código fonte é compilado uma única vez.

Uma vez submetidos ao cluster pelo Deemon, os jobs são processados e os resultados são capturados pelo Deemon e enviado para o Bando de Dados.

O usuário pode então ter acesso aos resultados através do Guaardian.

Um dos objetivos do IR (Interop Router) é a otimização dos recursos dos clusters no grid.

Isso é feito pelo Aangel, o escalonador do IR.

Sua função é os distribuir os jobs entre os diversos clusters da rede.

Para isso, analisa as características estruturais (hardware, programas e bibliotecas instalados) de cada cluster e os recursos livres presentes nos mesmos.

Como o Deemon, Aangel é implementado em Python 25, separada e externamente das outras estruturas do Interop Router, o que permite fáceis modificações e customizações.

Dessa forma, é simples alterar e calibrar o algoritmo de escalonamento.

A interface consiste em um website.

O foco do site é prover acesso aos usuários de uma forma simples e fácil.

Para isso, ele é implementado em PHP, o que o mantém genérico e portável.

Nesse site, o usuário tem acesso fácil à informação (multilinguagem) com uma interface amigável.

Ele pode acessar três abas, Projeto, Submissão e Recursos.

Na aba de Projetos, há uma lista de todos os projetos do usuário com a hora de submissão e seu código fonte.

Clicando no nome do projeto, é possível ver os detalhes dos jobs, o status, os arquivos de dados, arquivos de entrada e saída e mensagens de erro.

Na aba de Submissão os usuários podem criar um projeto e fazer upload dos dados.

A aba de Recursos informa detalhes sobre os clusters conectados ao Interop Router e dá uma curta descrição de sua infra-estrutura.

Usar a web como interface faz com que a operação do grid fique transparente e globalmente acessível.

Deste modo, os usuários não precisam ter acesso físico aos clusters ou conhecer pormenores das operações de processamento em cada plataforma ou modelo de cluster.

A interface entre o Interop Router e os clusters é feita pelo Deemon.

Sua função é criar uma camada que seja capaz de traduzir as operações necessárias para o processamento dos jobs para os comandos equivalentes ao cluster no qual está instalado.

Uma vez que esteja rodando em um head node do cluster, ele se conecta ao Banco de Dados e captura os jobs escalonados para o cluster em questão.

Quando o processamento do job termina, retorna os resultados para o Banco de Dados.

Durante esse processo, o Deemon cria uma thread que recebe, através de uma conexão SQL, o arquivo de dados e (caso seja o primeiro job de um projeto) o código fonte da aplicação do Banco de Dados.

Então, compila o código fonte usando o compilador local do sistema operacional.

Tal compilador é reconhecido quando o Deemon é iniciado.

Depois disso, o job é submetido ao cluster e a thread fica esperando que ele termine e gere os arquivos de saída.

Uma vez capturados, os arquivos de saída são enviados para o Banco de Dados e a thread é reiniciada.

O uso de threads permite um melhor uso do balanceamento de carga do escalonador do cluster, possibilitando a execução de jobs simultâneos (caso seja suportada pelo cluster).

Outra função do Deemon é enviar à base de dados informações sobre a carga do cluster (quantidade de processadores livres).

Esta é uma das informações usadas pelo Aangel ao distribuir jobs entre os clusters conectados ao Interop Router.

O Deemon é implementado em Python 25, linguagem que suporta diversas plataformas, e pode facilmente ser portado para novos clusters ou web servers, usando diferentes bancos de dados, escalonadores, compiladores e/ou linguagens de programação, ou mesmo sistemas operacionais.

Atualmente, foram testados Deemon em clusters Linux Beowulf, usando PBS.

Apesar de ainda estar em fase de desenvolvimento, o Interop Router já é uma resposta funcional para o que se propõe desde o começo, disponibilizar o poder de clusters de diferentes plataformas de forma transparente e otimizada.

Os próximos passos de desenvolvimento incluem um maior controle do processamento para administradores e novos níveis de acesso do usuário.

Também há a possibilidade do envio de jobs para aplicações previamente instaladas no cluster, no caso de processamento de tarefas paralelas específicas.

Têm por objetivo contribuir com um ambiente para o desenvolvimento e execução distribúida de aplicações para a computação científica, considerando o ambiente de desenvolvimento visual denominado VPE-GM (Visual Programming Environment for the Geometric Machine Model), no qual são modeladas as aplicações, e o ambiente de execução distribúida caracterizado pelo VirD-GM (Virtual Distributed Geometric Machine Model).

Ambos componentes de software fundamentam-se nas abstrações no modelo GM (Geometric Machine) quando de sua construção.

Assim, aplicações desenvolvidas no VPE-GM são exportadas através de um arquivo descritor da aplicação, em XML, possibilitando que o VirD-GM execute a aplicação de forma distribúida.

O suporte à execução distribúida é implementado sobre o middleware EXEHDA (Execution Environment for High Distributed Applications), o qual provê abstrações inerentes ao processamento paralelo e/ou distribúido.

A especificação do paralelismo nas aplicações desenvolvidas é feita de forma explícita pelo programador no ambiente VPE-GM.

Por sua vez o ambiente de execução VirD-GM analisa a aplicação e gerencia a disponibilidade de recursos computacionais para a execução.

A especificação arquitetural do ambiente de execução VirD-GM teve por objetivo prover suporte à execução de processos elementares e a dois tipos de construtores da GM, processos seqüenciais e processos paralelos, já validados quando do desenvolvimento de aplicações.

Considerando este cenário, como motivação para este trabalho, tem-se a extensão do VirD-GM para o suporte as computações não-determinísticas e estruturas de repetição de acordo com a modelagem do construtor Soma N ao-Determin istica e do construtor Iterativo, respectivamente.

A estrutura dual ao construtor produto paralelo que modela o conflito de acesso à memória GM corresponde ao construtor Soma N ao-Determin istica (SND), tendo como principal característica a aleatoriedade da escolha dentre dois ou mais processos GM a serem executados.

Pela aplicação do construtor Soma N ao-Determin istica a dois processos elementares d e e, tem-se a construção de processos mutuamente exclusivos, os quais estão definidos pelo mesmo índice k, representando a posição conflitante de escrita na memória, e pelas operações distintas d.

Quando de sua execução, tem-se que a troca de valor da varíavel associada à k-esima posição de memória ocorre de formá aleatória.

Representa o processo resultante da aplicação do construtor Soma Não-Determin istica sobre os processos elementares d (k) e e(k).

Construtor Soma Nao-Determinística.

No diagrama de atividades apresentado na Figura 2, resume-se o comportamento do construtor SND.

Neste caso, tem-se, Cria Lista de Processos, Esta etapa é responsável pela interpretação do arquivo XML, referente aos parâmetros que descrevem o construtor SND.

Os parâmetros que definem a lista de processos referentes ao construtor podem ser processos elementares ou composições envolvendo outros construtores, neste caso correspondendo a estruturas de dados do tipo Envelope.

Apresenta o arquivo XML gerado no ambiente VPE-GM do construtor SND, o qual contém três processos.

Define Intervalo de Seleção, Nesta etapa, determina-se o intervalo no qual será realizada a seleção aleatória de processo elementar ou composição de construtores de processos (Envelope) a ser enviado para lista de execução.

Seleciona Processo, ocorre a escolha do processo a ser executado na posição de conflito, com base no método aleatório e no intervalo de seleção. 

Adiciona Processo Selecionado, Finalizando, ocorre o envio do processo selecionado para a lista de processos a serem executados, incluindo a verificação de suas dependências.

Diagrama de atividades do construtor Soma Nao-Determinística.

De acordo com as abstrações do modelo D-GM, e correspondente modelagem na VPE-GM, a aplicação do construtor Iterativo ocorre a partir da estrutura do tipo envelope, caracterizando a repetição de operações, com número de repetições varíavel e definido por parâmetro de controle (tipo inteiro n) no momento de sua instanciação.

Originalmente, o construtor Iterativo já modelado no VPE-GM considerava somente a construção seqüencial de processos.

Neste trabalho propõe-se duas extensões deste construtor, a iteração paralela de processos, os quais não possuem dependência de dados entre os processos, e a iteração referente à posição da memória.

Neste caso, na varíavel de controle indica-se não o número de iterações do construtor mas um intervalo de valores.

Assim, a execução ocorre não apenas na primeira posição de memória indicada no processo elementar, e sim nas posições definidas no intervalo indicado pela varíavel de controle do construtor.

Mostra um diagrama de atividades que descreve o funcionamento do construtor de Iteração Seqüencial, cuja principal característica é a inserção de dependências entre cada uma das iterações, ou seja, a n-ésima iteração depende da correspondente execução da iteração n 1.

Mais especificamente, tem-se, Verifica Dependencias Externas, com verificação de parâmetros para execução do processo referente à corrente iteração.

Testa Fim da Iteração, realiza controle do número de iterações considerando duas etapas, descritas na seqüencia, caso o número de iterações não seja satisfeito, verificação de dependências internas, diferenciando a estrutura de controle nos dois tipos de construtores Iterativos.

Inserção do processo correspondente à iteração atual em uma lista de armazenamento temporário, já com as dependências de dados resolvidas.

Retorna Processos Iterados, quando finaliza o processo de iteração, os processos gerados na iteração são enviados para a lista de processos de execução.

Salienta-se que a principal diferença no funcionamento do construtor de Iteração Sequencial para o construtor de Iteração Paralela é a inexistência de dependências entre cada uma das iterações, ou seja, as iterações podem ser executadas concorrentemente.

Na Figura 3, verifica-se que o diagrama de atividades modelando o funcionamento da Iteração Seqüencial é análogo ao construtor de Iteração Paralela, exceto pela etapa de verificação de dependências internas, garantindo neste caso, a não ocorrência de conflito no acesso as posições de memórià dos correspondentes processos.

O trabalho introduziu estruturas de controle de iteração ao VirD-GM, incluindo uma análise do não-determinismo.

Obteve-se a modelagem do construtor Soma Não-Determinística e do construtor Iterativo, que nortearam a implementação dos mecanismos de gerência da execução do VirD-GM para atender as demandas introduzidas por estes novos construtores.

Diagrama de atividades da Iteração Seqüencial e Paralela.

Como continuidade da proposta de colaborar no Projeto D-GM, tem-se a modelagem e implementação do construtor Soma Determinística e da estrutura para suporte aos tipos de teste (universal, existencial e determinístico) disponibilizados nas interfaces do VPE-GM.

Assim também, somam-se novos construtores relacionados com a geração de processos e alocação de memória de natureza dinâmica.

Consideram-se ainda a construção dos módulos de suporte aos novos construtores no VirD-GM e indicação de adaptação nas interfaces do VPE-GM.

Visando a validação dos construtores, são relevantes em etapas futuras as aplicações da computação científica no VirD-GM, independente do núcleo de gerência e, as quais possibilitem uma abstração nas etapas de desenvolvimento do Projeto D-GM.

A resolução de sistemas lineares esparsos do tipo Ax = b, onde A é a matriz de coeficientes, b o vetor de termos independentes e x o vetor solução, é um dos principais problemas da álgebra linear e representa o núcleo de muitos problemas na engenharia e na computação científica.

A importância do estudo de sistemas esparsos não se deve somente por ter aplicação em diversos problemas reais, mas também por envolver algoritmos e estruturas de dados mais complexas que em sistemas densos.

Este trabalho tem por meta apresentar uma versão paralela do solver BAND.

O solver verificado BAND atua na resolução de sistemas lineares com matrizes esparsas do tipo banda e corresponde a um método numérico desenvolvido através da biblioteca de alta exatidão C-XSC.

Além disso, os algoritmos utilizados no solver, possibilitam a geração de resultados com alta exatidão e com controle automático de erros.

Com a finalidade de melhorar o desempenho do solver, foi desenvolvida uma versão paralela do BAND para máquinas paralelas do tipo clusters de computadores, pois o solver apresentou um alto tempo de processamento na avaliação realizada.

A partir desta avaliação, pôde-se observar que o desempenho do solver é prejudicado em um nível mais elevado pelo aumento da largura de banda da matriz do que pelo aumento da dimensão.

O desenvolvimento da versão paralela do BAND foi realizada com uso da biblioteca MPI e através de threads OpenMP.

Com o auxílio da biblioteca MPI, foi implementado um algoritmo de divisão e conquista na fase de refinamento iterativo do solver.

Para a realização da comunicação entre processos, foram utilizadas rotinas de comunicação coletiva da biblioteca MPI, mais especificamente de uma adaptação da biblioteca MPI desenvolvida por, com suporte para determinados tipos de dados do C-XSC.

Além disso, algumas rotinas internas do solver foram paralelizadas com a biblioteca OpenMP.

Este trabalho é uma extensão da tese de doutorado de e corresponde a um dos trabalhos que fazem parte do projeto em cooperação internacional com as universidades alemãs Wuppertal e Karlsruhe para a otimização da biblioteca C-XSC.

O solver BAND tem por objetivo a resolução de sistemas de equações lineares esparsos com matrizes do tipo banda.

As matrizes do tipo banda são matrizes onde os elementos não-nulos são alocados em faixas (bandwidths) em torno da diagonal principal.

Para matrizes do tipo banda, o algoritmo utilizado para resolver sistemas densos não é eficiente, uma vez que é necessária uma grande quantidade de memória para alocação dos sistemas e, além disso, há um aumento no tempo de execução.

Esse problema pode ser amenizado com a utilização de técnicas de armazenamento de matrizes esparsas.

No solver BAND, pode ser visto a utilização de uma dessas técnicas, conhecida como Compressed Diagonal Storage (CDS).

Em foram realizados testes de desempenho da versão seqüencial do solver, com variação da dimensão da matriz de entrada e também com variação da largura de banda.

A partir desses testes, pôde-se verificar o impacto do aumento da largura de banda em relação ao aumento da dimensão da matriz.

A estrutura do programa original foi mantida, de forma a executar as rotinas seqüencialmente até a região do refinamento iterativo, apenas o cálculo do defeito da aproximação da solução é executado em paralelo através da criação de threads OpenMP.

Para a região do programa onde ocorre o refinamento iterativo foi determinada uma partição por domínio.

A partir disto, o algoritmo implementado foi o de divisão e conquista.

Seguindo essa estratégia foi determinado que os dados necessários para a execução dessa região fossem distribuídos por meio de um particionamento bloco-linha, ou seja, cada processo recebe um bloco de linhas do conjunto de dados global para posteriormente realizar a sua parte da tarefa.

Desta forma, o problema original foi divido em subproblemas menores de mesmo tamanho, onde cada subproblema representa um bloco-linha dos dados principais.

Para isso, foram utilizados as funções de comunicação coletiva MPI_Scatter e MPI_Gather com suporte ao C-XSC.

O ambiente utilizado para a realização dos testes de desempenho da versão paralela do solver BAND corresponde a uma máquina paralela de arquitetura NOW (Network of Workstations), composta por 16 nodos homogêneos entre si, com a seguinte configuração.
Processador Intel Core 2 Duo E4500 (freqüência de 22 GHz, 2 MB de cache L2), com 2 GB de memória principal, sistema operacional Ubuntu 704 com compilador GCC 42, biblioteca C-XSC versão 2 2 2 e LAM/MPI 7 1 2 Os nodos foram interligados por meio de uma rede fast ethernet (100 Mbps).

O padrão para os testes foi a definição de uma matriz de Toeplitz simétrica A com 5 (cinco) bandas contendo os valores 1 (um), 2 (dois), 4 (quatro), 2 (dois), 1 (um) e todos os valores de b iguais a 1 (um).

O tempo de execução do programa foi calculado para sistemas de ordem 50000, 100000, 200000, 400000, 800000 e 1600000.

Para cada ordem de matriz foram feitas 10 (dez) execuções e, além disso, foram alocados 2 (dois) processos por nodo, a fim de tirar um melhor proveito da máquina paralela disponível.

No gráfico da Figura 1, é apresentado o speedup obtido com a paralelização.

Gráfico de speedup da versão paralela do BAND.

Através dos testes realizados, pode-se verificar que um ganho de desempenho foi obtido com a paralelização do solver.

O melhor speedup foi alcançado com 16 cores nos testes realizados para matrizes de ordem igual a 1600000.

Além disso, para matrizes de ordem 1600000, 800000, 400000 e 200000 ocorreu uma queda na aceleração do solver paralelo, quando executado com 32 cores.

Apenas, com as dimensões 50000 e 100000 ocorreu um crescimento constante do speedup.

A fim de verificar os resultados da paralelização realizada com threads OpenMP no cálculo do defeito da aproximação, foram realizados testes de desempenho de forma isolada, com a criação de 2 (duas) threads em um computador com a mesma configuração dos nodos do cluster.

A Tabela 1 apresenta os tempos de processamento do método seqüencial e paralelo, e o speedup para cada dimensão de sistema.

Valores de speedup da paralelização do cálculo do defeito.

Através dos valores de tempo apresentados, foi possível observar um ganho em desempenho de, aproximadamente, 44% com relação aos tempos seqüenciais do método do cálculo do defeito.

A partir deste trabalho, verificou-se que o aumento no tempo de processamento do solver é mais acentuado à medida que o número de bandas da matriz aumenta, enquanto que à medida que a dimensão aumenta o tempo de processamento não sofre um aumento tão acentuado quanto o da variação do número de bandas.

Em relação à paralelização do solver, embora tenha sido obtido um ganho de desempenho, foi observado que o speedup e a eficiência ficaram distantes do que seria ideal.

Nota-se também que a execução do solver paralelo mostrou-se mais eficiente com um número de cores igual a 4 (quatro), enquanto que o tempo de processamento diminuiu a medida que se aumentou o número de cores, porém não de forma proporcional.

Um speedup mais próximo do ideal foi obtido na utilização de threads OpenMP na rotina que realiza o cálculo do defeito da aproximação.

Como trabalhos futuros, uma das possibilidades seria a adoção do algoritmo DotK, ao custo de uma possível perda de precisão nos resultados numéricos.

Além disso, como somente parte do solver foi paralelizada, poderia ser realizada a paralelização da rotina que faz o cálculo da decomposição LU.

Uma alternativa de paralelização seria a utilização de bibliotecas externas no solver para a realização de alguns cálculos específicos.

Neste caso, poderia ser utilizada a biblioteca numérica ScaLAPACK (Scalable LAPACK), para a realização tanto da decomposição LU quanto da fase onde são executadas a pós-substituição e retrosubstituição intervalares.

Os resultados de pesquisas dos grandes fabricantes de processadores possibilitam que os consumidores tenham a disposição novos processadores com maiores capacidades de processamento.

Dentre as inovações encontradas nos núcleos da arquitetura Intel Core da Intel pode-se destacar o cache L2 compartilhado (4 MB e 2 MB), pipeline de 14 estágios, fusão de instruções (macro-fusion), Enhanced SpeedStep, Advanced Digital Media Boost, Execute Disable Bit, Intel TXT (Trusted Execution Technology), chaveamento elétrico avançado e desambiguação de memória.

Baseados nesta arquitetura foram lançados alguns núcleos entre eles o Conroe, Allendale, Kentsfield e Merom sendo este último a versão desta arquitetura para notebooks.

Já a AMD, lançou a AMD64 que inclui entre outras tecnologias o processamento nativo em 32 e 64 bits, o controlador de memória integrado ao processador, a tecnologia HyperTransport, e as tecnologia Cool'n'quiet, PowerNow e Enhanced Virus Protection.

Verdadeiro, dual-core, seus dois cores são fabricados no mesmo die e não em separados, os dois núcleos são construídos sobre o mesmo waffer.

Com essa arquitetura foram lançados alguns núcleos entre eles Manchester, Toledo, Windsor, Brisbane, Taylor, Trinidad e Tyler.

Há algum tempo muitos programadores vem utilizando o conceito de múltiplas threads para obter um melhor desempenho em seus aplicativos.

Essa tecnologia aliada a multi-core, faz com que os aplicativos sejam executados com um melhor desempenho fazendo com que as threads sejam executadas em paralelo, uma em cada núcleo do processador.
São abordados os dois últimos escalonadores lançados utilizados no GNU/Linux, a sua definição e características.


Abordado sobre a implementação do algoritmo.

Possui alguns estudos de casos com seus respectivos resultados, análises e um comparativo com os resultados obtidos e na seção 6 algumas considerações finais.

O escalonador do SO Linux kernel 26, tornado público no final de 2003, foi chamado de O, pois mantém o tempo de seleção de um processo para executar constante, independentemente do número de processos.

Este escalonador é baseado em tempo compartilhado, onde o tempo do processador é dividido em fatias de tempo (quantum) que são alocadas aos processos.

A partir da versão 2 6 23 do kernel do Linux, o escalonador O foi substituído pelo Completely Fair Scheduler (CFS).

De acordo com Ingo Molnar, "80% do projeto do CFS pode ser resumido em uma única frase, CFS basicamente modela em um hardware real um processador ideal e precisamente multi-tarefa".

Um processador ideal executa vários processos em paralelo, em que cada processo ocupa exatamente a mesma fração de energia do processador, ou seja, se apenas um processo está sendo executado, ele ocupa 100% do processador, mas se três processos estão sendo executados, cada um ocupa 33,3% do processador.

Mas na prática, apenas uma tarefa pode ser executada de cada vez, o que torna essa situação injusta para os processos que devem ficar esperando.

O CFS torna o escalonamento justo, quando um processo espera pela CPU, é calculado o tempo que ele ocuparia em um processador ideal (tempo calculado é igual ao tempo de espera dividido pelo número de processos esperando pelo processador, em nanossegundos), este é o tempo de execução destinado ao processo.

O problema definido para utilizar os recursos da programação multi-thread no intuito de explorar os recursos oferecidos pelos novos processadores multi-core foi o cálculo da absorção da água pelas raízes.

Calculo este realizado através da paralelização do problema inversométodo de procura em rede, que demanda de grande poder de processamento.

O modelo matemático que descreve o movimento da água no solo, a paralelização do cálculo da retirada de água do solo pelas raízes de plantas, com base em dados do teor de água foi publicado em.

O problema proposto neste trabalho é a paralelização de um algoritmo seqüencial, cuja finalidade é estimar a quantidade de água retirada do solo pelas raízes de plantas, com base em dados do monitoramento do teor de água do solo.

Os dados experimentais e resultados da implementação seqüencial demandam elevados tempos de processamento mesmo em uma malha com poucos pontos.

Assim, torna-se difícil a exploração de resultados mais precisos com malhas maiores na resolução do problema direto, devido ao crescimento exponencial do tempo de execução e da grande dependência de dados imposta pela aplicação.

Então, uma nova implementação paralela foi desenvolvida em linguagem C utilizando threads, onde o escalonamento das atividades foi efetivado de modo que cada thread receba os argumentos do processo principal e efetue o cálculo do problema direto retornando os resultados.

Assim o processo principal recebe as soluções parciais e seleciona os melhores aproximando o resultado da solução ótima.

Primeiramente para programar o aplicativo com a utilização de threads foi utilizado a biblioteca pthread, do padrão POSIX, esta foi incluída no programa pelo comando, "#include <pthread h>".

Esta biblioteca inclui diversos comandos que são utilizados para gerenciar as threads entre eles os mais importantes são, "pthread_create(thread, attr, rotina, arg)", Permite criar uma thread, partindo do código de uma função existente no corpo do programa.

"Pthread_join(thread, retorno)", faz com que o programa aguarde o término da thread indicada para dar seqüência para a execução.

"Pthread_t var",cria uma variável do tipo pthread_t que é o descritor das threads.

"Pthread_attr_t var", cria uma variável do tipo pthread_attr_t onde serão armazenadas as informações ou atributos utilizadas na criação das threads.

O estudo de caso a seguir foi realizado em um notebook com a seguinte configuração processador Intel Mobile Core 2 Duo T7500 de 22 GHz, núcleo Merom, 4 MB de Cache L2, FSB 800 MHz, 4 GB DDR2 de memória RAM, OpenSUSE 11 64 bits (kernel 2 6 255-11-default e gcc 4 3 1), 4 GB de Swap.

OpenSUSE 103 64 bits (kernel 2 6 225-31-default e gcc 4 2 1).

Como se pode observar os resultados demonstram que utilizando threads o tempo de execução é consideravelmente reduzido em comparação a um programa seqüencial, 44,98% de redução no O e 44,74% de redução no CFS.

Pode-se observar que em relação ao antigo escalonador, o O, o escalonador CFS possui um ganho de desempenho na casa de 3%, isso sem alterar nada de hardware, somente alterando o algoritmo de escalonamento.

Tempo de Execução de uma malha 128 x128 (Intel) com dois escalonadores.

O segundo estudo de caso também foi realizado em um notebook, com a seguinte configuração AMD Turion 64 X2 Mobile TL-50, núcleo Taylor, 2 x 256 KB de Cache L2, 16 GHz, FSB HyperTransport 800 MHz, 1 GB DDR2 de memória RAM, OpenSUSE 11 64 bits (kernel 2 6 255-11-default e gcc 4 3 1), 1 GB de Swap.

OpenSUSE 103 64 bits (kernel 2 6 225-31-default e gcc 4 2 1).

Observa-se uma redução no tempo de execução do algoritmo utilizando as threads, em média cerca de 48,77% de redução no O e 49,11% de redução no CFS.

Ainda foi observado um ganho de desempenho de até 8% na utilização do novo escalonador CFS ao O.

Tempo de Execução de uma malha 128 x128 (AMD) com dois escalonadores.

O problema abordado neste trabalho constitui-se da paralelização do problema inverso para a resolução do problema da retirada de água do solo pelas raízes de plantas.

Foi observado uma importante ganho de desempenho nos tempos de execução em relação ao programa sequencial e ainda uma redução no tempo de execução com o novo escalonador do GNU/Linux, o CFS, em relação ao O.

Além disso, mesmo aumentando o número de threads de 2 para 60, o desempenho é pouco alterado.

Isto ocorre, pois não há mais núcleos disponíveis para executar novamente em paralelo.

A resolução de sistemas densos de equações lineares é uma operação que possui várias aplicações nas áreas das ciências e engenharias, em especial para os campos da simulação e modelagem.

O solver LSS (Linear System Solver) é um programa que tem como tarefa a resolução de sistemas lineares com matrizes densas, controlando a instabilidade numérica de máquina por meio do paradigma da Computação Verificada, com auxílio da biblioteca de alta exatidão C-XSC.

Através disso, o solver é capaz de garantir a exatidão da solução final, porém ao custo de um baixo desempenho, como pode ser observado em.

Devido ao alto tempo de processamento do solver, este trabalho apresenta uma versão paralela do LSS para máquinas de memória distribuída, bem como uma versão sequencial otimizada, ambas mantendo tanto a qualidade numérica do solver original e suas características essenciais com relação à Computação Verificada.

Para a implementação do programa paralelo, foi adotado um modelo de algoritmo por divisão e conquista com o auxílio da biblioteca MPI, que permite a exploração de um paralelismo entre-nós em um agregado, e através da criação de POSIX threads em cada processo, capaz de tirar proveito de arquiteturas multi-core de processadores.

Além disso, para efetuar a distribuição dos dados das matrizes entre os processos MPI, foram desenvolvidas funções de comunicação coletiva com suporte a determinados tipos de dados da biblioteca C-XSC, como uma tentativa de se reduzir o custo da comunicação entre os processos paralelos.

O desenvolvimento dessas funções foi baseado nas funções para comunicação ponto-a-ponto apresentadas.

O solver LSS é composto, basicamente, por dois algoritmos principais, uma etapa de precisão simples e uma etapa de precisão dupla.

Quando um sistema linear é submetido ao solver, o algoritmo de precisão simples executa uma série de sub-tarefas a fim de tentar encontrar uma solução para o sistema linear.

Caso a etapa de precisão simples não seja capaz de encontrar uma solução (por exemplo, para sistemas mal-condicionados), o programa submete o sistema linear à etapa de precisão dupla, que tenta encontrar uma solução para o sistema utilizando uma maior precisão.

A versão sequencial do solver LSS foi otimizada por meio da eliminação de computações redundantes identificadas entre as sub-tarefas das etapas de precisão simples e dupla, e através da otimização no método de Gauss-Jordan, utilizado para efetuar a operação de inversão de matrizes.

Além da eliminação de recálculos de operações no algoritmo do solver, o método de Gauss-Jordan foi otimizado substituindo-se os objetos do tipo real da biblioteca C-XSC, por variáveis do tipo primitivo double, nativo da linguagem C++, com o objetivo de se fazer uma redução da sobrecarga introduzida pelo uso da biblioteca C-XSC na função.

Entretanto, para que o método de Gauss-Jordan continuasse numericamente estável, o uso dos objetos do C-XSC foi mantido apenas nos pontos do código que efetuavam operações aritméticas, de tal forma que a instabilidade numérica ainda fosse capaz de ser controlada pela biblioteca.

Através das otimizações implementadas no LSS sequencial, foi possível observar um ganho considerável no desempenho do solver.

Por exemplo, considerando a execução de um sistema de ordem 2048 em um computador com processador Intel Core 2 Duo E4500, à frequência de 2,2 GHz, e 2 GB de memória principal, o tempo de processamento foi reduzido de, aproximadamente, 455,7 minutos para 251,4 minutos na versão otimizada, o que representa uma diferença de 44,8% no tempo de execução.

A versão paralela do solver LSS foi desenvolvida com base na versão otimizada descrita na Sessão 2.

Através dos testes de desempenho do LSS sequencial apresentados em foi possível observar que a degradação do tempo de processamento geral do solver é causada, principalmente, por determinadas sub-tarefas das etapas de precisão simples e dupla, sendo, portanto, os trechos do programa que foram paralelizados.

O modelo de algoritmo paralelo adotado foi por divisão e conquista, onde a execução do programa principal é realizada por um processo gerenciador que, ao atingir as sub-tarefas paralelas, distribui os dados das matrizes entre n processos trabalhadores (incluindo o próprio gerenciador).

O método de particionamento das matrizes entre os processos foi por blocos de linhas contíguas (block-striped partitioning), onde as matrizes são quebradas em blocos homogêneos de linhas completas, que são alocadas aos processos.

Além disso, foram implementadas funções de comunicação coletiva para a troca de dados da biblioteca C-XSC, sendo descritas na sessão 3 1 Em são apresentadas funções MPI para a troca de dados do C-XSC entre processos, através de um modelo de comunicação ponto-a-ponto (send/receive).

Como a comunicação entre processos paralelos é um fator que compromete diretamente a eficiência de aplicações de memória distribuída, foram implementadas funções de comunicação coletiva para a distribuição dos dados das matrizes do solver.

É importante salientar que as novas rotinas foram baseadas nas implementações de Grimmer.

As novas funções MPI para comunicação coletiva suportam as operações de MPI_Bcast, MPI_Gather, MPI_Scatter e MPI_Allgather para o tipo rmatrix do C-XSC.

Para os tipos imatrix e ivector, apenas as funções MPI_Gather e MPI_Scatter foram implementadas.

Os testes de desempenho da versão paralela foram conduzidos em um ambiente do tipo NOW (Network of Workstations) no Laboratório Central de Informática da Universidade de Passo Fundo.

O agregado foi constituída por 32 computadores, cada um com um processador dual-core Intel Core 2 Duo E4500, à frequência de 2,2 GHz, e com 2 GB de memória principal.

A comunicação entre os nodos é feita por uma rede fast ethernet, e intermediada por um switch.

O sistema operacional instalado em cada nodo foi o GNU/Linux Ubuntu 704 (kernel 2 6 22), com distribuição LAM/MPI 7 1 2, biblioteca C-XSC 2 2 2 e compilador GCC 4 1 Os tempos de processamento do solver paralelo foram obtidos para sistemas de ordem 64, 128, 256, 512, 1024 e 2048, com 2, 4, 8, 16, 32 e 64 processos, alocando-se 2 processos MPI por nodo do agregado.

A média dos tempos foi feita com base em 10 repetições para cada caso.

O gráfico ilustra o speedup da versão paralela do solver LSS.

Embora tenha sido possível notar uma aceleração razoável para as sub-tarefas paralelas de forma isolada, os testes de desempenho do solver LSS como um todo mostraram um crescimento mais tímido para o speedup, e, consequentemente, uma queda bastante acentuada para a eficiência.

Esses resultados foram obtidos devido ao fato de que a sub-tarefa de inversão de matrizes pelo método de Gauss-Jordan não foi paralelizada, sendo que essa sub-tarefa representa um dos gargalos do sistema.

Speedup do solver paralelo.

Com o objetivo de verificar o ganho em se utilizar um modelo de comunicação coletiva com relação a um modelo de comunicação ponto-a-ponto, foi feita uma comparação entre os tempos de comunicação para o envio de uma matriz do tipo rmatrix de ordem 2048, utilizando a função de comunicação coletiva MPI_Bcast, e as funções de comunicação ponto-a-ponto MPI_Send e MPI_Recv.

A matriz foi replicada entre 2, 4, 8, 16, 32 e 64 processos (dois processos por nodo).

Através dos tempos de comunicação obtidos, foi possível observar uma redução considerável no tempo de replicação da matriz.

Com 64 processos, o tempo gasto pelas funções de send/receive foi de 408,1 segundos, enquanto que a função que efetua broadcast levou em torno de 14,4 segundos, representando uma diminuição de, aproximadamente, 96,5% no tempo de envio.

Por meio do detalhamento das sub-tarefas que compõe o algoritmo do solver foi possível obter um considerável ganho em desempenho na versão sequencial através da eliminação de recálculos efetuados no algoritmo, além da otimização do método de Gauss-Jordan, que também contribuiu para o ganho em desempenho dessa versão.

Em relação ao uso da biblioteca C-XSC em um ambiente paralelo, foi possível perceber que o uso de funções da biblioteca MPI para comunicação coletiva reduz consideravelmente o custo da comunicação de dados pela rede, se comparado com funções de comunicação ponto-a-ponto.

Através do desenvolvimento dessas funções, estendeu-se ainda o conjunto de funções MPI para comunicação ponto-a-ponto dos tipos de dados do C-XSC, disponibilizadas por Grimmer.

Embora as sub-tarefas paralelas, de forma isolada, apresentaram um speedup mais próximo do ideal, como é verificado em, o desempenho geral do solver LSS mostrou-se com um crescimento não muito próximo ao ideal à medida que a execução é particionada em mais processos.

Esse efeito é causado, principalmente, pelo método de inversão de matrizes, que é realizado de forma sequencial no processo gerenciador.

Portanto, como trabalhos futuros ficam o estudo do método de Gauss-Jordan a fim de paralelizá-lo e a busca por métodos alternativos mais eficientes para realizar a inversão de matrizes como, por exemplo, o método da decomposição LU.

Além do mais, o método de inversão a ser adotado deve continuar com a característica de ser numericamente estável, para que a qualidade numérica dos resultados possa ser mantida.

A Fluidodinâmica Computacional, conhecida como CFD (Computational Fluid Dynamics) é uma área de grande interesse científico que objetiva o entendimento de problemas práticos da aerodinâmica, termodinâmica, hidráulica entre outros.

As análises desses problemas podem ser desenvolvidas numericamente através de simulações, essas são feitas através de métodos computacionais aplicados a um modelo matemático.

Os modelos matemáticos que descrevem o comportamento de escoamentos são equações diferenciais parciais não-lineares e que não apresentam solução analítica.

As simulações numéricas do projeto envolvem um grande número de iterações, o que torna uma simulação feita com um algoritmo totalmente seqüencial dispendiosa.

A possibilidade de paralelizar a rotina utilizando a interface de programação aplicativa OpenMP, surge como uma alternativa para adaptar o código e tornar as simulações mais rápidas e precisas.

Inicialmente o código utilizado nas simulações foi reescrito na linguagem C++, o objetivo agora é fazer simulações com o mesmo para depois então paralelizá-lo e simular os escoamentos utilizando um computador multi-core.

O modelo matemático de Navier-Stokes na forma adimensional consiste de uma equação para a quantidade de movimento do fluido na direção x, uma equação para a quantidade de movimento do fluido na direção y, com a aproximação de Boussinesq, uma equação do tipo Poisson para a pressão e uma equação para a temperatura, respectivamente, representadas pelas equações, e.

As equações foram reescritas usando coordenadas generalizadas, isso porque a distância entre os pontos da malha computacional é variável, as derivadas foram aproximadas usando diferenças-finitas e os sistemas algébricos obtidos das aproximações foram resolvidos usando Runge-Kutta de terceira ordem e relaxações sucessivas.

As simulações numéricas são obtidas considerando uma malha computacional em que para cada ponto (nó) há a necessidade do cálculo das propriedades físicas temperatura, pressão e velocidades através do modelo matemático.

Representam uma malha computacional de 34 por 70 pontos, respectivamente ao longo do raio e angularmente.
Com refinamento nas paredes, obtida pela rotina seqüencial e visualizadas pelo software Visual, versão 1 2 Considerando que o cálculo das propriedades físicas para cada ponto da malha computacional é feito repetidamente até que o escoamento atinja o regime permanente (quando não ocorrem mais mudanças no escoamento em função das iterações, o que acaba tornando às simulações muito trabalhosas e requerendo grande capacidade de processamento.
Planeja-se que com o uso do OpenMP o trabalho seja dividido em threads, diminuindo de maneira significativa o tempo de execução do algoritmo.

A primeira simulação numérica considera a convecção natural entre cilindros concêntricos, admitindo que a parede do cilindro interno tem temperatura adimensional maior do que a parede do cilindro externo.

Foram feitas 1829000 iterações utilizando um computador Pentium IV 3,0 GHz com Windows XP.

O tempo para realizar uma simulação foi de aproximadamente 120 horas.

A malha utilizada, tem um total de 2380 pontos.

Quanto maior o número de pontos da malha melhor serão os resultados, entretanto, maior será o tempo de processamento para realizar a simulação.

Linhas de corrente obtidas pelo presente trabalho e obtidas por Ming-I, p 323-343.

Ilustra o bom resultado obtido pela primeira simulação, se comparado com o resultado obtido por Ming-I, que usou técnicas numéricas diferentes das usadas no presente trabalho, podemos considerar que a rotina é válida, e pode ser utilizada em outros tipos de simulações.

Com o código devidamente adaptado pretende-se usar computadores com maior capacidade de processamento.

É possível realizar inicialmente as simulações com um computador Intel Core 2 Duo 1,73 GHz, 2 GB de memória RAM e Windows Vista Ultimate.

O problema que foi apresentado no decorrer do artigo trata de um código para simulação numérica de fluidos que foi construído em Fortran 90 e depois, visando à paralelização, desenvolvido em C++.

Com a paralelização será possível diminuir o tempo das simulações e aumentar a capacidade de processamento do algoritmo, e então realizar simulações com malhas mais complexas e obter dessa forma resultados mais precisos.

As arquiteturas multicore já fazem parte da realidade da maioria dos usuários de computadores.

Este fato implica em um grande aumento da importância de dominar os conceitos de programação paralela.

É raro encontrar programadores que conseguem ter um entendimento pleno dessa área, principalmente devido a alta complexidade na expressão do paralelismo das aplicações.

Linguagens funcionais apresentam uma propriedade interessante nesse contexto.

Um programa funcional puro não necessita que suas subexpressões sejam avaliadas seqüencialmente e em uma ordem definida.

O controle das comunicações de resultados se dá pelo controle do fluxo de dados.

Tais propriedades favorecem o uso de algoritmos de lista.

No projeto Anahy foi proposto um ambiente de execução eficiente baseado em algoritmo de lista.

A implementação deste em Cordeiro confirmou que programas paralelos cujo controle acontece pelo fluxo de dados podem ser executados de forma eficiente em arquiteturas multiprocessadas.

Du Bois propôs uma extensão a linguagem funcional Haskell, com o objetivo de facilitar a notação do paralelismo presente nas aplicações.

Esta nova linguagem usou como ambiente de execução uma máquina virtual (RTS, RunTime System) a ser executada em arquiteturas distribuídas.

Este trabalho propõe a modificação desta máquina virtual de modo que a execução passe a acontecer em arquiteturas multicore, tendo como suporte um núcleo de escalonamento baseado em algoritmo de lista.

A linguagem pFun é uma linguagem funcional pura que herda a sintaxe de Haskell, porém acrescida de duas primitivas para a descrição da concorrência, par e sync.

A primitiva par indica que a expressão pode ser avaliada em paralelo e a primitiva sync retorna o resultado de uma expressão par.

A implementação da linguagem está organizada em três camadas, sendo a de mais alto nível correspondente à interface de programação.

A camada intermediária é responsável pela portabilidade de código.

A execução em ambientes heterogêneos é garantida através da compilação para bytecode, o qual é executado na máquina virtual pFun (RTS).

O interpretador de bytecodes é baseado na SECD machine, utilizando-se apenas uma pilha.

As estruturas internas são alocadas em tempo de execução em uma área de memória controlada denominada heap, as quais são automaticamente liberadas por um coletor de lixo quando não são mais necessárias.

A camada de mais baixo nível é responsável pela portabilidade e desempenho.

A distribuição da carga computacional é feita por um escalonador, o qual garante a ordem de execução das expressões segundo as anotações no código das primitivas par e sync.

Há duas estruturas relevantes no funcionamento da linguagem, a fila de tarefas e a fila de threads executáveis.

A fila de tarefas, também chamada de taskpool (ou ready queue) é uma lista de tarefas que podem potencialmente ser executadas em paralelo.

Tarefas são objetos Par que são criados ao ser encontrada a primitiva par.

A fila de threads executáveis, ou threadpool, tem a função de guardar threads que bloquearam-se, esperando avaliação de outra threads, mas foram desbloqueadas e estão aptas a seguirem sua execução.

O conceito thread aqui representa um objeto Par em execução.

A linguagem pFun foi desenvolvida originalmente para arquiteturas com memória distribuída, como clusters.

A estratégia de escalonamento é próxima ao modelo cliente-servidor.

A troca de mensagens implementada pela RTS ocorre de maneira distribuída entre um work server (servidor) e seus slaves (clientes).

Um programa pFun é executado inicialmente por um work server, o qual gera tarefas para serem distribuídas entre seus slaves ou até mesmo outros work severs.

Quando um slave pede por trabalho, o work server retira uma tarefa da sua taskpool, e envia para ser avaliada pelo slave.

O slave não possui taskpool, portanto, se durante a avaliação do programa o slave encontra uma chamada par, uma thread é criada para avaliar a expressão localmente, enquanto isso a thread que estava em execução fica bloqueada aguardando.

Em virtude disto, surge o principal problema desta implementação, quando o programa gera tarefas de diferentes granulosidades pode ocorrer um grave desequilíbrio na distribuição de carga no sistema.
Onde alguns slaves possuem tarefas grandes para serem avaliadas ficando assim sobrecarregados, enquanto outros slaves avaliam pequenas tarefas, podendo ficar grande parte do tempo ociosos.

Nesta implementação não há comunicação entre slaves.

Quando um work server recebe um pedido de trabalho, a representação da tarefa no heap (o Par), é serializada e enviado para o destinatário.

Caso o work server não possua tarefas em sua taskpool, ele então envia uma mensagem comunicando que não possui trabalho.

Nesta versão, atualmente em desenvolvimento, é feito o uso de apenas uma máquina virtual pFun.

A organização deixa de ser no modelo cliente-servidor em que o escalonamento de tarefas é realizado pelo work server e passa a ser utilizada a idéia de processadores virtuais, onde o escalonamento é controlado por mecanismos de sincronização.

O uso de processadores virtuais eliminam a hierarquia existente no modelo distribuído entre work server e slaves.

Os processadores virtuais são processos leves que compartilham o mesmo espaço de memória.

Ilustra as principais diferenças entre o modelo distribuído e o para memória compartilhada.

Cada processador virtual manterá um heap e uma threadpool locais, de forma semelhante à versão distribuída.

O motivo dessa decisão é evitar que o uso de um heap global possa serializar a execução, além de evitar possíveis condições de corrida.

Os processadores virtuais possuem suas próprias taskpool, porém há uma taskpool global contendo ponteiros para a taskpool de cada processador virtual, criando a abstração de uma taskpool globalmente compartilhada.

Esta abstração foi adotada para permitir a estratégia de escalonamento baseada em Work Stealing.

Esta estratégia baseia-se no balanceamento de carga entre os processadores, de modo que um processador ocioso roube uma tarefa de outro processador sobrecarregado.

O roubo deve ser de tarefas com granulosidade grossa, que apresentam maior carga computacional, pois evita a necessidade de muitos roubos sucessivos e diminui a sincronização.

Comparação entre Implementação Distribuída e Implementação com Memória Compartilhada.

O modelo distribuído não apresentava bom desempenho em aplicações com diferentes granulosidades.

A versão para memória compartilhada adota o uso de work stealing entre os processadores virtuais, o que faz com que a carga computacional seja melhor distribuída, eliminando este problema.

Em aplicações do tipo bag of tasks, onde todas as tarefas possuem granulosidade homogênea, o desempenho foi maior devido a não haver os custos de comunicação da rede.

Neste artigo foi apresentada a implementação adaptativa da linguagem pFun para sistemas multiprocessados de memória compartilhada.

Foram mostradas soluções para problemas encontrados na versão distribuída.

Em trabalhos futuros pretende-se terminar a validação e realizar testes de desempenho no pFun com work stealing.

Além disso, pretende-se implementar a estratégia de work stealing na versão distribuída e após, uma versão híbrida para possibilitar a exploração de paralelismo inter e intra nó.

Apesar do desenvolvimento dos equipamentos de informática ocorrido nas últimas décadas, a necessidade de processar dados cada vez mais complexos e em maior quantidade faz com que a construção de computadores de alto desempenho continue sendo uma importante área de pesquisa em Computação.

Uma alternativa econômica e muito importante aos computadores de alto desempenho são os clusters de computadores.

Um cluster é basicamente um sistema formado de múltiplos computadores, cada um dos quais de poder computacional limitado, conectados por meios de comunicação adequados, onde é possível executar um único programa usando todas as máquinas simultaneamente.

Este procedimento faz com que o sistema como um todo tenha um poder computacional elevado e em muitos casos comparável ao de um computador de alto desempenho.

No caso de problemas paralelizáveis, que exigem muito processamento, o uso de clusters é uma solução alternativa a um custo razoavelmente baixo se comparado com os altos valores necessários para a aquisição de um computador da mesma classe de processamento.

A utilização de clusters apresenta outras vantagens, escalabilidade, pois é possível que novos componentes sejam adicionados à medida que cresce a carga de trabalho, tolerância a falhas, pois o sistema pode continuar funcionando mesmo caso alguma máquina falhe, independência de fornecedores, pois, diferentemente dos sistemas proprietários dos supercomputadores, clusters podem utilizar máquinas com hardware aberto e software livre.

Este trabalho objetiva avaliar o potencial dos clusters em um procedimento específico de mineração de dados.

As tarefas de mineração de dados são sabidamente computacionalmente intensivas, pois muitas vezes exigem o tratamento de problemas de otimização combinatória.

Com isso representam uma tarefa adequada para avaliar o desempenho de um ambiente computacional paralelo.

Em linhas gerais este trabalho consiste no desenvolvimento de um programa que realiza a distribuição de uma tarefa de mineração de dados em um cluster, na execução do algoritmo de mineração considerado em cada componente do cluster, e na obtenção dos resultados de desempenho.

De forma a avaliar a vantagem ou não desta solução, são obtidos os resultados da execução da tarefa em um único computador e no cluster, que tem o número de nós gradativamente aumentado, e feitas as comparações necessárias.

Para possibilitar a execução de testes de desempenho neste trabalho, utilizou-se um cluster do tipo Beowulf, usando software livre, com 11 computadores interligados em rede Gigabit Ethernet.

Com o crescimento das bases de dados geradas de forma automática pelos sistemas computacionais, realizar análises manuais e intuitivas para extração de informações relevantes a um determinado problema tornou-se inviável.

Entretanto, a descoberta de novas informações para identificar novas oportunidades é de extrema importância para a tomada de decisões estratégicas por empresas.

A Mineração de Dados é uma das etapas da área denominada Descoberta de Conhecimento em Bases de Dados (Knowledge Discovery in Databases, KDD), que desperta grande interesse junto às comunidades científicas e industriais.

O processo de Mineração de Dados consiste, basicamente, em extrair informações interessantes, previamente desconhecidas e não-triviais de conjuntos de dados.

Esses dados podem provir de sistemas de banco de dados, de arquivos texto, ou mesmo de outros sistemas, utilizando data warehousing.

Dentre as ferramentas desenvolvidas para a tarefa de mineração de dados destaca-se o ambiente Weka (Waikato Environment for Knowledge Analysis).

Trata-se de uma coleção de algoritmos para aprendizado de máquina integrada em um ambiente computacional desenvolvido pela Universidade de Waikato.

O ambiente Weka é escrito em Java e publicado sob licença GPL, o que facilita seu uso e divulgação.

Além da possibilidade de uso via interface gráfica nativa e linha de comando, as classes integrantes do pacote Weka podem ser chamadas por qualquer outra aplicação Java, o que possibilita ao usuário escrever seus próprios programas, utilizando os algoritmos para mineração de dados.

Estas facilidades foram consideradas na escolha deste ambiente para a realização deste trabalho, visto que se desejava realizar todo o desenvolvimento do projeto em Linguagem Java.

O Weka possui procedimentos para tratar as diversas etapas do processo de extração do conhecimento.

Inicialmente, ele pode atuar na seleção, pré-processamento e transformação dos dados.

Este processo consiste basicamente na escolha dos dados a serem analisados, na redução de valores ruidosos, no tratamento de inconsistências e em modificações nos dados que levam a um formato apropriado para a análise.

Em seguida, ocorre a mineração de dados propriamente dita, onde se aplicam algoritmos específicos aos dados, obtendo-se os resultados da mineração.

Finalmente, ocorre a visualização e interpretação dos dados, quando os resultados obtidos são analisados permitindo a obtenção do conhecimento que será útil ao problema em questão.

Tipicamente em um processo de aprendizagem supervisionada em mineração de dados, após o pré-processamento e a formatação, os dados são fragmentados em dois subconjuntos, denominados base de treinamento e base de testes.

Numa primeira etapa um algoritmo de indução de conhecimento é aplicado à base de treinamento.

Com isso se obtém um modelo "treinado", que de certa forma representa o conhecimento extraído.

Numa segunda etapa o modelo obtido é aplicado ao fragmento da base de dados denominado base de testes.

Como a base de testes também é previamente rotulada, se pode medir a taxa de acerto do modelo, comparando-se o resultado obtido com a rotulação disponível na base de testes.

A técnica de Validação Cruzada consiste em dividir a base de dados em x partes (folds).

Destas, x-1 partes são utilizadas para o treinamento e uma serve como base de testes.

O processo é repetido x vezes, de forma que cada parte seja usada uma vez como conjunto de testes.

Ao final, a correção total é calculada pela média dos resultados obtidos em cada etapa, obtendo-se assim uma estimativa da qualidade do modelo de conhecimento gerado e permitindo análises estatísticas.

O pacote Weka permite várias formas de customização.

Apesar da possibilidade da criação de scripts Shell que executem os procedimentos de Mineração de Dados via linha de comando, a opção considerada ideal foi escrever programas Java para executar as tarefas necessárias.

Essa escolha, além de permitir a reutilização de classes nativas do Weka e melhor integração com os algoritmos de Mineração de Dados, possibilita a criação de várias threads e de um sistema de conexão cliente-servidor.

Assim foi criado um programa para o servidor, e outro para os clientes.

O programa servidor, primeiramente, prepara os dados que serão processados pelos clientes.

Em seguida, conecta-se com todos os clientes simultaneamente, criando uma thread para cada conexão e enviando as informações necessárias para cada cliente.

Após os clientes enviarem os resultados de seu processamento, o servidor analisa as informações para gerar um relatório final do modelo de Validação Cruzada.

Composta por 14 atributos em 48842 instâncias.

A base de dados Soybean, por sua vez, é composta por 36 atributos em 683 instâncias.

Assim, enquanto a primeira possui muitas instâncias com poucos atributos, a segunda possui poucas instâncias, porém com um número maior de atributos em cada instância.

As comparações estão presentes para Validação Cruzada usando de 2 a 10 folds, cada qual processado em um nodo.

Contém um resumo dos resultados obtidos, indicado-se a base de dados trabalhada, e se o processamento foi realizado em máquina local ou em cluster.

Resultados aplicando o algoritmo J48, e o algoritmo NaiveBayes.

O eixo X indica o número de nós do cluster e o eixo Y o tempo de processamento em segundos.

Este trabalho apresentou um experimento que utiliza clusters de computadores para reduzir o tempo de execução em problemas de Mineração de Dados.

Buscou-se avaliar a redução obtida na Validação Cruzada, empregada para quantificar o desempenho do modelo de classificação obtido.

Os resultados obtidos comprovam que esta é uma alternativa a ser considerada no caso de problemas de alta complexidade computacional.

Este artigo apresenta uma avaliação do impacto de MigBSP sobre a execução do algoritmo de Smith-Waterman de alinhamento de sequências.

O interesse aqui é observar a influência da densidade computacional na escolha dos processos à migrar.

Pesquisa parcialmente apoiada pelo CNPq e Microsoft.

O reescalonamento de processos BSP ocorre de forma dinâmica de acordo com o estado do ambiente.

A idéia do modelo MigBSP é o ganho de desempenho através da redução do tempo de execução das superetapas.

A duração de cada superetapa é igual ao tempo do processo mais lento.

Assim, o modelo busca migrar os processos com maiores cargas de computação para processadores mais rápidos, além de aproximar processos que se comunicam muito.

Uma descrição detalhada de MigBSP pode ser vista em.

A escolha dos processos candidatos à migração é baseada no Potencial de Migração (PM) de cada processo para um dado Conjunto.

Um Conjunto é a abstração de uma rede e pode ser entendido como sendo um cluster, uma rede local ou um supercomputador.

Cada um possui um gerente responsável pelo escalonamento.

O PM é encontrado através da combinação das métricas Computação, Comunicação e Memória.

Computação e Comunicação agem a favor da migração, enquanto a Memória trabalha no sentido contrário, trazendo a idéia de custo.

A métrica Computação considera informações sobre o grau de desempenho do Conjunto considerado, a predição do tempo de computação do processo e o Padrão de Computação do mesmo.

O Padrão de Computação mede a regularidade do processo quanto as instruções que realiza em cada superetapa.

O seu valor é próximo a 1 se o processo é regular e próximo a 0, caso contrário.

Padrões foram empregados pois espera-se que o processo desempenhe um comportamento similar no lugar de destino.

Assim como a métrica Computação, a Comunicação computa o Padrão de Comunicação.

A cada superetapa, esse padrão considera a quantidade de dados recebidos provenientes de processos de determinado Conjunto.

Além disso, essa métrica usa a predição do tempo de comunicação do processo.

A métrica Memória trabalha com a memória do processo, os custos de migração e a taxa de transferência entre o processo e o Conjunto alvo.

Com o PM calculado, é possível definir quais processos são candidatos a migração.

Antes de migrá-los, verifica-se a viabilidade da migração.

Isto é feito através da simulação da execução do processo em um novo recurso no Conjunto alvo.

Caso a execução no novo recurso adicionado ao tempo gasto na migração seja maior que o tempo de execução no recurso atual, a migração do processo é cancelada.

A decisão de remapeamento de processos é tomada após o final de uma superetapa, pois neste momento é possível analisar dados de todos os processos BSP sobre suas fases de computação e comunicação.

Contrário a biblioteca PUBWCL, MigBSP não realiza o reescalonamento a cada superetapas mas sim de acordo com o estado do ambiente.

Para isso, foram aplicadas adaptações que gerenciam o valor de, onde representa o número de superetapas entre duas chamadas de reescalonamento.

As idéias gerais das adaptações são, atrasar as chamadas se o sistema estiver balanceado ou torná-las mais frequentes, caso contrário, atrasar as chamadas se o sistema apresentar um padrão sem migrações nas ultimas tentativas.

A segunda adaptação é importanté para cenários onde os processos estão desbalanceados e migrações são invíaveis.

Programação dinâmica é um método de resolução para problemas que exibem as propriedades de subestruturas otimas com superposição.

Aqui tratamos do algoritmo de Smith-Waterman, utilizado para encontrar o alinhamento otimo entre duas sequências.

Ele determina a similaridade entre duas sequências de nucleotídeos ou protéinas.

O algoritmo age sobre uma matriz n×m, onde n e m são os tamanhos das sequências a serem comparadas.

Cada célula i,j da matriz, onde 1 in e 1 jm, depende da computação das células i-1,j e i,j-1.

Portanto, a computação paralela somente ocorre entre células na mesma anti-diagonal.

Além disso, a densidade da computação aumenta de acordo com a elevação de n,m, tornando o algoritmo irregular.

As colunas da matriz são mapeadas para processos.

A execução do algoritmo acontece de forma que cada anti-diagonal da matriz representa uma superetapa.

Cada processo computa os dados referentes a uma célula de sua coluna e os envia ao seu próximo vizinho.

Não há comunicação dentro de uma mesma coluna, pois as células desta estão alocadas ao mesmo processo.

Caso a matriz for quadrada de ordem n, cada processo se envolve com n superetapas, em um total de 2 n 1.

E comum em aplicações BSP o fato de nem todos processos estarem envolvidos em todas as superetapas.

O principal objetivo desta avaliação experimental é observar o impacto do modelo MigBSP em uma aplicação irregular.

Para isso, foram simulados três cenários diferentes na ferramenta SimGrid, (i) Apenas execução da aplicaçã. 
Execução da aplicação com MigBSP mas sem migrações.
Execução da aplicação com MigBSP com migrações.

Aqui serão apresentados os resultados da simulação com matrizes de 50×50 e 200×200, as quais executam por 99 e 399 superetapas, respectivamente.

Os testes foram executados com inicial igual a 2, 4, 8 e 16.

Em todos os casos, o sistema se encontra balanceado e o valor de sempre aumenta.

Além disso, os processos que apresentam PM>MAX(PM)×080 são candidatos à migração.

Os resultados da simulação são ilustrados, normalizados pelos cenários i.

Ambiente simulado e mapeamento inicial de processos para recursos.

A execução de 50 processos ocorre quando a matriz 50×50 é testada.

Os processos ocupam 800 KBytes em memória e as comunicações envolvem 50 KBytes.

Um sobrecusto de até 3% é percebido na comparação dos cenário i e ii.

Além disso, sempre se obteve um ganho de desempenho nos cenários iii.

Os processos sempre acabam migrando para o cluster Aquario.

Quando utilizado = 2, cinco chamadas de reescalonamento são feitas.

Nenhuma migração ocorre nas três primeiras chamadas.

Com = 4 se observa um ganho maior do que com = 2.

Isso acontece devido a ultima chamada de reescalonamento ocorrer na superetapa 60, tendo assim mais duas superetapas para usufruir de processadores mais rápidos.

O melhor desempenho na simulação acontece com = 8.

Neste caso, apenas 3 chamadas de reescalonamento acontecem.

Além disso, na ultima chamada, todos os processos do cluster Frontal são migrados.

Por fim, ao considerar = 16, um dos processos que executam no cluster Frontal acaba não sendo migrado, resultando em um desempenho pior do que = 4 e 8.
Tempos de execução em diferentes cenarios, matriz 50×50.

O gráfico apresenta o desempenho na simulação de 200 processos.

Cada processo ocupa 725 KBytes em memória e as comunicações envolvem 25 KBytes.

O gráfico mostra um sobrecusto menor do que 2% em todos os casos.

Além disso, resultados satisfatórios foram obtidos com a migração de processos.

Não há problemas em ter mais de um processo mapeado para um mesmo processador, pois nem todos executam em cada superetapa.

Considerando inicial igual a 2, 8 chamadas para o reescalonamento de processos aconteceram.

Todas migrações acontecem para o cluster Aquario.

Nas três primeiras vezes nenhum processo é migrado.

Na superetapa 62, todos processos do cluster Frontal são migrados.

Além disso, nenhuma migração é víavel na superetapa 254.

Por fim, 12 processos são migrados na ultima chamada de reescalonamento.

Neste momento, todos os outros processos alocados ao cluster estão inativos e a migração é víavel.

Porém, a aplicação executa apenas mais 10 superetapas para amortizar os custos da migração.

Este artigo apresentou brevemente o modelo MigBSP, assim como a avaliação de seu sobrecusto e desempenho na execução de uma aplicação irregular.

Inicialmente, MigBSP foi desenvolvido para tratar processos com comportamento regular.

Porém, ele permite, através de parâmetros flexíveis, o tratamento da irregularidade de uma maneira eficiente.

Nos casos simulados, o modelo impõe uma baixa sobrecarga na execução da aplicação quando migrações não acontecem (custo inferior a 3%).

Além disso, obteve-se ganhos de até 10% com migrações no ambiente simulado.

Com a crescente demanda por poder de processamento e com o limite físico na fabricação de transistores em camadas de silício cada vez menos espessas, busca-se novas tecnologias para a fabricação de circuitos integrados e microprocessadores.

Uma das alternativas para suprir a demanda por processamento é o uso de arquiteturas paralelas, sendo um dos seus representantes as arquiteturas que empregam a tecnologia multicore.

A tecnologia multicore (múltiplos núcleos) consiste em dois ou mais processadores no interior de um unico chip.

O sistema operacional trata esses núcleos como se cadá um fosse um processador diferente, com seus próprios recursos de execução.

Isto possibilita que aplicações sejam designadas para núcleos diferentes, aumentando seu desempenho e proporcionando um ambiente multitarefa mais eficiente.

Para que isso aconteça, a aplicação deve ser desenvolvida de modo a explorar esse paralelismo eficientemente.

Para explorar o paralelismo em processadores multicore, algumas bibliotecas como OpenMP e Pthreads podem ser utilizadas.

Assim o objetivo do trabalho é comparar a solução do problema da difusão de calor empregando estas duas bibliotecas.

Todo e qualquer problema téorico que se deseja modelar, ou seja, demonstrar como as grandezas físicas atuam e afetam o sistema físico das quais foram extráidas, deve seguir um conjunto de etapas, as quais permitem extrair modelos que possam ser representados computacionalmente.

Apresenta um fluxograma, adaptado de, demonstrando as principais etapas da modelagem matemática.

Etapas da Modelagem Matematica.

O método de solução da equação de difusão de calor, proposto neste trabalho, é composto por duas etapas.
Montagem do sistema de equações.

Resolução do sistema de equações através do Gradiente Conjugado (GC).

A montagem do sistema de equações se da através do método das diferenças finitas.

Para o resolver o sistema foi utilizado o método numérico Gradiente Conjugado.

Pelo fato das operações que formam o método do GC serem de caráter vetorial e matricial, a estratégia adotada nas implementações da solução será a decomposição por dados, que permite segmentar uma estrutura de dados em diversos blocos e distribúi-la entre as unidades de execução, para serem processadas simultaneamente.

Na paralelização utilizando Pthreads em cada chamada de função de cálculo matricial e/ou vetorial cria-se um conjunto de threads.

Cada thread executa um segmento da estrutura de dados, assim, não há dependência de dados entre as threads, com exceção no cálculo do produto escalar, onde há a necessidade de acesso a uma mesma varíavel por todas as threads, e sua consistência é garantida utilizando técnicas de controle de seção crítica.

A API OpenMP explora o paralelismo em máquinas com memória compartilhada, tendo como principais componentes as diretivas de compilação (pragmas), cláusulas, bibliotecas de serviços e varíaveis de ambiente.

Da mesma forma utilizada na implementação com Pthreads, a paralelização com o OpenMP foi realizada nas operações matriciais e/ou vetoriais.

São apresentadas quatro máquinas onde os testes foram realizados, os quais foram desenvolvidos em linguagem C e compilados com o gcc-43, utilizando sistema operacional Debian Linux.

Maquinas utilizadas.

Apresenta-se os tempos de execução obtidos nas execuções do método utilizando as três bibliotecas.

Utilizou-se domínios de 10 x10 células, 50 x50 células e 80 x80 células.

Pode-se notar que as soluções paralelas no domínio 10 x10 consumiram mais tempo que a implementação seqüencial, isto acontece devido à sobrecarga de criação, destruição e sincronização das threads, em OpenMP e Pthreads.

Como matrizes com dimensões pequenas têm baixo custo de processamento, o tempo gasto com o gerenciamento das threads/processos não é compensado.

Entretanto, com o aumento do domínio amostrado, há um maior custo de processamento.

Pode-se notar que as implementações paralelas foram mais rápidas que a implementação seqüencial.

Como o tempo de gerenciamento, utilizando ferramentas de memória compartilhada, são relativamente constantes, quanto maior o domínio a ser calculado melhores ganhos de desempenho poderão ser obtidos.

O resultado de uma simulação usando um domínio 25 x25 é mostrado.

Amostras (0, 500, 1000 e 1500 segundos) da evolução da solução da difusao de calor, domínio 25 x25 com tempo de simulação de 1500 segundos O uso de múltiplos núcleos já é uma tendência na construção de processadores.

No entanto para aproveitar todo o potencial que esta arquitetura oferece, deve-se utilizar as ferramentas apropriadas para a construção do software.

Nestre trabalho as implementações utilizando OpenMP e Pthreads apresentaram desempenhos satisfatórios, porém observa-se que sua eficiência cresceu de acordo com o tamanho do domínio, pois o custo de criar, sincronizar e destruir as threads não são compensados pelo baixo custo computacional na solução de pequenos domínios.

As Unidade de Processamento Gráfico GPUs têm evolúido muito na ultima década, se tornando processadores flexíveis com um vasto poder computacional, chegando facilmente a 250 GFLOPs.

Essas GPUs proporcionam amplos recursos computacionais tanto para o processamento gráfico, como para o processamento de uso geral voltado a area científica.

Isso se deve à especialização em computação intensa e altamente paralelizada, possível pela atual arquitetura empregada nesses dispositivos.

Ferramentas tais como o CUDA (Compute Unified Device Architecture), permitem utilizar as capacidades computacionais das GPUs sem a necessidade de usar as APIs, evitando a adaptação dos dados.

Sob este escopo, o objetivo desse trabalho é apresentar um estudo sobre o uso das GPUs e da ferramenta CUDA na solução de problemas de natureza científica.

Como estudo de caso é mostrado uma solução para o problema da Equação de Difusão de Calor usando a GPU para realizar os cálculos necessários.

CUDA é uma nova arquitetura de hardware e software que permite utilizar e gerenciar as capacidades computacionais paralelas das GPUs NVIDIA sem a necessidade de usar um mapeamento das computações para as APIs gráficas.

A estrutura de software CUDA consiste em 4 camadas, a primeira é a aplicação, seguida das bibliotecas de alto nível matemático (CUFFT e CUBLAS), sua API e runtime e finalmente o driver de hardware.

Sendo uma extensão a linguagem de programação C, a interface de programação CUDA tem como objetivo fornecer uma forma simples e rápida de desenvolvimento de aplicações gerais para serem executadas pela CPU, diminuindo a curva de aprendizado e permitindo ao programador selecionar quais partes do código fonte sejam executados na CPU ou na GPU.

Como estudo de caso para realização de testes comparativos do tempo de processamento entre CPU e GPU.
Foi escolhido um problema clássico de aplicação de métodos numéricos conhecido como difusão de calor em uma placa homogênea onde o objetivo é determinar a temperatura em qualquer ponto interno da placa em um dado instante de tempo levando em consideração que essa temperatura no instante inicial t0 seja diferente das temperaturas das bordas.

O processo inicia com a escolha do domínio, onde é determinado o número de nós, isto e, a discretização da placa, quanto maior for o número de nós, maior será a precisão da temperatura em uma area da placa.

Como condição inicial, defini-se ó tempo total determinado para a simulação, dado em segundos, e também o tempo de cada iteração, juntamente com os valores das temperaturas das bordas e internas da placa.

Em seguida, é gerado a matriz de coeficientes e o vetor de termos independentes.

Na seqüência utiliza-se o método do Gradiente Conjugado para encontrar as aproximações para o vetor resposta (temperaturas internas da placa) na iteração n.

Atualiza-se o vetor de termos independentes e verificam-se as condições de parada, que é definido por um número máximo de iterações do gradiente conjugado ou se a diferença da resposta atual com a resposta da iteração de tempo anterior é menor que a precisão desejada.

Caso uma dessas condições seja satisfeita, temos a resposta para essa iteração de tempo, caso contrário, uma nova iteração da simulação é realizada.

Como ambiente de software foi utilizado o sistema operacional Windows XP SP2, ambiente de desenvolvimento Microsoft Visual Studio 2005 (por questão de compatibilidade com a ferramenta CUDA 20).

Utilizou-se uma CPU C2 D E8400 3 Ghz em um sistema com dois módulos de 1 Gb de memória RAM DDR2 operando a 800 Mhz em modo Dual Channel (totalizando 1600 Mhz) e o chipset NVIDIA 650 SLI com suporte a um slot PCI-E 16 x ou a dois slots PCI-E 8 x.

Em relação as GPUs, foram utilizados nos testes dois modelos diferentes de dispositivos, NVIDIA 8500 GT e NVIDIA 8600 GT.

Realizaram-se testes comparando os desempenhos entre as GPUs em modo compartilhado com o sistema (uma GPU gerando imagens para o monitor), em modo dedicado (onde são utilizadas duas GPUs sendo uma utilizada para sáida de vídeo e outra apenas para os cálculos) e também em modo SLI (Serial Link Interface).
Também utilizando duas GPUs mas trabalhando em conjunto como se fosse apenas uma unica GPU para o sistema (para cálculos e sáida de vídeo).

Para os testes realizados com a solução da equação da difusão de calor foram utilizados dois códigos, o primeiro código em linguagem C de execução serial, isto é, sem nenhum tipo de paralelização das operações.
Para ser executado pela CPU e tomado como ponto de referência para avaliar o segundo código, que foi escrito utilizando a ferramenta CUDA em conjunto com a biblioteca CUBLAS para que a GPU ficasse responsável pelos cálculos requeridos na resolução do problema.

Foram utilizadas como condições iniciais da simulação, tempo total da simulação de 10 minutos, tempo de cada iteração de 0,5 segundo, assim resultando em 1200 iterações de simulação.

Para as condições de bordas, utilizaram-se os valores de 25±C para todo o domínio e apenas uma das bordas contendo valores de 45 C.

Optou-se por ± utilizar três tamanhos de domínio, o primeiro de tamanho 22 x 22 células, o segundo 42 x 42 céluas e o ultimo como sendo domínio de 82 x 82 células.

Mostram os resultados obtidos da solução da equação de calor entre a CPU e as GPUs nos três domínios com as condições iniciais determinadas anteriormente.

Mostra os resultados obtidos na simulação no passo inicial, após 5 minutos e após 10 minutos.

Passos da Solução de Difusao de Calor.

De modo geral, os resultados obtidos foram extremamente satisfatórios em relação à solução da equação de calor, pois o ganho no tempo de execução em GPU em comparação com a execução em CPU chegou a valores aproximados de 90% no domínio 22 x22, 95% no domínio 42 x42 e significativos 98% para o domínio 82 x82.

Considerando que o custo da GPU 8600 GT, que obteve os melhores resultados, não passa de 50% do valor da CPU utilizada.

Como principal conclusão tem-se que o uso das GPUs (utilizando CUDA) aplicado a execuções de soluções para problemas que requerem grandes níveis de computações é uma abordagem válida em relação ao poder computacional, pois apresenta grande desempenho, certa facilidade na aprendizagem da linguagem, grande comunidade de discussão e custo/benefício atrativo, tornando uma alternativa barata para computação intensa e paralela.

Durante muito tempo a utilização de programação leve com o objetivo de melhorar o desempenho de aplicações com alto custo computacional foi um recurso utilizado apenas em meios acadêmico ou grandes empresas que possuiam dinheiro para adquirir o caro hardware multiprocessado (SMP Symmetric Multi Processing) e pessoas especializadas para tirar proveito dos recursos oferecidos por este hardware.

Com a popularização dos processadores multicore, a tecnologia que permite a exploração de programação leve ultrapassou a fronteira antes limitada pelo alto custo.

Praticamente todos os computadores pessoais produzidos hoje já possuem mais de um núcleo de processamento.

Juntamente com a possibilidade de ganho de desempenho pelo uso de processadores multiprocessados, veio a dificuldade de explorar corretamente a grande variedade de configurações destes recursos que hoje estão disponíveis.

E apresentado neste trabalho a implementação do modelo split-compute-merge (SCM) sobre o modelo Anahy Vanilla, que tem como objetivo facilitar a utilização de recursos de processadores multiprocessados pelo uso de threads, sobre aplicações que permitem a utilização deste modelo.

Este trabalho esta organizado da seguinte forma, a seção 2 apresenta o modelo Anahy Vanilla.

Na seção 3 é abordado o modelo SCM e como ele pode ser utilizado.

A seção 4 apresenta um estudo de caso da utilização do SCM juntamente com análises de performance.

A conclusão deste trabalho é feita na seção 5.

Anahy Vanilla é um modelo concebido para programação e execução paralela.

Fazendo uso de um núcleo de escalonamento no nível de usúario, ele permite que toda a concorrência definida pela aplicação seja mapeada de forma a não sobrecarregar os recursos disponíveis pelo hardware, permitindo então que a mesma aplicação possa migrar entre diferentes configurações de arquiteturas sem a necessidade de alterações no código.

Esta portabilidade é obtida pela utilização de Processadores Virtuais (PV).

Os PVs são threads de sistema que são criadas durante o inicio da execução de um programa usando o modelo Anahy Vanilla, e o sistema operacional fica responsável pelo escalonamento destas threads.

Pode-se afirmar que a concorrência real é limitada pelo número de PVs criados durante o início da execução e não pelo número de threads definidas pela aplicação, podendo então definir o paralelismo da aplicação baseada nas suas características, idependentemente do hardware alvo da aplicação, tornando a aplicação adaptável à diferentes configurações.

O ambiente Anahy Vanilla oferece uma interface de programação, permitindo que novos fluxos concorrentes sejam sejam criados sincronizados utilizando primitivas do tipo split join.

Esta interface segue o padrão POSIX para threads.

Além de possuir como característica portabilidade de desempenho entre diferentes configurações de arquitetura, o ambiente Anahy Vanilla também possue a característica de tirar proveito da localidade espacial do uso da memória durante a execução.

Todos os fluxos criados (e ativos) são mantidos em um grafo organizado pela dependência entre as threads e diferentes algoritmos são utilizados para examinar o grafo e definir a ordem de execução, sendo por padrão utilizado o algoritmo que executa as threads mais novas (base do grafo) inicialmente.

Mostra o grafo que mantém as threads criadas no modelo Anahy Vanilla.

Grafo do núcleo de gerenciamento do modelo Anahy Vanilla.

Existe uma grande parcela de problemas que possuem um comportamento em comum, uma grande quantidade de dados pode ser dividida, computada independentemente e o resultado final obtido pela combinação destes resultados.

Tendo em vista esta grande quantidade de aplicações deste tipo, foi incorporado o SCM ao modelo Anahy Vanilla, permitindo que o programador possa utilizá-lo com poucas configurações.

Esquema da execução do modelo Split-Compute-Merge.

Mostra como a partir de um conjunto global de dados, estes dados são quebrados em pequenos conjuntos, que serão passados de forma transparente à threads que irão computar os resultados parciais concorrentemente, retornando-o ao final.

A função merge fica responsável por organizar estas informações da forma que melhor representa as características da aplicação e retornar o resultado final.

Um ponto importante a ser notado é que o programador em momento algum precisa se preocupar com a criação sincronização de threads.

Apenas foi definido o número de partes que o bloco de memória será dividido (splitfactor) e o contéudo das funções responsráveis pela divisão do conjunto de dados, computação dos resultados parciais e combinação dos resultados parciais, sendo o ambiente de execução responsável pela definição da concorrência da aplicação.

A aplicação do modelo SCM não só torna a aplicação mais simples de ser desenvolvida, uma vez que as definições feitas são desenvolvidas como programação seqüêncial, más também torna muito mais rápida a resolução de problemas que utilizam este modelo.

A fim de validar e testar o modelo SCM, foi desenvolvido como estudo de caso uma aplicação 2 que aplica um algoritmo de compactação de forma paralela em um determinado arquivo.

A partir de um arquivo informado pela linha de comando, a aplicação divide o contéudo deste arquivo em blocos de tamanhos iguais e os compacta concorrentemente, utilizado a biblioteca zlib para esta compactação.

Este estudo de caso foi executado em um ambiente GNU Linux com 16 processadores (com acesso a memória não uniforme NUMA) e um total de 8 GB de memória.

Os arquivos a serem compactados possuem respectivamente 410 e 610 Mb e foram utilizados em todos os casos valores de splitfactor = 2, 4, 6 e 8.

Graficos da execução das versoes paralelas e versao seqüencial usando o modelo SCM Os resultados obtidos pelo estudo de caso mostram um grande ganho de desempenho em todas as versões paralelas, quando comparado aos resultados obtidos durante a execução seqüêncial da aplicação.

Pode ser verificado pelos gráficos, que é possível escalar o desempenho a medida que há uma variação no número de processadores virtuais, sendo os melhores resultados obtidos quando o número de PVs se aproxima aos recursos reais de paralelismo da arquitetura utilizada.

A análise de desempenho de aplicações paralelas é uma importante tarefa para melhor compreender o seu comportamento.

Com isso, é possível detectar problemas durante a execução, normalmente caracterizados através do excesso de operações bloqueantes e gargalos de desempenho.

Em geral, esses problemas prejudicam uma boa exploração dos recursos computacionais pela aplicação paralela.

Diferentes técnicas e ferramentas são utilizadas para a análise de aplicações paralelas.

Uma abordagem comumente empregada é o uso de ferramentas de visualização.

Em geral, essas ferramentas são compostas de gráficos de duas dimensões, também conhecidos como Gantt-charts, onde os processos são dispostos no eixo vertical, e o tempo de execução no eixo horizontal.

Alguns exemplos de ferramentas que utilizam a abordagem 2 D são Pajé, Paradyn, Paraver e Vampir.

A abordagem que estamos utilizando para a análise de aplicações paralelas consiste em uma visualização com três dimensões.

Ela é implementada no protótipo Triva.

Pelo fato do ambiente de três dimensões ser novo para a análise de aplicações paralelas, há pouca investigação de novas técnicas de interação visual com os dados.

De uma maneira geral, essas técnicas são diferentes das utilizadas no espaço de duas dimensões, visto que agora temos uma dimensão a mais para gerenciar.

Este artigo apresenta duas formas de interação com a representação dos dados no ambiente 3 D, auxiliando o desenvolvedor na análise da evolução dos componentes da aplicação.

A primeira permite ao usúario acompanhar o deslocamento tridimensional dos objetos de interesse.

Acompanhar o deslocamento significa ter uma representação visual do objeto para cada mudança em suas coordenadas espaciais, na transição de um ponto a outro.

A segunda auxilia o usúario na clarificação do ambiente para melhor visualização dos detalhes de um determinado objeto.

Apresentamos o ambiente 3 D e conceitos básicos.

As novas técnicas de interação são explicadas na seção 3.

Os resultados obtidos são apresentados na seção 4.

O protótipo de visualização 3 D que estamos usando chama-se Triva.

Ele recebe como entrada um fluxo de eventos que descreve o comportamento do conjunto de processos e threads de uma aplicação paralela.

O formato desses eventos segue a definição de eventos do Pajé.

Os componentes das aplicações paralelas são chamados no contexto do protótipo 3 D de entidades.

Pesquisa parcialmente apoiada pelo CNPq.

Apresentamos dois exemplos de cenários de visualização tridimensional.

Temos a representação dos três eixos que compõem o ambiente 3 D.

No eixo vertical representamos o tempo.

O plano formado pelos dois eixos restantes denominamos de base da visualização, onde posicionamos as entidades.

Diferentes técnicas podem ser usadas para construir o desenho da base da visualização.

Neste trabalho a técnica utilizada na base mostra sempre o padrão de comunicação da aplicação paralela.

Representação do espaço tridimensional.

No protótipo Triva, as entidades são dispostas na base da visualização 3 D com o auxílio de um algoritmo que evita a sobreposição das mesmas.

Ao surgirem novos processos, novas entidades são criadas para representá-los.

Para evitar o posicionamento aleatório das mesmas, pois isso pode acarretar na sobreposição das novas com as já existentes, reposicionam-se todas as entidades na base.

Com isso, o usúario pode perder a referência sobre as entidades que estava observando.

A solução encontrada consiste em animar as movimentações de todas as entidades, de suas posições antigas para as novas posições.

A animação é uma boa alternativa para este problema pois permite com facilidade que o usúario mantenha percepção do deslocamento de suas entidades ao ocorrer o reposicionamento das mesmas na base.

Para realizar esta animação, primeiramente realizamos uma busca para capturar todas as posições atuais das entidades existentes.

Após, a biblioteca Graphviz calcula as novas posições das entidades, e por fim, a biblioteca Ogre3 D calcula os pontos da trajetória a ser percorrida entre a posição inicial e destino de cada entidade, e interpola esses pontos para produzir a animação.

Outro problema encontrado foi o usúario não poder colocar um processo em evidência em relação aos outros.

Por exemplo, ao analisar um grande conjunto de processos, temos diversas entidades sendo reproduzidas na tela.

Quando é feita a análise de um unico processo, existe um limitante visual, os processos (entidades) estão próximos um dos outros para facilitar a visualização global, no entanto, isso dificulta a visualização individual.

Duas alternativas já existentes no Triva para facilitar a análise individual são a possibilidade de aproximar a câmera da entidade e o aumento do tamanho das entidades.

Entretanto isso não desfaz o problema da poluição visual, uma vez que as entidades ficam próximas, dificultando a observação detalhada dos dados de um processo específico.

A solução encontrada para o problema da poluição visual foi distanciar as entidades próximas da entidade alvo (entidade que se deseja analisar com maior detalhamento).

O distanciamento é limitado aquelas entidades que estejam dentro de um determinadò raio, tendo como centro a entidade alvo.

Foi criado o raio de ação considerando que se deslocarmos todas as entidades a visualização torna-se difícil, pois as entidades que já estiverem longe ficarão ainda mais longe.

Calculamos a distância média de todas as entidades que estão dentro do raio de ação, em relação a entidade alvo.

Essa distância é multiplicada pelo fator de distanciamento desejado, gerando o deslocamento.

Por exemplo, considerando um fator de distanciamento de 0,5, uma entidade que estiver a 100 pixels de distância, será deslocada de 50 pixels.

E uma entidade que estiver a 10 pixels de distância, será deslocada de 5 pixels.

Desta forma, com a distância média, ambas as entidades serão deslocadas de 27 pixels, mantendo a proporção entre elas.

Para utilizar esta função, o usúario seleciona a entidade alvo, e as que estiverem próximas e dentro do raio de ação serão afastadas.

Com a animação dos deslocamentos, o usúario mantém a referência sobre as entidades, pois acompanha visualmente as movimentações na base.

Desta forma, o usúario sabe a cada instante a posição exata de cada entidade.

A animação foi utilizada na análise de aplicações paralelas com até 100 processos, sendo também utilizada quando ocorre a mudança do algoritmo de posicionamento na base da visualização.

Com a função de evidenciar uma entidade, é possível analisar as características de um determinado processo com maior clareza, pois as entidades que antes estavam muito próximas gerando maior poluição visual, agora permanecem distantes.

Com a utilização do novo ambiente tridimensional para a análise de aplicações paralelas percebemos a necessidade de investigar novas técnicas de interação com os dados.

Este artigo apresentou duas novas formas de interação para o ambiente 3 D do protótipo Triva.

Estas novas formas de interação auxiliam o usúario na análise da evolução da aplicação, para uma melhor compreensão de seu comportamento.

Durante a visualização 3 D pode ocorrer o reposicionamento das entidades na base da visualização.

Assim, o usúario perde a referência sobre as entidades que estava observando.

Este problema foi resolvido implementando a animação espacial do deslocamento das entidades.

Outro problema que surge é a poluição visual, quando tentamos analisar detalhadamente uma entidade específica.

Poluição esta que é causada pela proximidade das entidades.

A solução empregada foi afastar as entidades próximas que estiverem dentro de um determinado raio, mantendo a relação da posição espacial e da distância em relação a entidade alvo que se está observando.

Os resultados alcançados melhoram a interação com os dados de monitoramento da aplicação paralela, facilitando a manutenção da referência sobre as entidades observadas durante o reposicionamento das mesmas na base da visualização.

E auxiliam a visualização detalhada dos dados de uma entidade com redução da poluição visual gerada pela proximidade das mesmas.

Como trabalhos futuros, pretende-se trabalhar com técnicas de interação que auxiliam a definição pelo usúario de intervalos de tempo a serem analisados.

Outro caminho a ser seguido consiste na adaptação do protótipo para gerar um replay da execução, possibilitando a observação de como a aplicação paralela evoluiu.

O problema de designar professores a disciplinas e alocar o conjunto de disciplinas num horário de um curso é um problema de difícil solução manual, principalmente quando o número de disciplinas e professores é elevado.

Na maior parte dos casos, ocorre uma restrição em relação aos horários disponíveis dos em virtude de suas demais atividades.

Quando o caso é levado a uma universidade, com dezenas de cursos e turmas em andamento concomitantemente, envolvendo centenas de professores e disciplinas, o problema se acentua ainda mais.

A busca por uma solução manual do problema pode demandar o esforço de muitas pessoas durante vários dias e, mesmo assim, não gerar uma boa solução.

Segundo Rela, Algoritmos Evolucionários são técnicas de computação inspiradas na teoria da seleção natural de Charles Darwin.

A idéia principal destas técnicas é a de manter uma população de soluções candidatas (indivíduos) do problema, e também, a criação de uma função para a avaliação dos indivíduos melhor adaptados.

Esta função é utilizada para mensurar a aptidão (fitness) de cada indivíduo da população.

Em muitos casos, o uso de algoritmos evolucionários, em especial o uso de algoritmos genéticos (AG), representam soluções de moderado consumo computacional.

Porém, em casos onde o espaço de busca dos algoritmos é elevado, pode ocorrer um aumento considerável da capacidade computacional necessária para sua resolução.

Esse aumento pode ocorrer em virtude do consumo de memória ou em relação à necessidade de processamento.

Para os casos onde os algoritmos genéticos consomem grande quantidade de recursos computacionais, demandando com isso elevado tempo de execução, o uso de técnicas para a paralelização dos AGs é uma solução que deve ser investigada.

O uso do paralelismo pode proporcionar ao AG condições de explorar melhor e de forma mais aprofundada o espaço de busca.

Além disso, caso a paralelização seja realizada em ambientes com diferente espaço de endereçamento de memória (multicomputadores), pode-se aplicar operados genéticos com diferentes parâmetros em cada AG sendo executado, aumentando ainda mais a diversidade das soluções encontradas.

Este texto descreve o projeto de uma solução para o problema de timetabling de professores a disciplinas em horários usando algoritmos genéticos.

Ele é fortemente centrado no trabalho descrito em, pois pode representar sua continuidade.

Ele começa com as principais deficiências do AG descrito e complementa com o modelo do novo AG, suas estruturas de memória e descreve algumas das possibilidades para a exploração do paralelismo.

O modelo de algoritmo genético foi implementado e teve seus principais resultados descritos em.

Os resultados foram obtidos após 500 gerações, consumindo aproximadamente 12 minutos e 25 segundos, gerando indivíduos para 10 semestres de disciplinas em turno único (somente noite, por exemplo) e turno duplo (tarde e noite).

A execução não foi encerrada por convergência e sim por número de gerações e obteve indivíduos com resultados validos, ou seja, com nenhuma hard constraint e com um bom score máximo de 182,55.

O AG implementado gera o horário para um curso por vez, porém o modelo de solução pode ser empregado para geração de horários de vários cursos.

Para tanto, após uma execução do AG é retirado da disponibilidade de horários dos professores os alocados a uma disciplina, sendo após o AG novamente executado com dados de oferecimentos do novo curso.

A referida versão do AG possui limitações como apenas a alocação de disciplinas pares, a não alocação de horários concomitantes na mesma turma, e a geração de apenas um curso por execução AG.

Somente podem ser geradas disciplinas com número de créditos pares uma vez que a granularidade de horários é sempre de 2 horas/aula.

Nos casos de turmas práticas divididas entre diferentes professores no mesmo horário (isso ocorre em virtude de divisão de turmas em cursos com apenas uma turno de oferecimento) o AG gera uma hard constraint, embora essa constraint não seja válida uma vez que os alunos não são os mesmos.

A alocação de mais de um curso por execução do AG é limitada, principalmente, em função do número variado de semestres que os cursos podem apresentar.

Esses problemas ensejam a definição de um novo modelo de AG, que suplante essas limitações.

Para tanto, estima-se que o tempo de execução seja majorado, possivelmente de forma exponencial.

Assim, a implementação no novo modelo de AG terá de fazer uso de paralelismo na execução.

Uma nova versão do modelo proposto foi definida para solucionar os problemas detectados.

O novo modelo altera a representação cromossômica e, conseqüentemente, funções de fitness e operadores genéticos de mutação e crossover.

Grande parte das mudanças e melhorias ocorreu em virtude da nova representação cromossômica da solução.

Diferentemente da antiga abordagem em um vetor de duas dimensões, o novo modelo pode ser dividido em três partes.

Representação do novo modelo cromossômico.

A primeira parte representa os cursos, armazenados num conjunto de dados de mesmo tipo, como, por exemplo, um vetor (Conjunto de Cursos).

A segunda parte do cromossomo possuirá as informações dos níveis de um curso, armazenada em estrutura de dados de tamanho variável, como por exemplo, uma lista (Níveis dos cursos).

A terceira parte será um vetor de 63 posições que serão os slots de horários de um nível, cada slot será um crédito e poderá ter mais de uma disciplina referenciada nele (Relação de disciplinas em Horários).

A população inicial é gerada por nível, com um inicializador distinto para cada nível.

Este inicializador trabalha percorrendo as disciplinas oferecidas no nível, para cada uma ele encontra todas as relações professor/disciplina referente a ela.

Depois de encontrada todas as relações, aleatoriamente uma é escolhida.

O mesmo método é utilizado para escolher o slot onde a relação será alocada, encontram-se todos os slots livres, e aleatoriamente um é escolhido.

Devido ao tamanho do novo cromossomo essa será a seção critica do algoritmo, onde o maior custo computacional será empregado.

As constraints podem ser separadas em dois tipos principais, Hard e Soft.

Segundo Fang (1994, p 49), as do tipo hard não devem ser violadas uma vez que são críticas e afetam de modo significativo a qualidade do resultado.

As constraints do tipo soft são mais sutis, ou seja, devem ser preferencialmente seguidas, porém podem ser violadas caso haja a necessidade.

Foram elaboradas três hard constraints, todas as disciplinas oferecidas foram alocadas, disponibilidade do professor no slot, professor dando aula no mesmo slot em níveis ou cursos diferentes.

E é na verificação delas, e principalmente na constraint "a", onde boa parte do processamento ocorrerá.

As soft constraints, ainda estão sendo elaboradas, visando aprimorar avaliação de cada um dos cromossomos.

A verificação de se todas as disciplinas oferecidas foram alocadas, consiste em cada nível de todos os cursos, comparar o timeslot gerado, como o oferecimento que foi exigido.

Dependendo da quantidade de cursos que está sendo utilizado, além do tamanho do cromossomo, o tamanho da lista de oferecimentos será grande.

Para limitar o espaço de busca e comparação, a lista de oferecimentos será gerada em um vetor bidimensional, onde cada uma das linhas representará um curso.

A função de fitness foi modelada com base na abordagem de penalizações e bonificações.

Para tanto, um acumulador para o score bruto armazena o somatório de cada uma das constraints multiplicado pelo seu respectivo peso.

O operador genético de crossover foi projetado para funcionar de maneira distinta das implementações clássicas, fato decorrente da representação utilizada para modelar os níveis e slots de tempo.

Serão utilizadas duas formas distintas para realizar o crossover em ambas foi utilizada a técnica one-point crossover.

Na primeira o ponto de corte a partir do qual se terá duas partes distintas do material genético é realizada de modo a permitir a cópia de níveis em sua totalidade, isto é, quando cruzados, os indivíduos farão um intercâmbio dos níveis completos.

A segunda vai agir da mesma forma, porém possibilitando a troca dos cursos em sua totalidade.

A mutação dos genes acontece em duas etapas, a mutação do professor/disciplina e o swap das disciplinas.

Na mutação do professor/disciplina, o algoritmo de mutação procura a disponibilidade de algum outro professor para a mesma disciplina e muta-o para este outro professor/disciplina, na segunda etapa da mutação, o algoritmo realiza a troca da posição do professor/disciplina para outro slot disponível.

A partir da nova representação cromossômica, mais complexa para manuseio das informações e com elevado consumo de memória, a implementação necessita de um grande esforço computacional para ser executado.

Para resolver esse problema à nova implementação está sendo desenvolvida para usar recursos de exploração explícita do paralelismo, com uso da biblioteca Messaging Passing Interface (MPI) para as funções necessárias ao controle e execução paralela.

O modelo de AG paralelo sendo estudado é o coarse-grained uma vez que o ambiente de execução será um cluster ou máquina Atualmente, o processamento paralelo é essencial em áreas onde as aplicações demandam altos recursos de processamento.

A popularização de configurações para máquinas multiprocessadas, devido ao barateamento de processadores multi-core, ocasionou uma alta demanda por aplicações que incorporem o modelo da multiprogramação leve, o qual permite que diversos fluxos de execuções (threads) estejam ativos durante a execução do programa.

Diversas ferramentas comerciais oferecem recursos para explorar este modelo de programação.

Entre as mais populares estão Pthreads e OpenMP.

Enquanto Pthreads provê uma interface para destacar o paralelismo de tarefas da aplicação, OpenMP provê uma interface voltada ao paralelismo de dados.

No meio acadêmico, Anahy destaca-se por oferecer estratégias de escalonamento de tarefas que permitam obter o máximo desempenho de uma arquitetura paralela pela aplicação de algoritmos de listas.

Este trabalho apresenta a ferramenta de pré-processamento ApenMP, permitindo a execução de programas utilizando o modelo de execução de Anahy, mas com especificações de programação baseadas no modelo OpenMP.

Uma análise de desempenho permitiu verificar que o desempenho de programas ApenMP reflete o comportamento obtido para programas Athreads, uma implementação baseada em Pthreads do modelo Anahy.

Na continuação, nas Seções 2 e 3 são apresentados os modelos e recursos de programação e de escalonamento de OpenMP e Anahy, respectivamente.

Na Seção 4 é descrita a ferramenta de pré-processamento ApenMP.

Na Seção 5 são discutidos resultados de desempenho de ApenMP.

Finalmente na Seção 6, são apresentadas conclusões sobre o projeto e propostas para trabalhos futuros.

O modelo de programação de OpenMP baseia-se no modelo fork/join para criação e sincronização de threads.

A aplicação, quando em execução, dispara trechos de códigos que são executados em regiões paralelas pelos threads de serviços.

Estes threads de suporte à execução são reunidos em um núcleo denominado thread-pool.

A interface de programação de OpenMP possui diretivas de compilação como abstração para criação e sincronização de threads e chamadas a serviços de biblioteca para permitir a interação do programa em execução com o seu ambiente.

O protótipo básico de uma região paralela em OpenMP é ilustrado.

Protótipo de uma região paralela em OpenMP.

Nesta sintaxe, diretiva pode ser parallel, a qual indica a criação de trechos de códigos concorrentes, e for que indica a paralelização de um laço.

Em clausula é identificada a natureza dos dados manipulados.

Em relação ao escalonamento, este pode ser Static, Dynamic, Guided e Runtime.

Estes escalonamentos identificam políticas próprias em relação à distribuição de chunks (conjuntos de dados) entre os threads do mesmo thread-pool.

O ambiente de execução Anahy propõe um modelo de programação composto por uma camada definindo uma interface de programação de alto nível e por outra camada definindo um mecanismo de escalonamento de tarefas.

Com estas duas camadas, o modelo permite que a concorrência de uma aplicação possa ser especificada sem considerar o paralelismo real oferecido pela arquitetura.

É ilustrado o processo de criação e sincronização de threads.

Criação e sincronização de threads em Anahy.

No serviço athread_create, o parâmetro func indica a função que deve ser executada pelo thread, attr especifica os atributos do thread, th é um valor que será atualizado para identificar o thread criado, in é o endereço onde os dados de entrada serão armazenados.

No serviço athread_join, th indica o thread no qual a sincronização será realizada, res aponta para uma posição na memória onde os resultados da função executada pelo thread th serão encontrados.

Em relação ao escalonamento, Anahy utiliza algoritmos de listas.

Uma camada entre os recursos de programação e o mecanismo de escalonamento é responsável pela criação de um grafo direcionado de tarefas acíclico (DAG) representando a execução do programa.

Este grafo é submetido ao núcleo executivo para escalonar as tarefas em execução sobre os recursos computacionais disponíveis.

A ferramenta de programação ApenMP foi implementada em linguagem C e oferece suporte às diretivas parallel e for, e também às cláusulas private, firstprivate, lastprivate e reduction.

As ferramentas utilizadas para a criação de ApenMP foram Flex e Bison.

O pré-processador de ApenMP deve receber como entrada um código escrito segundo a especificação OpenMP.

Cabe a este pré-processador analisar os resultados de pré-processamento no código original e realizar as transformações necessárias para a geração de código Athreads.

A tradução consiste em identificar no código definições das cláusulas dos dados locais aos pragmas e identificar as diretivas OpenMP (parallel e for).

O programa tem então seu conteúdo expresso em OpenMP traduzido em seu equivalente em Athreads.

Código traduzido para Athreads.

A obtenção dos resultados de desempenho foi realizada utilizando um computador com processador Intel Core 2 Duo com freqüência de clock 1,86 GHz, memória cache de 4 MB e memória RAM de 2 GB.

O Sistema Operacional utilizado foi o GNU-Linux kernel 2 6 24, compilador Intel e GCC 4 2 4 para compilação de aplicações OpenMP.

A metodologia de avaliação de desempenho foi contabilizar o tempo de execução de um programa em segundos.

O tempo de execução somente contabiliza o tempo das regiões paralelas.

Os tempos apresentados representam uma média e desvio padrão de 15 execuções e para cada caso foram utilizados 2, 4 e 8 threads.

Foram escolhidas duas abordagens de paralelismo, tarefas e dados para avaliar o desempenho de execução final de Anahy em relação à OpenMP e Pthreads através das duas abordagens.

Desse modo, foi utilizada uma aplicação para cada abordagem, sendo que, para cada aplicação, foram obtidas 3 versões, OpenMP, ApenMP (versão em Athreads) e Pthreads.

É mostrado o resultado de desempenho utilizando uma aplicação que especifique o paralelismo de tarefas.

Como pode ser observado, o tempo de execução da versão através ApenMP foi inferior aos demais.

Este resultado é satisfatório, mas cabe ressaltar que o paralelismo de tarefas não é o nicho de OpenMP.

Tempo de execução das aplicações caracterizando paralelismo de tarefas.

Por outro lado, é ilustrado o desempenho obtido utilizando uma aplicação que especifica o paralelismo de dados.

Nesta abordagem, o tempo OpenMP foi inferior aos demais.

Porém, o tempo de ApenMP foi menor do que Pthreads.

Tempo de execução das aplicações caracterizando paralelismo de dados.

Com uso de ApenMP, o programador pode se beneficiar das vantagens da simplicidade de paralelização de um programa escrito em OpenMP e obter um desempenho eficiente pela utilização de escalonamento de algoritmos de listas provido por Athreads.

Os resultados de desempenhos foram satisfatórios, refletindo a abordagem de paralelismo de cada experimento.

Como trabalhos futuros, espera-se adicionar novas funções de OpenMP e um núcleo executivo próprio, eliminando a geração de código intermediário.

Com o aumento do poder computacional, a utilização de máquinas virtuais possibilita compatibilidade, desempenho e simplicidade.

Compatibilidade no sentido de poder executar qualquer software em qualquer ambiente computacional, incremento no desempenho pois a virtualização aproveita ciclos ociosos da máquina, e simplicidade na manutenção dos sistemas virtualizados.

Datacenters têm como objetivo gerenciar servidores e a utilização da virtualização reduz custos de modo que servidores subutilizados por determinadas aplicações podem alocar outras aplicações concomitantemente, reduzindo a utilização de energia e manutenção, o que chamamos consolidação de servidores.

Porém, hoje em dia, a virtualização não está restrita à consolidação de servidores pois, ela já se faz presente em aplicações de pequeno porte.

Sendo assim, pode-se utilizar a tecnologia de máquinas virtuais através de novas tecnologias presentes em computadores pessoais, que herdam características do alto desempenho e propõem aumento de desempenho através do incremento do número de processadores ou cores.

Este trabalho apresenta uma introdução sobre o monitor de máquinas virtuais Xen, seu escalonador default e uma análise de suas vantagens.

Também são abordadas algumas do limitações do escalonador sobre máquinas SMP (Symmetric Multiprocessor), Dual Core e Quad Core.

Finalizando, mostram-se algumas limitações do escalonador que possibilitem oportunidades de otimização e melhoria à realocação de recursos das máquinas virtuais visando a arquitetura de hardware proposta.

O Xen foi desenvolvido pelo Systems Research Group da Universidade de Cambridge, e é parte de um projeto maior chamado XenoServers, projeto este que provê um ambiente de computação global distribuída.

O Xen permite compartilhar uma simples máquina para vários clientes rodando sistemas operacionais e programas.

Com a renovação da utilização de máquinas virtuais através da consolidação de servidores, o Xen tem se tornado acessível a um número cada vez maior de usuários.

Esta acessibilidade vem proporcionando ganhos de desempenho, o que o torna uma alternativa interessante para vários sistemas de computação, ainda mais por suas vantagens em relação ao baixo custo e a portabilidade.

O Xen utiliza o conceito de paravirtualização o que possibilita ao sistema operacional que executa sobre uma máquina virtual ter a ilusão de estar sendo executado diretamente sobre o hardware.

O Xen se encarrega de organizar as requisições feitas pelas máquinas virtuais e repassá-las ao domínio0 como veremos a seguir.

Pode-se observar que o Xen se divide em quatro níves.

O primeiro nível (nível de aplicação) corresponde às aplicações que rodam dentro das máquinas virtuais (domíniosU) juntamente com os softwares adicionais que monitoram o funcionamento do Xen.

No segundo nível (sistemas operacionais convencionais) encontram-se o sistema operacional hospedeiro com seu kernel modificado para proporcionar virtualização e os sistemas operacionais convidados (máquinas virtuais).

No terceiro nível, hypervisor (domínio0), está a camada que controla as chamadas de sistema entre as máquinas virtuais e o hardware, e por fim, localiza-se o hardware.

Estrutura do Xen.

O Xen tem suporte a escalonadores tanto estáticos quanto dinâmicos.

Algoritmos de escalonamento são estáticos quando o cálculo da escala é feito tomando como base parâmetros fixos.

Os dinâmicos, ao contrário, são baseados em parâmetros que podem ser alterados em tempo de execução com a evolução do sistema.

Para máquinas multiprocessadas, a utilização de escalonadores dinâmicos é mais apropriada do que os escalonadores estáticos uma vez que estas máquinas podem sofrer reconfiguração em tempo de execução.

Veremos a seguir o escalonador padrão do Xen.

O SMP Credit é um escalonador construído para manter um trabalho justo para todos os processadores em máquinas SMP.

A cada domínio convidado são atribuídos dois valores, um peso e um limite.

Um domínio com peso 512 terá duas vezes mais direito à execução que um domínio com peso 256.

Os valores de peso variam de 1 a 65535, sendo o padrão 256.

O limite geralmente fixa o máximo de CPU (Central Processor Unit) que um sistema convidado vai poder consumir, mesmo que o sistema anfitrião tenha ciclos de CPU inativos.

O limite de uma CPU é expresso em porcentagem de uma CPU física como, 100 é uma CPU física, 50 é meia CPU, 400 são 4 CPUs, e assim por diante.

O escalonador SMP Credit provê, de forma automática, balanceamento de carga entre as VCPUs (CPUs virtuais) dos domínios convidados, utilizando todas as CPUs de uma máquina SMP.

Uma vantagem desse escalonador, é que o administrador não necessita referenciar cada VCPU a cada CPU, o escalonador já trabalha dessa maneira, associando uma VCPU a uma CPU.

Cada CPU controla uma fila local do funcionamento de VCPUs em execução.

Esta fila é classificada pela prioridade de cada VCPU.

Uma prioridade de VCPUs pode ser um dos dois valores, over ou under, que representam se a VCPU excedeu ou não a sua fatia de acesso justa ao recurso do processador.

Enquanto uma VCPU executa, consome créditos.

Assim, frequentemente é recalculada a contabilidade de quantos créditos cada máquina virtual ativa ganhou.

Os créditos negativos implicam em uma prioridade over.

Até que uma VCPU consuma todos seus créditos, sua prioridade estará under.

Quando uma CPU não encontra uma VCPU de prioridade under em sua fila local de funcionamento, buscará em outra CPU por uma VCPU de prioridade under.

Assim, garante o balaceamento de carga, onde cada máquina virtual recebe sua parte justa de recursos do processador.

Antes que um processador fique inativo, olhará em outro processador para encontrar uma VCPU executável.

Isto garante que nenhum processador trabalhe lentamente quando existem execuções a cumprir no sistema.

Portanto, o escalonador tem a capacidade de quando uma CPU estiver parada, buscar VCPUs de outras CPUs para execução.

Mostra como a CPU0 (acima) é dividida em duas VCPUs e como uma VCPU da CPU0 é realocada para a CPU1, sendo depois a VCPU, que estava originalmente na CPU1, passa para a CPU0, e assim por diante.

Atualmente, este projeto se encontra em andamento, na fase de avaliação de desempenho do escalonador em diferentes arquiteturas (arquiteturas SMP, dual core e quad core).

Considerando a disponibilidade de acesso ao ambiente proposto, tanto de hardware quanto de software, é possível a obtenção de resultados mais próximos aos reais.

Neste trabalho, são utilizados benchmarks, softwares com estágios cíclicos, que interagem com o sistema operacional coletando estatísticas do seu funcionamento, medidas através da carga de processamento executada por eles.

Através de seus relatórios será possível comparar os diversos cenários e chegar a conclusões de ganho ou perda de desempenho.

Esse trabalho trata de realocação de recursos em máquinas virtuais, em específico o Xen, com foco no escalonador SMP Credit em máquinas SMP e Multi Core.

Como a pesquisa não se encontra finalizada, ainda não foi iniciada a fase de otimização do escalonamento do Xen em máquinas que permitam processamento paralelo.

Porém já podem-se elencar algumas considerações, como as citadas a seguir.

Embora indicado para máquinas com processadores simétricos, o SMP Credit só consegue fazer um bom balanceamento de recursos se existir algum processador livre, o que é difícil de ocorrer em um ambiente de produção.

Ainda, quando utiliza processos que utilizam intensivamente entrada/saída não existe como fixar limites aos domínios sobre a utilização das CPUs, pois a escrita/leitura é controlada pelo domínio0 e não pelas máquinas virtuais.

Nota-se também que, se tivermos dois processos (A1 e A2) onde A1 está utilizando o processador e A2 está bloqueado, o processo A1 está consumindo créditos e proporcionalmente perdendo prioridade.

Quando o processo A2 entrar em estado de execução, ele receberá uma fatia de processamento maior que o processo A1, pois terá menos créditos e maior prioridade, não garantindo um compartilhamento justo que é o princípio do escalonador Credit, pois penaliza o processo A1.

Assim, o objetivo principal deste trabalho é propor uma melhoria no algoritmo de escalonamento do Xen, que propicie otimizar seu desempenho em máquinas com multiprocessadores simétricos e multi core.

Atualmente, alguns aplicativos necessitam de uma alta taxa de processamento para serem executados, no entanto, uma máquina apenas não consegue prover esse poder de processamento necessário para a execução desses aplicativos.

Existem alguns métodos para a resolução desse problema como por exemplo, programação paralela em máquinas multicore, e clusters, que constitui em um conjunto de computadores interligados através de redes de alto desempenho, com a característica de que cada máquina possui a mesma arquitetura.

Uma outra proposta de solução interessante é empregar o processamento em GRID, esta técnica consiste em um conjunto de clusters com arquiteturas heterogêneas distribuídos globalmente, a idéia é dividir a computação disponível para cada uma das máquinas.

Programas escritos em linguagens funcionais podem ser facilmente particionados para uma execução em paralelo.

Os programas desse tipo de linguagem têm a característica de serem formados por sub expressões que podem ser simplificadas separadamente, além de serem programas pequenos com grande poder expressivo.

Dessa maneira, os programas funcionais são adequados para a computação paralela em GRIDs, uma vez que é permitida partição desses programas e a conseqüente execução em paralelo de cada uma de suas partes.

O objetivo do presente trabalho, é fazer um estudo sobre o uso da linguagem pFun no desenvolvimento de programas paralelos para GRIDs.

Para avaliar a linguagem, foi desenvolvida uma aplicação para calcular a aproximação do Pi através do método Monte Carlo.

Essa aplicação foi executada em um pequeno GRID e a partir dos resultados, algumas modificações na máquina virtual da linguagem são propostas para melhorar o desempenho de programas pFun em GRIDs.

A linguagem pFun é uma linguagem funcional de paralelismo semiexplícito construída especialmente para a programação de diferentes arquiteturas paralelas incluindo, máquinas multicore e clusters.

Esta linguagem está sendo implementada no projeto de pesquisa Mobilidade de Código usando uma Linguagem Funcional Pura, na UCPel.

PFun possui um compilador, que gera bytecodes, os quais são executados sobre a máquina virtual pFun.

Assim como toda a linguagem funcional pura, pFun é baseada no modelo matemático do cálculo lambda e, ainda, possui as seguintes características, é puramente funcional (inexistência de atribuição), avaliação estrita (os argumentos da função são avaliados antes da chamada da função) e funções da alta ordem (funções que recebem funções como argumento).

A seguir são apresentadas as primitivas de paralelismo, e a máquina virtual da linguagem pFun.

A criação de programas paralelos em pFun é feita através de duas primitivas da linguagem, par e sync.

A primeira indica que uma tarefa pode ser executada em paralelo, mas quem decide se essa tarefa será avaliada em paralelo ou não é a máquina virtual pFun.

A segunda devolve o resultado para o programa de uma avaliação que pode ter sido executada em paralelo.

A primitiva par recebe como argumento uma expressão de qualquer tipo e retorna uma referência para um objeto Par (tarefa).

A primitiva sync recebe como argumento uma referência para um Par e devolve o resultado da avaliação dessa tarefa.

As primitivas de paralelismo par e sync podem ser vistas como construções de baixo nível para a programação paralela, entretanto, com o uso delas é possível escrever construções mais poderosas, como por exemplo, os esqueletos de paralelismo.

Estes são funções de alta ordem que encapsulam padrões recorrentes da computação.

Essas funções podem ser usadas pelos programadores para escrever software paralelo mais facilmente, uma vez que o programador não precisa se preocupar com alocação e sincronização de tarefas.

Um esqueleto disponível na linguagem pFun é o parMap, que recebe como argumentos uma função e uma lista e avalia em paralelo a aplicação dessa função a todos os elementos da lista, gerando uma lista de resultados.

O modelo distribuído pFun está baseado na idéia de que usuários voluntários possam doar tempo ocioso de CPU de seus computadores.

Essa arquitetura distribuída é composta por dois tipos de hosts, slaves e work servers.

Os slaves, após conectados a um work server, ficam pedindo por uma tarefa para executar.

O work server gera tarefas para os slaves, estes recebem o trabalho, o executam e enviam o resultado de volta ao seu work server.

Geralmente vamos ter sempre um work server principal que inicia a avaliação do programa pFun.

Toda a vez que ocorre uma chamada a primitiva par, uma nova tarefa é adicionada a fila de tarefas do work server, que depois a distribui para um slave executar.

Um usuário que deseja contribuir com o projeto, pode instalar uma máquina slave em casa e este deve ser configurado para acessar um work server conhecido.

Um cluster pode ser usado no sistema pFun, instalando uma das máquinas como work server e as outras como slaves.

A aplicação consiste em uma implementação paralela do método Monte Carlo usando a linguagem pFun.

O método Monte Carlo é um método estatístico, o qual é utilizado em aplicações que usam em sua resolução simulações que estão relacionadas com o acaso ou possíveis de serem enunciadas através de probabilidades estatísticas.

O paralelismo é introduzido na aplicação usando o esqueleto parMap da linguagem pFun.

O programa foi executado num pequeno GRID.

GRID usado nos testes.

A aplicação cria tarefas em dois níveis usando parMap, inicialmente cria 30 tarefas, para serem executadas em paralelo, que calculam o valor do Pi.

Cada uma delas divide seu trabalho e cria 15 novas tarefas também utilizando parMap.

O tempo de execução da aplicação em uma máquina do cluster da esquerda (um Pentium 4 16 Ghz) foi de 4 horas, 58 minutos e 13 segundos.

Em uma máquina do cluster da direita (um Athlom XP 2400+), o tempo de execução foi de 2 horas, 49 minutos e 20 segundos.

Usando o GRID, o tempo de execução foi de 27 minutos e 18 segundos que possui um speed up de 11 comparado com as máquinas mais lentas e de 62 comparado às mais rápidas.

Dessa forma, os resultados mostram que a máquina virtual consegue distribuir o trabalho de forma igual para todas as máquinas, entretanto, no final da execução do programa, computadores mais rápidos ficam sem trabalho enquanto esperam por computadores mais lentos terminar o processamento.

A linguagem funcional paralela pFun, assim como as outras linguagens funcionais, estimula os programadores a escrever programas pensando somente em funções e expressões.

A programação funcional traz consigo várias vantagens, seus programas são pequenos e com alto poder de expressão.

O desenvolvimento, a implementação e a avaliação da versão do método Monte Carlo em paralelo foi uma boa proposta para explorar o potencial de paralelismo da linguagem pFun devido a necessidade da alta taxa de processamento exigida por esta aplicação.

Nos experimentos realizados notouse que em alguns momentos existem máquinas lentas trabalhando enquanto máquinas mais rápidas ficam ociosas pois não existe mais trabalho para ser realizado.

Isso acaba gerando uma grande perda de desempenho.

Uma proposta para solucionar esse problema seria o uso de uma técnica chamada Workqueue With Replication (WQR).

Esta técnica consiste em, após não haver mais tarefas na fila de tarefas do work server, associar réplicas das tarefas que estão sendo executadas nas máquinas lentas às máquinas ociosas.

Se uma tarefa terminar, as réplicas que estavam em execução são abortadas.

A solução final devolvida ao work server vem da máquina que terminar a execução primeiro.

O emprego de técnicas de identificação de sistemas tem sido muito estudado devido a sua vasta utilização nos campos da engenharia.

Em engenharia estrutural, a identificação de sistemas é geralmente conhecida como problema inverso de dinâmica estrutural.

No projeto de obras de engenharia uma das principais preocupações é a determinação do amortecimento.

Neste trabalho deseja-se estimar o valor dos coeficientes de amortecimento de uma estrutura vibratória simples, representada por um sistema massa-mola amortecido com N graus-de-liberdade (GDL).

Este problema pode ser formulado através de uma forma funcional bem posta, baseada em dados de deslocamento da estrutura, cuja solução pode ser obtida através do emprego da metaheurística Algoritmo Genético (AG).

O AG apresenta um grande potencial e facilidade em ser empregado de forma paralela em ambientes distribuídos.

Este trabalho apresenta um estudo sobre diferentes modelos de paralelização do AG quando aplicados na solução de um problema inverso em vibrações, a saber, AG Global, AG Stepping Stones e AG Ilha.

O uso de diferentes modelos de paralelização tem como objetivo, além da redução do tempo de processamento, também a obtenção de soluções de melhor qualidade.

A implementação paralela foi realizada através da linguagem C, utilizando-se a biblioteca de troca de mensagens MPI em ambiente MPI/LAM.

O problema direto é aquele no qual são conhecidas as grandezas de entrada, tais como força externa, condições iniciais e parâmetros estruturais (massa M, amortecimento C e rigidez K ), e busca-se a resposta do sistema, representada pelo deslocamento (x(t) ), freqüência e/ou modos de vibração.

Através da modelagem matemática baseada na segunda lei de Newton, a equação da dinâmica vibratória de um sistema com N-GDL, apresenta-se da seguinte forma, Neste trabalho, a solução do problema direto foi obtida através da utilização do método de Runge-Kutta de 4ª ordem.

Para solução do problema inverso assume-se conhecida a resposta do sistema vibratório, aqui representada pelo deslocamento, e deseja-se estimar os coeficientes de amortecimento.

Com este objetivo, foi utilizada a metaheurística Algoritmo Genético.

Uma vez que os problemas inversos são classificados como problemas mal-postos, isto é, pequenas variações nos valores de entrada podem causar grandes mudanças nos valores de saída, a solução foi buscada através da minimização de uma forma funcional bem posta, onde x EXP representa o deslocamento obtido experimentalmente e x MOD representa o deslocamento fornecido pelo modelo matemático descrito na Equação.

O AG inicialmente cria e avalia uma população aleatória de soluções, em seguida entra em um processo evolutivo, selecionando pais (seleção torneio), cruzando-os (cruzamento aritmético), realizando ou não mutação nos filhos gerados (mutação não-uniforme), avaliando-os e substituindo os antigos indivíduos da população.

Esse processo continua até que uma condição de parada seja satisfeita (número máximo de gerações).

Juntamente com os operadores genéticos são definidos também alguns outros parâmetros, tamanho da população, pressão seletiva, probabilidade de cruzamento, probabilidade de mutação e tamanho da população elite.

O método do AG exige um grande esforço computacional e, além disso, deseja-se analisar estruturas com um grande número de GDL, mais próximas da realidade, consequentemente com uma exigência computacional ainda maior.

Foram avaliadas diferentes estratégias de paralelização, o modelo AG Global, ou mestre-escravo, onde uma única população é utilizada, porém o cálculo da função aptidão tem sua execução distribuída entre os processadores, utilizando-se para isso os comandos Send e Recv da biblioteca MPI, e modelos baseados em múltiplas populações, modelos Ilha e Stepping Stones.
Onde a população total é dividida igualmente entre os processadores de forma que cada sub-população (ilha) evolua isoladamente, havendo transferência dos melhores indivíduos de cada sub-população após um número fixo de gerações (migração).

A comunicação entre os processadores é realizada através do comando Allgather da biblioteca MPI, fazendo com que todos os processos coletem os melhores indivíduos de cada processo em execução.

Mostra os diferentes paradigmas de migração utilizados neste trabalho.

O uso de múltiplas populações de forma simultânea permite uma busca ampla no espaço de soluções, permitindo a obtenção de estimativas de melhor qualidade, bem como a manutenção da diversidade das sub-populações, alcançada através da chegada de novos indivíduos de populações vizinhas, que introduzirão nova informação genética, conduzindo a busca de soluções a regiões mais promissoras.

Além disso, comumente a migração ocorre poucas vezes, fazendo com que a parcela de comunicação no tempo total de execução da aplicação seja reduzida, proporcionando speedups elevados.

Modelos de Paralelização.

A fim de simular os erros inerentes ao processo de medição (má transmissão de sinal captado pelos aparelhos de medida, fios mal conectados, precisão limitada do aparato experimental), foram utilizados dados experimentais de deslocamento contaminados ou não por ruídos, sendo simulados computacionalmente da seguinte forma, onde equivale ao desvio padrão dos erros de medida e N indica valores aleatórios com distribuição Normal (Gaussiana).

Mostra a comparação entre os coeficientes de amortecimento estimado e exato para um sistema com 10-GDL, utilizando-se dados experimentais contaminados por diferentes níveis de ruído.

Amortecimento estimado e exato para diferentes níveis de ruído.

Como esperado, a qualidade das estimativas diminui conforme aumenta a intensidade do ruído presente nos dados experimentais, uma vez que o método emprega, justamente, o valor do dado experimental para obtenção da solução, conforme a Equação.

Com relação ao desempenho dos diferentes modelos de paralelização, observou-se um speedup próximo ao linear, com uma eficiência acima de 95%, para os modelos de sub-populações avaliados, quando utilizados até 4 processadores.

Para quantidades maiores de processadores a eficiência diminuiu, porém manteve-se acima de 85%.

Foram também realizados testes para avaliar a influência da abordagem distribuída na qualidade da solução do problema inverso.

Observou-se que a partir de valores específicos para o tamanho da população e/ou número máximo de gerações, independentemente do número de processadores utilizados na versão paralela, o AG Ilha apresentou uma capacidade maior de fornecer melhores estimativas dos coeficientes de amortecimento se comparado com a sua versão seqüencial, evidenciando que o AG Ilha é um algoritmo diferente do AG seqüencial.

Comparação da precisão entre o AG Ilha e o AG seqüencial.

É apresentada uma proposta de solução para o problema inverso de estimativa dos coeficientes de amortecimento de um sistema mass-mola-amortecedor com N-GDL, baseada no uso da metaheurística algoritmo genético, utilizando-se diferentes modelos de paralelização.

Observou-se que, mesmo assumindo dados experimentais contaminados por ruídos de medida, o problema inverso foi resolvido de forma satisfatória.

Além disso, observou-se que o uso de modelos distribuídos do AG, além de reduzirem o tempo computacional necessário para a obtenção de uma boa solução, também forneceu soluções de melhor qualidade se comparado ao modelo seqüencial.

Arquiteturas multiprocessadas são caracterizadas por possuir múltiplas unidades de processamento compartilhando acesso a um espaço de endereçamento compartilhado.

Arquiteturas UMA (Uniform Memory Access) e NUMA (Non-Uniform Memory Access) consistem em organizações desta classe de arquitetura, onde as arquiteturas UMA representam organizações onde o tempo de acesso a qualquer posição de memória é uniforme e as arquiteturas NUMA organizações nas quais o espaço de endereçamento encontra-se distribuído em módulos de memória não eqüidistante de todos os processadores.

Nos multiprocessadores, cada processador é independente para efetuar sua própria computação, lendo e escrevendo dados em todo espaço de endereçamento.

Mais ainda, o próprio espaço de endereçamento serve como substrato às comunicações (troca de dados) entre processadores.

Os populares processadores multicore também podem ser considerados multiprocessadores, uma vez que possuem múltiplas unidades completas de processamento compartilhando acesso a um espaço de endereçamento comum.

É comum o emprego de arquiteturas multiprocessadas no processamento paralelo e de alto desempenho.

Para este fim, a multiprogramação leve é o modelo de programação que mais adaptado, uma vez que prevê a execução concorrente de fluxos de execução trocando dados via uma área memória global.

Existem diversas ferramentas de programação que oferecem este modelo de programação, entre elas POSIX Threads, ou Pthreads, é uma das mais populares.

Esta ferramenta permite decompor a execução de um programa em termos de fluxos de execução concorrentes, usualmente denominados threads, submetidos à execução sobre os recursos de hardware disponíveis.

Uma política de escalonamento é aplicada sobre estes threads para determinar o acesso dos threads aos processadores disponíveis.

Este trabalho tem por finalidade identificar se a associação de afinidade de threads a processadores pode trazer benefícios, em termos de melhora de desempenho, na execução de programas multithread.

O estudo inicial está sendo realizado sobre processadores multicore e terá seqüência sobre arquiteturas NUMA.

O mercado denomina "processadores multicore" os circuitos integrados que possuem dois ou mais núcleos de processamento completos no seu interior.

Como uma arquitetura multiprocessada UMA ordinária, todos os núcleos de processamento, ou cores, compartilham acesso ao mesmo espaço de endereçamento.

Existem diversas opções de configuração de processadores multicore, variando a o número e a velocidade dos cores, mas, em particular neste trabalho, atenta-se a variedade de opções para organização das memórias cache.

Além de diversas alternativas em termos de capacidade (tamanho em bytes) destas memórias, as opções atuais contemplam o uso de um  a três níveis de cache (L1, L2 e L3), havendo a possibilidade de compartilhamento de determinados níveis entre os cores ou de uso exclusivo.

Ilustra o esquema de blocos de duas arquiteturas dualcore, onde uma possui níveis L1 e L2 dedicados e a segunda L2 compartilhado.

Esquema de cache de processadores multicore com  e sem compartilhamento de cache L2.

Dado ao baixo custo e o potencial de paralelismo, este tipo de arquitetura vem sendo incorporado a soluções para o processamento de alto desempenho.

No entanto, deve-se atentar que um limitador do poder de processamento dos multiprocessadores está associado à capacidade da memória atender as múltiplas solicitações de acesso.

Moore discute que a adequação deste tipo de processador para processamento de aplicações que manipulam intensivamente dados não é direta, em função dos atrasos de computação decorrentes de acesso de um grande número, oito ou mais cores, segundo este autor, à memória compartilhada.

Uma alternativa é explorar a localidade de acesso a dados pelos threads via estratégias de escalonamento.

Desta forma, identificando threads a cores pode ser possível reduzir o número de caches-misses efetuados devido à movimentação de threads entre cores.

Threads no padrão Pthreads consistem em unidades de descrição de concorrência de um programa.

Quando em execução, os recursos de processamento de um determinado processo são compartilhados entre todos os threads que o compõem.

No entanto, cada thread é uma unidade de escalonamento independente, podendo, os threads, serem escalonados de três maneiras distintas, conforme o modelo de implementação empregado na ferramenta.

Os modelos para implementação de threads são conhecidos como, 1,1, N,1 e N,M.

No modelo 1,1, ou one-to-one, threads são denominadas threads kernel, manipuladas de maneira individual pelo sistema operacional, se um thread é bloqueado para a realização de outra operação as demais threads continuam suas atividades sem prejuízo.

No modelo N,1, ou many-to-one, threads usuário são escalonadas no escopo do processo, quando ocorre o escalonamento deste.

Ou seja, o sistema operacional reconhece apenas o processo como unidade de escalonamento, os threads consistem em unidades de escalonamento internas ao processo.

Nesta situação, quando um thread é bloqueado em uma sincronização externa ao processo, todos os threads do processo bloqueiam igualmente.

No modelo N,M, ou many-to-many, existe uma distinção uma dissociação entre a concorrência da aplicação, descrita por N threads usuário e o paralelismo de execução, caracterizadas por M threads sistema.

Neste modelo, o sistema operacional é responsável pelo escalonamento dos threads sistema enquanto que no nível aplicativo é realizado o escalonamento dos threads usuário sobre os threads sistema.

Algumas primitivas definidas pelo padrão Pthreads são apresentados na seqüência.

Respectivamente para criação, término e sincronização entre duas threads.

Como threads compartilham uma mesma área na memória, casos dois threads acessem o mesmo valor da memória, os trechos de código que realizam estes acessos são considerados sessões críticas.

Cada thread deve obter exclusividade ao dado na memória, essa exclusividade é garantida pelo uso de mutex.

As primitivas na seqüência permitem inicializar um mutex, obter e liberar um mutex, controlando o avanço da execução de threads sobre as sessões críticas.

Observa-se que o padrão define que, quando um thread é criado, a ele pode ser associado um atributo de escalonamento e de escopo.

Este último atributo permite indicar se o thread deve ser escalonado como thread usuário ou thread sistema (modelos N,1 ou 1,1).

O atributo de escalonamento permite indicar se o thread deve ser escalonado com a política FIFO, Round-Robin ou a provida pelo sistema operacional.

O suporte a estes atributos é dependente da implementação da ferramenta Pthreads utilizada, devendo ser consultado as alternativas oferecidas por esta.

A associação de afinidade de thread a um, ou a um grupo de, processador é possível na maioria dos sistemas operacionais disponíveis.

Em ambiente GNU-Linux as operações para manipular afinidade são, Estas primitivas permitem, respectivamente, associar afinidade de um thread a um grupo de processadores e obter qual afinidade um thread possui.

Os parâmetros representam o identificador do processo, o tamanho, em bytes, da máscara de afinidade e a própria máscara.

Esta máscara deve ser vista como uma seqüência de bits, cada bit correspondendo a um determinado processador/core, onde o bit menos significativo representa o processador/core um.

Caso nesta máscara o valor de um determinado bit seja um, então o processador/core correspondente deve ser associado ao thread.

Por exemplo, o valor 00000101 indica que o thread deve ser associado aos processadores/cores 1 e 3.

A expectativa é que o uso da afinidade permita explorar a localidade de referência a dados em memória e em cache resultando em um maior desempenho de execução.

No entanto, alguns experimentos, como os realizados por Foong e Costa mostram que o ganho de desempenho não é significativo, havendo, ainda, perda de portabilidade do código.

Foong ainda afirma que aplicar unicamente a técnica de controle de afinidade não garante melhora de desempenho de execução.

O trabalho de pesquisa documentado neste artigo esta sendo realizado no contexto do projeto Anahy e vislumbra-se integração dos resultados no núcleo de execução de Athreads.

A continuidade será no estudo dos recursos que permitem, nos modernos processadores multicore, selecionar diferentes freqüências de trabalho para cada core.

O objetivo a ser atingido está associado à obtenção de uma estratégia de escalonamento que considere a carga de trabalho gerada por um programa desenvolvido em Athreads para adequar a velocidade de operação de cada core de um chip multicore.

