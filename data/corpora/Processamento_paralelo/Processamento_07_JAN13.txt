Classificação de dados é uma das atividades mais antigas e comuns ao homem.

ANDERBERG define alguns exemplos de classificação abordando categorias típicas, como religião, etnia, posição política, tipo de emprego, estilo de roupa, etc, em função das características que definem cada classe.

Define também classificação (ou identificação), como sendo o processo ou ato de associar um novo item ou observação a uma categoria.

Como exemplo, listam-se abaixo alguns grupos em que uma pessoa pode ser classificada, seguida dos atributos correspondentes.
Classificação 1, sexo (feminino ou masculino).
Classificação 2, nacionalidade (país onde nasceu).
Classificação 3, naturalidade (unidade da federação onde nasceu).
Classificação 4, grau de instrução (analfabeta, alfabetizada).
Classificação 5, estatura (baixa, alta).
Assim sendo, um indivíduo que nasça no Rio de Janeiro (Brasil) é classificado como brasileiro, se for engenheiro é classificado como alfabetizado e se tem 1,80 m de altura é considerado alto.

Visando a classificação de vários indivíduos utilizando-se representação computacional, faz-se a correlação dos valores não-numéricos de cada classe com um valor numérico correspondente, como descrito abaixo, Sexo, 0 = feminino 1 = masculino.

Nacionalidade, utilizar-se-á o valor correspondente ao do código de barra (EAN-13), código aceito internacionalmente, onde o Brasil tem número 789.

Naturalidade, Vê-se que o Rio de Janeiro, na ordem alfabética de estados, encontra-se na 19 posição.
Grau de instrução, 0 = analfabeto 1 = alfabetizado.

Estatura, valor numérico.

Não necessita conversão.

Unidades da Federação do Brasil no ano de 2001 E, assim, a pessoa descrita acima (homem, nascido no Rio de Janeiro Brasil de grau superior, com 1,80 m de altura).

Atributos de um indivíduo.

Cada coluna de caracterização de um indivíduo é denominada variável (ou atributo) e cada indivíduo (contendo uma série de variáveis) compõe uma linha (ou amostra).

Este tipo de representação facilita a manipulação por computador de grandes massas de dados.

Tem-se que tomar cuidado com as correlações de dados, o que faz com que uma variável seja dependente de outra, ou seja, alterando-se a quantidade de uma variável não deve haver alteração em qualquer outro atributo.

Por exemplo, não faz sentido definir, para um mesmo indivíduo, as variáveis Data de Nascimento e Idade, pois a alteração em data de nascimento deve de forma direta alterar o valor representado em idade.

Diz-se então que os dados são correlacionados, e não devem estar juntos, pois além de representarem a mesma coisa, a alteração em data de nascimento sem alterar idade de forma conveniente faz com que haja inconsistência dos dados.

Observe também que nem sempre é tão fácil descrever um indivíduo.

Por exemplo, o indivíduo descrito acima poderia ser classificado como potencial candidato a jogar pela seleção brasileira masculina de futebol, pois ele é do sexo 1 (masculino), da nacionalidade 789 (Brasil) e tem boa estatura (1,8 m).

Os atributos naturalidade e grau de instrução, neste caso, tem pouca importância para a classificação, ora em andamento.

Em compensação, para a seleção brasileira masculina de vôlei, o atributo estatura não teria boa classificação, não caracterizando o indivíduo como alto.

Isto significa dizer que, em função da classificação proposta, cada variável deve ser ponderada em virtude de sua importância para a classificação, variando de 0 (zero) a n, sendo n um valor contido no conjunto dos reais.

A maioria dos métodos de classificação requer uma medida de similaridade entre indivíduos conhecidos e desconhecidos, medida essa que é usualmente conhecida por distância.

Vários são os métodos de classificação existentes, tais como ferramentas estatísticas, rede neural, algoritmo genético e lógica nebulosa (fuzzy logic).

Este trabalho terá como foco a metodologia de classificação de dados por KNN (K-Nearest Neighbor), ou K-Vizinhos mais Próximos, pela qual um indivíduo é classificado pela similaridade encontrada com K indivíduos previamente agrupados por coincidência de características.

A metodologia KNN será auxiliada por um Algoritmo Genético devido à sua capacidade de encontrar, dentro do espaço de busca desejado, valores de peso no conjunto dos reais positivos.

Esses pesos serão utilizados na ponderação de cada atributo do indivíduo, determinando seu grau de importância para a classificação.

A análise dos resultados será feita à luz da lógica nebulosa (fuzzy logic), e não da rigidez da matemática exata (crisp sets) do pertencer ou não pertencer.

Assim sendo, um indivíduo poderá ter todas ou parte das caraterísticas de um ou mais grupos de dados.

O objetivo desta tese será o de contribuir com uma metodologia de classificação fuzzy de dados que agrupe indivíduos em classes de acordo com o seu grau de pertinência, utilizando otimização por algoritmo genético dos diversos parâmetros envolvidos na classificação, em ambiente de computação de alto desempenho, pela paralelização das tarefas de classificação, visando o processamento de grandes massas de dados.

Além disso, este trabalho contribuirá com a automatização da tarefa de se encontrar os parâmetros ótimos de classificação (pesos das variáveis, valores reais), o valor de K (valores inteiros) número de vizinhos mais próximos e do parâmetro m (utilizado no algoritmo de classificação KNN-FUZZY, valores reais), atualmente sob responsabilidade do usuário do sistema de classificação.

Desconhece-se trabalho anterior que utilize valores de m não inteiros, sendo usual utilizar-se os valores 2 ou 3.

Modelos descritivos de dados produzem somente uma saída formatada, e o fazem pelo ordenamento de um ou mais atributos dos n atributos de uma amostra.

Modelos de predição (ou de classificação) tem por função identificar a classe correta de uma amostra pela comparação com um conjunto de amostras cujas classes são conhecidas.
Ou seja, se C = {ci1, ci2,cin-1, cin} for um conjunto de amostras cujo enésimo atributo corresponde à classe da amostra, e d = (d1, d2, dn-1) for a amostra desconhecida (a classificar), utiliza-se então um dos métodos de classificação supervisionada para classificar a amostra desconhecida dentre uma das classes existentes.

Uma amostra será definida por seus atributos, e é através deles que a amostra será comparada.

Quando todos os atributos utilizados na classificação são numéricos, a similaridade entre amostras pode ser medida numericamente, geralmente pela distância entre os pontos.

Várias são as medidas de distância empregadas em classificação de dados, A distância Euclidiana é o caso particular da distância de Minkowski fazendo-se q = 2 e cada peso wi igual a 1.

Das medidas de distância apresentadas, a mais utilizada é a distância Euclidiana.

Sendo assim, o modelo preditivo geralmente define uma função S que objetiva calcular a diferença entre o resultado predito y'(i) e o valor real y(i).

Há casos porém em que a quantidade de amostras da base de dados é relativamente pequena.

E, com isso, pode-se não ter boa representatividade da base de dados a classificar, ou seja, na divisão da base total em base de dados das amostras classificadas, base de dados das amostras de treino (utilizada para encontrar) e base de dados das amostras de teste, pode-se não ter uma proporção correta das classes envolvidas pelas bases citadas.

A razão em se ter bases de treino e teste, é baseado na necessidade em se conhecer a proporção dos erros cometidos entre a fase de obtenção das regras de classificação (treino) e a fase de classificação de novas observações (teste) sem o conhecimento, por parte do classificador, das verdadeiras classes dessa última.

Mais detalhes podem ser vistos em MICHIE Para resolver este problema, pode-se repetir todo o processo de classificação compondo-se bases diferentes a partir da base total, computando-se a média dos erros encontrados.

Pode-se também inverter as bases de treino e de teste, de tal sorte que o erro médio seja a média entre os erros encontrados nas duas "rodadas".

Uma terceira técnica, derivada desta última (inversão das bases de treino e teste) e conhecida por validação cruzada (cross-validation), propõe a divisão da base de dados total em diversas bases menores, por exemplo, três, cada base resultante desta divisão conterá a mesma quantidade de amostras de mesma classe.

Por três vezes haverá um rodízio no papel desempenhado por cada uma das bases, ou seja, ora uma das bases será a base de dados das amostras classificadas, ora será a base de dados das amostras de treino e ora será a base de dados das amostras de teste.

Os erros de cada rodada são então somados, obtendo-se com isso o erro médio.

Diversos métodos de classificação supervisionada foram desenvolvidos a partir de técnicas que compreendem ferramentas estatísticas, redes neurais, algoritmos genéticos e lógica nebulosa (fuzzy).

Um dos mais antigos métodos.

Realiza classificação pela divisão do espaço amostral em uma série de linhas retas em duas dimensões, as quais passam pela metade da distância dos centros de duas classes a separar, e cujas direções são determinadas pelas formas das classes.

Apresenta a separação das classes Setosa, Versicolor e Virgínica do bem conhecido conjunto de dados Íris, o qual divide igualmente 150 amostras da flor Iris nessas 3 classes (sendo a classe setosa linearmente separada das outras duas).

Comprimento de Pétala.

Classificação da base de dados Íris por discriminantes lineares.

Pela sua natureza linear, vê-se que o método é bastante limitado e apresenta falhas substanciais de agrupamento nos casos em que as classes são muito próximas.

É um método baseado na repartição recursiva do espaço amostral, sendo o espaço redividido em subespaços subseqüentes, utilizando a estratégia "dividir-para-conquistar", muito utilizada em análise de sistemas, que consiste em decompor um problema complexo em subproblemas mais simples.

Inicia-se a árvore selecionando um atributo qualquer para compor o nó raiz.

Através da utilização de regras adiciona-se novos ramos, os quais comporão diferentes ramos, até que se encontre a classe da amostra, obtendo-se um nó folha.

Por exemplo, utilizando-se uma vez mais o conjunto de dados Iris, constrói-se a árvore.

Classificação do conjunto de dados Iris por árvore de decisão.

As regras utilizadas na construção da árvore são mostradas.

Regras de classificação do conjunto de dados Iris.

Como pode ser visto acima, conforme a quantidade de regras vai aumentando, aumenta também o custo para se proceder à classificação dos dados (observação semelhante pode ser feita em relação ao número médio de condições por regra).

Sendo uma base de dados com m atributos e n instâncias (classes), o custo em operações ( O ) envolvido na classificação dessa base é igual a, Isto significa dizer que, conforme o número de instâncias aumenta, aumenta, de forma logarítmica, a carga de processamento envolvido na classificação dos dados.

Redes neurais artificiais ou, simplesmente, redes neurais, são sistemas baseados no funcionamento do cérebro humano.

Uma rede neural é um grande processador paralelo distribuído e que tem a propensão natural por armazenar conhecimento experimental e disponibilizá-lo para uso.

Sua semelhança com o cérebro se deve por adquirir conhecimento por meio de processo de aprendizado e pelas "forças" de interconexão entre os neurônios, conhecidas por pesos sinápticos, serem usadas para armazenar esse conhecimento.

A rede neural é composta de várias unidades independentes (neurônios) cuja função é produzir uma separação linear, no espaço, de suas entradas.

Cada unidade é ativada em função dos valores de entrada e, ultrapassado um valor de resistência denominado limiar (threshold), produz um valor de saída que servirá em alguns casos como valor de entrada de outros neurônios.

Modelo de neurônio artificial.

O neurônio, pode ser utilizado para construir qualquer expressão lógica finita, consistindo da soma de várias entradas ponderadas a qual servirá como parâmetro de uma função não-linear denominada "função de ativação" (threshold function).

Várias são as funções de ativação utilizadas em redes neurais, que contém as funções degrau, rampa e sigmóide.

Função degrau com intervalo entre 0 e 1.

Função degrau com intervalo entre 1 e 1.

Função rampa com intervalo entre 0 e 1.

Função rampa com intervalo entre 1 e 1.

Função sigmóide.

De uma forma geral, uma função ou pode possuir um argumento que deva exceder um certo patamar para alcançar um novo estado de ativação (funções degrau e rampa) ou pode ser diferenciável (função sigmóide).

Diversas outras funções, como tanh são também comumente utilizadas.

Como dito anteriormente, um neurônio separa o espaço de entrada em duas classes por meio de um plano, o que é uma limitação, pois permite a execução de poucas funções lógicas.

Separação do espaço amostral por um neurônio.

Por exemplo, as funções lógicas E (AND) e OU (OR) podem ser resolvidas com um simples neurônio, O neurônio e a função lógica E Tabela verdade da função lógica E (AND).

O neurônio e a função lógica OU.

Ao passo que a função OU EXCLUSIVO (XOR).

Tabela verdade da função lógica necessita de duas retas para a separação dos dados em classes iguais.

Representação espacial 2 D das saídas produzidas pela lógica XOR (OU EXCLUSIVO), o que pode ser alcançado com uma configuração de rede com dois neurônios.

Rede neural para a função lógica OU EXCLUSIVO.

Considerando duas saídas parciais (y1 e y2), produzidas por cada um dos neurônios iniciais.

Saída produzida pelo primeiro neurônio (y1).

Saída produzida pelo segundo neurônio (y2).

O último neurônio produz a tabela verdade resultante apresentado.

Tabela verdade resultante da função lógica OU EXCLUSIVO (XOR).

Ou seja, uma rede neural utiliza pesos (wi) para ponderar os valores das conexões (sinapses).

Uma vez que um conjunto de entradas não forneça a saída desejada, deve-se treinar a rede para que as sinapses sejam devidamente alteradas.

Um exemplo de algoritmo de treinamento de redes neurais é o algoritmo de backpropagation.

Iniciar os pesos (wij) com valores aleatórios.

Apresentar o vetor de entrada, xn.

Obter a(s) saída(s) desejada(s), ym.

Calcular o erro obtido, ej.

Se a condição de parada não for satisfeita.

Recalcular os pesos (da camada de saída para a camada de entrada).

Retornar ao item 2.

Condição de parada satisfeita.

Imprimir os vetores de entrada e de saída Algoritmo de treinamento backpropagation.

De acordo com o algoritmo, vê-se o potencialmente grande esforço computacional na procura dos pesos sinápticos que produzam a saída desejada, que podem não ser encontrados caso a topologia de rede utilizada não seja a adequada (número de neurônios, número de camadas da rede, etc), acarretando um tempo de treinamento extremamente grande.

Esta metodologia de classificação consiste na identificação de grupos de indivíduos com características similares e seu posterior agrupamento (clustering).

O método aqui estudado, KNN, ou K Nearest Neighbor (K vizinhos mais próximos), é um método muito simples, não exigindo conhecimento sobre funções densidade.

Utiliza o conceito de distância entre amostras (Equações 1, 2 e 3), considerando-se uma amostra um vetor (linha) contendo várias colunas (variáveis), ou seja, uma medida de similaridade entre pontos poderia ser baseado nas várias distâncias entre um ponto a ser classificado em uma dada classe e pontos de classes conhecidas.

Isto quer dizer que, se duas classes A e B possuem vários pontos em seus domínios, dado um ponto desconhecido x, este ponto será classificado em função da quantidade de pontos cujas distâncias forem as menores possíveis em relação às classes A e B.

Obtida de LEMOS, vê-se o conjunto de dados Iris representado graficamente no plano 3-D, em função do comprimento e largura da pétala e do comprimento da sépala.

Nota-se que a classe Setosa é bem definida.

Porém, as classes Virgínica e Versicolor têm alguns dados com grande similaridade (muito próximos).

Modelo tridimensional de separação entre as classes setosa (cor preta, canto inferior esquerdo), versicolor (cor cinza, mais ao centro) e virgínica (cor preta, canto superior direito) considerando-se somente o comprimento e largura de pétala e comprimento de sépala.

Faz-se a representação no plano 2-D da separação entre as classes setosa (representada pelo caracter #), versicolor (representada pelo caracter o) e virgínica (representada pelo caracter +) considerando-se somente o comprimento e largura de pétala.

Modelo bidimensional de separação entre as classes setosa (#, canto inferior esquerdo), versicolor (o, mais ao centro) e virgínica (+, canto superior direito).

Suponha que uma nova amostra de flor íris tenha que ser classificada, representada pelo caracter d (de desconhecida).

Amostra desconhecida (representada pelo caracter d).

Vê-se claramente que a amostra desconhecida não tem similaridade com a classe Setosa (devido à sua posição no gráfico em relação à classe Setosa), mas tem similaridade com as classes Versicolor e Virgínica.

Pela observação dos K vizinhos mais próximos (calculadas as distâncias representadas por setas), dentro do espaço de busca considerado, pode-se concluir, para alguns valores de K, K = 2, não se define uma classe para a amostra desconhecida.

As distâncias são iguais para as amostras de ambas as classes observadas.

A amostra desconhecida tem maior similaridade (65%) com a classe Versicolor, pois as menores distâncias convergem para as amostras dessa classe.

Dois vizinhos mais próximos ( K = 2).

Nenhuma classe definida.

Seis vizinhos mais próximos ( K = 6).

A classe Versicolor tem mais pontos próximos da amostra desconhecida E, em função da proximidade do ponto da amostra desconhecida d dos pontos das amostras da classe Versicolor, classifica-se d como sendo da classe Versicolor.

O cálculo da distância da amostra desconhecida em relação às amostras conhecidas é feita pela raiz quadrada do somatório do quadrado da diferença de cada uma das m variáveis (colunas) da amostra desconhecida (vd) em relação às m variáveis (colunas) de cada uma das i (i = 1 n) amostras conhecidas (vc).

Assumindo i amostras conhecidas e duas classes 0 e 1.
O algoritmo de classificação pelo método KNN pode ser descrito como indicado na Em MICHIE.
Vê-se que o valor de K deve ser aquele que satisfaz a função max que define o máximo valor de vizinhos (amostras) Kmáx que estão mais próximos da amostra desconhecida, em relação a todas as amostras do conjunto considerado, K tot, K máx = max ( K tot) ( 6 ) Ou seja, quanto mais amostras (xnn) de uma classe (Cnn) estiverem próximas à amostra cuja classe é desconhecida (xi), mais fortemente se pode dizer que xi pertence também à classe Cnn.

A classe Cnn associada com os vizinhos mais próximos é uma variável aleatória, e a probabilidade de que a classe correta da amostra desconhecida Ci seja a mesma da classe Cnn, onde xnn é a população próxima à xi.

Quando o número de amostras é suficientemente grande, pode-se assumir que xnn é suficientemente próximo de xi, o que acarreta, ou seja, a probabilidade de a amostra de classe desconhecida xi e das amostras de classe conhecida xnn pertencerem à mesma classe é praticamente a mesma.

Porém, se xi for classificada como sendo da classe Ci, porque o conjunto de vizinhos mais próximos (xnn) pertencem a essa classe Ci, e a classe correta de xi for a classe Ck, diz-se que um erro ocorreu, onde r ( xi, xnn ) é o erro condicional ocorrido, fornecidos os valores de xnn e xi.

Este erro também pode ser representado por, E, mais uma vez, se o número de amostras é suficientemente grande, de tal forma que xnn é suficiente próximo de xi, tem-se a aproximação de (9), o que simplifica (11), ( 14 ) De acordo com FUKUNAGA, (10) converge para (12), conforme o número de amostras tende ao infinito.

Aplicações do método KNN podem ser vistas em.

O método KNN utiliza como métrica de classificação a distância, ou similaridade, entre amostras.

A função distância euclidiana, ajustada para atuar no domínio compreendido entre 0 e 1, atua satisfatoriamente quando os atributos são igualmente relevantes.

Tais domínios, contudo, são exceções à regra.

Na classificação de uma dada amostra desconhecida em relação às amostras contidas na hiperesfera de volume V, propõe-se a mesma importância para as diversas variáveis envolvidas, sem contudo levar em consideração que uma variável possa ter papel mais relevante na definição da classe que as demais variáveis.

Por exemplo, se uma base de dados contém informações (tamanho, habitat, classe) sobre elefantes, baleias e peixes, não bastaria verificar as variáveis tamanho e habitat para definir o animal, pois se um elefante tem 3 metros, uma baleia ou um tubarão também podem tê-los.

Seriam muito mais relevantes as variáveis habitat e classe.

A variável tamanho é necessária para diferenciar tubarões de outros peixes.

Para tentar resolver este impasse, propõe-se ponderar as variáveis de acordo com o seu grau de importância para a classificação das amostras.

Segundo ANDERBERG, com a ponderação das variáveis há a transformação do espaço original em um espaço de representação que pode ser mais amplo ou de dimensões menores em função da ponderação envolvida.

Representa no plano 3-D uma aproximação do hiperespaço considerado.

Nelas pode ser visto que hiperesferas delimitadoras de duas classes hipotéticas, o e +, devido à não ponderação das variáveis, englobam poucos dados.

Hiperesferas representadas no espaço tridimensional.

Ao sofrerem transformações as hiperesferas originam hiperelipses que podem ser também rotacionadas, englobando cada vez mais dados.

A quantidade de transformação e rotação sofrida pelas hiperelipses, é definida pelos pesos de cada uma das variáveis no contexto da classificação.

Espaço de representação com melhor delimitação das amostras das duas classes ( O e +) pela transformação do espaço original Uma vez ponderadas as variáveis e mapeado o espaço de representação de forma conveniente, procede-se a melhores taxas de classificação.

Sendo esses pesos valores numéricos do conjunto dos Reais, os mesmos podem vir a ter qualquer valor discreto, inclusive podem ser nulos, indicando a não necessidade de uma dada variável para o contexto da classificação (eliminação do problema de correlação entre variáveis).

Em ROSA vê-se como ponderar as variáveis com o auxílio de algoritmo genético.

"Conforme a complexidade de um sistema aumenta, nossa habilidade para fazer observações precisas e significantes sobre seu comportamento diminui até um ponto que precisão e significância (ou relevância) tornam-se características mutuamente exclusivas".

Com essa palavras, ZADEH definiu, em 1973, o que viria a ser um dos estudos mais avançados no campo dos sistemas cognitivos, explicando o porquê de sistemas digitais não conseguirem manipular a complexidade da vida real, diferindo-os dos humanos.

A frase acima, encontrada em ROSS, faz considerações acerca da imprecisão e da incerteza, conceitos utilizados intuitivamente no dia-a-dia, mas que são desprezados quando se utiliza lógica digital.

Por exemplo, 30 C é uma temperatura considerada alta, já 20 C é uma temperatura considerada baixa.

O que dizer de 26 C É uma temperatura "alta" ou uma temperatura "baixa"?

Ou é uma temperatura "média"?

A noção de temperatura então varia de acordo com o sentimento de cada um, em função do cenário encontrado.

Outro exemplo pode ser visto com altura de pessoas.

Uma dada pessoa pode ser considerada alta se tiver mais de 1,80 m de altura.

Para jogar basquete, entretanto, essa pessoa seria considerada baixa.

Seria necessário no mínimo 1,90 m de altura.

Dito isto, uma pessoa com 1,88 m seria baixa ou seria alta?

Ou seria mais ou menos alta?

Ou mais ou menos baixa?

O quão alta (ou baixa) essa pessoa seria classificada?

Avaliando alturas de forma exata (ou crisp), pessoas com menos de 1,90 m seriam baixas e, com 1,90 m ou mais, seriam altas.

Ou seja, uma pessoa ou seria baixa, ou seria alta.

Em conjuntos exatos (crisp sets), um elemento x de um conjunto X pertence ou não ao subconjunto A, se ele tem 100% de características dos elementos desse subconjunto, ou seja, Isto significa dizer que, em um conjunto exato não há redundância, sendo sempre inequívoco e sem ambigüidades, ou seja, seus elementos são sempre sim ou não, verdadeiro ou falso, sem considerar a hipótese de algo ser parcialmente verdadeiro e parcialmente falso.

Em geral, um conjunto nebuloso é um subconjunto definido em torno de um conjunto exato, o qual contém todos os dados do universo considerado.

Definido o subconjunto A contendo elementos x do conjunto universo X, representa-se a pertinência de cada elemento x em A pela função de pertinência, µ A ( x ).

A função de pertinência representa quaisquer elementos de X que pertençam a A, total ou parcialmente, dentro do intervalo.

Se µ ( x ) = 1, então x pertence totalmente ao subconjunto A, se µ A ( x ) = 0, então o elemento não pertence a esse subconjunto.

Se µ A ( x ) = 05, então o elemento possui, dentro do contexto da classificação adotada, 50% das características que definem os elementos do subconjunto A, e assim por diante.

Assim sendo, um conjunto nebuloso A é, então, definido em torno de seus elementos e respectivas pertinências, Por exemplo, dado o conjunto X, com A sendo subconjunto de X e definido como, diz-se que x1 tem 10% de pertinência ao subconjunto A, x2 tem 80% de pertinência e x3 tem 20% de pertinência.

Observa-se que a soma das pertinências não resulta em 100%, porque trata-se da pertinência de cada elemento ao subconjunto considerado.

Retornando ao caso da altura do basquete, assumir-se-á as designações abaixo (considerando um intervalo de 2 cm entre uma e outra altura), baixa, até alcançar 1,90, quando, então, é considerada como sendo 5% baixa.

As pertinências de cada elemento em cada um dos subconjuntos considerados, atém-se ao "sentimento" que o especialista tem do universo considerado.

Pode-se agora redefinir o conceito de pertinência envolvendo a classificação multivariável de dados, onde o dado não será mais de um ou de outro conjunto, mas será considerado como pertinente a um e a outro conjunto, levando-se em conta a pertinência do dado a cada classe envolvida.

Com isso, melhora-se acentuadamente a taxa de classificação, como mostram as hiperelipses alteradas de acordo com as regras nebulosas para conter todos os dados do universo considerado, Espaço de representação englobando as amostras das duas classes.

Mostra que as hiperelipses estão bem próximas das fronteiras uma da outra, mas que não estão superpostas em função das diferenças das classes envolvidas.

Quanto maior a nebulosidade considerada (menos exata menos rígida for a análise dos dados) maior a quantidade de dados considerados em ambas as classes, em função das pertinências de cada elemento.

Isto significa dizer que o método é capaz de alcançar altas taxas de classificação por considerar que um dado possa conter características de mais de uma classe.

De acordo com KELLER, o algoritmo de classificação KNN-Fuzzy baseia-se na associação da relevância de um vetor aos seus mais próximos K vizinhos.

Seja W = { Z1, Z2,Zc } o conjunto de c padrões representando c classes.

Seja µ ( x ) a pertinência (a ser computada) associada ao vetor x de amostras de classes desconhecidas.

De acordo com ( 19 ), as pertinências associadas a x são influenciadas pelo inverso das distâncias dos vizinhos e suas c classes.

O inverso das distâncias serve como um peso, levando em consideração quão perto, ou não, o vetor a classificar está do vetor classificado.

A variável m determina quão forte é o peso atribuído à distância de cada vizinho ao valor de pertinência.

Se o valor de m for igual a dois, então a contribuição de cada ponto da vizinhança é considerada recíproca à sua distância em relação ao ponto a classificar.

Conforme m cresce, cresce a contribuição de toda a vizinhança, fazendo com que as distâncias relativas ao ponto a classificar tenham menos efeito.

Quanto mais o valor de m se aproxima de um, mais forte se torna a importância, para a classificação, dos vizinhos mais próximos ao ponto a classificar, o que reduz o número de pontos que contribuem para o valor de pertinência.

Assumindo i amostras conhecidas, o algoritmo de classificação pelo método KNN-Fuzzy pode ser visto.

Algoritmo do método classificação KNN-Fuzzy.

Aplicações do método podem ser vistas em ROSA e HUNG.

Conquanto o método KNN-FUZZY seja particularmente eficaz na classificação de dados, ajustar seus parâmetros K e m, além dos pesos das variáveis (atributos) envolvidas sem dispor de um mecanismo automático para tal, torna-se uma tarefa extremamente árdua.

Para resolver este problema, utiliza-se algoritmo genético (AG) para encontrar os valores desses parâmetros.

Algoritmos Genéticos, ou AGs, introduzidos por HOLLAND, são métodos de busca que têm sido extensamente utilizados em aplicações onde o espaço de busca é muito grande.

Em essência um AG é, segundo GOLDBERG, "um algoritmo de busca baseado nos mecanismos da seleção natural e na genética".

AGs são inspirados no princípio de sobrevivência do melhor indivíduo, ou seja, aquele de melhor adequabilidade (fitness), o qual tem maior probabilidade de sobreviver e produzir descendência para a geração seguinte.

Dito isto, tem-se que a reprodução dos melhores indivíduos significa a reprodução das melhores soluções candidatas.

Em termos matemáticos, a otimização consiste em achar a solução que corresponda ao ponto de máximo ou mínimo da função objetivo.

As técnicas de busca e otimização geralmente apresentam, Um espaço de busca, onde estão todas as possíveis soluções do problema.

Uma função objetivo (algumas vezes chamada de função de aptidão, ou de função de adequabilidade ou fitness function), que é utilizada para avaliar as soluções produzidas, associando a cada uma delas uma nota.

Um AG típico compreende a geração de uma população inicial de cromossomos, seguindo-se a avaliação desses indivíduos e a aplicação dos operadores genéticos, caso necessário.

Passos de um Algoritmo Genético Típico.

Num primeiro passo, gera-se a população inicial com um conjunto aleatório de cromossomos que representem possíveis soluções do problema a ser resolvido.

Durante o processo evolutivo a população é avaliada, e cada cromossomo recebe uma nota (ou aptidão no jargão usual de AG) refletindo a qualidade da solução que ele representa.

Em geral, os cromossomos mais aptos são selecionados e os menos aptos são descartados (darwinismo).

Os membros selecionados podem sofrer modificações em suas características fundamentais através dos operadores genéticos de crossover e mutação, gerando descendentes para a próxima geração.

Este processo é repetido até que uma solução, satisfatória ou não, seja encontrada.

A condição de término pode estar contida num dos itens abaixo, Tempo limite de processamento Quantidade máxima de gerações Homogeneidade, em certo grau, entre os indivíduos da população de cromossomos Um indivíduo com uma determinada adequabilidade seja encontrado A codificação binária (HEATH ) é aquela em que cada componente elementar de um cromossomo (gene) recebe um dos elementos do alfabeto binário, formado pelo conjunto {0,1}.

Cada símbolo deste é chamado alelo.

Devido ao alfabeto binário ser formado por dois alelos, permite tão somente a representação de 2 cromossomos, em que "n" é a quantidade de genes que cada cromossomo possui.

O conjunto de todas as configurações que o cromossomo pode assumir forma o seu "espaço de busca".

Se o cromossomo representa "k" parâmetros de uma função, então o espaço de busca é um espaço com "k" dimensões, no mínimo, adotando-se uma cadeia de genes em quantidade suficiente para representar todos os parâmetros envolvidos.

Por exemplo, se a função de adequabilidade contém três parâmetros que devem receber pesos iniciados com 3, 7 e 0, respectivamente.
Pode-se codificar os valores da base dez na base binária (utilizando as operações usuais de conversão de bases).
E o cromossomo, mantida uma quantidade constante de alelos por gene (facilitando assim a extração de cada um dos genes) ficaria, A iniciação da população deve ser feita de tal modo que não gere uma quantidade de cromossomos (população) tão grande que prejudique o desempenho do algoritmo, nem tão pequena que limite seu poder de busca, ou seja, o tamanho da população depende de cada aplicação (ROBERTSON ).

Uma vez determinado o tamanho de uma população, seus indivíduos precisam ser iniciados de modo a introduzir a diversidade tão importante para o sucesso da evolução, pois do contrário, a convergência pode se dar prematuramente (KALLEL e SCHOENAUER ).

O método mais comum, e que será utilizado neste trabalho, é o que realiza uma atribuição aleatória dos alelos aos genes dos cromossomos, com a mesma probabilidade de escolha desses alelos.

Uma vez iniciados, os indivíduos da população são avaliados por uma função de adequabilidade, cujo objetivo é avaliar a adaptação de cada indivíduo ao ambiente em que está presente, sendo por isso responsável por grande parte do tempo de processamento de um AG.

Uma função de adequabilidade deve gerar um valor não negativo denominado adequabilidade do indivíduo.

Uma adequabilidade negativa, no contexto aqui considerado, não faz sentido devido ao fato de o cromossomo englobar todos os valores possíveis dentro do espaço de busca, o que implica em um indivíduo estar classificado entre pouco e muito adequado ao meio.

Assim sendo, quanto maior a adequabilidade de um indivíduo, maiores serão as suas chances de sobrevivência, reprodução e representatividade na geração seguinte, o que garante a evolução da população.

Para evoluir a população utilizam-se operadores genéticos, os quais promovem modificações que objetivam melhorar o desempenho médio da população a cada geração, utilizando informações sobre as gerações anteriores, o que faz surgir uma população descendente com poucos ou nenhum indivíduo da população inicial, em função da adequabilidade dos indivíduos nesta população.

De JONG e SPEARS estudaram a importância de cada um dos operadores genéticos geralmente empregados na evolução de uma população.

A formação da nova população começa com a seleção dos indivíduos da população ascendente com maior probabilidade de compor a geração seguinte.

Sendo c um cromossomo constituinte de uma população de n indivíduos, cada qual com um valor de adequabilidade f, tem-se a probabilidade p de seleção deste indivíduo dada por, São vários os métodos de seleção, como descrito em ESPÍNDOLA.

Neste trabalho será utilizado o método da Seleção por Torneio, o qual selecionará aleatoriamente (com probabilidades iguais) três cromossomos da população.

Destes cromossomos, o de maior aptidão será selecionado para a população seguinte, sendo o processo repetido até que se preencha a população descendente.

Uma vez selecionados os indivíduos da população descendente, entram em cena os operadores de crossover e mutação, que são mecanismos de busca dos AGs para explorar regiões desconhecidas dentro do espaço de busca.

O operador de crossover efetua a troca de fragmentos entre pares de cromossomos, cujas cadeias são cortadas numa posição aleatória produzindo, para cada indivíduo, duas quantidades de alelos que deverão ser remanejadas de cadeia.

Ponto de corte.

Operação de crossover entre dois cromossomos.

Este processo é conhecido por recombinação simples.

A recombinação pode prever vários pontos de corte, quando então é conhecida como recombinação múltipla.

O outro operador genético a ser aplicado é a mutação, o qual inverte um ou mais bits de um indivíduo numa probabilidade geralmente baixa, pois se de um lado melhora a diversidade dos cromossomos na população, por outro destrói a informação contida no cromossomo que sofreu a mutação.

Esta probabilidade geralmente encontra-se entre 0,1% e 5%.

Assim, o algoritmo deve prever para cada alelo, de forma aleatória, se haverá ou não a inversão do mesmo, Operação de mutação em três alelos de um cromossomo.

Outros operadores podem ser vistos em ESPÍNDOLA.

Este trabalho utilizará AG para determinar os pesos das variáveis utilizadas na classificação de dados por KNN.

Em resumo, utiliza-se AG com uma população inicial de cromossomos formada pela codificação binária dos parâmetros K e m e dos pesos iniciais de cada um dos atributos, de tal sorte que o espaço de busca seja razoavelmente coberto, ou seja, gera-se uma população inicial com um conjunto aleatório de cromossomos que representem possíveis soluções do problema a ser resolvido.

Avalia-se então a população de cromossomos decodificando-os e empregando esses valores na medida de similaridade entre as amostras, obtendo-se assim uma taxa de classificação correta das amostras de teste.

Encontrando-se uma taxa de classificação que satisfaça a condição de parada, imprimem-se os valores dos parâmetros desejados, o quais servem para as classificações que se sucederem.

Em caso contrário, procede-se à evolução da população de cromossomos.

A população de cromossomos é evoluída selecionando-se os indivíduos de maior adequabilidade, utilizando-se para tal o método de Seleção por Torneio, repetindo-se o processo até que se preencha a população descendente.

Uma vez selecionados os indivíduos da nova população, entram em cena os operadores genéticos de crossover e mutação, que são mecanismos de busca dos AGs para explorar regiões desconhecidas dentro do espaço de busca.

Repete-se então o processo de avaliação verificando-se a adequabilidade de cada cromossomo.

A idéia básica é bem simples.

Para treinamento do sistema, todos os pares de entrada-saída são armazenados em uma base de dados.

Quando uma nova classificação sobre um novo padrão tem que ser feita, a resposta é baseada nos padrões de treinamento da base de dados, pelo cálculo das distâncias entre o padrão a classificar e todos os padrões do conjunto de treinamento.

Para proceder-se à classificação, deve-se obter, primeiro, os valores ótimos dos pesos (Wi) de cada variável, de K e de m.

Para isso, faz-se a otimização do algoritmo pelo uso de algoritmo genético, cuja função é tão somente obter os valores mencionados na etapa de treinamento.

Para a obtenção dos resultados, foram desenvolvidos os aplicativos PREPARA, FAGKNN ( de, Fuzzy Algoritmo Genético K Nearest Neighbors) e FAGKNNP ( de, Fuzzy Algoritmo Genético K Nearest Neighbors Paralelo ), na linguagem C padrão, rodando em console tanto no ambiente UNIX como no ambiente Linux, com compilação direta em ambos os sistemas operacionais.

Os dois aplicativos são não visuais, rodando em modo console a fim de se otimizar tempo de processamento, executando as tarefas de classificação em segundo plano com o auxílio do comando unix shell "nohup aplicativo &", o que permite longos tempos de processamento sem a necessidade de se estar "logado" ao sistema operacional.

O aplicativo PREPARA tem por função preparar quatro bases de dados pela divisão da base de dados original em, Base de dados das amostras classificadas (denominada arq_cl) Base de dados das amostras de treino (denominada arq_tr) Base de dados das amostras de teste (denominada arq_te) Base de dados das classes das amostras de teste (denominada arq_te_cl).

Repartição da base de dados original.

O aplicativo interage com o usuário, obtendo as informações.
Nome da base de dados que contém as amostras.

Porcentagem do total de amostras que comporá a base de amostras classificadas.

Porcentagem do total de amostras que comporá a base de dados das amostras de treino.

Evidentemente, a porcentagem das amostras que compõem a base de dados das amostras de teste (cuja quantidade é a mesma da base de dados das classes das amostras de teste) se dá pela subtração de 100% das porcentagens definidas para a base de amostras classificada e para a base de amostras de treino.

A fim de se comprovar a teoria exposta foi criado o aplicativo FAGKNN, que classifica os dados de acordo com a metodologia KNN-Fuzzy, cujo algoritmo envolvido é descrito.

Algoritmo do aplicativo FAGKNN.

O arquivo de configuração FAGKNN CFG, contém as seguintes informações, Essas informações são lidas pelo aplicativo de classificação FAGKNN para iniciar as variáveis internas de controle, evitando recompilação a cada mudança de base de dados a classificar, bem como permitindo a alteração de fatores tais como Taxa de Mutação e Taxa de Crossover.

O aplicativo FAGKNN tem por função obter os valores dos pesos ( Wi ), de K e de m, além de treinar e/ou testar uma ou mais amostras, apresentando os resultados de classificação em um relatório.

O aplicativo FAGKNNP foi concebido para ser executado em ambiente paralelo, isto devido à necessidade de se otimizar o tempo de processamento para bases de dados consideradas muito grandes (de acordo com FAYYAD et al, bases de dados muito grandes são aquelas que contém mais de 100 K amostras com vários atributos).

Ambos os aplicativos ( FAGKNN e FAGKNNP ) realizam as mesmas tarefas.

O que os difere é o ambiente em que são executados.

O FAGKNN foi concebido para ser executado de forma serial (um processador somente), enquanto que o FAGKNNP foi concebido para ser executado em um ambiente multiprocessado, como os sistemas IBM RS/6000 SP, ou mesmo clusters de PCs utilizando código da biblioteca MPI (Message Passing Interface) devido à homogeneidade dos sistemas utilizados (PACHECO ).

Há, essencialmente, dois modos de se implementar um algoritmo em paralelo, paralelismo de dados e paralelismo de controle (PACHECO ).

No paralelismo de dados, obtém-se o paralelismo repartindo-se os dados entre os nós, no paralelismo de controle, repartem-se as instruções.

Devido à adequabilidade do problema ao paralelismo de dados, optou-se por esse esquema de repartição.

Numa primeira estratégia, a base de dados das amostras classificadas foi dividida pelos nós, sendo as bases de treino e de teste copiadas para cada um dos nós.

Após o cálculo das distâncias, uma ordenação (sort) paralela foi realizada, causando sobrecarga (overhead) no processo de classificação e, por vezes, a paralisação do processamento.

Numa segunda estratégia, considerou-se distribuir as bases de dados de treino e de teste entre os nós, enviando para todos eles, porém, uma cópia da base de dados das amostras classificadas.

Após o cálculo das distâncias, uma ordenação (sort) serial foi realizada seguindo-se a classificação da base de treino.

Esta última estratégia mostrou-se correta, pois a classificação se deu sem maiores problemas, com a consolidação dos resultados ocorrendo no nó zero, apesar de a memória RAM disponível no sistema ser um limitante, em função do tamanho da base de dados das amostras classificadas.

Divisão das bases de dados pelos nós.

Todos os nós iniciam com os mesmos parâmetros internos (configurados do arquivo FAGKNN CFG), mas com diferentes subconjuntos de dados de treino e de teste, divididos igualmente entre os nós.

O esquema utilizado neste trabalho contém um nó denominado "nó mestre" (ou nó zero), responsável por conter a população de cromossomos, operar geneticamente sobre essa população e, após cada operação genética, decodificar cada um dos cromossomos da população, enviando (via broadcasting) para os outros nós.
Os valores obtidos dessa decodificação (pesos das variáveis e os valores de K e de m), além controlar a condição de parada de execução do aplicativo e de totalizar os resultados obtidos afim de se produzir o relatório final.

Esquema de funcionamento do aplicativo FAGKNNP.

Durante a fase treinamento do método KNN-Fuzzy, cada nó obtém a sua taxa de erro parcial, a qual deve ser remetida (via MPI) para o nó mestre.

O nó mestre, então, processa cada erro parcial enviado de cada um dos nós com o seu próprio erro parcial, produzindo o erro total de classificação dos dados.

Este erro é utilizado para checar se a condição de parada foi satisfeita ou não, visto que pode-se escolher um dos subitens (listados anteriormente), 5 2 12, Total mínimo aceitável de classificações corretas no treino (%),5 2 13, Quantidade máxima de épocas (Digite ' 0 ' para quantidade infinita) Caso a condição de parada não tenha sido satisfeita, aplicam-se os operadores genéticos sobre a população de cromossomos, reiniciando o processo de classificação.

O esquema de repartição utilizado permitiu reduzir drasticamente a comunicação entre os nós.

A base Insur foi apresentada ao mundo acadêmico em 1998.

Utiliza dados extraídos de uma companhia de seguros de vida que descrevem o relacionamento entre clientes, contratos de seguro e componentes de tarifa.

A tabela original apresenta 147478 registros com 80 colunas cada registro, definindo duas classes.

Porém, muitos atributos não foram divulgados, obrigando a um pré-processamento da base de dados, gerando uma tabela final contendo 130143 registros com 64 colunas cada um.

No processamento serial, o aplicativo FAGKNN consumiu 105057 s de processamento e a rotina KNN, onde se tem o maior dispêndio de tempo de processamento, consumiu 104634 s.

De acordo com PACHECO, sendo T o tempo de processamento do programa serial, supõe-se que haja uma fração do programa r que seja "perfeitamente paralelizável" (aspas do autor) e uma parte igual a (1 r) que é inerentemente serial.

Com a execução do aplicativo de classificação obtém-se os valores de T e rT como sendo, A Lei de Amdahl quando aplicada mostra um coeficiente de paralelização de 99,6%.

Isto é explicado pela grande quantidade de tempo despendida pela rotina KNN-Fuzzy, que equivale a quase 99% do tempo total de processamento.

E, assim, obtém-se os valores de r e de (1 r), Obtidos os valores de r e (1-r), passa-se ao cálculo da Aceleração, ou Speedup (Sp), que é a razão entre a solução serial e a solução paralela, ao cálculo da Eficiência (Ep), que é a razão entre o Speedup e o número de processos envolvidos (cada processo sendo executado em um processador diferente), conforme (25), e ao cálculo do Speedup Ideal (IdSp), obtido de acordo com a lei de Amdahl, conforme (26).

Em função destes valores, um estudo foi feito visando a avaliação do desempenho do aplicativo FAGKNNP.

São apresentados os tempos computados para diversas submissões de classificação (todos parando a execução em uma época), variando-se o número de nós em cada uma das submissões.

Também apresenta os valores calculados de "speedup ideal" (IdSp), "speedup obtido" (Sp) e da "eficiência" (Ep).

Relatório sobre submissões de trabalhos (jobs).

Observa-se que o código tem eficiência acima de 97%, o que comprova que a estratégia de paralelização adotada foi a estratégia correta.

Apresenta a medição no sistema paralelo do speedup ideal e do speedup obtido pela Lei de Amdahl.

Representação gráfica dos valores de speedup.

Determinou-se também o desempenho do aplicativo pelo cálculo da eficiência.

Representação gráfica da eficiência do aplicativo FAGKNNP.

Observa-se que o código é altamente paralelizável, demonstrando alto ganho de desempenho com a adição de processadores para a realização da tarefa em paralelo, estando o speedup muito próximo do speedup ideal.

Resultados de classificação da base Insur.

Esses resultados foram obtidos com 97502 amostras classificadas, 103 amostras de treino e 32581 amostras de teste.

Todas as classificações foram obtidas com uma geração somente, pois a intenção aqui era o cálculo do speedup e da eficiência.

Este conjunto contém 150 amostras divididas igualmente em 3 classes, virgínica, versicolor e setosa (esta linearmente separável das outras duas), as quais são tipos da flor conhecida por Iris.

As classes são definidas em termos de quatro atributos numéricos e que trazem as informações de comprimento e largura de sépala e comprimento e largura de pétala, todas as dimensões dadas em centímetros.

De forma a proceder a classificação dos dados, distribuiu-se as amostras por conjuntos de dados específicos (conjunto das amostras cuja classe é conhecida, conjunto das amostras cujas classes servirão de treino para o algoritmo e conjunto das amostras de teste).

Distribuição das amostras por conjuntos de dados.

Foram realizados 121 testes, sendo adotadas as seguintes nomenclaturas, %Fuzzy, quantidade de "fuzzificação" a ser adotada na classificação.

Quanto menor esse valor, maior será considerada a taxa fuzzy de classificação.

Se, por exemplo, essa taxa for de 40%, considerar-se-á como pertinente a uma determinada classe, todo e qualquer elemento que tiver pelo menos 40% apenas de características dessa classe.

Uma taxa fuzzy de 100% implica em o elemento ter 100% das características da classe (classificação exata crisp).

Conjunto de Dados de Treino Acerto mín (%), total de amostras do conjunto de dados de treino que devem ser classificadas corretamente, interrompendo o treinamento e iniciando a classificação da base de teste Obtido (%).

Quantidade de amostras do conjunto de dados de treino que foram classificadas corretamente após n Épocas,%Acerto (Conjunto de Dados de Teste), quantidade de amostras do conjunto de dados de teste que foram classificadas corretamente, utilizando os dados obtidos no treinamento,Tempo (s), tempo total em segundos, compreendendo desde o carregamento dos conjuntos de dados até o término da classificação do conjunto de dados de teste,nc, não conseguido.

A classificação não foi conseguida após várias épocas de tentativa,indet, indeterminado.

O tempo para se conseguir algum resultado positivo ultrapassou mais de 12 horas de processamento ininterrupto.

Resultados obtidos com o conjunto de dados Iris Data Flower com um processador.

Classificou-se corretamente 100% do conjunto de dados considerado (Iris Data Flower) com diversas configurações diferentes, demonstrando a boa aplicabilidade do método KNN-Fuzzy.

Listam-se, valores de K, de m e de pesos dos atributos (W1, W2, W3 e, W4 ) para algumas classificações iguais a 100% do conjunto de dados de teste Iris.

Valores de K, de m e dos pesos (Wi) para o conjunto Iris.

Os valores (valores de K associados a valores específicos de m e de pesos) maximizam a função de adequação, vista em ( 19 ), por meio da otimização desses parâmetros via algoritmo genético.

Observa-se, em alguns casos, a redução da dimensão pela ponderação igual a 0 (zero) do atributo W3 (comprimento de pétala).

Esta base de dados contém 768 amostras, divididas em duas classes que representam a presença ou não de diabetes mellitus em mulheres, Classe 0 (Não-diabéticos), 500 amostras (65%) Classe 1 (Diabéticos), 268 amostras (35%) As classes são definidas em termos de oito atributos numéricos e que trazem as informações, Número de gestações.

Taxa de glicose no plasma sanguíneo após 2 horas do teste de tolerância oral de glicose.

Pressão sanguínea diastólica.

Espessura (mm) da dobra da pele do tríceps.

Taxa de insulina no soro sanguíneo.

Índice de massa corporal.

Tendência a ter diabetes (função de pedigree).

Idade.

De forma a proceder a classificação dos dados, distribuiu-se as amostras por conjuntos de dados específicos (conjunto das amostras cuja classe é conhecida, conjunto das amostras cujas classes servirão de treino para o algoritmo e conjunto das amostras de teste).

Distribuição das amostras por conjuntos de dados.

Foram realizados 15 testes, nos quais vê-se claramente as vantagens de se utilizar a lógica fuzzy e o algoritmo genético, no método KNN de classificação de dados.

Resultados obtidos com a base Pima Indians Diabetes com um processador.

Pode-se observar que altas taxas de classificação (89,0% na classificação #1 com m = 0,001074 e 85,0% na classificação #12, com m = 0,004297) são obtidas com uma taxa de fuzzificação de 40%, o que significa dizer que as classes são muito próximas.

Os valores de K, de m e dos pesos dos atributos são obtidos via algoritmo genético, valores esses que maximizam a função de adequação ( 19 ).

Este conjunto de dados contém 1070 amostras divididas em dois arquivos, PRO2_CL2 lrn (arquivo com 300 amostras, contendo as classes de cada amostra, divididas em dois grupos iguais de compradores e não compradores) e PRO2_EVL dat (arquivo com 770 amostras, não contém as classes das amostras).

As classes das amostras do arquivo PRO2_EVL at estão contidas no arquivo RESULTpr2.

As classes (duas) definem perfis de compradores, obtidos durante campanha de marketing, identificados por meio de vinte e sete atributos (alguns deles codificados para proteger a identificação do consumidor), onze atributos contendo dados pessoais (tais como renda familiar, número de carros, idade, etc) e dezesseis atributos contendo informações de consumo (valor 0 zero para não consumidor e valor 1 um para consumidor de determinado produto).

De forma a proceder a classificação dos dados, distribuiu-se as amostras por conjuntos de dados específicos (conjunto das amostras cuja classe é conhecida, conjunto das amostras cujas classes servirão de treino para o algoritmo e conjunto das amostras de teste).

Distribuição das amostras por conjuntos de dados.

Foram realizados então 10 testes.

Resultados obtidos com a base Erudit 1998 com um processador.

Nesta classificação procurou-se trabalhar com três faixas de fuzzificação (40%, 45% e 50%), anotando-se o valor de K para conhecer a evolução do fator "tempo/classificação" sobre a quantidade de vizinhos utilizados.

Pode-se observar que a separação das amostras por classe é plenamente atingido com algumas variações na configuração, como mostram as classificações #05 (m = 0,001074), #08(m = 0,554292) e #09(m = 0,002148).

Os valores de K, de m e dos pesos dos atributos são obtidos via algoritmo genético, valores esses que maximizam a função de adequação ( 19 ).

Para avaliar o desempenho do método KNN-Fuzzy, comparou-se alguns resultados obtidos neste trabalho com os resultados apresentados em CARVALHO (que se vale de algoritmo genético para otimizar estratégias de classificação de dados, compostas de ávores de decisão CN2, C45 e C50) com o conjunto de dados Iris.

Características das bases de dados Iris, Balance e Bupa.

Os resultados obtidos com várias bases de dados corroboram com a tese de que uma classificação nebulosa (fuzzy) pode minimizar o tempo de uso da CPU, encontrando, com o auxílio do algoritmo genético, em tempo viável de processamento, os parâmetros necessários para uma boa/ótima taxa de classificação.

Tempo despendido na classificação por diferentes metodologias.

Pode-se notar que os resultados obtidos pela estratégia proposta resultam em melhor precisão e desempenho.

Foi efetuada também uma comparação do ganho obtido pelo processo de otimização utilizando-se os conjuntos de dados iris e Erudit e a base Pima.

Comparação entre os resultados obtidos na classificação por KNN-Fuzzy sem otimização e com otimização por Algoritmo Genético.

Verifica-se o enorme ganho em precisão na classificação correta dos dados, resultante da obtenção dos valores de K associados a valores específicos de m, em função da otimização desses parâmetros por algoritmo genético.

Os valores de %Fuzzy/%Acerto da Base de Treino podem ser configurados sem perda de classificação para valores mais baixos, o que facilita sobremaneira a classificação da base de teste.

Isto é explicado pela teoria desenvolvida em cima da esfericidade do espaço de delimitação.

Quanto maiores esses valores, mais esférico será esse espaço, e mais trabalho terá o algoritmo genético para alterar os pesos das variáveis envolvidas, de tal sorte que as hiperelipses consigam abordar a maior quantidade possível de dados.

Valores de %Fuzzy/%Acerto da Base de Treino que têm-se mostrado ótimos (com taxas de classificação excelente, dentro de tempos razoavelmente baixos) são, %Fuzzy, entre 40% e 50%.

Acerto da Base de Treino, entre 50 % e 65%.

Uma melhor representação da abordagem nebulosa (fuzzy), em vez de hiperelipses, seria o de se representar elementos deformados no hiperespaço, como nuvens de probabilidade, abordagem parecida com a densidade de probabilidade estudada na mecânica quântica, onde um elétron tem certa probabilidade de estar numa determinada órbita no instante de tempo observado.

O dado teria certa probabilidade de pertinência à classe especificada, de acordo com a abordagem de classificação considerada.

Quanto à população de cromossomos, quanto maior essa quantidade, verifica-se maior facilidade de se "cair" em um mínimo local, dificultando o descobrimento dos parâmetros.

Um valor próximo de 10 (e não maior do que 50) mostrou-se, para o universo pesquisado, a quantidade ideal para o tamanho da população.

A utilização de Algoritmo Genético mostrou-se útil na ponderação das variáveis, não só por evitar a sua necessidade de normalização, como também por diminuir e até mesmo eliminar a correlação entre os dados, fornecendo pesos próximos ou iguais a zero para algumas variáveis das bases de dados utilizadas.

Os primeiros treinos e testes foram realizados com o conjunto de dados Iris pela sua extrema simplicidade, em função de suas classes estarem bem separadas no hiperespaço (pode-se obter, com certa facilidade, uma separação próxima de 100% entre as três classes presentes).

Como se pode observar pelos resultados obtidos, mesmo para um conjunto de dados simples como esse, a questão de tempo de processamento continua sendo um impecilho para uma separação crisp efetiva.
A convergência do algoritmo genético presente demanda tempos de CPU acima de 15 min ao passo que com a aplicação da lógica nebulosa à classificação dos dados, resulta num aumento de desempenho estupendo, pois o tempo de CPU baixa para um valor entre 3 s e 56 s, sendo que esses tempos relacionam-se a classificações próximas de 100% de acerto da base de teste.

As variações encontradas para esses tempos, devem-se ao fato de termos vários mínimos locais e, dependendo do cromossomo inicial, que é construído de maneira aleatória (tentando cobrir o mais que possível o espaço de busca), podem ser necessárias várias etapas de crossover e mutação dos cromossomos da população, de modo que se migre para o mínimo global, representado pelo cromossomo de melhor classificação da base de treino.

Pelo mesmo motivo, várias execuções utilizando-se os mesmos parâmetros de classificação podem ser necessárias, pois com todo o esforço do algoritmo genético em convergir para o mínimo global, tendo caído em um mínimo local, pode haver uma elevada demanda de tempo de CPU, descaracterizando a vantagem de se utilizar a lógica nebulosa (fuzzy) no processo.

Neste caso, deve-se dispensar o treino e fazer uma nova execução, de tal sorte que uma nova população de cromossomos seja gerada aleatoriamente.

Como foi possível classificar a base de dados de teste em 100% através do cromossomo obtido com parâmetros nebulosos (fuzzy) mínimos sobre a base de treino, em tempos bem inferiores aos relacionados para parâmetros nebulosos (fuzzy) rígidos (próximos de uma classificação exata, ou crisp).
Tentou-ser provar que o método aqui empregado (classificação da base de dados com parâmetros nebulosos (fuzzy) mais flexíveis) aplica-se para bases de dados diversas, com tempos de classificação relativamente pequenos em relação à mesma classificação exata (crisp), que em algumas situações pode até inviabilizar a aplicação do algoritmo genético.

Já para a base PIMA, como demonstram os resultados obtidos, a classificação de 61% da base de dados (utilizando parâmetros medianamente rígidos de classificação) foi conseguida com tempo de CPU excessivo, levando a crer que a população de cromossomos estivesse presa em um mínimo local.

Observa-se que os parâmetros exigidos na classificação da base de treino (%Fuzzy = 505% e %Acerto = 95%) não foram obtidos após 1800 iterações.

Comprovando que gerar aleatoriamente uma nova população é mais vantajoso do que continuar a classificação partindo da população antiga (que pode estar presa a um ou mais mínimos locais), experimentou-se uma nova iteração a partir da população acima e que consumiu um tempo exagerado de execução do programa de classificação, obtendo-se a taxa de 57% (uma baixa de 4% em relação à classificação inicial), o que representou um dispêndio excessivo, desnecessário e improdutivo de tempo de CPU.

Alterando-se os parâmetros de classificação da base de treino (%Fuzzy e %Acerto) para os valores abaixo, obteve-se as taxas de classificação em tempos de CPU respectivos, O que significa dizer que, tomando-se como meta a classificação total da base de treino (%Acerto = 100%), tem-se que um aumento na taxa de fuzzificação de 30% para 40%, ocasiona um aumento de aproximadamente 14 horas no processo de classificação.

Além desses resultados, fez-se várias iterações utilizando-se como parâmetros de classificação, %Fuzzy = 51% e %Acerto = 60%, obtendo-se classificações em taxas de até 89% da base de teste, salientando ser, esta base de dados, de difícil classificação, principalmente quanto à classe 1 (diabéticos), que tem seus elementos muito próximos à fronteira da classe 0 (não-diabéticos).

Observa-se nos resultados obtidos, que a classe 0 obteve classificação de até 100%, ao contrário da classe 1 cuja classificação máxima alcançou somente 71,4%.

Os tempos de classificação também são animadores, pois as classificações foram obtidas com tempos que variaram entre 79 s e 82 s.

Como se pode comprovar com o conjunto de dados ERUDIT, foram obtidas taxas de acertos acima de 94,4% da base de dados de teste, conforme relatado na classificação #1, onde temos, para um fator fuzzy de 45%.
Bem como, uma classificação 100% correta da base de dados de teste, de acordo com o relatado na classificação #8, para o mesmo fator fuzzy de 45%.
Mais uma vez, explica-se que os diferentes tempos apresentados, referem-se à quantidade de evoluções genéticas que a população de cromossomos foi obrigada a fazer para sair dos mínimos locais, migrando, assim, para o mínimo global, representado pelo cromossomo de melhor classificação da base de treino.

Também se observa que o valor de K para altas taxas de classificação correta situa-se entre 30% e 100% da base conhecida.

Como esse valor também é dependente dos pesos das variáveis, o interessante é encontrar os pesos que minimizem esse valor de K, o que faz com que o tempo de processamento seja também minimizado.

A seguir são sumarizadas as configurações utilizadas na comprovação prática do método proposto neste trabalho.

Especificação do sistema de processamento paralelo PC-Cluster do Laboratório de Computação de Alta Performance do Núcleo Avançado de Computação de Alto Desempenho ( NACAD ) da COPPE/UFRJ, Especificação do sistema de processamento paralelo IBM RS/6000 SP localizado no NACAD/COPPE/UFRJ, A evolução dos sistemas de gerenciamento de dados levou a um aumento exponencial da quantidade de dados armazenados.

E essa grande quantidade, por sua vez, demanda cada vez mais formas eficazes de busca e classificação.

E não só a busca de dados formatados (tal como a listagem de nomes de clientes cuja idade seja superior a 40 anos), mas a busca de novos padrões, tem sido um tema importante em aprendizado de máquina.

Para tal, utilizam-se técnicas de Data Mining, importante ferramenta com aplicações em medicina, economia etc.

A utilização de modelos baseados na distância entre os dados e os centros das classes existentes, apesar de sofrer críticas acerca de seu desempenho (pelo esforço computacional envolvido no cálculo das distâncias) merece considerações a favor devido à sua simplicidade, tornando o modelo extremamente fácil de implementar.

Além disso, a eficiência computacional pode ser melhorada pelo conhecimento do domínio do problema (pela "limpeza" prévia dos dados tal como eliminar dados considerados desnecessários) e pelo emprego de processamento paralelo e distribuído.

Já a eficiência de classificação fica a cargo de técnicas de apoio ao modelo, tais como o emprego de lógica fuzzy (o que implica em aumento da taxa de classificação pela "flexibilização" do modelo exato crisp de "pertencer" ou "não pertencer") e de algoritmo genético, que permite uma varredura de parâmetros dentro de espaços de busca de grande complexidade, de forma bem simples.

O método KNN tem como grande mérito ser extremamente simples e fácil de implementar, possuindo porém grande capacidade de classificação.

Falta contudo ao método KNN, a facilidade do ajuste do parâmetro K, de tal sorte que esse valor seja mínimo, o que influi diretamente na taxa de classificação e no tempo de execução para o caso de bases de dados muito grandes.

O modelo aqui empregado, método KNN aliado à Lógica Fuzzy (KNN-Fuzzy), mostrou-se satisfatório para todos os exemplos empregados, tendo sido fortemente otimizado pelas técnicas descritas anteriormente (algoritmo genético e processamento paralelo e distribuído de dados), o que pode ser descrito como a grande contribuição deste trabalho.

O método KNN-Fuzzy, otimizado por algoritmo genético, tem a habilidade de produzir um modelo muito preciso de classificação comparando-se com outros paradigmas utilizados em problemas de classificação, resolvendo-se o problema descrito acima para o método sem a otimização.

O grande volume de dados existentes no mundo real com complexas relações entre si, como é o caso da base de dados Insur, somente podem ser manipulados por métodos implementados em arquiteturas de alto desempenho com uma metodologia paralela eficiente.

O uso de primitivas de uma biblioteca padrão (MPI) permite a portabilidade da aplicação entre diversos ambientes computacionais, fornecendo grandes vantagens para a metodologia.

O desempenho obtido pela implementação paralela mostrou excelentes resultados.

A principal limitação apresentada mostrou ser o tempo computacional utilizado na obtenção das distâncias, cerne do método aqui apresentado.

De forma a contribuir com trabalhos que, no futuro, venham a se guiar pelo trabalho ora apresentado, sugere-se os itens abaixo, A necessidade de se otimizar o processo de cálculo e comparação das distâncias entre amostras no intuito de se diminuir o tempo de processamento de grandes massas de dados.

Estudo de sgbds em arquiteturas paralelas/distribuídas, com a criação de uma interface entre o aplicativo e o sgbd correspondente.

Estudo de novas formas de repartição dos dados envolvendo a base de dados das amostras classificadas, apesar dos resultados experimentais obtidos neste trabalho comprovarem que o grupamento desses dados (clustering) alcança, pela complexidade em se implementar um algoritmo de quicksort paralelo, resultados com desempenho mais satisfatório, apesar de sua fragmentação apresentar desempenho melhor, o que pode ser aprofundado com um estudo em MATTOSO.

