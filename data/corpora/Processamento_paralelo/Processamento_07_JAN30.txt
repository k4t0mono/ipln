Pesquisas em sistemas paralelos e distribuídos de alto desempenho apresentam limitações no que se refere a análise, projeto, implementação e execução automática e transparente de aplicações.

Essas limitações motivaram o projeto do MidHPC (do inglês Middleware for High Performance Computing, ou seja, Middleware para Computação de Alto Desempenho), que balanceia transparente e automaticamente cargas de trabalho considerando a capacidade dos recursos computacionais e o comportamento das aplicações envolvendo, processamento, acesso a disco, memória e rede.

Para utilizar todo o potencial do MidHPC, aplicações devem ser escritas utilizando o modelo de programação concorrente, tal como o padrão POSIX de threads (pthreads).

Aplicações desenvolvidas seguindo esse modelo de programação podem ser executadas em ambientes de Grid sem alteração de código fonte ou recompilação.

Durante a execução, tarefas de uma mesma aplicação paralela comunicam-se, transparentemente, por meio de um sistema de memória compartilhada distribuída.

O objetivo deste trabalho foi desenvolver alguns dos módulos do projeto MidHPC e integrar demais ferramentas que haviam sido previamente desenvolvidas pelo grupo.

Este trabalho permite aplicar, em ambientes reais, todos os conceitos de escalonamento de processos estudados e desenvolvidos durante o projeto MidHPC.

A disponibilidade de microprocessadores de baixo custo e a evolução das redes de computadores tornaram economicamente viável o desenvolvimento de sistemas distribuídos.

Em tais sistemas, processos comunicam-se a fim de realizar a mesma tarefa computacional.

Além de diminuir o custo, esses sistemas permitem a construção de ambientes escaláveis e mais flexíveis que máquinas paralelas.

Com o advento dos sistemas distribuídos, pesquisas foram desenvolvidas com o objetivo de, maximizar o uso dos recursos computacionais, obter alto desempenho, desenvolver ambientes e bibliotecas de comunicação para simplificar o projeto e implementação de sistemas distribuídos.

Posteriormente, os conceitos de sistemas distribuídos evoluíram partindo de ambientes construídos em redes locais para ambientes internet1.

Nesses ambientes, redes interagem entre si através de roteadores.

Assim, surgem novas necessidades de pesquisas para o desenvolvimento de técnicas de alocação de recursos e obtenção de alto desempenho em ambientes de larga escala.

Esses ambientes, localizados em regiões geograficamente distintas e compostos por recursos computacionais de capacidade heterogênea, são denominados Grids computacionais.

Dada a complexidade desses ambientes, oriunda da heterogeneidade e da distância entre os recursos, políticas de escalonamento de tarefas tiveram que ser adaptadas.

Escalonadores, anteriormente utilizados em ambientes homogêneos, apresentam um desempenho inferior quando utilizados em Grids computacionais.

Para melhorar o desempenho desses escalonadores e, conseqüentemente, de aplicações paralelas, diversas pesquisas foram conduzidas nas áreas de balanceamento de carga, imagem única de sistema, protocolos de baixa latência, bibliotecas de comunicação e arquiteturas de hardware de alto desempenho.

Entretanto, os resultados dessas pesquisas apresentavam limitações em relação ao desenvolvimento e execução de aplicações, pois, em geral, exigiam que o usuário tivesse conhecimento sobre características específicas de ambientes, bibliotecas e técnicas de distribuição e balanceamento de carga.

O orientador deste trabalho, juntamente com demais colaboradores, procurando solucionar esse impasse, passou a desenvolver pesquisas com o objetivo de criar um ambiente transparente e distribuído para a execução de aplicações paralelas.

Inicialmente, o foco dessas pesquisas estava em clusters de computadores e, posteriormente, foi modificado para ambientes de Grid computacional.

Os primeiros trabalhos realizados abordavam a avaliação de desempenho de protocolos de comunicação, utilizando modelos que exploravam as características de diferentes ambientes e bibliotecas.

Os estudos realizados permitiram a extração de dados sobre a latência e overhead de pacotes utilizando diferentes protocolos e redes, os quais serviram de base para compreender o ambiente no qual as aplicações paralelas são executadas.

Os resultados de medições de rede e a possibilidade de compreender a distribuição de cargas de trabalho (processamento) motivaram pesquisas com o objetivo de projetar um modelo de migração de processos.

Esse modelo, baseado nos trabalhos de Harchol-Balter & Downey, possibilitou avaliar o tempo de execução de processos, como projetado pelos outros autores, como as cargas de trabalho sobressalentes.

Com a viabilização da migração de processos, novas pesquisas foram realizadas sobre escalonamento, resultando no desenvolvimento de algoritmos que adotam técnicas como, localidade de computadores para a distribuição de processos em Grids computacionais, algoritmos genéticos, otimização baseada em colônias de formigas, conhecimento sobre a ocupação de recursos para a distribuição de processos, dentre outras pesquisas.

As pesquisas sobre escalonamento de processos motivaram a realização de estudos sobre o comportamento de aplicações paralelas e distribuídas, uma vez que o uso eficiente de um ambiente distribuído depende do conhecimento de características específicas de cada aplicação, tais como o uso de recursos de processamento, memória, acesso a disco e rede.

A integração dessas pesquisas motivou o desenvolvimento do projeto MidHPC (Mid-dleware for High Performance Computing) que provê um middleware para a execução de aplicações paralelas e distribuídas desenvolvidas segundo o paradigma concorrente.

A principal contribuição do MidHPC é oriunda do fato de que projetistas de software possam utilizar apenas o paradigma concorrente, usando threads, para o desenvolvimento de aplicações que são, automática e transparentemente, paralelizadas e distribuídas.

O MidHPC oferece suporte a essas aplicações (multithreads) ao interceptar a criação de threads para transformá-las em processos, de modo a permitir o balanceamento de carga transparente.

A comunicação entre esses processos é realizada por meio de um suporte de DSM, que é transparente ao usuário.

A utilização de módulos de inteligência artificial para conduzir o balanceamento de carga é um diferencial do MidHPC em relação a outras implementações de Grids.

Esses módulos consideram otimizações em comunicação e em tempo de resposta de aplicações.

O modelo de programação do MidHPC torna a complexidade de escalonamento, de transferência de mensagens e de gerenciamento de ambiente transparentes ao desenvolvedor.

Assim, o projetista de software precisa apenas levar em consideração os problemas relativos à aplicação em desenvolvimento, como gerenciamento de condições de corrida.

O presente trabalho tem como objetivo o desenvolvimento de alguns módulos e a integração de outras ferramentas que compõem o projeto do MidHPC (Middleware for High Performance Computing)2.

O MidHPC permite a execução de aplicações paralelas e distribuídas que seguem o paradigma concorrente em ambientes distribuídos de larga escala, tais como Grids computacionais.

Visando esse fim, foram realizadas as seguintes tarefas, ressaltando que o termo Desenvolvimento refere-se aos módulos projetados e implementados neste mestrado e o termo Utilização refere-se à adoção de módulos previamente projetados e implementados pelo grupo, Desenvolvimento3 o módulo Broker para gerenciar, de maneira escalável, os computadores que participam do ambiente de Grid.

O Broker é dividido em dois sub-módulos, um Local para controle dos Schedulers e um Global, para gerenciar a interação entre redes distintas.

Desenvolvimento do módulo Scheduler que permite acoplar um algoritmo de balanceamento de carga responsável pela tomada de decisões.

Desenvolvimento do módulo Shell, baseado no GNU/Bash, que implementa a interface com usuários.

Esse Shell provê ferramentas para início e término de aplicações, além de funções para verificar a execução de processos e a carga do sistema.

Utilização de uma técnica para permitir o uso do paradigma concorrente por meio da conversão, transparente, de threads para processos, o que permite a transferência de carga.

Utilização de um Monitor que extrai características da aplicação e as armazena em uma base de dados (knowledge base).

Tais informações são posteriormente utilizadas pelo Scheduler, através da técnica de aprendizado baseado em instâncias (IBL).

 Utilização de um suporte de memória compartilhada distribuída (DSM, Distributed Shared Memory) que visa permitir a comunição entre processos.

As questões relativas a segurança, alta disponibilidade e replicação não são abordadas no contexto deste trabalho.

Este trabalho permite aplicar, em abientes reais, todos os conceitos de escalonamento de processos estudados e desenvolvidos durante o projeto MidHPC.

O projeto MidHPC tem como principal função o escalonamento de processos, com o objetivo de balanceamento de carga, em ambientes distribuídos heterogêneos de larga escala.

Assim, fez-se necessário um estudo sobre a Taxonomia de Algoritmos de Escalonamentos de Processos proposta por Casavant & Kuhl de modo a fornecer uma visão geral sobre os algoritmos de escalonamento de processos e permitindo compreender maiores detalhes sobre o funcionamento do MidHPC.

Casavant & Kuhl propuseram uma taxonomia para enumerar e definir termos utilizados para qualificar algoritmos de escalonamento de processos de acordo com a orgazização e utilização das informações para o escalonamento.

Essa taxonomia foi dividida em dois aspectos apresentados nas seções seguintes, hierárquico e plano.

No aspecto hierárquico, os autores propuseram uma estrutura de árvore para classificação das técnicas de escalonamento de processos.

Essa classificação é apresentada a seguir.

Classificação Hierárquica de Escalonamento de Processos.

A primeira divisão na hierarquia proposta contém dois ramos, Local e Global.

Esse nível subdivide algoritmos de escalonamento segundo a localização dos processos.

O Local refere-se ao escalonamento em máquinas com apenas um processador e o Global refere-se ao escalonamento em sistemas multiprocessados.

Para máquinas com apenas um processador, o escalonamento é feito pelo sistema operacional, assim, não faz sentido discorrer sobre esse ramo, pois, este trabalho é voltado para o escalonamento de processos em sistemas distribuídos.

Os algoritmos de escalonamento classificados como Globais são subdivididos em Estáticos e Dinâmicos.

Essa subdivisão indica o momento em que as atribuições ou escalonamentos são realizados.

No caso do escalonamento Estático as decisões são tomadas na fase de projeto.

No Dinâmico, as decisões de escalonamento são tomadas durante a execução dos processos.

Os algoritmos Estáticos são classificados em Ótimos e Subótimos.

O Ótimo requer que todas as informações necessárias para escalonamento estejam disponíveis.

Sobre tais informações são aplicadas funções e seus resultados definem a alocação dos processos.

Quando um problema não é computacionalmente factível, soluções Subótimas devem ser utilizadas.

Dentre as soluções Subótimas, pode-se adotar duas abordagens, Aproximação e Heurística.

A diferença entre as técnicas estáticas subótimas de Aproximação e Heurística é que a primeira, em vez de procurar pela melhor resposta em todo espaço de soluções do problema, busca uma boa solução.

Essa, é obtida adotando uma métrica ou função de avaliação que analisa parâmetros e restrições do problema de escalonamento.

A segunda, técnica Heurística, é baseada em análises sobre o conhecimento a priori de informações do sistema e do comportamento da aplicação.

Casavant & Kuhl unificam os conceitos das técnicas Globais, Estáticas Ótimas e Subótimas Aproximadas para classificar quatro técnicas, 1-Busca e enumeração do espaço de solução (Shen & Tsai apud Casavant & Kuhl ).

Teoria dos grafos.

Programação matemática (Bokhari apud Casavant & Kuhl ).

Teoria de filas (Kleinrock apud Casavant & Kuhl).

Nesse ramo da classificação Global, encontram-se algoritmos de escalonamento que tomam decisões durante a execução dos processos.

Esses algoritmos são descritos e comparados de acordo com o nível que assumem na árvore.

Os algoritmos Dinâmicos são subdivididos em Fisicamente Distribuídos e Não-distribuídos.

Os Não-distribuídos são representados por algoritmos onde a responsabilidade de escalonamento é de um único processador.

Nos Distribuídos, a responsabilidade é partilhada entre processadores.

Os algoritmos Fisicamente Distribuídos são subdivididos em Cooperativos e Não-cooperativos.

Nesses algoritmos, cada processador executa um módulo de software que captura suas informações locais de carga e capacidade de processamento, as quais são utilizadas para tomada de decisões.

Nos Cooperativos, o módulo de software de um processador utiliza informações dos demais módulos que executam em outros processadores e, com base nessas informações, são tomadas decisões de alocação de processos.

Nos Não-cooperativos, os processadores consideram somente as informações locais para a tomada de decisões.

Nesse ponto da taxonomia, soluções ótimas, subótimas aproximadas e subótimas por heurísticas podem ser utilizadas.

A discussão apresentada no ramo estático aplica-se, também, nesse caso.

Segundo Casavant & Kuhl, alguns tópicos não podem ser acoplados à estrutura hierárquica, pois dificultam a compreensão e, por esse motivo, foram enumerados em uma classificação auxiliar, denominada Plana.

Os termos contidos nessa classificação são apresentados e definidos a seguir.
Adaptativo versus Não-adaptativo, Algoritmos de escalonamento de processos Adaptativos são aqueles que utilizam informações do sistema para modificar suas heurísticas na tomada de decisões.

Os Não-adaptativos desconsideram mudanças no sistema.

Atribuição Inicial versus Reatribuição Dinâmica, Na Atribuição Inicial, uma vez que um processo foi alocado a um processador ele não é mais transferido.

Na Reatribuição Dinâmica (ou migração), um processo pode ser transferido para outro processador durante sua execução.

Balanceamento de Carga, Algoritmos de escalonamento de processos que visam Balanceamento de Carga distribuem, eqüitativamente, a carga de processos no sistema.

Através dessa técnica, minimiza-se o tempo de resposta dos processos e a variância na carga dos processadores.

O balanceamento de carga utiliza-se da técnica de Reatribuição Dinâmica.

Compartilhamento de Carga, Algoritmos de escalonamento de processos que visam Compartilhamento de Carga buscam atribuir processos a todos elementos de processamento do sistema.

Probabilístico, Esse termo refere-se às políticas de escalonamento que fazem escolhas probabilísticas para alocação de processos, de acordo com funções de distribuição de probabilidades.

Segundo o aspecto plano da taxonomia apresentada por Casavant & Kuhl os algoritmos com suporte a balanceamento de carga distribuem, eqüitativamente, a carga de trabalho entre elementos de processamento (EPs) do sistema, diminuindo os tempos de resposta dos processos.

Apresenta maiores detalhes sobre os principais componentes de um algoritmo de balanceamento de carga definidos por Shivaratri.

Tais detalhes devem auxiliar na melhor compreensão das operações realizadas pelos algoritmos de balanceamento de carga e, conseqüentemente, do projeto MidHPC que adota tais conceitos.

São eles, Política de transferência, determina se um EP está em um estado de carga que permita participar de uma operação de transferência de processos, seja como emissor ou receptor.

Os receptores são aqueles cuja carga de trabalho é baixa, isto é, estão ociosos, já os emissores ou servidores, são aqueles que se encontram sobrecarregados.

Política de seleção, é responsável por decidir qual processo será transferido para outro EP do ambiente.

Se não existirem processos a serem transferidos, o elemento não é mais considerado um emissor, mesmo que esteja sobrecarregado.

Política de localização, tem como função encontrar o melhor emissor de processos para um determinado receptor, ou vice-versa.

O melhor emissor é aquele que está mais sobrecarregado e o melhor receptor, o que está mais ocioso.

Política de informação, sua função é decidir sobre a periodicidade e captura de informações sobre o estado de carga dos EPs.

Tais informações são utilizadas para a tomada de decisões de balanceamento.

Esses dados são utilizados para a obtenção de um índice de carga que identifica a ocupação de cada EP.

Esse índice é utilizado pela política de localização para definir emissores e receptores.

Bons algoritmos de balanceamento de carga utilizam poucos recursos computacionais do ambiente e conseguem alocá-los de forma otimizada, aumentando o desempenho das aplicações.

Este capítulo apresentou conceitos sobre escalonamento de processos e sua classificação hierárquica e plana segundo Casavant & Kuhl.

Após identificar o termo balanceamento de carga nessa taxonomia, foram apresentados conceitos adotados pelos algoritmos de balanceamento de carga.

Esses conceitos são importantes, uma vez que a principal atividade do MidHPC é o escalonamento de processos em ambientes distribuídos.

Segundo a taxonomia apresentada, os dois algoritmos de escalonamento atualmente suportados pelo MidHPC são classificados como, Global, Dinâmico, Fisicamente Distribuído, Cooperativo, Sub-ótimo por Heurísticas, contemplando, também, o Balanceamento de Carga.

Neste capítulo são abordados conceitos e sistemas voltados para a implementação de Grids computacionais.

Inicialmente são apresentadas motivações para o desenvolvimento de Grids computacionais, em seguida, definições de Grids, infra-estrutura, classificação e a diferença entre Grids e outros domínios.

Por fim, são apresentadas algumas implementações de Grids.

Computadores são utilizados para modelar e simular problemas complexos de ciência e engenharia, diagnosticar condições médicas, controlar equipamentos industriais, prever o tempo, gerenciar portfólios de ações e vários outros propósitos.

Esses problemas necessitam de processamento de alto desempenho.

Uma das primeiras abordagens para solucionar tal questão foi por meio da utilização de supercomputadores paralelos.

Com o avanço da tecnologia e a conseqüente melhoria dos computadores pessoais, surgiu a idéia de utilizá-los para realizar as tarefas antes feitas pelos supercomputadores.

Inicialmente, os computadores compartilhavam seus recursos de processamento em uma rede local e, em geral, apresentavam a mesma configuração (ambiente homogêneo).

Esses agrupamentos são também denominados clusters.

Com a melhoria das conexões, os agrupamentos começaram a ser interligados com o intuito de reduzir custos, dando origem à infra-estrutura denominada Grid Computacional.

Stevens definiram o termo Grid em analogia às redes de energia elétrica (Eletric Power Grids).

Assim como a rede de energia elétrica, os Grids computacionais permitem um acesso de baixo custo a recursos compartilhados.

Bote-Lorenzo definem um Grid computacional como uma infra-estrutura de hardware e software distribuída geograficamente, em larga escala, composta por recursos heterogêneos interligados e compartilhados.

Esses recursos pertencem a várias organizações administrativas e são coordenados para prover um suporte computacional transparente, confiável e persistente para uma grande variedade de aplicações.

As aplicações podem realizar computação distribuída, computação de alta vazão (high-throughput), computação sob demanda, computação colaborativa ou computação multimídia.

À partir dessa definição foram enumeradas diversas características, as quais foram estudadas por Stockinger, Larga escala, é necessário que Grids suportem grandes quantidades de recursos, levando em consideração a manutenção de desempenho, à medida que essa quantidade aumenta.

Distribuição geográfica, recursos podem estar distribuídos em locais geograficamente distantes.

Heterogeneidade, recursos são os mais variados possíveis, tanto em termos de software quanto de hardware.

Controle descentralizado, recursos do Grid são gerenciados por diversas organizações.

Compartilhamento de recursos, apesar de pertencerem a organizações distintas, os recursos são compartilhados entre todos os participantes.

Transparência, virtualmente, Grids são vistos como um único computador, que representa a soma da capacidade de todos os recursos.

Consistência, Grids devem ser construídos sobre padrões existentes, sejam de hardware ou software, de modo a facilitar a interconexão dos recursos.

Persistência, Grids devem se adaptar ao ambiente dinâmico e manter o acesso aos recursos.

Segurança, acesso seguro aos recursos é uma característica importante, visto que o controle dos recursos é de responsabilidade de organizações distintas.

Suporte a aplicação, Grids têm suporte a diversos tipos de aplicações, porém, alguns são voltados para tarefas específicas.

Modelo computacional, em geral, Grids têm suporte a diversos modelos de execução (batch, interativo, paralelo e distribuído etc).

Modelo de licença, devido à sua origem acadêmica, existe uma tendência de usar softwares de código aberto.

Para prover as características enumeradas na seção 32, uma infra-estrutura de Grid computacional deve conter um conjunto de técnicas, relacionadas a seguir.
Modelagem de recursos, descreve os recursos disponíveis, suas capacidades e as suas relações, de modo a facilitar a descoberta e o gerenciamento.

Monitoração e notificação, permite identificar os estados de um recurso e notificar a aplicação (ou a infraestrutura) sobre a alteração desses estados.

Alocação, identifica recursos que possam ser utilizados para executar a aplicação.

Provisionamento, gerenciamento de ciclo de vida e liberação, permite que um recurso seja configurado automaticamente para ser usado por uma aplicação, gerenciando sua utilização durante execução e liberando o recurso ao término da aplicação.

Relatório e auditoria, gerencia o uso dos recursos compartilhados pelos usuários.

Além disso, um Grid disponibiliza aos usuários um ponto único de entrada para executar aplicações.

Grids computacionais podem ser classificados de acordo com a relação entre as instituições que os mantêm.

Assim, são divididos em colaborativos e enterprise.

Como essa classificação é baseada nas relações entre as instituições que mantém o Grid, uma mesma implementação pode ser classificada em quaisquer das classes.

Por exemplo, uma determinada implementação ao ser utilizada em conjunto por vários departamentos de uma mesma instituição é classificada como enterprise.

Porém, essa mesma implementação pode ser classificada como colaborativa se um conjunto de instituições diferentes (empresas, universidades etc) utilizá-la.

Vale ressaltar que alguns autores classificam um cluster, que tem como objetivo alto desempenho/vazão computacional, como um Grid.

Porém, clusters tendem a ser estáticos, homogêneos e gerenciados por um único domínio.

Assim, clusters não deveriam ser denominados de Grids, a não ser que estejam conectados a outros clusters.

Segundo Stockinger, ao final da década de 1990, Grids computacionais surgiram como um novo domínio na ciência da computação, embora diversas técnicas e protocolos tenham sido herdados de outros domínios tais como a computação distribuída.

Assim, Grids computacionais não podem ser discutidos isoladamente, pois essa área da computação apresenta diversas intersecções com outras.

Devido à sobreposição de diversas características é difícil diferenciar o domínio de Grids computacionais da computação distribuída.

Para Stockinger, alguns pesquisadores consideram o Grid como um caso especial de computação distribuída.

A escalabilidade (possibilidade de aumentar o número de recursos computacionais) e a transparência (independência de plataforma e capacidade de usar recursos heterogêneos de hardware e software) são características particulares que evidenciam sua complexidade.

Entretanto, alguns autores afirmam que não existe uma diferença real entre Grids computacionais e a computação distribuída, pois eles são complementares e um parte do outro.

Um conjunto de caractéricas de Clusters, Grids e sistemas P2 P é apresentado.

Os principais fatores para diferenciar um cluster de Grids computacionais e sistemas P2 P vêm do fato dele apresentar gerenciamento centralizado, posse única dos recusos computacionais e arquiteturas homogêneas.

Diferenciar sistemas P2 P de Grids torna-se cada vez mais complexo, pois o primeiro pode ser utilizado para a construção de diferentes aplicações, incluindo o processamento distribuído, característico de Grids.

O projeto SETI@Home, desenvolvido pela Universidade da Califórnia em Berkeley, tem o propósito de analisar sinais de rádio captados no observatório em Arecibo (Porto Rico), em busca de vida inteligente fora da Terra (Search for ExtraTerrestrial Intelligence).

Historicamente, é considerado como o primeiro Grid computacio-nal, porém, é voltado para uma aplicação específica, a análise de sinais.

Características de Clusters, Grids e sistemas P2 P.

A arquitetura desse sistema é simples, dados capturados por um rádio-telescópio em Arecibo são gravados em fita e enviados para Berkeley (EUA), onde são divididos em pequenas unidades de trabalho (work units), que correspondem a 10 KHz de largura de banda de 107 segundos de duração (aproximadamente 350 Kb).

Essas unidades são transferidas para um meio de armazenamento temporário e, posteriormente, distribuídas entre os usuários.

Os clientes SETI@home são executados em computadores pessoais espalhados pelo mundo.

Basicamente, eles requisitam uma unidade de trabalho, processam a mesma, enviam os resultados para o servidor central e, nesse momento, recebem outra unidade.

O protocolo de comunicação entre os clientes e o servidor é baseado em HTTP, de maneira a permitir que máquinas atrás de firewalls consigam contatar o servidor1.

Não há dependência entre as unidades de trabalho, o que implica em não existir comunicação entre clientes.

Além disso, a conexão à Internet é necessária apenas no momento de envio e recepção de dados, permitindo o processamento offline.

O cliente periodicamente salva o estado da computação de maneira a permitir o progresso, mesmo se o computador pessoal é freqüentemente ligado e desligado.

O projeto SETI@home tem um sistema de escalonamento de processos simplificado, enviando uma unidade de trabalho para os computadores requisitantes.

Isso é possível pois operações em uma unidade de trabalho são independentes.

Uma mesma unidade de trabalho é replicada, para diversos computadores, de modo a garantir que seja executada.

Um computador solicita uma outra unidade de trabalho somente após finalizar uma análise.

O Globus Toolkit, atualmente na versão 4, é um conjunto de ferramentas de software e módulos que implementam a arquitetura OGSA (Open Grid Service Architecture), definida pelo comitê The Globus Alliance.

Essa arquitetura apresenta, basicamente, as seguintes funcionalidades, 1-Localização e alocação de recursos, o mecanismo de localização de recursos é importante, pois a maioria das aplicações não conhece exatamente o local dos recursos requisitados, principalmente quando a carga e a disponibilidade dos recursos variam.

A alocação de recursos diz respeito ao escalonamento e envolve tarefas básicas, tais como, criação de processos, acesso a dados, etc.

O Globus Resource Allocation Manager (GRAM) fornece alocação de recursos, criação de processos, monitoração e gerenciamento de serviços.

O GRAM mapeia requisições escritas em uma linguagem de especificação de recursos (RSL Resource Specification Language) em comandos para escalonadores e computadores locais.

Interface de autenticação, fornece mecanismos básicos de autenticação, utilizados para identificar tanto usuários quanto recursos.

O Grid Security Infrastructure (GSI) oferece uma interface simples de autenticação, com suporte a controle local sobre direitos de acesso e mapeamento de identidades globais de usuários em identidades locais.

Comunicação, oferece mecanismos básicos para comunicação.

Deve permitir a implementação abrangente de vários métodos de comunicação, tais como, passagem de mensagens, chamada de procedimento remoto, memória compartilhada distribuída, multicast, etc.

No Globus, o módulo de comunicação é baseado na biblioteca de comunicação Nexus.

O Nexus define cinco abstrações básicas, nós, contexto, threads, canais de comunicação e requisições de serviço remoto.

As funções do Nexus que manipulam tais abstrações formam a interface de comunicação do Globus.

Essa interface é utilizada amplamente por outros módulos do Globus e também para construção de serviços de alto nível, incluindo ferramentas para programação paralela.

Serviço de informação de recursos unificado, permite, em tempo de execução, a coleta de informações a respeito da infra-estrutura do Grid e do seu estado.

Esse mecanismo permite a atualização, a coleta de informações e o controle de acesso.

Acesso a dados, oferece acesso remoto para armazenamento persistente.

O módulo de acesso a dados do Globus trata do problema de armazenamento de alto desempenho, assim que ocorre acesso a sistemas de arquivos paralelos e dispositivos de entrada e saída embutidos na rede, tal como o HPSS (High Performance Storage System).

O GridFTP (Grid File Transfer Protocol) implementa uma variedade de operações automáticas e programadas para movimentação e estratégias para acesso a dados, habilitando programas que executam em locais remotos para leitura e escrita de dados.

O Globus Toolkit também utiliza tecnologias de serviços Web, tais como, SOAP (Simple Object Access Protocol), WSDL (Web Service Definition Language) e WSIL (Web Services Inspection Language), que suportam gerenciamento de estado distribuído, inspeção, descoberta, invocação e notificações distribuídas.

Especificamente, a interface dos serviços do Grid é definida em formato WSDL.

Outros sistemas são baseados na arquitetura do Globus, tais como o ESG (Earth System Grid) que tem como propósito modelar e simular o clima terrestre.

O Progrid, desenvolvido pela Universidade Federal de São Carlos, é uma arquitetura para gerenciamento e operação de Grids baseada em servidores proxy.

No Progrid, o servidor proxy atua como um ponto de interconexão entre as redes que compõe o Grid.

Para incorporar a estrutura do Progrid a uma rede, não é necessário realizar modificações na infra-estrutura, pois os serviços são oferecidos em um ponto único localizado na borda da rede.

Nesse ambiente, a comunicação entre os computares é realizada pelo proxy através de uma conexão segura (utilizando SSL).

Para a execução de uma aplicação é utilizado uma versão MPI modificada com suporte a comunicação através de proxies.

Essa modificação é transparente ao usuário e à aplicação, que não necessita de alterações para executar ambiente.

É necessário que cada rede participante do ambiente tenha, pelo menos, um proxy, de modo que esse permita a comunicação entre as tarefas gerenciadas pelo MPI.

Esse proxy também é responsável por controlar e gerenciar as informações da rede em que está alocado.

O status geral do ambiente é obtido através da compilação dos dados dos proxies de todas as redes, o que diminui o overhead no controle de comunicação.

O propósito geral no NAREGI é desenvolver os fundamentos tecnológicos de Grids voltados para computação científica de larga escala.

Esse projeto é dividido em seis grupos de pesquisa e desenvolvimento (Work Package, WP).

Cada um dos WPs enfoca seus trabalhos em uma área e serão descritos, em linhas gerais, a seguir.
O WP-1 é responsável pelos middlewares, de camada mais baixa e intermediária, para o gerenciamento de recursos como superscheduler (voltado para a alocação de tarefas em recursos que possam atender seus requisitos), GridVM (para controle de recursos locais) e serviços de informação no Grid.

O WP-2 coordena o desenvolvimento de ferramentas básicas para programação no Grid, que consistem, principalmente, de, GridRPC, que possibilita a distribuição de aplicações em ambientes de larga escala, e GridMPI, que provê suporte a aplicações desenvolvidas em MPI.

O WP-3 trabalha com ferramentas de Grids para o usuário final, incluindo Grid workflows (ferramenta visual para preparar, submeter e consultar aplicações executando em recursos remotos), ambientes para resolução de problemas (PSE, Problem-Solving Environment, utilizado para incorporar as aplicações ao Grid) e ferramentas de visualização (específica para nanosimulações).

O WP-4 lida com a junção (packing) e o gerenciamento da configuração dos softwares do projeto, que provê um serviço de gerenciamento de informações sobre recursos, escalável e seguro, utilizado na execução de um ambiente de Grid computacional de larga escala e multidisciplinar.

O WP-5 investiga questões de rede, segurança e gerenciamento para infra-estrutura de Grids de alto desempenho, como medidas de tráfego em tempo real, provisionamento de QoS, roteamento otimizado entre organizações virtuais e protocolos de transferência de dados.

Por último, o WP-6 desenvolve componentes específicos de middlewares para a utilização do Grid em aplicações de nanociência de larga escala.

A maneira com que esses módulos são utilizados depende de como a aplicação foi projetada, para aplicações legadas, ou seja, que não foram desenvolvidas para ambientes de Grid, o usuário especifica onde e como, de uma maneira declarativa, suas tarefas serão preferivelmente executadas.

Para aplicações compostas por vários processos, é preciso definir um workflow entre as tarefas.

Em seguida, o superscheduler decide, automaticamente, onde os processos serão executados.

Novas aplicações, que assumem a execução em paralelo em Grids, são desenvolvidas com o auxílio das ferramentas GridRPC e GridMPI, que possibilitam paralelização de aplicações e portabilidade.

Outras facilidades como transferência de arquivos e instalação são realizadas em conjunto com o GridPSE.

Paralelas possam ser executadas em um ambiente distribuído, beneficiando-se do poder computacional ocioso que existe nas organizações.

Isso é conseguido ao integrar recursos e máquinas de usuário em laboratórios compartilhados em uma intranet ou em um Grid.

Ele tem arquitetura orientada a objetos, onde cada módulo do sistema comunica-se com os demais a partir de chamadas de método remotas.

O InteGrade utiliza CORBA como sua infra-estrutura de objetos distribuídos que facilita a implementação, uma vez que a comunicação entre os módulos do sistema é abstraída pelas chamadas de método remotas.

Esse ambiente é estruturado em clusters que podem conter estações de trabalhos ou máquinas dedicadas ao Grid.

Esses clusters são organizados de maneira hierárquica para permitir a inclusão de diversos recursos.

Mostra as principais funções que uma máquina do cluster pode realizar.
Os gerenciadores de cluster (Cluster Manager) representam computadores responsáveis por gerenciar aquele cluster e a comunicação com os demais, uma máquina do usuário (User Node) é aquela que submete aplicações ao Grid, uma provedora de recursos (Resource Provider Node) exporta parte de sua capacidade para o Grid e, em geral, são estações de trabalho, e um recurso dedicado (Dedicated Node) é reservado exclusivamente para utilização pelo Grid.

Vale ressaltar que esses papéis podem ser sobrepostos, por exemplo, uma máquina de usuário pode ser uma provedora de recurso.

Estrutura dos clusters do InteGrade.

O gerenciamento interno do cluster é realizado, conjuntamente, pelo gerenciador de recursos locais (LRM, Local Resource Manager) e pelo gerenciador de recursos globais (GRM, Global Resource Manager).

O LRM é executado em todas as máquinas do cluster que compartilham recursos e tem por objetivo coletar informações sobre a utilização de memória, CPU, disco e rede, que são enviadas para o GRM periodicamente.

O GRM, então, utiliza essas informações para o escalonamento dentro do cluster.

Quando um usuário submete uma aplicação para execução, o GRM seleciona máquinas com recursos disponíveis para serem candidatas a executar a aplicação, com base nas informações locais.

Em seguida, devido à natureza dinâmica do Grid, o GRM verifica se o melhor candidato tem recursos disponíveis naquele momento e o reserva para iniciar a execução.

Caso o recurso não esteja disponível, outro candidato é selecionado e o processo se repete.

É importante ressaltar que recursos em quaisquer um dos clusters do Grid podem ser acessados, caso seja necessário.

O LUPA (Local Usage Pattern Analyzer) e o GUPA (Global Usage Pattern Analyzer) gerenciam a coleta e a análise dos padrões de uso dentro de um cluster.

O LUPA executa em máquinas do cluster e coleta dados sobre padrões de uso.

Baseando-se nesses dados, é derivado um padrão de uso semanal para a máquina o qual é enviado periodicamente para o GUPA.

Essa informação sobre os padrões de uso é disponibilizada ao GRM para a tomada de decisões de balanceamento de carga.

O centro de controle (NCC, Node Control Center) permite que o dono do provedor de recursos defina condições para o compartilhamento.

Parâmetros como período de uso, porção de recursos compartilhado no Grid ou condições para considerar o recurso como disponível podem ser configurados com essa ferramenta.

O ASCT (Application Submission and Control Tool) permite que um usuário submeta aplicações para execução no Grid.

O usuário pode especificar pré-requisitos, como hardwares ou softwares a serem utilizados, e requisitos dos recursos, como quantidade mínima de memória, dentre outras.

Essa ferramenta pode ser utilizada para monitorar o progresso da aplicação.

O InteGrade utiliza o modelo de programação paralela BSP desenvolvido por Goldchleger.

O projeto OurGrid identifica quatro pontos-chave na execução de aplicações do tipo bag-of-tasks em Grids.

São eles, ambiente de Grid, permissão de acesso automática, escalonamento de aplicação eficiente, e mecanismo de tratamento de erro.

O OurGrid utiliza o broker chamado MyGrid, que escalona a aplicação nos diversos recursos que o usuário tenha permissão de acesso, seja por uma infraestrutura de Grid (como o Globus) ou via acesso remoto.

O MyGrid foi projetado para ser transparente e completo, englobando todo o ciclo de produção, e apresentado em duas visualizações, a GridMachine, que provê os serviços básicos necessários para usar essa máquina e a GridMachineProvider, que abstrai os gerenciadores de recursos.

Esse modelo simplifica a interoperação com novos recursos.

Sob esse projeto está em desenvolvimento um sistema P2 P de compartilhamento de recursos visando aplicações bag-of-tasks, chamado OurGrid Community3.

O intuito dessa comunidade é servir como uma rede de favores, um usuário disponibiliza seus recursos ociosos com o objetivo de poder utilizar os de outros quando necessitar.

Além disso, com uma conta autônoma, a rede de favores garante a igualdade no compartilhamento de recursos, incentivando doações de recursos.

O sistema de escalonamento utilizado pelo OurGrid é denominado de WQR, Workqueue with Replication-, que apresenta um bom desempenho sem utilizar informações de recursos ou das tarefas ao replicar aplicações em execução em máquinas disponíveis no ambiente.

A tecnologia XGrid da Apple é parte integrante dos sistemas Mac OS X e Mac OS X Server.

Por esse motivo sua configuração e utilização é simplificada, necessitando apenas habilitar esse serviço e preparar a infra-estrutura.

A arquitetura do XGrid é dividida em três camadas.

Essa arquitetura é composta por clientes, uma unidade controladora  e os agentes dedicados, parciais ou internet (3, 4 e 5 respectivamente).

Arquitetura do XGrid.

O elemento principal do XGrid é o Controller.

Ele é responsável por distribuir as aplicações do Cliente  entre os agentes do Grid (3, 4 e 5).

Ao término da execução, o Controller coleta os resultados e os apresenta ao Cliente.

O escalonamento de processos no XGrid é feito seguindo a ordem de chegada das tarefas no controlador.

A partir desse elemento, o processo é encaminhado para o computador mais rápido naquele momento.

É possível definir dependências nas tarefas de modo que essas sejam escalonadas em uma ordem determinada.

O XGrid não é heterogêneo quanto a sistema operacional, pois, necessita de computadores com sistema Mac OS X, dificultando sua utilização.

Outro problema é a presença de um elemento centralizador único, pois, caso esse pare de funcionar, todo o Grid perde sua funcionalidade.

Este capítulo apresentou definições de Grids computacionais propostas por BoteLorenzo e Stockinger.

Tomando como referência a definição desses autores, as principais características foram apresentadas e comentadas.

A tabela 32 apresenta um comparativo entre as características do MidHPC e das implementações descritas.

Características presentes nas implementações de Grids.

Como pode ser observado nessa tabela apenas o MidHPC tem suporte ao escalonamento de threads, o que facilita o desenvolvimento de aplicações paralelas e permite o uso de aplicações legadas (multithread).

Um outro ponto importante é a utilização de informações de comportamento com o objetivo de otimizar a execução de aplicações.

Dentre as implementações apresentadas, apenas o InteGrade utiliza esse tipo de informação ao coletar o histórico de uso de computadores, enquanto o MidHPC utiliza tanto informações de uso de computadores como relativas ao comportamento de execução de aplicações.

Ressalta-se, também, que somente o MidHPC e o Progrid apresentam suporte a comunicação entre processos.

Entretanto, enquanto o Progrid utiliza um sistema semelhante ao MPI que exige conhecimento do usuário para utilizar a biblioteca de comunicação, o MidHPC permite a comunicação entre processos de maneira transparente por meio de suporte de DSM.

Informações adicionais sobre as características do MidHPC são descritas no capítulo 5.

Diversas técnicas foram desenvolvidas no escopo do projeto MidHPC.

Este trabalho de mestrado foi responsável pela unificação dessas técnicas com o objetivo de construir um ambiente que realiza o balanceamento de carga de processos, adotando técnicas de inteligência artificial.

Esse ambiente suporta aplicações legadas (multithread) e permite comunicação transparente entre processos.

As técnicas de inteligência artificial utilizadas foram, Aprendizado Baseado em Instâncias (seção 42), que é usado para caracterizar o comportamento médio de execução de aplicações, e Algoritmos Genéticos (seção 43), utilizados para otimizar o balanceamento de carga com base no comportamento das aplicações.

Como o balanceamento pressupõe o uso de reatribuição dinâmica (migração), checkpoints (seção 44) são usados para permitir essa tarefa.

Esse ambiente suporta o escalonamento das diferentes linhas de execução de aplicações multithread por meio da conversão de threads em processos (discutido no capítulo 5 e no apêndice G).

Esses processos são escalonados no Grid e comunicam-se, transparentemente, por meio de um suporte de DSM (seção 45).

Essa conversão permite que desenvolvedores utilizem o modelo de programação concorrente, altamente difundido.

Aprendizado Baseado em Instâncias (Instance-Based Learning, IBL) é um paradigma de aprendizado que orienta a construção de algoritmos para encontrar instâncias similares em uma base de experiências, o qual visa aproximar funções de domínio real ou discreto.

O aprendizado consiste simplesmente em armazenar dados de treinamento e utilizá-los para realizar buscas e aproximações.

Cada instância é composta de um conjunto de atributos, os quais podem ser classificados como entradas ou saídas, atributos de entrada descrevem as condições em que uma experiência foi observada, enquanto atributos de saída representam os resultados de acordo com as condições descritas pelos atributos de entrada.

Os algoritmos IBL calculam a similaridade entre uma instância de consulta, xq, e as instâncias da base de experiências, retornando um conjunto de instâncias relacionadas ao ponto de consulta.

Nesses algoritmos, apenas as instâncias mais relevantes são usadas para classificar e realizar predições para o ponto de consulta.

Eles podem construir uma aproximação distinta por meio de uma função objetivo para cada ponto de consulta apresentado.

O algoritmo IBL, proposto por Senger, é baseado no aprendizado dos k-vizinhos mais próximos (k-nearest neighbor learning).

O método assume que todas as instâncias em uma base de experiências correspondem a pontos em um espaço <n e que uma função de distância, Euclidiana, é utilizada para definir pontos próximos à consulta.

Um problema recorrente da distância Euclidiana é que quando um dos atributos apresenta um intervalo de valores muito mais amplo, ele pode dominar os valores obtidos com a equação de distância básica, levando a um pobre desempenho classificatório do algoritmo.

Por exemplo, se o atributo responsável por armazenar o número de EPs (elementos de processamento) apresentar valores entre 1 e 1024 e o atributo que representa o identificador da fila estiver entre 1 e 5, a influência do identificador da fila será dominada pelo número de EPs alocados.

Pode-se solucionar essa questão por meio da normalização dos atributos numéricos em uma mesma escala.

Senger adotam uma função de normalização linear h(xi), que é usada para reduzir os atributos numéricos a uma mesma escala, entre 0 e 1.

Assim, para aproximar uma função de domínio de valores reais f, <n <, utilizando pesos para cada um dos k vizinhos mais relevantes, de acordo com a distância ao ponto de consulta xq, é utilizada a equação 46.

A função de peso wi deve gerar resultados próximos de um valor constante quando a distância tende a 0 e aproximar-se de 1 à medida que a distância tende ao infinito.

Embora diferentes funções de ponderação possam ser empregadas, Senger adotam uma função Gaussiana centrada no ponto xq com uma variância t, conforme equação 47.

Apresenta exemplos de valores de w para um intervalo de distâncias Euclidianas entre 0 e 10, empregando 0125, 0250, 0500, 1000 e 2000 como valores de t2.

Através das técnicas de ponderação descritas, o algoritmo proposto por Senger apresenta a vantagem de encontrar aplicações paralelas similares trabalhando com atributos em valores reais (como número de EPs utilizados, memória utilizada e tempo de execução) e atributos de valores discretos, tais como, usuário que submeteu a aplicação e grupo de trabalho ao qual pertence, assim como o identificador da fila do software de escalonamento.

No MidHPC a técnica IBL é utilizada para obter o comportamento médio de uso de recursos de uma aplicação.

Essa técnica é acionada antes de iniciar aplicações e tem sido utilizada para parametrizar o algoritmo de escalonamento de processos.

Comportamento da função de ponderação.

O IBL é usado do seguinte modo, suponha uma aplicação composta por três processos (1,2 e 3).

Durante a execução da aplicação, os processos 1,2 e 3 são monitorados e seus dados de ocupação de recursos são salvos em uma base de dados.

Em seguida, a média de uso dos recursos por esses processos é calculada e armazenada.

É com base nessas médias históricas de uso que o IBL define o comportamento médio de ocupação de recursos ao escalonar uma nova aplicação.

Algoritmos Genéticos (AGs) são aplicados como técnicas de busca e otimização em diversas áreas.

São baseados no mecanismo de seleção natural, focando na sobrevivência do indivíduo mais apto.

AGs nem sempre encontram a melhor solução possível, porém, fornecem boas soluções locais para problemas NP-completos.

A solução de problemas utilizando algoritmos genéticos envolve dois aspectos, codificação da solução na forma de cromossomos, onde cada cromossomo representa uma possível solução, e uma função de aptidão (fitness) que é aplicada para encontrar a melhor solução.

Várias técnicas de codificação podem ser utilizadas para diferentes tipos de problemas, como strings binárias, bitmaps, números reais dentre outras.

A função de aptidão é responsável por avaliar as possíveis soluções.

Essa função recebe um cromossomo como parâmetro e retorna um número real que representa a qualidade da solução obtida, por exemplo, o quão adequada é a solução para o problema em estudo.

Cromossomos mais aptos são identificados e armazenados durante o processo de evolução.

Os menos aptos, por outro lado, são eliminados.

Diferentes técnicas podem ser aplicadas para a identificação dos melhores cromossomos, como a seleção proporcional, seleção por ranking e a seleção baseada em torneio.

Na seleção proporcional, indivíduos são transferidos para a próxima geração de acordo com o valor proporcional ao resultado de sua função de aptidão.

Uma das possíveis implementações dessa técnica consiste no uso de uma roleta, dividida em N partes, com N sendo o número de indivíduos (cromossomos) da população atual.

O tamanho de cada parte é proporcional ao valor da função de aptidão de cada indivíduo.

A roleta é girada N vezes e, a cada turno, o indivíduo apontado é selecionado e inserido na próxima geração.

A seleção baseada em ranking pode ser dividida em duas etapas.

Durante a primeira, as soluções são ordenadas de acordo com seu valor da função de aptidão.

Com a lista ordenada, cada indivíduo recebe um novo valor de aptidão de acordo com sua posição no ranking.

Após isso, um procedimento que seleciona os indivíduos conforme sua posição no ranking é aplicado.

Assim, os indivíduos de melhores posições têm maiores chances de serem selecionados.

Na seleção baseada em torneio não se atribui automaticamente probabilidades a indivíduos.

Um torneio de tamanho k é definido, com k 2 indivíduos.

Então, k indivíduos são escolhidos aleatoriamente na população atual, e seus valores de aptidão são comparados e o indivíduo com melhor valor de aptidão é selecionado para reprodução.

O valor de k é definido pelo usuário, representando a pressão de seleção, isto é, a velocidade com a qual os indivíduos mais aptos vão dominar a população, gerando o extermínio dos mais fracos.

Uma vez selecionados os indivíduos para reprodução, é necessário modificar suas características genéticas usando técnicas de reprodução conhecidas como operadores genéticos.

Os operadores mais comuns são o cruzamento e a mutação.

O operador de cruzamento permite a troca de material genético entre dois indivíduos, conhecidos como pais, combinando suas informações de modo a aumentar a possibilidade de gerar um novo indivíduo com melhores características que os originais.

O cruzamento de um ponto é o mais usado.

Para aplicá-lo, dois indivíduos (pais) são selecionados e dois novos indivíduos são criados a partir deles (filhos).

Um único ponto de quebra aleatório é selecionado nos cromossomos pais, e novos cromossomos são criados à partir da combinação dos primeiros.

Mostra os indivíduos pais e o ponto de separação marcado pelo símbolo |.

Novos indivíduos criados a partir da combinação dos cromossomos pais são mostrados, ilustrando o operador de cruzamento.

Operador de Cruzamento, um ponto.

O cruzamento de dois pontos também é bastante utilizado.

Semelhante ao de um ponto, onde dois indivíduos são selecionados e dois novos, criados.

A diferença é que esse operador utiliza dois pontos de quebra aleatórios, ao invés de apenas um.

Exemplifica um cruzamento com os locais de quebra representados pelo símbolo |.

Mostra os cromossomos pais e os gerados após o cruzamento.

Operador de Cruzamento, dois pontos.

O operador de mutação é utilizado para alterar um único gene por um valor aleatório.

Quando um indivíduo é representado por um bitmap, esse operador escolhe de forma aleatória um gene do cromossomo e troca seu valor de 1 para 0 e vice-versa.

O objetivo do operador de mutação é manter a diversidade da população, sempre permitindo que um cromossomo cubra um amplo espaço de busca.

Esse operador é geralmente aplicado com baixa probabilidade, pois, com alta, o resultado tende a ser aleatório.

No contexto do MidHPC, os conceitos abordados nessa seção foram utilizados para o desenvolvimento do algoritmo de balanceamento de carga RouteGA, que é detalhado na seção 542.

Uma ferramenta de checkpoint captura o contexto de um processo, como registradores, memória e arquivos abertos, de modo a permitir o reinício da aplicação, no mesmo ou em outro processador.

É possível classificar as ferramentas de checkpoint conforme a forma de implementação, modificações do kernel, implementação de módulos e no espaço de usuário.

Os mecanismos que modificam o kernel são implementados por meio de alterações núcleo do sistema operacional.

Esses mecanismos têm a vantagem de oferecer acesso direto às estruturas do kernel e a desvantagem de depender das estruturas da versão utilizada, aumentando a complexidade de implementação e dificultando a portabilidade.

Os mecanismos que implementam módulos de kernel interceptam chamadas de sistema.

Alguns desses mecanismos necessitam da adaptação de bibliotecas de funções.

Eles têm a vantagem de diminuir a dependência do kernel.

Finalmente, alguns sistemas de checkpoint são implementados como programas de usuário que podem necessitar de privilégios de root.

Como outros sistemas de checkpoint ele também captura o estado de um processo em execução e o grava em um arquivo executável.

Esse arquivo pode ser utilizado para reiniciar o processo posteriormente.

CryoPID1, sistema utilizado pelo MidHPC, é uma ferramenta de checkpoint implementada como programa de usuário.

Ela apresenta o diferencial de não necessitar de permissões de superusuário (root) para realizar o checkpoint (executa em sistemas Linux-kernel 24 e 26).

Essa ferramenta permite que uma imagem completa do executável seja gerada, inclusive com as bibliotecas dinâmicas utilizadas para a execução em máquinas com versões diferentes.

Sua execução é simplificada, pois é necessário apenas o comando frezze <output-file> <pid>, onde output-file é o nome do arquivo que conterá o checkpoint e pid representa o identificador do processo sobre o qual será realizado o checkpoint.

Executado esse comando, um arquivo binário executável, de nome <output-file>, é gerado com as informações do processo e o seu reinício será efetuado com a execução desse arquivo.

No MidHPC, o checkpoint é utilizado para possibilitar a migração de processos.

Atualmente, ele é acionado no início da aplicação, após a conversão de threads em processos, para que esses sejam migrados de acordo com o resultado do algoritmo de escalonamento.

Vale ressaltar que esse algoritmo pode indicar a migração de processos em execução, o que significa realizar um checkpoint sobre eles.

Em um DSM (Distributed Shared Memory), cada processador participante de um sistema distribuído oferece parte de sua memória local para os demais, visando ampliar o espaço de endereçamento global de memória.

O suporte de DSM deve oferecer consistência de acesso aos dados para tarefas, localizadas em processadores distintos, que compartilham regiões de memória.

Um mecanismo de DSM pode ser definido como uma memória que oferece acesso distribuído a dados de maneira uniforme, transparente e consistente.

Assim, dados localizados na memória compartilhada são tratados como se estivessem na memória local.

Um suporte a DSM pode ser classificado de acordo com três itens, algoritmos, implementação e modelo de consistência, detalhados a seguir.

Algoritmos Os suportes de DSM consideram algoritmos com diferentes abordagens para distribuição e consistência de acesso aos dados.

Com relação à distribuição de dados, duas estratégias podem ser utilizadas, replicação e migração.

Na replicação, várias cópias de um mesmo arquivo podem existir na memória local de diferentes processadores.

Essas cópias permitem que cada processador acesse a informação localmente, evitando, assim, atrasos de comunicação.

A replicação beneficia aplicações que efetuam, principalmente, a leitura de dados.

Na migração, há somente uma cópia do dado, que trafega pelo meio de comunicação.

Sempre que um processador necessita de um dado, esse é copiado para a memória local, através da rede.

A migração garante a exclusividade de acesso ao dado, porém, prejudica o acesso simultâneo.

Quanto à consistência de acesso aos dados, os algoritmos utilizam três abordagens, SRSW (Single Reader / Single Writer), nessa abordagem há apenas um processo realizando escritas e outro realizando leituras de dados da memória compartilhada.

Para esse tipo de algoritmo geralmente é utilizado um servidor central que controla o acesso à memória compartilhada, o que pode levar a um baixo desempenho.

MRSW (Multiple Reader / Single Writer), nessa abordagem vários processos realizam leituras enquanto somente um faz operações de escrita na memória compartilhada.

Essa abordagem utiliza a replicação dos dados para servir a várias requisições de leitura.

Esse tipo de algoritmo apresenta um bom desempenho para operações de leitura, visto que os processadores acessam cópias locais da memória.

Porém, o desempenho para a escrita é baixo, pois é necessário que todas as cópias sejam atualizadas sempre que um conteúdo for modificado.

MRMW (Multiple Reader / Multiple Writer), esse tipo de algoritmo utiliza a replicação de dados tanto para a leitura quanto para a escrita de dados na memória compartilhada, permitindo, assim, a existência de múltiplos leitores e escritores.

Apesar de oferecer um bom desempenho para as operações de escrita, essa abordagem utiliza excessivamente o meio de comunicação, devido às mensagens para atualização de dados.

Um suporte de DSM pode ser classificado, de acordo com seu nível de implementação, em, Software, Hardware e Híbrido.

O primeiro tipo utiliza recursos de software para gerenciar distribuição, acesso e consistência dos dados localizados na memória distribuída.

Esse tipo de sistema é mais difundido, pois não necessita de dispositivos específicos e, geralmente, não apresenta restrições quanto ao sistema operacional.

Os sistemas de DSM em nível de software podem ser implementados tanto em nível de usuário quanto em nível de sistema.

Os sistemas implementados em nível de usuário não necessitam de modificações no sistema operacional, o que o torna mais flexível e independente.

Nesses sistemas, todo o processamento de dados é feito pelas aplicações que acessam a memória compartilhada, ou por uma biblioteca de funções.

Contudo, a utilização do espaço de usuário para esse processamento prejudica o desempenho dessas implementações.

Por outro lado, os sistemas implementados em nível de sistema são menos flexíveis e dependem de modificações no sistema operacional.

Mas, apresentam um melhor desempenho, pois, todo o processamento de dados, ou uma parte dele, é feito pelo sistema operacional.

Alguns exemplos de sistema de memória compartilhada distribuída no nível de software são, Mermaid, Blizzard, Mirage, Clouds, Linda, dentre outros.

O segundo tipo de suporte a DSM necessita de componentes e dispositivos de hardware.

O desempenho dessa abordagem é superior, uma vez que o processamento é feito por um hardware específico.

Porém, ela é restrita, uma vez que os sistemas devem suportar e ter o mesmo dispositivo hardware.

Entre os exemplos desse tipo de sistemas de memória compartilhada distribuída pode-se mencionar, Dash, SCI, DDM, Merlin, entre outros.

No terceiro tipo de suporte a DSM, o híbrido, operações de memória compartilhada são realizadas por softwares e em dispositivos de hardware específicos.

Exemplos dessa classe de sistemas de memória distribuída incluem Plus, Galactica Net, Alewife, Flash, Typhoon e Shrimp, entre outros.

Um sistema de DSM é também classificado, de acordo com o modelo de consistência, como forte ou fraco.

Um modelo de consistência forte apresenta uma menor complexidade de implementação ao custo de uma maior latência de acesso à memória.

Modelos fracos, ou relaxados, utilizam técnicas de reorganização da memória, pipelining e sobreposição de dados, que melhoram o desempenho das operações de entrada e saída ao custo de maior complexidade computacional.

Modelos de consistência fortes são classificados em, seqüencial e controlado pelo processador.

O primeiro, utiliza uma mesma seqüência de acesso à memória para todos os processadores do sistema, por exemplo, uma fila de acesso baseada no protocolo FIFO (First In First Out).

O segundo é semelhante ao anterior, porém, a ordem dos acessos é controlada por um único dispositivo do sistema.

Esse modelo, oferece um melhor desempenho, mas é dependente do hardware utilizado.

Modelos de consistência fracos utilizam-se de acesso comum à memória e acesso sincronizado, sendo que a consistência da memória é verificada apenas quando ocorrem acessos sincronizados.

Os acessos sincronizados podem ser de dois tipos, requisição (acquire) e liberação de páginas (release).

Alguns exemplos de modelos de consistência fracos são apresentadas a seguir.

No modelo denominado release consistency, os acessos comuns de leitura e escrita podem ser realizados apenas nas páginas que foram requisitadas previamente (acquire) e as modificações são efetuadas após a liberação (release) dessas páginas.

Nas técnicas lazy release consistency e entry consistency, as modificações das páginas não são armazenadas na memória compartilhada após a liberação das mesmas.

As alterações somente são feitas na memória compartilhada na próxima requisição às páginas modificadas, sendo efetuadas de maneira tardia, visando obter maior desempenho.

No caso do lazy release consistency a consistência é mantida por uma camada de software, enquanto no entry consistency essa é realizada por meio de recursos do compilador.

O sistema de memória compartilhada distribuída utilizado pelo MidHPC (detalhado na seção 56) foi desenvolvido e implementado por Dodonov.

Esse sistema permite o acesso distribuído aos dados por múltiplos leitores e escritores simultaneamente e o modelo de consistência utilizado foi o seqüencial.

Atualmente, um arquivo (localizado no sistema de arquivos distribuídos) é utilizado como memória e é mapeado pelos processos.

Para realizar operações de leitura e escrita, comandos como dsm_write e dsm_read são utilizados.

Como atividade futura, espera-se modificar esse suporte de DSM para que essas operações sejam transparentes ao usuário.

Um modelo de programação é um conjunto de tecnologias de softwares para escrever algoritmos de acordo com o sistema (hardware ou software) utilizado.

Segundo Lee & Talia os modelos de programação são classificados em, estados compartilhados, tipicamente associados com modelos de execução e linguagens síncronas direcionadas para máquinas de memória compartilhada ou de memória distribuída com alta largura de banda e de baixa latência.

O JavaSpaces é um exemplo desse tipo de modelo.

Passagem de mensagens, processos executam em espaços de endereços distintos, onde informações são trocadas através de mensagens.

Essas mensagens são geradas por funções implementadas em bibliotecas.

Esse modelo força o desenvolvedor a verificar exatamente onde uma comunicação potencialmente custosa pode ocorrer.

O PVM, o MPI e suas variantes são exemplos do modelo de passagem de mensagens.

RPC (Remote Procedure Call) e RMI (Remote Method Invocation), esses modelos, implementados como construções de linguagem, provêem um mecanismo simples para gerenciar computação remota.

Além de gerenciar o fluxo de controle e de dados, RPC e RMI possibilitam a checagem de aridade e de tipos de argumentos.

Eles também podem ser utilizados para construir outros modelos para programação em Grids.

Ponto a ponto (P2 P), computação P2 P permite o compartilhamento de recursos computacionais e serviços sem intermediários.

Esse modelo aproveita-se da capacidade computacional dos desktops e da conectividade de rede.

Em um sistema P2 P, computadores que antes eram apenas clientes também atuam com servidores, assumindo o papel que for mais eficiente para o sistema.

Isso reduz a carga em servidores dedicados e permite a realização de serviços especializados eficientemente.

Dentre as ferramentas que implementam diferentes modelos de programação vale destacar o PVM (Parallel Virtual Machine), o MPI (Message Passing Interface) e o OpenMP (Open Multi-Processing), pois esses são os mais utilizados e os que mais se assemelham ao modelo de programação utilizado no MidHPC.

PVM é um conjunto integrado de ferramentas de software e bibliotecas, que foi projetado para permitir que uma rede de computadores heterogêneos possa ser utilizada como um único processador paralelo distribuído.

O modelo de computação PVM é baseado na idéia de que uma aplicação é composta por várias tarefas.

Cada tarefa é responsável por uma parte da carga de trabalho da aplicação.

Essas tarefas são executadas na máquina virtual e a sincronização é feita através de trocas de mensagens definidas pelos usuários.

O MPI é um padrão para a comunicação de dados em computação paralela definido pelo Message Passing Interface Forum, diferentemente do PVM que é um conjunto de software e bibliotecas.

Nesse padrão, os processos de uma aplicação comunicam-se por meio de funções que controlam o envio e o recebimento de mensagens.

Existem diversas implementações do padrão MPI, dentre elas o LAM-MPI2 e o MPICH3.

As funções do OpenMP suportam a utilização de multiprocessamento de memória compartilhada em ambiente multi-plataforma (para C/C++ e Fortran) e consiste em um conjunto de diretivas de compilação, bibliotecas de procedimentos e variáveis de ambiente que influenciam o comportamento de execução.

As linhas de comando originais (processamento em série) não precisam, em geral, ser modificadas quando paralelizada com o OpenMP, diminuindo, assim, o risco de introduzir erros.

Entretanto, o OpenMP executa de modo eficiente apenas em sistemas de memória distribuída.

Além disso, apresenta baixo paralelismo, pois ele depende da paralelização de loops, mantendo seqüencial um percentual relativamente alto de códigos fora de loops.

O modelo de programação do MidHPC é baseado no padrão de threads POSIX, permitindo que aplicações legadas possam ser executadas com pouca ou nenhuma adaptação.

Novas aplicações podem ser desenvolvidas utilizando-se conceitos amplamente difundifos (programação concorrente), sem a necessidade de adquirir conhecimentos específicos de modelos ou bibliotecas.

Assim, o desenvolvedor precisará se preocupar apenas com sua aplicação e com problemas clássicos da implementação, que envolvem sincronização de variáveis e condições de corrida.

As técnicas apresentadas neste capítulo são utilizadas pelo MidHPC para otimizar o balanceamento de carga e para tornar transparente a interação do usuário com o ambiente.

O IBL é utilizado para obter o comportamento médio de uso de recursos de uma aplicação.

Essa técnica é acionada antes de se iniciar uma aplicação e tem sido utilizada para parametrizar o algoritmo de escalonamento de processos.

Atualmente, esse algoritmo de escalonamento utiliza Algoritmos Genéticos para otimizar o balanceamento de carga, esse algoritmo foi denominado RouteGA e é detalhado na seção 542.

A migração de processos, com base no resultado do algoritmo RouteGA, é conduzida por meio da transferência de arquivos de checkpoint entre computadores (gerados pela ferramenta CryoPID).

A comunicação desses processos ocorre por meio de um suporte de DSM.

Para executar no MidHPC, as aplicações devem seguir o modelo de programação baseado no padrão de threads POSIX.

Devido ao uso do modelo de programação concorrente, as aplicações podem ser desenvolvidas utilizando-se conceitos amplamente difundidos (programação concorrente), sem a necessidade de adquirir conhecimentos específicos de modelos ou bibliotecas.

Assim, o desenvolvedor precisará se preocupar apenas com sua aplicação e com problemas clássicos da implementação, que envolvem sincronização de variáveis e condições de corrida.

O objetivo do MidHPC (Middleware for High Performance Computing) é simplificar o processo de análise, projeto, implementação e execução de aplicações paralelas em ambientes heterogêneos de larga escala.

Outras implementações de Grid apresentadas no capítulo 3 abordaram essas questões.

Entretanto, essas implementações não suprem necessidades de transparência de uso, suporte a aplicações legadas e otimização de comunicação e escalonamento.

Além de suprir tais necessidades, o MidHPC oferece suporte a aplicações legadas (multithreads) ao interceptar a criação de threads para convertê-las em processos e permitir o balanceamento de carga transparente.

A comunicação entre esses processos é realizada por meio de um suporte de DSM transparente.

O escalonamento de processos é conduzido com base em módulos de inteligência artificial que consideram o comportamento de aplicações.

Com o objetivo de fornecer transparência no uso de um Grid, o MidHPC organiza e gerencia os recursos disponíveis no ambiente por meio dos módulos Broker Global e Broker Local.

O suporte a aplicações legadas e o balanceamento de carga são providos pelo módulo Scheduler, que adota técnicas de inteligência artificial.

Usuários acessam recursos do Grid e requisitam a execução de aplicações por meio de um módulo baseado no GNU/bash, denominado Shell.

Apresenta a disposição desses módulos no MidHPC.

O Scheduler utiliza informações sobre capacidade dos recursos e comportamento das Estrutura do MidHPC.

Aplicações para tomar decisões de escalonamento, adotando o algoritmo de balanceamento de carga RouteGA parametrizado pela técnica de inteligência artificial IBL.

A parametrização é baseada em informações extraídas por um módulo denominado Monitor.

Esse módulo coleta informações das aplicações durante a execução, as quais podem ser utilizadas para predição de comportamento futuro, como proposto por Dodonov.

Essas informações são armazenadas na base de dados e posteriormente utilizadas pelo módulo IBL para permitir ao RouteGA planejar a migração de processos com o objetivo de otimizar o tempo de resposta das aplicações.

Processos de aplicações paralelas comunicam-se de maneira transparente através de um sistema de memória compartilhada distribuída (DSM Distributed Shared Memory).

A implementação atual do DSM executa sobre um sistema de arquivos distribuídos (DFS-Distributed File System) que é acessível pelos computadores do ambiente.

O presente trabalho tem como objetivo o desenvolvimento dos módulos Broker Global, Broker Local Scheduler e Shell.

Além da integração das demais ferramentas que compõem o projeto MidHPC.

O Broker é o módulo utilizado para conectar, de modo transparente, as redes de computadores que compõem o Grid.

Esse módulo é composto por dois gerenciadores distintos, BL (Broker Local) e o BG (Broker Global).

Cada BG é responsável por gerenciar um conjunto de BL.

O gerenciador BL é responsável por oferecer e controlar o acesso de computadores de uma rede local ao Grid.

Esse gerenciador armazena uma lista que contém informações sobre os computadores que executam o módulo Scheduler (o que é um requisito para oferecer recursos ao Grid).

Essas informações incluem capacidade de processamento, latência de acesso à memória principal e virtual, vazão de leitura e escrita em disco e a carga1 atual de cada computador em sua rede.

Tais informações são resumidas em médias e desvios-padrão.

Essas medidas-resumo e o número de computadores na rede do BL são submetidos ao e armazenados pelo BG.

A interligação entre redes é feita através dos Brokers Globais.

O primeiro Broker Global iniciado exercerá o papel de Primário.

As requisições de usuários, oriundas do Shell, que iniciam aplicações ou obtém informações sobre suas execuções são atendidas pelo gerenciador BG Primário.

O Broker Global resume as informações de cada um de seus BLs em uma única entrada replicada em todos os Brokers Globais para fins de tolerância a falhas.

Além disso, a localização (endereço de rede) dos outros BGs também é armazenada.

A integração dos Brokers com os outros módulos do MidHPC está detalhada na seção 57 e os apêndices A e B apresentam informações detalhadas sobre a implementação do Broker Global e Local, respectivamente.

Para simplificar a interação do usuário foram desenvolvidas ferramentas para um Shell, utilizadas para submissão e controle da execução de aplicações.

O Shell executa sobre um sistema de arquivos distribuídos que contém uma imagem do sistema operacional Linux2.

Atualmente, o sistema de arquivos utilizado é o NFS (Network File System) que centraliza a imagem do sistema operacional e oferece uma visão única dessa partição para os demais computadores do sistema, inclusive Brokers e Schedulers.

Um usuário, quando necessita executar aplicações paralelas e distribuídas, deve apenas copiar sua aplicação para a imagem única do sistema de arquivos do Grid.

Caso a imagem do sitema operacional Linux não contenha alguma das bibliotecas, o computador onde a aplicação foi lançada instala as dependências3.

Para controlar a execução de aplicações e gerenciar o ambiente, o usuário pode utilizar as seguintes ferramentas implementadas no Shell, 1-Mkill, a aplicação mkill requisita que o Broker Global interrompa a execução de uma aplicação.

Essa requisição será redirecionada para qualquer Scheduler que execute um processo da aplicação, para que seja possível interrompê-lo (usando o comando kill do Unix).

Mps, a aplicação mps requisita informações sobre o ambiente.

Isso inclui os Brokers Globais, Broker Locais, Schedulers, tabela de RTT (entre Brokers Globais) e aplicações (com seus processos).

Mstart, a aplicação mstart requisita ao Broker Global o início de uma aplicação.

O apêndice C apresenta mais informações sobre o módulo Shell.

As interações com outros módulos são apresentadas na seção 57.

O módulo Scheduler tem funções de gerenciamento de recursos e de troca de informações com o Broker Local e com o algoritmo de balanceamento de carga.

As informações trocadas com o Broker Local são detalhadas na seção 57, visto que é por meio do Broker Local que o Scheduler se integra ao ambiente.

A troca de informações entre o módulo Scheduler e o algoritmo de balanceamento de carga é feita através de dois arquivos no padrão XML.

O primeiro contém dados sobre capacidades e carga dos recursos computacionais envolvidos, aplicações em execução e localização dos processos da aplicação.

O segundo, contém a resposta do algoritmo de escalonamento (como os processos serão alocados nos recursos disponíveis).

Comunicação entre os módulos de Scheduler.

O módulo Scheduler permite a substituição de seu algoritmo de escalonamento, uma vez que há uma interface bem definida por meio de arquivos no padrão XML.

A seguir são apresentados os algoritmos de balanceamento de carga Route, resultado de estudos realizados pelo grupo de pesquisa (Mello, 2006 b), e RouteGA, também desenvolvido pelo grupo.

O algoritmo RouteGA é atualmente utilizado como escalonador de processos do MidHPC.

O Route é um algoritmo de balanceamento de carga totalmente distribuído, voltado para ambientes de Grid, onde existe uma grande quantidade de recursos, heterogeneidade, alta latência de comunicação (quando comparado com máquinas paralelas virtuais executadas em LANs) e um grande número de usuários.

Os algoritmos de balanceamento de carga utilizam-se de reatribuição dinâmica de processos para prover uma distribuição eqüitativa, baseando-se na capacidade computacional dos recursos.

O algoritmo Route foi desenvolvido à partir do conceito de roteamento de mensagens, o qual é utilizado para definir uma vizinhança de computadores.

Cada computador pode distribuir processos de aplicações paralelas entre seus vizinhos.

Para entender esse algoritmo, suponha três redes de computadores distintas, e.

Cada computador, composto por diferentes capacidades de processamento (memória, disco e acesso a rede), deve executar um módulo capaz de tomar decisões de escalonamento.

Depois de iniciar esse módulo em um computador Cn, (computador n da rede ), calcula-se o atraso de comunicação dessa rede, definido como RTT (RTT, Round-Trip Time).

O atraso (ou latência de comunicação) é o custo para transmitir e receber uma mensagem de tamanho mínimo entre dois computadores da rede.

Após essa etapa define-se o RTT máximo, RTTvizinhanca, usado para definir a vizinhança de Cn, dada na equação 51, onde k é um parâmetro de atraso para comunicação entre computadores.

O valor de k pode ser definido de maneira fixa ou adaptativa, baseando-se nas capacidades das redes envolvidas no Grid.

Todos os computadores do ambiente que apresentarem um atraso de comunicação, em relação ao computador Cn, menor que RTTvizinhanca são definidos como vizinhos.

A distribuição dos processos da aplicação paralela é feita sobre tal vizinhança.

Quando um computador Cn, inicia uma aplicação paralela, ele requisita informações sobre a carga de seus vizinhos com o objetivo identificar os mais ociosos.

Como a distribuição de carga é feita entre vizinhos, e a vizinhança é definida pelo atraso de comunicação entre os computadores, o uso de uma rede de alto desempenho garante menor atraso de sincronização entre os processos da mesma aplicação paralela.

Assim, se um recurso computacional está sobrecarregado, ele avaliará a carga de seus vizinhos.

Migrações de processos (ou tarefas) são conduzidas utilizando o modelo de custo de migração definido por Mello & Senger.

Esse modelo considera a carga dos processos, o quanto foi e será executado, além do custo de transferí-los para outros computadores.

Melhorias no algoritmo Route foram realizadas gerando um novo balanceador de carga denominado RouteGA 4 que otimiza a primeira alocação de processos.

Para ilustrar o problema da primeira alocação no Route, suponha que um computador ci inicie uma aplicação paralela aj composta por n tarefas, ou processos (p0,pn), em um tempo t e que a vizinhança v desse computador, definida pelo atraso de comunicação (equação 51), é formada pelos recursos vc0,vck, que apresentam, respectivamente, capacidades de processamento de cap0,capk.

Suponha, também, que cada recurso da vizinhança tem uma longa fila de processos gerando alta carga por longos períodos.

No Route, os processos são designados para os v vizinhos do computador ci, que os redistribuem, de acordo com o modelo de migração proposto por Mello & Senger, até convergir para áreas do Grid capazes de atender esses processos.

Essa convergência pode ser lenta, o que aumenta o tempo de resposta das aplicações paralelas.

Altos tempos de resposta implicam em demora na execução de aplicações e diminuem, conseqüentemente, o desempenho do sistema.

Essa limitação motivou o desenvolvimento do RouteGA, que utiliza algoritmos genéticos e otimiza a etapa de alocação considerando as necessidades da aplicação, a capacidade e a carga do ambiente.

As necessidades (o quê, quando e quanto) são armazenadas em uma base de experiências.

Um algoritmo IBL é executado utilizando essa base para encontrar informações de modo a parametrizar o algoritmo genético do RouteGA.

Para isso, o IBL requer detalhes das aplicações paralelas tais como, nome do executável, parâmetros e usuário.

O IBL retorna a média e o desvio padrão de uso dos recursos, incluindo a largura de banda necessária para comunicação entre processos, que são usados para definir a vizinhança para o RouteGA.

Quanto maior e mais freqüente a troca de informações entre processos de uma mesma aplicação paralela, menor deve ser a latência entre os processadores vizinhos considerado na alocação do RouteGA.

O algoritmo genético gera soluções, ou cromossomos, que são avaliados de acordo com uma função de aptidão.

Um exemplo de cromossomo é i = {0, 3, 8, 5, 7}, onde cada índice j do vetor representa um processo que compõe a aplicação paralela (o tamanho do cromossomo |i| = 5, representa o número de processos da aplicação paralela) e cada elemento do vetor representa onde o processo será alocado.

Por exemplo, o processo de índice 0 será alocado no computador 0, o de índice 1 no computador 3 e assim por diante.

O algoritmo genético do RouteGA considera a função de aptidão multi-objetivo (Fi) apresentada na equação 52, onde, MRTi é o tempo de resposta do processo, com maior custo computacional, de uma aplicação paralela, é um parâmetro para evitar divisões por zero e LVi é a variação de carga que a solução proposta causa ao ambiente (equação 4 Outstanding paper no IEEE 21 st International Conference on Advanced Information Networking and Applications, 444 submissões.

O tempo de resposta MRTi é obtido pela equação 54 onde Cj é o custo de execução total para o processo j.

A equação 55 define Cj, onde PMj é o custo de processamento total do processo j, CAPd é a capacidade do computador d onde o processo j está alocado, e CCj é o custo de comunicação do processo j com qualquer outro processo da mesma aplicação paralela executando em outros processadores.

A equação 56 define como o custo de comunicação é obtido, onde CPS(ij,ik) é o custo de comunicação entre tarefas nos índices j e k do cromossomo i que executam, respectivamente, nos processadores ij e A função multi-objetivo visa minimizar o tempo de resposta do processo com maior custo (MRTi) e causar a menor variação de carga no ambiente, o que significa um melhor balanceamento.

Quanto maior o valor Fi, melhor é a representação da solução.

Após executar o algoritmo genético por várias gerações, onde, em cada uma, n indivíduos são criados, cruzados e mutados, ele deve convergir para uma solução válida que apresente o maior valor de aptidão que, nesse caso, significa uma melhor alocação de processos no Grid.

Esse algoritmo é usado na primeira distribuição de processos de uma aplicação paralela no Grid.

Depois disso, processos podem ser migrados, de acordo com o modelo de migração apresentado por Mello & Senger, entre computadores vizinhos, tal como no Route.

No MidHPC, existem duas frentes de trabalho para a extração, classificação e predição.

A primeira segue o modelo proposto por Dodonov, que utiliza redes neurais e abordagens estatísticas.

A segunda, mais simples, utiliza a técnica IBL para obter médias de uso de recursos.

No modelo proposto por Dodonov, a primeira fase é responsável por monitorar a execução de aplicações utilizando uma ferramenta de análise, a qual é baseada no projeto GridBox.

Essa abordagem intercepta as seguintes chamadas pelo Monitor, leitura e escrita em disco, envio e recebimento em rede.

Entre pares de eventos consecutivos, captura-se a carga de processamento e a ocupação de memória gerada pelo processo.

As informações obtidas são utilizadas para fazer predições sobre o comportamento da aplicação.

Em seguida, o comportamento da aplicação é classificado com o objetivo de redução de dimensionalidade.

Essa redução elimina pequenos ruídos dos dados, que aumentam o desempenho da etapa de predição.

A classificação é feita utilizando uma arquitetura de rede neural ART-2 A, que é adaptativa, não supervisionada e on-line, classificando padrões sem conhecimento prévio.

O comportamento classificado da aplicação é utilizado por duas técnicas de predição diferentes e independentes, a primeira cria uma cadeia de Markov para definir arcos entre os estados classificados, de acordo com a variação do comportamento da aplicação durante sua execução.
A segunda constrói uma série temporal, seguindo as mudanças da aplicação, que é submetida para uma rede neural de predição, capaz de aproximar o comportamento futuro.

Essa predição é utilizada para tomar decisões sobre a migração de processos.

A abordagem utilizando a técnica IBL faz uso de parte do modelo descrito anteriormente, mais especificamente da extração de comportamento de aplicações, para preencher a base de dados que é adotada para obter médias de ocupação de recursos, conforme detalhado na seção 42.

Atualmente, o monitor, a base de dados e o IBL estão integrados ao MidHPC, os quais são utilizados para caracterizar comportamento histórico de aplicações visando otimizar o balanceamento de carga.

Paralelamente, outras pesquisas têm sido desenvolvidas com o intuito de predizer o comportamento de aplicações com base nas informações capturadas da execução corrente.

O MidHPC utiliza um suporte de DSM para a comunicação entre processos.

Para compreender o seu funcionamento considere uma aplicação, composta por um grupo de processos T.

Os processos T e T comunicam-se através da mudança do valor da variável x, que é compartilhada por ambos.

Portanto, quando T modifica o valor de x, essa alteração também é percebida em T.

Como T e T podem estar em computadores distintos, o DSM (Distributed Shared Memory) é utilizado para manter a consistência do valor em memória.

Quando a aplicação inicia, o módulo Scheduler cria, no sistema de arquivos distribuídos, um arquivo5 f que será utilizado como memória principal pelo DSM.

A função mmap faz a associação desse arquivo com a aplicação.

Assim, quando essa aplicação alocar um espaço de memória, ela utilizará uma área no arquivo f.

O tamanho do arquivo f é determinado pela informação contida na base de conhecimento IBL (uso média da memória principal), sendo o padrão de 32 Mb.

O módulo DSM intercepta as chamadas POSIX para controle de condição de corrida, como mutex e semáforos, visando criar e modificar variáveis compartilhadas armazenadas no arquivo do DSM.

Esse módulo utiliza o modelo de consistência seqüencial, também adotado no padrão POSIX.

Qualquer operação de leitura e escrita é executada no arquivo do DSM e o desenvolvedor é responsável por utilizar locks e unlocks para manter a memória consistente em caso de condição de corrida (como no problema produtor/consumidor).

Para utilizar o DSM, faz-se necessária a adoção de um sistema de arquivos distribuídos, DFS (Distributed File System), pois o arquivo f precisa ser acessível pelos computadores que executam a aplicação.

Qualquer DFS pode ser utilizado desde que ele permita a leitura distribuída, mesmo que os dados sejam centralizados.

Se os dados forem distribuídos, ou replicados, o desempenho do DSM aumenta.

Atualmente, o sistema de arquivos distribuídos utilizado em experimentos é o NFS (Network File System), devido à sua simplicidade de instalação e configuração.

Sistemas de arquivos como o SSHFS (que provê a segurança), o AFS e o CODA (que aumentam o desempenho), dentre outros, podem ser utilizados como DFS base, o qual pode ser alterado sem haver modificações na aplicação nem em qualquer módulo do MidHPC.

Esta seção detalha a integração e uso dos módulos do MidHPC, que devem ser iniciados em uma ordem pré-definida para colocar em funcionamento o Grid.

Ordem de execução dos módulos do MidHPC.

Utilizando a linha de comando Unix dd if=/dev/zero of=MEMORY_FILE bs=1 M count=32, para um arquivo com 32 Mb de tamanho, por exemplo.

Observa-se que, após iniciar o Broker Local, pode-se escolher entre iniciar o módulo Scheduler ou o Shell.

Porém, todas as funcionalidades do MidHPC somente estarão disponíveis caso hajam módulos Schedulers iniciados, pois esses são os responsáveis pela execução de aplicações.

O primeiro módulo a ser iniciado é o Broker Global que procura por um Broker Global Primário.

Se esse não responder, o Broker iniciado assume e desempenha o papel de Primário.

Diagrama de Seqüência, Início do Broker (Caso 1).

Caso o Broker Primário responda, o gerenciador BG iniciado registra-se no Primário.

Em seguida, dois tipos de mensagens são enviadas pelo Broker Global Primário, o primeiro, para o novo BG com as localizações dos demais BGs no Grid (endereços IP) e o segundo, para outros BGs informando sobre a localização do novo Broker Global.

Diagrama de Seqüência, Início do Broker (Caso 2).

A busca pelo Broker Global Primário utiliza uma entrada em DNS dinâmico, tal como broker midhpc usp br.

Para verificar se esse Broker está ativo, tenta-se uma conexão com esse nome.

Caso não seja bem sucedida, o Broker Global que tentou a conexão modifica a entrada no DNS e, assim, novas requisições serão direcionadas a ele.

Embora o nome seja utilizado para referenciar um Broker Global como Primário, não existe diferença entre os Brokers Globais, pois eles replicam todas as informações.

Caso o Primário fique inoperante, o primeiro Broker Global que requisitar alguma informação assumirá o papel de Primário.

Além disso, periodicamente, ocorre uma verificação para identificar quais Brokers estão ativos.

A distância entre os BGs é definida na forma de latência do sistema de comunicação (RTT, Round-Trip Time) que é calculada, periodicamente, de modo a manter um valor atualizado.

Essa medida é realizada através do comando ping e é armazenada em uma base de dados.

Após o início do Broker Global, deve-se iniciar o Broker Local que se registra no Broker Global indicado em seu arquivo de configuração.

À partir desse momento, a infraestrutura básica do ambiente está criada.

Com os Brokers Global e Local em execução, o próximo módulo a ser iniciado é o Scheduler (seção 54).

Ao iniciar, o Scheduler verifica as capacidades do computador6 e envia uma mensagem para o Broker Local definido no arquivo de configuração.

É através dessa mensagem que o Scheduler é adicionado na lista do Broker Local que, por sua vez, retorna ao Scheduler o endereço IP do Broker Global.

Em seguida, o Scheduler armazena as informações de capacidade (quantidade e desempenho de memória principal e swap, capacidade em Mips e desempenho de disco), juntamente com os endereços IPs dos Brokers Local e Global (utilizados para identificar a rede à qual o Scheduler faz parte), na base de dados.

Posteriormente, o gerenciador do BL atualiza suas informações, devido à adição de um novo Scheduler no ambiente, e envia uma mensagem para atualizar as informações do Broker Global.

Diagrama de Seqüência, Início do Scheduler.

O módulo Shell necessita que apenas o Broker Global esteja em execução, pois ele não se conecta diretamente a um Scheduler.

Entretanto, nenhuma aplicação pode ser iniciada sem Schedulers em execução, pois é por meio deles que recursos computacionais 6 Capacidade em Mips, throughput de leitura e escrita em disco, quantidade e desempenho de memórias principal e swap.

Essas informações são coletadas através de ferramentas de benchmark proposta por Mello & Senger.

São disponibilizados ao Grid.

Requisições de usuário são enviadas ao Broker Global, que é responsável por respondê-la.

Requisições provenientes do comando mps solicitam ao Broker Global que colete informações sobre aplicações e ambiente, para que essas sejam exibidas para o usuário.

Aquelas oriundas de um comando mkill <gid> fazem com que o Broker Global envie mensagens para interromper a execução dos processos da aplicação com o identificador <gid>.

Esse <gid> caracteriza todos os processos da mesma aplicação.

Quando uma requisição mstart myapp (onde myapp é o nome da aplicação seguida de seus parâmetros) é recebida pelo Broker Global Primário, um BG com menor índice de carga IB é selecionado para iniciar a execução.

O índice IB é definido na equação 57, onde BCap é o somatório da capacidade de todos os computadores do Broker Global, e BCarga é o somatório da carga a ser executada.

O Broker Global envia as informações da aplicação para o Broker Local de menor carga em sua rede, que seleciona um Scheduler, também com menor carga, para enviar a aplicação.

Em seguida, essa aplicação é iniciada e suas threads são convertidas em processos.

Checkpoints são gerados sobre esses processos que têm suas execuções interrompidas.

O Scheduler executa o algoritmo IBL para obter as médias e desvios-padrão de consumo de CPU, memória, disco e rede.

Com as informações do IBL e de todo o ambiente, o algoritmo de balanceamento de carga é executado pelo Scheduler.

O algoritmo de balanceamento retorna uma distribuição de processos que será utilizada para realizar a migração.

Após migrados, os processos são reiniciados e seus comportamentos, monitorados.

As etapas que fazem parte da execução de uma aplicação estão demonstradas.

Diagrama de Seqüência, Iniciando uma aplicação.

Atualmente, há dois algoritmos de escalonamento implementados, Route e RouteGA.

Ambos consideram a vizinhança para a distribuição de processos contudo, somente o RouteGA analisa os resultados gerados pelo algoritmo IBL.

Os valores médios de ocupação obtidos com o IBL são utilizados para parametrizar o algoritmo genético e obter uma boa distribuição de processos, conforme apresentado na seção 542.

Para suportar a migração, utiliza-se uma ferramenta de checkpoint (atualmente o CryoPID) que armazena o contexto dos processos no DFS na forma de arquivos binários, os quais são utilizados para reiniciar os processos em outros computadores.

Após migrar os processos, o módulo Scheduler, responsável pela inicialização, atualiza as informações sobre a localização dos processos na base de dados.

Neste capítulo foram apresentados os componentes do MidHPC e suas relações, destacando as trocas de mensagens para a execução e gerenciamento de aplicações.

O modelo de programação do MidHPC torna a complexidade de escalonamento, de transferência de mensagens e de gerenciamento de ambiente transparentes ao desenvolvedor.

Assim, o desenvolvedor precisa levar em consideração apenas os problemas relativos à aplicação que está desenvolvendo, como gerenciamento de condições de corrida.

Apresentou os módulos do MidHPC e suas interações.

Este capítulo detalha a instalação e os requisitos necessários para realizá-la.

Apresenta, ainda, a montagem de um ambiente, mostrando os arquivos de configuração utilizados e a arquitetura construída.

Finalmente, apresenta a execução de uma aplicação genérica, detalhada passo-a-passo.

Para instalar o MidHPC e executar seus módulos, os seguinte itens são necessários, Linux kernel 2,6,x, PostgreSQL (ou outro banco de dados), Compilador e máquina virtual Java, 4-Compilador C/C++.

Com esses requisitos atendidos, é necessário apenas descompactar o arquivo de instalação e executar um comando make.

Esse comando inicia a compilação de todos os módulos do MidHPC e executa o script de criação da base de dados, atualmente em PostgreSQL (as tabelas da base de dados são detalhadas no apêndice I).

Em seguida, o usuário precisa modificar os arquivos de configuração dos módulos para que se adequem à sua necessidade.

Após configurá-los, é possível iniciar os módulos Broker Global, Broker Local e Scheduler.

Para configurar o ambiente do MidHPC é necessário definir quais computadores serão responsáveis por executar cada módulo.

Além disso, é preciso definir a entrada DNS (Domain Name Server) que servirá de referência para o Broker Global Primário.

A seguir são apresentadas as configurações dos módulos Broker Global, Broker Local, Scheduler e Shell.

O Broker Global necessita que o usuário configure as seguintes informações.
Endereço IP local utilizado.

Porta local.

Nome do Broker Global Primário.

Porta do Broker Global Primário.

Apresenta um exemplo de arquivo de configuração que será utilizado no ambiente exemplo, detalhado na seção 64.

Informações de implementação do Broker Global podem ser encontradas no apêndice A, inclusive a definição de cada um dos itens do arquivo de configuração.

Exemplo de arquivo de configuração de um Broker Global.

O Broker Local necessita que o usuário modifique as seguintes informações, Endereço IP local, Porta local, Endereço IP do Broker Global, Porta do Broker Global.

Desse modo, o Broker Local apenas se conecta ao Broker Global definido.

Detalhes de implementação sobre o Broker Local encontram-se no apêndice B.

Um exemplo de arquivo de configuração do Broker Local é apresentado.

Esse arquivo é utilizado no ambiente exemplo construído.

Exemplo de arquivo de configuração de um Broker Local.

Para o módulo Scheduler, o usuário precisa modificar os seguintes itens, Endereço IP local, Porta local, Endereço IP do Broker Local, Porta do Broker Local.

Assim, o Scheduler irá se conectar apenas ao Broker Local definido no arquivo de configuração.

O apêndice D apresenta mais detalhes sobre o Scheduler.

O arquivo de configuração apresentado é utilizado no ambiente exemplo que é construído na seção 64.

Todas as ferramentas do Shell compartilham o mesmo arquivo de configuração, pois, necessitam comunicar-se apenas com o Broker Global Primário.

Assim, apenas a entrada DNS e a porta para o Broker Global Primário são necessárias no arquivo de configuração.

O apêndice C detalha o módulo Shell e as ferramentas que dele fazem parte.

Exemplo de arquivo de configuração de um Scheduler.

Exemplo de arquivo de configuração de um Shell.

A arquitetura representada foi construída para que testes fossem realizados no ambiente.

Encontram-se informações como endereço IP, porta e nome de cada computador do ambiente.

Arquitetura implementada para testes.

Inicialmente, a aplicação mps é executada.

Como não existe um Broker Global Primário definido, um erro é apresentado.

Executando um mps sem um Broker Global.

Assim, é preciso iniciar um Broker Global para que esse seja definido como Primário.

Iniciando um Broker Global.

O detalhe mostra que o Broker Global Primário estava inativo e que essa função passou a ser exercida pelo Broker Global iniciado (IP 1921680181).

Agora, é possível executar um mps e obter uma resposta.

Com um Broker Global Primário ativo, o mps retorna um resultado.

Porém, não existem Broker Locais ou Scheduler ativos no ambiente.

Continuando com a construção do ambiente, é iniciado um Broker Local.

Executando um mps com um Broker Global ativo.

Iniciando um Broker Local.

Esse Broker Local (IP 19216802) conectou-se ao Broker Global, definido no arquivo de configuração, para requisitar a sua inclusão no ambiente.

Resultado da inclusão de um Broker Local.

Executando novamente um mps, é possível identificar o Broker Local que foi iniciado.

Resultado do mps após a inclusão do Broker Local.

Em seguida, um outro Broker Global (IP 19216806) é iniciado.

Nesse ponto, já existe um Broker Global Primário ativo e, portanto, o registro é requisitado (detalhe 1).

Após o registro, o Broker Global Primário envia a lista de Brokers Globais em execução no ambiente.

Iniciando outro Broker Global.

Resultado da requisição de registro do novo Broker Global.

Executando novamente o mps é possível identificar os dois Brokers Globais, sendo que um deles contém um Broker Local ativo.

Como existem dois Brokers Globais ativos no ambiente, a tabela de RTT é preenchida com a distância (latência em milissegundos) entre eles.

Execução do mps após a inclusão do segundo Broker Global.

Em seguida, mais três Brokers Locais são iniciados.

A disposição desses Brokers Locais segue o arquitetura definida e o resultado de um mps após a inclusão é mostrado.

Resultado de um mps com dois Brokers Globais e quatro Brokers Locais.

Finalmente, é iniciado um Scheduler.

Esse, conecta-se ao Broker Local definido no arquivo de configuração, além disso, inclui suas informações na base de dados e recebe um identificador (sua chave primária).

Iniciando um Scheduler.

O Broker Local, ao receber o pedido de inclusão, insere o Scheduler em sua lista.

Resultado da inclusão de um Scheduler.

Ao executar um mps, o Broker Global mostra que existe um Scheduler em sua rede (detalhe 1) e, mais abaixo (detalhe 2), é mostrada a informação do Scheduler que foi iniciado.

O ambiente completo é composto por dois Brokers Globais, quatro Brokers Locais e oito Schedulers.

Execução de um mps após a inclusão de um Scheduler.

A remoção de um módulo do ambiente é feita através de sua interrupção por meio de um Ctrl+C ou kill <pid>.

Ao ser interrompido, o módulo envia uma mensagem informando sobre sua interrupção.

Inicialmente, um Scheduler é interrompido por um Ctrl+C.

O Scheduler escolhido (IP 19216803) envia a mensagem de interrupção (no detalhe) para seu Broker Local (IP 19216803).

Ao receber a mensagem, o Broker Local remove o Scheduler da sua lista.

Ao executar um mps, é possível notar que o Broker Local de IP 19216803, agora, mostra apenas um Scheduler em sua rede (detalhe 1).

Além disso, a informação referente ao Scheduler foi removida.

Para remover um Broker Local, é necessário que seus Schedulers sejam primeiramente removidos.

Ao interromper um Broker Local, uma mensagem é enviada para o Broker Global definido no arquivo de configuração.

Ao receber a mensagem, o Broker Global remove o Broker Local de sua lista.

Ao executar um mps, o Broker Local removido não mais aparece na lista e o Broker Global mostra que existe apenas um Broker Local em sua lista.

Ao finalizar um Broker Global, uma mensagem é enviada a todos os Brokers Globais.

Semelhante ao Broker Local, para finalizar um Broker Global todos os Brokers Locais e Schedulers devem ser previamente finalizados.

Assim que outro Broker Global recebe a mensagem, ele atualiza sua lista removendo o Broker Global indicado pela mensagem.

Após a remoção do Broker Global, ao executar um mps, é possível ver que apenas 1 Broker Local removendo um Scheduler.

Resultado de um mps após a remoção de um Scheduler.

Interrompendo um Broker Local.

A aplicação a ser executada é composta por uma única thread que executa cálculos e armazena os resultados em um arquivo.

Para que possa ser executada, essa aplicação deve estar em um diretório do ambiente, de modo que todos os computadores possam ter acesso à mesma.

A seguir é detalhado o passo-a-passo da execução dessa aplicação genérica.

A aplicação a ser executada está localizada no diretório /nfs/tabuada.

Esse diretório está numa área que todos os computadores tem acesso por meio de um compartilhamento via NFS.

Mensagem de remoção recebida pelo Broker Global.

Execução de um mps após a finalização do Broker Global.

Localização da aplicação exemplo.

Para iniciar a aplicação, a ferramenta do Shell mstart é utilizada.

A aplicação é denominada /nfs/tabuada/tabuada e possui parâmetro 1.

Executando a aplicação no MidHPC.

Ao executar o comando mstart, o Shell conecta-se ao Broker Global Primário e requisita o início da aplicação.

O Broker Global Primário cria um Gid (identificador global) para a aplicação ao adicioná-la na base de dados.

Em seguida, verifica qual dos Brokers Globais da lista possui menor carga, o escolhido, neste caso, foi o Broker Global 19216806 (figura 631, no detalhe).

Broker Global Primário recebe o pedido de início de aplicação.

O Broker Global selecionado, ao receber o pedido da aplicação, envia a requisição para o Broker Local com menor carga, no caso o 19216807.

Broker Global repassa a aplicação para um Broker Local.

Por sua vez, o Broker Local seleciona o Scheduler com menor carga para iniciar a aplicação, no caso o 19216807.

Broker Local envia a aplicação para um Scheduler.

Ao receber a aplicação, o Scheduler executa o IBL para obter uma predição do comportamento médio da mesma (detalhes sobre o IBL no apêndice H e na seção 42).

Com os dados da predição, o Scheduler inicia o escalonamento de processos para definir onde a aplicação executará.

Nesse caso, foi definido que a aplicação deverá executar no computador 1.

Assim, uma mensagem de reinício da aplicação é enviada para o computador escolhido, no caso o 1921680181.

Início da aplicação e envio da mensagem de reinício.

Ao receber a mensagem, o Broker Local a encaminha para o Broker Global.

Broker Local repassa a mensagem para o Broker Global.

Em seguida, o Broker Global modifica a requisição (mudando de restart para start) e a envia para o Broker Global definido na mensagem.

Broker Global encaminha a requisição para o Broker Global correto.

No Broker Global correto, a mensagem é encaminhada para o Broker Local definido.

Do mesmo modo, o Broker Local encaminha a mensagem para o Scheduler.

No Scheduler, a aplicação é reiniciada por meio do checkpoint definido na mensagem.

Nesse momento, o Monitor de Processos inicia a coleta de informações sobre o comportamento (detalhes do Monitor no apêndice E).

Broker Global encaminha a requisição para o Broker Local definido.

Broker Local encaminha a mensagem para o Scheduler.

No Scheduler, a aplicação é reiniciada.

Assim, no mps é possível ver que a aplicação é composta por uma thread, que está execução no computador 1921680181.

Resultado de um mps que mostra a aplicação em execução.

Ao final da execução da aplicação, dados sobre a thread são removidos do ambiente.

O mps mostra que não existem mais threads em execução, indicando o fim da aplicação Execução de um mps após o término da aplicação.

Posteriormente, uma outra aplicação foi executada.

Ela é composta por duas threads que compartilham informações e está localizada em /nfs/p-c-mutex/testmutex.

Apresenta um mps com a aplicação testmutex em execução, mostrando que as threads estão executando nos computares 19216807 e 19216808.

Execução de um mps com uma nova aplicação quem contém duas threads.

O código da aplicação /nfs/p-c-mutex/testmutex é mostrado que apresentam os módulos Producer, Consumer e Main.

É importante salientar o uso das funções de acesso ao DSM para escrita (dsm_read), leitura (dsm_write) e controle de concorrência (dsm_lock e dsm_unlock).

Essas funções serão modificadas em trabalhos futuros de modo a tornar completamente transparente o uso do DSM pelas threads.

Este capítulo mostrou a configuração e montagem de um ambiente exemplo, além da execução de uma aplicação genérica.

Mostrando, passo-a-passo, o resultado de cada comando e as mensagens que são trafegadas.

Trecho de código do testmutex, Producer.

Trecho de código do testmutex, Consumer.

Trecho de código do testmutex, Main.

O desenvolvimento de uma aplicação distribuída pode ser dividido em três etapas, análise, projeto e implementação.

Na análise, o problema e as técnicas computacionais necessárias para resolvê-lo são definidos.

Na etapa de projeto, a modularização é um dos aspecto a ser considerado e os módulos devem ser o mais independentes possível, de modo a permitir execução em paralelo com o mínimo de comunicação e, assim, possibilitar o aumento de desempenho.

A modularização deve considerar as características das tarefas, tais como, processamento, comunicação, memória e acesso a disco.

Na etapa denominada implementação, os módulos são escritos segundo modelos de programação tais como MPI, PVM, OpenMP.

Após a implementação, é possível identificar uma outra fase relevante, a execução.

Durante essa fase, as tarefas da aplicação são escalonadas de acordo com a disponibilidade de recursos e, tradicionalmente, é função do desenvolvedor avaliar o desempenho desses, a fim de definir como o escalonamento será feito.

A complexidade dessa definição é alta, pois é preciso conhecer o comportamento da aplicação durante a execução, assim como o dos recursos computacionais disponíveis, a fim de evitar sua ociosidade em detrimento da sobrecarga de outros.

Uma alternativa para reduzir essa complexidade é o uso de algoritmos de balanceamento de carga, que automatizam e homogenizam a distribuição de processos baseando-se na ocupação de recursos.

Tais algoritmos visam distribuir os processos igualmente entre os computadores do sistema.

A modularização, na etapa de projeto, e a alocação de tarefas, na etapa de execução, são os problemas mais complexos no desenvolvimento e utilização de uma aplicação paralela e distribuída.

Visando abordar tais problemas, o projeto MidHPC (Middleware for High Performance Computing) foi proposto, o qual é composto pelos módulos, Shell, responsável pela submissão de aplicações, fazendo a interface com usuários e desenvolvedores.

Broker, que conecta as redes que compõem o Grid e é responsável por responder as requisições do usuário, e Scheduler, que obtém informações sobre os recursos e sobre a aplicação para tomar decisões sobre o escalonamento.

Além desses três módulos principais, existem outros secundários, tais como o Monitor e o DSM.

O Monitor é responsável por coletar informações para o Scheduler e para a realização de predições sobre o comportamento futuro da aplicação (utilizando a técnica IBL).

A predição possibilita que o MidHPC tome decisões sobre a migração de tarefas para otimizar o uso dos recursos.

O DSM é um suporte a memória compartilhada distribuída que permite a comunicação, de maneira transparente, entre as tarefas da aplicação sendo, também, o responsável por distribuir os dados no Grid de modo a otimizar o acesso a escrita e a leitura.

A solução utilizada no MidHPC para reduzir as dificuldades da modularização e da implementação é utilizar um modelo de programação conhecido pelo desenvolvedor, pois, geralmente, ele precisa adquirir conhecimentos sobre linguagens e bibliotecas específicas para desenvolver aplicações paralelas e distribuídas.

O modelo de programação do MidHPC é baseado no padrão de threads POSIX, para que aplicações legadas possam ser executadas no MidHPC com pouca ou nenhuma adaptação.

Novas aplicações podem ser desenvolvidas utilizando-se conceitos amplamente divulgados, sem a necessidade de adquirir conhecimentos específicos de um modelo ou biblioteca.

Assim, o desenvolvedor precisará se preocupar apenas com sua aplicação e com problemas clássicos da implementação, que envolvem sincronização de variáveis e condições de corrida.

Para simplificar a etapa de execução o MidHPC usa, no módulo Scheduler, o algoritmo de balanceamento de carga RouteGA, o qual se utiliza de conhecimento prévio do comportamento da aplicação para parametrizar um algoritmo genético e, assim, gerar uma distribuição otimizada de processos sobre computadores.

A quantidade de recursos que o RouteGA considera no escalonamento é limitada pela quantidade de comunicação entre os processos, quanto maior a troca de informação entre os processos, menor deve ser a distância entre os computadores em termos de atraso de rede.

Aqueles cujo atraso é menor do que o definido pelo Scheduler, baseado no conhecimento prévio da aplicação, são considerados vizinhos.

Com essa distribuição, o próprio middleware se encarrega de fazer a migração dos processos na vizinhança e, mesmo em máquinas diferentes, a comunicação não é alterada devido ao uso do DSM.

Além de simplificar o desenvolvimento e a execução de uma aplicação paralela e distribuída, o MidHPC permite que seus próprios módulos, ou componentes desses, sejam substituídos sem que haja a necessidade de modificar outra parte do middleware.

Por exemplo, é possível trocar o algoritmo de balanceamento de carga, utilizado no módulo Scheduler, do RouteGA para o Route (Mello, 2006 a,b) e manter os outros módulos inalterados.

Essa flexibilidade ainda permite que o MidHPC utilize vários algoritmos de balanceamento de carga diferentes ao mesmo tempo, desde que em módulos Scheduler diferentes.

O único requisito para que essa troca seja realizada é que o novo módulo atenda ao protocolo de mensagens do MidHPC.

A realização do presente trabalho contou com a colaboração de Rodrigo Fernandes de Mello, desenvolvendo os módulos referentes a Interceptação de Threads, Monitor e IBL (em Java).

Evgueni Dodonov colaborou desenvolvendo os módulos de DSM e Monitor.

Finalmente, o Prof Dr Luciano Senger, da Universidade Estadual de Ponta Grossa Paraná, colaborou com o desenvolvimento do módulo IBL (em C).

O Algoritmo de escalonamento RouteGA foi desenvolvido em colaboração com Rodrigo Fernandes de Mello e Luciano Senger.

O desenvolvimento do MidHPC abordado neste trabalho de mestrado pode ser ampliado, aprofundando-se nos temas a seguir enumerados.
Pesquisas relativas à segurança do ambiente, especialmente no que se refere ao acesso ao ambiente e compartilhamento de dados e informações.

Desenvolvimento de outros algoritmos de escalonamento de processos ou o aprimoramento dos já existentes.

Melhoria da interface com o usuário, pois a atual é composta apenas por aplicações de linha de comando.

Análise de desempenho do middleware em ambiente real, utilizando diversos algoritmos de escalonamento de processos.

Comparação dos resultados experimentais do RouteGA e do Route (Mello, 2006 a,b) com os resultados obtidos em ambiente real.

Sistemas de extração, classificação e predição de comportamento das aplicações (em estudo pelo aluno Evgueni Dodonov).

Distribuição e replicação de dados, tanto das informações do Broker quanto dos dados do usuário.

Utilização de uma base de dados distribuída para permitir a replicação de informações e melhorar o desempenho no acesso a dados em locais distintos do Grid.

Estudos para verificar a possibilidade de rebalanceamento periódico de cargas.

O módulo Broker é utilizado para interligar transparentemente redes de computadores e construir uma plataforma de Grid.

Esse módulo é composto por dois gerenciadores distintos, BG (Broker Global) e BL (Broker Local).

Vários BLs podem estar ligados a um único BG.

Este apêndice trata especificamente do Broker Global.

A conexão entre Brokers através da Internet é controlada pelo gerenciador BG.

Informações sobre aplicações em execução no Grid e sobre os BLs são armazenadas nesses gerenciadores que também armazenam réplicas de informações de outros Brokers Globais e a localização dos mesmos.

Essa replicação mantém o ambiente em execução quando qualquer Broker Global falha.

Somente aplicações nos BLs do Broker que falhou são interrompidas.

Requisições de usuário, originadas do Shell, são atendidas por um gerenciador BG.

Essas requisições iniciam aplicações ou obtém informações presentes em quaisquer dos Brokers Globais no ambiente.

Um BG gerencia vários BLs e sumariza algumas de suas informações, que são médias e desvios-padrões das seguintes características, capacidade em MIPS, uso e performance de memória (principal e swap).

Essas informações são replicadas entre BGs para tolerância a falhas.

Este apêndice está dividido nas seguintes seções, Interações.

Troca de mensagens.
Mensagens de saída.
Mensagens de entrada.
Interações SQL.
Arquivo de configuração.

Apresenta as interações do Broker Global com os demais módulos do MidHPC.

Essas interações são descritas na tabela A1.

Interações do Broker Global com outros módulos.

Descrição das interações do Broker Global com outros módulos.

Apresenta as mensagens que são trafegadas pelo Broker Global.

Essas mensagens são detalhadas nas seções seguintes.

Como o Broker Global Primário é apenas um Broker Global que é designado pelo nome DNS (bg_root, no arquivo de configuração), as mensagens trocadas com o Primário serão mostradas apenas na seção de mensagens de saída (seção A4).

Interações do Broker Global com outros módulos mensagens.

A seguir, são detalhadas as mensagens que são originadas no Broker Global com destino a outros módulos.

O módulo Broker é utilizado para interligar transparentemente redes de computadores e construir uma plataforma de Grid.

Esse módulo é composto por dois gerenciadores distintos, BG (Broker Global) e BL (Broker Local).

Vários BLs podem estar ligados a um único BG.

Este apêndice trata especificamente do Broker Local.

O gerenciador BL é responsável por oferecer e controlar o acesso ao Grid por recursos em uma rede local.

Esse gerenciador armazena uma lista com informações sobre todos os recursos que executam o Scheduler, o que é um requisito para participar do Grid.

Essas informações incluem capacidade em mips, carga, throughput de leitura/escrita em disco e memória.

Este apêndice está dividido nas seguintes seções, Interações, Troca de mensagens, Mensagens de saída, Mensagens de entrada, e Arquivo de configuração.

B2 Interações.

Apresenta as interações do Broker Local com os demais módulos do MidHPC.

Essas interações são descritas na tabela B1.

Interações do Broker Global com outros módulos.

Descrição das interações do Broker Local com outros módulos.

Apresenta as mensagens que são trafegadas pelo Broker Global.

Essas mensagens são detalhadas nas seções seguintes.

Interações do Broker Local com outros módulos mensagens.

A seguir, são detalhadas as mensagens que são originadas no Broker Local com destino a outros módulos.

Para iniciar uma aplicação paralela, o usuário precisa apenas copiar os binários e bibliotecas para o sistema de arquivos distribuídos, sobre o qual está a imagem do sistema.

Depois disso, o usuário inicia a aplicação executando o comando mstart e o MidHPC trata da distribuição das tarefas.

O Shell também apresenta ferramentas semelhantes ao kill e ao ps, encontradas no Unix, para auxiliar no gerenciamento de processos e do ambiente.

Este apêndice está dividido nas seguintes seções, Interações, Requisições, e  Arquivo de configuração.

Apresenta as interações do Shell com os demais módulos do MidHPC.

Essas interações são descritas na tabela C1.

Interações do Shell com outros módulos.

Descrição das interações do Shell com outros módulos.

A seguir, são detalhadas as requisições de usuário que são feitas através do Shell.

Essas requisições são geradas pelos comandos, mkill, mps e mstart.

O módulo Scheduler é responsável por tomar decisões sobre o balanceamento de carga.

O primeiro algoritmo implementado foi o Route.

Depois disso, o conceito de módulos de otimização foi desenvolvido e o RouteGA foi o primeiro módulo implementado, que utiliza algoritmos genéticos para otimizar a distribuição de processos.

Este apêndice está dividido nas seguintes seções, Interações, Troca de mensagens, Mensagens de saída, Mensagens de entrada, Interações SQL, Aplicações e comandos, Arquivo de interação, pc xml.

Arquivo de interação, scheduling xml.

Arquivo de interação, config_ibl xml.

Arquivo de interação, environment xml, e  Arquivo de configuração.

Apresenta as interações do Scheduler com o os seguintes módulos do MidHPC, Broker Local, IBL, RouteGA e Monitor de Sistema.

Essas interações são descritas na tabela D1.

Interações do Scheduler com outros módulos (parte 1).

Descrição das interações do Scheduler com outros Módulos (parte 1).

Apresenta as interações do Scheduler com, Monitor de Processos, Check-point, Camada de Interceptação e com o Benchmark.

Essas interações são descritas na tabela D2.

Apresentam as mensagens que são trafegadas pelo Scheduler.

Essas mensagens são detalhadas nas seções seguintes.

Interações do Scheduler com outros módulos (parte 2).

Descrição das interações do Scheduler com outros módulos (parte 2).

Interações do Scheduler com outros módulos mensagens (parte 1).

Interações do Scheduler com outros módulos mensagens (parte 2).

O MidHPC utiliza dois monitores diferentes pra capturar informações.

O Monitor de Sistema captura informações sobre o sistema como um todo (considerando todas as aplicações, inclusive aquelas que não fazem parte do Grid) enquanto que o Monitor de Processos colhe informações sobre um único processo.

Este apêndice está dividido nas seguintes seções, Interações, Monitor de Sistema, Monitor de Processos, e  Interações SQL.

Apresenta as interações do Scheduler com o os seguintes módulos do MidHPC, Broker Local, IBL, RouteGA e Monitor de Sistema.

Essas interações são descritas na tabela E1.

Ao iniciar o Monitor de Sistema, o Scheduler envia sua identificação (chave primária da tabela computer).

Assim, os dados podem ser armazenados com sua referência correta.

Iterações dos Monitores com outros módulos Descrição das interações dos Monitores com outros módulos Esse monitor, periodicamente, registra o estado do sistema e considera todas as aplicações que possam estar executando (pertencentes ou não ao Grid).

Os dados obtidos pelo Monitor de Sistema são (representam cada linha na base de dados), Tempo decorrido desde o início do monitoramento.

Tempo usado pelo processador como usertime.

Tempo usado pelo processador como systemtime.

Percentual do tempo gasto pelo processador como usertime (userload).

Percentual do tempo gasto pelo processador como systemtime (systemload).

Valores máximos de userload e systemload.

Tráfego de rede em ambas as direções (entrada e saída).

Uso do disco, throughputs de leitura e escrita.

Ao iniciar o Monitor de Processos, o Scheduler envia sua identificação (chave primária da tabela computer) e a identificação do processo (chave primária da tabela process), assim os dados podem ser armazenados com suas referências.

Diferente do Monitor de Sistema, esse monitor registra o estado de um único processo.

Os dados obtidos pelo Monitor de Processos são (representam cada linha na base de dados), Hora atual do sistema.

Identificador do evento.

Tempo usado pelo processo no processador como usertime.

Tempo usado pelo processo no processador como systemtime.

Pagefaults e Pagereclaims (falhas de memória).

Uso do disco (leitura e escrita).

Os endereços de memória de onde os dados foram lidos ou escritos.

Quantidade total de bytes lidos ou escritos.

A descrição do evento, isto é, read, write, open, close.

O MidHPC utiliza ferramentas de benchmark para identificar as características de computadores que executam o módulo Scheduler.

Toda vez que o Scheduler inicia, ele verifica se o arquivo que contém as informações sobre o PC (por exemplo, pc xml) existe.

Caso esse arquivo não exista as ferramentas de benchmark são utilizadas para criar esse arquivo.

As ferramentas de benchmark são definidas no arquivo de configuração do Scheduler.

O MidHPC intercepta as chamadas de sistema para a criação de threads (pthread_create) para que seja possível criar processos ao invés de threads.

Isso é feito para permitir a migração das tarefas.

Threads, agora convertidas em processos, comunicam-se através de um arquivo localizado no DSM que atua como a memória.

Assim, toda vez que uma thread modificar um valor na memória, essa modificação é feita nesse arquivo.

O DSM deve executar sobre um sistema de arquivos distribuídos (DFS) para permitir o acesso ao arquivo de memória pelos computadores no ambiente.

O MidHPC utiliza o banco de dados PostgreSQL.

A base de dados do MidHPC é chamada de MIDHPCDB (para referência no postgres) e é composta pelas seguinte tabelas, Computer, Armazena os computadores que executam (ou executaram) o módulo Scheduler.

RTT, A distância em milissegundos entre dois Brokers Globais.

Application, Informações sobre aplicações, armazena as cargas preditas e realizadas.

Process, Processos de aplicações.

Permite verificar em quais computadores uma determinada aplicação executou.
History, Dados obtidos pelo monitor de processos.

System, Armazena informações sobre o computador que executa o Scheduler.

A seguir são detalhadas as tabelas que compõem a base de dados.

Apresentado tipo e descrição de cada campo.

Descrição dos campos da tabela Computer.

Descrição dos campos da tabela RTT.

Descrição dos campos da tabela Application.

Descrição dos campos da tabela Process.

Descrição dos campos da tabela History.

Descrição dos campos da tabela System.

