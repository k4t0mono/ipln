Modelos Ocultos de Markov (Hidden Markov Models, HMM) trata-se de um formalismo Markoviano usado para modelar situações nas quais a fonte geradora dos sinais observados está oculta do observador.

Esse formalismo pode ser usado tanto para estudar a natureza dessa fonte quanto para ajudar a prever observações futuras.

Este trabalho tem caráter introdutório, sendo o escopo do mesmo limitado a modelos discretos tanto no espaço de estados quanto no tempo.

Inicialmente, é feita a fundamentação de modelos Markovianos e Cadeias de Markov, princípio básico para o desenvolvimento do formalismo de HMM.

Em seguida, descreve-se o formalismo propriamente dito e a resolução de uma série de problemas-controle, que auxiliam na calibração do modelo.

O primeiro problema calcula a probabilidade de uma sequência de observáveis através da resolução da parte forward do algoritmo forward-backward, o segundo busca identificar, pelo uso do algoritmo de Viterbi, a sequência de estados mais provável, dada a sequência observada, o último problema-controle, resolvido pelo uso do algoritmo de Baum-Welch, trata de buscar melhores parâmetros para o modelo, otimizando a probabilidade de observação de uma dada sequência.

Restrições adicionais a esse tratamento incluem a forma regular e homogênea da matriz de transição, a finitude do espaço de observáveis, a independência entre observações, e o fato de que toda transição entre estados da Markov embutida emite um observável.

A intenção é aprofundar esse estudo em trabalhos futuros, buscando por uma descrição mais genérica através da eliminação das restrições acima relacionadas.

A grande maioria dos processos envolvendo sistemas reais são tão complexos que mesmo que haja forma analítica de resolvê-los, muitas vezes acaba sendo mais produtivo lançar mão do uso de teoria de probabilidade.

Segundo Reichl, para aplicar teoria de probabilidade ao mundo real, é necessário introduzir o conceito de -variável estocástica-.

Assim, X é dita variável estocástica se seu valor, dentre o conjunto {xi} de possíveis realizações, é determinado pelo resultado de um experimento.

Talvez não seja possível observar diretamente a dinâmica estocástica que rege um dado processo do mundo real, mas muito provavelmente esse processo produz observáveis, também chamados -sinais-, a partir dos quais o sistema pode ser modelado.

Esses sinais podem ou não ser de fonte estacionária (sistema em equilíbrio), ser de natureza discreta ou contínua, tratar-se de sinais limpos ou ruidosos, dentre outras características imagináveis.

Poderíamos encontrar vários motivos para fazer modelagens baseadas em sinais.

Rabiner sugere que uma modelagem desse tipo pode servir para prover descrição teórica de uma ferramenta para processamento de sinais.

Um exemplo de uso seria a otimização de um sinal de audio pela remoção de ruído e distorções de transmissão.

Modelos de sinais também podem ajudar na compreensão de sua fonte, caso não seja possível observar o processo diretamente ou caso o custo dessa observação seja muito alto.

Assim, a fonte pode ser simulada e muito pode-se aprender dessa simulação.

São vários os modelos estocásticos baseados em sinais.

Alguns exemplos são os modelos para processos Gaussianos, processos de Poisson, processos Markovianos e os modelos para processos ocultos de Markov, sobre o qual versa essa monografia.

Encontramos o formalismo deModelos Ocultos deMarkov (HiddenMarkovModels, HMM) sob os mais diversos nomes, dentre eles Processos Ocultos de Markov (Hidden Markov Processes), Fontes Markovianas (Markov Sources), Cadeias de Markov Ocultas (Hidden Markov Chains), Funções Probabilísticas de Cadeias de Markov (Probabilistic Functions of Markov Chains).

O formalismo de Modelos Ocultos de Markov (HMM) é usado para modelar processos que são governados por um processo Markoviano embutido, cuja dinâmica não pode ser diretamente observada.

Esse processo Markoviano evolui no tempo por meio de transições entre seus estados, as quais são responsáveis pela emissão de sinais observáveis.

Todo modelo passa por uma fase de calibração, e para modelos em HMM não poderia ser diferente.

Aborda o assunto por meio da resolução de três problemas fundamentais, organização essa proposta por Jack Ferguson (IDA, Institute for Defense Analysis, USA).

O primeiro problema consiste em, tendo a proposta de modelo em HMM, determinar a probabilidade de observação de uma determinada sequência de sinais.

O segundo problema trata de descobrir qual a sequência de estados mais provável, no contexto desse modelo, que levou à sequência de sinais observados.

E por fim, o terceiro problema trata da calibração propriamente dita, buscando aperfeiçoar os parâmetros do modelo, tendo em vista melhorar as probabilidades de geração, ou emissão, de sinais.

Há vários tutoriais que tratam de HMM.

Contudo, as bases desse estudo sobreModelos Ocultos deMarkov são fundamentadas em duas fontes.

Uma delas é o artigo -A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition-, escrito por Lawrence Rabiner.

A outra fonte é o capítulo sobre -Hidden Markov Models-, do livro -Estatistical Methods for Speech Recognition-, de Frederick Jelinek.

Muitas das idéias desses autores são aqui reproduzidas, dando a essa monografia um caráter de Review.

As fórmulas apresentadas no decorrer do texto são resultado de uma tentativa de unificação entre as notações usadas por esses autores.

As figuras incluídas nessa monografia também foram todas obtidas desses dois trabalhos de Jelinek e Rabiner.

Esse estudo sobreModelos Ocultos deMarkov está organizado da forma que segue.

Num primeiro momento, conceituam-se Cadeias de Markov, fundamental para a compreensão e desenvolvimento de modelagens em HMM.

Após, trata-se de Modelos Ocultos de Markov de forma geral, definindo as bases do formalismo.

O capítulo seguinte aborda com detalhes a resolução dos três problemas fundamentais (ou canônicos) desse formalismo.

O fechamento do trabalho trata das considerações finais desse estudo e trabalhos futuros.

ProcessosMarkovianos são um tipo especial de processos estocásticos que têm aplicabilidade quase universal.

Exemplos de aplicações podem ser encontrados junto à química, biologia, física, informática, entre outros, provavelmente abrangendo todas as áreas do conhecimento humano.

De forma muito clara e precisa, define processos estocásticos de tempo discreto sobre um espaço de estados também discreto e finito.

Seja uma sequência de variáveis estoc ásticas X0, X1,Xt, XT, onde 0 = t = T representa uma ordenação discreta no tempo, definidas para um mesmo espaço de estados discreto e finito.

Se nada mais é dito, a probabilidade conjunta dessas váriáveis estocásticas é dada pela fórmula de Bayes.

Um processo estocástico, tal como descrito pela equação, é dito Markoviano de grau 1 se satisfaz a propriedade, ou seja, dada a sequência temporal de realizações em um processo estocástico, a probabilidade desse processo evoluir para um estado qualquer do espaço de amostras no instante seguinte é dependente única e exclusivamente do estado corrente no qual o sistema se encontra.

Em outras palavras, em um processo Markoviano de grau 1, a probabilidade do próximo passo, de Xt1 para Xt, depende apenas do estado de origem desse passo, Xt1.

Essa propriedade é conhecida como Markov property, ou -propriedade de Markov-, em tradução literal.

O fato de estarmos tratando de processos estocásticos discretos tanto no espaço de estados quanto no tempo não implica em real restrição, trata-se apenas de uma forma de simplificar os cálculos para chegar mais rapidamente à definição de Cadeias de Markov, objetivo dessa seção.

Assim, se desejado for, essas equações podem ser modificadas para representar a probabilidade conjunta e probabilidade condicional (de transição) para o caso de processos em espaço e tempo contínuos, assim como para processos em espaço de estados discreto e tempo contínuo.

De acordo com a grande maioria dos autores, processosMarkovianos1 em espaços de estados discretos são chamados Cadeias de Markov (Markov Chains), podendo esses processos ser tanto de tempo discreto como contínuo.

Reichl restringe o termo para contemplar apenas processosMarkovianos em espaços de estados discretos a tempos discretos.

Independentemente de qual das definições é mais correta, optou-se aqui por tratar apenas de Cadeias de Markov de tempo discreto.

Nesse trabalho, ainda são feitas restrições adicionais quanto à forma da matriz de transição.

Trataremos apenas de matrizes de transição regulares e daremos preferência às homogêneas.

Matriz de transição, A-, é aquela que guarda em suas células as probabilidades de transição definidas para um espaço de estados, sendo que essas probabilidades podem ou não variar com o tempo.

Dizemos que essa matriz é homogênea se A-(t) = A-, ou seja, se a matriz é estacionaria, tendo probabilidades de transição independentes do tempo.

De acordo com Reichl, a matriz A- sera regular se todos os elementos de alguma potência A-N,N inteiro, forem não-nulos.

Cadeias deMarkov governadas por matrizes de transição regulares são ditas ergódicas, isto é, todos os estados são atingíveis através de sucessivas transições.

Sistemas ergódicos tendem à estacionariedade após algum tempo, ou seja, a distribuição de probabilidade nos estados passa a ser constante.

Como lembra Trivedi, dizer que uma matriz de transição chegou à sua forma estacion ária, ou homogênea, não é o mesmo que dizer que o sistema alcançou estacionariedade.

Como vimos, a estacionariedade da matriz diz respeito às probabilidades de transição, enquanto que a estacionariedade do processo diz respeito à probabilidade conjunta das variáveis estocásticas.

A propriedade determina que as probabilidades de transição são todas maiores ou iguais a zero.

A propriedade, por sua vez, mostra que resulta em um a soma das probabilidades de todas as transições partindo do estado si para os estados definidos no espaço de estados S.

Como visto anteriormente, a equação decorre da propriedade de Markov e, portanto, também vale para Cadeias de Markov.

Considere, então, essa equação para a probabilidade conjunta das variáveis estocásticas.

Ao aplicarmos um somatório sobre a variável estocástica X1, obtemos a probabilidade conjunta.

Se dividirmos a equação resultante por P(X0), obteremos a probabilidade condicional, P(X2|X0), do sistema ocupar um determinado estado no instante t = 2 sendo que ocupou algum outro estado no instante t = 0.

A equação, conhecida como equação de Chapman-Kolmogorov, reflete esse raciocínio.

A equação de Chapman-Kolmogorov evidencia a propriedade de Markov, pois a partir dela fica clara a independência entre passos sucessivos na evolução de um sistema governado por uma cadeia de Markov.

Em outras palavras, a probabilidade de transição entre X1 e X2 não é afetada pelo fato de ter sido precedida pela transição entre X0 e X1.

Ou, mais sucintamente, passos sucessivos são estatisticamente independentes.

Redefinindo um dos termos da equação, onde aij é um dos elementos da Matriz de Transição, A-(t).

Como dito anteriormente, no decorrer desse estudo, daremos preferência ao estudo de matrizes de transição independentes do tempo, ou homogêneas A-(t) = A-.

Assim, tendo uma matriz de transição homogênea, podemos ampliar a equação de Chapman-Kolmogorov, para dar conta de um número arbitrário de passos.

Com relação à equação, o número de termos no produto é o número de passos desde o instante t até o instante t'.

A cada par de passos, soma-se sobre todos os possíveis estados intermediários, como feito na equação, até que a sequência de passos seja completada.

Isso é equivalente a elevar a matriz de transição A- à potência (tt), que é justamente o numero de passos, e escolher o valor guardado na célula (i,j) dessa nova matriz.

Essa célula guarda a probabilidade de sair do estado si e chegar ao estado sj em (t't) passos.

Em seu artigo sobre HMM e suas aplicações em reconhecimento de fala, Rabiner usa um ótimo exemplo para ilustrar a aplicação de Cadeias de Markov de forma simples.

O exemplo trata da modelagem do tempo no decorrer dos dias.

Assim, seja a variável estocástica X, que representa o tempo e tem suas realizações definidas no conjunto discreto {S1 = chuvoso, S2 = nublado, S3 = ensolarado}.

Determina-se que as observações são feitas uma vez ao dia, que o resultado obtido será sempre um único desses três estados possíveis, sem combinação entre estados, e que as probabilidades de transição entre esses estados são dadas pela matriz A-.

Dado que o tempo no dia 1 é -ensolarado- (X0 = S3), qual a probabilidade (de acordo com o modelo) de que o tempo para os próximos 7 dias seja -ensolarado, ensolarado, chuvoso, chuvoso, ensolarado, nublado, ensolarado-? Mais formalmente, definimos a sequência de observação O.

Queremos obter a probabilidade de O, dado o modelo onde a notação é usada para indicar a probabilidade inicial de cada estado.

Outra questão de interesse é dado que o modelo está em um estado conhecido, qual a probabilidade dele permanecer nesse estado por exatamente d dias? Essa probabilidade pode ser avaliada como sendo a probabilidade da sequência de observação O, dado o modelo, a qual é P(O|Model,X0 = Si).

A quantidade pi é a densidade de probabilidade de duração d no estado i.

Essa duração exponencial é característica do tempo de permanência em um estado numa Cadeia de Markov.

A partir de pi podemos calcular o número médio de observações subseqüentes em um dado estado, condicionado ao fato desse ter sido o estado inicial di.

Assim, de acordo com o modelo, o número esperado de dias ensolarados consecutivos é 1/(0,2) = 5 (o atual, mais 4 dias de sol), de dias nublados é 2,5 e de dias chuvosos é 1,67.

No final da década de 1960, Leonard E.

Baum e colaboradores publicaram uma série de artigos lançando as bases para o formalismo de Modelos Ocultos de Markov (Hidden Markov Models, HMM)1.

As primeira aplicações dessa modelagem estavam voltadas para o reconhecimento de fala, sendo os trabalhos de Baker (Carnegie Mellon University, CMU), no começo dos anos 70, pioneiros no uso de HMM.

Na segunda metade da década de 80, HMM foi aplicado em seqüenciamento de DNA, alcançando posteriormente grande importância em todo o campo da bioinformática.

Nas palavras de Rabiner, na maioria dos processosMarkovianos, cada estado corresponde a um observável do sistema.

Para esclarecer a idéia, consideremos o exemplo sobremodelagem do tempo, introduzido no capítulo 2, sobre Cadeias de Markov.

Ao verificar a condição do tempo em um determinado dia, o observador obterá diretamente um dos estados da Markov como resposta, {S1 = chuvoso, S2 = nublado, S3 = ensolarado}.

Por outro lado, Modelos Ocultos de Markov são usados na modelagem de processos Markovianos que geram observáveis de forma indireta, em função das transições entre os estados da cadeia de Markov que governa o processo, mas que não pode ser diretamente observada.

Em outras palavras, a evolução da cadeia deMarkov está escondida do observador.

Em comparação à proposta anterior de modelagem do tempo por Cadeias de Markov, uma possível modelagem em HMM poderia tratar da observação do comportamento de um trabalhador em sua forma de transporte ao trabalho.

Esse trabalhador se locomove de bicicleta ou taxi em função do tempo ou de sua previsão.

Geralmente vai ao trabalho de bicicleta, mas costuma pegar taxi em dias chuvosos.

Assim, se esse trabalhador foi trabalhar de bicicleta em um determinado dia, há uma probabilidade maior de que o dia esteja ensolarado do que chuvoso, mas ainda assim pode se tratar de um dia de chuva.

Assim, a diferença fundamental entre HMM e o resto dos formalismos Markovianos está na forma de se observar o sistema.

Enquanto que na maioria dos processos Markovianos a observação é direta, pois os observáveis são os próprios estados, em HMM a observação é indireta, feita por inferência, pois os observáveis são funções probabilísticas dos estados da Markov ou das transições entre esses estados.

No contexto desse trabalho, Modelos Ocultos de Markov são definidos como modelos Markovianos onde a observação da evolução do sistema se dá de forma indireta, como função probabil ística da transição entre os estados definidos num espaço de estados discreto e finito.

Por mais que conheçamos todos os parâmetros do modelo3, continua oculta a evolução da Markov que governa esse processo.

Em outras palavras, não se sabe qual o caminho ou sequência de passos exatos que levaram a uma determinada observação.

Jelinek cita três possíveis definições para o formalismo deModelos Ocultos deMarkov.

A primeira delas, adotada pelo próprio autor, trata dos observáveis como função das transições entre os estados daMarkov oculta.

Outra definição, adotada por Rabiner, trata dos observáveis como função dos próprios estados da Markov.

A terceira definição, muito usada em modelos acústicos, remove a restrição quanto à finitude de Y.

Em se tratando de mais uma variante de processos Markovianos, onde trabalhamos com uma cadeia de Markov que está escondida do observador, todas as equações que valem para Cadeias de Markov também valem para Modelos Ocultos de Markov.

Como observado por Jelinek, se o espaço de estados da Markov não for muito grande, podemos usar autômatos para analisar graficamente as relações entre os estados, suas transições e os observáveis gerados.

Suponha, então, a Cadeia de Markov de três estados mostrada.

Algumas transições não são mostradas em , significando que a21 = a22 = a33 = 0.

A HMM correspondente, de três estados e observáveis y, pode ser vista em.

Em temos a representação da HMM evidenciando os observáveis gerados em função da ocorrência de transições entre os estados da Markov que governa o processo.

Seguindo na abordagem adotada por Jelinek no estudo de Modelos Ocultos de Markov (HMM), além de autômatos, usaremos outro artifício gráfico para nos ajudar na compreensão de HMM.

Esse artifício é conhecido como trellis4.

Ele auxilia no cálculo da probabilidade de uma sequência de observáveis P(yk), colocando em evidência a evolução temporal do processo gerador dessa sequência.

O trellis consiste na concatenação de estágios elementares atribuídos, um a um, a cada observável.

Esses -estágios-, mostram as transições entre os estados da Markov que poderiam gerar aquele observável específico.

Voltaremos ao trellis várias vezes no decorrer desse estudo, como forma de ilustrar os problemas canônicos de HMM, abordados no capítulo 4, cujas resoluções são fundamentais na calibração de modelos criados com base no formalismo de Modelos Ocultos de Markov.

Por definição, a modelagem de um sitema físico ou realidade qualquer é uma versão bastante simplificada da realidade propriamente dita.

Dessa forma, não há modelo absoluto, existem apenas modelos mais ou menos adequados para uma dado sistema.

Tendo isso em mente, poderíamos tratar o processo de modelagem como sendo composto de duas etapas, a definição dos parâmetros do modelo e o ajuste do mesmo pela resolução de uma série de -problemas-controle-.

No contexto de modelagens em HMM, há três problemas fundamentais (ou canônicos) a serem resolvidos antes de fazer uso de um modelo.

Esses problemas, responsáveis pelo ajuste fino de um modelo, são os seguintes, sejam os parâmetros do modelo e a sequência de observáveis.

Queremos calcular P, a problabilidade de gerar a sequência O a partir desse modelo.

A maneira mais direta de realizar esse cálculo parte da identificação de cada sequência de estados Q que possa gerar O.

Usando o jargão da área, essa seria a resolução à -força bruta- e, por consequência, tende a ser onerosa, pois dispende mais tempo e poder computacional.

Veremos adiante um algoritmo mais eficiente, mas por ora vamos nos deter à análise dessa resolução -mais direta-.

Para simplificar os cálculos, consideramos que cada transição entre estados qt1 e qt gera um observável Ot.

Uma simplificação adicional é feita ao considerar que, além de ser ergódico, o sistema tem a característica especial, ou seja, transições estão previstas entre quaisquer pares de estados do modelo.

Dadas essas considerações, suponha que O tenha sido gerado pela seguinte sequência de estados Q na qual o índice numérico é um inteiro, 0 = t = T, que indica um instante no tempo.

Assim, q0 significa o estado da Markov no instante t = 0, ou simplesmente o estado inicial.

Assumindo que as observações são estatisticamente independentes entre si2.

Para entender o significado dessa equação, considere uma única sequência de estados Q.

A probalididade da Markov ocupar um dos N possíveis estados no instante t = 0 é dada por q0.

Em t = 1, o sistema sofre transição do estado q0 para o estado q1, gerando o observável O1, de acordo com as propabilidades de transição e de observação, aq0q1 e bq0q1(O1), respectivamente.

Esse procedimento se repete até t = T.

Tendo calculado a probabilidade para uma dada sequência Q, passa-se à próxima, dentre as sequências restantes.

A soma sobre todas as sequências resulta na probalididade do modelo gerar a sequência O de observáveis.

O índice numérico 0 = t = T também pode ser visto como uma coluna do trellis, onde q0 é o estado da Markov na coluna 0 do trellis, q1 é o estado da Markov na coluna 1, e assim por diante.

Na transição entre duas colunas do trellis, um observável é emitido.

Essa dinâmica é mostrada para a sequência de observação {0 1 1 0}, referente ao exemplo apresentado em capítulo anterior para o caso de uma Markov de 3 estados, gerando observáveis y.

Da equação, observamos que existem NT sequências Q de T posições feitas a partir de N estados.

Assim, há NT termos no somatório dessa equação, o que implica em NT1 adições.

Também existem T operações de multiplicação entre os termos aqt-1qt bqt1qt(Ot), sendo que 1 = t = T, e são T1 as multiplicações entre esse conjunto de termos e seus correlatos, desde aq0q1bq0q1(O1) até aqT1qTbqT1qT (OT).

Assim, são (2T1) multiplicações em cada termo do somatório, totalizando (2T1) NT multiplicações.

Portanto, a resolução envolve um total de 2TNT1 operações.

Esse cálculo talvez não seja computacionalmente impossível, mas certamente é muitíssimo oneroso.

Como exemplo, considere um sistema com N = 5 estados e uma sequência de T = 100 observáveis.

Para esse exemplo, a resolução envolve 2 · 100 · 5100 - 1072 operações.

Contudo, como frisa Rabiner, existe um procedimento muito mais eficiente para resolver o Problema 1.

Esse algoritmo é conhecido como forward-backward procedure, do qual precisamos apenas da parte -forward-.

Considere a variável forward, isto é, a probabilidade da observação parcial da sequência de observáveis, de O1 até Ot, conjunta com a probabilidade de ocupação do estado Si da Markov no instante t.

Como estamos trabalhando com sequências em função do tempo, podemos dizer que trabalhamos com conjuntos ordenados de eventos, o que nos permite assumir, por indução, que t vale para qualquer instante de tempo dentro dos limites do problema, 0 = t = T.

Assim, resolvemos o Problema 1 pela aplicação do seguinte procedimento.

Inicialização.
Indução.

Finalização.

A Indução é a parte mais importante desse procedimento, então vamos tentar compreendêla.

O termo t é a probabilidade conjunta da observação parcial O = O1 O2 Ot, ocupação do estado qt = Si.

Ao multiplicar aij e por bij(Ot+1) estamos calculando a probabilidade conjunta de transição do estado qt = Si para o estado qt+1 = Sj, emissão do observável Ot+1 em consequência da transição aij.

Multiplicando, então, os termos t, aij e bij(Ot+1), e somando sobre todos os estados 1 = i = N, obtemos a probabilidade conjunta de - observação parcial O = O1 O2 Ot.

Ocupação do estado qt+1 = Sj, qualquer que tenha sido o estado no instante anterior.

Emissão do observável Ot+1 em consequência de todas as transições com destino a qt+1 = Sj, que nada mais é do que a o valor de t+1(j).

Para finalizar o procedimento, faz-se o somatório de T sobre todos os estados 1 = i = N.

Isso faz todo o sentido quando analisamos a definição da variável forward por ocasião do último instante de observação T.

Essa equação nada mais é do que a probabilidade conjunta da sequência completa de observação com a probabilidade de ocupar o estado Si no instante T.

Dessa forma, ao somarmos a equação sobre todos os estados, obtemos a probabilidade de que um dado modelo gere a sequência de observáveis O = O1 O2 · · · OT, ou seja.

O procedimento inteiro, envolve 2N2T multiplicações e (N1)NT adições, totalizando (3N1)NT operações4.

Nesse momento, cabe comparar a eficiência desse procedimento com relação ao anterior.

Para tal, usamos o mesmo exemplo, com espaço de estados N = 5 e uma sequência de T = 100 observáveis.

Enquanto que o método de resolução por -força bruta- envolve aproximadamente 1072 operações, a parte forward do procedimento forward-backward precisa de 7000 operações, uma diferença de 1069 ordens de grandeza.

Com esse exemplo, não há o que discutir sobre a superioridade do forward-backward nas resolução do Problema 1.

Considere agora a variável backward, ou seja, a probabilidade conjunta de a Markov estar no estado Si no instante t com a probabilidade da observação parcial, Ot+1 Ot+2OT, nos instantes subseqüentes a t.

A parte backward do procedimento forward-backward é muito semelhante ao que acabamos de ver para a parte forward.

Logo, por analogia, segue Inicialização, Indução.

Rabiner não apresenta uma finalização para esse procedimento, contudo, se assumirmos que o modelo tem um determinado estado inicial, Si, com probabilidade P(Si) = 1, podemos dizer que o que buscamos calcular é justamente 0, que é a probabilidade da sequência completa de observação, dado que o estado inicial foi q0.

De acordo com Jelinek, a inicialização apresentada nesse procedimento trata-se de uma questão de convenção.

Para facilitar a compreensão, vamos desenvolver os primeiros termos desse procedimento.

Assumindo que as observações são independentes.

Assim, a sequência de observação está se criando de trás para a frente.

Esse é o problema de achar a sequência ótima de estados associada à sequência de observ áveis.

Rabiner defende que a dificuldade nesse problema é a de se estabelecer um critério de otimização, dentre vários que possam existir.

Assim, a resolução do Problema 2 poderia se dar de diferentes formas, indicando diferentes sequências supostamente ótimas, tudo em função do critério de otimização escolhido.

Já Jelinek nem entra nesse mérito, passando direto ao estudo do algoritmo de Viterbi.

Para ilustrar a dificuldade na escolha do critério de otimização, num primeiro momento, Rabiner resolve o problema adotando o seguinte critério, a cada instante t escolhe-se o estado individualmente mais provável.

No desenvolvimento dessa solução, Rabiner usa as definições das partes do algoritmo forward-backward, como veremos adiante.

Agora considere a definição da seguinte variável t.

Por definição, t é a probabilidade de que, dado um modelo e uma sequência completa de observáveis O1 O2 OT, o sistema tenha ocupado o estado Si no instante t.

Essa equação pode ser posta em termos das variáveis forward e backward.

Assim, descobrimos o estado qt individualmente mais provável no instante t através da busca pelo argumento i que retorna o maior valor de t naquele instante.

Contudo, obter o estado mais provável no instante t1 para, em seguida, obter o estado mais provável no instante t não é garantia de termos a sequência parcial {qt1 = Si, qt = Sj} mais provável, pois pode acontecer que a transição entre Si e Sj não esteja prevista.

Assim, Rabiner explica que essa solução não se aplica para o caso em que aij = 0 para algum par (i, j) de estados do modelo.

Realmente, o critério do -estado mais provável no instante t- só faz sentido se o sistema, além de ergódico, tiver aij > 0.

Parece ser, então, mais simples passar direto à resolução pelo algoritmo de Viterbi (abordagem adotada por Jelinek), pois ele comtempla apenas transições possíveis, não apresentando o problema que acabamos de ver.

Segundo Forney, o algoritmo de Viterbi, proposto em 1967 e desde então usado em uma grande gama de aplicações, é uma solução ótima recursiva para o problema de estimar a sequência de estados de um processo Markoviano de estado finito e tempo discreto.

De acordo com Rabiner o critério mais usado para a resolução do Problema 2 é o de achar a melhor, ou mais provável, sequência completa de estados, e o algoritmo de Viterbi seria a técnica formal usada em vista desse critério.

Ora, esse critério é na verdade o enunciado do Problema 2, o que justifica passar direto ao algoritmo de Viterbi.

Ainda, achar a sequência de estados mais provável, Q = q1 q2 · · · qT, dada a sequência de observação O = O1 O2 · · · OT, ou mais formalmente, maximizar P é equivalente a maximizar P, pois ambas as operações de maximização vão devolver a sequência de estados mais provável.

Para guardar a sequência de estados, usamos um vetor auxiliar t(k) que guarda em cada posição t o índice j do estado qt1 = Sj que maximiza a sequência até o estado qt = Sk.

Mostra a sequência mais provável para cada um dos estados finais, de acordo com o algoritmo de Viterbi, para um dado modelo e sequência O de observáveis.

Assim, a sequência mais provável que leva ao estado 1 é {1 2 3 1 1}, a sequência mais provável que leva ao estado 2 é {1 2 3 1 2}, e aquela levando ao estado 3 é {1 2 3 2 3}.

Ao invés de prover explicações formais para cada uma das equações que compõem o algoritmo de Viterbi, vamos explicar informalmente, através do uso do trellis, como funciona esse algoritmo.

Para facilitar, useremos a notação (estado)coluna para indicar em que coluna está o estado do qual falamos.

Não vamos recriar as sequências que levam aos três estados da última coluna do trellis, e sequer vamos calcular uma sequência completa.

Para ilustrar o método, basta nos atermos ao estado 22.

Assim, dois possíveis caminhos levam a 22, são eles {1 1 2} e {1 3 2}.

O estado 11 só pode ter sido precedido pelo estado 10, então atribuímos -peso 1- a essa transição, enquanto que o estado 31 pode ter sido precedido tanto por 10 quanto por 20, assim, atribuímos -peso 0,5- a cada uma dessas transições.

O estado 22 pode ter sido precedido tanto por 11 quanto por 31, e mais uma vez atribuímos -peso 0,5- a cada uma das transições.

Se multiplicarmos esses pesos, teremos o valor 0,25 para a sequência parcial {1 3 2} e o valor 0,5 para a sequência parcial {1 1 2}, fazendo desta última a sequência parcial mais provável.

Como dito anteriormente, mostra a sequência mais provável para cada um dos estados finais (coluna 4 do trellis).

Essas sequências são {1 2 3 1 1}, {1 2 3 1 2} e {1 2 3 2 3}.

Para descobrir qual dentre essas três é de fato a mais provável, basta seguir o procedimento recém explicado e obteremos probabilidades iguais para as duas primeiras sequências, sendo que a última é menos provável que as anteriores.

O algoritmo de Viterbi resolve esse problema escolhendo arbitrariamente uma dentre as duas sequências igualmente prováveis.

Dentre os três Problemas Canônicos, este é de longe o mais difícil de resolver, pois não existe método analítico que permita obter os parâmetros que maximizam a probabilidade de um modelo gerar a sequência completa de observáveis, P.

No entanto, existe um algoritmo capaz de maximizar a probabilidade local.

Esse algoritmo, de acordo com Jelinek é citado na literatura sob diferentes nomes, tais como algoritmo de Baum, Baum-Welch ou algoritmo forward-backward.

Passemos então ao método.

Se fizermos o somatório de t sobre o tempo de observação, T, obteremos a estimativa do número de vezes que o estado Si é visitado em todo esse período.

Se quisermos saber o número de transições a partir de Si, basta levar o somatório até o instante T1.

Analogamente, ao fazer o somatório de t(i, j) até T1, obtemos a estimativa do número de vezes que ocorreram transições entre os estados qt-1 = Si e qt = Sj.

Usando essas fórmulas, podemos usar o seguinte método para reestimar os parâmetros de um modelo.

Esse trabalho tratou das bases do formalismo de Modelos Ocultos de Markov (Hidden Markov Models, HMM), como fundamentado por Rabiner e Jelinek.

Antes de mais nada, conceituou-se Cadeias de Markov, visto que há sempre uma Markov governando uma modelagem em HMM.

Logo após, apresentou-se a definição de HMM, seguida da apresentação dos problemas canônicos que, uma vez resolvidos, permitem fazer os devidos ajustes para finalização da modelagem.

Assim, um dos problemas se dedicava a calcular a probabilidade de um modelo gerar uma sequência de observáveis, outro problema tratou de identificar, dentre as possíveis sequências de estados na Markov embutida, aquela que tivesse a maior probabilidade de gerar uma determinada sequência, o último problema buscava novos parâmetros para o modelo, tentando elevar a probabilidade de geração da sequência observada.

Conduzimos esses tópicos tratando de uma classe muito restrita de problemas, a começar pelo fato de termos trabalhado apenas com modelos Markovianos discretos no espaço de estados e no tempo.

Outras restrições foram quanto à forma da matriz de transição, pois atacamos apenas matrizes estacionárias, regulares e, na sua grande maioria, homogêneas, quanto à finitude do espaço de observáveis e à independência entre observações.

Ainda, não consideramos nesse trabalho a probabilidade de transições entre estados daMarkov não emitirem observáveis.

Essas simplificações são justificadas por esse se tratar de um trabalho de caráter introdutório ao assunto.

Em trabalhos futuros, pretendemos expandir o conceito com o objetivo de criar modelos mais realistas, eliminando as restrições acima enumeradas e aplicando a problemas de interesse.

Dentre as atividades planejadas para a continuação desse estudo, está a identificação de uma forma de relacionar HMMcom Rede de Autômatos Estocásticos (Stochastic Automata Network, SAN), um formalismoMarkoviano muito utilizado em nosso grupo de pesquisa (Performance Evaluation Group, PEG).

