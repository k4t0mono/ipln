Neste artigo analisa-se um sistema de manutenção de máquinas com dois servidores diferentes, tempos até a quebra das máquinas exponencialmente distribuídos e tempos de reparo seguindo uma distribuição do tipo fase com configuração de Cox.

Consideram-se dois modelos que se diferenciam pela possibilidade ou não de se observar as fases das distribuições do tipo fase, um modelo com informação completa e um modelo com informação parcial.

No primeiro caso, modela-se o sistema por um processo markoviano de decisão a tempo contínuo e no segundo por um processo markoviano de decisão com informação parcial.

Sistema de manutenção, distribuição do tipo fase, processo markoviano de decisão.

O crescimento do processo produtivo e a busca da diminuição dos custos de produção têm incentivado o estudo de sistemas de manutenção de máquinas.

Estes sistemas são intrinsecamente estocásticos devido à aleatoriedade inerente aos processos de falhas.

Dentre os sistemas estocásticos, os markovianos merecem um lugar de destaque devido à sua relativa facilidade de tratamento matemático e computacional.

Atualmente, dispõe-se de resultados teóricos e de recursos computacionais que permitem a aplicação dos processos markovianos na modelagem de sistemas do porte de sistemas reais.

Os processos markovianos podem ser considerados a tempo discreto ou a tempo contínuo.

Nos dois casos, deve-se garantir a propriedade da ausência de memória que caracteriza esses processos.

Esta propriedade pode ser resumida em o comportamento probabilístico futuro do processo ser condicionalmente independente do comportamento passado, dado o estado presente.

Alguns sistemas estão sujeitos a um controle dinâmico onde um supervisor deve observar continuamente o estado corrente do sistema e decidir sobre uma ação que influenciará o comportamento futuro do sistema.

Periodicamente ou a cada mudança de estado (instantes de decisão) uma nova ação deve ser escolhida dentre um conjunto de ações disponíveis para cada estado observado.

Uma regra que dita a forma com que as ações devem ser escolhidas ao longo do tempo define uma política de controle para o sistema em estudo.

No caso dos sistemas estocásticos, sob a hipótese da ausência de memória, este tipo de controle dinâmico caracteriza o sistema como um processo markoviano de decisão (PMD).

A modelagem de sistemas por PMD permite a obtenção de uma política de controle que minimiza o custo do sistema.

Nos modelos a tempo contínuo, uma hipótese necessária para se garantir a propriedade de ausência de memória dos processos markovianos é que o tempo de permanência do sistema em cada estado seja aleatório e exponencialmente distribuído.

Geralmente, na literatura sobre modelos estocásticos de sistemas de manutenção, admite-se esta hipótese tanto para os tempos até a quebra de máquinas quanto para os tempos de reparo.

No entanto, principalmente em relação aos tempos de reparo, esta hipótese nem sempre é verificada na prática.

Trabalhando sobre dados de manutenção de centros de usinagens e robôs, Carvalho verificou que, para os dados tratados, a hipótese de tempos até a quebra de máquinas exponencialmente distribuídos foi aceitável mas o mesmo não ocorreu para os tempos de reparo.

Nos casos onde as variáveis aleatórias exponenciais negativas não modelam bem os processos reais, uma boa alternativa é o uso de distribuições do tipo fase (distribuições PH).

Define-se uma variável aleatória do tipo fase como uma variável aleatória positiva que pode representar o tempo até a absorção numa cadeia de Markov a tempo contínuo composta de um número finito de estados transitórios (fases) e um único estado absorvente.

Estas variáveis aleatórias generalizam as variáveis aleatórias exponenciais negativas e podem ser facilmente inseridas em modelos markovianos a tempo contínuo.

Para inseri-las nestes modelos, deve-se acrescentar ao espaço de estados do modelo a informação sobre a fase corrente de cada processo modelado por uma variável aleatória do tipo fase.

Nos modelos markovianos de decisão a tempo contínuo, pode-se utilizar o mesmo procedimento acima.

No entanto, em cada instante de decisão, deve-se observar o estado corrente do sistema e tomar uma decisão em função do estado observado.

Nesses processos, ao se introduzir no espaço de estados as informações sobre a fase corrente das variáveis do tipo fase introduzidas no modelo, pode-se considerar as fases observáveis para se garantir a propriedade markoviana do processo.

Assim, se é possível observar as fases correntes das variáveis aleatórias do tipo fase do modelo e dado que o tempo de permanência do sistema em cada fase é exponencial, este sistema é facilmente modelado por um processo markoviano de decisão a tempo contínuo.

Desde sua formalização, a teoria das distribuições PH tem evoluído continuamente.

Um importante resultado teórico apresentado é que qualquer distribuição PH acíclica, ou seja, que é a soma ou mistura (combinação convexa) de distribuições exponenciais negativas, pode ser reduzida à configuração de Cox.

Este resultado é estendido a qualquer distribuição PH onde os auto-valores da matriz geradora das transições entre os estados transitórios são reais.

Uma distribuição PH de ordem N com configuração de Cox é definida pelos parâmetros onde N é o número de estados transitórios, são as taxas de transições das fases e pi é a probabilidade do processo ir da fase i para a próxima fase, o processo vai diretamente para o estado absorvente.

Note que uma distribuição PH com configuração de Cox é um caso particular da distribuição proposta.

Nesse trabalho, Cox considerou que os parâmetros da distribuição podem ser complexos.

Quando distribuições PH são obtidas através da aproximação de outras distribuições positivas arbitrárias e são inseridas em processos markovianos de decisão, as fases destas distribuições não são observáveis (por terem sido obtidas a partir de um artifício matemático).

Neste caso, a teoria clássica dos processos markovianos de decisão não pode ser aplicada.

Com este trabalho tem-se como objetivo estudar um sistema de manutenção de máquinas considerando os tempos de reparo distribuídos segundo uma distribuição PH com configuração de Cox, a qual, por simplificação, será chamada por todo o texto de distribuição PH.

Generalizam-se trabalhos anteriores dos autores, utilizando-se um modelo baseado em processos markovianos de decisão com informação parcial para a otimização do sistema sem considerar as fases observáveis.

O sistema de manutenção estudado é composto de um número finito de máquinas idênticas na linha de produção, um número finito de máquinas de reserva e uma estação de reparo com dois servidores.

O tempo até a quebra de cada máquina é exponencial e os tempos de reparo seguem uma distribuição PH que depende do servidor que está executando o reparo.

Os servidores se diferenciam por um ser mais rápido do que o outro, em média, e pelo custo de sua utilização.

Deseja-se escolher entre não ativar nenhum servidor, ativar somente o servidor 1, ativar somente o servidor 2 ou ativar ambos os servidores, em função do estado do sistema, de maneira a minimizar o custo médio por unidade de tempo do sistema a longo prazo.

Considerando-se a possibilidade ou não de se observar as fases das distribuições PH, modela-se o sistema de manutenção de máquinas em estudo, respectivamente, por um processo markoviano de decisão a tempo contínuo (modelo com informação completa) e por um processo markoviano de decisão com informação parcial (modelo com informação parcial).

Os processos markovianos de decisão com informação parcial são construídos a partir de processos markovianos de decisão, onde o espaço de estados é particionado e as decisões são tomadas com base apenas no elemento da partição em que se encontra o estado corrente do sistema.

Apresentam um algoritmo do tipo aproximação sucessivas que encontra políticas markovianas sub-ótimas para modelos a tempo discreto (os tempos entre decisões são eqüidistantes) sob o mesmo critério de otimalidade adotado neste trabalho, ou seja, o custo esperado médio a longo prazo.

Este algoritmo por todo o texto será chamado de Algoritmo HL.

A apresentação deste trabalho está dividida em seis seções.

Descreve-se o sistema de manutenção em estudo que é modelado como um processo markoviano de decisão (modelo com informação completa).

Os processos markovianos de decisão com informação parcial e o Algoritmo HL são apresentados.

Modela-se o sistema de manutenção em estudo por um processo markoviano com informação parcial (modelo com informação parcial).

Comparam-se os modelos utilizando-se um conjunto de dados numéricos.

O sistema de manutenção estudado é composto de uma linha de produção com M máquinas idênticas e independentes trabalhando em paralelo e Mr máquinas de reserva idênticas às da linha de produção.

Quando uma máquina na linha de produção quebra, ela é enviada à estação de reparo e substituída imediatamente por uma máquina de reserva, se existir alguma disponível.

A estação de reparo dispõe de dois servidores.

Cada servidor só poderá consertar uma máquina quebrada de cada vez, portanto no máximo duas máquinas serão atendidas simultaneamente.

O servidor não pode ficar ocioso se estiver ativado e houver máquina quebrada esperando reparo.

O tempo até a falha das máquinas em operação é exponencialmente distribuído, as máquinas de reserva não estão sujeitas a falha enquanto não forem utilizadas.

À medida que as máquinas quebram, elas são enviadas à estação de reparo, formando uma fila com disciplina FIFO (first in, first out).

O tempo de reparo de cada máquina executado pelo servidor segue uma distribuição PH de ordem nk com taxas e probabilidades, onde k é o servidor que está reparando a máquina e f a fase de reparo.

Assim, como o reparo executado pelo servidor k tem nk fases de reparo, se este estiver na fase, o reparo terminará logo após o término da fase corrente e com probabilidade pkf a máquina passará para a fase de reparo seguinte.

Supõe-se que o servidor 2 seja mais rápido em média que o servidor 1.

Apresenta-se uma representação do sistema em estudo.

Como o tempo até a falha das máquinas é exponencial e o tempo de reparo tem distribuição PH, que é constituída de fases exponenciais, é possível modelar este sistema segundo um processo markoviano de decisão a tempo contínuo.

Observa-se o estado do sistema nos instantes de quebra de máquina e final de fase de reparo e, com base nesta observação, uma ação deve ser tomada.

Um conjunto de ações forma a política que controla o sistema, ou seja, a política que determina em cada instante de decisão a ativação ou a desativação de cada servidor.

Em cada instante de decisão, o estado do sistema é definido pelo conjunto de valores que são, respectivamente, o número de máquinas na linha de produção, o número de máquinas de reserva, o número de máquinas quebradas, o estado em que se encontra o servidor 1, o estado em que se encontra o servidor 2, a fase de reparo do servidor 1 e a fase de reparo do servidor 2.

O estado de cada servidor k é dado onde D significa desativado, Ao significa servidor ativado mas ocioso (ou imediatamente após um final de reparo) e Ae significa servidor ativado executando um reparo.

A fase de reparo do servidor é definida, onde os valores de 1 até nk significam as fases em curso do reparo executado pelo servidor k, e fk = 0 significa que o servidor k está ocioso ou desativado.

Considera-se que, se o servidor k terminou um reparo, seu estado passa a ser sk = Ao.

Tem-se que ambos os servidores estão ociosos porque não existem máquinas quebradas para serem consertadas.

Significa que um servidor está ocioso porque terminou um reparo e o outro servidor está ocioso pois não existem máquinas para serem consertadas.

Significa que o servidor, cujo estado é Ao, ou seja, sk = Ao, está ocioso porque terminou um reparo.

O comportamento dinâmico do sistema é descrito pelas mudanças de estado ao longo do tempo.

Cada vez que o sistema muda de estado, deve-se observar o novo estado atingido e decidir sobre ativar ou não cada servidor em função do estado observado.

Cada ação é representada por um par ordenado (a1,a2) onde, ak = A significa ativar ou manter ativado o servidor k e ak = N significa desativar ou manter desativado o servidor k.

Por simplificação, em todo o texto, a ação representada pelo par ordenado (a1,a2) será denotada por a1a2.

Considera-se que se todas as máquinas do sistema estão quebradas, ou seja,a ação a = NN não é uma ação possível pois levaria a um estado absorvente.

Considera-se, também, que se não existem máquinas quebradas, uma máquina quebra e a ação a = AA é escolhida, o servidor 2 (mais rápido) tem prioridade para consertar a máquina que quebrou.

A estrutura de custos do sistema inclui um custo de perda de produção dado pelo produto da taxa pela quantidade de máquinas faltando para completar o número máximo M de máquinas na linha de produção, um custo de executar um reparo à taxa quando o servidor k estiver reparando uma máquina.
Um custo de espera no sistema de reparo dado pelo produto da taxa h pelo número de máquinas quebradas no sistema de reparo (incluindo a máquina que está sendo reparada), um custo à taxa cmk de manter o servidor k ativado, e um custo fixo cak de ativar o servidor.

Deseja-se obter uma política estacionária f* que minimize o custo médio por unidade de tempo do sistema a longo prazo.

Para obtenção desta política, utilizou-se o Algoritmo de Iteração de Valores.

Considera-se um processo estocástico onde um supervisor, em instantes de decisão, deve observar o processo, classificá-lo em um estado i pertencente a um espaço de estados E e decidir sobre uma ação pertencente a um conjunto de ações A possíveis para o estado observado.

Este processo de decisão é um processo semi-markoviano de decisão (PSMD) se o tempo até o próximo instante de decisão, o estado em que o sistema estará nesse instante e o custo incorrido até esse instante são aleatórios e dependentes somente do estado observado e da ação escolhida.

Um PSMD deve ser controlado por uma política que escolhe a ação a ser tomada em cada instante de decisão.

Uma política corresponde à aplicação seqüencial de regras de decisão que prescrevem a ação a ser escolhida em cada instante de decisão t.

De forma geral, uma política de controle pode ser escrita.

Consideram-se políticas markovianas estacionárias determinísticas, ou seja, políticas que prescrevem em cada instante de decisão uma ação que depende apenas do último estado observado, sem considerar o comportamento do processo no passado.

Uma política desse tipo é construída a partir de uma regra de decisão, tal que, se o estado observado é i, então uma única ação é escolhida, independente do instante de decisão corrente t.

Uma política markoviana estacionária determinística R, caracterizada pela regra de decisão f, prescreve a ação f sempre que o estado i for observado.

O problema da otimização de um PSMD consiste em obter uma política de controle que otimiza o processo.

No presente trabalho, considera-se como critério para a otimalidade a minimização do custo esperado médio a longo prazo do processo.

Para escrever a expressão deste custo, define-se Z(t) como o custo total incorrido até o instante t.

Para uma política de controle R e para um estado inicial i, define-se o custo esperado médio a longo prazo do processo sob esta política.

Ei corresponde ao operador valor esperado quando o estado inicial é i e a política R é usada.

Mostrou que o limite acima existe e que no caso unichain, ou seja, quando sob a política R o processo possui um único conjunto fechado de estados, o custo gi(R) é independente do estado inicial i e pode ser denotado por g(R).

Para os PSMD, é provado que existe uma política markoviana estacionária determinística R* ótima, ou seja, que minimiza o custo esperado médio do sistema a longo prazo dentro do espaço de políticas de controle possíveis.

Quando o sistema é controlado por uma política markoviana estacionária determinística R fixada, pode-se considerar a cadeia de Markov imersa no processo.

Esta cadeia representa a seqüência de estados visitados pelo processo, ou seja, X0 é o estado inicial do processo e,Xn é o estado atingido pelo processo logo após a n-ésima transição.

Esta cadeia de Markov tem o mesmo espaço de estados do processo original e probabilidades de transição dadas, onde Ri é a ação prescrita pela política R para o estado i.

Se sob a política R, a cadeia de Markov imersa no processo é unichain, então esta possui probabilidades limites.

A expressão do custo de uma política R, em função das probabilidades limites da cadeia de Markov imersa no processo, não é utilizada no Algoritmo de Iteração de Valores considerado neste trabalho para a obtenção de uma política R* de custo mínimo.

Apesar disto, esta é importante porque pode facilmente ser estendida para a obtenção de medidas de desempenho do sistema sob esta política.

De forma geral estas medidas representam o valor esperado médio de -funções de estado- definidas de forma análoga aos custos.

O tratamento computacional do modelo baseado num PSMD apresentado neste trabalho pode ser resumido em obtenção de uma política de controle markoviana estacionária determinística ótima pelo Algoritmo de Iteração de Valores, cálculo das probabilidades limite da cadeia de Markov imersa no processo quando a política de controle ótima é utilizada e cálculo de medidas de desempenho do processo sob a política de controle ótima baseado nas probabilidades limite calculadas.

Mais especificamente, o modelo apresentado neste trabalho baseia-se num processo markoviano de decisão a tempo contínuo (PMDTC).

Os PMDTCs são casos particulares dos PSMD onde o tempo entre decisões sucessivas são exponencialmente distribuídos com parâmetro dependente do último estado observado.

Para estes processos, define-se a taxa de transição do estado i ao estado j quando a última ação escolhida foi A.

A partir das taxas de transição, obtém-se facilmente a taxa total de saída de cada estado.

Note que é o parâmetro da distribuição exponencial negativa que descreve o tempo de permanência no estado i quando a ação a é escolhida.

A partir das taxas de transição, as probabilidades de transição são dadas e o tempo esperado entre transições é dado.

Assim, um PMDTC pode ser caracterizado pelo espaço de estados E, pelos conjuntos de ações A, pelas taxas de transição e pelos custos.

O tratamento matemático e computacional dos PMDTCs é idêntico àquele dos PSMDs.

Na prática, a diferença entre esses dois processos se encontra na forma que são construídos.

Geralmente, para um PSMD, a obtenção das probabilidades de transição pij e dos custos Ci envolve a teoria das renovações e dificilmente os cálculos efetuados para um modelo podem ser diretamente reutilizado num outro modelo.

Por outro lado, as probabilidades de transição e os custos necessários à construção de um PMDTC possuem expressões simples.

A construção de um PMDTC permite o uso direto de uma modelagem por eventos, onde cada taxa de transição está associada a um evento.

Dois eventos podem ocorrer, o final de um reparo ou a mudança de fase de reparo de uma das máquinas que está sendo consertada.

Se ocorrer o final de um reparo, o número de máquinas quebradas diminui de uma unidade, o número de máquinas na linha de produção aumenta de uma unidade e a fase de reparo do servidor que terminou o reparo passa a ser zero.

Caso contrário, apenas o reparo que mudou de fase é acrescido de uma unidade.

O custo esperado entre decisões é composto por todos os custos acarretados ao sistema.

Assim, os custos esperados de perda de produção, de reparo, de espera no sistema de reparo, de manter o servidor 1 ativado, de manter o servidor 2 ativado, de ativar o servidor 1 e de ativar o servidor 2 incorridos até o próximo instante de decisão.

Suponha que o sistema está no estado i.

Se q > Mr, isto significa que faltam qMr máquinas na linha de produção.

Então, tem-se que o custo esperado de perda de produção é dado pelo produto entre a taxa de perda de produção, o número de máquinas que faltam na linha de produção e o tempo esperado até a próxima decisão.

Se q > 0 e a ação escolhida é a = AN, ou seja, ativa-se o servidor 1 e desativa-se o servidor 2, o custo esperado de reparo é dado pelo produto entre a taxa de execução de reparo do servidor 1 e o tempo esperado até o próximo instante de decisão.

Analogamente, se q > 0 mas escolhe-se a ação a = NA, o custo esperado de reparo é o produto entre a taxa de execução de reparo do servidor 2 e o tempo esperado até o próximo instante de decisão.

Se q > 1 e a = AA, tem-se que o custo esperado de reparo é dado pelo produto entre a taxa de execução de reparo do servidor 1 somada a taxa de reparo do servidor 2 e o tempo esperado até o próximo instante de decisão.

Finalmente, se q = 1 e a ação escolhida é a = AA, ou seja, ambos os servidores são ativados, tem-se que o custo esperado de reparo é o produto entre a taxa de execução de reparo do servidor que está realizando o reparo e o tempo esperado até o próximo instante de decisão.

Se q > 0, ou seja, se existem máquinas quebradas, o custo médio de espera no sistema de reparo é dado pelo produto entre a taxa de espera no sistema, o número de máquinas quebradas no sistema de reparo (incluindo a máquina em reparo) e o tempo esperado até o próximo instante de decisão.

Analogamente, o custo de ativar o servidor 2, Ca2, assume o valor fixo ca2 quando o servidor 2 está desativado e a ação escolhida prescreve sua ativação.

Neste trabalho, para modelar o sistema em estudo, quando não é possível observar as fases das distribuições PH, utilizam-se processos markovianos de decisão com informação parcial.

Nestes processos, o espaço de estados E é particionado em subconjuntos, tal que no instante de decisão a única informação disponível é o subconjunto Ej no qual o estado está contido.

Escolhe-se uma ação, ou seja, para todos os estados pertencentes ao subconjunto Es observado a mesma decisão é tomada.

Isto segue para todos os subconjuntos de estados pertencentes à partição.

A regra de decisão que satisfaz esta condição é chamada markoviana admissível ou, simplesmente, admissível.

Uma regra de decisão markoviana depende apenas do estado corrente, ou seja, ela determina, para cada estado, que ação deve ser escolhida se o estado i for observado no instante de decisão t.

Logo, em cada instante de decisão t, a regra de decisão é um elemento do conjunto, onde é o conjunto de regras de decisão markovianas aleatórias e denota-se por F o conjunto de todas as regras de decisão markovianas determinísticas.

Uma regra de decisão aleatória admissível p e uma regra de decisão admissível determinística f são elementos dos conjuntos, L é chamado período da política.

Note que se L = 1 a política não é realmente periódica, mas uma política estacionária.

Por todo o texto, u será chamado de passo do período.

Para otimizar os processos markovianos de decisão com informação parcial, propõem-se um algoritmo (Algoritmo HL) baseado no Algoritmo de Iteração de Valores.

Ao contrário dos processos markovianos de decisão a tempo contínuo, não se pode garantir a existência de uma política markoviana de custo mínimo para um processo markoviano de decisão com informação parcial pois, como estes processos são um caso particular dos processos markovianos de decisão parcialmente observáveis, a política ótima pode depender de todo o histórico do processo.

Portanto, este algoritmo busca uma -boa- política dentro da classe das políticas admissíveis.

Denotando-se a matriz de transição do processo e o vetor do custo até a próxima transição quando a regra de decisão f é usada, e V o vetor do custo esperado total mínimo.

O Algoritmo HL foi proposto para modelos markovianos de decisão a tempo discreto.

Para utilizá-lo em modelos a tempo contínuo, aplicou-se o método de uniformização.

Dois problemas se apresentam quando se consideram políticas periódicas em modelos a tempo contínuo.

Ao se aplicar o método da uniformização, a equação funcional de um processo a tempo contínuo sob uma política markoviana periódica é dada.

Esta equação funcional pode ser reduzida àquela do processo original somente se os tempos esperados forem iguais para todos os estados do processo.

Neste caso, a equação anterior praticamente se reduz àquela do tempo discreto.

Testes computacionais confirmaram este fato.

Quando existem transições não observáveis, ou seja, transições entre estados de um mesmo subconjunto da partição do espaço de estados, pode ser inviável na prática a implementação da política pela impossibilidade de se saber em que passo do período o sistema se encontra.

No modelo apresentado neste trabalho, tais transições ocorrem quando há mudanças de fase em uma distribuição PH inserida no modelo.

Para evitar os problemas apresentados, se a política obtida pelo Algoritmo HL for periódica, propõe-se a heurística extraem-se todas as políticas estacionárias possíveis a partir das regras de decisão da política periódica obtida, calcula-se o custo de cada uma destas políticas estacionárias, comparam-se os custos e escolhe-se a política estacionária caracterizada pela regra de decisão de menor custo.

A política assim obtida é admissível e será adotada como uma solução sub-ótima para o problema.

Considera-se um modelo com informação parcial em que nos instantes de decisão as informações disponíveis são o número de máquinas na linha de produção, o número de máquinas de reserva, o número de máquinas quebradas, o estado do servidor 1 e o estado do servidor 2.

Não se tem nenhuma informação sobre as fases de reparo do servidor 1 e as fases de reparo do servidor 2.

Para este modelo consideram-se somente políticas admissíveis, ou seja, políticas que determinam a mesma ação para todos os estados de um mesmo subconjunto da partição.

Analogamente ao modelo anterior, neste modelo com informação parcial existe a possibilidade de utilizar os dois servidores, lento e rápido, simultaneamente.

Então, quando o sistema muda de um conjunto da partição para outro deve-se escolher entre não utilizar nenhum servidor, utilizar o servidor lento, o servidor rápido (supostamente mais caro) ou ambos os servidores.

Neste modelo com informação parcial, consideram-se os mesmos custos do modelo com informação completa.

Para se obter a política admissível de custo mínimo utiliza-se o Algoritmo HL.

Considerando-se os dados anteriores, no modelo com informação completa, o custo médio do sistema a longo prazo foi de 181,98 sob a política ótima.

O símbolo utilizado nas tabelas significa que a ação escolhida independe do valor da variável correspondente à coluna.

Por exemplo,se existem de 4 a 12 máquinas quebradas, independente dos valores de s1, s2, f1 e f2, a ação escolhida é AA, ou seja, ativar ambos os servidores.

No modelo com informação parcial, o custo médio mínimo obtido foi de 182,63 sob a política apresentada.

Como esta política é estacionária, não foi necessário utilizar a heurística apresentada.

Este aumento se deve ao fato de no modelo com informação parcial, por não se levar em conta a fase do reparo, utilizou-se mais o servidor 1.

Este servidor, embora mais barato, é mais lento e por isto aumentou-se o número de máquinas quebradas e, conseqüentemente, o custo de perda de produção e o custo de espera no sistema de reparo.

Nesta seção analisam-se os modelos descritos nas Seções, variando-se alguns parâmetros considerados.

Inicialmente, variou-se o número de máquinas de reserva.

Na Tabela são apresentados os custos obtidos para ambos os modelos quando se variou o número de máquinas de reserva de zero (sistema sem máquinas de reserva) até oito (número de maquinas de reserva igual ao número de máquinas na linha de produção).

Como a maior diferença observada entres os custos obtidos para ambos os modelos foi quando se considerou o sistema com duas máquinas de reserva, realizou-se um experimento variando-se a taxa de quebra de máquinas para este sistema.

Dos exemplos apresentados, aquele em que se obteve a maior diferença entre os custos obtidos para o modelo com informação completa e para o modelo com informação parcial foi quando se considerou o sistema com duas máquinas de reserva e taxa de quebra das máquinas igual a 0,3.

São apresentadas as medidas de desempenho para este sistema.

Observando-se os custos incorridos com os diversos experimentos realizados, nos quais vários parâmetros foram variados, nota-se que as diferenças não foram significativas, sendo que no máximo o custo incorrido com o modelo de informação parcial foi 1,89% maior que o incorrido com o modelo com informação total.

O tratamento matemático analítico e computacional de sistemas onde os eventos relevantes ocorrem em intervalos de tempos não exponenciais são muitos complexos.

Para contornar algumas dificuldades de modelagem propõe-se a utilização da distribuição PH com configuração de Cox para modelar estes tempos.

A distribuição PH com configuração de Cox é uma mistura de distribuições exponenciais com diferentes médias.

A grande vantagem da utilização destas distribuições é o fato de elas aproximarem bem qualquer variável aleatória contínua positiva.

Por ser a distribuição PH construída a partir de componentes exponenciais (fases) pode-se modelar sistemas por processos markovianos de decisão, bastando para isto supor que as fases são observáveis e tomar as decisões nos instantes de final de fases.

Em sistemas reais dois casos podem ocorrer as fases são realmente observáveis (por exemplo, no sistema analisado neste trabalho, o reparo seria executado em duas etapas, diagnóstico do defeito e reparo menores e reparos maiores) ou as fases são um artifício matemático para aproximar distribuições não exponenciais negativas.

No primeiro caso, a hipótese de se observar as fases é realística mas no segundo não corresponde à realidade.

A questão que se coloca, portanto, é como tomar decisões em final de fases que não são observáveis.

A solução proposta neste trabalho é modelar o sistema por um processo markoviano de decisão com informação parcial e utilizar o algoritmo apresentado para obtenção da política admissível de custo mínimo.

Quando esta política é periódica, propõe-se uma heurística para obtenção de uma política admissível estacionária.

Deste modo, neste trabalho, considerou-se um sistema que foi modelado admitindo-se que as fases de reparo são observadas, modelo com informação completa, e que estas não são observadas, modelo com informação parcial.

Testes computacionais foram realizados utilizando dados numéricos e observou-se que, no modelo com informação parcial, o custo médio a longo prazo do sistema teve um aumento de no máximo 1,89% quando comparado com o custo obtido no modelo com informação completa.

Este aumento pode ser dependente dos dados considerados e se deve ao fato que as fases não são observáveis.

Finalmente salienta-se que para os experimentos computacionais realizados, utilizou-se uma biblioteca de classes desenvolvida por um dos autores em linguagem C++.

Talvez seja interessante comentar que para obtenção dos parâmetros das distribuições PH várias autores têm utilizado o método dos momentos, o método da máxima verossimilhança e o método da minimização de distâncias.

Os procedimentos baseados em ajustes de momentos podem ser divididos em dois grupos distintos, o método dos momentos tradicional onde se utiliza os momentos de baixa ordem, média e desvio padrão, e as técnicas nas quais momentos de mais alta ordem são ajustados.

O método dos momentos pode ser usado tanto para estimar parâmetros a partir de dados amostrais como para obter parâmetros para uma distribuição PH que aproxime uma distribuição contínua e positiva dada.

O método da máxima verossimilhança foi empregado para estimação de parâmetros a partir de dados amostrais para obtenção de parâmetros a partir de uma distribuição dada, para isso, esta referência se serve da geração de amostras sistemáticas.

É proposto um método de minimização de distância estocástica para obtenção dos parâmetros de uma distribuição PH a partir de uma distribuição dada.

