Nos sistemas que constituem o estado da arte na área de reconhecimento de fala predominam os modelos estatísticos, notadamente aqueles baseados em Modelos Ocultos de Markov (Hidden Markov Models, HMM).

Os HMM-s são estruturas poderosas pois são capazes de modelar ao mesmo tempo as variabilidades acústicas e temporais do sinal de voz.

Métodos estatísticos são extremamente vorazes quando se trata de dados de treinamento.

Deste modo, nos sistemas de reconhecimento de fala contínua e vocabulário extenso, as palavras são geralmente modeladas a partir da concatenação de sub-unidades fonéticas, pois o número destas é bem menor do que o de palavras, e em uma locução geralmente existem vários exemplos de sub-unidades fonéticas.

O reconhecimento de fala contínua difere do de palavras isoladas, pois neste o locutor não precisa fazer pausas entre as palavras.

Deste modo, a determinação das fronteiras entre as palavras e do número destas na locução deve ser feita pelo sistema de reconhecimento.

Para isto são utilizados os algoritmos de busca, que podem ter ainda modelos de duração e de linguagem incorporados.

O objetivo deste trabalho é estudar o problema de reconhecimento de fala contínua, com independência de locutor e vocabulário médio (aproximadamente 700 palavras) utilizando HMM-s.

É investigada a influência de alguns conjuntos de subunidades fonéticas, e dos modelos de duração e de linguagem no desempenho do sistema.

Também são propostos alguns métodos de redução do tempo de processamento para os algoritmos de busca.

Para a avaliação do sistema foi confeccionada uma base de dados formada de 200 frases foneticamente balanceadas, com gravações de 40 locutores adultos, sendo 20 de cada sexo As interfaces via voz estão rapidamente se tornando uma necessidade.

Em um futuro próximo, sistemas interativos irão fornecer fácil acesso a milhares de informações e serviços que irão afetar de forma profunda a vida cotidiana das pessoas.

Hoje em dia, tais sistemas estão limitados a pessoas que tenham acesso aos computadores, uma parte relativamente pequena da população, mesmo nos países mais desenvolvidos.

São necessários avanços na tecnologia de linguagem humana para que o cidadão médio possa acessar estes sistemas, usando habilidades de comunicação naturais e empregando aparelhos domésticos, tais como o telefone.

Sem avanços fundamentais em interfaces voltadas ao usuário, uma larga fração da sociedade será impedida de participar da era da informação, resultando em uma maior extratificação da sociedade, agravando ainda mais o panorama social dos dias de hoje.

Uma interface via voz, na linguagem do usuário, seria ideal pois é a mais natural, flexível, eficiente, e econômica forma de comunicação humana.

Depois de vários anos de pesquisa, a tecnologia de reconhecimento de fala está passando o limiar da praticabilidade.

A última década testemunhou um progresso assombroso na tecnologia de reconhecimento de fala, no sentido de que estão se tornando disponíveis algoritmos e sistemas de alto desempenho.

Em muitos casos, a transição de protótipos de laboratório para sistemas comerciais já se iniciou.

Algumas das principais áreas de aplicação comercial para os sistemas de reconhecimento automático de fala são ditado, interfaces para computadores pessoais, serviços de telefonia automáticos e aplicações industriais especiais.

A principal razão para o sucesso comercial tem sido o aumento na produtividade proporcionado por estes sistemas que auxiliam ou substituem operadores humanos.

Os sistemas de ditado de vocabulário extenso podem ser de dois tipos, ditado irrestrito (por exemplo cartas de negócios ou artigos de jornais) e geração de documentos estruturados (por exemplo, receitas médicas, apólices de seguro, relatórios radiológicos, etc).

Tais sistemas podem ser dependentes do locutor ou adaptativos desde que se espera que geralmente um único usuário irá utilizá-lo por um período extenso de tempo.

Até bem pouco tempo atrás, os sistemas de palavras isoladas predominaram no mercado.

Agora, sistemas de reconhecimento de fala contínua começam a aparecer.

Os vocabulários são de aproximadamente 60000 palavras.

Estes sistemas são projetados para operar em condições favoráveis (por exemplo, em escritórios, com microfones fixos na cabeça do operador e com cancelamento de ruído).

Para aumentar a taxa de acertos, os sistemas de ditado irrestrito contam com modelos de linguagem estatísticos para favorecer palavras ou sequências de palavras mais frequentes.

Os sistemas de domínio específico podem aumentar o seu desempenho incorporando um padrão de documento estruturado para gerar um relatório completo, embora muitas vezes isto exija um processo de planejamento bastante laborioso.

Um sistema de ditado torna-se mais poderoso se possui a habilidade de se adaptar à voz de um determinado usuário (adaptação ao locutor), vocabulário (aprendizado de novas palavras), e tarefas (adaptação do modelo de linguagem).

A fala tende a se tornar uma componente importante na interface com os computadores.

Algumas das possíveis aplicações poderiam ser fala como atalho, ao invés de abrir um arquivo através de vários níveis de hierarquia, o usuário apenas diz -Abra o estoque-.

Recuperação de informação, interfaces gráficas são inconvenientes para especificar recuperação de informações baseada em restrições (-encontre todos os documentos de Fábio criados depois de março-) Computadores de bolso, à medida em que o tamanho dos computadores diminui hoje existem palm-tops minúsculos, teclados e mouses tornam-se cada vez mais difíceis de usar, tornando a fala uma alternativa bastante atraente.

Embora o reconhecimento de fala em computadores seja uma alternativa bastante atraente, as interfaces atuais, teclado e mouse, representam uma alternativa madura e extremamente eficiente.

É improvável que a fala possa substituir completamente estes dispositivos.

Ao invés disso, a nova interface deve combinar estes dispositivos e permitir que o usuário defina qual combinação de dispositivos é a mais adequada para determinada tarefa.

O uso apropriado da fala nos computadores pessoais irá provavelmente requerer o desenvolvimento de um novo conceito de interação com o usuário ao invés de simplesmente modificar as interfaces gráficas existentes.

Uma questão social também está envolvida neste tipo de interface, a dos deficientes físicos.

Com interfaces via voz, pessoas que não têm acesso ao computador por causa de suas deficiências, poderiam utilizá-lo normalmente, permitindo um ingresso ao mercado de trabalho e uma competição em pé de igualdade com as outras pessoas.

O reconhecimento de fala baseado na rede telefônica oferece um potencial enorme por ser um meio de comunicação extremamente difundido.

É também a área tecnicamente mais difícil para o reconhecimento devido à impossibilidade de controle sobre as condições de uso.

Os problemas envolvem uma grande e imprevisível população de usuários, diferenças nos microfones dos aparelhos, a presença de ruído de canal e banda estreita.

Os sistemas mais bem sucedidos são os que se limitam a vocabulários extremamente pequenos, da ordem de 10 a 20 palavras.

Para que um sistema seja útil não é necessário um vocabulário muito grande, alguns sistemas tem um vocabulário de apenas duas palavras (-sim- e -não-).

Além do pouco controle sobre a qualidade do sinal, o reconhecimento através da linha telefônica apresenta problemas devido à expectativa dos usuários que o sistema se comporte como um interlocutor humano.

Dois exemplos clássicos seriam o usuário fala enquanto o sistema ainda está formulando as questões (intromissão), de modo que na hora em que o sistema entra em modo de gravação para coletar a resposta, o usuário já está no meio da resposta ou já terminou de falar, usuário adiciona palavras à resposta, que não estão no vocabulário do sistema (-sim, por favor-).

Neste caso podem ser usadas técnicas de identificação de palavras para conseguir taxas de reconhecimento aceitáveis.

Estes serviços de operação envolvem vocabulários pequenos, diálogo interativo e avisos.

As possíveis aplicações seriam validação de cartões de crédito, compras por catálogo, reservas para hotéis, restaurantes, teatros, passagens aéreas, consultas a telefones e endereços, etc.

Os sistemas de reconhecimento de fala também podem ser utilizados em aplicações mais simples de vocabulário restrito, como o controle de máquinas e dispositivos, abertura e fechamento de portas e válvulas, acendimento de luzes, operações financeiras e outros.

Para muitas aplicações o reconhecimento dependente de locutor é suficiente, desde que um dispositivo particular será utilizado por uma única pessoa durante um período de tempo relativamente extenso, por exemplo um turno de trabalho.

Por outro lado, seria conveniente para algumas aplicações que o sistema pudesse fazer reconhecimento de palavras conectadas, uma vez que uma entrada por palavras isoladas pode ser muito lenta e desconfortável.

Dispositivos de reconhecimento de fala podem ser também utilizados como parte de simuladores, permitindo que um sistema automático substitua um treinador humano.

Outra aplicação possível é a de sistemas de inspeção móvel e controle de inventário, por exemplo no caso de atividades envolvendo microscopia e trabalho em quartos escuros de fotografia.

A cada dia é mais comum ver aparelhos de telefonia celular com discagem por voz (-Ligue-me com o Fábio-).

Estes exemplos significam uma nova era na interação homem-máquina, onde cada vez mais a tecnologia procura criar interfaces que sejam mais naturais ao homem.

Com o amadurecimento da tecnologia de reconhecimento de fala, será possível fazer com que todos estes serviços sejam oferecidos de forma segura e eficiente.

Dentre as várias aplicações citadas para os sistemas de reconhecimento de fala, este trabalho focalizou o problema de reconhecimento de fala contínua, com independência de locutor e vocabulário médio, sendo um caso típico o de editor de texto comandado por voz.

Além do desenvolvimento de um sistema completo para treinamento e reconhecimento, foram estudadas todas as etapas envolvidas no processo, desde o planejamento, gravação e transcrição fonética da base de dados utilizada até a implementação final do sistema.

Também houve a preocupação de se criar um sistema que pudesse ser utilizado por outros pesquisadores, tendo uma interface visual bastante intuitiva e documentação bastante cuidadosa, com o intuito de diminuir o tempo de desenvolvimento e facilitar as pesquisas futuras.

Como contribuições principais deste trabalho pode-se citar a proposta de um conjunto de fones dependentes de contexto consistente e razoavelmente menor do que os trifones propriamente ditos, e a verificação da influência da transcrição fonética das locuções de treinamento no desempenho do sistema.

O estudo de todas as etapas do desenvolvimento de um sistema de reconhecimento também proporcionou uma visão bastante ampla e clara dos problemas envolvidos, e serviu para um melhor direcionamento das linhas de pesquisa.

A tese está organizada da seguinte maneira.

No Capítulo 2 é feito um levantamento dos principais problemas observados na tarefa de reconhecimento de fala, com ênfase especial no problema de reconhecimento de fala contínua, é também apresentada uma visão geral do estado da arte atual para os sistemas de reconhecimento de fala em várias aplicações.

O Capítulo 3 discute a questão das bases de dados, como são feitas, como deveriam ser feitas, as dificuldades de confecção, e finalmente os trabalhos realizados para a confecção da base de dados utilizada neste trabalho.

No Capítulo 4 é apresentada a teoria sobre modelos ocultos de Markov.

O Capítulo 5 trata dos algoritmos de busca com ênfase para o Level Building e o One Step.

O sistema desenvolvido neste trabalho é descrito no Capítulo 6, e os testes e resultados obtidos são apresentados no Capítulo 7.

Finalmente, no Capítulo 8 são feitas as análises sobre os resultados e tiradas conclusões a partir destas.

Também são feitas sugestões para a continuação das pesquisas a partir das deficiências observadas.

O reconhecimento de fala consiste em mapear um sinal acústico, capturado por um transdutor (usualmente um microfone ou um telefone) em um conjunto de palavras.

Os sistemas de reconhecimento de fala podem ser caracterizados por vários parâmetros sendo que alguns dos mais importantes se encontram resumidos.

Parâmetros típicos usados para caracterizar a capacidade de sistemas de reconhecimento de fala.

A fala quando gerada de modo espontâneo é mais relaxada, contém mais coarticulações, e portanto é muito mais difícil de reconhecer do que quando gerada através de leitura.

Os sistemas dependentes de locutor necessitam de uma fase de treinamento para cada usuário antes de serem utilizados, o que não acontece com sistemas independentes do locutor, desde que estes já foram previamente treinados com vários locutores.

O reconhecimento torna-se mais difícil à medida em que o vocabulário cresce, ou apresenta palavras parecidas.

Quando a fala é produzida em sequências de palavras, são usados modelos de linguagem para restringir as possibilidades de sequências de palavras.

O modelo mais simples pode ser definido como uma máquina de estados finita, onde são explicitadas as palavras que podem seguir uma dada palavra.

Os modelos de linguagem mais gerais, que aproximam-se da linguagem natural, são definidos em termos de gramáticas sensíveis a contexto.

Uma medida popular da dificuldade da tarefa, que combina o tamanho do vocabulário e o modelo de linguagem, é a perplexidade, grosseiramente definida como a média do número de palavras que pode seguir uma palavra depois que o modelo de linguagem foi aplicado.

Existem também parâmetros externos que podem afetar o desempenho de um sistema de reconhecimento de fala, incluindo as características do ruído ambiente e o tipo e posição do microfone.

O reconhecimento de fala é um problema difícil devido às várias fontes de variabilidade associadas ao sinal de voz, variabilidades fonéticas, as realizações acústicas dos fonemas, a menor unidade sonora das quais as palavras são compostas, são altamente dependentes do contexto em que aparecem.

Por exemplo o fonema /t/ em tatu tem uma articulação puramente oclusiva, e em tia, dependendo do locutor, pode ter uma articulação africada, onde à oclusão se segue um ruído fricativo semelhante ao do início da palavra -chuva-.

Além disso, nas fronteiras entre palavras, as variações contextuais podem tornar-se bem mais acentuadas.

Variabilidades acústicas, podem resultar de mudanças no ambiente assim como da posição e características do transdutor.

Variabilidades intra-locutor, podem resultar de mudanças do estado físico/emocional dos locutores, velocidade de pronúncia ou qualidade de voz.

Variabilidades entre-locutores, originam-se das diferenças na condição sócio-cultural, dialeto, tamanho e forma do trato vocal para cada uma das pessoas.

Os sistemas de reconhecimento tentam modelar as fontes de variabilidade descritas acima de várias maneiras.

Em termos fonético acústicos, a variabilidade dos locutores é tipicamente modelada usando técnicas estatísticas aplicadas a grandes quantidades de dados de treinamento.

Também têm sido desenvolvidos algoritmos de adaptação ao locutor que adaptam modelos acústicos independentes do locutor para os do locutor corrente durante o uso.

As variações acústicas são tratadas com o uso de adaptação dinâmica de parâmetros , uso de múltiplos microfones e processamento de sinal.

Na parametrização dos sinais, os pesquisadores desenvolveram representações que enfatizam características independentes do locutor, e desprezam características dependentes do locutor.

Os efeitos do contexto linguístico em termos fonético-acústicos são tipicamente resolvidos treinando modelos fonéticos separados para fonemas em diferentes contextos, isto é chamado de modelamento acústico dependente de contexto.

O problema da diferença de pronúncias das palavras pode ser tratado permitindo pronúncias alternativas de palavras em representações conhecidas como redes de pronúncia.

As pronúncias alternativas mais comuns de cada palavra, assim como os efeitos de dialeto e sotaque são tratados ao se permitir aos algoritmos de busca encontrarem caminhos alternativos de fonemas através destas redes.

Modelos estatísticos de linguagem, baseados na estimativa de ocorrência de sequências de palavras, são geralmente utilizados para guiar a busca através da sequência de palavras mais provável.

Outro problema encontrado na tarefa de reconhecimento de fala contínua é o procedimento de decodificação da locução.

Este, em sistemas de reconhecimento de fala contínua com vocabulário extenso, tem um custo computacional elevadíssimo, fazendo com que seja necessário buscar maneiras inteligentes de guiar o processo de busca.

Este tópico será abordado com mais detalhes na seção seguinte.

Atualmente, os algoritmos mais populares na área de reconhecimento de fala baseiam-se em métodos estatísticos.

Dentre estes, dois métodos têm se destacado as redes neurais artificiais (Artificial Neural Networks, ANN) e os modelos ocultos de Markov (Hidden Markov Models, HMM).

Mais recentemente, implementações híbridas que tentam utilizar as características mais favoráveis de cada um destes métodos também têm obtido bons resultados.

Em sistemas de vocabulário pequeno (algumas dezenas de palavras), é comum utilizar-se as palavras como unidades fundamentais.

Para um treinamento adequado destes sistemas, deve-se ter um grande número de exemplos de cada palavra.

Entretanto, para sistemas com vocabulários maiores, a disponibilidade de um grande número de exemplos de cada palavra torna-se inviável.

A utilização de sub-unidades fonéticas, tais como fonemas, sílabas, demissílabas, etc, é uma alternativa bastante razoável, pois agora é necessário ter vários exemplos de cada sub-unidade, e não vários exemplos de cada palavra.

Dois critérios bastante importantes para uma boa escolha de sub-unidades são consistência, exemplos diferentes de uma unidade devem ter características similares.

Treinabilidade, devem existir exemplos de treinamento suficientes de cada subunidade para criar um modelo robusto.

Sub-unidades maiores tais como sílabas, demisssílabas, difones, etc, são consistentes, mas difíceis de treinar, enquanto que unidades menores, tais como os fones, são treináveis, mas inconsistentes.

Uma alternativa que mostrou ser bastante atrativa é a de fones dependentes de contexto.

Estas unidades são bastante consistentes, pois levam em consideração o efeito de coarticulação com os fones vizinhos.

Os fones dependentes de contexto, como o nome sugere, modelam o fone em seu contexto.

Um contexto geralmente refere-se ao fones imediatamente vizinhos à direita e à esquerda.

Um fone dependente do contexto à esquerda é aquele modificado pelo fone imediatamente anterior, enquanto que um fone dependente do contexto à direita é aquele modificado pelo fone imediatamente posterior.

O modelo trifone leva em consideração tanto o contexto à esquerda como o contexto à direita, deste modo, se dois fones têm a mesma identidade mas contextos à esquerda e/ou à direita diferentes, então são considerados trifones distintos.

Estes modelos são em geral insuficientemente treinados devido à sua grande quantidade.

Entretanto, como os modelos de trifones são modelos de fones específicos, podem ser interpolados com modelos de fones independentes de contexto, fones dependentes de contexto à esquerda, e fones dependentes de contexto à direita, que são modelos menos consistentes, mas melhor treinados.

A história dos HMM-s precede seu uso no processamento de voz e somente mais tarde, gradualmente, foi se tornando bem conhecido e usado no campo da fala.

A introdução dos HMM-s no campo da voz é usualmente creditada aos trabalhos independentes de Baker na Carnegie Mellon University e Jelinek e colegas na IBM.

Os HMM-s podem ser classificados em modelos discretos, contínuos e semicontínuos, de acordo com a natureza dos elementos na matriz de emissão de símbolos , que são funções de distribuição.

Nos modelos discretos, as distribuições são definidas em espaços finitos.

Neste caso, as observações são vetores de símbolos de um alfabeto finito de N elementos distintos.

Outra possibilidade é definir distribuições como densidades de probabilidade em espaços de observação contínuos (modelos contínuos).

Neste caso, devem ser impostas fortes restrições à forma funcional das distribuições, de modo a se obter um número razoável de parâmetros a serem estimados.

A estratégia mais popular é caracterizar as transições do modelo através de misturas de densidades que tenham uma forma paramétrica simples (por exemplo Gaussianas ou Laplacianas), e que possam ser caracterizadas pelo vetor média e pela matriz de covariância.

De modo a modelar distribuições complexas desta maneira pode ser necessário usar um grande número destas funções em cada mistura.

Isto pode requerer um conjunto de treinamento muito grande para uma estimação robusta dos parâmetros das distribuições.

Nos modelos semicontínuos, todas as misturas são expressas em termos de um conjunto comum de densidades base.

As diferentes misturas são caracterizadas somente através de fatores de ponderação diferentes.

A idéia de se utilizar um modelo de duração de palavras é penalizar hipóteses levantadas pelo decodificador que estejam fora da duração média (em segundos, por exemplo) da realização de uma dada palavra.

Por exemplo, se o decodificador reconheceu a palavra -casa- e atribuiu a ela uma duração de 20 segundos, obviamente esta hipótese deve ser severamente punida, pois está muito longe da realidade.

Para isto, devemos conhecer a priori a duração média de cada uma das palavras que constituem o vocabulário do sistema de reconhecimento.

Em sistemas dependentes do locutor, esta estimativa pode ser razoavelmente precisa, mas para sistemas independentes de locutor, torna-se um problema sério estimar a duração média de cada palavra.

Além disso, para sistemas com vocabulário grande, a determinação da duração média de cada uma das palavras pode se tornar inviável.

A decodificação é um processo de busca no qual uma sequência de vetores correspondentes a características acústicas do sinal de voz é comparada com modelos de palavras.

De uma maneira geral, o sinal de voz e suas transformações não fornecem uma indicação clara das fronteiras entre palavras nem do número total de palavras em uma dada locução, de modo que a determinação destas é parte do processo de decodificação.

Neste processo, todos os modelos das palavras são comparados com uma sequência de vetores acústicos.

Os algoritmos mais utilizados nesta fase do reconhecimento são todos baseados no algoritmo de Viterbi e, dentre eles, podemos citar Level Building, One Step , Stack Decoding, entre outros.

Estes modelos crescem com o vocabulário, e podem gerar espaços de busca extremamente grandes, o que torna o processo de busca bastante oneroso em termos computacionais, e portanto lento.

Algumas estratégias para diminuir o custo computacional nesta etapa envolvem procedimentos de poda, tais como o Viterbi Beam Search.

Deve-se acrescentar que esta etapa do reconhecimento é responsável por praticamente 100% do esforço computacional de um sistema de reconhecimento de fala contínua e, portanto, é a que determina a velocidade final de reconhecimento.

Um sistema de reconhecimento de fala converte o sinal acústico observado em sua representação ortográfica correspondente.

O sistema faz a sua escolha a partir de um vocabulário finito de palavras que podem ser reconhecidas.

Por simplicidade, assume-se que uma palavra é identificada somente por sua pronúncia 1.

Foi conseguido um progresso dramático na resolução do problema de reconhecimento de fala através do uso de um modelo estatístico da distribuição conjunta p(W,O) da sequência W de palavras pronunciadas e da sequência de informação acústica observada O.

Este método é chamado de modelo de fonte-canal.

Neste método, o sistema determina uma estimativa W$ da identidade da sequência de palavras pronunciadas a partir da evidência acústica observada O usando a distribuição a posteriori p(W|O).

Para minimizar a taxa de erro, o sistema escolhe a sequência de palavras que maximiza a distribuição a posteriori, p(W) é a probabilidade da sequência de n palavras W e p(O|W) é a probabilidade de observar a evidência acústica O quando a sequência W é pronunciada.

A distribuição a priori p(W) de quais palavras poderiam ter sido pronunciadas (a fonte) refere-se ao modelo de linguagem.

O modelo da probabilidade de observação p(O|W) (o canal) é chamado de modelo acústico.

Por exemplo, a palavra -macaco- é considerada uma palavra só, embora possa ter mais de um significado (animal ou objeto).

Para uma dada sequência de palavras, de n palavras, pode-se reescrever a probabilidade do modelo de linguagem, onde w0 é escolhido de forma conveniente para lidar com a condição inicial.

A probabilidade da próxima palavra wi depende da história das palavras que já foram pronunciadas.

Com esta fatoração, a complexidade do modelo de linguagem cresce exponencialmente com o comprimento da história.

De modo a obter um modelo mais prático e parcimonioso, a história de palavras pronunciadas é truncada, de modo que apenas alguns termos são utilizados para calcular a probabilidade da próxima palavra seguir a palavra atual.

Os modelo mais bem sucedidos das últimas duas décadas são os modelos ngram, onde somente as n palavras mais recentes da história são usadas para condicionar a probabilidade da próxima palavra.

O desenvolvimento a seguir refere-se ao caso particular de gramáticas bigrama.

Para estimar as probabilidades bigrama, pode-se usar um grande corpus de textos para estimar as respectivas frequências bigrama, c é o número de vezes que a sequência de palavras, w é observada e c é o número de vezes que w é observada.

Para um vocabulário de V palavras existem V2 bigramas possíveis, o que para um vocabulário de 20000 palavras significa 400 milhões de bigramas.

Muitos destes bigramas não serão observados no corpus de treinamento, e deste modo estes bigramas não observados irão ter probabilidade zero quando se usa a frequência bigrama como uma estimativa da probabilidade bigrama.

Para resolver este problema, é necessário uma estimativa suavizada da probabilidade de eventos não observados.

Isto pode ser feito pela interpolação linear das frequências bigrama e unigram e uma distribuição uniforme no vocabulário.

São estimadas pela razão das contagens bigrama e unigrama apropriadas.

Os pesos da interpolação linear são estimados a partir de dados de validação, maximizando a probabilidade de novos dados diferentes daqueles usados para estimar as frequências n-gram.

O algoritmo forward-backward pode ser usado para resolver este problema de estimação de máxima verossimilhança.

No trabalho de modelamento de linguagem têm sido usadas bases de dados de um milhão a 500 milhões de palavras, correspondendo a vocabulários de 1000 a 267000 palavras distintas, para construir modelos trigrama.

Para gramáticas do tipo bigrama as necessidades são um pouco menores, mas ainda astronômicas.

Na comparação de modelos de linguagem, é importante ser capaz de quantificar a dificuldade que estes impõem ao sistema de reconhecimento.

Um modo de se fazer isso é utilizá-los em um sistema de reconhecimento e determinar qual deles fornece a menor taxa de erro.

Este é ainda a melhor maneira de avaliar um modelo de linguagem, embora seja um método altamente custoso.

Os modelos de linguagem tendem a minorar as incertezas (diminuir a entropia) do conteúdo das sentenças e facilitar o reconhecimento.

Por exemplo, se existem, em média, muito poucas palavras que podem seguir uma dada palavra em um modelo de linguagem, o sistema de reconhecimento terá menos opções para verificar, e o desempenho será melhor do que se existissem muitas palavras possíveis.

Este exemplo sugere que uma medida conveniente da dificuldade de um modelo de linguagem deva envolver alguma medida do número médio de palavras que possam seguir outras.

Se o modelo de linguagem for visto como um grafo, com terminais associados a transições entre palavras, por exemplo, então esta medida estaria relacionada com o fator de ramificação médio em todos os pontos de decisão do grafo.

Grosseiramente falando, esta é a quantidade medida pela perplexidade, formalizada a seguir.

Um modelo estocástico formal de linguagem gera sequências terminais com certas probabilidades.

Estas sequências terminais podem ser vistas como realizações de um processo estocástico estacionário discreto cujas variáveis aleatórias assumem valores discretos.

Estes valores discretos correspondem aos terminais individuais, e o tempo indica simplesmente a posição do terminal aleatório na sequência de palavras.

Por simplicidade, vamos assumir que os terminais correspondam a palavras, e este processo aleatório será indicado por w.

Se existem W palavras possíveis, a entropia associada com este processo aleatório ou -fonte- é dada.

W é uma variável aleatória arbitrária em w se a fonte tem variáveis aleatórias independentes e identicamente distribuídas.

Se não, a entropia é dada onde w1 denota a sequência de variáveis aleatórias, e wN denota a realização parcial, e a soma é tomada sobre todas estas realizações.

Desde que as palavras em um modelo de linguagem não são independentes e nem equiprováveis.

Para uma fonte ergódica, a entropia pode ser calculada utilizando-se uma média temporal.

Na prática, quanto mais longa a sentença (N maior) utilizada para estimar H, melhor será a estimativa, H representa o número médio de bits de informação inerente a uma palavra no modelo de linguagem.

Isto significa que, em média, H(w) bits precisam ser extraídos dos dados acústicos para reconhecer uma palavra.

As probabilidades são desconhecidas e precisam ser estimadas a partir de dados de treinamento (que podem ser vitos como exemplos de produções do modelo de linguagem).

Pode-se mostrar que se w for um processo ergódico.

Embora a entropia forneça uma medida de dificuldade perfeitamente válida, na área de processamento de fala, prefere-se usar a perplexidade, definida para algum N grande.

Para verificar o sentido desta medida, note que se o modelo de linguagem tem W palavras equiprováveis que ocorrem independentemente em qualquer sequência de palavras, segue que a quantidade de entropia em qualquer sequência é dada.

Comparando, pode-se ver que a perplexidade de um modelo de linguagem pode ser interpretada como o tamanho do vocabulário (número de terminais) em outro modelo de linguagem com palavras equiprováveis e independentes, que seja igualmente difícil de reconhecer.

Portanto a perplexidade indica um fator de ramificação médio de um modelo de linguagem modelado por w.

O desempenho dos sistemas de reconhecimento de fala é tipicamente descrito em termos de taxa de erros de palavra, onde N é o número total de palavras no conjunto de teste, S, I e D são o número total de substituições, inserções e deleções, respectivamente.

A última década tem testemunhado um progresso significativo na tecnologia de reconhecimento de fala.

As taxas de erro de palavra caem de um fator de 2 a cada dois anos.

Foi feito um progresso substancial na tecnologia básica, o que levou a vencer as barreira de independência de locutor, fala contínua e vocabulários extensos.

Existem vários fatores que contribuíram para este rápido progresso.

A chegada da era do HMM.

O HMM é poderoso no sentido de que, com a disponibilidade de dados de treinamento, os parâmetros do modelo podem ser treinados automaticamente para fornecer um desempenho ótimo.

Foi feito um grande esforço no sentido de desenvolver grandes bases de dados de fala para o desenvolvimento, treinamento e avaliação de sistemas.

Estabelecimento de normas de avaliação de desempenho.

Até uma década atrás, os pesquisadores treinavam e testavam seus sistemas usando dados coletados localmente, e não foram muito cuidadosos em delinear os conjuntos de treinamento e testes.

Consequentemente, era muito difícil comparar o desempenho dos vários sistemas, e ainda, o desempenho de um sistema era geralmente degradado quando este era apresentado a dados novos.

A recente disponibilidade de grandes bases de dados no domínio público, associada à especificação de padrões de avaliação, resultou em uma documentação uniforme de resultados de testes, contribuindo para uma maior confiabilidade na monitoração dos progressos alcançados.

Os avanços na tecnologia dos computadores influenciaram indiretamente o progresso nesta área.

A disponibilidade de computadores rápidos com grandes capacidades de memória permitiu aos pesquisadores realizar várias experiências em larga escala e em um curto espaço de tempo.

Isto significa que o intervalo de tempo entre uma idéia e a sua implementação e avaliação foi bastante reduzido.

De fato, sistemas de reconhecimento de fala com desempenho razoável podem rodar em microcomputadores comuns em tempo real, sem hardware adicional, um fato inimaginável a alguns anos atrás.

Talvez a tarefa mais popular, e potencialmente mais útil, com baixa perplexidade (PP = 11) é o reconhecimento de dígitos conectados.

Para o inglês americano, o reconhecimento independente de locutor de sequências de dígitos pronunciados de forma contínua e restringido à largura de banda telefônica pode alcançar uma taxa de erro de 0,3% quando o comprimento da sequência é conhecido.

Uma das tarefas de média perplexidade mais conhecidas é a de 1000 palavras chamada de Resource Management, na qual podem-se fazer indagações sobre vários navios no oceano Pacífico.

O melhor desempenho independente de locutor nesta tarefa é de menos de 4%, usando um modelo de linguagem de pares de palavras que limita as palavras possíveis que seguem uma dada palavra (PP = 60).

Mais recentemente, os pesquisadores começaram a estudar a questão do reconhecimento de fala espontânea.

Por exemplo, no domínio do Serviço de Informação de Viagens Aéreas (Air Travel Information Service, ATIS), foram relatadas taxas de erros de menos de 3% para um vocabulário de aproximadamente 2000 palavras e um modelo de linguagem bigrama com uma perplexidade por volta de 15.

Tarefas com alta perplexidade, com vocabulários de milhares de palavras, são destinadas principalmente para aplicações de ditado.

Depois de trabalhar em sistemas de palavras isoladas, dependentes de locutor, por muitos anos, a comunidade tem voltado suas atenções desde 1992 para o reconhecimento de fala contínua para grandes vocabulários (20 palavras ou mais), alta perplexidade (PP » 200), independente de locutor.

O melhor sistema em 1997 conseguiu uma taxa de erro de 9,9% em testes realizados regularmente nos EUA através do Departamento de Defesa.

A linguagem falada é a forma mais natural de comunicação humana.

Sua estrutura é moldada pelas estruturas fonológicas, sintáticas e prosódicas da língua, do ambiente acústico, do contexto em que a fala está sendo produzida (por exemplo, as pessoas falam de maneira diferente em ambientes ruidosos e silenciosos), e do canal através do qual viaja, telefone, microfone, diretamente pelo ar, etc.

A fala é produzida de maneira diferente por cada pessoa, sendo as variações devidas ao dialeto, forma e tamanho do trato vocal, ritmo de pronúncia, entre outros fatores.

Ainda, os padrões de fala são modificados pelo ambiente físico, contexto social, e estado físico e emocional das pessoas.

As tecnologias mais promissoras na área de reconhecimento de fala (redes neurais e HMM-s) utilizam métodos de modelagem estatística que aprendem por exemplos, exigindo conjuntos de dados de treinamento extremamente grandes, que cubram todas estas variações.

O efeito causado por variáveis não modeladas ou mal modeladas (tais como diferenças de canal ou microfones, palavras fora do vocabulário, sub-unidades fonéticas mal treinadas) no desempenho dos sistemas de reconhecimento de fala é devastador.

Deste modo, para fornecer exemplos em número suficiente para que os métodos estatísticos funcionem adequadamente, a base de dados precisa ser extremamente grande e, consequentemente, custosa, tanto em termos de trabalho como em termos financeiros.

Estes altos custos só podem ser arcados por um esforço conjunto de empresas, instituições de pesquisa e agências financiadoras, de modo a evitar duplicação de esforços e distribuir as tarefas.

Para envolver um número maior de agentes neste processo, é necessário que este material não seja direcionado a um sistema ou tarefa específicos, mas atender as necessidades de vários grupos e linhas de pesquisa e desenvolvimento, em diversas áreas do conhecimento, síntese e reconhecimento de fala, estudos fonéticos, estudos linguísticos.

A base de dados foi criada com o mesmo número de locutores (30 homens e 30 mulheres), escolhidos através dos mesmos critérios e gravados em condições acústicas semelhantes, e no mesmo formato.

Ainda, em Portugal, foi criada uma base de dados chamada BD-PUBLICO (Base de Dados em Português Europeu, Vocabulário Largo, Independente do orador e fala Contínua), com aproximadamente 10 milhões de palavras em aproximadamente 156 mil frases, pronunciadas por 120 locutores (60 de cada sexo).

Como não poderia deixar de ser, esta base foi confeccionada através do esforço conjunto de instituições de pesquisa, órgãos governamentais e também empresas do setor privado.

Nos EUA também foi feito um grande esforço neste sentido, e já existem disponíveis no domínio público, várias bases de dados (TIMIT, TI-DIGITS, SWITCHBOARD, etc) para desenvolvimento e teste de sistemas.

A disponibilidade destas bases impulsionou de forma expressiva o desenvolvimento da tecnologia de fala, não só devido ao fato de os centros de pesquisa não terem que criar suas próprias bases de dados, um trabalho por si só extremamente árduo, caro e demorado, como também pela possibilidade de comparar os resultados de cada nova idéia de uma forma estatisticamente significativa.

No caso do Brasil este tipo de consórcio ainda não foi sequer cogitado, e os pesquisadores têm que desenvolver seus trabalhos como os americanos faziam há 20 anos atrás, com bases caseiras e pequenas, que tentam cobrir os fenômenos mais significativos da língua falada, na maioria dos casos sem sucesso.

Os desafios em linguagem falada são muitos.

Um desafio básico está na definição da metodologia, como projetar bases de dados compactas que possam ser utilizadas em várias aplicações.

Como projetar bases de dados que possam ser comparadas em várias línguas, como selecionar locutores para que se tenha uma população representativa em relação a vários fatores, tais como sotaque, dialeto, e modo de pronúncia, como selecionar as frases a serem pronunciadas de modo a cobrir todas as aplicações, como selecionar um conjunto de dados de teste estatisticamente representativo para a avaliação dos sistemas.

Outro desafio é desenvolver padrões para transcrever as locuções em diferentes níveis e entre línguas diferentes, estabelecer conjuntos de símbolos, convenções de alinhamento, definir níveis de transcrição (acústica, fonética, de palavras, e outros), convenções para prosódia e tom, convenções para controle de qualidade das transcrições (por exemplo várias pessoas transcrevendo as mesmas locuções para uma estatística confiável).

Também seria interessante classificar as gravações de acordo com o ambiente em que foram feitas, assim como o canal utilizado ambientes silenciosos ou ruidosos, com música ambiente, gravações feitas através da linha telefônica, etc.

No caso brasileiro, ainda é necessário juntar os esforços para obter pelo menos uma base de dados padrão, para que os pesquisadores possam comparar métodos e resultados, e assim evitar duplicações de esforços.

Com dito anteriormente, não se tem disponível para a língua portuguesa uma base de dados de referência sobre a qual se possa desenvolver e testar o desempenho dos sistemas de reconhecimento de fala, tornando-se necessário confeccionar nossas próprias bases de dados.

Por um lado, isto significa um grande dispêndio de tempo e trabalho, que poderiam ser utilizados na elaboração, desenvolvimento e avaliação de novas idéias.

Por outro lado, o planejamento e a confecção de uma base de dados traz uma compreensão valiosa da forma com que as pessoas interagem com um sistema de reconhecimento de fala.

As variações de pronúncia e qualidade de voz devido à presença de um microfone, condição sócio-cultural, região de origem, estado emocional e até à hora do dia ficam bem claras quando se confecciona uma base de dados relativamente grande.

Os trabalhos de confecção da base de dados consistiram de escolha das frases, escolha dos locutores, gravação das locuções, transcrição fonética.

As frases foram escolhidas segundo o trabalho realizado por Alcaim.

Neste, foram criadas 20 listas de 10 frases foneticamente balanceadas, segundo o português falado no Rio de Janeiro, listadas no Apêndice A.

Nestas listas, contou-se 694 palavras distintas.

O termo foneticamente balanceado, neste caso, significa que a lista de frases gerada tem uma distribuição fonética similar àquela encontrada na fala espontânea.

Esta distribuição foi levantada a partir da transcrição fonética de gravações de inquéritos, obtidas a partir do projeto NURC-RJ (Projeto de Estudo da Norma Linguística Urbana culta na cidade do Rio de Janeiro).

Para a confecção da base de dados, foram selecionados 40 locutores adultos, sendo 20 homens e 20 mulheres.

A maioria dos locutores nasceu no interior do estado de São Paulo, embora alguns sejam nativos de outros estados (Pernambuco, Ceará, Paraná e Amazonas).

A maioria tem o nível superior, e todos tem pelo menos o segundo grau completo.

Um resumo informativo de cada um dos locutores pode ser encontrado no Apêndice B.

Os locutores foram divididos igualmente em 5 grupos, ou seja, 4 homens e 4 mulheres para cada grupo.

Para cada grupo foram designadas 4 das 20 listas da base de dados da seguinte forma, as primeiras 4 listas para o primeiro grupo, as 4 seguintes para o segundo grupo, e assim por diante.

Desta forma, cada locutor pronunciou no total 40 frases, e cada frase foi repetida por 8 locutores diferentes.

Um locutor extra do sexo masculino completa a base de dados.

Este locutor pronunciou todas as 200 frases, repetindo-as 3 vezes.

Estas locuções foram utilizadas para testes com dependência de locutor.

As gravações foram realizadas em ambiente relativamente silencioso, com um microfone direcional de boa qualidade, utilizando uma placa de som SoundBlaster AWE.

A taxa de amostragem utilizada foi de 11,025 kHz, e resolução de 16 bits.

Os dados foram armazenados em formato Windows PCM (WAV).

A transcrição fonética foi feita manualmente para cada locução, utilizando programa de visualização gráfica do espectrograma e forma de onda do sinal, e fones de ouvido para audição da mesma.

É importante frisar que os fones utilizados na transcrição fonética deste trabalho e daquele realizado por Alcaim não são os mesmos.

No presente trabalho foi utilizado um conjunto menor de sub-unidades fonéticas, resultante da fusão de algumas classes propostas em , principalmente entre as vogais.

Mesmo com estas restrições, pode-se observar que, de uma forma geral, o levantamento dos fones a partir da transcrição fonética da base de dados gravada acompanhou a distribuição encontrada.

Entretanto, a comparação da frequência relativa da ocorrência dos fones mostra algumas diferenças significativas, possivelmente decorrentes das variações regionais de pronúncia dos locutores.

Tem-se um histograma comparativo para a ocorrência dos fones em ambos os casos.

Considerando que a maioria dos locutores selecionados para este trabalho tem origem no estado de São Paulo, pode-se considerar que é uma base -paulista-, e como o trabalho do Prof.

Alcaim foi realizado somente com locutores cariocas, pode-se considerar que é uma base -carioca-.

Assim, com ressalvas, pode-se fazer algumas comparações interessantes, a diferença de pronúncia do -s- entre consoantes é bem visível se observarmos os histogramas correspondentes aos fones -s- e -x-.

Sub-unidades acústicas utilizadas na transcrição fonética das locuções, com exemplos e frequências relativas de ocorrência e aquelas encontradas na transcrição fonética da base de dados coletada.

Também são listados os números de ocorrências observados para cada sub-unidade.

A teoria relativa aos modelos ocultos de Markov já é bem conhecida e extensivamente documentada.

Desta forma, neste capítulo são apresentados apenas alguns conceitos básicos e notações importantes para a compreensão das seções posteriores.

Textos com explicações bastante claras e precisas podem ser encontrados.

Em um sistema estatístico de reconhecimento de fala contínua, geralmente as palavras do vocabulário são representadas através de um conjunto de modelos probabilísticos de unidades linguísticas elementares (por exemplo fones).

Uma sequência de parâmetros acústicos, extraídos de uma locução, é vista como uma realização de uma concatenação de processos elementares descritos por Modelos Ocultos de Markov (em inglês, Hidden Markov Models, HMM).

Um HMM é uma composição de dois processos estocásticos, uma cadeia de Markov oculta, relacionada à variação temporal, e um processo observável, relacionado à variabilidade espectral.

Esta combinação provou ser poderosa para lidar com as fontes mais importantes de ambiguidade, e flexível o suficiente para permitir a realização de sistemas de reconhecimento com dicionários extremamente grandes (dezenas de milhares de palavras).

Um HMM é definido como um par de processos estocásticos (X,Y).

O processo X é uma cadeia de Markov de primeira ordem, e não é diretamente observável, enquanto que o processo Y é uma sequência de variáveis aleatórias que assumem valores no espaço de parâmetros acústicos (observações).

Um HMM gera sequências de observações pulando de um estado para outro, emitindo uma observação a cada salto.

Em geral, para o reconhecimento de fala, é utilizado um modelo simplificado de HMM conhecido como modelo left-right, ou modelo de Bakis, no qual a sequência de estados associada ao modelo tem a propriedade de, à medida que o tempo aumenta, o índice do estado aumenta (ou permanece o mesmo), isto é, o sistema caminha da esquerda para a direita no modelo.

São usadas duas formas ligeiramente diferentes para os HMM-s.

Uma delas usualmente (mas nem sempre) utilizada no processamento acústico (modelamento do sinal) emite uma observação no instante de chegada ao estado.

A outra forma, geralmente utilizada em processamento de linguagem, emite uma observação durante a transição.

A forma de estado emissor é também chamada de máquina de Moore na teoria de autômatos, enquanto que a forma de transição emissora é uma máquina de Mealy.

Neste trabalho, seguindo a tendência geral, foi utilizada a forma de Moore.

Na Na tarefa de reconhecimento de fala, geralmente são adotadas duas simplificações da teoria de modelos de Markov, que podem ser formalizadas da seguinte maneira.

Hipótese de Markov de primeira ordem, a história não tem influência na evolução futura da cadeia se o presente é especificado.

Hipótese de independência das saídas, nem a evolução da cadeia nem as observações passadas influenciam a observação atual se a última transição da cadeia é especificada.

Estas duas hipóteses podem ser escritas da seguinte maneira, seja y ÎY a variável que representa as observações e i, j ÎX as variáveis que representam os estados do modelo.

A é a matriz com as probabilidades de transição, B é a matriz de densidades de probabilidade de emissão dos símbolos de saída, e P é a matriz de probabilidades iniciais.

Os HMM-s podem ser classificados de acordo com a natureza dos elementos da matriz B, que são funções densidade de probabilidade.

Nos HMM-s discretos as densidades de probabilidades são definidas em espaços finitos.

Neste caso, as observações são vetores de símbolos de um alfabeto finito de N elementos diferentes.

Outra possibilidade é definir as densidades de probabilidade em espaços de observação contínuos.

Neste caso é necessário impor severas restrições na forma funcional das densidades de modo a ter um número manipulável de parâmetros estatísticos para estimar.

A aproximação mais popular consiste em caracterizar as densidades de emissão do modelo como misturas de densidades base g de uma família G com uma forma paramétrica simples.

As densidades base g ÎG são geralmente Gaussianas ou Laplacianas, e podem ser parametrizadas pelo vetor média e pela matriz de covariância.

HMM-s com este tipo de distribuição são chamados de HMM-s contínuos.

De modo a modelar distribuições complexas desta maneira é necessário usar um grande número de densidades base em cada mistura.

Os problemas que surgem quando o corpus de treinamento não é suficientemente grande podem ser aliviados pelo compartilhamento de distribuições entre emissões de estados diferentes.

Nos modelos semicontínuos, todas as misturas são expressas em termos de um conjunto comum C de densidades base.

Neste caso, as misturas são diferenciadas pelos pesos atribuídos a cada uma das funções base de C.

O cálculo das probabilidades com modelos discretos é mais rápido do que com modelos contínuos, embora seja possível acelerar o cálculo das misturas de densidades aplicando a quantização vetorial nas gaussianas das misturas.

Levando em consideração o grande apetite por exemplos de treinamento dos modelos contínuos e o fato de a base de dados utilizada ser relativamente pequena, optou-se por utilizar a forma discreta neste trabalho.

A estimação dos parâmetros dos HMM-s, como em todos os sistemas estocásticos, é baseada em exemplos de treinamento e é geralmente feita utilizando o algoritmo forward-backward, também conhecido como algoritmo Baum-Welch.

O critério utilizado para a reestimação dos parâmetros é o de máxima verossimilhança ML (Maximum Likelihood), que consiste em aumentar, a cada época de treinamento, a probabilidade a posteriori, ou seja, a probabilidade do modelo gerar a sequência de observações.

Dada uma locução de entrada, um sistema de reconhecimento de fala gera hipóteses de palavras ou sequências de palavras.

Destas hipóteses pode resultar uma única sequência de palavras, uma coleção de n melhores sequências de palavras, ou uma treliça de hipóteses de palavras parcialmente superpostas.

Isto é feito num processo de busca no qual se compara uma sequência de vetores de características acústicas com os modelos das palavras que estão no vocabulário do sistema.

Em geral, o sinal de fala e suas transformações não exibem indicações claras sobre as fronteiras das palavras, de modo que a detecção destas fronteiras faz parte do processo de geração de hipóteses realizado no procedimento de busca.

No procedimento de geração de hipóteses, todos os modelos de palavras são comparados com uma sequência de vetores acústicos.

Em um sistema probabilístico, a comparação entre uma sequência acústica e um modelo envolve o cálculo da probabilidade que o modelo associa a uma dada sequência.

Uma aproximação para calcular esta probabilidade consiste em seguir somente o caminho de máxima probabilidade.

Isto pode ser feito utilizando-se a quantidade y.

Esta aproximação corresponde ao algoritmo de Viterbi.

O cálculo das probabilidades acima é realizado em uma estrutura em forma de treliça.

Por simplicidade, pode-se assumir que o HMM representa uma palavra e que o sinal de entrada corresponde à pronúncia de uma única palavra.

Exemplo de funcionamento do algoritmo de Viterbi.

Cada coluna da treliça armazena os valores das verossimilhanças acumuladas em cada estado do HMM para todos os instantes de tempo, e todo intervalo entre duas colunas consecutivas corresponde a um quadro3 de entrada.

As setas na treliça representam transições no modelo que correspondem a possíveis caminhos no modelo do instante inicial até o final.

O cálculo é realizado por colunas, atualizando as probabilidades dos nós a cada quadro, utilizando fórmulas de recursão as quais envolvem os valores de uma coluna adjacente, as probabilidades de transição dos modelos, e os valores das densidades de saída para o quadro correspondente.

Para os coeficientes y, o cálculo começa na primeira coluna à esquerda, cujos valores iniciais são dados por pi, e termina na última coluna à direita, com a probabilidade final dada pela equação.

Um quadro é definido como o intervalo de tempo em que é gerado um vetor de parâmetros acústicos.

Valores típicos estão entre 10 e 20 ms.

O algoritmo usado para calcular os coeficientes y é conhecido como algoritmo de Viterbi, e pode ser visto como uma aplicação de programação dinâmica para encontrar o caminho de máxima verossimilhança em um grafo.

A fórmula de recursão é dada.

Monitorando o estado j que fornece a maior probabilidade na fórmula de recursão acima, é possível, no final da sequência de entrada, recuperar a sequência de estados visitada pelo melhor caminho, realizando então um tipo de alinhamento temporal dos quadros de entrada com os estados do modelo.

Todos estes algoritmos têm uma complexidade O(MT), onde M é o número de transições não nulas e T o comprimento da sequência de entrada.

M pode ser no máximo igual a S, onde S é o número de estados no modelo, mas é geralmente muito menor, uma vez que a matriz de probabilidades de transição é geralmente esparsa.

De fato, nos modelos left-right, uma escolha comum é fazer como no caso ilustrado.

Geralmente, o reconhecimento é baseado em um processo de busca que leva em conta todas as segmentações possíveis da sequência de entrada em palavras, e as probabilidades a priori que o modelo de linguagem associa a sequências de palavras.

Podem ser obtidos bons resultados com modelos de linguagem simples tais como probabilidades bigrama ou trigrama.

O tamanho do espaço de busca cresce de acordo com o número de palavras no vocabulário.

Para sistemas de ditado, onde são comuns vocabulários de dezenas de milhares de palavras, o espaço de busca torna-se tão grande que o custo computacional torna-se proibitivo.

Entretanto a distribuição irregular das probabilidades nos diferentes caminhos pode ajudar.

O que acontece é que, quando o número de estados é grande, em cada instante de tempo, uma grande parte destes estados têm uma verossimilhança acumulada que é muito menor do que a verossimilhança máxima, de modo que é bastante improvável que um caminho que passe por um destes estados venha a ser o melhor ao final da locução.

Esta consideração leva a uma técnica de redução da complexidade chamada de Beam Search, que consiste em desprezar, em cada instante de tempo, os estados cuja verossimilhança acumulada seja menor do que a verossimilhança máxima menos um dado limiar.

Desta maneira, os cálculos necessários para expandir nós ruins são evitados.

Está claro pela natureza do critério de poda desta técnica de redução que ela pode causar a perda do melhor caminho.

Na prática, uma boa escolha do limiar de poda resulta em um ganho de velocidade de uma ordem de magnitude, introduzindo uma quantidade desprezível de erros de busca.

O reconhecimento de fala contínua difere do reconhecimento de palavras isoladas no modo com que o usuário deve pronunciar as palavras.

No reconhecimento de palavras isoladas é necessário que o locutor efetue pausas breves entre as palavras de modo que o sistema possa determinar as fronteiras entre estas de forma precisa.

Já em fala contínua, o locutor pode falar de modo natural, sem efetuar pausas entre as palavras.

Neste caso, a determinação das fronteiras entre as palavras e consequentemente do número de palavras na locução fica a cargo do sistema de reconhecimento.

A premissa básica do reconhecimento de fala contínua é que o reconhecimento é baseado em modelos de palavras (possivelmente formadas a partir da concatenação de sub-unidades fonéticas para os casos de grandes vocabulários).

Uma vez definidos os modelos das palavras, o problema do reconhecimento resume-se em encontrar a sequência ótima (concatenação) de modelos de palavras que combine melhor (em um sentido de máxima verossimilhança) com a locução desconhecida.

Uma grande variedade de aproximações, todas baseadas na técnica de programação dinâmica, foram propostas e avaliadas.

O primeiro algoritmo para o reconhecimento de palavras conectadas foi proposto por Vintsyuk que mostrou como as técnicas de programação dinâmica poderiam ser utilizadas para descobrir a sequência de palavras ótima que combina com uma dada locução de entrada.

O procedimento de Vintsyuk processa o sinal de fala de maneira síncrona, e portanto o seu trabalho pioneiro formou a base para várias soluções baseadas em programação dinâmica para os problemas de reconhecimento de fala.

Várias outras estruturas de busca baseadas em programação dinâmica foram propostas para resolver o problema de reconhecimento de fala, baseadas na concatenação de modelos de palavras e sub-unidades, incluindo a aproximação de sistemas estatísticos de Baker desenvolvido na Carnegie Mellon University (a qual foi seguida pela pesquisa de Lowerre na mesma instituição), a aproximação estatística dos pesquisadores da IBM, e vários algoritmos de casamento de modelos de palavras.

A maior contribuição destas pesquisas iniciais é a idéia de representar todas as fontes de conhecimento usadas no reconhecimento representação das palavras, modelo de linguagem como redes (tanto determinísticas como estocásticas) que poderiam ser facilmente integradas com a rede básica que representa as unidades básicas (palavras ou sub-unidades fonéticas).

A busca poderia então ser realizada eficiente e acuradamente utilizando técnicas de programação dinâmica.

Foram propostos vários algoritmos para encontrar o melhor caminho através de uma rede, o Stack Algorithm desenvolvido por Jelinek, o Two Level de Sakoe, o Level Building de Myers e Rabiner, o One Step de Ney, entre outros.

Estes algoritmos são capazes de obter a melhor sequência de palavras que combina com uma dada locução de entrada, sujeita a uma grande variedade de limitações sintáticas (modelos de linguagem).

Reconhecimento de fala contínua via decodificação de rede finita de estados.

Como dito anteriormente, o problema do reconhecimento de fala pode ser organizado em uma hierarquia de redes de estados finitos com um número finito de nós e arcos correspondentes às fontes de conhecimento acústico, fonético e sintático e suas interações.

Em um sistema de reconhecimento de fala, o conhecimento acústico está relacionado à forma de parametrização do sinal de voz, parâmetros LPC, cepstrais, o conhecimento fonético, à transcrição fonética das palavras do vocabulário, e o conhecimento sintático, ao modelo de linguagem.

O reconhecimento de uma locução corresponde a encontrar o caminho ótimo através da rede de estados finitos.

Esta busca pode ser realizada através de decodificação sequencial usando os conceitos de programação dinâmica e o princípio da otimalidade definido por Bellman como -um conjunto de decisões ótimas tem a propriedade de, qualquer que tenha sido a primeira decisão, as decisões restantes precisam ser ótimas em relação à saída da primeira decisão-.

Em termos do problema de encontrar o melhor caminho através de uma rede de estados finita, o princípio da otimalidade permite que a decodificação seja feita de modo síncrono, pois toda a informação requerida para os caminhos ótimos locais está disponível, e os caminhos ótimos globais podem ser encontrados a partir dos caminhos ótimos locais.

Para a tarefa de reconhecimento de palavras conectadas, é conveniente decompor a rede em dois níveis, nível de frases (gramático) e nível intra-palavra.

Cada um dos níveis tem propriedades completamente diferentes.

O nível intra-palavra é geralmente um modelo de palavra, que pode ser um HMM da palavra inteira, ou uma representação da palavra formada pela concatenação de modelos HMM de sub-unidades acústicas.

O nível gramático é representado por uma rede gramática (de acordo com o modelo de linguagem), na qual os nós representam fronteiras de palavras, e os arcos representam modelos de palavras.

Estas representações vão desde redes simples com poucas restrições sintáticas (por exemplo, gramáticas bigrama ou trigrama) a redes gramáticas altamente complexas e restritivas (por exemplo, gramáticas sensíveis a contexto).

Para realizar a busca em uma rede de estados finita é necessário estabelecer uma medida de custo (por exemplo, distância, verossimilhança) associada ao caminho.

Esta medida inclui o custo de estar em um nó intra-palavra, o custo de fazer transições de um nó intra-palavra a outro, e o custo de entrar em um arco gramático.

Na tarefa de reconhecimento de fala, na qual os modelos das palavras são caracterizados por um HMM (ou concatenação de HMM-s), o custo acumulado de um caminho que passa por um determinado nó na rede de estados finita no instante t pode ser definida como o negativo da verossimilhança acumulada do caminho no instante t.

Esta verossimilhança é definida como o logaritmo da probabilidade daquele caminho.

Assim, a rede resultante é uma rede de estados finita estocástica onde o custo de um caminho depende da sequência de observação, do tempo que o sistema ficou em determinado nó, e da história de transições do caminho.

O custo de estar em um nó interno no instante t é relacionado à probabilidade de observar o vetor acústico naquele estado, no instante t, e pode ser definido como o negativo do logaritmo da probabilidade da observação no estado.

O custo de fazer uma transição interna inclui o negativo do logaritmo da probabilidade de transição, mais alguma possível penalidade de duração de estados, que depende do tempo em que o sistema permaneceu naquele estado.

Finalmente, o custo de deixar o nó gramático esquerdo de um arco gramático inclui uma possível penalização de transição de palavra.

Com todos os custos atribuídos convenientemente, o procedimento de busca pelo melhor caminho na rede de estados finita é essencialmente o mesmo de encontrar o caminho de custo mínimo através da rede, ou equivalentemente, realizar uma decodificação de máxima verossimilhança.

Seja x(t) um sinal de voz digitalizado.

A intervalos regulares de tempo, tipicamente a cada 10 ou 20 ms, é calculado um vetor de parâmetros acústicos yt.

As sequências de vetores de parâmetros acústicos são tratadas como observações dos modelos das palavras, usados para calcular P, a probabilidade de observar a sequência de vetores quando se pronuncia uma sequência de palavras W.

Dada uma sequência, o sistema de reconhecimento de fala gera uma sequência W de palavras, através de um processo de busca dado pela regra, onde W corresponde à sequência de palavras que apresentou a máxima probabilidade a posteriori.

P é calculado a partir de modelos acústicos, enquanto que P(W) é calculado a partir de modelos de linguagem.

Neste trabalho, as palavras são caracterizadas por modelos HMM-s os quais, por sua vez, são formados pela concatenação dos modelos HMM-s das sub-unidades fonéticas de sua transcrição fonética.

Das várias técnicas propostas para a decodificação, duas foram estudadas e implementadas neste trabalho, o Level Building de Myers e Rabiner e o One Step de Ney.

O algoritmo One Step diferencia-se do Level Building na forma de implementação, enquanto o Level Building é síncrono por palavras, o One Step é síncrono por quadros.

O Level Building teve uma grande importância histórica na redução da complexidade dos cálculos necessários ao reconhecimento de fala contínua.

Entretanto, com o advento de máquinas mais poderosas, que permitiram o reconhecimento de fala contínua em tempo real, esta abordagem passou a ser inviável pois, como é síncrona por palavra, tem que esperar até o final da locução para iniciar os processamentos, o que não acontece com o One Step por ser síncrono com o tempo.

Entretanto, em termos de resultados da decodificação, ambos são equivalentes.

Seja l o conjunto de V modelos HMM das palavras do vocabulário de reconhecimento, e lv cada um dos modelos de palavras deste vocabulário, possivelmente formadas a partir da concatenação de modelos HMM de sub-unidades fonéticas.

Para achar a sequência ótima de HMM-s que melhor combine com a sequência de observação O (maximize a verossimilhança), utiliza-se um processo de busca baseado no algoritmo de Viterbi.

Para cada modelo HMM de palavra lv e, a cada nível l, faz-se uma busca de Viterbi, iniciando no quadro 1 no nível 1, e armazena-se para cada quadro t.

Ao final de cada nível l (onde o nível corresponde à posição da palavra na sequência de palavras), realiza-se uma maximização sobre v para obter o melhor modelo em cada quadro t.

Cada novo nível começa com a maior verossimilhança do quadro anterior do nível anterior e incrementa o valor da verossimilhança combinando os modelos das palavras que começam no quadro inicial.

Este processo é repetido através de um número de níveis equivalente ao número máximo de palavras esperado para uma dada locução.

Ao final de cada nível, a melhor sequência de palavras de comprimento l com probabilidade P é obtida usando o vetor B.

A melhor sequência de palavras é a de máximo P sobre todos os níveis l.

O algoritmo Level Building é ilustrado.

O algoritmo One Step realiza os mesmos cálculos do Level Building, com a diferença de que o processamento é feito por quadros e não por palavras.

Esta diferença sutil é bastante poderosa, pois oferece a oportunidade de processamento em tempo real, coisa que não é possível com o algoritmo Level Building.

Pode-se ver o One Step como um algoritmo -transposto- em relação ao Level Building.

No desenvolvimento a seguir, isto ficará mais claro.

Para cada nó na rede de estados finita, em todo instante t, o algoritmo procura pelo melhor caminho que chega ao nó naquele instante, e constrói o caminho ótimo de duração t para aquele nó a partir de todos os caminhos de duração (t1).

O princípio da otimalidade da programação dinâmica garante que o melhor caminho para qualquer nó i, no instante t, pode ser determinado a partir dos melhores caminhos para todos os nós j, no instante (t1), mais o custo de fazer a transição do nó j para o nó i no instante t.

Pode-se ver que existem laços principais, o mais externo em relação aos quadros de entrada, outro intermediário, relacionado aos níveis e, finalmente o mais interno, relacionado às palavras do vocabulário.

Neste algoritmo, as seguintes quantidades são armazenadas a cada quadro t.

Estruturas intra-palavra, verossimilhança do estado k da palavra j no nível gramático i.

Elapse, duração do melhor caminho desde o início até o instante atual para o estado k da palavra j no nível gramático i.

Glike, verossimilhança do melhor caminho que chega ao nó gramático i no instante t.

Word, palavra vencedora para o nó gramático i, no instante t.

Bp, instante em que o caminho que chegou ao nó gramático i, no instante t se iniciou Probant, probabilidade de transição a partir do nó gramático i, no instante t.

Para atualização das estruturas intra-palavra é utilizado o algoritmo de Viterbi.

Neste é utilizado um vetor temporário, aqui denominado scratch.

Ilustração do funcionamento do algoritmo de Viterbi na implementação do algoritmo One Step.

A atualização das estruturas gramaticais, em cada nível i, e em cada instante de tempo t, é realizada pelo algoritmo de fusão de caminhos.

Ao final deste procedimento, as palavras reconhecidas são armazenadas no vetor palavra, em ordem invertida.

As durações de cada uma delas é armazenada no vetor duração.

É importante frisar que, em termos de resultados, o Level Building e o One Step são algoritmos equivalentes.

Entretanto, na implementação, o One Step proporciona facilidades como a possibilidade de processamento em tempo real e a redução de complexidade através do procedimento Beam Search.

Na determinação do caminho ótimo através do algoritmo de Viterbi, tanto no caso do Level Building como no One Step, pode-se associar uma duração a cada palavra, em cada nível de busca.

Para os HMM-s, a probabilidade de duração Pi associada ao estado S com probabilidade de auto-transição.

A quantidade P pode ser vista como a probabilidade de d observações consecutivas no estado i.

Este modelo de duração exponencial é bastante inadequado para representar sinais reais, podendo fazer com que a duração encontrada esteja muito distante de uma duração -média- atribuída à locução daquela palavra.

Pode-se modelar explicitamente a densidade de duração através de formas explicitamente analíticas, mas o custo computacional é elevadíssimo.

Uma forma alternativa proposta por Rabiner associa à duração d de cada palavra i do vocabulário uma função densidade de probabilidade gaussiana onde i e s são, respectivamente, a média e a variância da duração da palavra.

Estes valores são obtidos a partir da segmentação das locuções de treinamento.

O procedimento para incorporar o modelo de duração aos algoritmos de busca é o seguinte.

A cada instante t, determina-se a duração da palavra i, no nível l, através da recuperação do caminho ótimo determinado pelo algoritmo de Viterbi.

A verossimilhança acumulada para uma dada palavra é penalizada de acordo com a função densidade de probabilidade gaussiana, com os parâmetros da palavra em análise.

Embora claramente heurístico, este método proporcionou uma melhora significativa em testes anteriores realizados com uma base de dados dependente de locutor.

No presente trabalho, o modelo de duração de palavras foi levantado manualmente a partir das locuções de um único locutor (o mesmo utilizado nos testes com dependência de locutor, cujos resultados são mostrados no Capítulo 7).

Nos testes com independência de locutor, sempre haverá casos em que as durações das palavras nas locuções de teste estejam significativamente distantes daquelas armazenadas no modelo de duração.

Isto pode fazer com que o reconhecimento seja prejudicado nestes casos, mas algum procedimento de adaptação poderia minimizar este problema.

No sistema foi utilizado um modelo de linguagem do tipo pares-de-palavras, que é uma simplificação do modelo bigrama, descrito no Capítulo 2.

Este modelo de linguagem pode ser visto como uma versão determinística do modelo bigrama.

A escolha por este modelo em detrimento do bigrama é devida à limitação da base de dados, como é muito pequena, a utilização das frequências bigrama poderia polarizar o algoritmo de busca em alguns casos.

Por exemplo, supondo que a sequência -a casa- tenha ocorrido duas vezes, e a sequência -a taça- apenas uma vez, o sistema poderia reconhecer a locução -a taça- como -a casa-, visto que são parecidas, e o modelo de linguagem atribuiria uma probabilidade duas vezes maior para a sequência -a casa-.

A incorporação do modelo de linguagem aos algoritmos de busca é trivial, ao início de cada nível, em cada instante t, verifica-se qual a palavra vencedora no nível anterior e, se a palavra sob análise for permitida pelo modelo de linguagem, é expandida.

O sistema desenvolvido é formado por três módulos principais, módulo de extração de parâmetros e quantização vetorial, módulo de treinamento, módulo de geração de modelo de linguagem, módulo de reconhecimento.

O primeiro módulo é formado por dois programas, o programa de extração de parâmetros, que converte um sinal de voz digitalizado em vetores acústicos, e o programa de quantização vetorial.

O segundo é formado por quatro programas, programa de treinamento dos HMM-s, programa de detecção de trifones, programa de combinação de modelos baseado no procedimento deleted interpolation, e o programa de geração de gramática bigrama.

O terceiro é o responsável pela geração do modelo de linguagem, baseado no modelo de gramática bigrama.

Por fim, o último módulo é formado pelo programa de reconhecimento.

Os programas foram implementados em linguagem C++ para a plataforma Windows.

Na implementação, teve-se o cuidado de criar uma interface visual bastante amigável e intuitiva, um código estruturado e extensamente documentado, de forma que outros pesquisadores possam desenvolver as suas idéias a partir deste sistema.

Com isto, o tempo necessário para testar novas idéias ficou bastante reduzido.

Neste laboratório, este sistema já está sendo utilizado por outros pesquisadores, nas áreas de adaptação ao locutor e reconhecimento de dígitos conectados.

Nas seções seguintes o sistema será mostrado com mais detalhes.

Este módulo é o responsável por transformar as locuções de entrada em parâmetros que possam ser interpretados pelos módulos seguintes.

Tem-se um diagrama de blocos onde é mostrada a arquitetura deste módulo.

Este módulo tem por entrada um sinal de voz em formato WAV ou binário, e calcula parâmetros da locução.

Atualmente estão disponíveis os parâmetros melcepstrais de ordem 12 e log-energia normalizada, bem como seus respectivos parâmetros delta e delta-delta, nas frequências de amostragem de 8, 11,025 e 16 kHz, tanto para 8 como 16 bits de resolução.

Embora algumas destas opções não sejam utilizadas neste trabalho, optou-se por criar um programa mais versátil, que pudesse ser utilizado com outras bases de dados.

Os parâmetros são calculados utilizando-se janelas de 20 ms, atualizadas a cada 10 ms.

Antes da extração, o sinal é submetido a alguns pré-processamentos, retirada do nível DC, pré-ênfase com um filtro passa altas (10,95 z1), e janelamento através de uma janela de Hamming.

Os parâmetros log-energia foram normalizados tomando como referência o quadro de maior energia em toda a locução sob análise.

Para os parâmetros mel-cepstrais a ordem utilizada foi 12.

A estes parâmetros foi aplicado o procedimento de remoção da média espectral, que consiste em calcular o vetor média x de todos os vetores acústicos que representam uma dada locução.

Este vetor média é então subtraído de cada um dos vetores acústicos da locução, gerando uma versão modificada do vetor acústico x.

Diagrama de blocos do processo de extração dos parâmetros mel-cepstrais com remoção da média espectral.

A justificativa para o procedimento pode ser resumida da seguinte maneira, na extração dos parâmetros de uma locução, o sinal de voz é segmentado em trechos curtos, geralmente entre 10 e 20 ms.

Desta forma, o sistema não consegue distinguir o sinal quasi-estacionário de curto termo (sinal de voz) do sinal quasi-estacionário de longo termo (ruído ambiente e/ou característica do canal).

O cálculo da média dos vetores ao longo de toda a locução traria então informações sobre o sinal quasi-estacionário de longo termo, e a sua remoção teria um efeito de minimizar a influência deste sinal de longo termo no sinal de voz.

Testes preliminares mostraram uma melhoria no desempenho do sistema utilizando este procedimento.

O sinal parametrizado é armazenado em arquivo de mesmo nome do original, mas com extensão diferente (-mel- para parâmetros mel-cepstrais e -ene- para parâmetros log-energia normalizada).

Como o sistema de reconhecimento é baseado em modelos de Markov discretos, torna-se necessário quantizar os parâmetros de entrada através de um quantizador vetorial.

O sistema de quantização é formado por dois módulos, um módulo de treinamento, responsável pela geração dos vetores código do quantizador e outro responsável pela quantização propriamente dita.

Para a parte de treinamento foi utilizado o algoritmo LBG em sua versão splitting para gerar os vetores código do quantizador.

No presente trabalho foram utilizados dicionários de códigos de 256 vetores para cada um dos parâmetros de entrada.

Estes dicionários foram gerados a partir das locuções de treinamento.

A quantização foi realizada através de comparação exaustiva de cada vetor de entrada com cada um dos 256 vetores do dicionário de códigos, utilizando como medida de distorção a distância euclidiana.

Os parâmetros log-energia normalizada, bem como suas derivadas primeira e segunda são grandezas escalares.

Ao invés de realizar uma quantização escalar para cada um destes parâmetros, optou-se por agrupá-los em um único vetor de três posições e realizar uma quantização vetorial sobre estes vetores.

Este módulo é o responsável pelo treinamento dos modelos HMM das subunidades fonéticas a serem utilizados na etapa de reconhecimento.

O processo de treinamento das sub-unidades fonéticas é dividido em duas partes, primeiro são gerados modelos de fones independentes de contexto (os mesmos utilizados para a transcrição fonética das locuções de treinamento).

Estes irão servir como modelos iniciais para o treinamento dos modelos de fones dependentes de contexto (trifones).

Foram desenvolvidos três programas para esta parte do sistema, um para o treinamento das sub-unidades acústicas, outro para detecção e inicialização dos fones dependentes de contexto, e o último responsável por mesclar os modelos de fones independentes de contexto com modelos de trifones utilizando o algoritmo Deleted Interpolation.

Este programa tem por função treinar os modelos HMM das sub-unidades acústicas a partir de locuções de treinamento parametrizadas e quantizadas e das respectivas transcrições fonéticas.

O algoritmo de treinamento utilizado é o Baum-Welch.

É necessário fornecer ao programa de treinamento as seguintes informações.

Sub-unidades acústicas a serem utilizadas na transcrição fonética das locuções de treinamento (fones independentes de contexto).

Transcrição das locuções utilizando estas sub-unidades fonéticas.

Locuções de treinamento parametrizadas e quantizadas.

O procedimento adotado para o treinamento das sub-unidades é o seguinte, inicialmente são criados modelos HMM para cada uma das sub-unidades acústicas.

Modelo HMM utilizado para cada uma das sub-unidades fonéticas.

A probabilidade de transição akl indica a probabilidade de fazer uma transição para a sub-unidade seguinte.

Para a inicialização destes modelos foram testadas duas abordagens, uma utilizando uma distribuição uniforme e outra utilizando o algoritmo Segmental K-Means.

Pelo método da distribuição uniforme, assume-se que inicialmente todos os símbolos são equiprováveis, e as probabilidades de emissão de símbolos de saida são inicializadas com valor num_vet, onde num_vet é o número de vetores com o qual foi feita a quantização vetorial dos parâmetros acústicos (256 neste trabalho).

As probabilidades de transição são inicializadas como sendo equiprováveis.

O método via Segmental K-Means é dividido em duas partes, inicialização e pré-treinamento.

Na etapa de inicialização, as locuções de treinamento são divididas em m partes iguais (de mesmo comprimento), sendo m o número de sub-unidades fonéticas da transcrição fonética multiplicada pelo número de estados de cada modelo HMM (3 neste trabalho).

É criado um modelo HMM para a locução concatenando-se os modelos HMM das sub-unidades acústicas referentes à sua transcrição fonética.

Faz-se então uma contagem dos símbolos que ocorreram em cada uma destas partes, e estas contagens, depois de transformadas em medidas de probabilidade, serão os valores iniciais de cada estado dos modelos HMM correspondentes.

As probabilidades de transição são inicializadas como no caso anterior.

Temos então 8 sub-unidades acústicas a serem treinadas, e como cada uma é modelada por um HMM de 3 estados, temos um total de 24 fdp-s a serem estimadas.

Supondo que esta locução foi parametrizada com 240 quadros, teríamos 10 quadros para cada estado.

Os 10 primeiros símbolos irão inicializar o primeiro estado da primeira subunidade acústica, os 10 seguintes o segundo, e assim por diante.

A inicialização é feita por simples contagem, verifica-se quantas vezes cada um dos símbolos ocorreu nestes 10 quadros, atualizando as contagens destes símbolos nas fdp-s discretas correspondentes.

É interessante verificar que no exemplo dado, existem dois exemplares dos fones.

Neste caso, as contagens de cada um deles é acumulada na mesma fdp.

Similarmente, se tivermos mais locuções de treinamento, as contagens dos fones vão sendo acumuladas na mesma fdp.

Ao final, estas contagens são transformadas em medidas de probabilidade.

Mesmo com vários exemplos de treinamento para cada fone, é muito comum a ocorrência de valores nulos para algumas posições destas fdp-s discretas.

O efeito de valores nulos no processo de reconhecimento é devastador, e um dos métodos empregados para evitar este inconveniente é substituir estes valores nulos por um valor pequeno.

Isto equivale a dizer que a ocorrência do símbolo ao invés de ser impossível, é improvável, uma condição bem menos drástica.

Depois da inicialização dos modelos faz-se o pré-treinamento utilizando o algoritmo de Viterbi, que corresponde ao procedimento Segmental K-Means.

O procedimento é parecido com o da inicialização descrito acima, com a diferença de que agora a segmentação não é uniforme, ou seja, a cada um dos estados são associados mais ou menos quadros dependendo do caminho escolhido pelo algoritmo de Viterbi.

As probabilidades de emissão são atualizadas, como no caso anterior, pela contagem dos símbolos emitidos em cada estado, e as de transição, pelo número de quadros que o sistema ficou em cada estado.

Os testes realizados com ambas as inicializações não mostraram diferenças significativas entre um e outro procedimento, de modo que o primeiro procedimento foi adotado, por ser mais simples.

Após a inicialização vem o treinamento propriamente dito, onde é utilizado o algoritmo Baum-Welch.

O procedimento é similar ao da inicialização utilizando o método Segmental K-Means, para cada locução de treinamento é gerado um modelo HMM através da concatenação dos modelos referentes às sub-unidades acústicas da sua transcrição fonética.

Este modelo composto pode então ser tratado como uma única palavra, e a locução da frase, a palavra correspondente a este modelo composto.

Desta forma o algoritmo de treinamento maximiza a probabilidade de o modelo composto gerar a locução correspondente.

Depois disso, os modelos individuais das sub-unidades fonéticas são separados, e as contagens geradas pelo algoritmo Baum-Welch são acumuladas durante todo o processo de treinamento, e somente após serem processadas todas as locuções de treinamento (uma época de treinamento), são transformadas em medidas de probabilidade.

Após cada época de treinamento, faz-se uma verificação da convergência da seguinte maneira, para cada locução de treinamento monta-se o modelo HMM correspondente através da concatenação dos modelos das sub-unidades fonéticas e aplica-se o algoritmo de Viterbi para calcular a probabilidade de o modelo gerar a locução correspondente.

Repetindo este procedimento para todas as locuções de treinamento, pode-se calcular uma -probabilidade média- de os modelos gerarem as sequências de vetores acústicos correspondentes às locuções de treinamento.

A cada época esta probabilidade média cresce até que um patamar é atingido.

O treinamento é realizado até que a probabilidade média pare de crescer.

A base de dados gerada para estes testes não apresenta todos os trifones possíveis.

Considerando que a transcrição fonética das locuções foi realizada com 36 fones (35 fones independentes de contexto mais um, correspondente ao silêncio), teríamos, em termos grosseiros, 363 = 46656 trifones.

A grande maioria deles não é observada nas locuções que constituem a base de dados.

Desta forma, o dicionário de fones dependentes de contexto será limitado àqueles observados nas locuções de treinamento.

O procedimento adotado para a geração dos trifones é o seguinte, inicialmente são tomadas as transcrições fonéticas das locuções feitas com fones independentes de contexto.

Para cada fone da transcrição são identificados o fone imediatamente anterior e o imediatamente posterior, gerando-se assim o trifone correspondente.

Os trifones detectados são armazenados em uma lista.

Deve-se observar que o fone inicial não tem o contexto à esquerda, e o fone final não tem o contexto à direita, mas como eles são sempre o fone correspondente ao silêncio, isto não chega a ser um problema.

Aliás, não são gerados modelos trifones para o silêncio, ele é sempre considerado um fone independente de contexto.

São também atualizados os arquivos com as transcrições fonéticas para uma versão utilizando os trifones, e o arquivo de vocabulário utilizado no reconhecimento.

É preciso ainda gerar modelos HMM para estas novas sub-unidades.

Este programa se encarrega desta tarefa, atribuindo a cada modelo trifone os parâmetros dos modelos HMM dos fones independentes de contexto correspondentes.

Estes modelos devem então ser retreinados utilizando o programa de treinamento descrito na seção.

A geração de todos os trifones contidos na base de dados criaria um conjunto de sub-unidades muito grande para a base de dados de treinamento.

De fato, foram detectados 3655 trifones trifones somente no subconjunto de treinamento.

Isto faz com que haja uma escassez muito grande de dados de treinamento para estimar de forma consistente todos os parâmetros envolvidos.

A solução seria então agrupar os trifones em classes convenientemente definidas de modo a diminuir o número de sub-unidades acústicas sem perder a propriedade de consistência que estas possuem.

LEE propõe um método baseado na medida de quantidade de informação da Teoria de Informação para determinar um número razoável de trifones baseado no tamanho da base de dados de treinamento.

Neste trabalho foram testadas duas abordagens alternativas, baseadas em informações linguísticas.

Na primeira, a idéia é associar a cada um dos fones independentes de contexto uma etiqueta correspondente à sua classe fonética.

As classes fonéticas utilizadas são vogais, vogais nasais, plosivas, fricativas, laterais, vibrantes e nasais.

O silêncio foi considerado como uma classe separada.

Na segunda abordagem, as classes são definidas segundo uma classificação baseada na fonética acústica, onde é levada em conta a configuração do trato vocal.

Neste caso, como existem muitas classes, algumas delas foram agrupadas para diminuir o seu número.

Como uma ilustração do processo da substituição da transcrição fonética utilizando fones independentes de contexto para a transcrição utilizando trifones, considere uma locução correspondente à palavra -casa-.

São realizados testes de reconhecimento utilizando fones independentes de contexto, trifones baseados nas classes fonéticas e trifones baseados na configuração do trato vocal.

Também são feitos comentários a respeito das características e influência no desempenho do sistema no reconhecimento para cada um destes conjuntos de sub-unidades, bem como análises sobre o custo computacional e de armazenamento em memória.

Classes fonéticas baseadas na posição do trato vocal e seus respectivos fones.

Como mencionado no Capítulo 1, os trifones, embora sejam unidades consistentes, não são treináveis, devido ao seu número muito elevado.

Entretanto, podemos observar que os trifones correspondem a fones específicos e, deste modo, seus modelos podem ser interpolados com os dos fones independentes de contexto, que são melhor treinados, embora pouco consistentes.

O Deleted Interpolation é um método para obter um modelo -híbrido-, que inclui automaticamente uma proporção adequada de cada um dos modelos originais.

O método pode ser resumido da seguinte maneira, seja T o conjunto de locuções de treinamento do sistema.

Suponha que dividamos T em dois sub-conjuntos disjuntos, T- e T-.

Usamos T- para treinar os modelos dos fones independentes de contexto (Mf) e os modelos dos trifones (Mt).

Depois, fazemos experimentos de reconhecimento de cada uma das sequências em T-, usando Mf e Mt em cada experimento.

Em cada caso, um dos modelos irá produzir uma verossimilhança maior.

Seja ef a fração das sequências em T- para as quais Mf produziu verossimilhanças maiores.

Se af e bf são as matrizes com as probabilidades de transição e de emissão para Mf, e at e bt as matrizes correspondentes para Mt, então o modelo interpolado terá matrizes.

Na verdade o termo Deleted Interpolation é geralmente usado para descrever um procedimento um pouco mais complicado do que o descrito acima.

Neste caso, o conjunto de treinamento T é reparticionado iterativamente e o procedimento é repetido sobre cada partição.

Existem muitas maneiras de construir as partições múltiplas.

Por exemplo, T- pode ser composto pelos primeiros 10% de T na primeira iteração, pelos próximos 10% na segunda, e assim por diante.

Na abordagem por sub-unidades utilizada neste trabalho, o termo ef não pode ser obtido pela simples substituição da transcrição fonética em fones pela transcrição fonética em trifones de uma dada locução.

Isto porque deve ser verificada a influência de cada trifone separadamente sobre o desempenho do sistema.

Assim, o procedimento adotado foi o seguinte para cada locução do conjunto de validação T-.

Inicialmente é formado o modelo de fones independentes de contexto da locução e calcula-se a verossimilhança correspondente.

Substitui-se apenas um fone independente de contexto pelo seu trifone correspondente e calcula-se a verossimilhança deste novo modelo.

Repete-se este processo para todos os fones da locução.

Para cada trifone calcula-se o percentual de vezes em que o uso do fone correspondente foi melhor que o uso do trifone.

A partir do percentual de vezes em que a verossimilhança dos fones foi maior que a dos trifones, calcula-se o valor de ef.

Este procedimento produz valores ef diferentes para cada um dos trifones, e garante que a influência de cada um deles foi avaliada de forma particular.

Por escassez de dados de treinamento, optou-se por fazer a avaliação de ef com os mesmos dados utilizados para os testes.

Como discutido no Capítulo 2, o modelo de linguagem faz com que a perplexidade seja reduzida no processo de reconhecimento.

Foi dito também que um modelo de linguagem robusto necessita de bases de dados extensas para um bom desempenho.

A exemplo da base de dados de vozes, a construção de uma base de dados para treinamento do modelo de linguagem é bastante onerosa.

Desta forma optou-se por utilizar apenas as frases utilizadas para gerar a base de dados como material de treinamento para o modelo de linguagem.

O procedimento adotado para a construção da gramática foi o seguinte.

Gera-se um arquivo texto com todas as frases a serem consideradas para o modelo de linguagem.

Realiza-se um levantamento das palavras que compõem as frases, bem como a frequência de ocorrência destas.

Realiza-se um levantamento das sequências de duas palavras que ocorrem nas frases, e também a frequência de ocorrência destas sequências.

As frequências de ocorrência de pares de palavras que podem ocorrer são transformadas em probabilidades através da expressão.

Os valores das probabilidades não foram utilizados neste trabalho.

Como mencionado na seção, foi utilizada uma gramática de pares de palavras, que é um modelo simplificado da gramática bigrama.

Neste caso, o sistema de reconhecimento verifica apenas se a probabilidade de uma sequência de palavras é nula ou não.

O cálculo das frequências foi inserido neste programa visando trabalhos futuros.

Para um vocabulário de N palavras, seria necessário criar uma matriz N x N para armazenar todos os dados.

Entretanto, como esta matriz é esparsa, não é necessário armazenar todos os elementos.

Com isto foram armazenados, para cada posição não nula desta matriz, os índices da primeira e segunda palavras e a probabilidade de uma seguir a outra.

O módulo de reconhecimento é o responsável pelo mapeamento dos parâmetros acústicos correspondentes à locução de entrada em sua transcrição ortográfica.

Foram implementados dois algoritmos de busca para o reconhecimento de fala contínua, o Level Building e o One Step.

Para melhorar o desempenho do sistema em termos de taxa de acertos foram incluídos o modelo de duração de palavras e o modelo de linguagem bigrama.

Também foram incorporadas estratégias para diminuição do tempo de processamento, o Viterbi Beam Search para o algoritmo One Step e um esquema de deteção automática do número de níveis de reconhecimento para o algoritmo Level Building.

Observando-se a Figura, verifica-se que os dados necessário para o reconhecimento de uma dada locução são os parâmetros quantizados desta, os modelos HMM das sub-unidades acústicas e o vocabulário com o universo das palavras que podem ser reconhecidas.

Os dois primeiros itens já foram abordados em seções anteriores, de modo que nesta seção será dada ênfase à construção do arquivo de vocabulário.

O vocabulário de um sistema de reconhecimento de fala é a unidade que define o universo de palavras que podem ser reconhecidas, ou seja, toda e qualquer locução será mapeada em uma sequência de palavras deste universo.

Em termos gerais, quanto maior e mais abrangente o vocabulário, mais flexível é o sistema, embora o reconhecimento torne-se cada vez mais difícil à medida em que o vocabulário cresce.

Com estas considerações em mente, neste trabalho o vocabulário foi definido a partir das frases que compõem a base de dados (200 frases foneticamente balanceadas).

Outra questão a ser abordada é a das diferenças de pronúncia.

Dependendo do locutor, a mesma palavra pode ser pronunciada de várias maneiras diferentes.

Como ressaltado no início desta tese, a variedade de pronúncias é muito grande e, se formos listar todas as variantes possíveis para todas as palavras do vocabulário, este se torna muito grande, aumentando muito a perplexidade no momento da busca.

Se por um lado temos todas as variantes possíveis (ou que julgamos possíveis) para uma dada palavra, teoricamente temos um casamento mais preciso da locução com os modelos de palavras contidos no vocabulário.

Entretanto, o aumento de modelos a serem comparados pode trazer mais dificuldades para o algoritmo de busca pelo aumento da perplexidade.

De modo a investigar estes efeitos, foram gerados dois arquivos de vocabulário para este sistema.

Vocabulário 1, foi gerada apenas uma versão de cada palavra, ou seja, assumiu-se que todos os locutores pronunciaram as palavras da mesma maneira.

Neste caso, o arquivo de vocabulário apresenta 694 palavras distintas.

Vocabulário 2, neste, procurou-se cobrir a maior parte das pronúncias mais comuns, tentando prever alguns regionalismos e efeitos de coarticulação.

Para evitar um aumento excessivo do vocabulário, foram consideradas no máximo seis variantes para cada palavra.

Com estes cuidados, o vocabulário, que tem 694 palavras distintas, passou a ter 1633 palavras.

O arquivo de vocabulário é formado de duas partes, na primeira são listadas as sub-unidades fonéticas utilizadas para transcrever as palavras, e na segunda, a descrição das palavras, sua transcrição fonética e informação de duração das mesmas (média e variância das durações).

Como mencionado, o modelo de duração foi levantado a partir das locuções de um único locutor.

Nestas, algumas palavras ocorrem apenas uma vez, de modo que a variância da duração é nula.

Para não prejudicar o reconhecimento com restrições tão rígidas, adotou-se para o desvio padrão da duração destas palavras um valor arbitrário correspondente a 1/3 do valor médio da duração.

Um exemplo do arquivo de vocabulário é mostrado.

Podemos observar o seguinte, inicialmente tem-se uma palavra chave (fonemas), seguida de uma listagem das sub-unidades fonéticas utilizadas para a transcrição das palavras do vocabulário (uma sub-unidade por linha), e depois por outra palavra chave fim, que indica o final da listagem das sub-unidades.

Em seguida, vem outra palavra chave (vocab), que indica o início da listagem das palavras do vocabulário, para cada palavra (ou variante desta) tem-se uma linha com a seguinte estrutura, transcrição gráfica / transcrição fonética / média da duração (ms) / desvio padrão da duração.

Quando é utilizado o algoritmo Level Building para o reconhecimento, é necessário informar o número máximo L de palavras nas locuções.

O algoritmo processa então os dados de entrada até o nível L e então decide quantas palavras existem na locução.

O custo computacional para este algoritmo é O(LMT), onde M é o número de palavras no vocabulário e T é o número de quadros da locução a ser reconhecida, e estas variáveis são fixas para um dado vocabulário e uma locução sendo reconhecida.

Desta forma, quanto maior o valor de L, maior é o custo computacional e, consequentemente, o tempo de processamento.

A diminuição de L diminui o tempo de processamento, mas pode causar erros de deleção em locuções longas.

Como não se sabe a priori o número de palavras na locução, este valor deve ser relativamente alto.

O inconveniente deste procedimento é óbvio, o reconhecimento das locuções mais curtas levará um tempo desnecessariamente longo pois o sistema irá efetuar os cálculos até o nível L antes de fornecer uma resposta.

O ideal seria que o sistema pudesse determinar de forma automática o número de palavras na frase durante o processo de busca, e fazer os cálculos somente até o nível correspondente.

Uma forma extremamente simples de se fazer isso é observar que, de uma maneira geral, a verossimilhança P ou seja, a probabilidade do modelo l gerar a sequência de observação O, tende a crescer com o número de níveis até atingir um pico, voltando a cair depois disso.

Ainda, nos testes realizados notou-se que, geralmente, o ponto de máximo ocorre em um nível próximo ao número de palavras reconhecidas pelo sistema quando não é utilizado este procedimento.

Se o crescimento e decaimento de P fossem sempre monotônicos, como mostrado na Figura, a decisão de parada poderia ser feita verificando apenas o valor de P no nível anterior, e parando a busca no ponto de inflexão da curva.

Neste caso, se fosse adotado o procedimento de verificar apenas a verossimilhança no nível anterior, o sistema reconheceria uma locução de 5 palavras, e não de 8, incorrendo em um erro de deleção bastante nocivo ao resultado, pois quase metade das palavras da locução não seriam detectadas.

Verifica-se um comportamento monotônico de crescimento e decaimento nos valores da logverossimilhança com o número de níveis.

Verifica-se um comportamento não monotônico de crescimento e decaimento nos valores da logverossimilhança com o número de níveis.

Para resolver este problema, pensou-se inicialmente em estabelecer o critério de parada com base na derivada da curva log-verossimilhança versus número de níveis.

O procedimento seria o seguinte, define-se a derivada da curva através da expressão atual.

Pode-se ver da expressão acima que enquanto a log-verossimilhança cresce, o valor d é positivo.

No decaimento, este valor torna-se negativo.

O procedimento para detecção automática do número de níveis utilizando esta medida é o seguinte, define-se um limiar para d, abaixo do qual o algoritmo para o processamento.

Se este limiar for negativo, o sistema permite que haja não monotonicidades no comportamento da log-verossimilhança, desde que não sejam muito fortes.

A segunda alternativa encontrada foi estabelecer o seguinte critério de parada, o algoritmo para se a log-verossimilhança cair por l níveis consecutivos.

Desta forma, o sistema pode atravessar as quebras no comportamento da log-verossimilhança que sejam menores do que l.

Este procedimento pode ser visto como um detetor de tendência de queda no comportamento do valor da log-verossimilhança, uma vez que se este cai durante alguns níveis é muito provável que o máximo global já tenha sido atingido, e já esteja em regime de queda.

É importante ressaltar que estes procedimentos nem sempre diminuem o tempo de processamento.

Se for utilizado o procedimento de fixar um número de níveis de busca e este for o número de palavras da locução, a quantidade de cálculos efetuados pelo sistema será o estritamente necessário, enquanto que no modo de detecção automática, pode ocorrer de o sistema avançar mais alguns níveis.

De uma forma geral, entretanto, o procedimento proposto gera uma economia de esforço computacional, pois o sistema irá sempre realizar os cálculos com um número de níveis próximo ao de palavras na locução.

Ainda, a economia será maior nas locuções mais curtas.

Foram realizados alguns testes para verificar o quanto se pode ganhar em tempo de processamento, e os resultados podem ser vistos no capítulo a seguir.

Neste capítulo são apresentados os resultados de avaliação do sistema desenvolvido utilizando a base de dados descrita no Capítulo 3.

Foram realizados vários testes, e feitas as seguintes análises, desempenho do sistema utilizando fones independentes de contexto, desempenho do sistema utilizando fones dependentes de contexto baseados nas classes fonéticas, desempenho do sistema utilizando fones dependentes de contexto baseados na posição do trato vocal.

Avaliação dos procedimentos para diminuição do tempo de processamento, influência da dependência do locutor com testes dependentes de locutor, independentes de locutor e dependentes de sexo, influência da transcrição fonética das locuções de treinamento no desempenho do sistema, influência do número de versões de cada palavra no arquivo de vocabulário.

Antes dos testes propriamente ditos foram realizados alguns testes preliminares para a determinação do conjunto de sub-unidades fonéticas a serem utilizadas neste trabalho.

A motivação e o procedimento são descritos na seção seguinte.

Para todos os testes foram utilizados parâmetros mel-cepstrais, delta melcepstrais e delta-delta mel-cepstrais, quantizados separadamente, modelo de duração de palavras e gramática de pares de palavras.

Os parâmetros log-energia normalizada, bem como suas derivadas não foram utilizados pois foi detectada uma piora no desempenho do sistema quando adotados.

No português falado no Brasil existem 39 fones distintos.

Na Tabela são listados os fones utilizados na variante brasileira da língua portuguesa.

Como dito anteriormente, alguns deles são bastante próximos e seria interessante agrupá-los, ou seja, considerá-los como sendo o mesmo fonema, pois isto diminuiria o número de sub-unidades fonéticas, fazendo com que houvesse mais exemplos de treinamento para cada uma.

Entretanto, este agrupamento deve ser feito de forma a não juntar fones com características diferentes, o que pode fazer com que as sub-unidades resultantes se tornem inconsistentes.

O procedimento utilizado para verificar se uma fusão deveria ou não ser adotada foi o seguinte.

Inicialmente foram gerados e treinados os modelos HMM de todos os 39 fones listados.

Com esses modelos calculou-se a probabilidade média dos modelos HMM das locuções de treinamento gerarem as sequências de observação correspondentes.

Esta probabilidade é tomada então como referência.

Para cada uma das fusões propostas acima, foram criados e testados os modelos HMM correspondentes e calculada novamente a probabilidade de os modelos gerarem as sequências de observação.

Se esta probabilidade fosse maior que a de referência, a fusão era adotada.

Tem-se os resultados destes testes.

Na verdade esta fusão foi feita já na transcrição fonética original, e não foi nem testada nesta etapa.

Com este procedimento, foram adotadas todas as fusões testadas, exceto a primeira.

A única fusão adotada que não resultou em uma diminuição da verossimilhança média foi a do teste c.

Mesmo assim ela foi adotada uma vez que, na transcrição fonética, nem sempre era possível ter certeza da ocorrência de um ou outro, o que poderia causar erros.

A base de dados coletada é formada por 40 locutores, sendo 20 do sexo masculino e 20 do sexo feminino.

Como mencionado, os locutores foram separados em 5 grupos, onde cada grupo pronunciou 4 das 20 listas.

Deste modo, temos quatro locutores de cada sexo em cada grupo.

Para a formação do subconjunto de teste foram escolhidos de cada grupo, e de forma aleatória, um locutor do sexo masculino e um do sexo feminino, resultando no total 5 locutores femininos e 5 masculinos.

Os demais locutores, 15 masculinos e 15 femininos, formam o subconjunto de treinamento.

Nos testes com dependência de sexo, os locutores de treinamento e teste são extraídos dos subconjuntos anteriores, resultando em 5 locutores de teste e 15 de treinamento.

Para os testes com dependência de locutor, uma única pessoa do sexo masculino pronunciou todas as frases 3 vezes.

Duas repetições formam o subconjunto de treinamento e a terceira, o subconjunto de testes.

Os primeiros testes foram realizados utilizando os fones independentes de contexto.

Estes testes têm por finalidade estabelecer um desempenho de referência a partir do qual será analisada a influência dos fones dependentes de contexto no desempenho do sistema desenvolvido.

Nesta etapa foi utilizado o algoritmo Level Building com 15 níveis para todas as locuções.

A escolha deste número de níveis está relacionada às frases da base de dados.

A frase mais longa tem 11 palavras, e contando os silêncios inicial e final, temos 13 palavras.

Com 15 níveis é possível reconhecer todas as frases, e ainda verificar se ocorrem erros de inclusão, mesmo nas frases mais longas.

Foram realizados 4 testes, variando-se os locutores envolvidos, teste com independência de locutor (todos os 10 locutores de teste), teste com dependência de sexo para os locutores masculinos (5 locutores de teste do sexo masculino), teste com dependência de sexo para os locutores femininos (5 locutores de teste do sexo feminino), teste com dependência do locutor (1 locutor do sexo masculino).

Uma vez estabelecida uma referência para a taxa de acertos do sistema, foram realizados testes para verificar a influência dos fones dependentes de contexto no seu desempenho.

Como mencionado na seção, foram testados dois conjuntos de fones dependentes de contexto, um baseado nas classes fonéticas e outro baseado na configuração do trato vocal.

O levantamento dos trifones é feito através das transcrições fonéticas das locuções de treinamento, de modo que para cada teste, dependente de locutor, independente, temos um número diferente destes, devido à variação na pronúncia dos locutores envolvidos.

Um comentário deve ser feito acerca do arquivo de vocabulário do sistema, as palavras contidas neste vocabulário são as mesmas da base de dados.

Entretanto, as transcrições das mesmas foram feitas tentando prever como as pessoas poderiam pronunciá-las, o que nem sempre ocorre.

Desta forma, alguns trifones podem não constar da lista de sub-unidades treinadas.

Neste caso, estes trifones foram substituídos pelos fones independentes de contexto correspondentes.

Da mesma maneira, o primeiro e o último trifone da palavra foram substituídos pelos respectivos fones independentes de contexto pois, no caso do primeiro fone, não se conhece o contexto à esquerda e, no caso do último, não se conhece o contexto à direita.

Como no caso anterior, foi utilizado o algoritmo Level Building, com 15 níveis.

Para os testes com trifones baseados na configuração do trato vocal, foram utilizadas as classes definidas.

O número de trifones gerados é mostrado, e os resultados dos testes são apresentados.

Número de trifones baseados na configuração do trato vocal gerados a partir do subconjunto de locuções de treinamento.

Os testes para avaliação dos procedimentos para diminuição do tempo de processamento na etapa de busca (parada automática para o Level Building, e Viterbi Beam Search para o One Step) foram realizados apenas utilizando a base dependente de locutor, e os trifones gerados a partir da posição do trato vocal.

Os tempos de reconhecimento foram obtidos com base em um microcomputador PC com processador AMD-K6 350 MHz e 64 MB de memória RAM.

O primeiro passo foi estabelecer um tempo padrão de referência em relação ao qual seriam comparados os resultados.

Foi realizado um teste de reconhecimento utilizando 15 níveis de busca para ambos os algoritmos de busca (Level Building e One Step), e o tempo médio de reconhecimento foi adotado como o padrão de referência.

Para o Level Building foram testadas as duas idéias para a redução no tempo de processamento na etapa de busca, parada pela derivada da curva de evolução da curva de log-verossimilhança com o número de níveis e parada pela contagem de níveis em que ocorre queda na log-verossimilhança.

Para o primeiro procedimento foram dois testes, variando-se o limiar de parada.

Para o segundo procedimento também foram feitos dois testes, variando-se o número de níveis em que se observa queda no valor da log-verossimilhança.

Comparação do tempo médio de reconhecimento e taxa de erro de palavra para o procedimento de detecção automática do número de níveis baseado na derivada da curva de evolução da log-verossimilhança com o número de níveis.

Comparação do tempo médio de reconhecimento e taxa de erro de palavra para o procedimento de detecção automática do número de níveis de acordo com a contagem do número de níveis em que a verossimilhança cai.

O procedimento Beam Search foi testado variando-se o limiar de poda D e verificando-se o compromisso entre a taxa de erro de palavra e o tempo de processamento.

Os resultados referem-se a testes realizados com o algoritmo One Step com 15 níveis.

Comparação do tempo médio de reconhecimento e taxa de erro de palavra para vários valores do limiar de poda no algoritmo Viterbi Beam Search.

Verificação da influência da transcrição fonética das locuções de treinamento no desempenho do sistema.

A confecção de uma base de dados compreende dois processos, a gravação das locuções e a transcrição fonética das mesmas.

Esta última tarefa em especial é bastante penosa e demorada, pois é necessário ouvir com atenção as locuções, e com a ajuda de programas de visualização gráfica da forma de onda e do espectro do sinal, estabelecer exatamente o que foi pronunciado.

Quando se realiza esta tarefa para milhares de locutores, cada qual pronunciando centenas de frases, verifica-se que o trabalho e tempo necessários são bastante grandes.

Poderia-se aliviar a carga de trabalho necessária para a confecção da base de dados se no processo de transcrição fonética fosse adotada uma transcrição padrão para todas as locuções, isto é, dada uma frase a ser pronunciada por vários locutores, faz-se uma transcrição fonética para um dos locutores, e esta é adotada para todas as locuções daquela mesma frase.

Espera-se que com este procedimento, o desempenho do sistema caia, pois um fonema poderia estar sendo treinado com a locução de outro.

Entretanto a questão é quanto? Talvez a queda verificada no desempenho não seja tão grande, e com essa facilidade talvez seja possível construir bases de dados maiores, o que significa mais exemplos de treinamento e, consequentemente, sub-unidades fonéticas mais bem treinadas.

Este compromisso pode fazer com que, mesmo que as transcrições fonéticas padronizadas atrapalhem o treinamento, o maior número de exemplos de treinamento acabe por compensar a queda no desempenho provocada por este procedimento.

Para testar esta idéia, o sistema foi treinado tomando-se as transcrições fonéticas das locuções dos testes dependentes de locutor e associando-as às locuções dos 30 locutores de treinamento da base de dados independente de locutor.

Os testes foram realizados utilizando os trifones gerados a partir da configuração do trato vocal e comparados com os resultados obtidos.

O algoritmo de busca foi o Level Building, com 15 níveis.

Desempenho do sistema em função das transcrições fonéticas das locuções de treinamento.

Com os resultados obtidos nos testes da seção anterior, pôde-se verificar que uma transcrição fonética padronizada para todos os locutores não degrada de forma apreciável o desempenho do sistema.

A partir deste resultado, foi investigada a influência que teria a mesma idéia quando aplicada ao arquivo de vocabulário.

A vantagem deste procedimento é a diminuição do espaço de busca, ao invés de o sistema testar, a cada nível, 1633 palavras, testaria apenas 694, o que corresponde a uma redução significativa no número de cálculos a serem realizados.

Com isso, pode-se ganhar bastante em termos de tempo de processamento.

Comentou-se que foram construídos dois arquivos de vocabulário, um com várias versões de cada palavra, correspondendo às várias formas de locução, resultantes das diferenças de sotaque, coarticulações, e outro, com apenas uma versão para cada palavra.

Este segundo arquivo de vocabulário foi derivado do primeiro, escolhendo-se apenas uma variante e excluindo as demais, no caso das palavras com mais de uma versão.

O critério adotado para a escolha da versão de cada palavra foi a variante a ser selecionada é a que ocorreu com mais frequência nas locuções da base de dados.

Foram feitos testes com o arquivo de vocabulário simplificado, utlizando dois conjuntos de subunidades, os fones independentes de contexto e os trifones baseados na configuração do trato vocal.

Não foram realizados testes com os trifones baseados nas classes fonéticas porque estas subunidades não apresentaram bons resultados nos testes anteriores.

São apresentados os resultados dos testes realizados com o vocabulário simplificado, utilizando fones independentes de contexto.

Os resultados para testes com trifones baseados na configuração do trato vocal.

Finalmente, tem-se um quadro comparativo dos tempos de processamento utlizando os dois arquivos de vocabulário.

Para todos estes testes foi utilizado o algoritmo Level Building com 15 níveis, modelo de duração de palavras e modelo de linguagem de pares de palavras.

Foi realizada uma rodada final de testes para estabelecer qual seria o desempenho final do sistema, utilizando as técnicas que proporcionaram os maiores ganhos ao sistema, tanto em termos de taxa de acerto como de tempo de processamento.

Analisando os resultados de todos os testes anteriores, chega-se à conclusão que a configuração ideal deste sistema seria a seguinte (para a tarefa específica deste trabalho).

Algoritmo de busca, One Step com Viterbi Beam Search (limiar de poda D = 30).

Subunidades fonéticas, trifones baseados na configuração do trato vocal.

Arquivo de vocabulário, apenas uma versão de cada palavra.

Os demais parâmetros do sistema (parâmetros das locuções, quantização vetorial, modelo de duração de palavras e modelo de linguagem) permanecem os mesmos dos testes anteriores.

Neste capítulo foram apresentados os testes de avaliação do sistema implementado, utilizando a base de dados.

Foram avaliados o desempenho do sistema utilizando fones independentes de contexto e a influência do modo de operação do sistema (dependente de locutor, dependente de sexo e independente de locutor) na taxa de acertos.

A influência de dois tipos de fones dependentes de contexto na taxa de acertos, a influência dos procedimentos de diminuição dos cálculos necessários na etapa de busca no tempo de reconhecimento, a influência da transcrição fonética das frases de treinamento no desempenho do sistema, a influência do número de versões de cada palavra no arquivo de vocabulário, o desempenho final do sistema.

A seguir, cada um destes itens será analisado com maiores detalhes.

Desempenho do sistema utilizando fones independentes de contexto e influência do modo de operação na taxa de acertos de palavra.

Estes testes iniciais serviram para estabelecer uma base de comparação para as melhorias implementadas no sistema.

Pode-se verificar que as sub-unidades apresentam um número de exemplos de treinamento razoável, sendo que a sub-unidade com menos exemplos é o [S] com 132 ocorrências.

As taxas de acerto são razoavelmente boas, com um índice de aproximadamente 80% de acerto de palavra para o caso independente de locutor, chegando a quase 90% no caso dependente de locutor.

Esperava-se que para os testes com dependência de sexo, os resultados fossem ficar entre estes dois extremos, o que realmente aconteceu com os testes utilizando os locutores masculinos.

Entretanto, para os testes com locutores femininos, a taxa de acertos ficou abaixo dos testes realizados com independência de locutor.

Uma possível causa para este resultado é a presença de um locutor feminino para o qual o sistema apresentou um resultado muito ruim.

Para investigar este fato, levantouse inicialmente o número de erros de palavra cometidos pelo sistema para cada um dos locutores de teste.

Este levantamento inicial foi feito para os testes com independência de locutor.

Verifica-se que existem dois locutores para os quais o desempenho do sistema foi relativamente pior do que para os demais, f (feminino) e m (masculino).

Nos testes com dependência de sexo, este comportamento se repetiu, embora menos acentuadamente para os locutores masculinos.

Coincidentemente, os dois locutores pronunciaram o mesmo conjunto de frases (listas 5 a 8).

Este subconjunto de frases poderia apresentar maiores dificuldades para o reconhecimento, e portanto o problema não estaria nos locutores.

Para investigar este fato, levantou-se o histograma de erros para os testes com dependência de locutor.

Uma análise derruba a hipótese de que o subconunto de frases formado pelas listas 5 a 8 apresenta maiores dificuldades para o reconhecimento.

Os piores desempenhos foram observados nos subconjuntos formados pelas listas 17 a 20 no caso dos fones independentes de contexto, 9 a 12 para os trifones baseados nas classes fonéticas, e 13 a 16 para os trifones baseados na configuração do trato vocal.

Estes resultados parecem indicar que, de fato, a presença de um locutor feminino para o qual o desempenho do sistema foi bastante ruim, está polarizando os resultados.

Em relação ao modelo de linguagem, este mostrou ser bastante eficaz no direcionamento do processo de busca.

Entretanto, alguns erros não puderam ser evitados, o sistema não é capaz de discernir palavras que tenham a mesma transcrição fonética.

Deste modo, existem muitos erros de substituição entre as palavras a, à e há, por exemplo.

Podem também ocorrer erros de deleção de palavras curtas (geralmente artigos) quando precedem palavras que se iniciam com o mesmo fonema.

Por exemplo, as frases -a atriz- e -o ônibus- são geralmente reconhecidas como -atriz- e -ônibus-, com deleção dos artigos.

Isto mostra que a decodificação acústica está sendo bem feita, visto que no primeiro caso, todas as palavras são pronunciadas da mesma maneira e, no segundo caso, os locutores não se dão ao trabalho de pronunciar separadamente o artigo.

Estes erros poderiam ser corrigidos com o uso de gramáticas sensíveis a contexto ou com parsers.

As gramáticas sensíveis a contexto verificam a possibilidade de sequências de palavras de acordo com a função sintática das palavras dentro da frase, e os parsers, além disso, procuram verificar o significado semântico da frase reconhecida.

A aglutinação dos fones em classes fonéticas mostrou ser útil na redução do número total de trifones gerados.

Entretanto, esta aglutinação deve ser consistente para que os fones pertencentes a uma mesma classe tenham influências próximas nos fones adjacentes.

Os trifones baseados nas classes fonética não são consistentes, chegando a atrapalhar o desempenho do sistema nos testes com locutores femininos e dependente de locutor.

Para o caso dos trifones gerados a partir da configuração do trato vocal, notou-se uma melhora pequena, mas consistente em todos os resultados.

Esta melhora é um pouco mascarada pelo modelo de linguagem que, por ser bastante restritivo, pode fazer com que o desempenho dos testes com fones independentes de contexto tenham um resultado acima do que se poderia esperar.

Talvez a utilização de uma gramática gerada a partir de mais exemplos possa dar uma idéia melhor do ganho que se obtém com os modelos trifones.

Outro fator que pode estar prejudicando o desempenho dos trifones é o reduzido número de exemplos de treinamento para cada uma destas sub-unidades.

São mostrados gráficos em forma de histogramas onde são contados o número de subunidades fonéticas com menos de 10 exemplos de treinamento, o número de subunidades com menos de 20 exemplos, e assim por diante, para todos os testes realizados.

Pode-se notar que a grande maioria das sub-unidades tem menos de 20 exemplos de treinamento, enquanto que para os fones independentes de contexto, o número mínimo de exemplos de treinamento foi 132 para o fone [S].

Este fato é corroborado no processo de interpolação dos trifones com os fones independentes de contexto através do algoritmo Deleted Interpolation, na grande maioria dos casos, os fones independentes de contexto apresentaram uma verossimilhança maior do que os trifones, indicando claramente que estes modelos estão mal treinados.

Neste trabalho, foram gerados 1018 trifones baseados na configuração do trato vocal, para o caso independente do locutor.

Sistemas comerciais de fala contínua trabalham com pelo menos o dobro de trifones.

Desta forma, para se conseguir unidades razoavelmente bem treinadas torna-se necessária uma base de dados muitíssimo maior do que a que foi utilizada neste trabalho.

Outra alternativa seria gerar um conjunto menor de sub-unidades que pudesse ser treinada com menos exemplos.

Número de exemplos de treinamento para os trifones.

Os gráficos da coluna da esquerda referem-se aos trifones gerados através das classes fonéticas, e os da direita, aos trifones gerados a partir da configuração do trato vocal.

Em relação ao algoritmo Level Building foram propostos dois métodos para evitar a parada do processamento quando se atingem máximos locais, um baseado na derivada da curva de verossimilhança, e outro no número de níveis consecutivos em que se verifica queda no valor da verossimilhança.

O primeiro procedimento requer que a queda no valor da verossimilhança seja maior que um determinado limiar para sinalizar a parada do algoritmo.

Isto pode fazer com que o processamento continue indefinidamente, se o comportamento de queda da verossimilhança for muito suave.

De fato, nos testes realizados, notou-se que para um limiar pequeno (0,003), o sistema economiza tempo, mas também incorre em muitos erros de deleção e, aumentando-se este limiar (0,004), estes erros de deleção desaparecem mas, em compensação, o tempo de reconhecimento é quase o mesmo daquele obtido quando se usa um número fixo de níveis, o que indica que não houve quase nenhuma redução no número de cálculos.

O segundo procedimento realiza a parada do processamento utilizando uma informação que pode ser vista como sendo a tendência de queda do valor da verossimilhança, se este valor cai por l níveis consecutivos, é bem provável que o máximo global já tenha sido atingido, e o processo de busca pode ser encerrado.

Este parece ser um critério mais robusto para a detecção automática do número de níveis, e os resultados experimentais apresentados comprovam este fato.

Em relação a estes resultados foi observado um fato curioso para os testes realizados com l = 2, o número de erros diminuiu em relação aos testes de referência com 15 níveis.

Pode-se atribuir esta diminuição a um mero acaso, em algumas frases nas quais o sistema cometeu erros de inserção, o procedimento de parada automática pode ter interrompido a busca em um nível anterior ao máximo global, resultando assim em correções destes erros.

Desta forma, não se pode afirmar que este procedimento, além de diminuir o tempo de processamento, tem o poder de diminuir a taxa de erro de palavras.

Em termos de economia de cálculos, o segundo procedimento foi o que conseguiu uma maior redução no tempo de processamento, 23,7% contra 2,5% do primeiro, para um desempenho igual ao do Level Building com número fixo de níveis.

Para o algoritmo One Step foi testado o procedimento Beam Search e conseguiuse obter uma redução substancial no tempo de processamento sem prejudicar a taxa de acerto de palavras através da escolha de um limiar de poda conveniente.

De fato, observou-se uma redução de 46,6% no tempo de processamento, sem deteriorar a taxa de acertos com um limiar D = 30.

Se for permitida uma pequena queda de desempenho (0,31%), é possível obter uma redução de 51,5% no tempo de processamento, fazendo D = 25, o que parece ser uma escolha razoável.

Não se conseguiu uma redução no tempo de processamento de uma ordem de grandeza, como reportado na literatura, mas espera-se que com uma revisão na implementação do programa este valor venha a ser eventualmente atingido.

Um comentário deve ser feito acerca das implementações dos dois algoritmos de busca, sem utilizar nenhuma otimização, e para um mesmo número de níveis, o Level Building apresenta tempos de processamento menores do que o One Step (dados apresentados para o Level Building e para o One Step).

Isto se deve à forma de implementação dos códigos, mas não invalida os resultados e as análises.

Os resultados dos testes mostraram que a transcrição padronizada para todas as locuções não afeta de forma significativa o desempenho do sistema.

Isto pode ser uma informação valiosa quando se deseja construir grandes bases de dados envolvendo centenas ou milhares de locutores.

Entretanto, este resultado precisa ser visto com cuidado, visto que, novamente, os testes foram realizados com um modelo de linguagem bastante restritivo, o que poderia mascarar o efeito nocivo da transcrição padronizada no desempenho do sistema.

A utilização de um arquivo de vocabulário simplificado, com apenas uma versão para cada palavra, fez com que o tempo de processamento caísse mais de 50 % em todos os casos.

Ainda, a taxa de acertos subiu cerca de 1 % para todos os testes.

O primeiro resultado era esperado, uma vez que a diminuiçào do número de palavras no vocabulário corresponde a uma diminuição no espaço de busca e, consequentemente, num menor tempo de reconhecimento.

Já o segundo resultado parece ser estranho, uma vez que com menos versões de cada palavra teríamos um casamento pior das diferentes locuções de entrada com os modelos previstos no vocabulário.

Entretanto, é bom lembrar que as versões escolhidas para cada palavra foram as que ocorreram com maior frequência nas locuções da base de dados e, desta forma, pode-se considerar que este vocabulário foi otimizado para estes locutores.

O que ajudou bastante neste bom desempenho foi a uniformidade das pronúncias dos locutores de teste, já que a maioria nasceu no estado de São Paulo.

Provavelmente ao usarmos este mesmo arquivo de vocabulário com locutores de outras regiões o desempenho do sistema irá cair.

Este resultado vem comprovar as afirmações feitas, de que a tarefa de reconhecimento fica mais difícil à medida que o vocabulário aumenta.

Existe então um compromisso entre flexibilidade e perplexidade que deve ser tratado de forma adequada.

Uma solução possível seria criar vários arquivos de vocabulário, especializados em várias regiões do país.

Comparando os resultados dos primeiros testes, com os resultados dos testes finais, pode-se verificar que a taxa de acertos subiu entre 1,52 %, no caso dependente de locutor, e 3,34 %, para os testes realizados com locutores masculinos, e o tempo de processamento caiu quase 76 % para os testes com dependência de locutor.

Neste trabalho foram estudados alguns apectos da teoria referente ao reconhecimento de fala, com ênfase especial ao problema de reconhecimento de fala contínua com vocabulário extenso e independência do locutor.

Todas as etapas da construção de um sistema completo foram percorridas, desde o projeto e confecção de uma base de dados para treinamento e testes, passando por todas as ferramentas necessárias ao tratamento dos dados, até o desenvolvimento final do sistema.

Isto proporcionou uma boa compreensão das questões envolvidas em cada uma das etapas do desenvolvimento de tais sistemas, terminando em um software bastante amigável que será utilizado e ampliado em pesquisas futuras.

A tempo, o sistema implementado já está sendo utilizado por outros pesquisadores em trabalhos de reconhecimento de dígitos conectados e adaptação ao locutor.

Na confecção da base de dados pôde-se perceber que, em fala contínua, mesmo sendo produzida a partir da leitura de um texto, as coarticulações são bastante fortes.

Ainda, a variação de pronúncia e de ritmo de uma mesma palavra devido ao sotaque, nível de educação, e outros fatores é bastante grande.

Todos estes fatores contribuem para tornar mais difícil o problema de reconhecimento de fala contínua com independência do locutor.

Neste sentido, as técnicas de adaptação ao locutor são de grande importância no sentido de minimizar a amplitude destas variações para os sistemas de reconhecimento.

Os locutores da base de dados foram agrupados de diferentes maneiras de modo a realizar os seguintes testes, independente de locutor, somente locutores femininos, somente locutores masculinos e dependente de locutor.

Estes testes têm o objetivo de investigar a influência do conjunto de locutores no desempenho do sistema.

Verificou-se que quando o sistema é utilizado no modo independente de locutor, o seu desempenho cai em relação ao modo dependente do locutor, o que é esperado.

Para os testes com dependência de sexo, os testes com locutores masculinos apresentaram os resultados esperados, sendo o desempenho do sistema situado em uma posição intermediária entre aquele observado no modo independente do locutor e no dependente de locutor.

A surpresa ficou para os testes com locutores femininos, com um desempenho pior do que aquele observado no modo independente do locutor.

Esta discrepância nos resultados parece ter sido causada pela presença de um locutor com o qual o desempenho do sistema foi bastante ruim, distorcendo os resultados.

Como a base de dados não é muito grande, um desempenho ruim para um dos locutores influi de forma significativa nos resultados finais.

Em relação às sub-unidades acústicas, foram avaliados três conjuntos, fones independentes de contexto, trifones baseados nas classes fonéticas, e trifones baseados na configuração do trato vocal.

Na geração dos modelos trifones, os fones independentes de contexto foram utilizados para inicialização.

Após o treinamento, os modelos dos trifones foram mesclados com os modelos dos fones independentes de contexto correspondentes utilizando o procedimento Deleted Interpolation.

Não foram utilizados os trifones da forma usual pois o número destes seria muito grande e não haveria dados de treinamento suficientes.

Agrupando-se os fones em classes, procurou-se diminuir o número de trifones, tentando manter a consistência, que é a característica interessante destas sub-unidades.

Inicialmente foram feitos testes com os fones independentes de contexto para estabelecer um desempenho padrão para o sistema.

Os testes com trifones gerados a partir das classes fonéticas mostraram um ligeiro aumento de desempenho para os modos independente de locutor e para os locutores masculinos, mas apresentaram um resultado pior do que os fones independentes de contexto para os locutores femininos e para os testes com dependência de locutor.

Já os testes com os trifones gerados a partir da configuração do trato vocal apresentaram uma melhora do desempenho em todos os casos.

Desta forma, pode-se concluir que os trifones baseados na configuração do trato vocal são unidades consistentes, enquanto que a divisão dos fones nas respectivas classes fonéticas parece não ser uma escolha adequada.

O aumento na taxa de acertos com o uso dos trifones foi bastante pequeno, o que, à primeira vista, não justificaria o seu uso, já que as necessidades de armazenamento aumentam consideravelmente, são 1018 trifones contra 36 fones independentes de contexto.

Entretanto, uma análise mais profunda revela o seguinte, o modelo de linguagem de pares de palavras utilizado é bastante restritivos, o que poderia estar elevando de forma exagerada o desempenho dos fones independentes de contexto, mascarando o resultado, o número de exemplos de treinamento para os trifones é muito pequeno, resultando em sub-unidades extremamente mal treinadas.

No procedimento Deleted Interpolation este fato fica bastante claro pois, na maioria dos casos, os fones independentes de contexto obtiveram um desempenho melhor do que os trifones.

Com estas considerações, talvez o uso de um modelo de linguagem menos restritivo e uma base de dados maior, que proporcione um treinamento adequado aos trifones, possam mudar este quadro.

Foram propostos dois métodos para diminuir o tempo de processamento para o reconhecimento utilizando o algoritmo Level Building.

A idéia destes métodos é tentar determinar de forma automática o número de níveis de busca necessários para reconhecer cada locução.

O primeiro método baseia-se na informação fornecida pela derivada da curva de verossimilhança e, o segundo, no número de níveis consecutivos em que o valor da verossimilhança cai.

O primeiro método conseguiu uma redução de 2,5% no tempo de processamento e, o segundo, 23,7%, sem queda no desempenho.

Foi também testada a técnica Beam Search para o algoritmo One Step, conseguindo-se uma redução de 46,6% no tempo de processamento, sem alterar a taxa de acertos, e de 51,5% com uma deterioração de apenas 0,31% na taxa de acertos de palavra.

Também foi verificada a influência da precisão da transcrição fonética das locuções de treinamento no desempenho do sistema.

Estes mostraram uma deterioração muito pequena quando se adota uma transcrição fonética padrão todas as pessoas, o que parece indicar que este procedimento possa ser adotado sem maiores problemas.

Novamente, o uso de um modelo de linguagem bastante restritivo pode estar mascarando estes resultados, e o efeito nocivo deste procedimento simplificado pode ser um pouco maior.

De qualquer forma, este procedimento é adotado em muitos sistemas comerciais IBM por exemplo uma vez que uma transcrição fonética criteriosa de cada locutor é uma atividade extremamente tediosa e consome um tempo bastante grande.

A possibilidade de se conseguir bases de dados maiores para o treinamento do sistema sem a preocupação de uma transcrição personalizada para cada locutor é um fator que compensa em excesso a pequena degradação no desempenho provocada pela transcrição fonética padronizada.

A falta de grandes bases de dados em português para o treinamento e avaliação parece ser o grande entrave para um desenvolvimento mais rápido e consistente das pesquisas em reconhecimento de fala no Brasil.

Infelizmente este trabalho não pode ser feito por uma pessoa ou instituição isolada, mas requer um grande esforço conjunto de órgãos governamentais, iniciativa privada e comunidade científica.

De fato, nos EUA e na Europa, houve um grande avanço na tecnologia de voz após a criação de grandes bases de dados, o que permitiu comparar os resultados de forma consistente, e determinar quais idéias são realmente boas, evitando duplicação de esforços.

Aplicando a idéia de uma transcrição fonética padronizada ao arquivo de vocabulário, foi possível reduzir o universo de busca de 1633 palavras para apenas 694, resultando em uma diminuição bastante significativa no tempo de processamento.

Como as versões escolhidas para representar cada palavra foram selecionadas a partir das realizações mais comuns observadas na base de dados, conseguiu-se até uma melhora no desempenho do sistema, um fato que não era esperado, mas que pode ser explicado pela menor perplexidade imposta ao sistema de reconhecimento, aliada a modelos que correspondem de fato às locuções apresentadas para o reconhecimento.

Espera-se entretanto, que se o sistema for treinado com locutores provenientes de outras regiões, e portanto com formas de pronúncia diferentes, o desempenho do sistema venha a cair.

A construçào de arquivos de vocabulário diferentes para cada região do país parece ser uma alternativa viável para resolver este problema.

Utilizando todas as otimizações apresentadas neste trabalho.

A configuração ideal para este sistema seria (para o reconhecimento das frases desta base de dados).

O algoritmo de busca, One Step com Viterbi Beam Search e limiar de poda D = 30, modelo de duração de palavras, modelo de linguagem de pares de palavras, parâmetros mel-cepstrais, com respectivos parâmetros delta e delta-delta, vocabulário simplificado, com apenas uma versão para cada palavra, subunidades fonéticas, trifones baseados na configuração do trato vocal.

Com estas configurações, o sistema atingiu uma taxa de acertos de 81,24 % no modo independente de locutor, com um tempo médio de reconhecimento por volta de 01,30 minutos em uma máquina com processador AMD-K6 350 MHz com 64 MB de memória RAM.

Como sugestões para trabalhos futuros, pode-se citar o estudo e desenvolvimento de um sistema de reconhecimento de fala baseado em Modelos de Markov Contínuos, e o treinamento destes baseado em critérios discriminativos.

Também poderiam ser estudados algoritmos de busca mais velozes como o Stack Decoder e o algoritmo Herrmann-Ney.

Modelos de linguagem mais avançados tais como gramáticas dependentes de contexto, e métodos de adaptação ao locutor também contribuiriam para a melhoria do desempenho final do sistema