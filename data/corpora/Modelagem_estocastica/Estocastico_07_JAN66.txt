A teoria de Markov tem importante aplicação nas áreas de dinâmica de populações, genética, meteorologia, processos de nascimento e morte e teoria de filas.

Problemas de consumidores que consomem determinado produto e mudam de fornecedor devido a propaganda, preços, conveniência, satisfação, os que envolvem o modelo presa-predador, aqueles no qual o comportamento futuro depende exclusivamente do presente e não do passado ou, ainda, modelo nascimento-morte podem ser caracterizados por meio da Teoria de Markov, complementados da propriedade que somente é possível mudar o estado de um sistema em um ponto.

Em geral, há fenômenos que têm período de evolução definido, ressaltando-se que a grande contribuição da teoria consiste em calcular os valores nos quais o sistema ficará em equilíbrio.

É um sistema essencialmente estocástico (probabilístico), ou seja, uma abstração matemática de um processo empírico cujo desenvolvimento é governado pelas leis da probabilidade, sendo muito adequado para resolução, por exemplo, de problemas de Teoria de Filas.

Essa teoria possibilita a resolução de problemas do tipo Fila cíclica, na qual um conjunto de filas seqüenciais serve a uma população de tamanho fixo, onde a unidade atendida retorna e espera na ordem de seqüencial até chegar novamente sua vez.

Sistema de filas bloqueadas, no qual um sistema de atendimentos seriados terá a fase seguinte iniciada se, e somente se, a anterior for concluída.

Na nossa rotina diária vivenciamos diversos exemplos em que o conceito de cadeia de Markov pode ser aplicado o tempo em Brasília estar chuvoso ou seco.

O amigo que pode ser fumante ou não-fumante.

Eu freqüentar o curso de Tratamento de Imagem ou não.

Vivo em área rural ou urbana.

Pertenço à classe baixa, média ou alta.

Compro carro da marca Chevrolet, Ford ou Fiat.

Seguem alguns exemplos de aplicação prática da teoria, na Meteorologia podemos admitir que o tempo em Brasília é chuvoso ou seco.

Como resultado do número de registros existentes no serviço de meteorologia, determinou-se que a probabilidade de se ter um dia chuvoso logo após um dia seco é de 1/3, e a probabilidade de se ter um dia chuvoso logo após um dia chuvoso é de ½.

Se S representar um dia chuvoso e C um dia chuvoso, então podemos montar a matriz de transição da cadeia de Markov.

Na Psicologia behaviorista coloca-se um rato cada dia em uma gaiola com duas portas, A e B.

O rato pode passar pela porta A, onde recebe um choque elétrico, ou pela porta B, onde recebe comida.

No início do experimento, em uma segunda-feira, o rato tem a mesma probabilidade de escolher a porta A ou a B.

Depois de passar pela porta A e receber um choque, a probabilidade de usar a mesma porta no próximo dia é de 0,3.

Depois de passar pela porta B e receber comida, a probabilidade de usar a mesma porta no próximo dia é de 0,6.

Pela matriz de transição pode-se calcular qual a probabilidade de o rato passar pela porta A na quinta-feira.

Na Administração o departamento de assinaturas do Correio Braziliense manda uma carta para uma lista de endereços convidando os destinatários a preencher e enviar um pedido de assinatura.

Alguns já assinam o jornal, enquanto outros não.

Dessa lista de endereços, 60% dos que já são assinantes enviam o pedido, renovando suas assinaturas, enquanto 25% dos que não são assinantes enviam o pedido, fazendo uma assinatura nova.

Se na última carta enviada 40% dos que a receberam enviaram o pedido de assinatura, pode-se calcular o percentual esperado de pedidos para a carta atual.

Na Sociologia por meio de estatísticas pode-se verificar a atividade que meu filho irá exercer ao se tornar adulto, pois ela depende da minha atividade através da matriz de transição do processo de Markov.

Assim, pode-se calcular a probabilidade do meu neto tornar-se um profissional da minha atividade.

Na Genética, considere uma planta do cerrado que pode ter flores vermelhas (V), cor-de-rosa ou brancas, dependendo dos genótipos.

Ao cruzar cada um desses genótipos com um genótipo específico gera-se uma matriz de transição.

Pode-se calcular quando o processo atinge o equilíbrio.

Nos Transportes o metrô de Brasília começou a funcionar.

As autoridades fizeram um estudo que previu o percentual de pessoas que usarão o sistema ou que continuarão a dirigir seus automóveis, por meio de uma matriz de transição.

Supondo que a população da área permanece constante e que inicialmente 30% das pessoas usam o transporte de massa e 70% usam seus carros, pode-se prever qual a porcentagem de pessoas que usarão o metro depois de t anos.

No Tratamento de Imagens, a estatística espacial, covariância, dos modelos unidimensionais contínuos utiliza a fórmula C = C(0)e-ah, derivada do modelo exponencial, que é resolvida pela aplicação do processo de cadeia de Markov.

Como podemos verificar, as aplicações são ricas de exemplos e justificam a importância da teoria.

Do ponto de vista da teoria matemática probabilística, o processo estocástico é mais bem definido por uma família de variáveis randômicas { X(t), t Î T}, sobre algum conjunto de valores ou parâmetros do espaço T.

T é também chamado de período de tempo e X(t) denota as observações no tempo t.

Dependendo da natureza do período do tempo, o processo é classificado como um processo a parâmetro discreto ou a parâmetro contínuo, como se segue.

Se T é uma seqüência infinita, por exemplo T = {0, ±1, ±2, ±3} ou T { 0, 1, 2, 3 }, então o processo estocástico { X(t), t Î T} é dito processo a parâmetro discreto definido no conjunto de índices T.

Se T é um intervalo de combinações algébricas, então o processo estocástico {X(t), t Î T} é chamado de um processo a parâmetros contínuos no conjunto de índices T.

O processo a parâmetros discretos { X(t), t = 0, 1, 2, 3} ou o processo estocástico a parâmetros contínuos { X(t), t > 0} é chamado de PROCESSO DE MARKOV se, para algum conjunto de n pontos do tempo t1 < t2 < tn de um conjunto de valores ou período de tempo do processo, a distribuição condicional de X(tn), que fornece valores de X(t1), X(t2), X(t3), X(tn1), depende somente de X(tn1), ou do valor imediatamente precedente.

Mais precisamente, para qualquer número real x1, x2, x3, xnPr {Xtn £ xn | X(t1) = x1, X(tn1) = xn-1} = Pr {Xtn) £ xn | X(tn1) = xn1}.

Na linguagem coloquial diz-se que, dada a condição -presente- de um processo, a condição -futura- independe da condição -passada-.

Assim, o processo de Markov é classificado de acordo com a natureza do conjunto de valores do processo (parâmetros discretos ou contínuos) e a natureza dos estados do processo.

Um número real x é o estado de um processo estocástico { X(t), t Î T} se houver um tempo t no conjunto T tal que a Pr { x - h < X(t) < x + h} é positiva para qualquer h > 0).

O conjunto dos estados possíveis constitui o espaço dos estados do processo.

Se o espaço dos estados é discreto, o processo de Markov é chamado de Cadeia de Markov.

O parâmetro discreto do processo de Markov com o espaço discreto é chamado de parâmetro discreto da Cadeia de Markov.

A cadeia de Markov é finita se o espaço dos estados for finito, caso contrário, é não numerável ou infinita.

Desde que o sistema seja observado como um conjunto de pontos temporais discretos, o conjunto de observações sucessivas é chamado de X(1), X(2), X(3), X(n), assume-se que X(n) é uma variável randômica cujos valores representam o estado do sistema no n-ésimo ponto temporal.

A seqüência X(n) é chamada de cadeia se for assumido que existem somente números ou estados numeráveis de estados nos quais o sistema é definido em qualquer ponto dado no período de tempo.

A seqüência X(n) é então uma cadeia de Markov se cada variável randômica X(n) é discreta e obedecer às seguintes condições.

Para qualquer m > 2 e qualquer conjunto de m pontos, tal que n1 < n2 < n3, nn a distribuição condicional de X(nn) dada pelos valores, X(n1), X(n2), X(nm1) depende somente de X(nn1).

Valor imediatamente anterior, tal que Pr {Xnm £ xnn | X = xn1, X(nn1) = xnn1} = Pr {Xnn) £ xnn | X(nn1) = xnn1} O parâmetro contínuo do processo de Markov com espaço de estado discreto é chamado de cadeia de Markov de parâmetro contínuo, enquanto que para o espaço de estado contínuo e parâmetro de espaço discreto, o processo é chamado de processo de Markov a parâmetro discreto.

Se ambos, o espaço de estado e o espaço de parâmetro são contínuos, então o processo de Markov é chamado de processo de Markov a parâmetro contínuo.

Considere a seqüência de variáveis randômicas { X(n)m n = 0, 1, 2 | X(n) = 0, 1, 2} a qual forma a cadeia de Markov com parâmetro de espaço discreto, isto é, para todo nPr {Xn = j | X(0) = i0, X(1) = i1, X(n1) = in1 } = Pr {Xn = j | X(n1) = in1}, se o valor da variável randômica Xn é j, então o sistema está no estado j depois de n passos ou transições.

A probabilidade condicional Pr {Xn = j | X(n1) = in1} é chamada de probabilidade transacional de passo simples ou probabilidade de transição.

Se estas probabilidades são independentes de n, então a cadeia é dita homogênea e a probabilidade Pr {Xn =j | X(n1) = in1} pode ser escrita como P0.

A matriz formada por P0 na (i,j) localização é conhecida como matriz de transição ou matriz de cadeia.

Para cadeias homogêneas, as probabilidades transacionais de m-ésimo passo, Pr {Xn+m = j | X(n) = i } = Pij (m) são também independentes de n.

A probabilidade incondicional do estado j no n-ésimo passo é escrito como Pr {Xn = j } = P0 (n), com a distribuição inicial dada por Pj (0) das leis básicas da probabilidade, uma pode ser facilmente mostrada como sendo a matriz formada pelos elementos P0 (m), chamada de P(m) pela simples multiplicação de P (mk) por P0 (k) para qualquer valor de k, 0< k < m.

Esta matriz é equivalente à conhecida matriz de equações Chapman-Kolmogorov para o processo de Markov.

Dois estados i e j, são comunicáveis (i « j) se i é acessível de j e j é acessível de i.

A cadeia é chamada de irredutível se todos os estados se comunicam, isto é, se existe um n tal que Pij (0) > 0 para qualquer par (i, j).

O período de retorno do estado k da cadeia é definida como o MDC do conjunto de inteiros {n} para o qual Pkk (0) > 0.

O estado é dito aperiódico se o MDC for 1, isto é, se tem período igual a 1.

A cadeia é chamada de aperiódica se cada um dos estados for aperiódico.

Define-se fjj (n) como a probabilidade que a cadeia comece no estado j retornando para o primeiro tempo para j em n iterações.

Se fjj = 1, então é chamado de estado recorrente.

Se fjj < 1, j é chamado de estado transiente.

É a média do tempo de recorrência.

Se mij < ¥ então j é conhecido como estado recorrente positivo, enquanto que se mij = ¥, diz-se que é um estado recorrente nulo.

Define-se como a probabilidade da primeira ocorrência da passagem do estado i para o estado j no exato passo n.

O valor esperado da seqüência { f(n) ij, n = 1, 2} da primeira passagem da probabilidade para o par fixo (i,j), i ¹ j é chamado de mij e significa o tempo médio da primeira ocorrência.

Se i = j, então mij torna-se o tempo médio de recorrência do estado i.

Assim, podemos verificar os teoremas mais importantes pertencentes à cadeia de Markov e que formam o embasamento da teoria.

Teorema 1.

Seja a cadeia C irredutível.

Então C é recorrente ou transiente, isto é, ou todos os estados são recorrentes ou todos são não recorrentes.

Teorema 2.

Seja C irredutível, e seja k uma constante fixa do estado em C.

Então C é recorrente se, e somente se, para cada estado j, j ¹ k, fjk = 1.

Teorema 3.

A cadeia é recorrente se existe uma solução { yi } da desigualdade.

Teorema 4.

Uma cadeia de Markov irredutível é transiente se, e somente se, existe uma solução de fronteira não constante para o conjunto de equações.

Teorema 5.

Seja k um estado fixo e irredutível da cadeia recorrente.

Então o conjunto dos primeiros tempos médios, (mjk, j¹k), é a única que satisfaz o sistema de equações.

Teorema 6.

Se C é irredutível e uma cadeia recorrente, então todos os estados são positivos ou todos são nulos.

Além disso, considera-se que uma cadeia aperiódica irredutível e recorrente positiva é chamada de ergódica.

A distribuição de probabilidade (pj, j Î C) é chamada de distribuição estacionária.

Uma cadeia de Markov é conhecida por possuir uma longa vida ou distribuição limitada se existir uma distribuição de probabilidade { pj, j Î C } se tiver a propriedade.

Teorema 7.

Em uma cadeia irredutível e aperiódica, a probabilidade limite sempre existirá e será independente da distribuição inicial dos estados.

Se os estados são transientes ou recorrentes nulos, então pj = 0 para todo j, e existirá uma distribuição estacionária.

Se, contudo, todos os estados são recorrentes positivos, então pj > 0, para todo j, e { pj } é uma distribuição de probabilidade, com pj = 1/ mij.

A distribuição limite é solução única do sistema estacionário das equações.

Destas reflexões acerca da teoria de cadeias de Markov, podemos observar que problemas, cujos modelos recaiam no processo de estado onde o anterior independe do posterior, são resolvidos de maneira satisfatória utilizando-se as cadeias de Markov.

Encaixam-se bem na resolução de problemas de Teoria de Filas onde os serviços são dependentes dos estados, ou seja, a taxa média de atendimento de um serviço depende do estado do sistema.

Um sistema no qual existem dois estados, um rápido e outro lento, começa a trabalhar de modo lento até que existam k usuários no sistema, a partir deste momento ele passa a trabalhar mais rapidamente até que restem usuários menores que k.

A Teoria de Filas é assunto da disciplina Fundamentos de Pesquisa Operacional oferecida, atualmente, aos alunos do 6º semestre do curso de Tecnologia de Processamento de Dados, do Departamento de Informática da UPIS.

Um processo estocástico é uma abstração matemática de um processo empírico governado por leis probabilísticas.

Do ponto de vista da teoria matemática da Probabilidade, define-se melhor o processo estocástico como sendo uma família de variáveis randômicas.

Se em determinada situação uma condição atual de um processo independe totalmente da condição futura, então aplica-se o processo de Markov, que pode ser classificado pela natureza do índice do conjunto do processo e pela natureza estática do processo.

