{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xmltodict\n",
    "import random\n",
    "import pickle\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from pprint import pprint\n",
    "from utils.tui import Progress\n",
    "from utils.lexical import Preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "BASE_DIR = '../data/corpora/trainset'\n",
    "TEST_PERCENT = 0.2\n",
    "PP = Preprocessing()\n",
    "LOAD_CORPUS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count():\n",
    "    count = 0\n",
    "    for product in os.listdir(BASE_DIR):\n",
    "        corpus = {}\n",
    "\n",
    "        product_path = '{}/{}'.format(BASE_DIR, product)\n",
    "        for rank in os.listdir(product_path):  \n",
    "            rank_path = '{}/{}'.format(product_path, rank)\n",
    "            fls = os.listdir(rank_path)\n",
    "            fls = [ x for x in fls if '.xml' in x ]\n",
    "            count += len(fls)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpora():\n",
    "    corpora_kabum = {}\n",
    "    errors = []\n",
    "    count = get_count()\n",
    "    P = Progress(count, '')\n",
    "    for product in os.listdir(BASE_DIR):\n",
    "        corpus = {}\n",
    "\n",
    "        product_path = '{}/{}'.format(BASE_DIR, product)\n",
    "        for rank in os.listdir(product_path):  \n",
    "            rank_path = '{}/{}'.format(product_path, rank)\n",
    "            fls = os.listdir(rank_path)\n",
    "            fls = [ x for x in fls if '.xml' in x ]\n",
    "\n",
    "            reviews = []\n",
    "            for fl_name in fls:\n",
    "                fl_path = '{}/{}'.format(rank_path, fl_name)\n",
    "                with open(fl_path) as fl_:\n",
    "                    try:\n",
    "                        r = xmltodict.parse(fl_.read())['review']\n",
    "                        if not r['opinion']:\n",
    "                            raise Exception(fl_path)\n",
    "                        reviews.append(r)\n",
    "                    except:\n",
    "                        errors.append(fl_path)\n",
    "                P.progressStep()\n",
    "            corpus[rank] = reviews\n",
    "        corpora_kabum[product] = corpus\n",
    "    return (corpora_kabum, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(corpora_kabum):\n",
    "    dataset = dict(test=[], category=[], score=[], review=[])\n",
    "    for cat in corpora_kabum.keys():\n",
    "        for stars in corpora_kabum[cat].keys():\n",
    "            for review in corpora_kabum[cat][stars]:\n",
    "                dataset['test'].append(random.choices([0,1], weights=[0.8, 0.2],k=1)[0])\n",
    "                dataset['category'].append(review['category']['@value'])\n",
    "                dataset['score'].append(float(review['stars']['@value']))\n",
    "                dataset['review'].append(review['opinion'])\n",
    "    dataframe = pd.DataFrame(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_PATH = 'df_kabum.pkl.gz'\n",
    "\n",
    "def save(data, path):\n",
    "    with gzip.open(path, 'wb') as f:\n",
    "        f.write(pickle.dumps(data))\n",
    "\n",
    "def load(path):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        return pickle.loads(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(text):\n",
    "    text = PP.lowercase(text)\n",
    "    text = PP.remove_punctuation(text)\n",
    "    tokens = PP.tokenize_words(text)\n",
    "    return tokens\n",
    "\n",
    "def normalize_review(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[pra, quem, quer, emagrecer, sem, sair, de, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[o, aparelho, é, muito, instável, com, pessoas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[comprei, sem, muita, certeza, da, resistencia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[ótimo, produtoadorei, o, design, exatamente, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[uso, o, de, minha, vizinha, e, acho, muito, b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test  score                                             tokens\n",
       "0     0    3.0  [pra, quem, quer, emagrecer, sem, sair, de, ca...\n",
       "1     1    3.0  [o, aparelho, é, muito, instável, com, pessoas...\n",
       "2     0    5.0  [comprei, sem, muita, certeza, da, resistencia...\n",
       "3     0    5.0  [ótimo, produtoadorei, o, design, exatamente, ...\n",
       "4     0    5.0  [uso, o, de, minha, vizinha, e, acho, muito, b..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c = get_corpora()\n",
    "# df = create_dataframe(c)\n",
    "# df['tokens'] = df.review.apply(tokenize_review)\n",
    "# df['normalized'] = df['review'].apply(normalize_review)\n",
    "# save(df, DF_PATH)\n",
    "\n",
    "df = load(DF_PATH)\n",
    "df['tokens'] = df['review'].apply(tokenize_review)\n",
    "a = df[['test','score','tokens']]\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k4t0mono/.local/share/virtualenvs/ipln-LePBjxLu/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('cbow_s50.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize'] = df.review.apply(tokenize_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[['score','tokenize','test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "for i in range(100):\n",
    "    l = []\n",
    "    for w in a.iloc[i].tokenize:\n",
    "        try:\n",
    "            l.append(model[w])\n",
    "        except:\n",
    "            pass\n",
    "    train_x.append(numpy.average(l, axis=0))\n",
    "    train_y.append(a.iloc[i].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.]\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict([train_x[5]]))\n",
    "print(train_y[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l\n",
    "for w in a.iloc[].tokenize:\n",
    "    l.append(model[w])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
